{
    "paper_title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation",
    "authors": [
        "Liao Qu",
        "Huichao Zhang",
        "Yiheng Liu",
        "Xu Wang",
        "Yi Jiang",
        "Yiming Gao",
        "Hu Ye",
        "Daniel K. Du",
        "Zehuan Yuan",
        "Xinglong Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 9 6 0 3 0 . 2 1 4 2 : r TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation Liao Qu*, Huichao Zhang*, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, Xinglong Wu ByteDance https://byteflow-ai.github.io/TokenFlow/"
        },
        {
            "title": "Abstract",
            "content": "We present TokenFlow, novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlows superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving 7.2% average improvement. For image reconstruction, we achieve strong FID score of 0.63 at 384384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with GenEval score of 0.55 at 256256 resolution, achieving comparable results to SDXL. 1. Introduction Large Language Models (LLMs) have revolutionized natural language processing through their unified autoregressive framework, demonstrating remarkable capabilities across diverse tasks [1, 2]. However, in the multimodal domain of vision and language, fundamental divide persists *Equal contribution project lead Figure 1. Multimodal Understanding Results with TokenFlow. We demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving 7.2% average improvement. between perception and generation paradigms. Current approaches address them through distinct architectures: multimodal understanding models leverage vision encoders and projection layers to align visual representations with pretrained LLMs [29, 52], while visual generation relies on either diffusion-based methods [39, 41] or discrete image tokens for autoregressive generation [38, 44, 51, 65]. This divergence motivates the pursuit of unified approaches capable of both understanding and generation. The advent of GPT-4o [59] has greatly boosted interest in developing more generalist multimodal models. Early efforts to unify perception and generation capabilities [27, 46] have primarily focused on equipping LLMs with the power of diffusion models. However, these ap1 Figure 2. Visual Generation Results with TokenFlow. We present diverse 256256 results across various styles, subjects, and scenarios. proaches introduce substantial architectural complexity and computational overhead, highlighting the need for more elegant unified solution. Recent efforts have explored one promising direction: using single transformer architecture to unify visual and textual information within the next-token prediction framework [48, 55]. This approach relies on VQ encoders to convert visual inputs into discrete tokens that can be processed alongside text, offering potentially simpler and more efficient framework. By treating both modalities as sequences of discrete tokens, this framework enables end-to-end training within single architecture. However, fundamental challenge exists in such unified approaches. Multimodal understanding demands rich semantic representations to support complex reasoning, while visual generation, on the other hand, requires precise encoding of spatial structure and textural details. Current methods predominantly employ reconstruction-targeted VQ encoders [13, 73], which are primarily optimized for reconstruction fidelity. While this optimization makes them wellsuited for generation tasks, it potentially limits their ability to capture the high-level semantic features crucial for understanding tasks. While Janus [57] attempts to address this conflict by employing separate encoders for understanding and generation tasks, this increases model complexity without fundamentally resolving the underlying representation disparity. These limitations underscore critical gap in the field: the absence of unified visual encoding mechanism that can effectively serve both perception and generation objectives. This motivates our central research question: Can one single image tokenizer derive representations suitable for both multimodal understanding and generation? To address this challenge, we propose TokenFlow, novel unified image tokenizer that bridges the gap between understanding and generation through unique dual-flow design. The key insight is to decouple the learning of semantic and pixel-level features while maintaining their alignment through shared index mapping. By mapping patches with both semantic and pixel-level similarities to the quantized features can be directly identical indices, applied to both autoregressive visual generation and multimodal understanding. Unlike concurrent approach that constrains different feature levels within single codebook [60], TokenFlows dual-codebook design enables specialized learning while maintaining cross-level correlations through shared indices. This innovation allows simultaneous access to both semantic and pixel-level representations without compromising either aspect. Specifically, TokenFlow adopts dual-encoder architecture coupled with corresponding specialized codebooks. The semantic encoder, learned from CLIP-style teacher, provides strong semantic priors, while the pixel encoder captures detailed visual information. The extracted features are then quantized by minimizing the weighted summation of semantic and pixellevel distances, creating joint representation space. Our framework exhibits remarkable scalability, maintaining exceptional codebook utilization (95%+) even with large-scale codebooks of over 130K entries - substantially advancing beyond prior approaches [13] in both capacity and efficiency. TokenFlow also achieves strong FID score of 0.63 at 384384 resolution. For text-to-image synthesis, we establish new state-of-the-art GenEval score of 0.55 at 256256 resolution in the autoregressive paradigm 2 while requiring significantly fewer sampling steps compared to existing methods like EMU3 [55] and LlamaGen [44]. On multimodal understanding benchmarks, TokenFlow achieves new state-of-the-art performance with minimal training overhead, surpassing LLaVA-1.5 13B by 7.2% on average - for the first time discrete visual inputs can outperform this strong baseline. These results validate TokenFlows effectiveness as unified visual tokenizer that bridges the long-standing gap between understanding and generation tasks. 2. Related Work 2.1. Tokenization for Visual Generation. Vector quantized (VQ) image tokenizers have played crucial role in recent advancements in autoregressive image generation [28, 34, 44, 51, 65]. [54] proposed the VQVAE, quantizing patch-level features using the nearest codebook entry, with the codebook learned with the encoder-decoder structure through reconstruction loss. VQVAE-2 [40] advanced this framework through exponential moving average updates and hierarchical multi-scale approach. VQGAN [13] further enhanced the architecture by incorporating adversarial and perceptual losses, yielding more precise and detailed representations. Recent advances in VQ tokenizers have focused on three main directions: improving reconstruction fidelity and generation quality [21, 64, 73], enhancing codebook utilization [64, 70, 76], and exploring novel architectures such as the multi-scale VQVAE [25, 51] for next-scale prediction of images. While these methods effectively preserve local details after quantization, they often struggle to capture semantic-level information, limiting their effectiveness in autoregressive multi-modal image understanding tasks. Our proposed TokenFlow addresses this limitation by introducing dual codebooks with shared mapping, achieving state-of-the-art performance in both autoregressive generation and multimodal understanding. 2.2. Tokenization for Unified Multimodal Understanding and Generation Recent efforts have emerged to bridge the gap between multimodal understanding and generation [23, 48, 55, 57, 60, 62]. Approaches like Chameleon [48], EMU3 [55] and Show-o [62] employ VQ tokenizers [13, 66, 73] to encode images for both tasks. However, these methods typically require multimodal training from scratch and often suffer performance degradation in visual perception tasks due to limited semantic representation in their tokenized features. SEED-LLaMA [23] introduced novel VQ tokenizer incorporating high-level semantics for understanding and utilize SD [41] as generation decoder. Janus [57] attempted to address the modality gap by employing separate tokenizers for understanding [69] and generation [44], though this leads to increased model complexity without fundamentally resolving the underlying challenge. Concurrent work [60] proposed unified vision tower aligning discrete visual features with text during pre-training. However, their approach constrains low-level and high-level representations within single flow, limiting the upper bound of downstream performance. In contrast, our work posits that the key to unifying understanding and generation lies in learning universal mapping. By defining dual codebooks with shared mapping, TokenFlow enables flexible combinations of low and high-level features, resulting in superior performance across all downstream tasks. 3. Method 3.1. Motivation Table 1. Comparison of various visual encoders on multimodal understanding [14, 23, 43] within the LLaVA-1.5 framework. VQKD is distilled from CLIP ViT-B/14. Sem. refers to semantic encoders that learn semantic-level representations, while Pix. indicates pixel-level tokenizers that focus on low-level visual features. # Exp. Visual Encoder Type MME-P SEEDB TQA Continuous: 1 CLIP ViT-B/14 [37] Sem. 1460.9 64.1 53. Discrete: 2 3 4 5 VQGAN [13] VQGAN-LC [76] LFQ [66] VQKD [35] Pix. Pix. Pix. Sem. 756.1 744.8 889.5 1252.4 38.2 38.2 41.1 57. 46.8 45.7 46.4 48.2 Unifying multimodal understanding and generation into cohesive next-token prediction paradigm requires VQ tokenizer for extracting indices from input images. While traditional VQ tokenizers [13, 54, 66, 76] excel at pixellevel image reconstruction, our investigation reveals significant limitation in their image understanding capabilities. We conducted experiments utilizing these tokenizers as feature extractors within the LLaVA-1.5 [29] framework. As shown in Exp. 2-4 of Tab. 1, the performance of these discrete tokenizers consistently lags behind that of the continuous tokenizer CLIP ViT-B/14 [37]. We posit that this performance gap stems from their pre-training objectives, which primarily optimize towards better low-level reconstruction quality. Consequently, the extracted features mainly encode low-level information, lacking the semantic-level understanding, which is crucial for complex visual reasoning. Another straight forward solution for unified understanding and generation can be distill discrete tokens from pretrained CLIP [8, 37, 45, 69], and then equip it with image reconstruction capability. As demonstrated in Exp. 5, VQKD, distilled from CLIP ViT-B/14, substantially reduces the performance gap compared to other discrete tokenizers. We 3 Figure 3. Overview of TokenFlow. We incorporate dual encoders and codebooks with shared mapping, enabling the joint optimization of high-level semantics and low-level pixel details. For given input image, distances dsem and dpix are calculated from the pixel-level and semantic-level codebooks, respectively, with the final codebook index and features determined by minimizing the weighted sum dsem + wdis dpix. The resulting quantized features are independently decoded for both semantic alignment and image reconstruction training, and then concatenated to provide unified representation for downstream tasks in understanding and generation. gregated by VQKD becomes extremely challenging. These observations highlight the necessity of developing novel tokenization approach that can effectively handle high-level semantic understanding and low-level visual reconstruction tasks. 3.2. Unified Image Tokenizer To bridge this gap, we propose TokenFlow  (Fig. 3)  , novel unified image tokenizer that enables joint representation learning at both semantic and pixel level. We find the key to unifying understanding and generation lies in learning an universal mapping. If the tokenizer can map patches that are both high-level and low-level similar to the same codebook index, then the quantized features can be easily decoded and directly applied to both autoregressive visual generation tasks and multimodal understanding tasks. Encoder. Unlike previous approaches that utilize one single encoder to extract low-level image information, we propose dual-encoder architecture comprising semantic encoder Esem and pixel encoder Epix. This design enables the extraction of two distinct types of image features. For the semantic encoder, we initialize it with pre-trained textaligned vision encoder (e.g., CLIP ViT-B/14). This initialization strategy facilitates better learning of high-level textaligned embeddings in the semantic codebook, ultimately enhancing the models multimodal understanding capabilities. For brevity here, we omit the spatial indices of feature representations, where ˆzsem = Esem(x) Rdsem and ˆzpix = Epix(x) Rdpix are the encoded features from semantic and pixel encoder. Quantization. We introduce an innovative quantization approach that employs dual codebooks: semantic-level emi=1 RKdsem and pixel-level beddings Zsem = {zsem,i}K i=1 RKdpix, where is the embeddings Zpix = {zpix,i}K Figure 4. Visualization of images clustered by (a) VQKD [35], (b) VQGAN [13], and (c) Our TokenFlow. VQKD clusters exhibit semantic similarity, while VQGAN clusters exhibit low-level similarity (i.e. color). Our TokenFlow can successfully combine both semantic and low-level similarity. Implementation details of image clustering can be found in Appendix A.1. further conducted an experiment to reconstruct the original image from quantized features extracted by VQKD. The reconstructed images exhibited significant blurring and evident loss of high-frequency details, as shown in Fig. 8. We attribute this outcome to the nature of VQKDs encoder, which maps semantically close patches into same codebook index. As visualized in Fig. 4 (a), it tends to map images with same semantical meaning to the same codebook index, while VQGAN (Fig. 4 (b)) tends to map visually similar images to the same codebook index, prioritizing low-level features over semantic content. Therefore, the reconstruction of fine-grained details from low-level dissimilar patches ag4 number of codebook entries. These two codebooks share unified mapping, enabling simultaneous consideration of high-level semantic information and low-level pixel details during the quantization process. Given the encoded feature representations ˆzsem and ˆzpix, we compute the distances to their respective codebook embeddings after l2-norm [64]: dsem,i = ˆzsem zsem,i2 2, for = 1, . . . , dpix,i = ˆzpix zpix,i2 2, for = 1, . . . , i = arg min (dsem,i + wdis dpix,i) (1) (2) (3) represents adversarial loss with λG as its weight coefficient. Following vector quantization conventions, we employ straight-through gradient estimator: = sg[z ˆz]+ ˆz where sg[] denotes the stop-gradient operation. The codebook 2 + βˆz sg[z]2 learning objective is: LVQ = sg[ˆz] z2 2 where the second term represents commitment loss with balancing factor β. The total training objective is the sum of all losses: Ltotal = Lsem + LVQ + Lpix. The optimal quantization index is determined by minimizing the weighted sum of these two distances, where wdis is the distance balance weight, as shown in Eq. (3). This joint optimization approach differs significantly from previous VQ methods that typically focus on learning the distribution of single feature type. We further adopt the multiscale VQ (MSVQ) structure [51] to to enhance the richness of the codebook representation. Our shared mapping strategy enables the codebook to learn the joint distribution of high-level semantics and low-level features, resulting in several key advantages: ❶ Scalability: Our approach demonstrates consistent performance improvements in both generative and understanding tasks as the codebook size increases, since large codebook size offers more highand low-level feature combination possibilities. With an expanded codebook size of 131,072, it can still maintain remarkably high utilization rate of over 95% while achieving best image reconstruction quality and multimodal understanding performance. ❷ Multi-task Capabilities: By learning the joint distribution of semantic and pixel-level features, our method bridges the gap between generation and understanding tasks. This unified representation enables single tokenizer to excel in both domains. This design also allows seamless integration of more codebooks to embed other type of feature representations, enabling extensibility to more downstream tasks without architectural modifications. Decoder and Training Objective. Our architecture incorporates two distinct decoders, including semantic decoder Dsem and pixel decoder Dpix for reconstructing semantic features and original image. We employ teacher model [35] (identical to the semantic encoders initialization) for target feature extraction. The semantic loss Lsem is computed as the l2 distance between decoded and teacherextracted features. The reconstruction loss is formulated as: Lpix = ℓ2(x, ˆx) + LP(x, ˆx) + λGLG(ˆx) (4) where ˆx = Dpix(z), ℓ2 represents pixel-wise reconstruction loss, LP() denotes perceptual loss using LPIPS, and LG() 5 Figure 5. Qualitative comparison of different sampling strategies in our framework. (a) Single-pass top-k (k=1200) and top-p (p=0.8) sampling exhibits inconsistent patterns and artifacts. (b) Our proposed multi-step sampling strategy produces more coherent and visually appealing results. Best zoomed in for details. 3.3. Visual Generation with TokenFlow TokenFlow helps us achieve SOTA performance in autoregressive text-to-image generation using the next-scale prediction paradigm. Below, we detail our training and inference strategy for high-quality image synthesis. Training Strategy. Our visual generation architecture builds upon pre-trained LLM model [53]. For text encoding, we leverage the models native BPE tokenizer to transform input text into discrete token sequences and extract feature representations. The original vocabulary is extended with specialized visual tokens. We extract the image tokens using TokenFlow, pass it through MLP, and concatenate it with text tokens for training. Given the models autoregressive nature, we employ cross-entropy loss computed exclusively on image tokens. To enable classifier-free guidance [17] during inference, we randomly replace conditioned text with an empty string with probability pdrop = 0.1 during training. Following [11, 48, 56], we incorporate QKnormalization and norm re-ordering to enhance training stability and prevent loss spikes. Inference Strategy. We observed that conventional topk-top-p sampling strategies, when employed in the nextscale paradigm, often lead to image collapse and repetitive local patterns. This can be attributed to the cross-entropy training objective, which establishes attention-based relationships primarily with the top-1 prediction. Independent top-k sampling for each token during inference can result in tokens lacking direct correlations, leading to inconsistent or repetitive patterns that can only be partially remedied through subsequent scales attention. This issue becomes more severe particularly with limited inference steps. To address this fundamental limitation, we propose novel multi-step sampling approach: (i) Initial sampling: Perform top-k top-p sampling with parameters k1 and p1. (ii) Refinement: Use the sampled output as input for second round of sampling in the same scale with reduced parameters k2 < k1 and p2 < p1. This progressive narrowing of the sampling space maintains creative diversity while enforcing consistency through refinement steps. Empirical results demonstrate significantly more coherent and visually appealing generations compared to single-pass sampling methods (see Fig. 5 and detailed ablation in Appendix B.1). 3.4. Multimodal Understanding with TokenFlow TokenFlow functions as multi-scale VQ tokenizer, where the quantized multi-scale features can be directly fed into pre-trained LLM for multimodal understanding training, following the LLaVA-1.5 [29] paradigm. The joint feature representations from dual flow serve as input to the model. We validate multiple feature input strategies: (i) Feature from all scales (ii) Final-scale feature only (iii) Residual features from all scales. We discover that features from the final scale achieves best overall performance, as detailed in Appendix B.1. This suggests that the final scale captures the most relevant semantic information for multimodal understanding, while additional scale features or residual features may introduce noise that compromises performance. Our model demonstrates substantial improvements over existing discrete multimodal methods. Notably, the performance gains can be achieved with minimal computational overhead, requiring less than 24 hour training on 8A100 GPUs using LLaVA 1.5 training data. 4. Experiments 4.1. Experimental Setup Datasets. TokenFlow is trained on LAION [42] and COYO-700M [5] and evaluate it on ImageNet [12]. To enhance face generation quality, we follow [48] and upsample the percentage of images with faces during tokenizer training by 2 times. For ablation studies, we train the tokenizer for 50 epochs on ImageNet-1K with CLIP ViT-B/14-224 [37]. For visual generation with TokenFlow, we trained it on curated dataset of 60M high-quality images, with captions generated using Qwen-VL [3]. Implement Details. We employ three variants of TokenFlow (B/L/XL), using CLIP ViT-B/14-224 [37], ViTaminXL-256 [8], and SigLIP-SO400M-patch14-384 [69] as respective teacher models and semantic encoder initializations. Detailed configurations are provided in Appendix A.2. For multimodal understanding, we employ Vicuna-v1.5-13B [10] and Qwen-2.5-14B [50] as the language backbone. For 256256 visual generation training, we truncate captions to first sentence with 0.2 probability to enhance short prompt generation capabilities. The model is initialized with Llama-2-7b [53], and being trained for 2 epochs. At inference, we apply classifier-free guidance [17] with scale factor of 7.5. Evaluation Metrics. We assess reconstruction quality using rFID, PSNR, and SSIM on the ImageNet-1K validation set [12]. For multimodal understanding, we evaluate on comprehensive suite of vision-language benchmarks: SEEDBench [22], MMVet [67], POPE [26], VQAv2 [16], GQA [19], TextVQA [43], AI2D [20], RealWorldQA [61], MMMU [68], MMBench [32], and MME [14]. Visual generation capabilities are evaluated using GenEval [15] and DPG-Bench [18]. We opt not to include FID scores as argued that it does not correlate well with human assessment of the overall performance of generative models [7, 36, 46]. 4.2. Unified Image Tokenizer Table 2. Comparison of reconstruction quality on the ImageNet 50k validation set. #Lvls. represents the number of residual levels used. For 384384 resolution, the downsample ratio of 14.2 is derived from 384/27. Model Res. ratio #Lvls. rFID PSNR SSIM VQ-GAN [13] LlamaGen [44] RQ-VAE [21] RQ-VAE [21] VAR [51] VILA-U [60] Ours LlamaGen [60] VILA-U [60] VAR [51] Ours 256 256 256 256 256 256 384 384 384 384 16 16 32 16 16 16 16 14.2 14.2 16 14.2 1 1 4 4 10 4 9 1 16 13 15 4.98 2.19 3.20 1.30 1.00 1.80 1. 0.94 1.25 2.09 0.63 20.00 20.79 22.63 21.41 21.94 22.73 22.77 0.629 0.675 0.755 0.687 0.726 0.774 0.731 In Tab. 2, we present reconstruction metrics of TokenFlow on 256256 and 384384 resolutions. The metric of VAR [51] is tested with the released checkpoint. At 256256 resolution with 16 compression ratio, TokenFlow achieves competitive performance with an rFID of 1.37, comparable to RQ-VAE while significantly outperforming previous methods such as VQ-GAN and LlamaGen. TokenFlow demonstrates superior reconstruction quality across all metrics in 384384 resolutiona standard size in multimodal understanding tasks. These results validate the effectiveness of dual codebook design in preserving fine-grained visual details. Moreover, the incorporation of shared mapping enables TokenFlow to maintain high-level semantic features, as verified in Sec. 4.3. 4.3. Multimodal Understanding TokenFlow, as discrete visual encoder, demonstrates state-of-the-art performance across comprehensive suite 6 Table 3. Evaluation on multimodal understanding benchmarks. We collect evaluations including: SEEDB: SEED Bench-Img [22]; MMV: MM-Vet [67]; POPE [26]; VQAv2 [16]; GQA [19]; TQA: TextVQA [43]; AI2D [20]; RWQA: RealWorldQA [61]; MMMU [68]; MMB: MMBench [32]; MME [14] and MME-P: MME-Perception. We include approaches with continuous visual inputs (top) versus discrete visual inputs (bottom). The best results among approaches with discrete visual input are highlighted in bold. * results are not reported in original paper and tested with lmms-eval [71] using the released checkpoint. When calculating average, we use MME-P and divide it by 20 to have the same scale with other benchmarks. Method # Params Res. SEEDB MMV POPE VQAv2 GQA TQA AI2D RWQA MMMU MMB MME MME-P Avg. Continuous Visual Input InstructBLIP [30] MiniGPT-4 [75] BLIP-2 [24] ShareGPT4V [9] NExT-GPT [58] Qwen-VL-Chat [3] Janus [57] LLaVA-1.5 [29] Discrete Visual Input Gemini-Nano-1 [49] Chameleon [48] LWM [31] SEED-LLaMA [23] Show-o [62] VILA-U [60] VILA-U [60] EMU3 [55] TokenFlow-B TokenFlow-L TokenFlow-XL TokenFlow-XL Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-7B Vicuna-7B Qwen-7B 224 224 224 336 224 448 DeepSeek-LLM-1.3B 384 336 Vicuna-13B 1.8B from scratch 34B from scratch LLaMA-2-7B LLaMA-2-13B Phi-1.5-1.3B LLaMA-2-7B LLaMA-2-7B 8B from scratch Vicuna-13B Vicuna-13B Vicuna-13B Qwen-2.5-14B 256 256 224 256 256 384 512 224 256 384 384 58.8 46.4 69.7 57.5 57.7 63.7 68. 53.7 56.3 59.0 68.2 60.4 62.6 68.7 72.6 25.6 22.4 37.6 34.3 36.1 9.6 27.7 33.5 37.2 22.4 27.7 40.7 48.2 78.9 87.0 85.9 75.2 80.0 83.9 85.8 85.2 84.0 85.0 86.8 87.8 80.6 66.0 78.2 77.3 80. 62.7 69.6 55.8 63.4 69.4 75.3 79.4 75.1 70.2 73.9 77.9 77.6 49.5 63.3 57.5 59.1 63.3 44.8 58.0 58.3 60.8 60.3 59.3 60.3 62.7 62.5 50.7 42.5 60.4 61.3 18.8 48.3 60.8 64.7 49.8 54.1 61.5 62.3 58.0 61. 70.0 54.2 56.6 66.7 75.8 54.9 55.3 57.4 49.4 49.2 53.7 56.6 26.6 37.2 30.5 36.4 26.3 26.7 31.6 34.2 34.4 38.7 43.2 36.0 68.8 58.0 69.4 67. 58.5 55.3 60.3 68.9 76.8 1158.7 1943.8 1848.3 1826.7 1509.9* 1660.4 1622.9 1840.9 1922.2 1212.8 866.6 1293.8 1567.4 1487.5 1338.0 1531.3 1097.2 1336.2 1401.8 1243.8* 1353.6 1365.4 1545.9 1551.1 62. 60.9 55.2 57.5 64.0 67.4 Following of multimodal understanding benchmarks. LLaVA-1.5s training pipeline, we train TokenFlow-B and TokenFlow-L using LLaVA-Pretrain558K for adapter pretraining and LLaVA-v1.5-mix-665K for instruction tuning. For TokenFlow-XL, inspired by recent findings in [52], we leverage Cambrian-Alignment and Cambrian-10M for pretraining and instruction tuning respectively, as the teacher model SigLIP-SO400M benefits significantly from increased training data. As evidenced in Tab. 3, TokenFlowXL achieves competitive or superior results compared to leading approaches with continuous inputs from CLIPstyle encoders. Using the same language backbone (Vicuna 13B), TokenFlow-XL outperforms LLaVA-1.5 13B by 1.7% on average, for the first time demonstrates that model with discrete visual input can surpass this strong baseline. By simply changing the LLM backbone to Qwen-2.5-14B [50], we further surpass LLaVA-1.5 by 7.2%. When compared to methods using discrete inputs, our approach demonstrates superior performance while maintaining training efficiency. Unlike models trained from scratch such as Chameleon and EMU3, our method requires less than 24 hour of training on 8A100 GPUs using LLaVA 1.5 data. TokenFlow-XL 14B significantly outperforms EMU3 with an overall improvement of 10.7%. Given these promising empirical results, we position TokenFlow as potential next-generation vision tokenizer for unified understanding and generation tasks. Our findings suggest that discrete visual representations can not only match but exceed the performance of continuous counterparts while maintaining practical training requirements. 4.4. Visual Generation We evaluate our models generation capabilities against state-of-the-art methods including diffusion-based, autoregressive-based, and hybrid approaches on standard benchmarks GenEval [15] and DPG-Bench [18]. As shown in Tab. 4, our approach achieves competitive performance while requiring significantly fewer generation steps. For 256256 image generation, we employ multistep sampling strategy instead of the original 9-step sampling (one per tokenizer scale). Specifically, we apply three steps per scale with top-k=[1200,100,1] and topp=[0.8,0.8,1.0] across all scales except the first, totaling 25 steps. Under this inference scheme, our model achieves GenEval score of 0.55, surpassing prominent diffusion models like Stable Diffusion v2.1 and PixArtalpha. More significantly, it surpasses autoregressive methods such as Chameleon, LlamaGen, and EMU3, which require thousands of inference steps. With prompt rewriting, our model achieves 0.63, approaching DALL-E 3s performance. On DPG-Bench, it achieves an average score of 72.9, outperforming LlamaGen, Show-o, SD v1.5, and PixArt-alpha. Moreover, our model only requires 2.7 seconds to infer one image with 1A100 GPU, which is significantly faster than other autoregressive-based methods. We further conduct additional text-to-image compari7 Table 4. Comparison of generation quality on GenEval [15] and DPG-Bench [18]. #Step: the number of model runs needed to generate an image. result is with rewriting. Table 5. Impact of key design choices on reconstruction quality and multimodal understanding benchmarks. Best results for each metric are highlighted in bold. Model Text Pretrain Res. #Steps GenEval Overall DPG-Bench Average Diffusion-based SD v1.5 [41] DALL-E 2 [39] SD v2.1 [41] SDXL [36] PixArt-alpha [7] DALL-E 3 [4] 512 CLIP ViT-L/14 1024 CLIP ViT-H/16 CLIP ViT-H/14 768 CLIP ViT-bigG 1024 512 Flan-T5-XXL 1024 Flan-T5-XXL Autoregressive meets diffusion Show-o [62] Transfusion [74] Phi-1.5 Autoregressive-based Chameleon [48] LlamaGen [44] EMU3 [55] VAR [51] Ours Flan-T5-XL 256 256 512 512 512 256 256 50 50 40 20 16 250 1024 1024 4096 28 25 0.43 0.52 0.50 0.55 0.48 0.67 0.53 0.63 0.39 0.32 0.54 / 0.66 0.53 0.55 / 0.63 63.18 74.65 71.11 83. 67.27 64.84 80.60 71.08 73.38 Figure 6. Impact of codebook size on reconstruction quality, class-conditional generation, and multimodal understanding benchmarks. MME is divide by 28 to have the same scale. son between TokenFlow and the released VAR tokenizer [51]. Under identical training configurations and dataset settings, our model consistently demonstrates better performance across all benchmark metrics, this further showcasing the effectiveness of our unified tokenization approach. 4.5. Ablation Studies Effect of Codebook Size. In Fig. 6, we experimented the impact of codebook size in our unified tokenizer, varying from 8,192 to 131,072. Our evaluation spans reconstruction quality, class-conditional generation, and multimodal understanding capabilities. For class-conditional generation, we employ the VAR transformer [51] with d=16, resulting in approximately 310M parameters. Notably, our approach maintains consistently high codebook utilization rate exceeding 95% even with codebook size of 131,072, attributed to our shared mapping design. The shared mapping allows for effective combinations of high-level semantic features and low-level details, addressing common limitation of 8 Shared Mapping MSVQ CLIP Init. rFID MME-P SEEDB TQA 8.07 3.96 2.18 2.16 1252.38 1212.51 1209.90 1312. 57.84 55.97 56.08 58.99 49.16 47.42 47.40 49.29 conventional VQ tokenizers [13] that typically suffer from deteriorating utilization rates at larger scales. Our results reveal that increasing codebook size enhances performance across multimodal understanding benchmarks and reconstruction quality. However, when codebook size exceeds 32,768, we observe slight degradation in class-conditional generation performance. This phenomenon can be attributed to the increased complexity of learning for autoregressive generation with larger codebooks. Based on this finding, we adopt codebook size of 32,768 for our text-to-image generation experiments. Effect of Key Design Choice. We validate the effectiveness of our key design choices in TokenFlow: shared mapping, multi-scale vector quantization (MSVQ), and CLIP initialization for the semantic encoder. As shown in Tab. 5, we start with baseline that uses one single codebook distilled from CLIP ViT-B/14, coupled with pixel decoder for direct image reconstruction from semantic features. This baseline yields high reconstruction FID of 8.07, primarily due to the challenge of reconstructing fine-grained pixel details solely from semantic features, as visualized in Fig. 8. The introduction of shared mapping (Row 2) enables the two codebooks to capture high-level and low-level features simultaneously. By weighted distance computation, we quantize the input with optimal combinations of high-level and low-level features. This design significantly improves reconstruction quality (-4.11 rFID) while maintaining comparable understanding capabilities. We further find that incorporating MSVQ [51] (Row 3) introduces multi-granular information into the codebook embeddings, which results in enhanced reconstruction performance, with rFID of 2.18. Moreover, this hierarchical design enables next-scale prediction paradigm in downstream text-to-image generation tasks, offering significant inference speed advantages over traditional next-token prediction approaches [47, 51]. Initializing the semantic encoder with pretrained CLIP weights (Row 4) while making it unfrozen during tokenizer training provides strong semantic priors for codebook embeddings. This results in substantial improvements across all understanding metrics (+8.4% in MME-Perception, +5.2% in SEED-Bench, and +4.0% in TextVQA). Given these empirical results, we adopt this configuration as our final model architecture and extend our experiments with stronger teacher models, additional training data, and longer training iterations. 5. Conclusion In this work, we introduce TokenFlow, novel unified image tokenizer that effectively bridges the gap between multimodal understanding and generation through its innovative dual-codebook architecture. By decoupling semantic and pixel-level feature learning while maintaining their alignment via shared mapping, TokenFlow successfully addresses the fundamental issue between different granularities of visual information required for understanding and generation tasks. Our comprehensive experiments demonstrate its effectiveness across multiple dimensions: superior reconstruction quality at different resolutions, state-of-theart performance in multimodal understanding with minimal training costs, and competitive visual generation capabilities with substantially fewer inference steps. These results validate that decoupled yet aligned feature learning through our shared mapping can effectively unify understanding and generation while maintaining superior performance in both domains, suggesting TokenFlow as promising next-era foundation tokenizer for vision-language systems."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 6, 7 [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 8, 4 [5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: https : / / github . com / Image-text pair dataset. kakaobrain/coyo-dataset, 2022. 6 [6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 6, 8, 4 [8] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision In Proceedings of the models in the vision-language era. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1295412966, 2024. 3, 6, 1, 4, 7 [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 7 [10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 6 [11] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512. PMLR, 2023. [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6 [13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2, 3, 4, 6, 8, 1, 5 [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 3, 6, 7 [15] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 6, 7, 8, 2, 4 [16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 6, 7 [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5, 6 [18] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 6, 7, 8, Ella: [19] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 6, 7 9 [20] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In Computer VisionECCV 2016: worth dozen images. 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. 6, 7 [21] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 3, 6 [22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 6, [23] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 3, 7 [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 7 [25] Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. 3 [26] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 6, 7 [27] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [28] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 3 [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1, 3, 6, 7 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 7 [31] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. 7 [32] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? Vision, pages 216233. Springer, 2025. 6, 7 In European Conference on Computer [33] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 1 [34] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-toimage generation via auto-regressive representations. arXiv preprint arXiv:2406.10797, 2024. 3 [35] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022. 3, 4, 5, 1, 2 [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 6, 8, 4 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 6, 1, 4, 7 [38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 1 [39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 1, 8, [40] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 3 [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 1, 3, 8, 4 [42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 6 [43] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 3, 6, 7 [44] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 3, 6, 8, 4 [45] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 3 10 [46] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 1, 6 [47] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. [48] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2, 3, 5, 6, 7, 8, 4 [49] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 7 [50] Qwen Team. Qwen2.5: party of foundation models, 2024. 6, 7 [51] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. 1, 3, 5, 6, 8, 4 [52] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 1, [53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 5, 6, 2 [54] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [55] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2, 3, 7, 8, 4 [56] Mitchell Wortsman, Peter Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023. 5 [57] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 2, 3, 7 [58] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [59] Yiqi Wu, Xiaodan Hu, Ziming Fu, Siling Zhou, and Jiangong Li. Gpt-4o: Visual perception performance of multimodal large language models in piglet activity understanding. arXiv preprint arXiv:2406.09781, 2024. 1 [60] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 2, 3, 6, 7 [61] XAI. Realworldqa, 2024. 6, 7 [62] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3, 7, 8, 4 [63] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36, 2024. 2 [64] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3, 5 [65] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 1, [66] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 3, 1 [67] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 6, 7 [68] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 6, 7 [69] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 3, 6, 1, 4, 7 [70] Jiahui Zhang, Fangneng Zhan, Christian Theobalt, and Shijian Lu. Regularized vector quantization for tokenized image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18467 18476, 2023. 3 [71] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. 7 [72] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024. 2 [73] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for highfidelity image generation. Advances in Neural Information Processing Systems, 35:2341223425, 2022. 2, 3 [74] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 8, 4 [75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 7 [76] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vqgan to 100,000 with utilization rate of 99%. arXiv preprint arXiv:2406.11837, 2024. 3, 1 12 TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Motivation Experimental Setup for Multimodal Understanding. To evaluate the multimodal understanding capabilities of current VQ tokenizers, we conduct experiments as detailed in Tab. 1. For LFQ [66], we utilize the open-source implementation [33], which demonstrates comparable performance to the original paper. The codebook size of LFQ is 262,144. For VQGAN-LC [76], we employ features before its projection layer, which is clustered from the pretrained CLIP image encoder, with codebook size of 100,000. Experimental Setup for Visual Comparison of VQKD, VQGAN and TokenFlow. To generate the visualizations in Fig. 4, we perform an experiment using 50,000 images from the ImageNet-1k validation set. We process these images through the encoders of VQKD, VQGAN and TokenFlow, applying average pooling to the extracted features to obtain 1 1 representation. Subsequently, we identify the closest index in their respective codebooks using l2 distance. We provide more visualizations in Fig. 11, and visualize the cluster size distribution in Fig. 7. Experimental Setup for Image Reconstruction from Quantized Semantic Feature. We conducted an experiment to reconstruct original images from quantized features extracted by VQKD [35]. In this setup, we maintained the original encoder and quantizer of VQKD, while introducing an additional decoder aimed at reconstructing the input image. The architecture of this decoder is identical to the pixel decoder employed in our TokenFlow. We trained this decoder on the ImageNet-1K dataset for 100 epochs. Fig. 8 presents visual comparison between the original and the reconstructed images. As observed, while the reconstructed images maintain the overall semantic content, they exhibit noticeable loss of high-frequency details. This phenomenon suggests that the quantized semantic features cannot fully preserve fine-grained visual details, which is crucial for visual generation. A.2. Tokenizer Training Details training configurations"
        },
        {
            "title": "We provide detailed",
            "content": "for TokenFlow-B, TokenFlow-L, and TokenFlow-XL variants in Tab. 11. All models share common hyperparameters including learning rate, batch size, commitment loss factor, adversarial loss factor and distance balance weight. The models primarily differ in their input resolution (224, 256, and 384) and semantic teacher models, utilizing CLIP Figure 7. Comparison of cluster size distributions between VQKD [35], VQGAN [13], and TokenFlow (ours), with fixed codebook size of 8,192. Analysis performed on 50,000 images from the ImageNet-1k validation set. TokenFlow exhibits significantly smoother distribution compared to others, attributed to our shared mapping design that learns joint distributions of semantic and pixel-level features. This joint learning approach helps maintain high codebook utilization (95%+) even with large-scale codebooks containing over 131K entries. ViT-B/14 [37], ViTamin-XL [8], and SigLIP-SO400M [69]. B. Additional Results B.1. Additional Ablation Study Effect of Sampling Strategy to Visual Generation. We conduct comprehensive ablation studies to analyze the im1 Figure 8. Comparison of original images and their reconstructions from quantized semantic features extracted by VQKD [35]. The reconstructed images preserve the semantic content but exhibit significant loss of high-frequency details. pact of different sampling strategies on generation quality. As shown in Table 6, we evaluate various configurations using GenEval [15] and ImageReward [63] metrics. We choose ImageReward for ablation due to its strong correlation with human preferences, particularly in capturing local artifacts and overall visual quality. The ImageReward is average over 10k prompts from the MS-COCO validation set. For multi-step configurations, we denote the top-p and topk values for each step using bracket notation [x1, ..., xn]. Our multi-step approach with two-step strategy (topk=[1200, 1], top-p=[0.8, 0]) significantly improves generation quality, yielding gains of +0.039 in GenEval and +0.084 in ImageReward compared to single-step sampling. This validates our hypothesis that progressive refinement helps maintain global consistency. When increasing the second-step value to 10 or 100 while maintaining top-p, we observe slightly degraded performance. This degradation suggests that excessive sampling freedom in refinement steps can lead to increased artifacts and local inconsistencies. Most notably, three-step strategy (top-k=[1200, 100, 1], top-p=[0.8, 0.8, 0]) achieves the best performance across both metrics. This represents substantial improvements of 10.2% and 14.3% over traditional single-step sampling, respectively. The gradual narrowing of sampling space (12001001) strikes balance between generation diversity and local consistency. As illustrated in Figure 5, our multi-step approach produces more coherent and visually appealing results. These quantitative and qualitative results demonstrates that progressive refinement in top-p topk sampling is crucial for high-quality generation in nextscale prediction frameworks. Effect of Model Size to Visual Generation. We conduct ablation studies to investigate the impact of model size on our decoder-only visual generation architecture. Specifically, we initialize our framework with two different backbone models: TinyLlama-1B [72] and Llama-2-7B Table 6. Impact of sampling strategy to visual generation. We compare single-step v.s. multi-step sampling strategy using GenEval and ImageReward. For multi-step approaches, values in brackets indicate parameters for successive sampling steps. Top-p GenEval ImageReward Strategy Single Step Top-k 1200 0.8 Multi Step [1200, 1] [1200, 10] [1200, 100] [1200, 100, 1] [0.8, 0] [0.8, 0.8] [0.8, 0.8] [0.8, 0.8, 0] 0.502 0.541 0.531 0.529 0.553 0.722 0.806 0.799 0.745 0.825 Table 7. Impact of model size to visual generation. Model size Training epoches GenEval ImageReward 1B 7B 4 2 0.485 0.553 0.677 0. Table 8. Impact of different input strategies on multimodal understanding. Best results for each metric are highlighted in bold. Input strategy MME MME-P SEEDB TQA Full scale Full scale residual Last scale semantic feat. only Last scale 1610.1 1527.5 1580.3 1634.3 1315.1 1216.5 1315.6 1356.5 59.6 57.0 60.1 59.9 49.5 48.1 49.7 49.1 [53]. Experiments demonstrate that model size plays crucial role in generation performance. As shown in Tab. 7 and Fig. 9, under identical sampling strategies and training dataset configurations, the 1B model significantly underperforms compared to its 7B counterpart, even with doubled training epochs. Effect of Input Strategy to Multimodal Understanding. We validate different feature input strategies for multi2 Figure 9. Qualitative comparison of visual generation capabilities between 1B and 7B models. Prompts (from left to right): (1) pizza sitting on top of wooden cutting board, (2) Television set being held by hand, (3) The guy is nicely dressed in suit and tie, and (4) sailing ship rests on waters. The 7B model demonstrates enhanced quality compared to its 1B counterpart. modal understanding with TokenFlow. As shown in Tab. 8, final-scale features consistently outperform both full-scale features and full-scale residual features across all benchmarks. This suggests that the final scale captures the most relevant semantic information for multimodal understanding, while additional scale features or residual features may introduce noise that compromises performance. Our experiments also reveal that utilizing semantic features only does not improve the overall understanding performance. Effect of Tokenizer Decoder Finetuning. To further improve our models ability to generate fine details, we follow [6] and double both the number of residual layers and channel dimensions in the decoder. We exclusively finetune these enhanced decoder layers while keeping all other components frozen, thereby preserving the learned visual token mappings. This enables us to improve reconstruction fidelity without compromising perception ability of TokenFlow. As shown in Fig. 10, the enhanced decoder yields notable improvements in reconstruction quality. It demonstrates superior preservation of high-frequency details, particularly in facial details and text elements. B.2. More Analysis of TokenFlow Analysis of Joint Distribution Learning. To evaluate the effectiveness of our shared mapping mechanism, we conduct comparative experiments against VQKD [35] and VQGAN [13]. All models are configured with identical codebook sizes of 8,192 tokens for fair comparison. For baseline models, we utilize the official pretrained checkpoints from [35] and [48], respectively. Our TokenFlow model is trained on ImageNet-1K for 50 epochs. We deliberately excludes the multi-scale VQ design [51] to isolate the effects of the shared mapping in this experiment. Figure 10. Comparison of image reconstruction quality. (a) Original images. (b) Reconstructions using the base pixel decoder. (c) Reconstructions using the enhanced (2 capacity) decoder. The enhanced decoder demonstrates superior preservation of finegrained details, particularly in facial details and textual elements. We apply average pooling to the extracted features to obtain 1 1 representation, and then identify the closest index in their respective codebooks using l2 distance. As shown in Fig. 7, TokenFlow exhibits significantly smoother distribution against compared to others. The total non-empty clusters of TokenFlow are 7161/8192 (87.4%), which is significantly larger than that of VQGAN (2.5%) and VQKD (27.1%). These results demonstrate that our shared mapping design enables effective learning of joint distributions across high-level semantic and low-level pixel representations. By simultaneously encoding multiple levels of visual information, we induces joint representation space compared to single-representation architectures. This directly contributes to the superior codebook utilization observed in our experiments. Even when expanding the codebook to over 131K entries, TokenFlow maintains an exceptional utilization ratio exceeding 95%. The clustered results is shown in Fig. 11. Automatic Balancing between Semantic Distance and Pixel Distance. In our structure, the optimal quantize index is determined by arg mini(dsem,i + wdis dpix,i). There exists an automatic balancing mechanism between semantic distance and pixel distance. For instance, when encountering case where dsem,i is relatively small while dpix,i is large, during backpropagation, both commit loss and perceptual loss will contribute to reducing the distance between the encoded features and their quantized counterparts. This mechanism naturally narrows the gap between these two distance metrics. Therefore, we set wdis to 1.0 across all experiments. For evaluation, we process 50,000 images from the ImageNet-1K validation set through each models encoder. Comparison between TokenFlow and their corresponding semantic teachers. Table 9 presents fair Table 9. Quantitative comparison of multimodal understanding capabilities between our discrete TokenFlow and their corresponding continuous semantic teachers. All experiments are trained with LLaVA-1.5 data for fair comparison. When calculating average, we use MME-P and divide it by 20 to have the same scale with other benchmarks. Method # Params Visual Encoder Res. SEEDB MMV POPE VQAv2 GQA TQA AI2D RWQA MMMU MMB MME MME-P Avg. Continuous Visual Input LLaVA-1.5 Vicuna-13B Discrete Visual Input Ours Vicuna-13B CLIP ViT-B/14 [37] ViTamin-XL [8] SigLIP-SO400M [69] TokenFlow-B TokenFlow-L TokenFlow-XL 224 256 384 224 256 384 64.1 65.7 67.5 60.4 62.6 65. 30.8 34.6 38.1 22.4 27.7 41.2 85.1 85.8 86.5 84.0 85.0 86.2 73.8 76.8 78.6 70.2 73.9 76. 61.3 62.6 63.8 59.3 60.3 63.0 53.4 57.4 62.2 49.8 54.1 57.5 57.8 59.4 59.5 54.2 56.6 56. 50.9 54.4 57.4 49.4 49.2 53.3 35.1 35.0 35.4 34.2 34.4 34.7 62.0 66.4 68.3 55.3 60.3 62. 1737.0 1839.1 1802.1 1460.9 1514.5 1488.2 58.9 61.3 62.9 1660.4 1622.9 1794.4 1353.6 1365.4 1502.3 55.2 (93.7%) 57.5 (93.8%) 61.1 (97.1%) Table 10. Comparison of generation quality on GenEval and DPG-Bench. Obj.: Object. Attri.: Attribute. result is with rewriting. Method Overall Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Global Entity Attribute Relation Other GenEval DPG-Bench Diffusion-based SDv1.5 [41] DALL-E 2 [39] SDv2.1 [41] SDXL [36] PixArt-alpha [7] DALL-E 3 [4] 0.43 0.52 0.50 0.55 0.48 0.67 Autoregressive meets diffusion Show-o [62] Transfusion [74] 0.53 0.63 Autoregressive-based Chameleon [48] LlamaGen [44] EMU3 [55] VAR [51] Ours 0.39 0.32 0.54 0.53 0.55 0.63 0.97 0.94 0.98 0.98 0.98 0.96 0.95 0.71 0.98 0.95 0.97 0.93 0.38 0.66 0.51 0.74 0.50 0.87 0.52 0.34 0.71 0.60 0.66 0.72 0.35 0.49 0.44 0.39 0.44 0.47 0.76 0.77 0.85 0.85 0.80 0.83 0.04 0.10 0.07 0.15 0.08 0.43 0.49 0.82 0.11 0.21 0.34 0.41 0.40 0.45 0.58 0.81 0.81 0.84 0.82 0.07 0.17 0.16 0.17 0.45 0.06 0.19 0.17 0.23 0.07 0.45 0.28 0.04 0.21 0.24 0.26 0.42 63.18 74.65 71.11 83. 74.63 83.27 74.97 90.97 74.23 82.43 79.32 89.61 75.39 80.91 78.60 88.39 73.49 86.76 82.57 90.58 67.81 80.41 76.96 89.83 67.27 79.33 75.44 78.02 84.45 60.80 64.84 80.60 71. 73.38 81.76 85.21 77.51 75.43 86.68 78.17 78.72 79.22 76.17 86.84 77. 81.29 84.76 90.22 85.80 58.40 83.15 62.00 85.22 71.20 comparison between our discrete TokenFlow variants and their corresponding semantic teachers under the LLaVA-1.5 training paradigm. TokenFlow exhibits relative performance gap compared to its semantic teachers due to vector quantized distillation. However, this gap diminishes as from 6.3% at 224224 to 6.2% at resolution increases: 256256, and finally to 2.9% at 384384. This improvement can be attributed to the increased number of discrete tokens and additional scales supplementing the residual features at higher resolutions. B.3. More Visual Generation Results Quantitative Results. In Tab. 10, we present the complete scores for both GenEval [15] and DPG-Bench [18]. Following DALL-E 3 [4], we report our GenEval results using GPT-4V as rewriter. For DPG-Bench, we tested the results of LlamaGen and Show-o using their released checkpoints. We compare against VAR [51] by using their released tokenizer and training the visual generation model under identical settings to ensure fair comparison. Qualitative Results. We present additional visual generation results in Fig. 12. Our method can generate images 4 with various styles, subjects, and scenarios. C. Limitation and Future Work primary limitation of TokenFlow lies in the performance gap in multimodal understanding between our discrete tokenizer and its continuous semantic teacher, which stems from the vector quantization distillation process. While this gap narrows to 2.9% at 384384 resolution, several methods remain for further improvement, such as incorporating text alignment loss during tokenizer training. In this work, we primarily focused on designing TokenFlow and validating its effectiveness separately in multimodal understanding and visual generation tasks. natural extension of this work is the development of fully unified model for both multimodal understanding and generation. This unification can be achieved through joint training on interleaved vision-language data. This is currently in our high priority for exploration. Figure 11. Qualitative comparison of images clustered by VQKD [35], VQGAN [13] and our TokenFlow. VQKD clusters exhibit semantic similarity, while VQGAN clusters exhibit low-level similarity (i.e. color and texture). Our TokenFlow can successfully combine both semantic and low-level similarity (e.g. birds with different background can be mapped into two different index). 5 Figure 12. More Visual Generation Results with TokenFlow. We present diverse 256256 results across various styles, subjects, and scenarios. Table 11. Detail settings of TokenFlow-B, TokenFlow-L and TokenFlow-XL. Tokenizer Tokenizer settings: TokenFlow-B TokenFlow-L TokenFlow-XL Input resolution Codebook size Semantic teacher Multi-scale settings Semantic codebook embedding dimension Pixel codebook embedding dimension 224 32,768 CLIP ViT-B/14-224 [37] [1, 2, 4, 6, 8, 10, 12, 14] 32 8 256 32,768 ViTamin-XL-256 [8] [1, 2, 3, 4, 6, 8, 10, 12, 14, 16] 32 8 384 32,768 SigLIP-SO400M-patch14-384 [69] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 17, 22, 27] 32 8 Training settings: Learning rate Batch size Training steps Distance balance weight wdis Commitment loss factor β Adversarial loss factor λG Max gradient norm 1e-4 256 1,000,000 1.0 0.25 0.5 1.0 1e-4 256 500,000 1.0 0.25 0.5 1.0 1e-4 256 500,000 1.0 0.25 0.5 1."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}