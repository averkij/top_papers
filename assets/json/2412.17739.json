{
    "paper_title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization",
    "authors": [
        "Ermo Hua",
        "Che Jiang",
        "Xingtai Lv",
        "Kaiyan Zhang",
        "Ning Ding",
        "Youbang Sun",
        "Biqing Qi",
        "Yuchen Fan",
        "Xue Kai Zhu",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend. While existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales show that, within varying context windows, FoPE can maintain a more stable perplexity and a more consistent accuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling."
        },
        {
            "title": "Start",
            "content": "Fourier Position Embedding: Enhancing Attentions Periodic Extension for Length Generalization Ermo Hua1, Che Jiang1, Xingtai Lv1, Kaiyan Zhang1, Ning Ding1 Youbang Sun1,2, Biqing Qi3, Yuchen Fan1,4, Xuekai Zhu1,4, Bowen Zhou1,3,* 1Tsinghua University, 2Northeastern University 3Shanghai AI Laboratory, 4Shanghai Jiaotong University hem23@mails.tsinghua.edu.cn, zhoubowen@tsinghua.edu.cn https://github.com/TsinghuaC3I/Fourier-Position-Embedding 4 2 0 2 3 2 ] . [ 1 9 3 7 7 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become trend. While existing works mainly address RoPEs limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attentions frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales show that, within varying context windows, FoPE can maintain more stable perplexity and more consistent accuracy in needle-in-haystack task compared to RoPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling."
        },
        {
            "title": "Introduction",
            "content": "Generation based on the information from long contexts is crucial for Language Models (LMs). However, LMs are typically trained on fixed context window (Vaswani, 2017; Touvron et al., 2023; Groeneveld et al., 2024) and tends to overfit to the specific context length. Many studies consider the absolute position embedding (Vaswani, 2017) to be the source of overfitting in length generalization. As mitigation, several *Corresponding Author 1 (a) Accuracy on Passkey Retrieval (higher is better) (b) Perplexity on C4 (lower is better) Figure 1: Training with max_seq_length=512. relative position embedding methods have been proposed (Press et al., 2021; Su et al., 2024; Peng et al., 2023; Jin et al., 2024) to improve LMs long-distance dependency. Among these, ALiBi (Press et al., 2021) introduced position-biased attention mask, which linearly declines the attention weights based on distance. ALiBi delivers stable perplexity in pre-training, but it loses the information from long-distance tokens, resulting in poor performance on long-context downstream tasks. Another method, RoPE (Su et al., 2024), uses the phase of complex numbers to store the position information. Combined with continual pre-training and other interpolation-based methods (Peng et al., 2023; Xiong et al., 2024; Chen et al., 2024; Jin et al., 2024), RoPE provides better access to long-distance information, making it one of the most widely used position embedding. However, RoPE-based LMs still struggle with length generalization without supplementary methods. Figure 2: The reasons why RoPEs periodic extension deteriorates and how FoPE addresses these issues to improve length generalization. (a) As signals pass through linear and nonlinear transformations, this causes spectral leakage and distortion, mixing multiple frequencies into single dimension. Under RoPE, each dimension is treated as single-frequency component. By contrast, FoPE models each dimension as Fourier series of different frequency components, thereby separating information more effectively and mitigating spectral damage. (b) FoPE eliminates inadequately trained frequency components, which are harmful for periodic extension. By preserving only the zero-frequency component, FoPE safeguards periodic extension and delivers more robust length generalization. In this paper, we take closer look at RoPE in the frequency-domain with tools from Discrete Signal Processing (DSP) theory. Our modeling reveals that RoPE implicitly performs Non-Uniform Discrete Fourier Transform (NUDFT) on the hidden states, enabling periodic attention based on the frequency-domain encoding. However, we find that the periodicity is hindered by the spectral damage caused by: 1) linear layers and activation functions outside attention; 2) inadequately-trained frequency components within attention (See Fig 2). This explains why RoPE fails to achieve length generalization without assistance from other methods. Building on our observations above, we propose Fourier Position Embedding (FoPE) to further improve the attentions periodic extension for better length generalization. Compared to RoPE, FoPE introduces two main improvements: 1) While RoPE treats each dimension as single-frequency function, FoPE models each dimension as Fourier Series, consisting of dominate frequency component and several harmonic components. This approach better mirrors the actual spectrum in LMs and helps attention separate information across different wavelengths, mitigating the negative effects of Spectral Damage. 2) FoPE clips inadequately trained frequency components that is harmful to length generalization. To keep the passing of long wavelength information, we substitute these components with zero, as the zero-frequency component corresponds to the longest wavelength. We summarize our contribution as follows: 1. Based on DSP, we provide frequency-domain analysis to reveal the negative influence from nearly all parts from LMs. We find that the length generalization is hindered by the Spectrum Damage arised from: 1) linear layers and activation functions; 2) undertrained frequency components. 2. We propose FoPE to improve attentions robustness on the Spectrum Damage. FoPE construct Fourier Series to extract multi-frequency information in each dimension, and clip the frequency of destructive components to zero. Thus, FoPE delivers better periodic extension of attention, thus bringing better length generalizaion. 3. We conduct experiments across several model scales and datasets. The perplexity in pre-training and the accuracy in needle-in-haystack demonstrate FoPEs superiority over RoPE and ALiBi on length generalization. Ablations on both frequency and time domain bring further support to our method and theoretical modeling."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Non-Uniform Discrete Fourier Transform Given finite sequence of equally-spaced samples {xn} := x0, x1, ..., xN 1 of continuous function x, Discrete Fourier Transform (DFT) converts them into equally-spaced frequency-domain components {Xm} := X0, X1, ..., XM 1, the original samples can be recovered by Inverse DFT (IDFT): Xm = 1 (cid:80) n=0 xnei2π m, xn = 1 Xmei2π M 1 (cid:80) m=0 (1) As eiωn = cos ωn + sin ωn is periodic in the original domain, DFT implicitly transforms the original function into linear combination of periodic waves with frequency ωm = 2π . Thus, DFT is an estimation of the original function, which is lossless only if the original function exactly composed of these specific periodic components. To achieve more precise approximation, the sampled frequencies can follow any arbitrary distribution {ωm} := ω0, ω1, ..., ωM 1, where the only constraint is ωm [0, 2π). The generalization of DFT is called Non-Uniform DFT (NUDFT). 2.2 RoPE implicitly achieves Periodic Attention based on NUDFT Given dimension Query and Key of token and b, RoPE rotates them to different phase based on the dimension m: (cid:102)qm(na) = Qmeiωmna, (cid:102)km(nb) = Kmeiωmnb (2) where ωm = 1/θ(2m/M ) and θ is the pre-defined parameters in RoPE. Then, the attention weight hm(n) in each dimension will be calculated as: (cid:102)hm(n) = (cid:102)qm(na) (cid:102)km (nb) = Hmeiωmn (3) where = nanb and Hm = QmKm. Finally, the overall attention weight between different tokens can be formalized as: h(n) = 1 (cid:88) m=0 (cid:102)hm(n) = 1 (cid:88) m=0 Hmeiωmn (4) Comparing it with Eq (1), it can be observed that RoPE implicitly achieves token-level Inverse NUDFT with frequency components {ωm}. several periodic components, which brings periodic extension in each dimension m: (cid:102)hm(n + Nωm) = (cid:102)hm(n) (5) where Nωm = 2π ωm property can generalize LMs to longer context. is this components period. This"
        },
        {
            "title": "Generalization",
            "content": "Ideally, RoPE-based Attention achieves periodic extension in any length scenario. However, this extension is confined as key ideal property that is not guaranteed in LMs. 3.1 Negative Influence of Spectrum Damage The ideal coefficients and frequencies of NUDFT have one-to-one correspondence. The coefficient of each frequency represents the influence of each token on others propagated at specific wavelength. However, the periodic extension is hindered, if the coefficient also contains the information from another frequency component ωo with coefficient Hωo = σHω, called the Spectrum Damage. If we define the damaged function as = Hωm[(1 σ)eiωmn + σeiωon], we find: m(n + Nωm) = h m(n) (6) as Nωm is not the period of hωo. In other words, the information from each component is transmitted through waves with mismatched wavelengths, leading to inaccurate estimation of the influence propagated within each wavelength. As result, the periodic extension and length generalization of attention are adversely affected. 3.2 Spectrum Damage Outside Attention The LMs linear layers and activation functions outside attention bring two types of spectrum damage, destroying the one-to-one correspondence between coefficients and frequencies. Linear Layer uses weights RM to map dimension hidden state RM to another hidden state RM . Thus, each dimension of will be linear combination of different components of X: Ym = 1 (cid:88) k=0 WkmXk (7) Based on NUDFT, RoPE models the interactions between different tokens as functions composed of This results in Spectrum Leakage, as different frequency components exhibit interplay. 3 Activation Function has non-linearity in the time domain, generating harmonic frequencies as described by the following Lemma: Lemma 1. Given double-frequency sinusoid function x(n) = cos ω1n + cos ω2n and any timeindependent non-linear function g. The effect of on x(n) will produce waves whose frequencies are the linear combinations of ω1 and ω2: g(x(n)) = (cid:88) (cid:88) jN kN aj,k cos(jω1 + kω2)n (8) which can be generalized to any multi-frequency function x(n) = (cid:80)(aω sin ωn + bω cos ωn)1. As the hidden states have been transformed into multi-frequency functions by Linear Layer, passing them across Activation Functions introduces additional harmonic components, leading to serious Spectrum Distortion. These two types of Spectrum Damage undermine the periodic extension of attention (as shown in Eq.(5)(6)), hindering the models length generalization property. 3.3 Spectrum Damage Inside Attention Besides the spectrum damage outside attention, the undertrained components of attention within extremely low frequencies (ωm < 2π ) also bring spectrum damage. Consider single-frequency function xm(n) = eiωmnrect(n) truncated by square wave: rect(n) = (cid:26) 1 , 0 , > (9) Based on the results of DFT, the spectrum estimation of x(n) is1: X(ω) = αδ(ωm) + sin[(N αNm)(ω ωm)] ω ωm (10) where α = Nm and Nm = 2π ωm . In the frequency domain, time-domain truncation introduces noisy components via the latter sub-function. When the period of the primary frequency component exceeds the truncation length, its amplitude is significantly weakened. Consequently, noisy components dominate these dimensions, impairing the periodic extension (as defined by Eq.(5)(6)). In contrast, high-frequency components are minimally affected because their coefficients α dominate over the noisy components. 1From (Oppenheim et al., 1982) Intuitively, when sampling sinusoidal functions based on token positions, these low-frequency components cannot cover complete cycle. Therefore, for positions exceeding the pre-training sequence length, these dimensions may sample outside the training domain, leading to difficulties in generalization. Although previous works (Peng et al., 2023) have identified this issue, we are the first to model it from Fourier perspective and provide theoretical explanation."
        },
        {
            "title": "4 Fourier Position Embedding",
            "content": "To mitigate the negative affect of the non-ideal frequency-domain properties in LMs, we propose Fourier Position Embedding (FoPE) to modify frequency-domain properties of attention: Treating Each Dimension as Multi-Frequency. Although Linear Layers and Activation Functions bring serious Spectrum Leakage and Spectrum Distortion, they are crucial for enhancing expressive capacity. Therefore, we keep these modules unchanged but focus on modifying how attention processes information within each dimension. To achieve this, we replace the single frequency in each dimension with Fourier Series: hm(n) = Hm(n)(eiωmn + aωeiωn) (11) (cid:88) ω where aω < 1 because ωm is the dominant frequency. This allows attention modules to capture multi-frequency information in each dimension. We initialize vector {ωm} as same as RoPE, and initialize vector {ω} and matrix {aω} based on the analysis in Sec 3.2: For {ω} RD, we make sure so that {ωm} {ω}, and the other frequencies can be sampled within [0, π] in any distribution. For {aω} RDM , we initialize it with (0, σ) based on the hypothesis that the Spectrum Damage obeys the similar distribution as the Linear Layers. The coefficients for the real and imaginary part of the frequency are sampled separately in our implementation, which can also use the same coefficient. The and σ are kept as hyper-parameters to be adjusted. Zero-out Undertrained Frequencies. As anthe inadequate training of alyzed in Sec 3.3, extremely-low frequencies ωm < 2π impairs the frequency-domain properties of attention. Thus, we define the floor frequency as ωl = 2π , and clip the frequencies under the floor frequency to zero. We choose zero as the substitute because the zero-frequency component can represent any pe- (a) Accuracy on Passkey Retrieval (higher is better) Figure 3: Effectiveness of FoPE in length extrapolation. Starting point models trained with maximum sequence length of 512 are extrapolated using YARN and FoPE on corpus with maximum sequence length of 1024. (b) Perplexity on C4 (lower is better) riod, making it easier to train and ensuring stable periodic extensions. Also, since the zero-frequency component has the longest wavelength and typically carries the most information, this substitution does not compromise the length generalization or hinder model fitting. Overall function of FoPE can be formalized as: hm(n) = Hm(n)f (ωm) (12) (cid:40) (ωm) = 1 eiωmn + (cid:80) ω aωeiωn , ωm < ωl , ωm ωl (13) which treats each dimension either as Fourier Series or as zero-frequency component. Implementation of FoPE can be easily achieved with weight matrix RD(M M0), where M0 is the number of zero-frequency components in each head (details in B). This matrix maps the coefficients of all frequencies to Fourier Series for each dimension. Since the zero-frequency sinusoidal function does not affect the original hidden states, the output dimension is less than the dimension of each head. To introduce more diversity and better simulate the randomness of the Spectrum Damage, we assign separate weights for different heads, as well as for the cosine and sine functions. In our implementation, gradients are not required for these matrices, so FoPE adds negligible memory and computation overhead compared to RoPE."
        },
        {
            "title": "5 Experiments",
            "content": "To demonstrate the effectiveness of FoPE as both position embedding and an extrapolation method, we conduct experiments during pre-training (Sec. 5.2) and fine-tuning (Sec. 5.3). Additionally, we perform ablation studies to analyze the impact of hyperparameters on FoPE (Sec. 5.4) and analysis to demonstrate the necessity to zero-out undertrained components (Sec. 5.5). 5.1 Basic Settings We mainly consider two metrics: perplexity for pre-training and accuracy on Passkey Retrieval. Perplexity quantifies how well language model predicts sequence of words or tokens. lower perplexity indicates the model is more confident and accurate in its predictions. Accuracy on Passkey Retrieval (Mohtashami and Jaggi, 2023) measures the models ability in retrieving short passkey (i.e., five-digit number) from large context full of meaningless text. We conduct this evaluation based on the implementation from (Peng et al., 2023). During evaluation, the passkey is randomly positioned at uniformly distributed locations within the context. For each context length, we test for 1000 trials to ensure the positions sampled are sufficiently dispersed. We conduct experiments with the OLMo (Groeneveld et al., 2024) framework and consider different scale models having 60M, 180M, 1.2B parameters. its linearly declining attention is unable to capture information from long distances. In contrast, FoPE maintains stable retrieval accuracy for passkeys at any position, demonstrating strong ability to extract subtle information from long sequences. 5.3 Length Generalization after Fine-Tuning Beyond the use of positional embeddings during the pre-training phase, several post-pre-training extrapolation methods (Peng et al., 2023; Chen et al., 2024) have been proven critical for enhancing length generalization. Thus, we investigate two key aspects of FoPE: 1) whether existing extrapolation methods are also effective for FoPE; 2) whether FoPE can enable extrapolation on RoPEbased models, thereby allowing seamless integration with existing open-source models. In this subexperiment, we select representative extrapolation method, YARN (Peng et al., 2023), as our baseline. We fine-tune the last checkpoint from pre-training for 1B tokens in this setting. Results (See Fig 3). Compared to RoPE+YARN, FoPE+YARN achieves significantly better length generalization performance, as demonstrated by lower perplexity on the C4 dataset and higher accuracy in the Passkey Retrieval task. Moreover, FoPE outperforms YARN in length extrapolation for both RoPE-based and FoPE-based models. These findings underscore the effectiveness and practical utility of FoPE, which holds the potential to enhance all RoPE-based open-source models. 5.4 Ablation Studies We also conduct ablation studies on various hyperparameters to observe their effects on our algorithm. Considering the consistent performance of FoPE across different parameter scales, we only evaluate the 60M models in ablation studies. Both sub-methods of FoPE are useful (See Fig 5a). FoPE is constitutive of two parts, called Fourier Series (FS) and Clip Floor to Zero (CF). Although these two sub-methods are both useful for length generalization, combining them together brings more significant improvement. On one hand, FS contributes more to length generalization, which demonstrates that the Spectrum Damage have significant influence on length generalization. On the other hand, CF contributes more to fitting the current dataset and sequence length, which implies the zero-frequency component is the most informative and indispensable component. Increasing the dimension of attention heads Figure 4: Training with max_seq_length=512 on Gutenberg Books and evaluating on validation set of C4, FoPE also demonstrates its ability to generalize across different data distributions. 5.2 Length Generalization after Pre-Training We consider two settings to evaluate both the intradomain and out-of-domain generalization: Setting 1: We train models with 10B-tokens subset of C4 (Raffel et al., 2020) and evaluate them in validation set from C4. Setting 2: We train models with 5B tokens from Gutenberg Books (Hart, 2007) and evaluate them in the same validation set as Setting 1. In this setting, the language distribution is different between the validation set and the training set, which can further evaluate the generalization ability of different methods. Results of Perplexity. (See Fig 1.b & 4) In both settings, FoPE shows significant advantage over RoPE. But FoPE is slightly worse than ALiBi, as there is an issue when ALiBi meets this training corpus, which is also mentioned in other papers (Peng et al., 2023; Chen et al., 2024). On the one hand, the corpus in C4 and Books mainly have short-distance dependency, thus the information from short context window is enough for the prediction of almost all tokens. On the other hand, AliBi uses linear declined attention to eliminate long-distance information, and only pays attention to short-distance dependency. Based on these two reasons, ALiBi does not have any decline in perplexity as the context length increases. Results of Passkey. (See Fig 1.a) In this task, FoPE demonstrates significant advantage over both RoPE and ALiBi. RoPEs accuracy drops sharply to zero at twice the training length and remains at zero for longer sequences. ALiBi shows linear decline in accuracy, further illustrating that 6 (a) Ablation for different sub-methods (b) Ablation for different σ (c) Ablation for different Figure 5: Ablation Studies. (a)(b) evaluate PPL Ratio = PPLc4/PPLbooks, (c) evaluate accuracy on Passkey. is more beneficial than increasing the number of attention heads or layers (See Fig 5a). More dimensions introduce more frequency components, making attention more robust to Spectral Damage. In contrast, adding more attention heads and layers aggravates Spectrum Damage, which diminishes the benefits of expanding the parameter scale. Variance σ of {aω} (See Fig 5b). We keep = 16 to only evaluate σs influence. By grid searching σ from 0 to 0.5, we find that setting σ = 0.3 for 60M model can obtain the best perplexity, especially for long context length. The best σ implies the estimated strength of Spectrum Damage in 60M models, and the estimation may become larger as the parameter scale increases. Number of {ω}. We keep σ = 0.3 to only evaluate Ds influence. By grid searching σ from 16 to 128, we find that does not significantly influence the perplexity, but it is important for Passkey Retrieval. Setting = 64 can obtain the best accuracy for Passkey Retrieval. The best is the estimated number of strong enough noisy components of each model, and this number may become larger as the parameter scale increases. The harmonic frequencies tend to be weaker than the base frequencies, and this phenomenon is more significant to the higher-order harmonics. Thus, there are limited noisy frequency components that have enough intensity to disturb the passing of the base wave, paying attention to not important components hinders the effectiveness of the model. 5.5 The Necessity to Zero-Out Undertrained Components To further explain the negative influence of the extremely low-frequency components and demonstrate the necessity to zero-out them, we also investigate their properties in the time domain. By visualizing the numerical expectations of the Figure 6: Average activations of q,k vectors. and vectors in each dimension (details in Appendix A.1), we observe that the absolute values of the dimensions corresponding to undertrained frequencies are noticeably greater than zero, whereas those of adequately trained dimensions are close to zero. On the other side, the components that do not complete full cycle during pre-training introduce weights with non-zero means when applied to q,k vectors. Thus, these components introduce positional bias (shown in Fig 7) and may adversely affect robustness to out-of-domain rotation matrix values during length generalization. To verify this hypothesis, we normalized the q,k vectors (enforcing mean of 0 and variance of 1) before applying the rotation matrix to eliminate the positional bias. Based on the results in Table 1, normalization on naive RoPE showed positive impact on length generalization. However, if all frequency components are pre-trained to complete full 7 Thorp et al., 2022; Gillman et al., 2024) employed Fourier features into neural networks to enhance performance on NLP or CV tasks. S4 (Gu et al., 2021) also leveraged FFT and IFFT to shift its core computation into the frequency-domain, delivering more efficient computation. (Wang et al., 2019; Su et al., 2024) improved the attention mechanism by defining position embedding with complex number, while \"phase\" used in these methods is typical concept in frequency-domain. Length Generalization. Due to resource constraints, LMs are trained on limited-length corpus chunks and struggle with longer contexts (Voita et al., 2023; Dong et al., 2024; Hong et al., 2024). While absolute position embeddings (Vaswani, 2017) restrict the general use of positional information, methods as (Shaw et al., 2018; Yang, 2019) directly adjust the attention mechanism, another intuitive method is to redesign the position embedding (Press et al., 2021; Chi et al., 2022; Kazemnejad et al., 2024; Su et al., 2024; Wang et al., 2024; Choromanski et al., 2024). Among these, RoPE (Su et al., 2024) encodes positional information using the phase of complex numbers, leveraging their periodicity to enhance access to long-distance dependencies. Several training-free or fine-tuningbased methods can also improve the LMs length generalization by refining RoPE (Peng et al., 2023; Chen et al., 2024; Jin et al., 2024; Lin et al., 2024a). However, these works mainly address the drawbacks of RoPE in attention mechanism, neglecting the influence of other components in LMs."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we analyze RoPE-based attention by modeling it in the frequency domain using Discrete Signal Processing (DSP) theory. Our analysis reveals that RoPE achieves periodic attention by implicitly performing Non-Uniform Discrete Fourier Transform (NUDFT), corrupted by the non-ideal spectrum properties brought by other parts in LMs. We propose Fourier Position Embedding (FoPE) to enhances attentions periodic extension and length generalization. FoPE models each dimension as Fourier Series and zero-out inadequately-trained frequency components. Experiments demonstrate that FoPE significantly improves length generalization compared to baselines across diverse tasks and datasets. Our ablation studies and visualizations provide further support for our method and theoretical modeling. Figure 7: The statistical average contribution of RoPE to attention scores. We sample 1k and vectors from Gaussian distributions with mean of 0, 0.1, and 1.0. The long-distance decay effect weakens as the mean decreases, disappearing entirely when the mean is 0. Sequence Length 512 1024 4096 8192 RoPE RoPE + QK_Norm 5.50 5.46 RoPE-A 5.72 RoPE-A + QK_Norm 5.69 NoPE NoPE + QK_Norm 5.65 5.59 6.01 5.56 5.86 5.89 6.03 6.18 6.58 5.89 6.18 6. 6.60 6.87 6.99 6.32 6.46 6.59 6.81 7.10 7.16 6.66 6.67 6. 6.99 7.43 Table 1: The loss of 20M toy models trained on sequence length of 512. We consider three types of position embeddings, among which only RoPE has frequencies that cannot complete full cycles. For RoPE-A, all frequencies are adjusted from RoPE to the nearest values that exactly complete full cycles. cycles, such normalization does not improve generalization. These experimental results validate our hypothesis. Additionally, the normalization completely disrupts the decay property of RoPEs attention scores with respect to token distances. This suggests that the locality induced by long-distance decay is not critical for length generalization."
        },
        {
            "title": "6 Related Work",
            "content": "Frequency-Domain Embedding. Discrete Fourier Transform (DFT) (Oppenheim et al., 1982) has been widely used in various areas having periodic signals (Edfors et al., 2000; Sanchez, 2010). In machine learning, (Uteuliyeva et al., 2020; Lin et al., 2024b; Tancik et al., 2020; Tamkin et al., 2020; Lee-"
        },
        {
            "title": "8 Limitations",
            "content": "Our DSP-based modeling in the frequency domain provides novel perspective for LMs to enhance length generalization and explore broader applications. These include aligning frequency-domain representations for better model collaboration, optimizing kv-cache compression via spectral analysis, and improving semantic communication with learnable frequency-domain embeddings. However, as our work focuses on modeling length generalization into the frequency domain, we only pay attention to the undesirable properties hindering this objective. Extending the applicability of this modeling to areas such as kv-cache compression, model collaboration, and semantic communication may require additional effort. more generalized definition and analysis is left for future work."
        },
        {
            "title": "Acknowledgments",
            "content": "Special thanks to Yihao Liu, Yizhou Jiang and Yiming Shi for contributing to idea formulation, Xinwei Long and Guoli Jia for insightful discussions during the experiments."
        },
        {
            "title": "References",
            "content": "Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. 2024. CLEX: Continuous length extrapolation for large language models. In The Twelfth International Conference on Learning Representations. Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. 2022. Kerple: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:83868399. Krzysztof Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tamas Sarlos, Thomas Weingarten, and Adrian Weller. 2024. Learning fourier transform for linear relative positional encodings in transformers. In International Conference on Artificial Intelligence and Statistics, pages 22782286. PMLR. Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingbing Wang, Zhen Tian, Weipeng Chen, and Ji-Rong Wen. 2024. Exploring context window of large language models via decomposed positional vectors. arXiv preprint arXiv:2405.18009. Ove Edfors, Magnus Sandell, Jan-Jaap Van De Beek, Sarah Kate Wilson, and Per Ola Börjesson. 2000. Analysis of dft-based channel estimators for ofdm. Wireless Personal Communications, 12:5570. Nate Gillman, Daksh Aggarwal, Michael Freeman, Saurabh Singh, and Chen Sun. 2024. Fourier head: Helping large language models learn comarXiv preprint plex probability distributions. arXiv:2410.22269. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838. Albert Gu, Karan Goel, and Christopher Ré. 2021. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396. Michael Hart. 2007. Project gutenberg. https:// www.gutenberg.org. Accessed by OLMo Team on Unknown Date. Xiangyu Hong, Che Jiang, Biqing Qi, Fandong Meng, Mo Yu, Bowen Zhou, and Jie Zhou. 2024. On the token distance modeling ability of higher rope attention dimension. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 58775888. Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. 2024. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2024. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36. James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2022. Fnet: Mixing tokens with fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 42964313. Hongzhan Lin, Ang Lv, Yuhan Chen, Chen Zhu, Yang Song, Hengshu Zhu, and Rui Yan. 2024a. Mixture of in-context experts enhance llms long context awareness. arXiv preprint arXiv:2406.19598. Shengsheng Lin, Weiwei Lin, Xinyi Hu, Wentai Wu, Ruichao Mo, and Haocheng Zhong. 2024b. Cyclenet: enhancing time series forecasting through modeling periodic patterns. arXiv preprint arXiv:2409.18479. Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite conarXiv preprint text transformers. arXiv:2305.16300. length for Alan V. Oppenheim, Alan S. Willsky, and S.Hamid Nawab. 1982. Signals and Systems. Prentice Hall. 9 Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. 2019. Encoding word order in complex embeddings. arXiv preprint arXiv:1912.12333. Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. 2024. Length generalization of causal transformarXiv preprint ers without position encoding. arXiv:2404.12224. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. 2024. Effective long-context scaling of foundation models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 46434663, Mexico City, Mexico. Association for Computational Linguistics. Zhilin Yang. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations. Ofir Press, Noah Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. JM Sanchez. 2010. Cluster expansion and the Physical Retheory of alloys. configurational view BCondensed Matter and Materials Physics, 81(22):224202. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063. Alex Tamkin, Dan Jurafsky, and Noah Goodman. 2020. Language through prism: spectral approach for multiscale language representations. Advances in Neural Information Processing Systems, 33:5492 5504. Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. 2020. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Malika Uteuliyeva, Abylay Zhumekenov, Rustem Takhanov, Zhenisbek Assylbekov, Alejandro Castro, and Olzhas Kabdolov. 2020. Fourier neural networks: comparative study. Intelligent Data Analysis, 24(5):11071120. Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems. Elena Voita, Javier Ferrando, and Christoforos Nalmpantis. 2023. Neurons in large language models: Dead, ngram, positional. arXiv preprint arXiv:2309.04827."
        },
        {
            "title": "A More Experimental Results",
            "content": "A.1 Visualization of q,k vectors before applying RoPE We conduct visualization experiments using the Llama2-7B model, which features an attention module with 32 heads, each comprising 128 dimensions, and pretraining sequence length of 4096 tokens. The number of cycles sampled by each sinusoidal function during pretraining is calculated as ri = θiLtrain , where denotes the dimension index. Based on this, we determined that the dimensions corresponding to incomplete cycles fall within the ranges [45, 64] [109, 128]. 2π We randomly sample 1000 tokens and compute the average activation values across heads for each dimension. The average activation values for every dimension of each layer are then plotted in Fig 6. The absolute activation values are significantly higher for dimensions corresponding to undertrained frequencies compared to others. This indicates that the RoPE rotation matrix pattern during pretraining has notable impact on the distribution of q,k vector activations. A. Influence of undertrained components in time-domain We conduct visualization and ablations to further investigate the influence of undertrained components in RoPE. In Fig 7, we visualize the time-domain pattern of RoPE. To get the \"Ideal RoPEs Contribution\", we suppose the Query and Key vectors are equal to 1 constantly (as in the original paper of RoPE (Su et al., 2024). To get the \"Actual RoPEs Contribution\", we suppose the Query and Key vectors obey the Gaussian distribution and sample 1000 times to get the mathematical expectation of the \"Actual RoPEs Contribution\". It can be seen that RoPE brings decay in attention score in its naive setting, which is brought by the undertrained components in RoPE. We hypothesis that this decay brings positional bias that may adversely affect robustness to out-of-domain rotation matrix values during length generalization. As the positional bias can be eased if the mean of Query and Key vectors is set to zero, we conduct further ablation studies to normalize the Query and Key vectors before attention. Based on the results in 1, only if the position embedding contains components that cannot complete full cycle, the normalization brings better length generalization. Thus, Model Scale 60M 180M 1.2B Num_Heads Num_Layers MLP_Ratio Head_Dim Mini_Batchsize Var_Freq σ Num_Freq 8 8 8 64 64/32 0.3 64 8 8 8 128 32/16 0.4 16 16 8 128 8/4 0.6 128 Table 2: The hyper-parameter of different model scales. The upper part is the parameter shared by all experiments, while the lower part is the parameter specific to FoPE. We use different mini-batch sizes depending on the max_seq_length: the first corresponds to max_seq_length=512, and the second corresponds to max_seq_length=1024. these components are partially proved to deliver negative affect by positional bias. This experiment also demonstrate that the positional decay does not have beneficial influence on length generalization."
        },
        {
            "title": "B Implementation Details",
            "content": "Training Settings. Our main experiments are conducted with 4 cards NVIDIA A6000 (maximum GPU memory=48GB), the pre-training of 60M/180M/1.2B in 20B tokens lasts for 10/20/100 hours, respectively. The time-consuming has linear relation with the number of tokens in other settings. For all model scales and experimental settings, we select 6e-4 as the learning rate and warmup for 10000 steps with cosine scheduler. While the mini-batchsize on each device is different for each model, we accumulate gradients until the global batchsize reaches 1024 in all experiments. Evaluation Settings. For pre-training, evaluations are conducted on the checkpoint from the last step. For fine-tuning, we save checkpoints every 100 steps and report the best result for each method. This is partly due to YARN (Peng et al., 2023) being prone to overfitting with excessive fine-tuning steps, limitation not observed in FoPE. Pseudo-code of FoPE is shown in the final page."
        },
        {
            "title": "C Theoretical Details",
            "content": "C.1 Derivation of Lemma 1 Given non-linear function g(x), it can be rewritten as power series by Taylor expansion: g(x) = apxp (cid:88) pN (14) 11 Suppose the input is double-frequencies function: x(n) = cos ω1n + cos ω2n (15) , the output becomes: g(x(n)) = (cid:88) pN ap(cos ω1n + cos ω2n)p (16) Considering the product conversion formula of sinusoid function: cos α cos θ = 1 2 [cos(α θ) + cos(α + θ)] (17) Thus, each sub-function of Eq.(16) can generate harmonic functions. For example, when = 2: (cos ω1n + cos ω2n)2 = (cos ω1n)2 + (cos ω2n)2 + 2 cos ω1n cos ω2n = 1 + 1 2 cos 2ω1n + 1 2 cos 2ω2n + cos(ω1 ω2)n + cos(ω1 + ω2)n (18) C.2 Derivation of Equal (10) Before we begin the derivation, lets familiarize with two functions in time domain: Rectangular Pulse Function and Single-Frequency Function. Given Rectangular Pulse function: r(t) = (cid:26) 1, 0, t > whose Fourier Transform is Sa Function: R(ω) = (cid:90) ejωtdt = sin(ωT ) ω Given Single-Frequency function: (t) = ejωmt whose Fourier Transform is: (19) (20) (21) (ω) = (cid:90) + ejωmtejωtdt (22) which is equal to an Impulse Function in frequency domain: δ(ωm) = (cid:26) 1, ω = ωm 0, ω = ωm (23) Then, Eq (10) can be easily derivated, as the context truncated with length can be seen as: x(t) = (t) r(t) (24) Its Fourier Transform in discrete case is Eq. (10). 12 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 class FourierEmbedding ( RotaryEmbedding ) : def __init__ ( self , config ): super () . __init__ ( config ) self . input_dim = self . inv_freq . size ( -1) self . output_dim = self . input_dim if self . input_dim <= self . head_dim //4 else self . head_dim //4 self . sin_coef = nn . Parameter ( torch . randn ( self . config . n_heads , self . input_dim , self . output_dim ) , requires_grad = False ) self . cos_coef = nn . Parameter ( torch . randn ( self . config . n_heads , self . input_dim , self . output_dim ) , requires_grad = False ) torch . nn . init . xavier_normal_ ( self . sin_coef , gain = self . config . rope_fourier_init_norm_gain ) torch . nn . init . xavier_normal_ ( self . cos_coef , gain = self . config . rope_fourier_init_norm_gain ) self . sin_coef += torch . eye ( self . input_dim , device = self . sin_coef . device ) self . cos_coef += torch . eye ( self . input_dim , device = self . cos_coef . device ) def apply_rotary_pos_embed ( self , pos_sin , pos_cos , t): fourier_sin = torch . einsum ( \" bhtD , hDd -> bhtd \" , pos_sin , self . sin_coef / self . sin_coef . sum ( dim = -2 , keepdim = True )) fourier_cos = torch . einsum ( \" bhtD , hDd -> bhtd \" , pos_cos , self . cos_coef / self . cos_coef . sum ( dim = -2 , keepdim = True )) fourier_sin = F. pad ( input = fourier_sin , pad =(0 , self . head_dim //2 - fourier_sin . size ( -1) ) , mode =\" constant \" , value =1) fourier_cos = F. pad ( input = fourier_cos , pad =(0 , self . head_dim //2 - fourier_cos . size ( -1) ) , mode =\" constant \" , value =1) fourier_sin = torch . cat (( fourier_sin , fourier_sin ) , dim = -1) fourier_cos = torch . cat (( fourier_cos , fourier_cos ) , dim = -1) return (( * fourier_cos ) - ( self . rotate_half ( t) * fourier_sin )) . to (t . dtype ) class RotaryEmbedding ( nn . Module ): def __init__ ( self , config ): super () . __init__ () self . config = config self . dim = self . config . d_model // self . config . n_heads self . inv_freq = self . get_inv_freq ( self . dim ) def get_inv_freq ( self , dim ): inv_freq = 1.0 / ( self . config . rope_theta ** ( torch . arange (0 , dim , 2 , device = device , dtype = torch . float ) / dim ) ) if self . config . fope is True : inv_freq [ inv_freq < 2* torch . pi / self . config . max_sequence_length ] = 0 inv_freq = inv_freq [ inv_freq != 0.0] return inv_freq 13 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 def get_rotary_embedding ( self , seq_len ) : with torch . autocast ( device . type , enabled = False ): seq = torch . arange ( seq_len , device = device , dtype = torch . float ) freqs = torch . einsum (\"t , hd -> htd \" , seq , self . inv_freq ) if self . config . fope is True : positions = freqs . unsqueeze (0) else : positions = torch . cat (( freqs , freqs ) , dim = -1) . unsqueeze (0) return positions . sin () , positions . cos () def rotate_half ( self , x): , nh , , hs = x. size () = x. view (B , nh , , 2, hs // 2) x1 , x2 = x. unbind ( dim = -2) return torch . cat (( - x2 , x1 ) , dim = -1) def apply_rotary_pos_emb ( self , pos_sin , pos_cos , t): return (( * pos_cos ) - ( self . rotate_half ( t) * pos_sin )) . to (t . dtype ) def forward ( self , , all_len ): with torch . autocast (x. device . type , enabled = False ): x_len = x_ . shape [ -2] pos_sin , pos_cos = self . get_rotary_embedding ( all_len ) pos_sin = pos_sin . type_as ( x_ ) pos_cos = pos_cos . type_as ( x_ ) x_ = self . apply_rotary_pos_emb ( pos_sin [: , : , all_len - x_len : all_len , :] , pos_cos [: , : , all_len - x_len : all_len , :] , x_ , ) return x_ . type_as (x)"
        }
    ],
    "affiliations": [
        "Northeastern University",
        "Shanghai AI Laboratory",
        "Shanghai Jiaotong University",
        "Tsinghua University"
    ]
}