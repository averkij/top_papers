{
    "paper_title": "Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens",
    "authors": [
        "Weihao Liu",
        "Dehai Min",
        "Lu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy."
        },
        {
            "title": "Start",
            "content": "Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Weihao Liu 1 Dehai Min 1 Lu Cheng"
        },
        {
            "title": "Abstract",
            "content": "While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces ContextPrediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy. 2 6 2 0 2 0 1 ] . [ 1 9 2 2 0 1 . 2 0 6 2 : r 1. Introduction The capability of Large Language Models (LLMs) to perform multi-step reasoning has largely depended on generating explicit text steps, known as Chain-of-Thought (CoT) 1Department of Computer Science, University of Illinois Chicago, Chicago, IL, USA. Correspondence to: Weihao Liu <wliu681@uic.edu>, Lu Cheng <lucheng@uic.edu>. Preprint. February 12, 2026. 2Repository URL. 1 Figure 1. Comparison of reasoning paradigms. Explicit CoT verbalizes all steps as text tokens. Coconut uses fixed number of latent tokens from hidden states. Soft-Thinking constructs latent tokens via probability-weighted interpolation with entropy-based stopping. Assistant-based methods rely on external models. Our LT-Tuning dynamically interleaves text and latent tokens through confidence-driven insertion and Context-Prediction Fusion. (Wei et al., 2022; Chen et al., 2023). Although effective, this approach requires the model to perform reasoning in discrete token sequence, which means the model can not think twice before acting, or they demand extra cost on extremely long text output (Jaech et al., 2024; Yeo et al., 2025; Guo et al., 2025; Seed et al., 2025) and self-reflection (Renze & Guven, 2024; Kang et al., 2025; Yu et al., 2025). Motivated by these limitations, recent work has explored reasoning in continuous latent spaces as an alternative (Zhu et al., 2025a; Chen et al., 2025). By allowing models to reason directly in high-dimensional hidden states rather than explicit tokens (Hao et al., 2024; Shen et al., 2025; Wei Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens et al., 2025), this line of research aims to decouple internal reasoning from explicit text generation. While promising, latent-space reasoning methods face two fundamental challenges: Constructing well-aligned latent representations. Latent tokens must be semantically expressive while remaining compatible with the models internal embedding space. Methods relying on external assistant models (Xu et al., 2025; He et al., 2025) struggle with representational misalignment, whereas purely intrinsic approaches (Chen et al., 2025) risk distribution mismatch between input embeddings and output hidden statesparticularly in models with untied input and output embeddingswhich can lead to instability or feature collapse. Adapting reasoning cost dynamically. Most existing methods employ static reasoning schedules, ignoring the fact that step difficulty varies. This fixed allocation is often inefficient, as it wastes computation on trivial steps while failing to provide sufficient depth for complex reasoning. To address these challenges, we propose Latent Thoughts Tuning (LT-Tuning), framework that enables LLMs to perform robust latent reasoning without external assistants. An illustration of the difference between our method and mainstream methods in latent reasoning is visualized in Figure 1. Our core innovation is Context-Prediction Fusion mechanism that constructs latent tokens by combining two complementary sources: the contextual history encoded in hidden states, and the predictive semantic guidance from probability-weighted vocabulary embeddings. This fusion bridges the gap between the models output space and input embedding manifold, mitigating feature collapse in larger models. Additionally, we introduce confidence-driven strategy that allows the model to dynamically determine when to engage latent reasoning, avoiding the inefficiency of static allocation. The entire framework is trained through three-stage curriculum learning that progressively transitions from purely explicit CoT to reasoning with latent thoughts. Our main contributions can be summarized as follows. (1) unified latent reasoning method. We introduce LTTuning, latent-space reasoning framework that enables adaptive and stable continuous reasoning without architectural modifications. The method integrates (i) confidencedriven dynamic decision on explicit CoT or latent reasoning, (ii) contextprediction fusion to construct well-aligned latent tokens by combining contextual hidden states with predictive semantic guidance, and (iii) progressive curriculum learning strategy that stabilizes latent-space optimization and mitigates feature collapse. 2 (2) Comprehensive empirical evaluation and scaling analysis. We conduct extensive experiments on mathematical reasoning benchmarks across model scales from 1B to 8B parameters. Results show that LT-Tuning consistently outperforms existing latent reasoning baselines at all scales, achieving up to 4.3% average improvement over the strongest prior method. Notably, while prior approaches such as Coconut (Hao et al., 2024) degrade severely on larger models due to feature collapse, LT-Tuning exhibits robust and healthy scaling behavior across benchmarks. 2. Related Work Explicit Reasoning. The emergence of Chain-of-Thought (CoT) prompting (Wei et al., 2022) marked paradigm shift in how we elicit reasoning from large language models. By decomposing complex problems into verbalizable intermediate steps, CoT enables models to tackle tasks that would otherwise exceed their direct inference capabilities. Subsequent works have extended this paradigm through programaided reasoning (Chen et al., 2023), self-consistency decoding (Wang et al., 2022), and tree-structured exploration (Yao et al., 2024). More recently, reasoning-focused models such as OpenAI o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025) leverage reinforcement learning to generate extended reasoning chains, achieving strong performance on complex tasks. However, these approaches often produce extremely long reasoning traces, incurring substantial computational cost and inference latency. Although some works have explored condensed reasoning (Deng et al., 2023; Cheng & Van Durme, 2024), they share fundamental constraint: reasoning must be externalized as discrete tokens, restricting the models thoughts to concepts expressible in natural language. Reasoning with Latent Tokens. The constraints of explicit reasoning have motivated exploration into continuous latent spaces. Coconut (Hao et al., 2024) pioneered this direction by feeding the last hidden state directly as the next input embedding, enabling recurrent reasoning without token generation (Shen et al., 2025; Wei et al., 2025). However, this approach directly reuses hidden states as input embeddings, ignoring the distributional gap between the two spaces. Soft-Thinking (Zhang et al., 2025; Zhou et al., 2025) addresses this partially by constructing latent tokens via probabilistic mixtures over vocabulary embeddings, but discards the contextual information encoded in hidden states. Another line of work employs assistant models to generate latent representations (Xu et al., 2025; He et al., 2025), avoiding training large models but introducing potential misalignment between the assistants output space and the reasoning models embedding space. Recent work also explores reinforcement learning for latent reasoning (Butt et al., 2025). Additionally, recurrent transformers (Dehghani Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens et al., 2018; Yang et al., 2023; Gatmiry et al., 2024) offer related paradigm for iterative refinement, but typically require pretraining from scratch (Zhu et al., 2025b; Bae et al., 2025), limiting applicability to existing LLMs. In contrast, LT-Tuning enables recurrent latent computation through post-training pipeline applicable to off-the-shelf models. 3. Preliminaries 3.1. Autoregressive Language Modeling Let denote the discrete vocabulary space and = {x1, . . . , xT } be sequence of tokens where xt V. standard decoder-only Transformer (Vaswani et al., 2017) defines probability distribution over the sequence factorization: (cid:89) pθ(x) = pθ(xt x<t), (1) t= where x<t represents the prefix history. The model processes the input through multiple layers, producing final hidden state ht Rd at step t. The probability distribution for the next token is obtained via linear projection head Wu RVd followed by softmax function: pθ(xt+1 x<t) = Softmax(Wuht). (2) Crucially, the input embedding for the next step + 1 is strictly coupled to the discrete token selection: et+1 = Embed(argmax(pθ)) or sampled from the distribution. This restricts the reasoning trace to the discrete grid of V. 3.2. Using Latent Tokens for Reasoning Latent reasoning is typically formulated as recurrent state evolution process over continuous vectors. Let Rd be the continuous latent space. Unlike standard decoding where the input at step is strictly constrained to the embedding of discrete token index E(wt), latent reasoning allows the model to process sequence of continuous latent tokens z1, . . . , zk, where zi Z. These vectors serve as the direct input to the Transformer block function Fθ: ht = Fθ(ht1, zt), (3) where ht represents the updated contextualized state. This formulation allows the model to maintain and evolve thought process in the high-dimensional vector space without collapsing into discrete tokens at every step. For example, Coconut (Hao et al., 2024) treats the input latent embedding as direct recurrence of the preceding output state, i.e., zt := ht1. While computationally convenient, this approach introduces distribution mismatch, as ht1 resides in the output contextualized space rather than the input embedding manifold for which the Transformer weights were trained. Furthermore, relying solely on the 3 raw hidden state ignores the semantic probabilistic guidance provided by the vocabulary projection (Eq. 2), which typically helps organize reasoning steps. Our goal is to resolve this mismatch by defining constructive mapping function Φ() such that the latent input zt = Φ(ht1, pθ()), effectively fusing the contextual history captured in the hidden state with the predictive semantic guidance of the vocabulary distribution to stabilize the latent reasoning trajectory. 4. Methodology In this section, we present Latent Thoughts Tuning (LTTuning), post-training framework designed to enhance latent reasoning capabilities. Unlike prior approaches that enforce static allocation of latent tokens, our method empowers models to dynamically determine when to engage in latent reasoning and when to revert to explicit text generation. As illustrated in Figure 2, this is achieved through progressive three-stage curriculum that evolves the latent space from simple hidden-state recurrence into guided latent reasoning process, effectively mitigating optimization instability. 4.1. Stage 1: Explicit Reasoning Warm-up To establish foundation for reasoning, we first perform supervised fine-tuning (SFT) on the pretrained base model using CoT data. Let = {(x, ycot, yans)} be the dataset containing questions, explicit reasoning steps, and final answers. The model is trained to maximize the likelihood of the CoT sequence and the answer: LCoT = (cid:88) log pθ(yt x, y<t). (4) This stage ensures the model acquires the fundamental capability to decompose complex problems through step-by-step reasoning, serving as the foundation for the subsequent latent phases. 4.2. Stage 2: Dynamic Latent Tokens Generation To achieve more efficient and robust latent reasoning, instead of uniformly applying fixed number of latent tokens, we train the model to dynamically determine whether to engage latent reasoning based on prediction confidence. Confidence-Driven Data Construction. We preprocess the training data by identifying positions where the model is uncertain. Specifically, for target token yt, if the models prediction confidence pθ(yty<t) falls below threshold τ , we insert <thinking> placeholders at that position: Mode(t) = (cid:40) Latent (<thinking>), Explicit (Text), if pθ(yty<t) < τ otherwise (5) Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Figure 2. Overview of the three-stage LT-Tuning framework. Stage 1: standard explicit CoT fine-tuning to establish reasoning capabilities. Stage 2: learning to generate latent tokens with confidence-driven insertion, where hidden states serve as the initial latent representations. Stage 3: Context-Prediction Fusion, which combines contextual history information (hidden states) with predicted semantic guidance (fused embeddings) to construct high-quality latent tokens. Notably, the <thinking> token functions exclusively as control signal. Since its input representation is dynamically derived from previous hidden states rather than static embedding, the model treats it as non-verbalizable latent step during generation. models output distribution. Given the logit distribution lt1 from the previous step, we apply temperature scaling and Top-p filtering to focus on high-confidence predictions. After masking the <thinking> token and renormalizing, we compute: Latent Token Initialization. The input embeddings for <thinking> tokens are initialized using the hidden states ht1,I from layer at position 1. This ensures latent reasoning is reserved for uncertain steps, preventing the model from learning spurious patterns on trivial tokens. The model is then trained to predict the subsequent explicit tokens conditioned on this mixed sequence of text and latent tokens. 4.3. Stage 3: Context-Prediction Fusion in Latent Tokens While Stage 2 uses raw hidden states as latent token embeddings, this can cause distribution mismatch between the output and input spaces. Stage 3 addresses this by fusing two complementary sources of information. Predictive Component. Similar to Soft-Thinking, we compute probability-weighted embedding from the epred = (cid:88) wV ˆP (w) E(w), (6) where E(w) Rd is the embedding vector for token w. This projects the models predictive distribution onto the embedding manifold. Context-Prediction Fusion. To avoid relying exclusively on this predictive vector, we fuse it with the hidden state to preserve contextual history. Specifically, we combine epred with the hidden state ht1,I from layer I: efusion = α ht1,I + (1 α) epred, (7) where α is balancing coefficient. This fused representation serves as the input embedding zt for the <thinking> token, ensuring compatibility with the input space while retaining contextual information. 4 Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Algorithm 1 LT-Tuning Forward Pass Input: Sequence x, Model M, Embedding matrix E, Fusion weight α, Top-p threshold p, Layer index I, Temperature Output: Logits sequence idx 0; KV ; while idx < len(x) do index of next <thinking> in x[idx :], or len(x) if none h, logits, KV M.forward(x[idx : k], KV ) Append logits to if < len(x) then // Context component hctx h[1][I] // Prediction component ˆP TopP(Softmax(logits[1]/T ), p) epred ˆP // Fusion efusion α hctx + (1 α) epred Use efusion as input embedding for position idx + 1 else break end if end while return 5. Experiments 5.1. Setup Models and Datasets. We conduct experiments on three different model sizes to ensure the robustness of our method. More specifically, we use Llama-3.2-1B, Llama-3.2-3B and Llama-3.1-8B (Grattafiori et al., 2024) as the backbone LLMs in our method. All models are trained on the GSM8K training set (Cobbe et al., 2021) and evaluated on four mathematical reasoning benchmarks, including GSM8KNL (Cobbe et al., 2021), ASDiv-Aug (Xu et al., 2025), MultiArith (Roy & Roth, 2015) and SVAMP (Patel et al., 2021). We report accuracy for all experiments. Please refer to the Appendix to get the statistics of these datasets. Implementation Details. We adjust batch size and learning rate for each model scale to accommodate GPU memory constraints and ensure stable optimization. For the 8B model, whose input and output embedding matrices are not shared, we add lightweight adapter to bridge the representation gap. For the 1B and 3B models, no adapter is applied as they use tied input-output embeddings. All experiments were conducted on 4 NVIDIA A100 80GB GPUs. Full training details and hyperparameters are provided in Appendix B. 5 Figure 3. Average number of <thinking> tokens generated versus question difficulty across models of varying sizes. Difficulty is measured by the error rate of Llama-3.1-8B-Instruct over 5 sampling trials. Models generally demonstrate positive correlation between question difficulty and the number of generated latent tokens, indicating that our method learns to adaptively scale latent reasoning effort based on problem complexity. 5.2. Baselines To comprehensively evaluate the effectiveness of our proposed framework, we compare LT-Tuning against diverse set of baselines. These latent reasoning baselines represent different strategies for utilizing latent representations for continuous space reasoning: (1) Explicit CoT (Wei et al., 2022): Standard explicit reasoning where the model generates discrete text tokens as intermediate steps. (2) Coconut (Hao et al., 2024): An intrinsic latent reasoning method that directly feeds the previous hidden state as the next input embedding (zt = ht1). (3) SoftThinking (Zhang et al., 2025): An intrinsic training-free method that constructs soft concept tokens via probabilityweighted sum of top-k vocabulary embeddings, without incorporating hidden-state context (zt = epred). (4) SoftCoT (Xu et al., 2025): An assistant-based method that uses separate assistant model to speculatively generate instancespecific soft thought tokens as the initial chain of thoughts. (5) SemCoT (He et al., 2025): An assistant-based approach that employs distilled model to generate semantic consistent latent embeddings through training via contrastive learning, improving the interpretability and stability of the latent space. 5.3. Results and Analysis Table 1 presents results across three model scales on all four datasets. We have the following observations according to the experimental results and further analysis: Consistent Improvements Across Scales. LT-Tuning achieves the best average performance at all model scales: 36.4% (1B), 52.4% (3B), and 68.8% (8B). In contrast, baseLatent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B Model GSM8K-NL ASDiv-Aug MultiArith SVAMP Average Explicit CoT Soft-Thinking Coconut SoftCoT SemCoT LT-Tuning (ours) Explicit CoT Soft-Thinking Coconut SoftCoT SemCoT LT-Tuning (ours) Explicit CoT Soft-Thinking Coconut SoftCoT SemCoT LT-Tuning (ours) LT-Tuning + Adapter (ours) 14.9 13.7 14.6 14.9 15.0 15.8 ( +0.8) 29.5 24.2 31.8 26.9 16.0 32.1 ( +0.3) 49.5 53.1 32.7 36.8 21.5 58.1 58.5 ( +5.4) 44.8 39.3 42.5 54.1 40.0 53.9 69.8 70.3 61.9 55.5 52.5 67.2 69.6 74.9 38.8 46.2 77.0 72.2 70.7 37.8 32.2 22.8 38.9 33.3 51.7 ( +12.8) 57.2 46.1 63.3 57.8 32.7 64.4 ( +1.1) 78.3 85.0 51.7 74.4 67.8 92.8 96.1 ( +11.1) 22.3 20.3 21.0 25.0 25.5 24.3 45.7 43.7 44.0 34.5 31.5 45.7 49.3 51.0 43.0 40.0 46.5 52.3 55.7 ( +4.7) 29.9 26.4 25.2 33.2 28.5 36.4 ( +3.2) 50.5 44.5 50.3 43.7 33.2 52.4 ( +1.9) 61.7 66.0 41.5 46.1 53.2 68.8 70.3 ( +4.3) Table 1. Main results on mathematical reasoning benchmarks. All models are fine-tuned on the GSM8K training set (Yu et al., 2023) and evaluated on four test benchmarks. Bold: best results. Underline: best baseline. (): absolute gain over best baseline. Setting 3B LT-Tuning w/o Stage 2 w/o Stage 3 w/o Latent w/o TT-Latent 8B LT-Tuning w/o Stage 2 w/o Stage 3 w/o Latent w/o TT-Latent GSM8KNL ASDivAug MultiArith SVAMP Average 32.1 29.3 31.2 26.0 24.9 58.1 51.4 33.7 49.7 52.4 67.2 63.0 52.8 55.0 62.0 72.2 62.0 36.5 58.7 55. 64.4 60.0 56.1 51.1 42.8 92.8 88.3 82.2 93.3 87.8 45.7 41.7 37.3 32.3 43.3 52.3 46.7 28.7 44.7 54.0 52.4 48.5 ( -3.9) 44.4 ( -8.0) 41.1 ( -11.3) 43.3 ( -9.1) 68.8 62.1 ( -6.7) 45.3 ( -23.5) 61.6 ( -7.2) 62.3 ( -6.5) Table 2. Ablation study on the contribution of each training stage and component. w/o Stage 2: static latent token allocation. w/o Stage 3: raw hidden states without fusion. w/o Latent: treat <thinking> tokens as pause tokens. w/o TT-Latent: use latent tokens while training but ignore them during Test-Time (TT). Highlighted : critical degradation. line methods exhibit inconsistent behavior and lack scaling robustness. Most notably, Coconut performs reasonably on smaller models but degrades sharply at the 8B scale (50.3% 41.5% average), falling below even explicit CoT. This degradation reflects our theoretical motivation: larger models with untied embedding weights suffer severely when hidden states are directly recycled as inputs. LT-Tuning exhibits healthy scaling behavior, with the 8B model achieving nearly double Coconuts accuracy. Adding an adapter layer for the 8B model further improves performance to 70.3%, 6 with notable gains on MultiArith (92.8% 96.1%), confirming that explicit projection improves compatibility in architectures without weight tying. Intrinsic vs. Assistant-based Methods. Assistantbased methods (SoftCoT, SemCoT) show erratic performanceSemCoT achieves 73.5% on ASDiv-Aug but collapses to 6.6% on MultiArith for the 3B model. This volatility suggests that externally generated representations may fail to align with specific reasoning patterns required by different tasks. In contrast, our intrinsic approach constructs latent tokens from the models own distributions, avoiding such alignment failures and delivering stable improvements across all benchmarks. Adaptive Latent Computation for Varying Difficulty. We conducted statistical analysis across the entire test set to examine the relationship between latent computational allocation and problem complexity. To rigorously quantify difficulty, we employed consistency-based metric using Llama-3.1-8B-Instruct. Specifically, each question was sampled five times, and the difficulty score was defined as the aggregate count of incorrect responses (ranging from 0 to 5). As illustrated in Figure 3, we track the average quantity of generated <thinking> tokens across these difficulty tiers for the 1B, 3B, and 8B models. distinct positive correlation is observable, particularly in the 8B model, where the number of the latent tokens grows consistently with problem difficulty. This demonstrates that LT-Tuning effectively empowers the model with difficulty-aware dynamic latent token generation capabilities, achieving desirable balance between inference efficiency and reasoning robustness. Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Figure 4. Visualization of step-wise model entropy and attention weights on latent tokens for Llama-3.1-8B. Shaded regions indicate 1 standard error. Generation steps beyond 400 are truncated for clarity. 5.4. Ablation Study To validate the contribution of each component, we conduct extensive ablation experiments on the 3B and 8B models. We ablate training stages (w/o Stage 2: no curriculum learning; w/o Stage 3: no latent fusion) and two variations of latent thinking strategies: w/o Latent for treating <thinking> tokens as explicit pause tokens (Goyal et al., 2024; Pfau et al., 2024) throughout the pipeline, and w/o TT-Latent for ignoring latent tokens during test-time (Butt et al., 2025). As shown in Table 2, removing Stage 2 reduces average accuracy by 3.9% (3B) and 6.7% (8B), demonstrating the importance of confidence-driven dynamic allocation. Stage 3 and latent reasoning are also critical, with their removal causing substantial performance drops. Notably, the dominant bottleneck differs by scale. For 3B, removing latent reasoning entirely (w/o Latent) leads to the largest degradation (11.3%), indicating that latent reasoning itself is most impactful at smaller scales. In contrast, for 8B, removing Stage 3 (fusion) causes the most severe drop (23.5%), while w/o Latent reduces accuracy by only 7.2%. This supports our hypothesis that larger models suffer more from distribution mismatch, making high-quality latent token construction via fusion essential. Notably, on 8B, w/o Latent (61.6%) significantly outperforms w/o Stage 3 (45.3%), showing that poorly constructed latent tokens can be worse than no latent reasoning at all. The w/o TT-Latent variant shows consistent degradation (9.1% for 3B, 6.5% for 8B), confirming that our latent reasoning at test time is Figure 5. PCA visualization of latent token embeddings across different reasoning steps for intrinsic methods on Llama-3.1-8B (we only show four key steps here for better view). Each point represents different sample from the test set. Coconut (green) exhibits severe feature collapse, where latent tokens from different samples converge to nearly identical points after just two reasoning steps. LT-Tuning w/o Stage 3 (blue) shows initial exploration in early positions but gradually collapses to similar representations in later steps. LT-Tuning (red) maintains semantic diversity even at six latent tokens, demonstrating its effectiveness in mitigating feature collapse while preserving exploration capacity in the latent space. indeed necessary and beneficial. 6. In-Depth Analyses of LT-Tuning To further demonstrate the effectiveness of our latent thinking approach, we conduct in-depth analyses of models trained with LT-Tuning. Generation Entropy and Attention Allocation. We analyzed the generation dynamics by computing the entropy of the output distribution and the attention allocated to <thinking> tokens at each generation step. Specifically, for each token position t, we compute the entropy Ht = (cid:80) pi log pi where pi is the softmax probability of token i. For attention analysis, we extract the last-layer attention weights, average across all heads, and compute the proportion of attention directed to <thinking> token positions. As shown in Figure 4, we tested the models with 100 samples and found that our LT-Tuning can effectively reduce the uncertainty during generation, and have much fewer uncertainty peaks compared with using pause tokens (w/o Latent). Meanwhile, our method allocates substantially Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens more attention to the latent <thinking> tokens compared to the baselines attention allocation on pause tokens. This suggests that the model actively leverages the information encoded in the generated latent tokens during reasoning, rather than merely benefiting from additional computation time as in the pause token approach. Feature Collapse Mitigation. key challenge in latent reasoning is feature collapse, where latent token representations from different samples converge to similar points, causing the model to lose the ability to maintain sample-specific reasoning information. To investigate whether different methods suffer from this problem, we visualize latent token embeddings using Principal Component Analysis (PCA) in Figure 5. Specifically, we prepend six latent tokens for each of twenty samples, extract the input embeddings at each position, and project them into 3D space. We show four key steps (1, 3, 4, 6) for clarity. The visualization reveals critical distinctions among methods. Coconut (green) exhibits severe feature collapse, with latent tokens from different samples converging to nearly identical points after just two reasoning steps. LT-Tuning w/o Stage 3 (blue) shows initial exploration in early positions but gradually collapses in later steps, suggesting that relying solely on hidden states is insufficient. In contrast, LT-Tuning (red) maintains semantic diversity even at Step 6, demonstrating that our fusion mechanism effectively mitigates feature collapse. Figure 6. Effect of hidden layer selection on Llama-3.2-3B. Performance remains stable across different layer indices, indicating that LT-Tuning is robust to this hyperparameter choice. Layer Selection for Context Information. Traditional latent methods select the last hidden states as the initial input embedding for the latent token (Hao et al., 2024; Shen et al., 2025; Wei et al., 2025). So we test the impact of choosing different layers where we get the past context information. Figure 6 shows that performance is relatively robust to the choice of hidden layer for context extraction. Llama-3.2-3B generally shows little performance change when selecting different layers. For Llama-3.1-8B, it is better using the last 8 layer. The related analysis are provided in Appendix C.2. This robustness also suggests that the fusion learning in Stage 3 compensates for suboptimal layer choices, playing more important role in the training framework. 7. Conclusion In this work, we present Latent Thoughts Tuning (LTTuning), novel framework that advances the capability of LLMs to reason within continuous latent space. We identified critical bottleneck in existing paradigms: the distribution mismatch and lack of semantic guidance arising from the direct recurrence of raw hidden states or the reliance on purely probabilistic vectors. To bridge this gap, we proposed the Context-Prediction-Fusion, which synthesizes the dense semantic history of the model with the semantic foresight of the vocabulary distribution. Coupled with our confidence-driven dynamic switching and progressive three-stage curriculum learning, our method effectively boosts the performance and efficiency of latent thinking. Empirical evaluations across model scales from 1B to 8B demonstrate that LT-Tuning significantly outperforms existing baselines on mathematical reasoning benchmarks while successfully mitigating feature collapse, particularly severe issue in larger models with untied embeddings. By enabling LLMs to think with both historical context and predictive structure, LT-Tuning establishes foundation for efficient, robust, and scalable latent cognition. Future work may explore the integration of process-based supervision or reinforcement learning to further refine latent reasoning within these fused latent paths."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning, specifically in improving the reasoning capabilities of Large Language Models (LLM) through latent space computation. Our method enables more efficient and robust reasoning by reducing the reliance on verbose intermediate text generation, which has potential benefits for reducing computational costs and inference latency in deployed systems. We acknowledge several considerations regarding broader impact. On the positive side, more efficient reasoning could democratize access to capable AI systems by reducing computational requirements. The interpretability of our dynamic insertion mechanism, which explicitly marks positions of model uncertainty, may also provide useful signals for understanding model behavior. On the other hand, as with any advancement in language model capabilities, improved reasoning could be misused for generating more convincing misinformation or automating harmful content creation. However, we believe these risks are not uniquely exacerbated by our specific contributions, as our work focuses on the efficiency and robustness Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens of reasoning rather than fundamentally new capabilities. We will release our code and trained models to facilitate reproducibility and encourage the research community to build upon this work responsibly. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024."
        },
        {
            "title": "References",
            "content": "Bae, S., Kim, Y., Bayat, R., Kim, S., Ha, J., Schuster, T., Fisch, A., Harutyunyan, H., Ji, Z., Courville, A., et al. Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level computation. arXiv preprint arXiv:2507.10524, 2025. Butt, N., Kwiatkowski, A., Labiad, I., Kempe, J., and Ollivier, Y. Soft tokens, hard truths. arXiv preprint arXiv:2509.19170, 2025. Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. ISSN 28358856. URL https://openreview.net/forum? id=YfZ4ZPt8zd. Chen, X., Zhao, A., Xia, H., Lu, X., Wang, H., Chen, Y., Zhang, W., Wang, J., Li, W., and Shen, X. Reasoning beyond language: comprehensive survey on latent chainof-thought reasoning. arXiv preprint arXiv:2505.16782, 2025. Cheng, J. and Van Durme, B. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. Universal transformers. In International Conference on Learning Representations, 2018. Deng, Y., Prasad, K., Fernandez, R., Smolensky, P., Chaudhary, V., and Shieber, S. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023. Gatmiry, K., Saunshi, N., Reddi, S. J., Jegelka, S., and Kumar, S. Can looped transformers learn to implement multi-step gradient descent for in-context learning? In International Conference on Machine Learning, pp. 15130 15152. PMLR, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., et al. Deepseekr1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J., and Tian, Y. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. He, Y., Zheng, W., Zhu, Y., Zheng, Z., Su, L., Vasudevan, S., Guo, Q., Hong, L., and Li, J. Semcot: Accelerating chain-of-thought reasoning through semantically-aligned implicit tokens. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Kang, L., Deng, Y., Xiao, Y., Mo, Z., Lee, W. S., and Bing, L. First try matters: Revisiting the role of reflection in reasoning models. arXiv preprint arXiv:2510.08308, 2025. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations, 2019. URL https://openreview. net/forum?id=Bkg6RiCqY7. Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 20802094, 2021. Pfau, J., Merrill, W., and Bowman, S. R. Lets think dot by dot: Hidden computation in transformer language models. In First Conference on Language Modeling, 2024. Renze, M. and Guven, E. Self-reflection in llm agents: Effects on problem-solving performance. arXiv preprint arXiv:2405.06682, 2024. Roy, S. and Roth, D. Solving general arithmetic word In Proceedings of the 2015 conference on problems. empirical methods in natural language processing, pp. 17431752, 2015. Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and Nagarajan, V. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. Seed, B., Chen, J., Fan, T., Liu, X., Liu, L., Lin, Z., Wang, M., Wang, C., Wei, X., Xu, W., et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. 9 Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Shen, Z., Yan, H., Zhang, L., Hu, Z., Du, Y., and He, Y. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arxiv:2502.21074, 2025. Zhou, Y., Wang, Y., Yin, X., Zhou, S., and Zhang, A. R. The geometry of reasoning: Flowing logics in representation space. arXiv preprint arXiv:2510.09782, 2025. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Zhu, R.-J., Peng, T., Cheng, T., Qu, X., Huang, J., Zhu, D., Wang, H., Xue, K., Zhang, X., Shan, Y., et al. survey on latent reasoning. arXiv preprint arXiv:2507.06203, 2025a. Zhu, R.-J., Wang, Z., Hua, K., Zhang, T., Li, Z., Que, H., Wei, B., Wen, Z., Yin, F., Xing, H., et al. Scaling latent reasoning via looped language models. arXiv preprint arXiv:2510.25741, 2025b. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wei, X., Liu, X., Zang, Y., Dong, X., Cao, Y., Wang, J., Qiu, X., and Lin, D. Sim-cot: Supervised implicit chain-ofthought. arXiv preprint arXiv:2509.20317, 2025. Xu, Y., Guo, X., Zeng, Z., and Miao, C. SoftCoT: Soft chain-of-thought for efficient reasoning with llms. In Proceedings of ACL, 2025. Yang, L., Lee, K., Nowak, R. D., and Papailiopoulos, D. Looped transformers are better at learning learning algorithms. In The Twelfth International Conference on Learning Representations, 2023. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Yeo, E., Tong, Y., Niu, M., Neubig, G., and Yue, X. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Yu, Z., Xia, W., Yan, X., XU, B., Zhang, H., Du, Y., and Wang, J. Self-verifying reflection helps transformers with cot reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Zhang, Z., He, X., Yan, W., Shen, A., Zhao, C., Wang, S., Shen, Y., and Wang, X. E. Soft thinking: Unlocking the reasoning potential of llms in continuous concept space. arXiv preprint arXiv:2505.15778, 2025. 10 Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens A. Dataset Statistics Table 3 summarizes the metadata of the training data (GSM8K-NL) and evaluation benchmarks used in our experiments. All datasets focus on mathematical word problems requiring multi-step arithmetic reasoning."
        },
        {
            "title": "Dataset",
            "content": "#Train #Test GSM8K-NL ASDiv-Aug MultiArith SVAMP 7,473 4,180 420 700 1,319 1,041 180 300 Table 3. Statistics of the evaluation datasets used in our experiments. B. Training Configuration B.1. Training Hyperparameters Table 4 presents the detailed training hyperparameters for each model scale across all three stages of LT-Tuning. We adopt different batch sizes and learning rates to accommodate the varying memory requirements and optimization dynamics of different model sizes. Model Stage LR BS Epochs Optimizer Scheduler Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B Stage 1 (CoT) Stage 2 (Dynamic) Stage 3 (Fusion) Stage 1 (CoT) Stage 2 (Dynamic) Stage 3 (Fusion) Stage 1 (CoT) Stage 2 (Dynamic) Stage 3 (Fusion) 5e-5 5e-5 5e-5 5e-5 5e-5 5e-5 1e-5 1e-5 1e-5 32 16 16 16 8 8 4 4 1 2 7 1 2 7 1 1 3 AdamW AdamW AdamW AdamW AdamW AdamW AdamW AdamW AdamW Cosine Cosine Cosine Cosine Cosine Cosine Cosine Cosine Cosine Table 4. Training hyperparameters for different model scales. LR denotes learning rate and BS denotes batch size. For all experiments, we use AdamW optimizer (Loshchilov & Hutter, 2019) with β1 = 0.9, β2 = 0.999, and weight decay of 0.01. C. Stage-Specific Implementation Details C.1. Stage 1: Explicit Reasoning Warm-up In the first stage, we perform standard supervised fine-tuning on Chain-of-Thought (CoT) data. The CoT annotations are sourced from the GSM8K training set, where each problem is paired with step-by-step natural language solution. We use the following prompt template: Stage 1 Prompt Template {question} {cot reasoning} The final answer is:n### {answer}. Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Model Threshold τ #Maximum Latent Tokens per Insert Layer Selection Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B 0.7 0.7 0. 2 2 4 -2 -2 -1 Table 5. Confidence threshold τ , hidden states layer selection and latent token configurations. C.2. Stage 2: Dynamic Latent Tokens Generation Confidence Threshold Selection. The confidence threshold τ controls the granularity of latent token insertion. lower threshold results in fewer latent tokens (only for highly uncertain predictions), while higher threshold leads to more frequent latent reasoning. Table 5 shows the threshold values used for different model scales. For the number of latent tokens per insert, we will randomly insert 0 latent tokens for each candidate position according to the models confidence. Hidden States Layer Selection. As described in Section 6, our method is relatively robust to the hidden states layer selection when applied to small models (e.g., Llama-3.2-3B). However, it does have larger impact on the 8B model if we do not use the last few layers  (Table 6)  . We guess the reason is that intermediate hidden states is harder for large models to interpret due to the distribution misalignment. In this case, we generally use the last or the penultimate layer as shown in Table 5. negative index refers to the layer position counted backward from the end. Model GSM8K-NL ASDiv-Aug MultiArith SVAMP Average Llama-3.2-8B Layer-1 Layer-2 Layer-3 58.1 56.6 57. 72.2 69.8 49.3 92.8 93.9 89.4 52.3 54.0 40.0 68.8 68.5 59.1 Table 6. Result for different hidden states layer selection of Llama-3.1-8B. Data Preprocessing Pipeline. For each training sample, we perform the following preprocessing steps: 1. Run forward pass with the checkpoint from the previous training stage to obtain token-level prediction confidences. 2. Identify positions where pθ(yty<t) < τ . 3. Randomly insert 0 latent tokens for each candidate position according to the model confidence. 4. Store the modified sequences with latent token annotations. C.3. Stage 3: Context-Prediction Fusion Fusion Hyperparameters. Table 7 presents the hyperparameters for the semantic-predictive fusion mechanism in Stage 3. Model Fusion α Temperature Top-p Adapter Hidden Dim Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B 0.6 0.6 0.6 1.0 1.0 1. 0.8 0.8 0.9 1024 Table 7. Fusion mechanism hyperparameters for Stage 3. Adapter Architecture for 8B Model. Since Llama-3.1-8B does not apply weight sharing between the input embedding layer and the output projection head, distributional mismatch exists between the hidden state space and the embedding space. To address this, we introduce lightweight adapter module: Adapter(h) = Wup GELU(Wdown h), (8) where Wdown Rddhidden projects the hidden state to lower-dimensional space, and Wup Rdhiddend projects it back to the embedding dimension. The adapter is trained jointly with the model parameters during Stage 2 and Stage 3. Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens D. Inference Configuration During inference, we use greedy decoding for deterministic evaluation. The model dynamically generates <thinking> tokens based on its learned confidence patterns from Stage 2 and Stage 3. Answer Extraction. For all datasets, we extract the final numerical answer using regex pattern that identifies the last number in the models output. E. Baseline Implementation Details To ensure fair comparison, we re-implement or adapt the baseline methods under the same experimental settings. Table 8 summarizes the key implementation details for each baseline. For fair comparison, all baselines start from the same our Stage-1 checkpoint (Explicit CoT fine-tuned model) as our method. We then train each baseline on the GSM8K training set using their official code and evaluate on the test sets of all four benchmarks. Method CoT Fine-tuning Coconut (Hao et al., 2024) Implementation Notes Standard supervised fine-tuning on CoT data. We trained the model using the official implementation with GSM8K-NL training set. Soft-Thinking (Zhang et al., 2025) We implemented the soft-thinking method on the model SoftCoT (Xu et al., 2025) SemCoT (He et al., 2025) trained with the original CoT data. We used the CoT-tuned model as the larger model, while the assistant model was the original version since it required training. Similar to SoftCoT. Notably, we used the same model size for the main model and the assistant model for the 1B setting. Table 8. Key Implementation details for baseline methods. F. Qualitative Examples We provide qualitative examples to illustrate the behavior of LT-Tuning. Figures below show few examples of the reasoning trajectories compared between LT-Tuning and regular CoT reasoning for Llama-3.1-8B. We can see that incorporating latent tokens enhances the reasoning capabilities of LLMs and achieve higher accuracy. 13 Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Example Question: Six years ago, when Noah was half as old as Cera, the population of Chile was half what it is now. Currently, Cera is 46 years old. If the population of Chile six years ago was 3000 times the age of Noah, calculate the population of Chile now. Model Output (CoT): ## Step 1: How old was Noah six years ago? ** If Cera is 46 years old today, six years ago, when Noah was half as old as Cera, Noah was 46-6=<<46-6=40>>40 years old.n## Step 2: How many people were in Chile six years ago? ** If the population of Chile six years ago was 3000 times the age of Noah, there were 3000*40=<<3000*40=120000>>120000 people in Chile six years ago.n## Step 3: How many people are in Chile now? ** Since the population of Chile is currently twice what it was six years ago, there are 120000*2=<<120000*2=240000>>240000 people in Chile now.nThe final answer is :n### 240000 Model Output (LT-Tuning): <thinking><thinking>## Step 1: How old was Cera six years ago? ** If Cera is currently<thinking> 46 years old, six years ago she was 46-6 = <<46-6=40 <thinking>>>40 years old.n## Step 2: How old was Noah six years ago? ** Noah was half Cera's age six years ago, meaning he was 1/2*40 = 20 years old .n## Step 3: What was the population of Chile six years ago? ** If the population of Chile six years ago was 3000 times the age of Noah, it was 3000*20 = <<3000<thinking><thinking>*20=60000>>60000n## Step 4: What is the population of Chile now? ** Noah and Cera are now 46<thinking>+20 = <<46+20=66>>66 years old, 66 years<thinking> after the population of Chile was 60000.n## Step 5: What is the population of Chile now? ** The population of Chile now is 60000*2 = <<60000*2=120000>>120000nThe final answer is:n### 120000 14 Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Example 2 Question: Farmer Brown has 60 animals on his farm, all either chickens or cows. He has twice as many chickens as cows. How many legs do the animals have, all together? Model Output (CoT): ## Step 1: How many chickens are there? ** There are 60/3=<<60/3=20>>20 chickensn## Step 2: How many cows are there? ** There are 20/2=<<20/2=10>>10 cowsn## Step 3: How many legs do the animals have, all together? ** There are 20+10=<<20+10=30>>30 legs for the cowsn## Step 4: How many legs do the animals have, all together? ** There are 20*2=<<20*2=40>>40 legs for the chickensn## Step 5: How many legs do the animals have, all together? ** There are 30+40=<<30+40=70>>70 legs in total nThe final answer is:n### 70 Model Output (LT-Tuning): <thinking><thinking>## Step 1: How many<thinking><thinking> cows are there? ** There are 60/3=<<60/3=20>>20<thinking> cows on the farm.n## Step 2: How many chickens are there? ** There are 20*2=<<20*<thinking>2=40>>40 chickens on the farm.n## Step 3: How many legs do the cows have? ** The cows have 20*4=<thinking><<20*4=80>>80 legs.n## Step 4: How many legs do the chickens have? ** The chickens have 40*2=<<40*2=<thinking><thinking>80>>80 legs.n## Step 5: How many legs do the animals have, all together? ** The animals have 80+80=<<80+80=160>>160 legs.nThe final answer is:n### 160 Example 3 Question: Suzie loves to chew fruit-flavored gum. She bought four packs of gum the last time she was at the store. She got two packs of her favorite flavor, strawberry. She paid $2 for pack of grape gum that she also liked. She wanted to try something new, so she paid half as much for small pack of green apple gum. If she paid $7 in all, how many dollars did each pack of strawberry gum cost? Model Output (CoT): ## Step 1: How much did the green apple gum cost? ** The green apple gum cost $7 / 2 = $<<7/2=3.50>>3.50.n## Step 2: How much did the grape gum cost ? ** The grape gum cost $2 + $3.50 = $<<2+3.5=5.50>>5.50.n## Step 3: How much did the two packs of strawberry gum cost? ** The two packs of strawberry gum cost $5.50 + $5.50 = $<<5.5+5.5=11>>11.n## Step 4: How much did each pack of strawberry gum cost? ** Each pack of strawberry gum cost $11 / 2 = $<<11/2=5.50>>5.50.nThe final answer is:n### 5.50 Model Output (LT-Tuning): <thinking> **<thinking><thinking> The grape gum cost $2, and the green apple gum cost $2 / 2 = $<<2/2=1>>1.n<thinking><thinking>** The strawberry gum cost<thinking> $7 - $2 - $1 = $<<7-2-<thinking>1=4>>4.n<thinking>** Each pack of strawberry gum cost $<thinking>4 / 2 = $<<4/2=2>>2.nThe final answer is:n### 2 Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Example 4 Question: basket contains 25 oranges among which 1 is bad, 20% are unripe, 2 are sour and the rest are good. How many oranges are good? Model Output (CoT): There are 25 - 1 - 20% - 2 = <<25-1-20*.01-2=2>>2 good oranges in the basket .nThe final answer is:n### 2 Model Output (LT-Tuning): <thinking><thinking> ## Step 1: How many oranges are unripe? ** The number of oranges that are unripe is 25 * 20% = <<25*20*.01<thinking>=5>>5 <thinking>n## Step 2: How many oranges are bad or unripe or sour? ** The total number of oranges that are bad or unripe or sour is 5 + 1 + 2 = <<5 <thinking>+1+2=8>>8n## Step 3: How many oranges are good? ** The number of oranges that are good is <thinking>25<thinking> - 8 = <<25-8=17>>17nThe final answer is:n### 17 Example 5 Question: Jade bought new cell phone with an incredible camera, so she spent all week taking pictures of her daily life. At the end of the week, she had taken 210 photos. She wants to upload all her photos on her Instagram. If she uploads 7 photos in one batch and she uploads 6 batches each day, how many days will she need to upload all of her photos? Model Output (CoT): ** First find the total number of batches Jade needs to upload all of her photos: 210 photos / 7 photos/batch = <<210/7=30>>30 batchesn## Step 1: How many batches does Jade upload each day? ** Then find the total number of batches Jade uploads each day: 6 batches + 30 batches = <<6+30=36>>36 batchesn## Step 2: How many days will Jade need to upload all of her photos ? ** Then divide the total number of batches by the number of batches she uploads each day to find the number of days it will take her to upload all of her photos: 36 batches / 36 batches/day = <<36/36=1>>1 daynThe final answer is:n### 1 Model Output (LT-Tuning): <thinking><thinking>## Step 1: How many photos does Jade upload each day? ** Jade uploads 7 6 = <<7*6=42>>42 photos<thinking> each day.n## Step 2: How many days will Jade need to upload all of her photos? **<thinking> <thinking> She will need 210 / 42 = <<210/42=5<thinking>>><thinking>5 days to upload all of her photos.nThe final answer is:n###"
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Illinois Chicago, Chicago, IL, USA"
    ]
}