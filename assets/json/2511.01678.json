{
    "paper_title": "UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback",
    "authors": [
        "Ropeway Liu",
        "Hangjie Yuan",
        "Bo Dong",
        "Jiazheng Xing",
        "Jinwang Wang",
        "Rui Zhao",
        "Yan Xing",
        "Weihua Chen",
        "Fan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Relighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at https://github.com/alibaba-damo-academy/Lumos-Custom."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 8 7 6 1 0 . 1 1 5 2 : r UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback Ropeway Liu1,2,, Hangjie Yuan2,3,1,, Bo Dong2,3, Jiazheng Xing1,2,4, Jinwang Wang2,3,1, Rui Zhao4, Yan Xing2,3, Weihua Chen2,3, Fan Wang2 1Zhejiang University, 2DAMO Academy, Alibaba Group, 3Hupan Lab, 4National University of Singapore Equal contributions. Corresponding author. {yuanhangjie.yhj, kugang.cwh}@alibaba-inc.com Relighting is crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic resultssuch as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, unified relighting framework for both images and videos that brings RGB-space geometry feedback into flow-matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering 20x speedup for both image and video relighting. Code is available at https://github.com/alibaba-damo-academy/Lumos-Custom. Date: November 4,"
        },
        {
            "title": "1 Introudction",
            "content": "Relighting, altering illumination in images or videos while preserving intrinsic scene attributes such as geometry, reflectance, and content, is longstanding problem in computer vision and graphics [33, 55]. It underpins wide range of applications in film production, gaming, and augmented reality, where seamless lighting integration is critical to visual fidelity. Beyond realism, lighting conveys rich aesthetic and semantic cuesit defines atmosphere, evokes emotion, and reinforces narrative structure. Relighting is thus not only technical challenge but also creative tool that shapes how characters, objects, and environments are perceived. However, achieving physically consistent lighting remains core challengerequiring the alignment of illumination effects with different scene attributes across both space and time. To address this, traditional approaches often rely on inverse rendering pipelines [46, 50, 2] that estimate intrinsic scene properties, such as geometry, reflectance, and environmental lighting, from input images. While these methods provide physically grounded results, they typically require complex inputssuch as high dynamic range images or spherical harmonics coefficientsand are limited to constrained domains. This makes them impractical for real-world applications, where users often provide only single image, short video, or high-level lighting prompt as input. These limitations underscore the need for new paradigmone that can deliver high-quality, physically plausible relighting while operating under minimal and naturalistic input conditions. Recent diffusion-based relighting methods [49, 6, 12, 57] have shown promise by leveraging large-scale image and video datasets to produce diverse and controllable lighting effects under various user-defined conditions, 1 Figure 1 UniLumos performs physically plausible image and video relighting, conditioned on textual prompts and reference videos. such as reference images or text prompts. However, this strength reveals fundamental weakness: diffusion models typically operate in semantic latent space, where similarity does not guarantee physical correctness in the visual domain. As result, they often fail to respect scene geometry, especially in complex scenes with dynamic lighting or temporal constraints. For example, IC-Light [49] and SynthLight [6], which are primarily designed for image relighting, lack both temporal modeling and explicit physical supervision in the visual domain. Instead, they rely on latent-space representations, such as MLP-based embeddings (IC-Light) or multi-stage training (SynthLight), which often lead to artifacts like misaligned shadows, overexposed highlights, or incorrect lighting directionsparticularly under complex geometry or extreme illumination. Light-A-Video [12] and RelightVid [12] extend these methods to the video domain, aiming to improve temporal coherence while retaining the visual quality of diffusion-based relighting. Light-A-Video is training-free framework that combines IC-Light with pre-trained video diffusion model via iterative alignment. While this improves frame-wise consistency, it incurs high inference costs due to multiple model passes. RelightVid adopts joint training strategy with video diffusion backbone, which enhances temporal stability compared to training-free approaches. However, it still operates without explicit physical supervision, resulting in inaccurate light-scene interactions and limited generalization to complex or dynamic environments. In short, existing diffusion-based methods excel in synthesizing plausible appearances but fall short in enforcing the physical plausibility that is essential for high-quality relighting. To bridge the gap between generative flexibility and physical correctness, we propose UniLumos, unified relighting framework for both images and videos that brings RGB-space geometry feedback into flow-matching 2 backbone. Unlike existing diffusion-based approaches that operate purely in latent space, UniLumos introduces physics-plausible feedback mechanism that supervises generation with dense geometric signalsspecifically, depth and surface normalsestimated from its outputs. These lighting-invariant cues serve as ideal supervision signals, enabling the model to explicitly align illumination with the scene structure, significantly improving shadow alignment, shading consistency, and spatial coherence. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning [13], allowing supervision to remain effective even under few-step training regimes. Beyond model-level improvements, existing relighting methods lack structured illumination descriptions and dedicated evaluation metrics. Generic generation scores (e.g., FID, LPIPS) fail to capture lighting-specific errors such as shadow misalignment, intensity mismatch, or incorrect light direction. To address this, we introduce LumosData, scalable data pipeline that extracts diverse relighting pairs from real-world videos. At its core is structured six-dimensional annotation protocol covering direction, light source type, intensity, color temperature, temporal dynamics, and optical phenomenaenabling both fine-grained conditioning during training and physically grounded evaluation at test time. Building upon this, we propose LumosBench, disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Our contributions are summarized as follows: Unified Relighting with Physics-Plausible Feedback: We propose UniLumos, unified relighting framework for both images and videos that incorporates RGB-space geometry feedback into flow-matching backbone, explicitly aligning lighting effects with the scene structure to enhance the physical plausibility of relighting. Structured Illumination Annotation and Evaluation Benchmark: We design structured six-dimensional annotation protocol that captures core illumination attributes, enabling fine-grained control and supervision. Building upon this, we introduce LumosBench, disentangled attribute-level benchmark that leverages large vision-language models to automatically and interpretably evaluate relighting controllability across individual lighting dimensions. Extensive Validation: Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering 20x speedup for both image and video relighting."
        },
        {
            "title": "2 Related Work",
            "content": "Video Diffusion Models. Recent advances in video diffusion models [3, 4, 7, 38] have enabled the generation of temporally coherent video sequences conditioned on various inputs such as text or images. In the field of text-to-video (T2V) generation [45], most methods extend existing text-to-image diffusion backbones with additional modules that capture temporal dynamics across frames. In contrast, few approaches train video diffusion models from scratch to directly learn spatiotemporal priors [38]. For image-to-video (I2V) tasks, where static images are animated with plausible motion, several methods propose specialized architectures tailored for image animation [35, 40]. Other strategies offer lightweight, plug-and-play adapters that can be integrated into pre-trained models. Stable Video Diffusion [2], for example, fine-tunes T2V models for I2V tasks, achieving state-of-the-art performance. Beyond synthesis quality, growing body of work emphasizes controllability, allowing users to guide generation with fine-grained constraints [52, 48]. Relighting Methods. Recent advances in deep neural networks have significantly improved lighting control for 2D and 3D visual content, especially in portrait relighting. Methods such as Relightful Harmonization [32], SwitchLight [21], ConceptSliders [14], Intrinsic Image Diffusion [22], Neural Gaffer [20], DI-Light [44], SynthLight [6], and IC-Light [49] demonstrate progress in realism and controllability. While numerous portrait relighting approaches exist [31, 47, 42, 5], most of them rely heavily on portrait-specific priors. In contrast, UniLumos is designed as general-purpose relighting framework that is not constrained to any particular object category. With the rise of diffusion-based generative models, approaches like LumiSculpt [51] extend lighting control to text-to-video (T2V) generation. Moreover, RelightVid [12] and Light-A-Video [57] implemented 3 video relighting based on IC-Light. However, achieving both precise lighting control and high visual quality in video remains challenging due to the trade-off between spatial realism and temporal consistency. Feedback Learning in Generative Models. Feedback learning has become powerful tool to improve output alignment in generative models, from language systems trained with human preferences [36, 30] to visual diffusion models guided by aesthetic or attribute-based rewards [9, 43, 37, 25], e.g., InstructVideo [43] and DRaFT [10]. In visual domains, feedback can also be physicalfor example, using geometric cues to guide generation toward realism. Recent advances in distillation and consistency training [15, 34, 29, 39, 53, 13] have accelerated diffusion inference by reducing the number of denoising steps, enabling models to recover high-quality RGB outputs with just few iterations. However, most existing techniques focus on appearance synthesis and overlook geometry-aware feedback, which typically requires high-fidelity outputs and is incompatible with few-step inference. UniLumos bridges this gap by combining physically grounded supervision with path-consistency learning, enabling efficient and physically plausible relighting under fast sampling regimes."
        },
        {
            "title": "3 Preliminaries",
            "content": "Problem Formulation. Given an image or video S1 RT HW with intrinsic scene properties (e.g., geometry, reflectance, content) under initial illumination L1, the goal of relighting is to modify the illumination within subject region specified by binary mask {0, 1}T HW to match target lighting condition C. The condition may take the form of an image, video, or text description and implicitly defines desired illumination field L2. The relit output S2 RT HW should exhibit lighting consistent with L2 in the masked region while preserving the intrinsic attributes of S1. This can be formulated as conditional generation problem: S2 = fθ(S1, C, M), (1) where fθ is the relighting model parameterized by θ. Flow Matching. To efficiently model complex illumination transformations in relighting, we build upon Wan2.1 [38], foundation video generation model based on flow matching [28, 11]. Flow matching formulates generative modeling as learning velocity fields between noise x0 (0, I) and data x1, using linear interpolation: xt = x1 + (1 t) x0, vt = dxt dt = x1 x0, (2) where [0, 1] is sampled from logit-normal distribution. The model learns to predict vt from xt, conditioned on timestep and context (e.g., text embeddings), by minimizing the mean squared error: L0 = Ex0,x1,t,c vθ(xt, t, c) vt2 2 . (3) Path Consistency Learning. To further accelerate inference, we adopt path consistency learning [13], which encourages consistent velocity predictions under larger integration steps. Given velocity field vθ and step size > 0, we recursively define: Moreover, enforce two-step consistency using: xt+d = xt + vθ(xt, t, d), Lfast = Ext,t,d (cid:13) (cid:13) (cid:13) (cid:13) vθ(xt, t, 2d) 1 2 [vθ(xt, t, d) + vθ(xt+d, + d, d)] (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 . (4) (5) This objective enables the model to learn shortcut-consistent velocity fields without separate teacher-student stages, allowing for fast and high-quality generation with arbitrary step budgets."
        },
        {
            "title": "4 Methodology",
            "content": "We present UniLumos, unified framework for physically plausible image and video relighting, as illustrated in Fig. 2. Built upon Wan 2.1 [38], flow-matching diffusion model for video generation, UniLumos relights 4 Figure 2 The overall pipeline of UniLumos. The left is LumosData, our proposed data construction pipeline, which consists of four stages for generating diverse relighting pairs from real-world sources. The right shows the architecture of UniLumos, unified framework for image and video relighting, designed to achieve physically plausible illumination control. images and videos under user-specified lighting conditionsincluding reference images, video clips, or text promptswhile preserving scene content and temporal coherence. To bridge the gap between semantic generation and physical correctness, UniLumos incorporates two key innovations: (1) physics-plausible feedback that supervises the model with geometry signals from RGB space, and (2) structured illumination annotation protocol that enables fine-grained control and evaluation. We jointly train the model with geometry-aware supervision and lighting-conditioned objectives, achieving high-quality and efficient few-step inference."
        },
        {
            "title": "4.1 Physics-Plausible Feedback",
            "content": "While most relighting methods rely on photometric reconstruction or latent-space consistency, such signals offer limited geometric groundingoften resulting in misaligned shadows, implausible shading, and incorrect light directions. To address this, UniLumos enforces consistency between generated illumination and underlying scene geometry, promoting more realistic lightscene interactions. As illustrated in Fig. 2 (right), we introduce physics-plausible feedback that guides the generation process using geometry-aware supervision. This component complements the flow-matching architecture with explicit structural priors, enhancing physical plausibility without altering the models inference inputs. We adopt depth and surface normals as our supervisory targets due to their generality, accessibility, and strong disentanglement from illumination. Unlike shadow masks or material properties, which are often ambiguous, entangled with lighting, or costly to obtain, monocular depth and normal maps capture intrinsic scene structure. They can be reliably estimated by pre-trained dense estimator (e.g., Lotus [17]). Specifically, after decoding the predicted latent variable into RGB frames via the Wan-VAE Decoder [38], we extract estimated depth ˆD RT HW and normals ˆN RT HW using frozen dense estimator. These are 5 compared against pseudo-ground-truth maps (D, N) from the reference input to compute the geometry-aware feedback loss: Lphy = Ex0,x1,t,c (cid:34) (cid:32) ˆD D2 D2 + ˆN N2 (cid:33)(cid:35) , (6) where RT HW denotes the foreground subject mask. This feedback encourages the model to align its lighting predictions with consistent structural interpretation while keeping inference lightweight and geometry-free. However, the proposed physics-plausible feedback requires supervision in the RGB domain, which relies on high-quality predictions that are typically only available after full-step denoising has been completed. This poses major computational bottleneck for standard diffusion models. To mitigate this, we adopt path consistency learning [13], which reformulates denoising as velocity regression task, thereby supporting practical training under few-step regimes. Enforcing consistency between intermediate outputs and final predictions enables reliable geometric feedback without sacrificing inference efficiency."
        },
        {
            "title": "4.2 Structured Illumination Annotation and Evaluation Benchmark\nIn the problem formulation, the relighting task involves conditioning on a target illumination descriptor C.\nHowever, most prior work treats C as unstructured prompts—such as text, images, or reference frames—offering\nlimited control or interpretability. Moreover, conventional evaluation metrics, such as FID or LPIPS, focus\non perceptual similarity but fail to capture lighting-specific discrepancies, such as shadow misalignment or\nintensity mismatches.",
            "content": "To address this, we construct LumosData, scalable dataset pipeline that enriches with structured lighting semantics. As shown in Fig. 2 (left), we extract relighting pairs from real-world videos. Given an input sequence Vreal R[T +1,H,W,3], we first obtain subject masks {0, 1}[T +1,H,W ] using BiRefNet [54] to isolate the foreground. We then apply pre-trained relighting model such as IC-Light [49] to generate synthetic relit versions Vdeg under diverse lighting conditions, guided by curated prompt set. To avoid entanglement with background semantics, we inpaint the background using Gaussian noise, ensuring clean illumination signals without introducing artifacts. Beyond this relighting pipeline, LumosData introduces structured six-dimensional annotation protocol that covers direction, intensity, color temperature, light source type, temporal dynamics, and optical phenomena. These attributes are automatically generated using vision-language models (e.g., Qwen2.5-VL [1]) with carefully designed prompts, and are integrated into to provide an enriched semantic label. This protocol serves dual purposes: (1) Fine-grained conditioning: During training, the model is guided by explicit lighting attributes embedded in C, promoting more interpretable and controllable generation across scenarios. (2) Attributealigned Benchmark: Building upon the same attribute protocol, we construct LumosBench. This automatic benchmark uses vision-language models to assess whether generated outputs accurately reflect intended lighting conditions, enabling interpretable, attribute-level controllability evaluation beyond pixel-based metrics. Each training tuple is structured as (Vdeg, Vbg, M, C) Vreal, where the model learns to restore realistic lighting consistent with the semantic cue and structural context. LumosData introduces rich diversity in content and illumination by leveraging variety of real-world sources. Built on Panda70M [8], we curate 110K high-quality video pairs and augment training with 1.2M additional relit images using IC-Light. This combination supports robust learning of physically plausible relighting without relying on expensive hardware or manual annotations. See more details in Appendix B."
        },
        {
            "title": "4.3 Joint Learning Objective and Training Strategy\nModel Implementation. The inputs of aligned videos (Vdeg, Vbg, Vreal) are passed through a Wan-VAE\nEncoder [38] to obtain semantic latent representations (xdeg, xbg, x0). During training, we generate the noisy\nlatent input xt via Eq. 2, and concatenate it with the strong conditional signals xdeg and xbg along the\nchannel dimension. This combined tensor is injected into the DiT blocks of the Wan backbone. Additionally,\nto support path-consistency learning, the diffusion step t and the expected denoising step d are appended as",
            "content": "6 temporal condition vectors. All new projection and fusion layers are initialized with zero weights to preserve compatibility with the pre-trained Wan initialization and ensure stable optimization from the outset. Joint Objective. Our training objective integrates three complementary losses to balance appearance fidelity, geometric consistency, and fast inference. The full loss is defined as: = λ0L0 + λ1Lfast + λ2Lphy, (7) where L0 is the standard flow-matching loss that aligns the predicted velocity field with the ground-truth field, Lfast is the path consistency loss that improves model performance under few-step denoising regimes, and Lphy is physics-guided loss that supervises the RGB outputs using estimated depth and normal maps. We adopt fixed weights of λ0 = 1.0 and λ1 = λ2 = 0.1 for all experiments. This unified objective encourages the model to generate relit results that are photorealistic, temporally smooth, and physically grounded while supporting efficient inference without sacrificing output quality. Algorithm 1 Loss Sampling Strategy per Iteration Require: Batch size B, total training samples 1: for each training iteration do 2: 3: 4: 5: Training Strategy. To balance physical supervision and training efficiency, we adopt selective optimization strategy inspired by path consistency scheduling [13]. In each training iteration, we divide the batch based on supervision type, following an 80/20 split to avoid prohibitive costs from full supervision while still maintaining effective learning signals. As shown in Alg. 1, 20% of each batch is allocated to compute the path consistency loss Lfast, which involves three forward passes and one backward pass to enforce consistency across timesteps. The remaining 80% is used for the standard flowmatching loss L0, with 50% of these samples further supervised using RGB-space geometry feedback via Lphy (i.e., depth and normal alignment). This probabilistic scheduling ensures high training throughput while allowing the model to benefit from multi-level supervision. To further enhance illumination diversity during training, we apply randomized lighting augmentations on the degraded subject Vdeg, which introduces realistic lighting variability without the need for explicitly paired captures. Randomly sample 20% of batch Lfast Compute path consistency loss: 3 forward, 1 backward Among those, 50% RGB reconstruction Compute physics-guided loss Lphy 6: 7: 8: end for Remaining 80% L"
        },
        {
            "title": "5.1 Experimental Details\nTraining Details. We adopt the Wan2.1-T2V-1.3B-480P [38] as the base model, and initialize all new trainable\nparameters with zeros to minimize its influence at the beginning of training. We use the AdamW optimizer\nwith the learning rate of 1e-5 for training the entire framework. All the models are trained with a batch size\nof 8 for 5,000 iterations on 8 NVIDIA H20 GPUs (with 96GB RAM).",
            "content": "Baselines. We compare UniLumos against range of representative image and video relighting methods. For image-based relighting, we include SwitchLight [21], DiLightNet [44], IC-Light [49], and SynthLight [6], which leverage various forms of latent modeling or light disentanglement to relight single images. For video relighting, we apply IC-Light via frame-by-frame and include Light-A-VideoCogVideoX-2B[41] and another using Wan 2.1 T2V-1.3B [38]. These baselines together represent state-of-the-art performance across both image and video relighting settings. Dataset. For testing, we selected samples from the internal dataset, processed using the method described in Sec. B. These samples were evenly split: half for image generation at 768x512 resolution, and half for video generation at 480p resolution (832x480), with each video sample containing 49 frames. To further demonstrate the models generalization to non-human scenes, we conducted additional evaluations on two public object-centric relighting benchmarks: StanfordOrb [24] and Navi [19], which include objects and sculptures under variety of lighting environments and are completely disjoint from our training data, and see more results in Appendix C.1. 7 Table 1 Quantitative comparison. Bold number indicate the best performance. PSNR (a) Quality SSIM LPIPS (b) Temporal Consistency R-Motion (c) Lumos Consistency Avg. Score Dense L2 Error Model SwitchLight [21] DiLightNet [44] IC-Light [49] SynthLight [6] UniLumos 20.483 21.894 24.316 25. Image Relighting 0.094 0.131 0.108 0.102 0.901 0.860 0.884 0.905 26.719 0.913 0.089 IC-Light Per Frame [49] Light-A-Video [57] + CogVideoX[41] Light-A-Video [57] + Wan2.1[38] 20.132 19.851 20.784 Video Relighting 0.133 0.124 0.129 0.851 0.859 0.876 UniLumos 25.031 0. 0.109 - - - - - 2.437 1.784 1.582 1.436 0.717 0.682 0.703 0. 0.912 0.672 0.641 0.682 0.871 0.388 0.401 0.447 0.214 0.103 0.432 0.383 0. 0.147 Evaluation metrics. We evaluate relighting performance across three key dimensions: (1) visual fidelity: We assess image quality using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). For video relighting, we report the average metric across all frames. (2) temporal consistency: Following VBench [18], we adopt the R-Motion metric, which measures temporal smoothness using motion priors from pre-trained video frame interpolation model [27]. This captures the coherence of lighting transitions across frames. (3) lumos consistency: (i) Lumos Score, computed by applying the same caption-based lighting annotation protocol as in our LumosData construction. Six lighting attributes are predicted and compared with targets, and each is weighted equally to yield an average consistency score. (ii) Dense L2 Error, which quantifies the relative L2 error between predicted and reference depth/normal maps, estimated via pre-trained geometry model (e.g., Lotus [17]). This provides physically grounded measure of illumination-geometry alignment."
        },
        {
            "title": "5.2 Main Results\nQuantitative Evaluation. As shown in Tab. 1, UniLumos delivers consistent improvements across three key\ndimensions: visual fidelity, temporal consistency, and physically grounded lighting alignment. (1) Visual\nFidelity. UniLumos produces higher-quality relighting results across both images and videos. Benefiting\nfrom structured lighting supervision and geometry-guided feedback, our model generates outputs with clearer\nshading, sharper details, and more coherent illumination compared to prior works. (2) Temporal Consistency.\nFor video relighting, UniLumos ensures smoother transitions and reduced flickering artifacts. Our use of flow-\nmatching architecture and path consistency learning helps maintain stable lighting across frames, addressing\na key limitation in frame-wise or training-free methods.",
            "content": "(3) Lumos Consistency. Going beyond appearancebased evaluation, UniLumos aligns well with intended lighting semantics. Through structured caption conditioning and physics-guided training, the model better preserves lighting direction, tone, and geometryvalidated by both vision-language alignment and dense geometric error metrics. Efficiency. To assess inference efficiency, we evaluate the video relighting task under standardized setting, generating 49-frame videos at 480p resolution. As shown in Fig. 5, UniLumos achieves significant speedup compared to prior methods, benefiting from its geometry-free inference and few-step generation. While existing models, such as Light-A-Video or IC-Light, require either iterative frame-by-frame processing or complex sampling schedules, UniLumos completes generation over 20 times faster without sacrificing visual fidelity or physical plausibility. This efficiency advantage makes it well-suited for real-time relighting applications and scalable deployment scenarios. Figure 5 Comparison of inference time costs of different methods under the same settings. 8 Figure 3 Qualitative comparison of baseline methods. Each method takes subject video and textual illumination description as input, generating the related subject with the corresponding background under the specified lighting condition. Qualitative Results We present qualitative comparisons in Fig. 3 and Fig. 4, highlighting the advantages of UniLumos in terms of lighting realism, temporal coherence, and controllability. (1) Lighting Quality and Controllability: In Fig. 3, UniLumos produces lighting effects that better match the target description, capturing nuanced directional shadows, color tone, and intensity. Competing methods either fail to reflect the intended lighting change or produce overly uniform results that lack realism. (2) Temporal Consistency: Compared to baseline methods such as frame-wise IC-Light and Light-A-Video, UniLumos achieves smoother frame transitions without flickering or structural distortion. This benefit arises from the joint modeling of space and time, which is further reinforced by physics-aware supervision and path consistency training. (3) Foreground Detail Preservation: UniLumos preserves fine subject detailssuch as facial structure and clothing texturebetter than baselines. For instance, Light-A-Video occasionally introduces deformation or identity drift, while our model maintains high fidelity over long sequences. (4) Relighting with Reference Videos: Fig. 4 showcases UniLumos conditioned on different reference videos. The model successfully adapts both global lighting direction and subtle spatial variations across scenes, demonstrating strong generalization under diverse illumination cues. LumosBench To evaluate the fine-grained controllability of lighting generation, we introduce LumosBench, structured benchmark that targets six core illumination attributes defined in our annotation protocol. Unlike prior works that treat lighting holistically or implicitly, LumosBench provides disentangled, attribute-level evaluation, enabling precise diagnosis of model behavior under controllable lighting conditions. See more results in Appendix C.2."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "As shown in Tab. 2 and Fig. 6, we conduct ablation studies to analyze the effectiveness of different components. Physics-Guided Feedback. Removing both depth and normal feedback (w/o All Feedback) leads to significant degradation in both image quality and physical consistency, confirming the necessity of our physics-guided loss. Notably, omitting only normal supervision causes larger drop than removing depth, suggesting that surface orientation plays more critical role than distance in shaping lightshadow interactions. Path Consistency Learning. Excluding this component (w/o Path Consistency) yields only minor drops in physical metrics while maintaining competitive SSIM and LPIPS scores. This shows that path consistency incurs little performance 9 Figure 4 UniLumos performs physically plausible video relighting conditioned on different reference videos. Table 2 Quantitative comparison. Bold number indicate the best performance. Model PSNR (a) Quality SSIM LPIPS (b) Temporal Consistency R-Motion (c) Lumos Consistency Avg. Score Dense L2 Error w/o Depth Feedback w/o Normal Feedback w/o All Feedback w/o Path Consistency 23.472 22.115 21.433 0.883 0.874 0. 25.317 0.902 0.118 0.123 0.139 0.113 1.443 1.446 1.473 1.438 Ablative Study Only Video Only Image UniLumos 22.487 24.471 25.031 0.863 0.872 0.891 0.119 0. 0.109 1.487 2.429 1.436 Effect of Training Domain 0.870 0.863 0.859 0. 0.857 0.841 0.871 0.265 0.173 0.297 0.153 0.173 0.182 0.147 cost but offers substantial efficiency benefits in few-step regimes, justifying its inclusion. Training Modality. To evaluate the effectiveness of our unified training paradigm, we compare domain-specific variants: training solely on videos leads to poor visual quality, while image-only training sacrifices temporal smoothness. In contrast, our unified approach strikes the best balanceachieving high-quality and temporally coherent relighting across both input types."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce UniLumos, unified framework for physically plausible image and video relighting. It aligns illumination with scene geometry via RGB-space depth and normal supervision, improving shadow accuracy and spatial consistency. To enhance controllability and evaluation, we propose structured six-dimensional lighting annotation protocol, enabling fine-grained conditioning and physically grounded assessment through VLMs. Experiments show that UniLumos achieves superior relighting quality, physical consistency, and inference efficiency. 10 Figure 6 Ablation study. We compare the effects of different components under few-step denoising. For 1-step, we show the impact of flow-matching with and without path consistency. For 5-step, we visualize results before and after introducing physics-plausible feedback."
        },
        {
            "title": "Acknowledgment",
            "content": "This work was supported by Damo Academy through Damo Academy Research Intern Program."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. [5] Ziqi Cai, Kaiwen Jiang, Shu-Yu Chen, Yu-Kun Lai, Hongbo Fu, Boxin Shi, and Lin Gao. Real-time 3d-aware portrait video relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62216231, 2024. [6] Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy, Jingyuan Liu, Julie Dorsey, and Zhixin Shu. Synthlight: Portrait relighting with diffusion model by learning to re-render synthetic faces. arXiv preprint arXiv:2501.09756, 2025. [7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [9] Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, and Liang Lin. Id-aligner: Enhancing identity-preserving text-to-image generation with reward feedback learning. arXiv preprint arXiv:2404.15449, 2024. [10] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [12] Ye Fang, Zeyi Sun, Shangzhan Zhang, Tong Wu, Yinghao Xu, Pan Zhang, Jiaqi Wang, Gordon Wetzstein, and Dahua Lin. Relightvid: Temporal-consistent diffusion model for video relighting. arXiv preprint arXiv:2501.16330, 2025. [13] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. [14] Rohit Gandikota, Joanna Materzyńska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models. In European Conference on Computer Vision, pages 172188. Springer, 2024. [15] Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. International Journal of Computer Vision, 129(6):17891819, 2021. [16] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [17] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. arXiv preprint arXiv:2409.18124, 2024. [18] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [19] Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov, André Araujo, Ricardo Martin Brualla, Kaushal Patel, et al. Navi: Category-agnostic image collections with high-quality 3d shape and pose annotations. Advances in Neural Information Processing Systems, 36:7606176084, 2023. [20] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion. Advances in Neural Information Processing Systems, 37:141129141152, 2024. [21] Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, and Sanghyun Woo. Switchlight: Co-design of physics-driven architecture and pre-training framework for human portrait relighting. 12 In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2509625106, 2024. [22] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for indoor single-view material estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51985208, 2024. [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [24] Zhengfei Kuang, Yunzhi Zhang, Hong-Xing Yu, Samir Agarwala, Elliott Wu, Jiajun Wu, et al. Stanfordorb: real-world 3d object inverse rendering benchmark. Advances in Neural Information Processing Systems, 36:4693846957, 2023. [25] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [26] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. Diffueraser: diffusion model for video inpainting. arXiv preprint arXiv:2501.10018, 2025. [27] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98019810, 2023. [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [29] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [31] Rohit Pandey, Sergio Orts-Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Ryan Fanello. Total relighting: learning to relight portraits for background replacement. ACM Trans. Graph., 40(4):431, 2021. [32] Mengwei Ren, Wei Xiong, Jae Shin Yoon, Zhixin Shu, Jianming Zhang, HyunJoon Jung, Guido Gerig, and He Zhang. Relightful harmonization: Lighting-aware portrait background replacement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64526462, 2024. [33] Peiran Ren, Yue Dong, Stephen Lin, Xin Tong, and Baining Guo. Image based relighting using neural networks. ACM Transactions on Graphics (ToG), 34(4):112, 2015. [34] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [35] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. [36] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. [37] Yuiga Wada, Kanta Kaneda, Daichi Saito, and Komei Sugiura. Polos: Multimodal metric learning from human feedback for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1355913568, 2024. [38] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai 13 Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [39] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023. [40] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [41] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [42] Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz, Ming-Yu Liu, and Ting-Chun Wang. Learning to relight portrait images via virtual light stage and synthetic-to-real adaptation. ACM Transactions on Graphics (TOG), 41(6):121, 2022. [43] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. Instructvideo: Instructing video diffusion models with human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64636474, 2024. [44] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Finegrained lighting control for diffusion-based image generation. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [45] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024. [46] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54535462, 2021. [47] Longwen Zhang, Qixuan Zhang, Minye Wu, Jingyi Yu, and Lan Xu. Neural video portrait relighting in real-time via consistency modeling. In Proceedings of the IEEE/CVF international conference on computer vision, pages 802812, 2021. [48] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based In The Thirteenth illumination harmonization and editing by imposing consistent light transport. International Conference on Learning Representations, 2025. [50] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1864318652, 2022. [51] Yuxin Zhang, Dandan Zheng, Biao Gong, Jingdong Chen, Ming Yang, Weiming Dong, and Changsheng Xu. Lumisculpt: consistency lighting control network for video generation. arXiv preprint arXiv:2410.22979, 2024. 14 [52] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:1112711150, 2023. [53] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv e-prints, pages arXiv2402, 2024. [54] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation. CAAI Artificial Intelligence Research, 3:9150038, 2024. [55] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David Jacobs. Deep single-image portrait relighting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 71947202, 2019. [56] Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Chen Change Loy. Propainter: Improving propagation and transformer for video inpainting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1047710486, 2023. [57] Yujie Zhou, Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Qidong Huang, Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, et al. Light-a-video: Training-free video relighting via progressive light fusion. arXiv preprint arXiv:2502.08590, 2025. 15 Appendix of UniLumos In this appendix, we provide additional details to complement the main paper. First, we explain the motivation behind introducing physics-plausible feedback in Sec. A. Next, we present the detailed pipeline of our proposed LumosData relighting data construction process in Sec. B. Then, Sec. contains three additional experimental results on the public dataset (Sec. C.1) and the LumosBench (Sec. C.2). Sec. provides additional qualitative results to further illustrate the effectiveness of UniLumos. Finally, Sec. discusses the limitations of our method. Physics-Plausible Feedback To further clarify the motivation and design behind our physics-plausible feedback mechanism, we present breakdown of key questions and considerations addressed during its development. Q1: What is the motivation for introducing physical constraints in relighting? A1: The primary goal of relighting is to generate visually plausible illumination under new lighting conditions. However, many diffusion-based methods lack explicit physical modeling, leading to artifacts such as overexposed highlights, misaligned shadows, or inconsistent light directions. Introducing physical constraints serves as refinement mechanism that aligns generated light with the scenes underlying geometry. This helps enforce realism and spatial consistency in illumination, which is especially critical under complex lighting or HDR scenarios. Q2: Why are depth and normal maps chosen as the targets for physical supervision? A2: Depth and surface normals are among the most accessible and general-purpose dense scene attributes. By design, these estimations intentionally suppress fine-scale lighting effects to focus on intrinsic geometry. This makes them ideal for supervising relighting, where the goal is to decouple geometry from illumination and enforce spatial structure in lighting behavior. In the proposed UniLumos, we align the predicted lighting with reference geometry by minimizing the L2 norm error between generated and reference-aligned depth/normal maps via pre-trained dense estimation model (e.g., Lotus [17]) with frozen parameters. This provides simple yet effective metric to quantify physical plausibility. Q3: Why are alternative physical signalssuch as albedo, shadow, or materialnot used instead? A3: While albedo, shadow masks, and material properties can provide rich supervision, they come with significant drawbacks. Albedo and shadow estimation often rely on inverse rendering and suffer from domain sensitivity or ambiguity. Material annotations are expensive and dataset-dependent. Moreover, many of these properties are entangled with illumination, making them less reliable as supervisory signals. In contrast, depth and normals can be predicted from monocular images with high availability and generalize well across scenes, offering favorable balance between supervision quality and computational cost. Q4: Why are depth and normal maps used as training-time constraints rather than as model inputs? A4: While it is possible to condition the model directly on estimated depth and normal maps, doing so increases the input dimensionality and model complexity. It would also introduce dependency on external estimators during inference, complicating the pipeline and potentially propagating errors. Instead, we use them as supervision signals during training. This design keeps the inference pipeline simplerelying only on image and lighting condition inputswhile still allowing the model to learn geometry-aware behaviors. The supervision acts as form of inductive bias, guiding the model toward physically plausible outputs without requiring additional input channels at the test phase."
        },
        {
            "title": "B Details of Datasets",
            "content": "Step 1: Subject Mask. Given an input video Vreal R[T +1,H,W,3], we first extract per-frame subject masks R[T +1,H,W ] using BiRefNet [54]. These subject masks allow us to isolate the target subject foreground and the target background. 16 Table 3 Lighting-related textual prompts used in Lumos Augmentation from IC-Light [49]. Each prompt can be combined with different canonical light directions during training. Example Light Direction None Left Light Right Light Top Light Bottom Light sunshine from window neon light, city sunset over sea golden time sci-fi RGB glowing, cyberpunk natural lighting warm atmosphere, at home, bedroom ID Lighting Prompt 1 2 3 4 5 6 7 8 magic lit 9 10 11 12 13 evil, gothic, Yharnam light and shadow shadow from window soft studio lighting home atmosphere, cozy bedroom illumination neon, Wong Kar-wai, warm Step 2: Lumos Augmentation. To simulate diverse lighting degradations for training, we relight each subject sequence under multiple lighting conditions using pre-trained 2D relighting model, such as IC-Light [49]. This operation is applied independently to each frame of the subject region, resulting in degenerated video Vdeg R[T +1,H,W,3]. To generate rich illumination variations, we refer to the description of light and shadow given by IC-Light [49], as listed in Tab. 3, which serve as the semantic guidance for image-level relighting and the light source directions. For each input video, we randomly sample 5 prompts and 3 directions, forming 5 3 = 15 unique prompt-direction pairs. The relighting is applied only to the subject region, extracted using the subject masks R[T +1,H,W ] from Step 1. Notably, we randomly sample one degradation condition from the 15 prompt-direction combinations for each subject in each iteration. This strategy reduces training cost while exposing the model to diverse illumination patterns, thereby improving generalization. Step 3: Gaussian Background. To provide external lighting context during training, we generate background video Vbg R[T +1,H,W,3] to accompany the relit subject. Instead of relying on complex inpainting-based synthesis (e.g., ProPainter [56], DiffuEraser [26]), we adopt simple yet effective strategy by filling the background with either pure color or Gaussian noise. This design avoids injecting semantic or structural priors, allowing the model to focus solely on illumination learning. Specifically, for each frame [1, + 1] and channel {R, G, B}, we first define the background region using the subject mask Mt RHW obtained in Step 1. Let Ωt bg = {(i, j) Mt(i, j) = 0} denote the set of background pixels. We compute the mean and standard deviation of background pixel intensities as: µt = 1 Ωt bg (cid:88) (i,j)Ωt bg Vt(i, j, c), σt = (cid:118) (cid:117) (cid:117) (cid:116) 1 Ωt bg (cid:88) (i,j)Ωt bg (Vt(i, j, c) µt c)2 We then fill the background with pixel-wise samples from Gaussian distribution: Vt bg(i, j, c) (µt c, (σt c)2), (i, j) Ωt bg. (8) (9) This procedure ensures that the background region maintains similar color distribution to the original video while avoiding structural detail that may bias learning. As shown in Fig. 7, for comparison, we also test variant that uses pure-color background, where each background pixel is set to µt = 0). In practice, we observe that such statistically consistent placeholdersparticularly Gaussian-filled onesaccelerate early-stage (i.e., σt 17 Figure 7 Comparison of background inpainting strategies of four representative cases. Here, Gaussian Inpainting fills the background using random noise sampled with the same mean and variance as the subject region, ensuring statistical consistency. Pure Inpainting directly fills the background with uniform color (i.e., gray), without modeling spatial or color variation. The Gaussian strategy provides more realistic signal distribution and accelerates early-stage convergence in training. convergence during training. We attribute this to the reduced visual complexity and improved normalization behavior, which make the model less sensitive to background variation. The resulting Vbg serves as clean, distribution-aligned conditioning signal for the relighting network. Step 4: Caption Augmentation. In addition to relighting augmentation, we generate lighting-aware captions to provide rich semantic supervision aligned with physical lighting behavior. Specifically, we leverage Qwen2.5VL [1], vision-language model with fine-grained visual reasoning capabilities, to analyze each input video and generate structured captions describing its lighting attributes. The input to Qwen2.5-VL consists of the original video and its corresponding scene-level caption. We then apply custom-designed prompt (see Listing 1) to steer the model toward predicting six categories of lighting-related labels as shown in Tab. 4, including all subcategories and their physical interpretations. The output of this process is structured caption for each video, formatted as dictionary mapping the six categories to their predicted labels (see example in Listing 1). These structured captions serve as auxiliary supervision and evaluation labels in later stages, helping the model better align with interpretable physical lighting semantics. They also enhance downstream controllability and facilitate attribute-based retrieval or evaluation. Table 4 Classification criteria and definitions for light-related scene attributes. Primary Category Direction of Light Light Source Type Light Intensity Color Temperature Subcategory Front Light Side Light Back Light Top Light Bottom Light Split Light Ambient Light Without Clear Direction Definition The light source is positioned directly in front of the subject, illuminating it head-on. The light source is positioned at 90-degree or 45-degree angle to the subject, coming from the side. The light source is located behind the subject, directed towards the camera. The light source is positioned directly above the subject, casting light downwards. The light source is positioned below the subject, casting light upwards. The light source illuminates one side of the subject while leaving the other side in shadow. Ambient light is non-directional, uniformly illuminating the environment from multiple sources. Natural Light Artificial Light Rendering Light Glare Moderate Dim Cool Tone Neutral Warm Tone Illumination from nature without human intervention, varying with time of day, weather, and location. Human-made light sources (e.g., bulbs, LEDs) used in indoor/outdoor spaces for functional or artistic effects. Digitally simulated light in CGI, games, or animations using techniques like ray tracing. Extremely bright light over 1000 lumens that can cause discomfort or obscure detail. Balanced lighting (2001000 lumens), suitable for most activities and comfortable viewing. Low lighting under 200 lumens, often cozy but may reduce visibility and detail recognition. 5000K10000K; bluish hues, common in daylight or overcast scenes. 4000K5000K; balanced light with no strong blue or yellow tint. 2000K4000K; reddish or yellowish hues, typical in sunrise/sunset or indoor lighting. Light Changes in Time Static Light Dynamic Light (Intensity Changing) Dynamic Light (Moving Source) Illumination remains constant in both intensity and direction over time. Light intensity changes gradually over time (e.g., dawn to daylight). Direction of light changes due to movement of light source (e.g., headlights, stage lights). Optical Phenomena Transmission (Glass) Refraction/Reflection (Water Surface, Mirror) Scattering (Fog Effect) None Light passes through transparent materials like glass, with possible scattering or absorption. Light bends or reflects at water or mirror surfaces, altering its direction. Light diffuses through particles like fog or mist, reducing visibility. No significant optical phenomena are observed in the scene. 18 SYSTEM_PROMPT = \"\"\" You are helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.nnIf question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you dont know the answer to question, please dont share false information. \"\"\" PROMPT = \"\"\" Role: You are an expert in image/video light and shadow analysis, good at analyzing light and shadow from multiple angles. ... Tasks: Analyze the input image/video, provide corresponding classification results for the following multiple categories, and return them in the specified output format. 1. Direction of Light: Task 1: Analyze the image and classify the light source direction as front light, side light, back light, top light, bottom light, or split light. Identify the angle of the light source relative to the subject, and describe its effect on shadow formation. 2. Light Source Type: Task 2: Analyze the image and classify the light source type as either Natural Light, Artificial Light, or Rendering Light. 3. Light Intensity: Task 3: Analyze the image/video to assess the light intensity present. Classify the light intensity into three categories: Glare, Moderate, and Dim. Special attention should be given to situations where bright light sources may create glaring effect even in otherwise dim environments. 4. Color Temperature: Task 4: Analyze the image/video to assess the color temperature present. Classify the color temperature into three categories: Cool Tone, Neutral, and Warm Tone. 5. Light Changes in Time: Task 5: Analyze the video to assess light changes over time. Classify the light changes into two main categories: Static Light and Dynamic Light. For Dynamic Light, further categorize it into two subtypes: Intensity Gradient and Moving Light Source. 6. Optical Phenomena: Task 6: Analyze the image/video with focus on the specific scene to assess the optical phenomena present. Pay close attention to scenarios involving glass, water surfaces, mirrors , and fog. Classify the phenomena into the following categories: Transmission (Glass), Refraction/Reflection (Water Surface, Mirror), Refraction/Reflection (Mirror), Scattering ( Fog Effect), and None. Guidelines: 1. Accuracy: Assign each tag to the most appropriate category and subcategory. 2. Multiple Tags: If an action fits multiple categories, assign all relevant tags. 3. Comprehensiveness: Capture all detectable dynamic attributes without omissions. 4. JSON Validity: Ensure the output JSON is correctly formatted and adheres to the specified structure. Example Output: { \"Direction of Light\": \"Front Light\", 19 \"Light Source Type\": \"Artificial Light\", \"Light Intensity\": \"Moderate\", \"Color Temperature\": \"Cool Tone\", \"Light Changes in Time\": \"Dynamic Light (Intensity Changing Light)\", \"Optical Phenomena\": \"Transmission (Glass)\" } \"\"\" Listing 1 Prompt Definition"
        },
        {
            "title": "C Additional Experimental Results",
            "content": "C.1 Additional Main Results To further demonstrate the models generalization to non-human scenes, we conducted additional evaluations on two public object-centric relighting benchmarks: StanfordOrb [24] and Navi [19]. These datasets include objects and sculptures under variety of lighting environments and are completely disjoint from our training data. StanfordOrb contains canonical 3D scanned objects such as the Stanford bunny and dragon, while Navi includes wide range of everyday objects like containers, toys, and mugs. As shown in Tab. 5 (StanfordOrb) and Tab. 6 (Navi), UniLumos achieves state-of-the-art results across perceptual (LPIPS), structural (SSIM), and physical (R-Motion) metrics, despite the significant domain gap and without any test-time fine-tuning, outperforming all baselines. Table 5 Quantitative comparison on the StanfordOrb dataset. Bold number indicate the best performance. Model IC-Light Per Frame [49] Light-A-Video [57] + CogVideoX [41] Light-A-Video [57] + Wan2.1 [38] PSNR 24.132 25.617 25.784 SSIM 0.914 0.923 0. 0.126 0.108 0.104 LPIPS R-Motion UniLumos 26.512 0.934 0. 1.742 1.279 1.241 1.103 Table 6 Quantitative comparison on the Navi dataset. Bold number indicate the best performance. Model IC-Light Per Frame [49] Light-A-Video [57] + CogVideoX [41] Light-A-Video [57] + Wan2.1 [38] PSNR 22.021 23.912 23.474 SSIM 0.883 0.891 0. UniLumos 24.977 0.911 LPIPS R-Motion 0.125 0.121 0.116 0. 1.974 1.378 1.341 1.203 C.2 LumosBench: An Attribute-level Controllability Benchmark To evaluate the fine-grained controllability of lighting generation, we introduce LumosBench, structured benchmark that targets six core illumination attributes defined in our annotation protocol. Unlike prior works that treat lighting holistically or implicitly, LumosBench provides disentangled, attribute-level evaluation, enabling precise diagnosis of model behavior under controllable lighting conditions. Specifically, we construct set of 2k test prompts, each consisting of video and structured caption designed to isolate one lighting attribute at time, while holding other variables constant. These prompts span six categoriesdirection, light source type, intensity, color temperature, temporal dynamics, and optical phenomenawith multiple subtypes per category (e.g., front/side/back for direction). This design facilitates controlled and interpretable evaluation across lighting axes that are often conflated in prior datasets. Table 7 Quantitative comparison of the attribute-level controllability. Bold number indicate the best performance. Model LTX-Video [16] CogVideoX [41] HunyuanVideo [23] Wan2.1[38] Wan2.1[38] IC-Light Per-Frame [49] Light-A-Video [57] + CogVideoX[41] Light-A-Video [57] + Wan2.1[38] UniLumos w/o lumos captions UniLumos #Params Direction Light Source Type Intensity Color Temperature Temporal Dynamics Optical Phenomena Avg. Score 1.9B 5.6B 13B 1.3B 14B 0.9B 2.9B 2.2B 1.3B 1.3B 0.794 0.837 0.863 0.842 0.871 0.793 0.787 0. 0.868 0.893 General Models 0.487 0.552 0.599 0.436 0.674 Specialized Models 0.349 0.327 0. 0.529 0.832 0.644 0.692 0.741 0.685 0.794 0.547 0.581 0.603 0.774 0. 0.708 0.739 0.802 0.741 0.829 0.493 0.536 0.582 0.798 0.813 0.487 0.532 0.655 0.504 0. 0.284 0.493 0.557 0.543 0.662 0.403 0.449 0.481 0.433 0.505 0.339 0.373 0.412 0.457 0. 0.587 0.634 0.690 0.607 0.735 0.468 0.516 0.553 0.662 0.773 To assess alignment between intended and generated lighting attributes, we use the vision-language model Qwen2.5-VL [1] to analyze relit outputs and classify whether the target attribute is correctly expressed. Each dimension is scored independently, and the final controllability score is the average across all six dimensions. General vs. Specialized Models. Tab. 7 presents results for both general-purpose and task-specific relighting models. This benchmark allows us not only to assess overall lighting quality but also to dissect models ability to interpret and respond to individual lighting controls. Among general models, Wan 14B shows the highest raw capability, demonstrating the strength of large-scale pretraining for visual generation. Notably, our fine-tuned Wan 1.3B variant achieves substantial gains across all six lighting dimensions, surpassing even much larger models. This highlights the benefit of relighting-specific supervision: fine-tuning on LumosData with structured lighting annotations significantly enhances the models ability to reason about illumination in controllable and disentangled manner. By contrast, specialized relighting models consistently underperform despite being designed for lighting manipulation. This is primarily due to the limitations of their base architecturestypically smaller and trained from scratch or on narrow domainsresulting in weaker generalization to diverse lighting attributes. While they may encode some prior knowledge of lighting physics (e.g., through latent constraints), their restricted modeling capacity hinders semantic alignment with user-intended lighting conditions. These findings underscore the importance of starting from strong pre-trained backbone and introducing structured, high-level lighting supervision to achieve controllable and physically plausible relighting. Effectiveness of Structured Captions. Within the specialized group, we conduct an ablation to assess the importance of our proposed lumos captions. w/o lumos captions uses only vanilla scene-level captions during training, omitting structured lighting tags. The performance dropparticularly in controllable dimensions like intensity and optical phenomenaconfirms that our semantic annotations play key role in teaching the model fine-grained illumination control. Compared to strong baselines, UniLumos achieves superior scores across nearly all dimensions, demonstrating the impact of LumosBench in pushing model understanding and control of illumination."
        },
        {
            "title": "D Additional Result Visualization",
            "content": "We present additional image relighting results in Fig. 8. We present additional background-conditioned video relighting results in Fig. 9 and Fig. 10."
        },
        {
            "title": "E Limitation and Future Work",
            "content": "UniLumos is still limited by broader challengeachieving physically precise and controllable relighting. UniLumos enforces geometry-aware consistency (e.g., shadows aligned with depth and normals) but does not yet produce physically quantifiable lighting outputs such as radiance or illuminance. Future work may explore finer control over lighting, including editable key lights, intensity ramps, and environmental reflections. 21 Figure 8 UniLumos performs physically plausible image relighting, conditioned on textual prompts. 22 Figure 9 UniLumos performs physically plausible video relighting, conditioned on reference videos. Figure 10 UniLumos performs physically plausible video relighting, conditioned on reference videos."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "National University of Singapore",
        "Zhejiang University"
    ]
}