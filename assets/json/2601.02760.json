{
    "paper_title": "AnyDepth: Depth Estimation Made Easy",
    "authors": [
        "Zeyu Ren",
        "Zeyu Zhang",
        "Wukai Li",
        "Qingxiang Liu",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 0 6 7 2 0 . 1 0 6 2 : r AnyDepth: Depth Estimation Made Easy ANYDEPTH: DEPTH ESTIMATION MADE EASY Zeyu Ren1 Zeyu Zhang2 Wukai Li2 Qingxiang Liu3 Hao Tang2 1The University of Melbourne 2Peking University 3Shanghai University of Engineering Science Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. Simplicity is prerequisite for reliability. Edsger W. Dijkstra Figure 1: We present AnyDepth, simple and efficient training framework for zero-shot monocular depth estimation, which achieves impressive performance across variety of indoor and outdoor scenes."
        },
        {
            "title": "ABSTRACT",
            "content": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), compact transformer-based decoder. Compared to the DPT, it uses single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%89%. Furthermore, we propose quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth."
        },
        {
            "title": "INTRODUCTION",
            "content": "Monocular depth estimation is gaining increasing attention due to its wide range of downstream applications. Depth maps are not only used to measure scene distances (Bhat et al., 2023; 2021; Godard et al., 2017), but can also be embedded as conditional information within models in the 3D reconstruction (Wang et al., 2025b;c;a), generation (Zhang et al., 2023; Rombach et al., 2022; Poole et al., 2022; Mildenhall et al., 2021; Li et al., 2024a; Yang et al., 2023), and embodied AI (Wu et al., 2025; Huang et al., 2025a; Liu et al., 2025b;a; Huang et al., 2025b; Song et al., 2025; Ye et al., 2025; Huang et al., 2025c;d), providing complementary information to improve granularity and geometric 1 AnyDepth: Depth Estimation Made Easy (a) Model Comparison (b) FLOPs Comparison Figure 2: Comparison of the number of parameters (left) and computational complexity (right) of AnyDepth and DPT for different model sizes and input resolutions. Our method significantly reduces the number of model parameters and computational cost while maintaining competitive accuracy. consistency.The MiDaS series (Ranftl et al., 2020; Birkl et al., 2023), through extensive and systematic experiments, compared the transfer performance of various pretrained vision transformers (such as ViT (Dosovitskiy et al., 2020), Swin (Liu et al., 2021), DINO (Oquab et al., 2023), and BeiT (Bao et al., 2021)) on monocular depth estimation tasks. DPT (Ranftl et al., 2021) has demonstrated impressive performance in various dense prediction tasks and is currently used as the decoder in mainstream models. DPT aims to achieve finer-grained predictions by fusing features at different scales. The Depth Anything series (Yang et al., 2024a;b) represents typical data-driven approach, aiming to improve understanding and generalization capabilities of model for complex scenarios by leveraging massive datasets. These methods have significantly improved performance in zero-shot scenarios, demonstrating the potential of data scalability in the field of depth estimation. However, We rethink the monocular depth estimation pipeline from both architectural and data-centric perspectives. From the architectural perspective, we observe that each Transformer layer in DPT requires dedicated Reassemble module to map features to different scales, followed by multiple alignment operations. This design introduces unnecessary complexity, large parameter counts, and slow inference speed. DPT uses fixed bilinear interpolation for upsampling, which lacks adaptability to local geometric structures and often leads to blurred edges and loss of fine spatial details. From the data perspective, purely datadriven approaches such as the Depth Anything series rely heavily on massive datasets. However, large-scale data collection is costly and inevitably introduces noisy samples that degrade training quality. Simply scaling model size and data quantity therefore provides limited gains and poor reproducibility. Figure 3: Comparison of inference time between AnyDepth and DPT at different input resolutions. Our method consistently achieves lower latency, especially at higher resolutions. Based on these findings and limitations, we aim to design lightweight and efficient training framework that maintains competitive performance while being widely adopted by the research community  (Fig. 2)  . Specifically, our contributions are reflected in three aspects: We design novel decoder that aligns and fuses features before restoring resolution through one-shot reconstruction and upsampling. This architecture avoids multi-branch crossscale alignment and repeated reconstruction, better preserving high-frequency details and geometric consistency. 2 AnyDepth: Depth Estimation Made Easy We analyze sample quality issues in deep learning datasets and proposed two metrics to quickly measure sample quality, which we then used to filter out low-quality samples. This reduced dataset size while improving overall data quality, demonstrating that our framework can achieve better performance with fewer resources. On multiple benchmarks, our framework achieves comparable accuracy and generalization to DPT with significantly fewer parameters and lower training overhead, demonstrating superior efficiency-accuracy trade-off and academic reproducibility."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Zero-Shot Monocular Depth Estimation. To enable widespread use of depth images in realworld scenarios without relying on specific environments, zero-shot depth estimation has become key research direction in recent years (Chen et al., 2016; Piccinelli et al., 2024; Chen et al., 2020; Yin et al., 2021). Due to the lack of strict geometric constraints on MDE, many zero-shot models learn to predict affine-invariant depth, i.e., recovering relative structure while maintaining scale and translation invariance (Ranftl et al., 2020; Yang et al., 2024a;b). For example, DiverseDepth (Yin et al., 2020) uses web images as training data to improve zero-shot generalization performance. MiDaS (Ranftl et al., 2020) proposed scale-shift-invariant losses to solve the ambiguity problem of different deep numerical representation methods of different datasets, so that the model can be trained on large scale. In order to eliminate the inherent problems of the CNN backbone, the performance of Zero-Shot Monocular Depth Estimation was further improved by using the vision transformer architecture, such as DPT (Ranftl et al., 2021), Omnidata (Eftekhar et al., 2021), Depthformer (Li et al., 2023) and Zoepdeth (Bhat et al., 2023). Marigold (Ke et al., 2024) directly utilizes the standard diffusion model paradigm and stable diffusion pre-trained weights for fine-tuning to produce high-quality results. Depth Anything series (Yang et al., 2024a;b) used 62 million unlabeled images for larger-scale training. Geowizard (Fu et al., 2024) uses the high consistency between dense prediction tasks to jointly predict depth and normals. Lotus (He et al., 2024) analyzes the diffusion process to achieve single-step diffusion and speed up the inference process. Genpercept (Xu et al., 2024) uses experiments to prove that the diffusion model requires specific details to be optimized in dense prediction tasks. Decoder for Dense Prediction. Currently, many methods for dense prediction tasks employ multiscale feature fusion strategies to compensate for the lack of information from single-layer features (Lin et al., 2017; Liu et al., 2018; Tan et al., 2020; Chen et al., 2018; Ghiasi et al., 2019; Xu et al., 2021; Eigen & Fergus, 2015). FPN (Lin et al., 2017) proposes top-down architecture where high-level semantic representations are successively merged with low-level features to enhance multi-scale features. (Lee et al., 2019) designed multi-scale local plane guidance layer to more effectively guide the fusion of features at each layer to achieve performance improvement. Swin-Depth (Cheng et al., 2021) designs lightweight multi-scale attention mechanism module to enhance the ability to learn global information at multiple scales. PVT (Wang et al., 2021) and Uformer (Wang et al., 2022) use multi-scale pyramid decoder structure to capture long-range visual dependencies.DPT (Ranftl et al., 2021) utilizes the ViT (Dosovitskiy et al., 2020) backbone network to generate high-resolution features, thereby achieving finer-grained representation and improving prediction accuracy. However, multi-branch reassembly incurs significant overhead, especially in the case of high-resolution input."
        },
        {
            "title": "3 THE PROPOSED METHOD",
            "content": "3.1 OVERVIEW The proposed AnyDepth uses pre-trained DINOv3 (Simeoni et al., 2025) encoder and SDT decoder; as shown in Fig.4, given an input image I, we extract multi-scale representations from four intermediate Transformer layers 1, 2, 3, 4 and input them into the SDT head for depth reconstruction, thereby capturing different levels of detail and semantic information. These tokens are linearly projected onto common dimension and fused to capture complementary semantic levels. The fused representations are then reshaped into feature maps and refined by Spatial Detail Enhancer (SDE). Finally, dense depth map is generated through two learnable Upsampler and head 3 AnyDepth: Depth Estimation Made Easy Figure 4: AnyDepth architecture overview. The input image is encoded into tokens by frozen DINOv3 backbone network, then decoded by our lightweight SDT decoder. Tokens undergo only single projection and weighted fusion. The Spatial Detail Enhancer (SDE) module ensures finergrained predictions. The feature map is upsampled by an efficient and learnable upsampler dysample, and the depth is finally output by the head. prediction.Our method differs from the Depth Anything series (Yang et al., 2024a;b) and DPT (Ranftl et al., 2021) in that we fuse tokens using only single linear projection, followed by upsampling in single path, without multi-branch cross-scale alignment, significantly reducing the number of parameters and computational overhead. 3.2 SIMPLE DEPTH TRANSFORMER (SDT) Our decoder adopts simple single-path fusion and reconstruction strategy, aiming to take advantage of the high-resolution feature of DINOv3 and further unleash its performance at high resolution. We first project the tokens extracted from the encoder into 256-dimensional space using linear layer followed by GELU non-linearity (Hendrycks & Gimpel, 2016), which preserves sufficient informative content while substantially reducing the computational overhead in the subsequent decoding stages. For the class token, we keep the same processing as DPT (Ranftl et al., 2021), concatenate it with the spatial token, and then fuse it through the learnable projection. Fusion. To fuse tokens from multiple layers of representation, we then employ learnable weighted fusion strategy (Eq. 1). Specifically, we assign learnable scalar weight to each layer of tokens and normalize them using softmax function to form uniform probability distribution, preventing initial instability in training. This strategy enables the model to adaptively balance low-level structural details with high-level semantic information. (cid:88) = αi Proji(Ti), Ti RNpD, (1) Where Ti denotes the token in layer after projection, and contains Np tokens of dimension D. iL Spatial Detail Enhancer. After the fusion block, we reshape the sequence token output into spatial feature map. Because the reorganized feature map lacks local continuity and, after multilevel fusion, easily obscures shallow texture details, which are crucial for dense prediction tasks such as depth estimation, we designed the Spatial Detail Enhancer.The SDE can be expressed by Eq. 2, = ReLU (F + BN (DW Conv33(F ))), (2) We implement this operation first using 3 3 Depthwise convolution for local spatial modeling, followed by batch normalization. We then add the normalized response to the input feature via residual connection, and finally pass it through an activation layer. 16 256. 16 Upsampler. In the upsampling stage, we abandon the commonly used bilinear interpolation, which easily blurs high-frequency details, and instead adopt learnable dynamic sampler (Eq. 6). 4 AnyDepth: Depth Estimation Made Easy Specifically, we use DySample (Liu et al., 2023) as the upsampler, which adaptively constructs an offset sampling grid based on the learned low-resolution features to adjust the sampling position, and then uses differentiable grid sampling to resample to high-resolution features. We first define three operators: the DySample block B(), the DySample stage S(), and the refinement block R(): BN(cid:0)Conv33(DySample2(X))(cid:1)(cid:17) (cid:16) B(X) = ReLU S(X) = B(cid:0)B(X)(cid:1), , (cid:16) R(X) = ReLU BN(cid:0)Conv33(X)(cid:1)(cid:17) . Based on these definitions (Eq. 3, 4, 5), the complete upsampling process can be expressed as: (cid:16) U(X) = S(cid:0)R(S(X))(cid:1)(cid:17) , (3) (4) (5) (6) In this way, the compact feature map of size H/16 W/16 can be progressively upsampled back to the original resolution . We want to emphasize that we do not jump to all at once, but rather decompose the upsampling into two 4 upsamplers, using four dysamples of scale 2. Singlestage 16 upsampling forces the sampler to infer large offsets from very low-resolution features, which amplifies errors and destabilizes gradients. Our progressive design keeps the offsets small, inserting local refinement after each resampling, resulting in model with better detail recovery capabilities. 3.3 SDT VS. DPT key difference between SDT and DPT (Ranftl et al., 2021) is the order of feature reassembly. DPT employs reassemble-fusion strategy. Specifically, DPT first applies the reassemble module to the tokens extracted by each Transformer layer, mapping the tokens to feature maps of different scales. These feature maps are then fused in cascade across scales, which inevitably introduces multiple branches and repeated cross-scale alignment overhead. In contrast, SDT employs fusionreassemble strategy, directly projecting and fusing groups of tokens. Only after this stage do we perform spatial reassembly and upsampling along single path. This fusion-reassemble strategy avoids the high cost of per-layer token reassembly and feature map cross-scale alignment, making it more efficient and stable, especially when processing high-resolution inputs."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 DATASETS AND METRICS Training Datasets. We use five synthetic datasets covering various indoor and outdoor scenes for training. (1) Hypersim (Roberts et al., 2021) after filtering incomplete samples, we have approximately 39K. (2) Virtual KITTI (Cabon et al., 2020) we selected four scenes, totaling approximately 20K. (3) BlendedMVS (Yao et al., 2020) (4) IRS (Wang et al., 2019) (5) TartanAir (Wang et al.) As shown in Table 1, we only use 369K datasets for training. The far plane is set to 100 m. To improve the robustness and generalization of the model, we used data augmentation of flipping and rotation. Evaluation Datasets and Metrics. For Zero-shot monocular depth estimation, we evaluate SDT using five datasets containing various scenes: NYUv2 (Silberman et al., 2012), KITTI (Geiger et al., 2013), ETH3D (Schops et al., 2017), ScanNet (Dai et al., 2017), and DIODE (Vasiljevic et al., 2019). We use the absolute mean relative error(AbsRel), i.e., 1 , where is the total number of valid pixels, di denotes the ground truth, and ˆdi is the predicted depth. We report accuracy thresholds δτ , which denote the fraction of pixels where the prediction and ground truth differ by less than multiplicative factor τ = 1.25. ˆdidi di (cid:80)M i= 4.2 IMPLEMENTATION DETAILS Our setup differs slightly from Depth Anything V2 (Yang et al., 2024b). To better utilize the highresolution features of DINOv3 (Simeoni et al., 2025), we increase the input image resolution to 5 AnyDepth: Depth Estimation Made Easy (a) Total Score (b) Depth Distribution Score Figure 5: Dataset quality across the Total Score, Depth Distribution Score, and Gradient Continuity Score (higher is better). (c) Gradient Continuity Score 768 768. The encoder is kept frozen throughout training, and we use features from four intermediate layers as decoder inputs: [2, 5, 8, 11] for DINOv3 S/16 and DINOv3 B/16, and [4, 11, 17, 23] for DINOv3 L/16. We perform simple regression to predict disparity = 1/d, where denotes disparity and denotes depth. Both the input image and the groundtruth are normalized to [0, 1]. We follow the settings of Depth Anything v2 (Yang et al., 2024b) and use scaleand shift-invariant loss Lssi and gradient matching loss Lgm, and the weight ratio of Lssi and Lgm is set to 1 : 2. To stabilize optimization, we follow an optimization strategy similar to DINOv3 (Simeoni et al., 2025). We use AdamW with base learning rate of 1 103, PolyLR scheduler with power 0.9, and linear warm-up for the first two epochs. We train for total of five epochs. 4.3 MAIN RESULTS 4.3.1 RESULTS OF DATA CENTRIC LEARNING Dataset Table 1: Dataset statistics of good and bad samples. We applied the metrics proposed in Section A.2 to all training datasets, with the results shown in Fig. 5. We observe that Hypersim performed well in both the Depth Distribution Score and Gradient Continuity Score, achieving the highest overall score. This indicates relatively balanced depth distribution, smooth gradients, and low concentration of noisy samples. In contrast, datasets containing outdoor samples, such as VKITTI2, BlendedMVS, and TartanAir, had significantly lower Depth Distribution Scores, indicating more severe depth distribution. This is likely common problem across all outdoor datasets. The low Gradient Continuity Score for VKITTI2 may be due to the presence of numerous fine-grained structures (e.g., leaves) in the samples, resulting in abundant edges and severe gradient abruptness, which is considered noisy. Hypersim VKITTI2 BlendedMVS IRS TartanAir 26,912 12,643 74,838 68,211 186,693 39,648 19,559 115,142 103,316 306,637 12,736 6,916 40,304 35,105 119, Summary 215,005 584,302 369,297 Good Total Bad Following the methods described in Section A.2, we filtered the entire dataset. Specifically, we first filtered out samples whose valid depth values accounted for less than 20% of the total pixels. We then sorted the remaining samples based on the Depth Distribution Score and Gradient Continuity Score, filtering out the 20% with the lowest scores for each metric. The number of filtered samples for each dataset is shown in Table 1. For visualizations of low-quality samples, please see the A.3. The merged dataset contains 584K samples, of which approximately 369K are used for training and 215K are filtered out. 4.3.2 QUANTITATIVE COMPARISONS Table 2 reports quantitative comparison results for zero-shot affine-invariant depth estimation. Since the baselines in the Depth Anything series all use DPT head, we primarily compare our proposed SDT decoder with the DPT under the same backbone settings. 6 AnyDepth: Depth Estimation Made Easy Table 2: Quantitative comparison of zero-shot affine-invariant depth estimation. Lower AbsRel values are better; higher δ1 values are better. DINOv3 (Simeoni et al., 2025) uses the ViT-7B encoder, and Depth Anything v2 (DAv2) (Yang et al., 2024b) is trained on 62.6M datasets. For fair comparison, the baseline (DPT) uses frozen DINOv3 encoder and DPT head, while our method replaces the DPT head with the proposed SDT. The bold numbers in the table refer to the best results between DPT and AnyDepth. Method Training Data Encoder #Params (M) NYUv2 ETH3D AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 ScanNet DIODE KITTI DINOv3 595K ViT-7B DAv2 62.6M DPT 584K AnyDepth 369K ViT-S ViT-B ViT-L ViT-S ViT-B ViT-L ViT-S ViT-B ViT-L 91.19 71.8 162.1 399.6 71.8 162.1 399.6 26.5 95.5 313. 4.3 5.3 4.9 4.5 8.4 7.5 6.1 8.2 7.2 6.0 98.0 97.3 97.6 97. 93.3 95.1 96.8 93.2 95.0 96.8 7.3 7.8 7.8 7.4 10.8 10.8 8.9 10.2 9.7 8. 96.7 93.6 93.9 94.6 89.1 88.9 92.5 88.3 90.1 92.6 5.4 14.2 13.7 13. 12.7 10.0 13.0 8.4 8.0 9.6 97.5 85.1 85.8 86.5 92.0 92.9 94.9 93.5 94.5 95. 4.4 98.1 25.6 8.3 7.1 6.0 8.0 6.8 5. 93.5 95.3 97.0 93.6 95.6 97.4 7.3 6.8 6.6 26.0 24.5 23.4 24.7 23.6 22. 82.2 94.2 95.0 95.2 71.4 73.4 73.9 71.4 72.7 73.6 Table 3: Comparison of zero-shot affine-invariant depth estimation with different encoders and decoders. Green cells indicate the best results within each method. Method Encoder Decoder NYUv2 KITTI ETH3D ScanNet DIODE AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 DAv2 ViT-B DAv3 ViT-L VGGT VGGT-1B DPT SDT DPT Dual-DPT SDT DPT SDT 5.8 5.6 4.9 4.9 4.9 4.8 4.8 96.2 96.4 96.9 97.0 97.1 97.7 98. 10.4 10.7 8.8 8.9 8.9 15.6 15.5 89.1 89.6 92.4 92.4 92.4 77.9 80. 8.8 7.5 6.9 7.0 5.8 7.2 7.0 94.6 95.8 95.9 95.8 96.6 94.7 95. 6.2 6.1 5.0 4.9 5.0 4.6 4.6 95.3 95.4 96.6 96.6 96.6 97.6 98. 23.4 23.9 22.5 22.3 21.9 30.7 30.6 73.8 73.9 74.6 74.6 74.9 76.2 76. While our approach does not yet surpass the state-of-the-art results reported by fully datadriven methods (e.g., the Depth Anything series (Yang et al., 2024a;b) and DINOv3-7B (Simeoni et al., 2025), which require hundreds of millions of parameters or massive datasets), we emphasize that our entire AnyDepth is designed from light-weight and simple perspective, focusing not only on model design but also on data quality and quantity. Inspired by the principles of data-centric learning, we conclude that our model can achieve superior performance even with relatively small amount of high-quality data (369K). Table 5: Decoder parameter comparison across different ViT backbones. Lower is better. Decoder ViT Backbone Params (M) DPT ViT-S ViT-B ViT-L 50.83 76.05 99. SDT ViT-S ViT-B ViT-L 5.51 9.45 13.38 SDT uses only 513M parameters and outperforms DPT with various encoder sizes. Our results show that SDT significantly reduces the number of parameters and training cost while maintaining comparable accuracy to DPT, and there is slight improvement in inference speed  (Fig. 3)  . AnyDepth provides lightweight, efficient, and computationally friendly alternative. 4.4 EFFICIENCY We comprehensively evaluated efficiency advantages of AnyDepth. Compared to DPT, AnyDepth not only significantly reduces the number of parameters (Fig.2a), but also shows that AnyDepth significantly reduces FLOPs by 37% when using models of varying sizes, particularly at high res7 AnyDepth: Depth Estimation Made Easy Figure 6: Qualitative results of zero-shot monocular depth estimation using AnyDepth of ViT-B and comparison with DPT-B. Table 4: Multi-resolution efficiency comparison of SDT and DPT heads under ViT-L encoder. Latency is averaged over 1000 runs on an NVIDIA H100 GPU. Lower is better. Resolution Decoder FLOPs (G) Latency (ms) 256256 512512 10241024 DPT SDT (Ours) DPT SDT (Ours) DPT SDT (Ours) 444.14 234.17 1776.56 936.70 7106.22 3746.79 6.66 0.22 6.10 0.33 24.65 0.22 23.17 0.54 99.79 0.79 93.09 0. olutions (Fig.2b). It also slightly improves inference speed  (Fig.3)  . Furthermore, Average iteration time of AnyDepth during training is 10% shorter than that of DPT. To explore the sources of these efficiency improvements, we further compared the efficiency of the proposed SDT decoder and DPT decoder under the same experimental settings. As shown in Tables 5 and Table 4, SDT consistently and significantly reduces the number of parameters and computational cost across different ViT backbone network sizes and input resolutions. Importantly, the reduction in model size did not affect runtime performance, as the inference latency of SDT is comparable to or even slightly faster than that of DPT. 4.5 REAL WORLD EVALUATION The robot As shown in Fig. 7, We use the WHEELTEC R550 as the mobile platform for realworld evaluation. is equipped with Jetson Orin Nano 4GB as the onboard computing unit and an Astra Pro RGB-D camera as the perception unit. To evaluate its universality under various realworld conditions, we set up three differa conference room, corrient scenarios: dor, and rest area. Under the same encoder experimental setup, we used different the SDT dedecoders for real-world qualitative evaluation. coder performs better than the DPT decoder, displaying clearer boundaries in complex areas. Figure 7: Hardware and Evaluation Pipeline for Real-World Experiments As shown in Figure 10, Furthermore, we compared the efficiency performance of SDT and DPT on edge devices. As shown in Table 6, we compared the inference latency and throughput of the SDT and DPT decoders on the Jetson Orin Nano (4GB) at two input resolutions. At both 256256 and 512512 resolutions, SDT consistently outperforms DPT in terms of inference laTable 7: Peak GPU memory usage during inference at 256 256 resolution on Jetson Orin Nano (4GB). Decoder Peak Memory (MB) DPT SDT (Ours) 589.5 395.2 8 AnyDepth: Depth Estimation Made Easy Table 6: Inference latency comparison of SDT and DPT decoders on Jetson Orin Nano (4GB). Resolution Decoder Latency (ms) FPS 256256 512512 DPT SDT (Ours) DPT SDT (Ours) 305.65 213. 1107.64 831.48 3.3 4.7 0.9 1.2 Table 8: Ablation experiments of AnyDepth-B on five benchmarks. We report AbsRel (lower is better) and δ1 (higher is better). Method NYUv ETH3D AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 ScanNet DIODE KITTI w/o Filtering Filtering Filtering + SDE Filtering + SDE + Dysample 9.5 9.3 8.8 7. 91.1 91.6 92.4 95.0 15.4 15.1 14.7 9.7 77.3 78.1 79.6 90.1 14.0 12.8 11.5 8.0 91.2 90.5 91.0 94.5 8.3 8.0 7.9 6. 93.5 93.9 94.1 95.6 25.0 24.8 24.3 23.6 71.1 71.1 71.1 72.7 tency and frame rate. As shown in Table 7, at 256256 resolution, SDT requires approximately 33% less peak memory than the DPT decoder. 4.6 ABLATION STUDY We conducted ablation studies to validate our design. We used AnyDepth of ViT-B to progressively test our components, including data filtering, SDE, and DySample. As shown in the table 8, these ablation studies further support the effectiveness of data-centric learning in monocular depth estimation and demonstrate the detail enrichment capability of the SDE module and the additional gain of DySample compared to bilinear upsampling."
        },
        {
            "title": "5 LIMITATIONS AND FUTURE WORK",
            "content": "While our work demonstrates advantages, it also has some limitations. First, the current pipeline has not been evaluated in large-scale fully supervised or fine-tuned settings. Second, further analysis In future work, we can extend our of the dataset can be used to optimize the filtering strategy. lightweight framework to wider range of tasks, such as metric depth and normal estimation."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce AnyDepth, simple and efficient-to-train framework for zero-shot monocular depth estimation. In our setup, powerful self-supervised visual backbone paired with single-path lightweight decoder is sufficient to achieve competitive performance without the need for large-scale, costly training. The goal of AnyDepth is not to surpass large-scale state-of-the-art methods, but rather to provide more practical and academically valuable approach through its lightweight design and improved data quality."
        },
        {
            "title": "REFERENCES",
            "content": "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 40094018, 2021. Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zeroshot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 9 AnyDepth: Depth Estimation Made Easy Reiner Birkl, Diana Wofk, and Matthias Muller. Midas v3. 1a model zoo for robust monocular relative depth estimation. arXiv preprint arXiv:2307.14460, 2023. Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoderdecoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pp. 801818, 2018. Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-image depth perception in the wild. Advances in neural information processing systems, 29, 2016. Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. Oasis: large-scale dataset for single image 3d in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 679688, 2020. Zeyu Cheng, Yi Zhang, and Chengkai Tang. Swin-depth: Using transformers and multi-scale fusion for monocular-based depth estimation. IEEE Sensors Journal, 21(23):2691226920, 2021. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1078610796, 2021. David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with common multi-scale convolutional architecture. In Proceedings of the IEEE international conference on computer vision, pp. 26502658, 2015. Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In European Conference on Computer Vision, pp. 241258. Springer, 2024. Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):12311237, 2013. Golnaz Ghiasi, Tsung-Yi Lin, and Quoc Le. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 70367045, 2019. Clement Godard, Oisin Mac Aodha, and Gabriel Brostow. Unsupervised monocular depth estimation with left-right consistency. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 270279, 2017. Jianzhong He, Shiliang Zhang, Ming Yang, Yanhu Shan, and Tiejun Huang. Bi-directional cascade network for perceptual edge detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 38283837, 2019. Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. arXiv preprint arXiv:2409.18124, 2024. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 10 AnyDepth: Depth Estimation Made Easy Ting Huang, Dongjian Li, Rui Yang, Zeyu Zhang, Zida Yang, and Hao Tang. Mobilevla-r1: Reinforcing vision-language-action for mobile robots. arXiv preprint arXiv:2511.17889, 2025a. Ting Huang, Zeyu Zhang, and Hao Tang. 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding. arXiv preprint arXiv:2507.23478, 2025b. Ting Huang, Zeyu Zhang, Yemin Wang, and Hao Tang. 3d coca: Contrastive learners are 3d captioners. arXiv preprint arXiv:2504.09518, 2025c. Ting Huang, Zeyu Zhang, Ruicheng Zhang, and Yang Zhao. Dc-scene: Data-centric learning for 3d scene understanding. arXiv preprint arXiv:2505.15232, 2025d. Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9492 9502, 2024. Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019. Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, and Lin Gu. Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2077520785, 2024a. Pengzhi Li, Yikang Ding, Haohan Wang, Chengshuai Tang, and Zhiheng Li. The devil is in the edges: monocular depth estimation with edge-aware consistency fusion. arXiv preprint arXiv:2404.00373, 2024b. Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang. Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation. Machine Intelligence Research, 20(6):837854, 2023. Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 21172125, 2017. Qingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang. Nav-r1: Reasoning and navigation in embodied scenes. arXiv preprint arXiv:2509.10884, 2025a. Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 87598768, 2018. Wenze Liu, Hao Lu, Hongtao Fu, and Zhiguo Cao. Learning to upsample by learning to sample. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 60276037, 2023. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. Zeting Liu, Zida Yang, Zeyu Zhang, and Hao Tang. Evovla: Self-evolving vision-language-action model. arXiv preprint arXiv:2511.16166, 2025b. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 11 AnyDepth: Depth Estimation Made Easy Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and In Proceedings of the Fisher Yu. Unidepth: Universal monocular metric depth estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1010610116, 2024. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, and Haibin Ling. Edter: Edge detection with transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14021412, 2022. Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1217912188, 2021. Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1091210922, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and In Proceedings of the IEEE conference on computer vision and pattern multi-camera videos. recognition, pp. 32603269, 2017. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and supIn European conference on computer vision, pp. 746760. port inference from rgbd images. Springer, 2012. Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. Prerna Singh. Systematic review of data-centric approaches in artificial intelligence and machine learning. Data Science and Management, 6(3):144157, 2023. Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, et al. Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulation with large vision-language models. arXiv preprint arXiv:2505.16517, 2025. Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi Tian, Matti Pietikainen, and Li Liu. Pixel difference networks for efficient edge detection. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 51175127, 2021. Mingxing Tan, Ruoming Pang, and Quoc Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1078110790, 2020. Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. Irs: large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation. arXiv preprint arXiv:1912.09678, 2019. 12 AnyDepth: Depth Estimation Made Easy Weijie Wang, Donny Chen, Zeyu Zhang, Duochao Shi, Akide Liu, and Bohan Zhuang. Zpressor: Bottleneck-aware compression for scalable feed-forward 3dgs. arXiv preprint arXiv:2505.23734, 2025a. Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Chen, and Bohan Zhuang. Volsplat: Rethinking feed-forward 3d gaussian splatting with voxel-aligned prediction. arXiv preprint arXiv:2509.19297, 2025b. Weijie Wang, Jiagang Zhu, Zeyu Zhang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Haoxiao Wang, Guan Huang, Xinze Chen, et al. Drivegen3d: Boosting feed-forward driving scene generation with efficient video diffusion. arXiv preprint arXiv:2510.15264, 2025c. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 568578, 2021. Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In RSJ International Conference on Intelligent Robots and Systems (IROS), pp. in 2020 ieee. 49094916. Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1768317693, 2022. Zhengri Wu, Yiran Wang, Yu Wen, Zeyu Zhang, Biao Wu, and Hao Tang. Stereoadapter: Adapting stereo depth estimation to underwater scenes. arXiv preprint arXiv:2509.16415, 2025. Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. What matters when repurposing diffusion models for general dense perception tasks? arXiv preprint arXiv:2403.06090, 2024. Xianfa Xu, Zhe Chen, and Fuliang Yin. Monocular depth estimation with multi-scale feature fusion. IEEE Signal Processing Letters, 28:678682, 2021. Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 82548263, 2023. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1037110381, 2024a. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024b. Zhenheng Yang, Peng Wang, Wei Xu, Liang Zhao, and Ramakant Nevatia. Unsupervised learning of geometry from videos with edge-aware depth-normal consistency. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: large-scale dataset for generalized multi-view stereo networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 17901799, 2020. Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, and Zheng Zhu. Vla-r1: Enhancing reasoning in vision-language-action models. arXiv preprint arXiv:2510.01623, 2025. Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth prediction using diverse data. arXiv preprint arXiv:2002.00569, 2020. 13 AnyDepth: Depth Estimation Made Easy Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 204213, 2021. Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-centric artificial intelligence: survey. ACM Computing Surveys, 57(5):142, 2025. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023. 14 AnyDepth: Depth Estimation Made Easy"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LLM USE DECLARATION Large Language Models (ChatGPT) were used exclusively to improve the clarity and fluency of English writing. They were not involved in research ideation, experimental design, data analysis, or interpretation. The authors take full responsibility for all content. A.2 DATA CENTRIC LEARNING Although MiDaS (Ranftl et al., 2020) uses an affine-invariant loss to accommodate multi-dataset training, the varying degrees of noise and scale ambiguity introduced by these datasets can easily negatively impact training, especially in dense prediction tasks (Fig.8, 9). Inspired by data-centric learning (Singh, 2023; Zha et al., 2025), for the monocular depth estimation task and our setting, we believe that high-quality samples should possess two properties: (i) depth values should be evenly distributed throughout the image, rather than being overly concentrated within specific range; and (ii) gradient magnitudes should vary slightly across continuous surfaces, while exhibiting more pronounced changes near object edges. Based on these two properties, we define two metrics to measure sample quality. These metrics aim to reduce low-quality samples, facilitate model training, and reduce dataset size and training cost. A.2.1 DEPTH DISTRIBUTION SCORE Some samples have depths that are primarily concentrated near or far, while other depth ranges are relatively small. As shown in Fig. 8 , this phenomenon is common in outdoor datasets. This unbalanced depth distribution can cause the model to favor learning depth values within specific range rather than the entire valid depth range, leading to unstable training and poor model generalization. To quantify this phenomenon, we propose Depth Distribution Score that evaluates how uniformly depth values are distributed across the available depth range. For depth map RHW , we divide the depth values into bins of equal width, and we use = 20 by default to balance granularity and robustness. Chi-square Deviation (Sχ2). We measure the deviation from uniform distribution using the chisquare statistic: χ2 = (cid:88) k=1 (nk n)2 (cid:18) , Sχ2 = exp (cid:19) , χ2 (7) where nk is the number of depth bins k, = N/K is the expected number under uniform distribution, and is the total number of valid depth values. We use an exponential transformation to map the chi-squared statistic (Eq. 7) to [0, 1], with higher scores indicating more uniform distribution. Maximum Concentration Index (Sconc). To prevent excessive concentration in any single depth interval, we penalize the maximum bin occupancy: Sconc = (cid:40)1, 1 min (cid:16) 1, pmax2/K 0.52/K (cid:17) , if pmax 2/K otherwise (8) where pmax = maxk(nk)/N is the maximum bin probability. This formulation (Eq. 8) tolerates up to twice the ideal concentration (2/K) without penalty, then linearly decreases the score as concentration increases. Range Utilization (Srange) . Partition the available depth range into equal-width bins and let nk be the count in bin k. Define the number of non-empty bins K+ = { {1, . . . , K} nk > 0 }. The range utilization score is Srange = K+/K, which penalizes samples whose depths concentrate within narrow portion of the range. The final Depth Distribution Score Sdist is the weighted sum of these three scores: Sdist = λ1 Sχ2 + λ2 Sconc + λ3 Srange, (9) where we empirically set λ1 = 0.5, λ2 = 0.3, and λ3 = 0.2. 15 AnyDepth: Depth Estimation Made Easy A.2.2 GRADIENT CONTINUITY SCORE In the real world, continuous physical surfaces should have smoothly transitioning depth values, without drastic random fluctuations. However, perhaps due to rendering defects in synthetic data, some sample depth maps exhibit gradient abrupt changes caused by noise on smooth surfaces. If these samples are used for training, the model will learn incorrect depth changes, thus affecting prediction quality. Inspired by the gradient loss function ((Li et al., 2024b; Yang et al., 2018; Ranftl et al., 2020)), we propose gradient continuity score to assess the noise content of each sample. We first calculate the gradient magnitude G(i, j) = (cid:112)(xD)2 + (yD)2. To distinguish reasonable gradient abrupt changes at normal object edges from those caused by abnormal noise, we define edge pixels as pixels with gradient magnitudes in the top 10%. Within the smooth region, we use the coefficient of variation CV = σG µG to assess gradient consistency: Sgrad = 1 1 + CV , (10) where µG and σG are the mean and standard deviation of the gradient magnitude in the region, respectively. A.2. TOTAL SCORE The depth distribution score and gradient continuity score capture different aspects of sample quality. We combine them into Total Score, defined as Stotal = (Sgrad +Sdist)/2, to assess the overall quality of each sample for dataset filtering (Eq. 9, 10). Its important to note that our goal is not to provide particularly precise quality assessment method, but rather to design efficient indicators to quickly filter out samples with quality issues. For example, when performing edge detection, we did not use traditional Canny or Sobel algorithms because the detected edge maps often produce unnecessary artifacts and details. Learning-based methods, on the other hand, predict edges that are always several pixels off from their exact locations (Li et al., 2024b; He et al., 2019; Pu et al., 2022; Su et al., 2021), and their inference time is time-consuming, making them unsuitable for rapid filtering of large datasets. A.3 VISUALIZATION OF LOW-QUALITY SAMPLES Figure 8 provides qualitative examples of low-quality samples from five training datasets. It can be seen that some datasets contain samples with highly uneven depth value distributions, leading to biased supervision. This situation motivates us to use depth distribution score when evaluating dataset quality. In addition, Figure 9 shows RGB images, gradient maps, and ground-truth depth examples from the same five datasets. The highlighted areas indicate the presence of severe gradient noise or inconsistent edges, which can negatively impact training stability. These qualitative findings support our quantitative gradient consistency metric. AnyDepth: Depth Estimation Made Easy Figure 8: RGB images and GT of each dataset, showing that the depth value distribution of some samples is not uniform. 17 AnyDepth: Depth Estimation Made Easy Figure 9: Examples of RGB, gradient, and GT depth from five datasets. The dotted box highlights the noisy area. AnyDepth: Depth Estimation Made Easy Figure 10: Qualitative results of zero-shot monocular depth estimation with different decoders (DPT, Dual-DPT, and SDT) using the same encoder."
        }
    ],
    "affiliations": [
        "Peking University",
        "Shanghai University of Engineering Science",
        "The University of Melbourne"
    ]
}