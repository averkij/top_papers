{
    "paper_title": "Stable Consistency Tuning: Understanding and Improving Consistency Models",
    "authors": [
        "Fu-Yun Wang",
        "Zhengyang Geng",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling. These models are trained either through consistency distillation, which leverages pretrained diffusion models, or consistency training/tuning directly from raw data. In this work, we propose a novel framework for understanding consistency models by modeling the denoising process of the diffusion model as a Markov Decision Process (MDP) and framing consistency model training as the value estimation through Temporal Difference~(TD) Learning. More importantly, this framework allows us to analyze the limitations of current consistency training/tuning strategies. Built upon Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT), which incorporates variance-reduced learning using the score identity. SCT leads to significant performance improvements on benchmarks such as CIFAR-10 and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID 1.55, a new SoTA for consistency models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 2 8 5 9 8 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "STABLE CONSISTENCY TUNING: UNDERSTANDING AND IMPROVING CONSISTENCY MODELS Fu-Yun Wang MMLab, CUHK Hong Kong SAR fywang@link.cuhk.edu.hk Zhengyang Geng Carnegie Mellon University Pittsburgh, USA zhengyanggeng@gmail.com Hongsheng Li MMLab, CUHK Hong Kong SAR hsli@ee.cuhk.edu.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, new generative family, achieve competitive performance with significantly faster sampling. These models are trained either through consistency distillation, which leverages pretrained diffusion models, or consistency training/tuning directly from raw data. In this work, we propose novel framework for understanding consistency models by modeling the denoising process of the diffusion model as Markov Decision Process (MDP) and framing consistency model training as the value estimation through Temporal Difference (TD) Learning. More importantly, this framework allows us to analyze the limitations of current consistency training/tuning strategies. Built upon Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT), which incorporates variance-reduced learning using the score identity. SCT leads to significant performance improvements on benchmarks such as CIFAR-10 and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID 1.55, new SoTA for consistency models. Code is available at https://github.com/G-U-N/Stable-Consistency-Tuning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have significantly advanced the field of visual generation, delivering state-of-the-art performance in images (Dhariwal & Nichol, 2021; Rombach et al., 2022; Song & Ermon, 2019; Karras et al., 2022; 2024b), videos (Shi et al., 2024; Blattmann et al., 2023; Singer et al., 2022; Brooks et al., 2024; Bao et al., 2024), 3D (Gao et al., 2024; Shi et al., 2023), and 4D data (Ling et al., 2024). The core principle of diffusion models is the iterative transformation of pure noise into clean samples. However, this iterative nature comes with tradeoff: while it enables superior generation quality and training stability compared to traditional methods (Goodfellow et al., 2020; Sauer et al., 2023a), it requires substantial computational resources and longer sampling time (Song et al., 2020; Ho et al., 2020). This limitation becomes substantial bottleneck when generating high-dimensional data, such as high-resolution images and videos, where the increased generation cost slows practical application. Consistency models (Song et al., 2023), an emerging generative family, largely address these challenges by enabling high-quality, one-step generation without adversarial training. Recent studies (Song & Dhariwal, 2023; Geng et al., 2024) have shown that one-step and two-step performance of consistency models can rival that of leading diffusion models, which typically require dozens or even hundreds of inference steps, underscoring the tremendous potential of consistency models. The primary training objective of consistency models is to enforce the self-consistency condition (Song et al., 2023), where predictions for any two points along the same trajectory of the probability flow ODE (PF-ODE) converge to the same solution. To achieve this, consistency models adopt two training methods: consistency distillation (CD) and consistency training/tuning (CT). Consistency distillation leverages frozen pretrained diffusion model to simulate the PF-ODE, while consistency training/tuning directly learns from real data with no need for extra teacher models. The motivation of this paper is to propose novel understanding of consistency models from the perspective of bootstrapping. Specifically, we first frame the numerical solving process of the PF-"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides unifying perspective to understand different training strategies of consistency models. ODE (i.e., the reverse diffusion process) as Markov Decision Process (MDP), also indicated in prior works (Black et al., 2023; Fan et al., 2024). The initial state of the MDP is randomly sampled from Gaussian. The intermediate state consists of the denoised sample xt and the corresponding conditional information, including the timestep t. The policy function of the MDP corresponds to the action of applying the ODE solver to perform single-step denoising, resulting in the transition to the new state. Building on this MDP, we show that the training of consistency models, including consistency distillation, consistency training/tuning, and their variants, can be interpreted as Temporal difference (TD) learning (Sutton & Barto, 2018), with specific reward and value functions aligned with the PF-ODE. From this viewpoint, we can derive, as we will elaborate later, that the key difference between consistency distillation and consistency learning lies in how the ground truth reward is estimated. The difference leads to distinct behaviors: Consistency distillation has lower performance upper bound (being limited by the performance of the pretrained diffusion model) but exhibits lower variance and greater training stability. Conversely, consistency training/tuning offers higher performance upper bound but suffers from larger variance in reward estimation, which can lead to unstable training. Additionally, for both CD and CT, smaller ODE steps (i.e., r) can improve the performance ceiling but complicate the optimization. Building upon the foundation of Easy Consistency Tuning (ECT), we introduce Stable Consistency Tuning (SCT), which incorporates several enhancements for variance reduction and faster convergence: 1) We introduce variance-reduced training target for consistency training/tuning via the score identity (Vincent, 2011; Xu et al., 2023), which provides better approximation of the ground truth score. This helps improve training stability and facilitates better performance and convergence. Additionally, we show that variance-reduced estimation can be applied to conditional generation settings for the first time. 2) Our method adopts smoother progressive training schedule that facilitates training dynamics and reduces discretization error. 3) We extend the scope of ECT to multistep settings, allowing for deterministic multistep sampling. Additionally, we investigate the potential capacity and optimization challenges of multistep consistency models and propose an edge-skipping multistep inference strategy to improve the performance of multistep consistency models. 4) We validate the effectiveness of classifier-free guidance in consistency models, where generation is guided by sub-optimal version of the consistency model itself."
        },
        {
            "title": "2 PRELIMINARIES ON CONSISTENCY MODELS",
            "content": "In this section, we present the essential background on consistency models to ensure more selfcontained explanation. Diffusion Models define forward stochastic process with the intermediate distributions Ptpxtx0q conditioned on the initial data x0 P0 (Lipman et al., 2022; Kingma et al., 2021a). The intermediate states follow general form xt αtx0 ` σtϵ with x1 ϵ p0, Iq. The forward process can be"
        },
        {
            "title": "Preprint",
            "content": "described with the following stochastic differential equation (SDE): dxt ftx0dt ` gtdwt, (1) where wt is the standard Wiener process, ft log αt . For the above forward SDE, remarkable property is that there exists reverse-time ODE trajectory for data sampling, which is termed as probability flow ODE (PF-ODE) (Song et al., 2023) That is, dt 2 log αt dt σ , and g2 dt dσ2 dx ft ȷ log Ptpxq g2 2 dt. (2) It allows for data sampling without introducing additional stochasticity while satisfying the predefined marginal distributions Ptpxtq EPpx0qrPpxt x0qs. In diffusion models, neural network ϕ is typically trained to approximate the score function sϕpxt, tq log Ptpxtq, enabling us to apply numerical solver to approximately solving the PF-ODE for sampling. Many works apply epsilon-prediction ϵϕpxt, tq σtx log Ptpxtq form for training. Consistency Models propose training approach that teaches the model to directly predict the solution point of the PF-ODE, thus enabling 1-step generation. Specifically, for given trajectory txtutPr0,1s, the consistency model fθpxt, tq is trained to satisfy fθpxt, tq x0, @t r0, 1s, where x0 is the solution point on the same PF-ODE with xt. The training strategies of consistency models can be categorized into consistency distillation and consistency training. But they share the same training loss design, dpfθpxt, tq, fθpxr, rqq , (3) where dp, is the loss function, 0 ď ă ď 1, θ is the EMA weight of θ or simply set to θ with gradient disabled for backpropagation. Both xt and xr should be approximately on the same PF-ODE trajectory."
        },
        {
            "title": "3 UNDERSTANDING CONSISTENCY MODELS",
            "content": "Consistency model as bootstrapping. For general form of diffusion xt αtx0 ` σtϵ, there exists an exact solution form of PF-ODE as shown in previous work (Song et al., 2021; Lu et al., 2022), ż xs xt αs eλϵpxtλ, tλqdλ , λs αs αt (4) λt where λt lnpαt{σtq, tλ is the reverse function of tλ, and ϵpxtλ,tq σtλ log Ptλpxtλq is the scaled score function. Consistency models aim to learn x0 predictor with only the information from xt, @t r0, 1s. The left term is already known with xt, and thereby we can write the consistency model-based x0 prediction as ˆx0pxt, t; θq 1 αt xt hθpxt, tq, (5) where is set to 0 with αs 1, θ is the model weights, and hθ is applied to approximate the weighted integral of ϵ from to 0. The loss of consistency models penalize the x0 prediction distance between xt and xr at adjacent timesteps, where 0 ď ă and θ is the EMA weight of θ. Therefore, we have the following learning target ˆx0pxt, t; θq fitÐÝ ˆx0pxr, r; θq , (6) Noting that xr αr αt and have xt αr 1 αt ş λr λt xt hθpxt, tq fitÐÝ 1 αr xr hθ pxr, rq (7) eλϵpxtλ, tλqdλ, and hence we replace the xr in the above equation hθpxt, tq fitÐÝ ` hθ pxr, rq , (8)"
        },
        {
            "title": "Preprint",
            "content": "Table 1: The definition of symbols in the value estimation of the PF-ODE equivalent MDP."
        },
        {
            "title": "Definition",
            "content": "stn atn P0ps0q pstn`1 stn , atnq πpatn stn Rpstn, atn Vθpstnq ptN n, xtN xtN n1 : ΦpxtN , tN n, tN n1q ptN , p0, Iqq pδtN n1, δxtN n1 δxtN n1 şλtN n1 λtN hθpxtN n, tN nq eλϵpxtλ, tλqdλ ş λr λt where estimation at state xt, hθ pxr, rq is the value estimation at state xr, and is the step reward. eλϵpxtλ, tλqdλ. The above equation is Bellman Equation. hθpxt, tq is the value Standard formulation. It is known that the diffusion generation process can be modeled as Markov Decision Process (MDP) (Black et al., 2023; Fan et al., 2024), and here we show that the training of consistency models can be viewed as value estimation learning process, which is also known as Temporal Difference Learning (TD-Learning), in the equivalent MDP. We show the standard formulation in Table 1. In Table 1, stn and atn are the state and action at timestep tn, P0 and are the initial state distribution and state transition distribution, ΦpxtN n, tN n, tN n1q is the ODE solver, π is the policy following the PF-ODE, reward is equivalent to the defined above and value function Vθ is corresponding to hθ. π is the Dirac distribution δ due to the deterministic nature of PF-ODE. From this perspective, we can have unifying understanding of consistency model variants and their behaviors. Fig. 1 provides straightforward illustration of our insight. One of the most important factors of the consistency model performance is how we estimate in the equation. Understanding consistency distillation. For consistency distillation, the approximation of is dependent on the pretrained diffusion model ϵϕ and the ODE solver applied. For instance, if the first-order DDIM (Song et al., 2020) is applied, then the approximation is formulated as, ϵϕpxt, tq ż λr λt eλdλ ` Oppλr λtq2q . (9) We can observe that the error comes from two aspects: one is the prediction error between the pretrained diffusion model ϵϕpxt, tq and the ground truth ϵpxt, tq; the other is the first-order assumption that ϵpxtλ, tλq ϵpxt, tq, @tλ rt, rs. The first error indicates better pretrained diffusion model can lead to better performance of consistency distillation. The second error indicates that the distance between and should be small enough to eliminate errors caused by low-order approximation. This perspective also connects the n-step TD algorithm with the consistency distillation. The n-step TD is equivalent to applying the multistep (n-step) ODE solver to compute the xr from xt. Understanding consistency training/tuning. For consistency training/tuning, the approximation of is achieved through approximating the ground truth ϵpxt, tq with the conditional ϵpxt, t; x0q, where x0 is sampled from the dataset and xt αtx0 ` σtϵ with ϵ p0, Iq. It is known that the ground truth ϵpxt, tq is equivalent to ϵpxt, tq σtxt log Ptpxtq σtEPtpx0xtq rxt log Ppxt x0qs ȷ σtEPpx0xtq xt αtx0 σ2 EPpx0xtq ȷ xt αtx0 σt EPpx0xtq rϵpxt, t; x0qs , (10)"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Phasing the ODE path along the time axis for consistency training. We visualize both training and inference techniques in discrete form for easier understanding. where ϵpxt, t; x0q xtαtx0 . In simple terms, the ground truth ϵpxt, tq is the expectation of σt all possible conditional epsilon ϵpxt, t; x0q, @x0 D. Instead, previous works on consistency training/tuning apply the conditional epsilon to approximate the ground truth epsilon, which can be regarded as one-shot MCMC approximation. The approximation is formulated as, ϵpxt, t; x0q ż λr λt eλdλ ` Oppλr λtq2q (11) Similarly, the error comes from two aspects: one is the difference between conditional epsilon ϵpxt, t; x0q and ground truth epsilon ϵpxt, tq; the other is the first-order approximation error. Even though, it is shown by previous work that the final learning objective will converge to the ground truth under minor assumptions (e.g., L-Lipschitz continuity) (Song et al., 2023). hθpxt, tq EPpx0xtq rϵpxt, t; x0qs ż λr λt eλdλ ` hθpxr, rq . (12) However, the variance of one-shot MCMC is large. This causes the consistency training/tuning is not as stable as distillation methods even though it has better upper bound. Summary. In summary, the main performance bottlenecks in improving consistency training/tuning can be attributed to two factors: 1. Training Variance: This refers to the gap between the conditional epsilon ϵpxt, t; x0q and the ground truth epsilon ϵpxt, tq. Although, in theory, the conditional epsilon is expected to match the ground truth epsilon on average, it exhibits higher variance, which introduces instability and deviations during training. 2. Discretization Error: In the numerical solving of the ODE for consistency training/tuning, only first-order solvers can be approximated. To push performance to its upper limit, the time intervals between sampled points, and r, must be minimized, i.e., dt limpt rq Ñ 0. However, smaller dt results in longer information propagation process (with large ). If the training process lacks stability, error accumulation through bootstrapping may occur, potentially causing training failure."
        },
        {
            "title": "4 STABLE CONSISTENCY TUNING",
            "content": "Our method builds upon Easy Consistency Tuning (ECT) (Geng et al., 2024), chosen for its efficiency in prototyping. Given our analysis of consistency models from the bootstrapping perspective, we introduce several techniques to enhance performance."
        },
        {
            "title": "4.1 REDUCING THE TRAINING VARIANCE",
            "content": "Previous research has shown that reducing the variance for diffusion training can lead to improved training stability and performance (Xu et al., 2023). However, this technique has only been applied to unconditional generation and diffusion model training. We generalize this technique to both conditional/unconditional generation and consistency training/tuning for variance reduction. Let represent the conditional inputs (e.g., class labels). We begin with xt log Ptpxt cq EPpx0xt,cq rxt log Ptpxt x0, cqs EPpx0cq EPpx0cq EPpx0cq Ppx0 xt, cq Ppx0 cq Ppxt x0, cq Ppxt cq Ppxt x0q Ppxt cq ȷ xt log Ptpxt x0, cq ȷ xt log Ptpxt x0, cq ȷ xt log Ptpxt x0q 1 1 1 i0,...n1ÿ txpiq 0 uPpx0cq i0,...n1ÿ txpiq 0 uPpx0cq i0,...n1ÿ txpiq 0 uPpx0cq Ppxt xpiq 0 Ppxt cq xt log Ptpxt xpiq 0 (13) ř ř Ppxt xpiq 0 Ppxt xpjq 0 Ptxpiq xpjq 0 Ppxt xpiq 0 Ppxt xpjq 0 0 Ptxpiq xpjq 0 0 , cq xt log Ptpxt xpiq 0 xt log Ptpxt xpiq 0 The key difference between the variance-reduced score estimation of conditional generation and unconditional generation is whether the samples utilized for computing the variance-reduced target are sampled from the conditional distribution Ppx0 cq or not. In the class-conditional generation, this means we compute stable targets only within each class cluster. For text-to-image generation, we might estimate probabilities using CLIP (Radford et al., 2021) text-image similarity, though we leave this for future study. Therefore, the conditional epsilon estimation adopted in previous consistency training/tuning can be replaced by our variance-reduced estimation: ϵpxt, tq σtxt log Ptpxtq i0,...n1ÿ ř 0 uPpx0cq 0 Ptxpiq xpjq 0 Ppxt xpiq 0 Ppxt xpjq 0 pσtxt log Ptpxt xpiq 0 qq (14) 1 1 txpiq n1ÿ i0 Wiϵpxt, t; xpiq 0 qq , where Wi ř Ppxtxpiq 0 Ppxtxpjq 0 pjq 0 Ptx piq 0 is the weight of conditional ϵpxt, t; xpiq 0 q. 4.2 REDUCING THE DISCRETIZATION ERROR As discussed earlier, to achieve higher performance, we need to minimize pt rq. On one hand, when is relatively large, the model suffers from increased discretization errors. On the other hand, when is too small, it may lead to error accumulation or even training failure. Previous works (Song"
        },
        {
            "title": "Preprint",
            "content": "et al., 2023; Song & Dhariwal, 2023; Geng et al., 2024) employ progressive training strategy, which has consistently been shown to be effective. The model is initially trained with relatively large t, and as training progresses, is gradually reduced. Although larger introduces higher discretization errors, it allows for faster optimization, enabling the model to quickly learn coarse solution. Gradually decreasing allows the model to learn more fine-grained results, ultimately improving performance. In the ECT, the training schedule is determined by LogNormalpPmean, Pstdq, : ReLU 1 nptq (15) ˆ 1 qtiter{du , where is to determine the shrinking speed, is to determine the shrinking frequency, ReLU is equivalent to maxp, 0q, and nptq is pre-defined monotonic function. We note that it is beneficial to apply smoother shrinking process. That is, we reduce both and to obtain smoother shrinking process than the original ECT settings. This provides our method with faster and smoother training process. In addition to the training schedule, training weight is important to balance the training across different timesteps. We apply the weighting 1{pt rq following previous work (Song & Dhariwal, 2023; Geng et al., 2024). Suppose αt, the weighting can be decomposed into 1 p1αq . The weighting scheme has two key effects: First, 1{t assigns higher weights to smaller timesteps, where uncertainty is lower. Predictions at smaller timesteps serve as teacher models for larger timesteps, making stable training at these smaller steps crucial. Second, 1{p1 αq ensures that as decreases, the weight dynamically increases, preventing gradient vanishing during training. We apply smooth term δ ą 0 in the weighting function 1{pt ` δq ď 1 δ to avoid potential numerical issues and instability when the becomes too tiny. ˆ 4.3 PHASING THE ODE FOR CONSISTENCY TUNING Previous works (Heek et al., 2024; Wang et al., 2024a) propose dividing the ODE path along the time axis into multiple segments during training, enabling consistency models to support deterministic multi-step sampling with improved performance. We test our method in this scenario and find that, while this training approach increases the minimum required sampling steps, it improves the fidelity and stability of the generated results. We apply the Euler solver to achieve multistep re-parameterization, formulated as: where Dθ denotes the original consistency model, predicting the ODE solution point x0, and is the edge timestep. We propose new training schedule to adapt to the multistep training setting. xs Dθpxtq ` pxt Dθpxtqq, (16) LogNormalpPmean, Pstdq, : ReLU 1 nptq pt sq ` (17) ˆ 1 qtiter{du"
        },
        {
            "title": "4.4 EXPLORING BETTER INFERENCE FOR CONSISTENCY MODEL",
            "content": "Guiding consistency models with bad version of itself. Previous work (Karras et al., 2024a) demonstrates that even unconditional diffusion models can benefit from classifier-free guidance (Ho & Salimans, 2022). It suggests that the unconditional outputs in classifier-free guidance can be replaced with outputs from sub-optimal version of the same diffusion model, thus extending the applicability of classifier-free guidance. xt log Pθpxtc; tq ` xt log ȷ Pθpxtc; tq Pθpxtc; tq ω , (18) where ω is the guidance strength, θ is sub-optimal version of θ, and represents the optional label conditions. Our empirical investigations confirm that this strategy can be applied to consistency models, resulting in enhanced sample quality. Edge-skipping inference for multistep consistency model. While segmenting the ODE path to train multistep consistency model can enhance generation quality, it may encounter optimization challenges, especially around the edge timesteps tsiun i1 with s1 1 ą ą si ą ą sn 0. For timesteps si1 ě ą si, the consistency model learns to predict xsi from xt. However, for si ě t1 ą si`1, the model learns to predict xsi`1 from xt1 . When and t1 are very close to si,"
        },
        {
            "title": "Preprint",
            "content": "(a) 1-step (b) 2-step Figure 3: FID vs Training iterations. SCT has faster convergence speed and better performance upper bound than ECT. and t1 denoted as s` model is expected to predict two distinct results (xsi and xsi`1) from very similar inputs (xs` xs can be very similar. However, the and , it is apparent that xs` and xs ). i Neural networks typically follow L-Lipschitz continuity, where small input changes result in small output changes. This property conflicts with the requirement to produce distinct outputs from similar inputs near edge timesteps, potentially leading to insufficient training, particularly near . To address this, we propose skipping the edge timesteps during multistep sampling. Specifically, even though we aim for the model to perform sampling through the timesteps we instead achieve multistep sampling via s1 : 1 Ñ s2 Ñ s3 Ñ Ñ sn : 0 , s1 : 1 Ñ ηs2 Ñ ηs3 Ñ Ñ ηsn : 0 , (19) (20) where η ą 0 is scaling factor. When η is set to 1, the process reverts to normal multistep sampling. This method works because the predictions of xsi and xηsi are close when η is near 1, allowing for tolerable degree of approximation error. Fig. 2 illustrates this concept with discrete example. The model is designed to sample via the sequence x1 Ñ x3{6 Ñ x0; however, it instead samples through the sequence x1 Ñ x2{6 Ñ x0."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENT SETUPS Evaluation Benchmarks. Following the evaluation protocols of iCT (Song & Dhariwal, 2023) and ECT (Geng et al., 2024), we validate the effectiveness of SCT on CIFAR-10 (unconditional and conditional) (Krizhevsky et al., 2009) and ImageNet-64 (conditional) (Deng et al., 2009). Performance is measured using Frechet Inception Distance (FID, lower is better) (Heusel et al., 2017) consistent with recent studies (Geng et al., 2024; Karras et al., 2024b). Compared baselines. We compare our method against accelerated samplers (Lu et al., 2022; Zhao et al., 2024), state-of-the-art diffusion-based methods (Ho et al., 2020; Song & Ermon, 2019; 2020; Karras et al., 2022), distillation methods (Zhou et al., 2024; Salimans & Ho, 2022), alongside consistency training and tuning approaches. Among these models, consistency training and tuning methods serve as key baselines, including CT (LIPIPS) (Song et al., 2023), iCT (Song & Dhariwal, 2023), ECT (Geng et al., 2024), and MCM (CT) (Heek et al., 2024). CT introduces the first consistency training algorithm, utilizing LIPIPS loss to improve FID performance. iCT presents an improved training strategy over CT, making the performance of consistency training comparable to state-of-the-art diffusion models for the first time. MCM (CT) proposes segmenting the ODE path for consistency training, while ECT introduces the concept of consistency tuning along with continuous-time training strategy, achieving notable results with significantly reduced training costs."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The effectiveness of variance reduced training target. Figure 5: The effectiveness of edge-skipping multistep sampling. Model Architectures and Training Configurations From model perspective, iCT is based on the ADM (Dhariwal & Nichol, 2021), ECT is built on EDM2 (Karras et al., 2024b), and MCM follows the UViTs of Simple Diffusion (Hoogeboom et al., 2023). The model size of ECT is similar to that of iCT, while MCM does not explicitly specify the model size. The iCT model is randomly initialized, whereas both ECT and MCM use pretrained diffusion models for initialization. In terms of training costs, iCT uses batch size of 4096 across 800,000 iterations, MCM employs batch size of 2048 for 200,000 iterations, and ECT utilizes batch size of 128 for 100,000 iterations. SCT follows ECTs model architecture and training configuration. 5.2 RESULTS AND ANALYSIS Training efficiency and efficacy. In Fig. 3b, we plot 1-step FID and 2-step FID for SCT and ECT along the number of training epochs, under the same training configuration. From the figure, we observe that SCT significantly improves convergence speed compared to ECT, demonstrating the efficiency and efficacy of SCT training. Additionally, the performance comparisons in Tables 2 and 3 also show that SCT outperforms ECT across different settings. Quantitative evaluation. We present results in Table 2 and Table 3. Our approach consistently outperforms ECT across various scenarios, achieving results comparable to advanced distillation strategies and diffusion/score-based models. The effectiveness of training variance reduction. It is worth noting that SCT and ECT employ different progressive training schedules. To exclude this effect, we adopt ECTs fixed training schedule, in which the 2-step FID surpasses Consistency Distillation within single A100 GPU hour. We use t{256 as fixed partition, with batch size of 128, over 16k iterations on CIFAR-10, while keeping all other settings unchanged. For SCT models on CIFAR10, we calculate the variance-reduced target only within the training batch, which is also the default setting of all our experiments on CIFAR10. To further showcase the effectiveness of the variance-reduced target, we use all 50,000 training samples as reference to compute the target. Although more reference samples are used, they do not directly influence the models computations; they are solely utilized for calculating the training target. Fig. 4 presents comparison of these three methods, showing that our approach achieves notable improvements in both 1-step and 2-step FID. Notably, when using the entire sample set as the reference batch, the improvement becomes more pronounced, with the 1-step FID dropping from 5.61 to 4.56. Figure 6: The effectiveness of classifier-free guidance on consistency models. The Effectiveness of CFG. Inspired by prior work Karras et al. (2024a), we adopt the outputs of the sub-optimal version of the model as the negative part in classifier-free guidance (CFG). We set the"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Comparing the quality of samples on CIFAR-10. Table 3: Comparing the quality of classconditional samples on ImageNet-64. METHOD NFE (Ó) FID (Ó) METHOD NFE (Ó) FID (Ó) Fast samplers & distillation for diffusion models DDIM (Song et al., 2020) DPM-solver-fast (Lu et al., 2022) 3-DEIS (Zhang & Chen, 2022) UniPC (Zhao et al., 2024) Knowledge Distillation (Luhman & Luhman, 2021) DFNO (LPIPS) (Zheng et al., 2022) 2-Rectified Flow (+distill) (Liu et al., 2022) TRACT (Berthelot et al., 2023) Diff-Instruct (Luo et al., 2023) PD (Salimans & Ho, 2022) CTM (Kim et al., 2023) CTM (+GAN +CRJ) SiD (α 1.0) (Zhou et al., 2024) SiD (α 1.2) (Zhou et al., 2024) CD (LPIPS) (Song et al., 2023) Direct Generation Score SDE (Song et al., 2021) Score SDE (deep) (Song et al., 2021) DDPM (Ho et al., 2020) LSGM (Vahdat et al., 2021) PFGM (Xu et al., 2022) EDM (Karras et al., 2022) EDM-G++ (Kim et al., 2022) NVAE (Vahdat & Kautz, 2020) Glow (Kingma & Dhariwal, 2018) Residual Flow (Chen et al., 2019) BigGAN (Brock et al., 2019) StyleGAN2 (Karras et al., 2020b) StyleGAN2-ADA (Karras et al., 2020a) Consistency Training/Tuning CT (LPIPS) (Song et al., 2023) iCT (Song & Dhariwal, 2023) iCT-deep (Song & Dhariwal, 2023) ECT (Geng et al., 2024) SCT SCT SCT (Phased) Cond-SCT Cond-SCT 10 10 10 10 1 1 1 1 2 1 1 2 1 18 1 2 1 1 1 2 2000 2000 1000 147 110 35 35 1 1 1 1 1 1 1 2 1 2 1 2 1 2 1 2 1 2 4 8 1 2 1 2 13.36 4.70 4.17 3.87 9.36 3.78 4.85 3.78 3.32 4.53 8.34 5.58 5.19 3.00 1.98 1.87 2.03 1.98 3.55 2. 2.38 2.20 3.17 2.10 2.35 2.04 1.77 23.5 48.9 14.7 8.32 2.92 8.70 5.83 2.83 2.46 2.51 2.24 3.78 2.13 3.11 (2.98) 2.05 (2.05) 2.92 (2.78) 2.02 (1.94) 1.95 1.86 3.03 (2.94) 1.88 (1.86) 2.88 (2.82) 1.87 (1.84) Fast samplers & distillation for diffusion models DDIM (Song et al., 2020) DPM solver (Lu et al., 2022) DEIS (Zhang & Chen, 2022) DFNO (LPIPS) (Zheng et al., 2022) TRACT (Berthelot et al., 2023) BOOT (Gu et al., 2023) Diff-Instruct (Luo et al., 2023) PD (Salimans & Ho, 2022) CTM (+GAN + CRJ) (Kim et al., 2023) SID (α 1.0) (Zhou et al., 2024) PD (LPIPS) (Song et al., 2023) CD (LPIPS) (Song et al., 2023) Direct Generation RIN (Jabri et al., 2022) DDPM (Ho et al., 2020) iDDPM (Nichol & Dhariwal, 2021) ADM (Dhariwal & Nichol, 2021) EDM (Karras et al., 2022) EDM (Heun) (Karras et al., 2022) BigGAN-deep (Brock et al., 2019) Consistency Training/Tuning CT (LPIPS) (Song et al., 2023) iCT (Song & Dhariwal, 2023) iCT-deep (Song & Dhariwal, 2023) MCM (CT) (Heek et al., 2024) ECT-S (Geng et al., 2024) ECT-M (Geng et al., 2024) ECT-XL (Geng et al., 2024) SCT-S SCT-M SCT-M 50 10 10 20 10 20 1 1 2 1 1 1 2 4 1 1 1 2 4 1 2 3 1000 250 250 250 511 79 1 1 2 1 2 1 2 1 2 4 1 2 1 2 1 2 1 2 4 1 2 4 1 2 13.7 18.3 7.93 3.42 6.65 3.10 7.83 7.43 4.97 16.3 5.57 15.39 8.95 6.77 1.92 2.03 7.88 5.74 4.92 6.20 4.70 4. 1.23 11.0 2.92 2.07 1.36 2.44 4.06 13.0 11.1 4.02 3.20 3.25 2.77 7.2 2.7 1.8 5.51 3.18 3.67 2.35 3.35 1.96 5.10 (4.59) 3.05 (2.98) 2.51 (2.43) 3.30 (3.06) 2.13 (2.09) 1.83 (1.78) 2.42 (2.23) 1.55 (1.47) Results for existing methods are taken from previous papers. Results of SCT on CIFAR-10 without are trained with batch size 128 for 200k iterations. Results of SCT on CIFAR-10 with are trained with batch size 512 for 300k iterations. Results of SCT on ImageNet-64 without are trained with batch size 128 for 100k iterations. Results of SCT on ImageNet-64 with are trained with batch size 1024 for 100k iterations. The metrics inside the parentheses were obtained using CFG. CTM applies classifier rejection sampling (CRJ) for better FID, which needs to generate more samples than other methods. CFG strength as 1.2 and the sub-optimal version as the ema weight with half training iterations by default. We investigate the influence of the two factors on SCT-S models on ImageNet. As illustrated in Fig. 6, an appropriate CFG setting can significantly enhance the quality of generation. Edge-skipping Multistep Sampling. To demonstrate the effectiveness of our method, we record the 4-step FID curve at various training stages, utilizing different η values for edge-skipping multistep inference. We find that smaller η at the beginning of training yields superior performance. As training progresses, the models estimates of multi-stage results become increasingly accurate, and larger η values gradually enhance performance. However, as previously analyzed, the multistep model struggles to achieve perfect multistep training, leading to better overall performance for η 0.9 compared to η 1.0 (the default method)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose Stable Consistency Tuning (SCT), novel approach that unifies and improves consistency models. By addressing the challenges in training variance and discretization errors, SCT achieves faster convergence and offers insights for further improvements. Our experiments demonstrate state-of-the-art 1-step and few-step generative performance on both CIFAR-10 and ImageNet-64ˆ64, offering new perspective for future studies on consistency models."
        },
        {
            "title": "REFERENCES",
            "content": "Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-tovideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. 1 David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023. 10 Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 2, 4 Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023. 1 Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. 1 Ricky TQ Chen and Yaron Lipman. Riemannian flow matching on general geometries. arXiv preprint arXiv:2302.03660, 2023. 1 Ricky TQ Chen, Jens Behrmann, David Duvenaud, and Jorn-Henrik Jacobsen. Residual flows for invertible generative modeling. In Advances in Neural Information Processing Systems, pp. 99169926, 2019. 10 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. 8 Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:87808794, 2021. 1, 9, 10 Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 4 Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024. 1 Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and J. Zico Kolter. Consistency models made easy, 2024. URL https://arxiv.org/abs/2406.14548. 1, 6, 7, 8, 10 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. BOOT: Datafree distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference tz&u Generative Modeling, 2023. 10 Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. 7, 8, 10,"
        },
        {
            "title": "Preprint",
            "content": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 8 Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 1, 8, 10 Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. arXiv preprint arXiv:2301.11093, 2023. 9 Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. 10 Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33:1210412114, 2020a. 10 Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2020b. 10 Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=k7FuTOWMOc7. 1, 8, Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024a. 7, 9 Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing In Proceedings of the IEEE/CVF and improving the training dynamics of diffusion models. Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024b. 1, 8, 9 Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, and Il-Chul Moon. Refining generative process with discriminator guidance in score-based diffusion models. arXiv preprint arXiv:2211.17091, 2022. 10 Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. 10, 1 Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021a. 2 Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), NeurIPS, 2021b. URL https://openreview.net/forum?id=2LdBqxc1Yv. 1 Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in Neural Information Processing Systems 31, pp. 1021510224, 2018. 10 Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Xu, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, and Kaidi Xu. Act-diffusion: Efficient adversarial consistency training for one-step diffusion models, 2024. 1 Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models, 2024. 1 Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009."
        },
        {
            "title": "Preprint",
            "content": "Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. 1 Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 85768588, 2024. 1 Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 1 Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 10 Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 3, 8, 10, 1 Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. 10 Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. arXiv preprint arXiv:2305.18455, 2023. 10 Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Wenbing Zhu, Jiangning Zhang, Hao Chen, Mingmin Chi, and Yabiao Wang. Osv: One step is enough for high-quality image to video generation. arXiv preprint arXiv:2409.11367, 2024. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In CVPR, pp. 1429714306, 2023. 1 Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 81628171. PMLR, 2021. 10 William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pp. 41954205, October 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1 Aaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, and Jeannette Bohg. Consistency policy: Accelerated visuomotor policies via consistency distillation, 2024. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICLR, pp. 87488763. PMLR, 2021. 6 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. 1 Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. 8, 10, 1 Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In ICLR, pp. 3010530118. PMLR, 2023a. 1 Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023b."
        },
        {
            "title": "Preprint",
            "content": "Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024. 1 Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 1 Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 1, 4, Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. 1, 7, 8, 10 Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pp. 1191811930, 2019. 1, 8 Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:1243812448, 2020. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=PxTIG12RRHS. 3, 10, 1 Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. 1, 3, 5, 6, 8, 10 Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 2018. 2 Arash Vahdat and Jan Kautz. NVAE: deep hierarchical variational autoencoder. In Advances in neural information processing systems, 2020. Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34:1128711302, 2021. 10 Pascal Vincent. connection between score matching and denoising autoencoders. Neural Computation, 23:16611674, 2011. URL https://api.semanticscholar.org/CorpusID: 5560643. 2 Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency model. arXiv preprint arXiv:2405.18407, 2024a. 7, 1 Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Accelerating the animation of personalized diffusion models and adapters with decoupled consistency learning. arXiv preprint arXiv:2402.00769, 2024b. 1 Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024c. 1 Yilun Xu, Ziming Liu, Max Tegmark, and Tommi S. Jaakkola. Poisson flow generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=voV_TRqcWh. 10 Yilun Xu, Shangyuan Tong, and Tommi Jaakkola. Stable target field for reduced variance score estimation in diffusion models. arXiv preprint arXiv:2302.00670, 2023. 2, 6 Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022."
        },
        {
            "title": "Preprint",
            "content": "Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 8, 10 Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. arXiv preprint arXiv:2211.13449, 2022. 10 Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 8,"
        },
        {
            "title": "I RELATED WORKS",
            "content": "Diffusion Models. Diffusion models (Ho et al., 2020; Song et al., 2021; Karras et al., 2022) have emerged as leading foundational models in image synthesis. Recent studies have developed their theoretical foundations (Lipman et al., 2022; Chen & Lipman, 2023; Song et al., 2021; Kingma et al., 2021b) and sought to expand and improve the sampling and design space of these models (Song et al., 2020; Karras et al., 2022; Kingma et al., 2021b). Other research has explored architectural innovations for diffusion models (Dhariwal & Nichol, 2021; Peebles & Xie, 2023), while some have focused on scaling these models for text-conditioned image synthesis and various real-world applications (Shi et al., 2024; Rombach et al., 2022; Podell et al., 2023). Efforts to accelerate the sampling process include approaches at the scheduler level (Karras et al., 2022; Lu et al., 2022; Song et al., 2020) and the training level (Meng et al., 2023; Song et al., 2023), with the former often aiming to improve the approximation of the probability flow ODE (Lu et al., 2022; Song et al., 2020). The latter primarily involves distillation techniques (Meng et al., 2023; Salimans & Ho, 2022; Wang et al., 2024c) or initializing diffusion model weights for GAN training (Sauer et al., 2023b; Lin et al., 2024). Consistency Models. Consistency models are an emerging class of generative models (Song et al., 2023; Song & Dhariwal, 2023) for fast high-quality generation. It can be trained through either consistency distillation or consistency training. Advanced methods have demonstrated that consistency training can surpass diffusion model training in performance (Song & Dhariwal, 2023; Geng et al., 2024). Several studies propose different strategies for segmenting the ODE (Kim et al., 2023; Heek et al., 2024; Wang et al., 2024a), while others explore combining consistency training with GANs to enhance training efficiency (Kong et al., 2024). Additionally, the consistency model framework has been applied to video generation (Wang et al., 2024b; Mao et al., 2024), language modeling (Kou et al., 2024) and policy learning (Prasad et al., 2024)."
        },
        {
            "title": "II LIMITATIONS",
            "content": "The work is limited to traditional benchmarks with CIFAR-10 and ImageNet-64 to validate the effectiveness of unconditional generation and class-conditional generation. However, previous works, including iCT (Song & Dhariwal, 2023) and ECT (Geng et al., 2024), only validate their effectiveness on these two benchmarks. We hope future research explores consistency training/tuning at larger scales such as text-to-image generation."
        },
        {
            "title": "III QUALITATIVE RESULTS",
            "content": ""
        },
        {
            "title": "Preprint",
            "content": "Figure 7: 1-step samples from class-conditional SCT trained on CIFAR-10. Each row corresponds to different class."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: 2-step samples from class-conditional SCT trained on CIFAR-10. Each row corresponds to different class."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: 1-step samples from unconditional SCT trained on CIFAR-10."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: 2-step samples from unconditional SCT trained on CIFAR-10."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: 4-step samples from unconditional SCT trained on CIFAR-10."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: 8-step samples from unconditional SCT trained on CIFAR-10."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to different class."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: 2-step samples from class-conditional SCT trained on ImageNet-64 (FID 1.47). Each row corresponds to different class."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: 4-step samples from class-conditional SCT trained on ImageNet-64 (FID 1.78). Each row corresponds to different class."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University Pittsburgh, USA",
        "MMLab, CUHK Hong Kong SAR"
    ]
}