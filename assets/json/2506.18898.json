{
    "paper_title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
    "authors": [
        "Jiaming Han",
        "Hao Chen",
        "Yang Zhao",
        "Hanyu Wang",
        "Qi Zhao",
        "Ziyan Yang",
        "Hao He",
        "Xiangyu Yue",
        "Lu Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 9 8 8 1 . 6 0 5 2 : r Vision as Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations Jiaming Han12, Hao Chen2, Yang Zhao2, Hanyu Wang2, Qi Zhao2, Ziyan Yang2, Hao He12, Xiangyu Yue1, Lu Jiang2 1CUHK MMLab 2ByteDance Seed"
        },
        {
            "title": "Abstract",
            "content": "This paper presents multimodal framework that attempts to unify visual understanding and generation within shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using text-aligned codebook projected from large language models (LLM) vocabulary. By integrating vision and text into unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: fast autoregressive model and diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com"
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) [2, 12, 23, 40, 81] have demonstrated the ability of LLMs to handle visual understanding tasks within an autoregressive framework. true MLLM is expected not only to understand images but also to generate them, laying the foundation for perception, reasoning, and interaction with the world. For instance, MLLMs for visual understanding typically have three components: semantic visual encoder (e.g., CLIP [49]), an LLM, and vision-to-language adapter [40, 81]. With pre-aligned visual representation, LLaVA [40] efficiently aligns CLIP features to an LLMs latent space using just 0.6M image-text pairs and simple linear adapter. However, visual generation representation remains an open research problem, with several key design choices outlined below. Separate vs. Shared. Visual understanding and generation often rely on features at different levels of abstraction, leading some methods to adopt separate representations. For example, CLIP for understanding and VQVAE for generation [10, 69]. However, this separation limits unified reasoning and complicates tasks like interleaved generation or multi-turn editing. We therefore use shared representation for both tasks, which ensure understanding and generation do not conflict but instead complement each other, as both modalities are learned within single latent space. Continuous vs. Discrete. Continuous visual features preserve rich information and work well for the understanding tasks but require objectives such as regression [56] or diffusion [83] for generation, Project lead. Corresponding authors. Preprint. Under review. Figure 1: Text-to-Image Generation Results, using Tar-7B and 1024 pixel de-tokenizer. diverging from the autoregressive paradigm essential for scaling LLMs. Discrete tokens [17] align naturally with LLMs but often face quantization errors [57, 72]. We propose unifying vision and language with shared discrete representation, simplifying the modeling paradigm, improving scalability, and reducing complexity by operating in more efficient space. To address quantization errors, we propose scale-adaptive representation that uses longer sequences to minimize errors and generative de-tokenizer to enhance generation capabilities. Pixel vs. Semantic. Pixel-level tokens (e.g., VAE [17]) provide fine-grained details but are difficult to align with LLMs [57, 75, 83]. Semantic representations like CLIP [49] efficiently capture high-level semantics for interaction with LLMs but struggle to recover image details. Hybrid methods [43, 72, 82] attempt to combine both, but balancing them remains challenging. Building on the success of semantic representations in visual understanding, we extend their use to visual generation, enabling faster convergence and simplifying the unification of understanding and generation. This paper studies Text-aligned representation (Tar), fully discrete and semantic representation that attempts to unify visual understanding and generation within shared space. Central to our method is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using text-aligned codebook initialized from an LLMs vocabulary and adapted to vision through learnable projection layers. This approach enables seamless cross-modal input and output without relying on modality-specific designs, and supports advanced multimodal reasoning within unified framework. To balance efficiency and detail, we introduce Scale-Adaptive Pooling and Decoding, which lets the model adjust token length as needed: coarse-grained tokens for efficient generation and finegrained tokens for detailed understanding. For decoding, we use two complementary Generative De-Tokenizers: fast autoregressive (AR) model for discrete VAE latents, and diffusion-based model for continuous VAE latents. The AR de-tokenizer is fast and works well with discrete LLM tokens, while the diffusion de-tokenizer leverages powerful pretrained image generators [74] for high-quality outputs. Together, they provide flexible balance between speed, compatibility, and visual fidelity. Besides the common understanding (image-to-text) and generation (text-to-image) tasks, we further improve modality fusion with new pre-training tasks like image-to-image, textimage-to-image, boosting both visual understanding and generation. We summarize our key features as follows: We propose Text-Aligned Tokenizer that unifies visual understanding and generation in shared semantic, discrete space. This multimodal framework eliminates the need for modality-specific designs and allows seamless input and output across modalities through common interface. Our Scale-Adaptive Pooling and Decoding provide flexible control over visual detail for different tasks. Additionally, we introduce Generative De-Tokenizers that generate images from discrete semantic tokens using either autoregressive or diffusion-based models. We explore advanced pre-training schemes to enable both visual understanding and generation within single model, achieving strong performance on various benchmarks. 2 Figure 2: Architecture of Tar, multimodal LLM that unifies visual understanding and generation in an autoregressive paradigm. Refer to Sec. 3.3 for training and inference detail."
        },
        {
            "title": "2 Related Work",
            "content": "Unified Multimodal Large Language Models. With the development of LLMs [1, 4, 46, 62], MLLMs have attracted lot of research interest due to their strong multimodal understanding and reasoning capabilities [2, 12, 30, 40, 81]. Beyond visual understanding, several recent works [14, 20, 56, 66, 69, 75, 83] attempt to integrate both visual understanding and generation within unified MLLM. Emu2 [56] enables LLMs to generate CLIP embeddings, which are decoded into images using diffusion model. Emu3 [68] and Chameleon [57] use VQVAE [17] as both the visual encoder and decoder, allowing unified next token prediction across images and text. However, VQVAEs focus on pixel dependency limits MLLMs ability to handle both low-level image details and high-level semantics. Show-o [75] and Transfusion [83] integrate diffusion objectives into LLMs for image generation, but this design breaks the autoregressive paradigm and complicates the unification of the two tasks. Janus [10, 69] takes modular approach with separate encoders for understanding and generation, but results in distinct modalities for image understanding and generation, which can hinder tasks like multi-turn image editing and interleaved generation. VILA-U [72] and UniTok [43] train fused tokenizer using both pixel reconstruction and image-text alignment losses, but the model struggles to converge optimally for both tasks. ILLUME [66] applies vector quantization to semantic visual encoder, using discrete tokens for image generation. However, ILLUME still relies on continuous visual features for visual understanding, resulting in separate encoders for the two tasks. In contrast, we propose fully discrete, semantic and shared representation that unifies understanding and generation within single MLLM."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first propose Text-Aligned Tokenizer (TA-Tok), which converts images into text-aligned, scale-adaptive, discrete tokens in Sec. 3.1. In Sec. 3.2, we introduce Generative De-Tokenizers, leveraging powerful generative models (e.g., autoregressive [55] and diffusion models [73]) for decoding high-quality images from our text-aligned visual conditions. Built on TA-Tok and de-tokenizers, we design unified MLLM with the simple next token prediction approach. The overall architecture is shown in Fig. 2. Finally, we illustrate in Sec. 3.4 the training recipe, especially the proposed unified pretraining tasks."
        },
        {
            "title": "3.1 Text-Aligned Tokenizer",
            "content": "TA-Tok is designed to align visual representations to LLMs latent space. Below we will introduce the basic concept and architecture of TA-Tok. The overall architecture of TA-Tok is shown in Fig. 3. Vector Quantization (VQ) is technique to discretize continuous representations into finite set of tokens, thereby transforming high-dimensional vectors into set of quantized representations. Given continuous input vector zI = E(I), which is obtained from an image encoded by visual encoder E, the goal is to map it to the closest vector in codebook C. The quantization process is formulated as: zq = argmincCzI c2, where = {c1, c2, . . . , cK} and is the number of codebook entries. The aim is to map the input to the most representative vector ck in C. 3 Figure 3: Text-Aligned Tokenizer. An input image is first encoded into continuous tokens using SigLIP2 vision model [63], followed by Scale-Adaptive (SA) Pooling to adjust spatial resolution. These tokens are then discretized via Text-Aligned Codebook, which is initialized from LLM embeddings. To guide training, SA Decoding reconstructs the pre-quantized tokens and is supervised by SigLIP2 teacher model using reconstruction loss Lrec. The decoder and teacher are only used during training and discarded at inference. Once trained, the codebook can serve as visual vocabulary of the LLM. Text-Aligned Codebook. Traditional VQ codebooks are usually random initialized. To align visual and textual tokens in the latent space of LLM, we initialize the VQ codebook using the token embeddings of pretrained LLM RKD = {e1, e2, . . . , eK} and projection matrix WDD = {w1, w2 . . . , wD}. The codebook RKD is defined as: = EW = {Ew1, Ew2, . . . , EwD}, Ewd = {e1wd, e2wd, . . . , eKwd}. (1) (2) Note the LLM embeddings are always frozen and we only train the projection matrix W, which ensures each codebook entry is projected version of corresponding LLM token embedding. By grounding the visual codebook in the LLMs latent space, this design ensures that visual tokens are semantically aligned with textual tokens, facilitating unified representation across modalities. Since LLM vocabularies are often large (e.g., 150K for Qwen [77]), using the entire embedding set as codebook is computational impractical. We select the top-k most representative embeddings based on their average distance to others, ensuring broad semantic coverage with minimal redundancy. Scale-Adaptive Pooling and Decoding. Different tasks vary in their need for visual details, e.g., understanding and editing often rely on fine-grained features, while generation may benefit from coarser representations [18]. To accommodate these differences, we introduce Scale-Adaptive Pooling (SAP) and Decoding (SAD) to extract multi-granularity features. Given image features zI , we apply SAP with scale factor {1, 2, 3} to obtain zp = SAP(zI , s), allowing control over visual details based on task needs or compute budget. During decoding, we follow SigLIP2 [63] by resizing 2D positional embedding to match the input scale, enabling the ViT decoder [63] to process multi-scale latent features effectively. Architecture and Training Objective. As shown in Fig. 3, TA-Tok consists of SigLIP2 encoder, SAP, Text-Aligned Codebook, SAD and SigLIP2 teacher model. SAP is implemented via adaptive pooling, while SAD uses lightweight ViT decoder with three ViT blocks. TA-Tok is trained with combination of feature reconstruction loss Lrec and codebook losses Lcode. The reconstruction loss encourages semantic alignment between the decoded features and the SigLIP2 teacher output, defined as: Lrec = 1 zyˆzy zyˆzy , where zy and ˆzy are features from SAD and SigLIP2 teacher, respectively. The codebook losses Lcode ensures the quantized features are close to the codebook entries. We freeze the LLM token embeddings and only train the projection matrix W. The loss is defined as: Lcode = sg(C) zq2 2 + sg(zq)2 2 = sg(W) zq2 2 + EW sg(zq)2 2, (3) where sg() denotes the stop-gradient operation. During training, the SigLIP2 encoder, SAP and SAD are jointly optimized, while the SigLIP2 teacher remains frozen."
        },
        {
            "title": "3.2 Generative De-Tokenizer",
            "content": "Since TA-Tok produces only semantic tokens without image generation capability, we introduce Generative DeTokenizer to decode high-quality images from its quantized outputs. As shown in Fig. 4, we propose two variants: autoregressive de-tokenizer (AR-DTok) and diffusion de-tokenizer (Dif-DTok), corresponding to two dominant paradigms in image generation. Autoregressive De-Tokenizer. In Fig. 4 (a), we formulate image decoding as an autoregressive generation task. Let θAR be the parameters of AR-DTok, and = [y1, y2, . . . , yT ] be the image tokens (in yellow) from VQVAE encoder. The AR-DTok predicts each token yt conditioned on the semantic visual tokenszq (in blue) from TA-Tok and previous generated tokens y<t: (cid:88) L(θAR) = logp(ytzq, y<t; θAR). (4) t= Figure 4: Architecture of Generative De-Tokenizer Variants. Diffusion De-Tokenizer. Fig. 4 (b) illustrates the second variant, where the quantized tokens zq serve as conditioning input to diffusion model vis cross attention, similar to text conditioning in traditional diffusion models [51, 53]. Let be the diffusion model parameterized by θdif , which denoised noised latent yt to predict the original clean latent y0. The training objective is: L(θdif ) = Et[F (yt, zq; θdif )2]. In practice, we reuse most parameters of pretrained diffusion model, simply replacing the original text condition with TA-Toks visual tokens, enabling high-fidelity image synthesis with minimal adaption. (5) Discussion. AR-DTok and Dif-DTok offer complementary strengths. AR-DTok aligns naturally with our discrete condition zq, enabling unified autoregressive modeling framework. It also benefits from faster inference due to its sequential decoding. In contrast, Dif-DTok leverages powerful pretrained diffusion models, allowing quick adaptation to new conditions like zq. While it is more computational intensive at inference time, it requires less training data and remains competitive with AR-DTok in overall performance. Its strong generation prior also makes it particular useful in tasks involving complex scenes or where high visual fidelity is desired."
        },
        {
            "title": "3.3 Unified Multimodal Modeling",
            "content": "Built on TA-Tok and Generative De-Tokenizers, we propose unified MLLM, Tar (Text-aligned representation), with simple autoregressive objective and eliminating the need for modality-specific designs. The architecture is shown in Fig. 2. Visual Embedding Initialization. We represent both text and images as discrete tokens in shared vocabulary by expanding the LLMs text embedding matrix RM with visual token set RKD. Rather than randomly initializing C, we use our Text-Aligned Codebook = EW (with WDD) as the visual embeddings. Since and share the same dimensionality, we can simply set: {E, C} = {E, EW}. This operation eliminates any extra embedding alignment stage. The unified embedding enables the LLM to natively process and generate both modalities without additional connectors or decoding heads. Training. We train Tar with the standard Cross-Entropy loss over mixed sequence of text and visual tokens, Given target sequence = [u1, u2, . . . , uN ] (ui may be text or visual token) and model parameter θ, the loss is: LCE = (cid:80)N Inference. At inference time, Tar can either take tokens from both modalities as input or generate them. (a) Visual understanding: Feed TA-Toks visual tokens zq and any text prompt into LLM, and generate text tokens for image captioning, visual question answering, etc. (b) Visual generation: Provide text prompt and autoregressively sample sequence of visual tokens, then pass these tokens to de-tokenizer to decoding the final image. i=1 log(uiu<i; θ)."
        },
        {
            "title": "3.4 Training Recipe",
            "content": "Data Curation. Our training data consists of image, text and multimodal datasets. Since opensourced datasets for image, text and image-to-text tasks are widely available [13, 52, 59], our focus is on curating high-quality data for image generation. The pipeline includes: (1) Image caption. We use Qwen2.5-VL [3] to generate rich, detailed captions for general image datasets [13, 15, 50]. (2) Synthetic Image generation. We adopt FLUX [27] to generate high quality images based on real user prompts [54, 65] and image captions from Step 1, which yield diverse, prompt-aligned content. In total, we curate dataset of 23M high-quality text-image pairs for training. Tokenizer and De-Tokenizer Training. TA-Tok is trained on 100M raw and 100M aesthetic-filtered images from LAION [52], balancing its capability in visual understanding and generation. For AR-DTok, due to the lack of pretrained models, we train from scratch at 256px resolution and finetune to 512px and 1024px using 50M aesthetic images from LAION [52] and 23M synthetic images. For Dif-DTok, we initialize from pretrained SANA-0.6B [73], allowing direct finetuning at 512px resolution on the 23M synthetic dataset. Unified MLLM Pretraining. Our LLM is trained on diverse mix of data types, including standard image-to-text (IT), text-to-image (TI) and text-only (TT) tasks. To further bridge the gap between visual understanding and generation, we introduce two additional task type: text-image-totext (TII) and image-to-image (II). For II, we use FLUX to generate two images from the same prompt. For TII, we let Qwen [77] to split prompt into text with an image placeholder and reference caption. For example, \"A dog running on the grass\" becomes: \"A dog running on <image>\" and \"the grass\". The goal is to generate an image based on the input image (II) or image and text (TII), which encourage deeper multimodal integration and empirically, we observe that they accelerate convergence and improve alignment between understanding and generation. Supervised MLLM Finetuning. For visual understanding, we use open-source instruction tuning datasets from LLaVA-v1.5 [38] and LLaVA-Next [39]. For visual generation, we filter high-quality subset from our pretraining datasets with CLIP score >0.25. Additional, we collect few humanpreferenced and task-aligned examples using advanced generation models [48, 74]. For more details on the training datasets, please refer to Appendix Sec. D."
        },
        {
            "title": "4.1 Experiment Details",
            "content": "For TA-Tok, we use siglip2-so400m-patch14-384 [63] as the visual encoder and three-layers ViT [63] as the decoder. We select 65536 tokens from Qwen2.5 [77] as LLM embeddings in TA Codebook. An 384384 image is encoded into {729, 169, 81} tokens at scale {1, 2, 3}. For ARDTok, we adopt the LLaMA architecture [62] implemented in Llamagen [55]. The autoregressive model is trained from scratch. The discrete VAE for image decoding is pretrained by Llamagen. For Dif-DTok, we use pretrained SANA-0.6B [73] and only finetune the cross attention and condition embedding layers. For MLLM, we adopt Qwen2.5-Instruct [77] as the backbone LLM. The LLM is fully finetuned at both pretraining and finetuning stages. For training, we random select scale from {1, 2, 3}. For inference, we set it to 1 if not specified. See Appendix Sec. for more detail."
        },
        {
            "title": "4.2 Main Results",
            "content": "Visual Understanding. As shown in Tab. 1, Tar model demonstrates strong visual understanding performance across broad range of benchmarks. Our 1.5B model surpasses most understanding only models and unified models at 1.5B/7B scale. Our 7B model matches the performance of Janus-Pro-7B, state-of-the-art model with continuous visual tokens. These results confirm that unified modeling framework using fully discrete tokens, when coupled with strong text-aligned representation, can match and even surpass specialized continuous-token models in visual understanding. Visual Generation. In Tab. 2, Tar achieves strong performance on both GenEval [22] and DPG Bench [24]. On GenEval, it reaches 0.76/0.84 overall score, surpassing all unified models. On DPG Bench, Tar-1.5B achieves 82.96 score, outperforming Janus-Pro-1B and even approaching Janus-Pro7B. To fully leverage Tars multimodal reasoning ability, we propose Self Reflect strategy, which allows the model to assess image-prompt alignment using its own visual understanding capabilities, 6 Table 1: Results on Visual Understanding Benchmarks, including POPE [33], MME [19], MMB [41], SEED [28], GQA [25] and MMMU [78]. Token: Token type, including Continuous (C), Discrete (D), Semantic (S), Pixel (P) and Hybrid (H). Model # LLM Token POPE MME-P MME-C MMB SEED GQA MMMU Understanding Only Model LLaVA-Phi [75] MobileVLM-V2 [11] DeepSeekVL [42] MiniGemini [32] LLaVA-v1.5 [38] Qwen-VL-Chat [2] Emu3-Chat [68] 1.3B 1.4B 1.3B 2B 7B 7B 8B Unified Model Show-o [75] Harmon [71] Janus [69] Janus-Pro [10] D-Dit [35] Tar (Ours) ILLUME [66] Chameleon [57] LWM [37] Liquid[70] UniTok [43] VILA-U [72] Janus-Pro [10] MetaMorph [61] Tar (Ours) 1.3B 1.5B 1.5B 1.5B 2.0B 1.5B 7B 7B 7B 7B 7B 7B 7B 8B 7B C,S C,S C,S C,S C,S C,S D,P D,P C,H C,S C,S C,P D,S C,S D,P D,P D,P D,H D,H C,S C,S D,S 84.1 84.3 88.3 83.9 85.9 - 85.2 80.0 87.6 87.0 86.2 84.0 88.4 88.5 - 75.2 81.1 83.2 85.8 87.4 - 87.8 1128 1303 1307 1341 1511 1488 1244 1097 1155 1338 1444 1125 1390 1445 - - 1119 1448 1402 1567 - - - - - - - - 248 321 222 268 - 342 - - - - - - 260 - 355 - 57.7 64.6 59.8 64.3 60.6 58.5 - 65.5 69.4 75.5 - 65.6 65.1 - - - - - 79.2 75.2 74.4 - - - - 58.6 58.2 68.2 - 67.1 63.7 68.3 - 70.4 72.9 - - - 59.0 72.1 71.8 73.0 56.5 59.3 59.3 59.9 62.0 57.5 60.3 58.0 58.9 59.1 59.3 59.2 61.1 - - 44.8 58.4 61.1 60.8 62.0 - 61.3 30.7 - 33.8 - 35.4 - 31.6 26.7 38.9 30.5 36.3 - 36.0 38.2 22.4 - - - - 41.0 41.8 39.0 Table 2: Results on Visual Generation Benchmarks. : We use an AR-DTok with 256px resolution. Due to space limit, we omit some metrics and put full results to Appendix Sec. G. Method Two Obj. Counting Color Attri. Overall Entity Attribute Relation Overall GenEval [22] DPG Bench [24] Generation Only Model Emu3-Gen [68] SDXL [47] Playground v2.5 [29] Hunyuan DiT [34] PixArt-Σ [8] DALLE3 [36] SD3-Medium [16] SANA-1.5 [74] Unified Model Chameleon-7B [57] LWM-7B [37] SEED-X-13B [21] Show-o-1.3B [75] Transfusion-7B [83] D-DiT-2B [35] ILLUME-7B [66] Janus-1.3B [69] Janus-Pro-1B [10] Harmon-1.5B [71] Janus-Pro-7B [10] Tar-1.5B (Ours) w/ Self Reflect Tar-7B (Ours) w/ Self Reflect 0.71 0.74 - - - 0.87 0.94 0.93 - 0.41 0.58 0.52 - 0.80 0.86 0.68 0.82 0.86 0.89 0.91 0.92 0.92 0.93 0.34 0.39 - - - 0.47 0.72 0.86 - 0.46 0.26 0.49 - 0.54 0.45 0.30 0.51 0.57 0.59 0.76 0.77 0.83 0.86 0.21 0.23 - - - 0.45 0.60 0.65 - 0.15 0.14 0.28 - 0.50 0.28 0.42 0.56 0.48 0.66 0.51 0.55 0.65 0. 0.54 0.55 - - - 0.67 0.74 0.81 0.39 0.47 0.49 0.53 0.63 0.65 0.61 0.61 0.73 0.76 0.80 0.76 0.78 0.84 0.85 86.68 82.43 82.59 80.59 82.89 89.61 91.01 - - - - - - - - 87.38 88.63 - 88.90 89.35 88.48 88.62 88.60 86.84 80.91 81.20 88.01 88.94 88.39 88.83 - - - - - - - - 87.70 88.17 - 89.40 86.91 87.83 88.05 88. 90.22 86.76 84.08 74.36 86.59 90.58 80.70 - - - - - - - - 85.46 88.98 - 89.32 93.50 93.38 93.98 93.59 80.60 74.65 75.47 78.87 80.54 83.50 84.08 84.70 - - - - - - - 79.68 82.63 - 84.19 82.96 84.10 84.19 84.65 leading to further performance improvement. The image generation results are visualized in Fig. 1 and Appendix Fig. 8."
        },
        {
            "title": "4.3 Comparisons with Other Visual Representations",
            "content": "In this section, we compare our text-aligned representation (TA-Tok) with other visual representations for unified visual understanding and generation. We considering the following recent methods [43, 68, 69]: (a) VQVAE: full pixel-level representation. We use pretrained VQVAE from Llamagen [55], which transforms images into discrete tokens for multimodal modeling. (b) Janus: Use separate 7 Figure 5: Comparisons of Visual Representations on Generation and Understanding Tasks. Left: Generation performance evaluated by DPG Score [24]. Right: Understanding performance measured by the harmonic mean over benchmarks [19, 25, 28, 33]. Figure 6: Qualitative Comparison of Different Representations (Left) and De-Tokenizers (Right). encoders for understanding (SigLIP2) and generation (VQVAE). Note our implementation follows this approach but differs from the original one [69]. (c) Hybrid: hybrid model that maintains both pixel and semantic representations. We follow UniTok [43] to train hybrid tokenizer on 100M image-text pairs [52], using both pixel reconstruction loss and image-text alignment loss. For training MLLMs with these representations, we sample subset of our training data for controlled experiments: 10M T2I data, 10M I2T data and 5M text-only data. All models are trained with the same configuration and tested on visual understanding [19, 25, 28, 33] and generation tasks [24]. TA-Tok Outperforms in Visual Generation. As shown in the left of Fig. 5, TA-Tok excels in visual generation. It achieves the highest performance across all data scales, with VQVAE, Janus and TA-Tok showing similar convergence curves. Although Hybrid starts with higher performance, it does not scale effectively with increasing data. Notably, Janus underperforms VQVAE, likely due to conflicts between the understanding and generation tasks. Besides, we found TA-Tok generates high-fidelity images (see appendix), while models using pixel representations are struggle with image details, suggesting semantic representation is more suitable for LLM-based image generation. TA-Tok Matches Continuous-Semantic Representations in Visual Understanding. In visual understanding tasks (right of Fig. 5), Janus performs slightly better due to its continuous semantic encoder. However, TA-Tok is close behind with scores of 93 vs. 94 at 10M data. Hybrid, despite joint training with pixel and semantic losses, performs similar to VQVAE, not achieving the expected performance due to its bias to pixel representation. Overall, TA-Tok achieves the best balance between visual generation and understanding tasks, outperforms other methods in both domains."
        },
        {
            "title": "4.4 Ablation Experiments",
            "content": "In this section, we conduct ablation experiments on our key designs and demonstrate the effectiveness of the proposed method. We use the same subset of training data in Sec. 4.3 if not specified. Text-Aligned Codebook for Better MLLM Initialization. One benefit of our TA-Tok is the learned codebook can be directly transferred to LLM for image embedding initialization, as discussed in Sec. 3.3. In Tab. 3, we show that initializing the LLM embeddings with TA Codebook leads to better performance on both understanding and generation tasks compared to random initialization. Another common approach pre-align, used in previous works [10, 69], involves separate alignment stage 8 Table 3: MLLM Embedding Initialization. Und: Harmonic mean of understanding benchmarks [19, 25, 28, 33]. Gen: DPG score [24]. emb. init random pre-align TA-Tok pre-align Data Und Gen 25M 91.1 50M+25M 89.8 25M 92.9 100M+25M 91.5 69.6 70.3 70.6 72.5 Table 4: Ablation of Different De-Tokenizers. Type Size Res GenEval DPG AR 256 775M 775M 512 775M 1024 256 111M 256 22M Dif. 600M 0.76 0.79 0.79 0.75 0.74 0.77 83.0 82.7 82.6 82.2 81.7 82.4 Table 5: The Effect of Scale-Adaptive Pooling. Und: Results on understanding tasks with 10M data model. Right: Different data scale on DPG Bench. : Model after supervised finetuning. Table 6: Separate Train vs. Joint Train. We evaluate the performance of different visual representations under joint understanding & generation training or separate training. #Token 729 169 81 Und Avg 92.9 89.1 86.6 Data Scale on Generation 10M 20M 40M Avg 70.6 72.7 73.0 73.8 74.0 73.5 82.3 82.2 80.1 75.6 76.3 75.5 Model Janus VQVAE Ours Und Gen (sep. joint) (sep. joint) -0.6 (95.0 94.4) +0.3 (73.9 74.2) +0.1 (92.8 92.9) -0.1 (33.5 33.4) +8.1 (40.1 48.2) +5.3 (65.3 70.6) with additional training data. While pre-align with 50M extra data can match TA-Toks performance on generation, which further highlights the efficiency and effectiveness of our approach. Generative De-Tokenizer. Tab. 4 shows that AR-DTok performs well across resolution and scales, with 512px offering the best trade-off between quality and efficiency. Larger models (775M) performs slightly better, though smaller variants remain competitive. Dif-DTok achieves similar scores at 512px, but adapts quickly to generating high-fidelity images thanks to its pretrained diffusion backbone. However, Fig. 6 (right) shows that the visual quality of generated images varies between de-tokenizers, even when their scores are similar. Scale-Adaptive Pooling for Multi-Granularity Visual Tasks. The design of SAP makes our model can produce multi-granularity visual tokens. In Tab. 5, we show that visual understanding tasks require more tokens to capture image details. However, adding more visual tokens does not significantly improve image generation performance. On the contrary, longer visual sequences make it harder for LLMs to learn (e.g., 81 tokens are optimal at 10M training data). While longer sequences can enhance T2I performance as the training data increase, we found 169 tokens are already sufficient, which is similar to conventional T2I models [16, 74]. Shared Representation Unifies Visual Understanding and Generation. Previous works [61, 71] studied the mutual effect of both tasks. In Tab. 6, we further show that the two tasks can benefit each other as long as we use shared visual representation. For Janus-style representation, jointly training does not decrease the performance of each task under separate training. But the two tasks can benefit each other with shared representation such as VQVAE and our text-aligned representation, around 8.1% and 5.3% improvement on the generation task, respectively. Unified Multimodal Pretraining with Advanced Tasks. In the right table, we demonstrate that the proposed TI2I and I2I tasks can further improve the generation performance, thus narrowing the gap between understanding and generation. Ratio: the data ratio of T2I vs. I2I vs. TI2I. Task Ratio Und Gen baseline I2I TI2I I2I+TI2I (3:0:0) (2:1:0) (2:0:1) (4:1:1) 89.2 89.4 89.8 89.8 66.4 68.5 70.1 70."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced Tar, unified model that bridges visual understanding and generation using shared, discrete, text-aligned representation. By aligning image tokens with LLM embeddings via TATok, and supporting scale-adaptive tokenization and generative de-tokenizers, Tar achieves strong performance across both tasks without modality-specific designs. Our results show that fully discrete, semantic representation enables efficient and effective multimodal learning, moving toward true unification of vision and language."
        },
        {
            "title": "A Overview",
            "content": "Sec. B: Additional Related Work Sec. C: Additional method Sec. D: Datasets Sec. E: Additional Implement Details Sec. F: Additional Ablation Experiments Sec. G: Additional Main Results Sec. H: More Visualization Sec. I: Limitation Sec. J: Social Impact"
        },
        {
            "title": "B Additional Related Work",
            "content": "Visual Tokenization. For visual generation, both continuous and and discrete VAEs [26, 64] are popular visual tokenizers. The continuous VAE [26] is used as the default tokenizer for diffusion models [27, 51], while discrete VAE [64] is used for masked generative models [6] and autoregressive models [17, 60, 67]. For visual understanding, the best practice is employing continuous, semantic tokenizer like CLIP [49], SigLIP [80] and DINO [5]. However, it is still unclear what type of tokenizer is good for both visual understanding and generation. As discussed in Sec. 1, some works [57, 68] leverage continuous tokenizers [56, 83], while others use discrete tokenizers [57, 68, 75]. Some works use pixel-level tokenizers [57, 68], other works leverage semantic tokenizers [56, 66] or hybrid tokenizers [43, 72]. In contrast, we leverage discrete and LLM-aligned tokenizer for visual understanding and generation. The generated visual tokens are then decoded with autoregressive or diffusion de-tokenizer for high quality image generation."
        },
        {
            "title": "C Additional Method",
            "content": "C.1 Training Recipe TA-Tok Training TA-Tok is trained on both 100M raw web images and 100M aesthetic-filtered images from LAION-5B [52] to balance its ability on encoding general images for understanding and high-quality images for generation. Note we only use the images (without text) in LAION-5B. The aesthetic images are with resolution above 512px and resized to 384px. To enable Scale-Adaptive Pooling and Decoding, we random select scale from {1, 2, 3}, resulting in {729, 169, 81} tokens. Since learning longer sequence is usually harder, we set the sampling ratio of scale {1, 2, 3} to (2 : 1 : 1). The training of De-Tokenizers and MLLM also follows the same sampling ratio. De-Tokenizer Training The training of De-Tokenizers is to align low-level image representations (e.g., VAE [26] and VQVAE [64]) with TA-Toks semantic representation. For AR-DTok, we train series of models with different resolutions, ranging from 256px to 1024px. (a) 256px. Since AR-DTok is only developed from high-quality image generation, we train 256px AR-DTok on 50M aesthetic-filtered images from LAION. (b) 512px. We finetune 256px AR-DTok to 512px using 23M synthetic images (described in Sec. 3.4 Data Curation). 1024px. To demonstrate the resolution decoupling between our TA-Tok and De-Tokenizer, we further finetune AR-DTok from 512px to 1024px using 3M synthetic images of 1024px resolution from [15]. For Dif-DTok, we directly leverage pretrained text-to-image model, SANA-0.6B [73] as the starting model. We train Dif-DTok with 23M synthetic images at 512px. Notably, we observe that Dif-DTok achieves good convergence with just 5M training samples. However, we continue training on the full 23M dataset to ensure broader image converge and diversity. 10 MLLM Prompt Format Since TA-Tok and MLLM are connected with discrete tokens, we expand the vocabulary of MLLM and convert TA-Tok tokens into text that MLLM can understand. The newly added vocabularies are: <im_start>, <im_end>, <S0>, <S1>, <S2>, <I0>, <I1>, ..., <I65535>, where <im_start> and <im_end> are the starting and end tags, <S[0-2]> are the scale tokens and <I[0-65535]> are image tokens. Following Qwen2.5-Instruct [77], the prompt of one-turn conversation is formatted as: <im_start>usern{Q}<im_end><im_start>assistantn{A}<im_end>, where <im_start> and <im_end> are the starting and end tags of an instruction (Q) or response (A). To distinguish image understanding and generation, we use different prompt format for them. For image understanding, Q={text}<I1><I3><I1314>...<I520>{text}, A={text response}, where we do not add image start and end tags. Note the length of image tokens can be {729, 169, 81}, depending on the selected scale. For image generation, the prompt is: Q={text prompt}, A=<im_start><I1><I3><I1314>...<I520><im_end>, where we add image tags to the prompt. During inference, the generation prompt is: <im_start>usern{text prompt}<im_end><im_start>assistantn<im_start>. The image start tag <im_start> at the end of prompt encourages the model to generate image tokens instead of text tokens."
        },
        {
            "title": "D Datasets",
            "content": "D.1 Dataset Curation Details The common text-to-image data collection process include complex pipeline, e.g., aesthetic filtering, watermark/logo detection and image-text alignment filtering. Instead, training with synthetic datasets is more efficient solution [10]. We only need to focus on the quality of text prompts. The state-ofthe-art generation models can provide high-quality images of given prompt. Therefore, we develop simple and efficient data curation pipeline: Image Caption To encourage the diversity of prompts, we use an open-source MLLM, Qwen2.5VL-7B[3] to generate long and detailed captions for multiple datasets, including image classification datasets ImageNet-1K [13] nad ImageNet-21K [50], and photo dataset Megalith-10M [15]. The caption prompt to Qwen2.5-VL is: Here are some text-to-image prompts: Example 1: {long and detailed prompt 1} Example 2: {prompt 2} Example 3: {prompt 3} Now generate prompt for this image. Do not copy the above content. their prompt style. Do not output anything else. The image is: {image}"
        },
        {
            "title": "Just follow",
            "content": "This step is similar to re-caption, but differently, we only need the text prompts generated from these images, no matter the quality of these images. 11 Synthetic Image Generation In the above step, we have obtained millions of text prompts of real world images. However, users may not input such long prompts of real world objects to the model, they often provides short, simple, and creative prompts made up of few words, e.g., \"boy, dog, the grass, happy play, 4K\". Therefore, we also collect user prompts datasets: JourneyDB [54] and Midjourney-Prompts [65]. Although JourneyDB is text-to-image dataset, we argue that its images are generated by very early generation models like MidJourney * before May, 2023. Here we adopt state-of-the-art and fast model, FLUX.1-schnell [27] as the image generator. With only 4 sampling steps and 512px resolution, we can quickly generate lot of images using prompts from Qwen2.5-VL and user prompts. Finally, we collect 23M high-quality synthetic dataset. D.2 Unified MLLM Pretraining Data Except the traditional pretraining tasks, i.e, , image-to-text (IT), text-to-image (TI), text-only (II), we also propose two new tasks: image-to-image (II) and text-image-to-image (TII). Image-to-Image Given text prompt, we ask FLUX to generate two images with different seed. This task requires the MLLM to understand the input image first and generate similar one with the same semantic. The prompt is: Q=Generate an image similar to {image 1}, A={image 2}. Image-Text-to-Image Given text prompt, we first split it into two parts using an open-source LLM Qwen2.5-7B [77]: For given prompt, you need to replace part of its content with placeholder <image>. For example: INPUT: In cinematic and realistic portrayal of 1920s girls at college, we see them studying in university with dark academic atmosphere, where haunting and creepy ghost story unfolds. OUTPUT EXAMPLE 1: {\"prompt\": \"In cinematic and realistic portrayal of <image>, we see them studying in university with dark academic atmosphere, where haunting and creepy ghost story unfolds.\", \"<image>\": \"1920s girls at college.\"} OUTPUT EXAMPLE 2: {omitted} Now will give you another prompt, you need to output one sample for each prompt. Just output the result in json format, and do not output anything else. The task is to generate an image based on multimodal prompt. Therefore, to generate the target image, the model must understand both the text part and the image part of the multimodal prompt. At the training time, we organize the data as: Q={text part 1}{image 1}{text part 2}, A={image 2}. Using the same data source as Sec. D.1, we finally curate 15M dataset for II and TII tasks. Relations to Other Tasks Our proposed tasks have similar format of tasks like image editing and subject-driven image generation. However, their goals are different. For image editing, the goal is to modify part of an input image, following text instruction. This task focuses on low-level image detail. For subject-driven image generation, its goal is to compose the objects of the input image into scene, emphasizing visual consistency and preserving pixel-level details. In contrast, our task is to mitigating the modality gap between visual understanding and generation. Rather than persevering exact pixel information, we focus on capturing and conveying the semantic content of the input image. *https://www.midjourney.com 12 Table 7: Dataset Summary. Model Stage Data TA-Tok - LAION (100M) [52] , LAION Aes (100M) [52] AR-DTok 256px 512px LAION Aes (50M) [52] Gen23M [13, 15, 50, 54, 65] 1024px Gen3M [15] Dif-DTok 512px Gen23M [13, 15, 50, 54, 65] MLLM LLaVA-Recap-CC12M [39], DataComp (20M) [31] Pretrain Gen23M [13, 15, 50, 54, 65] Gen15M [13, 15, 50, 54] Magpie (4M) [76], WebInstruct (12M) [79] OpenHermes [58], GenQA [7], Infinity-Instruct [45] LLaVA-v1.5 (665K) [38], LLaVA-Recap-558K [39], LLaVA-Recap-118K [39], Self-Reflect-340K SFT Gen-SFT-1.6M Magpie (1M) [76] Table 8: Training Parameters. Type Image Image Image Image Image I2T T2I I2I, TI2I Size 200M 50M 23M 3M 23M 32M 23M 15M Text 28.5M I2T T2I Text 1.6M 1.6M 1M config learning rate lr schedule optimizer optimizer params weight decay input resolution warmup epochs epochs total samples total batch size codebook loss reconstruction loss gradient clip token drop prob data ratio TA-Tok - 2e-4 cosine AdamW 256px 4e-4 AR-DTok 512px 1e-4 cosine AdamW 1024px 1eβ1=0.9,β2=0.99 1e-4 384 0.04 1 200M 512 1.0 1.0 1.0 - - 1024 β1=0.9,β2=0.95 0.05 512 0.04 1 23M 96 - - 1.0 0.1 - 256 50M 768 - - - 3M 48 - - - Dif-DTok 512px MLLM Prertain SFT 1e-4 constant CAME β1=0.9,β2=0.999 β3=0.9999 0.0 512 0.01 1 23M 48 - - 0.1 0.1 - 5e-5 consine AdamW β1=0.9,β2=0.999 0.0 384 0.03 1 100M 1024 - - - 1. 4M 256 - - - 2(und):2(gen):1(text) D.3 Dataset Summary We summary all the training data in Tab. 7."
        },
        {
            "title": "E Additional Implement Details",
            "content": "We list the training hyper-parameters in Tab."
        },
        {
            "title": "F Additional Ablation Experiments",
            "content": "Ablation of Text-Aligned Codebook As shown in Tab. 9, we compared different settings of TA Codebook. Tab. 9 (a) is conventional setting, with random initialized, low dimension codebook; Tab. 9 (b) increases the codebook dimension to 1536, but fails to converge during training; Tab. 9 (c) is our LLM embedding aligned codebook with large vocabulary and high dimension. The results indicate that our TA Codebook (c) outperforms the conventional codebook (a) by large margin in understanding tasks and is comparable on the generation task. 13 Table 9: Ablation of Different Codebook Settings. setting init size dim GQA MME POPE DPG (a) (b) (c) random 32768 random 32768 65536 LLM 16 1536 1536 59.4 61. 1261 86.7 not converge 87.4 1428 70.9 70.6 Self Reflect Since unified MLLM can handle both image understanding and generation tasks, we leverage its understanding ability to assess the generation quality, i.e., image-prompt alignment, which is relative simple task for modern MLLMs. Therefore, we build Self-Reflect-340K, dataset to judge image-prompt alignment. We collect set of prompts in the style of GenEval and DPG Bench, and query Qwen2.5-VL-7B to generate judgments. In Tab. 10, the model trained with Self Reflect show consistent improvements on the visual generation task. Notably, weaker models benefit more: the relative improvements are 0.04 for 10K-step model, but 0.016 for 60K-step model. Table 10: Ablation of Self Reflect. The evaluation metric is GenEval overall score."
        },
        {
            "title": "Train step",
            "content": "baseline w/ Self Reflect 10K 0.717 0.757 20K 0.747 0.779 30K 0.753 0.788 40K 0.757 0.784 50K 0.759 0.785 60K 0.764 0.780 +4.0% +3.2% +3.5% +2.7% +2.6% +1.6% Table 11: Performance on ImageNet 256256 Benchmark. FID: Frechet inception distance. IS: inception score. cfg: classifier-free guidance. Model #Param cfg FID IS Precision Recall Llamagen-Tok Llamagen-XL Llamagen-XXL TA-Tok+AR-DTok 775M 1.4B 1.2B 1.75 1.75 10. 2.19 (rFID) 2.62 2.34 2.60 244.08 253.90 208.46 0.80 0.80 0.76 0.57 0.59 0.64 Image AutoEncoding Performance of TA-Tok Our TA-Tok and de-tokenizers can be viewed as an image autoencoding pipeline, where an input an image is encoded and then reconstructed. Since our de-tokenizers are based on generative models (AR or diffusion), one concern is whether they can faithfully recover the original image. In Tab. 11, we compare our method against LLamagens tokenizer (Llamagen-Tok), Llamagen-XL/XXL on the ImageNet 256256 benchmark. Note that we take TA-Toks output as condition and Llamagen-XL/XXL are conditioned on class labels, so the results are not directly aligned. Anyway, this serves as comparison to assess the autoencoding capability of our method. Our method achieves comparable FID compared with Llamagen-XL, demonstrating effective reconstruction quality. We also observe that our model requires higher classifier-free guidance (10.0) compared with Llamagens 1.5, indicating stronger reliance of condition. Compositional Generation Emerges from Unified Pretraining Tasks In Sec. 3.4, we propose unified pretraining with two new tasks: image-to-image (II) and text-image-to-image (TII), which encourages multimodal prompt conditioned generation. As shown in Fig. 7, Tar demonstrates emergent subject-driven generation and reference-based style transfer."
        },
        {
            "title": "G Additional Main Results",
            "content": "We list the full visual generation results in Tab. 12 and Tab. 13. 14 Figure 7: Emergent Compositional Generation Ability. The first row denotes subject-driven generation. The middle and bottom rows are reference-based style transfer. For each example, the left image is <image>, the right image is the generated image."
        },
        {
            "title": "H More Visualization",
            "content": "We give more visualization of our model on the text-to-image generation task, shown in Fig. 8. Notably, our model can follow Chinese prompts even it is not trained with Chinese prompt dataset, by leveraging the multilingual ability of the base LLM, Qwen2.5 [77]."
        },
        {
            "title": "I Limitation and Future Work",
            "content": "Our method has several limitations. First, the vector quantization in TA-Tok introduces quantization errors. Despite mitigating this with larger codebook and extended training, some information loss remainsparticularly in tasks requiring fine-grained understanding, such as OCR. This could be improved by adopting longer visual sequences and incorporating techniques like Token-Shuffle [44]. Second, while our method excels at generating diverse images, it underperforms in accurately reconstructing the input image compared to traditional tokenizers [64]. This limitation stems from using generative models as de-tokenizers. potential solution is to train the de-tokenizer as an image super-resolution model to better preserve local consistency between the input and reconstructed images."
        },
        {
            "title": "J Societal Impact",
            "content": "This work advances unified MLLMs by introducing shared, discrete semantic representation that enhances both visual understanding and generation, enabling efficient language-vision integration for applications like assistive tools, creative content, and education. However, high-quality image generation also poses risks, including potential misuse for misinformation or manipulation. While our model is not identity-specific, downstream use should include safeguards such as watermarking and prompt filtering. We advocate for ethical use, emphasizing fairness, robustness, and transparency. 15 Figure 8: Visual Generation Results. We use Tar-7B and 1024px AR-DTok to generate these images. Most prompts are from the web, and few prompts are from [48, 74]. Zoom in for better view. 16 Table 12: Visual Generation Results on GenEval [22]. Method Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Generation Only Model SDv1.5 [51] PixArt-α [9] SDv2.1 [51] Emu3-Gen [68] SDXL [47] DALLE3 [36] SD3-Medium [16] SANA-1.5 [74] Unified Model Chameleon-7B [57] LWM-7B [37] SEED-X-13B [21] Show-o-1.3B [75] Transfusion-7B [83] D-DiT-2B [35] ILLUME-7B [66] Janus-1.3B [69] Janus-Pro-1B [10] Harmon-1.5B [71] Janus-Pro-7B [10] Tar-1.5B (Ours) w/ Self Reflect Tar-7B (Ours) w/ Self Reflect 0.97 0.98 0.98 0.98 0.98 0.96 0.99 0. - 0.93 0.97 0.95 - 0.97 0.99 0.97 0.98 0.99 0.99 0.99 0.99 0.98 0.98 0.38 0.50 0.51 0.71 0.74 0.87 0.94 0.93 - 0.41 0.58 0.52 - 0.80 0.86 0.68 0.82 0.86 0.89 0.91 0.92 0.92 0.93 0.35 0.44 0.44 0.34 0.39 0.47 0.72 0.86 - 0.46 0.26 0.49 - 0.54 0.45 0.30 0.51 0.66 0.59 0.76 0.77 0.83 0.86 0.76 0.80 0.85 0.81 0.85 0.83 0.89 0. - 0.79 0.80 0.82 - 0.76 0.71 0.84 0.89 0.85 0.90 0.81 0.81 0.85 0.85 0.04 0.08 0.07 0.17 0.15 0.43 0.33 0.59 - 0.09 0.19 0.11 - 0.32 0.39 0.46 0.65 0.74 0.79 0.57 0.62 0.80 0.80 0.06 0.07 0.17 0.21 0.23 0.45 0.60 0.65 - 0.15 0.14 0.28 - 0.50 0.28 0.42 0.56 0.48 0.66 0.51 0.55 0.65 0.70 0.43 0.48 0.50 0.54 0.55 0.67 0.74 0. 0.39 0.47 0.49 0.53 0.63 0.65 0.61 0.61 0.73 0.76 0.80 0.76 0.78 0.84 0.85 Table 13: Visual Generation Results on DPG Bench [24]. Method Global Entity Attribute Relation Other Overall Generation Only Model SDv1.5 [51] PixArt-α [9] Emu3-Gen [68] SDXL [47] Playground v2.5 [29] Hunyuan DiT [34] PixArt-Σ [8] DALLE3 [36] SD3-Medium [16] SANA-1.5 [74] Unified Model Janus-1.3B [69] Janus-Pro-1B [10] Janus-Pro-7B [10] Tar-1.5B (Ours) w/ Self Reflect Tar-7B (Ours) w/ Self Reflect 74.63 74.97 85.21 83.27 83.06 84.59 86.89 90.97 87.90 - 82.33 87.58 86.90 83.59 84.17 83.98 84.09 74.23 79.32 86.68 82.43 82.59 80.59 82.89 89.61 91.01 - 87.38 88.63 88.90 89.35 88.48 88.62 88.60 75.39 78.60 86.84 80.91 81.20 88.01 88.94 88.39 88.83 - 87.70 88.17 89.40 86.91 87.83 88.05 88. 73.49 82.57 90.22 86.76 84.08 74.36 86.59 90.58 80.70 - 85.46 88.98 89.32 93.50 93.38 93.98 93.59 67.81 76.96 83.15 80.41 83.50 86.41 87.68 89.83 88.68 - 86.41 88.30 89.48 80.80 84.07 84.86 85.15 63.18 71.11 80.60 74.65 75.47 78.87 80.54 83.50 84.08 84.70 79.68 82.63 84.19 82.96 84.10 84.19 84."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. Technical report, Alibaba Group, 2023. https://arxiv.org/ abs/2303.08774. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 17 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc., 2020. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. [6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [7] Jiuhai Chen, Rifaa Qadri, Yuxin Wen, Neel Jain, John Kirchenbauer, Tianyi Zhou, and Tom Goldstein. Genqa: Generating millions of instructions from handful of prompts. arXiv preprint arXiv:2406.10323, 2024. [8] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. [9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [11] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [14] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. [15] DrawThingsAI. Megalith-10m dataset. https://huggingface.co/datasets/drawthingsai/ megalith-10m, 2024. Accessed: 2025-05-09. [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [18] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, and Hongsheng Li. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. [19] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. [20] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. 18 [21] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [22] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [23] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [24] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. [25] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [26] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [27] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [28] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [29] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [31] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024. [32] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [34] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [35] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024. [36] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. arXiv preprint arXiv:2404.01291, 2024. [37] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. [38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [39] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 19 [41] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. [42] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [43] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [44] Xu Ma, Peize Sun, Haoyu Ma, Hao Tang, Chih-Yao Ma, Jialiang Wang, Kunpeng Li, Xiaoliang Dai, Yujun Shi, Xuan Ju, et al. Token-shuffle: Towards high-resolution image generation with autoregressive models. arXiv preprint arXiv:2504.17789, 2025. [45] Beijing Academy of Artificial Intelligence (BAAI). Infinity-instruct. https://huggingface.co/ datasets/BAAI/Infinity-Instruct, 2024. Accessed: 2025-05-19. [46] OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023. [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [48] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Will Beddow, Erwann Millon, Wenhai Wang Victor Perez, Yu Qiao, Bo Zhang, Xiaohong Liu, Hongsheng Li, Chang Xu, and Peng Gao. Lumina-image 2.0: unified and efficient image generative framework, 2025. [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [50] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021. [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [52] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. [53] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. [54] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in neural information processing systems, 36:4965949678, 2023. [55] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [56] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023. [57] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024. [58] Teknium. Openhermes. https://huggingface.co/datasets/teknium/openhermes, 2023. Accessed: 2025-05-19. [59] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. 20 [60] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [61] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [62] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023. [63] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [64] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [65] Vivym. Midjourney prompts dataset. https://huggingface.co/datasets/vivym/ midjourney-prompts, 2023. Accessed: 2025-05-09. [66] Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu. Illume: Illuminating your llms to see, draw, and self-enhance. arXiv preprint arXiv:2412.06673, 2024. [67] Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, and Abhinav Shrivastava. Larp: Tokenizing videos with learned autoregressive generative prior. arXiv preprint arXiv:2410.21264, 2024. [68] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [69] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [70] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. [71] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation, 2025. [72] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [73] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformer, 2024. [74] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer, 2025. [75] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [76] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. [77] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 21 [78] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [79] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 2024. [80] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages pre-training. 1197511986, 2023. [81] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. [82] Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Krähenbühl, and De-An Huang. Qlip: Text-aligned visual tokenization unifies auto-regressive multimodal understanding and generation. arXiv preprint arXiv:2502.yyyyy, 2025. [83] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "CUHK MMLab"
    ]
}