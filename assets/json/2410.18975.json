{
    "paper_title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
    "authors": [
        "Jialu Li",
        "Yuanzhen Li",
        "Neal Wadhwa",
        "Yael Pritch",
        "David E. Jacobs",
        "Michael Rubinstein",
        "Mohit Bansal",
        "Nataniel Ruiz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 2 5 7 9 8 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "UNBOUNDED: GENERATIVE INFINITE GAME OF CHARACTER LIFE SIMULATION Jialu Li1,2 Yuanzhen Li1 Neal Wadhwa1 Yael Pritch1 David E. Jacobs1 Michael Rubinstein1 Mohit Bansal2 Nataniel Ruiz1 2The University of North Carolina at Chapel Hill 1Google generative-infinite-game.github.io Figure 1: An example of UNBOUNDED. We follow the life of Archibus, the users custom wizard character. The user can interact with the generative game using natural language, and Archibus hunger, energy and fun meters update accordingly. spontaneous and unconstrained story unfolds while the user plays, and the character can explore new environments with myriad of possible actions and unexpected interactions. The game runs in interactive speeds, refreshing every second."
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce the concept of generative infinite game, video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carses distinction between finite and infinite games (Carse, 1986), we leverage recent advances in generative AI to create UNBOUNDED: game of character life simulation that is fully encapsulated in generative models. Specifically, UNBOUNDED draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop UNBOUNDED, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches. Work done during an internship at Google"
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "In his work Finite and Infinite Games: Vision of Life as Play and Possibility (Carse, 1986), James P. Carse introduces distinction between two types of games. Carse defines finite games as those played for the purpose of winning, with boundaries, fixed rules, and definitive endpoint. In contrast, infinite games are played for the purpose of continuing the play, with no fixed boundaries and evolving rules. Traditional video games are, inherently, finite games due to the limitations of programming and computer graphics. For example, game mechanics have to be fully pre-defined in the programming language and graphics assets usually have to be pre-designed (modulo traditional procedural generation which still grapples with structural limitations). This allows for only finite, and sometimes predefined, set of actions and paths that can be taken. They also feature predefined rules, boundaries, and win conditions, which run counter to the infinite game definition. Recent advances in generative models have been impressive. We hypothesize that these developments have finally opened up the possibility of creating the first generative infinite video game - an infinite game that is also fully subsumed in generative models with no logic or graphics being controlled by other, more traditional, processes. Two advancements have made this achievable: (1) large language models (LLMs) that can encode persistent video game mechanics (e.g. interactions with the game environment or characters, character state tracking, object permanence), generate interactive stories, and produce spontaneous, and sometimes emergent, behaviors; and (2) visual generative models capable of producing high-quality images that follow prompts. In this work, we present UNBOUNDED, what we believe to be the first interactive generative infinite game, where all game behaviors and graphics are generated by AI models, transcending the constraints of hardcoded systems. UNBOUNDED draws inspiration from sandbox life simulations and digital pet games such as Little Computer People, The Sims and Tamagotchi. It incorporates elements from tabletop roleplaying games like Dungeons & Dragons, which offer unconstrained storytelling experiences that have been unattainable in video games. Some useful prior work has proposed ideas that are related to our concept of generative infinite games. Gaudl et al. (2018) proposes fluidic games that merge gameplay with game design, automated game design (Cook et al., 2016) explores automated systems that can help in making new games, and work on AI-based games (Treanor et al., 2015) proposes design techniques for games that put AI at the forefront of user experience. These concepts are different from our definition of generative infinite games since they emphasize structured exploration within predefined game parameters and tools, where the design space is extensive yet ultimately bounded. In contrast, infinite games, as we envision them, aim to continuously evolve beyond predefined structures, with generative models dynamically creating both content and mechanics in real-time, enabling an open-ended gameplay experience with no fixed endpoint or limitations on possible interactions. There is also important work on using machine learning models as components that generate parts of game. Agarwal (2024) generates novel interactions, Sun et al. (2023) generates new mechanics and items as well as some graphical assets, and there is also work on orchestration of different models for game design (Liapis et al., 2019). These works do not generate all components of the game. In contrast, in our work, all of the game mechanics, characters, environments, narrative, and graphics are fully produced by generative models. This is more similar in spirit to recent work Genie (Bruce et al., 2024) and GameNGen (Valevski et al., 2024), although in contrast to these works that mainly generate platformers with diffuse mechanics or regenerate behaviors of one pre-existing game, our work proposes an open-ended narrative experience with stable game mechanics enabled by an LLM-based game engine. UNBOUNDED offers gameplay loop centered around character simulation and open-ended interaction (Figure 2). Players can insert their characters into the game, defining their appearance and personality. The game generates world where these characters can explore environments, interact with objects, and engage in conversations. The game generates new scenarios, stories, and challenges based on the players actions and choices, creating personalized and infinite gaming experience. Some generative game examples are shown in Figure 3. Specifically, UNBOUNDED has the following capabilities: (1) Character Personalization: players can insert their characters into the game, defining their appearance and personality. (2) Game Environment Generation: UNBOUNDED generates persistent world that the characters can explore and interact. (3) Open-Ended Interaction: Players can interact with the character using natural lan-"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Example of UNBOUNDED. Based on an initial user input, UNBOUNDED sets up game simulation environments, and generates character actions in the environments. Users can interact with the character with natural language instructions, exploring the game with unlimited options. guage instructions, and there are no pre-defined rules to constraint the interaction. (4) Real-Time Generation: we pay special attention to the speed of the game and achieve 5-10x speedups over naive implementation, serving each new scene with latency of about one second. Our approach introduces technical innovations in both LLM and vision generation domains. On the language side, we developed LLM based game engine capable of maintaining consistent game mechanics, generating coherent narratives, and producing contextual character responses in real-time. Our distilled specialized model is fine-tuned on data automatically generated with two collaborative strong LLM agents, without the need for human annotation in the loop. Our distilled LLM model handles the dynamic generation of game rules and scenarios, adapting to player input and game state. In visual generation, we introduce new regional IP-Adapter, which allows for the consistent generation of characters and environments while maintaining visual coherence across multiple images. Specifically, our regional IP-Adapter conditions the image generation on the game environment and character appearance encoded modulated by dynamic mask obtained from attention outputs in cross-attention layers. This is in order to mitigate the interference between the environment and character, in order to have both reliably appear in the scene. This approach enables real-time image generation that reflects the game state and player actions. The contributions of this work are conceptual and technical. We introduce the notion of generative infinite game, demonstrating its feasibility and potential impact on the future of interactive entertainment. We present new paradigm for game design where the game logic and content are encapsulated within generative models. Our main technical contributions include the specialized distilled LLM for game logic and narrative generation and the regional IP-Adapter for consistent visual generation. We demonstrate the effectiveness of our regional IP-Adapter through both quantitative and qualitative evaluations, surpassing state-of-the-art in both character and environment consistency. Furthermore, we show that our distilled LLM performs comparably to very large LLM while having interactive speed. These advancements enable the creation of UNBOUNDED and lay the groundwork for future research and development in the field of AI-driven interactive experiences."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Controllable Text-to-Image Generation. Controllable text-to-image generation becomes key research direction in diffusion model applications, enabling diverse ways to guide the generation process. For instance, ControlNet (Zhang et al., 2023) introduces conditioning mechanisms that utilize control signals such as depth maps, poses, edges, and segmentation maps to guide image generation. Other works focus on layout control using bounding boxes to control object placement within the generated images (Li et al., 2023; Shin et al., 2022). Beyond these control signals, another major area of research involves personalization, where the goal is to generate consistent characters (Ruiz et al., 2023; Gal et al., 2022; Kumari et al., 2023; Avrahami et al., 2024) or consistent face identities (Li et al., 2024b; Wang et al., 2024b; Yan et al., 2023; Ruiz et al., 2024) across multiple generations. However, most existing approaches lack support for conditioning both characters and environments separately, often requiring predefined masks for generating characters (Chen et al., 2024; Lugmayr et al., 2022; Yang et al., 2023), with the environment remaining identical to the input image. This limitation makes it difficult to seamlessly integrate characters into different environments while ensuring both consistency and alignment with the input prompts. IP-Adapter (Ye et al., 2023) tackles this task by conditioning the generation on the environment and character images. However, IP-Adapter tends to over reconstruct the conditions, which causes interference between them. In this paper, we build our approach on IP-Adapter and propose an improved regional IP-Adapter with block drop, separating character and environment generation to enhance consistency. Large Language Models in Image Generation. Large language models have demonstrated strong in-context learning capability (Brown, 2020), which enables them to solve diverse customized tasks based on human instructions and in-context examples. In the field of image generation, large language models have been employed for various tasks, such as image layout generator based on prompts (Cheng et al., 2024a;b), interactive multi-turn image generation (Zeqiang et al., 2023; Huang et al., 2024; Gong et al., 2023; Wang et al., 2023b), and interleaved text and image generator (Team, 2024; Zhou et al., 2024a). Unlike these applications, our work focuses on distilling specialized LLM, serving as game engine, responsible for generating game mechanics, narratives and character interactions. Game Generation. In the field of Procedural Content Generation (PCG), diverse approaches have been explored to create dynamic game content (Summerville et al., 2018). Early works employed concept maps (Treanor et al., 2012), Markov Chains (Snodgrass & Ontanon, 2014), Bayes Nets (Guzdial & Riedl, 2016), and LSTMs (Summerville & Mateas, 2016) to simulate interactive game environments. Recent research has advanced to using Generative Adversarial Networks (GANs) for generating game levels or dynamic environments (Volz et al., 2018; Kumaran et al., 2019; Schubert et al., 2021; Kim et al., 2020), exploiting the use of diffusion models for game generation (Zhou et al., 2024b), utilizing LLMs to design and generate the game environments or mechanics (Sudhakaran et al., 2024; Todd et al., 2023; Nasir & Togelius, 2023; Hu et al., 2024; Zala et al., 2024; Anjum et al., 2024; Chung & Kreminski, 2024; Chung et al., 2024) and employing LLMs to generate items and narratives, as well as using diffusion models for scene generation in 1001 Nights (Sun et al., 2023). One major feature of this body of work is that AI is often employed to help in game design, or to generate one component of the game, for example environments or interactions. Our work aims to fully generate all behaviors of the game, including all the games graphics, characters, environments, and narrative, using generative models. Bruce et al. (2024) explore synthesizing completely new interactive game with video diffusion model conditioned on previous game frames and user actions. This work achieves impressive initial results for this direction, at present it has limitations on the scope of the games being generated, with mostly 2D platformers being represented. This differs from our work since these are not infinite games. We also believe that generating compelling infinite games by only modeling pixels is hard problem and hence we opt to use language models to generate all the open-ended game mechanics instead. Valevski et al. (2024) train diffusion model to be game engine running Doom, and achieve consistent generation of game mechanics by only modeling pixels. Nevertheless, this work currently fits the diffusion model to single finite game that already exists."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Generative game examples of UNBOUNDED. The user can insert custom character into the game, engage with the character through natural language instructions, bring the character to different environments, and interact with it to maintain healthy state under the games mechanics. Besides generating the game content, there has been exploration in letting AI take different roles (e.g., competitor, designer, or teammate) in games (Zhu et al., 2021; Gallotta et al., 2024; Pell, 1992; Agarwal et al., 2023), and system with continuous creativity for automated game design for games that are played on grid of up to 8x8 squares is built in Cook (2022). Alongside generating visual game environments, another line of research builds narrative text games with conditional language generation models to complete knowledge graph (Ammanabrolu et al., 2020) or utilizes LLMs or other AI tools to generate the game (Li et al., 2024a; Latitude Inc., 2023; Wang et al., 2023a) or act in different roles in the game (Zhu et al., 2023; Kreminski et al., 2020; Cui et al., 2023; Zhou et al., 2023; Dambekodi et al., 2020). Our work diverges from these by (1) tackling both generation of all game mechanics, graphics, characters, environments, and narrative of the game using generative models (2) introducing personalization into generative games, where user can insert custom character and personalize the game world and initial story arc, and (3) achieving interactive real-time refresh speed through our innovations in the vision and language domains."
        },
        {
            "title": "3 METHOD",
            "content": "We introduce UNBOUNDED, an interactive generative infinite game powered by text-to-image generation models and large language models. UNBOUNDED offers: (1) Custom Character Personalization: users create unique characters with customizable appearances and personalities; (2) Dynamic World Creation: the system generates persistent, interactive game world for exploration; (3) Open-Ended Interaction and Gameplay: players interact with their characters via natural language, with the game dynamically generating new scenarios and storylines based on player actions; and (4) Generation in Interactive Speed: the game runs with near real-time interactivity, achieving refresh rate close to one second. We detail the methods enabling these capabilities in this section."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: (a) Our overall image generation method. We achieve real-time image generation with LCM LoRA, maintain character consistency with DreamBooth LoRAs, and introduce regional IP-Adapter (shown in (c)) for improved environment and character consistency. (b) Our proposed dynamic mask genreation separating the environment and character conditioning, preventing interference between the two. 3.1 PERSONALIZATION OF LATENT CONSISTENCY MODELS FOR CHARACTER CONSISTENCY key feature of UNBOUNDED is its ability to serve fully generative model-based game with realtime interaction. This is made possible through the use of latent consistency models (LCM) (Luo et al., 2023) which allow for high-resolution image generation with as few as two diffusion steps. By utilizing LCMs, UNBOUNDED achieves real-time text-to-image (T2I) generation, critical for delivering an interactive gaming experience with refresh rate close to one second. To support the use of custom characters in the game, we incorporate DreamBooth (Ruiz et al., 2023) into the T2I model. Given set of character images, we fine-tune the diffusion model using LoRA modules (Hu et al., 2021). During fine-tuning, we append unique identifier, [V], which has weak prior in the model, to denote the subject. DreamBooth personalization is performed on the base diffusion model, and we merge the subject-specific LoRA with the LCM LoRA trained for few-step diffusion. This simple arithmetic LoRA merging works surprisingly well in maintaining both inference speed and subject preservation. Other alternatives for personalization exist, and many do not need setup time, yet we find that they often fail in strongly preserving the characters features, which is critical component in order to have satisfactory experience in this type of game. 3.2 REGIONAL IP-ADAPTER WITH BLOCK DROP FOR ENVIRONMENT CONSISTENCY Another key feature of UNBOUNDED is generating the character in pre-defined environments performing different actions based on user instructions. Thus, maintaining both character and environment consistency is essential. While character consistency is handled as discussed in Sec. 3.1, two additional challenges arise: ensuring the environment consistency across different generations and accurately placing the character within the environment, without losing alignment with text prompts. We find that existing method fails to consistently perform well for all requirements in interactive speed. As one of our main technical contributions we propose novel regional IP-Adapter in order to consistently implant character in pre-defined environments following text prompt."
        },
        {
            "title": "3.2.1 REGIONAL IP-ADAPTER",
            "content": "We propose an improved version of IP-Adapters (Ye et al., 2023) that enables dual-conditioning on both subjects and environments, allowing for the generation of pre-defined character in userspecified environment. Unlike the original IP-Adapters, which focus on single-image conditioning, our approach introduces dual-conditioning and dynamic regional injection mechanism to represent both concepts simultaneously in the generated images. Let us start with an example. As shown in Figure 4, given the text prompt Beneath the desert sky, [V] witch coaxes the cacti to blossom with vibrant, glowing flowers and the desert environment image, the model needs to know that the character should be generated beside cacti and within the desert environment. This requires the model to correctly (1) preserve the environment (2) preserve the character (3) follow the prompt. Utilizing IP-Adapter to encode the environment greatly harms both (2) and (3) (Figure 8). Our regional IP-Adapter solves this by implementing novel attention separation mechanism for generating the two elements. Specifically, we introduce dynamic mask-based approach that leverages cross-attentions between the character text embedding and the hidden states at each layer of the model. As shown in Figure 4, our approach applies the adapter to the regions corresponding to the environment and character separately, preventing the environment conditioning to interfere with the characters appearance, and vice versa. At each cross-attention layer, we calculate the attention map between pre-defined character text embedding Kc and the output hidden states from the text cross-attention layer Ot: Ac = WqOt WkK (1) where Wq and Wk are projection weights adopted from the text cross-attention layers. The dynamic mask we use for the regional IP-Adapter is defined as: Mc = (cid:26) 1 Ac threshold 0 Ac > threshold (2) We rank the attention scores Ac and set the threshold at top r%, dynamically updating the mask at each attention layer throughout the diffusion process. The output of the cross-attention blocks is calculated as: = Ot + αeMc Oe + αc(1 Mc) Oc (3) where Oe and Oc represent the outputs of the IP-Adapter image cross-attention layers of environment and character, and αe and αc are the IP-Adapter scales to adjust the strength of the environment and character conditioning respectively. Our regional IP-Adapter with dynamic masks allows the model to generate the character without the interference from the environment conditioning, greatly enhancing character consistency preservation (Figure 8). Besides, it preserves environment consistency by generating the background around the character conditioning on the environment image, while maintaining the implicit layout information in the hidden states from the text cross-attention layers Ot, ensuring the character is placed accurately following the text prompt. 3.2.2 BLOCK-WISE DROP ON ENVIRONMENT CONDITIONING For our regional IP-Adapter, we use dynamic mask from the crossattention between character text and hidden states. This masks quality is key for separating character and environment generation. Figure 5 shows attention maps between character embeddings and hidden states in cross-attention layers of down sample blocks. We observe that the attention doesnt focus on the character Figure 5: Attention map between character embedding and hidden states in cross-attention layers in different blocks. The character embedding we use is [V] witch."
        },
        {
            "title": "Preprint",
            "content": "but spreads across the full image for these blocks. We take this as strong indication that the diffusion model doesnt separate character and environment generation in these layers and instead focuses on overall image structure based on text prompts. This aligns with Wang et al. (2024a)s finding that down sample blocks capture spatial layouts more, while up sample blocks capture style. We drop the regional IP-Adapter in down sample blocks, using it only in mid and up sample blocks. This allows for better spatial layout generation between character and environment. Additionally, we find that adding dynamic regional IP-Adapters in up sample blocks more strongly aligns the generated background with the conditioning environment by extracting relevant semantic information while preserving character-specific details like appearance and poses."
        },
        {
            "title": "INTEGRATED GAME MECHANICS",
            "content": "UNBOUNDED simulates character actions in pre-defined environments with images generated based on scene text descriptions while monitoring the characters state and providing the user with natural language interaction with the character and game environment. For example, the user can take the character to different environments, interact with the character (e.g. pet the character on its head), or essentially take any open ended action e.g. pet the character and then take them for rocket ride at the space station.. Since the game is ultimately built on language model, these expansive capabilities present several challenges: (1) Environment Binding: the model needs to place the character in the correct environment based on the natural language instructions. (2) Coherent Story Generation: the model generates coherent narrative descriptions and character responses that the model needs to align with user-specified character traits. monitor the state of the character (i.e., hunger, energy, fun, hygiene), and update them based on user interactions and story events. (4) Prompt Rewriting: the model needs to rewrite the narratives for the diffusion model (i.e., append special token [V] for the character, align environment descriptions to the pre-generated environments for better environment consistency). (3) Game Mechanics Generation: Surprisingly, we find that very large language model (e.g., GPT-4, GPT-4o) with detailed instructions and using in-context learning (Brown, 2020) can exhibit these capabilities. Nevertheless, using such large models as game engines is not directly feasible due to the large latency (e.g., 5 seconds for 7B model to give one response). Given this, we propose to distill these capabilities from very large model into smaller model based on Gemma-2B (Team et al., 2024) for game logic and narrative generation that supports real-time interaction. In this section we propose two key technical contributions (1) design for character life simulation game using two very large language models that control for world modeling and user interaction respectively (2) framework for distilling this knowledge into one smaller language model that is fast enough to achieve interactive speeds. 3.3.1 CHARACTER LIFE SIMULATION WITH MULTI-LLM COLLABORATION We build character life simulation game with two LLMs agents. One agent serves as the world simulation model, responsible for setting up game environments, generating narratives and image descriptions, tracking character states and simulating the characters behavior. The second agent functions as user model, simulating the players interactions with the world simulation model. It has three types of interactions: continuing the story within the current environment, moving the character into different environments, or interacting with the character to maintain the healthy state of the character. In each interaction category, the user has the option to provide personality details of the character or guide the character actions that, in turn, guide the simulators narrative generation. This interaction between the world simulation LLM and the user LLM allows for dynamic character life simulation game with virtually unlimited interaction possibilities and narrative paths. 3.3.2 FRAMEWORK FOR SMALL LLM DISTILLATION We propose framework for distilling the capabilities of larger LLMs into the smaller, more efficient model using synthetic data generated by the multiple stronger LLMs. Our framework contains two steps: (1) automated data collection (Figure 6) and (2) small model distillation. Automated Data Collection Our goal is to build general-purpose character life simulator capable of understanding wide range of character traits and generating games across diverse topics."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Overview of our user-simulation data collection process for LLM distillation. (a) We begin by collecting diverse topic and character data, filtered using ROUGE-L for diversity. (b) The World LLM and User LLM interact to generate user-simulation data through multi-round exchanges. To achieve this, the first step is to gather diverse dataset of topics and characters. We prompt large LLM to generate pairs of topics and corresponding main characters. To ensure data diversity, we retain only the generated pairs whose ROUGE-L similarity to existing data is below 0.7, following (Wang et al., 2022), which demonstrated the importance of diverse data for enhancing an LLMs ability to follow instructions. This process results in 5,000 unique topic-character pairs, which serve as the basis for user-simulator interaction data. In the second step, we collect multi-round interaction data between the world simulation LLM and the user LLM. The process begins with the world simulation LLM setting up the game environment and initiating character action based on randomly sampled topic-character pair from the dataset. The user LLM is then prompted to provide interaction inputs, while the world simulation LLM generates updated character actions, states, and responses. This iterative process continues for five interaction rounds per session, resulting in total of 5,000 user-simulator interaction examples. All the prompt templates are in Appendix. Distillation Once the interaction data has been collected, we fine-tune the smaller Gemma-2B model using the 5,000 synthetic user-simulator interaction samples. During supervised fine-tuning, we mask out the loss on user input data, focusing the optimization on learning the world simulation models behavior based on multi-round interaction history and current user input. This approach enables Gemma-2B to replicate the capabilities of larger LLMs as game engine while supporting real-time interaction. Our distilled Gemma-2B demonstrates performance comparable to GPT-4o, effectively following user input and supporting unbounded interactions."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "4.1 EVALUATION BENCHMARKS Evaluation of Image Generations To evaluate our image generation approach, we collect an evaluation dataset consisting of 5,000 (character image, environment description, text prompt) triplets with GPT4o (OpenAI, 2023). It includes 5 characters (dog, cat, panda, witch, and wizard), 100 diverse environments, and 1,000 text prompts (10 per environment). We evaluate the image generation performance under three criteria: environment consistency, character consistency and semantic alignment with the text prompt. To measure similarity between images, we employ CLIP-I (Radford et al., 2021), DINO (Caron et al., 2021), and DreamSim (Fu et al., 2023). We denote the similarity between environment reference image and the generated images as CLIP-IE, DINOE, DreamSimE, and the similarity between the character reference image and the generated images as CLIP-IC, DINOC, DreamSimC. Additionally, we use CLIP-T (Radford et al., 2021) to evaluate the semantic alignment with the text prompt. Given that UNBOUNDED is character life simulation game, ensuring the presence of the character in the image is important. Therefore, we further utilize Grounding-DINO (Liu et al., 2023) to detect the presence of the character in the generated images. We set similarity scores to 0 and distance scores to 1 if there is no character in the generated image. Evaluation of LLM Generations We collect an additional evaluation dataset with 100 usersimulator interaction samples using the pipeline in Sec. 3.3. Each user-simulator interaction sample contains five rounds of interaction between the user and the world model. We use GPT-4 (OpenAI,"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison of UNBOUNDED and other methods for maintaining environment consistency and character consistency. UNBOUNDED achieves the best performance in maintaining consistency, while maintaining comparable semantic alignment with the text prompt. Best scores are in bold. Methods Environment Consistency Character Consistency Semantic Alignment CLIP-IE DINOE DreamSimE CLIP-IC DINOC DreamSimC CLIP-T IP-Adapter (Ye et al., 2023) IP-Adapter-Instruct (Rowles et al., 2024) StoryDiffusion (Zhou et al., 2024c) Ours 0.470 0.334 0.528 0.563 0.381 0.151 0.257 0.322 0.595 0.832 0.733 0.675 0.366 0.246 0.629 0. 0.139 0.124 0.464 0.470 0.832 0.872 0.545 0.488 0.168 0.098 0.242 0.242 Table 2: Ablations of the effectiveness of dynamic regional IP-Adapter and block drop in our consistent image generation approach. Block Regional No. Scale Drop IP-Adapter 1.0 1.0 1.0 0.5 0.5 0.5 1. 2. 3. 4. 5. 6. Environment Consistency Character Consistency Alignment CLIP-IE DINOE DreamSimE CLIP-IC DINOC DreamSimC CLIP-T 0.123 0.414 0.563 0.470 0.577 0.549 0.111 0.331 0. 0.381 0.332 0.263 0.885 0.647 0.675 0.595 0.640 0.726 0.073 0.337 0.676 0.366 0.627 0.705 0.024 0.147 0. 0.139 0.374 0.514 0.973 0.832 0.488 0.832 0.575 0.450 0.034 0.149 0.242 0.168 0.252 0.246 2023) as judge, scoring the response between two models (baseline model vs. our model) in overall score, and then in four aspects: accuracy of character state update, environment relevance, story coherence, and user input instruction following. Scores range from 0 to 10. 4.2 IMPLEMENTATION DETAILS Our image generator is built on SDXL (Podell et al., 2023). We train DreamBooth LoRA of rank 16 with batch size 1 and constant learning rate 1e-4 for 500 steps on single A100, which takes approximately 30 mins. The special token we append before the character is sks. During inference, we merge the LCM-LoRA with DreamBooth LoRA with scale 1.0 for each. We use IP-Adapter-plus-sdxl-vit-h for encoding the environment, and IP-Adapter-plus-face-sdxl-vit-h for encoding the character. The dynamic mask ratio r% in set to be 60%. Our LLM is built on Gemma-2B (Team et al., 2024). We distill the LLM using 5,000 user-simulator interaction samples collected from GPT-4o. We train the LLM for 6,500 steps, with batch size 8, distributed across 4 A100s, and learning rate 1e-4. The learning rate scheduler is set to be cosine annealing (Loshchilov & Hutter, 2016), and the warmup steps ratio is 0.03. In evaluation, we use an LLM to generate responses with sampling. The sampling hyperparameters are set to be default."
        },
        {
            "title": "5 RESULTS AND ANALYSIS",
            "content": "5.1 COMPARISON WITH DIFFERENT APPROACHES FOR MAINTAINING ENVIRONMENT CONSISTENCY AND CHARACTER CONSISTENCY Quantitative Results We compare our regional IP-Adapter with block drop with previous approaches in maintaining environment consistency and character consistency. For all the approaches, we merge the character LoRA and LCM LoRA with the model to support fast inference and improve character consistency and provide an apples-to-apples comparison. As shown in Table 1, our approach consistently outperforms previous approach in maintaining environment consistency and character consistency, while achieving comparable performance in maintaining semantic alignment. Specifically, our approach significantly overtakes StoryDiffusion (Zhou et al., 2024c) by 0.047 in CLIP-IC, and 0.057 in DreamSimC for character consistency, and 0.035 in CLIP-IE, 0.065 in DINOE, and 0.058 in DreamSimE for environment consistency, demonstrating the effectiveness of our approach. Besides, our approach also achieves comparable performance in maintaining semantic alignment, suggesting strong text following capabilities. Qualitative Results We present qualitative comparison with other approaches in Figure 7. Our regional IP-Adapter with block drop consistently generates images with high character consistency, whereas other methods may fail to include the character or generate characters with inconsistent"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Comparison with other approaches for generating environment and character consistent images based on text prompts. We observe that our method strongly outperfoms related work. Figure 8: Comparison between our regional IP-Adapter approach and baseline approaches. appearances (see Example 1 & 2). Furthermore, we show that our approach balances environment consistency and character consistency well, while other approaches might generate environments that differ from the condition environment (e.g., StoryDiffusion in Example 1 & 3). 5.2 EFFECTIVENESS OF DYNAMIC REGIONAL IP-ADAPTER WITH BLOCK DROP Quantitative Results We demonstrate that our regional IP-Adapter with block drop is essential for placing the character in the environments following the text prompt, while maintaining both environment and character consistency with ablation studies. As shown in Table 2, adding block drop improves both environment and character consistency compared with multi-IP-Adapter (No. 2."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Comparison of UNBOUNDED and different LLMs on serving as game engines for open ended interactions and integrated game mechanics. We use GPT-4 to provide pairwise scores between our model and other LLMs. Model Overall State Update Environment Relevance Story Coherence Instruction Following Base Ours Base Ours Base Ours Base Ours Base Ours Gemma-2B (Team et al., 2024) Gemma-7B (Team et al., 2024) Llama3.2-3B (Meta, 2024) Ours-1k GPT-4o (OpenAI, 2023) 6.22 6.80 7.21 7.65 7. 7.44 7.39 7.50 7.82 7.68 5.60 6.29 6.86 7.50 7.69 7.47 7.43 7.38 7.74 7.66 6.12 7.07 7.63 8.10 8.20 7.94 7.91 7.93 8.19 8.10 6.34 6.90 7.36 7.78 7. 7.57 7.48 7.56 7.93 7.82 6.43 6.89 7.31 7.82 7.85 7.67 7.53 7.67 7.97 7.82 vs. No. 1.), with an increase of 0.291 in CLIP-IE and 0.264 in CLIP-IC, alongside better alignment between the text prompt and the generated image. Furthermore, our regional IP-Adapter enhances character consistency and text alignment while maintaining comparable performance in environment consistency (No. 3 vs. No. 2). We also explore the effect of the environment IP-Adapter scale, and our findings indicate that using smaller scale (e.g., 0.5) generally improves character consistency, though it slightly compromises environment consistency (No. 6 vs. No. 3). Qualitative Results As shown in Figure 8, conditioning on the environment using IP-Adapter achieves good environment reconstruction, but the character consistency is influenced by the environment style. Introducing block drop improves adherence to the text prompt, resulting in images with the correct spatial layout for both the character and the environment. However, the characters appearance remains influenced by the surrounding environment. By incorporating our proposed regional injection mechanism with our proposed dynamic mask scheme, the generated images achieve strong character consistency while maintaining effective conditioning on the environment. 5.3 EFFECTIVENESS OF DISTILLING SPECIALIZED LARGE LANGUAGE MODEL We show that our diverse user-simulator interaction data effectively distills Gemma-2B into capable game engine. As shown in Table 3, zero-shot inference with small LLMs (i.e., Gemma-2B, Llama3.2-3B), or slightly larger LLM (i.e., Gemma-7B) results in lower performance compared to ours, highlighting the importance of distillation from stronger LLM for game world and character action simulation. Furthermore, we show that our model achieves performance comparable to GPT4o, validating the effectiveness of our approach. We also investigate the impact of distillation data size on performance by comparing Gemma-2B model distilled with 1K data and 5K data. Results show that using larger dataset consistently improves performance across all aspects, highlighting the potential for further enhancements with more data to fully match GPT-4os performance."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce UNBOUNDED, an interactive generative infinite game based on generative models. UNBOUNDED is built on two main components, specialized, distilled LLM for real-time interaction, and fast diffusion model with our proposed regional IP-Adapter for consistent generation across multiple scenes. We show that our proposed approach allows for an interactive game subsumed in generative models, with consistent characters, environments and story and an expansive gameplay characteristic of an infinite game."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Shiran Zada, Peyman Millanfar, Shlomi Fruchter, Michael Goin and Matthew Guzdial for the thoughtful feedback and discussion."
        },
        {
            "title": "REFERENCES",
            "content": "Neal Agarwal. Infinite craft. https://neal.fun/infinite-craft/, 2024. URL https: //neal.fun/infinite-craft/. Accessed: 2024-10-29."
        },
        {
            "title": "Preprint",
            "content": "Rohan Agarwal, Zhiyu Lin, and Mark Riedl. controllable co-creative agent for game system design. arXiv preprint arXiv:2308.02317, 2023. Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, William Broniec, and Mark Riedl. Bringing stories alive: Generating interactive fiction worlds. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 16, pp. 39, 2020. Asad Anjum, Yuting Li, Noelle Law, Megan Charity, and Julian Togelius. The ink splotch effect: case study on chatgpt as co-creative game designer. In Proceedings of the 19th International Conference on the Foundations of Digital Games, pp. 115, 2024. Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel CohenOr, and Dani Lischinski. The chosen one: Consistent characters in text-to-image diffusion models. In ACM SIGGRAPH 2024 conference papers, pp. 112, 2024. Tom Brown. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165, 2020. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. James P. Carse. Finite and Infinite Games: Vision of Life as Play and Possibility. Free Press, 1986. Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 65936602, 2024. Junhao Cheng, Xi Lu, Hanhui Li, Khun Loun Zai, Baiqiao Yin, Yuhao Cheng, Yiqiang Yan, and Xiaodan Liang. Autostudio: Crafting consistent subjects in multi-turn interactive image generation. arXiv preprint arXiv:2406.01388, 2024a. Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, et al. Theatergen: Character management with llm for consistent multi-turn image generation. arXiv preprint arXiv:2404.18919, 2024b. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. John Joon Young Chung and Max Kreminski. Patchview: Llm-powered worldbuilding with generative dust and magnet visualization. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pp. 119, 2024. John Joon Young Chung, Melissa Roemmele, and Max Kreminski. Toyteller: Toy-playing with character symbols for ai-powered visual storytelling. In Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pp. 15, 2024. Michael Cook. Puck: slow and personal automated game designer. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 18, pp. 232 239, 2022. Michael Cook, Simon Colton, and Jeremy Gow. The angelina videogame design systempart i. IEEE Transactions on Computational Intelligence and AI in Games, 9(2):192203, 2016. Christopher Cui, Xiangyu Peng, and Mark Riedl. Thespian: Multi-character text role-playing game agents. arXiv preprint arXiv:2308.01872, 2023."
        },
        {
            "title": "Preprint",
            "content": "Sahith Dambekodi, Spencer Frazier, Prithviraj Ammanabrolu, and Mark Riedl. Playing text-based games with common sense. arXiv preprint arXiv:2012.02757, 2020. Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, and Georgios Yannakakis. Large language models and games: survey and roadmap. arXiv preprint arXiv:2402.18659, 2024. Swen Gaudl, Mark Nelson, Simon Colton, Rob Saunders, Edward Powley, Peter Ivey, Blanca Perez Ferrer, and Michael Cook. Exploring novel game spaces with fluidic games. arXiv preprint arXiv:1803.01403, 2018. Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, et al. Talecrafter: Interactive story visualization with multiple characters. arXiv preprint arXiv:2305.18247, 2023. Matthew Guzdial and Mark Riedl. Game level generation from gameplay videos. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 12, pp. 4450, 2016. Chengpeng Hu, Yunlong Zhao, and Jialin Liu. Game generation via large language models. In 2024 IEEE Conference on Games (CoG), pp. 14. IEEE, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. Dialoggen: Multi-modal interactive dialogue system for multiturn text-to-image generation. arXiv preprint arXiv:2403.08857, 2024. Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, and Sanja Fidler. Learning to simulate dynamic environments with gamegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12311240, 2020. Max Kreminski, Melanie Dickinson, Michael Mateas, and Noah Wardrip-Fruin. Why are we like this?: The ai architecture of co-creative storytelling game. In Proceedings of the 15th International Conference on the Foundations of Digital Games, pp. 14, 2020. Vikram Kumaran, Bradford Mott, and James Lester. Generating game levels for multiple distinct games with common latent space. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 15, pp. 102108, 2019. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19311941, 2023. Latitude Inc. Ai dungeon, 2023. URL https://aidungeon.com/. Accessed: 2024-10-29. Yoko Li, Paul Phu, Danilowicz Alex, and AP. Ai tamago. https://github.com/ykhli/ AI-tamago, 2024a. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2251122521, 2023."
        },
        {
            "title": "Preprint",
            "content": "Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 86408650, 2024b. Antonios Liapis, Georgios N. Yannakakis, Mark J. Nelson, Mike Preuss, and Rafael Bidarra. Orchestrating game generation. IEEE Transactions on Games, 11(1):4868, 2019. doi: 10.1109/ TG.2018.2870876. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. In Proceedings of the Repaint: IEEE/CVF conference on computer vision and pattern recognition, pp. 1146111471, 2022. Inpainting using denoising diffusion probabilistic models. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Meta. Llama3.2, 2024. URL https://ai.meta.com/blog/ llama-3-2-connect-2024-vision-edge-mobile-devices/. Muhammad Nasir and Julian Togelius. Practical pcg through large language models. In 2023 IEEE Conference on Games (CoG), pp. 14. IEEE, 2023. OpenAI. Openai models, 2023. URL https://platform.openai.com/docs/models. Barney Pell. Metagame: new challenge for games and learning. 1992. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Ciara Rowles, Shimon Vainer, Dante De Nigris, Slava Elizarov, Konstantin Kutsy, and Simon Ipadapter-instruct: Resolving ambiguity in image-based conditioning using instruct Donne. prompts. arXiv preprint arXiv:2408.03209, 2024. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 65276536, June 2024. Frederik Schubert, Maren Awiszus, and Bodo Rosenhahn. Toad-gan: flexible framework for fewshot level generation in token-based games. IEEE Transactions on Games, 14(2):284293, 2021. Gyungin Shin, Weidi Xie, and Samuel Albanie. Reco: Retrieve and co-segment for zero-shot transfer. Advances in Neural Information Processing Systems, 35:3375433767, 2022. Sam Snodgrass and Santiago Ontanon. Experiments in map generation using markov chains. In FDG, 2014."
        },
        {
            "title": "Preprint",
            "content": "Shyam Sudhakaran, Miguel Gonzalez-Duque, Matthias Freiberger, Claire Glanois, Elias Najarro, and Sebastian Risi. Mariogpt: Open-ended text2level generation through large language models. Advances in Neural Information Processing Systems, 36, 2024. Adam Summerville and Michael Mateas. Super mario as string: Platformer level generation via lstms. arXiv preprint arXiv:1603.00930, 2016. Adam Summerville, Sam Snodgrass, Matthew Guzdial, Christoffer Holmgard, Amy Hoover, Aaron Isaksen, Andy Nealen, and Julian Togelius. Procedural content generation via machine learning (pcgml). IEEE Transactions on Games, 10(3):257270, 2018. Yuqian Sun, Zhouyi Li, Ke Fang, Chang Hee Lee, and Ali Asadipour. Language as reality: cocreative storytelling game experience in 1001 nights using generative ai. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 19, pp. 425434, 2023. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, and Julian Togelius. Level generation through large language models. In Proceedings of the 18th International Conference on the Foundations of Digital Games, pp. 18, 2023. Mike Treanor, Bryan Blackford, Michael Mateas, and Ian Bogost. Game-o-matic: Generating videogames that represent ideas. In Proceedings of the The third workshop on Procedural Content Generation in Games, pp. 18, 2012. Mike Treanor, Alexander Zook, Mirjam Palosaari Eladhari, Julian Togelius, Gillian Smith, Michael Cook, Tommy Thompson, John Levine, and Adam Smith. Ai-based game design patterns. 06 2015. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Vanessa Volz, Jacob Schrum, Jialin Liu, Simon Lucas, Adam Smith, and Sebastian Risi. Evolving mario levels in the latent space of deep convolutional generative adversarial network. In Proceedings of the genetic and evolutionary computation conference, pp. 221228, 2018. Haofan Wang, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024a. Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024b. Ruoyao Wang, Graham Todd, Eric Yuan, Ziang Xiao, Marc-Alexandre Cˆote, and Peter Jansen. Bytesized32: corpus and challenge task for generating task-specific world models expressed as text games. arXiv preprint arXiv:2305.14879, 2023a. Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen. Autostory: Generating diverse storytelling images with minimal human effort. arXiv preprint arXiv:2311.11243, 2023b. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Zhang, Pei Cheng, Gang Yu, and Bin Fu. Facestudio: Put your face everywhere in seconds. arXiv preprint arXiv:2312.02663, 2023."
        },
        {
            "title": "Preprint",
            "content": "Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and In ProFang Wen. Paint by example: Exemplar-based image editing with diffusion models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18381 18391, 2023. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. Envgen: Generating and adapting environments via llms for training embodied agents. arXiv preprint arXiv:2403.12014, 2024. Lai Zeqiang, Zhu Xizhou, Dai Jifeng, Qiao Yu, and Wang Wenhai. Mini-dalle3: Interactive text to image by prompting large language models. arXiv preprint arXiv:2310.07653, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024a. Hongwei Zhou, Jichen Zhu, Michael Mateas, and Noah Wardrip-Fruin. The eyes, the hands and the brain: What can text-to-image models offer for game design and visual creativity? In Proceedings of the 19th International Conference on the Foundations of Digital Games, pp. 113, 2024b. Wei Zhou, Xiangyu Peng, and Mark Riedl. Dialogue shaping: Empowering agents through npc interaction. arXiv preprint arXiv:2307.15833, 2023. Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. sion: Consistent self-attention for long-range image and video generation. arXiv:2405.01434, 2024c. StorydiffuarXiv preprint Andrew Zhu, Lara Martin, Andrew Head, and Chris Callison-Burch. Calypso: Llms as dungeon masters assistants. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 19, pp. 380390, 2023. Jichen Zhu, Jennifer Villareale, Nithesh Javvaji, Sebastian Risi, Mathias Lowe, Rush Weigelt, and Casper Harteveld. Player-ai interaction: What neural network games reveal about ai as play. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 117, 2021."
        },
        {
            "title": "APPENDIX",
            "content": "In this appendix, we present the following: Prompts we use for user-simulation data collection in Sec. A. Evaluation prompts we use for querying GPT-4 as judge for LLM evaluation in Sec. B. Reproducibility statement in Sec. C. PROMPT FOR SYNTHETIC USER-SIMULATOR INTERACTION DATA COLLECTION In this section, we provide the prompt templates we use to collect the user-simulation data. Specifically, we query GPT-3.5 to generate diverse topics and character descriptions using the template shown in Figure 9. The prompt template for guiding the potential user interactions for user LLM is shown in Figure 10. To ensure user interactions align with the dialogue history, we include the interaction history as input to the user LLM. The world simulation LLM prompt template, shown in Figure 11, also takes in the dialogue history and generates the next character actions, states and narratives. We constraint the world simulation LLM to generate one storyline at time, allowing users to choose how to continue the story."
        },
        {
            "title": "B EVALUATION PROMPT",
            "content": "We include the prompt template we use to compare the outputs from two LLMs in Figure 12. The prompt is adapted from Vicuna (Chiang et al., 2023), and has been validated as an effective tool for comparing the performance of two LLMs on given task."
        },
        {
            "title": "C REPRODUCIBILITY STATEMENT",
            "content": "First, we include the implementation details of UNBOUNDED in Sec. 4.2, covering the training hyperparameters for Dreambooth fine-tuning, and LLM distillation, and the hyperparameters we use during inference for both LLM and image generation. Second, we provide detailed description of the user-simulation data we collect for training in Sec. 3.3, and further include the prompt template used to query GPT models in Appendix Sec. A. Lastly, for the LLM-based evaluation, the prompt template for querying GPT-4 to compare two LLM outputs is provided in Appendix Sec. B. Figure 9: Prompt template used to collect diverse topic and character data."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Prompt template used to query user LLM."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Prompt template used to query world simulation LLM."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Prompt template used to query GPT4 to compare the performance between two LLM outputs. The prompt is adapted from Vicuna (Chiang et al., 2023)."
        }
    ],
    "affiliations": [
        "Google",
        "The University of North Carolina at Chapel Hill"
    ]
}