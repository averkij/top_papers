{
    "paper_title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified Framework",
    "authors": [
        "Jing Wang",
        "Fengzhuo Zhang",
        "Xiaoli Li",
        "Vincent Y. F. Tan",
        "Tianyu Pang",
        "Chao Du",
        "Aixin Sun",
        "Zhuoran Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, a unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDM -- error accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve a significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate a Pareto-frontier between the error accumulation and memory bottleneck across different methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 4 0 7 0 1 . 3 0 5 2 : r Error Analyses of Auto-Regressive Video Diffusion Models: Unified Framework Jing Wang1,2,3,, Fengzhuo Zhang1,4,, Xiaoli Li2,3 Vincent Y. F. Tan4 Tianyu Pang1 Chao Du1, Aixin Sun3 Zhuoran Yang5 1 Sea AI Lab 2 Nanyang Technological University 3 A*STAR 4 National University of Singapore 5Yale University {jing005@e.,axsun@}ntu.edu.sg fzzhang@u.nus.edu xlli@i2r.a-star.edu.sg vtan@nus.edu.sg {tianyupang, duchao}@sea.com zhuoran.yang@yale.edu Abstract variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDMerror accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate Pareto-frontier between the error accumulation and memory bottleneck across different methods. The project webpage is https://sail-sg.github.io/AR-Video-Diffusion."
        },
        {
            "title": "Introduction",
            "content": "The Video Diffusion Model (VDM) has achieved tremendous successes in generating highquality and temporally coherent videos (Xing et al., 2024; Ho et al., 2022b). Among all the VDMs, the Auto-Regressive Video Diffusion Model (ARVDM) has emerged as particularly effective approach to generate long videos (Kim et al., 2024; Henschel et al., 2024). By modeling video data in an Auto-Regressive (AR) manner, these models can generate complex motion equal contribution Work done by Jing Wang and Fengzhuo Zhang as associate members at Sea AI Lab. corresponding author 1 patterns and rich textures that align closely with real-world observations. Their versatility spans diverse applications, including video games (Che et al., 2024), world models (Chen et al., 2024a), and robotics (Zhang et al., 2024). Despite these empirical achievements, systematic and theoretical studies of the characteristics of ARVDMs are scant. It is not clear what is the essential difference between ARVDMs and the VDMs that generate all the frames simultaneously.1 We pose the following questions: Does there exist any type of error/inconsistency that is shared by most ARVDMs but does not appear in NATVDMs? Why does such an error appear and how can we mitigate its adverse effects? In this work, we address the above questions. To analyze the error shared by all ARVDMs, we propose general framework of the generation process of ARVDMs (Section 3). This framework requires minimal assumptions, including monotonicity, circularity, 0 boundary, and causality, for plausible AR generation. Then we analyze the performance of both long videos and short clips generated by ARVDMs based on this framework (Section 4). By analyzing the error of long videos, we find that it stems from four sources: the noise initialization error, the score estimation error, the discretization error, and the memory bottleneck. The first three terms all appear in NATVDM. The memory bottleneck term, which is unique to ARVDM, measures the conditional mutual information between past and output frames conditioned on input frames. Via the derivation of an information-theoretic lower bound, we show that this term is unavoidable. By analyzing the error of short clips, we show that there exists error accumulation, i.e., the later generated video clips usually contain larger errors. The phenomena memory bottleneck and error accumulation are also supported by empirical observations. Finally, we mitigate the memory bottleneck by modifying the network structure (Section 5) to include prepending and channel concatenation structures. By retrieving scenes from past frames using the modified structure, we achieve desirable tradeoff between the performance and inference cost via compression. Our theoretical findings are supported by experiments. Our main contributions are as follows: We propose unified framework Meta-ARVDM of the generation process of ARVDMs. We identify several requirements for the plausible AR generation. Our study can be viewed as generalization of the previous works and firm basis to analyze the common characteristics shared by most ARVDMs. Using the proposed framework, we analyze the error of the ARVDMs. For the generated long video, we show that there are four types of errors. Among them, the memory bottleneck is unique to ARVDMs, representing the inconsistency of the scenes across different AR steps. For the generated short clips, we show that there exists error accumulation in the later generated clips. In addition, we show that the memory bottleneck is unavoidable via the derivation of an information-theoretic lower bound of the learning error. We design structures to efficiently mitigate the memory bottleneck. Our experiments verify the efficacy of the prepending and channel concatenation structure, where compression of historical information further improves efficiency. The correlation between error accumulation and memory bottleneck observed in our experiments serves as as basis for future improvements. 1We will call this Non-AuTo-regressive VDM (NATVDM). 2 Figure 1: FVDs of short clips generated by different models and methods."
        },
        {
            "title": "2 Preliminaries",
            "content": "Video Diffusion Models Video Diffusion models generate video frames with the help of the reversed process of the diffusion process (Ho et al., 2022b,a). We denote video as concatenation of frames along the time axis, i.e., X1:w = (X1, , Xw) Rwd. Here is the number of frames, and is the dimension of each frame. For example, = 512 512 for video with 512 pixels in each row and column. Starting from the clean video frames 0 1:w = X1:w, the diffusion process gradually adds Gaussian noise to them, which can be described by the following diffusion process. dX 1:w = (X 1:w, t) dt + g(t) dBt for [0, ], (2.1) where denotes the time index of the diffusion process, 1:w denotes the noisy version of video frames at diffusion time t, and the functions : Rwd Rwd and : determine 1:w evolves, and Bt is (w d)-dimensional Brownian motion with how the diffusion process the identity covariance matrix. Throughout the whole paper, we adopt the subscript and superscript to denote the index of the frame in the video and the time in the diffusion process, respectively. We will use the time index of the diffusion process as the noise level in this work to distinguish it from the time index of the frame w. As discussed in Song et al. (2020), the variance-exploding scheme (SMLD) (Song and Ermon, 2019) and the variance-preserving method (DDPM) (Ho et al., 2020) adopt respectively the configuration (X1:w, t) = 0, g(t) = β(t) and (X1:w, t) = 0.5β(t) X1:w, g(t) = (cid:112)β(t) to generate images, where β : is scalar function. In these two configurations, the distribution of 1:w converges to the Gaussian distribution as grows. To generate images and videos from Gaussian noise, we would like to consider the evolution of 1:w backward in time, i.e., from = to = 0. This is captured by the following reverse-time version of the forward diffusion process (Anderson, 1982). dX 1:w = (cid:0)f (X 1:w, t) g(t)2 log Pt(X 1:w)(cid:1) dt + g(t) (cid:101)Bt (2.2) Figure 2: Examples of error accumulation (first two rows) and memory bottleneck (last two rows). for [0, ], where (cid:101)Bt is the reverse-time Brownian motion, and Pt is the distribution of 1:w for [0, ]. Here log Pt(X 1:w) is known as the score function, and previous works trained neural networks to learn it (Ho et al., 2022b). Considering the transformation on the reverse-time process, we have that (cid:101)X 1:w = (cid:0) ( (cid:101)X 1:w, t) + g(T t)2 logPT t( (cid:101)X 1:w)(cid:1) dt + g(T t) dBt. (2.3) We will write (cid:101)X (resp. (cid:101)Bt) and (resp. Bt) to denote the reverse-time and original stochastic processes (Brownian motion), respectively. We will focus on this form of the reversetime process for the theoretical analysis of our work. Auto-regressive Video Diffusion Models To generate long videos, line of recent works adopt the auto-regressive framework. Concretely, different from Eqn. (2.3), which denoises all the frames from the same noise level simultaneously, the ARVDMs autoregressively generate the frames of long video according to their positions. During the generation process, the denoised frames share the same noise level or have different noise levels. The generation of the later frames is conditioned on the previously generated ones. FIFO-Diffusion (Kim et al., 2024), and outpainting are two methods that include two stages. In the first stage, they generate several clean frames according to Eqn. (2.3). The second stage then generates frames in an AR manner. FIFO-Diffusion first adds different levels of Gaussian noises to the clean frames generated in the first stage. Then it denoises the frames at different noise levels simultaneously. Each AR step generates one clean frame. The outpainting algorithm generates new frames with guidance from the already generated frames by concatenating the noisy version of the clean frames with the frames to denoise. Then it just denoises all the frames but only retains the newly generated frames. New phenomena of ARVDMs Different from the short videos that are simultaneously generated, the long videos generated by ARVDMs present some new phenomena. Here we demonstrate two of them: error accumulation and memory bottleneck. Error accumulation refers to the fact that the later generated frames usually have poorer equality than the previous 4 Algorithm 1 Meta-ARVDM Input: VDM sθ, the AR step-size N+, effective window size N+, the length of the video N+, the input noise levels LI = (tI 1 , , tO w), reference frames sets {Ri}i[N ] Procedure: w), the output noise levels LO = (tO 1, , tI 1 1 , , tI ) = Initialization(sθ, LI). 1: // Initialization Stage: 2: (Y tI 3: // Auto-Regressive Generation Stage: 4: for = 1, . . . , N/ do 5: (k1)+1:k, (Y tI 0 1 tI k+1, , k+w) = Auto-Regressive Step(cid:0){Y tI (k1)+j}w j=1 , sθ, w, , LI, LO, R(k1)+1, 0 R(k1)+ (cid:1) 6: end for 7: Output 0 1:N . Algorithm 2 Initialization Input: VDM sθ, the size of the effective window N, the input noise levels LI = (tI Procedure: 1: Generate i0 independent Gaussian vectors 1:i0, and set the reference frames set as 1, . . . , tI w) empty = . 1:i0 to 0 2: Denoise 3: Add noises to 0 4: Return (Y tI 1:i0 with Eqn. (3.1). tI 1:i0 to get (Y tI w ). 1 ,. . ., tI w ). 1 , . . . , 1 1 ones. Figure 1 shows the Frechet Video Distance (FVD) between the generated clips with 16 frames and the true video clips of VideoCrafter2 (Chen et al., 2024b), Open-Sora-Plan (Lin et al., 2024), StreamingT2V (Henschel et al., 2024), and FIFO-Diffusion (Kim et al., 2024), which gradually increases with increasing time index. Visually, the large FVD represents the low image quality or the unnatural deformation in the videos in Figure 2, which depends on the inference methods. Memory bottleneck refers to the fact that later generated frames are unaware of the contents in the previous frames, i.e., the ARVDMs do not extract and memorize the contents of the previous frames. As shown in the video generated by Oasis (Decart, 2024) in Figure 2, the generated videos do not present consistent environment."
        },
        {
            "title": "3 A General Framework of ARVDMs",
            "content": "In this section, we propose general framework for the inference of ARVDMs, which includes most of the previous ARVDM designs. We will explain our framework via two examples: outpainting and FIFO-Diffusion. This framework is presented in Algorithm 1. As mentioned in Section 2, this meta-algorithm consists of two stages. The initialization stage (Algorithm 2) 5 Figure 3: This figure indicates Meta-ARVDM framework. The left part if the initialization stage, which denoises with Minit steps. We add noise to the output of this stage to form the starting point of the AR generation stage. We then auto-regressively apply Algorithm 3 for denoising. The monotonicity, circularity, and 0 boundary requirements for plausible implementation j=1) are all the frames before the execution of the tI i+4+j}4 are marked in the figure. Here H({Y (i/2 + 2)-th iteration. 1, . . . , tI w), the output noise levels LO = (tO tI ), the reference frames index set R, reference frames 0 Algorithm 3 Auto-Regressive Step Input: VDM sθ, the size of the effective window N+, the AR step-size N+, the input noise levels LI = (tI w), the input noisy video (Y tI 1 , . . . , Procedure: 1: Generate independent Gaussian vectors (Y 2: Concatenate (Y tI 3: Denoise (Y tI 4: Return {Y tO ). ) with the generated noise to form (Y tI 1 , . . . , tO ) with the reference frames 0 tO )(cid:1) +1 ,. . ., tO +1 ),(Y ). as Eqn. (3.3). ) to (Y tO 1 ,. . ., tO 1 , . . . , tI }w w+1 , . . . , tI i=1 = (cid:0)(Y tO 1 , . . . , tI 1 , . . . , tO 1 , . . . , tI w+1 tI w w 1 1 1 1 n (cid:101)X , (cid:101)tinit 1:w , (cid:101)tinit )2sθ( (cid:101)X (cid:101)tinit ) + g(T (cid:101)tinit 1:w = (cid:0) (X (cid:101)tinit adopts VDM to generate i0 frame with the following EulerMaruyama scheme. )(cid:1)dt + g(T (cid:101)tinit 1 . . . tinit Minit n+1] with = 0, . . . , Minit 1, where 0 = tinit = tinit 0 tinit for [(cid:101)tinit = partitions the interval [0, ] into Minit intervals, and (cid:101)tinit Minitn is the reversed noise level. Here sθ denotes the neural network with parameter θ Θ that estimates the score function log , and Θ is the set of network parameters. This process is shown in the left part of Figure 3. In outpainting, this initialization stage corresponds to the procedure that generates the first block of videos. In FIFO-Diffusion, this corresponds to generating the first frames, consisting of several blocks, and adding different-level noises to them. 1:w , (cid:101)tinit )dBt (3.1) The auto-regressive generation stage of some methods, including FIFO-Diffusion and StreamingT2V, denoise frames at different noise levels simultaneously and have additional reference frames. Thus, we extend the ground-truth reverse equation (Eqn. (2.2)) from these two perspectives to include these methods. To denoise the frames at different noise levels, we first define the extended time t(t) = (t + δ1, + δ2, . . . , + δw), where δi for [w] denotes the difference between the noise levels of the i-th frame and an evolving noise level t. We will abbreviate t(t) as when is clear from the context. In addition, we assume that the initial 6 distribution of 0 R, where is the index set of the reference frames. Then the true version of the corresponding reverse-time equation of different noise levels with reference frames is 1:w in Eqn. (2.1) is conditioned on set of reference clean frames 0 (cid:101)X 1:w = (cid:0) ( (cid:101)X 1:w, t) + g( t)2 log t ( (cid:101)X 1:wX 0 R)(cid:1)dt + g( t)dBt 1:w (3.2) 1:w is the abbreviation of frames at different noise levels, i.e., for [T tI, tO], where tI and tO are the input and output noise levels of each AR diffusion step, = (T, . . . , ) is the concatenation of , is the window size of each AR denoise step, ), is the distribution of 1:w, and functions : Rwd Rw Rwd and : Rw determine the evolution of the noisy frames. Here we implicitly assume that all the frames are denoised at the same rate for ease of notation. The analysis for the different rates can be easily generated from our analysis in this work, see the discussion in Appendix C. In the following, we would like to fix the input and output noise levels of the frames in window as LI = (tI w) and 1 in the definition of t(t).2 Then each AR step tI 1 = tO LO = (tO will denoise the frames according to the EulerMaruyama scheme as w). We set δi = tI 1:w = (X t+δ1 , . . . , t+δw 1 , . . . , tO 1, . . . , tI tO 1 (cid:101)Y 1:w = (cid:0) ( (cid:101)Y ar ar ) + g( 1:w, )2sθ( (cid:101)Y (cid:101)t (cid:101)t ar 1:w, , 0 (cid:101)t ar R)(cid:1) dt+g( ) dBt (cid:101)t 1:w ar (cid:101)t ar (cid:101)t (3.3) , (cid:101)tar 0 . . . tar for [(cid:101)tar n+1] with = 0, . . . , Mar 1. This is an approximate and implementable version of Eqn. (3.2). We discretize the interval between the noise levels of the first frame [tI 1 ] into Mar 1 = tar intervals according to tO Marn is the ar reverse noise level, and = t((cid:101)tar . Here (cid:101)t the subscript of tar denotes the index of the denoising step. The discretization of denoising steps of other frames is induced by t(tar ). For ease of notation, we denote the denoising neural network we use in both initialization and AR stages as sθ. The score estiamte sθ takes the current noisy frames (cid:101)Y as inputs. Auto-Regressive Step implements Eqn. (3.3) to denoise the frames. 1. Similar to Eqn. (3.1), (cid:101)tar ) is th reverse noise levels of all frames induced by (cid:101)tar t((cid:101)tar ) 1:w , current noise level (cid:101)tar , and the reference frames 0 1, tO = tar Mar = tI The AR generation stage is an AR implementation of Auto-Regressive Step, i.e., Algorithm 3. This is shown as the right part of Figure 3. It auto-regressively denoises the frames in the effective window with size w. For outpainting, if we utilize 12 already generated frames to denoise 4 frames, then the effective window is formed by these 4 frames. For FIFO-Diffusion, the latent partitioning concatenates the VDM network to denoise frame concurrently. The effective window is formed by all these concurrently denoised frames. In each AR step, we concurrently denoise the frames from the noise level LI = (tI w). To form plausible AR generation, i.e., the output of the current Auto-Regressive Step can be used as the input of the next step, we put the following requirements for LI, LO, and R. w) to LO = (tO 1 , . . . , tO 1, . . . , tI Requirement 1 . (Design of LI, LO and R) (Monotonicity) The noise levels are monotone, i.e., tI i1 tI i, tO i1 tO , and tO < tI for [w]. (0 Boundary) The boundaries of LI and LO are and 0, respectively, i.e., tI = for + 1, and tO = 0 for . 2The equation tI tI 1 = tO tO 1 results from that we assume that the noise levels of all the frames evolve at the same rate. 7 (Circularity) The output noise levels are the same as the input noise levels up to position shift, i.e., tI = tO +i for all [w ]. (Constant Pace) The difference between the input and output noise levels are constant, i.e., tO tI = tO tI for all i, [w]. (Causality) When denoising frames indexed from + 1 to + w, the reference frames set is subset of [i], i.e., Ri [i] for all N+. and tO Figure 3 shows these conditions for = 4, = 2. The monotonicity ensures that the frame that appears early in the time, i.e., the index is small, will have lower noise levels tI during denoising. The 0 Boundary ensures that the input to Auto-Regressive Step contains new frames, i.e., there are Gaussian vectors, and that the output contains clean frames. The circularity guarantees that the output of Auto-Regressive Step can be used as the input for the next-step AR generation. We put the constant pace requirement just for the conciseness of theoretical analysis. This is relaxed in Appendix C. The causality requirement is necessary for practical algorithm, since the generation of the current frames cannot be conditioned on the future frames. We note that these requirements are satisfied by both outpainting and FIFO-Diffusion. For outpainting, tI = 0 for all [w], and = w. In each step, we denoise Gaussian noise vectors to clean frames. For FIFO-Diffusion, we have = 1, and (0, tI w1, ) are set to the noise levels of pre-chosen scheduler. With all these requirements satisfied, in each iteration, Auto-Regressive Step takes the output of the previous iteration and new noises as inputs and denoises them to generate clean frames and the inputs for the next iteration. The denoising process is achieved via Eqn. (3.3). = and tO w1) = (tO 1 , . . . , tO 1, . . . , tI"
        },
        {
            "title": "4 Performance Analysis of Meta-ARVDM",
            "content": "In this section, we analyze the performance of the Meta-ARVDM framework proposed in Section 3. All the algorithms that are included by Meta-ARVDM will share the common error analysis we will present in this section. Similar to the previous theoretical analysis of the diffusion process (Chen et al., 2022, 2023a,b), we consider specific configuration of ( (cid:101)Y 1:w and g(t) = 1 for ease of notation, i.e., we analyze DDPM in Section 2 with β(t) = 1 as representative setting. The results for other configurations can be generalized from our analysis. We first state some mild assumptions required by the analysis. 1:w, t) = 0.5 (cid:101)Y Assumption 4.1 (Boundness of Pixel Values). For the frames with the prefixed size = max{, i0}, the ℓ2-norm of 0 i+1:i+l is upper-bounded by constant > 0 almost surely, i.e., 0 i+1:i+l2 a.s. for any N. In latent diffusion models (Chen et al., 2024b; Guo et al., 2023), frames are denoised in the latent space, where each pixel value is bounded due to quantization. Assumption 4.2 (Lipschitz continuity of Score Function). For any [0, ], log log t(t) are L-Lipschitz with respect to ℓ2-norm. and Intuitively, the larger the Lipschitz constant is, the larger the discretization error. This assumption is also widely adopted in numerical analysis and diffusion theory (Gautschi, 2011; Suli and Mayers, 2003; Chen et al., 2023a). 8 Assumption 4.3 (Score Estimate Error (informal)). The average score estimation error evaluated at the discretization steps is upper bounded by ϵ2 est > 0 for both the initialization and AR stages. The formal version of this assumption is stated in Appendix K. This assumption measures the score estimation error along the discretized diffusion trajectory, which originates from the training process of the denoising networks. This assumption is widely adopted by the diffusion model analysis works (Chen et al., 2022, 2024c). Theorem 4.4. Under Assumptions 4.1, 4.2, and 4.3, the KL-divergence between the distributions of the video generated by Meta-ARVDM and the nominal video is KL(X 0 1:K 0 1:K) = IE + (cid:88) k=1 AREk. (4.1) Here the error of the initialization stage IE and the error of k-th AR step AREk are respectively bounded as follows. IE NIE + SEE + DE, AREk NIEAR + SEEAR + DEAR + MBk (4.2) where the noise initialization errors NIE := (di0 + B2) exp(T ) and NIEAR := (d + B2) exp(T ), the score estimation errors SEE := ϵ2 est, the discretization errors DE := wdL2 (cid:80)Minit n1)2, and finally, the memory bottleneck est and SEEAR := (tI n1)2 and DEAR := wdL2 (cid:80)Mar 1 tO n=1(tar 1 )ϵ2 tar tinit n=1 (tinit MBk := I(cid:0)Outputk; Pastk (cid:12) (cid:12)Inputk (cid:1), (4.3) where the Outputk, Pastk, and Inputk are defined as Outputk = {X tO k+j}w j=1, Pastk = (cid:16)(cid:8)X tO k+j (cid:9)w j= (cid:17) (cid:8)X 0 Rk+1 (cid:9), Inputk = {X 0 Rk+1 } {X tI k+j}w j=1. The history H() is formally defined in Appendix L, and an example is shown in Figure 3. If = , the KL-divergence between the distributions of the K-th video clip generated by Meta-ARVDM and the nominal video is KL (cid:0)X 0 K+1:(K+1) (cid:13) (cid:13)Y K+1:(K+1) (cid:1) IE + K(cid:2)NIEAR + SEEAR + DEAR (cid:3). (4.4) The proof is provided in Appendix L. We write to mean Cy for an absolute constant > 0. This theorem delineates the errors of the generated long videos (Eqn. (4.1)) and the generated short clips (Eqn. (4.4)) measured in terms of the KL-divergence. The error of the generated long videos consists of the errors of the initial stage (Algorithm 2) and the AR step (Algorithm 3). Both of them contain the noise initialization error, the score estimation error, and the discretization error. The noise initialization error originates from the fact that the random variables w+1:w are not exactly Gaussian. The difference between them and the Gaussian random variables contributes to this error, which decreases exponentially with increasing . The score estimation error originates from the difference between the true score function and the estimate from the network. The concrete rate of this term is not relevant to the main topic of our work, which is considered in Chen et al. (2023b). Intuitively, this term should decrease with increasing number of training data points. The discretization error originates 1:w and 9 Figure 4: The simplified setting for the proof of lower bounds. init) or O(M 1 from the fact that we discretize the trajectory with the Euler-Maruyama scheme. We can see that this term decreases with increasing discretization steps Minit and Mar. If we adopt uniform discretization, the discretization error will scale as O(M 1 ar ). This error term can be improved with more advanced numerical methods, such as DPM-solver (Lu et al., 2022). Effect of Memory Bottleneck In Eqn. (4.1), the memory bottleneck term of the k-th AR step is the conditional mutual information between the output of the k-th AR step and all the past frames conditioned on the input and reference frames of the k-th AR step. This is intuitive since the information of the past can be gained by the output of the k-th AR step only through the input of it. This contributes to special category of inconsistency in the generated videos. As shown in the video generated by Oasis in Figure 2, after the view of the camera is moved towards the sky, the ARVDM seems to forget all the past frames and generate totally different scene when the camera is moved down. Thus, we call this term Memory Bottleneck. In our bound, Pastk is the frames of the previous scenes, Inputk is the frames of the sky, and Outputk is the scene to generate. Since the frames of the sky are not informative about how the past scenes are, this mutual information term is large. In fact, this term is 0 when Inputk is proper summarization of Pastk, i.e., they form the following Markov chain. OutputkInputkPastk. (4.5) Till now, we have shown that this intuitive memory bottleneck appears in the upper bound of the error. However, it is not clear whether this term arises from the inaccuracy of our analysis or it is inevitable for ARVDM, i.e., the lower bound of the error contains this term. Unfortunately, we find that this is inevitable. We will show this at the end of this section and discuss how to improve this term. Effect of Error Accumulation Eqn. (4.4) shows the error of the (K + 1)-th short video clip with length generated by ARVDM. The result shows that the error of this short clip still contains the error of the initial stage and all AR steps, which include the noise initialization error, the score estimation error, and the discretization error. Different from Eqn. (4.1), Eqn. (4.4) only measures the quality of short clip. Thus, there is no memory bottleneck term, which represents the inconsistency across multiple AR steps. As shown in Figure 1, FVD of the short clips increases with the increasing AR steps, which represents the image quality degradation and the unnatural motion. In the following, we would like to justify the inevitability of the memory bottleneck term and discuss how to improve it. This information-theoretic barrier arises from the fact that the score function sθ does not include all the historical data into account to denoise the current frames. This problem can be simplified to the following setting, which is shown in Figure 4. The goal is to learn the joint distribution (X, Y, Z) of random variables X, Y, Z, which represent Past, 10 Figure 5: Network structure of adding information of previous frames into each AR step. Here wm is the number of past frames provided for the denoising network, and is the number of frames to denoise. The superscript refers to the past frames and actions (memory). Input and Output, respectively. However, ARVDM only use the Input to predict Output, which means that we only utilize the partial observations. Thus, we simplify the training and prediction of ARVDM as the problem where we only have access to the samples of the marginalization distribution, i.e., {Xi, Yi}N i=N +1 (Y, Z). This corresponds to the fact that the score functions sθ is only trained on part of the long videos. We would like to derive an estimate (cid:98)P of based on the samples DN (P ) = {Xi, Yi}N i=N +1. Here, (cid:98)P depends on both the training and inference processes of ARVDMs. For ease of demonstration, we consider the case where X, Y, takes values in set Ω = {0, 1}. i=1 (X, ) and {Yi, Zi}2N i=1 {Yi, Zi}2N Theorem 4.5. For [0, 1], we define the conditional mutual information-constrained distribution set as S(s) = {P (X, Y, Z) P(Ω3), I(X; ZY ) s}. Then for any N, S(s), and any estimate (cid:98)P of the distribution derived from DN (P ), we have that inf (cid:98)P σ(DN (P )) sup S(s) (cid:0) KL(P (cid:98)P ) s2/2(cid:1) 0.5. The proof is provided in Appendix M. This theorem implies that (KL(P (cid:98)P ) 0.5I 2(X; ZY )) (KL(P (cid:98)P ) 0.5s2) 0.5. Thus, the conditional mutual information is an informationtheoretic barrier of the ARVDM. We note that this lower bound is independent of . This implies that no matter how large is , we will always suffer from this error. We leave the analysis of the higher-order terms that depend on as future works (Tan et al., 2014; Watanabe et al., 2015). Next, we would like to discuss how to mitigate the memory bottleneck. Intuitively, if the input contains more information about the past, this term should decrease. This is captured by the following result. Proposition 4.6 (Blessing of Historical Information). For any random variables X, Y, Ω (representing Past, Input and Output, respectively) and any measurable function : Ω Ω, we have that I(X; ZY, g(X)) I(X; ZY ). 11 Table 1: Successful retrieval rate in DMLab Memory Length Compression Network Memory Budget 0-16 16-48 48-112 Prepend 16 48 112 48 48 48 48 Channel Concat 16 48 112 48 48 48 48 N/A N/A N/A joint joint joint joint N/A N/A N/A modulated modulated modulated modulated N/A N/A N/A 8 16 32 48 N/A N/A N/A 8 16 32 48 0.52 0.44 0.48 0.28 0.36 0.44 0. 0.24 0.16 0.12 0.16 0.20 0.24 0.28 0.03 0.32 0.28 0.22 0.25 0.28 0.34 0.03 0.19 0.13 0.16 0.19 0.22 0.22 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.08 0.00 0.00 0.00 0.00 Cross Attention 112 N/A N/A 0.00 0.00 0. The proof is in Appendix N.1. This result shows that adding more information from the past will monotonically decrease the memory bottleneck term, and we call this Blessing of Historical Information. There are several ways to construct proper g(). The most direct way is to let the denoising networks take or part of as the input. We note that there can be redundant information in all the past frames. Thus, for efficient inference, it is also important to compress and distill the important information. We will explore this method in the next section. The function g() can also be induced by an agent framework. For example, an agent can summarize all the past frames and extract the relevant information for the current iteration. The summarization and the extracted information can be in the language space or the image space, then the extracted information will be the condition of the denoising network."
        },
        {
            "title": "5 Mitigation of the Memory Bottleneck",
            "content": "In this section, we modify the denoising network structures to take past frames as inputs. As discussed in Section 4, this can decrease the memory bottleneck term if the network can efficiently process this information. Network Structure We design three structures to inject the information of the past frames into the current AR generation based on Unet: prepending, channel concatenation, and cross attention. Figure 5 indicates three structures we adopt. We test them on action-condition generation tasks. There is one action for each frame. The prepending structure directly prepends the past wm frames to the currently denoised frames. The past actions are utilized as the 12 Table 2: SSIM of generated videos in Minecraft Context Length Compression Network Memory Budget 0-16 16-48 48-112 Prepend 16 48 112 48 48 48 48 Channel Concat 16 48 112 48 48 48 48 N/A N/A N/A joint joint joint joint N/A N/A N/A modulated modulated modulated modulated N/A N/A N/A 8 16 32 48 N/A N/A N/A 8 16 32 48 0.75 0.74 0.72 0.63 0.67 0.70 0.72 0.61 0.58 0.59 0.54 0.56 0.57 0.57 0.39 0.62 0.60 0.53 0.55 0.57 0. 0.38 0.57 0.56 0.53 0.54 0.56 0.57 0.37 0.35 0.55 0.38 0.37 0.38 0.35 0.38 0.37 0.51 0.36 0.35 0.36 0.37 control signal to transform the past frames via layer norm to share the same feature space with the frames to denoise. During training, we will only denoise the current frames. Channel concatenation directly merges the information of the past frames into the frames to denoise at the input of the denoising network. Here we first concatenate the past frames and their action embeddings. Then they are added to the frames to denoise via convolution in channel-wise manner. Similar to the channel concatenation, the cross attention also concatenates the past frames and actions. Then it incorporates the information of these frames and actions into the frames to denoise via cross attention module. In addition to these structures, we also compress the past frames and actions to achieve the trade-off between performance and efficiency. Here we use several transformer blocks to compress them by only using the last several tokens of the output. We have two methods for compression, joint and modulated, which are detailed introduced in Appendix G. This compression can be combined with any of the three structures we introduce. The comparison with existing works is provided in Appendix F. Experimental Setting We train and implement the models with = = 16. We focus on the action-conditioned generation of two video games: DMLab (Yan et al., 2023), and Minecraft. We focus on measuring the performance of the memory bottleneck of our models. In DMLab, we adopt the consistency of the scene features, including the color of the floor and walls and the appearance of the decorative picture on the wall as the performance metric, shown in Figure 7. We provide the trained model with the past frame and actions with various lengths. Then we test whether the model can successfully retrieve the scene features. For example, as shown in Figure 6, we provide the ground-truth memory (observations along the red line) to VDM. We would like to retrieve (predict) the correct scene (observations along the blue line) according to the frame and action history. In Minecraft, we adopt SSIM (Wang et al., 2004) to measure the similarity of scenes due to the high complexity of scenes. Similar to Figure 6: An example of the memory retrieval task in DMLab. Figure 7: Retrieval examples of DMLab. In contrast to Oasis in Figure 2, which fails to generate consistent scene, the improved models can retrieve the wall/floor color, texture, and decoration (e.g. paint on the wall) from previously seen mazes. DMLab setting, past frames and actions are provided to the model to predict the next frames, which should be consistent with previous frames. More details are in Appendix J. Experimental Results For DMLab, the successful retrieval rate is reported in Table 1. For the uncompressed memory, we consider three memory lengths: 16, 48, 112. The experimental results show that the model can hardly retrieve the scenes beyond the memory length, which is also reflected by our analysis in Section 4. Among all the structures, prepending has the best performance. In contrast, we note that cross attention and its variants do not work in this setting. In addition to the structure we indicate in Figure 5, we also test two variants of it. The first variant is the control-net-like structure. It first processes the past frames and actions via an encoder of Unet. Then the output features of this encoder are added to the decoder part via cross-attention. The second variant is the combination of the structure reported in Figure 5 and the first variant. However, these three structures all fail to mitigate the memory bottleneck, i.e., their successful retrieval rates in DMLab are 0. The compression results in Table 1 show that there are many redundancies in the past frames and actions. When compressing memory with lengths 48 to 8, the performance does Figure 8: Retrieval examples of Minecraft. The memory-enhanced model accurately recalls and retrieves scenes previously encountered in earlier views. Figure 9: Correlation between Memory Bottleneck and Error Accumulation. not drop in the same ratio. In addition, the compression of the memory tends to evenly allocate the retrieval ability across different memory lengths. Similar insights are shared by the experimental results of Minecraft in Table 2. When the value of SSIM is below 0.4, the compared two scenes are very different, representing the failure of retrieval. More details can be found in the benchmarking in Appendix I. In addition, we also observe correlation between the memory bottleneck and the error accumulation. Figure 9 plots the error accumulation (Max. PSNR decay, explained in Appendix H) and the memory bottleneck (Average successful retrieval rate over 0 16 and 16 48) for compressions in prepending. When the retrieval rate is higher (the memory bottleneck is better mitigated), the decay of PSNR is larger (the quality of the video clip decays faster). This is intuitive since mitigating the memory bottleneck requires extracting information from past frames, which could extract the noise of them."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we studied some fundamental phenomena shared by most ARVDMs. We proposed unified generation framework of ARVDMs. Based on this, we derived the error analysis of both generated long videos and short video clips. The memory bottleneck and error accumulation are two phenomena proved in the theoretical analysis and experimental results. We also showed that memory bottleneck is inevitable via lower bound. Then we provided some structures to mitigate the memory bottleneck and achieved the trade-off between performance and efficiency 15 via compression. In addition, we observe correlation between memory bottleneck and error accumulation. For the future work, from the theoretical perspective, it will be beneficial to provide lower bound of the error accumulation. In addition, our lower bound for memory bottleneck is not tight and can be further improved. From the experimental perspective, we mainly explore the compression via transformer in this work. We will design the compression module via state space models, such as Mamba (Gu and Dao, 2023), in the future."
        },
        {
            "title": "References",
            "content": "Anderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12 313326. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A. et al. (2023a). Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127. Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S. and Kreis, K. (2023b). Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Che, H., He, X., Liu, Q., Jin, C. and Chen, H. (2024). Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769. Chen, B., Monso, D. M., Du, Y., Simchowitz, M., Tedrake, R. and Sitzmann, V. (2024a). DifarXiv preprint fusion forcing: Next-token prediction meets full-sequence diffusion. arXiv:2407.01392. Chen, H., Lee, H. and Lu, J. (2023a). Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning. PMLR. Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C. (2024b). Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. and Shan, Y. Chen, M., Huang, K., Zhao, T. and Wang, M. (2023b). Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning. PMLR. Chen, S., Chewi, S., Lee, H., Li, Y., Lu, J. and Salim, A. (2024c). The probability flow ode is provably fast. Advances in Neural Information Processing Systems, 36. Chen, S., Chewi, S., Li, J., Li, Y., Salim, A. and Zhang, A. R. (2022). Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215. Chen, T.-S., Lin, C. H., Tseng, H.-Y., Lin, T.-Y. and Yang, M.-H. (2023c). Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404. 16 Chen, X., Wang, Y., Zhang, L., Zhuang, S., Ma, X., Yu, J., Wang, Y., Lin, D., Qiao, Y. and Liu, Z. (2023d). Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations. Chewi, S., Erdogdu, M. A., Li, M., Shen, R. and Zhang, M. S. (2024). Analysis of langevin monte carlo from poincare to log-sobolev. Foundations of Computational Mathematics 151. Decart, E. (2024). Oasis: universe in transformer. https://oasis-model.github.io/. Gao, Y., Huang, J., Sun, X., Jie, Z., Zhong, Y. and Ma, L. (2024). Matten: Video generation with mamba-attention. arXiv preprint arXiv:2405.03025. Gautschi, W. (2011). Numerical analysis. Springer Science & Business Media. Gu, A. and Dao, T. (2023). Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752. Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D. and Dai, B. (2023). Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725. Henschel, R., Khachatryan, L., Hayrapetyan, D., Poghosyan, H., Tadevosyan, V., Wang, Z., Navasardyan, S. and Shi, H. (2024). Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J. et al. (2022a). Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303. Ho, J., Jain, A. and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33 68406851. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M. and Fleet, D. J. (2022b). Video diffusion models. Advances in Neural Information Processing Systems, 35 86338646. Jin, Y., Sun, Z., Li, N., Xu, K., Jiang, H., Zhuang, N., Huang, Q., Song, Y., Mu, Y. and Lin, Z. (2024). Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954. Karras, J., Holynski, A., Wang, T.-C. and Kemelmacher-Shlizerman, I. (2023). Dreampose: Fashion image-to-video synthesis via stable diffusion. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE. Kim, J., Kang, J., Choi, J. and Han, B. (2024). FIFO-Diffusion: Generating infinite videos from text without training. arXiv preprint arXiv:2405.11473. Lin, B., Ge, Y., Cheng, X., Li, Z., Zhu, B., Wang, S., He, X., Ye, Y., Yuan, S., Chen, L. et al. arXiv preprint (2024). Open-sora plan: Open-source large video generation model. arXiv:2412.00131. Liu, V., Long, T., Raw, N. and Chilton, L. (2023). Generative disco: Text-to-video generation for music visualization. arXiv preprint arXiv:2304.08551. 17 Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C. and Zhu, J. (2022). Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35 57755787. Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y.-F., Chen, C. and Qiao, Y. (2024a). Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048. Ma, Y., He, Y., Cun, X., Wang, X., Chen, S., Li, X. and Chen, Q. (2024b). Follow your pose: Pose-guided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence. Mo, S. and Tian, Y. (2024). Scaling diffusion mamba with bidirectional ssms for efficient image and video generation. arXiv preprint arXiv:2405.15881. Ren, W., Yang, H., Zhang, G., Wei, C., Du, X., Huang, W. and Chen, W. (2024). Consisti2v: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324. Ruhe, D., Heek, J., Salimans, T. and Hoogeboom, E. (2024). Rolling diffusion models. arXiv preprint arXiv:2402.09470. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O. et al. (2022). Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792. Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S. and Poole, B. (2020). ScorearXiv preprint based generative modeling through stochastic differential equations. arXiv:2011.13456. Suli, E. and Mayers, D. F. (2003). An introduction to numerical analysis. Cambridge university press. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P. and Yuan, Z. (2024). Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525. Tan, V. Y. et al. (2014). Asymptotic estimates in information theory with non-vanishing error probabilities. Foundations and Trends in Communications and Information Theory, 11 1184. Valevski, D., Leviathan, Y., Arar, M. and Fruchter, S. (2024). Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837. Verdu, S. (2014). Total variation distance and the distribution of relative information. In 2014 information theory and applications workshop (ITA). IEEE. Wainwright, M. J. (2019). High-dimensional statistics: non-asymptotic viewpoint, vol. 48. Cambridge university press. Wang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X. and Zhang, S. (2023). Modelscope textto-video technical report. arXiv preprint arXiv:2308.06571. 18 Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q. et al. (2024a). Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869. Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P. et al. (2024b). Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision 120. Wang, Y., Xiong, T., Zhou, D., Lin, Z., Zhao, Y., Kang, B., Feng, J. and Liu, X. (2024c). Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757. Wang, Z., Bovik, A. C., Sheikh, H. R. and Simoncelli, E. P. (2004). Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process., 13 600612. https://doi.org/10.1109/TIP.2003.819861 Wang, Z., Yuan, Z., Wang, X., Li, Y., Chen, T., Xia, M., Luo, P. and Shan, Y. (2024d). Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers. Watanabe, S., Kuzuoka, S. and Tan, V. Y. (2015). Nonasymptotic and second-order achievability bounds for coding with side-information. IEEE Transactions on Information Theory, 61 15741605. Weng, W., Feng, R., Wang, Y., Dai, Q., Wang, C., Yin, D., Zhao, Z., Qiu, K., Bao, J., Yuan, Y. et al. (2024). Art-v: Auto-regressive text-to-video generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Wu, T., Fan, Z., Liu, X., Zheng, H.-T., Gong, Y., Jiao, J., Li, J., Guo, J., Duan, N., Chen, W. et al. (2023). Ar-diffusion: Auto-regressive diffusion model for text generation. Advances in Neural Information Processing Systems, 36 3995739974. Xing, J., Xia, M., Zhang, Y., Chen, H., Yu, W., Liu, H., Liu, G., Wang, X., Shan, Y. and Wong, T.-T. (2025). Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision. Springer. Xing, Z., Feng, Q., Chen, H., Dai, Q., Hu, H., Xu, H., Wu, Z. and Jiang, Y.-G. (2024). survey on video diffusion models. ACM Computing Surveys, 57 142. Yan, W., Hafner, D., James, S. and Abbeel, P. (2023). Temporally consistent transformers for video generation. In International Conference on Machine Learning. PMLR. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G. et al. (2024). Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072. Yuan, H., Zhang, S., Wang, X., Wei, Y., Feng, T., Pan, Y., Zhang, Y., Liu, Z., Albanie, S. and Ni, D. (2024). Instructvideo: instructing video diffusion models with human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19 Zhang, S., Wang, J., Zhang, Y., Zhao, K., Yuan, H., Qin, Z., Wang, X., Zhao, D. and Zhou, J. (2023). I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145. Zhang, X., Liu, Y., Chang, H., Schramm, L. and Boularias, A. (2024). Autoregressive action sequence learning for robotic manipulation. arXiv preprint arXiv:2410.03132. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T. and You, Y. (2024). Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404."
        },
        {
            "title": "A Notations",
            "content": "We let [N ] := {1, , }. For two random variables and with distributions PX and PY defined on Ω, we let KL(X ) = KL(PX PY ) = (cid:82) dPX to denote the KLdivergence between the distributions of and . For three random variables X, Y, Z, the conditional mutual information between X, conditioned on is denoted as I(X; Z) = EZ[KL(PX,Y PXZPY Z)]. For four random variables X0, X1, Y0, Y1, we adopt KL(X1X0 Y1Y0) = (cid:82) Ω2 dPX(X0 = u, X1 = v) log(PX(X1 = vX0 = u)/PY (Y1 = vY0 = u)) to denote the KLdivergence between the conditional distributions of and .We write to mean Cy for an absolute constant > 0. Ω log dPX dPY"
        },
        {
            "title": "B Related Works",
            "content": "Video Diffusion Models Given the tremendous successes of diffusion models in image generation, line of initial Text-To-Video (T2V) works proposed to treat the whole video as vector and learn the joint score functions (Ho et al., 2022b; Blattmann et al., 2023a; Ma et al., 2024a; Chen et al., 2024b; Guo et al., 2023; Singer et al., 2022; Yuan et al., 2024). To capture the correlation between frames, these works designed temporal attention module, which applies attention module across the temporal dimension. The whole network consists of the temporal attention layers and the spatial attention layers (Wang et al., 2023; Blattmann et al., 2023b; Wang et al., 2024b; Xing et al., 2025). In order to make use of the fine-grained spatial-temporal relationship of pixels, line of works adopts the 3D attention module as the basis of the denoising networks (Lin et al., 2024; Zheng et al., 2024; Yang et al., 2024). In addition to the 3D attention, some works also explore the combination of the Mamba and the attention (Gao et al., 2024; Mo and Tian, 2024). Except these T2V works, the generation of videos can also be conditioned on images (Zhang et al., 2023; Chen et al., 2023d; Ren et al., 2024), pose (Karras et al., 2023; Ma et al., 2024b), motion (Chen et al., 2023c; Wang et al., 2024d), and sound (Liu et al., 2023). Auto-Regressive Video Models There is thread of work adopting the AR framework to generate long videos. These works adopt two kinds of methods: adapting diffusion to the AR framework, and tokenizing the frames then using next-token-prediction. We first introduce some works of the first kind. FIFO-Diffusion (Kim et al., 2024) is training-free method that makes use of the pretrained models to denoise the frames with different noise levels. To mitigate the training-inference gap, they adopt latent partition to decrease the difference between the noise levels. AR-Diffusion (Wu et al., 2023), Rolling Diffusion Models (Ruhe et al., 2024), Diffusion-forcing (Chen et al., 2024a), and Pyramidal flow matching (Jin et al., 2024) train the networks to denoise the frames with different noise levels. In contrast, ART-V (Weng et al., 2024), StreamingT2V (Henschel et al., 2024), GameNGen (Valevski et al., 2024), and GameGen-X (Che et al., 2024) denoise the frames with the same noise level. For the second kind of work, people tokenize the frames and train the network to conduct next-token prediction. LlamaGen (Sun et al., 2024), Emu3 (Wang et al., 2024a), and Loong (Wang et al., 2024c) all generate visual tokens according to their spatial-temporal rank. Our work provides an analysis of the methods of the first kind."
        },
        {
            "title": "C Discussion of Extensions",
            "content": "In this section, we discuss the extension of the constant pace and scalar function g() in Eqn. (3.2). In fact, we can extend the constant pace setting to the various paces setting by defining new extend time as t(t) = (α1t + δ1, α2t + δ2, , αwt + δw). Here the coefficients αi for [w] describe the different paces of different frames. To extend the scale function g(t) to the matrix form G(X1:2, t), we adopt the same method as (Song et al., 2020). In fact, we have that (cid:16) dX 1:w = (X 1:w, t) [G(X + G(X 1:w, t)d (cid:101)Bt 1:w. 1:w, t)G(X 1:w, t)] G(X 1:w, t)G(X 1:w, t) log X(X 1:wX 0 R) (cid:17) dt"
        },
        {
            "title": "D Demos of Existing Methods and Models",
            "content": "Outpainting The building exhibits unnatural distortions, and the number of pillars is inconsistent with the initially generated frames. Compared to the initial frames, the skeletal structure of the flyer is significantly less distinct. One of the cheerleaders legs abruptly transforms into facial feature, resulting in an unnatural deformation. 22 FIFO The STOP sign exhibits jittering, followed by progressive deformation, and eventually disappears in the later frames. The largest loaf of bread in the showcase abruptly fragments into multiple smaller loaves in an unnatural manner. Following an unnatural blurring effect, the buss color undergoes an unexpected change. StreamingT2V The baseball players appearance is inconsistent at the beginning, and the motion sequence for hitting the baseball appears unnatural. The girls appearance fluctuates continuously throughout the sequence, and noticeable facial distortions emerge. The color of the monkeys fur changes inconsistently. 23 Oasis After the camera tilts up and down, sections of the stone walls unexpectedly transform into shoreline. single lake unexpectedly splits into two distinct lakes following vertical camera movement. hill and an additional lake abruptly emerge near the grassy area after the camera shifts up and down."
        },
        {
            "title": "E Demos of Our Methods",
            "content": "Figure 10: Recall demonstrations on DMLab. The left frames represent the expected ground truth, while the right frames, outlined with red square, are generated by the model. 25 Figure 11: Recall demonstrations on Minecraft. The left frames represent the expected ground truth, while the right frames, outlined with red square, are generated by the model. The first 4 frames without red squares are provided context. 26 Comparison of Che et al. (2024) and Our Channel"
        },
        {
            "title": "Concatenation Structure",
            "content": "We note that recent work Che et al. (2024) also adopts channel concatenation method to let the diffusion models take into account some past frames. However, their work focuses on extending an image-generation diffusion model to generate videos. In contrast, we focus on the VDMs. As consequence, our method adopts different way of processing past actions. We use several transformer blocks to merge the action information and the frame information, which is shown effective in our results. In contrast, Che et al. (2024) does not explicitly merge the frame and action information, since the image generation diffusion models naturally take the action condition via cross attention. In addition, since the VDM will denoise several frames simultaneously, we add the past information to all the frames to denoise."
        },
        {
            "title": "G Explanations of the Compression Network",
            "content": "Here we adopt two methods to compress the past frames and actions. The joint method, which is shown in the left part of Figure 12, processes the concatenation of the frame and actions jointly. The network consists of the feedforward module and spatial and temporal attention modules. The output is also the concatenation of compressed frames and actions. The modulated method, which is shown in the right part of Figure 12, utilizes the actions to modulate the frames. In both methods, we only retain the last several frames and tokens as the final compressed memory. In the experiment, the prepending and channel concatenation structures respectively adopts the joint compression and modulated compression. The reason is that the prepending structure also requires compressed actions to transform the frames to different feature spaces, and the channel concatenation needs to merge the frame and action information. 27 Figure 12: The network structure adopted to compress the past frames and actions."
        },
        {
            "title": "H Error Accumulation in DMLabs and Minecraft",
            "content": "This section indicates the error accumulation of the various compression sizes in prepending structure. The Max. PSNR difference is the difference of the PSNRs between 0 16 and 48 64, indicating the rate of error accumulation. Here the column names i-j indicates the metrics are calculated from i-th frames to j-th frames from generated video. Table 3: Error Accumulation on DMLab (PSNR) Network Memory Budget 016 1632 3248 4864 8 16 32 48 10.24 10.14 10.46 10. 9.47 8.81 8.19 8.00 8.83 8.24 7.82 7.58 7.88 7.39 6.83 6.76 28 Table 4: Error Accumulation on Minecraft (PSNR) Network Memory Budget 016 1632 3248 4864 8 16 32 48 15.25 15.47 15.36 15.46 13.43 12.92 12.64 12.31 11.39 11.10 11.01 10.57 10.29 9.98 9.73 9."
        },
        {
            "title": "I SSIM Values Benchmark For Minecraft",
            "content": "Figure 13: Minecraft Example Pairs from Minecraft Across Varying SSIM Score Ranges We analyze approximately 1,000 Minecraft video trajectories and compute the Structural Similarity Index (SSIM) for pairs of frames by iterating through these trajectories. Our observations are as follows: SSIM < 0.4: The visual similarity between frames is generally weak, making it difficult to discern clear relationship between them. 0.4 SSIM < 0.9: The two frames typically share an overall resemblance in scene composition but exhibit noticeable differences in details, such as slight variations in camera angle or object placement. SSIM 0.9: The frames are nearly identical, with only minor pixel-level differences that are imperceptible to the human eye."
        },
        {
            "title": "J Experimental Details",
            "content": "Training. We use the Adam optimizer (β1, β2) = (0.9, 0.999) with no weight decay. During training, gradients are clipped to maximum norm of 1.0, and we additionally apply noise clipping at norm of 6.0. For the Minecraft experiments, we adopt the VAE from stable-diffusion3-medium to enable latent diffusion. Across all experiments, we use fused SNR reweighting with cumulative SNR decay of 0.96, and train with v-prediction as our target. Our denoiser is 3D UNet; its basic block includes ResNet block alongside spatial and temporal attention modules. Both the downsampling and upsampling paths comprise 4 blocks each. Data. For the DMLab experiments, we follow the TECO (Yan et al., 2023) training split and select evaluation cases from the corresponding validation split. For the Minecraft experiments, we render scenarios using MineRL while preserving the original frame rate (i.e., no frame skipping). Formal statement of Assumption 4.3 We provide the formal statement of Assumption 4.3 here. Assumption K.1 (Score Estimate Error (formal)). The average score estimation error evaluated at the discretization steps is upper bounded by ϵ2 est > 0 for both the initialization and AR stages, i.e., the following holds for any N. Minit(cid:88) n=1 (tinit tinit n1)E (cid:104)(cid:13) (cid:13) log tinit (X tinit 1:i0 ) sθ(X tinit 1:i0 2(cid:105) )(cid:13) , tinit (cid:13) ϵ2 est (tI 1 tO 1 )1 (tar tar n1)E (cid:104)(cid:13) (cid:13) log tar (X tar k+1:k+w 0 Rk+1 ) sθ(X tar k+1:k+w, tar , 0 Rk+1 2(cid:105) )(cid:13) (cid:13) ϵ est. Mar(cid:88) n=1 We note that the estimation error of the diffusion models is averaged along the whole trajectory of the diffusion process. Proof of Theorem 4. We provide the proof of Theorem 4.4 in this section. We respectively prove Eqn. (4.1) and (4.4) in Sections L.1 and L.2. L.1 Proof of Eqn. (4.1) Our proof consists of five main procedures. Decomposition of the concatenation of all the noisy and clean frames. Decomposition of the KL-divergence of all the frames into the KL-divergence of each auto-regressive generation step. Bounding the denoising error in the KL-divergence decomposition. Bounding the initialization error in the KL-divergence decomposition. 30 Figure 14: The examples of G() and H(). Concluding the proof. Step 1: Decomposition of the concatenation of all the noisy and clean frames. During each implementation of Auto-Regressive Step, we denoise the random vectors tI i+j}w j=1. We define all the already generated random vectors, including inputs and outputs {Y of Auto-Regressive Step, by this implementation as (cid:16)(cid:8)Y tI i+j (cid:9)w j= (cid:17) = {X 0 1:i} (cid:8)X tI 1:i+j1 (cid:9) jSI for , and (cid:16)(cid:8)Y tI i+j (cid:9)w j=1 (cid:17) = for < , (L.1) where the set LI is the set of the starting indexes of each noise input noise level. It is defined as follows. = (cid:8)sI(cid:0)tI (cid:1) [w](cid:9), and sI(cid:0)tI (cid:1) = min (cid:8)j tI = tI i, [w](cid:9), i) is the smallest index whose input noise level is equal to tI where sI(tI i. The examples of this definition of H() are provided in Figure 14. After each implementation of Auto-Regressive j=1. We define all the generated random vectors, including inputs and Step, we output {Y outputs of Auto-Regressive Step, after this implementation as tO i+j}w (cid:16)(cid:8)Y tO i+j (cid:9)w j= (cid:17) = (cid:16)(cid:8)Y tI i+j (cid:9)w j= (cid:17) (cid:8)Y tI i+j j=1 (cid:8)Y (cid:9)w tO i+j (cid:9)w j=1. (L.2) With these definitions, we can prove the following relationship between them. Proposition L.1. For any , we have that (cid:16)(cid:8)Y (cid:16)(cid:8)Y = (cid:9)w (cid:17) tO i+j j=1 (cid:17) tI i+j (cid:9)w j=1 (cid:8)Y tI i+j (cid:9)w j=1 . The proof of this proposition is provided in Appendix N.2. Step 2: Decomposition of the KL-divergence of all the frames into the KLdivergence of each auto-regressive generation step. 31 Then we would like to decompose the upper bound of the KL-divergence between videos. In fact, we have that KL(X 0 1:K 0 1:K) (cid:18) (cid:16)(cid:8)X tO (K1)+j KL (cid:18) = KL (cid:16)(cid:8)X tO 0+j (cid:9)w j=1 (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) (cid:9)w j= (cid:16)(cid:8)Y (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) (cid:16)(cid:8)Y (cid:9)w tO 0+j j= tO (K1)+j (cid:17)(cid:19) (cid:17)(cid:19) (cid:9)w j=1 (cid:18) KL {X (cid:18) KL {X + + K2 (cid:88) k=1 K2 (cid:88) k=1 tI k+j}w j=w+1 (cid:16)(cid:8)X (cid:12) (cid:12) (cid:12)G tO (k1)+j (cid:9)w j=1 (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) {Y tI k+j}w j=w+1 (cid:16)(cid:8)Y (cid:12) (cid:12) (cid:12)G tO (k1)+j (cid:9)w j=1 (cid:17)(cid:19) tO k+j}w j=1 (cid:16)(cid:8)X (cid:12) (cid:12) (cid:12)G tO (k1)+j (cid:9)w j=1 (cid:17) {X tI k+j}w j=w+1 {Y tO k+j}w j=1 (cid:16)(cid:8)Y (cid:12) (cid:12) (cid:12)G tO (k1)+j (cid:9)w j=1 (cid:17) {Y tI k+j}w j=w+1 (cid:19) (cid:13) (cid:13) (cid:13) (cid:13) = (I) + (II) + (III), (L.3) where the definition of the KL-divergence between conditional distributions is provided in Appendix A, the inequality results from the data processing inequality, and the first equality results from the chain-rule of KL-divergence and Proposition L.1. To see this, we have that (cid:16)(cid:8)Y tO (k1)+j (cid:9)w (cid:17) {Y (cid:16)(cid:8)Y (cid:16)(cid:8)Y (cid:16)(cid:8)Y = = = j=1 (cid:9)w j= (cid:9)w (cid:17) (cid:17) tO k+j tO k+j j=1 (cid:17) tO k+j (cid:9)w j=1 , tI k+j}w tI k+j (cid:8)Y (cid:8)Y tI k+j tO k+j}w j=w+1 {Y tI (cid:9)w k+j}w j=1 {Y tO (cid:9)w k+j}w j=1 {Y j=1 j=1 j=w+1 {Y tO k+j}w j=1 where the first equality results from Proposition L.1, the second equality results from combining the middle two terms, and the last equality results from the definition of in Eqn. (L.2). We note that in the right-hand side of Eqn. (L.3), the term (I) corresponds to the error from the initialization stage of Algorithm 1, the term (II) corresponds to the error from the noise initialization (Line 1 of Algorithm 3), and the term (III) corresponds to the error from the denoising step (Line 3 of Algorithm 3). We would like to further decompose the term (II). We first transform each term in (II) as follows. (cid:18) KL {X tO k+j}w j=1 (cid:16)(cid:8)X (cid:12) (cid:12) (cid:12)G tO (k1)+j (cid:9)w (cid:17) {X tI k+j}w j=w+1 {Y tO k+j}w j=1 (cid:12) (cid:12) (cid:12)G (cid:16)(cid:8)Y tO (k1)+j (cid:9)w j= (cid:17) {Y tI k+j}w j=w+1 (cid:19) (cid:18) = KL {X tO k+j}w j=1 (cid:12) (cid:12) (cid:12)H (cid:16)(cid:8)X tO k+j (cid:9)w (cid:17) {X tI k+j}w j= {Y tO k+j}w j=1 (cid:12) (cid:12) (cid:12)H (cid:16)(cid:8)Y tO k+j (cid:9)w j=1 (cid:17) {Y tI k+j}w j= (cid:19) , (L.4) j=1 (cid:13) (cid:13) (cid:13) (cid:13) j=1 (cid:13) (cid:13) (cid:13) (cid:13) 32 where the equality follows from Proposition L.1. From the denoising step in Eqn. (3.3), we have the following Markov chain. (cid:16)(cid:8)Y (cid:9) (cid:8)Y (L.5) (cid:9)w (cid:17) (cid:9)(cid:8)YR (cid:8)YR0 tO k+j tI k+j tO k+j (cid:9)w j=1(cid:8)Y (cid:9)w j=1. j=1 k+1 k+1 Thus, we have that (cid:18) KL {X tO k+j}w j=1 (cid:12) (cid:12) (cid:12)H (cid:16)(cid:8)X tO k+j (cid:9)w j=1 (cid:17) {X tI k+j}w j= {Y tO k+j}w j=1 (cid:12) (cid:12) (cid:12)H (cid:16)(cid:8)Y tO k+j (cid:9)w j=1 (cid:17) {Y tI k+j}w j= (cid:19) (cid:13) (cid:13) (cid:13) (cid:13) tO k+j}w j=1 (cid:12) (cid:12) (cid:12)H (cid:16)(cid:8)X tO k+j (cid:9)w j=1 (cid:17) {X tI k+j}w j=1 (cid:18) = KL {X (cid:18) = {X tO k+j}w (cid:18) j=1; (cid:16)(cid:8)X tO k+j (cid:9)w j=1 (cid:17) (cid:8)XR0 k+1 + KL {X tO k+j}w j=1 (cid:12) (cid:12) (cid:12) (cid:8)XR0 k+1 (cid:9) {X tI k+j}w j=1 tO k+j}w j=1 (cid:13) (cid:13) (cid:13) (cid:13) {Y tO k+j}w j=1 (cid:12) (cid:12) (cid:12) (cid:9) {Y tI k+j}w j= (cid:19) (cid:9) {X tI k+j}w j=1 (cid:19) k+ k+1 (cid:9) (cid:8)YR0 (cid:12) (cid:12) (cid:8)XR0 (cid:12) (cid:12) (cid:13) (cid:13) (cid:13) (cid:13) {Y (cid:12) (cid:12) (cid:12) (cid:8)YR k+1 (cid:9) {Y tI k+j}w j=1 (cid:19) , (L.6) where the first equality results from the Markov chain in (L.5), and the second equality results from the definition of the conditional mutual information. Step 3: Bounding the denoising error in the KL-divergence decomposition. Eqn. (L.4) and (L.6) show that to bound the denoising error term (III) in Eqn. (L.3), we only need to bound the second term in the right-hand side of Eqn. (L.6). We first define the multiple noise-level version of Eqn. (3.2) and apply the change of variable as follows. (cid:101)X 1:w = (cid:18) 1 2 (cid:101)X 1:w + log t ( (cid:101)X 1:w 0 R) dt + dBt 1:w for tI 1 tO 1 . (cid:19) (L.7) Recall that we define the joint time as t(t) = (t, + tI We define the probability of (cid:101)X change of variable to Eqn. (3.3). 1:w as (cid:101)P X. Then we have that (cid:101)P 2 tI 1, , + tI = t 1) = (t1, , tw). . We also applied the tI (cid:101)Y 1:w = (cid:18) 1 ar (cid:101)t 1:w + sθ( (cid:101)Y (cid:101)Y ar (cid:101)t ar 1:w, , 0 R) (cid:101)t (cid:19) dt + dBt 1:w for (cid:101)tar (cid:101)tar n+1. (L.8) In the following proof of this step, we will omit the symbols 0 since all the proof in this step is conditioned on them. We define the probability of (cid:101)Y and have that (cid:101)P Conditioned on the value at t, the conditional distribution of (cid:101)X for ease of notation, 1:w as (cid:101)P . For notation simplicity, we define tI = LO and tO = LI. 1:w are denoted as 1:w and (cid:101)Y Y = t and 0 33 tt and (cid:101)P tt , respectively. For any (cid:101)tar (cid:101)P (cid:101)tar n+1, Lemma O.1 shows that dt KL (cid:0) tt (a) (cid:13) (cid:101)P (cid:13) (cid:101)P = tt (a) (cid:101)P (cid:101)P (a) tt (a)(cid:1) (cid:20)(cid:13) (cid:13) (cid:13) (cid:13) log (cid:20)(cid:28) + tt (a) (cid:101)P (cid:101)P 1 tt (a) (cid:101)P (cid:101)P (a) (a) (cid:104)(cid:13) (cid:13) log t tt ( (cid:101)X (cid:101)P tt ( (cid:101)X (cid:101)P 1:wa) 1:wa) 2(cid:21) (cid:13) (cid:13) (cid:13) (cid:13) log t ( (cid:101)X 1:w 0 R) sθ( (cid:101)X ar (cid:101)t ar 1:w, , 0 (cid:101)t R), log (cid:29)(cid:21) tt ( (cid:101)X (cid:101)P tt ( (cid:101)X (cid:101)P 1:wa) 1:wa) ( (cid:101)X 1:w 0 R) sθ( (cid:101)X ar (cid:101)t ar 1:w, , 0 (cid:101)t R)(cid:13) (cid:13) 2(cid:105) , where the inequality results from Cauchy inequality. Then we have that KL (cid:0) tOtI (a) (cid:13) tOtI (cid:101)P (cid:13) (cid:101)P (a) (cid:13) = KL (cid:0) (cid:101)P Mar(cid:88) (cid:90) (cid:101)tar tOtI (a)(cid:1) tOtI (cid:13) (cid:101)P n=1 (cid:101)tar n1 ttI (a) (cid:101)P tI (cid:101)P (a) 1 2 (a)(cid:1) KL (cid:0) tItI (a) (cid:13) (cid:101)P tItI (cid:13) (cid:101)P (a)(cid:1) (cid:104)(cid:13) (cid:13) log t ( (cid:101)X 1:w 0 R) sθ( (cid:101)X ar 1:w, , 0 (cid:101)t R)(cid:13) (cid:13) 2(cid:105) dt. Thus, for the second term on the right-hand side of Eqn. (L.6), we have that (cid:18) KL {X tO k+j}w j=1 (cid:12) (cid:12) (cid:12) (cid:8)XR k+1 (cid:9) {X tI k+j}w j=1 (cid:13) (cid:13) (cid:13) (cid:13) {Y tO k+j}w j=1 (cid:12) (cid:12) (cid:12) (cid:8)YR0 k+1 (cid:9) {Y tI k+j}w j=1 (cid:19) 1 2 Mar(cid:88) (cid:90) tar n=1 tar n1 (cid:104)(cid:13) (cid:13) log X(X k+1:k+w 0 Rk+1 ) sθ(X tar k+1:k+w, tar , 0 Rk+ 2(cid:105) )(cid:13) (cid:13) dt. Then we would like to upper bound each term in the right-hand side of Eqn. (L.9). In fact, we have that (L.9) (cid:90) tar tar n1 (cid:104)(cid:13) (cid:13) log X(X k+1:k+w 0 Rk+ ) sθ(X tar k+1:k+w, tar , 0 Rk+1 2(cid:105) )(cid:13) (cid:13) dt 2 (cid:90) tar tar n1 (cid:104)(cid:13) (cid:13) log X(X k+1:k+w 0 Rk+1 ) log tar (X tar k+1:k+w 0 Rk+1 2(cid:105) )(cid:13) (cid:13) dt (cid:90) tar E (cid:104)(cid:13) (cid:13) log tar (X tar k+1:k+w 0 Rk+1 ) sθ(X tar k+1:k+w, tar , 0 Rk+1 2(cid:105) )(cid:13) (cid:13) dt + 2 (cid:90) tar = 2 tar n1 tar n1 (cid:104)(cid:13) (cid:13) log E X(X k+1:k+w 0 Rk+1 ) log tar (X tar k+1:k+w 0 Rk+1 2(cid:105) )(cid:13) (cid:13) dt + 2(tar tar n1)E (cid:104)(cid:13) (cid:13) log tar (X tar k+1:k+w 0 Rk+ ) sθ(X tar k+1:k+w, tar , 0 Rk+1 2(cid:105) , )(cid:13) (cid:13) (L.10) where the inequality results from the property of the ℓ2-norm. Then we use the following proposition to upper bound the first term in the right-hand side of Eqn. (L.10). 34 Proposition L.2. Let denote the marginal distribution function of the following stochastic process at time t. dX = 1 2 tdt + dBt, 0 PX. If log is L-Lipschitz for [t0, t1] with t1 t0 1. Then for any t0 < t1, we have that (cid:104)(cid:13) (cid:13) log u(X u) log v(X v)(cid:13) (cid:13) 2(cid:105) dL2(v u). The proof is provided in Appendix N.4. This proposition implies that (cid:104)(cid:13) (cid:13) log X(X k+1:k+w 0 Rk+1 ) log tar (X tar k+1:k+w 0 Rk+1 2(cid:105) )(cid:13) (cid:13) dt (cid:90) tar tar n1 (cid:90) tar tar n1 wdL2(tar wdL2(tar t)dt n1)2. Combining Eqn. (L.9), (L.10), (L.11), we have that tar (cid:18) KL {X tO k+j}w j=1 (cid:12) (cid:12) (cid:12) (cid:8)XR0 k+1 (cid:9) {X tI k+j}w j=1 (cid:13) (cid:13) (cid:13) (cid:13) {Y tO k+j}w j= (cid:12) (cid:12) (cid:12) (cid:8)YR0 k+1 (cid:9) {Y tI k+j}w j= (cid:19) Mar(cid:88) n=1 (tar tar n1)E (cid:104)(cid:13) (cid:13) log tar (X tar k+1:k+w 0 Rk+1 ) sθ(X + wdL2 Mar(cid:88) (tar tar n1)2. tar k+1:k+w, tar n , 0 Rk+1 n=1 Step 4: Bounding the initialization error in the KL-divergence decomposition. We note that there are two initialization error terms in Eqn. (L.3), i.e., (I) and (II). Here (I) arises from Line 2 of Algorithm 1, while (II) arises from Line 1 of Algorithm 3. We first derive the bound for term (II). In fact, we have that j=w+1 (cid:16)(cid:8)X (cid:12) (cid:12) (cid:12)G tO (k1)+j (cid:9)w j=1 (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) {X k+j}w j=w+1 (cid:16)(cid:8)X tO (k1)+j (cid:9)w j= j=w+1 {Y tI k+j}w (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) (0, I) (cid:19) tI k+j}w (cid:18) (cid:18) KL {X = KL = KL (cid:18) (cid:12) (cid:12) (cid:12)G (cid:12) (cid:12) (cid:12)G (cid:16)(cid:8)X tO (k1)+j (cid:9)w j=1 (cid:17) {X 0 k+j}w j=w+1 (cid:19) (0, I) (cid:13) (cid:13) (cid:13) (cid:13) (cid:16)(cid:8)Y (cid:12) (cid:12) (cid:12)G tO (k1)+j (cid:9)w j=1 (cid:17)(cid:19) {X k+j}w (cid:18) j=w+1 KL {X k+j}w j=w+1 (cid:16)(cid:8)X (cid:12) (cid:12) (cid:12)G (cid:17) (cid:9)w j= {X 0 k+j}w j=w+1 k+j}w (cid:16)(cid:8)X (cid:12) (cid:12) (cid:12)G j=w+ tO (k1)+j (cid:13) (cid:13) (cid:13) (cid:13) j=w+1 (0, I) (cid:9)w j=1 (cid:17)(cid:19) (cid:19) (cid:18) KL {X k+j}w j=w+ = KL (cid:18) {X k+j}w j=w+1 (cid:16)(cid:8)X (cid:12) (cid:12) (cid:12)G (cid:12) (cid:12){X 0 (cid:12) tO (k1)+j (cid:9)w (cid:17) {X 0 k+j}w j=w+1 (0, I) k+j}w (cid:19) , (L.13) tO (k1)+j (cid:13) (cid:13) (cid:13) (cid:13) {X j=1 (cid:13) (cid:13) (cid:13) (cid:13) 35 (L.11) 2(cid:105) )(cid:13) (cid:13) (L.12) where the first equality results from Line 1 of Algorithm 3, and the last equality results from the following Markov chain for the forward process of {X k+j}w j=w+1{X 0 k+j}w j=w+1G (cid:16)(cid:8)X tO (k1)+j (cid:17) . (cid:9)w j= With Assumption 4.1 and Lemma O.3, we have that (cid:18) {X k+j}w j=w+1 KL (cid:12) (cid:12){X 0 (cid:12) k+j}w j=w+1 (cid:19) (0, I) (cid:13) (cid:13) (cid:13) (cid:13) (d + B2) exp(T ). (L.14) Combining these inequalities, we have that KL (cid:18) {X tI k+j}w (cid:12) (cid:12) (cid:12)G (d + B2) exp(T ). j=w+1 (cid:16)(cid:8)X tO (k1)+j (cid:9)w j=1 (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) {Y tI k+j}w j=w+1 (cid:16)(cid:8)Y (cid:12) (cid:12) (cid:12)G tO (k1)+j (cid:9)w j=1 (cid:17)(cid:19) (L.15) We then upper bound the term (I). From the procedures of Algorithm 2, we have that (cid:16)(cid:8)Y (cid:17)(cid:19) KL (cid:9)w (cid:9)w (cid:18) tO 0+j tO 0+j (cid:16)(cid:8)X (cid:18) (cid:8)X = KL KL (cid:0)X 0 1:i0 KL (cid:0)X 0 1:i0 j=1 (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) j=1 (cid:8)X (cid:9)w (cid:1) tI i+j (cid:13) (cid:13) 0 1:i0 (cid:12) (cid:13) (cid:12)X (cid:13) 0 1:i0 1:i0 (cid:9)w tO i+j j=1 (cid:8)Y tI i+j j=1 (cid:8)Y (cid:9)w tO i+j (cid:9)w j=1 (cid:19) j=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:12) (cid:12)Y 1:i0 (cid:1) + KL (cid:0)X 1:i (cid:13) (cid:13) 1:i0 (cid:1), (L.16) where the equality follows from the definition of in Eqn. (L.2), these two inequalities follows from the data processing inequality. For the first term in the right-hand side of Eqn. (L.16), we follow the same procedures in Step 2 and derive that (cid:1) KL (cid:0)X 0 1:i0 Mar(cid:88) (cid:12) (cid:12)X 1:i0 (cid:13) (cid:13) 0 1:i0 (tar tar n1)E (cid:12) (cid:12)Y 1:i0 (cid:104)(cid:13) (cid:13) log n=1 + wdL2 Mar(cid:88) n=1 (tar tar n1)2. tar (X tar k+1:k+w 0 Rk+1 ) sθ(X tar k+1:k+w, tar n , 0 Rk+1 2(cid:105) )(cid:13) (cid:13) (L.17) For the second term in the right-hand side of Eqn. (L.16), Assumption (4.1) and Lemma (O.3) show that KL (cid:0)X 1:i0 (cid:13) (cid:13) 1:i0 (cid:1) (d i0 + B2) exp(T ). (L.18) Step 5: Concluding the proof. Now we have derived all the components to upper bound the KL-divergence in Eqn. (L.3). Combining Eqn. (L.16), (L.17) and (L.18), we have that (I) (d i0 + B2) exp(T ) + Minit(cid:88) n=1 (tinit tinit n1)E (cid:104)(cid:13) (cid:13) log tinit (X tinit k+1:k+w) sθ(X tinit k+1:k+w, tinit n )(cid:13) (cid:13) 2(cid:105) + wdL2 Minit(cid:88) n=1 (tinit tinit n1)2. (L.19) Combining Eqn. (L.13) and (L.14), we have that (II) K(d + B2) exp(T ). (L.20) Combining Eqn. (L.4), Eqn. (L.6), (L.12), we have that (III) (cid:18) {X K2 (cid:88) k=1 tO k+j}w j=1; (cid:16)(cid:8)X tO k+j (cid:17) (cid:9)w j=1 (cid:8)X Rk+1 (cid:8)X 0 Rk+1 (cid:9) {X tI k+j}w j= (cid:19) (cid:9) (cid:12) (cid:12) (cid:12) (cid:12) + + K2 (cid:88) Mar(cid:88) (tar tar n1)E (cid:104)(cid:13) (cid:13) log tar (X tar k+1:k+w 0 Rk+1 ) sθ(X tar k+1:k+w, tar , 0 Rk+ 2(cid:105) )(cid:13) (cid:13) n=1 wdL2 k=1 K2 (cid:88) k= Mar(cid:88) (tar tar n1)2. n=1 (L.21) Combining Eqn. (L.3), (L.19), (L.20), and (L.19), we conclude the proof of Eqn. (4.1). L.2 Proof of Eqn. (4.4) The proof of Eqn. (4.4) largely shares the similar procedures with the proof of Eqn. (4.1). Thus, we will reuse some intermediate results in the proof of Eqn. (4.1). Here we consider the special case Ri = , and the results of the general case Ri = can be derived by our analysis with complicated calculations. The proof of Eqn. (4.4) consists of the following two steps. Derive the recursive formula of the output error. Conclude the proof by aggregating the recursive formula. Step 1: Derive the recursive formula of the output error. To derive the KL-divergence between generated clip and the nominal video clip, we upper bound the error as follows. KL (cid:0)X 0 K+1:(K+1) KL KL = KL (cid:16)(cid:8)X tO (cid:16)(cid:8)X tO (cid:16)(cid:8)X tO + KL (cid:16)(cid:8)X tO = KL (cid:16)(cid:8)X + KL (cid:16)(cid:8)X tO KL K+i K+i K+i (cid:9)w (cid:16)(cid:8)X tI (cid:9)w K+i i=1 i=1 i (cid:1) (cid:12) (cid:12) i=1 i= i=1 (cid:9)w (cid:9)w (cid:9)w K+i K+i K+i K+1:(K+1) (cid:9)w (cid:13) (cid:13)Y 0 (cid:13) (cid:9)w (cid:8)Y tO (cid:13) (cid:13) (cid:12) (cid:8)X tI (cid:12) (cid:8)X tI i=1 (cid:8)X tI (cid:9)w (cid:8)X tI (cid:9)w i=1 (cid:8)X tI (cid:9)w (cid:8)X tI (cid:9)w tO +i K+i (cid:12) (cid:12) K+i (cid:12) (cid:12) K+i i=1 i i= i=1 i=1 K+i (cid:9)w (cid:16)(cid:8)X tO (cid:16)(cid:8)X tI i (K1)+i (cid:9)w K+i i=w+1 + KL + KL (cid:17) (cid:13) (cid:8)Y tO (cid:13) (cid:13) (cid:13) (cid:8)Y tO (cid:13) (cid:13) (cid:9)w K+i K+i K+i K+i (cid:9)w (cid:9)w i i=1 (cid:8)Y tI (cid:12) (cid:12) (cid:12) (cid:8)Y tI (cid:12) i=1 (cid:13) (cid:8)Y tI (cid:13) (cid:13) (cid:12) (cid:8)Y tI (cid:12) i=1 (cid:13) (cid:8)Y (cid:13) (cid:13) (cid:12) (cid:8)Y tI (cid:12) K+i K+i tO +i K+i K+i + KL (cid:17) (cid:17) (cid:9)w i=1 (cid:9)w (cid:16)(cid:8)X tI K+i (cid:9)w i=1 (cid:13) (cid:8)Y tI (cid:13) (cid:13) K+i (cid:17) (cid:9)w i=1 i=1 i=1 (cid:8)Y tI (cid:9)w (cid:9)w (cid:17) K+i i=1 i=1 (cid:8)Y tI (cid:9)w (cid:9)w (cid:17) K+i i=1 (cid:17) (cid:17) (cid:9)w i=w+ (cid:9)w i=w+1 K+i (cid:13) (cid:8)Y tO (cid:13) (cid:13) i=w+1 (cid:9)w K+i (cid:9)w (cid:9)w i=w+1 (cid:12) (cid:12) (cid:12) (cid:8)Y tO +i K+i (cid:9)w i=1 (cid:17) , (L.22) i=w+1 (cid:9)w K+i i=1 (cid:17) (cid:9)w i=1 K+i (cid:13) (cid:8)Y tO (cid:13) (cid:13) K+i (cid:13) (cid:8)Y tO (cid:13) (cid:13) (cid:12) (cid:12) (cid:12) tO +i K+i (cid:8)X i=1 (K1)+i (cid:9)w i=1 (cid:13) (cid:8)Y tI (cid:9)w (cid:13) (cid:13) i=1 K+i 37 where the inequalities follows from the data processing inequality, and the second equality follows from the Circularity of Requirement 1. To derive the upper bound of the right-hand side of Eqn. (L.22), we can summing up the following recursive equation implied by Eqn. (L.22). In fact, we have that (cid:16)(cid:8)X tO KL (cid:9)w (cid:17) k+i (cid:9)w (cid:16)(cid:8)X tO KL i=1 k+i (cid:13) (cid:8)Y tO (cid:13) (cid:13) k+i (cid:12) (cid:9)w (cid:12) (cid:16)(cid:8)X tO (cid:16)(cid:8)X tI (cid:9)w (k1)+i i=1 k+i i=1 (cid:8)X tI k+i (cid:9)w i=1 (cid:9)w (cid:13) (cid:8)Y tO (cid:13) (cid:13) k+i (cid:13) (cid:8)Y tO (cid:13) (cid:13) (cid:12) (cid:12) (cid:12) tO +i k+i (cid:8)X i=1 (k1)+i i=w+ + KL + KL (cid:8)Y tI k+i (cid:9)w i= (cid:17) (cid:9)w (cid:12) (cid:12) i=1 (cid:17) (cid:9)w i=1 (cid:13) (cid:8)Y tI (cid:9)w (cid:13) (cid:13) i=1 k+i (cid:9)w i=w+1 (cid:12) (cid:12) (cid:12) (cid:8)Y tO +i k+i (cid:9)w i= (cid:17) . This inequality indicates the recursive relationship of the error of the clips at different time steps. Step 2: Conclude the proof by aggregating the recursive formula. Summing the inequality about the recursive relationship from = 1 to = K, we have that (cid:16)(cid:8)X tO KL KL (cid:17) (cid:9)w i=1 (cid:17) (cid:9)w i=1 K+i (cid:9)w (cid:16)(cid:8)X tO i K+i (cid:13) (cid:8)Y tO (cid:13) (cid:13) i=1 (cid:13) (cid:8)Y tO (cid:9)w (cid:13) (cid:13) (cid:16)(cid:8)X tO i=1 k+i KL + + (cid:88) k=1 (cid:88) k=1 (cid:9)w i=1 (cid:12) (cid:12) (cid:8)X tI k+i (cid:9)w i=1 (cid:13) (cid:8)Y tO (cid:13) (cid:13) k+i (cid:9)w i=1 (cid:12) (cid:12) (cid:8)Y tI k+i (cid:9)w i=1 (cid:17) KL (cid:16)(cid:8)X tI k+i (cid:9)w i=w+1 (cid:12) (cid:12) (cid:12) (cid:8)X tO +i k+i (cid:9)w i= (cid:13) (cid:8)Y tI (cid:13) (cid:13) k+i (cid:9)w i=w+1 (cid:12) (cid:12) (cid:12) (cid:8)Y tO +i k+i (cid:9)w i=1 (cid:17) . (L.23) Then we upper bound the second and third terms in the right-hand side of Eqn. (L.23). Similar to Eqn. (L.21), we have that (cid:9)w i=1 (cid:12) (cid:12) (cid:8)X tI k+i (cid:9)w i=1 (cid:13) (cid:8)Y tO (cid:13) (cid:13) k+i (cid:9)w i= (cid:12) (cid:12) (cid:8)Y tI k+i (cid:9)w i=1 (cid:17) KL k+i (cid:16)(cid:8)X tO Mar(cid:88) (tar tar n1)E (cid:104)(cid:13) (cid:13) log tar (X tar k+1:k+w 0 Rk+ ) sθ(X tar k+1:k+w, tar , 0 Rk+1 2(cid:105) )(cid:13) (cid:13) n=1 + wdL2 Mar(cid:88) n=1 (tar tar n1)2 KL (cid:9)w (cid:16)(cid:8)X tI (cid:8)X (d i0 + B2) exp(T ). i=w+ k+i (cid:12) (cid:12) (cid:12) tO +i k+i (cid:9)w i=1 (cid:13) (cid:8)Y tI (cid:13) (cid:13) k+i (cid:9)w i=w+1 (cid:12) (cid:12) (cid:12) (cid:8)Y tO +i k+i (cid:9)w i= (cid:17) Thus, we conclude the proof by combining these inequalities. Proof of Theorem 4.5 In the following, we would like to prove (cid:18) inf (cid:98)P sup S(s) TV(P, (cid:98)P ) (cid:19) 2 1 2 . 38 Then Pinsker inequality implies that (cid:18) inf (cid:98)P sup S(s) KL(P (cid:98)P ) (cid:19) s2 2 inf (cid:98)P sup S(s) (cid:18) (cid:18) 2(cid:2) TV(P, (cid:98)P )(cid:3)2 (cid:19) (cid:19) s2 2 sup S(s) TV(P, (cid:98)P ) = inf (cid:98)P 1 2 . 2 We would like to follow the procedures of the proof of impossibility results in the non-parametric statistics (Wainwright, 2019). We start the proof by constructing two distributions P0 and P1 as follows. P0(X = x, = y, = z) = 1 8 for all x, y, {0, 1}, P1(X = x, = y, = z) = P1(X = x, = z)P1(Y = y) = (M.1) 1 2 P1(X = x, = z)P1(Y = y), (M.2) P1(X = x, = 1 x) = ϵ 2 , P1(X = x, = x) = 1 ϵ 2 , where ϵ [0, 1/2] is hyperparameter. For P0, X, and are independent Bernoulli(1/2) random variables. For P1, is independent Bernoulli(1/2) with X, Z. The marginal distributions of and are Bernoulli(1/2), but these two variables are connected by flip channel with flip probability ϵ. We first verify that these two distributions are in S(s). In fact, we have that I0(X; ZY ) = 0, I1(X; ZY ) = I1(X; Z) = 1 H(ϵ), where Ii is the mutual information with respect to the distribution Pi, and H(x) = log (1 x) log(1 x) is the entropy of Bernoulli distribution. Obvisiously, P0 S(s). We note that H(x) [0, 1] and is monotone on [0, 1/2]. Thus, if ϵ [H 1(1 s), 1/2], P1 S(s). In the following, we set ϵ = 1(1 s), then I1(X; ZY ) = s. property of the constructed two distributions is that P0(X, ) = P1(X, ), P0(Y, Z) = P1(Y, Z). (M.3) We also show that the total variation between them can be bounded as follows. Proposition M.1. For the distributions defined in Eqn. (M.1) and (M.2) with ϵ [0, 1/2], we have that TV(P0, P1) 1 2 KL(P1 P0) = 1 2 I1(X; ZY ). The proof is provided in Appendix N.3. Then we consider the estimate (cid:98)P based on the data generated by these two distributions. For any (cid:98)P , we define classifier ψ that determines the data set is generated by P0 or P1 as ψ( (cid:98)P ) = argmin i{0,1} TV( (cid:98)P , Pi). 39 If ψ( (cid:98)P ) = for {0, 1}, then the triangle inequality shows that TV( (cid:98)P , Pi) 1 I1(X; ZY ) = 2 . Thus, we have that (cid:18) inf (cid:98)P sup S(s) TV( (cid:98)P , ) (cid:19) 2 inf (cid:98)P sup i{0,1} Pi (cid:18) TV( (cid:98)P , Pi) (cid:19) 2 inf (cid:98)P sup i{0,1} Pi (cid:0)ψ( (cid:98)P ) = i(cid:1). (M.4) We denote the data distribution as , i.e., (cid:0){Xi = xi, Yi = yi}N i i=1 {Yi = yi, Zi = zi}2N i=N +1 (cid:1) = (cid:89) i=1 Pi(Xi = xi, Yi = yi) 2N (cid:89) i=N +1 Pi(Yi = yi, Zi = zi). Then Eqn. (M.3) implies that the following error probability. 0 = 1 . Neyman-Pearson Theorem shows that any test ψ has P1(ψ = 0) + P0(ψ = 1) (cid:90) Then we can further lower bound Eqn. (M.4) as min(dP 0 , dP 1 ) = 1. inf (cid:98)P sup i{0,1} Pi (cid:0)ψ( (cid:98)P ) = i(cid:1) inf sup i{0,1} Pi ψ (cid:0)ψ = i(cid:1) 1 2 . Thus, we conclude the proof of Theorem 4.5."
        },
        {
            "title": "N Proof of Supporting Propositions",
            "content": "N.1 Proof of Proposition 4.6 From the chain rule of the conditional mutual information, we have that I(X, g(X); ZY ) = I(g(X); ZY ) + I(X; ZY, g(X)) = I(X; ZY ) + I(g(X); ZY, X). We note that g(X) degenerates when the probability is conditioned on X, which implies that I(g(X); ZY, X) = 0. Thus, we have that I(X; ZY ) = I(g(X); ZY ) + I(X; ZY, g(X)) I(X; ZY, g(X)). We conclude the proof of Proposition 4.6. N.2 Proof of Proposition L.1 We begin the proof by showing the recursive relationship for H. We note that (cid:16)(cid:8)Y tI i+j (cid:9)w j=1 (cid:17) = (cid:16)(cid:8)Y tI i+j (cid:9)w j=1 (cid:17) {Y 0 i+1:i} (cid:8)Y tI i+j (cid:9)w j=1, (N.1) 40 the equality follows from the definition of H() in Eqn. (L.1). Then we have (cid:16)(cid:8)Y tO i+j (cid:17) (cid:9)w j=1 (cid:16)(cid:8)Y (cid:16)(cid:8)Y (cid:16)(cid:8)Y (cid:16)(cid:8)Y = = = = tI i+j tI i+j tI i+j (cid:9)w j=1 (cid:9)w j=1 (cid:9)w (cid:17) (cid:17) (cid:17) j=1 (cid:17) (cid:9)w tI i+j j=1 (cid:8)Y tI i+j (cid:9)w j=1 , (cid:8)Y tI i+j (cid:8)Y tI i+j (cid:8)Y tI i+j (cid:9)w j=1 (cid:8)Y (cid:9)w j=1 (cid:8)Y (cid:9)w j=1 (cid:8)Y 0 tO i+j tO i+j i+j (cid:9)w j=1 (cid:9) j=1 (cid:8)Y (cid:9) j=1 (cid:8)Y tO i+j (cid:9)w j=+ tI i+j (cid:9)w j=1 where the first equality follows from the definition of in Eqn. (L.2), the third equality follows from 0 boundary and circularity in Requirement 1, and the last equality follows from Eqn. (N.1). Thus, we conclude the proof of Proposition L.1. N.3 Proof of Proposition M.1 First, we note that the KL-divergence between these two distributions is Lemma O.2 shows that KL(P1P0) = I1(X; ZY ). TV(P0, P1) 1 β1 log2 1/β1 KL(P1 P0), where β1 1 = 2(1 ϵ). To prove the desired result, it remains to show that 1 β1 log2 1/β1 = Then we define the function 1 2ϵ (cid:0)2(1 ϵ)(cid:1) 1 . 2(1 ϵ) log2 (ϵ) = 1 2ϵ (1 ϵ) log2 (cid:0)2(1 ϵ)(cid:1). The derivative of this function is that (ϵ) = 1 + 1 log 2 log(1 ϵ) log 2 , which is monotone function. Since (0) > 0 > (1/2), we have that inf x[0,1/2] (x) = min{f (0), (1/2)} 0. Thus, we conclude the proof of Proposition M.1. 41 N.4 Proof of Proposition L.2 For the difference between the scores to bound, we have that 2(cid:105) (cid:104)(cid:13) (cid:13) log u(X u) log v(X v)(cid:13) (cid:13) 4E (cid:104)(cid:13) (cid:13) log u(X u) log u(α1 u,vX v(cid:13) (cid:13) u,vX v)(cid:13) (cid:13) + L(1 α1 (cid:104)(cid:13) (cid:13)X α1 L2 L2(cid:0) exp(v u) 1(cid:1) + dL(v u)2 dL2(v u), 2(cid:105) u,v)2 2(cid:105) + 2(1 α1 u,v)2E (cid:104)(cid:13) (cid:13) log u(X u)(cid:13) (cid:13) 2(cid:105) where the first inequality results from Lemma O.3, the second inequality results from the Lipschitzness and Lemma O.4, the third inequality results from the definition of t, and the last inequality results from that 1."
        },
        {
            "title": "O Supporting Lemmas",
            "content": "Lemma O.1 (Lemma 6 in Chen et al. (2023a)). Consider the following two Ito processes dX = F1(X t, t)dt + g(t)dBt, 0 = 0 = a, dY = F2(Y t, t)dt + g(t)dBt, where F1, F2, and are continuous functions and may depend on a, We assume the uniqueness and regularity condition: These two Stochastic Differential Equation (SDE)s have unique solutions. The processes and admit densities pt, qt 2(Rd) for > 0. Define the relative Fisher information between pt and qt by J(pt qt) = (cid:90) (cid:13) (cid:13) pt(x) (cid:13) (cid:13) log pt(x) qt(x) (cid:13) 2 (cid:13) (cid:13) (cid:13) dx. Then for any > 0, the evolution of KL(ptqt) is given by dt KL(pt qt) = g(t)2J(pt qt) + Ept (cid:20)(cid:28) F1(X t, t) F2(X t, t), log pt(X t) qt(X t) (cid:29)(cid:21) . Lemma O.2 (Theorem 7 in Verdu (2014)). For two distributions P, P(Ω), we define Then we have that β1 1 = sup wΩ dP dQ (w). TV(P, Q) 1 β1 log2 1/β1 KL(P Q). Lemma O.3 (Lemmas 9 and 11 in Chen et al. (2023a)). For any distribution PX on Rd that has finite second moment, i.e., EPX X2 < , and evolves as dX = 1 2 tdt + dBt, 0 PX, where Bt is Brownian motion. We denote the distribution of as t. Then we have that KL(P (0, I)) (d + EPX X2) exp(T ). For any 0 , we define αt,s = exp( 1 2(s t)). Then we have that (cid:104)(cid:13) (cid:13) log t(X t) log s(X s)(cid:13) (cid:13) 4E (cid:104)(cid:13) (cid:13) log t(X t) log t(α1 2(cid:105) t,s s)(cid:13) (cid:13) 2(cid:105) + 2(1 α1 t,s )2E (cid:104)(cid:13) (cid:13) log t(X t)(cid:13) (cid:13) 2(cid:105) . Lemma O.4 (Chewi et al. (2024)). Let 1(Rd) be probability distribution. If log is L-Lipschitz, then we have that (cid:104)(cid:13) (cid:13) log (X)(cid:13) (cid:13) 2(cid:105) EP L."
        }
    ],
    "affiliations": [
        "A*STAR",
        "Nanyang Technological University",
        "National University of Singapore",
        "Sea AI Lab",
        "Yale University"
    ]
}