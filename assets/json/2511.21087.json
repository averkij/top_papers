{
    "paper_title": "MIRA: Multimodal Iterative Reasoning Agent for Image Editing",
    "authors": [
        "Ziyun Zeng",
        "Hang Hua",
        "Jiebo Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana."
        },
        {
            "title": "Start",
            "content": "MIRA: Multimodal Iterative Reasoning Agent for Image Editing Ziyun Zeng1 , Hang Hua2 , Jiebo Luo1 1University of Rochester, 2MIT-IBM Watson AI Lab ziyun.zeng@rochester.edu, hang.hua1@ibm.com, jluo@cs.rochester.edu 5 2 0 2 6 2 ] . [ 1 7 8 0 1 2 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructionsespecially those involving compositional relationships, contextual cues, or referring expressionsleading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perceptionreasoningaction loop, effectively simulating multiturn humanmodel interaction processes. Instead of issuing single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRAEDITING, combined with two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana. https://zzzmyyzeng.github.io/MIRA/ 1. Introduction As visual generation systems become increasingly interactive, instruction-guided image editing has attracted growing attention for its ability to follow user instructions to perform precise, controllable visual manipulations. Recent advanced diffusion-based editing models, such as Qwen-Image-Edit [53], Flux.1-Kontext [5], and Step1X-Edit [35], have demonstrated promising progress in aligning textual guidance with pixel-level transformations. However, these systems exhibit significant degradation when handling complex instructions that involve compositional reasoning, multiple-object interactions, or context-dependent manipulations. Even proprietary multimodal systems like Seedream 4.0 [41], GPT-Image [40], or Nano-Banana [3], though more capable, still struggle with maintaining fine-grained consistency and controllability in complex instruction-guided image editing. These limitations highlight the need for editing systems that can go beyond one-shot prompt execution and instead reason, adapt, and revise their edits interactively. To mitigate the discrepancy between user intent and editing outcomes, recent research has moved beyond purely text-to-edit systems and explored embedding reasoning and orchestration via visionlanguage models. One line of research augments editing pipelines by enriching text embeddings or refining user instructions before forwarding them to diffusion-based editors [7, 13, 55, 58, 62, 63, 65]. These approaches improve textual alignment and enhance editing controllability, but they remain constrained to static prompt refinement. complementary and more recent direction reframes instruction-guided image editing as reasoningdriven orchestration problem. Instead of executing single prompt once, vision-language model acts as an editing agent that decomposes complex image editing instructions into semantically coherent sub-tasks, plans an execution sequence, and coordinates suite of specialized expert models such as localization, inpainting, editing, composition, and global transformation. These agentic frameworks, characterized by programmatic decomposition, iterative reasoning, and multi-tool collaboration, achieve notable improvements on compositional, multi-step, and context-dependent editing tasks by integrating structured reasoning with iterative visual feedback mechanisms [18, 21, 22, 29, 33, 52, 56]. Although these architectures exhibit strong compositional reasoning and interpretability, they typically rely on large toolchains and complex model coordination, making them computationally intensive and difficult to scale within open-source ecosystems. Building upon these insights, we introduce MIRA, lightweight, plug-and-play visionlanguage agent that formulates instruction-guided image editing as an iterative visual reasoning process rather than static orchestration pipeline. Unlike prior editing systems that pre-plan an entire sequence of operations or rely on handcrafted tool 1 Figure 1. Qualitative comparison of MIRA against leading proprietary and open-source image editing models on complex instructions. The rightmost column illustrates MIRAs unique iterative reasoning and editing process, displaying the intermediate visual results after each step of its perception-reasoning-action loop. chains, MIRA performs step-wise agentic action selection: at each iteration, the vision-language model (VLM) observes the original image, the user instruction, and the intermediate editing state, reasons about the remaining visual discrepancy, and predicts the next atomic editing action or stop decision. This cycle defines simple yet effective agentic loop: state multimodal reasoning action environment feedback, where the environment corresponds to the visual editor that executes the predicted action. In this loop, reasoning is grounded in editing results; the agent learns and adapts by observing the discrepancy between the instruction and editing results. This cycle repeats iteratively until the complex instruction is fully satisfied. Through iterative reasoning and feedback-driven tool use, MIRA effectively improves editing performance in semantic consistency and perceptual quality through iterative multimodal reasoning and feedback-driven tool calling. By casting image editing as an agentic loop, MIRA enables open-source diffusion-based editors to handle complex, ambiguous, and context-dependent instructions with both interpretability and efficiency. This design significantly boosts the capabilities of open-source editing models, often allowing them to match or even surpass the performance of proprietary systems. lightweight, and agentic, Our main contributions are summarized as follows: plug-and-play visionlanguage model that can be seamlessly paired with existing open-source image editing backbones. By combining multimodal reasoning with explicit tool-use, MIRA empowers these models to handle complex editing instructions. It significantly reduces the performance gap between open-source and proprietary image editing systems. large-scale training dataset, MIRA-EDITING, tailored for training automatic tool-use models for instructionguided image editing. The dataset comprises 150,000 high-quality paired examples generated through hierarchical instruction aggregation, semantic rewriting, and ranking-based filtering, providing diverse and reliable multimodal supervision. two-stage training pipeline combining SFT and GRPO. During the GRPO phase, we incorporate novel composite reward model that couples an image-editing backbone with an image-editing reward model to assess the quality and fidelity of edit instructions. This design provides richer, more reliable, and semantically grounded optimization signals. 2. Related Works 2.1. Instruction-guided Image Editing Instruction-guided image editing enables precise and intuitive manipulation of visual content through natural language instructions. Early prompt-based diffusion editing methods [11, 19, 31, 37] demonstrated the potential of prompt-level control for semantic image manipulation by leveraging textual guidance to achieve localized and structurally coherent edits without additional training. These approaches revealed the expressive power of diffusion models in responding to prompt semantics, yet remained limited by their reliance on manually crafted text descriptions. Building on this foundation, InstructPix2Pix [6] proposed datadriven approach that learns direct text-to-edit mappings, inspiring wave of instruction-based editing methods such as MagicBrush [61], UltraEdit [63], InstructDiffusion [16], InstructCV [14], and PromptFix [58], which emphasize large-scale instructionimage alignment, as well as precise object-level manipulation and compositing [4345, 59, 64]. To strengthen instruction understanding and contextual reasoning, advanced systems such as MGIE [13], SmartEdit [28], InstructEdit [51], and OmniGen2 [54] leverage large language or visionlanguage models to enhance semantic interpretation, context awareness, and fine-grained control in instruction-guided image editing. Meanwhile, open-source models such as Qwen-Image-Edit [53], Flux.1Kontext [5], and Step1X-Edit [35] have advanced diffusionbased editing capabilities, combining high-resolution latent diffusion frameworks with robust controllability. In contrast, proprietary systems including Seedream 4.0 [41], GPT-Image [40], and Nano-Banana [3] represent the state of the art in multimodal reasoning, demonstrating superior fidelity, responsiveness, and adaptability in instructionguided image editing. 2.2. MLLMs for Image Editing Multimodal large language models (MLLMs) [17, 20, 23 26, 38, 46, 50, 57, 60, 66] have recently emerged as powerful engines for reasoning-driven image generation and editing. Early studies leverage MLLMs primarily for textto-image synthesis, where MLLMs interpret natural language descriptions and generate creative visual content in zero-shot manner [10, 27, 52]. These approaches highlight the strong cross-modal understanding of MLLMs but remain limited to one-shot generation without explicit control over iterative editing. To support instruction-guided editing, several studies employ MLLM prompt optimizers that rewrite or enrich user instructions before passing them to diffusion-based image editing models, such as MGIE [13], LLMGA [55], PromptFix [58], and HiDream-E1 [7]. This strategy enhances linguistic clarity and improves alignment between textual descriptions and visual execution. However, such methods operate in static manner. Once refined prompt is produced, the model does not evaluate whether the edited result aligns with the intended instruction, limiting its adaptability to complex or ambiguous instructions. More recent works extend MLLMs into agentic frameworks that decompose editing tasks and coordinate external tools. While agentic reasoning has shown remarkable success in scientific domains [8, 34, 4749], applying it to visual editing remains challenging. Systems such as X-Planner [56], RefineEdit-Agent [33], and CoSTA* [18]. These systems introduce structured reasoning and sequential planning for complex editing workflows. 3. Data Curation High-quality and compositionally rich data is essential for training MIRA. Due to the lack of publicly available data that satisfies the need for multi-step reasoning supervision, complex instruction alignment, and high-fidelity image editing trajectories, we construct new dataset with 150K paired samples. We also provide detailed quantitative and qualitative analyses in the supplementary material. 3.1. Data Preprocessing Pipeline Instruction Aggregation. We transform the multi-turn editing sequences in SeedEdit [15] into single complex instructions by merging their atomic edits. To further enrich compositional complexity, we generate in-order and permuted variants, allowing the model to learn from broader range of compositional structures. Two-Level Instruction Rewriting. To enhance linguistic diversity, we paraphrase instructions at two levels: atomic edits are paraphrased individually, and the full aggregated instruction is holistically rewritten with varied structures and connectives, yielding semantically equivalent yet stylistically diverse variants. Candidate Generation. Each rewritten instruction is executed using strong open-source image editing model, producing candidate edited trajectories. These variations ex3 Figure 2. Workflow of our multimodal reasoning and editing agent MIRA. Given an input image and complex natural-language instruction, MIRA engages in an iterative perceptionreasoningaction loop. At each step, the agent analyzes the current visual state and textual context to generate an atomic edit instruction, which is executed by an external image-editing model. The updated image is fed back into the agent to guide the next step. This loop continues until the full instruction is satisfied, yielding the final edited result. pose differences caused by paraphrasing and provide diverse pool of potential supervision signals. Semantic Consistency Ranking. To filter noisy or misaligned outputs, we evaluate all candidates using VieScore [32], choosing Gemini-2.5-Flash and Qwen2.5-VL72B-Instruct as backbone. They rank candidates by semantic consistency with the original complex instruction, and we retain only the top-1 result. Final Sample Construction. The final dataset entry consists of the input image, the selected high-quality edited trajectory, the aggregated complex instruction, and its rewritten variants. This ensures each sample provides clean, diverse, and semantically faithful supervision for iterative reasoning and editing. 3.2. Training Data Formulation. To train MIRA as an iterative reasoning agent in perception-reasoning-action loop, each curated editing trajectory is decomposed into step-wise supervision. Given complex instruction C, the original image I0, and the intermediate result It1, the target atomic edit ut and its execution via image editing It form transition: (It1, I0, C) ut It (1) We convert each trajectory into three structured data types. Type 1 (Start) samples supervise the first-step prediction conditioned only on I0 and C. Type 2 (Continue) samples, which constitute the majority of the dataset, train the model to iteratively refine edits based on visual feedback from It1. Type 3 (Stop) samples supervise termination detector during perception, enabling the agent to recognize task completion and avoid unnecessary edits. This structured formulation shows MIRAs inference time editing loop and provides compact and effective multimodal supervision for progressive, feedback-driven reasoning. Figure 3. Three types of editing samples in MIRA-EDITING. 4. Methodology 4.1. The MIRA Framework Our proposed MIRA, as shown in Figure 2, builds upon Qwen2.5-VL [4], leveraging its strong multimodal reasoning and visual understanding capabilities. The model is trained through supervised fine-tuning (SFT) followed by reinforcement learning post-training to improve semantic consistency and perceptual quality through rewarddriven optimization. At inference time, MIRA performs instruction-guided editing in strictly iterative and closedloop manner, issuing exactly one atomic editing instruction 4 Figure 4. Overview of the MIRA Training Pipeline. The training pipeline comprises two stages: (1) Supervised Fine-Tuning and (2) Reinforcement Learning. Stage 1 fine-tunes Qwen2.5-VL-7B-Instruct on paired samples of the input image, the previously edited image, and the complex instruction to initialize the policy model. Stage 2 applies GRPO to further refine the policy, using composite reward function that couples an image editing model with an editing reward model to score edit quality and provide optimization signals. at each step and dynamically adapting its plan based on the most recent visual feedback. Formally, given an input image I0 and complex instruction C, the agent maintains the current intermediate result It1 and, at step t, predicts the next editing instruction as ut = πθ(It1, I0, C) (2) where πθ denotes the policy induced by Qwen2.5-VL after SFT and RL post-training. The predicted instruction ut is then executed by diffusion-based image editing model Et E: It = Et(It1, ut) (3) where represents family of interchangeable opensource editors such as Flux.1-Kontext-Dev [5], QwenImage-Edit [53], and Step1X-Edit [35]. After each step, lightweight termination controller determines whether the process should continue: dt = τϕ(It, I0, C) {continue, stop} (4) where τϕ is trained jointly with the policy to predict task completion. If dt = stop, the procedure halts and returns It as the final output. Otherwise, the triplet (It, I0, C) is fed back into the policy to produce the next instruction ut+1. In contrast to prior agentic systems that predict the entire editing sequence in one shot, MIRA performs adaptive, feedback-driven instruction planning and goalconditioned refinement. This receding-horizon approach naturally yields sequence of semantically atomic operations ut, each delegated to an interchangeable editing backend. The design integrates the reasoning strengths of visionlanguage models with the modularity of open-source diffusion-based editors, enabling scalable and controllable execution of complex editing instructions without extensive multi-tool orchestration. 4.2. Training Pipeline As shown in Figure 4, MIRA is trained via two-stage pipeline that gradually shifts from imitation-based instruction following to reward-driven multimodal reasoning. In Stage 1, we initialize the policy model πθ from Qwen2.5VL-7B-Instruct and train it on curated instructionimage triplets to generate atomic editing actions as (2). This stage 5 Table 1. Quantitative comparison of baseline vs. MIRA-enhanced image editing models in terms of semantic consistency and perceptual quality. GPT-SC, Gemini-SC, and Qwen3VL-SC report semantic consistency measured by GPT-5, Gemini-2.5-Flash, and Qwen3-VL-30BA3B-Instruct using the viescore prompt. ARNIQA and TOPIQ assess perceptual quality via human preference modeling and traditional IQA metrics. EditScore-SC, EditScore-PQ, and EditScore-OA provide open-source semantic consistency, perceptual quality, and overall alignment scores. The best and second best results are in bold and underlined, respectively, excluding proprietary systems. Method GPT-SC Gemini-SC Qwen3VL-SC EditScore-SC ARNIQA TOPIQ EditScore-PQ EditScore-OA InstructPix2Pix [6] OmniGen2 [54] Bagel [12] Step1X-Edit [35] Flux.1-Kontext [5] Qwen-Image-Edit [53] GPT-Image [40] Nano-Banana [3] Step1X-Edit + MIRA Flux.1-Kontext + MIRA Qwen-Image-Edit + MIRA 1.804 4.750 6.228 5.942 5.908 6.290 7.410 7.972 6.322 6.202 6.882 2.298 5.204 6.482 6.440 6.290 6.934 7.232 8. 6.886 6.670 7.104 2.166 4.760 5.754 4.946 5.116 5.792 6.656 6.514 Open Source Image Editing Models 6.414 6.867 7.581 8.542 8.194 7.879 0.511 0.525 0.507 0.507 0.563 0. Proprietary Image Editing Models 8.629 8.924 0.635 0.599 0.277 0.310 0.283 0.278 0.330 0.341 0.383 0.318 Open Source Image Editing Models Plug and Play with MIRA 5.844 5.802 6.280 8.668 8.532 8.583 0.601 0.619 0.612 0.296 0.353 0.357 4.539 4.913 5.244 5.474 5.673 6.119 7.666 6. 5.640 5.473 5.801 4.937 5.468 5.949 6.636 6.507 6.625 7.977 7.551 6.800 6.610 6.871 focuses on imitation learning from high-quality paired data, aligning visual transformations with textual intent. }K In Stage 2, we apply Group Relative Policy Optimization (GRPO) [42] in step-wise manner. At each step t, the policy model generates multiple atomic edit instructions {uk k=1 conditioned on the current visual state It1. Each atomic edit instruction is executed by the external image model, such as Flux.1-Kontext, to produce an updated image ), and the quality of the edit is assessed by fixed reward model, such as EditScore. = R(I rk ) measures both semantic consistency and perceptual quality. Therefore, the reward can be formulated as = E(It1, uk , It1, uk = λsc rsc(I rk , It1, uk ) + λpq rpq(I t , It1, uk ), (5) where rsc and rpq quantify semantic consistency and perceptual quality. The resulting rewards {rk } are normalized into advantages {Ak }, and the policy is optimized based on all step-level transitions (It1, uk ). The update rule for the policy model is formulated as , rk θJt(θ) = Ek (cid:2)Ak (cid:16) πθ( It1, uk β DKL θ log πθ , rk (cid:1)(cid:3) (cid:0)uk ) (cid:13) It1, rk (cid:13) πref( It1, uk (cid:17) , rk ) where πref denotes the frozen, supervised fine-tuned policy model used as the reference model. Both and are used only during the forward evaluation and are not backpropagated. 5. Experiment 5.1. Experiment Settings Implement Details. All image-editing models are evaluated using their official pretrained checkpoints, producing 10241024 outputs with default settings from their 6 repositories to ensure fair comparison. MIRA acts as lightweight plug-and-play reasoning layer that decomposes complex instructions and iteratively guides the base imageediting model toward the final result. Baseline Models. To comprehensively evaluate the effectiveness of MIRA, we conduct experiments on diverse set of open-source and proprietary image editing models. For open-source baselines, we include InstructPix2Pix [6], OmniGen2 [54], Bagel [12], Step1X-Edit [35], Flux.1-Kontext [5], and Qwen-Image-Edit [53]. For proprietary systems, we evaluate Seedream 4.0 [41], GPTImage [40], and Nano-Banana [3]. To assess MIRA enhancement capability, we integrate it with representative open-source instruction-guided image editing models, Step1X-Edit, Flux.1-Kontext-Dev, and Qwen-Image-Edit, forming MIRA-enhanced variants. Furthermore, we compare MIRA-enhanced models against other VLM-enhanced models, such as GPT-5 [39], Qwen3-VL-8B-Instruct [4], and Qwen3-VL-30B-A3B-Instruct [4], under plug-and-play configurations with Flux.1-Kontext-Dev and Qwen-ImageEdit as base instruction-guided image editing models. Evaluation Benchmark. All models are evaluated on challenging instruction-guided image editing benchmark built from the multi-turn subset of MagicBrush [61] (following Section 3) and subset of CompBench [30], resulting in 500 test samples. Each sample includes rich, multisentence instructions requiring precise semantic alignment and high visual fidelity. For fair comparison, all evaluation images are standardized to 10241024 resolution, and instruction lengths are limited to 77 words. Evaluation Metrics. We adopt two major evaluation dimensions: semantic consistency and perceptual quality. Semantic consistency is measured by GPT-SC, Gemini-SC, and Qwen3VL-SC, representing scores judged by GPT5 [39], Gemini-2.5-Flash [2], and Qwen3-VL-30B-A3BTable 2. Quantitative comparison of MIRA-enhanced versus other VLM-enhanced image editing models on semantic consistency and perceptual quality. Metric definitions follow Table 1. All models are evaluated in plug-and-play setting with Flux.1-Kontext-Dev and Qwen-Image-Edit as the base instruction-guided editing backbones. Method GPT-SC Gemini-SC Qwen3VL-SC EditScore-SC ARNIQA TOPIQ EditScore-PQ EditScore-OA Qwen3-VL-8B [4] Qwen3-VL-30B [4] GPT-5 [39] MIRA 3B MIRA 7B Qwen3-VL-8B [4] Qwen3-VL-30B [4] GPT-5 [39] MIRA 3B MIRA 7B 5.488 3.792 7.074 5.628 6.202 6.066 4.092 7.498 6.622 6.882 5.688 3.920 6.566 6.422 6.670 6.168 4.284 7.560 7.036 7.104 Plug and Play with Flux.1-Kontext-Dev 8.243 8.330 8.358 8.168 8.532 0.616 0.583 0.653 0.615 0.619 Plug and Play with Qwen-Image-Edit 8.248 8.270 8.616 8.206 8.583 0.574 0.558 0.603 0.592 0.612 0.317 0.306 0.378 0.338 0. 0.339 0.296 0.337 0.339 0.357 4.264 2.808 5.610 5.754 5.802 4.894 2.904 6.060 6.244 6.280 5.226 5.729 5.129 5.094 5.473 5.078 5.525 5.310 5.461 5.801 6.279 6.455 6.284 6.177 6. 6.242 6.537 6.546 6.425 6.871 Instruct [4] respectively, following the viescore protocol [32]. In addition, EditScore-SC provides an open-source semantic consistency metric [36]. Perceptual quality is evaluated through ARNIQA [1] and TOPIQ [9], while EditScorePQ and EditScore-OA evaluate open-source perceptual and overall edit alignment quality [36]. We evaluate models across two key dimensions: semantic consistency and perceptual quality. 5.2. Evaluation of MIRA Performance 5.2.1. Quantitative Results Table 1 reports results for proprietary editors, open-source instruction-guided editors, and their MIRA-enhanced variants. Across all open-source models, integrating MIRA consistently improves both semantic consistency and perceptual quality. For example, with Flux.1-Kontext, MIRA delivers gains of 4.98%, 6.04%, 13.41%, and 4.12% on GPT-SC, Gemini-SC, Qwen3VL-SC, and EditScore-SC, respectively. Similar improvements appear for Step1XEdit and Qwen-Image-Edit, where GPT-SC increases by 6.40% and 9.41%. These results show that MIRA effectively enhances complex instruction following, enabling open-source editors to match or surpass proprietary systems such as GPT-Image and Nano-Banana. Besides semantic gains, MIRA also improves perceptual quality, raising both ARNIQA and TOPIQ scores. Although multi-step editing might intuitively introduce diffusion noise or structural drift, we find the opposite: MIRA provides clearer, modelfriendly instructions, reducing hallucination and artifact accumulation. Better-structured guidance allows the underlying generative priors to operate more stably, yielding edits that are both semantically faithful and visually cleaner. Table 2 further compares the same plug-and-play system equipped with different vision-language modusing Flux.1-Kontextsettings, ules under Dev and Qwen-Image-Edit as interchangeable external tools. Among Qwen3-VL-8B-Instruct, Qwen3-VL-30Bidentical A3B-Instruct, GPT-5, MIRA 3B and MIRA 7B, MIRA 7B reaches the best balance of semantic consistency and perceptual quality across both editing models. It outperforms open-source VLMs on almost every semantic metrics, with relative gains of 13%68% on GPT-SC and 15%70% on Gemini-SC, while achieving comparable or higher overall performance than GPT-5. This indicates that MIRA strength lies in multimodal reasoning for enhancing image editing, while its base model, Qwen2.5-VL-7B-Instruct is less capable in general reasoning and visual agentic caits strucpability than the Qwen3-VL series or GPT-5; tured and iterative reasoning enables superior performance in instruction-guided image editing. 5.2.2. Single-Turn Instruction Refinement Although MIRA is designed for multi-step iterative reasoning and editing, it also generalizes surprisingly well to single-turn instruction refinement, which is task outside its training domain. When given an ambiguous instruction, MIRA can clarify the intent and rewrite it into concise, executable editing instruction, enabling image editing models to follow user intent more reliably. We also provide detailed analysis of how one-shot refinement leads to consistent gains across diverse editing categories in the supplementary material. 5.3. Ablation Study 5.3.1. Effectiveness of RL To quantify the impact of reinforcement learning on multimodal reasoning and editing quality, we conduct an ablation comparing MIRA trained only with supervised fine-tuning against the full two-stage version incorporating Group Relative Policy Optimization, as illustrated in Figure 4. In this setting, MIRA-SFT corresponds to the policy learned purely by imitation from curated instructionimage pairs, while MIRA-GRPO further refines this policy through reward-driven optimization, using customized composite rewards derived from an image editing model and reward 7 Table 3. Comparison between MIRA with only SFT and MIRA further trained with GRPO after SFT. The additional GRPO stage consistently improves semantic consistency and perceptual quality across all backbones, with relative gains shown. Method GPT-SC Gemini-SC Qwen3VL-SC EditScore-SC ARNIQA TOPIQ EditScore-PQ EditScore-OA Step1X-Edit + MIRA-SFT Step1X-Edit + MIRA-GRPO Flux.1-Kontext + MIRA-SFT Flux.1-Kontext + MIRA-GRPO Qwen-Image-Edit + MIRA-SFT Qwen-Image-Edit + MIRA-GRPO 6.882+14.85% 7.104+5.56% 5.756 6.322+9.83% 5.474 6.202+13.30% 6.670+4.91% 5.992 6.666 6.886+3.30% 6.358 6. 5.906 5.8441.05% 5.846 5.8020.75% 6.196 6.244+0.77% 8.494 8.668+2.05% 8.257 8.532+3.33% 8.491 8.583+1.08% 0.296 5.306 0.601 0.610+1.50% 0.304+2.70% 5.640+6.29% 0.619 0.619+0.00% 0.353+2.32% 5.473+8.48% 0.605 0.612+1.16% 0.357+2.29% 5.801+3.13% 5. 5.045 0.349 0.345 6.522 6.800+4.26% 6.219 6.610+6.29% 6.703 6.871+2.51% Figure 5. Qualitative Case Study for MIRAs Error Mitigation Capability. Atomic 1: Replace the floor to wooden floor., Atomic 2: Change the color of the white cabinet to wooden brown., Atomic 3: Change the color of the white stove to black., Atomic 4: Change the color of the wooden refrigerator to white., Atomic 5: Change the color of the white stove to black, Atomic 6: Stop. model for image editing. As shown in Table 3, RL posttraining with our customized reward function yields consistent performance gains across nearly all metrics and opensource image editing models listed. For example, Step1XEdit + MIRA-GRPO achieves 9.83%, 6.29%, and 4.26% improvements on GPT-SC, EditScore-PQ, and EditScoreOA, respectively, compared to its SFT counterpart. Similar trends are observed for plugging-and-playing with Flux.1Kontext and Qwen-Image-Edit, indicating that the posttraining stage effectively enhances both semantic consistency and perceptual quality. These results validate that reward-driven optimization enables MIRA to learn more fine-grained multimodal reasoning policies, producing editing instructions that are semantically faithful, visually coherent, better aligned with human-preferred editing outcomes, and easier for image editing models to understand. 5.3.2. Reasoning Steps and Excution Quality To evaluate whether larger reasoning budget improves execution quality, we vary the maximum number of inference steps from 3 to 7 and report the corresponding performance in Table 4. Across both semantic consistency and perceptual quality metrics, we observe only minor variations as the maximum step budget increases. Allowing additional steps yields modest improvements, most notably in semantic consistency, suggesting that MIRA occasionally benefits from the extra reasoning capacity when handling more challenging edits. However, the overall changes remain small, indicating that iterative reasoning depth is not the primary factor governing final output quality. 5.4. Latency and Computational Cost Analysis Figure 1 and Table 1 illustrate that MIRA iterative reasoning markedly boosts editing quality, though at the expense of additional latency and computation. We analyze these overheads in this section. All latency measurements were conducted on single NVIDIA H100 GPU. For fair comparison, we report the time required to generate single 1024x1024 edited image. Using MIRA 7B paired with Flux.1-Kontext, and averaged across 500 benchmark samples, each MIRA reasoning step takes 0.746 seconds, while single Flux.1-Kontext inference takes 14.445 seconds. The framework performs an average of 4.111 iterations before issuing stop signal, yielding total end-to-end latency of 48.005 seconds. To contextualize this cost, we compare against two representative proprietary systems: GPT-Image requires 71.7 seconds and $0.17 per edit, while Nano-Banana achieves 12.3 seconds at $0.04 per edit. Although our iterative framework introduces additional latency, it remains practical, especially given that MIRA is fully open-source, and it provides substantial gains in semantic consistency and visual fidelity. 5.5. Inherent Robustness and Error Mitigation key strength of MIRAs iterative framework is its inherent robustness against error accumulation. Unlike other agentic frameworks that generate static and open-loop plan, MIRA performs stateful and closed-loop reasoning. At each step, it makes its decision by re-evaluating the latest editing output It1 against the constant goal defined by the input image I0 and the constant goal defined by C. This 8 Table 4. Ablation Study on the Impact of Maximum Inference Steps on MIRA Editing Performance. We evaluate Flux.1-Kontext + MIRA under different maximum number of inference step (from 3 to 7) and report semantic consistency and perceptual quality across multiple metrics. The best and second best results are in bold and underlined, respectively. Method GPT-SC Gemini-SC Qwen3VL-SC EditScore-SC ARNIQA TOPIQ EditScore-PQ EditScore-OA Flux.1-Kontext + MIRA + Max=3 Flux.1-Kontext + MIRA + Max=4 Flux.1-Kontext + MIRA + Max=5 Flux.1-Kontext + MIRA + Max=6 Flux.1-Kontext + MIRA + Max= 5.912 6.204 6.202 6.304 6.320 6.626 6.284 6.670 6.760 6.730 5.788 5.800 5.802 5.840 5.892 8.659 8.553 8.532 8.487 8.480 0.611 0.614 0.619 0.617 0.618 0.346 0.350 0.353 0.352 0. 5.550 5.410 5.473 5.406 5.337 6.785 6.654 6.610 6.628 6.556 allows MIRA to implicitly mitigate or correct minor deviations or execution errors from previous steps. Figure 5 provides representative example illustrating MIRAs inherent robustness to execution errors made by the external instruction-guided image editing model. Given the complex instruction: Change the white stove to black, let the floor be wooden, and let the white cabinets be wooden and brown, MIRA decomposes the request into sequence of visually grounded atomic edits and executes iteratively. In the first step (Atomic 1), MIRA successfully guides the base editing model to convert the floor into wooden texture. However, during Atomic 2, although all the white cabinets are correctly recolored to wooden and brown appearance, the refrigerator, whose color should remain white, is mistakenly changed to brown. It means that the editing model produces an incorrect or partially incorrect transformation. Because MIRA operates in closed-loop manner, it does not assume the correctness of any intermediate step. Instead, at each iteration, it re-analyzes the current intermediate image It1 together with the original image I0 and the input complex instruction C. After executing Atomic 3, MIRA detects that the refrigerator still does not match the intended appearance and therefore issues corrective atomic instruction in Atomic 4 (Change the wooden refrigerator to white). However, upon inspecting the result of Step 4, MIRA identifies new inconsistency: the stove, which should remain black, has been unintentionally changed to white during the previous correction. Consequently, MIRA generates another corrective instruction in Atomic 5 (Change the white stove to black). This demo instruction reflects an intentional errorstatemitigation strategy emerging from MIRA conditioned reasoning: the agent continuously verifies the current state and issues targeted corrections whenever deviations appear. After applying this final corrective action and confirming that the output image fully satisfies the instruction, MIRA emits the Stop signal in Atomic 6 to terminate the editing process. These results indicate that MIRA can effectively diagnose discrepancies introduced by editing models and dynamically adjust their editing trajectory, mitigating error propagation and preserving alignment with user intent. Table 5. Ablation Study on the Reliability of MIRA Termination Mechanism. We report the average number of inference steps taken by MIRA across our 500-sample benchmark when paired with Flux.1-Kontext under different maximum allowed inference step budgets. Method Max=3 Max=4 Max=5 Max=6 Max=7 Flux.1-Kontext + MIRA 2.976 3. 4.111 4.096 4.208 5.6. Reliability of the Termination Mechanism Table 5 reports the average number of actual reasoning steps taken by MIRA under different maximum step budgets. As the allowed iterations increase from 3 to 5, the average step count rises moderately (from 2.976 to 4.111). This trend reflects the fact that many instructions in our benchmark consist of multiple sub-tasks, often requiring at least three editing actions to complete. However, when the maximum step budget is further expanded from 5 to 7, the average actual steps remain nearly unchanged (4.096 to 4.208), indicating no meaningful scaling with the larger budget. These results show that MIRA termination behavior is goaldriven rather than budget-driven. Specifically: It refrains from using the full step budget, even when additional iterations are available. It mitigates over-editing while improveing the reliability. It consistently converges to stable number of reasoning steps on this benchmark ( 4 steps on average). 6. Conclusion We introduce MIRA, lightweight multimodal reasoning agent that reframes image editing as an iterative perceptionreasoningaction process. By generating atomic edit instructions and adapting with stepwise feedback, MIRA greatly enhances the capability of open-source image editing models on complex instruction-guided editing tasks. In addition, we also propose novel large-scale dataset, MIRA-EDITING, for training automatic tool-use models for instruction-guided image editing. Supported by MIRAEDITING and GRPO-based optimization with customized reward function, MIRA consistently improves semantic consistency and perceptual quality, narrowing the gap with proprietary systems. Empirical results highlight iterative multimodal reasoning as an effective, scalable, and novel paradigm for controllable and high-quality image editing."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by Goergen Institute for Data Science and Artificial Intelligence at University of Rochester."
        },
        {
            "title": "References",
            "content": "[1] Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, and Alberto Del Bimbo. Arniqa: Learning distortion maniIn Proceedings of the fold for image quality assessment. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 189198, 2024. 7 [2] Google AI. Gemini 2.5, . 6 [3] Google AI. Nano banana, . 1, 3, 6 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 6, 7 [5] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. 1, 3, 5, 6 [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 3, 6 [7] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. 1, 3 [8] He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, and Yu Li. Presto: Progressive pretraining enhances synthetic chemistry outcomes. arXiv preprint arXiv:2406.13193, 2024. [9] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 33:24042418, 2024. 7 [10] Chieh-Yun Chen, Min Shi, Gong Zhang, and Humphrey Shi. T2i-copilot: training-free multi-agent text-to-image system for enhanced prompt interpretation and interactive generation. arXiv preprint arXiv:2507.20536, 2025. 3 [11] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semanarXiv preprint tic image editing with mask guidance. arXiv:2210.11427, 2022. 3 [12] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 6 [13] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. 1, 3 [14] Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, and Ahmed Alaa. Instructcv: Instructiontuned text-to-image diffusion models as vision generalists. arXiv preprint arXiv:2310.00390, 2023. [15] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 3 [16] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1270912720, 2024. 3 [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3 [18] Advait Gupta, NandaKiran Velaga, Dang Nguyen, and Tianyi Zhou. Costa*: Cost-sensitive toolpath agent for multiturn image editing. arXiv preprint arXiv:2503.10613, 2025. 1, 3 [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 3 [20] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah Promptcap: Prompt-guided imSmith, and Jiebo Luo. In Proceedings of the age captioning for vqa with gpt-3. IEEE/CVF International Conference on Computer Vision, pages 29632975, 2023. [21] Yujia Hu, Songhua Liu, Zhenxiong Tan, Xingyi Yang, and Image editing as programs with diffusion Xinchao Wang. models. arXiv preprint arXiv:2506.04158, 2025. 1 [22] Hang Hua, Jing Shi, Kushal Kafle, Simon Jenni, Daoan Zhang, John Collomosse, Scott Cohen, and Jiebo Luo. Finematch: Aspect-based fine-grained image and text mismatch detection and correction. In European Conference on Computer Vision, pages 474491. Springer, 2024. 1 [23] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo Luo. Mmcomposition: Revisiting the compositionality arXiv preprint of pre-trained vision-language models. arXiv:2410.09733, 2024. 3 [24] Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Soo Ye Kim, Zhifei Zhang, Yilin Wang, Jianming Zhang, Zhe Lin, and Jiebo Luo. Finecaption: Compositional image captioning focusing on wherever you want at any granularity. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2476324773, 2025. [25] Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo Luo. V2xum-llm: Cross-modal video summarization with temIn Proceedings of the poral prompt instruction tuning. AAAI Conference on Artificial Intelligence, pages 3599 3607, 2025. 10 [26] Hang Hua, Ziyun Zeng, Yizhi Song, Yunlong Tang, Liu He, Daniel Aliaga, Wei Xiong, and Jiebo Luo. Mmigbench: Towards comprehensive and explainable evaluation of multi-modal image generation models. arXiv preprint arXiv:2505.19415, 2025. 3 [27] Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. Dialoggen: Multi-modal interactive dialogue system for multi-turn text-to-image generation. arXiv preprint arXiv:2403.08857, 2024. 3 [28] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8362 8371, 2024. 3 [29] Liya Ji, Chenyang Qi, and Qifeng Chen. Instruction-based image editing with planning, reasoning, and generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1750617515, 2025. 1 [30] Bohan Jia, Wenxuan Huang, Yuntian Tang, Junbo Qiao, Jincheng Liao, Shaosheng Cao, Fei Zhao, Zhaopeng Feng, Zhouhong Gu, Zhenfei Yin, et al. Compbench: Benchmarking complex instruction-guided image editing. arXiv preprint arXiv:2505.12200, 2025. [31] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. 3 [32] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for arXiv preprint conditional arXiv:2312.14867, 2023. 4, 7 image synthesis evaluation. [33] Zihan Liang, Jiahao Sun, and Haoran Ma. An llm-lvlm driven agent for iterative and fine-grained image editing. arXiv preprint arXiv:2508.17435, 2025. 1, 3 [34] Shuhang Lin, Wenyue Hua, Lingyao Li, Che-Jui Chang, Lizhou Fan, Jianchao Ji, Hang Hua, Mingyu Jin, Jiebo Luo, and Yongfeng Zhang. Battleagent: Multi-modal dynamic emulation on historical battles to complement historical analysis. arXiv preprint arXiv:2404.15532, 2024. 3 [35] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 1, 3, 5, [36] Xin Luo, Jiahao Wang, Chenyuan Wu, Shitao Xiao, Xiyan Jiang, Defu Lian, Jiajun Zhang, Dong Liu, et al. Editscore: Unlocking online rl for image editing via high-fidelity reward modeling. arXiv preprint arXiv:2509.23909, 2025. 7 [37] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 3 [38] AI Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai. meta. com/blog/llama-4-multimodal-intelligence/, checked on, 4 (7):2025, 2025. 3 [39] OpenAI. Chatgpt. https://openai.com, 2024. 6, 7 [40] OpenAI. Gpt-image. https://openai.com, 2025. 1, 3, [41] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward nextarXiv preprint generation multimodal image generation. arXiv:2509.20427, 2025. 1, 3, 6 [42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 6 [43] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. ObjectIn Prostitch: Object compositing with diffusion model. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1831018319, 2023. 3 [44] Yizhi Song, Liu He, Zhifei Zhang, Soo Ye Kim, He Zhang, Wei Xiong, Zhe Lin, Brian Price, Scott Cohen, Jianming Zhang, et al. Refine-by-align: Reference-guided artifacts arXiv preprint refinement through semantic alignment. arXiv:2412.00306, 2024. [45] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, and Daniel Aliaga. Imprint: Generative object compositing by learning identity-preserving representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80488058, 2024. 3 [46] Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang Tao. Latent chain-of-thought for visual reasoning. arXiv preprint arXiv:2510.23925, 2025. 3 [47] Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, et al. Chemagent: Self-updating library in large language models improves chemical reasoning. arXiv preprint arXiv:2501.06590, 2025. [48] Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, et al. Medagentsbench: Benchmarking thinking models and agent frameworks for complex medical reasoning. arXiv preprint arXiv:2503.07459, 2025. [49] Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, et al. Cellforge: Agentic design of virtual cell models. arXiv preprint arXiv:2508.02276, 2025. 3 [50] Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, et al. Video-lmm post-training: deep dive into video reasoning with large multimodal models. arXiv preprint arXiv:2510.05034, 2025. 3 [51] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Improving automatic masks for diffusionInstructedit: based image editing with user instructions. arXiv preprint arXiv:2305.18047, 2023. 3 11 region-aware vision language model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1309313103, 2025. [66] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 3 [52] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. Advances in Neural Information Processing Systems, 37:128374128395, 2024. 1, 3 [53] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 1, 3, 5, 6 [54] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 3, 6 [55] Bin Xia, Shiyin Wang, Yingfan Tao, Yitong Wang, and Jiaya Jia. Llmga: Multimodal large language model based generation assistant. In European Conference on Computer Vision, pages 389406. Springer, 2024. 1, 3 [56] Chun-Hsiao Yeh, Yilin Wang, Nanxuan Zhao, Richard Zhang, Yuheng Li, Yi Ma, and Krishna Kumar Singh. Beyond simple edits: X-planner for complex instruction-based image editing. arXiv preprint arXiv:2507.05259, 2025. 1, 3 [57] Yongsheng Yu and Jiebo Luo. Chain-of-thought prompting for demographic inference with large multimodal models. In 2024 IEEE International Conference on Multimedia and Expo (ICME), pages 17. IEEE, 2024. 3 [58] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. Promptfix: You prompt and we fix the photo. arXiv preprint arXiv:2405.16785, 2024. 1, [59] Yongsheng Yu, Ziyun Zeng, Haitian Zheng, and Jiebo Luo. Omnipaint: Mastering object-oriented editing via disentangled insertion-removal inpainting. arXiv preprint arXiv:2503.08677, 2025. 3 [60] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. 3 [61] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 3, 6 [62] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 1 [63] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 1, 3 [64] Haitian Zheng, Yuan Yao, Yongsheng Yu, Yuqian Zhou, Jiebo Luo, and Zhe Lin. Pixperfect: Seamless latent diffusion local editing with discriminative pixel-space refinement. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [65] Jun Zhou, Jiahao Li, Zunnan Xu, Hanhui Li, Yiji Cheng, Fa-Ting Hong, Qin Lin, Qinglin Lu, and Xiaodan Liang. Fireedit: Fine-grained instruction-based image editing via 12 Figure 6. Qualitative comparison of MIRA against leading proprietary and open-source image editing models on complex instructions. 13 Figure 7. Qualitative comparison of MIRA against leading proprietary and open-source image editing models on complex instructions."
        }
    ],
    "affiliations": [
        "MIT-IBM Watson AI Lab",
        "University of Rochester"
    ]
}