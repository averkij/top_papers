{
    "paper_title": "ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation",
    "authors": [
        "Edoardo Bianchi",
        "Jacopo Staiano",
        "Antonio Liotta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing approaches to skill proficiency estimation often rely on black-box video classifiers, ignoring multi-view context and lacking explainability. We present ProfVLM, a compact vision-language model that reformulates this task as generative reasoning: it jointly predicts skill level and generates expert-like feedback from egocentric and exocentric videos. Central to our method is an AttentiveGatedProjector that dynamically fuses multi-view features, projected from a frozen TimeSformer backbone into a language model tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60%. Our approach not only achieves superior accuracy across diverse activities, but also outputs natural language critiques aligned with performance, offering transparent reasoning. These results highlight generative vision-language modeling as a powerful new direction for skill assessment."
        },
        {
            "title": "Start",
            "content": "ProfVLM: Lightweight Video-Language Model for Multi-View Proficiency Estimation Edoardo Bianchi, Jacopo Staiano, Antonio Liotta 5 2 0 2 0 3 ] . [ 1 8 7 2 6 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Highlights",
            "content": "ProfVLM: Lightweight Video-Language Model for Multi-View Proficiency Estimation Edoardo Bianchi, Jacopo Staiano, Antonio Liotta Novel lightweight vision-language model architecture for multi-domain action quality assessment and proficiency estimation State-of-the-art performance achieved with 20x fewer parameters, 2x fewer frames, and 60% reduction in training time compared to existing baselines Efficient multi-view and multi-modal fusion architecture that generalizes across diverse domains Unified framework combining structured proficiency classification with open-ended conditional generation of expert-like feedback ProfVLM: Lightweight Video-Language Model for Multi-View Proficiency Estimation Edoardo Bianchia, Jacopo Staianob, Antonio Liottaa aFree University of Bozen-Bolzano, Via Bruno Buozzi 1, Bozen-Bolzano, 39100, Italy bUniversity of Trento, Via Inama 5, Trento, 38122, Italy Abstract Existing approaches to skill proficiency estimation often rely on black-box video classifiers, ignoring multi-view context and lacking explainability. We present ProfVLM, compact vision-language model that reformulates this task as generative reasoning: it jointly predicts skill level and generates expert-like feedback from egocentric and exocentric videos. Central to our method is an AttentiveGatedProjector that dynamically fuses multi-view features, projected from frozen TimeSformer backbone into language model tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60%. Our approach not only achieves superior accuracy across diverse activities, but also outputs natural language critiques aligned with performance, offering transparent reasoning. These results highlight generative vision-language modeling as powerful new direction for skill assessment. Keywords: Proficiency Estimation, Action Quality Assessment, Skill Assessment, Vision-Language Modeling, Video Understanding 1. Introduction Understanding proficiency in complex activities from video is crucial for applications such as coaching and rehabilitation. Unlike action recognition, proficiency estimation requires deep and technical understanding of the actions, non-trivial task even for state-of-the-art architectures. Further, producing simple proficiency label/score is not practical for the user: natural language feedback is, instead, actionable and self-explaining. Current approaches to proficiency estimation face several limitations. First, existing methods rely primarily on classification-only architectures that output numerical scores without explanatory context, limiting their practical applicability for end users who need actionable insights. Second, most prior work focuses on single-view analysis, despite the fact that comprehensive skill assessment often requires multiple camera perspectives to capture different aspects of performancesuch as hands positioning from egocentric views and overall technique from exocentric angles. Finally, while vision-language models have shown remarkable success in various multimodal tasks, their application to proficiency estimation remains unexplored, representing missed opportunity to combine visual understanding with interpretable textual feedback. To address these gaps, we present ProfVLM, vision-language model tailored for proficiency estimation, leveraging multi-view video encoders and lightweight backbone language model. ProfVLM generates both proficiency label and natural language feedback, bridging accurate assessment and practical usability. This paper introduces the following key contributions: lightweight vision-language model tailored for proficiency estimation across multiple domains, capable of generating both execution-level labels and natural language commentary; An AttentiveGatedProjector module that integrates and filters multiview video features via attention and learnable gating, projecting them into the LLM input space; An adaptive normalization mechanism that modulates the projected visual features to enhance alignment with the language embedding space. To the best of our knowledge, ProfVLM is the first vision-language model to perform multi-view proficiency estimation via open-ended text generation, moving beyond prior classification-only approaches. The results show how the proposed ProfVLM improves over baselines in multi-view accuracy, while using 20x fewer parameters (5.3M vs. 121M), 2x fewer frames (8 vs. 16), and 60% reduction in training time (6 vs. 15 epochs). Compared to recently published works, our model again achieves better accuracy using 5x fewer parameters (5.3M vs. 27M) and 2-4x fewer frames (8 vs. 16/24/32). This efficiency highlights the benefits of integrating language into skill assessment 2 Figure 1: ProfVLM architecture. Multi-view video is encoded by frozen TimeSformer, fused via trainable AttentiveGatedProjector, and decoded by LoRA-tuned SmolLM2 to generate both proficiency label and commentary. from multiple visual viewpoints. Additionally, ProfVLM introduces highquality feedback generation (BERTScore F1 of 85.53), feature not present in previous approaches, which provides actionable insights for users. 2. Background and Related Works The intersection of computer vision and sports analysis continues to expand across multiple domains, from team sports officiating systems that leverage multi-view analysis for foul detection (Held et al., 2025) to broadcast enhancement and performance analytics (Gade et al., 2024; Fujii, 2025). These applications demonstrate the growing integration of AI technologies in sports contexts, highlighting the need for robust and interpretable assessment systems that can operate across different skill levels and sporting disciplines. Action recognition remains highly active and evolving research area, with comprehensive surveys highlighting its rapid progress across diverse domains and methodologies (Zhao et al., 2024; sedaghati et al., 2025). Recent advances have explored temporal action segmentation in sparse action scenarios (Liu et al., 2024), multi-scale feature fusion with video-text constraints (Tian et al., 2025), graph neural networks for child activity recognition (Mohottala et al., 2025), and skeleton-enhanced spatio-temporal networks for sport action recognition (Bianchi and Lanz, 2025). Domain-specific applications have demonstrated the importance of tailored tracking solutions, as shown in skiing performance analysis where specialized tracking algorithms have been developed to handle the unique challenges of winter sports environments (Dunnhofer et al., 2024). Moving beyond classical action recognition tasks, Action Quality Assessment (AQA) and Proficiency Estimation target the evaluation of how well an action is performed. Early deep learning approaches relied on videoonly inputs and direct score regression using 3D CNNssee e.g. Tran et al. (2015, C3D), and Carreira and Zisserman (2017, I3D)or transformer-based modelssee e.g., Liu et al. (2021, VST)often optimized for continuous or discrete skill labels. More recent approaches employ spatio-temporal feature extraction and deep learning or self-supervised and transfer learning for action assessment in rehabilitation (Kryeem et al., 2025; Kourbane et al., 2025). Although effective, these models offer limited interpretability and require domain-specific supervision (Zhou et al., 2024). Multi-modal AQA and Proficiency Estimation enhance robustness and expressiveness by leveraging complementary modalities such as RGB, optical flow, depth, pose, and audio. Audio-visual fusion models such as SkatingMixer (Xia et al., 2022) adopt hierarchical integration strategies to exploit temporal alignment and rhythm. Others use RGB-depth pairs for industrial tasks (Bianchi and Lanz, 2024) or combine skeletal motion with visual input to capture nuanced kinematics in sport performance (Bianchi and Lanz, 2025; Du et al., 2015; Duan et al., 2022). Recent work has also explored domainspecific pose estimation challenges, such as extending human skeleton estimation to include particular sports equipment (Martinelli et al., 2024). Recent benchmarks such as EgoExoLearn (Huang et al., 2024) and EgoExo4D (Grauman et al., 2024) underscore the value of multi-view egocentric and exocentric perspectives. However, the baselines proposed in these works demonstrate that the task is far from trivial, emphasizing the critical need for specialized architectures and tailored training objectives. Parameter-efficient architectures like SkillFormer (Bianchi and Liotta, 2025b) have further advanced multi-view proficiency estimation by using cross-attention mechanisms, while physiological signal extraction approaches such as EgoPulseFormer (Braun et al., 2025) demonstrate how incorporating heart rate data from egocentric cameras can significantly improve skill assessment. Recent advances in temporal sampling strategies have demonstrated that effective skill assessment requires domain-specific adaptations, as different sports and activity categories necessitate distinct temporal parameters to capture fundamental movement patterns while preserving temporal continuity (Bianchi and Liotta, 4 2025a). However, most existing approaches rely on late or modality-specific fusion heuristics, which can hinder generalization across domains. In parallel, Vision-Language Models (VLMs) have been extended from image-text to video-text alignment. VideoBERT (Sun et al., 2019) pioneered joint video-text modeling by learning bidirectional joint distributions over sequences of visual and linguistic tokens derived from vector quantization. Image-focused VLMs like LLaVA (Liu et al., 2023) established strong baselines for visual instruction tuning and MiniGPT-4 (Zhu et al., 2023) demonstrated effective vision-language alignment with frozen image encoders. VideoLLaMA (Zhang et al., 2023) introduced instruction-tuned audio-visual language modeling with dual-branch architecture for comprehensive video understanding, while LLaMA-VID (Li et al., 2024) focused on efficient video comprehension through temporal token compression. VideoChatGPT (Maaz et al., 2024) and Video-LLaVA (Lin et al., 2024) further unified image and video modalities but initially lacked robust temporal grounding. Recent approaches include VALOR (Liu et al., 2025), which improved video-text alignment through contrastive learning, and SmolVLM (Marafioti et al., 2025), compact model that achieves state-of-the-art performance for its memory footprint while maintaining competitive multimodal capabilities. Advanced models such as Video-STaR (Zohar et al., 2024a) introduced instruction-tuning with labeled video corpora, while VideoAgent (Wang et al., 2024) and Apollo (Zohar et al., 2024b) framed video understanding as decision-making, improving long-form comprehension with efficient token compression, curriculum learning, or summarization tokens. Computational efficiency remains key consideration in both imageand video-based analysis, with recent advances combining knowledge distillation with tensor decomposition (Meneghetti et al., 2025) and applying pruning to transformer architectures to maintain accuracy while reducing computational costs (Li et al., 2025). Such efficiency considerations are particularly important for real-time applications in sports analysis and skill assessment, where rapid feedback can be crucial for training and performance improvement. Our proposed ProfVLM bridges generative VLMs and skill assessment by treating proficiency estimation as conditional language generation task. Built on frozen video transformers and LoRA-tuned language models, it jointly predicts structured proficiency levels and generates expert-like feedback. 5 3. Proposed Methodology We introduces ProfVLM, multimodal vision-language model for skill assessment from videos captured from multiple perspectives. The architecture comprises pre-trained TimeSformer (Bertasius et al., 2021) for visual encoding, an AttentiveGatedProjector which fuses multi-view features and projects them into the language embedding space, and parameter-efficient adaptation of the open-source SmolLM2-135M-Instruct (Allal et al., 2025), fine-tuned using LoRA (Hu et al., 2021). Figure 1 depicts the complete framework. 3.1. Problem Formulation The task of demonstrator proficiency estimation involves analyzing multiview video recordings of subject performing task, with the goal of assessing their skill level and providing qualitative feedback. Each sample consists of time-synchronized video streams from multiple viewpoints, specifically including one egocentric (first-person) and up to four exocentric (third-person) perspectives. Given this input, the objective is twofold: (1) predicting the demonstrators proficiency level as one of four predefined categories (Novice, Early Expert, Intermediate Expert, or Late Expert), and (2) generating natural language comment describing their performance. Formally, for given set of synchronized video streams V1, V2, . . . , Vn, the model must produce textual feedback , where denotes the space of valid commentary. The discrete proficiency label {0, 1, 2, 3} is implicitly generated as part of this feedback rather than being predicted through dedicated classification head. 3.2. Video Feature Extraction We employ TimeSformer backbone (Bertasius et al., 2021), pretrained on Kinetics-600 (Carreira and Zisserman, 2017), to extract spatio-temporal representations from video. Our choice of TimeSformer is motivated by two key factors: first, it serves as the established baseline architecture for evaluation in the EgoExo4D dataset (Grauman et al., 2024), enabling direct comparison with existing benchmarks; second, its divided space-time attention mechanism is specifically designed for video understanding, making it inherently superior to image-focused architectures that lack temporal modeling capabilities for proficiency estimation tasks. We retain the model frozen throughout training, without any fine-tuning or adaptation. 6 Figure 2: AttentiveGatedProjector architecture. Multi-view input features are initially normalized and combined using multi-head attention. The aggregated representation is then refined through feed-forward block with residual paths and trainable gating unit. Finally, the output undergoes linear projection, normalization, and alignment to the language model space via learned scaling and shifting parameters. For multi-view scenarios, we employ parallel processing by flattening the batch and view axes, enabling simultaneous feature extraction across all video streams through our shared frozen backbone. The extracted features are subsequently reshaped to maintain distinct view-specific representations before entering the fusion and projection module, preserving the view separation critical for effective cross-view integration. We fix inputs to 8-frame clips to maintain consistency with TimeSformers pretraining configuration while prioritizing computational efficiency. This choice enables our lightweight architecture to demonstrate significant advantages over existing approaches requiring 16-32 frames (see Table 3). Moreover, since the backbone remains frozen, increasing input frames would necessitate additional trainable parameters, compromising our efficiency objectives and real-world deployment capabilities on resource-constrained devices. 7 3.3. AttentiveGatedProjector: Multi-View Vision-Language Alignment We design the AttentiveGatedProjector, an architecture tailored for integrating multi-view video representations and aligning them with the input space of large language models. Though bearing resemblance to concurrent work (Bianchi and Liotta, 2025b) which targets classification, our module is instead optimized for joint vision-language modeling. While both approaches employ attention-based fusion mechanisms, they operate in fundamentally different learning paradigms: SkillFormer implements discriminative classification framework that maps multi-view videos through feature fusion and classification heads to discrete labels, whereas our AttentiveGatedProjector enables generative vision-language framework that projects multi-view videos through vision-language alignment into language models for natural language explanations with embedded labels. This difference necessitates distinct architectural components and optimization objectives. Unlike discriminative fusion modules that optimize for classification, our projector is designed for video-language integration and includes statistical distribution matching between visual and textual embeddings through learnable normalization parameters, cross-modal dimensional projection to match the target language models embedding space, and preservation of semantic coherence across autoregressive generation sequencesrequirements that are absent in classification-only architectures. The full architecture is illustrated in Figure 2 and detailed below. Independent View Normalization: Layer normalization is applied to each view (e.g., egoor exo-centric) independently. This ensures that statistical disparities across views are normalized early, reducing modalityspecific biases before fusion. Cross-View Multi-Head Attention: To learn inter-view dependencies, we employ multi-head attention mechanism where each view can contextualize its representation based on others through self-attention operations. This step flexibly captures complementary cues among perspectives, with attention weights dynamically determining the relevance of each view for the current sample. Learnable Gating Mechanism: The fused features are passed through gated transformation layer comprising feed-forward network followed by learnable sigmoid gate. This gate modulates each hidden unit independently through element-wise multiplication, enhancing discriminative signals while suppressing irrelevant or redundant information. The gating mechanism is 8 Role System User Assistant Content You are visual agent for human performance analysis. Here are 8 frames sampled from video: <video_start><video><video_end>. analyze the proficiency level of the subject. Proficiency Level: Intermediate Expert. Commentary: The subject demonstrates smooth and controlled movements throughout the sequence, with clear intent and minimal hesitation. Given this video, Proficiency Table 1: Prompt format used for training ProfVLM. During inference, only the System and User messages are provided, and the model generates both the proficiency level classification and explanatory commentary in unified text response. particularly crucial for generative tasks where noise suppression affects text coherence. Vision-Language Projection: Finally, the fused video features undergo two-stage projection process specifically designed for vision-language integration. The features are first mapped to the target language model dimensionality, then undergo statistical normalization that centers them to zero mean, scales to unit variance, and applies learnable parameters to match the statistical distribution of language model token embeddings. This statistical alignment is essential for maintaining semantic coherence during text generation. 3.4. Tokenization and LLM Prompt Structure To enable conditional generation, we extend the tokenizer vocabulary with three special tokens: <video_start>, <video_end>, and <video>. These delimiters explicitly mark the location of the visual input and allow the language model to attend to the fused video features in controlled way. Importantly, our model performs both tasksproficiency classification and commentary generationentirely through language modeling. That is, the discrete proficiency label is not produced via separate classification head but is instead embedded as part of the autoregressively generated textual response. We adopt structured prompt composed of system, user, and assistant messages in chat-style format, as shown in Table 1. Our approach follows the chat template used during the pretraining and instruction-tuning of the open-source SmolLM2-135M-Instruct (Allal et al., 2025) to maintain Official Experiment Train Val Train Scenario Basketball Cooking Dancing Music Bouldering Soccer 576 83 408 149 620 Total 1904 105 20 124 39 162 16 466 1104 381 1220 431 923 179 Val 105 20 124 39 162 16 466 Table 2: Comparison of original and experimental dataset statistics for train and validation splits. Our train set is larger because each sample includes multiple expert commentary annotations. The validation set is kept unchanged to ensure fair comparison. consistency with the models expected input structure. The system message defines the models role; the user message includes the visual input placeholder bracketed by the special tokens; and the assistant message contains the target label and commentary. During training, all three messages are used to condition and supervise generation. During inference, only the system and user messages are provided as input, and the model is required to generate the assistant message, producing both the predicted proficiency label and the natural language feedback as textual output. 4. Experimental Setup 4.1. Dataset We conduct experiments on the EgoExo4D dataset (Grauman et al., 2024), which contains over 1,200 hours of aligned egocentric and exocentric video from 740 participants across 123 locations. The dataset spans eight activitiescooking, health, bike maintenance, music, basketball, bouldering, soccer, and dancecaptured in diverse real-world settings. For the demonstrator proficiency estimation benchmark, six domains are used: cooking, music, basketball, bouldering, soccer, and dance. Each sample consists of one first-person video (Project Aria glasses (Engel et al., 2023)) and up to four third-person views (GoPro cameras), alongside synchronized audio, eye gaze, 3D pose, and language annotations. Notably, the proficiency label distribution is skewed toward higher skill levels (Intermediate and Late 10 Experts), reflecting the datasets intentional focus on recruiting participants with advanced abilities. The dataset provides two key supervision signals: discrete proficiency level (Novice, Early Expert, Intermediate Expert, or Late Expert) and freeform expert commentary from domain specialists. These commentaries were originally provided as verbal feedback by domain experts and subsequently transcribed from raw audio without manual curation or editing. This approach results in commentary text that preserves the evaluation style of domain experts, including spontaneous observations, colloquial language, and the natural flow of expert reasoning. However, it also introduces potential challenges such as transcription errors, incomplete sentences, repetitions, and varying levels of detail across commentaries. We use all videos with proficiency estimation labels from the dataset, adhering to the official benchmarks training and validation splits. As in prior work (Braun et al., 2025), we train on the official training set and evaluate on the full, held-out validation set to ensure comparability. Our training set effectively contains more samples, as each video may have multiple expert commentaries and we treat each unique video-commentary pair as separate training sample. Classification labels are explicitly integrated into the input text as prompts (see Sec. 3.4 and Tab. 1), enabling the model to leverage textual and label information jointly. Table 2 summarizes the dataset statistics, highlighting the expanded training size while maintaining the original validation set for fair evaluation. 4.2. Implementation Details We fine-tune ProfVLM in three view configurations: Ego (1 egocentric view), Exos (4 exocentric views), and Ego+Exos (all 5 views combined). Our training protocol consists of 6 epochs with batch size of 32, using cosine annealing learning rate schedule starting at 3e-4 with 1 warmup epoch. All experiments run on single NVIDIA A100 GPU (80GB). For video processing, we employ frozen TimeSformer (Bertasius et al., 2021) pre-trained on Kinetics-600 (Carreira and Zisserman, 2017). From each video, we extract 8 uniformly distributed frames from randomly positioned temporal window, applying sampling rate of 8 to ensure temporal coverage across variable-length clips. Each frame is pre-processed by resizing its shortest edge to 224 pixels, center-cropping to 224 224, rescaling to [0, 1], and normalizing with mean [0.45, 0.45, 0.45] and standard deviation [0.225, 0.225, 0.225]. 11 For language modeling, we use the open-source SmolLM2-135M-Instruct (Allal et al., 2025), fine-tuned via Low-Rank Adaptation (LoRA) (Hu et al., 2021) with rank = 8 and scaling factor α = 32. The AttentiveGatedProjector variant implements hidden size of 1024 with 4 attention heads, while the simpler MLP variant uses hidden size of 576 to match the LLMs input dimension. 4.3. Evaluation Metrics We employ comprehensive evaluation framework that addresses both the classification and natural language generation aspects of our model. For classification, we use accuracy as the primary metric since it is the standard measure in the official EgoExo4D benchmark (Grauman et al., 2024), ensuring direct comparability with existing methods. Given the class imbalance present in the dataset, which can affect accuracy scores, we additionally report F1-score to provide more robust evaluation. For natural language generation evaluation, we employ established automatic metrics: BERTScore (Zhang et al., 2020) for semantic similarity assessment using contextual embeddings, METEOR (Lavie and Agarwal, 2007) for comprehensive lexical and semantic alignment evaluation, and ROUGE (Lin, 2004) for n-gram overlap analysis. 5. Results We evaluate ProfVLM on the task of multimodal proficiency estimation using the EgoExo4D (Grauman et al., 2024) benchmark. The following sections provide detailed analysis of our models performance: the rationale behind our baseline selection (Sec.5.1), overall accuracy and efficiency (Sec.5.2), per-scenario accuracy (Sec.5.3), and qualitative evaluation of the generated textual feedback (Sec. 5.4). 5.1. Baseline Selection Rationale Before presenting our results, we clarify our choice of baselines. We focus our comparison on established multi-view proficiency estimation methods rather than general-purpose vision-language models (VLMs) for several key reasons. First, our approach differs fundamentally from general-purpose VLMs by reformulating proficiency estimation as unified generation task with specialized multi-view fusion mechanismscapabilities that generalpurpose VLMs inherently lack. Second, adapting general-purpose VLMs to 12 Pretrain Ego Exos Ego+Exos Frames Train Params Epochs Method Random Majority-class TimeSFormer TimeSFormer TimeSFormer TimeSFormer TimeSFormer - - - K400 HowTo100M EgoVLP EgoVLPv2 PandaGPT (0-shot) SmolVLM (0-shot) SmolVLM2 (0-shot) ImageBind + Vicuna - - EgoPulseFormer SkillFormer EgoPPG-DB K600 ProfVLM (MLP) ProfVLM (AGP) K600 + SmolLM2 K600 + SmolLM2 24.9 31.1 42.3 42.9 46.8 44.4 45.9 23.5 34.2 26.8 45.3 45.9 38.9 44.2 24.9 31.1 40.1 39.1 38.2 40.6 38. 25.6 30.7 30.5 35.9 46.3 44.2 45.1 24.9 31.1 40.8 38.6 39.7 39.5 37.8 25.8 31.1 32.2 42.4 47. 42.9 48.2 - - 16 16 16 16 16 - - - - - 121M 121M 121M 121M 121M 52M 256M 256M 16 16/24/ 121M 14/20/27M 8 8 2.2M 5.3M - - 15 15 15 15 15 - - - 15 6 6 Table 3: Comparison with EgoExo4D baselines, with 0-shot general-purpose VLMs (Su et al., 2023; Marafioti et al., 2025), and with concurrent approaches which adopt the official training-validation splits: EgoPulseFormer (Braun et al., 2025), and SkillFormer (Bianchi and Liotta, 2025b). Results are reported for Ego, Exos, and Ego+Exos views. Metric is top-1 accuracy (%) as in the official benchmark (Grauman et al., 2024) for direct comparison. ProfVLM+AGP surpasses state-of-the-art accuracy in the Ego+Exos setting using significantly less parameters and input frames. Best in bold, second-best underlined. our task would require substantial architectural modifications (e.g., multiview attention mechanisms, task-specific training protocols), making direct comparison unfair. To validate this approach, we tested general-purpose vision-language models on the EgoExo4D (Grauman et al., 2024) benchmark using zero-shot evaluation with basic frame concatenation (no fusion mechanisms). We chose three representative models with different scales and architectures. First, PandaGPT (Su et al., 2023) serves as our larger-scale baselinea multimodal model that combines an ImageBind encoder (Girdhar et al., 2023) with Vicuna-13B (Vicuna) and natively handles video input. Second, we evaluated SmolVLM and SmolVLM2 (Marafioti et al., 2025), which use SigLip-93M encoder (Zhai et al., 2023) paired with SmolLM2-135M (Allal et al., 2025). This second baseline is particularly important because it shares the same language model backbone as our approach, allowing us to assess whether using SmolLM2-135M alone could achieve competitive results without our specialized architectural components. Results are reported in Table 3 and confirm that general-purpose VLMs perform significantly worse than our approach due to their lack of proper Method View Hid Heads Train Params Acc (%) F1 (%) ProfVLM (MLP) ProfVLM (AGP) Ego Exos Ego+Exos Ego Exos Ego+Exos 576 576 576 1024 1024 1024 - - - 4 4 4 2.2M 2.2M 2.2M 5.3M 5.3M 5.3M 38.9 44.2 42.9 44.2 45.1 48.2 35.4 36.9 36.0 38.5 38.9 44.4 Table 4: Accuracy and F1 comparison between ProfVLM variants (MLP vs. AGP) across different view settings. All models share identical hyperparameters: learning rate = 3e-4, LoRA rank = 8, scaling = 32, 8 frames per video, 6 training epochs, batch size = 32. Bold: best results; underlined: second-best. Scenario Maj. Ego 51.43 Basketball 45.00 Cooking 55.65 Dancing 46.15 Music 25.31 Bouldering 56.25 Soccer 36.19 50.00 51.61 58.97 0.00 62.50 Baseline Exos Ego+Exos Ego ProfVLM (MLP) ProfVLM (AGP) Exos Ego+Exos Ego Exos Ego+Exos 52.30 35.00 42.74 69.23 17.28 75.00 55.24 35.00 42.74 56.41 17.28 75.00 34.00 41.00 40.02 58.89 33.08 69.75 38.00 51.00 48.97 56.26 38.11 69. 40.00 36.00 53.03 55.26 34.33 44.75 36.00 31.0 51.41 72.05 37.48 57.25 33.00 56.00 53.85 61.53 37.48 76.00 41.00 51.00 60.35 56.26 38.74 69.75 Table 5: Accuracy (%) comparison across six scenarios using egocentric (Ego), exocentric (Exos), and combined (Ego+Exos) views. Baseline from (Grauman et al., 2024). Maj. refers to the majority-class baseline. ProfVLM is evaluated with both simple MLP projector and our AttentiveGateProjector (AGP). Best results are in bold; second-best are underlined. multi-view fusion mechanisms and task-specific training. Therefore, we focus our evaluation against established multi-view methods like SkillFormer (Bianchi and Liotta, 2025b) and EgoPulseFormer (Braun et al., 2025), which represent the current state-of-the-art for this specific task on the EgoExo4D dataset. 5.2. Overall Classification Accuracy and Efficiency Table 3 presents classification accuracy (%) across egocentric (1 Ego), exocentric (4 Exos), and combined (1 Ego + 4 Exos) view configurations. Baseline results are taken directly from the EgoExo4D benchmark (Grauman et al., 2024), where models are trained exclusively on video inputs and perform classification using dedicated output head. In contrast, ProfVLM adopts vision-language formulation: it receives both visual input and tex14 tual supervision, and infers the proficiency level directly within the generated response, without relying on an explicit classification head. Our best-performing variant, ProfVLM with AttentiveGateProjector (AGP), surpasses the state-of-the-art with 48.2% accuracy in the combined Ego+Exos setting, outperforming the established EgoExo4D baselines as well as two contemporary works, SkillFormer (Bianchi and Liotta, 2025b) and EgoPulseFormer (Braun et al., 2025), that adopt the same standardized dataset splits. This alignment ensures fair comparison under identical evaluation protocols. The consistent gains across these benchmarks underscore the robustness of our approach. Table 4 shows that ProfVLM (AGP) consistently outperforms its MLP counterpart across all configurations. The gain is most notable in the Ego+Exos setting, where AGP achieves 48.2% accuracy and 44.4% F1, compared to 42.9% and 36.0% for the MLPa relative improvement of 12% in accuracy and 23% in F1. This highlights the effectiveness of our structured, attention-based cross-view fusion. ProfVLM is also considerably more efficient: it requires just 8 frames per video compared to 1632 in prior approaches, and utilizes only 5.3M trainable parameters versus 1427M in SkillFormer (5x reduction) and 121M in TimeSFormer-based models (20x reduction). Furthermore, our model converges in just 6 training epochs compared to 15 for TimeSFormer and EgoPulseFormer variants, representing 60% reduction in training time. These findings validate our vision-language formulation, which unifies classification and explanation within compact, interpretable framework. 5.3. Cross-Domain Accuracy Analysis Table 5 breaks down performance across six diverse scenarios in the EgoExo4D dataset. Our analysis reveals that ProfVLM outperforms the baselines in five out of six scenarios, with Basketball being the notable exception. Specifically, ProfVLM (AGP) achieves the highest accuracy in Dancing (60.35%), Bouldering (38.74%), Cooking (56.00%), Soccer (76.00%), and Music (72.05%), demonstrating remarkable cross-domain effectiveness. Several interesting patterns emerge from this analysis. First, the benefit of combining views varies by scenario: in Dancing, the Ego+Exos configuration significantly outperforms individual approaches (60.35% vs. 51.41%/53.8%), while in Soccer the exocentric view provides the strongest signal (76.00%). Second, the structured fusion mechanism in AGP consistently enhances performance over the simpler MLP projector, particularly in visually complex scenarios like Dancing and Bouldering. These domain-specific results highlight ProfVLMs ability to leverage complementary information from multiple viewpoints, where activities requiring spatial relationships and body positioning analysis (Dancing, Bouldering) benefit from integrated egocentric and exocentric information, while activities with standardized visual cues visible from specific angles (Music) perform equally well with single-view configurations. The Basketball scenario warrants deeper investigation as it represents the only domain where our approach underperforms the baseline (41.00% vs. 55.24%). This challenge likely stems from basketballs unique proficiency assessment requirements that differ fundamentally from other activities (Pan et al., 2025). Recent research has established basketball as one of the most analytically complex sports, with the BASKET dataset (Pan et al., 2025) demonstrating that state-of-the-art video models achieve accuracy barely above random chance despite advances in computer vision. Unlike domains where proficiency manifests through broader movement patterns, basketball skills depend on highly precise technical details: wrist angle during shooting, fingertip control in dribbling, and subtle postural adjustments (Sarlis and Tjortjis, 2020). Studies have shown that basketball analysis requires sophisticated spatio-temporal modeling to capture the complex interactions between players, ball dynamics, and tactical formations (Zhang et al., 2025; Lingrui et al., 2025). Our current temporal sampling strategy and spatial resolution may be insufficient to capture these fine-grained discriminative features that distinguish skill levels, particularly given basketballs requirements for spatial accuracy and high temporal resolution (Zhang et al., 2025). Additionally, the attentive fusion mechanism, while effective for integrating complementary information across views in other domains, may struggle with basketballs localized critical regions (hands, ball trajectory) where relevant signals could be suppressed by global attention patterns. The baselines superior performance suggests that simpler aggregation strategies may be more robust when discriminative features are highly localized and require precise temporal alignment, pointing toward the need for domain-adaptive attention mechanisms and higher-resolution temporal modeling for technical sports analysis in future work. 5.4. Language Generation Quality key capability of ProfVLM is its ability to generate detailed, expert-like feedback in natural language that explains the reasoning behind proficiency 16 Method View BERTScore (F1) METEOR ROUGE-L ProfVLM (MLP) ProfVLM (AGP) Ego Exos Ego+Exos Ego Exos Ego+Exos 85.41 85.41 85.51 85.41 85.51 85.53 18.07 17.98 18.22 18.06 17.33 18.23 15.55 15.63 15.49 14.47 15.67 15. Table 6: Evaluation of generated feedback quality for ProfVLM across different input views and projector architectures. BERTScore (F1) captures semantic similarity to expert commentaries, while METEOR and ROUGE assess lexical overlap. The variations in metric values are minimal across all configurations, suggesting consistent language generation quality regardless of view type or projector design. assessments. This dual functionalityproviding both classification scores and interpretable commentaryaddresses critical gap in automated skill evaluation where practitioners need actionable insights beyond numerical ratings. We evaluate this language generation capability using established text quality metrics as shown in Table 6. Since, to the best of our knowledge, no prior work addresses this task, we do not report comparisons with other models in this evaluation. 5.4.1. Semantic Alignment and BERTScore Validity Our approach achieves consistently high BERTScore (Zhang et al., 2020) F1 values (85.41-85.53) across all configurations, indicating strong semantic alignment with expert commentaries. For BERTScore, we use RoBERTalarge (Liu et al., 2019) as the underlying model, which captures semantic similarity beyond surface-level token matching through contextual embeddings. Importantly, the high BERTScore values reflect genuine semantic coherence rather than mere topical relevancethe metric effectively captures whether the generated feedback addresses the same technical aspects and maintains the domain-appropriate language patterns found in expert annotations. 5.4.2. Lexical vs. Semantic Metrics The relatively modest METEOR (Lavie and Agarwal, 2007) (17.33-18.23) and ROUGE-L (Lin, 2004) (14.47-15.67) scores reflect the open-ended nature of the feedback generation task. Unlike classification or short-answer generation, our model produces multi-sentence technical commentary where 17 Prediction BERTScore (F1) METEOR ROUGE-L High-quality example (bouldering): So here we can see the climber is going up to their right hand hold. And they have this really nice, pretty strong hold on this left side. But thats not good choice for this move because it would make them lose power in the same way as they are using it. So the climber was able to kind of get through with this one by just pushing off from the left foot and then putting down into the net so he could come out of his feet. The climbers got out of the grip and now theyre hitting this wall. Medium-quality example (dancing): So here we have the dancer performing step up. This is really great because its so simple and quick, but also very effective. So she does this step up with her arms out to the sides. Shes turning around at that point of time. So it feels really nice to see her do this movement in different direction. Here she steps through and into the space. This is what like about this move as well. Its kind of like dance-step where youre stepping forward. Low-quality example (basketball): Again, good job on that. Shes playing with her hips and just getting little bit of power here. But again, you can see shes got lot more energy in her hands than she should be having. And she has to really focus on the ball right now, which is why we want to know that youre throwing it straight down to your body. Right now, you have to hit this ball up high above your head so that when you release it, it will go off at that point. So would definitely give this one another shot. 87.14 36.22 20.22 85.10 24.04 14. 82.87 5.25 7.59 Table 7: ProfVLM commentary outputs with corresponding BERTScore (F1), METEOR, and ROUGE-L evaluation metrics. We present three samples representing high, medium, and low quality predicted proficiency feedback. semantic appropriateness is more critical than exact lexical overlap with reference text. This is particularly evident in Table 7, which shows that while BERTScore remains relatively stable across quality levels (87.14-82.87), METEOR and ROUGE-L metrics show substantial degradation in lower-quality examples (from 36.22 to 5.25 for METEOR and 20.22 to 7.59 for ROUGE-L). This pattern suggests that ProfVLM preserves domain-appropriate language and conceptual alignment even when diverging from the specific phrasing of expert commentaries. 5.4.3. Dataset-Driven Feedback Characteristics The nature of our generated feedback reflects the characteristics of the EgoExo4D datasets expert commentaries, which primarily focus on technical observations and skill assessment rather than prescriptive coaching or instructional guidance. Since the original expert commentaries were transcribed from raw audio without manual curation, our model learns to emulate both the expert evaluation style and the naturalistic speech patterns present in the training data, including occasional incomplete sentences and informal language. Our model generates feedback that identifies technical aspects of performance (e.g., \"grip positioning could be improved,\" \"timing of the movement shows good coordination\") rather than providing detailed corrective instructions. This alignment with the datasets annotation style explains both the semantic consistency captured by BERTScore and the technical, assessment-oriented nature of the generated text. 5.4.4. Model Efficiency Considerations Our choice of SmolLM2-135M as the language generation backbone represents an explicit trade-off between quality and deployment efficiency. With only 5.3M trainable parameters in the language component, ProfVLM maintains semantic coherence while enabling practical deployment in resourceconstrained environments. The consistently high BERTScore values across configurations demonstrate that this efficiency-focused design does not compromise semantic alignment with expert-level technical commentary. Overall, both quantitative metrics and qualitative analysis confirm that ProfVLM produces coherent and contextually appropriate commentary that maintains semantic alignment with expert annotations while reflecting the technical, observational style characteristic of the underlying dataset. 6. Limitations Despite its promising performance, ProfVLM has some limitations that suggest directions for future work. First, the model relies on language annotations during training, assuming that both proficiency labels and expert commentaries are available. In realworld scenarios, such supervision may be difficult to obtain at scale or may reflect subjective judgments that vary across annotators. Second, while ProfVLM demonstrates strong performance in generating coherent and semantically aligned feedback, as confirmed by our automatic 19 evaluation metrics, exploring additional dimensions of quality represents an exciting direction for future work. Although BERTScore effectively captures semantic similarity, aspects such as factual accuracy, pedagogical effectiveness, and actionability could be further assessed only through expert evaluation. Third, the model generates the class label as part of free-form text. Although this enables seamless integration of classification and explanation, it introduces potential downsidese.g., misformatted or ambiguous outputs may complicate downstream usage, especially in settings that require structured prediction or reliability guarantees. Fourth, the current design assumes fixed number of input views (e.g., Ego and Exos). Adapting the architecture to dynamically handle variable number of camera perspectives or synchronize inputs with inconsistent frame counts is an open challenge that would improve generalizability. Lastly, our implementation keeps the video encoder (TimeSFormer) frozen, which simplifies training and reduces computational cost but may limit domain adaptation. Fine-tuning the visual backbone or incorporating domainspecific motion priors could further enhance performance, particularly for tasks requiring fine-grained temporal understanding. Addressing these limitationsthrough improved supervision strategies, human-centered evaluation protocols, and more flexible fusion mechanismsoffers promising path for advancing vision-language models in skill assessment and beyond. 7. Discussion and Conclusions Our evaluation demonstrates that ProfVLM advances proficiency estimation through unified vision-language framework, surpassing state-of-theart classification accuracy (48.2% in Ego+Exos) on the EgoExo4D benchmark while generating semantically aligned textual feedback. The model outperforms video-only baselines and contemporary methods (SkillFormer, EgoPulseFormer) by integrating three key innovations: (1) proficiency inference via language generation instead of explicit classification heads, (2) the AttentiveGateProjector (AGP) for structured cross-view fusion, and (3) parameter-efficient design requiring only 5.3M trainable parameters and 6 training epochsyielding up to 20 computational efficiency gains versus transformer-based alternatives. 20 The AGP mechanism proves critical for multi-view integration, particularly in spatially demanding scenarios (e.g., Dancing, Bouldering), where it outperforms MLP-based fusion by 1223% in accuracy and F1. Cross-domain analysis reveals scenario-dependent view utility: exocentric inputs dominate in standardized tasks (e.g., Soccer), while combined views excel in activities requiring spatial reasoning. This adaptability underscores ProfVLMs capacity to balance complementary perspectives without architectural overcomplication. Performance in the Basketball domain remains challenging across all approaches, likely reflecting basketballs unique analytical complexity where proficiency depends on highly precise technical details that may exceed our current temporal and spatial resolution capabilities. Addressing these domainspecific challenges represents an important direction for future work. Beyond classification, ProfVLM generates feedback with strong semantic alignment to expert annotations (BERTScore >85), even when lexical overlap metrics (METEOR, ROUGE-L) are modesta reflection of its focus on conceptual correctness over rigid template matching. Qualitative analysis confirms coherent explanations across proficiency levels, validating the frameworks dual strength in perception and interpretability. In summary, ProfVLM establishes new paradigm unifying classification with natural language reasoning. Its efficient design bridges automated evaluation with expert-like analysis, offering transparent solution for multimodal proficiency estimation. Acknowledgements We acknowledge ISCRA for awarding this project access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CINECA (Italy). References Allal, L.B., Lozhkov, A., Bakouch, E., Blázquez, G.M., Penedo, G., Tunstall, L., Marafioti, A., Kydlíček, H., Lajarín, A.P., Srivastav, V., Lochner, J., Fahlgren, C., Nguyen, X.S., Fourrier, C., Burtenshaw, B., Larcher, H., Zhao, H., Zakka, C., Morlon, M., Raffel, C., von Werra, L., Wolf, T., 2025. Smollm2: When smol goes big data-centric training of small language model. URL: https://arxiv.org/abs/2502.02737, arXiv:2502.02737. 21 Bertasius, G., Wang, H., Torresani, L., 2021. Is space-time attention all you need for video understanding? URL: https://arxiv.org/abs/2102. 05095, arXiv:2102.05095. Bianchi, E., Lanz, O., 2024. Egocentric video-based human action recognition in industrial environments, in: Concli, F., Maccioni, L., Vidoni, R., Matt, D.T. (Eds.), Latest Advancements in Mechanical Engineering, Springer Nature Switzerland, Cham. pp. 257267. Bianchi, E., Lanz, O., 2025. Gate-shift-pose: Enhancing action recognition in sports with skeleton information, in: Proceedings of the Winter Conference on Applications of Computer Vision (WACV) Workshops, pp. 12571264. Bianchi, E., Liotta, A., 2025a. Pats: Proficiency-aware temporal sampling for multi-view sports skill assessment. URL: https://arxiv.org/abs/ 2506.04996, arXiv:2506.04996. Bianchi, E., Liotta, A., 2025b. Skillformer: Unified multi-view video understanding for proficiency estimation. URL: https://arxiv.org/abs/ 2505.08665, arXiv:2505.08665. Braun, B., Armani, R., Meier, M., Moebus, M., Holz, C., 2025. egoppg: Heart rate estimation from eye-tracking cameras in egocentric systems to benefit downstream vision tasks. URL: https://arxiv.org/abs/2502. 20879, arXiv:2502.20879. Carreira, J., Zisserman, A., 2017. Quo vadis, action recognition? new model and the kinetics dataset, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 47244733. doi:10.1109/ CVPR.2017.502. Du, Y., Wang, W., Wang, L., 2015. Hierarchical recurrent neural network for skeleton based action recognition, in: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11101118. doi:10.1109/ CVPR.2015.7298714. Duan, H., Zhao, Y., Chen, K., Lin, D., Dai, B., 2022. Revisiting skeletonbased action recognition, in: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 29592968. doi:10.1109/ CVPR52688.2022.00298. 22 Dunnhofer, M., Sordi, L., Martinel, N., Micheloni, C., 2024. Tracking skiers from the top to the bottom, in: 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 84968506. doi:10.1109/ WACV57701.2024.00832. Engel, J., Somasundaram, K., Goesele, M., Sun, A., Gamino, A., Turner, A., Talattof, A., Yuan, A., Souti, B., Meredith, B., Peng, C., Sweeney, C., Wilson, C., Barnes, D., DeTone, D., Caruso, D., Valleroy, D., Ginjupalli, D., Frost, D., Miller, E., Mueggler, E., Oleinik, E., Zhang, F., Somasundaram, G., Solaira, G., Lanaras, H., Howard-Jenkins, H., Tang, H., Kim, H.J., Rivera, J., Luo, J., Dong, J., Straub, J., Bailey, K., Eckenhoff, K., Ma, L., Pesqueira, L., Schwesinger, M., Monge, M., Yang, N., Charron, N., Raina, N., Parkhi, O., Borschowa, P., Moulon, P., Gupta, P., MurArtal, R., Pennington, R., Kulkarni, S., Miglani, S., Gondi, S., Solanki, S., Diener, S., Cheng, S., Green, S., Saarinen, S., Patra, S., Mourikis, T., Whelan, T., Singh, T., Balntas, V., Baiyya, V., Dreewes, W., Pan, X., Lou, Y., Zhao, Y., Mansour, Y., Zou, Y., Lv, Z., Wang, Z., Yan, M., Ren, C., Nardi, R.D., Newcombe, R., 2023. Project aria: new tool for egocentric multi-modal ai research. URL: https://arxiv.org/abs/2308.13561, arXiv:2308.13561. Fujii, K., 2025. Computer Vision for Sports Analytics. Springer Nature Singapore, Singapore. pp. 2157. URL: https://doi.org/10.1007/ 978-981-96-1445-5_2, doi:10.1007/978-981-96-1445-5_2. Gade, R., Merler, M., Thomas, G., Moeslund, T., 2024. The (Computer) Vision of Sports: Recent Trends in Research and Commercial Systems for Sport Analytics. CRC Press, Netherlands. pp. 296311. doi:10.1201/ 9781003328957-14. Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra, I., 2023. Imagebind one embedding space to bind them all. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 1518015190URL: https://api.semanticscholar.org/ CorpusID:258564264. Grauman, K., Westbury, A., Torresani, L., Kitani, K., Malik, J., Afouras, T., Ashutosh, K., Baiyya, V., Bansal, S., Boote, B., Byrne, E., Chavis, Z., Chen, J., Cheng, F., Chu, F.J., Crane, S., Dasgupta, A., Dong, J., Escobar, M., Forigua, C., Gebreselasie, A., Haresh, S., Huang, J., Islam, M.M., 23 Jain, S., Khirodkar, R., Kukreja, D., Liang, K.J., Liu, J.W., Majumder, S., Mao, Y., Martin, M., Mavroudi, E., Nagarajan, T., Ragusa, F., Ramakrishnan, S.K., Seminara, L., Somayazulu, A., Song, Y., Su, S., Xue, Z., Zhang, E., Zhang, J., Castillo, A., Chen, C., Fu, X., Furuta, R., Gonzalez, C., Gupta, P., Hu, J., Huang, Y., Huang, Y., Khoo, W., Kumar, A., Kuo, R., Lakhavani, S., Liu, M., Luo, M., Luo, Z., Meredith, B., Miller, A., Oguntola, O., Pan, X., Peng, P., Pramanick, S., Ramazanova, M., Ryan, F., Shan, W., Somasundaram, K., Song, C., Southerland, A., Tateno, M., Wang, H., Wang, Y., Yagi, T., Yan, M., Yang, X., Yu, Z., Zha, S.C., Zhao, C., Zhao, Z., Zhu, Z., Zhuo, J., Arbelaez, P., Bertasius, G., Damen, D., Engel, J., Farinella, G.M., Furnari, A., Ghanem, B., Hoffman, J., Jawahar, C., Newcombe, R., Park, H.S., Rehg, J.M., Sato, Y., Savva, M., Shi, J., Shou, M.Z., Wray, M., 2024. Ego-exo4d: Understanding skilled human activity from firstand third-person perspectives, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1938319400. Held, J., Cioppa, A., Giancola, S., Hamdi, A., Devue, C., Ghanem, B., Van Droogenbroeck, M., 2025. Towards an ai-powered video assistant referee system (vars) for association football, in: Petersen, J., Dahl, V.A. (Eds.), Image Analysis, Springer Nature Switzerland, Cham. pp. 295309. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., 2021. Lora: Low-rank adaptation of large language models. URL: https://arxiv.org/abs/2106.09685, arXiv:2106.09685. Huang, Y., Chen, G., Xu, J., Zhang, M., Yang, L., Pei, B., Zhang, H., Lu, D., Wang, Y., Wang, L., Qiao, Y., 2024. Egoexolearn: dataset for bridging asynchronous egoand exo-centric view of procedural activities in real world, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Kourbane, I., Papadakis, P., Andries, M., 2025. Ssl-rehab: Assessment of physical rehabilitation exercises through self-supervised learning of 3d skeleton representations. Computer Vision and Image Understanding 251, URL: https://www.sciencedirect.com/science/article/ 104275. pii/S1077314224003564, doi:https://doi.org/10.1016/j.cviu.2024. 104275. 24 Kryeem, A., Boutboul, N., Bear, I., Raz, S., Eluz, D., Itah, D., Hel-Or, H., Shimshoni, I., 2025. Action assessment in rehabilitation: Leveraging machine learning and vision-based analysis. Computer Vision and Image Understanding 251, 104228. URL: https://www.sciencedirect. com/science/article/pii/S1077314224003096, doi:https://doi.org/ 10.1016/j.cviu.2024.104228. Lavie, A., Agarwal, A., 2007. Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments, in: Proceedings of the Second Workshop on Statistical Machine Translation, Association for Computational Linguistics, USA. p. 228231. Li, W., Liu, M., Liu, H., Wang, P., Lu, S., Sebe, N., 2025. H2ot: Hierarchical hourglass tokenizer for efficient video pose transformers. IEEE Transactions on Pattern Analysis and Machine Intelligence , 115doi:10. 1109/TPAMI.2025.3608284. Li, Y., Wang, C., Jia, J., 2024. Llama-vid: An image is worth in: Computer Vision ECCV 2 tokens in large language models, 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part XLVI, Springer-Verlag, Berlin, Heidelberg. p. 323340. URL: https://doi.org/10.1007/978-3-031-72952-2_19, doi:10.1007/978-3-031-72952-2_19. Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., Yuan, L., 2024. Videollava: Learning united visual representation by alignment before projection. URL: https://arxiv.org/abs/2311.10122, arXiv:2311.10122. Lin, C.Y., 2004. ROUGE: package for automatic evaluation of summaries, in: Text Summarization Branches Out, Association for Computational Linguistics, Barcelona, Spain. pp. 7481. URL: https://aclanthology. org/W04-1013/. Lingrui, X., Mandi, L., Lei, Z., 2025. Tacticexpert: Spatial-temporal graph language model for basketball tactics. URL: https://arxiv.org/abs/ 2503.10722, arXiv:2503.10722. Liu, H., Li, C., Wu, Q., Lee, Y.J., 2023. Visual instruction tuning, in: NeurIPS. 25 Liu, J., Chen, S., He, X., Guo, L., Zhu, X., Wang, W., Tang, J., 2025. Valor: Vision-audio-language omni-perception pretraining model and dataset. IEEE Transactions on Pattern Analysis and Machine Intelligence 47, 708 724. doi:10.1109/TPAMI.2024.3479776. Liu, Y., Cheng, X., Li, Y., Ikenaga, T., 2024. Bidirectional temporal and frame-segment attention for sparse action segmentation of Computer Vision and Image Understanding 249, figure skating. 104186. URL: https://www.sciencedirect.com/science/article/ pii/S1077314224002674, doi:https://doi.org/10.1016/j.cviu.2024. 104186. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V., 2019. Roberta: robustly optimized bert pretraining approach. URL: https://arxiv.org/abs/1907.11692, arXiv:1907.11692. Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H., 2021. Video swin transformer. URL: https://arxiv.org/abs/2106.13230, arXiv:2106.13230. Maaz, M., Rasheed, H., Khan, S., Khan, F.S., 2024. Video-chatgpt: Towards detailed video understanding via large vision and language models. URL: https://arxiv.org/abs/2306.05424, arXiv:2306.05424. Marafioti, A., Zohar, O., Farré, M., Noyan, M., Bakouch, E., Cuenca, P., Zakka, C., Allal, L.B., Lozhkov, A., Tazi, N., Srivastav, V., Lochner, J., Larcher, H., Morlon, M., Tunstall, L., von Werra, L., Wolf, T., 2025. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299 . Martinelli, G., Diprima, F., Bisagno, N., Conci, N., 2024. Ski pose estimation, in: 2024 IEEE International Workshop on Sport, Technology and Research (STAR), pp. 120125. doi:10.1109/STAR62027.2024.10635966. Meneghetti, L., Bianchi, E., Demo, N., Rozza, G., 2025. Kd-ahosvd: Neural network compression via knowledge distillation and tensor decomposition, in: Lorandel, J., Kamaleldin, A. (Eds.), Design and Architecture for Signal and Image Processing, Springer Nature Switzerland, Cham. pp. 8192. 26 Mohottala, S., Gawesha, A., Kasthurirathna, D., Samarasinghe, P., Abhayaratne, C., 2025. Spatio-temporal graph neural network based child action recognition using data-efficient methods: systemComputer Vision and Image Understanding 259, atic analysis. 104410. URL: https://www.sciencedirect.com/science/article/ pii/S107731422500133X, doi:https://doi.org/10.1016/j.cviu.2025. 104410. Pan, Y., Zhang, C., Bertasius, G., 2025. Basket: large-scale video dataset for fine-grained skill estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 28952 28962. Sarlis, V., Tjortjis, C., 2020. Sports analytics evaluation of basInformation Systems 93, URL: https://www.sciencedirect.com/science/article/ doi:https://doi.org/10.1016/j.is.2020. ketball players and team performance. 101562. pii/S0306437920300557, 101562. sedaghati, N., ardebili, S., Ghaffari, A., 2025. Application of human activity/action recognition: review. Multimedia Tools and Applications 84, 3347533504. URL: https://doi.org/10.1007/s11042-024-20576-2, doi:10.1007/s11042-024-20576-2. Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., Cai, D., 2023. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355 . Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C., 2019. VideoBERT: Joint Model for Video and Language Representation Learning , in: 2019 IEEE/CVF International Conference on Computer Vision (ICCV), IEEE Computer Society, Los Alamitos, CA, USA. pp. 7463 7472. URL: https://doi.ieeecomputersociety.org/10.1109/ICCV. 2019.00756, doi:10.1109/ICCV.2019.00756. Tian, Q., Zeng, F., Ning, J., Zhang, L., 2025. Unimultnet: Action recognition method based on multi-scale feature fusion and video-text constraint guidance. Computer Vision and Image Understanding 260, 104456. URL: https://www.sciencedirect.com/science/article/ pii/S1077314225001791, doi:https://doi.org/10.1016/j.cviu.2025. 104456. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M., 2015. Learning spatiotemporal features with 3d convolutional networks. URL: https: //arxiv.org/abs/1412.0767, arXiv:1412.0767. Vicuna, . Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https://lmsys.org/blog/2023-03-30-vicuna/. Wang, X., Zhang, Y., Zohar, O., Yeung-Levy, S., 2024. Videoagent: Longform video understanding with large language model as agent. URL: https://arxiv.org/abs/2403.10517, arXiv:2403.10517. Xia, J., Zhuge, M., Geng, T., Fan, S., Wei, Y., He, Z., Zheng, F., 2022. Skating-mixer: Long-term sport audio-visual modeling with mlps. URL: https://arxiv.org/abs/2203.03990, arXiv:2203.03990. Zhai, X., Mustafa, B., Kolesnikov, A., Beyer, L., 2023. Sigmoid loss for language image pre-training, in: 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1194111952. doi:10.1109/ICCV51070. 2023.01100. Zhang, H., Li, X., Bing, L., 2023. Video-LLaMA: An instructiontuned audio-visual language model for video understanding, in: Feng, Y., Lefever, E. (Eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics, Singapore. pp. 543 553. URL: https://aclanthology.org/2023.emnlp-demo.49/, doi:10. 18653/v1/2023.emnlp-demo.49. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y., 2020. Bertscore: Evaluating text generation with bert. URL: https://arxiv.org/abs/ 1904.09675, arXiv:1904.09675. Zhang, Z., Liu, W., Zheng, Y., Du, L., Sun, L., 2025. Learning spatiotemporal context for basketball action pose estimation with multi-stream network. Scientific Reports 15, 29173. URL: https://doi.org/10.1038/ s41598-025-14985-y, doi:10.1038/s41598-025-14985-y. Zhao, L., Lin, Z., Sun, R., Wang, A., 2024. review of state-of-theart methodologies and applications in action recognition. Electronics 13. URL: https://www.mdpi.com/2079-9292/13/23/4733, doi:10.3390/ electronics13234733. Zhou, K., Cai, R., Wang, L., Shum, H.P.H., Liang, X., 2024. comprehensive survey of action quality assessment: Method and benchmark. URL: https: //arxiv.org/abs/2412.11149, arXiv:2412.11149. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M., 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. URL: https://arxiv.org/abs/2304.10592, arXiv:2304.10592. Zohar, O., Wang, X., Bitton, Y., Szpektor, I., Yeung-Levy, S., 2024a. Videostar: Self-training enables video instruction tuning with any supervision. URL: https://arxiv.org/abs/2407.06189, arXiv:2407.06189. Zohar, O., Wang, X., Dubois, Y., Mehta, N., Xiao, T., Hansen-Estruch, P., Yu, L., Wang, X., Juefei-Xu, F., Zhang, N., Yeung-Levy, S., Xia, X., 2024b. Apollo: An exploration of video understanding in large multimodal models. URL: https://arxiv.org/abs/2412.10360, arXiv:2412.10360."
        }
    ],
    "affiliations": [
        "Free University of Bozen-Bolzano, Via Bruno Buozzi 1, Bozen-Bolzano, 39100, Italy",
        "University of Trento, Via Inama 5, Trento, 38122, Italy"
    ]
}