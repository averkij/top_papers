{
    "paper_title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
    "authors": [
        "Songhua Liu",
        "Zhenxiong Tan",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 2 ] . [ 1 2 1 1 6 1 . 2 1 4 2 : r CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up"
        },
        {
            "title": "Songhua Liu",
            "content": "Zhenxiong Tan Xinchao Wang National University of Singapore {songhua.liu, zhenxiong}@u.nus.edu, xinchao@nus.edu.sg Figure 1. Ultra-resolution results generated by the linearized FLUX.1-dev model with our approach CLEAR. Resolution is marked on the top-right corner of each result in the format of widthheight. Corresponding prompts can be found in the appendix."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiT) have become leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce convolution-like local attention strategy termed CLEAR, which limits feature interactions to local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K selfgenerated samples for 10K iterations, we can effectively *Corresponding Author. transfer knowledge from pre-trained DiT to student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here. 1. Introduction Diffusion models [14, 25, 41, 49] have gained widespread attention in text-to-image generation, proving to be highly effective for producing high-quality and diverse images from textual prompts [10, 64]. Traditionally, architectures based on UNet [49, 50] have dominated this field due to their robust generative capabilities. In recent years, Diffusion Transformers (DiTs) [1, 5, 6, 17, 18, 35, 43] have emerged as promising alternative, achieving leading performance in this field. Unlike the UNet-based architectures, we present convolution-like linearization strategy termed CLEAR, where each query interacts only with tokens within predefined distance r. Since the number of key-value tokens interacting with each query is fixed, the resulting DiT achieves linear complexity with respect to image resolution. To our surprise, such concise design yields results comparable to original FLUX.1-dev after knowledge distillation process [24] with merely 10K fine-tuning iterations on 10K self-generated samples. As shown in Fig. 1, CLEAR exhibits satisfactory cross-resolution generalizability, property also reflected in UNet-based diffusion models [2, 15, 22, 28]. For ultra-high-resolution generation like 8K, it reduces attention computations by 99.5% and accelerates the original DiT by 6.3 times, as shown in Fig. 2. The distilled local attention layers are also compatible with different variants of the teacher model, e.g., FLUX.1-dev and FLUX.1-schnell, and various pre-trained plugins like ControlNet [69] without requiring any adaptation. As the token interactions are performed locally, it is convenient for CLEAR to support multi-GPU parallel inference. We further develop patch-parallel paradigm that minimizes communication overhead. Our contribution can be summarized as follows: We provide taxonomic overview of recent efficient attention mechanisms and identity four elements essential for linearizing pre-trained DiTs. Based on them, we propose convolution-like local attention mechanism termed CLEAR as an alternative to default attention, which is the first linearization strategy tailored for pre-trained DiT to the best of our knowledge. We delve into multiple satisfactory properties of CLEAR through experiments, including its comparable performance with the original DiT, linear complexity, crossresolution generalizability, cross-model/plugin generalizability, support for multi-GPU parallel inference, etc. The rest of this paper is organized as follows: we summarize recent efficient attention mechanisms in Sec. 2, present our main method in Sec. 3, experimentally validate its effectiveness in Sec. 4, and finally concludes this paper in Sec. 5. 2. Efficient Attention: Taxonomic Overview The attention mechanism [56] is known for its flexibility in modeling token-wise relationships. It takes query matrix Rnc, key matrix Rmc, and value matrix Rmc as input and produces an output matrix Rnc via: = softmax( QK )V, (1) where and are the numbers of query and key-values tokens respectively, and and are the feature dimensions for query-key and value tokens. In line with standard design conventions, we assume = throughout this paper, and Figure 2. Comparison of speed and GFLOPS between the proposed linearized DiT and the original FLUX.1-dev. Speed is evaluated by performing 20 denoising steps on single H100 GPU. FLOPS is calculated with the approximation: 4(cid:80) c, where is the feature dimension and denotes the attention masks. log2 is applied on both vertical axes for better visualization. The raw data are supplemented in the appendix. DiTs leverage the attention mechanism [56] to model intricate token-wise relationships with remarkable flexibility, enabling them to capture nuanced dependencies across all tokens in images and texts, and thus produce visually rich and coherent outputs. Despite their impressive performance, the attention layerswhich model intricate pairwise token relationships with quadratic complexitycan introduce substantial latency in high-resolution image generation. As shown in Fig. 2, FLUX.1-dev [33], state-of-the-art text-to-image DiT, requires over 30 minutes to generate 8K-resolution images with 20 denoising steps, even with hardware-aware optimizations like FlashAttention [11, 13]. Focusing on these drawbacks, we are curious about one Is it possible to convert prequestion in this paper: trained DiT to achieve linear complexity? The answer is not straightforward, in fact, as it remains unclear whether existing efficient attention mechanismsdespite their recent widespread exploration [3, 8, 12, 20, 27, 30, 48, 51, 58, 65, 67, 68, 71]can be effectively applied to pre-trained DiTs. To answer this question, we initiate our exploration with summary of previous methods dedicated to efficient attention, categorizing them into three main strategies: formulation variation, key-value compression, and key-value sampling. We then experiment with fine-tuning the model by replacing the original attention layers with these efficient alternatives. Results indicate that while formulation variation strategies have proven effective in attention-based UNets [38] and DiTs trained from scratch [62], they do not yield similar success with pre-trained DiTs. Key-value compression often leads to distorted details, and key-value sampling highlights the necessity of local tokens for each query to generate visually coherent results. Building on these observations, we figure out four elements crucial for for linearizing pre-trained DiTs, including locality, formulation consistency, high-rank attention maps, and feature integrity. Satisfying all these criteria, 2 in the case of self-attention, Q, K, and come from the same feature maps with = n. As shown in Eq. 1, self-attention involves constructing attention maps to model pair-wise token-to-token relationships, which results in both time and memory complexity. To address this issue, numerous studies focus on developing efficient attention mechanisms. In this section, we summarize recent work in this area and assess its applicability to DiT linearization. Specifically, we categorize existing approaches into three main categories: formulation variation, key-value compression, and key-value sampling. 2.1. Formulation Variation Revisiting Eq. 1, if the softmax operation is omitted, we can first compute , yielding matrix with linear time in relation to n. In this way, series of linear attention mechanisms apply kernel functions () and g() to and respectively to mimic the effect of softmax: = (Q)g(K)V, (2) such as Mamba2 [12], Gated Linear Attention [65], and Generalized Linear Attention [38]. Another mainstream of methods try to replace the softmax operation with efficient alternatives, like sigmoid [48], relu2 [27, 67], and Nystrom-based approximation [63]. 2.2. Key-Value Compression In the default setting of self-attention, the numbers of query and key-value tokens are consistent, i.e., = n, and the shape of the attention map would be nn. It is thus promising to compress key-value tokens so that can be smaller than to reduce the complexity. Following this routine, PixArt-Sigma [6] compress KV tokens locally with downsampling Conv2d operator. Agent Attention [20] first conducts attention with downsampled and full-sized and to select agent KV tokens for compression. Then, original would interact with these compressed tokens. Similarly, Slot Attention [71] adopts learnable slots to obtain agent KV. Linformer [58] introduces learnable maps to obtain compressed tokens from the original ones. 2.3. Key-Value Sampling Efficient attention based on key-value sampling is based on the assumption that not all key-value tokens are important for query and the attention matrix is highly sparse. Comparing with key-value compression, it prunes original key-value tokens for each token instead of producing new key-value tokens. For instance, Strided Attention [7] samples one key-value token at regular interval. Routing Attention [51] samples key-value tokens based on grouping. Swin Transformer [39] divides feature maps into nonoverlapping local windows and performs attention independently for each window. Neighborhood Attention [21] Method Locality Formulation Consistency High-Rank Attention Maps Feature Integrity Linear Attention [12, 30, 38, 65] Sigmoid Attention [48] PixArt-Sigma [6] Agent Attention [20] Strided Attention [7] Swin Transformer [39] Neighborhood Attention [21] Yes Yes Yes Maybe No Yes Yes No No Yes Yes Yes Yes Yes No Yes Yes Yes Yes No Yes Yes Yes No No Yes Yes Yes Table 1. Summary of existing efficient attention mechanisms based on the four factors crucial for linearizing DiTs. selects key-value tokens within local window around each query. BigBird [68] uses token selection strategy combining neighborhood attention and random attention, and LongFormer [3] combines neighborhood attention with some global tokens that are visible to all tokens. 3. Methods 3.1. What are Crucial for Linearizing DiTs? Building on the overview of recent efficient attention mechanisms in Sec. 2, we explore key question here: What specific features are essential for successfully linearizing pre-trained DiTs? We thus try substituting all the attention layers in FLUX.1-dev with various efficient alternatives and fine-tuning parameters in these layers. The preliminary text-to-image results are shown in Fig. 3, through which we figure out four key elements: locality, formulation consistency, high-rank attention maps, and feature integrity. According to these perspectives, we summarize some previous efficient attention methods in Tab. 1. Locality indicates that key-value tokens fallen in the neighborhood of query are included for attention. From Fig. 3, we observe that many methods equipped with this feature yield at least plausible results, like PixArt-Sigma, Swin Transformer, and Neighborhood Attention. Particularly, comparing the results of Neighborhood Attention and Strided Attention, we find that incorporating local key-value tokens diminishes lot of distorted patterns. The reason for these phenomenons is that pre-trained DiTs, such as FLUX, rely heavily on local features to manage token relationships. To validate this, we visualize attention maps in Fig. 4 and observe that most significant attention scores fall in the local area around each query. And in Fig. 5, we provide further evidence to illustrate the importance of local features, that perturbing remote features would not damage the quality of FLUX.1-dev much. Specifically, FLUX.1-dev relies on rotary position embedding [55] to perceive spatial relationships and is sensitive to the relative distance (d(x) ij , d(y) ij ) on the two axes of 2D feature map, where indices and denotes query and key token indices respectively. We perturb remote features by clipping the relative distances for rotary position embedding to maximum value when they exceed this threshold, i.e., d() ij .clip(r, r). As shown in Fig. 5(left), ij = d() Figure 3. Preliminary results of various efficient attention methods on FLUX-1.dev. The prompt is small blue plane sitting on top of field. Figure 5. We try perturbing remote and local features respectively through clipping the relative distances required for rotary position embedding. Perturbing remote features has no obvious impact on image quality, whereas altering local features results in significant distortion. The text prompt and the original generation result are consistent with Fig. 3. compression on deep layers would not hurt the performance much, this approach is not suitable for completely linearizing pre-trained DiTs. As shown in Fig. 3, methods based on KV compression, such as PixArt-Sigma and Agent Attention, tend to produce distorted textures compared to the results from Swin Transformer and Neighborhood Attention, which highlights the necessity to preserve the integrity of the raw query, key, and value tokens. 3.2. Conv-Like Linearization Given the above analysis of the crucial factors for linearizing DiTs, Neighborhood Attention is the only scheme satisfying all the four constraints. Motivated on this, we propose CLEAR, conv-like linearization strategy tailored for pre-trained DiTs. Specifically, given that state-of-the-art DiTs for text-to-image generation, like FLUX and StableDiffusion 3 series [17], typically adopt text-image joint selfattention for feature interaction, for each text query, it still gathers features from all text and image key-value tokens; while for each image query, it interacts with all text tokens and local key-value tokens fallen in local window around it. Since the number of text tokens and the local window size remain constant as resolution increases, the overall complexity scales linearly with the number of image tokens. Unlike Neighborhood Attention and standard 2D convolution, which use square sliding local window, CLEAR adopts circular windows, where key-value tokens within Euclidean distance less than predefined radius are considered for each query. Comparing with corresponding square windows, the computation overhead introduced by this design is π 4 times. Formally, the attention mask Figure 4. Visualization of attention maps by various heads for an intermediate denoising step. Attention in pre-trained DiTs is largely conducted in local fashion. the results are reasonable for 64 64 feature map when is as small as 8. Conversely, if we perturb local features by setting their minimum absolute distances to r, even with as small as 2, the result still collapses as shown in Fig. 5(right)emphasizing the importance of locality. Formulation Consistency denotes that the efficient attention still apply the softmax-based formulation of the scaled dot-product attention. LinFusion [38] has shown that linear attention approaches like linear attention achieve promising results in attention-based UNets. However, we find that it is not the case for pre-trained DiTs, as shown in Fig. 3. We speculate that it is due to attention layers being the only modules for token interactions in DiTs, unlike the case in U-Nets. Substituting all of them would have substantial impact on the final outputs. Other formulations like Sigmoid Attention fails to converge within limited number of iterations, unable to mitigate the divergence between the original and modified formulations. It is thus beneficial to maintain consistency with the original attention function. High-Rank Attention Maps means that attention maps calculated by efficient attention alternatives should be sufficient to capture the intricate token-wise relationships. As visualized in Fig. 4, extensive attention scores are concentrated along the diagonal, indicating that the attention maps do not exhibit the low-rank property assumed by many prior works. That is why methods like linear attention and Swin Transformer largely produce blocky patterns. Feature Integrity implies that raw query, key, and value features are more favorable than the compressed ones. Although PixArt-Sigma has demonstrated that applying KV 4 Figure 6. Illustration of the proposed convolution-like linearization strategy for pre-trained DiTs. In each text-image joint attention module, text queries aggregate information from all text and image tokens, while each image token gathers information only from tokens within local circular window. is constructed as follows: (cid:40) Mij = if ntext or ntext or d(x)2 ij + d(y)2 ij < r2; 1, 0, otherwise, (3) where ntext denotes the number of text tokens. Fig. 6 illustrates this paradigm. 3.3. Training and Optimization Although each query only has access to tokens within local window, stacking multiple Transformer blocks enables each token to gradually capture holistic informationsimilar to the way convolutional neural networks operate. To promote functional consistency between models before and after fine-tuning, we employ knowledge distillation objective during the fine-tuning process. Specifically, the conventional flow matching loss [17, 37] is included: Lf = (ϵ z0) ϵθ(zt, t, y)2 2, (4) where z0 is denotes the feature of an image encoded with pre-trained VAE encoder E() while zt is its noisy version at the t-th timestep, is the text condition, and ϵθ() is the DiT backbone for denoising with parameters θ. Beyond that, we encourage consistency between the linearized student model and the original teacher model, in terms of predictions and attention outputs: Lpred = ϵθ(zt, t, y) ϵθorg (zt, t, y)2 2, Lattn = 1 (cid:88) l= ϵ(l) θ (zt, t, y) ϵ(l) θorg (zt, t, y)2 2, (5) Figure 7. To enhance multi-GPU parallel inference, each text query aggregates only the key-value tokens from the patch managed by its assigned GPU, then averages the attention results across all GPUs, which also generates high-quality images. and the superscript (l) indicates the layer index. The training objectives can be written as: EzE(x),y,ϵN (0,1),t[Lf + αLpred + βLattn], (6) min θ where α and β are hyper-parameters controlling the weights of the corresponding loss terms. Only parameters in the attention layers are trainable. For the training data, we find that training on samples generated by the original DiT model yields significantly better results than training on real image dataset, even when the real dataset contains much more higher-quality data. Please refer to Sec. 4.3 for more discussions. 3.4. Multi-GPU Parallel Inference Since attention is confined to local window around each query, CLEAR offers greater efficiency for multi-GPU patch-wise parallel inference compared to the full attention in the original DiTs, which is particularly valuable for generating ultra-high-resolution images. Specifically, each GPU is responsible for processing an image patch, and the GPU communication is only required in the boundary areas. In other words, if we divide feature map into patches along the vertical dimension, with each GPU handling patch, the communication cost for image tokens between each adjacent GPUs is O(r ) in CLEAR comparing with O(H ) in the original DiT. Nevertheless, since each text token requires information from all image tokens, the exact attention computation in CLEAR still necessitates synchronization of all key-value tokens specially for text tokens, which compromises its potential in this regard. Fortunately, as shown in Fig. 7, we empirically find that without any training or adaptation, the original attention computation for text tokens can be effectively approximated by patch-wise average while not hurting the performance too much, i.e., Otext 1 (cid:88) p= softmax( QtextK )Vp, (7) where θorg denotes parameters of the original teacher DiT, is the number of attention layers applying the loss term, where is the patch/GPU index. Consequently, we only need to aggregate attention outputs for text tokens, resulting 5 Method/Setting Original FLUX-1.dev Sigmoid Attention [48] Linear Attention [12, 30, 38, 65] PixArt-Simga [6] Agent Attention [20] Strided Attention [7] Swin Transformer [39] CLEAR (r = 8) w. distill CLEAR (r = 16) w. distill CLEAR (r = 32) w. distill Against Original FID () LPIPS () CLIP-I () DINO () Against Real FID () LPIPS () CLIP-T () IS () GFLOPS () - 447.80 324.54 30.64 69.85 24.88 18.90 15.53 13.07 14.27 13.72 11.07 8.85 - 0.91 0.85 0.56 0.65 0.61 0. 0.64 0.62 0.60 0.58 0.52 0.46 - 41.34 51.37 86.43 78.18 85.50 85.72 86.47 88. 88.51 88.53 89.92 92.18 - 0.25 2.17 71.45 56.09 70.72 73.43 74.36 77.66 78.35 77. 81.20 85.44 34.93 457.69 325.58 33.38 54.31 35.27 32.20 32.06 33.06 32.36 33.63 33.47 34. 0.81 0.84 0.87 0.88 0.87 0.89 0.87 0.83 0.82 0.89 0.88 0.82 0.81 31. 17.53 19.16 31.12 30.38 30.62 30.64 30.69 30.82 30.90 30.65 30.96 31.00 38.25 1.15 2.91 32.14 21.03 32.05 34. 34.47 35.92 37.13 37.84 37.80 39.12 260.9 260.9 174.0 67.7 80.5 67.7 67.7 63.5 63. 80.6 80.6 154.1 154.1 Table 2. Quantitative results of the original FLUX-1.dev, previous efficient attention methods, and CLEAR proposed in this paper with various on 5,000 images from the COCO2014 validation dataset at resolution of 1024 1024. in constant communication cost and eliminating the need to transmit all key-value tokens. Moreover, our pipeline is orthogonal to existing strategies for patch parallelism such as Distrifusion [34], which applies asynchronous computation and communication by using staled feature maps. Building CLEAR on top of these optimizations would lead to even greater acceleration. 4. Experiments 4.1. Settings and Implementation Details In this paper, we primarily conduct experiments with the FLUX model series due to its state-of-the-art performance in text-to-image generation. Studies on more DiTs can be found in the appendix. We replace all the attention layers in FLUX-1.dev with the proposed CLEAR and experiment with three various window sizes with = 8, = 16, and = 32. Leveraging FlexAttention in PyTorch [42], CLEAR, as sparse attention mechanism, can be efficiently implemented on GPUs with low-level optimizations. We fine-tune parameters in attention layers on 10K samples with 1024 1024 resolution generated by FLUX-1.dev itself1 for 10K iterations under total batch size 32 using the loss function defined in Eq. 6. Lattn is applied on single transformer blocks of FLUX, whose layer indices are 20 57. Following previous works on architectural distillation for diffusion models [31, 38], both hyperparameters α and β are set as 0.5. Other hyper-parameters, including schedulers, optimizers, etc, follow the default settings provided by Diffusers2 [57]. The training is conducted on 4 H100 GPUs supported by DeepSpeed ZeRO-2 [47], which takes 1 day to finish. Unless otherwise specified, all inference is conducted on single H100 GPU. 1https://huggingface.co/datasets/jackyhate/text-to-image2M/tree/main/data 1024 10K 2https://github.com/huggingface/diffusers/blob/main/examples/ dreambooth/README flux.md Following previous works [34, 38], we quantitatively study the proposed method on the validation set of COCO2014 [36] and randomly sample 5,000 images along with their prompts for evaluation. Since CLEAR aims to linearize pre-trained DiT, we also benchmark our method against the results by the original DiT using consistent random seeds. Following conventions [32, 52, 60, 66], we consider FID [23], LPIPS [70], CLIP image similarity [46], and DINO image similarity [4] in this setting as metrics. For settings requiring pixel-wise alignment like image upsampling and ControlNet [69], we additionally incorporate PSNR [26] and multi-scale SSIM [59] for reference. While comparing with real images in COCO, we only include FID and LPIPS for distributional distances. Furthermore, CLIP text similarity [46], Inception Score (IS) [53], and the number of floating point operations (FLOPS) are adopted to reflect textual alignment, general image quality, and computational burden, respectively. Text prompts for qualitative examples are generated by GPT-4o3. 4.2. Main Comparisons We aim to linearize pre-trained DiT in this paper and the linearized model is expected to perform comparably with the original one. As illustrated in Sec. 3.1, most efficient attention algorithms result in suboptimal performance for the target problem, as quantitatively supported by the evaluation in Tab. 2. In contrast, the proposed convolution-like linearization strategy achieves comparable or even superior performance to the original FLUX-1.dev while requiring less computation, underscoring its potential for effectively linearizing pre-trained DiTs. Please refer to the appendix for an analysis of training dynamics and convergence. With the knowledge distillation loss terms defined in Eq. 5, the differences between outputs from the linearized models and the original model are further minimized. For 3https://openai.com/index/hello-gpt-4o/ 6 Figure 8. Qualitative examples by the linearized FLUX-1.dev models with CLEAR and the original model. Setting PSNR () SSIM () FID () LPIPS () CLIP-I () DINO () CLIP-T () IS () GFLOPS () FLUX-1.dev - CLEAR (r = 8) CLEAR (r = 16) CLEAR (r = 32) 27.57 27.60 28. FLUX-1.dev - CLEAR (r = 8) CLEAR (r = 16) CLEAR (r = 32) 26.19 26.98 27.70 1024 1024 2048 2048 - 13.55 13.43 10.87 - 0.12 0.12 0.10 - 98.97 98.97 99.23 - 98.37 98.34 98.82 2048 2048 4096 4096 - 20.87 16.20 13.56 - 0.22 0.19 0. - 98.02 98.48 98.72 - 96.56 97.64 98.21 - 0.91 0.92 0. - 0.87 0.88 0.90 31.11 31.09 31.08 31.09 31.29 31.16 31.25 31. 24.53 25.05 25.46 25.48 3507.9 246.2 352.6 724.3 24.36 53604. 25.87 25.13 24.81 979.3 1433.2 3141.7 Table 3. Quantitative results of the original FLUX-1.dev and our CLEAR with various on 1,000 images from the COCO2014 validation dataset at resolutions of 2048 2048 and 4096 4096. instance, the CLIP Image score exceeds 90 when = 32. Qualitatively, as shown in Fig. 8, the linearized models by the proposed CLEAR preserve the original outputs overall layout, texture, and tone. 4.3. Empirical Studies In this part, we examine several noteworthy properties of CLEAR, including resolution extrapolation, zero-shot generalization across different models and plugins, multi-GPU parallel inference, and the effects of various training data. Resolution Extrapolation. One of the key advantages of linearized diffusion model is its ability to efficiently generate ultra-high-resolution images [38]. However, many previous studies have revealed that it is challenging for diffusion models to generate images beyond their native resolution during training. [2, 15, 16, 22, 61]. They thus apply practical solution for generating high-resolution images in coarse-to-fine manner and devise adaptive strategies for components like position embeddings and attention scales. The proposed CLEAR, on the other hand, makes architectural modifications to pre-trained diffusion backbone, making it seamlessly applicable to them. In this paper, we adopt SDEdit [40], simple yet effective baseline adapting an image to larger scale, for generating high-resolution images. In addition, we also enlarge the NTK factor of rotary position embeddings from 1 to 10 following [44], balance the entropy shift of attention using log-scale attention factor [29], and disable the resolution-aware dynamic shifting [17] in the denoising scheduler. By adjusting the editing strength in SDEdit, as Setting Against Original Against Real FID () LPIPS () CLIP-I () DINO () FID () LPIPS () CLIP-T () IS () FLUX-1.dev - CLEAR (r = 8) CLEAR (r = 16) CLEAR (r = 32) 13.62 12.51 12.43 - 0.62 0.58 0.57 - 88.91 90.43 90.70 - 78.36 81.32 82.61 29. 33.51 34.43 33.57 0.83 0.81 0.82 0.83 31.53 31.35 31.38 31.48 36. 38.42 39.66 39.68 Table 4. Quantitative zero-shot generalization results to FLUX1.schnell using CLEAR layers trained on FLUX-1.dev. shown in Fig. 9(left), we can effectively control the tradeoff between fine details and content preservation. In the appendix, we also try building CLEAR on top of various methods for resolution extrapolation. Quantitatively, we measure the dependency between results by CLEAR with those by the original FLUX-1.dev. As shown in Tab. 3, we achieve MS-SSIM scores as high as 0.9, showcasing the effectiveness of the linearized model with CLEAR as an efficient alternative to the original FLUX. Cross-DiT Generalization. We empirically find that the trained CLEAR layers for one DiT are also applicable for others within the same series without any further adaptation efforts. For example, as shown in Fig. 9(middle), the CLEAR layers trained on FLUX-1.dev can be directly applied to inference on FLUX-1.schnell, timestep-distilled DiT supporting 4-step inference, yielding results similar to those of the original FLUX-1.schnell. Such zero-shot generalization is quantitatively evaluated in Tab. 4. Compatibility with DiT Plugins. It is favorable that substituting original attention layers with the linearized ones would not impact the functionality of plugins trained for the original DiT. As shown in Fig. 9(right), CLEAR demonstrates this property by supporting the pre-trained ControlNet [69], using grayscale images as conditional inputs to FLUX-1.dev4. Quantitative results in Tab. 5 further demonstrate performance comparable to the original model. More evaluations can be found in the appendix. Multi-GPU Parallel Inference. linear complexity DiT can inherently support patch-wise multi-GPU parallel inference. In practice, for text-image joint attention used in modern architectures of DiTs [17, 33], we have to figure out communication-efficient solution to handle text tokens, 4https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNetUnion-Pro 7 Figure 9. Qualitative examples of using CLEAR with SDEdit [40] for high-resolution generation (left), FLUX-1.schnell in zero-shot manner (middle), and ControlNet [69] (right). G.T. and Cond. denote ground-truth and condition images, separately. Setting PSNR () SSIM () FID () LPIPS () CLIP-I () DINO () Against Original Against Real FID () LPIPS () CLIP-T () IS () RMSE () FLUX-1.dev CLEAR (r = 8) CLEAR (r = 16) CLEAR (r = 32) - 25.95 28.24 30.59 - 0.93 0.95 0.97 - 26.14 16.86 11.57 - 0.19 0.13 0.09 - 93.39 96.00 97.33 - 94.24 96.73 98.12 40.25 43.82 40.45 40.21 0. 0.31 0.31 0.31 30.16 29.90 30.19 30.21 22.22 21.29 22.34 21.94 0. 0.0357 0.0395 0.0419 Table 5. Quantitative zero-shot generalization results of the proposed CLEAR to pre-trained ControlNet with grayscale image conditions on 1,000 images from the COCO2014 validation dataset. RMSE here denotes Root Mean Squared Error computed against condition images. Setting Against Original Against Real FID () LPIPS () CLIP-I () DINO () FID () LPIPS () CLIP-T () IS () CLEAR (r = 16) - = 2 = 4 = 8 11.55 12.78 14.21 - 0.51 0.54 0.57 - 90.46 89.74 88.92 - 80.89 79.99 78.65 33. 33.74 33.07 32.26 0.88 0.81 0.81 0.80 30.65 31.21 31.27 31.22 37. 39.26 40.01 39.34 Table 6. Results of patch-wise multi-GPU parallel inference with various numbers of patches using the approximation in Eq. 7. which require gathering information from all key-value tokens. In Eq. 7, we propose an approximation via patchwise average and validate the effectiveness quantitatively in Tab. 6. Results indicate high correlation in semantics before and after this approximation, demonstrating its practical effectiveness. Details on acceleration performance and compatibility with asynchronous communication methods like Distrifusion [34] are discussed in the appendix. Training Data. In our initial exploration, we fine-tune the FLUX model using 200K subset of LAION5B [54], consisting of high-quality real text-image pairs with aesthetic scores greater than 7. However, it would led to inferior results compared to the 10K synthetic samples generated by FLUX-1.dev itself. We then investigate the impact of different training data by fine-tuning the model without altering its structure. We find that the training dynamics differ significantly: training on synthetic data results in much lower loss. We speculate that the discrepancy arises from the mismatch between the LAION dataset distribution and the data distribution for training FLUX. Fine-tuning on LAION data may cause the model to struggle with this distribution shift, leading to increased training difficulty. 5. Conclusions In this paper, we present CLEAR, convolution-like local attention strategy that effectively linearizes the attention mechanism in pre-trained Diffusion Transformers (DiTs), making them significantly more efficient for high-resolution image generation. By comprehensively analyzing existing efficient attention methods, we identified four key elFigure 10. Fine-tuning on real data results in inferior performance compared to fine-tuning on self-generated synthetic data. ementslocality, formulation consistency, high-rank attention maps, and feature integritythat are essential for successful linearization in the context of pre-trained DiTs. CLEAR leverages these principles by restricting attention to circular local window around each query, achieving linear complexity while retaining high-quality results comparable to the original model. Our experiments demonstrate that fine-tuning on merely 10K self-generated samples allows for efficient knowledge transfer to student model, leading to 99.5% reduction in attention computations and 6.3 acceleration in 8K-resolution image generation. Moreover, CLEARs distilled attention layers support zero-shot generalization across different models and plugins and improve multi-GPU parallel inference capabilities, offering broader applicability and scalability. One limitation of our approach is that, the practical acceleration achieved by CLEAR does not fully meet the theoretical expectations indicated by FLOPS. It becomes less significant at relatively low resolutions and can even be slower than the original DiT when the resolution is below 1024 1024. This drawback arises partially because hardware optimization for sparse attention is inherently more challenging than the optimizations achieved by FlashAttention for full attention computation. Addressing this limitation may require developing fused CUDA operators specifically optimized for the specific sparse pattern of CLEAR, which is valuable direction for future works."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to acknowledge that the computational work involved in this research work is partially supported by NUS ITs Research Computing group using grant numbers NUSREC-HPC-00001."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 1 [2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In International Conference on Machine Learning, pages 17371752. PMLR, 2023. 2, 7 [3] Iz Beltagy, Matthew Peters, and Arman Cohan. LongarXiv preprint former: The long-document transformer. arXiv:2004.05150, 2020. 2, 3 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. 6 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [6] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024. 1, 3, 6, 12, 13 [7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. 3, 6, 12 [8] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. 2 [9] Djork-Arne Clevert. Fast and accurate deep network learnarXiv preprint linear units (elus). ing by exponential arXiv:1511.07289, 2015. [10] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 1 [11] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 2 [12] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. 2, 3, 6 [13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 2 [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1 [15] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising highresolution image generation with no $$$. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61596168, 2024. 2, 7 [16] Ruoyi Du, Dongyang Liu, Le Zhuo, Qin Qi, Hongsheng Li, Zhanyu Ma, and Peng Gao. I-max: Maximize the resolution potential of pre-trained rectified flow transformers with projected flow. arXiv preprint arXiv:2410.07536, 2024. 7, 13 [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 4, 5, 7, 13 [18] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [19] Dongchen Han, Ziyi Wang, Zhuofan Xia, Yizeng Han, Yifan Pu, Chunjiang Ge, Jun Song, Shiji Song, Bo Zheng, and Gao Huang. Demystify mamba in vision: linear attention perspective. arXiv preprint arXiv:2405.16605, 2024. 12 [20] Dongchen Han, Tianzhu Ye, Yizeng Han, Zhuofan Xia, Siyuan Pan, Pengfei Wan, Shiji Song, and Gao Huang. Agent attention: On the integration of softmax and linear attention. In European Conference on Computer Vision, pages 124 140. Springer, 2025. 2, 3, 6, 12 [21] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [22] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higherresolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 2, 7 [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [24] Geoffrey Hinton. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [26] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. 6 [27] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International conference on machine learning, pages 90999117. PMLR, 2022. 2, 3 [28] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: frequency perspective on training-free high-resolution image synthesis. arXiv preprint arXiv:2403.12963, 2024. 2 [29] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Trainingfree diffusion model adaptation for variable-sized text-toimage synthesis. Advances in Neural Information Processing Systems, 36:7084770860, 2023. 7 [30] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. 2, 3, 6, 12 [31] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: Architecturally compressed stable diffusion for efficient text-to-image generation. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. 6 [32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. [33] Black Forest Labs. Flux: Official inference repository for flux.1 models, 2024. Accessed: 2024-11-12. 2, 7 [34] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7183 7193, 2024. 6, 8, 13, 14 [35] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 1 [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6 [37] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [38] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. 2, 3, 4, 6, 7, 12 In Hierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 3, 6, 12 [40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 7, 8, 13 [41] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. Conference on Machine Learning, pages 81628171. PMLR, 2021. 1 [42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 6 [43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1 [44] Bowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation, 2023. 7 [45] Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, and Noah Smith. Abc: Attention with bounded-memory control. arXiv preprint arXiv:2110.02488, 2021. [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [47] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. 6 [48] Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge, Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, et al. Theory, analysis, and best practices for sigmoid selfattention. arXiv preprint arXiv:2409.04431, 2024. 2, 3, 6 [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1 [50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 1 [39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: [51] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with rout10 [64] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and MingHsuan Yang. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56(4): 139, 2023. 1 [65] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Gated linear attention transarXiv preprint Panda, and Yoon Kim. formers with hardware-efficient training. arXiv:2312.06635, 2023. 2, 3, 6 [66] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. 2023. [67] Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, et al. When linear attention meets autoregressive decoding: Towards more effective and efficient linearized large language models. arXiv preprint arXiv:2406.07368, 2024. 2, 3 [68] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. 2, 3 [69] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 6, 7, 8, 13 [70] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [71] Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, et al. Gated slot attention for efficient linear-time sequence modeling. arXiv preprint arXiv:2409.07146, 2024. 2, 3, 12 ing transformers. Transactions of the Association for Computational Linguistics, 9:5368, 2021. 2, 3 [52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6 [54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 8 [55] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 3 [56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [57] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. 6 [58] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 2, 3 [59] Zhou Wang, Eero Simoncelli, and Alan Bovik. Multiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, pages 13981402. Ieee, 2003. 6 [60] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023. 6 [61] Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, and Yanfeng Wang. Megafusion: Extend diffusion models towards higher-resolution image generation without further tuning. arXiv preprint arXiv:2408.11001, 2024. [62] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 2 [63] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: nystrom-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1413814148, 2021. 3 11 A. Details of Efficient Attention Alternatives map [20]: The vanilla scaled dot-product attention, although effective and flexible, introduces quadratic computational complexity. Many works have focused on its efficient alternatives. In Sec. 2, we provide taxonomic overview of recent works and will supplement more details regarding the specific formulations and implementations here. Linear Attention avoids the softmax operation in the vanilla attention, supporting computing first with the associative property of matrix-wise multiplication, and thus achieves linear complexity. Before that, non-negative kernel functions () and g() are applied on and respectively such that the similarity between each query-key pair is non-negative. Furthermore, the similarity score between each query-key pair is normalized by the sum of similarity scores of between this query and all key tokens separately, to mimic the functionalities of softmax. Following [19, 30, 38], we implement () and g() by the elu function [9]. Formally, the operation for the i-th query can be written as5: = softmax( Down(Q)K )V. (11) The derived agent tokens are then used as value tokens: = softmax( QDown(Q) )A. (12) Such operations can be viewed as an adaptive token downsampling strategy. Slot Attention implemented in this paper is adapted from [45, 71], which contain key-value memory slots derived by adaptively aggregating key-value tokens: = softmax( )K, = softmax( )V, (13) where Rsc is learnable and introduced for modeling the writing intensity of each input token to each memory slot. These slots are then used as alternatives to original key-value tokens for attention computation: Oi = (elu(Qi) + 1)(elu(K) + 1) (elu(Qi) + 1) (cid:80)m j=1(elu(Kj) + 1) V. (8) = softmax( K ) . (14) Sigmoid Attention replaces the softmax with the formulation of sigmoid, which removes the need to compute the softmax normalization, and thus achieves acceleration: = sigmoid( QK + b)V, (9) where is hyper-parameter. In this paper, we follow the official implementation of FlashSigmoid with hardware-aware optimization6 when applying Sigmoid Attention to DiTs. PixArt-Sigma achieves acceleration by spatially downsampling the key-value token maps [6]. Following the implementation7, we use learnable group-wise official Conv44 kernels with stride = 4 and initialize the weights to 1 16 so that it is equivalent to an average pooling operation at the beginning. Formally, it can be written as: = softmax( QConvk(K) )Convv(V ). (10) Although it has been demonstrated that such strategy can work well at relatively deep layers of DiTs, the results are still unsatisfactory for completely linearized DiT. Agent Attention performs efficient attention operations via agent tokens from down-sampled query token 5https://github.com/LeapLabTHU/MLLA 6https://github.com/apple/ml-sigmoid-attention 7https://github.com/PixArt-alpha/PixArt-sigma This strategy presents different fashion for adaptive keyvalue compression. Strided Attention samples tokens at regular interval [7]. As sparse attention strategy, the attention mask of the l-th layer with down-sampling ratio of can be constructed in the following way: (l) ij = 1, if ntext or ntext or (d(x) ij %r = rx and d(y) 0, otherwise, ij %r = ry); (15) where rx = l%r and ry = l//r ensure that each token has chances to be sampled as key-value tokens. Swin Transformer adopts sliding window partition strategy [39], where attention interactions are independently conducted for each window. Formally, the attention map can be constructed via: Mij = 1, if ntext or ntext or tokens and in the same window; (16) 0, otherwise. We set the window size to 16 and apply shift of 8 for windows in layers with odd indices, following the approach described in the original manuscript. The rank of the imageto-image attention mask corresponds to the number of windows, which poses challenges in achieving the high-rank requirement introduced in the main manuscript needed for linearizing DiTs. 12 Figure 12. The linearized DiTs by CLEAR are compatible with various pipelines dedicated for high-resolution inference. The prompt is shown in Fig. 15. E. More High-Resolution Results In the main manuscript, we build our CLEAR on top of SDEdit [40], simple yet effective strategy for image generation given conditional image, for coarse-to-fine high-resolution generation. We demonstrate here that our method is also compatible with variety of pipelines dedicated for resolution extrapolation. As shown in Fig. 12, we deploy CLEAR on I-Max [16], concurrent work for training-free high-resolution generation with pre-trained DiTs, and observe that it may yield more optimal balance between preserving low-resolution content and capturing high-resolution details. For instance, as shown in Fig. 12, I-Max preserves the textures of the dresses from the low-resolution result with minimal variation while effectively enhancing clear high-resolution details. F. More ControlNet Results We conduct additional experiments with pre-trained ControlNets [69] to demonstrate the zero-shot generalizability of the trained CLEAR layers. Results for tiled image conditions and blur image conditions are shown in Tab. 9. G. Efficient Multi-GPU Parallel Inference For CLEAR, since there are only token interactions in the boundary areas of the patch handled by each GPU, and the approximation of feature aggregation for text tokens defined in Eq. 7, we achieve satisfactory efficiency on multi-GPU parallel inference. Furthermore, we can adopt the asynchronous communication strategy in Distrifusion [34] to achieve even more significant acceleration. As shown in Tab. 10, the acceleration become more significant with the increasing of image resolution, while the original DiT encounters out-of-memory (OOM) error due to the necessity of caching all key-value tokens. Figure 11. Training dynamics of various efficient attention alternatives on FLUX-1.dev. B. Training Dynamics We supplement the curves of training losses of various efficient attention alternatives in Fig. 11. The conclusion is consistent with the main manuscript, that strategies fulfilling the requirements of locality, formulation consistency, high-rank attention map, and feature integrity yield the most satisfactory training convergence. C. Raw Data for Efficiency Comparisons We supplement raw data for Fig. 2 on efficiency comparisons in Tab. 7 in sake of better clarity. D. Results on More DiTs We additionally deploy our method on DiT models other than FLUX used in the main manuscript to demonstrate the universality of the proposed CLEAR. Here, we consider StableDiffusion3.5-Large8 [17] (SD3.5-L), another stateof-the-art text-to-image generation DiT. We use the default setting of = 16, which yields the best trade-off between quality and efficiency according to our experiments. Results on the COCO2014 validation dataset are shown in Tab. 8. We also supplement more qualitative comparisons with results by the original FLUX-1.dev and SD3.5-L in Fig. 13. Results indicate an overall comparable performance. Due to the absence of explicit long-distance token interactions, our method may underperform in capturing overall structural properties, such as potential symmetry. Additionally involving more or less global tokens, such as down-sampled tokens as employed in PixArt-Sigma [6], could potentially mitigate this issue. However, as the primary objective of this paper is to highlight the significance of locality as simple yet effective baseline, we leave detailed design explorations to future work. 8https://huggingface.co/stabilityai/stable-diffusion-3.5-large 13 Setting FLUX-1.dev CLEAR (r = 8) CLEAR (r = 16) CLEAR (r = 32) Running Time (Sec. / 50 Steps) TFLOPS / Layer 1024 1024 2048 2048 4096 4096 8192 8192 1024 1024 2048 4096 4096 8192 8192 4.45 4.40 4.56 5.45 20.90 15.67 17.19 19. 148.97 69.41 83.13 109.57 1842.48 293.50 360.83 496.22 0.26 0.06 0.09 0. 3.51 0.25 0.35 0.72 53.60 0.98 1.43 3.14 847.73 3.92 5.79 13. Table 7. Raw data for Fig. 2 on efficiency comparisons. Method/Setting Against Original FID () LPIPS () CLIP-I () DINO () Against Real FID () LPIPS () CLIP-T () IS () GFLOPS () SD3.5-L w. CLEAR (r = 16) - 11.21 - 0.57 - 90.9 - 81.47 34.10 36. 0.83 0.83 31.40 31.23 36.06 36.28 206.5 63.8 Table 8. Quantitative results of the original SD3-Large and its linearized version by CLEAR proposed in this paper on 5,000 images from the COCO2014 validation dataset at resolution of 1024 1024. Condition Setting PSNR () SSIM () FID () LPIPS () CLIP-I () DINO () FID () LPIPS () Against Original Against GT CLIP-T () IS () RMSE () Tile Blur FLUX-1.dev CLEAR (r = 16) FLUX-1.dev CLEAR (r = 16) - 30.12 - 28.92 - 0. - 0.96 - 9.1 - 10.56 - 0.13 - 0.13 - 99. - 99.02 - 99.04 - 98.67 38.20 39.73 38.72 39.66 0.31 0. 0.31 0.33 30.16 30.11 30.20 30.14 21.54 21.77 21.42 21.67 0.019 0. 0.028 0.033 Table 9. Quantitative zero-shot generalization results of the proposed CLEAR to pre-trained ControlNet with tiled image conditions and blur image conditions on 1,000 images from the COCO2014 validation dataset. RMSE here denotes Root Mean Squared Error computed against condition images. # of GPUs Synchronous FLUX-1.dev CLEAR (r = 16) CLEAR (r = 8) Asynchronous FLUX-1.dev CLEAR (r = 16) CLEAR (r = 8) 11. 7.981.39 5.931.88 4.842.30 52.25 30.961.69 18.942.76 12.974.03 11.40 8.521.34 6.011.90 NA 42. 26.261.64 15.642.75 9.724.42 1024 1024 11.00 7.851.40 5.382.04 4.372.52 2048 2048 39. 23.961.64 13.862.83 8.404.66 4096 4096 372.43 200.161.86 105.593.53 59.186.29 207.83 115.021.81 59.653.48 32.336. 173.53 96.651.80 49.703.49 26.886.46 - 7.641.46 5.641.97 4.492.48 - 30.171.73 18.582.81 12.574. - OOM OOM OOM - 8.101.41 5.672.01 NA - 25.411.69 15.122.84 9.304. - 112.341.85 57.423.62 31.236.65 - 7.501.47 5.112.15 3.902.82 - 23.011.70 13.42.92 8.044. - 91.841.89 48.573.57 26.266.61 1 2 4 8 1 2 4 1 2 4 8 Table 10. Efficiency of multi-GPU parallel inference measured by sec./50 denoising steps on HGX H100 8-GPU server. We adapt Distrifusion [34] to FLUX-1.dev here for asynchronous communication. The ratios of acceleration are highlighted with red. Results of CLEAR with = 16 at the 1024 1024 resolution are not available (NA) because the patch size processed by each GPU is smaller than the boundary size. OOM denotes encountering out-of-memory error. 14 Figure 13. Qualitative comparisons on FLUX-1.dev (top) and SD3.5-Large (bottom). The left subplots are results by the original models while the right ones are by the CLEAR linearized models. Prompts are listed in Fig. 16. Figure 14. More 4K examples by the CLEAR linearized FLUX-1.dev. Prompts are listed in Fig. 16. 16 // Fig. 1, according to the top-left corner, from top to bottom, from left to right // 1, also used in Fig. 4 and Fig. 6 \"A high fantasy scene where fierce battle is taking place in the sky between dragons and powerful wizards. One side of the scene shows wizards casting spells, their staffs glowing with magical energy, while on the other, dragons with scales of fire and lightning breathe torrents of flame. The sky is torn with storms of magic, and below, medieval kingdom watches in awe as the skies blaze with the fury of battle.\", // 2 \"futuristic cityscape, towering skyscrapers, neon lights, speeding cars, holographic advertisements, cyberpunk, ultra-realistic, high resolution, cinematic lighting, highly detailed, ultra HD, 8K, nighttime, rain-soaked streets, reflections on glass, vibrant colors, misty atmosphere\", // 3 \"A tiger is kissing rabbit\", // 4 \"classic fountain pen with detailed engravings, glass ink bottle with reflections, subtle ink stains, warm lighting, rich wood desk, soft shadows, high detail on pen and bottle, ultra-realistic textures, vintage and refined, calm and artistic feel, close-up, high resolution, deep blue and golden accents\", // 5 \"beautiful Chinese woman in hanfu, surrounded by blooming peonies, flowing silk robes, elegant and ethereal, soft lighting, pastel colors, highly detailed fabric textures, delicate hair ornaments, peony petals in the air, graceful pose, traditional hairpin\", // 6 \"charming countryside cottage, early morning sunlight, mist in the air, lush garden, rustic and cozy, ivy-covered walls, wooden fence, high detail, ultrarealistic, peaceful atmosphere, blooming flowers, warm light, quiet and serene\", // 7 \"futuristic racing car, sleek design, neon underglow, high-speed action, dust trail, dynamic motion blur, cinematic lighting, high resolution, ultra-realistic, ultra HD, 8K, dark background, neon lights, sparks flying, intense colors, reflections on car surface\", // 8 \"majestic Chinese dragon, swirling clouds, water and ink effect, powerful presence, dynamic and dramatic, monochromatic ink wash, swirling motion, high detail on dragon scales, whirlwind of clouds, dragons fierce eyes, ink splashes, ancient mystical aura\", // 9 \"traditional Chinese night market, red lanterns, crowded stalls, vibrant atmosphere, warm and lively, golden lighting, realistic and bustling, intricate market details, traditional snacks, merchants in robes, lanterns casting glow, animated crowd in background\", // 10, also used in Fig. 2 (Appendix) \"enchanted forest, glowing plants, towering ancient trees, mystical girl, magical aura, fantasy style, vibrant colors, ethereal lighting, bokeh effect, ultradetailed, painterly, ultra HD, 8K, soft glowing lights, mist and fog, otherworldly ambiance, glowing mushrooms, sparkling particles\", // 11 \"portrait of an elderly female artist with silver hair, gentle smile, wearing glasses and colorful scarf, soft studio lighting, high detail wrinkles, ultrarealistic, warm lighting, creative and thoughtful, calm and wise, subtle background, rich textures, peaceful and inviting, close-up\", // 12 \"futuristic soldier, robotic armor, high-tech weapon, visor with digital HUD, dark sci-fi, highly detailed, cinematic lighting, dynamic pose, ultra-realistic, ultra HD, 8K, neon accents, dark background, glowing HUD, intense expression, battle scars on armor\", // 13 \"A watercolor-style sign reading Hello CLEAR with soft gradients of blue, green, and purple, textured lettering, and subtle paint splashes\", // 14 \"hidden paradise with peach blossoms, flowing river, distant mountains, quaint cottages, dreamlike and serene, vibrant colors, soft and warm lighting, idyllic landscape, blossoming peach trees, mist over river, villagers in traditional attire, sunlight filtering through petals\", // 15 \"Parisian street at night, iconic street lights, cobblestone path, view of Eiffel Tower, vibrant city atmosphere, warm tones, rain reflections on street, historic architecture, romantic ambiance, ultra-realistic details, cinematic lighting, urban scene, high resolution\", // 16 \"astronaut meeting alien creatures, cosmic background, colorful nebula, stars in background, high detail spacesuit, atmospheric lighting, sci-fi setting, calm and peaceful, otherworldly creatures, ultra-realistic, adventure in space, detailed environment\", // 17 \"rustic wooden cabin interior, cozy and warm, fireplace glowing, wooden beams, vintage furniture, soft light from window, warm and earthy tones, ultrarealistic details, rich textures, cozy blankets and cushions, peaceful ambiance, high resolution, natural wood grain visible\", // 18 \"Chinese ink landscape painting, misty mountains, winding rivers, ancient pine trees, traditional ink wash painting, soft brushstrokes, monochromatic, ethereal and timeless, light mist among mountains, small thatched pavilion, subtle gradation of ink, natural flow\", // 19 \"phoenix rising from flames, vibrant feathers, traditional Chinese mythological style, vivid and majestic, dynamic colors, dramatic lighting, intricate feather details, golden flames, radiant plumage, traditional patterns on wings, sense of rebirth\", // 20 \"city street on rainy day, wet pavement with reflections, people under umbrellas, soft city lights reflecting in water puddles, detailed raindrops, warm and cozy tones, misty atmosphere, ultra-realistic details, vibrant and deep colors, high contrast, peaceful rain ambiance, soft shadows, street lights glowing\", // 21 \"ancient Chinese academy, surrounded by bamboo forest, stone paths, wooden study desks, calm and serene, warm lighting, natural greens, intricate woodwork, rustic textures, bamboo shadows on ground, calligraphy brushes, traditional scrolls, scholars in robes\", // 22 \"1950s American diner, red leather booths, checkerboard floor, neon signs, nostalgic atmosphere, warm lighting, retro decor, vintage menu, chrome accents, classic style, cozy and inviting, high detail, ultra-realistic\", // 23 \"ancient library, high shelves filled with old books, detailed wood carvings, dusty and dim lighting, massive wooden tables, vintage globes, warm light filtering through tall windows, ultra-realistic, intricate details on book spines, nostalgic atmosphere, high resolution, serene and historical feel\", // Fig. 7 \"A cat holding sign that says hello world\" Figure 15. GPT-generated prompts used in the main manuscript. 17 // Fig. 13, from top to bottom, from left to right // 1 \"a polar bear sitting on floating iceberg, holding an umbrella while it rains colorful paint, the surrounding ocean reflecting the vibrant colors, ultradetailed, photorealistic, ultra HD, 8K, surreal and artistic composition, bold contrasts, intricate reflections\", // 2 \"lush green valley surrounded by towering cliffs, winding river reflecting the blue sky, fluffy white clouds casting shadows, grazing deer in the distance, ultra-detailed, photorealistic, ultra HD, 8K, natural vibrancy, peaceful wilderness atmosphere, intricate water and vegetation textures\", // 3 \"peaceful Chinese lake scene, traditional pagoda on small island, still water reflecting the structure, distant misty mountains, pink lotus flowers floating, warm morning light, ultra-detailed, photorealistic, ultra HD, 8K, serene atmosphere, traditional aesthetics, vibrant yet soft colors\", // 4 \"owl in dense forest at night, glowing yellow eyes, dark and mysterious atmosphere\", // 5 \"a library where the books are glowing jellyfish floating mid-air, young girl reaching out to touch one, shelves filled with ancient tomes, soft ambient lighting, ultra-detailed, photorealistic, ultra HD, 8K, whimsical and magical atmosphere, intricate textures\", // 6 \"cyberpunk cityscape, glowing neon lights, futuristic skyscrapers, bustling streets, flying cars, nighttime setting, holographic advertisements, rain-soaked roads, ultra-detailed, cinematic lighting, ultra HD, 8K, vivid colors, dramatic atmosphere, intricate reflections, dystopian vibe\", // 7 \"a futuristic robot tending garden of glowing bioluminescent flowers, its metallic hands delicately handling the plants, waterfall of stars in the background , ultra-detailed, photorealistic, ultra HD, 8K, ethereal lighting, blending nature and technology\", // 8 \"alien desert landscape, multiple moons in the sky, strange rock formations, glowing plants, mysterious alien figures, science fiction style, ultra-detailed, cinematic lighting, ultra HD, 8K, vibrant colors, surreal ambiance, dramatic shadows, expansive vistas\", // 9 \"autumn forest in golden hour, trees with vibrant red, orange, and yellow leaves, narrow path covered in fallen foliage, sunlight casting warm hues, distant hills, ultra-detailed, photorealistic, ultra HD, 8K, rich colors, peaceful atmosphere, intricate details of leaves and bark\", // 10 \"endless lavender fields at sunset, soft purple hues blending with golden sky, small rustic farmhouse in the distance, rolling hills on the horizon, ultradetailed, photorealistic, ultra HD, 8K, delicate lavender flowers, serene ambiance, atmospheric depth\", // 11 \"cozy rural kitchen, wooden cabinets, fresh bread on the counter, sunlight streaming through lace curtains, ceramic jars and fresh herbs, rustic charm, warm tones, ultra-detailed, photorealistic, ultra HD, 8K, soft ambient light, intricate wood grain textures, peaceful atmosphere\", // 12 \"portrait of young girl with freckles, natural outdoor setting, sunlight filtering through leaves, soft focus background, vibrant hair and vivid eye color, ultra-detailed, photorealistic, ultra HD, 8K, delicate facial textures, bright and innocent atmosphere, warm golden tones\" // Fig. 14, according to the top-left corner, from top to bottom, from left to right // 1 \"sunlit vineyard in late summer, rows of grapevines heavy with ripe fruit, rustic farmhouse in the distance, soft hills and clear sky, warm golden light, ultradetailed, photorealistic, ultra HD, 8K, intricate grape and leaf textures, serene countryside atmosphere\", // 2 \"ancient Chinese temple on hill, red walls and golden roofs, surrounded by lush green bamboo forest, stone lanterns lining the path, soft golden hour light, ultra-detailed, photorealistic, ultra HD, 8K, traditional Chinese architecture, peaceful ambiance, intricate carvings and ornate designs\", // 3 \"bustling fish market at sunrise, vibrant colors of fresh seafood, fishermen unloading crates, intricate details of fish scales and ice, ambient light, bustling atmosphere, ultra-detailed, photorealistic, ultra HD, 8K, atmospheric realism, sharp textures, lively dynamics\", // 4 \"majestic dragon flying over medieval castle, fiery sunset, rolling hills, dramatic clouds, fantasy style, ultra-detailed, painterly aesthetic, ultra HD, 8K, warm hues, glowing embers, intricate textures, golden hour lighting\", // 5 \"futuristic laboratory interior, glowing screens, robotic arms, holographic displays, sleek design, science fiction style, ultra-detailed, ultra HD, 8K, cold lighting, metallic textures, high-tech ambiance, detailed equipment\", // 6 \"a serene Chinese garden, curved stone bridge over lotus-filled pond, elegant pavilions with ornate designs, weeping willow trees, koi fish swimming, gentle sunlight, ultra-detailed, photorealistic, ultra HD, 8K, traditional landscape design, tranquil atmosphere, vibrant yet harmonious colors\", // 7 \"ancient Greek temple on hilltop, surrounded by lush gardens, golden hour, marble columns, intricate carvings, mythological figures, painterly style, ultradetailed, ultra HD, 8K, warm lighting, serene atmosphere, historical accuracy\", // 8 \"a chessboard floating in cosmic void, pieces made of planets and stars, human hand reaching out to make move, ultra-detailed, photorealistic, ultra HD, 8K , cosmic and abstract design, vivid lighting, surreal and thought-provoking atmosphere\", // 9 \"a vintage gramophone in the middle of lush rainforest, vines wrapping around the horn, music notes visibly floating in the air, animals like parrots and monkeys curiously gathered, ultra-detailed, photorealistic, ultra HD, 8K, vibrant colors, magical and whimsical atmosphere, rich textures\", // 10 \"a giant clock embedded in mountain cliff, waterfalls flowing through the clocks gears, lush greenery surrounding the scene, ultra-detailed, photorealistic, ultra HD, 8K, timeless and surreal atmosphere, intricate mechanical details, dramatic lighting\", // 11 \"sunset over rocky coastline, waves crashing against jagged cliffs, vivid orange and purple hues in the sky, seabirds flying above, tide pools with reflections , ultra-detailed, photorealistic, ultra HD, 8K, dynamic motion, tranquil yet dramatic atmosphere, intricate rock and water textures\", // 12 \"a gigantic hourglass buried in desert, golden sand slowly flowing between the chambers, group of explorers climbing the hourglass, storm brewing in the background, ultra-detailed, photorealistic, ultra HD, 8K, dramatic lighting, surreal and adventurous ambiance\", // 13 \"portrait of wise old man with long white beard, wearing traditional robes, holding wooden staff, mountain landscape in the background, soft diffused light , ultra-detailed, photorealistic, ultra HD, 8K, deep wrinkles, serene expression, mystical and timeless atmosphere\" Figure 16. GPT-generated prompts used in the appendix."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}