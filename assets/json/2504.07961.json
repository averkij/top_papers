{
    "paper_title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
    "authors": [
        "Zeren Jiang",
        "Chuanxia Zheng",
        "Iro Laina",
        "Diane Larlus",
        "Andrea Vedaldi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 1 6 9 7 0 . 4 0 5 2 : r Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction Zeren Jiang1 Chuanxia Zheng1 Iro Laina1 Diane Larlus2 Andrea Vedaldi1 1Visual Geometry Group, University of Oxford 2Naver Labs Europe {zeren, cxzheng, iro, vedaldi}@robots.ox.ac.uk diane.larlus@naverlabs.com geo4d.github.io Figure 1. Geo4D repurposes video diffusion model [102] for monocular 4D reconstruction. It uses only synthetic data for training, but generalizes well to real data. It predicts several geometric modalities, including point maps, depth maps, and ray maps, fusing and aligning them to obtain state-of-the-art dynamic reconstruction even for scene with extreme object and camera motion."
        },
        {
            "title": "Abstract",
            "content": "We introduce Geo4D, method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in zeroshot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. We introduce new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes. 1. Introduction We consider the problem of feed-forward 4D reconstruction, which involves learning neural network to reconstruct the 3D geometry of dynamic scene from monocular video. This task is particularly challenging for videos captured in uncontrolled settings, such as those shot with handheld cameras or downloaded from the Internet. However, robust solution to this problem would have tremendous impact on wide range of applications, from video understanding to computer graphics and robotics. 4D reconstruction from videos is related to multi-view static 3D reconstruction, which is typically addressed using methods from visual geometry like bundle adjustment. New neural networks [89, 92] have recently emerged as powerful tools that can replace, or at least complement, bundle adjustment. They excel especially in difficult reconstruction scenarios, involving, e.g., textureless surfaces and occlusions, thanks to the priors that they learn from data. Because of the additional challenges involved in 4D reconstruction, we expect that such priors would benefit this task even more. In fact, powerful networks like DUSt3R [92], designed for static multi-view 3D reconstructions, have recently been extended to the dynamic case, for example by MonST3R [113]. However, these models are heavily engineered to solve specific 3D reconstruction problems. Most importantly, they require significant amounts of train1 ing data with 3D annotations for supervision. Such data is difficult to collect for dynamic scenes, especially in real life. This suggests using 4D synthetic training data instead. However, this data is difficult to obtain at scale, and the gap with the real world can compromise generalization. One way to mitigate this problem is to pre-train the model on tasks related to 3D reconstruction for which real data is easily available. For example, DUSt3R and derived methods use image matching for pre-training [98]. Here, we suggest starting instead from an off-the-shelf video generator. Video generators are powerful models, often considered proxies of world simulators [37, 54, 59]. More importantly for us, the videos that they generate show an understanding of effects like camera motion and perspective as well as typical object motion in the context of that scene. However, they only generate pixels, leaving any 3D or 4D understanding implicit and thus not directly actionable. In this work, we show that pre-trained off-the-shelf video generator can be turned into an effective monocular feed-forward 4D reconstructor. To this end, we introduce Geo4D, novel approach for adapting Video Generators for Geometric 4D Reconstruction. With Geo4D, we demonstrate that these generic video architectures can successfully solve complex 4D reconstruction tasks, which is step towards future video foundation models that natively integrate 4D geometry. Prior works like Marigold [28] and concurrent work DepthCrafter [22] have looked at adapting, respectively, image and video generators for depth estimation. Here, we go one step further and consider the full recovery of 4D geometry, including camera motion and dynamic 3D structure. With Geo4D, our goal is to make 4D geometry explicit in the video generator. This in turn requires us to choose an explicit representation of 4D information. We follow DUSt3R and adopt its viewpoint-invariant point maps. Namely, we associate each pixel in each frame with the coordinate of the corresponding 3D point, expressed relative to the first frame in the video, used as reference. Hence, the static parts of the point clouds extracted from the different frames line up, and the dynamic parts form 3D trace of the motion of the dynamic objects, as shown in Fig. 1. Viewpoint-invariant point maps are powerful representation because they implicitly encode the camera motion and intrinsics, and because they can be easily predicted by neural network [92]. However, they are not necessarily the best representation for all parts of the scene, particularly for points far away from the observer or even at infinity, such as the sky. We thus consider two more modalities, namely disparity maps and camera ray maps, with better dynamic range. Ray maps, in particular, are defined for all image pixels regardless of the scene geometry. Our model thus predicts three modalities: point, disparity, and ray maps. These modalities are redundant in principle, but complementary in practice. At test time, we reconcile them via fast, global optimization step and show that this leads to significantly more robust 4D reconstructions. Due to depth and ray map prediction, we show in particular very strong empirical results on video depth estimation and in the recovery of the camera orientation. One reason why monocular 4D reconstruction is difficult is that it is ambiguous, much more so than static 3D reconstruction. The stochastic nature of the video generator can help us deal with this ambiguity. We also introduce uncertainty maps in the encoder-decoder architecture that processes the geometric maps and that are integrated into the multi-modal alignment process. Overall, our contributions are as follows. (i) We introduce Geo4D, 4D feed-forward network for dynamic scene reconstruction that builds on top of an off-the-shelf video generator. (ii) We suggest generating multiple partiallyredundant geometric modalities and fusing them at test time via light-weight optimization. (iii) We show the benefits of this multi-modal fusion in terms of improved 4D prediction accuracy. Our experiments show that this model is able to reconstruct even highly dynamic scenes (such as the drifting scene in DAVIS [23] presented in Fig. 1) and outperforms current video depth and camera rotation estimation. 2. Related Work 2.1. Dynamic Scene Reconstruction Static 3D Reconstruction. Feed-forward 3D reconstruction has achieved remarkable success across various representations, including voxels [11, 74, 83], meshes [18, 72, 90], and point clouds [41, 110]. These advancements have been further propelled by implicit neural representations [52, 56, 60, 75] and the emergence of 3D Gaussian Splatting [7, 9, 29, 76, 79, 80]. Recently, DUSt3R [92] introduced point map representation for scene-level 3D reconstruction, followed by [35, 86, 89, 104]. However, these models predominantly focus on static 3D reconstruction. Our approach also uses point maps as representation but extends it to handle dynamic scenes, which presents more challenges, where objects undergo motion over time. Iterative 4D Reconstruction. Iterative or optimizationbased approaches reconstruct 4D models from monocular videos by iteratively fitting the observed data. Classical techniques often rely on RGB-D sensors [24, 53], but such steps are impractical for many real-world scenes. Recently, with advancements in neural representations [52, 56], NeRF-based approaches [27, 38, 39, 57, 58, 62] have shown impressive results. However, volume rendering in NeRF is computationally expensive. Convergence and rendering speed can be improved by using 3D Gaussian Splatting (3D-GS) representations [12, 29, 34, 43, 91, 99, 107, 111], which reduce but do not eliminate the cost of itera2 Figure 2. Overview of Geo4D. During training, video conditions are injected by locally concatenating the latent feature of the video with diffused geometric features zX and are injected globally via cross-attention in the denoising U-Net, after CLIP encoding and query transformer. The U-Net is fine-tuned via Eq. 2. During inference, iteratively denoised latent features ˆzX 0 are decoded by the fine-tuned VAE decoder, followed by multi-modal alignment optimization for coherent 4D reconstruction. 0 , ˆzD , zD 0 , ˆzr , zr tive optimization. Very recently, MegaSaM [40] achieved highly accurate and robust camera pose estimation and reconstruction for dynamic videos, but it requires accurate monocular depth prior. Similarly, Uni4D [108] produces accurate 4D reconstruction by leveraging various visual foundation models and performing multi-stage bundle adjustment. In contrast, our approach is diffusion-driven feed-forward framework, which eliminates the need for pervideo bundle adjustment and other depth estimation models. Feed-forward 4D Reconstruction. Similar to our approach, recent works have started to explore feed-forward 4D reconstruction for dynamic scenes: monocular video with dynamic objects is processed by neural network to recover 4D representation. For objects, L4GM [66] and Animate3D [26] first generate multi-view videos from monocular video input, and subsequently apply 3D-GS [29] For to reconstruct temporally consistent 4D model. scenes, notable example is MonST3R [113], which adapts the static scene reconstruction of DUSt3R [92] to handle dynamic scenes. Very recently, Easi3R [8] applies attention adaptation during inference and performs 4D reconstruction based on DUSt3R [92] in an efficient training-free manner. 2.2. Geometric Diffusion Model Our method builds upon advancements in video diffusion models [3, 4, 16, 19, 21, 31, 73, 88, 94, 102, 112], which generate temporally consistent videos from text or image prompts. Recent studies have explored the rich 3D priors embedded within large-scale pre-trained diffusion models, employing either knowledge distillation [25, 42, 51, 61, 87, 96] or fine-tuning [20, 36, 4547, 71, 85, 118] for 3D reconstruction and generation. While these methods have significantly advanced single-object 3D reconstruction from sparse inputs, they remain largely constrained to static, isolated objects centered within an image. Beyond single3 object reconstruction, several recent efforts have extended pre-trained diffusion models to tackle scene-level 3D tasks, such as optical flow estimation [69], view synthesis [10, 15, 44, 68, 81, 109], depth estimation [13, 28, 117], and normal estimation [14, 33, 63]. More related to our approach, Matrix3D [49] jointly predicts the depth and camera parameters, and WVD [115] introduces hybrid RGB+point maps representation for scene reconstruction. However, these approaches assume static 3D environments, whereas we address dynamic 4D scene reconstruction, which is much harder problem due to object motion across time. More related to our approach, concurrent GeometryCrafter [103] introduced point map VAE with dual encoder-decoder architecture to improve reconstruction accuracy. However, their point maps are defined in individual camera coordinates, necessitating the use of additional segmentation [30] and tracking models [101] to recover the global point map and estimate camera poses. Aether [82], on the other hand, outputs depth maps and ray maps from video diffusion model for 4D reconstruction. In contrast, our experiments demonstrate that performance can be significantly enhanced by jointly predicting multiple geometric modalities that capture diverse dynamic ranges, ensuring better temporal coherence and robustness. Importantly, our approach is self-contained and does not rely on external models, enhancing its generality and reliability. 3. Method Our goal is to learn neural network fθ that can reconstruct dynamic 3D scenes from monocular videos. Given as input monocular video = {I i}N i=1 consisting of frames, where each frame is an RGB image RHW 3, the network fθ returns representation of its 4D geometry: fθ : {I i}N i=1 (cid:55) {(Di, i, ri)}N i=1. (1) The network computes the disparity map Di RHW 1, the viewpoint-invariant point map RHW 3, and the ray map ri RHW 6 of each frame i, 1, . . . , . As we discuss in Sec. 3.2, these quantities collectively represent the 4D geometry of scene, including its dynamic structure and time-varying camera extrinsic and intrinsic parameters. No camera parameters are provided as input, so these are implicitly estimated by the model too. We implement fθ as video diffusion model, where θ are the learnable parameters of the model. We discuss the relevant background on video diffusion models in Sec. 3.1. Then, in Sec. 3.2, we describe how we extend the model to predict the three modalities of the 4D geometry. Finally, in Sec. 3.3, we describe how we fuse and align these modalities to obtain coherent 4D reconstruction at test time. 3.1. Preliminaries: Video Diffusion Model Our key insight is that by building on pre-trained video diffusion models, our approach can exploit the strong motion and scene geometry priors inherently encoded within these models. Specifically, we build Geo4D on top of the DynamiCrafter [102], foundation video diffusion model. DynamiCrafter is latent diffusion model [67]: it uses variational autoencoder (VAE) to obtain smaller video representation and thus reduce the computational complexity. During training, target sequence = x1:N , is first encoded into the latent space using the encoder z1:N = 0 αtz1:N E(x1:N ), and then perturbed by z1:N 0 + 1 αtϵ1:N , where ϵ (0, I) is the Gaussian noise, and αt is the noise level at step of noising steps. The denoising network ϵθ is then trained to reverse this noising process by optimizing the following objective: = min θ (cid:0)z1:N (cid:13) (cid:13)ϵ1:N ϵθ E(x1:N ,y),t,ϵ1:N (0,I) , t, y(cid:1)(cid:13) 2 2 , (cid:13) (2) where is the conditional input. Once trained, the model generates video prompted by via iteratively denoising from pure noise z1:N , and then decoding the denoised latent with decoder ˆX = D( ˆz1:N ). 0 3.2. Multi-modal Geometric 4D Diffusion We first provide more precise description of the 4D multimodal representation output by our model, and then explain how the latter is encoded in the latent space for generation. Multi-modal geometric representations. The dynmic 3D structure of scene is represented by sequence of point maps {X i}N i=1, one for each of its frames. Let (u, v) denote the pixel coordinates in the image plane. Then, the uv R3 is the 3D coordinate of the scene point value that lands at pixel (u, v) in frame i, expressed in the reference frame of camera = 1. Because the reference frame is fixed and independent of the time-varying viewpoint, we call these point maps viewpoint invariant. The 4 advantages of this representation are convincingly demonstrated by DUSt3R [92]. For static scene, or by knowing which image pixels correspond to the static part of scene, knowledge of the point maps allows recovery of the intrinsic and extrinsic camera parameters as well as the scene depth. This is done by solving an optimization problem that aligns the dynamic point maps with pinhole camera model. As noted in Sec. 1, while point maps {X i}N i=1 fully encode the 4D geometry of the scene, they may not be as effective for all parts of the scene. Their dynamic range is limited, and in fact point maps are not even defined for points at infinity (sky). Hence, consider two more modalities, namely disparity maps {Di}N i=1 and camera ray maps {ri}N i=1, also encouraged by prior evidence [14, 33, 49] that diffusion model can benefit from learning to predict multiple quantities. Disparity maps are not viewpoint-invariant, but have better dynamic range than point maps (the disparity is zero for points at infinity). Ray maps only represent the camera parameters and are defined for all image pixels, independent of the scene geometry. For the disparity map, the value Di uv is the disparity (inverse depth) of the scene point that lands at pixel (u, v), as seen in frame i. For the ray map, we adopt the Plucker coordinates [75, 97, 118], i.e., ruv = (duv, muv), where duv = RK1(u, v, 1) is the ray direction and muv = Rtduv, where (R, K, t) are the cameras rotation, calibration, and translation parameters. Multi-modal latent encoding. The three modalities come in the form of images and can thus be naturally predicted by the video diffusion architecture. However, this requires first mapping them to the latent space, for which we need suitable versions of the encoder and decoder from Sec. 3.1. Related prior works [14, 28] for depth prediction simply repurposed pre-trained image encoder-decoder without change. We found this to work well for depth and ray maps, but not for point maps. Hence, for the point maps only, we fine-tune the pre-trained decoder using the following objective function [100]: = (cid:88) ln uv 1 2σuv exp 2ℓ1(D(E(X))uv, Xuv) σuv , (3) where σ RHW is the uncertainty of the reconstructed point map, which is also predicted by an additional branch of our VAE decoder. We leave the encoder unchanged to modify the latent space as little as possible; instead, we normalize the point maps in the range of [1, 1] to make them more compatible with the pre-trained image encoder. Video Conditioning. The original video diffusion model is conditioned on single image, but here we need to condition it on the entire input video = {I i}N i=1. To this end, we use hybrid conditioning mechanism using two streams. As shown in the Fig. 2, in one stream, we extract global representation of each frame by passing it to CLIP [64] followed by lightweight learnable query transformer [1]. These vectors are incorporated in the transformer via cross-attention layers injected in each U-Net block. In the other stream, we extract local spatial features from the VAE encoder and concatenate them channel-wise to the noised latents, encoding the generated 4D modalities {(Di, i, ri)}N i=1. 3.3. Multi-Modal Alignment As noted, Geo4D predicts several non-independent geometric modalities. Furthermore, processing all frames of long monocular video simultaneously with video diffusion model is computationally prohibitive. Therefore, during inference, we use temporal sliding window that segments the video into multiple overlapping video clips, with partial overlap to facilitate joining them. The goal of this section is to fuse the resulting multi-modal and multi-window data into single, coherent reconstruction of the entire video. Temporal sliding window. Given video = {I i}N i=1 with frames, we construct it into several video clips = {gk}, S, where each clip gk contains frames {I i}k+V 1 , and the set of starting indices is = (cid:5) s} {N }. Here, is the sliding {0, s, 2s, . . . , (cid:4) window stride. The final term {N } ensures that the last clip always includes the final frames of the video. i=k Alignment objectives. First, given the predicted point maps i,g for each frame in each video clip G, we derive corresponding globally aligned point maps in world coordinates, as well as the relative camera motion and scale parameters. We denote these quantities with the subscript to emphasize that they are inferred from the point map predictions. To do so, we extend the pair-wise global alignment loss from DUSt3R to group-wise one: Lp (cid:0)X, λg , (cid:1) = (cid:88) (cid:88) (cid:88) gG ig uv (cid:13) (cid:13) (cid:13) (cid:13) g i,g uv"
        },
        {
            "title": "X i",
            "content": "uv λg σi,g uv (cid:13) (cid:13) (cid:13) (cid:13)1 , βg Ki1 = [Rg and p,uv(u, v, 1) + oi (4) where λg ] denote the group-wise scale and transformation matrix that align the group-relative point maps i,g to the point maps expressed in the global reference frame. We further parameterize each of these point Di1 uv = Ri maps as in terms of each cameras calibration Ki p, world-to-camera rotation Ri and center oi expressed in the global reference frame, and the disparity map Di p. Substituting this expression into the loss function (4) and minimizing it, we can thus recover Ki p, Ri p, oi The steps above thus infer the disparity maps Di from the point maps, but the model also predicts disparity maps Di directly, where the subscript denotes disparity predicp from the predicted point maps. , p, Di p, λg tion. We introduce the following loss to align them: (cid:13) (cid:13) (cid:13)1 Ld (Dp, λg βg (cid:13) (cid:13)Di (cid:13) λg Di,g , βg ) = (cid:88) (cid:88) gG ig , (5) where λg and βg are optimized scale and shift parameters. Finally, the ray maps also encode camera pose. To align them with the global camera parameters (Rp, Kp, op) obtained from the point map, we first solve an optimization problem to extract the camera parameters from the ray map ri,g = di,g, mi,g for each group at frame i. Following Raydiffusion [114], the camera center oi,g is solved by finding the 3D world coordinate closest to the intersection of all rays: oi,g = arg min pR3 (cid:88) uH,vW di,g uv mi,g uv 2. (6) The camera extrinsics are solved by optimizing for the matrix that transforms the predicted per-pixel ray directions di,g uv to the ray directions uuv of canonical camera: Hi,g = arg min H=1 (cid:88) (cid:13) (cid:13)Hdi,g uv uuv (cid:13) (cid:13) . (7) uH,vW Then the world-to-camera rotation matrix Ri,g matrix Ki,g Hi,g. Finally, the camera trajectory alignment loss is: and intrinsic can be solved using the RQ-decomposition of Lc(Rp, op, Rg , βg , λg ) = (cid:88) (cid:88) (cid:13) (cid:13)Ri (cid:13) ( Rg Ri,g (cid:13) (cid:13) (cid:13)f gG ig (cid:13)λg + (cid:13) oi,g + βg oi (cid:13) (cid:13)2 ), (8) , λg , βg where Rg are learnable group-wise rotation matrix, translation vector and scale respectively to align the global camera trajectory (Rp, op) and the predicted ones (Rc, oc). Following MonST3R [113], we also use loss to smooth the camera trajectory: Ls(Rp, op) = (cid:88) i=1 (cid:16)(cid:13) (cid:13)Ri (cid:13) Ri+1 (cid:13) (cid:13) (cid:13)f + (cid:13) (cid:13)oi+1 oi (cid:17) . (cid:13) (cid:13)2 (9) The final optimization objective is the weighted combination of the losses above: Lall = α1Lp + α2Ld + α3Lc + α4Ls. (10) note on the invariants. The model predicts point, disparity maps and ray map origins up to scale, as this cannot be uniquely determined from monocular video. The disparity map is also recovered up to translation, which discounts the focal length (this is sometimes difficult to estimate due to the dolly zoom effect). Likewise, the ray map origin is recovered up to shift, necessary to allow normalizing these maps. 5 Category Method Sintel [5] Bonn [55] KITTI [17] Abs Rel δ < 1.25 Abs Rel δ < 1.25 Abs Rel δ < 1.25 Single-frame depth Marigold [28] Depth-Anything-V2 [106] Video depth Video depth & Camera pose NVDS [95] ChronoDepth [70] DepthCrafter* [22] Robust-CVD [32] CasualSAM [116] MonST3R [113] Ours 0.532 0. 0.408 0.687 0.270 0.703 0.387 0.335 0.205 51.5 55.4 48.3 48.6 69.7 47.8 54.7 58.5 73.5 0.091 0. 0.167 0.100 0.071 - 0.169 0.063 0.059 93.1 92.1 76.6 91.1 97.2 - 73.7 96.4 97.2 0.149 0. 0.253 0.167 0.104 - 0.246 0.104 0.086 79.6 80.4 58.8 75.9 89.6 - 62.2 89.5 93.7 Table 1. Video depth estimation on Sintel [5], Bonn [55] and KITTI [17] datasets. We follow the evaluation protocols established in recent MonST3R [113] for fair comparison. Notably, results for DepthCrafter* are reported from its latest version (v1.0.1). The Best and the second best results are highlighted. 4. Experiments 4.1. Experimental Settings Training datasets. Geo4D is trained exclusively on synthetic datasets, yet demonstrates strong generalization Specifically, we use five synto real-world videos. thetic datasets for training: Spring [50], BEDLAM [2], PointOdyssey [119], TarTanAir [93], and VirtualKitti [6]. See the Supp. Mat Tab. 5 for the details. Training. Our Geo4D is initialized with the weight of Dynamicrafter [102] and trained using AdamW [48] with learning rate of 1 105 and batch size of 32. We use progressive training strategy to improve convergence and stability, First, we train the model to generate single geometric modality, i.e. the point maps, on fixed resolution of 512 320. Next, we introduce the multi-resolution training scheme to improve generalization and robustness, which includes various resolutions: 512384, 512320, 576256, 640 192. Finally, we progressively add additional geometric modalities, i.e., the ray and depth maps. Training is conducted on 4 NVIDIA H100 GPUs with total training time of approximately one week. Inference. As described in Sec. 3.2, given -frame video as input, we first split it into overlapping clips G, each containing = 16 frames, with stride of = 4. Each video clip, consisting of frames, is encoded and fed to the diffusion model to sample multi-modal 4D parameters (X i,g, Di,g, ri,g) for the video. For sampling, we use DDIM [77] with 5 steps. Finally, the alignment algorithm in Sec. 3.2 is used to fuse the clips into globally coherent 4D reconstruction of the entire video. 4.2. Video Depth Estimation Testing data. Our hypothesis is that, despite being trained on synthetic data, our model can generalize well to outof-distribution synthetic and real data as it is based on pre-trained video diffusion model. To test this hypothesis, we evaluate our model on three benchmarks: Sintel [5] is synthetic dataset that provides accurate depth annotations, covering diverse scenes with complex camera motion. KITTI [17] is large driving dataset collected using stereo cameras and LiDAR sensors. Bonn [55] focuses on dynamic indoor scenes. To ensure fair comparisons, we follow the evaluation protocol used by MonST3R [113], where depth sequences are uniformly sampled from the datasets, extracting 50110 frames per sequence for evaluation. Metrics. Following the standard affine-invariant depth evaluation protocol [65], we align the predicted video depth with the ground-truth depth before computing metrics. However, unlike single-image depth estimation [28, 105, 106], where depth alignment is performed per frame, we enforce global scale consistency by applying single scale and shift across the entire video sequence. For quantitative evaluation, we adopt two widely used depth metrics: absolute relative error (Abs Rel) and the percentage of inlier points (with threshold value of δ < 1.25). Baselines. We compare Geo4D to the state-of-the-art single-frame depth estimation (Marigold [28] and DepthAnything-V2 [106]), video depth prediction (NVDS [95], ChronoDepth [70] and DepthCrafter [22]), and joint video depth and camera pose prediction (Robust-CVD [32], CasualSAM [116], and MonST3R [113]). Results. As shown in Table 1, all versions of Geo4D outperform state-of-the-art methods by large margin. This includes DepthCrafter [22] and MonST3R [113], the most recent video depth diffusion model and the dynamic extension of DUSt3d to dynamic scenes, respectively. Notably, while both Geo4D and DepthCrafter are based on the same video diffusion model (DynamiCrafter), our model outperforms DepthCrafter by Abs Rel by 24.0% on Sintel and 17.3% on KITTI datasets despite solving more general problem. Qualitatively, Fig. 3 shows that Geo4D achieves more consistent results especially for fast moving objects. 6 Figure 3. Qualitative results comparing Geo4D with MonST3R [113]. Attribute to our group-wise inference manner and prior geometry knowledge from pretrained video diffusion, our model successfully produces consistent 4D geometry under fast motion (first row) and deceptive reflection in the water (second row). 4.3. Camera Pose Estimation Setup. We evaluate the performance of our Geo4D on both synthetic Sintel [5] dataset and realistic TUMdynamics [78] dataset. We follow the same evaluation protocol as in MonST3R [113]. Specifically, on Sintel, we select 14 dynamic sequences, and for TUM-dynamics, we sample the first 90 frames of each sequence with temporal stride of 3. After aligning the predicted camera trajectory with the ground truth one with Umeyama algorithm, we calculate 3 commonly used metrics: Absolute Translation Error (ATE), Relative Translation Error (RPE-T), and Relative Rotation Error (RPE-R). We compare our method with other state-of-the-art discriminative methods, which jointly predict camera pose and depth, including Robust-CVD [32], CausalSAM [116], and MonST3R [113]. Results. To the best of our knowledge, Geo4D is the first method that uses generative model to estimate the camera parameters in dynamic scene. As shown in Tab. 2, compared to existing non-generative alternatives, we achieve much better camera rotation prediction (RPE-R) and comparable camera translation prediction (ATE and RPE-T). 4.4. Qualitative Comparison 4D Reconstruction. We compare Geo4D with the state-ofthe-art MonST3R method on the DAVIS [23] dataset. UpMethod Sintel TUM-dynamics ATE RPE-T RPE-R ATE RPE-T RPE-R 0.360 Robust-CVD [32] CasualSAM [116] 0.141 0.108 MonST3R [113] Ours 0.185 0.154 0.035 0.042 0.063 3.443 0.615 0.732 0.547 0.153 0.071 0.063 0. 0.026 0.010 0.009 0.020 3.528 1.712 1.217 0.635 Table 2. Quantitative evaluation for camera pose estimation. We achieve comparable camera pose estimation performance with other discriminative SOTA methods. grading from pair-wise alignment as in MonsST3R to our group-wise alignment improves temporal consistency, leading to more stable and globally coherent 4D reconstruction of point maps and camera trajectory, particularly in highly dynamic scenes. As shown in the top row in Fig. 3, Geo4D successfully tracks in 4D the racing car, whereas MonST3R struggles due to the rapid motion between pairs of images. Furthermore, probably because of the strong prior captured by the pre-trained video generative model, Geo4D correctly reconstructs the reflection of the flamingo in the water (second row in Fig. 3), whereas MonST3R misinterprets the reflection as foreground object, resulting in incorrect depth. Video Depth Prediction. We compare Geo4D with stateof-the-art video depth predictors MonST3R [113] and DepthCrafter [22] on Sintel [5] dataset. Qualitatively, Geo4D produces more detailed geometry, for instance for Figure 4. Qualitative video depth results comparing Geo4D with MonST3R [113] and DepthCrafter [22]. Owing to our proposed multimodal training and alignment, as well as the prior knowledge from diffusion, our method can infer more detailed structure (first row) and more accurate spatial arrangement from video (second row). Training Point Map Depth Map Ray Map Inference Point Map Depth Map Ray Map Abs Rel δ < 1.25 ATE RPE trans RPE rot Video Depth Camera Pose - - - - - - - - - - 0.232 0.223 0.211 - 0.205 71.3 72.5 73.4 - 73. 0.335 0.237 - 0.268 0.185 0.076 0.070 - 0.192 0.063 0.731 0.566 - 1.476 0. Table 3. Ablation study for the different modalities of the geometric representation on the Sintel [5] dataset. We demonstrate the effectiveness of our key design choices that both leverage multi-modality as additional training supervision signal and postprocess through our proposed multi-modal alignment algorithm will improve the overall performance. Stride / frame Video Depth Camera Pose Abs Rel δ < 1.25 ATE RPE trans RPE rot 15 8 4 2 0.92 1.24 1.89 3.26 0.213 0.212 0.205 0.204 72.4 72.8 73.5 72.9 0.210 0.222 0.185 0. 0.092 0.074 0.063 0.058 0.574 0.524 0.547 0.518 Table 4. Ablation study for the temporal sliding window stride on the Sintel [5] dataset. Smaller strides are better. the rope on the stick in the first row of Fig. 4, and better spatial arrangement between different dynamic objects, as shown the second row of Fig. 4. 4.5. Ablation Study We ablate our key design choices and the effect of different modalities on the Sintel dataset. We study the effect of multi-modality in Tab. 3. The three modalities, i.e. point map, depth map, and ray map, and can be used either at training or inference time, or both. The first two rows show that the diffusion model trained with point maps as single modality performs worse in both video depth and camera pose estimation than the diffusion model trained with all three modalities. Therefore, the other two modalities, even if they can be seen as redundant, serve as additional supervisory signal during training, which improves the generalization ability of the diffusion model. We then investigate the effectiveness of our multi-modal alignment algorithm. Compared with the second to the fourth row in Tab. 3, which leverage only single modality during inference, multi-modal alignment optimization (last row) achieves the best performance, showing the benefits of fusing the multiple modalities at inference time. We ablate the sliding window stride in Tab. 4. Results improve with shorter stride, in part because this means that more windows and estimates are averaged, reducing the variance of the predictions by the denoising diffusion model, which is stochastic. We choose stride = 4 for our main results to balance the runtime and the performance. Note that MonST3R [113] requires 2.41 seconds to process one frame under the same setting. So, our method is 1.27 times faster than MonST3R [113]. 5. Discussion and Conclusion We have introduced Geo4D, novel approach that adapts video generator to dynamic 4D reconstruction. By building on pre-trained video generator, Geo4D achieves excellent generalization to real data despite being trained only on 4D synthetic data. We have also shown the benefits of predicting multiple modalities and fusing them at test time via optimization. Our model outperforms state-of-the-art methods on video depth and camera rotation prediction, particularly on challenging dynamic scenes. Despite these successes, our approach has limitations, one of which is that the point map encoder-decoder is still not entirely accurate, and that, in turn, is bottleneck on the overall reconstruction quality. Our approach also opens path to integrating 4D geometry in video foundation models, e.g., to generate 3D animations from text, or to provide more actionable signal when the video model is used as proxy for world model. Acknowledgments. The authors of this work were supported by Clarendon Scholarship, ERC s-UNION, and EPSRC EP/Z001811/1 SYN3D."
        },
        {
            "title": "References",
            "content": "[1] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An opensource framework for training large autoregressive visionlanguage models, 2023. 5 [2] Michael J. Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: synthetic dataset of bodies exhibiting detailed lifelike animated motion, 2023. 6, 15 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synIn Proceedings of thesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 2256322575, 2023. 3 [5] Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and Michael J. Black. naturalistic open source movie for optical flow evaluation. In European Conference on Computer Vision (ECCV), 2012. 6, 7, 8, 15 [6] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2, 2020. 6, [7] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1945719467, 2024. 2 [8] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. arXiv preprint arXiv:2503.24391, 2025. 3 [9] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. MVSplat: efficient 3d gaussian splatting from sparse multi-view images. arXiv, 2403.14627, 2024. 2 [10] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Feed-forward 360 scene synthesis from Mvsplat360: In Neural Information Processing Systems sparse views. (NeurIPS), 2024. 3 [11] Christopher Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision (ECCV), pages 628 644. Springer, 2016. 2 [12] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene generation from monocular videos. Advances in Neural Information Processing Systems (NeurIPS), 2024. 9 [13] Yiquan Duan, Xianda Guo, and Zheng Zhu. Diffusiondepth: Diffusion denoising approach for monocular depth In European Conference on Computer Vision estimation. (ECCV), pages 432449. Springer, 2024. 3 [14] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In European Conference on Computer Vision (ECCV), pages 241258. Springer, 2024. 3, 4 [15] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [16] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own corIn relation: noise prior for video diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2293022941, 2023. 3 [17] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. International Journal of Robotics Research (IJRR), 2013. 6 [18] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In Proceedings of the IEEE/CVF international conference on computer vision (CVPR), pages 97859795, 2019. 2 [19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. In International Conference on Learning Representations (ICLR), 2024. 3 [20] Junlin Han, Filippos Kokkinos, and Philip Torr. Vfusion3d: Learning scalable 3d generative models from video diffusion models. In European Conference on Computer Vision (ECCV), pages 333350. Springer, 2024. [21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Neural Information Processing Systems (NeurIPS), 35:86338646, 2022. 3 [22] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025. 2, 6, 7, 8, 15 [23] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Johannes Kopf. Temporally coherent completion of dynamic video. In ACM, 2016. 2, 7 [24] Matthias Innmann, Michael Zollhofer, Matthias Nießner, Christian Theobalt, and Marc Stamminger. Volumedeform: In EuroReal-time volumetric non-rigid reconstruction. pean conference on computer vision (ECCV), pages 362 379. Springer, 2016. 2 [25] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Farm3d: Learning articulated 3d animals by distilling 2d diffusion. In 2024 International Conference on 3D Vision (3DV), pages 852861. IEEE, 2024. 3 [26] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d model with multi-view video diffusion. Advances in Neural Information Processing Systems (NeurIPS), 2024. [27] Zeren Jiang, Chen Guo, Manuel Kaufmann, Tianjian Jiang, Julien Valentin, Otmar Hilliges, and Jie Song. Multiply: Reconstruction of multiple people from monocular video in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [28] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 94929502, 2024. 2, 3, 4, 6 [29] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3 [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023. 3 [31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [32] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16111621, 2021. 6, 7 [33] Akshay Krishnan, Xinchen Yan, Vincent Casser, and AbImage latent diffusion for joint arXiv preprint hijit Kundu. Orchid: appearance and geometry generation. arXiv:2501.13087, 2025. 3, 4 [34] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. 2 [35] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. 2 [36] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3D: Fast text-to-3D with sparse-view generation and large reconstruction model. In The Twelfth International Conference on Learning Representations (ICLR), 2024. 3 [37] Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, and Ming-Ming Cheng. Sora generates videos with stunning geometrical consistency. arXiv, 2402.17403, 2024. 2 [38] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 64986508, 2021. [39] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 42734284, 2023. 2 [40] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3 [41] Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning efficient point cloud generation for dense 3d object reconstruction. In proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2018. 2 [42] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: HighIn Proceedings of resolution text-to-3d content creation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 300309, 2023. 3 [43] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In CVPR, 2024. [44] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model, 2024. 3 [45] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision (ICCV), pages 92989309, 2023. 3 [46] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [47] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 99709980, 2024. 3 [48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [49] Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, and Shiwei Li. Matrix3d: Large photogrammetry model all-inone. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3, 4 10 [50] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: high-resolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 6, 15 [51] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of In Proceedings of the any object from single image. IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 84468455, 2023. 3 [52] Mildenhall, PP Srinivasan, Tancik, JT Barron, Ramamoorthi, and Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision (ECCV), 2020. 2 [53] Richard Newcombe, Dieter Fox, and Steven Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 343352, 2015. 2 [54] NVIDIA, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klar, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai. arXiv, 2501.03575, 2025. 2 [55] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Gigu`ere, and C. Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 78557862, 2019. [56] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 165 174, 2019. 2 [57] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 58655874, 2021. 2 [58] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinBrualla, and Steven Seitz. higherdimensional representation for topologically varying neural radiance fields. ACM Transactions on Graphics (TOG), 40 (6):112, 2021. 2 Hypernerf: [59] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 2: large-scale foundation world model, 2024. 2 [60] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy In European conference on computer vision networks. (ECCV), pages 523540. Springer, 2020. [61] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 3 [62] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields In Proceedings of the IEEE/CVF for dynamic scenes. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1031810327, 2021. 2 [63] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 99149925, 2024. 3 [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning vision. (ICML), pages 87488763. PmLR, 2021. 5 [65] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 44:16231637, 2019. 6 [66] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, and Huan Ling. L4gm: Large 4d gaussian reconstruction model. Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1068410695, 2022. 4 [68] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 94209429, 2024. 3 [69] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. Advances in Neural Information Processing Systems (NeurIPS), 36:39443 39469, 2023. 3 [70] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv, 2406.01493, 2024. 6 [71] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations (ICLR), 2024. 3 [72] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1961519625, 2024. 2 [73] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations (ICLR), 2023. [74] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 24372446, 2019. 2 [75] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems (NeurIPS), 34:1931319325, 2021. 2, 4 [76] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatarXiv preprint ting from uncalibarated image pairs. arXiv:2408.13912, 2024. 2 [77] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. 6 [78] Jurgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 573 580, 2012. struction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [80] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao F. Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3D: Feed-forward generalisable 3D scene reconstruction from single image. In Proceedings of the International Conference on 3D Vision (3DV), 2025. 2 [81] Stanislaw Szymanowicz, Jason Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan Barron, and Philipp Henzler. Bolt3d: Generating 3d scenes in seconds. arXiv preprint arXiv:2503.14445, 2025. 3 [82] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, and Tong He. Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945, 2025. 3 [83] Shubham Tulsiani, Tinghui Zhou, Alexei Efros, and Jitendra Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 26262634, 2017. 2 [84] S. Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(4):376 380, 1991. 15 [85] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision (ECCV), pages 439457. Springer, 2024. [86] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. In International Conference on 3D Vision (3DV), 2024. 2 [87] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1261912629, 2023. 3 [88] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [89] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: Visual geometry grounded network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 1, 2 [90] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 5267, 2018. 2 [79] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter Image: Ultra-fast single-view 3D recon- [91] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of 12 motion: 4d reconstruction from single video. preprint arXiv:2407.13764, 2024. 2 In arXiv [92] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2069720709, 2024. 1, 2, 3, 4 [93] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian A. Scherer. Tartanair: dataset to push the limits of visual slam. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 49094916, 2020. 6, 15 [94] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems (NeurIPS), 36:75947611, 2023. [95] Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin. Neural video depth stabilizer. In ICCV, 2023. 6 [96] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: Highfidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. 3 [97] Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 4 [98] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain BR EGIER, Yohann Cabon, Vaibhav ARORA, Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Jerome Revaud. CroCo: self-supervised pre-training for 3D vision tasks by cross-view completion. In Proc. NeurIPS, 2022. 2 [99] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In CVPR, 2024. 2 [100] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Unsupervised learning of probably symmetric deformable In Proceedings of 3d objects from images in the wild. the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 110, 2020. 4 [101] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: In Proceedings of Tracking any 2d pixels in 3d space. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [102] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision (ECCV), pages 399 417. Springer, 2024. 1, 3, 4, 6 [103] Tian-Xing Xu, Xiangjun Gao, Wenbo Hu, Xiaoyu Li, SongHai Zhang, and Ying Shan. Geometrycrafter: Consistent geometry estimation for open-world videos with diffusion priors, 2025. 3 [104] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of In Proceedings of 1000+ images in one forward pass. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [105] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proc. CVPR, 2024. 6 [106] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems (NeurIPS), 2024. 6 [107] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2033120341, 2024. [108] David Yifan Yao, Albert J. Zhai, and Shenlong Wang. Uni4d: Unifying visual foundation models for 4d modeling from single video, 2025. 3 [109] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [110] Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. Pointr: Diverse point cloud completion with geometry-aware transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1249812507, 2021. 2 [111] Yuheng Yuan, Qiuhong Shen, Xingyi Yang, and Xinchao Wang. 1000+ fps 4d gaussian splatting for dynamic scene rendering, 2025. 2 [112] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent difInternational fusion models for text-to-video generation. Journal of Computer Vision (IJCV), pages 115, 2024. 3 [113] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for esIn Intertimating geometry in the presence of motion. national Conference on Learning Representations (ICLR), 2025. 1, 3, 5, 6, 7, [114] Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as In International rays: Pose estimation via ray diffusion. Conference on Learning Representations (ICLR), 2024. 5 [115] Qihang Zhang, Shuangfei Zhai, Miguel Angel Bautista, Kevin Miao, Alexander Toshev, Joshua Susskind, and Jiatao Gu. World-consistent video diffusion with explicit 3d 13 modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3 [116] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William T. Freeman. Structure In European Conference and motion from casual videos. on Computer Vision (ECCV), 2022. 6, 7 [117] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffuIn Proceedings of the sion models for visual perception. IEEE/CVF International Conference on Computer Vision (ICCV), pages 57295739, 2023. 3 [118] Chuanxia Zheng and Andrea Vedaldi. Free3d: Consistent novel view synthesis without 3d representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97209731, 2024. 3, 4 [119] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1985519865, 2023. 6, 14 Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary document, we provide additional materials to supplement our main submission. In the supplementary video, we show more visual results using our method. The code will be made publicly available for research purposes. 6. Implementation details 6.1. Training dataset As shown in Tab. 5, we use five synthetic datasets for training: Spring [50], BEDLAM [2], PointOdyssey [119], TarTanAir [93], and VirtualKitti [6]. Although all datasets we used are synthetic data sets, we found that there are still some missing depth pixels in PointOdyssey [119]. So we apply max pooling to inpaint those missing pixels. During training, we sample each data set with the corresponding ratio in Tab. 5. For each sample, we sample 16 frames from the sampled sequence with the sampling stride randomly chosen from {1, 2, 3} to allow our diffusion model to adapt to different input videos with various FPS. 6.2. Optimization details The overall optimization process is outlined in Algorithm 1. We first predict all three modality maps using our diffusion model for each video clip g. The predicted point maps are then roughly aligned based on the overlapping frames using the Umeyama algorithm [84]. The camera intrinsic Kk is initialized by minimizing the projection error of the point map k,gk in its reference (first) frame within each window group gk. The camera extrinsics are then initialized using the RANSAC PnP algorithm. In the first stage of optimization, the point maps are roughly disentangled into camera pose and depth map. The disparity map is then aligned with the global depth inferred from point maps by solving Eq. (5) to obtain the scale and shift parameters. The camera parameters extracted from the predicted ray map are aligned with the global camera trajectory based on the reference (first) frame of each video clip via Eq. (8). After initializing all the alignment learnable parameter including rotation Rg across different modality, where {p, d, c}, we jointly optimize all the learnable parameters by Eq. (10). , and shift βg , scale λg 6.3. Ablation study for number of denoising steps We study the influence of the number of denoising steps during inference. As shown in Tab. 6, the model achieves optimal performance around steps equal to 5. Compared to the video generation task, where larger denoising step usually produces more detailed generated video, 4D recon15 Algorithm 1 Multi-Modal Alignment Optimization Initialized by Eqs. (6) and (7) from raymaps , βg , Rg Initialized by Ransac PnP from pointmaps , oi,g 1: i,g, Di,g, ri,g Predicted by our diffusion model p, λg 2: Di Initialized by Umeyama algorithm Optimized from Xk,gk 3: Kk 4: Ri p, oi 5: Ri,g ri,g 6: repeat 7: 8: 9: 10: 11: 12: 13: 14: 15: until max loop reached else if Iteration < Align start iteration then p, λg arg min Ld ( Eq. (5)) , βg if Iteration = Align start iteration then arg min Lc ( Eq. (8)) arg min Lall λg , βg , λg Rg , arg min Lp + Ls , Rg , Rg p, Ki p, Ki , βg , βg p, Ri p, Ri p, λg end if p, oi p, oi else Di Di Dataset Scene type #Frames #Sequences Ratio PointOdyssey [119] TartanAir [93] Spring [50] VirtualKITTI [6] BEDLAM [2] Indoors/Outdoors Indoors/Outdoors Outdoors Driving Indoors/Outdoors 200K 1000K 6K 43K 380K 131 163 37 320 10K 16.7% 16.7% 16.7% 16.7% 33.3% Table 5. Details of training datasets. Our method only uses synthetic datasets for training. Steps 1 5 10 25 Video Depth Camera Pose Abs Rel δ < 1.25 ATE RPE trans RPE rot 0.221 0.205 0.207 0.220 70.7 73.5 73.2 72.2 0.234 0.185 0.212 0.211 0.072 0.063 0.071 0. 0.753 0.547 0.508 0.564 Table 6. Ablation study for the DDIM sampling steps. on the Sintel [5] dataset. Method Video Depth Camera Pose Abs Rel δ < 1.25 ATE RPE trans RPE rot w/o fine-tuned fine-tuned 0.212 0.205 72.1 73.5 0.192 0.185 0.061 0. 0.577 0.547 Table 7. Ablation study for the fine-tuned point map VAE on the Sintel [5] dataset. The fine-tuned point map VAE performs better than the original one. struction is more deterministic task, which requires fewer steps. Similar phenomena are also observed in [22], which utilize video generator for the video depth estimation task. Figure 5. Additional qualitatively results. Our method generalizes well to various scenes with different 4D objects and performs robustly against different camera and object motion. 6.4. Ablation study for fine-tuned point map VAE demonstrating the necessity and the effectiveness of our fine-tuning strategy. As stated in the main paper, we added an additional branch to predict the uncertainty for our point map VAE and finetuned it based on Eq. 3. We perform the ablation study on our fine-tuning strategy. As shown in Tab. 7, our fine-tuned point map VAE achieves consistent better performance on both video depth estimation and camera pose estimation tasks compared with the original pre-trained image VAE, 7. Visualization As shown in Fig. 5, we show additional visualization results for both the indoor, outdoor, and driving scene. Although our model is only trained on synthetic datasets, it is generalized to real-world data with diverse objects and motions."
        }
    ],
    "affiliations": [
        "Naver Labs Europe",
        "Visual Geometry Group, University of Oxford"
    ]
}