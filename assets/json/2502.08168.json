{
    "paper_title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
    "authors": [
        "Zhiming Ma",
        "Xiayang Xiao",
        "Sihao Dong",
        "Peidong Wang",
        "HaiPeng Wang",
        "Qingyun Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models."
        },
        {
            "title": "Start",
            "content": "SARChat-Bench-2M: Multi-Task Vision-Language Benchmark for SAR Image Interpretation Zhiming Ma1,2, Xiayang Xiao1,2, Sihao Dong3, Peidong Wang4, HaiPeng Wang1, Qingyun Pan5 1The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China 2China Mobile Internet Company Ltd., Guangzhou, China 3The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China 4School of Computer Science and Engineering, Northeastern University, Shenyang, China 5China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China"
        },
        {
            "title": "Abstract",
            "content": "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs capabilities in SAR image interpretation, which provides paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https: //github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models. 5 2 0 2 2 1 ] . [ 1 8 6 1 8 0 . 2 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "In recent years, deep neural networks, particularly CNNs (Dosovitskiy, 2020) and ViTs (LeCun et al., 1998), have achieved remarkable progress in remote sensing data analysis, significantly enhancing the processing efficiency and analytical accuracy of large-scale datasets. However, existing research primarily focuses on visual feature extraction, while These authors contributed equally to this work. *Corresponding author. Xiayang Xiao and Haipeng Wang 1 lacking in deep semantic parsing and general reasoning capabilities (Li et al., 2024), limiting model applicability and interpretability in complex scenarios. With the advancement of Large-Language Models (LLMs) in natural language processing, VisionLanguage Models, through integrating generative pre-training and instruction tuning strategies, have demonstrated robust zero-shot learning and generalization capabilities in multimodal interactive tasks (Dai et al., 2023). This has inspired researchers to explore the deep integration of visual models with LLMs. Although preliminary achievements have been made with models like RSGPT (Hu et al., 2023) and GeoChat (Kuckreja et al., 2024), these approaches are primarily designed for natural images and struggle to perform well in SAR applications. SAR images inherently pose significant interpretation challenges due to their scattering imaging mechanisms, characterized by blurred target edges, dispersed speckles, and orientation sensitivity. Meanwhile, existing SAR datasets primarily focus on visual recognition tasks (Kuckreja et al., 2024; Cheng et al., 2022; Zhang et al., 2023), leaving critical shortage of large-scale, high-quality image-text alignment datasets. Both these intrinsic characteristics and data limitations impede the advancement of VLMs in the SAR domain. As shown in Figure 1, we present SARChat-2M, large-scale multimodal conversational dataset for SAR images, and establish SARChat-Bench, comprehensive multimodal task-oriented benchmark for the SAR domain. The SARChat-2M dataset contains approximately 2 million high-quality SAR image-text pairs across maritime, terrestrial, and urban scenarios, featuring fine-grained semantic descriptions and multi-scale resolutions (0.3-10 meters). Through cross-modal representation learning, the dataset enables multi-task learning capabilities, including image captioning, VQA(Visual Question Figure 1: An overview of SARChat-Bench-2M. The left figure demonstrates the representative tasks realized with the SAR image-text dataset, SARChat-2M, constructed in this paper. Validating the datasets efficacy and superiority in supporting multi-task applications. The right figure presents the correlation radar charts and quantitative line graphs derived from the performance evaluation of 16 VLMs basing on this dataset, establishing the benchmark (SARChat-Bench) within this domain. Answering), visual localization, and object detection. The SARChat-2M systematically evaluates vision-language models performance through six core tasks: classification, description, counting, localization, recognition, and refering."
        },
        {
            "title": "The primary contributions of this paper are as",
            "content": "follows. (1) The construction of SARChat-2M, the largest SAR remote sensing instruction-following dataset to date, comprising over 2 million high-quality image-text pairs across multi-scenario taskoriented dialogues, addressing the knowledge scarcity of VLMs in the SAR domain. (2) The development of SARChat-Bench, comprehensive SAR domain multimodal benchmark encompassing six core tasks (classification, description, counting, localization, recognition, and refering), enabling systematic evaluation of visionlanguage models through multi-dimensional assessment metrics. (3) It pioneers research paradigm applicable to the SAR field, providing reference ideas for the construction of models in other remote-sensing vertical domains. The methods and processes adopted in data collection, annotation, as well as model training and evaluation in this study have good generality and extensibility."
        },
        {
            "title": "2.1 VLMs for Remote Sensing",
            "content": "VLMs are capable of converting images into natural language descriptions and parsing the relationships between objects, demonstrating remarkable performance in tasks such as text-image retrieval, image captioning, and visual question answering. Recently, models like RemoteClip (Liu et al., 2024) have been applied to the field of remote sensing images, primarily focusing on cross-modal retrieval and zero-shot classification. However, these models have not addressed tasks such as image description generation and visual grounding. The RSGPT model has achieved text description and visual question answering for remote sensing images, but it has not expanded to tasks such as classification and detection. The GeoChat model has advanced multi-task conversational processing of high-resolution remote sensing imagery, including scene classification, visual question answering, multi-turn dialogue, visual grounding, and reference object detection. However, these models, including GeoChat, are primarily trained on optical remote sensing data, which limits their application scope. EarthGPT (Zhang et al., 2024) has extended the application of multimodal large language models to the remote sensing field through 2 Figure 2: Construction of SARChat-2M dataset. On the left, ten existing SAR detection benchmark datasets. The middle part is the SARDet-100K dataset, formed by integrating the ten datasets on the left. On the right, six core tasks constructed based on the dataset are presented, with each task corresponding to different task identifiers, operation steps, and relevant templates. instruction tuning, but its performance in SAR image multi-task processing still needs improvement. Compared with natural images, the interpretation of SAR images is more challenging, which poses higher demands on the models processing capabilities and adaptability."
        },
        {
            "title": "2.2 Remote Sensing Vision-Language Datasets",
            "content": "Remote sensing datasets are essential for intelligent interpretation models. Existing datasets such as UCM Captions, Sydney Captions (Qu et al., 2016), RSICD (Lu et al., 2017), RSITMD (Yuan et al., 2022), and RSVG (Zhan et al., 2023) provide preliminary resources for studying the correlation between remote sensing images and text, but they are limited in scale and only cover optical images. Although large-scale datasets like MillionAID (Long et al., 2021), FMoW (Christie et al., 2018), and BigEarthNet (Sumbul et al., 2019) exist, they lack text-image pairs. The RS5M dataset (Zhang et al., 2023), containing 5 million imagetext pairs, is still limited to optical images. The 3 MMRS-1M dataset (Zhang et al., 2024), which covers optical, infrared, and SAR modes, has very low proportion of SAR image-text data. Therefore, this paper constructs the SARChat-2M dataset, which focuses on SAR images and contains over 2 million image-text pairs, covering tasks such as classification, detection, caption generation, VQA, and visual grounding."
        },
        {
            "title": "3.1.1 Dataset Overall",
            "content": "As shown in Figure 2, we propose SARChat-Bench2M, multi-task benchmark dataset for SAR images, comprising 2 million multimodal dialogue samples (1,836,912 train and 226,636 test samples) to ensure robust model training and evaluation. The dataset covers six core SAR image analysis tasks: classification, fine-grained description, instance counting, spatial groundand refering, identification, cross-modal ring. Based on the SARDet-100K dataset (Dai et al., 2024), it incorporates multimodal adaptations and enhanced language annotations from ten established SAR detection benchmarks such as AIR-SARShip(1.0&2.0)(Wang et al., 2019a), HRSID(Wei et al., 2020), MSAR(Chen et al., 2022), SADD(Zhang et al., 2021a), SARAIRcraf(Zhirui et al., 2023), ShipDataset(Wang et al., 2019b), SSDD(Zhang et al., 2021b), OGSOD(Wang et al., 2023), and SIVED(Lin et al., 2023). The dataset establishes image-text correspondences across six semantic categories (ships, tanks, bridges, ports, aircraft, and automobiles) through cross-modal representation learning, resulting in 2 million curated annotations. This study designs the first evaluation framework for SAR remote-sensing visual-language models, addressing three key requirements: multi-task supervised pre-training, cross-domain adaptation between SAR and language domains, and comprehensive performance assessment using standardized metrics."
        },
        {
            "title": "3.1.2 Task Definition",
            "content": "Based on the characteristics of SAR images and the core capabilities of the VLM, this study constructs an evaluation system consisting of six tasks. The definitions of each task are as follows: (1) Classification: Classification is fundamental task in SAR image interpretation that evaluates the VLMs basic visual understanding through target category discrimination. (2) Fine-Grained Description: This task requires the VLM to identify SAR target categories and analyze their geometric attributes (morphological features and spatial orientations), evaluating the models ability to reason about SAR-specific spatial-geometric relationships. (3) Instance Counting: This task requires accurate counting of multiple SAR targets while extracting their spatial coordinates and orientation information. The key challenge lies in preventing double-counting errors, particularly in complex scenes where multiple targets overlap. The model must maintain robust counting performance while handling various target densities and background complexities. (4) Spatial Grounding: This task enables the VLM to understand and express spatial relationships (positions, distances, directions) among multiple targets in SAR images, which is crucial for military reconnaissance scenarios. (5) Cross-Modal Identification: Given specified spatial coordinates, the VLM infers target attributes and generates comprehensive descriptions (size, morphology, direction, distance). This task examines the models ability to fuse and reason about multimodal information in SAR interpretation. (6)Referring: This is reverse-reasoning task. The model is required to locate specific instances in SAR images based on the target semantic categories specified by users and output abstract spatial orientations, which is of great significance for achieving rapid target retrieval in human-machine collaboration."
        },
        {
            "title": "3.2 Task-Oriented Data Generation",
            "content": "Based on the characteristics of the six tasks, this study designs multimodal dialogue data generation scheme, and its specific rules and implementation logic are as follows: (0) Dataset Definitions Our dataset adopts unified representation scheme across all visual-language tasks to ensure consistency and interpretability. The spatial information is uniformly encoded using the bounding box format {<x1><y1><x2><y2>}, while spatial relationships are structured through standard 33 grid system (consisting of top-left, top, top-right, left, middle, right, bottom-left, bottom, bottomright regions). These definitions form the foundational framework for our task formulations and evaluation metrics, enabling systematic assessment of visuallanguage models capabilities. (1) Classification Task The Classification Task assesses the models SAR image recognition capabilities through 20 distinct question-answer template pairs. Random template combinations enhance data diversity, with standardized notation for multi-target scenarios. Q&A Example: User: \"Please identify the target categories in the figure (candidate categories: {total category list})\" AI: \"The {specific category} targets can be identified in the figure\" (2) Fine-Grained Description Task This task evaluates the models structured parsing of satellite imagery through comprehensive quality control. Following our dataset definitions, we filter images below 224224 pixels and exclude targets with area ratio < 1 4 = wbox hbox Wimg Himg 100 (1) ical analysis using bounding-box aspect ratios (>1.5:wide-body; 0.67ratio1.5:approximately square; <0.67:tall-body). where wbox and hbox denote the width and height of the target bounding box, respectively; Wimg and Himg represent the width and height of the image. Q&A Example: User: \"Please describe the content of the satellite image in detail\" AI: \"In the figure, {size}{category} can be seen located at {orientation},\"+\"and there is also {size}{category} located at {orientation}\" (3) Instance Counting Task This task evaluates the models quantitative counting capabilities. We prefix 15 question templates with [count] identifiers for task specification, while leveraging our unified bounding box format for structured outputs. The task supports extended expressions for multi-instance scenarios through coordinate serialization. Q&A Example: User: \"[count] How many {category} are there in the image?\"\" AI: \"There are 3 instances. {bbox}{bbox}{bbox}\" (4) Spatial Grounding Task This task assesses the models capability in expressing structural spatial relationships between multiple targets. Based on the established grid system, we quantify spatial relationships through relative distance metrics (proximal threshold: (Wimg + Himg)/8) and directional relationships (horizontal, vertical, diagonal orientations). The task comprises 15 spatial-relationship templates prefixed with [grounding] identifiers, adhering to our unified spatial representation scheme. Q&A Example: User: \"[grounding] Can you describe the spatial arrangement of various features in this SAR image?\" AI: \"In this SAR image, can identify several objects {category-1} in the top right portion {bbox}. There are 2 {category-2} in the bottom left: one in the bottom left portion {bbox}, another in the middle left portion {bbox}. The {category-1} is far from the {category-2}.\" (5) Cross-Modal Identification locations:"
        },
        {
            "title": "There",
            "content": "their and is This task verifies models cross-modal parsing capabilities using three-tier feature description system: spatial positioning employs 33 grid partitioning method for orientation description. Quantitative classification includes fivelevel size descriptions based on area-ratio thresholds (0.4:very large; 0.25:large; 0.1:medium; 0.03:small; <0.03:very small) and morpholog-"
        },
        {
            "title": "Feature",
            "content": "integration combines spatial-sizeinto comprehensive morphological elements target profiles. The system implements 20 differential response templates with dedicated [identif y] instruction identifier and structured output templates. Q&A Example: User: \"[identify] What object is located in {bbox}?\" AI: \"This is medium-sized {category} located in the top right portion of the image. It appears to be wide in shape.\" (6) Referring Task This task evaluates cross-modal correlation capabilities between natural language and image regions. Queries follow the pattern \"Where is the {category}?\", prefixed with [ref er] identifiers. The task outputs both precise bounding box coordinates and grid-based orientation descriptions, adhering to our unified spatial representation framework through nested parenthetical notation. Q&A Example: User: \"[refer] Where is the {category}?\" AI: \"{bbox} (top right).\""
        },
        {
            "title": "3.3 Quantitative Analysis of Datasets",
            "content": "This study conducts quantitative analyses from two dimensions: category distribution and object morphology. Table 1: Category Distribution Statistics"
        },
        {
            "title": "Ship\nAircraft\nCar\nTank\nBridge\nHarbor",
            "content": "93,373 (46.98%) 40,705 (20.48%) 9,561 (4.81%) 24,187 (12.17%) 27,615 (13.89%) 3,306 (1.66%) 10,741 (44.38%) 6,779 (28.01%) 1,230 (5.08%) 1,773 (7.33%) 3,281 (13.56%) 399 (1.65%) (1) Category Distribution Characteristics As shown in Table 1, the ship category dominates both training and test sets (46.98% and 44.38% respectively), while the harbor category represents less than 2%. significant distribution shift is observed in the aircraft category, with 7.53% increase in the test set compared to the training set. Categories such as cars, tanks, and bridges maintain moderate and stable proportions across both sets. This imbalanced distribution presents challenges for target detection tasks while offering opportunities to investigate solutions for handling imbalanced data. (2) Object Morphology Analysis This study quantify geometric characteristics using aspect ratio (AR): AR = hbox wbox 100% (2) Table 2: Aspect Ratio Distribution Comparison Metric Training Set Test Set Diff-Rate Mean Median SD 1.28 1.062 1.18 1.26 1.05 0. -2.03% -1.69% -22.82% As shown in Table 2, the differences in central tendency between training and test sets are minimal (mean: -2.03%, median: -1.69%). The test set exhibits 22.82% lower standard deviation, indicating more concentrated distribution. The key morphological distribution intervals of targets are illustrated in Appendix .1.2. The dataset exhibits three distinct morphological categories based on aspect ratio (AR): broadbodied (AR 0.67), nearly square-shaped (0.67 < AR 1.5), and tall-bodied (AR > 1.5). For more distribution analysis, please refer to the Appendix .1.2."
        },
        {
            "title": "4 Evaluation Method and Settings",
            "content": "This section aims to introduce the evaluation methods for six key tasks. These tasks encompass the models capabilities in multiple core areas such as information processing, target localization, and semantic understanding, and can reveal the models strengths and weaknesses from different dimensions."
        },
        {
            "title": "4.1 Evaluation Metrics",
            "content": "(1) Accuracy: core metric reflecting model prediction fit, calculated as: Acc = P + + 100% (3) where denotes correct positive predictions, represents false positive predictions, and indicates false negative predictions. (2) Intersection over Union (IoU): In tasks involving localization, identification, and reference, IoU is key metric measuring the overlap between predicted and ground-truth bounding boxes (bbox). Higher IoU values indicate greater overlap and better localization performance. All IoU-related calculations in this paper are performed with thresholds of 0.25 and 0.5. (3) Overall Score Calculation: Sm = (cid:88) tT am,t nt iT ni (cid:80) (4) Among them, nt represents the count of task t, am,t denotes the accuracy of model on task t, and is the set of all tasks. The detailed calculation of am,t for each task can be found in Appendix .3."
        },
        {
            "title": "4.2 Assessment Methods",
            "content": "This section elaborates on the specific evaluation method processes for six types of tasks. (1) Instance Counting: Compare predicted and label object counts for single-class evaluation. (2) Spatial Grounding: Evaluate spatial accuracy through IoU-based bbox matching and abstract position analysis (e.g., \"top\", \"bottom\") from natural language descriptions. (3) Cross-Modal Identification: Calculate IoU between predicted and ground-truth bboxes for both single and multiple target scenarios to assess crossmodal matching capability. (4) Referring: Assess referring accuracy through IoU metrics in both single-target and multi-target contexts. (5) Fine-Grained Description: Segment predictions and ground-truth into short phrases, extract category and position information, and compare content sets for detailed description evaluation. (6) Classification: Compare predicted and groundtruth categories to assess classification accuracy."
        },
        {
            "title": "5 Experiments and Analysis",
            "content": "5."
        },
        {
            "title": "Implementation Details",
            "content": "During the fine-tuning stage, the model is trained for 1 epoch with batch size of 4 (effective batch size 32 with gradient accumulation steps of 4). This study employs LoRA training method with rank=8 and alpha=32 on all linear layers. The learning rate is initialized at 1e-4 with 0.1 warm-up ratio. All experiments are conducted on 2 NVIDIA A100 GPUs using bfloat16 precision and gradient checkpointing."
        },
        {
            "title": "5.2 Benchmark Evaluation",
            "content": "To verify the effectiveness and practicality of the SARChat-2M dataset, this study presents 6 Table 3: Performance comparison of different vision-language models Model InternVL2.5 DeepSeekVL Phi-3.5-vision GLM-Edge-V mPLUG-Owl3 Qwen2-VL LLaVA Yi-VL Param Avg score Only count 92.79 74.14 8B 4B 91.57 72.68 2B 90.55 71.52 1B 88.89 69.87 7B 88.99 20.66 1.3B 84.01 19.61 4.2B 92.06 72.69 2B 90.20 71.59 5B 90.48 73.44 7B 91.71 71.00 2B 90.32 67.56 1B 89.68 67.03 90.76 72.79 7B 2B 90.27 69.63 7B 91.21 71.89 6B 84.35 32.62 Tasks Instance Count Abstract IoU=.25 IoU=.5 position 81.25 52.17 83.33 47.35 50.00 44.22 0.00 39.35 64.29 4.19 75.00 1.32 62.50 47.60 42.86 40.37 75.00 44.56 100.00 35.27 75.00 28.83 75.00 24.98 50.24 0.00 50.00 45.47 57.14 46.80 75.00 9.44 61.37 57.54 54.11 50.18 8.49 4.00 57.48 51.97 56.30 48.07 41.56 38.64 58.51 53.62 56.89 14.35 Spatial Ground Cross-Modal ID Multi-target Ref Single-target Ref IoU=.25 IoU=.5 IoU=.25 IoU=.5 Multi 60.13 62.25 55.29 60.89 52.16 60.81 44.99 56.30 48.84 65.32 34.28 60.38 55.70 58.85 46.46 59.15 51.81 61.38 38.00 56.37 30.16 45.65 24.02 44.07 57.04 64.17 51.53 59.04 56.70 62.70 16.63 53. Single Multi 98.84 87.91 98.01 85.90 97.79 81.92 96.98 82.24 98.97 85.78 96.40 82.00 98.93 87.29 97.54 86.33 96.69 89.96 99.27 93.32 98.95 97.58 98.72 97.19 97.54 83.87 97.55 78.49 97.84 85.79 93.63 72.38 Single 98.98 98.76 98.63 98.60 99.05 97.45 98.59 98.60 95.96 99.51 99.42 98.87 99.18 99.26 98.42 97.95 37.49 34.05 27.05 22.13 28.75 16.11 31.65 24.15 30.68 19.72 14.91 11.86 39.11 32.60 30.81 7.76 74.86 69.92 68.50 62.33 64.34 53.58 70.95 65.57 69.36 57.27 50.46 44.34 70.55 65.31 71.89 32.95 23.46 18.86 13.91 9.94 13.66 6.23 17.16 10.66 15.41 7.66 5.42 4.12 26.29 20.12 15.48 2.69 Descript Class 63.43 58.84 56.36 53.30 51.08 44.44 59.95 57.86 61.45 54.65 41.76 40.16 63.11 55.20 61.35 38.15 97.25 97.27 96.69 96.65 93.23 47.37 96.42 97.39 98.02 98.80 98.31 98.06 97.30 96.88 96.90 95.32 SARChat-Bench, series of benchmark experiments on trained models which are based on this dataset. As shown in Table 3, sixteen mainstream visual-language models, such as Qwen2VL(Wang et al., 2024), InternVL2.5(Chen et al., 2024), DeepSeekVL(Lu et al., 2024), Phi-3.5vision(Abdin et al., 2024), GLM-Edge-V(GLM et al., 2024), mPLUG-Owl3(Ye et al., 2024), YiVL(Young et al., 2024) and LLaVA(Liu et al., 2023). In experiments of different tasks, each model exhibits diverse characteristics: (1) Instance Counting: This task requires VLMs to identify the number of specific objects in the image. Two leading model families achieve stateof-the-art performance: InternVL2.5 and QwenVL2, reaching accuracies of 74.14% and 72.79% respectively. However, the accuracy of most other models fall below 60%, highlighting both the challenging nature of the counting task and the datasets effectiveness in differentiating model capabilities. (2) Spatial Grounding: This task evaluates models capability in spatial localization. For abstract position descriptions, mPLUG-Owl3-7B achieves 100% accuracy, significantly outperforming other models. The mPLUG-Owl3 family maintains superior performance (>90%) in single-target localization, while other models achieve 80%-85%. However, in multi-target scenarios, most models accuracy drops to approximately 60%. These results suggest that accurate multi-target spatial information processing remains crucial area for future model improvements. (3) Cross-Modal Identification: This task focuses on the models ability to build precise connections between visual information and other modal information. In this experiment, the process from image recognition to text description is mainly concerned. The experimental data shows that for both singletarget and multi-target tasks, the accuracy rates of most models exceed 90%. Among them, the mPLUG-Owl3-7B model performs the best, with the accuracy rates of single-target and multi-target tasks reaching 99.27% and 99.51% respectively, fully demonstrating the powerful capabilities of large language models in cross-modal identification tasks. (4) Referring: This task challenges models to precisely locate objects in SAR images based on textual descriptions. Our experiments reveal significant performance gaps: models achieve less than 75% accuracy on single-target tasks and below 40% on multi-target scenarios. These results highlight the current limitations in cross-modal alignment, particularly in establishing precise text-to-object correspondences within SAR imagery. (5) Fine-Grained Description: This task requires the model to provide detailed feature and attribute descriptions of the objects in the image. The experiment shows that the model accuracy rates are in the range of 40%-63%. Among them, models with larger parameters such as Qwen2-VL-7B and InternVL2.5-8B perform outstandingly and can give more detailed and accurate descriptions. In contrast, other models with smaller parameter sizes perform poorly, indicating that the accuracy rate of the fine-grained description task is highly sensitive to the models parameter size. (6) Classification: The image classification task evaluates models ability to categorize images based on their content. According to the table data, regardless of parameter size, series such as InternVL2.5, mPLUG-Owl3, Qwen2-VL, and several other models achieve accuracy rates exceeding 96%. The performance of these VLMs demonstrates competitiveness with traditional vision classification models. 7 Figure 3: Evaluation examples on SARChat-Bench. VLM predictions are shown in green/red for correct/incorrect descriptions, with the ground truth in green and the predictions in red boxes. And [Human], [Bot], and [Check] denote user input, VLMs response, and standard output, respectively. Summary: We benchmark 16 mainstream VLMs on SARChat-Bench. Model size strongly affects fine-grained description performance but shows little impact on classification. While large models excel in cross-modal and class identify tasks and basic spatial grounding, they struggle with referring, counting, detailed descriptions, and multi-target spatial relationships."
        },
        {
            "title": "5.3 Edge-side models for SAR Applications",
            "content": "This study has multiple edge-side models (5B parameters) trained on SARChat-2M and evaluates their performances. According to Table 3, it demonstrates that these models exhibit task-specific performance variations, achieving remarkable accuracy in cross-modal identification, while showing potential for improvement in referring tasks. These models support domain-specific fine-tuning for rapid task adaptation. After optimization, they can operate efficiently on satellite or ground-edge devices, enabling real-time SAR data processing while reducing dependence on cloud infrastructure and minimizing data transmission costs."
        },
        {
            "title": "5.4 Dialogue Visualizaion",
            "content": "Figure 3 demonstrates representative examples across six tasks from SARChat-Bench, displaying query-response pairs with correct answers marked in green. Examples include successful classification tasks (identifying \"ship\" among multiple options) and accurate instance counting (detecting \"4 instances\" of ships in the satellite image). These results illustrate the models effective performance across various SAR image understanding tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "This study constructs the SARChat-2M, comprehensive dataset containing two million highquality image-text pairs with fine-grained annotations across diverse SAR scenes. To systematically evaluate VLMs in the SAR domain, this work further propose SARChat-Bench, benchmark framework that facilitates the integration of domain-specific knowledge and advances the development of SAR-oriented visual-language models."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Jie Chen, Zhixiang Huang, Runfan Xia, Bocai Wu, Lei Sheng, Long Sun, and Baidong Yao. 2022. Largescale multi-class sar image target detection dataset1.0 [ol]. Journal of Radars, (1). Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Qimin Cheng, Haiyan Huang, Yuan Xu, Yuzhuo Zhou, Huanying Li, and Zhongyuan Wang. 2022. Nwpucaptions dataset and mlca-net for remote sensing image captioning. IEEE Transactions on Geoscience and Remote Sensing, 60:119. Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. 2018. Functional map of the world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 61726180. Wenliang Dai, Junnan Li, Li, AMH Tiong, Zhao, Wang, Li, Fung, and Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint arXiv:2305.06500, 2. Yimian Dai, Minrui Zou, Yuxuan Li, Xiang Li, Kang Ni, and Jian Yang. 2024. Denodet: Attention as deformable multi-subspace feature denoising for arXiv preprint target detection in sar images. arXiv:2406.02833. Alexey Dosovitskiy. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793. Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and Xiang Li. 2023. Rsgpt: remote sensing vision language model and benchmark. arXiv preprint arXiv:2307.15266. Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad Shahbaz Khan. 2024. Geochat: Grounded large vision-language model for remote sensing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2783127840. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324. Xiang Li, Congcong Wen, Yuan Hu, Zhenghang Yuan, and Xiao Xiang Zhu. 2024. Vision-language models in remote sensing: Current progress and future trends. IEEE Geoscience and Remote Sensing Magazine. Xin Lin, Bo Zhang, Fan Wu, Chao Wang, Yali Yang, and Huiqin Chen. 2023. Sived: sar image dataset for vehicle detection based on rotatable bounding box. Remote Sensing, 15(11):2825. Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong Zhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and Jun Zhou. 2024. Remoteclip: vision language foundation model for remote sensing. IEEE Transactions on Geoscience and Remote Sensing. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Yang Long, Gui-Song Xia, Shengyang Li, Wen Yang, Michael Ying Yang, Xiao Xiang Zhu, Liangpei Zhang, and Deren Li. 2021. On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-aid. IEEE Journal of selected topics in applied earth observations and remote sensing, 14:42054230. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. 2024. Deepseek-vl: Towards real-world vision-language understanding. Preprint, arXiv:2403.05525. Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, and Xuelong Li. 2017. Exploring models and data for IEEE remote sensing image caption generation. Transactions on Geoscience and Remote Sensing, 56(4):21832195. Bo Qu, Xuelong Li, Dacheng Tao, and Xiaoqiang Lu. 2016. Deep semantic understanding of high resolution remote sensing image. In 2016 International conference on computer, information and telecommunication systems (Cits), pages 15. IEEE. Alec Radford. 2018. Improving language understanding by generative pre-training. Gencer Sumbul, Marcela Charfuelan, Begüm Demir, and Volker Markl. 2019. Bigearthnet: large-scale benchmark archive for remote sensing image understanding. In IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pages 59015904. IEEE. Chao Wang, Rui Ruan, Zhicheng Zhao, Chenglong Li, and Jin Tang. 2023. Category-oriented localization distillation for sar object detection and unified benchmark. IEEE Transactions on Geoscience and Remote Sensing. 9 Zilun Zhang, Tiancheng Zhao, Yulong Guo, and Jianwei Yin. 2023. Rs5m: large scale vision-language dataset for remote sensing vision-language foundation model. arXiv preprint arXiv:2306.11300. Wang Zhirui, Kang Yuzhuo, Zeng Xuan, WANG Yuelei, ZHANG Ting, and SUN Xian. 2023. Sar-aircraft-1.0: High-resolution sar aircraft detection and recognition dataset. Journal of Radars, 12(4):906922. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Yuanyuan Wang, Chao Wang, Hong Zhang, Yingbo Dong, and Sisi Wei. 2019a. SAR dataset of ship detection for deep learning under complex backgrounds. Remote. Sens., 11(7):765. Yuanyuan Wang, Chao Wang, Hong Zhang, Yingbo Dong, and Sisi Wei. 2019b. sar dataset of ship detection for deep learning under complex backgrounds. remote sensing, 11(7):765. Shunjun Wei, Xiangfeng Zeng, Qizhe Qu, Mou Wang, Hao Su, and Jun Shi. 2020. Hrsid: high-resolution sar images dataset for ship detection and instance segmentation. IEEE Access, 8:120234120254. Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2024. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. 2024. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652. Zhiqiang Yuan, Wenkai Zhang, Kun Fu, Xuan Li, Chubo Deng, Hongqi Wang, and Xian Sun. 2022. Exploring fine-grained multiscale method for crossmodal remote sensing image retrieval. arXiv preprint arXiv:2204.09868. Yang Zhan, Zhitong Xiong, and Yuan Yuan. 2023. Rsvg: Exploring data and models for visual grounding on IEEE Transactions on Georemote sensing data. science and Remote Sensing, 61:113. Tianwen Zhang, Xiaoling Zhang, Jianwei Li, Xiaowo Xu, Baoyou Wang, Xu Zhan, Yanqin Xu, Xiao Ke, Tianjiao Zeng, Hao Su, Israr Ahmad, Dece Pan, Chang Liu, Yue Zhou, Jun Shi, and Shunjun Wei. 2021a. Sar ship detection dataset (ssdd): Official release and comprehensive data analysis. Remote Sensing, 13(18). Tianwen Zhang, Xiaoling Zhang, Jianwei Li, Xiaowo Xu, Baoyou Wang, Xu Zhan, Yanqin Xu, Xiao Ke, Tianjiao Zeng, Hao Su, et al. 2021b. Sar ship detection dataset (ssdd): Official release and comprehensive data analysis. Remote Sensing, 13(18):3690. Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, and Xuerui Mao. 2024. Earthgpt: universal multimodal large language model for multi-sensor image IEEE comprehension in remote sensing domain. Transactions on Geoscience and Remote Sensing."
        },
        {
            "title": "Appendix",
            "content": ".1 Data structure analysis .1.1 Word Frequency Analysis of SARChat-2M As shown in Figure 4 , location words (such as center, middle, top) and target object words (such as ship, aircraft, tank) have the highest occurrence frequencies in SAR image descriptions, and the adjective \"small\" is the most frequently used descriptive word. Figure 4: Cloud Map of Word-frequency Distribution Figure 5: The Proportion Distribution of Samples in the Training Set .1.2 Analysis of Datasets Composition Training Set Category Distribution As shown in Figure 5, in the SARChat-2M training set, the category distribution shows significant differences. Among them, the \"Ship\" category has the largest proportion, reaching 46.98%, followed by the \"Aircraft\" category, with proportion of 20.48%. These two categories account for the majority of the samples in the training set. It can be seen that the sample distribution in the training set is imbalanced, and the \"Ship\" and \"Aircraft\" categories dominate. This may enable the model to learn the features of these two categories more comprehensively during the training process. However, 11 Figure 6: The Proportion Distribution of Samples in the Testing Set since the samples of other categories are relatively few, the models ability to learn and generalize their features may be affected to certain extent. Test Set Category Distribution As shown in Figure 6, in the SARChat-2M test set, the \"Ship\" category has the largest proportion among all categories, reaching 44.38%, and the \"Aircraft\" category ranks second, with proportion of 28.01%. The distribution trends of these two major categories in the test set are similar to those in the training set. This indicates that the test set has certain similarity to the training set in terms of the overall category distribution and can be used to test the models generalization ability on data with similar distribution. However, the slight differences in the proportions also remind us to comprehensively consider various factors when evaluating the models performance. Cross-modal Shape Distribution Analysis As shown in Figure 7, in the cross-modal identification shape distribution, the \"Roughly Square\" shape has the largest proportion, with quantity of 179,652. This shape has an absolute advantage among all shape categories. This means that in the cross-modal identification task, the number of samples of the \"Roughly Square\" shape is much larger than that of other shapes. The model may be more sensitive to this shape and tend to identify the target as the \"Roughly Square\" shape during the recognition process. Therefore, when training and optimizing the model, attention should be paid to improving the recognition ability of other shapes to achieve more balanced recognition effect. Distribution Analysis of Morphological Categories As shown in Table 4, nearly square-shaped Table 4: Analysis of Aspect Ratio of Different Types of Targets Category Dataset Total Samples Mean AR Median AR Std Dev AR Distribution (%) AR0.67 0.67<AR1.5 AR>1. Ship Aircraft Car Tank Bridge Harbor Train Test Train Test Train Test Train Test Train Test Train Test 93,342 10,738 40,698 6,778 9,561 1,230 24,15 1,771 27,615 3,281 3,306 1.34 1.308 1.074 1.08 1.23 1.21 1.10 1.09 1.56 1.568 1.20 1. 1.07 1.026 1.047 1.041 1.08 1.07 1.00 1.00 1.18 1.2 1.01 1. 1.24 1.10 0.32 0.31 0.56 0.53 0.84 0.29 1.92 1.24 0.72 0. 28.37 29.34 5.85 4.56 13.18 12.28 1.58 1.41 18.38 18.01 14.19 15. 39.67 39.82 87.67 87.36 60.07 62.11 94.29 94.36 44.83 44.59 67.93 68. 31.96 30.84 6.48 8.08 26.75 25.61 4.13 4.23 36.79 37.4 17.88 16. Cars and ports maintain moderate distributions with 60-68% nearly square shapes and balanced remaining proportions. .1.3 Analysis of Task based on Datasets"
        },
        {
            "title": "Composition",
            "content": "Our dataset is designed to support unified multimodal tasks through comprehensive task taxonomy, as shown in Table 5. It encompasses six primary tasks: Classification, Fine-Grained Description, Instance Counting, Spatial Grounding, Cross-Modal Identification, and Referring. Among these, the first three tasks are target-quantity independent, while Spatial Grounding, Cross-Modal Identification, and Referring are further categorized into single-object and multi-object variants. This systematic organization enables diverse training scenarios and enhances model generalization capabilities. Table 5: Task type distribution in training and test sets Task Type Instance Counting Spatial Grounding Cross - Modal Identification Referring Fine - Grained Description Classification Train 95493 (5.2%) 94456 (5.1%) 1423548 (77.5%) 95486 (5.2%) 46141 (2.5%) 81788 (4.5%) Test 11794 (5.2%) 11608 (5.1%) 175565 (77.4%) 11703 (5.2%) 6032 (2.7%) 10024 (4.4%) As illustrated in Figure 8, the training set comprises 1,836,912 entries. Cross-Modal Identification dominates with 1,423,548 entries (77.50%), enabling robust cross-modal feature learning. Instance Counting and Referring tasks contain 95,493 (5.20%) and 95,486 (5.20%) entries respectively, while Spatial Grounding accounts for 94,456 entries (5.14%). Fine-Grained Description includes 46,141 entries (2.51%), with its relatively limited Figure 7: Morphological distribution morphology dominates both datasets, accounting for 39.67% in training and 59.37% in test sets, indicating its prevalence in target morphologies. Broad-bodied shapes maintain stable distributions (18.14% training, 17.72% test), while tall-bodied shapes show moderate decrease from training (31.96%) to test (22.91%) sets. This distribution diversity enhances the models generalization capability, though the significant increase in nearly square-shaped samples in the test set demands particular attention during model optimization. Category-Specific Morphological Patterns As shown in Table 4, each category displays distinctive morphological characteristics. Bridges exhibit the highest average aspect ratio (1.56) with balanced distribution across all morphologies (18% broad, 45% square, 37% tall). Ships demonstrate diverse shapes (28% broad, 40% square, 32% tall), reflecting their real-world variability. Tanks and aircraft show highly concentrated distributions, with nearly square shapes dominating at 94% and 87% respectively, facilitating efficient model learning. 12 single-target and multi-target data. The core research motivations are as follows: (1) Evaluation of the Models Target Discrimination Ability the The single-target scenario aims to test models basic recognition ability for independent targets. In contrast, the multi-target scenario focuses on examining the models ability to separate and select targets in complex environments, especially when multiple targets exhibit similar features. This dual-benchmark design can effectively diagnose the performance differences of the model in scenarios with varying degrees of complexity. (2) Revelation of Target Association Understanding Issues In multi-target scenarios, the model usually needs to understand the spatial and semantic relationships between targets. By comparing the performance differences of the model in single-target and multi-target scenarios, it is possible to evaluate whether the model truly understands the descriptions of the positional relationships between targets. This helps to identify the limitations of the model when dealing with relative position descriptions such as \"the vehicle on the left\" and \"the tank in the middle\". (3) Exposure of Attention Mechanism Defects In multi-target scenarios, the model is highly prone to problems such as attention divergence or overlap. When there are multiple similar targets in an image, the model may have difficulty accurately locating the specific target described by the user. Through the comparison between single-target and multi-target scenarios, the deficiencies of the model in attention allocation can be clearly demonstrated. (4) Simulation of Real-world Application Scenarios Real-world applications cover both simple single-target scenarios and complex multi-target environments. The establishment of the dualbenchmark is more in line with real-world usage requirements, providing more comprehensive dimension for model evaluation and helping to improve the applicability and reliability of the model in actual deployments. .3 Task-specific Performance Scoring To evaluate model performance on each task, the task-specific accuacy is caculate by Formula 5. For each task t, the accuracy score am,t of model is computed by averaging the accuracy scores across all subtasks: Figure 8: Train Task Distribution Figure 9: Test Task Distribution data volume potentially affecting model performance. The Classification task contains 81,788 entries (4.45%). The test set (Figure 9) maintains parallel distribution across its 226,636 entries. Cross-Modal Identification remains dominant with 175,565 entries (77.47%), followed by Instance Counting (11,704, 5.16%), Referring (11,703, 5.16%), Spatial Grounding (11,608, 5.12%), Classification (10,024, 4.42%), and Fine-Grained Description (6,032, 2.66%). This consistent distribution ensures reliable model evaluation. The dataset exhibits well-balanced task distribution that supports comprehensive model training. While the consistency between training and test sets ensures reliable evaluation, two aspects warrant attention: the relatively limited data in FineGrained Description tasks may constrain model performance and could benefit from expansion, while the dominant proportion of Cross-Modal Identification data necessitates careful consideration of overfitting during training through appropriate regularization methods. .2 Motivation for the Dual-benchmark Setup In tasks such as spatial localization, cross-modal identification, and referring expression comprehension, this study constructs dual-benchmark of 13 Table 6: Performance comparison across different model sizes 8B 2B 4B 1B Parameters 74.14 72.68 71.52 69.87 Instance Counting Accuracy 61.37 57.54 54.11 50.18 Instance Counting Accuracy (IoU = 0.25) 52.17 47.35 44.22 39.35 Instance Counting Accuracy (IoU = 0.5) 62.25 60.89 60.81 56.30 Spatial Grounding Accuracy Abstract Location in Spatial Grounding Accuracy 81.25 83.33 50.00 0.00 87.91 85.90 81.92 82.24 Spatial Grounding Single Accuracy 98.84 98.01 97.79 96.98 Cross-Modal Identification (Multi) Accuracy 98.98 98.76 98.63 98.60 Cross-Modal Identification (Single) Accuracy 37.49 34.05 27.05 22.13 Referring (Multi) Accuracy (IoU = 0.25) 23.46 18.86 13.91 9.94 Referring (Multi) Accuracy (IoU = 0.5) 74.86 69.92 68.50 62.33 Referring (Single) Accuracy (IoU = 0.25) 60.13 55.29 52.16 44.99 Referring (Single) Accuracy (IoU = 0.5) 63.43 58.84 56.36 53.30 Fine-Grained Description Accuracy 97.25 97.27 96.69 96.65 Classification Accuracy am,t = 1 (cid:88) am,t,i (5) i=1 where am,t is the average accuracy of model on task t, is the number of subtasks, and am,t,i is the accuracy of model on the i-th subtask of task t. This approach ensures that each subtask contributes equally to the overall task score. .4 The Analysis of Model Size Based on the data analysis in Table 6, it can be concluded that for most task-related metrics, there is trend of performance improvement as the model size increases from 1B to 8B. For example, the instance-counting accuracy rises from 69.87% to 74.14%, the spatial-grounding accuracy increases from 56.30% to 62.25%, the fine-grained description accuracy goes up from 53.30% to 63.43%, and the classification accuracy climbs from 96.65% to 97.25%. This indicates that an increase in model size is beneficial to enhancing the performance of these tasks. However, the accuracy of abstract locations in the spatial-grounding task shows unique trend of change. This metric increases from 0.00% for the 1B model to 83.33% for the 4B model, but then decreases to 81.25% for the 8B model, not increasing monotonically with the model size. Evidently, the influence of model size on some specific tasks follows complex patterns. Therefore, when selecting model, it is necessary to comprehensively consider the task type and model size to achieve optimal performance."
        }
    ],
    "affiliations": [
        "China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China",
        "China Mobile Internet Company Ltd., Guangzhou, China",
        "School of Computer Science and Engineering, Northeastern University, Shenyang, China",
        "The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China",
        "The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China"
    ]
}