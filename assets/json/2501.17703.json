{
    "paper_title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
    "authors": [
        "Yubo Wang",
        "Xiang Yue",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models."
        },
        {
            "title": "Start",
            "content": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate Yubo Wang 1 Xiang Yue 2 Wenhu Chen 1 3 https://tiger-ai-lab.github.io/CritiqueFineTuning/ 5 2 0 2 9 ] . [ 1 3 0 7 7 1 . 1 0 5 2 : r Figure 1: Comparison between CFT and SFT on 50K samples from WebInstruct (Yue et al., 2024b). SFT-verified means SFT training on the responses validated by GPT-4o, SFT-GPT4o means SFT training on the responses from GPT-4o. CFT is our approach, which trains on the critique provided by GPT-4o. Abstract Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated In this paresponses for given instructions. per, we challenge this paradigm and propose Critique Fine-Tuning (CFT), strategy where models learn to critique noisy responses rather Inspired by than simply imitate correct ones. human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understandingtraits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct 50K-sample dataset 1Department of Computer Science, University of Waterloo 2Carnegie Mellon University, Pittsburgh 3Vector Insitute, Toronto. Correspondence to: Yubo Wang <y726wang@uwaterloo.ca>, Wenhu Chen <wenhuchen@uwaterloo.ca>. from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields consistent 410% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT modeltrained on just 50K samplesmatches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critiquebased training offers more effective alternative to advance the reasoning of language models. 1 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate 1. Introduction Recently, large language models (LLMs) (Achiam et al., 2023; Team et al., 2023; Dubey et al., 2024) have shown unprecedented performance on tackling real-world problems. One of the core techniques is supervised fine-tuning (SFT), which trains these LLMs to follow natural language instructions (Wei et al., 2022; Ouyang et al., 2022; Sanh et al., 2022). In the process of SFT, LLMs are forced to imitate the annotated responses. Numerous efforts have been made to build high-quality SFT datasets using approaches like Self-Instruct (Wang et al., 2023b) and Evol-Instruct (Xu et al., 2024) to enhance LLMs general instruction-following capabilities. More recently, works such as MAmmoTH (Yue et al., 2024a;b), MetaMath (Yu et al., 2024), and WizardCoder (Luo et al., 2024) have employed SFT to improve the targeted capabilities of LLMs in areas like mathematical reasoning, coding, and more. While these approaches have shown significant gains on weaker base models such as Mistral (Jiang et al., 2023) or LLaMA3 (Dubey et al., 2024), diminishing returns become evident as SFT dataset size and quality scale up. This limitation is particularly pronounced for already-powerful base models (non-SFTed), such as Qwen2.5-base (Yang et al., 2024a), Qwen2.5-Mathbase (Yang et al., 2024b), or DeepSeek-Coder-V2-base (Guo et al., 2024), which have undergone extensive domainadaptive pretraining on reasoning-focused corpora comprising hundreds of billions of tokens. Our experiments in section 3 reveal that applying SFT to these models can even degrade performance without stringent quality control. In this paper, we challenge the prevailing paradigm of SFT and propose new learning framework called Critique FineTuning (CFT). Inspired by human learningwhere critical thinking and constructive feedback are vital for improvementwe shift the focus from simple imitation to critiquebased learning. When humans learn, they do not merely replicate provided answers but analyze, critique, and refine them. Similarly, in CFT, the model learns to provide critiques for noisy responses, identify flaws, suggest improvements, and verify correctness. Formally, CFT involves training the model to critique given query-response pair, maximizing the likelihood (c[x; y]), where is the annotated critique for query-response pair [x; y]. detailed visualization of CFT is presented in Figure 1. To validate CFTs effectiveness, we designed series of experiments. First, we constructed 50K critique dataset from WebInstruct (Yue et al., 2024b), with critiques synthesized by advanced models like GPT-4o (Achiam et al., 2023). We applied CFT to strong 7B base language models (i.e., non-instruction-tuned), such as DeepSeekMath-base (Shao et al., 2024), Qwen2.5 (Yang et al., 2024a), and Qwen2.5Math (Yang et al., 2024b). These models were compared against SFT-trained variants, such as WebInstruct-verified (SFT on WebInstruct responses verified by GPT-4o) and WebInstruct-GPT4o (SFT directly on responses generated by GPT-4o). When evaluated on six math benchmarks, including MATH and AIME24, CFT-trained models can consistently outperform the best SFT-trained models by an average of 410 absolute points. We expanded the evaluation to broader STEM benchmarks, including GPQA (Rein et al., 2023), TheoremQA (Chen et al., 2023), and MMLU-Pro (Wang et al., 2024b). Our results show that the best CFT-trained model, Qwen2.5Math-CFT, trained on 50K examples, outperformed strong competitors like AceMath (Liu et al., 2024) and Qwen2.5Math-Instruct (Yang et al., 2024b), which were trained on over 2M examples. This highlights the efficiency and effectiveness of CFT for reasoning-focused tasks. To better understand different factors of CFT, we conducted comprehensive ablation studies: Robustness to dataset sources: Comparing WebInstruct (Yue et al., 2024b) against MetaMathQA (Yu et al., 2024) and NuminaMath (Li et al., 2024b), we observed that WebInstruct provided slight advantage (3%+) due to its diversity and broader topic coverage. Robustness to noisy response sources: We experimented with both the original noisy responses and responses from Qwen2.5-base critiqued by GPT-4o. The performance differences were negligible. Flexibility to the teacher critique model: Using weaker critique dataset synthesized by GPT-4o-mini instead of GPT-4o, we still observed notable improvements over SFT despite 4% drop on the overall score. Through these experiments, we demonstrated CFTs efficiency and effectiveness over SFT. However, our approach has limitations. Firstly, the critique dataset was entirely synthesized by GPT-4o, with at least 20% of critiques containing errors. Improving the critique dataset quality could further enhance performance. Secondly, CFT-trained models currently lack the ability to perform self-critique, so we have not observed self-improvement effects. Future work will explore these directions further. 2. Method & Dataset To validate the effectiveness of CFT, we constructed several fine-tuning datasets. Most of our experiments are based on WebInstruct (Yue et al., 2024b), an instruction dataset sourced from the Internet with model-based synthetic processing in its pipeline. 2.1. WebInstruct WebInstruct spans wide range of topics, including Mathematics (65%), Physics (8%), Chemistry (4%), Business Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate (10%), Humanities (4%), and more. Unlike other datasets, which are primarily derived from math contests and competitions, WebInstruct offers broader topic coverage. The responses in WebInstruct are extracted and refined by large language models such as Qwen-72B (Bai et al., 2023) and Mixtral (Jiang et al., 2024), making them highly prone to noise due to the lack of verification or quality control. Table 1: The comparison of MAmmoTH3 vs other SFT and RL models including WizardMath (Luo et al., 2023), MathInstruct (Yue et al., 2024a), MetaMathQA (Yu et al., 2024), XWinMath (Li et al., 2024a), OrcaMath (Mitra et al., 2024), NuminaMath (Li et al., 2024b), AceMath (Liu et al., 2024), OpenMathInsstruct-2 (Toshniwal et al., 2024) and Qwen2.5-Math (Yang et al., 2024c). We curated the following subsets from WebInstruct: WebInstruct-SFT: 50K subset directly sampled from the original WebInstruct dataset. This subset has very high error ratio (over 50%). WebInstruct-verified: We adopted samples from WebInstruct and prompt GPT-4o-1120 to judge whether the original answers are correct or not. We retained the top 50K samples as verified SFT data. WebInstruct-GPT-4o: 50K subset that reuses questions from WebInstruct-SFT but replaces the answers with those generated by GPT-4o-1120. WebInstruct-CFT (Ours): 50K subset derived from WebInstruct-SFT, where GPT-4o-1120 provides detailed critiques of the original responses. Approximately 56% of the responses in this subset are judged as correct, while the rest are considered wrong. Despite containing some critique errors introduced by GPT-4o, this dataset is comparable in quality to WebInstruct-GPT-4o. WebInstruct-CFT-Tiny (Ours): smaller version of WebInstruct-CFT, containing only 4K examples, designed for training our 32B model. We compare our CFT datasets with existing SFT datasets in Table 1. As shown, our datasets cover broader range of topics while being significantly smaller in size, highlighting their efficiency in boosting LLMs reasoning abilities. 2.2. MetaMath & NuminaMath In addition to WebInstruct, we synthesized critiques for other datasets, including MetaMathQA (Yu et al., 2024) and NuminaMath (Li et al., 2024b). From each dataset, we randomly sampled 50K examples and used GPT-4o to critique the original responses. We then applied CFT on these datasets to demonstrate the generalizability of our approach across other datasets. 2.3. Training Objective The training objective of our approach is straightforward. We concatenate the question and noisy response as input, and then optimize the model parameters to generate the critique c. Formally, the training loss is: argmaxθ log (c[x; y]; θ) (1) where θ is the parameters of the language model. Dataset Size Source or Seed Discipline Supervised Fine-Tuning Data 96K WizardMath MathInstruct 260K MetaMathQA 395K 1.4M XwinMath 200K OrcaMath 860K GSM8K, MATH, AIME NuminaMath 1.6M GSM8K, MATH, AIME AceMath 14M OpenMath-2 GSM8K, MATH GSM8K, MATH, etc GSM8K, MATH GSM8K, MATH GSM8K GSM8K, MATH Math Math Math Math Math Math Math Math Critique Fine-Tuning Data (Ours) CFT CFT-tiny 50K 4K"
        },
        {
            "title": "STEM\nSTEM",
            "content": "3. Experiments In this section, we will detail our experiments. 3.1. Experimental Setup Evaluation Datasets We evaluate our method on wide range of mathematical reasoning benchmarks. For standard mathematical reasoning, we evaluate on MATH (Hendrycks et al., 2021), Minerva-Math (Lewkowycz et al., 2022) and GSM8K (Cobbe et al., 2021). To assess performance on more challenging competition-level mathematics, we incorporate AIME 2024 from the American Invitational Mathematics Examination, AMC 2023 from American Mathematics Competitions, and OlympiadBench (He et al., 2024) containing various difficulty levels of Mathematical Olympiad problems. We further extend our evaluation to broader STEM reasoning capabilities through TheoremQA (Chen et al., 2023) for mathematical theorem understanding, MMLU-Pro (Wang et al., 2024b) covering physics, chemistry, mathematics, etc., and GPQA (Rein et al., 2023) for complex problems requiring scientific reasoning. Training Details We evaluate three different SFT settings and one CFT setting in our experiments. For SFT, we explore: (1) SFT: directly training on original noisy responses, (2) SFT-verified: training on responses validated by GPT4o, and (3) SFT-gpt4o: training on responses generated by GPT-4o. For CFT, we train the model using our curated CFT datasets as described in section 2. We use MATH500 (Lightman et al., 2023b) as our validation set and select the best-performing checkpoint after training on the entire Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate Table 2: Performance comparison of SFT and CFT on different base models. All the experiments are trained with WebInstruct subset. We select the checkpoint with highest validation score and report their results. Model Method Base DeepSeek-Math-7B Qwen2.5-7B Qwen2.5-Math-7B WebInstruct-SFT WebInstruct-verified-SFT WebInstruct-GPT4o-SFT WebInstruct-CFT = CFTSFTbest Base WebInstruct-SFT WebInstruct-verified-SFT WebInstruct-GPT4o-SFT WebInstruct-CFT = CFTSFTbest Base WebInstruct-SFT WebInstruct-verified-SFT WebInstruct-GPT4o-SFT WebInstruct-CFT = CFTSFTbest MATH Minerva-Math GSM8K OlympiadBench AIME24 AMC23 AVG 33.8 26.3 35.8 31.7 42.2 6.4 49.8 30.8 61.5 45.5 71.1 9. 55.4 59.0 62.0 73.2 79.4 6.2 9.2 12.1 10.7 11.8 12.5 0.4 15.1 6.6 16.2 18.4 27.9 9. 13.6 13.2 12.5 25.7 36.8 11.1 64.3 34.7 67.5 70.9 74.5 3.6 85.4 59.5 70.8 77.4 88.8 11. 91.6 77.4 78.8 90.0 90.9 0.9 4.5 6.2 9.3 8.9 12.4 3.1 26.3 5.8 30.1 19.7 35.7 5. 16.1 19.9 22.1 37.6 41.6 4.0 0.0 0.0 0.0 3.3 3.3 0.0 10.0 3.3 13.3 10.0 13.3 0. 10.0 3.3 16.7 13.3 20.0 3.3 10.0 17.5 7.5 17.5 20.0 2.5 37.5 15.0 37.5 50.0 55.0 5. 40.0 37.5 50.0 62.5 67.5 5.0 20.3 16.1 21.8 24.0 27.5 3.5 37.4 20.2 38.2 36.8 48.6 10. 37.8 35.1 40.4 50.4 56.0 5.7 dataset for 1 epoch. We maintain consistent hyperparameters across all experiments with learning rate of 5e-6, cosine decay learning schedule with warm-up ratio of 0.1 and global batch size of 512. 3.2. Main Results (CFT vs. SFT) To evaluate the effectiveness of CFT, we compare it with various SFT methods on three 7B-scale base models using mathematical reasoning benchmarks. Table 2 presents the comprehensive results across different base models and methods. Our key findings are as follows: Base Model Selection We experiment with three 7Bscale base models: DeepSeek-Math-7B, Qwen2.5-7B, and Qwen2.5-Math-7B. Results show that Qwen2.5-Math-7B serves as stronger foundation, with its base version achieving 37.8% average accuracy across benchmarks. When enhanced with CFT, it achieves the best performance with 56.0% average accuracy. Figure 2: Training dynamics comparison of different methods on Qwen2.5-Math-7B across key mathematical reasoning benchmarks. We compare CFT with two SFT variants: SFT-V (trained on WebInstruct-verified) and SFT-G (trained on WebInstruct-GPT4o). The x-axis represents training steps, and the y-axis shows the accuracy on each benchmark. CFT demonstrates consistently better performance and faster convergence across most tasks. Performance Gains CFT consistently outperforms all SFT baselines across different models: On DeepSeek-Math-7B, it achieves 3.5% absolute improvement over the SFT-GPT4o. On Qwen2.5-7B, it demonstrates substantial 10.4% improvement over the SFT-verified. On Qwen2.5-Math-7B, it surpasses the strong GPT-4o SFT baseline by 5.7% over SFT-GPT4o. Training Dynamics Figure 2 illustrates the training dynamics of different methods on Qwen2.5-Math-7B. Several interesting patterns emerge: On MATH and Minerva-Math, CFT demonstrates faster convergence and reaches significantly higher performance compared to both SFT variants For OlympiadBench, while SFT-G shows competitive Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate early performance, CFT eventually achieves better results On AMC-2023, CFT maintains consistent superior pertiveness of our critique-based approach in learning from much fewer examples without Long-CoT. formance throughout training These results demonstrate that CFT achieves better performance with significantly less training data, making it more efficient and practical approach for developing highperformance mathematical reasoning models. The improvements are particularly notable on challenging benchmarks like MATH (79.4%) and OlympiadBench (41.6%). 3.3. More Results (CFT Models vs. Existing Models) In Table 3, we compare our best CFT-models with other competitive models with different scales. We expanded the evaluation benchmarks to cover broader STEM topics. 7B CFT Models Our Qwen2.5-Math-7B-CFT achieves the highest average performance (48.0%) among 7B-scale models while using significantly less training data (50K samples). Specifically: It substantially outperforms other specialized mathematical models like Deepseek-Math-7B-Instruct (23.9% avg.), Mathstral-7B (32.9% avg.), and NuminaMath-7B-CoT (29.9% avg.). This strong performance is achieved with remarkably less training data - only 50K samples compared to AceMathQwen2.5-Math (2.3M samples) and Qwen2.5-Math-7BInstruct (2.5M samples), demonstrating the superior data efficiency of our approach. Despite being smaller in scale, our Qwen2.5-Math-7B-CFT demonstrates strong performance compared to larger models. With only 7B parameters, it achieves better average performance (48.0%) than Llama-3.1-70B-Instruct (40.4%) and NuminaMath-72B-CoT (39.1%). When compared to Qwen2.5-Math-72B-Instruct (56.4%), our model shows competitive results on several benchmarks (e.g., 67.5% vs 70.0% on AMC23) despite using only one-tenth of the parameters and less training data. While frontier closed models like GPT-4o still maintain performance lead, our results demonstrate that efficient training strategies can help smaller models achieve strong performance with fewer resources. 32B CFT Models We conduct detailed comparison between Qwen2.5-32B-Instruct-CFT and Sky-T1-32BPreview, as shown in Table 4. The results reveal several important aspects: (1) Our most significant advantage lies in data efficiency. (2) The performance comparison across different benchmarks shows consistent advantages: Qwen2.5-32B-Instruct-CFT achieves optimal performance with only 4K training samples, compared to SkyT1-32B-Previews 17K samples. This 4x reduction in training data demonstrates the effec5 Our model achieves 52.5% accuracy on GPQA, surpassing Sky-T1s 49.5%. On TheoremQA, it shows comparable performance to Sky-T1, occasionally exceeding it during training. For AMC23, we observe substantial improvement to 77.5% compared to Sky-T1s 62.5%, maintaining stable performance above 75%. These results demonstrate that CFT not only achieves better performance but also requires significantly less training data, making it more efficient and practical approach for developing high-performance mathematical reasoning models. 3.4. Ablation Studies In order to understand the impact of different factors in CFT, we conduct several ablation studies: Dataset Source We ablate the impact of different training datasets on model performance. As shown in Table 5, when trained with SFT, both MetaMathQA and NuminaMath achieve better performance than WebInstruct (47.3% and 37.5% vs. 35.1% on average), indicating their higher data quality. However, when trained with CFT, WebInstruct surprisingly achieves the best performance (56.0%), outperforming both MetaMathQA and NuminaMath. This suggests that the effectiveness of CFT is not solely determined by the quality of solution data. Instead, by learning to identify and critique incorrect solutions, the model can develop stronger mathematical reasoning capabilities even from imperfect demonstrations, highlighting the robustness and effectiveness of our critique-based learning approach. Response Source We compare two sources of solutions for CFT training: solutions generated by Qwen2.5-Math7B itself and reference solutions from the WebInstruct dataset. Table 6 shows that using reference solutions achieves slightly better performance (56.0% vs. 54.5% on average). This improvement can be attributed to the higher quality of solutions in WebInstruct, which were obtained by refining crawled solutions from quiz websites using more capable models like Mixtral-87B (Jiang et al., 2024) and Qwen-72B (Bai et al., 2023). While these solutions still contain errors that are valuable for CFT training, they generally maintain better solution quality compared to responses directly generated by Qwen2.5-Math-7B, which occasionally produces low-quality responses that could hinder the effectiveness of CFT. The improvement is particularly notable on challenging datasets like Minerva-Math (7.4% increase). This suggests that while incorrect solutions are essential for CFT training, the base quality of these solutions still matters - they should be coherent and well-structured enough to serve as meaningful learning examples. Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate Table 3: Performance comparison of our models vs. other reasoning-specialized models. #Data means the total training set size, but we select the checkpoint with highest validation score. Model #Data MATH GPQA TheoremQA MMLU-Pro OlympiadBench AIME24 AMC23 AVG GPT-4o (2024-08-06) GPT-o1-mini Deepseek-Math-7B-Instruct Mathstral-7B-v0.1 NuminaMath-7B-CoT Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct NuminaMath-72B-CoT Qwen2.5-Math-72B-Instruct Frontier Models 81.1 90.0 51.6 60.0 54.7 57. 74.7 80.3 Other Open-sourced Reasoning LLMs 44.3 56.6 55.2 51.9 65.7 68.0 85.9 31.8 32.2 30.6 30.4 42.2 35.3 49.0 23.7 28.4 28.6 30.3 51.3 24.9 50.3 35.3 42.5 38.6 48.3 62.8 55.0 60. - - - - - - - - - Initialized from Qwen2.5-Math-7B-Base Qwen2.5-Math-Base Eurus-2-SFT rStar-Math@Greedy AceMath-Qwen2.5-Math Qwen2.5-Math-7B-Instruct 0 55.4 230 62.4 747 78.4 2.3 83.1 2.5 83. Qwen2.5-Math-7B-CFT"
        },
        {
            "title": "50 K 79.4",
            "content": "31.0 32.1 - 26.1 31.1 39.4 37.4 38.0 - 24.6 37.0 40.4 39.3 44.2 - 48.1 39.5 47. 43.3 65.3 13.6 21.5 19.9 14.4 14.4 35.0 49.0 16.1 29.8 47.1 42.2 41.6 41.6 9.3 56.7 3.3 6.7 6.7 6.7 16.7 3.3 30. 10.0 3.3 26.7 16.7 16.7 20.0 47.5 95.0 51.7 72.1 15.0 42.4 30.0 30.0 30.0 52.5 70.0 40.0 30.1 47.5 60.0 62. 67.5 23.9 32.9 29.9 30.3 40.4 39.1 56.4 32.7 34.3 - 43.0 44.6 48.0 Table 4: Performance Comparison of 32B Models across Mathematical Reasoning Benchmarks Table 6: Performance comparison between self-generated (by Qwen2.5-Math-7B) and reference solutions (from WebInstruct) for CFT training."
        },
        {
            "title": "Model",
            "content": "#Data GPQA TheoremQA AMC23 Qwen2.5-32B-Instruct - 49.5 Sky-T1-32B-Preview Qwen2.5-32B-Instruct-CFT"
        },
        {
            "title": "17 K 49.5\n4 K 52.5",
            "content": "(CFT - Sky-T1) - 3.0 44.6 48.9 48.1 -0. 72.5 67.5 77.5 10."
        },
        {
            "title": "Base",
            "content": "Self-generated Reference MATH Minerva-Math GSM8K OlympiadBench AIME24 AMC"
        },
        {
            "title": "AVG",
            "content": "55.4 13.6 91.6 16.1 10.0 40.0 37.8 78.2 29.4 92.4 42.5 16.7 67.5 54.5 79.4 36.8 90.9 41.6 20.0 67.5 56. Table 5: Performance comparison of SFT and CFT with different training datasets on Qwen2.5-Math-7B."
        },
        {
            "title": "SFT CFT SFT CFT",
            "content": "MATH 57.5 Minerva-Math 23.9 79.5 GSM8K OlympiadBench 20.0 6.7 AIME24 37.5 AMC"
        },
        {
            "title": "AVG",
            "content": "37.5 74.4 39.3 85.7 36.4 23.3 57.5 52.8 70.8 28.3 88.3 36.3 10.0 50.0 47.3 74.2 30.5 89.1 37.2 23.3 62. 52.8 59.0 13.2 77.4 19.9 3.3 37.5 79.4 36.8 90.9 41.6 20.0 67.5 35.1 56.0 Table 7: Performance comparison of CFT using different teacher critique models on Qwen2.5-Math-7B."
        },
        {
            "title": "Task",
            "content": "SFT-verified GPT-4o-mini GPT-4o-1120 MATH Minerva-Math GSM8K OlympiadBench AIME24 AMC"
        },
        {
            "title": "AVG",
            "content": "62.0 12.5 78.8 22.1 16.7 50.0 40.4 73.9 32.7 84.5 35.1 20.0 62.5 51.5 79.4 36.8 90.9 41.6 20.0 67.5 56. 6 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate Teacher Critique Model To understand the impact of critique model quality on CFT, we compare the performance when using GPT-4o-mini and GPT-4o-1120 as critique models in Table 7. First, we observe that even with relatively modest critique model GPT-4o-mini, CFT significantly outperforms SFT-verified baseline (51.5% vs. 40.4% on average), with substantial improvements on MATH (11.9% increase) and Minerva-Math (20.2% increase). This demonstrates the effectiveness of CFT without requiring an extremely powerful critique model. Furthermore, using stronger critique model GPT-4o-1120 leads to even better performance across all benchmarks (56.0% on average), with notable gains on GSM8K (6.4% increase) and OlympiadBench (6.5% increase). These results confirm that while CFT is effective with modest critique models, stronger critique models can provide more accurate and insightful feedback, leading to better mathematical reasoning capabilities. In the future, we plan to leverage o1 or even o3 as teacher critique model to understand the potential. 4. Limitations 4.1. The Noisy Critique Data Our ablation study on critique models demonstrates that the quality of critique feedback significantly impacts the effectiveness of CFT. To better understand the current limitations, we manually examined 50 randomly sampled critique instances generated by GPT-4o-1120 on WebInstruct. Despite being our best-performing critique data, approximately 20% of these critiques still contain errors or inaccurate feedback (see Appendix A.2 for detailed case studies). These errors can manifest in various forms, such as misidentifying correct steps as incorrect, overlooking crucial mistakes, or providing imprecise mathematical explanations. This finding suggests clear path for improvement: while our current approach shows promising results, the performance of CFT could be further enhanced by using more reliable and verifiable critique data. Future work could focus on developing methods to automatically verify critique accuracy, perhaps through formal mathematical verification tools or by cross-validation with multiple expert models. Additionally, creating curated dataset of human-verified mathematical critiques, though resource-intensive, could provide gold standard for training and evaluation. Such high-quality, verifiable critique data would likely lead to more robust and accurate mathematical reasoning capabilities in language models trained with CFT. Table 8: Performance comparison of different inference methods across various temperature settings. Method Temperature MATH Minerva-Math Direct inference Single-pass self-critique Two-stage self-critique 0.0 0.1 0.3 0.6 0.1 0.3 0.6 0.1 0.3 0.6 79.4 78.8 77.5 75.2 77.2 76.1 73. 77.9 75.8 74.6 36.8 35.9 34.7 33.1 33.7 32.2 31.3 35.2 32.4 31.5 inference. We explored two approaches: 1. Single-pass self-critique: The model solves the problem and critiques its solution in one pass. If errors are detected, it generates new solution. 2. Two-stage self-critique: The model first generates solution, then separately evaluates it. If issues are found, the model iterates this process (up to 8 attempts). Our results show that direct inference at temperature 0.0 achieves the best performance (79.4% on MATH and 36.8% on Minerva-Math). Both self-critique methods show performance degradation as temperature increases. The singlepass approach drops from 77.2% to 73.5% on MATH as temperature increases from 0.1 to 0.6, with similar trends on Minerva-Math. The two-stage approach performs slightly better but still falls short of direct inference. We observe two main issues with self-critique: The model often fails to maintain consistent evaluation standards, either missing real errors or incorrectly flagging correct solutions Higher temperatures, needed to avoid repetitive outputs in iterative attempts, introduce instability in the models reasoning process Based on these findings, our final CFT implementation uses direct inference without self-critique mechanisms. Detailed analysis of self-critique issues can be found in Appendix A.3. 5. Related Work 5.1. Instruction Tuning 4.2. Limitations of Self-Critique We investigated the potential of incorporating self-critique mechanisms into our framework. Table 8 shows that these mechanisms consistently underperform compared to direct Instruction tuning is one of the most crucial part of aligning pre-trained language models with human expectations. The current instruction-tuning datasets are either based on (1) human annotation: such as FLAN (Wei et al., 2022), T0 (Sanh et al., 2022), SuperNI (Wang et al., 2022), Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate which compiles large instruction-tuning datasets from existing human-labeled datasets; and (2) model synthesis: such as Self-Instruct (Wang et al., 2023b), WizardLM (Xu et al., 2024), WildChat (Zhao et al., 2024), which creates instruction-tuning datasets by synthesizing from powerful LLMs (Achiam et al., 2023). Both types of instruction datasets have shown great performance improvement of LMs on general evaluation tasks. More recently, Tulu (Wang et al., 2023a) and Tulu-3 (Lambert et al., 2024) have ablated the how to combine the existing post-training data and algorithms to the push LMs performance to the maximum. et al., 2023a; Yuan et al., 2024) have been explored to enhance LLMs reasoning capabilities. However, these critique models are mostly designed for directly estimating the reward score without intermediate reasoning. The closest to ours is critique-out-loud (Ankner et al.), which only serves as reward model instead of an actor. Our paper is very different from these two notions. We use critique simply as learning objective to push the model to gain deeper understanding of the problem. During inference time, the trained model simply generates the response without involving any critique or refinement process. 5.2. Mathematics Instruction Tuning Taking this further, math-instructed models have been developed to advance LLM performance in the mathematical domain (Luo et al., 2023; Yue et al., 2024a; Yu et al., 2024; Li et al., 2024a; Mitra et al., 2024; Li et al., 2024b). Recently, there has been wave to scale up the math instruction dataset to millions of examples like MAmmoTH2 (Yue et al., 2024b), Open-MathInstruct (Toshniwal et al., 2024), and AceMath (Liu et al., 2024) and Qwen2.5Math-Instruct (Yang et al., 2024b). These methods have shown tremendous performance gains on math reasoning datasets. However, we also observe diminishing marginal gain by further scaling the instruction data up, suggesting that more efficient training algorithm is needed. In this paper, we aim to challenge SFT and propose much more efficient learning algorithm CFT and show similar performance with only 1-10% of SFT data. 5.3. Critique Learning Teaching AIs to critique has been long standing goal in the pursuit of AGI. Self-Correction The concept of self-correction has been emerged as promising direction in LLMs since 2023. There has been line of work (Madaan et al., 2024; Welleck et al., 2023; Shinn et al., 2024; Bai et al., 2022; Ganguli et al., 2023; Gou et al., 2023) aiming to use feedback from the model itself to further improve its performance. However, later work (Huang et al., 2023; Valmeekam et al., 2023) revealed that the self-correction in reasoning is not quite reliable. More recently, with the rise of GPT-o1 (Jaech et al., 2024), LLM self-correction has again demonstrated its potential to improve LLMs own reasoning capabilities. 6. Conclusion In this paper, we introduced Critique Fine-Tuning (CFT), novel paradigm that fundamentally reimagines how language models learn from instruction data. Unlike traditional Supervised Fine-Tuning (SFT) that focuses on response imitation, CFT emphasizes critical thinking by teaching models to critique and analyze responses. Through extensive experiments across multiple base models and benchmarks, we demonstrated that CFT consistently outperforms SFT by 4-10% on mathematical reasoning tasks while requiring significantly fewer training samples (50K vs 2M+). The benefits of CFT extend beyond mathematical reasoning to broader STEM domains, as evidenced by superior performance on benchmarks like GPQA and TheoremQA. Even without traditional instruction tuning, CFT-trained models can effectively follow instructions, challenging the conventional wisdom that SFT is necessary for instruction following. These findings open up new directions for improving language model capabilities, such as combining CFT with other training paradigms, extending it to multi-modal settings, and investigating its theoretical foundations. We believe CFT represents significant step forward in making language model training more efficient and effective, potentially reducing the computational and data requirements for developing high-performing models while improving their reasoning capabilities."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Critique Model The critique model is different from selfcorrection, where specialized model is being used to provide feedback to an existing model to assist the generation process. Reward models are the most popular critique models used in mathematical reasoning. Recently, different outcome reward models (Uesato et al., 2022; Yang et al., 2024b) and process reward models (Wang et al., 2024a; Lightman Ankner, Z., Paul, M., Cui, B., Chang, J. D., and Ammanabrolu, P. Critique-out-loud reward models. In Pluralistic Alignment Workshop at NeurIPS 2024. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 8 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Chen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. Theoremqa: theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 78897901, 2023. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ganguli, D., Askell, A., Schiefer, N., Liao, T. I., Lukoˇsiute, K., Chen, A., Goldie, A., Mirhoseini, A., Olsson, C., Hernandez, D., et al. The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023. Gou, Z., Shao, Z., Gong, Y., Yang, Y., Duan, N., Chen, W., et al. Critic: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2023. Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y., et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2023. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Li, C., Wang, W., Hu, J., Wei, Y., Zheng, N., Hu, H., Zhang, Z., and Peng, H. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024a. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13, 2024b. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023a. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023b. Liu, Z., Chen, Y., Shoeybi, M., Catanzaro, B., and Ping, W. Acemath: Advancing frontier math reasoning with arXiv preprint post-training and reward modeling. arXiv:2412.15084, 2024. Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., and Zhang, D. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. 9 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with evol-instruct. In The Twelfth International Conference on Learning Representations, 2024. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with selffeedback. Advances in Neural Information Processing Systems, 36, 2024. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Valmeekam, K., Marquez, M., and Kambhampati, S. Can large language models really improve by self-critiquing their own plans? In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024a. Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 50855109, 2022. Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K., Wadden, D., MacMillan, K., Smith, N. A., Beltagy, I., et al. How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems, 36:7476474786, 2023a. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, 2023b. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https: //openreview.net/forum?id=y10DM6R2r3. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. Toshniwal, S., Du, W., Moshkov, I., Kisacanin, B., Ayrapetyan, A., and Gitman, I. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. Welleck, S., Lu, X., West, P., Brahman, F., Shen, T., Khashabi, D., and Choi, Y. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023. Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., and Jiang, D. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Confer10 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate ence on Learning Representations, 2024. URL https: //openreview.net/forum?id=CfXh93NDgH. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024b. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024c. Yu, L., Jiang, W., Shi, H., Jincheng, Y., Liu, Z., Zhang, Y., Kwok, J., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. Yuan, L., Li, W., Chen, H., Cui, G., Ding, N., Zhang, K., Zhou, B., Liu, Z., and Peng, H. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Yue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mammoth: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024a. Yue, X., Zheng, T., Zhang, G., and Chen, W. MAmmoTH2: Scaling instructions from the web. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/ forum?id=yVu5dnPlqA. Zhao, W., Ren, X., Hessel, J., Cardie, C., Choi, Y., and Deng, Y. Wildchat: 1m chatgpt interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2024. 11 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate A. Appendix A.1. Inference Method Prompts We present the prompt templates used in our different inference approaches: A.1.1. DIRECT INFERENCE TEMPLATE Please reason step by step, and put your final answer within boxed{}. Question: [Problem text here] Answer: Lets solve this step by step: [Solution steps] Therefore, the final answer is boxed{ANSWER}. A.1.2. SINGLE-PASS SELF-CRITIQUE TEMPLATE Please reason step by step to solve this problem and then critique your solution. If any errors are found, provide corrected solution. Please put your final answer within boxed{}. Question: [Problem text here] Answer: Lets solve this first: [Initial solution steps] Therefore, my initial answer is boxed{ANSWER}. Critique: [Critique points] [If errors found: Based on my critique, let me provide corrected solution: Corrected solution: ...] A.1.3. TWO-STAGE SELF-CRITIQUE TEMPLATE Stage 1 (Solution Generation): Please reason step by step, and put your final answer within boxed{}. Question: [Problem text here] Answer: Lets solve this step by step: [Solution steps] Therefore, the final answer is boxed{ANSWER}. Stage 2 (Critique): Please critique whether the following solution to the question is correct. Question: [Problem text here] Solution: [Previous solution] Critique: 1. [Critique point 1] 2. [Critique point 2] 12 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate ... Critique Conclusion: Correct/Incorrect If the conclusion is Incorrect, the process returns to Stage 1 for new solution attempt. This iterative process continues until either: The critique conclusion becomes Correct, indicating satisfactory solution has been found, or The maximum number of iterations (8) is reached, in which case the last generated solution is used as the final answer. The complete process can be represented as: For in range(1, 9): # Maximum 8 iterations 1. Generate solution (Stage 1) 2. Critique solution (Stage 2) 3. If critique conclusion is \"Correct\":"
        },
        {
            "title": "Return current solution and exit",
            "content": "4. If == 8:"
        },
        {
            "title": "Return current solution",
            "content": "5. Continue to next iteration A.2. Case Study: Noisy Critique Data Here we present examples of noisy critique data and analyze their impact on model performance: A.2.1. CASE 1: INCORRECT CRITIQUE OF VALID SOLUTION Problem: What is flawed in the counter-proof attempt against the statement that two randomly chosen points on the surface of an N-dimensional unit sphere with large are nearly orthogonal? The counter-proof involves selecting random arc with length in the interval (0, u03c0) and placing it on the spheres surface at random orientation, claiming that the ends of the arc dont have high probability of being nearly orthogonal. Original Solution: The flaw in the counter-proof attempt lies in the misunderstanding of the uniform distribution of randomly selected points on the spheres surface. The original statement assumes that the points are chosen independently and uniformly, whereas the counter-proof proposes specific distribution by creating random arc. This distribution does not represent the uniform selection of two points on the sphere.nnThe Bertrand Paradox, related concept in probability, demonstrates that different methods of selecting random chord in circle can lead to different probabilities for the chord being longer than half the circles radius. Similarly, in the N-dimensional case, the probability of orthogonality depends on the method of random selection, and the arc method does not refute the original statement about two uniformly chosen points.nnTo address the issue, one should focus on the probability of two points being nearly orthogonal when they are each chosen independently and uniformly from the spheres surface, which is the basis of the original statement. Incorrect Critique: 13 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate Your explanation is on the right track, but it could be clarified further. The key issue with the counter-proof is that it introduces biased method of selecting points on the sphere. By constructing random arc and using its endpoints, the counter-proof imposes specific correlation between the two points. This violates the assumption of independence and uniformity in the original statement. nnIn high-dimensional spaces (large ( )), the geometry of the sphere leads to the phenomenon where two points chosen independently and uniformly are almost always nearly orthogonal. This is result of the concentration of measure phenomenon, which is well-established property in high-dimensional geometry. The counter-proofs method of selecting points via an arc does not reflect this uniform and independent selection process, and thus cannot serve as valid refutation of the original statement.nnConclusion: wrong [END] Analysis: The critique is overly harsh, as the original solution already provides clear and sufficient answer to the core question. A.2.2. CASE 2: MISSED ERRORS IN SOLUTION Problem: Find the roots of the following polynomial: $frac{43 xˆ2}{5}+5 x+frac{28}{5}$. Original Solution: To find the roots of the polynomial, we can use the quadratic formula:nn$$x = frac{-b pm sqrt{bˆ2 - 4ac}}{2a}$$nnwhere a, b, and are the coefficients of the polynomial.nnIn this case, we have:nn$$a = frac{43}{5}, = 5, = frac{28}{5}$$nnSubstituting these values into the quadratic formula, we get: $$x = frac{-5 pm sqrt{5ˆ2 - 4left(frac{43}{5}right) left(frac{28}{5}right)}}{2left(frac{43}{5}right)}$$nnSimplifying this expression, we get:nn$$x = frac{-5 pm sqrt{25 - frac{43 cdot 28}{5}}}{2left(frac{43}{5}right)}$$nn$$x = frac{-5 pm sqrt{25 - 238}}{2left(frac{43}{5}right)}$$nn$$x = frac{-5 pm sqrt{-213}}{2left(frac{43}{5}right)}$$nn$$x = frac{-5 pm sqrt{213}}{2left(frac{43}{5}right)}$$nn$$x = frac{-5 pm sqrt{4191}}{86}$$nnTherefore, the roots of the polynomial are $x=frac{-5 pm sqrt{4191}}{86}$.n####nThe roots of the polynomial are $x=frac{-5 pm sqrt{4191}}{86}$. Inadequate Critique: Lets verify this step by step: The quadratic formula $frac{-b pm sqrt{bˆ2 - 4ac}}{2a}$ was correctly applied For the given equation $frac{43xˆ2}{5} + 5x + frac{28}{5}$: $a = frac{43}{5}$ $b = 5$ $c = frac{28}{5}$ When we substitute: $bˆ2 = 25$ 14 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate $4ac = 4(frac{43}{5})(frac{28}{5}) = 238$ $bˆ2 - 4ac = 25 - 238 = -213$ Therefore: $x = frac{-5 pm isqrt{213}}{2(frac{43}{5})} = frac{-5 pm isqrt{4191}}{86}$ The denominator 86 comes from $2 cdot frac{43}{5} cdot frac{5}{5} = 86$, and $sqrt{213} cdot sqrt{19.67} approx sqrt{4191}$ Thus, the answer $x=frac{-5 pm sqrt{4191}}{86}$ must be correct. Crituque Conclusion: correct [END] Analysis: The critical calculation error occurs in step 4: The transformation from $frac{-5 pm isqrt{213}}{2(frac{43}{5})}$ to $frac{-5 pm isqrt{4191}}{86}$ is incorrect. 1) While the denominators 5 was properly handled ($2 cdot frac{43}{5} = frac{86}{5}$) 2) It ignored that this coefficient 5 must affect the numerator as well The correct simplification should be: $frac{-5 pm isqrt{213}}{2(frac{43}{5})} = frac{-25 pm 5isqrt{213}}{86}$. A.3. Observed Issues in Self-Critique Methods Our analysis revealed several systematic issues with self-critique approaches: A.3.1. INCONSISTENT SELF-EVALUATION We observed the following patterns of inconsistency in self-evaluation: Over-lenient Evaluation: Cases where the model failed to identify clear logical errors in its own reasoning False Negatives: Instances where correct solutions were incorrectly flagged as erroneous A.3.2. SPECIFIC FAILURE CASES Here we document specific examples where self-critique methods failed: Example 1: Over-lenient Evaluation Problem: Chandra has four bowls. Each one is different color (red, blue, yellow, green). She also has exactly one glass the same color as each bowl. If she chooses bowl and glass from the cupboard, how many pairings are possible? One such pairing is blue bowl and yellow glass. Original Solution: To solve this, we need to determine the total number of possible pairings of one bowl and one glass when there are four bowls and four glasses, each uniquely paired by color.nn### Reasoning:n1. Chandra has **4 bowls** (red, blue, yellow, green) and **4 glasses** (red, blue, yellow, green).n2. Each bowl matches exactly one glass by color. For example:n - The red bowl matches with the red glassn - The blue bowl matches with the blue glassn - The yellow bowl matches with the yellow glassn - The green bowl matches with the green 15 Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate glassn3. Since each bowl must pair with its corresponding glass, there is only **1 valid pairing** (a red bowl with red glass, etc.).nn Self-Critique: The question asks for the number of possible pairings, but based on the problem setup, only one valid pairing exists. The reasoning above is correct.nn**Conclusion: Correct [END]** Example 2: False Error Detection Problem: Chandra has four bowls. Each one is different color (red, blue, yellow, green). She also has exactly one glass the same color as each bowl. If she chooses bowl and glass from the cupboard, how many pairings are possible? One such pairing is blue bowl and yellow glass. Original Solution: To solve this problem, we need to calculate the total number of possible pairings of bowls and glasses. Each bowl can be paired with any of the 4 glasses.nnHere are the steps:nn1. Chandra has 4 bowls and 4 glasses.n2. Each bowl can be paired with any one of the 4 glasses.n3. Therefore, the total number of pairings is calculated as the product of the number of bowls and the number of glasses:n [n 4 times 4 = 16n ]nn Self-Critique: The reasoning provided is incorrect. While the calculation of the total number of pairings as ( 4 times 4 = 16 ) is mathematically correct, the explanation is flawed. The problem does not state that each bowl must be paired with glass of different color. Therefore, the assumption that each bowl can only be paired with glass of the same color is invalid. nnConclusion: **Critique Conclusion: Incorrect**"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University, Pittsburgh",
        "Department of Computer Science, University of Waterloo",
        "Vector Institute, Toronto"
    ]
}