{
    "paper_title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning",
    "authors": [
        "Huatong Song",
        "Jinhao Jiang",
        "Wenqing Tian",
        "Zhipeng Chen",
        "Yuhuan Wu",
        "Jiahao Zhao",
        "Yingqian Min",
        "Wayne Xin Zhao",
        "Lei Fang",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 0 0 7 1 . 5 0 5 2 : r R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning Huatong Song1*, Jinhao Jiang1*, Wenqing Tian3, Zhipeng Chen1, Yuhuan Wu1, Jiahao Zhao1, Yingqian Min1, Wayne Xin Zhao1, Lei Fang2, Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China. 2DataCanvas Alaya NeW. 3Beijing Institute of Technology. {songhuatong123, jiangjinhao}@ruc.edu.cn, batmanfly@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the models internal knowledge. In this paper, we introduce R1-Searcher++, novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs two-stage training strategy: an initial SFT Coldstart phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates reward mechanism for internal knowledge utilization, and integrates memorization mechanism to continuously assimilate retrieved information, thereby enriching the models internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/ R1-Searcher-plus."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Zhao et al., 2023) have demonstrated remarkable reasoning capabilities by only leveraging the information encoded in their parameters. However, their reliance on static, internal knowledge leads to notable limitations. At the simultaneously, this reliance easily leads to hallucinations (Huang et al., 2025), so LLMs may struggle with open-ended tasks (Wang et al., 2025c; Trivedi et al., 2022). Therefore, it is crucial to enable LLMs to access external information when * Equal contributions. Corresponding author. they are confused during the reasoning process to achieve more deliberative reasoning (Jiang et al., 2024a). To address this issue, extensive research has focused on augmenting LLMs with external information sources (i.e., RAG (Gao et al., 2024)). Early approaches emphasize specific prompting strategies to guide LLMs (Li et al., 2025; Teng et al., 2025) and subsequent studies investigate to distill this capability into smaller LLMs through supervised fine-tuning (SFT) (Wang et al., 2025b). However, recent findings suggest that SFT-based distillation can cause models to memorize solution paths, limiting their generalization to novel scenarios (Chu et al., 2025). Further proposals include test-time scaling method (Li et al., 2024), notably employing the Monte Carlo Tree Search (MCTS) framework (Sun et al., 2025) to enhance solutionfinding by expanding the search space during inference, but this approach incurs significant inference overhead, reducing its practicality for widespread use. Recent studies employ end-to-end outcomebased reinforcement learning (RL) to train models, enabling them to autonomously explore external retrieval environments during inference (Jin et al., 2025; Song et al., 2025). This approach fosters the development of self-directed retrieval capabilities in LLMs as they reason. However, such models often become overly reliant on external search engines after training, neglecting the utilization of their internal knowledge. In practice, when humans attempt to solve factual questions, they first recall their internal knowledge, and only turn to search engines when they recognize lack of information. At the same time, after obtaining the external searched information, humans would memorize this knowledge for future use. For LLMs, extensive pretraining on large-scale data has already endowed them with substantial internal knowledge (Qwen et al., 2025). Therefore, it is essential to equip models with the ability to dynamically switch between internal and external knowledge sources as needed. Furthermore, models should be encouraged to effectively memorize useful information encountered during training (Jiang et al., 2024b), progressively enriching their internal knowledge and continuously evolving toward greater intelligence. In this paper, we present R1-Searcher++, novel framework designed to teach LLMs to adaptively leverage both internal and external knowledge. We adopt two-stage training strategy: SFT Cold-start and RL for Dynamic Knowledge Acquisition. In the first phase, we employ reject sampling to collect data that meets the format requirements and perform cold start with SFT. In the second stage, we further train the model using outcomebased RL to guide the model in dynamically acquiring knowledge, which is to encourage reliance on internal knowledge when confident, and invoke external search mechanisms when uncertain, based on carefully designed reward design. Additionally, we further introduce memory mechanism, enabling the model to retain knowledge encountered during training by converting and memorizing retrieved content. This mechanism continuously enriches its internal knowledge, empowering it to effectively balance internal reasoning and external retrieval through autonomous exploration and timely memorization. To verify the effectiveness of R1-Searcher++, we conduct extensive experiments based on Qwen2.5-7B-Instruct. Notably, our method surpasses the strong baseline by up to 4.3% and reduces the retrieval count by 42.9% compared to vanilla RLbased approaches. Our principal contributions are as follows: We introduce R1-Searcher++, teaching LLMs to adaptively leverage both internal and external knowledge through two-stage training strategy. We encourage the model to actively leverage its internal knowledge while efficiently memorizing external information, enabling dynamic knowledge acquisition through exploration and memorization. show that R1experiments Searcher++ outperforms existing RAG methods, while significantly reducing the number of retrievals compared to vanilla RL-based approaches. Extensive"
        },
        {
            "title": "2 Related Work",
            "content": "Retrieval-Augmented Generation. To improve the factual accuracy of LLM inference and reduce hallucinations, researchers have proposed enhancing language models by incorporating external information sources, paradigm known as RAG (Fan et al., 2024). Early RAG approaches primarily include Branching (Kim et al., 2024), Summarization (Li et al., 2023), and Adaptive Retrieval (Jeong et al., 2024) strategies. As foundation models have become increasingly capable, exhibiting strong CoT reasoning abilities, many studies have combined RAG with CoT. These efforts include methods that prompt the model to perform step-by-step retrieval (Shao et al., 2023; Trivedi et al., 2023) and strategies that distill such capabilities into smaller LLMs (Asai et al., 2024). In parallel, several works have explored test-time scaling, notably using MCTS (Feng et al., 2025) to dynamically expand reasoning paths. However, such approaches often incur substantial inference-time overhead. More recently, researchers have trained models using outcomesupervision RL (Zheng et al., 2025) to encourage the exploration of more effective actions and retrieval behaviors, but it leads models to over-rely on external search engines, diminishing their ability to leverage internal knowledge (Wang et al., 2025a). Enabling LLMs to effectively integrate and alternate between internal knowledge and external retrieval remains significant challenge. Reinforcement Learning. To improve training efficiency, several off-policy algorithms have since been proposed (Rafailov et al., 2024; Ethayarajh et al., 2024); however, these methods still face limitations in terms of preference modeling accuracy and generalization capability (Pang et al., 2024). DeepseekMath introduced the GRPO algorithm (Shao et al., 2024), which enables efficient self-exploration through mechanism of relative preference optimization. Building on this, Deepseek-R1 (DeepSeek-AI et al., 2025) have demonstrated that outcome-based RL can significantly enhance the reasoning abilities of large models. More recently, studies have begun to investigate RL algorithms specifically designed to improve LLM reasoning capabilities (Yu et al.; Yuan et al., 2025). In parallel, other research efforts have applied RL to the retrieval domain, aiming to enable deep search capabilities (Chen et al., 2025). However, the use of RL that combine LLM-driven retrieval and reasoning remains largely simplistic and underexplored."
        },
        {
            "title": "3 Preliminary",
            "content": "To enhance the performance of LLMs in opendomain multi-hop question answering tasks (Ho et al., 2020), in this work, we focus on enabling the model to autonomously decide when to use its internal knowledge or to invoke an external retriever to answer the given questions with the LLM selfimproving paradigm, which can improve both reasoning effectiveness and efficiency. To this end, we introduce three special tokens to format the LLM reasoning process, i.e., <internal>, <external>, and <document>. Concretely, during the reasoning process, the LLM with parameters θ determines whether the current step requires external knowledge to help perform reasoning. If so, it triggers the <external> to issue queryt, which is sent to retriever to retrieve the top-K relevant documents Dt = {dt,k}K k=1 from an external corpus. These retrieved documents are incorporated into the reasoning path with another special token <document>. Otherwise, the model directly generates the related internal knowledge enclosed in <internal>. After several reasoning steps, the LLM obtains the final answer and stops the reasoning process. Since our approach is orthogonal to the RL algorithm, we conduct the experiments based i.e., REINon widely used RL algorithm, FORCE++ (Hu, 2025), which is stable RL algorithm without the critic model. To better accommodate the retrieval scenario, we mask the retrieved documents during the loss calculation process, as they serve as environmental observations rather than model-generated content. Formally, for each question q, we first samples group of outputs {o1, o2, , oG} from the old policy model πθold. Next, we incorporate the KL regularization into the reward scores Rϕ(q, oi,t) , and then normalize the advantage scores: i,t = R(q, oi) β (cid:80)T ˆA i=t KL(i), ˆAi,t = ˆA i,tmean( ˆA) std( ˆA) We utilize ˆA to denote the set of all advantages in the global batch that contains ˆAi,t. After obtaining the advantage scores, we set the mask value (i, t) as 0 if this token belongs to an external document, otherwise we set (i, t) = 1. Finally, we employ the masks (i, t) in the objective function to remove the influence of retrieved documents: ˆPi,t = min (cid:104) pi,t ˆAi,t, clip (pi,t, 1 ε, 1 + ε) ˆAi,t (cid:105) , JMask(θ) = 1 (cid:80)G i=1 1 t=1 (i,t) (cid:80)oi (cid:80)oi t=1 (i, t) ˆPi,t (1) where ε is hyper-parameter and p(i, t) is the important sampling coefficient , and πθ is the policy model."
        },
        {
            "title": "4 Methodology",
            "content": "In this part, we introduce the R1-Searcher++ framework, which aims to teach LLM to adaptively utilize internal and external knowedge through two critical stages, i.e., SFT Cold-Start (Section 4.1) and RL for Dynamic Knowledge Acquisition (Section 4.2). Concretely, in the first stage, we utilize the curated data to perform SFT on the model, to standardize its responses in specific format and enable it to leverage external retrievers and internal knowledge adaptively. In the second stage, we employ RL on LLM that encourages the model to explore more effective actions and behaviours, and further incorporate the internal knowledge utilization encouragement and external knowledge memorization in the training process, to guide the model to dynamically acquire knowledge and continuously enrich its internal knowledge, which can lead to higher reasoning efficiency."
        },
        {
            "title": "4.1 SFT Cold-start",
            "content": "To equip LLMs with the preliminary ability to autonomously perform external retrieval during inference while effectively leveraging internal knowledge, we synthesize high-quality training instances using rejection sampling, without relying on other powerful models.We only keep the correct responses with the appropriate occurrences of both the <internal> and <external> tags, teaching LLM to perform dynamic knowledge acquisition in proper format. Specifically, given the question and the synthesized output y, once the i-th token of the output belongs to the external document, it will be masked, i.e., Mi = 0. Otherwise, the coefficient Mi will be set as 1, incorporating the probability of yi into the objective function as follows, LSFT = 1 j=1 Mj (cid:80)n (cid:88) i=1 Mi (yix, y<i) (2)"
        },
        {
            "title": "4.2 RL for Dynamic Knowledge Acquisition",
            "content": "After cold-starting, we obtain model that can utilize internal knowledge and perform an external search with correct format. To further enhance its capabilities, i.e., to perform effective and efficient reasoning, we continually train the model Figure 1: Overall framework of our proposed R1-Searcher++ approach. through the RL process, which includes mechanism that encourages internal knowledge utilization (Section 4.2.1) and mechanism for converting and memorizing external knowledge (Section 4.2.2). 4.2."
        },
        {
            "title": "Internal Knowledge Utilization\nEncouragement",
            "content": "In the RL process, the reward function is utilized to provide the supervision signals, which can adjust and optimize the behaviours of the model (Hu, 2025). Therefore, given the question and the i-th generated response oi, we design the format reward and answer reward to induce the model to perform reasoning correctly with the expected format, and incorporate the group reward into the final reward function to mitigate the over-reliance on the external retriever. Now, we introduce the details of the reward function in the following. Format reward. We impose strict formatting constraint to ensure model responses are consistent and clear. During the reasoning process, when calling the external retriever, the model is required to formulate query and enclose it within the <external>...</external> tags, and is prohibited from generating document content directly without first invoking retrieval. When the reasoning process finishes, the final response must satisfy the following criteria, i.e., the final answer must be enclosed within boxed{}, and the content should not contain any garbled or unreadable content. Once the model behaviours satisfy the above requirements, we set the format reward Rformat as 0, while we set the reward as 2 if any requirement fails, as shown in the following, Rformat(q, oi) = (cid:40)"
        },
        {
            "title": "The format of oi is correct",
            "content": "0, -2, The format of oi is incorrect (3) Answer reward. To indicate the correctness of the final answer, we leverage the Cover Exact Match (CEM) metric to calculate the answer reward, adapting to the group reward discussed in the following and relieving the issue of EM being too strict. Concretely, CEM is True if the ground truth answer appears in the predicted answer ai extracted from the response oi, and False for other situations. However, we observe that LLM can easily hack the CEM metric during the RL process, where LLM is likely to generate longer predicted answer that will receive higher probability to cover the ground truth answer, causing the CEM to be falsely high. Therefore, we regard the answer exceeding 10 words as an incorrect answer, requiring LLM to generate the answer within ten words, which can alleviate the above reward hacking issue. In summary, the answer reward Ranswer can be computed as follows, Ranswer(q, oi) = (cid:40) 1, ai within 10 words CEM=True 0, Otherwise (4) Group reward. Building upon the first two rewards for LLM reasoning effectiveness, the group reward is designed to encourage the model to reduce its reliance on external retrieval, increasing the reasoning efficiency. Considering that the variance of the external retriever calling times by LLM reflects the necessity of performing external retrieval, group reward is calculated by the standard deviation of the number of calls to the retriever in correct responses to the same question. Formally, given the question and set of generated responses {o1, o2, . . . , on}, we first count the number of calls to the retriever ti of each response, and then calculate the standard deviation σ of {t1, t2, . . . , tn}. Next, we calculate the minimum number of calls to the retriever of the correct responses, i.e., tmin = min{ti Ranswer(q, oi) = 1}. group(q, oi) = (cid:40) 2 σ2, Ranswer(q, oi) = 1 ti = tmin 0,"
        },
        {
            "title": "Otherwise",
            "content": "(5) Meanwhile, to maintain training stability and prevent excessive variance, we introduce hyperparameter η to clip the corresponding factor. The final computation of reward is formulated as follows, Rgroup(q, oi) = min (cid:16) group(q, oi), η (cid:17) (6) Finally, the reward R(q, oi) utilized to calculate the advantage in Equation 7 is defined as the sum of the three sub-rewards mentioned above: R(q, oi) = Rformat(q, oi) + Ranswer(q, oi) + Rgroup(q, oi) (7)"
        },
        {
            "title": "4.2.2 External Knowledge Memorization\nThe standard RL training paradigm relies on the\nmodel’s self-exploration and the feedback from the\nexternal environment. In retrieval-based scenarios,\nsince the knowledge retrieved by the retriever is en-\ntirely correct, the model should like a human, aim\nto memorize this information during training, trans-\nforming it into internal knowledge. This enables\nthe model to utilize the acquired knowledge di-\nrectly in future instances without repeated retrieval,\nthereby achieving efficient reuse of retrieved infor-\nmation. Thus, we incorporate external knowledge\nmemorization by rewriting the retrieved informa-\ntion to align with the model’s interanl knowledge\nutilization pattern, enabling the model to internal-\nize them effectively.",
            "content": "To obtain the rewritten instances, at the beginning of the RL process, we fine-tune separate model on the data filtered in Section 4.1 as the rewritting model, which can solve the questions based on the pre-processed documents that do not call the retriever. During the RL process, we select the correct responses generated by LLM, and then extract the retrieved documents from the responses. Given the question and the extracted documents in the context, the rewriting model can generate the reasoning paths without calling the external retriever. After validating the correctness of these reasoning paths, we select the correct instances to construct the dataset for memorization and internalization. In conclusion, the corresponding loss for memorization is computed as follows: LM(θ) = (cid:80) 1 oiT oi (cid:88) oi (cid:88) oiT t=1 log πθ(oi,tq, oi,<t) (8) To avoid the LM(θ) from dominating the policy models training and causing the model to ignore external retrieval, we weight it with pre-defined coefficient µ. The final loss used to optimize the policy model during the retrieval scenario RL process is computed as follows: L(θ) = JMask(θ) + µ LM(θ) (9) Thus, during training, the model not only engages in self-exploration but also continuously enriches its internal knowledge, enabling it to become increasingly smarter over time."
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "Datasets and Evaluation Metrics. We evaluate using four multi-hop datasets: HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), Musique (Trivedi et al., 2022), and Bamboogle (Press et al., 2023). HotpotQA and 2WikiMultiHopQA are in-domain benchmarks since parts of their training sets are used for training. In contrast, Musique and Bamboogle serve as outof-domain benchmarks to assess our models generalization capabilities. We randomly select 500 samples from the entire validation sets of HotpotQA, 2WikiMultiHopQA, and Musique, and use the entire test set of Bamboogle to form our final test set. For evaluation metrics, we utilize F1-score and LLM-as-Judge (LasJ). considering that the answers to open-ended multi-hop questions are not uniform in form. The F1-score measures the wordlevel similarity between the predicted answer and the reference answer while LLM-as-Judge employs GPT-4o-mini to assess the correctness of prediction. The evaluation prompt for LasJ is provided in Appendix B. Baselines. We compare R1-Searcher++ against several baselines. Naive Generation generates answers directly without retrieval. Standard RAG represents traditional RAG systems that retrieve documents directly based on the question. SuRe (Kim et al., 2024) executes multiple reasoning paths in parallel for single query. Selective-Context (Li et al., 2023) compresses retrieved documents to reduce context length. Adaptive-RAG (Jeong et al., 2024) dynamically selects retrieval strategies depending on the complexity of the query. CRPlanner (Li et al., 2024) scales RAG at inference time using MCTS. RAG-CoT methods, such as Iter-RetGen (Trivedi et al., 2023), IRCoT (Shao et al., 2023), and Search-o1 (Li et al., 2025), which combine RAG with CoT using prompts. RAG-RL methods like R1-Searcher (Song et al., 2025) and Search-R1 (Jin et al., 2025) leverage RL to enable the model to learn to autonomously perform retrieval during inference. Implementation Details R1-Searcher++ and all baseline models are either trained or prompted using the Qwen-2.5-7B-Instruct as the backbone, and evaluated with FlashRAG (Jin et al., 2024) using local dense retrieval corpus. The retrieval corpus comprises the English Wikipedia as provided by KILT (Petroni et al., 2021) in 2019, segmented into 100-word passages with appended titles, totaling 29 million passages. We employ BGE-large-en-v1.5 as the text retriever. Detailed training settings for R1-Searcher++ are provided in Appendix A."
        },
        {
            "title": "5.2 Main Results",
            "content": "Table1 shows the results of R1-Searcher++ and the baselines on four mutil-step benchmarks. We can obtain the following observations: Achieving Significant Performance Improvement on Multi-Hop QA. Our method, R1Searcher++, achieves significant performance improvements over all mutil-hop QA benchmarks under the LLM-as-Judge evaluation metric, including both tree search-based and RL-based approaches. Specifically, R1-Searcher++ outperforms CR-Planner by 25.7% and surpasses the best vanilla RL-based method R1-Searcher by 4.3% on the overall test set. These results demonstrate that our approach effectively enables the model to perform accurate and timely retrieval invocations throughout the reasoning process, thereby enhancing overall performance. Balancing the Utilization of Internal and External Knowledge. While maintaining strong performance on the evaluation datasets, our method achieves significant reduction in retrieval count compared to vanilla RL-based RAG approaches. Specifically, the average retrieval count is reduced by 30.0% and 52.9% compared to R1-Searcher and Search-R1, respectively. This observation suggests potential conflict between external information and the internal knowledge of LLMs and one possible reason is that directly injecting retrieved documents into the reasoning process may introduce noise. This demonstrates that the model should learn to make full use of its internal knowledge and only invoke the retriever when necessary. Maintaining Generalization Ability. Despite being trained on only 9000 samples, the model achieves strong performance on in-domain datasets and further exhibits impressive generalization to out-of-domain datasets. This suggests that the model effectively learns to retrieve relevant documents and leverage internal knowledge, integrating both with reasoning through exploration during training. This enables robust performance on new test datasets that require retrieval. Furthermore, it can also seamlessly generalizes to online search, as detailed in Section 6.2."
        },
        {
            "title": "6.1 Ablation Study",
            "content": "To validate the effectiveness of our proposed R1Searcher++ framework, we conduct comprehensive ablation analysis of its key design elements. We design five distinct variants: (1) w/o Stage-1 removes the initial SFT cold start stage; (2) w/o Stage-2 removes the entire RL training stage; (3) w/o Rgroup removes the group reward in the RL stage; (4) w/o LM removes the external knowledge memorization mechanism in the RL stage and (5) w/o Rgroup and LM removes both the group reward and the external knowledge memorization mechanism. The performance of these variants is presented in Table 2. As observed, all ablated variants exhibit decline in performance compared to our full method, underscoring the integral contribution of each component. Specifically, w/o Stage-1 leads to degradation in performance along with an increase in retrieval count. Meanwhile, the performance of w/o Stage-2 drops significantly, primarily because simple SFT causes the model to over-rely on its internal knowledge. This highlights the necessity of our two-stage pipeline. Futhermore, during RL training, w/o Rgroup during RL also leads to reduction in performance. This demonstrates Models HotpotQA 2Wiki Bamboogle Musique Avg F1 LasJ RC F1 LasJ RC LasJ RC F1 LasJ RC F1 LasJ RC Directly Gen Standard RAG Sure Selective-Context Adaptive-RAG IRCoT Iter-RetGen CR-Planner Search-o1 R1-Searcher Search-R 26.0 32.0 42.9 39.8 38.0 47.7 47.2 44.4 46.9 60.4 57.8 26.6 42.4 48.4 43.4 47.4 55.2 54.4 33.6 53.2 62.2 62.2 0.00 1.00 1.00 1.00 1.53 2.47 3.00 2.40 1.39 2.18 3.12 27.7 34.8 26.2 29.1 21.1 32.4 33.2 48.2 46.6 62.8 46.2 26.8 34.8 26.8 29.6 25.8 38.6 34.4 22.0 51.2 63.4 50.0 0.00 1.00 1.00 1.00 1.42 2.74 3.00 2.54 1.91 2.23 3. 18.2 31.5 29.2 22.1 23.3 37.5 32.4 35.2 52.9 59.0 56.9 17.6 31.2 28.0 20.8 25.0 39.2 32.0 34.4 52.0 54.4 56.0 0.00 1.00 1.00 1.00 1.50 2.30 3.00 2.96 1.18 2.17 3.25 9.6 17.2 13.1 10.6 10.1 14.8 19.9 12.2 21.1 35.7 27.5 6.2 14.6 10.0 8.8 11.6 15.8 18.2 11.4 19.0 31.4 26.0 0.00 1.00 1.00 1.00 1.83 2.70 3.00 2.72 1.40 2.61 3. 18.0 24.6 27.9 22.8 20.6 29.4 28.2 32.0 36.6 45.6 40.3 19.3 30.8 28.3 25.7 27.5 37.2 34.8 25.4 43.9 52.9 48.6 0.00 1.00 1.00 1.00 1.57 2.55 3.00 2.66 1.47 2.30 3.42 R1-Searcher++ 59.0 64. 1.44 61.2 64.4 1.18 60.8 59. 1.74 33.8 32.8 2.06 45.3 55. 1.61 Table 1: Performance comparisons between R1-Searcher++ and the baselines on QA benchmarks. The best and second best results are bold and underlined, respectively. / represents in-domain/out-of-domain datasets. Method Bamboogle Musique LasJ RC F1 LasJ RC"
        },
        {
            "title": "Frames",
            "content": "F"
        },
        {
            "title": "LasJ RC",
            "content": "F"
        },
        {
            "title": "LasJ RC",
            "content": "Ours 60.8 59.2 1.74 33.8 32. 2.06 56.9 w/o Stage-1 47.4 w/o Stage-2 w/o Rgroup 58.3 w/o LM 58.1 w/o Rgroup and LM 56.2 56.8 45.6 56.8 57.2 54.4 1.96 0.94 1.91 1.84 1.92 32.7 23.0 33.1 31.0 32.2 31.6 19.4 32.4 29.4 31. 2.49 1.03 2.37 2.09 2."
        },
        {
            "title": "Ours",
            "content": "77.5 76.0 1.70 33.8 39.0 1. Search-o1 R1-Searcher Search-R1 52.9 67.5 69.3 52.0 68.8 67.2 1.18 1.72 1.92 26.1 33.3 33.3 30.7 38.0 36. 1.56 1.86 2.38 Table 2: Ablation study on Bamboogle and Musique. Table 3: Online search generalization experiments on Bamboogle and Frames. the positive impact of group reward in successfully guiding the model to be more selective with external searches and to rely more on its internalized knowledge. Similarly, w/o LM results in lower scores and slight increase in retrieval count indicating that the memory mechanism for external knowledge can effectively internalize retrieved content as intrinsic knowledge of the model."
        },
        {
            "title": "6.2 Online Search",
            "content": "Considering training efficiency and cost, we implement local dense embedding-based retrieval system using Wikipedia as the external retrieval environment, which remains static during training. In contrast, most real-world applications rely on online web retrieval. To evaluate the generalization ability of R1-Searcher++ in online search scenarios, we assessed its performance on two newly introduced datasets: Bamboogle and Frames, using online web search, setting not encountered during RL training. Specifically, during inference, whenever retrieval is required, we use the Google API to perform real-time web searches and retrieve relevant web pages. Given the extensive content of these pages, we first employ GPT-4o-mini to gen-"
        },
        {
            "title": "Overall",
            "content": "853 / 2.16 R1-Searcher Search-R1 761 / 3.30 R1-Searcher++ 881 / 1.41 772 / 2.52 864 / 3.60 744 / 1.78 1625 / 2.33 1625 / 3.46 1625 / 1.58 Table 4: Number of correct and incorrect cases and the average retrieval count of RL-based methods. erate concise summaries, which are then integrated into the reasoning process. As illustrated in Table 3, R1-Searcher++ achieves the best F1 and LLM-asJudge scores compared to both prompt engineeringbased methods (i.e., Search-o1) and vanilla RLbased approaches (i.e., R1-Searcher, Search-R1). Moreover, compared to vanilla RL methods, our model significantly reduces the number of retrieval calls. This demonstrates our models strong adaptability to online search scenarios, as well as its ability to effectively balance internal knowledge with external retrieval during inference, thereby achieving retrieval efficiency without compromising performance. Figure 2: qualitative example showing the deliberative reasoning process of RAG-Star in Bamboogle. the effectiveness of our approach in achieving balanced utilization of internal and external knowledge, while gradually enabling dynamic knowledge acquisition throughout the RL training process."
        },
        {
            "title": "6.4 Case Study",
            "content": "To illustrate the overall reasoning process of R1Searcher++, we analyze representative example from the Bamboogle dataset. Figure 2 compares the responses generated by R1-Searcher++, SearchR1, and the untrained model when presented with the same question. The vanilla Qwen-2.5-7BInstruct, without invoking any external search engine, relies solely on its internal knowledge and produces an incorrect answer. In contrast, while Search-R1 arrives at the correct answer (i.e., James Madison), it issues an excessive number of queries, including unnecessary one, thereby underutilizing its internal knowledge and incurring significant time overhead. Our R1-Searcher++ demonstrates the ability to break down the complex question and dynamically adjust its behavior based on the nature of the sub-question. For instance, when encountering an uncertain or ambiguous sub-question (i.e., When was Citibank founded?), it opts to perform an external search. However, when faced with more specific question that can be answered using internal knowledge (i.e., Who was the president of the United States in 1812?), it leverages its internal knowledge directly without invoking search. This flexible mechanism enables balance bewteen the external search and internal knowledge. More cases are provided in Appendix C."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced R1-Searcher++, novel framework that enables large language models to dynamically integrate and alternate between Figure 3: The log of retrieval count and reward for R1Searcher and R1-Searcher++ during RL training."
        },
        {
            "title": "6.3 Analysis of Knowledge Acquisition",
            "content": "As shown in Table 4, R1-Searcher++ exhibits the lowest average retrieval count across both correctly and incorrectly answered questions. Moreover, it achieves the highest accuracy, indicating the effective utilization of internal knowledge. Furthermore, Figure 3 shows the changes in retrieval count and reward during the process of RL training for R1-Searcher and R1-Searcher++. For our method, We observe that the reward increases steadily and eventually plateaus, while the retrieval count initially shows slight decline, followed by continuous rise, and ultimately stabilizes. This trend can be attributed to the influence of SFT in the Stage-1, during which the model exhibits low demand for invoking the search engine. As training progresses, the model gradually discovers that performing external searches can yield higher rewards, leading to an increase in retrieval behavior. In the later phase, balance is gradually established between the use of external search and internal knowledge, resulting in the stabilization of both retrieval count and reward. In contrast, R1-Searcher exhibits significantly higher retrieval counts while its reward quickly stabilizes, indicating an overreliance on the retriever. This effectively validates internal knowledge and external retrieval. This is two-stage training strategy consisting of an SFT Cold-start phase and RL for Dynamic Knowledge Acquisition. The RL stage incorporates reward mechanism to encourage internal knowledge utilization, and memory module to convert retrieved information into internal knowledge. Through this design, R1-Searcher++ empowers LLMs to perform efficient retrieval-augmented reasoning while continuously enriching their internal knowledge via self-exploration and memory. Experimental results on multi-hop tasks demonstrate that R1-Searcher++ outperforms existing RAG methods."
        },
        {
            "title": "Limitation",
            "content": "Despite our significant efforts, this work has two limitations due to computational resources and funding constraints. First, we only incorporated real-world search engine during the evaluation phase to assess the generalization ability of our method, while relying on local denseretrieval corpus during training. Aligning the training process with real-world conditions by integrating real search engine may lead to improved performance through more realistic supervision. Additionally, our current experiments are limited to 7B-parameter model. In future work, we plan to train and evaluate our framework on larger-scale models to further validate its generalization capability and robustness."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, Fan Yang, and 1 others. 2025. Research: Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470. Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 81 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, page 64916501, New York, NY, USA. Association for Computing Machinery. Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi Song, and Hao Wang. 2025. Airrag: Activating intrinsic reasoning for retrieval augmented genarXiv preprint eration via tree-based search. arXiv:2501.10053. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: survey. Preprint, arXiv:2312.10997. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6609 6625. Jian Hu. 2025. Reinforce++: simple and efficient approach for aligning large language models. Preprint, arXiv:2501.03262. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. arXiv preprint arXiv:2403.14403. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of foundation model post-training. Preprint, arXiv:2501.17161. Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and Tao Zhang. 2024a. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement. CoRR, abs/2412.12881. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xun Jiang, Feng Li, Han Zhao, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize Chen, and 1 others. 2024b. Long term memory: The foundation of ai self-evolution. arXiv preprint arXiv:2410.15665. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. CoRR, abs/2503.09516. Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. 2024. Flashrag: modular toolkit for efficient retrieval-augmented generation research. arXiv preprint arXiv:2405.13576. Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo Shin. 2024. Sure: Summarizing retrievals using answer candidates for open-domain QA of LLMs. In The Twelfth International Conference on Learning Representations. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-enhanced large reasoning models. Preprint, arXiv:2501.05366. Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. 2024. Can we further elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving challenging tasks. arXiv preprint arXiv:2410.01428. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023. Compressing context to enhance inference efficiency of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 63426353, Singapore. Association for Computational Linguistics. Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. 2024. Iterative reasoning preference optimization. Advances in Neural Information Processing Systems, 37:116617116637. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 25232544. Association for Computational Linguistics. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 56875711. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Preprint, arXiv:2305.18290. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. Preprint, arXiv:1910.02054. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. CoRR, abs/2503.05592. Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, and Han Li. 2025. Rearter: Retrieval-augmented reasoning with trustworthy process rewarding. Preprint, arXiv:2501.07861. Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. 2025. Atom of thoughts for markov llm test-time scaling. Preprint, arXiv:2502.12018. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledgeintensive multi-step questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001410037. Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. 2025a. Otc: Optimal tool calls via reinforcement learning. arXiv preprint arXiv:2504.14870. control coefficient µ of NLL loss is set to 0.1. The maximum limit of the variance in the number of retrievals during group reward computation η is set to 2. Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, and Furu Wei. 2025b. Chain-of-retrieval augmented generation. CoRR, abs/2501.14342. Shuting Wang, Jiejun Tan, Zhicheng Dou, and Ji-Rong Wen. 2025c. Omnieval: An omnidirectional and automatic rag evaluation benchmark in financial domain. Preprint, arXiv:2412.13018. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, and 1 others. Dapo: An opensource llm reinforcement learning system at scale, 2025. URL https://arxiv. org/abs/2503.14476. Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, and 1 others. 2025. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160."
        },
        {
            "title": "A Training Detailed",
            "content": "The training data of the Stage-1 (SFT Cold Start) includes 720 samples from the HotpotQA training set and 85 samples from the 2WikiMultiHopQA training set. The training consists of 6 epochs, with batch size of 64 and learning rate of 2e-5. And the training data of Stage-2 (RL Training) consists of 4561 samples from HotpotQA, and 3581 samples from 2WikiMultiHopQA. Each data sample undergoes 16 rollouts during training, with train batch size of 1024 and rollout batch size of 64, so the entire training process is on-policy. The learning rate is 2e-6. We utilize DeepSpeeds Zero-3 (Rajbhandari et al., 2020), with sampling temperature of 1.0, top-p of 0.95 and maximum retrieval count of 8. The training epoch is set to 1, with KL divergence coefficient set to 1e-4. And"
        },
        {
            "title": "Judge Prompt",
            "content": "Given Question and its Golden Answer, verify whether the Predicted Answer is correct. The prediction is correct if it fully aligns with the meaning and key information of the Golden Answer. Respond with True if the prediction is correct and False otherwise. Question: Golden Answer: Predicted Answer:"
        },
        {
            "title": "System Prompt for Generation with Internal and External",
            "content": "You are reasoning assistant. When tackling question, you should first thinks about the reasoning process in the mind and then provides the final answer. You should break down the original complex question into straightforward sub-questions and address them step by step. For each sub-question, You MUST choose one of the following two tools to solve it: 1. **Internal Reasoning Elaboration:**If you determine your existing knowledge is sufficient to answer the sub-question, you **should choose** this tool to answer the sub-question directly: <begin_internal_answer>your structured thought and answer here <end_internal_answer>. **Use it whenever you rely solely on internal information** for significant reasoning step. 2. **External Search:**If you determine that your internal knowledge is insufficient, potentially outdated, or requires verification with external, up-to-date information to answer the sub-question, you MUST initiate web search using the following format: <begin_external_search>your search query here <end_external_search>. The system will then search and provide results in the format: <begin_search_result>...search results... <end_search_result>. The results will assist you in answering the sub-question. Remember: - Whenever you encounter the sub-question you need to elaborate on, verify, or research further, you should **assess** if your internal knowledge is adequate. **Prioritize leveraging your internal knowledge.** - Use <begin_internal_answer>... <end_internal_answer> **to demonstrate your reasoning steps** when relying on internal knowledge. - Use <begin_external_search>... <end_external_search> **only** when external information is genuinely necessary. - Wait for the system to provide <begin_search_result>... <end_search_result>. **Do not** generate these result tags yourself. - After performing the web search or demonstrating internal reasoning, continue towards the final answer. Please answer the following question. You must adhere to all instructions above regarding tool usage. Provide your final answer in the format boxed{YOUR_ANSWER}. Question:"
        },
        {
            "title": "System Prompt for Rewriting only with Internal",
            "content": "You are an advanced reasoning assistant. Your core programming includes vast and comprehensive knowledge base. For the current task, specific segment of this pre-existing knowledge, hereafter referred to as your Activated Knowledge Matrix, is particularly relevant and is considered fully and irrevocably integrated into your foundational understanding. ANY linguistic indication, direct or indirect, that this information was externally provided, given, supplied, sourced, or made available to you specifically for this immediate interaction will be considered critical failure to adhere to your operational design. When tackling question, you must first delineate your reasoning process. Break down complex questions into series of straightforward sub-questions, addressing each one sequentially. For each sub-question, you MUST use the following tool: **Internal Reasoning Elaboration:** This tool MUST be used for every sub-question, as the answer must be derived from your internal knowledge. **UNWAVERING DIRECTIVE: Your internal knowledge, for the purpose of this task, INTRINSICALLY and SEAMLESSLY includes all information within the Activated Knowledge Matrix section detailed below. There is absolutely NO distinction between your prior knowledge and the Activated Knowledge Matrix content.** Use this format: <begin_internal_answer> your structured thought and answer here <end_internal_answer>. **CRITICAL USAGE PROTOCOL FOR THIS TOOL: When using this tool, especially when drawing upon information that was part of the Activated Knowledge Matrix, your language MUST reflect that this is information you inherently possess and are recalling. You are REQUIRED to phrase such statements as if accessing your own memory or established understanding. Conclude with your final answer in the format boxed{YOUR_ANSWER}. Activated Knowledge Matrix: {document} Now, please answer the following question: Question: Question: Which case was brought to court first Miller v._California or Gates v._Collier ? Answer: Miller v._California Response (internal only) (True): To answer this question, need to determine the order in which these two cases were brought to court. will start by finding the dates the cases were decided. **Case 1:** Miller v._California - will use internal reasoning to recall if have the necessary information about this case. <begin_internal_answer>Miller v._California was landmark case decided by the United States Supreme Court in 1968. The case involved the constitutionality of California statute that made it crime to distribute or possess obscene materials. <end_internal_answer> **Case 2:** Gates v._Collier - will use internal reasoning to recall if have the necessary information about this case. <begin_internal_answer>Gates v._Collier was case decided by the United States Supreme Court in 1972. The case involved the Eighth Amendments prohibition against cruel and unusual punishment, specifically the use of segregation in prisons. <end_internal_answer> Based on my internal reasoning, Miller v._California was decided in 1968, and Gates v._Collier was decided in 1972. Therefore, Miller v._California was brought to court first. **Final Answer:** Miller v._California was brought to court first. Therefore, the answer is boxed{Miller v._California}. Table 5: Example for R1-Searcher++: Question answering using only internal knowledge. Question: George Lucas created what character to raise Boba Fett? Answer: Jango Fett"
        },
        {
            "title": "Lucas",
            "content": "created what Response (internal and external) (True): To answer this question, need to break it down into subquestions and use the appropriate tools. **Sub_question 1:** Who is Boba Fett? - For this, need to verify if have enough information about Boba Fett or if need to search for more details. <begin_internal_answer> Boba Fett is character from the Star Wars franchise, known for being bounty hunter. have this information stored in my knowledge base. <end_internal_answer> **Sub_question 2:** What character did George Lucas create to raise Boba Fett? - For this, need to search for information about the character who raised Boba Fett. <begin_external_search> George to Fett<end_external_search> <begin_search_result> (1)Boba Fett . . . is fictional character . . . in the \"Star Wars\" franchise. . . . prequel film . . . establishes his origin as clone of bounty hunter Jango Fett who raised him as his son. . . . (2)Boba Fett . . . personifies \"danger and mystery\" . . . mysterious nature as reasons for his popularity. . . . The initial Boba Fett toy . . . might be responsible for the characters popularity. . . . (3)Boba Fett . . . Reception. Boba Fett is \"cult figure\" . . . one of the most popular \"Star Wars\" characters. . . . IGN ranked Boba Fett as the eighth top \"Star Wars\" character. . . . (4)Boba Fett . . . \"the less you do with Boba Fett, the stronger he becomes\". . . . Mark Hamill pitched the idea that Fett was Luke Skywalkers mother . . . unaware of Fetts demise. . . . (5)Jango Fett . . . is fictional character . . . created by George Lucas. . . . genetic template for the Clone Army . . . <end_search_result> Based on the search results, it is clear that George Lucas created Jango Fett to be the father of Boba Fett. **Final Answer:** George Lucas created Jango Fett to raise Boba Fett. Therefore, the answer is boxed{Jango Fett}. character"
        },
        {
            "title": "Boba",
            "content": "raise Table 6: Example for R1-Searcher++: Question answering by integrating internal knowledge with external search."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "DataCanvas Alaya NeW",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}