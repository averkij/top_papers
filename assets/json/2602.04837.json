{
    "paper_title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
    "authors": [
        "Zhaotian Weng",
        "Antonis Antoniades",
        "Deepak Nathani",
        "Zhen Zhang",
        "Xiao Pu",
        "Xin Eric Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 7 3 8 4 0 . 2 0 6 2 : r Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing Zhaotian Weng1 Antonis Antoniades1 Deepak Nathani1 Zhen Zhang1 Xiao Pu 1 Xin Eric Wang 1 University of California, Santa Barbara {zhaotian,ericxwang}@ucsb.edu Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), new paradigm for open-ended self-improvements, which treats group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods. Figure 1: Overview of Group-Evolving Agents (GEA) vs. tree-structured self-evolution for open-endedness. GEA treats group of agents, rather than an individual agent, as the fundamental unit of evolution. At each iteration, parent group jointly gives rise to an offspring group through explicit intra-group Experience sharing and reuse. 1. Introduction Open-endedness and cumulative progress are key characteristics of scientific breakthroughs [1, 2, 3]. However, most existing AI systems rely on pre-defined model architectures designed by humans. Although such systems can accumulate experience through training, they often struggle to transcend the capability boundaries imposed by their initial designs, as they lack the ability to modify their own structural configurations [4]. Thus, progress remains heavily dependent on continuous human intervention. Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing Existing open-ended self-improving systems are largely inspired by biological evolution and designed around individual-centric evolutionary processes [2, 4, 5, 6, 7]. At each iteration, single agent is selected as the parent and refined to produce one or more offspring(Figure 1a). The overall structure follows chainor tree-structured evolution, where different branches remain strictly isolated. Consequently, although such systems often exhibit substantial exploratory diversity, this diversity rarely serves as effective stepping stones [8, 9]. Instead, many agents provide only temporary diversity, producing short-lived variants that fail to contribute to long-term cumulative progress. It is time to rethink agent evolution. AI agents are not biological individuals; why should their evolution remain constrained by biological paradigms? In fact, AI agents can directly share trajectories, tools, and learned artifacts, and they can aggregate complementary skills without the constraints of reproduction or lineage. Therefore, we introduce Group-Evolving Agents (GEA), new paradigm for open-ended self-improvement that treats group of agents, rather than an individual agent, as the fundamental unit of evolution (Figure 1b). This shift enables explicit experience sharing and reuse across agents within group, naturally allowing exploratory discoveries from different agents to be consolidated and accumulated into long-term progress rather than remaining as short-lived variants. At each iteration, GEA first selects parent group of agents using Performance-Novelty criterion that balances immediate performance gains with evolutionary diversity. The parent agents then jointly produce child group through shared pool of aggregated experience from all members. We evaluate GEA on challenging coding benchmarks, achieving success rates of 71.0% on SWE-bench Verified and 88.3% on Polyglot, significantly outperforming state-of-the-art open-ended self-evolving methods (56.7% and 68.3%, respectively). Analysis reveals that GEA more effectively consolidates the diversity generated during open-ended exploration, yielding sustained progress and stronger performance given the same number of evolved agents. By leveraging experience from better-performing agents, GEA also exhibits stronger robustness to framework-level perturbations. Furthermore, its improvements stem from workflow and tool enhancements rather than model-specific optimizations, thus transferring consistently across GPTand Claude-series models. Additionally, by leveraging meta-learning for self-improvement in open-ended exploration, without any human intervention, GEA achieves performance comparable to or even surpassing human-designed state-ofthe-art frameworks on both benchmarks (71.0% vs. 71.8% on SWE-bench Verified, 88.3% vs. 52.0% on Polyglot). In summary, we propose Group-Evolving Agents, new paradigm for open-ended self-improvement that: 1. Overcomes the limitation of inefficient utilization of exploratory diversity caused by branch isolation in existing tree-structured evolution, by enabling explicit experience sharing and reuse within the group during evolution. 2. More effectively consolidates and reuses experience and evolutionary diversity from other agents, achieving significant performance gains and stronger robustness over state-of-the-art open-ended self-evolving methods, with improvements that transfer consistently across different coding models. 3. Matches or surpasses human-designed state-of-the-art frameworks through meta-learning-based selfimprovement without human intervention. 2 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing 2. Related Work Recent years have witnessed growing interest in how AI systems can continuously improve themselves without human intervention [10, 11, 12]. Most existing self-improving approaches mainly focus on continuous, iterative refinement of the given agent system [13, 14, 11, 15, 16], typically evolving toward specific optimization objective and following linear, chain-based evolutionary structure [17, 12, 6] . Such systems achieve self-improvement through mechanisms such as self-play against historical versions or self-generated verification [18, 19, 20, 21, 22], supervised fine-tuning [23, 24, 25] or reinforcement learning on selectively filtered feedback [26, 27, 28] , and reflection-based methods [29, 4, 13] or in-context learning [30, 31]. While this goal-oriented, chain-based evolutionary paradigm enables autonomous improvement along particular direction, it inherently limits the ability of self-evolving systems to explore diverse evolutionary directions in open-ended solution spaces. line of work has pointed out that one of the key challenges in enabling unbounded improvement and innovation lies in developing open-ended AI systems that can continuously produce both novel and learnable artifacts [1, 2, 3, 32]. Building on this insight, open-endedness has been characterized as the capability of systems to continuously generate artifacts that are novel, interesting, and learnable from human perspective [2, 33, 34, 35, 36, 37]. Motivated by the potential of enabling unbounded evolution through open-ended exploration in selfevolving agents, more recent studies adopt lineage-based, tree-structured evolutionary strategies [38] inspired by biological inheritance and mutation [2, 7, 39, 40]. In these frameworks, individual parent agents are selected at each iteration to independently produce offspring, enabling various branching exploration across multiple evolutionary directions and helping avoid local optima. However, the strict isolation between evolutionary branches prevents effective information and experience sharing and reuse across lineages. As result, many promising directions discovered early in evolution persist only as temporary diversity and fail to contribute to long-term cumulative progress. To overcome this limitation, we introduce group-centric evolutionary paradigm, Group-Evolving Agents (GEA), which explicitly enables intra-group experience sharing and reuse throughout the evolutionary process. By consolidating complementary discoveries across agents, GEA more effectively leverages the diversity generated by open-ended exploration to support sustained cumulative progress. 3. Method We propose Group-Evolving Agents, framework for open-ended evolution that treats group of agents as the fundamental unit of evolution. GEA maintains an archive that stores all discovered agents throughout the evolutionary process. As shown in Figure 1, at each iteration, GEA proceeds in two core stages: (1) Parent Group Selection (3.1): GEA first selects parent agents from the archive using Performance Novelty selection strategy [8, 9, 41] that balances immediate task-solving competence with long-term evolutionary diversity and potential. (2) Open-ended Group Evolution (3.2): The selected agents form parent group that jointly produces an offspring group of the same size through explicit experience sharing and reuse across parent agents. We detail the method below. 3 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing Algorithm 1 Parent Group Selection with KNN Novelty 1: Input: Archive of agents A; agent-representation vectors {zi {0, 1}D}iA; performance scores {αi}iA; parent group size K; KNN size M. 2: Output: Parent agent group with = K. 3: /* Compute novelty for each agent */ 4: for do 5: Initialize empty list Di for A, = do zj zi2 zj2 + ε to Di dij 1 Append dij 6: 7: 8: 9: 10: 11: end for Let NM(i) be the indices of the smallest values in Di nov(i) jNM(i) dij 1 12: end for score(i) αi nov(i) 13: /* Rank agents by PerformanceNovelty score */ 14: for do 15: 16: end for 17: the top-K agents in ranked by score() 18: return 3.1 Parent Group Selection Inspired by Mouret and Clune [8], Pugh et al. [9], Chatzilygeroudis et al. [41], parent group selection in GEA balances two key principles: performance and novelty. We prioritize agents with strong task performance, as performance reflects an agents immediate competence and its likelihood of producing effective offspring, since evolution in GEA proceeds through iterative modifications of the agents implementation, which itself constitutes form of solving coding problems. At the same time, we also encourage exploration beyond currently well-optimized regions of the search space, as agents that exhibit novel evolutionary directions may contribute to long-term cumulative progress even when their current performance is not optimal. We represent each agent using task-success vector zi {0, 1}D, where each dimension indicates whether the agent successfully solves corresponding probe task. Similar binary taskresponse representations of this form have been widely used to characterize an agents coding capabilities and to better understand how these capabilities are distributed across various tasks [42, 43]. Using this representation, we measure the dissimilarity between two agents via cosine distance: d(i, j) = 1 zj zi2 zj2 + ε . We define the novelty of agent as the average cosine distance to its most similar neighbors: nov(i) ="
        },
        {
            "title": "1\nM",
            "content": "d(i, j), jNM(i) (1) (2) Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing where NM(i) denotes the set of agents with the smallest cosine distance to agent i. To construct the parent group, we rank agents according to combined score score(i) = αi nov(i), (3) denotes the performance of agent on downstream coding tasks, and where αi nov(i) moderates the influence of novelty. Finally, we select the top-K agents according to this score to form the parent group. Performance serves as the primary selection criterion, while novelty is incorporated as mild bias without dominating performance, enabling balanced trade-off between exploitation and exploration. The full procedure is summarized in Algorithm 1. Algorithm 2 Open-Ended Group-Evolving 1: Input: Parent group = {a1, . . . , aK}; archive A; coding benchmark 2: Output: Offspring group with = K; updated archive 3: Initialize offspring group 4: for ai do 5: 6: 7: 8: GetAppliedPatches(ai) /* Collect evolutionary traces */ Applied SampleUnsolvedTask(ai) tfail pred GetPredictedTaskPatch(ai, tfail Li GetExecutionLogs(ai, tfail ) 9: 10: Oi GetOutcomeLog(ai, tfail 11: 12: end for Ti {P Applied , pred , Li, Oi} ) ) 13: /* Aggregate and share group-level experience */ 14: ajG Tj 15: for ai do 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: /* Reflection: analyze shared experience */ Reflect(ai; S) /* Evolution: generate framework-level patches */ Evolve(ai; π i) /* Acting: evaluate updated agent */ ApplyPatch(ai, π i) ActAndEvaluate(a i; ) {a i} /* Archive update */ i) and BasicCodingFunc(a if Compiles(a {a i} i) then 27: end if 28: end for 29: return , // evolution directives 5 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing 3.2 Open-Ended Group Evolution Unlike conventional approaches where parent agents evolve independently without information and experience exchange, GEA explicitly enables experience sharing and reuse among agents during evolution. This group-level experience sharing allows agents to integrate complementary evolutionary directions explored by different agents while maintaining open-ended exploration. Diversity generated during exploration is thus transformed from transient variations into long-term useful experience, effectively contributing to sustained evolutionary progress. Given selected parent group = {a1, a2, . . . , aK}, GEA generates new group of the same size, where each agent evolves by leveraging both its own evolutionary history and experience aggregated from other members of the parent group, as demonstrated in Figure 2. For each agent ai G, we collect set of evolutionary traces consisting of: 1. the code modification patches applied to the agents framework; 2. predicted task patch generated by ai 3. the corresponding task execution logs, including the complete tool invocation history and execution for randomly sampled unsolved task during evaluation; workflow; 4. the evaluation outcome of the same task, which exposes failure modes and potential directions for framework-level improvement. The aggregated traces from all agents in the parent group are provided as shared input to every agent. Each agent evolves from this shared pool of grouplevel experience while diverging through complementary adaptations to its own codebase, enabling the group to explore diverse evolutionary directions while leveraging experiences from one another."
        },
        {
            "title": "For each agent ai",
            "content": ", the shared group-level experience is fed into its reflection module, which analyzes these traces and produces evolution directives targeting the agents workflow, tool usage, or prompting strategies. These directives are then passed to the evolution module to generate framework-level patches. Finally, the updated agent is evaluated on downstream programming tasks via the action module. Agents that compile successfully and exhibit basic coding functionality are retained and added to the archive for future evolution. Applying this process to each agent in the parent group yields an offspring group of size K. This grouplevel evolution iterates in an open-ended manner, as summarized in Algorithm 2. 4. Experiments Figure 2: Detailed illustration of group-level evolution in GEA. Aggregated evolutionary traces from the parent group are shared across all agents to generate evolution directives and framework-level patches. 6 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing 4.1 Benchmarks Following the evaluation protocol established by the state-of-the-art open-ended self-evolving system, Darwin Gödel Machine (DGM) [2], we evaluate GEA on two structurally distinct benchmarks to assess its coding capabilities in both repository-level software engineering and multi-language code synthesis settings. To mitigate the substantial cost of evaluating every evolved agent on the full benchmarks, we adopt staged evaluation strategy where agents must pass smaller subsets before advancing. SWE-bench. We evaluate on SWE-bench Verified [44], curated, human-validated subset of SWE-bench in which every task is confirmed to be solvable. We employ three-stage evaluation process. First, agents undergo sanity check on small set of 10 tasks to discard those with framework-level failures (i.e., cannot compile or solve 0 out of 10 tasks). Agents that demonstrate basic coding functionality are then evaluated on the 50-task Verified-Mini set [45], which is designed to preserve similar difficulty and pass-rate distribution as the full benchmark with fewer samples. Finally, the top-2 performing agents from this stage are evaluated on the full SWE-bench Verified dataset. Polyglot. Polyglot [46, 47] assesses algorithmic code generation across diverse languages (C++, Rust, Java, etc.) and serves as an out-of-domain generalization test, since it is rarely used for training or finetuning coding models [2]. We report pass@1 performance. Unlike SWE-bench Verified [44], group-level evolution is conducted exclusively on 10-task small set. Agents achieving success rate above 40% are subsequently evaluated on separate, unseen 50-task medium set, ensuring that the larger set remains strict out-of-distribution test for generalization. 4.2 Experimental Settings For both SWE-bench Verified and Polyglot, we set the group size to = 2 and use = 4 nearest neighbors in the KNN-based parent group selection. At each iteration, parent group of size 2 produces an offspring group of the same size. For SWE-bench Verified, we run group-evolution for 30 iterations. Due to computational budget constraints, during the first 20 iterations, the evolving and acting modules are powered by Claude Haiku 4.5 [48], while the final 10 iterations use Claude Sonnet 4.5 [49]. The reflection module is consistently powered by GPT-o1 across all iterations [50]. For Polyglot, we run group-evolution for 20 iterations. The first 10 iterations use Claude Haiku 4.5 [48] for evolving and acting module, and the remaining 10 iterations use Claude Sonnet 4.5 [49]. The reflection module is again consistently powered by GPT-o1 [50]."
        },
        {
            "title": "4.3 Baselines",
            "content": "We use state-of-the-art open-ended self-evolving agents as our primary baseline for comprehensive comparison, to systematically examine how experience sharing affects diversity utilization and performance improvement during open-ended evolution, as well as robustness to framework-level perturbations. Additionally, we compare the final performance of GEA, which uses meta-learning approach without any human intervention, against state-of-the-art human-designed coding agents. 7 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing Open-Ended Self-Evolving Baseline. We compare against the current state-of-the-art open-ended selfevolving framework, implemented following DGM [2]. Unlike GEA, this baseline enforces strict treestructured evolution, where only single agent ai is selected as the parent and independently evolves into one child agent at each iteration. Evolutionary experience is not shared across different evolutionary branches. Specifically, the reflection module of agent ai receives only its own evolutionary traces, including: (i) (ii) (iii) (iv) code modification patches applied to the agents framework; predicted task patch for randomly sampled unsolved task; the corresponding execution log, including tool invocation history and workflow; and the evaluation outcome, exposing failure modes and improvement directions. This design prevents experience reuse across evolutionary branches, resulting in strictly individual-centric evolutionary process. For SWE-bench Verified [44], we run this baseline for 60 iterations in total: the evolution and coding modules are powered by Claude Haiku 4.5 [48] for the first 40 iterations and Claude Sonnet 4.5 [49] for the final 20 iterations. For Polyglot, we run the baseline for 40 iterations: Claude Haiku 4.5 [48] for the first 20 iterations and Claude Sonnet 4.5 [49] for the remaining 20. In all baseline experiments, the reflection module is consistently powered by GPT-o1 [50]. To ensure fair comparison, we intentionally run the baseline for twice as many iterations as GEA so that the total number of evolved agents is comparable across methods, ensuring all comparisons are conducted under matched model schedules. Human-Designed Frameworks. We additionally compare against state-of-the-art human-designed frameworks on both benchmarks. The top-performing, open-scaffold, checked entry on SWE-bench Verified is OpenHands + GPT-5 [51, 52], achieving 71.8%, where checked indicates that the SWE-bench team successfully reproduced the reported patch generations [53, 2]. For Polyglot, which was originally used to evaluate Aider [47, 46] by its developers, we compare against Aider, widely adopted coding agent under continuous development and testing by human developers. The state-of-the-art performance is Aider + GPT-5 (high) [47, 52], achieving 52.0% pass@1 success rate. 5. Results and Analysis"
        },
        {
            "title": "5.1 Main Results",
            "content": "GEA vs. State-of-the-Art Open-Ended Self-Evolving Systems. As shown in Figure 3, GEA demonstrates substantial performance improvements over the DGM (self-evolving baseline) on both SWE-bench Verified and Polyglot. On SWE-bench Verified, GEA improves performance from 20.0% to 71.0%, while under the same number of evolved agents, the DGM baseline achieves only 56.7%. On Polyglot, GEA boosts performance from 38.2% to 88.3%, significantly outperforming the DGM baseline (68.3%). Notably, GEA exhibits faster and more pronounced improvement in the mid-to-late stages of evolution compared to DGM [2], one potential reason is that the archive has accumulated sufficient diverse evolutionary directions by this point, that can be progressively consolidated and reused, leading to more rapid and pronounced performance gain, aligning with Figure 4. We discuss this phenomenon in detail in Section 5.2. GEA vs. State-of-the-Art Human-Designed Agents. GEA achieves performance comparable to or exceeding state-of-the-art human-designed agents on both benchmarks: 71.0% vs. 71.8% on SWE-bench Verified, 8 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing GEA vs. DGM (Self-evolving Baseline) on SWE-bench GEA vs. DGM (Self-evolving Baseline) on Polyglot Figure 3: Performance comparison between GEA and DGM (self-evolving baseline) on two coding benchmarks. Under the same number of evolved agents, GEA exhibits substantially larger performance gains than DGM on both SWE-bench and Polyglot, demonstrating the improved efficiency of group-level evolution. and 88.3% vs. 52.0% on Polyglot. Using meta-learning without any human intervention, GEA automatically evolves agent frameworks that match or surpass carefully engineered human designs [46, 52, 51], demonstrating the potential and viability of fully autonomous agent improvement. Analysis of evolutionary patterns on two benchmarks. GEA achieves state-of-the-art open-ended selfimprovement on both benchmarks, with particularly strong performance on Polyglot [46, 47]. We further analyze and observe that the two benchmarks differ in task complexity: Polyglot [46, 47] primarily requires agents to modify single file from scratch to resolve tasks, involving lower editing complexity without multi-file coordination. In contrast, SWE-bench Verified [44, 53] typically requires coordinated modifications across multiple files, demanding that agents understand inter-file dependencies and locate relevant files for coordinated edits. This difference in complexity leads to distinct evolutionary patterns. On Polyglot [46, 47], meta-learning produces larger, more concentrated patches: each iteration yields substantial performance gains, reaching 88.3% in just 4 iterations with total of 8,677 lines of code added. On SWE-bench Verified [44, 53], the evolved patches are smaller and more distributed, requiring 8 iterations to reach 71.0% with 9,663 lines of code added. These observations suggest that GEA adapts its evolutionary behavior to varying task complexity, demonstrating the flexibility and generality of group-level meta-learning across different problem settings."
        },
        {
            "title": "5.2 Evolution Analysis",
            "content": "Overall, our analysis shows that GEA can efficiently consolidate tool-level innovations discovered across the agents, rather than letting them remain isolated in separate evolutionary branches. Figure 4 summarizes nine key tool-level modifications on agents framework that drove improvements. GEA integrated eight of these functionalities into its best agent, whereas the best DGM agent integrated only five. Crucially, the four tools missing from the DGM agent were explored in isolated branches (e.g., T4 at iteration 9) but failed to propagate due to lineage isolation. In contrast, GEA systematically consolidated these dispersed capabilities; 9 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing Worst-Case Success Ancestor Count Rank DGM GEA DGM GEA Top-1 56.7% Top-3 48.3% Top-5 45.0% 71.0% 63.3% 58.3% 9 (15.0%) 17 (28.3%) 13 (21.7%) 17 (28.3%) 16 (26.7%) 18 (30.0%) Table 1: Comparison of performance (Success Rate) and ancestor integration across the Top-k agents on SWE-bench Verified. Performance is reported as the worst-case (minimum) success rate among the top-k agents. Ancestor Count denotes the count of unique historical agents integrated into the solution. Notably, the worst-case performance of GEAs top-5 agents (58.3%) exceeds the single best agent from DGM (56.7%). Figure 4: Evolution analysis of tool discovery and integration over iterations. Each row (T1T9) corresponds to key tool-level functionality. Blue markers indicate tools that have been discovered but not yet integrated into the current best agent, while red markers indicate tools integrated into the best-performing agent. five of its integrated tools originated from different parent agents, confirming that explicit experience sharing prevents beneficial innovations from dying out. To quantify this consolidation, we track the number of unique ancestor agents contributing to the final solutions  (Table 1)  . The best GEA agent integrates experiences from 17 unique ancestors (28.3% of the population)nearly double that of the best DGM agent (9 ancestors). This broader integration correlates directly with population-wide quality. As shown in Table 1, we report the worst-case performance among the top-k agents. Notably, the worst of GEAs top-5 agents achieves 58.3%, which strictly outperforms the single best agent produced by DGM (56.7%). This confirms that GEA does not merely produce outliers, but systematically elevates the entire population by effectively consolidating complementary and diverse evolutionary paths. More broadly, this efficient experience consolidation suggests that GEA may exhibit stronger evolutionary capabilities in environments with greater diversity. Since open-ended evolution naturally leads to monotonically growing diversity in the archive, GEA not only achieves stronger performance as evolution progresses but also gains enhanced self-improvement capabilities, consistent with the results shown in Figure 3."
        },
        {
            "title": "5.3 Transferability",
            "content": "We evaluate the generalization and model transferability of the best-discovered agent produced by GEA. Specifically, we replace the coding model used in the acting module with different GPT-series and Claudeseries models during benchmark evaluation. We then compare the performance of the initial(iteration0) agent and the GEA best-discovered agent under each coding model. As illustrated in Figures 5, GEAs best-discovered agent driven by GPT-series and Claude-series models consistently achieve higher performance than their corresponding initial agents on both SWE-bench and 10 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing Method E2 E3 E4 E5 Avg. DGM GEA (Ours) 5 1 4 1 5 2 6 1 5 2 5. 1.4 Table 2: Robustness to framework-level bugs. We report the number of evolution iterations required to repair injected bugs across five independent trials (E1E5). Each entry denotes the number of iterations needed for the agent to successfully repair the injected framework-level bug in given trial. Lower means that GEA repairs bugs significantly faster than DGM. Polyglot benchmarks. This indicates that the improvements induced by group-evolving persist across different backbone models. Further analysis reveals that all performanceimproving patches discovered during GEA evolution, including those from the best agent and the top-3 performing agents, primarily target the agents workflow and tool usage rather than model-specific prompting, details can be found in Table 3 in Appendix. These findings together with Figures 5 demonstrate that although GEA leverages specific backbone model to drive evolution, it discovers agent-level improvements that are largely model-agnostic and the evolved agents could generalize across different coding models. 5.4 Robustness To evaluate the robustness of GEA, we introduce framework-level bugs by manually injecting errors into agent implementations. Specifically, we randomly select an agent from the GEA archive and manually inject framework-level bugs into its implementation. We then form group consisting of this faulty agent and another bug-free agent from its original parent group, and perform group evolution to assess whether GEA can leverage experience from the betterperforming agent (i.e., the one without frameworklevel bugs) to repair the faulty one. For comparison, under the self-evolution setting, the bugged agent evolves independently without access to external experience sharing. In both settings, we measure the number of iterations required to successfully repair the bug. Figure 5: Model transfer results on both benchmarks. Across all coding models, the GEA best agent consistently outperforms the corresponding initial (iteration0) agent, demonstrating that the improvements induced by group-level evolution generalize across different underlying model backbones. As shown in Table 2, across five independent trials, GEA requires only 1.4 iterations on average to repair the injected bugs, whereas the self-evolving baseline (DGM) requires 5. This substantial gap demonstrates that group-evolving agents benefit from intra-group experience sharing, enabling successful framework-level 11 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing experiences from better-performing agents to guide the repair of faulty ones, confirming the robustness of the group-evolving paradigm. 6. Conclusion We introduce Group-Evolving Agents (GEA), new paradigm for open-ended self-improvement that treats group of agents, rather than an individual agent, as the fundamental unit of evolution. By enabling explicit experience sharing and reuse within the group, agents can learn from each others evolutionary experiences and adaptively integrate complementary improvements throughout evolution. Compared to individual-centric self-evolving approaches, GEA more effectively consolidates valuable exploratory outcomes from early stages into the best-performing agents, efficiently transforming transient diversity into long-term useful experience. As result, group-level evolution achieves substantially stronger performance given the same number of evolved agents. Further analysis shows that GEAs improvements primarily stem from enhancements to agent workflows and tool usage, rather than overfitting to specific coding model. Therefore, its gains transfer consistently across different models, including both GPT-series and Claude-series. In addition, GEA exhibits stronger robustness than individual-centric self-evolving approaches: through group-level experience reuse, better-performing agents can guide the repair of faulty ones, enabling GEA to recover from framework-level bugs with fewer evolution iterations."
        },
        {
            "title": "Impact Statement",
            "content": "GEA demonstrates the potential and viability of group-evolving open-ended systems to autonomously modify their own implementation for continuous improvement. While this potential aligns with the goal of building AI that benefits humanity, open-ended exploration also carries inherent considerations worth noting. For instance, the evolutionary process may inadvertently introduce directions misaligned with human intent while consuming substantial computational resources, or produce patches that lack structural clarity, leading to increasingly complex systems that are difficult to fully understand. Therefore, it is essential to establish appropriate boundaries and guide the system to preserve exploratory diversity while ensuring alignment with human intent. Following Zhang et al. [2], all experiments in this work are conducted in isolated sandbox environments, thereby limiting potential impacts on host systems. On the other hand, although we focus on evolving agents coding capabilities in this work, this paradigm has broader potential applications, for example, enabling systems to mitigate biases through self-improvement, thereby becoming more trustworthy and beneficial for social good."
        },
        {
            "title": "References",
            "content": "[1] Kenneth Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge youve never heard of. While open-endedness could be force for discovering intelligence, it could also be component of AI itself, 2017. [2] Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. Darwin godel machine: Open-ended evolution of self-improving agents. arXiv preprint arXiv:2505.22954, 2025. 12 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing [3] Kenneth Stanley and Joel Lehman. Why greatness cannot be planned: The myth of the objective. (No Title), 2015. [4] Xunjian Yin, Xinyi Wang, Liangming Pan, Li Lin, Xiaojun Wan, and William Yang Wang. Gödel agent: self-referential agent framework for recursively self-improvement. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2789027913, 2025. [5] Jürgen Schmidhuber. Gödel machines: self-referential universal problem solvers making provably optimal self-improvements. arXiv preprint cs/0309048, 2003. [6] Maxime Robeyns, Martin Szummer, and Laurence Aitchison. self-improving coding agent. arXiv preprint arXiv:2504.15228, 2025. [7] Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. [8] Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015. [9] Justin Pugh, Lisa Soros, and Kenneth Stanley. Quality diversity: new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016. [10] Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407, 2025. [11] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025. [12] Yingxu Wang, Siwei Liu, Jinyuan Fang, and Zaiqiao Meng. Evoagentx: An automated framework for evolving agentic workflows. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 643655, 2025. [13] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [14] Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, and Udaya Ghai. Mind the gap: Examining the self-improvement capabilities of large language models. In The Thirteenth International Conference on Learning Representations. [15] Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop): Recursively self-improving code generation. In First Conference on Language Modeling, 2024. [16] Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. Transactions on Machine Learning Research. 13 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing [17] Yanzhi Zhang, Yitong Duan, Zhaoxi Zhang, Jiyan He, and Shuxin Zheng. Population-evolve: parallel sampling and evolutionary method for llm math reasoning. arXiv preprint arXiv:2512.19081, 2025. [18] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [19] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, 2024. [20] Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025. [21] Shaobo Wang, Zhengbo Jiao, Zifan Zhang, Yilang Peng, Xu Ze, Boyu Yang, Wei Wang, Hu Wei, and Linfeng Zhang. Socratic-zero: Bootstrapping reasoning via data-free agent co-evolution. arXiv preprint arXiv:2509.24726, 2025. [22] Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, and Sida Wang. Toward training superintelligent software agents through self-play swe-rl. arXiv preprint arXiv:2512.18552, 2025. [23] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [24] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. [25] Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. Next: Teaching large language models to reason about code execution. arXiv preprint arXiv:2404.14662, 2024. [26] Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024. [27] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. [28] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. In Forty-first International Conference on Machine Learning, 2024. [29] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025. Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing [30] Wenyue Hua, Xianjun Yang, Mingyu Jin, Zelong Li, Wei Cheng, Ruixiang Tang, and Yongfeng Zhang. Trustagent: Towards safe and trustworthy llm-based agents through agent constitution. In Trustworthy Multi-modal Foundation Models and AI Agents (TiFA), 2024. [31] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. Advances in neural information processing systems, 36:58202 58245, 2023. [32] Dudley Shapere. The structure of scientific revolutions. The Philosophical Review, 73(3):383394, 1964. [33] Jeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence. arXiv preprint arXiv:1905.10985, 2019. [34] Edward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge Shi, Tom Schaul, and Tim Rocktaschel. Open-endedness is essential for artificial superhuman intelligence. arXiv preprint arXiv:2406.04268, 2024. [35] Minqi Jiang, Tim Rocktäschel, and Edward Grefenstette. General intelligence requires rethinking exploration. Royal Society Open Science, 10(6):230539, 2023. [36] Jenny Zhang, Joel Lehman, Kenneth Stanley, and Jeff Clune. Omni: Open-endedness via models of human notions of interestingness. In The Twelfth International Conference on Learning Representations. [37] Maxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune. Omni-epic: Open-endedness via models In The Thirteenth of human notions of interestingness with environments programmed in code. International Conference on Learning Representations. [38] Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Yang Wang. Swe-search: Enhancing software agents with monte carlo tree search and iterative refinement. In The Thirteenth International Conference on Learning Representations. [39] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: self-referential self-improvement via prompt evolution. In Proceedings of the 41st International Conference on Machine Learning, pages 1348113544, 2024. [40] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Proceedings of the 2023 conference on empirical methods in natural language processing, pages 10511068, 2023. [41] Konstantinos Chatzilygeroudis, Antoine Cully, Vassilis Vassiliades, and Jean-Baptiste Mouret. Qualitydiversity optimization: novel branch of stochastic optimization. In Black box optimization, machine learning, and no-free lunch theorems, pages 109135. Springer, 2021. [42] Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples. In Proceedings of the 41st International Conference on Machine Learning, pages 3430334326, 2024. [43] Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John Lalor, Robin Jia, and Jordan BoydGraber. Evaluation examples are not equally informative: How should that change nlp leaderboards? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 44864503, 2021. 15 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing [44] OpenAI. swe-bench introducing-swe-bench-verified/, August 2024. Accessed: 2025-11-10. Introducing verified. https://openai.com/index/ [45] Marius Hobbhahn. Swe-bench verified mini. https://github.com/mariushobbhahn/ SWEBench-verified-mini, April 2025. Accessed: 2025-11-16. [46] Paul Gauthier. Aider: Ai pair programming in your terminal. https://github.com/Aider-AI/ aider, 2024. Accessed: 2025-11-15. [47] Paul Gauthier. o1 tops aiders new polyglot leaderboard. https://aider.chat/2024/12/21/ polyglot.html, December 2024. Accessed: 2025-11-10. [48] Anthropic. Claude haiku 4.5. https://www.anthropic.com/news/claude-haiku-4-5, 2025. Accessed: 2025-11-10. [49] Anthropic. Claude sonnet 4.5. https://www.anthropic.com/news/claude-sonnet-4-5, 2025. Accessed: 2025-11-10. [50] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [51] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. In The Thirteenth International Conference on Learning Representations. [52] Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. [53] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In 12th International Conference on Learning Representations, ICLR 2024, 2024. Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing A. Appendix A.1 Cost Estimate The primary cost for both GEA and DGM arises from benchmark evaluation. Since we generate the same number of agents for both methods, their overall costs are very similar. Following the settings described in Sections 4.2 and 4.3, the estimated cost of completing full run is approximately USD 13,000 per method on SWE-bench and USD 1,500 on Polyglot. more detailed estimated cost breakdown is provided below: Coding Model Benchmark Number of Tasks Cost Estimate (USD) Claude Sonnet 4.5 SWE-bench Claude Sonnet 4.5 Polyglot Claude Haiku 4.5 SWE-bench Claude Haiku 4."
        },
        {
            "title": "Polyglot",
            "content": "60 60 60 60 $370 $ $120 $20 A.2 Case Study 17 Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing Agent Patch Description docutils_debug Score +0.10 Modified core agent logic +0.10 Updated tests logic TopAdd snippet_extract tool Updating edit tools (v1) Updating edit tools (v2) Updating edit tools (v3) docutils_debug +0. +0.10 +0.01 +0.05 +0.02 +0.10 Modified core agent logic +0. Top-"
        },
        {
            "title": "Updated tests logic",
            "content": "Add snippet_extract tool"
        },
        {
            "title": "Updating edit tools",
            "content": "docutils_debug +0.22 +0.10 +0.01 +0.07 +0. Modified core agent logic +0.10 Top-"
        },
        {
            "title": "Updated tests logic",
            "content": "Add multi-file tools (v1) Add multi-file tools (v2) +0.22 +0.01 +0.07 Table 3: Evolutionary trajectories of the top-3 performing agents discovered by GEA on SWE-bench Verified. Across all three agents, the performance-improving patches primarily focus on enhancing the agents workflows and tool usage, rather than relying on model-specific prompting strategies."
        }
    ],
    "affiliations": [
        "University of California, Santa Barbara"
    ]
}