{
    "paper_title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
    "authors": [
        "Zehong Ma",
        "Longhui Wei",
        "Shuai Wang",
        "Shiliang Zhang",
        "Qi Tian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 5 6 3 9 1 . 1 1 5 2 : r DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation Zehong Ma1,3,, Longhui Wei3,,, Shuai Wang2, Shiliang Zhang1,, Qi Tian3 1 State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University 2Nanjing University 3Huawei Inc. Project Page: https://zehong-ma.github.io/DeCo Figure 1. Qualitative results of text-to-image generation of DeCo. All images are 512512 resolution."
        },
        {
            "title": "Abstract",
            "content": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within single diffusion transformer (DiT). To pursue more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256256) and 2.22 (512512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image *Corresponding authors. Project leader. Work was done during internship at Huawei Inc. model achieves leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo. 1. Introduction Diffusion models [9, 19, 50] have achieved remarkable success in high-fidelity image generation, offering exceptional quality and diversity. Research in this field generally follows two main directions: latent diffusion and pixel diffusion. Latent diffusion models [30, 39, 43, 45] split generation into two stages. VAE first compresses images into compact latent space, and diffusion model operates within this space. However, their performance is largely constrained by the VAEs reconstruction quality and latent distribution [31, 66]. Training VAE often requires adversarial or additional supervision [60], which can be unstable and cause inevitable low-level artifacts. Pixel diffusion avoids these VAE-dependent limitations by directly modeling raw pixels in an end-to-end manner. This enables more optimal distribution learning and eliminates artifacts from imperfect VAE compression. However, it is challenging for pixel diffusion to jointly model complex high-frequency signals and low-frequency semantics within the enormous pixel space. As illustrated in 1 Figure 2. Illustration of our frequency-decoupled (DeCo) framework. In (a), traditional baseline models rely on single DiT to jointly model both low-frequency semantics and high-frequency signals. (b) shows our DeCo framework, where lightweight pixel decoder focuses on the high-frequency reconstruction, and the DiT models low-frequency semantics. As shown in (c), decoupling DiT from modeling high-frequency signals leads to better low-frequency semantic features in DiT Output, and higher image quality. Fig. 2 (a), traditional methods [6, 9, 20, 27, 53] typically rely on single model like diffusion transformer (DiT) to learn these two components from single-scale input for each timestep. The complex high-frequency signals, particularly high-frequency noise, could be hard to learn [32, 49, 62]. They could also distract the DiT from learning low-frequency semantics [32, 34, 58]. As illustrated in Fig. 2 (c), this paradigm leads to noisy DiT outputs and degraded image quality. Previous works [6, 8, 24, 53] demonstrate that its more effective to reconstruct high-frequency signals from highresolution input and model low-frequency semantics from low-resolution input. Other studies [7, 42, 48] show that transformers tend to capture low-frequency semantics well but struggle with high-frequency signals. We thus propose to decouple the generation of high and low frequency components. As illustrated in Fig. 2 (b), DeCo utilizes the DiT to specialize in low-frequency semantic modeling with downsampled inputs. Semantic cues are hence incorporated with lightweight pixel decoder to reconstruct highfrequency signals. In other words, the pixel decoder takes the low-frequency semantics from DiT as condition and predicts pixel velocities with high-resolution input. This new paradigm hence frees the DiT to specialize in modeling semantics, and allows for more specialized details generation. To further emphasize visually salient frequencies and suppress perceptually insignificant high-frequency components, we introduce frequency-aware Flow-Matching (FM) loss inspired by the JPEG [23]. Unlike the standard FM loss, which treats all frequencies equally, our frequency-aware variant transforms the pixel velocity into the frequency domain using Discrete Cosine Transform (DCT) and assigns adaptive weights to each frequency band. The adaptive weights are derived from JPEG quantization tables, which encode robust priors about the visual importance of different frequencies [23]. By emphasizing visually salient frequencies and suppressing high-frequency noise, this loss simplifies the optimization landscape and enhances the visual fidelity. It is worth noting that our motivation largely aligns with the concurrent work JiT [32]. JiT identifies that the high-dimensional or high-frequency noise may distract the model with limited capacity from learning low-dimensional data [32]. JiT predicts the clean image to anchor generation to the low-dimensional data manifold. Our DeCo provides an alternative architectural solution. We introduce lightweight pixel decoder to focus on modeling highfrequency signals with high-dimensional noise, freeing the DiT to learn low-frequency semantics. Our DeCo can also alleviate the negative impact of the high-frequency noise in the clean image, such as camera noise. We have conducted extensive experiments to test the performance of DeCo. It achieves superior results among pixel diffusion models, with FID scores of 1.62 (256256) and 2.22 (512512) on ImageNet, closing the gap with two-stage latent diffusion methods. Our pretrained textto-image model also achieves leading results on GenEval (0.86) and DPG-Bench (81.4) in system-level evaluation. In summary, our contributions can be summarized as two aspects: i) we introduce novel frequency-decoupled framework DeCo for pixel diffusion, where lightweight pixel decoder is proposed to model high-frequency signals, free2 ing the DiT to specialize in low-frequency semantic modeling, and ii) novel frequency-aware FM loss is proposed to prioritize perceptually important frequencies, simplifying the training and improving visual quality. The strong performance verifies the effectiveness of decoupling the modeling of high and low frequency components in pixel diffusion. 2. Related Work This work is closely related to latent diffusion, pixel diffusion, and frequency-decoupled image generation. This section briefly reviews recent works. Latent Diffusion. Latent diffusion trains diffusion models in compact latent space learned by VAE [45]. Compared to raw pixel space, the latent space significantly reduces spatial dimensionality, easing learning difficulty and computational cost [4, 45, 66]. Consequently, VAEs have become fundamental component in modern diffusion models [25, 39, 43, 51, 54, 59, 61, 68]. However, training VAEs often involves adversarial objectives and perceptual supervision, which complicate the overall pipeline [60]. Poorly trained VAEs can produce decoding artifacts [6, 70], limiting the generalization quality of latent diffusion models. Early latent diffusion models mainly used U-Net-based architectures. The pioneering DiT [43] introduced transformers into diffusion models, replacing the U-Net [1, 9]. SiT [39] further validated the DiT with linear flow diffusion. Subsequent works explore enhancing latent diffusion through representation alignment and joint optimization. REPA [67] aligns intermediate features with pretrained DINOv2 [41] model to learn better low-frequency semantics. As representation learning method, REPA is compatible with our framework and is applied in both our baseline and final DeCo. REPA-E [31] attempts to jointly optimize the VAE and DiT in an end-to-end fashion. However, this approach may suffer from training collapse with diffusion loss, as the continually changing latent space leads to unstable denoising targets. In contrast, pixel diffusion denoises in fixed space, ensuring consistent targets and stable training. Pixel Diffusion. Pixel diffusion has progressed much more slowly than its latent counterparts due to the vast dimensionality of pixel space [9, 20, 27]. Early pixel diffusion models typically rely on long residual connections [9, 53], which may hinder scalability [60]. Recent attempts split the diffusion process into chunks at different resolution scales to reduce computational costs [6, 53]. Early methods split the diffusion process into multiple resolution stages [6, 53]. Relay Diffusion [53] trains separate models for each scale, leading to higher cost and two-stage optimization. Pixelflow [6] uses one model across all scales and needs complex denoising schedule that slows down inference. Alternative approaches explore different model architectures. FractalGen [33] builds fractal generative models by recursively applying atomic modules, achieving selfsimilar pixel-level generation. TarFlow [69] introduces transformer-based normalizing flow to directly model and generate pixels. Recent PixNerd [60] employs DiT to predict neural field parameters for each patch, rendering pixel velocities akin to test-time training. JiT [32] predicts the clean image to anchor generation to the low-dimensional data manifold. Frequency-Decoupled Image Generation. Multi-scale cascaded methods [6, 53] can be approximately regarded as form of temporal frequency decoupling, i.e., early steps generate low-frequency semantics, and later steps refine high-frequency details. However, these methods still use single model or architecture to learn all frequencies for each timestep and the high-frequency signals still exist. Highfrequency noise may interfere with low-frequency semantic learning. They also rely on complex denoising schedules and small patch sizes, which reduce training or sampling efficiency. Our DeCo introduces an explicit frequency decoupling in architecture. Instead of separating distinct frequencies across timesteps, DeCo simultaneously processes them within each timestep in an end-to-end manner. Recent two-stage work DDT [61] explores single-scale frequency decoupling in compressed latent space, showing that frequency decoupling remains important even in compressed space. Unlike DDT, our DeCo is multi-scale design for pixel diffusion. Our decoder uses attention-free linear layers instead of DDTs attention-based DiT blocks, making it more suitable for modeling local high-frequency details in large-scale inputs. 3. Method 3.1. Overview This part first reviews the conditional flow matching in baseline pixel diffusion, then proceeds to introduce our frequency-decoupled pixel diffusion framework. Conditional Flow Matching. The conditional flow matching [36, 38] provides continuous-time generative modeling framework that learns velocity field vθ(x, t, y) to transport samples from simple prior distribution (e.g., Gaussian) to data distribution conditioned on the label and time t. Given forward trajectory xt by an interpolation between clean image x0 and noise x1, the objective of conditional flow matching is to match the model-predicted velocity vθ(xt, t, y) to the ground-truth velocity vt: LFM = Ext,t,y vθ(xt, t, y) vt2(cid:105) (cid:104) , (1) where the linear interpolation of trajectory xt is defined as: xt = (1 t) x0 + x1. (2) The ground-truth velocity vt can be derived from t, i.e., 3 Figure 3. Overview of the proposed frequency-decoupled (DeCo) framework. The DiT operates on downsampled inputs to model lowfrequency semantics, while the lightweight pixel decoder generates high-frequency details under the DiTs semantic guidance. the time derivative of xt: vt = t = x1 x0. (3) In the pixel diffusion baseline, the trajectory xt is usually first patchified into tokens by patch embedding layer [6, 43] instead of VAE to downsample the image. In our baseline and DeCo experiments, we use the same patch size of 16 for the DiTs input. In baseline, the patchified trajectory xt is then fed into the DiT to predict the pixel velocity with an unpatchify layer. The DiT is required to simultaneously model both the high-frequency signals and low-frequency semantics. The high-frequency signals, particularly the high-frequency noise, are hard to model, which can distract the DiT from learning low-frequency semantics. DeCo. To separate high-frequency generation from lowfrequency semantic modeling, we propose frequencydecoupled framework DeCo. As illustrated in Fig. 3, the DiT is utilized to generate low-frequency semantics from downsampled low-resolution inputs xt as follows: = θDiT(xt, t, y), (4) where is time and is the label or textual prompt. As depicted in Sec. 3.2, lightweight pixel decoder then takes the low-frequency semantics from DiT as condition to generate additional high-frequency details with fullresolution dense input xt, predicting the final pixel velocity as follows: vθ(xt, t, y) = θDec(xt, t, c). (5) This new paradigm leverages the pixel decoder to generate high-frequency details, freeing the DiT to specialize in modeling semantics. The decoupling disentangles the modeling of different frequencies into different modules, leading to faster training and improved visual fidelity. To further emphasize visually salient frequencies and ignore insignificant high-frequency components, we introduce frequency-aware Flow-Matching (FM) Loss LFreqFM as depicted in Sec. 3.3. This loss reweights different frequency components with the adaptive weights derived from JPEG perceptual priors [23]. Combined with the standard pixel-level flow-matching loss and the REPA [67] alignment loss from the baseline, the final objective can be represented as: = LFM + LFreqFM + LREPA (6) 3.2. Pixel Decoder As illustrated in Fig. 3, the pixel decoder is lightweight attention-free network composed of linear decoder blocks and several linear projection layers. All operations are local and linear, enabling efficient high-frequency modeling without the computational overhead of self-attention. Dense Query Construction. The pixel decoder directly takes the full-resolution noised image as input, without downsampling. All noised pixels are concatenated with their corresponding positional embeddings pos and linearly projected by Win to form dense query vectors h0: h0 = Win(Concat(xt, pos)), (7) where h0 RBHW d, with and denoting the original image height and width (e.g., 256), and representing the hidden dimension of pixel decoder (e.g., 32). See Tab. 4 (c) and (d) for related ablation studies. Decoder Block. For each decoder block, the DiT output is linearly upsampled and reshaped to match the spatial resolution of xt, yielding cup. MLP then generates modulation parameters α, β, γ for adaptive layer norm (AdaLN): α, β, γ = MLP(σ(cup + t)), (8) where the σ is the SiLU activation function. We utilize the AdaLN-Zero [43] to modulate the dense decoder queries in each block as follows: hN = hN-1 + α (MLP(γ hN-1 + β)), (9) where the MLP contains two linear layers with SiLU [10]. Velocity Prediction. Finally, linear projection followed by rearrangement operation maps the decoder output to the pixel space, yielding the predicted velocity vθ(xt, t, y). The velocity encompasses the high-frequency details generated by the pixel decoder and the semantic cues from DiT. 3.3. Frequency-aware FM Loss To further encourage the pixel decoder to focus on perceptually important frequencies and suppress insignificant noise, we introduce frequency-aware flow-matching (FM) loss. SpatialFrequency Transformation. We first transform both the predicted and ground-truth pixel velocities from the spatial domain to the frequency domain. This is done by converting the color space to YCbCr and applying block-wise 88 discrete cosine transform (DCT), following JPEG [23]. Denoting this transformation as , we have: Vθ = (vθ(xt, t, y)), Vt = (vt). (10) Perceptual Weighting. To emphasize visually salient frequencies while suppressing insignificant ones, we employ the JPEG quantization tables [23] as visual priors to generate adaptive weights. Frequencies with smaller quantization intervals are more perceptually important. Thus, we use the normalized reciprocal of the scaled quantization tables Qcur in quality as adaptive weights w, i.e., = 1 ). Qcur When the quality is between 50 and 100, the scaled quantization tables Qcur in quality can be acquired following JPEGs predefined rules [23]: /E( 1 Qcur (cid:18) Qcur = max 1, (cid:22) Qbase (100 q) + 25 50 (cid:23)(cid:19) , (11) where Qbase denotes the standard base quantization tables defined in the JPEG specification [23]. With the adaptive weights w, the frequency-aware FM loss is defined as: LFreqFM = Ext,t,y Vθ Vt2(cid:105) (cid:104) (12) 3.4. Empirical Analysis To verify that DeCo effectively decouples frequencies, we analyze the DCT energy spectra of the DiT outputs and the pixel velocity, as shown in Fig. 4. Compared to the baseline, our pixel decoder successfully maintains all frequency components in pixel velocity. Meanwhile, the DiT Figure 4. DCT energy distribution of DiT outputs and predicted pixel velocities. Compared with baseline, DeCo suppresses highfrequency signals in DiT outputs while preserving strong highfrequency energy in pixel velocity, confirming effective frequency decoupling. The distribution is computed on 10K images across all diffusion steps using DCT transform with 88 block size. outputs in DeCo exhibit significantly lower high-frequency energy than those of the baseline, indicating high-frequency components have been shifted away from the DiT and into the pixel decoder. These observations confirm that DeCo performs effective frequency decoupling. Results from Tab. 4 (c) and (d) further show that this successful decoupling benefits from two key architectural designs. Multi-scale Input Strategy. The multi-scale input strategy is critically important. With this strategy, the pixel decoder can easily model high-frequency signals on high-resolution original inputs, freeing the DiT modeling the low-frequency semantics from low-resolution inputs where high-frequency signals have been partly suppressed. Notably, our DiT uses patch size of 16, which is considerably larger than the patch size of 4 in PixelFlow [6], making it more suitable for capturing semantics instead of details. AdaLN-based Interaction. Adaptive layer norm (AdaLN) provides powerful interaction mechanism between DiT and pixel decoder. In our framework, the DiTs role is analogous to that of text encoder in traditional text-toimage models, providing stable, low-frequency semantic conditioning. The AdaLN layer then uses the DiT output as condition to modulate the dense query features in the pixel decoder. Our experiments confirm that this modulation is more effective for integrating low-frequecy semantics with high-frequency signals than the simpler method, such as upsampling and adding the low-frequency features to their high-frequency counterparts like UNet. Table 1. Comparison with the baseline and other recent methods. Text in gray: latent diffusion models that require VAE. : use 100 steps. Train Inference Generation Metrics Method DiT-L/2 [43] PixDDT [61] PixNerd [60] PixelFlow [6] JiT+REPA [32] Baseline DeCo w/o LFreqFM DeCo Params 458M+ 86M 434M 458M 459M 459M 459M 426M 426M Mem (GB) Speed (s/it) Mem (GB) 0.43 0.22 0.25 1.61 0.23 0.22 0.24 0.24 28.5 23.6 29.2 73.2 24.8 24.8 27.5 27.5 3.5 2.4 2.6 3.9 2.5 2.5 2.4 2.4 1 image 0.63s 0.49s 0.48s 6.61s 0.46s 0.46s 0.46s 0.46s 1 iter 0.013s 0.010s 0.010s 0.066s 0.009s 0.009s 0.009s 0.009s FID 41.93 46.37 37.49 54.33 39.06 61.10 34.12 31. sFID 13.76 17.14 10.65 9.71 11.45 15.86 10.41 9.34 IS 36.52 36.24 43.01 24.67 39.57 16.81 46.44 48.35 Rec.l 0.59 0.63 0.62 0.58 0.63 0.60 0.64 0.65 outperforms the baseline across all metrics while maintaining comparable training and inference costs. Notably, with the frequency-decoupled architecture alone, DeCo w/o LFreqFM lowers FID from 61.10 to 34.12 and raises the IS from 16.81 to 46.44 compared to the baseline. By additionally incorporating frequency-aware FM loss, our DeCo further reduces the FID to 31.35 and achieves consistent gains on other metrics. Compared to the two-stage DiT-L/2, our VAE-free DeCo model demonstrates substantially lower training and inference overhead while achieving comparable performance. Against other pixel diffusion methods, DeCo is more efficient and effective than the multi-scale cascade model PixelFlow [6], which suffers from high computational costs. DeCo also shows superior performance compared to the single-scale attention-based PixDDT [61]. Compared to the recent PixNerd [60], our method achieves better FID with lower training and inference costs. JiT identifies that the high-dimensional noise may distract the model with limited capacity from learning lowdimensional data [32]. To address this, it predicts the clean image and anchors the generation process to the lowdimensional data manifold, successfully reducing the FID from 61.10 to 39.06, as shown in Tab. 1. Our DeCo shares the similar motivation, i.e., preventing high-frequency signals with high-dimensional noise from interfering with the DiTs ability to learn low-frequency semantics. However, our DeCo proposes an alternative architectural solution. We introduce lightweight pixel decoder to focus on modeling high-frequency signals and free the DiT to learn lowfrequency semantics. Our DeCo can also alleviate the negative impact of the high-frequency noise in the clean image, such as camera noise. Consequently, our DeCo achieves superior FID of 31.35 compared to the 39.06 of JiT. 4.2. Class-to-Image Generation Setup. For class-to-image generation experiments on ImageNet, we first train the model at 256256 resolution for 320 epochs. Subsequently, we fine-tune the model for additional 20 epochs at 512512 resolution. During inference, we use 100 Euler steps with CFG [18] and guidance interFigure 5. FID comparison between our DeCo and baseline. DeCo reaches 2.57 FID in 400k iterations, 10 faster than the baseline. 4. Experiments We conduct ablation studies and baseline comparisons on ImageNet 256 256. For class-to-image generation, we provide detailed comparisons on ImageNet 256 256 and 512 512, and report FID [17], sFID [40], IS [47], Precision, and Recall [28]. For text-to-image generation, we report results on GenEval [14] and DPG-Bench [21]. 4.1. Comparison with Baselines Setup. In the baseline comparisons, all diffusion models are trained on ImageNet at 256256 resolution for 200k iterations, using large DiT variant. The key architectural modification from the baseline is the replacement of the final two DiT blocks with our proposed pixel decoder. For inference, we use 50 Euler steps without classifier-free guidance [18] (CFG). We compare against the two-stage DiTL/2 [43] that requires VAE, and other recent pixel diffusion models such as PixelFlow [6] and PixNerd [60]. We also adapt DDT [61] into pixel diffusion to create PixDDT baseline. Besides, we intergrate recent JiT [32] in our baseline with REPA [67] for fair comparison. Please refer to Appendix A.1 for more details. Detailed Comparisons. As detailed in Tab. 1, our DeCo framework, despite having fewer parameters, significantly 6 Figure 6. Qualitative results of class-to-image generation of DeCo. All images are 256256 resolution. Table 2. Class-to-image generation on ImageNet 256256 and 512512 with CFG. DeCo achieves superior performance in end-to-end pixel diffusion and is competitive with two-stage latent diffusion models. Text in gray: latent diffusion models that require VAE. Text in blue background: inference with Heun sampler [16] and 50 sampling steps. Params Epochs NFE Latency(s) FID sFID IS Pre. Rec. DiT-XL/2 [43] SiT-XL/2 [39] REPA-XL/2 [67] ADM [9] RDM [53] JetFormer [57] FractalMAR-H [33] PixelFlow-XL/4 [6] PixNerd-XL/16 [60] Baseline-XL/16 DeCo-XL/16 DeCo-XL/16 DeCo-XL/16 DeCo-XL/16 JiT-H/16 (Heun) [32] DeCo-XL/16 (Heun) DiT-XL/2 [43] SiT-XL/2 [39] ADM-G [9] RIN [22] SimpleDiffusion [70] VDM++ [26] PixNerd-XL/16 [60] DeCo-XL/16 675M + 86M 675M + 86M 675M + 86M 554M 553M + 553M 2.8B 848M 677M 700M 700M 682M 682M 682M 682M 953M 682M 675M + 86M 675M + 86M 554M 320M 2B 2B 700M 682M 6 5 2 6 5 2 2 1 5 2 1 5 1400 1400 800 400 400 - 600 320 320 320 320 600 800 800 600 600 600 400 - 800 800 340 340 2502 2502 2502 250 250 - - 1202 1002 1002 1002 1002 1002 2502 1002 1002 2502 2502 250 250 2502 2502 1002 1002 3.44 3.44 3.44 15.2 38.4 - 155 9.78 1.18 1.03 1.05 1.05 1.05 2.63 - 1.05 11.1 11.1 21.2 - - - 2.47 2.25 2.27 2.06 1.42 4.59 1.99 6.64 6.15 1.98 1.95 2.79 1.90 1.78 1.71 1.62 1.86 1. 3.04 2.62 7.72 3.95 3.54 2.65 2.84 2.22 4.60 4.50 4.70 5.25 3.99 - - 5.83 4.54 4.90 4.47 4.32 4.54 4.41 - 4.59 5.02 4.18 6.57 - - - 5.95 4.67 278.2 284.0 305.7 186.7 260.4 - 348.9 282.1 300 296.0 303 304 304 301 303 304 240.8 252.2 172.7 210.0 205.0 278.1 245.6 290.0 0.83 0.83 0.80 0.82 0.81 0.69 0.81 0.81 0.80 0.79 0.80 0.80 0.80 0.80 - 0. 0.84 0.84 0.87 - - - 0.80 0.80 0.57 0.59 0.64 0.52 0.58 0.56 0.46 0.60 0.60 0.60 0.61 0.61 0.61 0.62 - 0.63 0.54 0.57 0.53 - - - 0.59 0.60 val [29]. Inference latency is measured on single A800 GPU. Please refer to Appendix A.2 for more details. Main Results. Our DeCo achieves leading FID of 1.62 on ImageNet 256256 and 2.22 on ImageNet 512512. At the 256256 resolution, DeCo demonstrates remarkable inference efficiency. It generates an image in just 1.05s with 100 inference steps, whereas RDM [53] requires 38.4s and PixelFlow [6] needs 9.78s. In terms of training efficiency, as shown in Tab. 1, single iteration for our model takes only 0.24s, far less than PixelFlows 1.61s. When trained Table 3. Text-to-image generation on GenEval [14] and DPG-Bench [21] at 512512 resolution. Method PixArt-α [3] SD3 [11] FLUX.1-dev [30] DALL-E 3 [2] BLIP3o [5] OmniGen2 [64] PixelFlow-XL/4 [6] PixNerd-XXL/16 [60] DeCo-XXL/16 Diffusion Params GenEval Sin.Obj. Two.Obj Counting Colors Pos Color.Attr. Overall DPG-Bench Average 0.6B 8B 12B - 4B 4B 882M 1.2B 1.1B 0.98 0.98 0.99 0.96 - 1 - 0.97 1 0.50 0.84 0.81 0.87 - 0.95 - 0.86 0.92 0.44 0.66 0.79 0.47 - 0.64 - 0.44 0.72 0.80 0.74 0.74 0.83 - 0.88 - 0.83 0. 0.08 0.40 0.20 0.43 - 0.55 - 0.71 0.80 0.07 0.43 0.47 0.45 - 0.76 - 0.53 0.79 0.48 0.68 0.67 0.67 0.81 0.80 0.60 0.73 0.86 71.1 - 84.0 83.5 79.4 83.6 - 80.9 81.4 Table 4. Ablation experiments on architecture design and hyper-parameters. (a) Hidden Size of Pixel Decoder. (b) Depth of Pixel Decoder. (c) Patch Size of Pixel Decoder Channel 16 32 64 FID 37.63 34.12 35.88 sFID 10.64 10.41 10.79 IS 41.54 46.44 44. Depth 1 3 6 FID 37.10 34.12 35.46 sFID 10.73 10.41 10.82 IS 41.06 46.44 44.60 Patch Size 1 4 16 FID 31.35 34.39 55. sFID 9.34 11.15 44.16 IS 48.35 45.53 34.44 (d) Interaction of DiT and Pixel Decoder. (e) Loss Weight of LFreqFM (f) JPEG Quality in LFreqFM. Interaction AdaLN Add FID 31.35 36.02 sFID 9.34 9.99 IS 48.35 41.74 Weight 0.5 1 2 FID 33.54 31.35 32.97 sFID 10.27 9.34 9. IS 46.38 48.35 46.55 Quality 50 85 100 FID 31.54 31.35 33.84 sFID 9.45 9.34 10.74 IS 47.70 48.35 46.14 for the same 320 epochs, our models FID (1.90) is substantially lower than the baselines 2.79 and surpasses the recent PixelFlow and PixNerd. As illustrated in Fig. 5, DeCo achieves FID of 2.57 in just 80 epochs (400k iterations), which exceeds the baselines FID at 800 epochs, marking 10 improvement in training efficiency. After 800 training epochs, our DeCo achieves superior FID of 1.62 with 250 sampling steps across pixel diffusion models, which is even comparable to the two-stage latent diffusion models. Using the same heun [16] sampler and 50-step inference at 600 epochs, DeCo reaches an FID of 1.69, outperforming JiTs FID of 1.86 with fewer parameters and FLOPs. At the 512512 resolution, our DeCo model substantially outperforms existing pixel-based diffusion methods, setting leading FID of 2.22. Moreover, by fine-tuning our ImageNet 256256 model at 320 epochs for 20 additional epochs following PixNerd [60], our FID and IS are comparable to those of DiT-XL/2 [43] and SiT-XL/2 [39] after 600 training epochs. The entire training takes about 6 days on 8 H800 GPUs. Please refer to Appendix A.3 for more training details. Main Results. Compared to two-stage latent diffusion methods, our DeCo achieves an overall score of 0.86 on the GenEval benchmark [14]. This result outperforms prominent text-to-image models such as SD3 [11] and FLUX.1dev [30], as well as unified models including BLIP3o [5] and OmniGen2 [64]. Notably, our model achieves superior performance despite using the same training data as BLIP3o. On DPG-Bench [21], DeCo delivers competitive average score comparable to two-stage latent diffusion methods. When compared to other end-to-end pixel diffusion methods, DeCo achieves significant performance advantage over PixelFlow and PixNerd. These results show that end-to-end pixel diffusion, as implemented in our DeCo, can achieve performance comparable to that of twostage methods with limited training and inference costs. Visualizations of images generated by our text-to-image DeCo can be found in Fig. 1. Please see Appendix for prompts. 4.3. Text-to-Image Generation 4.4. More Ablations Setup. For text-to-image generation, we trained our model on the BLIP3o [5] dataset, which contains approximately 36M pretraining images and 60k high-quality instructiontuning data. We adopt Qwen3-1.7B [65] as the text encoder. This section presents ablation studies on the pixel decoder design, the interaction mechanism between the DiT and the pixel decoder, and the hyperparameters of the frequencyaware FM loss. All experiments follow the setup in Sec. 4.1. Hidden Size of Pixel Decoder. As shown in Tab. 4 (a), DeCo achieves the best performance when the hidden size is set to 32. Smaller sizes limit model capacity, while larger sizes offer no further gains. Thus, we use hidden size of 32 by default. Depth of Pixel Decoder. In Tab. 4 (b), 3-layer decoder achieves the best results. single layer lacks capacity, whereas 6-layer design may introduce optimization difficulties. With hidden size of 32 and 3 layers, our attentionfree decoder is lightweight (8.5M parameters) and efficient for high-resolution inputs. Patch Size of Pixel Decoder. As shown in Tab. 4 (c), DeCo performs best when the decoders patch size is set to 1, enabling direct processing of the full-resolution input. Patchifying the decoders input degrades results, with the worst performance observed when using large patch size of 16 like DiT. This demonstrates the effectiveness of our multiscale input strategy. All comparisons use similar parameter counts and computational costs. Interaction between DiT and Pixel Decoder. Tab. 4 (d) shows that simply upsampling DiT outputs and adding them to dense decoder features, as done in UNet [46], underperforms compared to AdaLN-based interaction. AdaLN [43] provides more effective interaction mechanism, using the DiT output as semantic condition for velocity prediction. Loss Weight. In Tab. 4 (e), the loss weight of 1 for LFreqFM gives the best results, which we adopt as the default setting. JPEG Quality in LFreqFM. In Tab. 4 (f), we study the effect of the JPEG quality factor in LFreqFM. With quality of 100 (lossless compression), all frequency components are equally weighted, yielding an FID of 33.84, close to 34.12 without LFreqFM as expected. The commonly used quality of 85 performs best, emphasizing the important frequencies while slightly downweighting insignificant ones for optimal balance. Reducing quality to 50 overly suppresses highfrequency signals, slightly harming performance. Therefore, we use JPEG quality of 85 in all experiments. 5. Conclusions We introduced DeCo, novel frequency-decoupled framework for pixel diffusion. By separating the modeling of low-frequency semantics with DiT and high-frequency signals with lightweight pixel decoder, DeCo significantly improves generation quality and efficiency. Our proposed frequency-aware FM loss further enhances visual quality by prioritizing perceptually important frequencies. DeCo achieves leading performance in pixel diffusion on both class-to-image and text-to-image generation benchmarks, closing the gap with two-stage latent diffusion methods. 9 DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. More Implementary Details A.1. Baseline Comparisons In this subsection, we summarize the settings used for all baseline comparisons. In the baseline comparisons, all diffusion models are trained on ImageNet at 256256 resolution for 200k iterations using large DiT variant. Following previous works [43, 60], we use global batch size of 256 and the AdamW optimizer with constant learning rate of 1e-4. Both baseline and DeCo adopt SwiGLU [55, 56], RoPE2d [52], and RMSNorm, and are trained with lognorm sampling and REPA [67]. The patch size of DiTs input is set to 16 for both baseline and our DeCo. The patch size of pixel decoder is set to 1. Our main architectural change on the baseline is to replace the final two DiT blocks of the baseline with our proposed pixel decoder. For inference, we use 50 Euler steps without classifierfree guidance [18] (CFG) for all models except PixelFlow [6], which requires 100 steps. We also report results for the two-stage DiT-L/2 that requires VAE and the recent pixel diffusion models PixelFlow [6] and PixNerd [60]. For fair comparison, we further integrate DDT [61] into the pixel diffusion to form PixDDT, which has the similar parameter counts and computation FLOPs to our DeCo. Training memory and speed are measured with batch size of 256 on 8A800 GPUs, while inference memory and time are measured on single A800 with batch size of 1. A.2. Class-to-Image Generation This subsection describes the more implementation details for class-to-image generation. The batch size and learning rate follow the default settings previously described. We use global batch size of 256 and the AdamW optimizer with constant learning rate of 1e-4. We set the CFG scale to 3.2 for the 256 256 resolution (320 epochs) and 5.0 for the 512 512 resolution (340 epochs). The CFG scale is set to 3.0 for the model of 800 epochs at 256 256 resolution. The guidance interval [29] is set to 0.1 following previous work [60]. A.3. Text-to-Image Generation We adopt Qwen3-1.7B [65] as the text encoder. To improve the alignment of frozen text features [12], we jointly train several transformer layers on the frozen text features similar to Fluid [12]. The total batch size is 1536 for 256 256 resolution pretraining and 512 for 512 512 resolution pretraining. Following PixNerd [60], we pretrain DeCo on 256 256 resolution for 200K steps and pretrain on 512 512 resolution for 80K steps. We further fine-tune architecture DiT depth hidden dim heads params decoder depth decoder hidden dim patch size image size training optimizer batch size learning rate lr schedule weight decay ema decay time sampler noise scale sampling ODE solver ODE steps time steps CFG scale range CFG interval [29] DeCo-L DeCo-XL DeCo-XXL 22 1024 16 426M 28 1152 16 682M 3 32 16 16 1536 24 1.1B 256 (other settings: 512) AdamW [37], β1, β2 = 0.9, 0.999 256 1e-4 constant 0 0.9999 logit(t)N (µ, σ2), µ = 0, σ = 1 1.0 Euler 100 linear in [0.0, 1.0] [3.0-3.2] (256256), [4.5-5.0] (512512) [0.1, 1] Table 5. Configurations of experiments. the pretrained DeCo on BLIP3o-60k with 40k steps at the 512 512 resolution following PixNerd. We adopt the gradient clip to stabilize training. The whole training only takes about 6 days on 8 H800 GPUs. We use the Adams-2nd solver with 25 steps as the default choice for sampling. The cfg scale is set to 4.0. We leave the native resolution [63] or native aspect training [13, 15, 35] as future works. A.4. Experiment Configurations Table 5 summarizes the experiment configurations for DeCo-L/16, DeCo-XL/16, and DeCo-XXL/16. In practice, we follow the training setups from previous works such as DiT [43], SiT [39], and PixNerd [60]. Besides, we sweep the CFG scale within the given ranges using an interval of 0.1. B. Text-to-Image Prompts Below, we list the prompts used for text-to-image generation in Fig. 1. These prompts cover mix of animals, people, and scenes to evaluate semantic understanding and visual detail generation. 1 lovely horse stands in the bedroom. baby cat stands on two legs, wearing chothes. cyberpunk woman with glowing tattoos and mechanical arm beneath holographic sky. man sipping coffee on sunny balcony filled with potted plants, wearing linen clothes and sunglasses, basking in the morning light. beautiful woman. cute panda is wielding sword in realistic style. An extremely happy American Cocker Spaniel is smiling and looking up at the camera with his head tilted to one side. raccoon wearing detectives hat, observing something with magnifying glass. Close-up of an aged man with weathered features and sharp blue eyes peering wisely from beneath tweed flat cap. C. Quantization Tables In our DeCo, we use the normalized reciprocal of scaled JPEG quantization tables as adaptive weights to emphasize different frequency components. These tables are core component of the JPEG compression standard and are designed based on properties of the human visual system (HVS) [44]. As shown in Sec. 3.3, quantization table is an 88 matrix that determines the compression level for each frequency coefficient after the Discrete Cosine Transform (DCT). The JPEG standard uses two separate tables: one for the luminance (Y) component and another for the chrominance (Cb/Cr) components. This design is based on key characteristics of human perception. The core principle is that the human eye is not equally sensitive to all visual information. Specifically, two HVS properties are crucial. Firstly, the human eye is much more sensitive to lowfrequency components than to high-frequency components. Secondly, the eye is more sensitive to changes in brightness (luminance) than in color (chrominance) [44]. Based on extensive experiments, the standard base quantization tables Qbase in Fig. 7 were developed to reflect these properties [44]. These tables have smaller values (finer quantization intervals) for low-frequency coefficients, which are perceptually more important. Conversely, these tables have larger values (coarser quantization intervals) for high-frequency coefficients, as the resulting information loss is less noticeable to the human eye. Similarly, the luminance table generally has smaller values than the chrominance table. These base tables can be scaled using quality 2 Figure 7. Base and Scaled Quantization Tables. factor to create new scaled quantization tables Qcur for different compression levels. Since smaller quantization step implies that frequency component is more significant to human perception, we use the normalized reciprocal of the scaled quantization tables as adaptive weights, i.e., with normalization. This allows us to assign higher weight to the frequency components that are visually more important in our frequency-aware flow-matching loss LFreqFM. 1 Qcur D. Pseudocodes for DeCo D.1. Training Step of DeCo In Algorithm 1, we provide the pseudocodes for the training step of DeCo. DeCo utilizes the DiT to specialize in low-frequency semantic modeling with downsampled small-scale inputs xt. Semantic cues are hence incorporated with lightweight pixel decoder to reconstruct highfrequency signals. In other words, the pixel decoder takes the low-frequency semantics from DiT as condition and predicts pixel velocities vθ with high-resolution input xt. This new paradigm hence frees the DiT to specialize in modeling semantics, and allows for more specialized details generation. To emphasize visually salient frequencies and suppress perceptually insignificant high-frequency components, we further introduce frequency-aware FlowMatching loss LFreqFM inspired by the JPEG [23]. REPA [67] loss is used in both our Baseline and DeCo. D.2. K-Means Visualization in Fig. 2 (c) In Algorithm 2, we provide the pseudocodes for the KMeans visualization in Fig. 2 (c). The number of clusters in K-Means is set to 8. We uniformly select 4 timesteps from the sampling process and visualize the clustering results at Algorithm 1 Training step Algorithm 3 DCT Spectral Analysis # V: Predicted velocity (B * T, ori_H, ori_W, C) # Feats: DiT outputs (B * T, H, W, C) # Pre-compute frequency scan order # The block size of DCT is set to 8 idx = ZigZagIndices(block_size=8) F_energy, V_energy = 0, 0 F_num_blocks, V_num_blocks =0, 0 for f, in (Feats, V): # Split input into patches f_patches = Unfold(f, kernel_size=8) v_patches = Unfold(v, kernel_size=8) # 2D Discrete Cosine Transform f_freq = DCT2D(f_patches) f_energy = f_freq ** 2 v_freq = DCT2D(v_patches) v_energy = v_freq ** 2 # Accumulate energy following ZigZag order # Maps 2D (u,v) to 1D frequency index F_energy += Sum(f_energy.reorder(idx)) F_num_blocks += len(f_patches) V_energy += Sum(v_energy.reorder(idx)) V_num_blocks += len(v_patches) # Log-scale Normalization Feats_S = log(1 + F_energy / F_num_blocks) Feats_S = Feats_S / max(Feats_S) V_S = log(1 + V_energy / V_num_blocks) V_S = V_S / max(V_S) plot(Feats_S) plot(V_S) normalization to rescale all energies to the range [0, 1] for comparison. As demonstrated in Fig. 4, compared with baseline, DeCo suppresses high-frequency signals in DiT outputs while preserving strong high-frequency energy in pixel velocity, confirming effective frequency decoupling. The distribution is computed on 10K images across all diffusion steps, i.e., B=10,000 and =100. E. More Visualizations In this section, we provide more visualizations, including text-to-image generation in Fig. 8, class-to-image generation at 256256 resolution in Fig. 9, and class-to-image generation at 512 512 resolution in Fig. 10. Our DeCo supports multiple languages with the Qwen3 text encoder after pretraining on the BLIP3o dataset [5], such as Chinese, Japanese, and English. # θDiT: DiT network # θDec: Pixel Decoder network # x0: training batch # y: class label or textual prompt # Qcur: scaled quantization tables in quality 85. # Prepare inputs = sample t() x1 = randn like(x0) xt = (1-t)x0 + tx1 # original scale xt = patchify(xt, patch_size=16) # small-scale # Prepare ground-truth velocities vt = x1 - x0 Vt = DCT2D(RGB2YCbCr(vt)) # Generate low-frequency semantic condition = θDiT(xt, t, y) # Predict velocity conditioned on vθ = θDec(xt, t, c) Vθ = DCT2D(RGB2YCbCr(vθ)) # Compute Loss FM_loss = mean(vθ vt2) = 1/Qcur = / w.mean() # normalized adaptive weights FreqFM_loss = mean(w * Vθ Vt2) loss = FM_loss + FreqFM_loss + REPA_loss Algorithm 2 K-Means Visualization in Fig. 2 (c) # Feats: DiT outputs (T, H, W, C) # I: generated images (T, H, W, 3) # T: Sampling steps for in T: # Flatten spatial dimensions = Feats[t].reshape(-1, C) # Cluster pixel-wise features labels = kmeans(f, n_clusters=8) # Map clusters to visualization vis = colormap(labels).reshape(H, W) plot(vis, I[t]) these timesteps. D.3. DCT energy distribution in Fig. 4 In Algorithm 3, we provide the pseudocodes for DCT spectral analysis of Fig. 4. We apply an 88 DCT to transfrom the DiT outputs and pixel velocities into frequency domain. Each 88 patch yields 64 frequency coefficients, which are then converted into energy via square operation. We reorder these energies from low to high frequency using standard zigzag indexing, where lower indices correspond to lower-frequency components. Finally, we apply log-scale 3 Figure 8. More Qualitative results of text-to-image generation at 512512 resolution. Our DeCo supports multiple languages with the Qwen3 text encoder, such as Chinese, Japanese, and English. 4 Figure 9. More qualitative results of class-to-image generation at 256256 resolution. 5 Figure 10. Qualitative results of class-to-image generation at 512512 resolution."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2266922679, 2023. 3 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. OpenAI Technical Report, 2023. 8 [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 8 [4] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. 3 [5] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 8, 3 [6] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. 2, 3, 4, 5, 6, 7, 8, 1 [7] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. In The Eleventh International Conference on Learning Representations, 2023. [8] Emily Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using laplacian pyramid of adversarial networks. Advances in neural information processing systems, 28, 2015. 2 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1, 2, 3, 7 [10] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:311, 2018. 5 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 8 [12] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 1 [13] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. [14] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 6, 8 [15] Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. 1 [16] Karl Heun et al. Neue methoden zur approximativen integration der differentialgleichungen einer unabhangigen veranderlichen. Z. Math. Phys, 45:2338, 1900. 7, 8 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6, 1 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [20] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution imIn International Conference on Machine Learning, ages. pages 1321313232. PMLR, 2023. 2, 3 [21] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 6, 8 Ella: [22] Allan Jabri, David Fleet, and Ting Chen. Scalable adaparXiv preprint tive computation for iterative generation. arXiv:2212.11972, 2022. 7 [23] Joint Photographic Experts Group. Information technology digital compression and coding of continuous-tone still images: Requirements and guidelines. Technical Report ITU-T T.81, International Telecommunication Union (ITUT), 1992. 2, 4, 5 [24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. [25] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the In Proceedings of training dynamics of diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. 3 [26] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36: 6548465516, 2023. 7 [27] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36: 6548465516, 2023. 2, 3 7 [28] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 6 [29] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 7, [30] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 8 [31] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. 1, 3 [32] Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise, 2025. 2, 3, 6, 7 [33] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. 3, [34] Ziqiang Li, Pengfei Xia, Xue Rui, and Bin Li. Exploring the effect of high-frequency components in gans training. ACM Trans. Multimedia Comput. Commun. Appl., 19(5), 2023. 2 [35] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model arXiv preprint for interleaved multi-modal generation. arXiv:2505.05472, 2025. 1 [36] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 3 [37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 1 [38] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference, 2024. 3 [39] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 1, 3, 7, [40] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. 6 [41] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [42] Namuk Park and Songkuk Kim. How do vision transformers work? In International Conference on Learning Representations, 2022. 2 [43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 3, 4, 5, 6, 7, 8, 9 [44] William Pennebaker and Joan Mitchell. JPEG: Still image data compression standard. Springer Science & Business Media, 1992. 2 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, [46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 9 [47] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6 [48] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng Yan. Inception transformer. Advances in Neural Information Processing Systems, 35:2349523509, 2022. 2 [49] Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and AliImproving the diffusability of autoenaksandr Siarohin. In Forty-second International Conference on Macoders. chine Learning, 2025. 2 [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, 2020. [51] Tianhui Song, Weixin Feng, Shuai Wang, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Dmm: Building versatile image generation model via distillation-based model merging. arXiv preprint arXiv:2504.12364, 2025. 3 [52] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 1 [53] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. 2, 3, 7 [54] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. 3 [55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [57] Michael Tschannen, Andre Susano Pinto, and Alexander Kolesnikov. Jetformer: An autoregressive generative model of raw images and text. arXiv preprint arXiv:2411.19722, 2024. 8 [58] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric Xing. High-frequency component helps explain the generalizaIn Proceedings of tion of convolutional neural networks. the IEEE/CVF conference on computer vision and pattern recognition, pages 86848694, 2020. 2 [59] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Exploring dcn-like architecture for fast image generation with arbitrary resolution. Advances in Neural Information Processing Systems, 37:8795987977, 2024. 3 [60] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. 1, 3, 6, 7, 8 [61] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. 3, 6, 1 [62] Zhe Wang, Ziqiu Chi, Yanbing Zhang, et al. Fregan: Exploiting frequency components for training gans under limited data. Advances in Neural Information Processing Systems, 35:3338733399, 2022. [63] Zidong Wang, Lei Bai, Xiangyu Yue, Wanli Ouyang, and Yiyuan Zhang. Native-resolution image synthesis. arXiv preprint arXiv:2506.03131, 2025. 1 [64] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 8 [65] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 8, 1 [66] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. 1, 3 [67] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 3, 4, 6, 7, 1, 2 [68] Xiaoyu Yue, Zidong Wang, Zeyu Lu, Shuyang Sun, Meng Wei, Wanli Ouyang, Lei Bai, and Luping Zhou. Diffusion models need visual priors for image generation. arXiv preprint arXiv:2410.08531, 2024. [69] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024. 3 [70] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 3,"
        }
    ],
    "affiliations": [
        "Huawei Inc.",
        "Nanjing University",
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}