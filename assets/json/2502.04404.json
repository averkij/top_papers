{
    "paper_title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
    "authors": [
        "Xiao-Wen Yang",
        "Xuan-Yi Zhu",
        "Wen-Da Wei",
        "Ding-Chu Zhang",
        "Jie-Jing Shao",
        "Zhi Zhou",
        "Lan-Zhe Guo",
        "Yu-Feng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 4 0 4 4 0 . 2 0 5 2 : r Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Xiao-Wen Yang 1 2 Xuan-Yi Zhu 1 2 Wen-Da Wei 1 2 Ding-Chu Zhang 1 2 Jie-Jing Shao 1 Zhi Zhou 1 Lan-Zhe Guo 1 3 Yu-Feng Li 1 2 1 National Key Laboratory for Novel Software Technology, Nanjing University, China. 2 School of Artificial Intelligence, Nanjing University, China 3 School of Intelligence Science and Technology, Nanjing University, China {yangxw,zhuxy,weiwd,zhangdc,shaojj,zhouz,guolz,liyf}@lamda.nju.edu"
        },
        {
            "title": "Abstract",
            "content": "The integration of slow-thinking mechanisms into large language models (LLMs) offers promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAIs o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs inability to internalize the search process, key component of effective reasoning. critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, fundamental operation in traditional search algorithms. To this end, we propose self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fastthinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving performance gain of over 40% compared to the optimal-path supervised fine-tuning method. We believe this study introduces novel and promising pathway for developing more advanced and robust Reasoners. The code is available at https://github.com/ LAMDASZ-ML/Self-Backtracking. 1. Introduction The incorporation of slow-thinking mechanisms has become pivotal path for large language models (LLMs) to attain 0OpenAI has proposed five steps towards AGI: Chatbots, Reasoners, Agents, Innovators, and Organizations. Figure 1. Comparison between o1-like models and our selfbacktracking approach. O1-like models are prone to inefficient overthinking and exhibit overreliance on reward model. Level 2 AGI Reasoners 1. Notable advancements in this domain include OpenAIs o1 and o3 models (OpenAI, 2024), which have spurred the development of deep-thinking models such as DeepSeek R1 (Guo et al., 2025) and Qwen QwQ (Qwen, 2024). Representative techniques employ reinforcement learning to autonomously acquire deep thinking capabilities, enabling LLMs to engage in self-reflection and self-correction, analogous to the search mechanism. Nevertheless, do these techniques represent the ultimate paradigm for Reasoners? Our analysis uncovers several critical challenges in current approaches: 1) O1-like models frequently suffer from inefficient overthinking, even for simple problems (Chen et al., 2024b), resulting in considerable computational resource wastage; 2) Some o1-like methods (Choi et al., 2023; Zhang et al., 2024a; Qin et al., 2024) exhibit heavy reliance on auxiliary reward models for state evaluation, which not only incurs significant inefficiencies but also risks reward hacking (Chen et al., 2024a). The core issue lies in the fact that LLMs have yet to interStep Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models nalize the capability of search. Currently, search functions more as an external component (Gandhi et al., 2024; Zhang et al., 2024a), integrated with LLMs only at superficial level, with limited fusion between the two. This externalized approach constrains the performance of LLMs in more complex reasoning tasks, while simultaneously inducing excessive thinking in simpler tasks. As the saying goes, To err is human, to backtrack divine. Backtracking (Van Beek, 2006) plays critical role in many efficient search algorithms, particularly in reasoning problems, where most classical search algorithms employ backtracking strategies. Through backtracking, algorithms can learn from suboptimal paths, reassess, and seek optimal solutions. Therefore, we have strong reasons to believe that if LLMs could internalize the backtracking mechanism, they would potentially mitigate overthinking and reduce reliance on external reward models, paving the way toward becoming stronger reasoners. In this paper, we propose novel Self-Backtracking technique that equips language models with the ability to learn when and where to perform backtracking during the training phase. Specifically, the model is trained to recognize when its initial predictions or reasoning paths are suboptimal and to backtrack to earlier states in order to explore alternative possibilities. During the testing phase, the model leverages this learned backtracking capability to conduct dynamic search processes, systematically revisiting prior actions and exploring multiple reasoning paths. The model ultimately utilizes the improved results for expert iteration to achieve self-improvement. This process facilitates transition from slow thinking to fast thinking, significantly enhancing the models capability for fast reasoning in complex tasks. Our proposed method effectively addresses the limitations of o1-like models through the internalization of the backtrack process within the LLMs. Firstly, the model intelligently avoids unnecessary backtracking in simpler problems by learning optimal conditions for backtracking, thereby effectively mitigating the risk of overthinking. Secondly, it implicitly integrates the state evaluation mechanism within the model itself, obviating the need for external reward models. We compare o1-like models and our method in Figure 1. Experiments are conducted on the Countdown task (Gandhi et al., 2024) to evaluate our proposed methods advantages across models of different parameter scales. Our method demonstrates an accuracy enhancement exceeding 40% compared to the SFT approach that solely relies on the optimal reasoning solutions. Results show that the selfbacktracking technique significantly enhances the models reasoning flexibility and overall performance while exhibiting test-time scaling capabilities. We believe it provides the potential to make advancement toward achieving Level 2 AGI Reasoners. Our contributions are summarized as follows: Problem: Existing slow thinking techniques face significant challenges, including inefficient overthinking and excessive reliance on auxiliary reward models. We highlight that enabling LLMs to internalize the search process is promising direction for enhancing LLMs reasoning capabilities. Method: We introduce Self-Backtracking, novel technique that enables LLMs to internalize the backtracking ability during both training and inference. This approach not only mitigates inefficient overthinking and reduces dependencies on external reward models but also enhances reasoning efficiency by transforming slow-thinking processes into fast-thinking capabilities through self-improvement. Evaluation: Extensive experimental results on Countdown demonstrate that the proposal can significantly enhance LLMs reasoning performances, achieving performance gain of over 40% compared to the optimalpath SFT method. 2. Related Work Learn from Search Trajectories Recently, several studies have explored using symbolic search algorithms to construct trajectory data and train transformer models to learn these search strategies, with the aim of enabling models to perform reasoning tasks. For instance, Yang et al. (2022) employs Monte Carlo Tree Search (MCTS) or BFS to construct reasoning trajectories. Searchformer (Lehnert et al., 2024) and DualFormer (Su et al., 2024) utilize traces from A* search to train language models, with each trace containing state information, A* heuristic values, and search history. Stream of Search (SoS) (Gandhi et al., 2024) constructs trajectories using various search algorithms to help language models learn the commonalities across different search strategies, facilitating the discovery of new search strategies. GSoS (Moon et al., 2024) further extends SoS by integrating optimal solutions into the process of learning search trajectories. DeepSeek R1 (Guo et al., 2025) employs end-to-end reinforcement learning to autonomously acquire the capability of search in language. However, training these model to learn exploration trajectories may conflict with guiding it to generate optimal trajectories, leading to inefficient overthinking when solving easy problems. Learn from Mistakes Numerous recent studies have focused on exploring whether language models possess the ability to learn from their previous mistakes and subsequently correct them. One line of techniques (An et al., 2023; Tong et al., 2024; Zhang et al., 2024b) introduces the external verifier to evaluate the reasoning paths generated by LLMs. This evaluation is then used to construct preference training data for RLHF, with training conducted 2 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models using algorithms such as PPO (Schulman et al., 2017) or DPO (Rafailov et al., 2023), enabling self-improvement (Yuan et al., 2024) of the models. Another line of techniques (Cundy & Ermon, 2024; Zhang et al., 2024c; Ye et al., 2024) involves pre-annotating error examples, allowing models to identify whether their current outputs contain issues and adaptively perform backspace during testing to regenerate content. In this paper, our method shows an inherent ability to learn from mistakes and achieves further self-improvement by learning from its search paths. Inference Strategies of LLM Reasoning Many strategies for the inference phase have been proposed to enhance the reasoning capabilities of LLMs. Classical methods such as greedy decoding, beam search (Teller, 2000; Graves, 2012), and majority voting (Wang et al., 2022) have been widely adopted. Additionally, Best-of-N (BoN) (Li et al., 2022) is typical algorithm that generates complete answers through sampling and selects the optimal one based on the evaluation of reward model. Recently, approaches that combine search algorithms with LLMs, such as best-first search (Yao et al., 2024), guided beam search (Xie et al., 2024b) and MCTS (Choi et al., 2023; Wan et al., 2024; Zhang et al., 2024a; Xie et al., 2024a) have gained increasing attention due to their inference scaling law (Wu et al., 2024; Snell et al., 2024). These methods often require additional components such as verifier, outcome reward model (ORM) (Lightman et al., 2024), or process reward model (PRM) (Lightman et al., 2024; Wang et al., 2024), which increases computational cost. This paper proposes novel method that integrates the verifier within the model to save computational resources while leveraging the advantages of search algorithms, demonstrating scalability in reasoning. 3. Preliminary 3.1. Problem Setup We adopt the formal definition of reasoning problems as proposed in Stream of Search (Gandhi et al., 2024), where reasoning problem is modeled as Markov Decision Process (MDP). An MDP is characterized by the following components: state set S, representing all possible states within the problem domain; an action set A, denoting all permissible actions; transition function : S, which defines how states transition based on actions; and reward function : R, which assigns numerical reward to each state. typical reasoning task involves an initial state s0 and goal state sg S, where the goal state sg is associated with reward of 1 (R(sg) = 1), while all other states yield reward of zero (R(s) = 0, = sg). The solution to the reasoning problem is represented as trajectorya sequence of states and actions(s0, a0, s1, a1, . . . , sg1, ag1, sg), Algorithm 1 General Backtracking Algorithm Input: Reasoning problem initial state s0 Output: Solution path P, or None if no solution exists Function Backtracking(s0): Initialize [s0] while True do last state in if == sg then return next candidate action (s) (s, a) if valid state (s) then Append (a, s) to else"
        },
        {
            "title": "Remove last state from P\nif P is empty then\nreturn None",
            "content": "continue where each successive state si+1 is determined by the preceding state si and valid action ai via si+1 = (si, ai). The trajectory must terminate at the goal state sg. Let Σ represent an alphabet, which is finite, non-empty set of symbols. string is defined as finite sequence of symbols drawn from Σ. In this work, we focus on reasoning tasks that can be expressed purely in the form of language, where both the state set and the action set consist of strings. Each state could represent an intermediate conclusion or partial solutions and each action represents permissible operation that can be performed on the current state to advance the reasoning process. The transition function is defined as (s, a) = a, where denotes string concatenation. 3.2. Backtracking Based on the formalization above, we can extend the stateaction pairs into tree, allowing the search process on the tree to be naturally integrated. Backtracking is classic searching technique that incrementally constructs solution by exploring various options and retracting decisions when encountering dead end. This approach is particularly effective in scenarios that require exploring multiple possibilities to solve reasoning problem, such as navigating maze or solving puzzles like Sudoku. When the algorithm encounters dead end, it backtracks to the previous decision point to explore alternative paths, continuing this process until solution is found or all possibilities are exhausted. Backtracking forms the foundation for many well-known algorithms, including DFS, BFS and MCTS. The general backtracking algorithm is summarized in Algorithm 1. This study focuses on enabling LLMs to learn backtrack3 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Figure 2. The overall framework of Self-Backtracking. During the training phase, the language model is instructed on when and where to backtrack. The inference phase employs backtracking algorithm that considers both depth and breadth. The self-improvement phase utilizes expert iteration to enhance the fast thinking capabilities of the model. ing. We identify two core components in general backtracking algorithms: next candidate action and valid state. While the former can be effectively implemented through LLMs sampling mechanisms, as demonstrated in numerous studies (Wan et al., 2024; Polu & Sutskever, 2020), the latter plays crucial role in evaluating state validity. Traditional search algorithms determine backtracking by identifying terminal states, whereas other approaches, such as A* and MCTS, employ heuristic evaluations to enable early backtracking and accelerate search processes. In this work, we aim to internalize backtracking capabilities within LLMs without relying on external tools (e.g., symbolic verifiers) or models (e.g., reward models) to approximate valid state. Furthermore, we hope to enhance existing backtracking frameworks through parallel processing to expand search spaces and improve efficiency. 4. Self-Backtracking in Language Models In this section, we introduce our self-backtracking technique. First, during the training phase, we design specific optimization goal and tailored dataset format to help the language model learn when and where to backtrack (4.1) . Second, during the inference phase, we use the learned backtracking ability to create an efficient search algorithm, without the need for additional tools or reward models (4.2) . Finally, through self-improvement process, we feed the search results back into the model, further enhancing its fast-thinking performance (4.3). Figure 2 illustrates the comprehensive framework of our proposed method. 4.1. Learn to Backtrack the standard Supervised Fine-Tuning (SFT) Under framework, we typically employ dataset Dop = {(xi, yi)}i[nop], where for reasoning tasks, yi represents the natural language reasoning path representing the optimal solution. To enable the model to backtrack at appropriate times and positions, we introduce backtracking dataset: Dback = {(xj, prefix(yj) aerr backtrack)}j[n] Here, prefix(yj) denotes the prefix of the optimal solution yj, representing partial solution; aerr signifies an erroneous action extended from the partial solution, which cannot lead to the correct answer; and backtrack is special token indicating that the model needs to backtrack from the current state. The final dataset is = Dop Dback. For different tasks, there are various methods for constructing Dback. In our experimental setup, the questions for both Dop and Dback are configured to be identical. This configuration allows us to effectively model the dataset as preference dataset, so we can compare more baselines. Through this data construction, if the model learns to recognize aerr and utilize the backtrack token for backtracking, it acquires the ability of when to backtrack, fulfilling the requirement of valid state. Simultaneously, this dataset format implicitly contains information on where to backtrack, indicating that the model should revert to the state represented by prefix(yj). Although this design superficially appears to support only single-step backtracking, the recursive nature of the backtracking algorithm allows Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models the model to achieve multi-step backtracking once it masters single-step backtracking. Algorithm 2 Expert Iteration for Self-Improvement Input: Self-backtracking model M0, initial Dataset D0, For the given dataset D, we formulate the training loss function for the language model parameterized by θ as follows: L(θ) = LSF (θ) + Lbacktrack(θ) (1) The loss function comprises two primary components: firstly, the SFT loss: number of iterations for 0 to 1 do Dt {Mt(xi) xi D0} Dt {(xi, pi) (xi, pi) Dt, path pi is correct } Mt+1 SFT(Mt, Dt) Output: Optimized fast thinking model MK LSF = 1 nop nop (cid:88) i=1 log pθ(yixi), which aims to encourage the model to generate corresponding reasoning steps and final answers based on given questions. The second loss term Lbacktrack contains two parts: Lbacktrack(θ) = 1 nback nback(cid:88) j=1 log pθ(prefix(yj)xj) 1 nback nback(cid:88) j=1 log pθ(backtrackxj prefix(yj) aerr) The first part targets partially correct reasoning paths in backtracking samples, designed to enable the model to accurately predict partial solutions given the input. The second part focuses on the models ability to predict the backtrack token when it has deviated from the correct path, encouraging the model to learn when and where to backtrack. Notably, compared to the SFT loss applied on the Dback dataset, the combination of the second and third loss terms omits the loss component for predicting aerr. This design is reasonable as our objective is not to encourage the model to predict incorrect actions but to prevent it from falling into erroneous reasoning paths. In practical implementation, this can be achieved by applying mask to aerr when computing SFT loss, as illustrated in Figure 2. 4.2. Inference with Backtracking Upon learning when and where to backtrack, the backtracking algorithm (see Algorithm 1) can be integrated into the inference search process. We further propose selfbacktracking inference algorithm that consider both depth and breadth search, which primarily consists of three steps: Expansion, Backtracking, and Selection. Expansion. In the expansion phase, given the current state s, the algorithm samples predictions from the language model. These predictions are then categorized into two groups: those containing the backtrack token and those that do not. Predictions without the backtrack token are directly added to the candidate set, while those containing the token are processed further in the next phase. Backtracking. During the backtracking phase, algorithm selects the containing the predictions backtrack token. These predictions are rolled back by one reasoning step and re-expanded, returning to the expansion phase. This iterative process is repeated (predefined budget) times to expand the candidate set. Selection. Finally, in the selection phase, we compute the scores for all candidate reasoning paths by utilizing the negative perplexity as the metric, and subsequently return the result with the highest score. This algorithm enables flexible search mechanism, where the breadth of exploration is governed by parameter and the depth by parameter b. It leverages the inherent backtracking capabilities learned by the language model during training without requiring an external reward model, while maintaining controllable computational costs throughout the generation process. 4.3. Self-Improvement In this stage we aim to transfer the models slow thinking abilities to fast thinking through the self-improvement method. To achieve this, we employ an expert iteration strategy, which primarily consists of three steps: First, during the slow thinking data generation phase, we utilize the self-backtracking inference model to produce high-quality reasoning path data. Subsequently, in the expert screening phase, experts evaluate the generated data to select training samples suitable for the fast thinking model. In our experiment, we quantify the models accuracy using an evaluator. Finally, in the fast thinking model training phase, the selected high-quality data is used to train the fast thinking model by SFT. Through this iterative optimization, we get continuous enhancement in the performance of the fast thinking model. The process is shown in Algorithm 2. 5. Experiments In this section, we evaluate the capability of the selfbacktracking algorithm to enhance the reasoning performance of language models on the Countdown (Gandhi et al., 2024; Moon et al., 2024) task. This task requires the language model to exhibit robust reasoning abilities, also posing substantial challenge even for humans. We also analyze that our method exhibits the test-time scaling law and 5 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Table 1. Self-Backtracking Enhances Reasoning Performance. We report the accuracy (%) for the countdown task across two base models (Llama3.2-1B and Llama3.2-3B) with several baseline models. Best results for each base model are bolded. Methods Only Optimal Solution: SFT + Greedy SFT + Beam Search RLHF DPO (Rafailov et al., 2023) KTO (Ethayarajh et al., 2024) Best-of-N (N = 8) Best-of-N (N = 16) Best-of-N (N = 32) Backtracking Data: Greedy Beam Search Self-Backtracking (b = 0, = 8) Self-Backtracking (b = 0, = 32) Self-Backtracking (b = 1, = 8) Self-Backtracking (b = 1, = 32) Llama3.2-1B Llama3.2-3B Seen Targets New Targets Seen Targets New Targets 28.60 31.68 29.06 28.34 41.26 32.40 25.60 28.92 36.10 66.66 70.66 67.60 73.54 28.92 31. 27.64 27.74 40.68 33.94 27.04 27.06 34.30 67.40 72.14 68.02 73.78 33.98 35.82 34.46 33.70 47.84 47.28 44.38 27.56 23.10 58.76 60.18 61.06 64.12 32.68 34. 32.72 32.34 48.56 47.80 45.88 26.96 22.20 56.92 58.06 59.28 61.98 possesses the ability to self-improvement, demonstrating significant advantages over other approaches. 5.1. Experimental Setup Dataset. We employ the Countdown task (Gandhi et al., 2024) as our principal experimental framework to rigorously evaluate the reasoning capabilities of our self-backtracking approach. This task extends the traditional 24 Game (Yang et al., 2022) by necessitating that LLMs strategically combine provided set of input numbers using fundamental arithmetic operationsaddition, subtraction, multiplication, and divisionto achieve predefined target number. The complexity of the task stems from its expansive search space, which rigorously tests the models ability in reasoning the correct path. We construct datasets focusing on problem instances with four input numbers and target values 100. The training set consisted of 500,000 samples, balanced between optimal solutions and backtracking data. The test set was systematically partitioned into two distinct categories: one comprising seen targets paired with novel input combinations (denoted as Seen Targets), and the other incorporating entirely new targets (denoted as New Targets), consistent with the setting outlined in SoS (Gandhi et al., 2024). Each category contained 5,000 instances. Data Construction. In the construction of optimal solutions, we first randomly generate the target value and then select four operands within possible range. To solve the mathematical problem, we use recursive exhaustive method to systematically explore all potential solutions. The construction of backtracking data is categorized into three types based on different types of error patterns: 1) Exploration Errors: For given set of target numbers and operands, we employ DFS strategy to generate search steps. If the generated steps do not match the correct solution steps, subsequent searches are terminated. 2) Computational Errors: These errors are constructed by inserting invalid mathematical equations within the solution steps. 3) Rule Violations: This type of error is created by deliberately using operands not in the predefined list of available operands in the solution steps, thereby violating the solution rules. After appending backtrack tokens immediately following the erroneous steps in all backtracking samples, the final training dataset is formed, with the respective proportions of the above error modes being 1:2:2. Comparison Methods In the countdown task, we employ Llama3.2-1B (Dubey et al., 2024) and Llama3.2-3B (Dubey et al., 2024) as the base models. The comparative experiments primarily consist of three categories of methods: The first category involves supervised fine-tuning (SFT) using only optimal solution dataset Dop, and compares two typical sampling strategies, namely greedy search and beam search (beams=16). The second category models the data as preference data pairs and compares various RLHF algorithms, 6 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Table 2. Comparison between search-augmented methods. Methods Seen Targets New Targets DFS (b = 32) DFS (b = 64) SoS (Gandhi et al., 2024) GSoS (Moon et al., 2024) Self-Backtracking (best) 49.12 60.00 57.50 69.00 73.54 48.90 61.06 53.40 67.20 73.78 Figure 3. Self-improvement in accuracy of Llama3.2-1B. Figure 4. The performance curves when varing . including DPO (Rafailov et al., 2023), KTO (Ethayarajh et al., 2024), and the Best-of-N (Li et al., 2022) selection method based on the outcome reward model. The third category is based on backtracking data and compares the greedy sampling strategy. Furthermore, we compare the symbolic solver DFS under specified budgets (backtrack limits = 32 and = 64) with search-augmented methods, SoS (Gandhi et al., 2024) and GSoS (Moon et al., 2024), using their reported optimal performances. Experimental Details Our self-backtracking algorithm is implemented using PyTorch with Deepspeed Stage 2 optimization. Training is conducted on four NVIDIA A800 GPUs, using base learning rate of 1e-5 over three epochs. Model-specific precision is applied: FP32 for Lama3.2-1B and BF16 for Lama3.2-3B. Input sequences are truncated to 128 tokens. Detailed training specifications and baseline implementations are provided in the Appendix A. During the inference phase, we employ beam search with temperature of 0.7 for our method and baselines involving sampling. For an analysis of temperature stability, additional experiments are provided in Appendix C.3. 5.2. Main Results with enhancements of approximately 40% on Llama3.21B and over 30% on Llama3.2-3B. Notably, our method exhibits considerable advantages even when = 0, i.e., without backtracking, suggesting that the <backtrack> token can implicitly assess the quality of the current state, effectively substituting the function of the reward model. Additionally, when = 1, we find that performance further improves, indicating that backtracking to previous states enables the model to leverage search mechanisms to explore correct answers more effectively. The experimental results yield strange finding: our algorithms Llam3.2-3B underperforms compared to Llam3.2-1B, although it is larger. We observe that while the 3B model demonstrates superior computational accuracy, it always fails to achieve the target values. This phenomenon, which will be further examined in subsequent sections through the error type analysis. In addition, we compared the best performance of our method (Llama3.2-1B, = 1, = 32) with the classical symbolic algorithm BFS (with backtracking budget) and search-augmented methods, as shown in Table 2. The results demonstrate that our approach exhibits significant advantages over these algorithms in such reasoning task, even those already utilizing search frameworks. 5.2.1. SELF-BACKTRACKING BOOSTS REASONING 5.2.2. SELF-BACKTRACKING CAN SELF-IMPROVE In Table 1, we present the accuracy of various methods across different models for the countdown task. Overall, the self-backtracking technique demonstrates significant improvement over the baseline of greedy search after SFT, Further experiments demonstrate that our algorithm achieves self-improvement through expert iteration. Employing self-backtracking with configurations = 0, = 16 and = 1, = 16 on two base models and datasets 7 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Table 3. Performance evaluation of self-backtraking under varying ratios of Dop to Dback for Llama3.2-1B on Seen Targets. Parameter 1 : 0.5 1 : 1 1 : 2 1 : 3 1 : = 0, = 8 = 1, = 8 = 0, = 16 = 1, = 16 66.70 68.00 69.80 71.12 66.66 67.60 70.20 72.50 64.92 64.66 67.80 65.66 64.78 65.18 68.18 66.56 65.06 65.94 69.16 70. tal results shown in Figure 5 demonstrate that our method progressively reduces the proportion of Not reached target errors by expanding the search scope. As the parameter increases, the model explores more nodes. Allowing more backtracking provides the model with additional opportunities for error correction and retreat, thereby enhancing the probability of finding the correct answer. However, there is an increase in the proportion of computational errors, stemming from the models improper alteration of the final step results in order to approximate the target value. In contrast, the proportions of errors involving the use of non-compliant operands and formatting errors remain low and stable, having limited impact on overall performance. The whole results can be seen in Appendix C.2. Analysis for different ratio between Dop and Dback. To investigate the impact of backtracking data, we conducted additional experiments by controlling the quantity of Dback. We examined five ratios of Dop to Dback: 1:0.5, 1:1, 1:2, 1:3, and 1:4, testing them on Llama3.2-1B while keeping all training parameters consistent. The experimental results, as shown in 3, indicate that while an increase in backtracking data slightly diminishes the performance of our algorithm, the overall impact is minimal. Notably, the 1:0.5 ratio yields the best results when = 8, whereas the 1:1 ratio is optimal when = 16. We attribute this to the fact that with more backtracking data, the model can achieve more accurate results, thus benefiting from increased sampling. In conclusion, taking into account both the effectiveness and computational cost of training, we recommend maintaining Dop to Dback ratio of no less than 1. 6. Conclusion In this study, we propose novel Self-Backtracking technique that addresses critical limitations in current reasoning models by enabling them to internalize the search process, particularly the ability to autonomously determine when and where to backtrack. This approach not only mitigates inefficient overthinking and reduces dependencies on external reward models but also enhances reasoning efficiency by transforming slow-thinking processes into fast-thinking capabilities through self-improvement. Empirical evaluations on the Countdown task demonstrate that our method Figure 5. Error types of our method for different and on Seen Targets of Llama3.2-1B. respectively, we filtered correct reasoning paths from inference outputs for SFT. Figure 3 presents three-round improvement results, where bars indicate test performance using our algorithm (slow thinking) and lines represent greedy search (fast thinking). Each iteration brings moderate gains (1-2%) for slow thinking, while fast thinking achieves remarkable improvements: +40% after first iteration (approaching slow thinkings performance), ultimately surpassing slow thinking by 50% relative gain after third iteration. Notably, = 1 significantly outperforms = 0, confirming backtrackings importance. This evidences our methods capability to distill slow-thinking advantages into fast-thinking models through expert iteration, enabling optimal single-pass inference. 5.3. Analysis Analysis for different and . We conduct experiments by varying the and , and generate performance curves under different values as increases, as illustrated in Figure 4. The results demonstrate that the performance of BoN initially increases and then decreases with larger , which we attribute to the reward hacking. On the contrary, our method exhibits consistent improvement with increasing , eventually stabilizing, indicating clear test-time scaling law in breadth. Furthermore, when backtracking is permitted (b = 1), the performance improves more rapidly with and achieves higher overall performance, underscoring the necessity of backtracking. Surprisingly, increasing does not result in significant scaling phenomenon in depth. Our case study reveals that the diversity of outputs from secondary backtracking significantly decreases, leading to only marginal improvements compared to = 1. Analysis for error types. We conduct experiments to analyze the error types for different and . There are four error types: not reached target, invalid step format, incorrect result in step and unknown numbers in step. The experimen8 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models achieves performance gain of over 40% compared to the optimal-path supervised fine-tuning baseline, highlighting its effectiveness in improving reasoning capability. Limitations and future work. This study has several limitations. For instance, the method has not been adapted to broader range of general reasoning tasks, and need further scaling up. We plan to demonstrate the advantages of our method in more general reasoning tasks on larger LLMs in subsequent research. 7. Impact Statements This work advances LLMs reasoning by enabling the model to backtrack during both the training and inference phases. Our method has the potential to enhance wide range of applications that require reliable artificial intelligence reasoning, such as mathematical problem-solving. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "An, S., Ma, Z., Lin, Z., Zheng, N., Lou, J.-G., and Chen, W. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689, 2023. Chen, L., Zhu, C., Soselia, D., Chen, J., Zhou, T., Goldstein, T., Huang, H., Shoeybi, M., and Catanzaro, B. Odin: Disentangled reward mitigates hacking in rlhf. International Conferenceon Machine Learning, 2024a. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024b. Choi, S., Fang, T., Wang, Z., and Song, Y. Kcts: Knowledgeconstrained tree search decoding with token-level hallucination detection. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Cundy, C. and Ermon, S. Sequencematch: Imitation learning for autoregressive sequence modelling with backtracking. 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Gandhi, K., Lee, D., Grand, G., Liu, M., Cheng, W., Sharma, A., and Goodman, N. D. Stream of search (sos): Learning to search in language. First Conference on Language Modeling, 2024. Graves, A. Sequence transduction with recurrent neural networks, 2012. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Lehnert, L., Sukhbaatar, S., Su, D., Zheng, Q., McVay, P., Rabbat, M., and Tian, Y. Beyond a*: Better planning with transformers via search dynamics bootstrapping. 2024. Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.- G., and Chen, W. Making large language models better reasoners with step-aware verifier. arXiv preprint arXiv:2206.02336, 2022. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. 2024. Moon, S., Park, B., and Song, H. O. Guided stream of search: Learning to better search with language arXiv preprint models via optimal path guidance. arXiv:2410.02992, 2024. OpenAI. Learning to reason with large language models, September 2024. Polu, S. and Sutskever, I. Generative language modelarXiv preprint ing for automated theorem proving. arXiv:2009.03393, 2020. Qin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. Qwen. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 2023. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 9 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Su, D., Sukhbaatar, S., Rabbat, M., Tian, Y., and Zheng, Q. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024. Ye, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics of language models: Part 2.2, how to learn from mistakes on gradeschool math problems. arXiv preprint arXiv:2408.16293, 2024. Teller, V. Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition, 2000. Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. Zhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y., and Tang, J. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816, 2024a. Zhang, X., Du, C., Pang, T., Liu, Q., Gao, W., and Lin, M. Chain of preference optimization: Improving chain-ofthought reasoning in llms. Advances in Neural Information Processing Systems, 2024b. Zhang, Y., Chi, J., Nguyen, H., Upasani, K., Bikel, D. M., Weston, J., and Smith, E. M. Backtracking improves generation safety. arXiv preprint arXiv:2409.14586, 2024c. Tong, Y., Li, D., Wang, S., Wang, Y., Teng, F., and Shang, J. Can llms learn from previous mistakes? investigating llms errors to boost for reasoning. arXiv preprint arXiv:2403.20046, 2024. Van Beek, P. Backtracking search algorithms. In Foundations of artificial intelligence, pp. 85134. 2006. Wan, Z., Feng, X., Wen, M., McAleer, S. M., Wen, Y., Zhang, W., and Wang, J. Alphazero-like tree-search can guide large language model decoding and training. 2024. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pp. 94269439, 2024. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. International Conference on Learning Representations, 2022. Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. Advances in Neural Information Processing Systems, 2024. Xie, Y., Goyal, A., Zheng, W., Kan, M.-Y., Lillicrap, T. P., Kawaguchi, K., and Shieh, M. Monte carlo tree search boosts reasoning via iterative preference learning. 2024a. Xie, Y., Kawaguchi, K., Zhao, Y., Zhao, J. X., Kan, M.- Y., He, J., and Xie, M. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 2024b. Yang, M. S., Schuurmans, D., Abbeel, P., and Nachum, O. Chain of thought imitation with procedure cloning. Advances in Neural Information Processing Systems, pp. 3636636381, 2022. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 2024. 10 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models A. Training Details All baseline methods are trained on cluster with four NVIDIA A800 GPUs. Model-specific precision is applied: FP32 for Lama3.2-1B and BF16 for Lama3.2-3B. Input sequences are truncated to 128 tokens. Hyper-parameters: Learning rate: 1 105 Warmup steps: 1 Batch size: 16 Learning rate scheduler: Cosine Training epochs: 3 For DPO: β = 0.5, RPO α = 1.0. We also demonstrate the loss curve in Figure 6 of the training process of our self-backtracking method. Figure 6. Loss Curve of self-backtracking for Llama3.2-1B and Llama3.2-3B. B. Case Study We present two illustrative examples in Figure 7, both derived from our self-backtracking approach. Our analysis reveals that the proposed model is capable of identifying novel solutions distinct from the reference solutions, thereby demonstrating the superior reasoning capabilities of our algorithm. C. More Results C.1. Self-Improvement We supplement the experimental results of expert iteration on Llama3.2-3B in Figure 8. C.2. Analysis for Error Types We demonstrate more comprehensive analysis of error types, with results for Llama3.2-1B on Seen Targets shown in 9, results on New Targets in 10, results for Llama3.2-3B on Seen Targets shown in 11, and results on New Targets shown in 12. Through empirical observation, we identify that the Llama3.2-3B model demonstrates superior computational accuracy. However, it frequently commits errors that prevent it from reaching the target. We hypothesize that this phenomenon is attributable to the models robust foundational capabilities, which prioritize computational precision. potential solution to mitigate this issue could involve the incorporation of more training data. 11 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Figure 7. Case Study of our self-backtracking. Figure 8. Self-improvement in accuracy of Llama3.2-3B. 12 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Figure 9. Error types of Llama3.2-1B on seen targets. Figure 10. Error types of Llama3.2-1B on new targets. 13 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models Figure 11. Error types of Llama3.2-3B on seen targets. Figure 12. Error types of Llama3.2-3B on new targets. 14 Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models C.3. Analysis for Temperatures In previous experiments, we consistently set the temperature parameter to 0.7. To further investigate the sensitivity of our algorithm to temperature variations, we conduct additional experiments, with the results presented in Figure 13. The findings indicate that our method exhibits strong stability across different temperature settings, with only excessively low temperatures causing minor adverse effects specifically under the condition of = 1. Consequently, we recommend using the conventional temperature value of 0.7, which proves to be reasonable choice. Figure 13. The influence of different temperatures on accuracy. C.4. Analysis for Different Ratio Between Dop and Dback We supplement the experimental results on the test split of New Targets across varying ratios between Dop and Dback using the Llama3.1-1B model in Table 4. Table 4. Performance evaluation of self-backtraking under varying ratios of Dop to Dback for Llama3.2-1B on New Targets. Parameter 1 : 0. 1 : 1 1 : 2 1 : 3 1 : 4 = 0, = 8 = 1, = 8 = 0, = 16 = 1, = 16 68.58 69.36 71.98 72. 67.40 68.02 71.42 72.94 65.16 65.06 68.22 65.80 63.36 63.84 67.64 66.76 63.22 64.38 67.70 69."
        }
    ],
    "affiliations": [
        "National Key Laboratory for Novel Software Technology, Nanjing University, China",
        "School of Artificial Intelligence, Nanjing University, China",
        "School of Intelligence Science and Technology, Nanjing University, China"
    ]
}