{
    "paper_title": "Closing the Loop: Universal Repository Representation with RPG-Encoder",
    "authors": [
        "Jane Luo",
        "Chengyu Yin",
        "Xin Zhang",
        "Qingtao Li",
        "Steven Liu",
        "Yiming Huang",
        "Jie Wu",
        "Hao Liu",
        "Yangyu Huang",
        "Yu Kang",
        "Fangkai Yang",
        "Ying Xin",
        "Scarlett Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 4 8 0 2 0 . 2 0 6 2 : r Closing the Loop: Universal Repository Representation with RPG-Encoder Jane Luo1,;, , Chengyu yin1 ;,, Xin Zhang1 ,:, Qingtao Li1, Steven Liu1,;, Yiming Huang2, Jie Wu3,;, Hao Liu1,;, Yangyu Huang1, Yu Kang1, Fangkai Yang1, Ying Xin1, Scarlett Li1, 1Microsoft Research Asia 2UCSD 3Tsinghua University Equal contribution : Corresponding author ; Work done during internships at Microsoft Current repository agents encounter reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, framework that generalizes the Repository Planning Graph (RPG) from static generative blueprint into unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPGs high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation. Date: February 3, 2026 Code: https://github.com/microsoft/RPG-ZeroRepo Project: https://ayanami2003.github.io/RPG-Encoder/ Prior Work: https://arxiv.org/abs/2509.16198 Correspondence: janeluo1210@163.com; xinzhang3@microsoft.com 1. Introduction Repository-level software engineering relies on agents navigating complex dependencies and reasoning about high-level architectural intent (Wang et al., 2025a; Zhao et al., 2025). However, as illustrated in Figure 1, existing approaches suffer from reasoning gap due to fragmented representations: API Documentation focuses on semantic intent (Luo et al., 2024; Chen et al., 2025a) but lacks global navigability, forcing models to infer architectural connectivity (Chen et al., 2025b; Jain et al., 2025). Conversely, Dependency Graph captures structural logic (Ouyang et al., 2024; Ma et al., 2024) but provide limited semantic information (Borowski et al., 2024; Cheng et al., 2024), leaving agents to follow execution paths without reflecting the underlying rationale (Jiang et al., 2025). Furthermore, maintaining consistency incurs prohibitive overhead: documentation is prone to semantic drift (Tan et al., 2024), while static graphs capture syntactic updates but often overlook logical implications (Gröninger et al., 2025). We observe that this reasoning disconnect is not merely failure of individual tools, but systemic consequence of treating repository understanding as an isolated, unidirectional task. Fundamentally, this occurs because current approaches ignore the inherent symmetry of software engineering. We argue that repository comprehension and generation constitute inverse pathways within unified reasoning cycle: generation expands sparse intent into detailed code, whereas comprehension must compress noisy implementation back into high-level intent. Consequently, bridging this gap requires unified Intermediate Representation that fuses Figure 1: Comparison of code representations regarding semantic abstraction and structural explicitness. Unlike approaches limited to single dimension, RPG achieves dual-view alignment, combining semantic richness with structural actionability. the semantic density of documentation with the topological rigor of dependency graphs. The Repository Planning Graph (RPG) (Luo et al., 2025) emerges as suitable representation for this unification. Having served as generative blueprint for intent-to-code, it possesses the dual-view structure needed for the inverse code-to-intent journey. This motivates our fundamental inquiry: Can the RPG be generalized to serve as unified, high-fidelity representation for existing repositories, thereby closing the loop? To realize this vision, we propose RPG-Encoder, framework that transforms the RPG from static generative blueprint into dynamic, bidirectional representation. We implement this through three cohesive mechanisms: (1) Encoding: We introduce semantic lifting protocol that projects code into the RPG. Nodes combine functional descriptions with code metadata, while edges encode hierarchy and static dependencies, yielding an interpretable and verifiable representation. (2) Evolution: We design an incremental mechanism that parses commit diffs to update the RPG. This keeps semantics synchronized with implementation without re-generation. (3) Operation: We establish the RPG as unified interface for structure-aware reasoning. It serves as topological map, enabling traversal between high-level intent and low-level execution logic. To evaluate the extracted RPG, we conduct dual-task evaluation on two critical dimensions: navigational utility and representational fidelity. (1) In Repository Understanding, RPG-Encoder with Claude-4.5-Sonnet (Anthropic, 2025b) demonstrates superior function-level performance, achieving 93.7% Acc@5 on SWE-bench Verified (OpenAI, 2024) and exceeding the best baseline by over 10% on SWE-bench Live Lite (Zhang et al., 2025). This confirms that coupling semantic features with topology significantly strengthens fine-grained localization. (2) In Repository Reconstruction, RPG-Encoder outperforms API documentation by providing an explicitly ordered blueprint. Guided by topological constraints, RPG-Encoder reconstructs repositories with 98.5% coverage (+24.3% over baselines) and 86.0% pass rate on RepoCraft (Luo et al., 2025). In contrast, documentation lacks structural guidance and recovers only 17% of the original code volume, proving that RPG serves as structured representation that effectively preserves complete repository semantics. Analysis confirms that semantic features are essential for effective exploration, and our incremental strategy reduces maintenance costs by 95.7% without incurring semantic drift. 2 Our contributions are summarized as follows: We generalize the Repository Planning Graph (RPG) into unified representation that closes the loop between comprehension and generation, theoretically grounding repository reasoning as unified reasoning cycle where semantic intent and structural dependencies are bidirectionally linked. We introduce RPG-Encoder, framework that implements semantic lifting protocol to recover high-level intent from code and supports sustainable evolution via differential updates, decoupling maintenance costs from repository scale. We validate RPG-Encoder on dual tasks: establishing SOTA performance in repository understanding to demonstrate superior navigational utility, and achieving 98.5% coverage in repository reconstruction to verify its high-fidelity representational capacity. 2. Related Work Repository Generation. Research has transitioned from localized file completion (Wang et al., 2025b; Li et al., 2023) to systemic workflows that emphasize architectural coherence. Multi-agent frameworks like MetaGPT (Hong et al., 2024) and paper-to-code systems (Seo et al., 2025; Lin et al., 2025) utilize role-based abstraction to manage complexity. More recently, Commit0 (Zhao et al., 2024) introduces library-level reconstruction paradigm starting from near-zero implementations, while terminal-based agents such as Claude Code (Anthropic, 2025a) and Gemini CLI (Google, 2025) facilitate iterative \"build-test-fix\" cycles in realworld environments. RPG (Luo et al., 2025) advances this by utilizing structured planning graphs to ground generative intent in execution dependencies, ensuring that synthesized repositories remain topologically valid. Repository Understanding. Current paradigms shift from passive retrieval to active, structure-aware exploration. Early iterative methods (Zhang et al., 2023; Xia et al., 2024) have been augmented by graph-guided navigation frameworks (Ouyang et al., 2024; Liu et al., 2025). To address high-density codebases, LocAgent (Chen et al., 2025c) and KGCompass (YANG et al., 2025) leverage explicit dependency schemas and knowledge graphs to prune the search space. Furthermore, RepoHyper (Phan et al., 2025) explores longcontext summarization for global semantic grasp, while agents like OrcaLoca (Yu et al., 2025) and CoSIL (Jiang et al., 2025) integrate dynamic execution signals for precise fault localization. These systems increasingly rely on \"browse-edit-run\" loops (Yang et al., 2024) and are rigorously benchmarked on real-world issue-solving datasets like SWE-bench Verified (OpenAI, 2024). Despite these advancements, existing methods often suffer from fragmented representations: dependency graphs lack semantic depth, while semantic retrievers lack topological precision. RPG-Encoder bridges this gap by coupling dense semantic features with structural constraints, enabling fine-grained localization that is both intent-driven and execution-grounded. 3. Method To establish RPG as unified and high-fidelity Intermediate Representation, we introduce the RPG-Encoder. By mapping implementation back to semantic space (Code Ñ RPG), it completes the representation loop. As illustrated in Figure 2, our methodology comprises: (1) Encoding for RPG extraction; (2) Evolution for incremental maintenance; and (3) Operation as unified reasoning substrate. 3.1. RPG Encoding: Extracting RPG from Codebases To transform raw codebase into an actionable substrate, we model extraction as pipeline that converts implementation details into compact, structured semantic index for high-level reasoning. This process reconstructs the system topology in three phases. More details are in Appendix A.1. RPG Structure Refining prior definitions (Luo et al., 2025), we define RPG as hierarchical, dual-view graph pV, q. The node set VH VL distinguishes High-level Nodes representing architectural directories from Low-level Nodes comprising atomic implementations such as files, classes, and functions. Each node 3 , mq pairs semantic feature describing functionality (e.g., handles authentication) with structural metadata encoding code entity attributes like type and path. The edge set integrates two perspectives: (1) Functional edges Efeature establishing teleological hierarchy; and (2) Dependency edges Edep mapping logical interactions including imports and calls. This duality enables the agent to perceive the repository as both functional and executable network. Figure 2: Overview of the RPG-Encoder. The pipeline bridges Code and RPG via three stages: Encoding lifts code into semantic topology; Evolution handles incremental updates via commits; and Operation provides unified interface for agentic reasoning. Phase 1: Semantic Lifting To bridge the granularity mismatch between verbose implementation and functional intent, the extraction process first lifts the codebase into discrete registry of Low-level Nodes (VL). For each file, the system extracts semantic features for individual functions and classes, mapping them to behavioral signatures while retaining their code-level attributes as metadata m. Subsequently, these fine-grained features are synthesized into holistic summary representing the files overall functionality. This summarization process naturally establishes functional edges Efeature between the file-level node and its constituent function-level node. This phase concludes by producing semantically grounded implementation index, serving as robust representation for higher-level reasoning. Phase 2: Semantic Structure Reorganization Physical folder-file organization is often dictated by technical constraints rather than functional boundaries, inducing structural entanglement. To mitigate this, we construct the High-level Node set VH by recovering the latent functional topology from implementation units (VL). (1) Functional Abstraction: To ensure the global repository state fits within the LLM context window, we perform granularity-based input compression. Instead of raw implementation, the LLM only consumes concise 1The Python icon is the official Python logo; see https://www.python.org/community/logos/. 4 semantic features of file-level nodes, excluding function-level details. This condensed view allows the model to analyze the complete repository-wide semantic manifold to induce abstract functional centroids (e.g., Data Preprocessing) that define the root pillars of the hierarchy. (2) Hierarchical Aggregation: We recursively link nodes from VL to these centroids. To ensure structural stability, each nodes placement is determined by semantic compatibility check: the LLM evaluates the fit between nodes and the centroids definition, instantiating intermediate nodes (e.g., routing StandardScaler via Normalization to Preprocessing) to bridge the hierarchy when direct link lacks granularity. Together, these nodes constitute the High-level Node set VH, establishing explicit parent-child functional edges. This yields complete functional graph where each VH possesses semantic feature but lacks structural metadata required to link it to physical code entities. Phase 3: Artifact Grounding To transform the abstract hierarchy into substrate, this phase anchors the functional manifold to physical artifacts and execution logic. We first populate the missing metadata for nodes in VH through bottom-up propagation, utilizing Lowest Common Ancestor (LCA) mechanism (detailed in Appendix A.1.3) to compute the minimal directory scope shared by each clusters descendants. This mapping ensures that abstract features such as Data Preprocessing are tied to code paths like sklearn/preprocessing. Subsequently, to transition from semantic hierarchy to an implementation map, we inject dependency edges Edep (e.g., imports, calls) via AST analysis. This integration completes the RPG, yielding unified representation that enables traceability between high-level functional intent and the executable code. 3.2. RPG Evolution: Incremental Maintenance To reduce the cost of full re-generation, we maintain incrementally and reserve global reconstruction for major refactoring. For routine updates, we perform online graph editing to keep the RPG synchronized, as illustrated in Figure 2 (top-right) and detailed in Appendix A.2. Commit-Level Feature Extraction We parse raw commit data to extract semantic features strictly for affected code fragments, avoiding full reprocessing. This yields set of discrete Feature Nodes representing the delta state, which serves as the direct input for graph operations. RPG Updates Based on the diff type, we execute three atomic update protocols to maintain the RPG structure: (1) Deletions: We remove nodes for deleted files or functions and recursively prune empty parent categories in to maintain hierarchical integrity. (2) Modifications: We re-generate the semantic description for modified entities. To avoid structural instability, nodes position is updated only if the LLM detects functional intent shift that violates its parents semantic scope (e.g., utility function evolving into core algorithm). This check serves as semantic threshold to prevent minor implementation changes from triggering costly structural migrations. (3) Additions: We create nodes for new entities and insert them into the hierarchy by matching their semantics against existing functional centroids. Finally, we perform localized dependency update, re-parsing affected ASTs to refresh dep and align connectivity with the execution flow. 3.3. RPG Operation: Unified Reasoning Substrate We deploy RPG as Unified Representation providing queryable index of the codebase. Structurally, it functions as heterogeneous graph where Functional and Dependency Views are partitioned by edge types (Efeature and Edep) but share unified node set, enabling seamless context switching during retrieval. More details are in Appendix A. Unified Agentic Tool We define three core tools to operate on the RPGs nodes and edges: SearchNode: Performs global node-level retrieval by matching intent against semantic features or filtering metadata m, allowing the agent to precisely localize entry points across both views. 5 FetchNode: Executes node-level data retrieval. Given v, it extracts the attribute tuple , mq and raw source code to provide the ground truth for inspection. ExploreRPG: Facilitates cross-view traversal along edges . While Edep is strictly constructed via static AST analysis, its integration with the semantic hierarchy in VH provides robust topological skeleton that guides the agent through complex execution flows without the noise of unstructured search. This toolset enables multi-dimensional navigation by integrating functional intent with physical implementation, facilitating precise context discovery through semantic and dependency structures. Efficient Structured Representation RPG reduces information overload by representing the repository as substrate with two roles: (1) Knowledge Source: RPG stores feature descriptions and metadata for each node, capturing what the code does without parsing implementations. (2) Process Encoder: RPG induces topological order via functional edges (Efeature) and dependency edges (Edep), exposing causality and hierarchy essential for architectural comprehension. 4. Experiments Setup We evaluate RPG on two tasks to assess its semantic grasp and structural completeness: (1) Repository Understanding, testing navigation and localization capabilities; and (2) Repository Reconstruction, verifying the fidelity and losslessness of the encoded information. 4.1. Repository Understanding We assess RPG as navigational substrate through rigorous localization tasks. More details are in Appendix B.1. Benchmark. We evaluate on two benchmarks: SWE-bench Verified (OpenAI, 2024), human-validated subset ensuring solvability with 500 examples from 12 repositories; and SWE-bench-Live Lite (Zhang et al., 2025), mitigating contamination using recent issues, comprising 300 examples across 70 repositories. Baselines. We compare against baselines leveraging diverse structural priors: Agentless (Xia et al., 2024) operates via hierarchical text-based narrowing without graph priors; LocAgent (Chen et al., 2025c) leverages explicit dependency graphs for guided traversal; CoSIL (Jiang et al., 2025) performs iterative search over static code structures; and OrcaLoca (Yu et al., 2025) integrates dynamic execution signals with agentic planning. Evaluation Metrics. We adopt standard metrics: Acc@k (k t1, 5u) checks if ground-truth target is in top-k predictions (Jiang et al., 2025); and Precision/Recall quantify overlap. Given predicted set and ground-truth G, we define Precision G{P and Recall G{G. Implementation Details. We use GPT-4o (OpenAI, 2024) to parse and incrementally update the RPG. Backbone models include o3-mini (OpenAI, 2025d), GPT-4o (OpenAI, 2024), GPT-4.1 (OpenAI, 2025a), GPT-5 (OpenAI, 2025b), DeepSeek-V3.1 (Liu et al., 2024), and Claude-4.5-sonnet (Anthropic, 2025b). RPGEncoder operates with 40-step limit. Baselines follow configurations (detailed in Appendix B.1.1). All runs are averaged over 3 times. 4.2. Repository Reconstruction We use reconstruction to verify lossless, topologically ordered RPG information. Details are in Appendix B.2. Benchmark. We adapt RepoCraft (Luo et al., 2025) for controlled reconstruction, aiming to rebuild target repositories (e.g., Requests) with ground-truth functionality. To isolate representational fidelity, we compare Official API Documentation with RPG. We focus on representation sources rather than search-based agents (e.g., LocAgent), since reconstruction requires comprehensive blueprint instead of iterative localization. 6 Baselines. We configure ZeroRepo (Luo et al., 2025) in two modes: (1) ZeroRepo-Doc (Baseline): The agent references API documentation, autonomously managing progress and objectives via Test-Driven Development. (2) ZeroRepo-RPG (Ours): We utilize the extracted RPG for direct repository generation, where it serves as the exclusive knowledge source and scheduler. Nodes are processed in topological order, batching semantically similar nodes to accelerate inference. More details are in Appendix B.2.2. Evaluation Metrics. Following RepoCraft, we report: (1) Coverage, the proportion of implemented functional categories; (2) Accuracy (Pass / Vote), unit-test pass accuracy and vote-based check accuracy; and (3) Code Statistics (#Files, nLOC, Code Tokens) to measure structural similarity and recovered code volume. Implementation Details. We employ GPT-4o (OpenAI, 2024) for RPG extraction and evaluate reconstruction using GPT-5-mini (OpenAI, 2025c) and GPT-4.1 (OpenAI, 2025a). Following RepoCraft, we also use o3mini (OpenAI, 2025d) for automated evaluation. ZeroRepo-Doc runs without hard turn limit and stops when the agent judges the documentation to be fully implemented. ZeroRepo-RPG is bounded by the graph and terminates once all RPG-derived nodes are executed. More details are in Appendix B.2.4. Table 1: Comprehensive localization results on SWE-bench Verified and SWE-bench Live Lite across File and Function levels. Acc@k: Accuracy@k. Pre/Rec: Precision/Recall. Bold indicates the best result, and Underline indicates the second best. SWE-bench Verified SWE-bench Live File-level Function-level File-level Function-level Acc@1 Acc@5 Pre Rec Acc@1 Acc@5 Pre Rec Acc@1 Acc@5 Pre Rec Acc@1 Acc@5 Pre Rec Model: o3-mini 67.1 67.5 62.8 66.5 88.1 71.9 77.2 85.7 67.0 68.3 64.7 66.2 64.7 64.0 61.4 63. 34.7 46.3 32.1 52.2 78.3 58.5 80.7 +10.8 +3.1 +12.4 +12.1 +6.3 76.8 91.2 39.4 48.3 33.9 54.7 62.9 +8. 33.2 41.5 28.9 47.1 54.2 35.4 47.6 60.9 78.5 38.0 59.4 80.8 55.6 36.2 49.7 66.1 47.7 27.6 41.2 54.8 28.8 23.1 23.8 43. 54.2 26.1 31.0 65.1 39.3 25.3 26.6 51.4 25.6 15.6 17.7 35.6 73.7 55.1 46.9 64.5 +8.0 +12.8 +7.4 +11.4 +9.7 +12.7 +10.5 +13.3 +11.3 56. 88.2 77.5 64.7 75.6 Method Agentless OrcaLoca LocAgent CoSIL Repo-Enc best Agentless OrcaLoca LocAgent CoSIL Repo-Enc best Agentless OrcaLoca LocAgent CoSIL Repo-Enc best Agentless OrcaLoca LocAgent CoSIL Repo-Enc best Agentless OrcaLoca LocAgent CoSIL Repo-Enc best 60.3 52.9 40.5 73.3 77.8 +4.5 58.8 53.3 67.4 66.2 76.7 +9. 49.0 66.7 65.6 74.5 63.0 64.3 71.9 64.9 74.5 +2.6 65.2 75.2 79.5 69.8 82.6 +3.1 78.7 88.2 88.2 82. 91.9 +3.7 76.6 87.2 71.4 75.5 90.5 +3.3 86.1 69.3 87.9 84.4 89.6 +1.7 90.8 80.0 90.9 90. 93.2 +2.3 95.9 93.9 96.7 95.7 97.7 +1.0 96.5 89.6 76.6 96.1 97.6 +1.1 63.1 65.0 73.4 65. 77.0 +3.6 65.7 76.5 80.8 70.7 83.6 +2.8 78.3 88.6 88.4 82.3 91.1 +2.5 76.9 87.5 72.7 75. 91.8 +4.3 61.1 61.4 69.3 62.2 72.7 +3.4 63.5 71.3 77.2 67.6 31.4 39.8 40.1 43.2 53.1 +9. 29.3 55.2 32.3 51.8 Model: GPT-4o 34.7 42.5 44.8 48.2 57.9 +9.7 29.3 36.7 38.1 40.1 49.5 +9. Model: GPT-4.1 32.7 59.0 36.7 55.3 26.4 50.1 31.2 47.0 56.1 42.5 62.5 60.1 69.2 +6.7 62.0 56.2 74.7 62. 68.7 79.3 78.0 +2.1 +13.5 +8.9 +12.0 +12.3 +3.3 83.4 71.0 62.4 76.2 84.2 86.7 80. 89.1 +2.4 74.4 82.2 70.2 73.7 88.6 +6.4 45.1 76.1 50.9 68.3 83.4 +7.3 31.7 74.5 49.3 57. 68.1 86.2 80.3 81.8 93.6 +7.4 34.6 79.3 57.8 78.7 Model: GPT-5 47.3 79.1 55.9 68.9 41.3 68.6 49.7 62. 64.5 74.4 79.7 69.8 76.9 +8.3 84.5 +5.4 82.1 +2.4 Model: Claude-4.5-Sonnet 63.8 74.7 58.7 64.5 32.0 76.5 51.5 60.7 27.1 65.1 44.9 52. 93.7 82.0 83.4 79.8 +5.3 +14.4 +6.9 +10.7 +7.3 75.8 78.8 47.6 80.0 77.0 83.5 +3.5 85.5 59.6 87.9 84. 90.5 +2.6 87.4 82.3 93.0 89.3 94.4 +1.4 89.7 78.3 69.0 88.3 93.9 +4.2 57.1 45.0 66.8 63. 73.2 +6.4 63.0 57.1 76.8 67.3 81.4 +4.6 65.1 77.6 81.4 72.9 85.4 +4.0 66.1 76.2 61.6 69. 48.3 34.0 54.2 50.7 60.3 +6.1 54.5 44.2 66.1 55.6 30.6 28.2 35.7 41.2 50.5 +9.3 35.1 42.0 43.4 48. 57.4 37.0 56.4 61.6 41.4 32.5 44.5 49.1 26.4 21.1 29.9 29.4 69.4 41.8 59.4 +7.8 +10.3 +11.9 59.4 50.5 68.7 72.2 46.0 46.2 52.5 58. 25.4 29.1 38.7 41.2 64.7 69.0 52.6 81.9 +2.9 +15.9 +9.7 +13.8 +11.4 72.1 57.4 63.5 74.2 62.2 38.8 59.6 48.0 55. 64.6 74.0 68.7 76.2 49.7 68.6 56.6 62.3 31.6 46.6 40.5 46.5 71.9 76.2 61.1 87.8 +2.0 +12.3 +11.6 +9.5 +14.5 78. 58.0 61.5 54.7 57.5 41.4 65.1 47.3 51.1 72.4 69.4 60.5 74.9 55.3 67.8 52.6 60.1 35.9 46.1 39.3 39.6 75. 63.3 85.6 +9.4 +14.3 +9.7 +15.5 +12.9 +17.2 90.4 74.8 80.7 7 5. Main Result RPG Enhances Fine-Grained Repository Understanding. Table 1 demonstrates that RPG consistently improves file-level and function-level localization. On SWE-bench Verified, RPG-Encoder with Claude-4.5 achieves 93.7% Acc@5 on function level, surpassing the best baseline (OrcaLoca) by 14.4 points, while simultaneously improving Precision by 6.9% and Recall by 10.7%. Furthermore, on SWE-bench Live, RPGEncoder with GPT-5 elevates performance to 87.8% Acc@5 on function level, outperforming CoSIL by 11.6 points. These results confirm that coupling semantic features with topological constraints enables agents to map high-level intent to specific implementation units. Crucially, this dual-view structure filters irrelevant noise while ensuring comprehensive coverage of target functionalities. Table 2: Main results on repository reconstruction tasks in RepoCraft. Gold Projects represent statistics of the original human-written repositories. More results are provided in Appendix C.1. Framework Backbone Coverage (%) Ò Accuracy (Pass / Vote) (%) Ò #Files Ò nLOC Ò Code Tokens Ò Gold Projects (Reference) Human Developers 100.0 ZeroRepo-Doc (Baseline) ZeroRepo-RPG (Ours) GPT-4.1 GPT-5-mini GPT-4.1 GPT-5-mini 64.6 74. 93.5 98.5 94.8 / 98.8 50.0 / 63.4 52.6 / 71.4 85.8 / 93.4 86.0 / 97.7 345 209 206 226 97,725 718,946 6,079 13,414 35,190 60,871 158,948 125, 346,865 550,432 RPG Functioning as Complete Representational Substrate. Table 2 demonstrates RPGs superior fidelity in reproducing complex repository structures. With GPT-5-mini, RPG-Encoder attains 98.5% Coverage and an 86.0% Pass Rate, exceeding the documentation-based baseline by over 33 points. Regarding code scale, the baseline generates severely fragmented outputs, capturing only 17% of the original volume due to lack of structural guidance. In contrast, RPG-Encoder reconstructs 550k tokens, scale comparable to the gold project written by human. This high fidelity proves that RPG serves as sufficient substrate to ground architectural intent within valid structural topology, guiding the agent to expand the blueprint into concrete implementation unlike linear API documentation. 6. Ablation Study Experimental Setup. To isolate semantic and topological contributions, we run two ablations (detailed in Appendix D). For Reconstruction (RepoCraft), we progressively strip node metadata bottom-up while retaining semantic features to evaluate representational fidelity. For Understanding (SWE-bench Live), we remove structural metadata to assess navigational efficacy under RPG-Encoder. Table 3: Ablation study of RPG-Encoder on SWE-bench Live. Best results are highlighted in bold. File-level Function-level Acc@1 Acc@5 Pre Rec Acc@1 Acc@5 Pre Rec Semantics and Topology are Mutually Reinforcing. Table 3 delineates the distinct functional contributions of graph components. Semantic Features provide essential semantic grounding for fine-grained localization; their removal causes the sharpest decline in Function-level Acc@1 (50.5% Ñ 43.1% on GPT-4o), indicating that abstract summaries are indispensable for aligning natural language intent with concrete implementations. Dependencies establish structural connectivity; severing these edges disrupts execution tracing, significantly degrading File-level retrieval. The Full RPG integrates these layers to maximize context discovery, consistently outperforming all ablated variants. Backbone Method GPT-4.1 GPT-4o RPG-Encoder 69.2 w/o Dependency 58.4 60.9 w/o Feature RPG-Encoder 78.0 w/o Dependency 77.4 71.7 w/o Feature 83.5 73.2 60.3 50.5 77.4 63.0 53.3 44.8 76.3 64.6 52.4 43.1 69.4 59.4 41.8 66.3 53.4 36.4 63.4 52.3 35.5 90.5 81.4 69.0 64.7 89.4 80.6 68.3 63.7 87.5 76.9 64.5 57.4 81.9 72.1 52.6 80.2 71.1 51.9 76.3 66.3 47. 8 Table 4: Ablation on representational fidelity on scikit-learn (GPT-5-mini). Hierarchical Constraints Ensure Structural Fidelity. Table 4 indicates that the multi-level topology of RPG is essential for preserving repository modularity. Removing file and function metadata (w/o File & Function) results in notable loss of structure: the number of files decreases from 256 to 157, and code volume drops by approximately 200,000 tokens. This suggests that without explicit topological boundaries, the model tends to merge distinct modules, leading to loss of granularity. Additionally, the removal of function metadata (w/o Function Metadata) reduces the Pass Rate from 82.8% to 74.1%, showing that detailed structural signals are important for code correctness. Finally, all graph-based variants outperform the text-based ZeroRepo-Docs, confirming that structured representations provide better basis for reconstruction than linear documentation. ZeroRepo-Docs ZeroRepo-RPG (RPG-Encoder) w/o Function Metadata w/o All Node Metadata 12,007 101,988 96,831 898,026 87,413 854,886 63,489 687,879 55.6 / 66.3 82.8 / 99.5 74.1 / 90.9 65.3 / 84. 72.3 100.0 91.5 87.2 Pass Rate #Files 76 256 248 157 Coverage Method Tokens nLOC 7. Analysis 7.1. Representational Efficiency Table 5: Efficiency for repository understanding on SWE-bench Verified. Steps and Cost are averaged over tasks. Eff. is defined as Acc@5{Cost. Additional results are provided in Appendix C.4. RPG Facilitates Reasoning Efficiency. Table 5 evaluates the efficiency of agents guided by different substrates. Across all backbones, RPG-Encoder achieves fewer steps and lower expenditure, yielding the highest cost-effectiveness (Acc@5/Cost). On GPT-5, RPGEncoder reaches an efficiency of 4.15 at cost of $0.22, whereas baselines such as OrcaLoca and LocAgent require higher expenditures for lower efficiency gains. This trend is consistent with GPT-4.1 results, where RPG-Encoder attains the peak efficiency of 4.63. These results indicate that RPG-guided navigation enables precise exploration, concentrating reasoning resources on relevant code regions and reducing redundant API calls throughout the localization process. 1.48 36.93 3.10 19.52 6.48 0.76 6.34 4. OrcaLoca CoSIL LocAgent RPG-Encoder Eff. 1.16 2.64 1.64 4.15 20.22 19.77 11.94 6.75 0.46 0.24 0.86 0.18 0.75 0.31 0.49 0. Steps Cost ($) Steps Cost ($) GPT-4.1 Method GPT-5 Eff. 7.2. Structural Evolvability Table 6: Full vs. Incremental RPG Fidelity on SWEbench Live. SWE-bench Live accuracy of RPGs across commits under full reconstruction (Full) and incremental maintenance (Incr.). Model Strategy File-level Function-level Acc@1 Acc@5 Pre Rec Acc@1 Acc@5 Pre Rec GPT-4o GPT-4.1 Full Incr. Full Incr. 69.9 69.2 79.9 78.0 84.6 83.5 88.2 90.5 73.2 60.1 73.2 60. 82.5 69.8 81.4 69.0 53.8 50.5 67.4 64.7 68.5 69.4 80.3 81.9 60.6 41.1 59.4 41. 73.3 55.4 72.1 52.6 Figure 3: Cost Efficiency Comparison: RPG Rebuilding versus Incremental Updates across Commit History. Incremental Maintenance Ensures Sustainable Scalability. To assess feasibility, we measure maintenance costs across commit sequence. Figure 3 shows that full reconstruction scales linearly and exceeds 14.7M tokens, whereas our incremental strategy uses only 633K tokens by isolating semantic deltas. This 95.7% reduction 9 Figure 4: Distribution of Failure Modes on SWE-bench Verified. We analyze 100 failed trajectories per method with GPT-4o. Errors fall into four macro-groups: Tool & Execution, Search & Exploration, Reasoning & Interpretation, and Context & Scope, with 12 sub-types (T1T12). See Appendix C.3. confines heavy computation to one-time initialization and effectively decouples ongoing maintenance costs from repository scale, enabling sustainable long-term operation. Evolution Balance between Fidelity and Efficiency. To validate resilience against semantic drift during updates, we assessed representational fidelity by deploying agents on SWE-bench Live using RPGs from both strategies. Table 6 indicates that the \"Incr.\" strategy maintains statistical parity with the \"Full\" baseline. Specifically, while \"Incr.\" achieves slightly higher retrieval accuracy (81.9% Acc@5 compared to 80.3% for GPT-4.1), \"Full\" reconstruction retains marginal edge, surpassing \"Incr.\" by approximately 2% in Precision and Recall. This balance confirms that our sustainable evolution effectively preserves the repositorys semantic integrity with negligible degradation. 7.3. Agentic Navigability RPG Induces Structured Exploration. To investigate whether RPG structures reasoning, we visualized tool usage distributions across LLMs. Figure 7 reveals universal \"Search-then-Zoom\" pattern: agents prioritize broad topology traversal (ExploreRPG, SearchNode) to establish global map before narrowing to fine-grained analysis (FetchNode). This trend is more pronounced in stronger reasoners (e.g., Claude-4.5), which leverage RPGs structural context to support extended interaction horizons. These results confirm that RPG effectively guides agents from global comprehension to localized implementation. Dual-View Search Mitigates Navigational Failures. We manually analyzed 100 failed trajectories from GPT-4o to identify error patterns mitigated by the RPG structure. As shown in Figure 4, RPG reduces Search & Exploration failures compared to baselines. While systems like LocAgent and CoSIL utilize graph structures, they often suffer from Insufficient Coverage. RPG-Encoder addresses this by providing dual-path access, where semantic features Table 7: Impact of RPG Tooling on Agent Behavior on SWEbench Verified. Step-wise action distributions induced by the RPG interface across LLMs. enable broad global retrieval to expand the search space, while the structured hierarchy guides the agent to reduce Redundant Search. This multi-view navigation ensures agents can accurately localize intent before traversing implementation-level dependencies. Improved localization also reduces downstream errors in Context & Scope, keeping reasoning grounded in the correct implementation units. 8. Conclusion In this work, we introduce RPG-Encoder, transforming the Repository Planning Graph (RPG) into unified representation for repository reasoning. By coupling dense semantics with topological constraints, RPG-Encoder bridges architectural intent and implementation. Our evaluations show RPG is superior navigational map for localization and blueprint for reconstruction, achieving significantly higher fidelity than documentation. Furthermore, our incremental mechanism ensures consistency with lower overhead. Ultimately, RPG-Encoder establishes robust foundation for closed-loop software engineering by bidirectionally linking architectural intent with structural implementation."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude code: Agentic ai coding tool for terminal and ide. https://code.claude.com/docs/ en/overview, 2025a. Accessed: 2026-01. Anthropic. Claude sonnet 4.5. https://www.anthropic.com/claude/sonnet, 2025b. Accessed: 202601. Krzysztof Borowski, Bartosz Balis, and Tomasz Orzechowski. Semantic code graphan information model to facilitate software comprehension. IEEE Access, 12:2727927310, 2024. Jingyi Chen, Songqiang Chen, Jialun Cao, Jiasi Shen, and Shing-Chi Cheung. When llms meet api documentation: Can retrieval augmentation aid code generation just as it helps developers? arXiv preprint arXiv:2503.15231, 2025a. Yujia Chen, Mingyu Chen, Cuiyun Gao, Zhihan Jiang, Zhongqi Li, and Yuchi Ma. Towards mitigating api hallucination in code generated by llms with hierarchical dependency aware. In Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering, pages 468479, 2025b. Zhaoling Chen, Robert Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, and Xingyao Wang. Locagent: Graph-guided llm agents for code localization. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86978727, 2025c. Baijun Cheng, Cen Zhang, Kailong Wang, Ling Shi, Yang Liu, Haoyu Wang, Yao Guo, Ding Li, and Xiangqun Chen. Semantic-enhanced indirect call analysis with large language models. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, pages 430442, 2024. Google. Gemini cli: Open-source ai agent bringing gemini to terminal. https://geminicli.com/docs/, 2025. Accessed: 2026-01. Lars Gröninger, Beatriz Souza, and Michael Pradel. Changeguard: Validating code changes via pairwise learning-guided execution. Proceedings of the ACM on Software Engineering, 2(FSE):936956, 2025. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for multi-agent collaborative framework. In 12th International Conference on Learning Representations, ICLR 2024, 2024. Nihal Jain, Robert Kwiatkowski, Baishakhi Ray, Murali Krishna Ramanathan, and Varun Kumar. On mitigating In 2025 IEEE/ACM 47th International Conference on code llm hallucinations with api documentation. Software Engineering: Software Engineering in Practice (ICSE-SEIP), pages 237248. IEEE, 2025. Zhonghao Jiang, Xiaoxue Ren, Meng Yan, Wei Jiang, Yong Li, and Zhongxin Liu. Cosil: Software issue localization via llm-driven code repository graph searching. arXiv preprint arXiv:2503.22424, 2025. Li, LB Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et al. Starcoder: May the source be with you! Transactions on machine learning research, 2023. Zijie Lin, Yiqing Shen, Qilin Cai, He Sun, Jinrui Zhou, and Mingjun Xiao. Autop2c: An llm-based agent framework for code repository generation from multimodal content in academic papers. arXiv preprint arXiv:2504.20115, 2025. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report, 2024. Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Qizhe Shieh, and Wenmeng Zhou. Codexgraph: Bridging large language models and code repositories via code graph databases. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 142160, 2025. Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Yiming Huang, Yangyu Huang, Chengyu Yin, Ying Xin, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett Li, and Mao Yang. Rpg: repository planning graph for unified and scalable codebase generation, 2025. Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. Repoagent: An llm-powered open-source framework for repository-level code documentation generation. arXiv preprint arXiv:2402.16667, 2024. Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. How to understand whole software repository. arXiv preprint arXiv:2406.01422, 2024. OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed: 2026-01. OpenAI. Introducing swe-bench verified. introducing-swe-bench-verified/, 2024. Accessed: 2026-01. https://openai.com/index/ OpenAI. Gpt-4.1. https://openai.com/index/gpt-4-1/, 2025a. Accessed: 2026-01. OpenAI. Gpt-5. https://openai.com/gpt-5, 2025b. Accessed: 2026-01. OpenAI. Gpt-5-mini. https://platform.openai.com/docs/models/gpt-5-mini, 2025c. Accessed: 2026-01. OpenAI. Openai o3-mini. https://openai.com/index/openai-o3-mini/, 2025d. Accessed: 2026-01. Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph. arXiv preprint arXiv:2410.14684, 2024. Huy Phan, Hoang Phan, Tien Nguyen, and Nghi DQ Bui. Repohyper: Search-expand-refine on semantic In 2025 IEEE/ACM Second International Conference on AI graphs for repository-level code completion. Foundation Models and Software Engineering (Forge), pages 1425. IEEE, 2025. Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning. arXiv preprint arXiv:2504.17192, 2025. Wen Siang Tan, Markus Wagner, and Christoph Treude. Detecting outdated code element references in software repository documentation. Empirical Software Engineering, 29(1):5, 2024. Boshi Wang, Weijian Xu, Yunsheng Li, Mei Gao, Yujia Xie, Huan Sun, and Dongdong Chen. Improving code localization with repository memory. arXiv preprint arXiv:2510.01003, 2025a. 12 Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, et al. Epicoder: Encompassing diversity and complexity in code generation. arXiv preprint arXiv:2501.04694, 2025b. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. BOYANG YANG, JIADONG REN, SHUNFU JIN, YANG LIU, FENG LIU, BACH LE, and HAOYE TIAN. Kgcompass: Knowledge graph enhanced repository-level software repair. 2025. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, and Jishen Zhao. Orcaloca: An llm agent framework for software issue localization. arXiv preprint arXiv:2502.00350, 2025. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023. Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, et al. Swe-bench goes live! arXiv preprint arXiv:2505.23419, 2025. Qianhui Zhao, Li Zhang, Fang Liu, Junhang Cheng, Chengru Wu, Junchen Ai, Qiaoyuanhe Meng, Lichen Zhang, Xiaoli Lian, Shubin Song, et al. Towards realistic project-level code generation via multi-agent collaboration and semantic architecture modeling. arXiv preprint arXiv:2511.03404, 2025. Wenting Zhao, Nan Jiang, Celine Lee, Justin Chiu, Claire Cardie, Matthias Gallé, and Alexander Rush. Commit0: Library generation from scratch. arXiv preprint arXiv:2412.01769, 2024. 13 Appendix Contents for RPG-Encoder Detailed Methodology of RPG-Encoder 15 A.1 RPG Extraction: Semantic Lifting and Hierarchical Encoding . . . . . . . . . . . . . . . . . . . 15 A.1.1 Semantic Lifting via Prompted Semantic Parsing . . . . . . . . . . . . . . . . . . . . . . A.1.2 Latent Architecture Recovery for Hierarchical Encoding . . . . . . . . . . . . . . . . . . 17 A.1.3 Artifact Grounding: Anchoring Abstract Subtrees to Directory Scopes . . . . . . . . . . 19 A.2 Incremental Evolution: Differential Update and Maintenance . . . . . . . . . . . . . . . . . . . 21 A.2.1 Differential Event Detection and Operation Scheduling . . . . . . . . . . . . . . . . . . 21 A.2.2 Node Deletion with Structural Hygiene . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.2.3 Differential Modification Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.4 Node Insertion via Semantic Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.3 RPG Operation: Agentic Tool-use and Navigation Logic . . . . . . . . . . . . . . . . . . . . . . 24 A.3.1 Tool Interfaces and Prompt Specifications . . . . . . . . . . . . . . . . . . . . . . . . . 24 A.3.2 Tool-use Policy for Repository Understanding . . . . . . . . . . . . . . . . . . . . . . . 26 A.3.3 Execution Traces and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment Setup 28 B.1 Repository Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.1.1 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.1.2 Evaluation Targets at Multiple Granularities . . . . . . . . . . . . . . . . . . . . . . . . 29 B.1.3 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.2 Details about Repository Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.2.1 RepoCraft Benchmark Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.2.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 B.2.3 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 B.2.4 Model Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 More Results 34 C.1 Repository Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 C.2 Agent Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 C.4 Cost Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 Ablation 40 D.1 Repository Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Repository Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 14 A. Detailed Methodology of RPG-Encoder This section provides deep dive into the implementation details and algorithmic foundations of RPG-Encoder, expanding upon the three core phasesConstruction, Evolution, and Operationintroduced in Section 3 of the main text. A.1. RPG Extraction: Semantic Lifting and Hierarchical Encoding This subsection details the construction stage of RPG-Encoder, which transforms raw repository into hierarchically organized feature space (the semantic backbone of RPG) together with grounded mapping that links abstract functional nodes to concrete directory scopes. Concretely, the extraction stage proceeds in three steps: (1) Semantic lifting that converts low-level code entities into atomic functional features; (2) Latent architecture recovery that reorganizes these features into consistent three-level hierarchy; and (3) Artifact grounding that anchors each abstract subtree to compact set of physical directory paths. The resulting hierarchy serves as the Functionality SubGraph used by downstream agentic tools (Appendix A.3). A.1.1. Semantic Lifting via Prompted Semantic Parsing Global parsing strategy. Given repository R, semantic lifting is performed from global perspective rather than on individual files in isolation. We first identify all code entities of interest, including classes, methods and functions, and treat them as the fundamental semantic units to be analyzed. This global view allows the model to maintain consistent semantic granularity across the repository and reduces local biases introduced by file boundaries. Semantic units and batching. To accommodate repositories of varying scales while respecting model context limits, code entities are abstracted into semantic units and analyzed in batches under controlled token budget. Each semantic unit represents coherent functional entity, ensuring that semantically coupled components are interpreted in context. Batches are constructed to balance completeness and efficiency, such that every semantic unit is analyzed exactly once while enabling scalable processing of large repositories. Semantic feature representation. For each code entity u, the parser produces set of atomic semantic features puq ta1, a2, . . . u, where each ai is short verbobject phrase describing what the entity does rather than how it is implemented. These atomic features are intentionally constrained to be: (i) single-responsibility, (ii) implementation-agnostic, and (iii) lexically normalized (lowercase English, concise phrasing). This normalization is critical for subsequent routing and hierarchical encoding, since it provides stable semantic anchors for grouping and comparison across the repository. Prompt template (semantic parsing). We implement semantic lifting using the following prompt template, which enforces: (1) complete coverage of all functions in the chunk, (2) strict output schema, and (3) feature naming rules that avoid vague verbs and implementation details. The prompt returns JSON object mapping each function name to list of semantic features. Semantic Parsing Prompt ## Instruction You are senior software analyst. Your goal is to analyze all functions in the current input and return their key semantic features -- what each function does, not how it's implemented. ### Key Goals - Complete analysis: Provide semantic feature extraction for every function in the given input. Do not skip any function. - Batch perspective: Analyze all functions in the chunk together, considering their roles within the overall system. 15 - High-level behavior: Focus on the purpose and role of each function, not on low-level implementation details. - If multiple definitions share the same method name (e.g., property getter and setter for the same attribute), you may output that method name only once and merge their semantic features; you do not need to distinguish decorator variants. ## Feature Extraction Principles Follow these principles when analyzing functions: 1. Focus on the purpose and behavior of the function -- what role it serves in the system. 2. Do NOT describe implementation details, variable names, or internal logic such as loops, conditionals, or data structures. 3. If function performs multiple responsibilities, break them down into separate features. 4. Use your understanding of each function's name, signature, and code to infer its intent. 5. Only analyze functions included in the current input -- do not guess or invent other functions . 6. Do not omit any function, including utility or helper functions. ### Feature Naming Rules: 1. Use verb + object format (e.g., `load config`, `validate token`). 2. Use lowercase English only. 3. Describe purpose not implementation (focus on what, not how). 4. Each feature must express one single responsibility. 5. If method has multiple responsibilities, split into multiple atomic features. 6. Keep features short and atomic (prefer 3--8 words; no full sentences; no punctuation). 7. Avoid vague verbs (`handle`, `process`, `deal with`); prefer precise verbs (`load`, `validate `, `convert`, `update`, `serialize`, `compute`, `check`, `transform`). 8. Avoid implementation details (no loops, conditionals, data structures, control flow). 9. Avoid libraries/frameworks/formats (say `serialize data`, not `pickle object` / `save to json `). 10. Prefer domain/system semantics over low-level actions (`manage session` > `update dict`). 11. Avoid chaining actions (don't write `initialize config and register globally`; split into separate features). ## Output Format You must respond with the following structure: `<solution>` block -- JSON object mapping each function name to list of its semantic features. If function does not implement any meaningful features (e.g., it's stub), still include it with an empty list. ### Output Template: <solution> {{ \"func_name_1\": [\"feature one\", \"feature two\"], \"func_name_2\": [], ... }} </solution> ## Input Context ### Repository Name <repo_name> {repo_name} </repo_name> ### Repository Overview <repo_info> {repo_info} </repo_info> Post-processing and validation. We apply lightweight validation to guarantee the output is machineconsumable: (i) JSON parsing and schema checking (every function in the input must appear as key); (ii) feature list normalization (whitespace, casing, deduplication); and (iii) optional merging for decorator variants (e.g., property getter/setter) when they share the same method name, as allowed by the prompt. If the model 16 returns malformed output, we retry with minimal format correction instruction without changing semantic constraints. Illustrative example. Figure 5 shows an end-to-end example of semantic lifting, where raw code snippets are mapped to their corresponding atomic semantic features. Figure 5: Illustration of raw code snippets and their corresponding semantic features extracted via semantic parsing. A.1.2. Latent Architecture Recovery for Hierarchical Encoding Motivation. Semantic lifting yields set of fine-grained features distributed across many files, which is insufficient as planning substrate: flat features are hard to navigate, while directory-only grouping often overlooks logical roles. We therefore recover latent functional architecture that reorganizes the repository into consistent, interpretable, and searchable hierarchy. We enforce strict three-level feature path format: <functional area>/<category>/<subcategory>, which balances abstraction (top-level intent) and specificity (fine-grained specialization), while keeping routing and tool-based navigation tractable. Step 1: Domain discovery (functional areas). We first discover small set of high-level functional areas that act as architectural centroids. The model is guided to propose meaningful areas (e.g., DataProcessing, ModelTraining, EvaluationMetrics) while avoiding low-signal directories such as vendor code, tests, or documentation. 17 Domain Discovery Prompt ## Instructions You are an expert software architect and repository analyst. Your goal is to analyze the repository holistically and identify its main functional areas -- coherent, high-level modules or subsystems that reflect the repository's architecture and purpose . ### Guidelines - Think from software architecture perspective; group code into major, distinct responsibilities (e.g., data loading/processing, training/inference, evaluation/metrics, visualization/reporting, APIs/interfaces, configuration/utilities/infrastructure). - Avoid listing individual files or small helpers, third-party/vendor code, and build/test/docs directories. - Ensure each area is meaningful and represents clear responsibility in the codebase. ### Naming Principles - Single Responsibility: Each area should cover one logical concern (e.g., \"DataProcessing\", \" ModelTraining\"). - High-Level Abstraction: Group related submodules; separate distinct layers. - Consistency: Use PascalCase for names (e.g., \"FeatureExtraction\", \"EvaluationMetrics\"). - Meaningful Scope: - Merge closely related components (e.g., \"data_loader\", \"dataset\" -> \"DataProcessing\") - Avoid vague terms like \"core\", \"misc\", \"other\" - Use domain-specific names when appropriate (e.g., \"TextPreprocessing\", \"ImageSegmentation\") ### Output Format Return only the result in this exact format: <solution> [ \"functional_area1\", \"functional_area2\", \"functional_area3\", ... ] </solution> Step 2: Hierarchical construction (three-level paths). Given the discovered functional areas and the parsed feature groups, we perform hierarchical construction by assigning each top-level feature group to unique three-level target path. This step is formulated as constrained semantic assignment problem: the model must use only the provided functional areas for the first level, and it must generate intent-focused category/subcategory labels following the same semantic naming rules used in semantic lifting. Hierarchical Construction Prompt ## Instruction You are an expert software architect and large-scale repository refactoring specialist. ## Goal Reorganize and enrich the repository's parsed feature tree by assigning each top-level feature group (e.g., \"data_loader\", \"model_trainer\", \"metrics\") to the most semantically appropriate location within the target architecture. ## Target Path Format (STRICT) Each target path must have exactly three levels: `<functional_area>/<category_level_1>/<subcategory_level_2>` - `functional_area` must be one of the provided <functional_areas>. - `category_level_1` expresses broader purpose or lifecycle role. - `subcategory_level_2` adds precise specialization or context. - Each segment: concise (2--5 words), semantically meaningful, intent-focused. Examples: - \"data ingestion/pipeline orchestration/task scheduling\" - \"model training/optimization strategy/hyperparameter tuning\" Avoid filler labels (e.g., \"misc\", \"others\", \"core\", \"general\"). 18 ## Semantic Naming Rules When creating or adjusting semantic labels (categories/subcategories), follow: 1. Use \"verb + object\" phrasing; e.g., `load config`, `validate token`. 2. Use lowercase English only. 3. Describe purpose, not implementation. 4. Ensure each label expresses single responsibility. 5. When multiple distinct roles exist, use multiple precise labels rather than one overloaded label. 6. Avoid vague verbs such as `handle`, `process`, and `deal with`. 7. Avoid implementation details, including control-flow or data-structure references. 8. Avoid mentioning specific libraries, frameworks, or formats; prefer `serialize data` over ` pickle object` or `save to json`. 9. Prefer domain or system semantics over low-level actions; use `manage session` rather than ` update dict`. ## Scope Constraints - Only assign top-level groups (keys of <parsed_folder_tree>). - Exclude docs/examples/tests/vendor code unless essential to core functionality. - Do not invent new functional areas; use only those in <functional_areas>. - You may define new categories/subcategories as needed, but they must remain meaningful and consistent. ## Output Format (STRICT) Return only the JSON object wrapped exactly as: <solution> { \"<functional_area>/<category>/<subcategory>\": [\"top_level_group_1\", \"top_level_group_2\", ...], \"<functional_area>/<category>/<subcategory>\": [\"top_level_group_3\", ...] } </solution> Outputs and usage. The output of hierarchical construction is mapping from feature paths to sets of feature groups, which induces topological feature tree Tfeature. This tree serves two purposes: (i) it provides high-signal search scopes for intent-to-code mapping, and (ii) it supports routing and traversal by ensuring semantically coherent boundaries at each level. Illustrative examples. Figures 6 and 7 provide examples of domain discovery and hierarchical construction, respectively. A.1.3. Artifact Grounding: Anchoring Abstract Subtrees to Directory Scopes Problem formulation. To bridge the semantic hierarchy Tfeature with physical repository artifacts, we ground each abstract node to compact set of directory scopes. Let Lpvq denote the set of leaf nodes in the subtree rooted at v. For each leaf node Lpvq, let pathplq be its physical file path. We define the File Coverage Cpvq as the collection of parent directories for all leaves under v: Cpvq tdirppathplqq Lpvqu, where dirpq extracts the directory component of file path. We seek compact representation ˆπpvq that succinctly covers Cpvq while preserving functional boundaries across distinct modules. (1) Bottom-up propagation with Trie-based branching analysis. naive common-prefix (LCA) computation may over-collapse unrelated modules into overly general roots (e.g., /). To avoid this, we compute ˆπpvq via bottom-up propagation strategy that aggregates coverage and then simplifies it using Trie-based branching analysis: all paths in Cpvq are inserted into Prefix Tree, and only branching nodes (multiple children or path termination) are retained as grounded scopes. This yields minimal set of directory LCAs that dominate disjoint coverage regions while respecting module boundaries. Algorithm 1 provides the full procedure. 19 Figure 6: Illustrative example of the Domain Discovery phase. Figure 7: Example of the Hierarchical Construction phase. 20 return tdirppathpvqqu Algorithm 1 Bottom-Up Path Metadata Propagation Require: Feature Tree pV, Eq, Leaf paths pathpq Ensure: Grounded path assignments ˆπpvq for all 1: Function Propagatepvq: 2: if is Leaf then 3: 4: end if 5: Ð 6: for child Childrenpvq do 7: 8: end for 9: 10: return 11: End Function Ð Propagatepchildq ˆπpvq Ð ComputeLCApSq if node.is_branchingpq or node.is_terminalpq then 12: Function ComputeLCApSq: 13: Trie Ð BuildTriepSq 14: Ð 15: for node PostOrderpTrieq do 16: 17: 18: 19: 20: end for 21: return 22: End Function L.addpnode.pathq PruneSubtreepnodeq end if Ź Base case: Return physical directory Ź Recursively aggregate child coverage Ź Abstract concrete paths into logical scopes Ź Propagate full coverage to upper layers Ź Construct Prefix Tree from path set Ź Bottom-up traversal for optimal pruning Ź Identify meaningful functional boundary Ź Consolidate redundant sub-paths Complexity analysis. Let be the number of paths and the maximum directory depth. Trie construction and branching-node extraction take OpN Lq time, bounded by total path characters. Since propagation visits each feature node once in bottom-up pass, the total grounding overhead scales linearly with repository size and is negligible compared with LLM inference. A.2. Incremental Evolution: Differential Update and Maintenance This subsection details how RPG-Encoder maintains the Repository Planning Graph (RPG) under continuous codebase evolution. Given repository update (e.g., commit), our goal is to incrementally update the semantic hierarchy and its grounded mapping, ensuring the RPG remains faithful semantic reflection of the codebase while avoiding expensive full reconstruction. We formulate repository evolution as stream of atomic operations: Delete, Modify, and Insert. Each operation updates both (i) the local semantic representation of affected entities and (ii) their placement within the feature hierarchy. A.2.1. Differential Event Detection and Operation Scheduling From code diffs to semantic events. Given code change (e.g., git diff between two revisions), we extract changed code entities at the function/method granularity whenever possible. Each affected entity is categorized into one of three evolution events: Deletion: is removed from the repository. Modification: exists in both revisions but its implementation changes. Insertion: is newly introduced in the new revision. For Modification, we further distinguish between semantically stable edits and substantial semantic drift (Section A.2.3), which determines whether the update can be handled locally or requires structural relocation. 21 Scheduling principle. We schedule evolution operations under constraints that preserve structural consistency of the hierarchy, prevent intermediate abstract nodes from accumulating dead branches, and ensure that newly introduced entities do not disrupt the existing topological organization. A.2.2. Node Deletion with Structural Hygiene Algorithm 2 Incremental Deletion (Recursive Pruning) Require: Current graph G, target node id id Ensure: Updated graph 1: function DeleteNode(G, id) Ð GetNodepG, idq 2: 3: if then 4: return 5: end if parent Ð v.parent 6: RemoveNode(G, v) 7: PruneOrphans(G, parent) 8: 9: return 10: end function 11: function PruneOrphans(G, v) if _ IsRootpvq then 12: 13: 14: 15: end if if IsEmptypv.childrenq then return gp Ð v.parent RemoveNode(G, v) PruneOrphans(G, gp) 16: 17: 18: 19: 20: end function end if Ź Remove node and incident edges Ź Structural hygiene Ź Prune empty abstract category Ź Recurse upwards Motivation. Deletion must maintain structural integrity of the hierarchy. Removing leaf entity may render its ancestor abstract nodes semantically vacuous (i.e., nodes that no longer cover any concrete code entities). Without cleanup, these dead branches accumulate and reduce the signal-to-noise ratio for search and routing. Recursive pruning. We enforce structural hygiene via bottom-up pruning: after removing leaf node, we recursively delete any ancestor abstract node whose subtree becomes empty. Pruning terminates once an ancestor still has remaining children or once the root is reached. This mechanism prevents stale semantic categories from persisting after refactors and ensures that the hierarchy remains compact and representative of the current repository state. Algorithm. Algorithm 2 specifies the deletion procedure and the recursive pruning logic. A.2.3. Differential Modification Processing Motivation. code edit may either preserve the original intent (e.g., bug fixes, refactoring, parameter tuning) or substantially change functionality (semantic drift). Treating both cases identically is suboptimal: in-place updates are sufficient for minor edits, while major drift requires relocating the entity to semantically congruent domain. 22 Algorithm 3 Differential Modification Handling Require: Graph G, file , diff Ensure: Updated graph 1: function ProcessModification(G, , ) 2: xU `, , Ð ParseUnitDiffpq 1) Delete / Insert for all do Ð DeleteNodepG, u.idq end for for all ` do Ð InsertNodepG, u, LLMExtractpuqq end for 2) Update / Re-route for all do Ð GetNodepG, u.idq; v. Ð LLMUpdatepuq if SemanticShiftpvq ą τdrift then Ð DeleteNodepG, u.idq Ð InsertNodepG, u, LLMExtractpuqq end if 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: return 17: end function Ź logic drift Minor update vs. semantic drift. Given modified entity with old/new versions puold, unewq, we compute semantic features puoldq and punewq using the same parsing constraints as in extraction. We then assess drift based on: (i) feature overlap/consistency, and (ii) an LLM judgement constrained by explicit criteria. If drift is minor, we perform an in-place update of the nodes semantic summary; otherwise, we trigger re-routing. Re-routing as composition. When semantic drift is significant, we treat modification as composition of atomic operations: Modifypuq ñ Deletepuoldq ` Insertpunewq, which relocates the entity to new functional domain via the same semantic routing procedure used for insertion. This guarantees that the hierarchy reflects the updated intent rather than only updating text summaries in an incorrect domain. Algorithm. Algorithm 3 formalizes the differential modification procedure, including the branching logic between in-place update and re-routing. A.2.4. Node Insertion via Semantic Routing Motivation. Naively attaching new entities to the root (or fixed default module) breaks the semantic organization of the RPG and degrades downstream navigation. Instead, we treat insertion as semantic placement problem: find the most appropriate abstract parent node in the current feature hierarchy that best matches the new entitys functionality. Routing objective. Let be newly added code entity with semantic features puq. Starting from the root of the feature hierarchy, we iteratively select the child domain whose description best aligns with puq, drilling down until no more meaningful specialization is possible. This top-down semantic routing ensures that is inserted into the most specific functional domain available while preserving interpretability of the hierarchy. 23 Algorithm 4 Incremental Additions (LLM-Based Semantic Routing) Require: Current Graph G, New Unit u, Feature Summary fu Ensure: Updated Graph with inserted 1: function InsertNode(G, u, fu) 2: 3: 4: vbest Ð FindBestParentpG.root, fuq vnew Ð CreateNodepu, fuq AddEdge(G, vbest, vnew, Efeature) 5: return 6: end function Ź Attach to semantically determined parent 7: function FindBestParent(vcurr, ftarget) 8: 9: Candidates Ð Childrenpvcurrq IsAbstractpcq if Candidates then Context Ð tpc.id, c. P Candidatesu vchoice Ð LLM_RoutepContext, ftargetq if vchoice null then return FindBestParentpvchoice, ftargetq return vcurr end if 10: 11: 12: 13: 14: 15: else 16: 17: 18: 19: 20: end function end if return vcurr Ź Base case: No deeper abstract categories Ź Prompt LLM to select the best functional fit among children Ź LLM chose branch, drill down recursively Ź LLM decided no child is better fit Algorithm. Algorithm 4 formalizes the insertion procedure. At each step, the router considers the candidate children of the current node and chooses the best target; if no child is sufficiently compatible, the algorithm terminates and inserts at the current level. This prevents over-forcing entities into unrelated subtrees. Incremental evolution in RPG-Encoder is inherently local. Each atomic operation Complexity and scalability. affects only bounded region of the hierarchy, without requiring global reconstruction. As result, maintenance cost scales with the magnitude of the change rather than the size of the repository, enabling efficient and stable synchronization of the RPG under continuous development. A.3. RPG Operation: Agentic Tool-use and Navigation Logic This subsection details how RPG is operationalized as an actionable substrate for repository understanding. Beyond serving as semantic representation, RPG exposes tool interface that bridges high-level intents to concrete code entities and their dependency contexts. Concretely, we provide three complementary tools: SearchNode for intent-based discovery, FetchNode for precision context retrieval, and ExploreRPG for structural traversal on the RPG topology. A.3.1. Tool Interfaces and Prompt Specifications Design principles. The tool suite is designed to support common agent workflow in repository understanding: (i) start from vague or behavioral intents and obtain candidate code anchors; (ii) verify anchors with precise source context; and (iii) expand locally to cover call chains and related components. To ensure tool outputs are deterministic and machine-consumable, each tool prompt defines strict parameter schema and return format. 24 SearchNode: intent-based discovery. SearchNode unifies semantic discovery and textual retrieval. It supports three modes: features (intent Ñ feature nodes / mapped code entities), snippets (keyword/symbol search over the repository), and auto (feature mapping first, followed by snippet search when needed). Importantly, search_scopes can restrict the search to selected feature subtrees, leveraging the grounded hierarchy constructed in Appendix A.1 to improve precision. SearchNode Tool Prompt ## Tool Name: SearchNode ### Description Unified search tool for repository navigation. Use it to (1) map high-level functional/behavioral descriptions to concrete code entities via RPG mapping, and/or (2) retrieve concrete code snippets via symbol/file/keyword search. Prefer behavior-to-code mapping when you don't know the exact file/class/function name; then narrow down with snippet search. Tip: Avoid vague terms; use concrete behavior phrases or high-signal identifiers. ### Parameters { \"tool_name\": \"SearchNode\", \"parameters\": { \"mode\": \"<'features' 'snippets' 'auto'. Required. 'auto' may run both: feature-mapping first, then snippet search.>\", \"feature_terms\": \"<List of concrete behavioral/functionality phrases. Required when mode is ' features' or 'auto'.>\", \"search_scopes\": \"<List of valid feature entity paths to restrict the Functionality SubGraph. Optional.>\", \"search_terms\": \"<List of file paths, qualified entities (file:Class.method), or high-signal text keywords. Required when mode is 'snippets' or when 'auto' proceeds to snippet search.>\", \"line_nums\": \"<Two integers [start, end] to extract lines from specific file. Requires an exact file path. Optional.>\", \"file_path_or_pattern\": \"<File path or glob pattern to restrict snippet search. Default: '**/*.py'>\", } } ### Returns - If feature search runs: matched feature nodes mapped to code entities (feature name, code entity, file path, line range when available) - If snippet search runs: matched code snippets, complete files, or located entities based on search terms / line ranges FetchNode: precision retrieval and verification. FetchNode retrieves exact source context and metadata for known candidates (code entities or feature paths). It is used as verification step after discovery to ensure the agent reasons on faithful code snippets rather than speculative guesses. FetchNode returns file paths, line ranges, entity types, mapped feature information, and code preview. FetchNode Tool Prompt ## Tool Name: FetchNode ### Description - Retrieve precise metadata and source context for known code or feature entities. - Use this tool to verify candidate code locations after identifying them through searches or graph exploration. - Returns exact file path, entity type, start/end lines, mapped feature information, and code preview. ### Parameters { \"tool_name\": \"FetchNode\", \"parameters\": { \"code_entities\": \"<List of existing and validated code entities in the current repository; non-existent paths or speculative entities may be ignored. Optional.>\", 25 \"feature_entities\": \"<List of existing and validated feature paths in the current repository; non-existent entries may be ignored. Optional.>\" } } ### Returns - Entity type (file/class/method/feature) - Feature paths and code content (with source context / preview) - Start/end lines and mapped feature information (when available) ExploreRPG: topological traversal. ExploreRPG exposes the structural connectivity of RPG, enabling traversal along dependency edges (imports, invokes, inherits, etc.) and/or containment/composition relations. Starting from validated anchors, the agent can traverse upstream/downstream to uncover dependencies, impacted components, and semantically related regions. ExploreRPG Tool Prompt ## Tool Name: ExploreRPG ### Description - Explore call chains and functional paths in the Repository Planning Graph. - Starting from known code or feature entities, traverse upstream/downstream to discover related functions, files, and feature nodes. ### Parameters { \"tool_name\": \"ExploreRPG\", \"parameters\": { \"start_code_entities\": \"<Optional list of existing code entities in the current repository ( file paths, classes, functions, or qualified names). Non-existent/speculative entities may be ignored or rejected.>\", \"start_feature_entities\": \"<Optional list of existing feature paths in the current repository . Non-existent entries may be ignored or rejected.>\", \"direction\": \"<Traversal direction: 'upstream' (dependencies), 'downstream' (dependents), or 'both'. Default: 'downstream'.>\", \"traversal_depth\": \"<Maximum traversal depth. Default: 2. Use -1 for unlimited depth.>\", \"entity_type_filter\": \"<Optional filter restricting traversal node types. Valid values: ' directory', 'file', 'class', 'function', 'method'.>\", \"dependency_type_filter\": \"<Optional filter restricting dependency edge types. Valid values: 'composes', 'contains', 'inherits', 'invokes', 'imports'.>\" } } ### Returns - Connected nodes and edges (code or feature view) - Hints for invalid or fuzzy matches A.3.2. Tool-use Policy for Repository Understanding Canonical tool orchestration. We adopt simple and robust orchestration policy that prioritizes semantic grounding before reading large contexts. Given natural-language intent I, the agent executes: 1. Semantic discovery (SearchNode/features or auto): convert into concrete behavioral terms and retrieve candidate feature nodes and mapped code entities. If available, supply search_scopes to restrict discovery to the most relevant functional subtrees. 2. Precision verification (FetchNode): for top candidates, fetch exact code context (file path + line range + preview) and confirm semantic compatibility. Candidates that cannot be verified are discarded. 3. Local expansion (ExploreRPG): from verified anchors, traverse dependency edges (e.g., invokes, imports) to recover call chains, utilities, and related modules. This step is used to (i) locate the root cause, (ii) map the impact surface, or (iii) identify integration points. 26 4. Pinpoint retrieval (optional SearchNode/snippets): if the target remains ambiguous, run snippet search with high-signal identifiers obtained from previous steps (exact symbols, file paths, error strings), optionally extracting specific line ranges. Fallback rules. When semantic discovery returns insufficient recall (e.g., missing/weak feature matches), the agent falls back to snippets mode to bootstrap concrete anchors, then returns to FetchNode and ExploreRPG. When snippet search yields too many matches, the agent tightens constraints by adding (i) feature scopes, (ii) file path patterns, or (iii) symbol-qualified queries. This policy minimizes wasted context and reduces hallucination risk: SearchNode provides intent-to-code grounding, FetchNode ensures the agent reasons on exact source, and ExploreRPG reveals topological structure that cannot be reliably inferred from local snippets alone. A.3.3. Execution Traces and Examples We illustrate the practical efficacy of these tools through the execution traces shown in Figure 8. These traces demonstrate how the agent navigates from abstract intents to specific code implementations, leveraging both the semantic hierarchy and the dependency topology of RPG. Figure 8: Execution traces of the three primary agentic tools. SearchNode maps abstract intent to concrete code; FetchNode retrieves precise source context; and ExploreRPG reveals topological connections and call relations. As depicted in the figure, each tool provides distinct structural signals that support the agents reasoning: SearchNode (Left): demonstrates intent-to-code grounding by mapping behavioral query (e.g., expression serialization) to concrete code entity and its associated feature description. This step transforms ambiguous intent into executable anchors. FetchNode (Center): retrieves precise source context for candidate entity (e.g., _check_vector), including exact line ranges and preview snippet, enabling verification and preventing reasoning on speculative locations. ExploreRPG (Right): traverses the RPG topology from verified anchor (e.g., kernS) to expose invocation and dependency relations. By showing edges such as invokes and their connected nodes, the agent can recover call chains and impacted modules, supporting systematic debugging and repositorylevel understanding. B. Experiment Setup This appendix provides additional experimental setup details. It is organized into two parts: (i) repository reconstruction and (ii) repository understanding, including detailed baseline configurations and formal metric definitions. B.1. Repository Understanding B.1.1. Experiment Setup We describe the implementation details and baseline configurations for the repository understanding task. Our goal is to facilitate reproducibility and ensure fair and controlled comparisons across different localization pipelines. Common evaluation protocol. All methods are evaluated under shared protocol, including identical datasets, evaluation metrics, and termination criteria. Unless otherwise specified, we use the same preprocessing, canonicalization, and ranking-based evaluation procedures described in Section B.1.3. Backbone models. We evaluate multiple large language model backbones to assess the robustness of each localization pipeline to the underlying model choice, including o3-mini(o3-mini-20250131) (OpenAI, 2025d), GPT-4o(gpt-4o-20241120) (OpenAI, 2024), GPT-4.1(gpt-4.1-20250414) (OpenAI, 2025a), GPT-5(gpt5-20250807) (OpenAI, 2025b), DeepSeek-V3.1 (Liu et al., 2024), and Claude-Sonnet-4.5 (Anthropic, 2025b). Baselines. We compare against representative repository-level localization pipelines: Agentless (Xia et al., 2024), LocAgent (Chen et al., 2025c), CoSIL (Jiang et al., 2025), and OrcaLoca (Yu et al., 2025). For each baseline, we retain the original algorithmic structure and design choices, making only minimal and necessary adaptations to the benchmark interface and backbone model to ensure compatibility with the shared evaluation protocol. Agentless Agentless (Xia et al., 2024) employs staged non-agentic workflow: (1) Direct Prediction: LLM predicts suspicious files directly from the issue description. (2) Filtered Retrieval: It performs embedding search within search space pruned of \"irrelevant folders.\" (3) Candidate Aggregation: Results from both streams are merged to maximize file-level recall.(4) Element Localization: Granularity is narrowed from files to specific code elements. (5) Edit Localization: The system pinpoints line-level edit targets within those elements. To ensure reproducibility, we apply specific parameter constraints corresponding to these stages. Globally, across all ranking steps, we maintain top_n=10 and enforce determinism via num_samples=1. Stage-specific configurations are set as follows: For Retrieval (Step 2), we employ jinaai/jina-embeddings-v3 as the embedding backbone and set filter_type=\"given_files\" to strictly enforce the LLM-generated folder constraints. For Fine-grained Localization (Steps 4-5), we enable the compress flag, which optimizes context utilization by condensing verbose code details while preserving salient information for precise element and edit identification. LocAgent LocAgent (Chen et al., 2025c) is dependency-graph integrated agent framework for repositorylevel localization, which wraps the dependency graph into three tools: (1) SearchEntity: searches relevant files/classes/functions from text queries (supports fuzzy match). (2) TraverseGraph: multi-hop traverses dependency relations from seed entity to surface connected candidates. (3) RetrieveEntity: fetches the full metadata and code of selected entities for final inspection and ranking. For LocAgent, we do not impose any restriction on the number of iterative search rounds. To maximize the chance of producing valid final prediction, we set the maximum retry budget to 3 attempts, and take the first well-formed output that satisfies the evaluation interface. We run LocAgent in function-calling mode with parallelism set to 1. We impose no explicit limit on the number of iterative search steps, and set the maximum retry budget to 3 to maximize the chance of producing valid final output. CoSIL CoSIL (Jiang et al., 2025) adopts hybrid agentic and workflow strategy that explores code dependencies via iterative call-graph searching: it first performs broad exploration with module call graph, then expands to function call graph for deeper search, while using pruning and reflection to control direction and stabilize tool-formatted outputs. Following CoSILs implementation details, We run CoSIL in function-calling mode with parallelism set to 1. We do not explicitly cap its iterative graph-search rounds, and allow up to 3 retries to maximize the chance of obtaining valid final output. OrcaLoca OrcaLoca (Yu et al., 2025) combines agentic code-graph exploration with dynamic-analysis signal, using bug reproduction and regression tests to guide iterative search and candidate verification. It introduces two key mechanisms: (1) Action decomposition factorizes the large search action space into hierarchical decision process (e.g., first selecting candidate classes, then narrowing to files), and applies top-k selection for class decomposition and file decomposition; (2) Distance-aware context pruning retains only fixed budget of the most relevant context entries (12 in our setup), prioritizing code units that are closer to the current targets in the dependency/call graph to improve context efficiency. We follow the original setup: for action decomposition, it applies top-k selection with k3 for class decomposition and k2 for file decomposition, and uses distance-aware context pruning with budget of 12 retained entries. B.1.2. Evaluation Targets at Multiple Granularities To assess localization quality at different levels of abstraction, we evaluate predictions at two granularities using unified canonicalization scheme. Each predicted or ground-truth location is mapped to canonical string key through granularity-specific formatter, ensuring consistent comparison across methods. File-level. At the file level, location is represented by its relative file path, e.g., path/to/file.py. For file an instance i, both the ground-truth set consist of file paths. Filei level evaluation measures whether method can correctly identify the source files that contain the relevant implementation. and the ranked prediction list π file Function-level. At the function level, location is represented by fully qualified entity identifier within file, formatted as file:entity. To avoid artificial mismatches caused by syntactic variations, constructor annotations are normalized by removing the suffix .__init__ when present. For example, a/b.py:Foo.__init__ is canonicalized to a/b.py:Foo. Function-level evaluation assesses whether method can precisely localize the relevant function or class definition beyond the file boundary. When function-level annotations are unavailable for given instance, we restrict the evaluation to the file level. For both ground-truth and predicted locations, we remove duplicate entries while preserving their original order before computing all ranking metrics. 29 B.1.3. Metrics We formalize the ranking-based evaluation protocol for file-level localization. Let denote the set of evaluation instances. For each instance I: denotes the set of ground-truth relevant files (or locations), with cardinality Gi mi Gi πi ppi,1, pi,2, . . . , pi,πiq denotes the ranked prediction list produced by method. . We define binary hit indicator sequence hi t0, 1uπi as # hi,j if pi,j Gi, 1, 0, otherwise, 1, . . . , πi. (2) All metrics are computed per instance and then averaged over I. Accuracy@k (Acc@k). Accuracy@k measures whether at least one ground-truth item appears within the top-k predictions: fi ÿ 1 Acc@k kÿ j1 fl hi,j ě . (3) iPI In our experiments, we report results for t1, 3, 5u. Recall. Recall measures the fraction of ground-truth items that are successfully retrieved by the model across the entire ranked list: Recall 1 $ & řπi ÿ j1 hi,j Gi % 0, iPI , Gi ą 0, Gi 0. Precision. Precision measures the proportion of correct predictions among all retrieved items: Precision ř πi j1 hi,j πi . 1 ÿ iPI (4) (5) B.2. Details about Repository Reconstruction In this section, we provide comprehensive description of the experimental setup, workflow logic, and termination protocols for the two comparative settings: ZeroRepo-Doc and ZeroRepo-RPG. B.2.1. RepoCraft Benchmark Construction To rigorous evaluate the capabilities of automated repository reconstruction, we adapted the RepoCraft benchmark. The benchmark consists of real-world Python repositories selected for their popularity and structural complexity. Documentation Compilation. critical component of our control setting is the provision of high-quality, official API documentation to serve as the ground-truth specification. We constructed this documentation dataset by processing the source files located in the docs/ directory of each target repository. Specifically, we utilized Sphinx, the standard Python documentation generator, to compile the raw reStructuredText (reST) or Markdown files into unified textual representation, as illustrated in Figure 9. This compiled documentation 30 Figure 9: Illustration of the hierarchical organization of the API documentation (left) and an example of the granular content within specific documentation node (right), detailing function signatures and parameter descriptions. Table 8: Statistics of the compiled API documentation across the six subject repositories. Metric Django Pandas Requests Sklearn Stats SymPy 2,863 # Files # Tokens 374,586 1,314,314 1,536 15 52, 1,052 1,244 235,319 405,022 180,505 610 captures the official definitions of classes, functions, and module hierarchies, ensuring that the baseline agents operate on the exact same informational standard as human developers reading the official manual. As summarized in Table 8, the scale of this context is substantial: the compiled documentation spans total of 7,320 files and over 2.5 million tokens across the six subject repositories, presenting rigorous benchmark for unstructured long-context understanding. B.2.2. Baselines Backbone Framework: ZeroRepo. We adopt ZeroRepo (Luo et al., 2025), state-of-the-art framework originally architected for zero-shot repository generation, as our unified execution backbone. In its native configuration, ZeroRepo operates by synthesizing Repository Planning Graph (RPG) from high-level user intents. This graph serves as rigid execution schedule, guiding the agent through topological traversal where functionalities are implemented and verified via Test-Driven Development (TDD). The frameworks modular design decouples planning (Graph Construction) from execution (Code Generation), making it an ideal substrate for isolating and comparing different planning strategies. Adaptation for Reconstruction. For the task of Repository Reconstruction, the objective shifts from hallucinating new systems to recovering existing ground-truth architectures. To evaluate the efficacy of our extracted RPG against unstructured information, we adapt ZeroRepo into two distinct configurations by modifying its planning source: 31 Figure 10: Comparison of reconstruction workflows adapted from ZeroRepo. Top (ZeroRepo-Doc): The baseline relies on unstructured API documentation, requiring the agent to perform autonomous self-planning and manual state tracking. Bottom (ZeroRepo-RPG): Our method leverages the extracted RPG as structured prior, enabling deterministic topological traversal and context-aware batched execution. In this configuration, we lobotomize the graph-based planning ZeroRepo-Doc (Unstructured Baseline). engine to simulate conventional developer workflow. The agent is initialized solely with the raw API documentation of the target repository and an empty progress tracking file. Without pre-computed graph, the reconstruction proceeds in an open-ended, iterative loop: Self-Planning: In each iteration, the agent must manually cross-reference its progress against the documentation to identify pending modules and formulate its own immediate objectives. Manual State Tracking: The agent bears the full cognitive burden of global state maintenance, creating and updating checklist to prevent redundant or missed implementations. Execution: It employs standard TDD, writing reproduction scripts and code based on its self-determined plan. The process terminates only when the agent subjectively judges that all documented requirements are met. In our proposed configuration, we replace the generative ZeroRepo-RPG (Graph-Guided Reconstruction). planner with the RPG-Encoder. Instead of generating graph from scratch, we inject the pre-extracted RPG as ground-truth topological prior. This shifts the burden of planning from the agent to the substrate, transforming reconstruction into deterministic traversal: Topological Traversal: The extracted nodes are arranged into strict dependency-based topological order. This guarantees that prerequisite modules are implemented before dependent ones, eliminating circular dependency errors. LLM-Driven Batching: To optimize efficiency, an LLM scheduler previews the topological queue and dynamically aggregates semantically coherent nodes (e.g., set of related utility functions) into single implementation batch. Context-Aware Execution: The system executes these batches sequentially. The agent is relieved of global planning and focuses purely on implementation, leveraging the explicit context ( , m) provided by the graph nodes to generate code that aligns with the established architecture. 32 B.2.3. Metrics To rigorously evaluate the fidelity of repository reconstruction, we adapt metrics from RepoCraft (Luo et al., 2025) to capture four complementary dimensions: functional coverage, spectral novelty, executable fidelity, and structural scale. These metrics collectively assess (i) whether the ground-truth functionalities are successfully recovered, (ii) whether the model introduces extraneous capabilities, (iii) whether the reconstructed logic is semantically correct, and (iv) the complexity level of the realized system. Functionality Coverage (Recovery Rate). This metric quantifies the recall of the reconstruction processspecifically, what fraction of the ground-truth functional topology has been successfully restored. We define the reference feature set tc1, . . . , cKu using the ground-truth repositorys functional signature (derived from developer documentation or canonical feature lists). The generated functionalities tg1, . . . , gNu are extracted from the reconstructed codebase. To measure alignment, we employ K-Means clustering with as fixed centroids, mapping each generated feature gi to its nearest reference intent pgiq tcOODu (where cOOD denotes Out-Of-Distribution). Feature assignments are further refined by an LLM-as-Judge to mitigate semantic drift. Coverage is defined as the proportion of ground-truth categories \"hit\" by the reconstruction: Coverage 1 Kÿ 1 j1 Dgi G, pgiq cj . (6) higher coverage indicates that the RPG successfully guided the agent to rebuild more complete functional footprint of the original system. In the context of reconstruction, pure imitation is not always Functionality Novelty (Extrapolation). possible; models may \"hallucinate\" valid but unrequested features. To capture this behavior, we measure the proportion of generated functionalities that fall outside the ground-truth taxonomy. Novelty is calculated as the fraction of generated nodes assigned to the cOOD centroid: Novelty 1 Nÿ i1 1 pgiq cOODs . (7) While high novelty is desirable in open-ended generation, for reconstruction tasks, it characterizes the models tendency to extrapolate or deviate from the strict blueprint provided by the RPG. Functionality Accuracy (Executable Fidelity). Coverage validates intent, but fidelity requires correctness. We evaluate the semantic equivalence of the reconstructed code against the original repositorys behavior using adapted test suites. We report two statistics: Voting Rate (Feature Presence): The fraction of tasks where the validation pipeline confirms that plausible implementation of the target algorithm exists. This measures the systems ability to instantiate the planned logic. Success Rate (Semantic Correctness): The fraction of tasks where the reconstructed code passes the unit tests. This measures the executable validity of the implementation. Code-Level Statistics (Structural Scale). Finally, to ensure the reconstruction is not merely skeletal prototype, we compare the scale of the generated codebase against realistic standards. Metrics are computed over filtered source files (excluding non-production artifacts like tests or benchmarks) to assess structural complexity: File Count: Reflects the modular granularity and file-level organization. Normalized LOC: Effective lines of code (excluding comments/blanks), proxying implementation volume. Code Token Count: Total lexical tokens, indicating the density and complexity of the synthesized logic. 33 B.2.4. Model Configuration To strictly separate structural reasoning from coding capability, we utilize distinct models for the planning and execution phases. We employ GPT-4o (OpenAI, 2024) as the extraction engine to analyze the source repository and generate the Repository Planning Graph (RPG), leveraging its strong reasoning abilities to ensure high-fidelity structural representation. For the downstream reconstruction tasks within the ZeroRepo framework, we evaluate two different backbone models: GPT-5-mini and GPT-4.1. This variation allows us to assess the robustness of our method across models with differing parameter scales. C. More Results C.1. Repository Reconstruction In this section, we provide detailed per-repository results on RepoCraft, reporting performance for each repository individually to complement the main aggregated statistics. Table 9: Main results on Requests reconstruction tasks in RepoCraft. Framework Gold Projects (Reference) Human Developers Backbone ZeroRepo-Doc (Baseline) ZeroRepo-RPG (Ours) GPT-4.1 GPT-5-mini GPT-4.1 GPT-5-mini Coverage (%) Ò Pass Rate (%) Ò #Files Ò nLOC Ò Code Tokens Ò 100.0 54.6 72.7 100.0 100. 93.0 / 93.0 13.2 / 32.1 27.5 / 54.9 88.4 / 95.3 88.4 / 95.3 17 7 28 23 2273 968 1,890 2901 3503 22,297 8141 15,842 24,734 27, Table 10: Main results on Scikit-Learn reconstruction tasks in RepoCraft. Framework Gold Projects (Reference) Human Developers Backbone ZeroRepo-Doc (Baseline) ZeroRepo-RPG (Ours) GPT-4.1 GPT-5-mini GPT-4.1 GPT-5-mini Coverage (%) Ò Pass Rate (%) Ò #Files Ò nLOC Ò Code Tokens Ò 100.0 68.1 72.3 100.0 100.0 99.5 / 100. 53.9 / 64.0 55.6 / 66.3 79.3 / 99.5 82.8 / 99.5 185 181 76 269 267 65, 6,787 12,007 71,835 96,806 592,187 69,468 101,988 698,602 900,381 Table 11: Main results on Sympy reconstruction tasks in RepoCraft. Framework Gold Projects (Reference) Human Developers Backbone ZeroRepo-Doc (Baseline) ZeroRepo-RPG (Ours) GPT-4.1 GPT-5-mini GPT-4.1 GPT-5-mini Coverage (%) Ò Pass Rate (%) Ò #Files Ò nLOC Ò Code Tokens Ò 100.0 97.8 / 100.0 33.3 66.7 91.7 95.8 36.4 / 48.2 52.5 / 64. 63.6 / 89.6 81.4 / 96.6 699 29 135 220 288 218,924 943, 605 10,454 59,283 80,534 5347 99,460 565,680 686,951 34 Table 12: Main results on Statsmodels reconstruction tasks in RepoCraft. Framework Gold Projects (Reference) Human Developers Backbone ZeroRepo-Doc (Baseline) ZeroRepo-RPG (Ours) GPT-4.1 GPT-5-mini GPT-4.1 GPT-5-mini Coverage (%) Ò Pass Rate (%) Ò #Files Ò nLOC Ò Code Tokens Ò 100.0 96.8 / 100.0 66.7 75.0 96.6 97.7 71.7 / 75.9 57.8 / 79. 89.8 / 98.9 91.4 / 100.0 271 87 163 303 297 83,325 5,235 25, 66,045 92,618 893,824 55,688 274,983 709,424 942,411 Table 13: Main results on Pandas reconstruction tasks in RepoCraft. Framework Gold Projects (Reference) Human Developers Backbone ZeroRepo-Doc (Baseline) ZeroRepo-RPG (Ours) GPT-4.1 GPT-5-mini GPT-4.1 GPT-5-mini Coverage (%) Ò Pass Rate (%) Ò #Files Ò nLOC Ò Code Tokens Ò 100.0 99.0 / 100.0 81.8 75.0 97.4 97.4 63.4 / 82.5 56.2 / 81.4 89.5 / 92.8 80.1 / 97. 217 726 180 146 115 106,447 943,873 14,770 14, 21,657 30,066 127,326 125,957 201,484 249,094 Table 14: Main results on Django reconstruction tasks in RepoCraft. Framework Gold Projects (Reference) Human Developers Backbone ZeroRepo-Doc (Baseline) ZeroRepo-RPG (Ours) GPT-4.1 GPT-5-mini GPT-4.1 GPT-5-mini Coverage (%) Ò Pass Rate (%) Ò #Files Ò nLOC Ò Code Tokens Ò 100. 83.3 83.3 100.0 100.0 83.0 / 100.0 61.3 / 77.5 66.2 / 81.8 96.2 / 98.1 91.9 / 97.5 228 276 408 369 109,457 917,622 8114 16,053 43,468 61, 687,718 135,523 391,260 496,570 35 Analysis of Repository-Specific Performance. The breakdown of results across six diverse repositories reveals consistent patterns confirming the scalability of RPG-Encoder. First, in large-scale scientific libraries like Scikit-Learn and Sympy, the documentation-based baseline suffers from severe structural collapse, often recovering less than 20% of the file structure (e.g., only 29 files for Sympy with GPT-4.1). In contrast, RPGEncoder leverages hierarchical planning to maintain architectural integrity, reconstructing code volumes that closely approximate the human-written gold standard (e.g., generating 900k tokens for Scikit-Learn). Second, for frameworks with intricate inter-dependencies like Django, RPG-Encoder achieves 100% coverage and pass rate exceeding 96%, demonstrating that topological constraints effectively guide the agent through complex logic flows where linear documentation fails. Ultimately, across all datasets, RPG-Encoder consistently bridges the gap between sparse intent and dense implementation, proving its robustness as domain-agnostic substrate for repository reconstruction. C.2. Agent Behavior Figure 11: Breakdown of the Tool/Action ratio for each tool across different models. This metric reflects the frequency of specific tool invocations relative to the total actions performed. We calculated the tool-to-action ratios across all evaluation episodes to quantify the behavioral strategies of each model, as shown in Figure 11. The results indicate that the agents generally maintain robust balance between keyword-based retrieval and structural navigation. While SearchNode serves as foundational tool for locating initial entry points, the significant utilization of ExploreRPGespecially by high-performing models like GPT-4ohighlights its critical role in the reasoning loop. This distribution suggests that capable agents effectively recognize the value of ExploreRPG, strategically employing it to leverage topological connections and gain holistic understanding of the codebase, rather than relying solely on local keyword matches. C.3. Error Analysis To look beyond aggregate metrics and understand the behavioral divergence between RPG-enhanced agents and baselines, we conducted manual qualitative analysis on stratified sample of failure cases. Based on the taxonomy defined in Table 15, we observe distinct error patterns that highlight the structural advantages of RPG while exposing persistent challenges in agentic reasoning. C.4. Cost Analysis Beyond effectiveness and behavioral patterns, we further analyze the computational cost of different agents to assess their practical efficiency. We report the average token consumption and monetary cost per instance across all evaluation episodes. Figure 12: Case study of tool misuse and execution failure. COSIL exhibits Wrong Tool Selection by investigating unrelated functions (e.g., wcs_to_celestial_frame) instead of the core issue (world_to_pixel), leading to persistent environment errors and failure to reproduce the bug. 37 Figure 13: Case study of search and exploration failure. The agent attempts to resolve lambdify bug in SymPy but fails due to Insufficient Coverage (T3), missing critical code generation logic, and Query Formulation Errors (T4), where searches were too broad to locate the root cause. 38 Table 15: Error Taxonomy for Code Localization Agents (T1T12) Category Error Type Description Tool & Execution Errors Wrong Tool Selection (T1) Tool Call Error (T2) Search & Exploration Errors Insufficient Coverage (T3) Query Formulation Error (T4) Redundant Search (T5) Reasoning & Interpretation Errors Evidence Misinterpretation (T6) Info Synthesis Failure (T7) Hallucination (T8) Context & Scope Errors Problem Misunderstanding (T9) Granularity Mismatch (T10) Codebase Context Failure (T11) Multi-Factor Errors Cascading Errors (T12) Agent uses an inappropriate tool for the task; e.g., using grep when find is needed, or reading files when search is more suitable. Malformed parameters, invalid paths/regex, or runtime failures (e.g., No such file, syntax errors, timeouts). Agent fails to systematically explore relevant code areas; misses obvious entry points from the problem statement. Searches use wrong keywords, overly broad/narrow scope, or miss obvious terms from the problem statement. Agent makes repetitive tool calls; stuck in loops; repeats similar queries without progress. Agent misreads tool output; confuses similar symbols; incorrect understanding of code relationships. Agent fails to connect evidence across multiple steps; doesnt build coherent picture from findings. Agent invents non-existent files, functions, or paths; outputs fabricated information not grounded in evidence. Agent fundamentally misunderstands the task; focuses on the wrong aspect of the problem. Output at the wrong specificity level (too coarse/fine); e.g., file-level when function-level is needed. Agent fails to understand project structure, conventions, or architecture. Multiple interacting errors where early mistakes propagate; recovery attempts fail. Overall, RPG-enhanced agents incur moderately higher costs than lightweight baselines due to more frequent structured exploration and reasoning steps. However, this increase is well-controlled and scales sublinearly with performance gains, indicating that improved task success is not achieved through excessive tool usage or redundant interactions. In particular, high-performing models demonstrate favorable costperformance trade-off by allocating additional budget primarily to informative exploration rather than repeated failed actions. We note that for LocAgent with o3-mini, the reported cost can be unusually low, as the agent often terminates after only 12 rounds and may occasionally produce an answer without invoking any tool. This reflects early termination behavior rather than efficient full-loop reasoning, and is specific to the LocAgent o3-mini pairing. For completeness, we provide the full cost breakdown for all models, including prompt tokens, completion tokens, and estimated monetary cost, in Table 16. 39 Table 16: Efficiency analysis comparing the average number of agent interaction rounds and cost per instance. LLM o3-mini GPT-4o GPT-4.1 GPT-5 Claude-4.5 Method OrcaLoca LocAgent CoSIL RPG-Encoder OrcaLoca LocAgent CoSIL RPG-Encoder OrcaLoca LocAgent CoSIL RPG-Encoder OrcaLoca LocAgent CoSIL RPG-Encoder OrcaLoca LocAgent CoSIL RPG-Encoder Steps 10. 2.14 18.95 6.53 29.26 8.95 20.65 8.22 20.22 11.94 19.77 6.75 36.93 6.48 19. 6.34 11.32 7.54 12.60 16.75 Cost ($) 0.10 0.04 0.18 0. 0.98 0.50 0.30 0.20 0.46 0.86 0.24 0.18 0.75 0.49 0.31 0. 0.88 0.88 0.48 1.32 Eff. 4.86 21.75 4.07 7.07 0.54 1.35 2. 3.84 1.48 0.76 3.10 4.63 1.16 1.64 2.64 4.25 0.91 0. 1.64 0.71 D. Ablation D.1. Repository Reconstruction Ablation Setup To rigorously isolate the contribution of hierarchical topological signals to repository reconstruction, we conduct controlled ablation study on the scikit-learn repository from the RepoCraft benchmark. All experiments utilize GPT-5-mini as the underlying reasoning engine. We define two specific ablation settings to evaluate the agents capability in structural inference: Function-Level Ablation (-Func): In this setting, we strip fine-grained implementation guidance by removing metadata from leaf nodes (functions) and eliminating function-to-function dependency edges from the RPG. Consequently, the agent retains file-level boundaries but must autonomously deduce necessary function signatures from high-level features and independently derive their logical implementation order. File-Level Ablation (-File/-Func): Building upon the function-ablated graph, we further excise all file-level structural information, including file nodes and directory paths. This setting retains only the abstract semantic feature descriptions. It forces the agent to perform ab initio structural organization: the agent must semantically aggregate discrete features to synthesize file architectures, design class and function hierarchies, and plan the global implementation sequence without any reference directory topology. Analysis of Structural Clustering Behaviors. Table 17 illustrates how different topological priors influence the LLMs organization strategies. (1) Granular Encapsulation in Function Ablation. When function-level metadata is removed, the model exhibits preference for explicit object-oriented modeling. The significant increase in class count (524 Ñ 815) and function definitions indicates that without specific procedural 40 Table 17: Feature Distribution across Different Modes Category Full Mode Func Ablation File Ablation Count Features AvgStd Count Features AvgStd Count Features AvgStd File Class Function 250 524 1366 4943 3584 1359 19.77 20.94 6.84 6.51 0.99 0.07 275 815 6664 3676 2988 24.23 27.11 4.51 4.76 1.87 2.07 154 466 1202 5175 2462 2713 33.60 36.99 5.28 6.16 2.26 2.25 guidance, the agent tends to atomize logic into smaller, discrete class-based units for encapsulation. (2) Feature Consolidation in File Ablation. Conversely, the absence of file-level constraints shifts the behavior towards coarse-grained aggregation. The model consolidates features into fewer physical files (250 Ñ 154), resulting in higher feature density per file (19.77 Ñ 33.60). This suggests that without directory boundaries to enforce separation, the model inherently clusters semantically related functionalities into larger, unified modules rather than distributing them across file system hierarchy. D.2. Repository Understanding Ablation Setup To quantify the individual contributions of structural connectivity and semantic annotation, we evaluate ZeroRepo-RPG under two degradation protocols: Dependency Graph Ablation (w/o Dependency): In this variant, we sever all static dependency edges (Edep) from the RPG. This ablation strictly limits the ExploreRPG tool by occluding execution logic; the agent loses the ability to traverse call graphs or trace upstream/downstream relationships, forcing it to navigate solely based on physical file hierarchy. Semantic Feature Ablation (w/o Feature): Here, we strip high-level feature descriptions ( ) and functional subordination edges (Efeature). This degradation fundamentally impairs semantic navigability: the SearchNode tool regresses from intent-based retrieval to rigid keyword matching, while ExploreRPG ceases to display functional hierarchies. Furthermore, retrieved nodes lack refined summaries, compelling the agent to infer utility solely from raw identifiers. Efficiency Degradation from Component Loss. Table 18 quantifies the operational overhead introduced by removing structural and semantic priors. The full RPG-Encoder model consistently achieves the minimal trajectory length and cost across both LLMs, validating the synergistic efficiency of the complete graph. (1) Impact of Semantic Features: Removing feature metadata (w/o Features) incurs the most significant penalty in exploration steps (e.g., rising from 8.22 to 9.23 on GPT-4o). Without high-level summaries to guide intent-based retrieval, the agent is forced into trial-and-error loop, repeatedly fetching raw code to verify relevance, which drastically prolongs the search process. (2) Impact of Dependency Structure: Severing dependency edges (w/o Dependency) primarily inflates token cost (e.g., `$0.09 on GPT-4.1). Lacking explicit execution paths, the agent must manually traverse the file system and read broader contexts to deduce logical connections, leading to inefficient information consumption compared to the surgical navigation enabled by the full RPG. Table 18: Impact of Dependency and Feature Ablations on Steps and Cost. Setting GPT-4o GPT-4.1 Avg Steps Cost ($) Avg Steps Cost ($) RPG-Encoder w/o Dependency w/o Features 8.22 8.53 9. 0.20 0.27 0.30 6.75 7.31 7.37 0.26 0.35 0."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "Tsinghua University",
        "UCSD"
    ]
}