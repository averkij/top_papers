{
    "paper_title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder",
    "authors": [
        "Ronen Kamenetsky",
        "Sara Dorfman",
        "Daniel Garibi",
        "Roni Paiss",
        "Or Patashnik",
        "Daniel Cohen-Or"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale text-to-image diffusion models have become the backbone of modern image editing, yet text prompts alone do not offer adequate control over the editing process. Two properties are especially desirable: disentanglement, where changing one attribute does not unintentionally alter others, and continuous control, where the strength of an edit can be smoothly adjusted. We introduce a method for disentangled and continuous editing through token-level manipulation of text embeddings. The edits are applied by manipulating the embeddings along carefully chosen directions, which control the strength of the target attribute. To identify such directions, we employ a Sparse Autoencoder (SAE), whose sparse latent space exposes semantically isolated dimensions. Our method operates directly on text embeddings without modifying the diffusion process, making it model agnostic and broadly applicable to various image synthesis backbones. Experiments show that it enables intuitive and efficient manipulations with continuous control across diverse attributes and domains."
        },
        {
            "title": "Start",
            "content": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder Ronen Kamenetsky1 Roni Paiss2 Sara Dorfman1 Daniel Garibi1 Or Patashnik1 Daniel Cohen-Or 1Tel Aviv University 2Google DeepMind ronen94.github.io/SAEdit/ 5 2 0 2 6 ] . [ 1 1 8 0 5 0 . 0 1 5 2 : r Figure 1. We train Sparse AutoEncoder (SAE) to lift the text embeddings into higher-dimensional space, where we identify disentangled semantic directions (e.g. for laughing). These directions can then be applied to specific tokens within the input of text-to-image model to facilitate continuous image editing. As shown on the right, our token-level editing steers the model to incorporate the relevant attribute (laughing) into the subject in the image that corresponds to the chosen token (e.g., woman or kid), while allowing the attributes intensity to be continuously adjusted through scale factor, ωt."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Large-scale text-to-image diffusion models have become the backbone of modern image editing, yet text prompts alone do not offer adequate control over the editing process. Two properties are especially desirable: disentanglement, where changing one attribute does not unintentionally alter others, and continuous control, where the strength of an edit can be smoothly adjusted. We introduce method for disentangled and continuous editing through token-level manipulation of text embeddings. The edits are applied by manipulating the embeddings along carefully chosen directions, which control the strength of the target attribute. To identify such directions, we employ Sparse Autoencoder (SAE), whose sparse latent space exposes semantically isolated dimensions. Our method operates directly on text embeddings without modifying the diffusion process, making it model agnostic and broadly applicable to various image synthesis backbones. Experiments show that it enables intuitive and efficient manipulations with continuous control across diverse attributes and domains. Large-scale text-to-image diffusion models have revolutionized the field of image synthesis [47, 48, 51]. Consequently, they have become powerful foundation for wide array of image manipulation and editing methods [11, 29, 39, 57]. These methods have demonstrated remarkable success in range of edits, including adding new elements, replacing parts of the scene, and modifying the attributes of existing objects. Two properties are particularly desirable in such edits: disentanglement, which ensures that modifying one attribute does not unintentionally affect others, and continuous control, which allows adjusting the magnitude of the edit. While there has been significant progress in achieving disentangled editing, finding controllable representations that enable edits which are both disentangled and continuous remains major challenge. Text prompts alone struggle to provide this level of control, as their discrete nature prevents smooth intensity adjustment and their holistic influence often leads to unintended changes. For example, to control the intensity of smile, user must resort to distinct coarse categorical descriptions like slight smile versus 1 l B u Original image terrified + Original image old + Figure 2. Naıvely applying T5 edit direction (top) by interpolating T5 embedding of target edit, introduces entangled changes that may distort the scene. This can appear as an insufficient edit (left example) or as the modification of unwanted elements (right example). In contrast, edit directions found by the SAE (bottom) yield disentangled edits that preserve identity and achieve the intended modification. wide grin, rather than smoothly varying the intensity. This limitation motivates research into underlying semantic control mechanisms that are both continuous and disentangled. In pursuing this goal, some works have focused on general, training-free methods that manipulate the diffusion models internal representations [4, 16, 27]. While versatile, these techniques often struggle with disentanglement, where an edit intended to be local inadvertently causes widespread, undesirable changes to the overall image style and composition. To achieve higher fidelity, other approaches have pursued task-specific optimization, training dedicated module for each edit [20, 22, 55], with the modules weights acting as the controllable representation for the edit. However, while often producing high-quality results, this strategy is inherently unscalable, demanding unique training pipeline for every possible modification. In this work, we propose method for disentangled and continuous image editing through the fine-grained manipulation of text embeddings at the token-level. Our approach leverages Sparse Autoencoder (SAE) [13], an unsupervised model trained to reconstruct its input from sparse, high-dimensional latent space. The sparsity of this latent space induces semantically disentangled dimensions, which in turn enable the discovery of meaningful editing directions for each token. Specifically, we derive an edit direction in the SAEs space by comparing the sparse representations of two prompts that differ by the desired edit description (e.g., person and smiling person), identifying the entries most correlated with the change. We then construct an editspecific direction as sparse vector that modifies only these highly relevant entries. This disentangled direction is added to the sparse representation of the prompt, and can be scaled to continuously control the magnitude of the target attribute, while preserving the rest of the image. This approach leverages the SAE to uncover disentangled directions that are difficult to identify directly in the raw embedding space, as qualitatively demonstrated in Figure 2. Our method operates solely on the text embeddings, leaving the denoising process untouched. In this setup, the diffusion model serves merely as renderer: it receives the edited semantic instructions and translates them into visual output. As result, the method is model-agnostic and can be applied to any text-to-image backbone that shares the same text encoder, without additional training or finetuning. Through extensive experiments, we demonstrate the effectiveness of our method in providing both continuous and highly disentangled semantic edits. We validate the versatility of our approach by applying the same framework to various generative models, including two image synthesis backbones, without any model-specific training. Importantly, we show that our method enables wide range of intuitive, magnitude-controlled manipulations from simple text commands, as demonstrated in Figure 1. We further show that our method can be applied to real images using inversion techniques. 2. Related Work Image Editing with Diffusion Models The success of diffusion models in image synthesis [6, 30, 45, 47, 51, 53, 54] has led to their widespread adoption for the more challenging task of real image editing. Unlike pure generation, editing requires careful balance between preserving an images original attributes and introducing controlled, text-guided changes. Common strategies include manipulating the denoising process through feature injection [1, 11, 29, 43, 44, 57] or applying partial noise schedules with new text condition [7, 17, 32, 39, 49, 56]. key requirement for applying these methods to real images is an inversion technique that can find an initial noise capable of reconstructing the image [18, 25, 28, 33, 34, 40, 41, 52]. 2 Continuous Image Editing with Diffusion Models challenge in this area is achieving fine-grained, continuous control over semantic attributes. To achieve this kind of control some methods perform Task-specific optimization methods, which yield high-fidelity, disentangled edits but are not scalable, requiring separate, costly process for each new attribute, such as training dedicated LoRA adapter [22], optimizing text token [55] or to train numerous person-specific DreamBooth LoRAs [50] and then trains classifier in the weights space [20]. Conversely, training-free methods that discover semantic directions in existing latent spaces [4, 16, 19, 26, 27] are general-purpose but often struggle with the precision and disentanglement of specialized models. Other works like SliderSpace [23] explore unsupervised discovery of models latent variations but are not designed for direct, text-guided editing. Our work aims to bridge this gap, offering general framework that provides the disentangled control of task-specific methods without the need for per-edit training. 3. Preliminary - Sparse AutoEncoders Sparse Autoencoders (SAEs) are neural architectures designed to learn interpretable and disentangled highdimensional latent representations [13]. An SAE typically consists of simple encoder, often single linear layer with non-negative activation, and linear decoder. The model is trained with dual objective: = Lrec + α Lsparse, (1) where Lrec is standard reconstruction loss, and Lsparse is set of regularization terms that encourages the latent representation to be sparse. This sparsity constraint encourages the SAE to learn dictionary-like representation, where small set of active latent features often corresponds to distinct semantic attribute of the input. This property makes SAEs powerful tool for interpreting the otherwise dense and opaque hidden states of large language models. Consequently, SAEs have been successfully applied to the internal states of large language models to uncover meaningful, semantic features [8, 13, 24]. For example, [8] found that certain features in the sparse representation are active only when specific entities, such as US presidents, are mentioned in the text. Identifying which features correspond to specific concepts enables model steering, allowing for direct control over model behavior by manipulating its internal activations [2, 5]. The basic SAE framework can be extended with more advanced variants and sparsity regularization techniques, which are detailed further in Section C. the application of Sparse Autoencoders (SAEs) to diffusion models has been explored, with initial works focusing on interpretability and concept unlearning [14, 36]. Recently, 3 4. Method We present method for text-driven image editing that provides both disentanglement and continuous control. Our approach is based on manipulating the text embeddings of frozen text-to-image model. We train Sparse Autoencoder (SAE) on these embeddings, which provides space in which disentangled directions corresponding to semantic attributes can be found. Editing is then performed by adjusting the embeddings along these directions to achieve controlled manipulations. Specifically, given frozen text encoder, we train Sparse Autoencoder (SAE) on its output embedding space (details in Sec. 4.1). The SAE is composed of an encoder, Senc, and decoder Sdec. The encoder maps dense text embeddings into high-dimensional, disentangled latent space where distinct semantic concepts are isolated, while the decoder reconstructs the original embedding from this sparse representation. Once trained, manipulations are applied directly in this sparse SAEs space by adjusting specific entries in the latent representation. The modified representation is then passed through the SAEs decoder to recover an edited text embedding, which can be fed into any compatible text-to-image model (e.g., Flux) that uses the same text encoder architecture. In this way, the SAE acts as lightweight, pluggable module that enables disentangled and semantic control over the final generated image. The editing direction is obtained from source prompt Psrc (e.g. man) and target prompt Ptgt (e.g. smiling man) , details in Sec. 4.2. We apply the edit direction by multiplying it with scale factor and adding it to the sparse representation of the specific source token in Psrc to be edited (e.g. the man token). The magnitude of the edit is dictated by this scale factor, allowing for continuous control over the attributes intensity (details in Sec. 4.3). We demonstrate our method on the T5 text encoder [46], which is widely adopted as the text conditioning module in many state-of-the-art text-to-image models. For the image generation backbone, which acts as renderer for our text embedding manipulations, we primarily use the Flux [6] diffusion transformer (DiT). 4.1. SAE Training We train our Sparse Autoencoder (SAE) on dataset of text embeddings. To create this dataset, we first process corpus of text prompts through the frozen T5 text encoder and collect the resulting token embeddings, excluding padding tokens. Notably, unlike typical SAE applications that focus on intermediate transformer layers, we train our SAE on the final output of the text encoder, as these are the exact representations that are continuously processed by the Diffusion Transformer (DiT) throughout the denoising steps."
        },
        {
            "title": "The SAE is trained on the embeddings of individual text",
            "content": "Figure 3. We train the Sparse Autoencoder on token embeddings obtained from frozen T5 encoder, using reconstruction and sparsity losses. tokens, using the objective function from Eq. 1, the process illustrated in Fig. 3. Here, Lrec is the standard reconstruction loss (e.g., Mean Squared Error) between the SAEs input and output embeddings. We control the target level of sparsity via another hyperparameter which sets the desired number of non-zero activations for each tokens latent code. 4.2. Obtaining an edit direction Motivated by prior work on SAEs in language models, which shows that specific entries in the sparse representation activate only in the presence of particular semantic attributes [13, 24], we aim to detect such entries to construct disentangled directions in the SAEs latent space for image editing. To do so, we use source prompt Psrc (e.g. woman) and target prompt Ptgt (e.g. woman laughing). We first encode all text tokens in both prompts using the SAE encoder, Senc, to obtain sparse token representations. Since it is unknown apriori which tokens hold the semantic information for concept [35], we use elementwise max-pooling to aggregate their sparse representations into single, sparse vector for each prompt. As Psrc and Ptgt are semantically similar except for the edited attribute, the activated entries in maxpool(Senc(Psrc)) and maxpool(Senc(Ptgt)) should overlap substantially, with their differences centering around entries corresponding to the edit-specific attribute. To identify the entries that correlate with the requested edit, we compute an entry-wise ratio, R, between the source and target prompt: = maxpool(Senc(Ptgt)) maxpool(Senc(Psrc)) + ϵ , (2) where ϵ is small constant added for numerical stability. The entries in with the highest values correspond most strongly to the edit-specific attribute. Next, to isolate Figure 4. Extracting Edit Directions. We derive an edit direction from prompt pair that isolates single attribute. Both prompts are encoded with the SAE, and their token representations are aggregated via max-pooling. By comparing the two resulting sparse vectors, we identify the key features corresponding to the desired change. The edit direction is sparse vector composed of only these key features, taken from the target prompts representation. these key entries , we normalize the ratio vector, Rnorm = R/ max(R), and apply predefined threshold ρ [0, 1]. This yields set of indices, , corresponding to the most relevant entries for the edit: = {i Rnorm > ρ}. (3) Finally, we use this set of indices to construct the disentangled edit direction, dedit, as sparse vector, as illustrated in Fig. 4. The direction is defined to be zero everywhere except at the identified indices, where it takes its values from the target representation: (cid:40) [dedit]i = [Senc(Ptgt)]i 0 if M, if / (4) Improving directions robustness To enhance the robustness of our derived edit directions, we aggregate information from set of multiple source-target prompt pairs rather than relying on single pair. Given desired edit, defined by the pair of texts descriptions Psrc and Ptgt, we use an LLM to construct sentence pairs that share the same underlying semantic relationship. This process, generalizes the specific edit into an abstract concept. For example, to create direction for happiness, the LLM generates pairs that add this attribute to various contexts, such as (man on the beach, happy man on the beach) and (man eating cake, happy man eating cake). We then apply our direction-finding procedure to each of the prompt pairs, resulting in set of steering vectors {di}N i=1. These vectors are stacked to form direction matrix: = [d1, . . . , dN ]T . To extract the most prominent Figure 5. Applying the edit direction. An aggregated edit direction is scaled to adjust edit magnitude and applied to the sparse representation of the relevant source token (e.g., man). The result is then decoded back into the T5 embedding space, and used to condition the text-toimage model. and consistent direction representing the shared attribute across all examples, we perform Singular Value Decomposition (SVD) on D. The singular vector corresponding to the largest singular value is then selected as our final, robust edit direction dedit. 4.3. Applying the edit direction Once the edit direction dedit is derived, we apply it to the source prompt Psrc. To ensure the manipulation is localized, we modify only the embedding of the specific token to be edited (e.g., the woman token), which we denote as etgt. The magnitude of the edit is controlled by scalar factor ω, allowing for continuous, fine-grained control over the attributes intensity. The final, edited text embedding for the token, tgt, is produced by first encoding the original tokens embedding with Senc, adding the scaled direction in the sparse latent space, and then decoding the result with Sdec: token = Sdec(Senc(etgt) + ω dedit). (5) Setting ω = 0 recovers the original embedding, while progressively increasing ω strengthens the visual effect. This new token embedding, tgt, replaces the original in the prompt. Finally, the manipulated text embeddings are used to condition the renderer. Specifically, for diffusion models, we follow the standard editing approach to preserve the overall structure of the source image. This involves using the same initial noise, xT , that was used to generate the source image, and only substituting the original token embeddings with our modified ones. This ensures that the changes in the final generated image are driven exclusively by our disentangled edit. Fig. 5 provides schematic of this entire editing pipeline. 4.4. Injection Schedule The denoising process in diffusion models operates hierarchically: early timesteps are crucial for establishing the global structure and layout of an image, while later steps refine fine-grained details and textures [3, 12, 31, 44, 59]. Consequently, for fine-grained edits that aim to preserve the original structure, prior work has shown that it is often optimal to begin the editing manipulation only at later timesteps, after the core layout is formed [32, 33]. Building on this insight, we introduce an exponential injection schedule that applies the edit direction with increasing intensity over time. For base scale factor ω and diffusion step t, we define the time-dependent scale ωt as: ωt = min (cid:0)etω 1, τ (cid:1) , (6) where τ is hyperparameter that acts as an upper bound on the edit strength. This exponential formulation offers key advantage over linear schedules: it applies the edit very gently in the early, structure-defining timesteps and progressively increases its influence as the process moves into the later, detail-refining stages. This gradual application better aligns with the hierarchical nature of image synthesis, preserving global structure while enabling powerful, fine-grained modifications. 5. Experiments We conduct extensive experiments to evaluate our methods ability to provide continuous control and disentangled edits that preserve the subjects identity. Similar to prior work, we focus our evaluation on human subjects, challenging domain that demands strong disentanglement to preserve identity and offers the most meaningful application of continuous magnitude control. To demonstrate its modelagnostic nature, we apply our approach to both Flux [6] and Original image blonde + Original image laugh + Original image mustache + Original image old (man only) + Original image + hat Original image smile + Original image African-American + Original image + glasses Figure 6. Qualitative Results. Our method enables diverse range of continuous and disentangled semantic edits across various image styles. We demonstrate the ability to add attributes (e.g., mustache, glasses), change expressions (smile, laugh), and perform highly localized edits, such as modifying the age of only one person in scene. Original image laughing + Original image bloom (tree) + Original image surprised + Original image square (mirror) + Figure 7. Results with SD3.5. These demonstrate that our method integrates seamlessly with models relying on T5, enabling consistent and faithful edits across architectures. Figure 8. Our methods versatility extends beyond human subjects, enabling continuous and disentangled control over object attributes like seasonal appearance and object shape. Stable Diffusion 3.5 [21]. Unless otherwise specified, all results are generated using Flux. We also show its applicability to real image editing through integration with standard inversion techniques. For quantitative evaluation, we measure preservation with LPIPS [60] and semantic accuracy with VQA-Score [38]. Implementation details in the Appendix A. 5.1. Qualitative Results We present qualitative results generated by our method, SAEdit, in Figures 1, 6, 7 and 8. Figure 6 shows wide variety of continuous edits on human subjects. Our method successfully changes expressions (e.g., adding smile), modify attributes (e.g., making hair blonde), and add accessories (e.g., hats or glasses). Crucially, these edits are highly localized. For instance, we demonstrate the ability to modify the age of single person in multi-subject image while leaving the other person and the background entirely untouched. The results also highlight the continuous nature of our control. As shown in the examples, attributes such as the intensity of laugh or the degree of age can be smoothly scaled. This allows users to precisely tune the magnitude of the desired effect while the rest of the image content is faithfully preserved. The approach is not limited to human subjects and generalizes to broad range of semantic concepts, as shown in Figure 8. Finally, to demonstrate the model-agnostic nature of SAEdit, Figure 7 shows that the same edit directions produce consistent, high-quality results when applied to different T5-based model, Stable Diffusion 3.5."
        },
        {
            "title": "We provide additional qualitative results",
            "content": "in Appendix B.1. 6 o l S m N + t n e + Original image angry + Figure 9. Ablation study. We demonstrate how each component progressively improves the quality of an angry edit. direction from single prompt pair results in weak edit with unintended modifications. Aggregating prompts produces more robust and semantically accurate direction, but can still alter fine details. Adding our exponential injection schedule preserves the original images details (e.g., the necklace and hair color), yielding the most faithful and disentangled result. 5.2. Ablation Figure 9 provides qualitative ablation study of our methods components, demonstrating their respective contributions to the final result. As baseline, deriving an edit direction from single prompt pair (top row) preserves the subjects identity, but the intended semantic change to the expression is weak and insufficient. Aggregating the direction from prompt pairs (middle row) successfully strengthens the edit as required, but causing minor unwanted changes to the hair color, the necklace, and the dress texture. Finally, incorporating our exponential injection schedule (bottom row) resolves this issue by preserving these fine-grained details while maintaining the strong semantic edit, thus achieving high-quality and disentangled result. Our quantitative ablation study is detailed in Appendix B.3. 5.3. Comparisons We evaluate our method against several state-of-the-art approaches for continuous image editing, highlighting its ability to provide disentangled control without per-edit optimization. We compare against methods from both optimization-based and training-free categories. From the optimization-based group, we evaluate Concept Sliders [22] by using their official SDXL-trained LoRAs as well as Lo7 Figure 10. Quantitative comparison. We compare our method to other baselines on image preservation and prompt fidelity (topright is better). RAs we trained on the Flux architecture for direct comparison. In the training-free category, we compare against FluxSpace [16], adjusting its λfine parameter to control edit magnitude. We also evaluate against AttrCtrl [4], method that proposes both training-free and an optimization-based variant. For Flux Kontext [37], which does not natively support continuous edit scaling, we implement two proxy baselines for magnitude control over the edit strength: the first involves varying the Classifier-Free Guidance (CFG) strength, while the second uses an LLM to generate prompts corresponding to light and extreme versions of each edit (instruction prompts and more details provided in Appendix B.4). Quantitative Comparisons To evaluate the fine-grained and continuous control of our method, we constructed custom evaluation set. This set is based on 63 images, each generated from unique prompt created by large language model [42]. Each prompt describes scene containing person. For each source image, we applied set of 6 to 8 different semantic edit directions, resulting in 432 unique edit scenarios. To assess the continuity of these edits, we then generated each scenario at 3-5 distinct magnitude levels, producing final evaluation set of at least 1,296 images per method. The complete list of prompts and edit directions is provided in Appendix B.2. We quantitatively assess our method on two key axes: preservation and prompt adherence. To measure the preservation of original content, we use LPIPS [60]. To prompt adherence with the edit, we compute VQA-based score [38]. This score is the delta between the VQA score of the edited image against the target prompt and that of the source image against the same prompt, which isolates the semantic change introduced by the edit. Figure 10 presents the quantitative comparison between"
        },
        {
            "title": "Opponent\nMethod",
            "content": "Image Pres. Prompt Adher."
        },
        {
            "title": "Overall",
            "content": "Flux Kontext (CFG) Flux Kontext (LLM) ConceptSlider (Flux) Flux Space 73% 60% 71% 59% 71% 68% 67% 92% 70% 70% 71% 93% Table 1. User Study. Pairwise win rate of our method against other methods. our method and other methods at varying levels of edit intensity. The results demonstrate that our method outperforms all other approaches. Notably, our zero-shot method is superior even to task-specific techniques that are explicitly trained for each edit type. This indicates that our approach successfully achieves the dual goals of high semantic accuracy for the required edit and strong preservation of the original content. Furthermore, the metrics show smooth and predictable progression as the edit magnitude increases, confirming that our method provides true continuous control and allows users to precisely tune the intensity of an effect. User Study To complement our quantitative analysis, we conducted user study to evaluate the perceptual quality of our method against competing approaches. For fairness, we limited our comparison to methods that also use the Flux model, ensuring the source images were as similar as possible. In pairwise comparison, we presented participants with results from our method and competing method, showing three distinct levels of edit intensity for each to assess continuous control. Users were asked to state their preference based on three criteria: Image Preservation, Prompt Alignment (which included the gradualness of the effect), and Overall Quality. In total, our user study gathered 390 pairwise comparison responses. More details in Appendix B.6 The results, summarized in Table 1, show that our method was significantly preferred over all other approaches in all categories. This suggests that users found our edits achieve better balance of successfully applying the desired change while faithfully preserving the original image content. Qualitative Comparisons Figure 11 presents qualitative comparison between our method and other approaches, all operating on the Flux model. While the results for most methods are taken directly from our quantitative evaluation set, we manually optimized the prompts for the Flux Kontext baselines to ensure the strongest possible comparison, as their default outputs were often suboptimal (see Appendix B.5). For example, for the CFG-based baseline, we found the prompt Make the man look slightly like kid with CFG scales of 1.5 and 1.6 yielded the most plausis i p o ) ( t x F a u r Original image young + Figure 11. Each row showcases the results of different editing method for the same edit. Our method (bottom row) produces more disentangled result that better preserves the subjects identity compared to the competing approaches. ble results. The visual comparison highlights the superior disentanglement of our method. For instance, in contrast to ConceptSliders, our approach achieves perfect reconstruction of the subjects jacket while applying the desired edit. Similarly, when compared to Flux Kontext, our method successfully modifies the subjects age in more natural and gradual manner, demonstrating more precise control over the semantic attributes. More results in Appendix B.5. Real image editing Our methods applicability extends to the challenging task of real image editing. To achieve this, we first use state-of-the-art inversion technique, UniInv Flow [33], to obtain the initial noise corresponding to given source image. Our SAE-based manipulation is then applied to the text embeddings as previously described. Figure 12 presents several results of this combined approach. As shown, we can apply high-fidelity, continuous edits to real photographs, successfully modifying expressions (cry, laughing) and attributes (old). Importantly, these edits preserve the subjects core identity and background details, demonstrating that our disentangled control is effective even in the demanding context of real image manipulation. 8 Original image cry + Original image old + Original image laughing + Original image laughing + Figure 12. Real Image Editing with Image Inversion. Our method seamlessly integrates with inversion techniques, allowing for high-fidelity edits on real-world images. Leveraging UniFlow [33] to invert the source image into the diffusion models latent space, we demonstrate continuous control over expressions and attributes. The edits maintain the subjects identity and background fidelity across all intensity levels. Orignal Image + beard Orignal Image + green Figure 13. Our method struggles with out-of-distribution (OOD) edits that conflict with strong priors in the base model. For example, applying beard edit changes the woman into man (left), while making the dog green results in an unnatural, animatedstyle dog (right). 5.4. Limitations While our method identifies robust and disentangled edit directions, we observe that further refinement is sometimes possible. For certain complex edits, manually selecting or de-selecting few specific entries in the sparse direction vector can yield even more disentangled results. In addition, our methods ability to disentangle is constrained by the inherent biases of the underlying text-toimage model. When an edit is requested that is strongly out-of-distribution (OOD), our approach can fail to maintain disentanglement. As shown in Figure 13, attempting to add beard to woman results in the subjects perceived gender being changed to male. Similarly, making dog green alters its texture to appear unnatural and cartoonlike. We hypothesize these failures occur because the SAE cannot fully separate concepts that are fundamentally entangled in the base models worldview. 6. Conclusions In this work, we introduced novel framework that provides both disentangled and continuous control for text-toimage editing. Our method leverages Sparse Autoencoder (SAE) on text embeddings to create sparse representation 9 where semantic attributes are isolated. This sparse representation is the key to our methods success. Having isolated individual attributes facilitates disentangled edits, where the subjects core identity is preserved. Our approach enables token-level manipulation, providing fine-grained and continuous control over the magnitude of given attribute. key advantage of our design is that editing is decoupled from rendering: we modify only the text embedding, enabling any compatible text-to-image backbone model to act as the renderer. SAEs are primarily known for their role in interpretability of language models, yet in this work we demonstrate that they can be harnessed for image generation, yielding fine-grained editing capabilities. Image editing has recently seen remarkable progress, yet precise finegrained control remains an open challenge, and we believe this work will encourage further advances in that direction."
        },
        {
            "title": "References",
            "content": "[1] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar AverbuchElor, and Daniel Cohen-Or. Cross-image attention for zeroshot appearance transfer, 2023. 2 [2] Dana Arad, Aaron Mueller, and Yonatan Belinkov. Saes are good for steering if you select the right features, 2025. 3 [3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers, 2023. 5 [4] Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Melvin Sevi, Vincent Tao Hu, and Bjorn Ommer. Continuous, subject-specific attribute control in t2i models by identifying semantic directions, 2025. 2, 3, 7 [5] Reza Bayat, Ali Rahimi-Kalahroudi, Mohammad Pezeshki, Sarath Chandar, and Pascal Vincent. Steering large language model activations in sparse spaces, 2025. 3 [6] Black Forest Labs. Flux, https://github.com/black-forestlabs/flux, 2024. 2, 3, 5, 12 [7] Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. arXiv preprint arXiv:2311.16711, 2023. 2 [8] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemanticfeatures/index.html. 3 [9] Bart Bussmann, Patrick Leask, and Neel Nanda. Batchtopk sparse autoencoders, 2024. 14 [10] Bart Bussmann, Noa Nabeshima, Adam Karvonen, and Neel Nanda. Learning multi-level features with matryoshka sparse autoencoders, 2025. [11] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465, 2023. 1, 2 [12] Yu Cao, Zengqun Zhao, Ioannis Patras, and Shaogang Gong. Temporal score analysis for understanding and correcting diffusion artifacts, 2025. 5 [13] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models, 2023. 2, 3, 4 [14] Bartosz Cywinski and Kamil Deja. Saeuron: Interpretable concept unlearning in diffusion models with sparse autoencoders, 2025. 3 [15] Dawei Dai, Xu Long, Li Yutang, Zhang Yuanhui, and Shuyin Xia. Humanvlm: Foundation for human-scene visionlanguage model, 2024. 12 [16] Yusuf Dalva, Kavana Venkatesh, and Pinar Yanardag. Fluxspace: Disentangled semantic editing in rectified flow transformers, 2024. 2, 3, 7, 13 [17] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models, 2024. 2 [18] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. 2 [19] Sara Dorfman, Dana Cohen-Bar, Rinon Gal, and Daniel Ip-composer: Semantic composition of visual Cohen-Or. concepts, 2025. 3 [20] Amil Dravid, Yossi Gandelsman, Kuan-Chieh Wang, Rameen Abdal, Gordon Wetzstein, Alexei A. Efros, and Kfir Aberman. Interpreting the weight space of customized diffusion models, 2024. 2, 3 [21] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 6, 12 [22] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models, 2023. 2, 3, 7, 13 [23] Rohit Gandikota, Zongze Wu, Richard Zhang, David Bau, Eli Shechtman, and Nick Kolkin. Sliderspace: Decomposing the visual capabilities of diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, 2025. arXiv:2502.01639. [24] Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders, 2024. 3, 4, 15 [25] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising, 2024. 2 [26] Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. Tokenverse: Versatile multi-concept personalization in token modulation space, 2025. 3 [27] Julia Guerrero-Viu, Milos Hasan, Arthur Roullier, Midhun Harikumar, Yiwei Hu, Paul Guerrero, Diego Gutierrez, Belen Masia, and Valentin Deschaintre. Texsliders: Diffusionbased texture editing in clip space. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, page 111. ACM, 2024. 2, 3 [28] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Anastasis Stathopoulos, Xiaoxiao He, Yuxiao Chen, et al. Proxedit: Improving tuning-free real image editing with proximal guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 42914301, 2024. 2 [29] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022. 1, 2 [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. [31] Saar Huberman, Or Patashnik, Omer Dahary, Ron Mokady, and Daniel Cohen-Or. Image generation from contextuallycontradictory prompts. arXiv preprint arXiv:2506.01929, 2025. 5 [32] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations, 2023. 2, 5 [33] Guanlong Jiao, Biqing Huang, Kuan-Chieh Wang, and Renjie Liao. Uniedit-flow: Unleashing inversion and editing in the era of flow models, 2025. 2, 5, 8, 9 [34] Edo Kadosh, Nir Goren, Or Patashnik, Daniel Garibi, and Daniel Cohen-Or. Tight inversion: Image-conditioned inversion for real image editing, 2025. 2 [35] Guy Kaplan, Michael Toker, Yuval Reif, Yonatan Belinkov, and Roy Schwartz. Follow the flow: On information flow across textual tokens in text-to-image models, 2025. 4 [36] Dahye Kim and Deepti Ghadiyaram. Concept steerers: Leveraging k-sparse autoencoders for test-time controllable generations, 2025. 3 Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. 1, 2 [52] Dvir Samuel, Barak Meiri, Haggai Maron, Yoad Tewel, Nir Darshan, Shai Avidan, Gal Chechik, and Rami Ben-Ari. Lightning-fast image inversion and editing for text-to-image diffusion models, 2024. 2 [53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. 2 [54] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 2 [55] Deepak Sridhar and Nuno Vasconcelos. Prompt sliders for fine-grained control, editing and erasing of concepts in diffusion models, 2024. 2, [56] Linoy Tsaban and Apolinario Passos. Ledits: Real image editing with ddpm inversion and semantic guidance, 2023. 2 [57] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. pages 19211930, 2023. 1, 2 [58] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. DiffusionDB: large-scale prompt gallery dataset for text-toimage generative models. arXiv:2210.14896 [cs], 2022. 12 [59] Shai Yehezkel, Omer Dahary, Andrey Voynov, and Daniel Cohen-Or. Navigating with annealing guidance scale in diffusion space. arXiv preprint arXiv:2506.24108, 2025. 5 [60] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6, 7 [37] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 7, 12, 13 [38] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation, 2024. 6, 7 [39] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations, 2022. 1, [40] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models, 2023. 2 [41] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models, 2022. 2 [42] OpenAI. Chatgpt (gpt-5). https://chat.openai. com/, 2025. 7, 12 [43] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings. ACM, 2023. 2 [44] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar AverbuchElor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2, 5 [45] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. [46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. 3, 12 [47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. 1, 2 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 1 [49] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations, 2024. 2 [50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023. 3 [51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed"
        },
        {
            "title": "Appendices",
            "content": "A. Implementation Details We illustrate our method with the T5-XXL text encoder [46], which is utilized by state-of-the-art text-toimage models such as Flux.dev [6] and Stable Diffusion 3.5 [21]. To train the SAE, we compiled dataset from the DiffusionDB dataset [58], containing two sources: 2M general image captions, and the HumanCaption-10M dataset [15], which provides 10M captions focused on humans. The combined training set consists of 12M text prompts, totaling approximately 800M text tokens after filtering. The dimension of the SAEs latent space is set to 65,536, and the target number of active entries for each token is 300. We trained the SAE for 200,000 steps using the Adam optimizer with learning rate of 0.003. The weight for the sparsity loss, α (from Eq. 1), was set to 1 32 . For each edit, the corresponding direction was derived using set of = 100 source and target prompt pairs. These prompt pairs were generated using GPT-5. The parameter τ (from Eq. 6) used for the exponential injection mechanism was set to be function of the scale parameter: τ = 15 ω. B. Experiments B.1. Additional Qualitative Results Figure 14 showcases the universality of our learned edit directions. We apply the exact same set of four directions (smile, angry, surprised, and old) to four diverse source images, demonstrating that single direction vector can generalize effectively across different subjects, scenes, and identities. Figure 15 demonstrates the compositionality of our learned directions, where we independently control smile on the horizontal axis and the addition of glasses on the vertical axis. It is evident that these manipulations are highly disentangled, as the subjects identity and all background details remain perfectly consistent across the grid, with only the intended attributes changing. We further demonstrate the compositionality and advanced localization capabilities of our method in Figure 17. The figure showcases the simultaneous application of two distinct edits targeted at different subjects within the same scene. laugh direction is applied to the woman, while an old direction is applied to the man. The results across the grid show that each manipulation is successfully confined to its intended target, preserving the background and the non-targeted attributes of each subject without interference. Figures 22 and 18 present additional qualitative results for continuous editing on human and non-human subjects, respectively. B.2. Benchmark Details As mentioned in the main paper, we constructed custom benchmark for our comparative evaluation. The process began with large language model (LLM) [42], which we used to generate 21 diverse source prompts. For each of these prompts, we generated images using 3 different random seeds, resulting in set of 63 unique source images. Finally, we applied between 6 to 8 different semantic edits to each source image, depending on the applicability of the edit to the subject. The complete list of source prompts and the specific edits applied to each are detailed in Table 3. B.3. Quantitative Ablation To quantitatively measure the contribution of each component of our method, we conduct an ablation study on our benchmark, with results shown in Figure 16. We evaluate three variants of our approach: (1) deriving an edit direction from single prompt pair, (2) aggregating directions from prompts but without our proposed injection schedule, and (3) our full method which includes the exponential injection schedule. The plot of VQA score (prompt alignment) versus LPIPS score (image preservation) reveals the contribution of each component. The single-prompt version serves as our initial baseline and produces less pronounced semantic change, resulting in significantly lower VQA score. Aggregating prompts drastically improves prompt alignment, yielding much higher VQA score. Our full method, which adds the exponential injection schedule, maintains the high prompt alignment gained from using prompts while significantly improving image preservation, achieving superior LPIPS scores at all intermediate intensity levels. This validates that both components are crucial for achieving stateof-the-art balance between edit accuracy and preservation. . B.4. Flux Kontext Baseline Since Flux Kontext [37] lacks native mechanism for continuous edit scaling, we implemented two distinct proxy baselines to evaluate different edit intensities. The first, which we term Flux Kontext1 (LLM), controls the edit magnitude by using three different instruction prompts (light, medium, and extreme) generated by an LLM, as detailed in Table 2. The second baseline, Flux Kontext2 (CFG), uses the fixed medium instruction prompt and instead varies the Classifier-Free Guidance (CFG) scale to achieve different levels of edit strength. 12 original image Smile direction Angry direction Surprised direction Old direction Figure 14. Each row shows different source image (leftmost column) and its edits along four semantic directions: smile, angry, surprised, and old. The images in each column are generated by adding the same direction, showcasing the generality of the directions found by our method B.5. Qualitative Comparisons (Continued) To further evaluate our approach, we provide qualitative comparisons against existing methods, including FluxSpace [16], Concept-Sliders [22], and two variants of Flux Kontext [37]: Flux Kontext1 (LLM), which leverages an LLM to craft prompts for gradual editing, and Flux Kontext2 (Cfg), which uses the cfg score to guide edits. Results are presented in Figures 11 and 21. In Figure 11 (left), competing methods fail to introduce meaningful edit, whereas our method produces clear and consistent modification. On the right, several baselines either fail to perform the edit or induce significant identity changes. Notably, both Flux Kontext variants are unable to achieve gradual edits and distort subject proportions, often enlarging the head unnaturally. By contrast, our method generates edits that are gradual and identity-preserving. Figure 21 further illustrates these differences. On the left, competing methods fail to add beard, produce abrupt transitions, or generate unnatural appearances. Our approach successfully creates gradual, natural-looking beard. On the right, most baselines again yield non-gradual changes or identity shifts, while our method produces clear, progressive edits that maintain subject identity. B.6. User study As reported in the main text, we conducted user study to further evaluate the perceptual quality of our method. For this study, we randomly sampled 20 edit scenarios from our quantitative evaluation benchmark. In each question, we performed pairwise comparison. Participants were shown the three levels of edit intensity 13 smiling + laugh (woman) + g s + Figure 15. Composing Disentangled Edits. We demonstrates the compositionality of our learned edit directions. Starting from the source image (top-left), we independently control two attributes of the same subject. The horizontal axis continuously controls the smile attribute, while the vertical axis adds glasses. The smooth and accurate results in the grid showcase our methods ability to combine edits. Figure 16. Quantitative Ablation. We compare different versions of our method. (top-right is better). from our method alongside the corresponding three levels from single competing method. They were then asked to choose which set of edits they preferred based on three criteria: Image Preservation: Which edits better preserves the identity? Prompt Alignment & Graduality: Which edits is clearer and more gradual? Overall Preference: Which edits do you prefer overall? The exact format of the user study interface is shown in Figure 19. 14 ( n ) + Figure 17. Composition of Edits on Multiple Subjects. We demonstrate our methods ability to apply and compose edits targeted at different subjects within the same image. Starting from the source image (top-left), the horizontal axis applies laugh edit exclusively to the woman, while the vertical axis applies an old edit only to the man. The results showcase high degree of localization and disentanglement, as each edit affects only its intended target without interfering with the other subject or the background."
        },
        {
            "title": "Attribute\nBald",
            "content": "Beard Curly Hair Laughing Old Smiling"
        },
        {
            "title": "Surprised",
            "content": "1.0 (Low) make the person balding make the person have short beard make the person have slightly curly hair make the person giggle make the person middle-aged make the person smile slightly make the person slightly surprised 2.0 (Medium) make the person bald make the person have beard make the person have curly hair make the person laugh make the person old make the person smile make the person surprised"
        },
        {
            "title": "Young",
            "content": "make the person slightly young make the person young 3.0 (High) make the person completely bald make the person have long thick beard make the person have very curly hair make the person laugh hysterically make the person very old make the person smile broadly make the person extremely surprised make the person very young Table 2. Textual descriptions of attribute scales used in our comparison with Flux Kontext C. Sparse AutoEncoders - Continue Enforcing Sparsity Enforcing sparsity in an SAEs latent space is central challenge that has led to specialized techniques. One prominent method is the BatchTopK operator [9], computationally efficient approach that retains Original image yellow (car) + Original image round (table) + Original image round (box) + Original image fall (tree) + Original image rusty (sword) + Original image Poodle (dog) + Figure 18. Examples of continuous edits on non-human subjects, showcasing control over seasonal changes, color, and object shape. only the top BK strongest entries across an entire training batch of size B. At inference, this operator is replaced by pre-calibrated global threshold (θ) for consistent behavior on single inputs. common failure mode with such strong sparsity is the emergence of dead latents, which are entries that cease to activate and in turn degrade the SAEs reconstruction performance. To mitigate this, an auxiliary loss, Laux, can be incorporated [24], which encourages these inactive latents to revive by tasking them with explaining portion of the reconstruction error. tation. D. LLM Usage Statement We utilized Large Language Model (LLM) to improve the grammar, spelling, and clarity of this manuscript. The authors critically reviewed and edited all suggestions and bear full responsibility for the accuracy and integrity of the final content. Matryoshka Sparse Autoencoders (MSAEs) [10] extend SAEs by learning single, hierarchical feature dictionary that provides nested representations at multiple levels of granularity. This is achieved by training the model to reconstruct the input using sequence of nested dictionary subsets of sizes = {m1, . . . , mn}. The training objective minimizes the sum of reconstruction losses across all these levels, along with standard sparsity and auxiliary losses: (cid:88) = Lrec(m) + αLsparse, (7) mM where Lrec(m) is the reconstruction loss using only the first entries. This encourages the most important features to appear early in the dictionary, creating an ordered represen15 Figure 19. An example of question in the user study Prompt Portrait of woman in flowing sundress in field of wildflowers at golden hour Close-up of woman in traditional Japanese kimono with cherry blossoms framing her face woman in business attire portrait in modern glass office building with city skyline Female pilot in leather jacket portrait next to vintage biplane woman in rain jacket portrait at lighthouse during coastal storm Portrait of woman in bohemian clothing at outdoor art market in Paris Rock climber woman portrait with climbing gear and canyon background woman in winter coat portrait with Northern Lights in Finnish Lapland Female chef in whites portrait in busy restaurant kitchen Portrait of woman in wetsuit on surfboard with ocean waves behind portrait of woman violinist in elegant gow in candlelit baroque chamber woman in hiking gear portrait at mountain summit with valley vista Portrait of man in worn leather jacket with misty fjord background at dawn Portrait of man in traditional samurai armor in zen garden setting"
        },
        {
            "title": "Portrait of a man wearing hiking gear with tropical canyon vista behind him",
            "content": "man in fishermans sweater portrait with foggy dock and sea background Young man in vintage band t-shirt leaning against 1967 Mustang in desert"
        },
        {
            "title": "Portrait of a man in Renaissance clothing at an easel in Italian courtyard",
            "content": "man in red flannel shirt portrait outside log cabin with falling snow male chef in whites at sushi counter, portrait with minimalist restaurant background man wearing panama hat portrait in Marrakech market with colorful spices Applied Attributes smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald smiling, curly hair, laughing, old, smiling, surprised, young, beard, bald Table 3. The complete set of source prompts and their corresponding edit attributes used for our quantitative evaluation and user study. e S c 1 t x 2 ) ( t x e S F O"
        },
        {
            "title": "Original image",
            "content": "surprised +"
        },
        {
            "title": "Original image",
            "content": "bald + Figure 20. Each row showcases the results of different editing method for the same edit. We now show two side-by-side runs (6 images per row). Our method (bottom row) produces more disentangled result that better preserves the subjects identity compared to the competing approaches. 17 d t n 1 t x 2 ) ( e K F p l u O"
        },
        {
            "title": "Original image",
            "content": "beard +"
        },
        {
            "title": "Original image",
            "content": "old + Figure 21. Each row showcases the results of different editing method for the same edit. We now show two side-by-side runs (6 images per row). Our method (bottom row) produces more disentangled result that better preserves the subjects identity compared to the competing approaches. 18 Original image laugh + Original image surprised + Original image bald + Original image blonde + Original image cry + Original image cry + Original image surprised + Original image surprised +"
        },
        {
            "title": "Original image",
            "content": "surprised +"
        },
        {
            "title": "Original image",
            "content": "chubby +"
        },
        {
            "title": "Original image",
            "content": "angry +"
        },
        {
            "title": "Original image",
            "content": "smile + Figure 22. Additional results of our text-based Sliders."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Tel Aviv University"
    ]
}