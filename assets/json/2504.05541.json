{
    "paper_title": "Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting",
    "authors": [
        "Yunlong Tang",
        "Jing Bi",
        "Chao Huang",
        "Susan Liang",
        "Daiki Shimada",
        "Hang Hua",
        "Yunzhong Xiao",
        "Yizhi Song",
        "Pinxin Liu",
        "Mingqian Feng",
        "Junjia Guo",
        "Zhuo Liu",
        "Luchuan Song",
        "Ali Vosoughi",
        "Jinxi He",
        "Liu He",
        "Zeliang Zhang",
        "Jiebo Luo",
        "Chenliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 2 1 4 5 5 0 . 4 0 5 2 : r Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting Yunlong Tang1, Jing Bi1, Chao Huang1, Susan Liang1, Daiki Shimada2, Hang Hua1, Yunzhong Xiao3, Yizhi Song4, Pinxin Liu1, Mingqian Feng1, Junjia Guo1, Zhuo Liu1, Luchuan Song1, Ali Vosoughi1, Jinxi He1, Liu He4, Zeliang Zhang1, Jiebo Luo1, Chenliang Xu1 1University of Rochester, 2Sony Group Corporation, 3CMU, 4Purdue University"
        },
        {
            "title": "Abstract",
            "content": "We present CAT-V (Caption AnyThing in Video), trainingfree framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: Segmenter based on SAMURAI for precise object segmentation across frames, Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at: https:// github.com/ yunlong10/ CAT-V 1. Introduction Video captioning, which aims to generate coherent natural language descriptions of video content, remains fundamental challenge in vision-language learning. Given video as input, current multimodal large language models (MLLMs) that can handle video understanding tasks or video large language models (VidLLMs) [43] can be prompted to perform detailed vanilla video captioning, which is video-level and attempts to cover all aspects of the video content. However, Figure 1. Comparison of video captioning approaches: Vanilla (top-left), Dense (top-right), Dense Object (bottom-left), and our CAT-V framework (bottom-right) with integrated modules for usercontrolled object-centric captioning via integrated modules (Segmenter, Temporal Analyzer, Captioner with CoT reasoning). vanilla video captioning lacks the sensitivity and dynamics of time and space. For instance, video is dynamic [24, 62], and the same object can perform various actions at different times during the video, but most of the existing VidLLMs for general purposes [33, 37] tend to generate too abstract answers, which are more suitable for captioning static images. Dense video captioning (DVC) involves generating multiple captions for multiple events along with their temporal boundaries. However, the current task-specific model designed for DVC [50] tends to produce excessively concise outputs. Some existing works explore VidLLMs-based methods [15, 23, 45, 54, 60] that are fine-tuned on dense video captioning datasets [30, 63], but they somewhat compromise the ability to follow instructions and still struggle with more fine-grained, object-centric captioning. These methods also lack effective user interaction and only provide language interface for users. While some works have investigated controllable image captioning [25, 51], controllable fine-grained object-centric captioning in videos remains underexplored. Additionally, some studies [59] have sought to integrate the Segment Anything Model (SAM) with MLLMs/VidLLMs; however, these methods depend on annotated data for training both MLLMs and SAM. To address these limitations, we introduce Caption AnyThing in Video (CAT-V), training-free framework for object-centric video captioning augmented by pre-trained segmentation model built on VidLLMs. CAT-V consists of three main components: Segmenter, Temporal Analyzer, and Captioner. Figure 1 illustrates the key differences between our proposed approach and existing video captioning methods, highlighting how CAT-V integrates user control, object-level focus, and temporal awareness in unified framework. Specifically, the Segmenter is pre-trained video object segmentation model based on an improved version of SAM 2 [42], known as SAMURAI [55]. It generates pixel-level masklets of an object throughout the entire video as indicated by the user within single frame of the input video. Benefiting from the training of SAM 2, CAT-V supports range of visual prompts, including points and bounding boxes, to accurately identify the object desired by the user during interactions. The original video is then updated by injecting the predicted masklets of the selected object, which serve as spatiotemporal visual prompts. The Temporal Analyzer is based on TRACE-Uni [15], temporal-aware VidLLM pre-trained on dense video captioning datasets, enabling CAT-V to perceive the events and changes occurring in the video, produce coarse-grained event-level captions, and identify the corresponding boundaries. The Captioner is based on InternVL-2.5 [7] and takes the spatiotemporal prompted updated video as input, along with the temporal boundaries and coarse-grained event captions provided by the Temporal Analyzer. The Captioner also accepts Chainof-Thought (CoT) prompting as input. This approach encourages the Captioner to focus on the object selected/highlighted by the user, sufficiently identifying the objects attributes, actions, and statuses, the environments or backgrounds surrounding the object, any other objects interacting with the selected object, and events related to the selected object, ultimately generating fine-grained object-centric captions. Different from previous controllable captioning methods [59], CAT-V is training-free and does not rely on large amount of annotated data for training or finetuning, sufficiently utilizing the capabilities of pre-trained MLLMs/VidLLMs. Besides, CAT-V provides an efficient interaction mode for users to select the object that they want to accurately and fine-grained describe in the video, well inherent in the flexibility of SAM 2, where the limitation of previous general VidLLMs [33, 37], which could not interact through visual prompts, has been lifted. Moreover, by utilizing the temporal awareness of Trace-Uni, CAT-V is sensitive to dynamic changes in events related to the selected object, making it possible to capture the status changes. We present these strong capabilities of CAT-V through comprehensive array of qualitative examples in the experimental results. In short, our contribution is twofold: We propose CAT-V, training-free framework for objectcentric video captioning that leverages pre-trained models to generate fine-grained descriptions without requiring additional training data, addressing the limitations of existing video captioning approaches. We demonstrate that CAT-V achieves temporal-aware and spatially-precise object-centric video captioning by combining the temporal analysis capabilities of TRACE-Uni with the spatial segmentation abilities of SAMURAI, enabling detailed descriptions of object. 2. CAT-V: Caption Anything in Video Our proposed framework, CAT-V, is designed for finegrained object-centric video captioning via spatiotemporal multimodal prompting. It integrates three key modules: the Segmenter S, the Temporal Analyzer , and the Captioner C. This modular approach allows for dynamic user visual input, points or bounding boxes, and irregular regions, to guide the generation of detailed and contextually relevant captions. Figure 2 illustrates the architecture of CAT-V. Given an input video = {It}T t=1 with frames and user prompt p, the framework operates as follows. 2.1. Segmenter The Segmenter S, powered by SAMURAI [55], performs precise object segmentation in video frames based on userprovided visual prompts. For each frame It, the Segmenter produces binary mask Mt = S(It, p) where Mt {0, 1}HW represents the pixel-level segmentation of the target object, with and being the frame height and width respectively. The module uses the SAM 2s encoder [42] to embed the input video frames, prompt encoder to encode the user visual prompt, and SAM 2s decoder. SAMURAI enhances the capabilities of SAM 2 with Kalman filtering and motion-aware memory, enabling robust object mask extraction even in challenging scenarios with occlusions, motion blur, or complex backgrounds. 2 Figure 2. CAT-V consists of three modules: Segmenter, Temporal Analyzer, and Captioner. The Segmenter precisely segments objects in video frames using user-defined prompts (points, bounding boxes, or regions). The Temporal Analyzer captures video dynamics hierarchically. The Captioner creates object-centric captions using upstream information and CoT reasoning. 2.2. Temporal Analyzer The Temporal Analyzer , built upon TRACE-Uni [15], models the temporal dynamics of video sequences through hierarchical approach. It processes the video to identify events with their corresponding temporal boundaries {(si, ei)}N i=1, where si and ei represent the start and end timestamps of the i-th event. For each event, it generates coarse-grained caption ci = (V, si, ei). This temporal decomposition enables fine-grained analysis of object interactions and activities across different time scales. 2.3. Captioner The Captioner C, an MLLM implemented using InternVL2.5-8B [9], generates detailed object-centric captions by the original video , object integrating multiple inputs: masks {Mt}T i=1, coarse-grained event captions {ci}N i=1, and chain-of-thought prompts PCoT . This ensures that the generated captions are both spatially precise and temporally coherent. The final object-centric caption is generated as: t=1, temporal event boundaries {(si, ei)}N Cf inal = C(V ({Mt}T t=1, ), {(si, ei, ci)}N i=1, PCoT ), where controls how the masklets are injected into the original video (introduced in Section 3.2). 2.4. Chain-of-Thought Prompting We design fine-grained prompts to guide the Captioner in Chain-of-Thought (CoT) reasoning, enabling systematic and structured analysis of object-centric video content. Our prompting strategy can be represented as sequence of analytical components PCoT = {A1, A2, ..., AK}, where each component Ak focuses on specific aspect of object 3 analysis (attributes, actions, status changes, etc.). This structured approach helps the model first identify and analyze individual aspects before synthesizing them into coherent, temporally-aware narrative. By explicitly separating these analytical components, we ensure that no critical details are overlooked in the final description. (cid:17) Chain-of-Thought Prompting Above are the event captions given by the user, whose timestamps are very accurate but the subjects of the sentences are not necessarily what we want to highlight. Please pay attention to the object highlighted (HO) by colored bounding box and blue mask in the video frames, and generate accurate object-centric caption for the HO. Please make sure in object-centric paragraph caption, the sentences should be detailed and specific, and the subjects of all sentences MUST be HO. Please follow the format: HO: ... HOs attributes: ... All actions done by HO: ... All statuses of HO: ... All other objects interacted with HO: ... All environments/backgrounds of HO: ... All events related to HO: ... Final object-centric paragraph caption: The HO is [attributes], [environment]. From ... to ...s, the HO [status], [any action], [any status/attribute/environment changes]... From ... to ...s, the HO [status], [any action], [any status/attribute/environment changes]. The OHs [final status] is ... Figure 3. CAT-V can focus on different objects within the same video. The top sequence shows object-centric captioning for horse, while the bottom sequence demonstrates captioning for the cowboy, each with precise temporal segmentation of their respective actions and states. 3. Experiments In this section, we use extensive qualitative experiments to demonstrate the versatility and effectiveness of CAT-V in object-centric video captioning through various visual prompting, highlight styles, Chain-of-Thought prompting, and interactive chatting capabilities. 3.1. User Visual Prompts CAT-V supports versatile user interactions through various visual prompting mechanisms. As demonstrated in Figure 3, users can selectively highlight different objects within the same video for fine-grained captioning. In this example, the user can choose either the horse or the cowboy to generate object-centric temporal descriptions, with CAT-V accurately tracking and describing the selected entitys actions and state changes throughout the video. Figure 4 further illustrates CAT-Vs flexibility in accepting different types of visual prompts, including points, bounding boxes, trajectories, and irregular regions. This adaptability allows users to precisely indicate their object of interest using the most convenient or appropriate prompt type for the particular video content, while CAT-V maintains consistent accuracy in segmentation and captioning regardless of the prompt format. 3.2. SAM-generated Video Prompts CAT-V leverages SAM 2 to generate masklets of userselected objects throughout the video and injects these visual cues directly into the video frames as highlighted regions. These SAM-generated video prompts guide the MLLM to focus on the specific object of interest during captioning. Figure 5 illustrates an experiment comparing different highlight styles for injecting these visual prompts into the video frames. In this experiment, we bypass both the Temporal Analyzer and CoT Prompting components, directly feeding the prompt-injected video to the MLLM to evaluate the effectiveness of different visual prompt styles. The results show that bounding boxes and polygons produce the most accurate object-centric descriptions, while other methods like color block and mask tend to alter the objects original appearance, causing the MLLM to generate incorrect descriptions (e.g., identifying blue cup as \"pink\" or \"red\" when color blocks or masks are applied). Blur and circle methods, while preserving the objects color, provide less precise spatial guidance, sometimes resulting in generic or imprecise descriptions of the objects attributes and movements. 3.3. Chain-of-Thought Prompts As shown in Figure 6, we compare CAT-Vs captioning with and without Chain-of-Thought (CoT) prompting. With CoT, the system produces detailed temporal descriptions of highlighted objects, i.e., performer is doing acrobatics and woman is lifting weights, specifying precise time intervals and action sequences. Without CoT, descriptions become generic, lacking temporal precision and detailed object focus. The Temporal Analyzer provides basic scene 4 Figure 4. Examples of CAT-Vs support for various visual prompting formats. The system effectively handles points, bounding boxes, and irregular regions to identify and track diverse objects including pandas, birds, bottles, and people, demonstrating its flexibility and accuracy in accommodating different user input preferences. 5 Figure 5. Comparison of different visual prompt styles (Bounding Box, Blur, Circle, Color Block, Halo, Mask, and Polygon) for highlighting blue plastic cup, demonstrating their effects on object-centric captioning accuracy. descriptions without object-specific details, demonstrating how CoT prompting significantly enhances object-centric video captioning quality. 3.4. Object-centric Chatting CAT-V not only supports fine-grained object-centric video captioning but also enables interactive multi-round chatting focused on specific objects. As shown in Figure 7, users can engage in detailed conversations about the highlighted object, asking follow-up questions to explore its attributes, actions, and temporal behaviors. This conversational capability allows users to naturally explore different aspects of the objects appearance and behavior in the video through an intuitive dialogue interface. 4. Related Work 4.1. Dense Video Captioning The dense video captioning task aims to localize and describe events in given video by considering the interaction of the object, the spatial location, and the temporal information. The dense video captioning procedure can be divided into three steps: extraction of video features, localization of temporal events, and generation of captions. Previous works [2, 26, 27, 30, 48, 49] have performed event localization and caption generation individually. More recent approaches such as PDVC [64] and TRACE-Uni [15] jointly estimate event timestamps and captions. PDVC utilizes DETR-like model [6], and TRACD-Uni uses Large Language Model [28] as the backbone for end-to-end prediction. 4.2. Video Object Segmentation The video object segmentation task consists of first-frame video object segmentation [39] and interactive video object segmentation. In this paper, we focus on the interactive video object segmentation task, where user guidance is given as bounding boxes, points, or scribbles. The interactive video object segmentation task has gained lot of attention recently due to its convenient annotation and intuitive interaction between users and segmentation models. After obtaining user guidance, some works [10, 14, 16] design modular approaches to convert user input to mask in the first video frame and propagate this mask to the remaining video frames sequentially. After the presence of the SAM model, some works [11, 12, 41, 56] propose combining the image-based SAM model with video trackers to enable the video-based segmentation feature. However, in some cases, these hybrid models fail because the video tracker model amplifies and propagates errors caused by the image-based SAM model. Later, Ravi et al. [42] proposed unified segmentation model Figure 6. Comparison of object-centric video captioning using CAT-V with CoT prompting, without CoT prompting, and using only the Temporal Analyzer. (SAM 2) that natively supports video object segmentation with memory attention. SAMURAI [55] further enhances the segmentation capability of SAM 2 by integrating Kalman Filer [29] and motion-aware memory into SAM 2. 4.3. Multimodal Large Language Models Vision-Language models [5, 8, 19, 21, 31, 36, 40, 45, 46, 52, 53, 61] seek multimodal intelligence by jointly processing visual and linguistic information. Inspired by the remarkable success of recent large language models 7 Figure 7. Example of object-centric multi-round chatting with CAT-V, demonstrating the systems ability to maintain reference to the highlighted object while answering specific questions about its attributes and actions. (LLMs) [13, 18, 47], researchers are now exploring large VLMs that combine pretrained visual encoders and language decoders to tackle complex multimodal tasks. Flamingo [1] and BLIP-2 [32] are two of the early works that explore the integration of LLMs into vision-language pre-training. These models are trained as VL foundation models. Beginning with LLaVA [36], researchers have used LLMsynthesized instruction-following chat data in VQA format for instruction-tuning, achieving significantly improved results [4, 20, 22, 44, 57]. Subsequent work has further broadened the capabilities [3, 17, 22, 34, 35, 38, 58], of multimodal LLMs. However, comparatively little effort has been focused on improving the ability of models to track and describe video content by attending to specific temporal segments and regions. 5. Conclusion We presented CAT-V, training-free framework for objectcentric video captioning that addresses fundamental limitations in existing video understanding approaches. By integrating SAMURAIs robust object segmentation capabilities, TRACE-Unis hierarchical temporal analysis, and InternVL-2.5s multimodal understanding, our system enables fine-grained, temporally-aware descriptions of userselected objects without requiring additional training data. The use of CoT guides the model to systematically analyze object attributes, actions, status changes, and interactions, resulting in comprehensive and coherent captions. Our experiments demonstrate CAT-Vs versatility in supporting various visual prompt types (points, bounding boxes, and irregular regions) and its effectiveness in maintaining object focus across temporal boundaries. The system also enables natural conversational interaction about highlighted objects, allowing users to explore specific aspects of video content through intuitive dialogue. Future work could explore extending CAT-V to handle complex multi-object interactions, incorporating more sophisticated temporal reasoning capabilities, and enhancing its ability to understand causal relationships between objects and events in videos. 6. Limitations Despite CAT-Vs capabilities in object-centric video captioning, several limitations remain. First, CAT-V relies heavily on the segmentation quality of SAMURAI, which may struggle with highly complex scenes, fast motion, or severe occlusions. When segmentation fails, the subsequent captioning quality degrades significantly. Second, the frameworks temporal accuracy depends on TRACE-Unis event boundary detection, which can be imprecise for subtle state changes or when multiple events overlap. Third, while our interactive 8 approach allows flexible object selection, CAT-V currently lacks the ability to handle multiple highlighted objects simultaneously, limiting analysis of object interactions."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by Sony Group Corporation. We would like to thank Sayaka Nakamura and Jerry Jun Yokono for their insightful discussion."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 8 [2] Jing Bi, Jiebo Luo, and Chenliang Xu. Procedure planning in instructional videos via contextual modIn 2021 eling and model-based policy learning. IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1559115600. IEEE, October 2021. doi: 10.1109/iccv48922.2021.01532. 6 [3] Jing Bi, Nguyen Manh Nguyen, Ali Vosoughi, and Chenliang Xu. Misar: multimodal instructional system with augmented reality, 2023. 8 [4] Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, and Chenliang Xu. Unveiling visual perception in language models: An attention head analysis approach, 2024. 8 [5] Jing Bi, Yunlong Tang, Luchuan Song, Ali Vosoughi, Nguyen Nguyen, and Chenliang Xu. Eagle: Egocentric aggregated language-video engine. In Proceedings of the 32nd ACM International Conference on Multimedia, MM 24, pp. 16821691. ACM, October 2024. doi: 10.1145/3664647.3681618. 7 [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213229. Springer, 2020. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 2 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 7 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024. 3 [10] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5559 5568, 2021. 6 [11] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13161326, 2023. 6 [12] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023. [13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march 2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023. 8 [14] Thanos Delatolas, Vicky Kalogeiton, and Dim Papadopoulos. Learning the what and how of annotation in video object segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 69516961, 2024. 6 [15] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Qingbin Liu, and Xi Chen. Trace: Temporal grounding video llm via causal event modeling. arXiv preprint arXiv:2410.05643, 2024. 2, 3, 6 [16] Yuk Heo, Yeong Jun Koh, and Chang-Su Kim. Interactive video object segmentation using global and local transfer modules. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part XVII 16, pp. 297313. Springer, 2020. 6 [17] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah Smith, and Jiebo Luo. Promptcap: Promptguided image captioning for vqa with gpt-3. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 29632975, 2023. 8 [18] Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, and Jiebo Luo. Improving pretrained language model fine-tuning with noise stability regularization. IEEE Transactions on Neural Networks and Learning Systems, 2023. 9 [19] Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Zhifei Zhang, Yilin Wang, Jianming Zhang, and Jiebo Luo. Finecaption: Compositional image captioning focusing on wherever you want at any granularity. arXiv preprint arXiv:2411.15411, 2024. 7 [20] Hang Hua, Jing Shi, Kushal Kafle, Simon Jenni, Daoan Zhang, John Collomosse, Scott Cohen, and Jiebo Luo. Finematch: Aspect-based fine-grained image and text mismatch detection and correction. arXiv preprint arXiv:2404.14715, 2024. 8 [21] Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo Luo. V2xum-llm: Cross-modal video summarization with temporal prompt instruction tuning. arXiv preprint arXiv:2404.12353, 2024. 7 [22] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo Luo. Mmcomposition: Revisiting the compositionality of pre-trained vision-language models. arXiv preprint arXiv:2410.09733, 2024. 8 [23] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1427114280, 2024. [24] Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Egocentric audio-visual object localizaIn Proceedings of the IEEE/CVF Conference tion. on Computer Vision and Pattern Recognition (CVPR), pp. 2291022921, June 2023. 1 [25] Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, and Zicheng Liu. Segment and caption anything. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1340513417, 2024. 2 [26] Vladimir Iashin and Esa Rahtu. better use of audiovisual cues: Dense video captioning with bi-modal transformer. arXiv preprint arXiv:2005.08271, 2020. 6 [27] Vladimir Iashin and Esa Rahtu. Multi-modal dense video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 958959, 2020. 6 [28] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 6 [29] Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. 7 [30] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pp. 706715, 2017. 2, [31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. 7 [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. 8 [33] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 1, 2 [34] Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, and Jiebo Luo. Videoxum: Cross-modal visual and textural summarization of videos. IEEE Transactions on Multimedia, 2023. 8 [35] Shuhang Lin, Wenyue Hua, Lingyao Li, Che-Jui Chang, Lizhou Fan, Jianchao Ji, Hang Hua, Mingyu Jin, Jiebo Luo, and Yongfeng Zhang. Battleagent: Multi-modal dynamic emulation on historical battles to complement historical analysis. arXiv preprint arXiv:2404.15532, 2024. 8 [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 7, [37] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1, 2 [38] Alexander Martin, Reno Kriz, William Gantt Walden, Kate Sanders, Hannah Recknor, Eugene Yang, Francis Ferraro, and Benjamin Van Durme. Wikivideo: Article generation from multiple videos. arXiv preprint arXiv:2504.00939, 2025. 8 [39] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 6 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. 7 [41] Frano Rajiˇc, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu. Segment anything meets point tracking. arXiv preprint arXiv:2307.01197, 2023. 6 10 [42] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2, [43] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. arXiv preprint arXiv:2312.17432, 2023. 1 [44] Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, et al. Vidcomposition: Can mllms analyze compositions in compiled videos? arXiv preprint arXiv:2411.10979, 2024. 8 [45] Yunlong Tang, Daiki Shimada, Jing Bi, Mingqian Feng, Hang Hua, and Chenliang Xu. Empowering llms with pseudo-untrimmed videos for audio-visual temporal understanding. arXiv preprint arXiv:2403.16276, 2024. 2, 7 [46] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 7 [47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 8 [48] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong Xu. Bidirectional attentive fusion with context gating for dense video captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 71907198, 2018. [49] Teng Wang, Huicheng Zheng, Mingjing Yu, Qian Tian, and Haifeng Hu. Event-centric hierarchical representation for dense video captioning. IEEE Transactions on Circuits and Systems for Video Technology, 31(5): 18901900, 2020. 6 [50] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end dense video captioning with parallel decoding. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 68476857, 2021. 2 [51] Teng Wang, Jinrui Zhang, Junjie Fei, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, and Shanshan Zhao. Caption anything: Interactive image description with diverse multimodal controls. arXiv preprint arXiv:2305.02677, 2023. 2 [52] Zidian Xie, Shijian Deng, Pinxin Liu, Xubin Lou, Chenliang Xu, and Dongmei Li. Characterizing antivaping posts for effective communication on instagram using multimodal deep learning. Nicotine and Tobacco Research, 26(Supplement_1):S43S48, 2024. 7 [53] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al. mplug-2: modularized multi-modal In foundation model across text, image and video. International Conference on Machine Learning, pp. 3872838748. PMLR, 2023. 7 [54] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1071410726, 2023. 2 [55] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware memory. arXiv preprint arXiv:2411.11922, 2024. 2, [56] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023. 6 [57] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal arXiv preprint models for integrated capabilities. arXiv:2308.02490, 2023. 8 [58] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. Promptfix: You prompt and we fix the photo. arXiv preprint arXiv:2405.16785, 2024. 8 [59] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. 2 [60] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 2 [61] Pengfei Zhang, Pinxin Liu, Hyeongwoo Kim, Pablo Garrido, and Bindita Chaudhuri. Kinmo: Kinematicaware human motion understanding and generation. arXiv preprint arXiv:2411.15472, 2024. 7 [62] Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, and Xilin Chen. Unicon: Unified context network for robust active speaker detection. In Proceedings of the 29th ACM international conference on multimedia, pp. 39643972, 2021. 1 [63] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. 2 [64] Wanrong Zhu, Bo Pang, Ashish Thapliyal, William Yang Wang, and Radu Soricut. End-to-end dense video captioning as sequence generation. arXiv preprint arXiv:2204.08121, 2022."
        }
    ],
    "affiliations": [
        "CMU",
        "Purdue University",
        "Sony Group Corporation",
        "University of Rochester"
    ]
}