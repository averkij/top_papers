{
    "paper_title": "Guiding a Diffusion Transformer with the Internal Dynamics of Itself",
    "authors": [
        "Xingyu Zhou",
        "Qifan Li",
        "Xiaobin Hu",
        "Hai Chen",
        "Shuhang Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer's outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19."
        },
        {
            "title": "Start",
            "content": "Xingyu Zhou1 Qifan Li1 Xiaobin Hu2 Hai Chen3,4 Shuhang Gu1* 1University of Electronic Science and Technology of China 2National University of Singapore 3Sun Yat-sen University 4North China Institute of Computer Systems Engineering {xy.chous526, shuhanggu}@gmail.com 5 2 0 2 0 3 ] . [ 1 6 7 1 4 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "focused research topic. The diffusion model presents powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover lowprobability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layers outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256256, SiTXL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19. 1. Introduction Denoising Diffusion models have achieved notable success in high-dimensional data generation and rapidly applied to fields such as images, videos and 3D. Recently, the introduction of the Transformer architecture has further enhanced the scalability of the diffusion model in these fields [11, 52, 55], demonstrating its outstanding performance. As the scale of the diffusion model continues to increase, how to efficiently achieve excellent generation results has become highly *Corresponding author. Project page: Internal Guidance The training objective of the diffusion model is to cover the entire (conditional) data distribution. The model is severely penalized for failing to cover low-probability areas, but it lacks sufficient training and data to learn to generate high-quality images corresponding to these areas. Several studies introduce additional guidance strategies during the sampling stage to encourage the generative process to focus on high-probability areas of the data distribution. Among them, classifier-free guidance (CFG) [24] leads to better alignment between the prompt and the result by avoiding the unconditional results. Nevertheless, an excessively high CFG coefficient may push the sampling trajectory beyond the expected range of the conditional distribution, resulting in over-simplified or distorted samples [44]. An alternative line of research attempts to guide well-trained model using bad version of itself [38] instead of relying on CFG. By avoiding the results of the corresponding degraded version of the generative model during the sampling stage, the generation quality is improved while avoiding overly simplified results. Although promising, these approaches still require carefully designed degradation strategies [26, 27], extra training [38] or additional sampling steps [5, 29], thereby limiting their potential for the large-scale application. To address this, we propose simple yet effective sampling guidance strategy, named Internal Guidance (IG), which utilizes the intermediate-layer outputs within the Diffusion Transformer to enhance generation quality. Specifically, we introduce an auxiliary supervisory signal at an intermediate layer during training process, enabling the model to produce weaker generative output from mid-level representations. Building upon this, during the sampling stage, we leverages the relationship between intermediate and deep outputs during sampling to extrapolate. This internal guidance achieves an Autoguidance-like [38] effect that maintains diversity and improves the generation quality but without any additional sampling steps. Moreover, we show that IG can also combine with the guidance interval [44] or CFG to further enhance generative performance. Besides providing intermediate results for guidance, we observe that 1 Figure 1. Visualization Results. We visualize our latent diffusion system with proposed IG together with LightningDiT-XL trained on ImageNet 256 256 resolution. With an IG scale of 1.4 and CFG scale of 1.45, and further combining the guidance interval, we ultimately achieved the state-of-the-art FID = 1.19. More uncurated samples are provided in Supplementary Material. the introduction of intermediate supervision alone can alleviate partial vanishing gradients. Its convergence performance comparable to, or even better than, those achieved with self-supervised regularization applied to intermediate layers. Further leveraging the insight in IG, we discuss how modeling the discrepancy between intermediate and deep outputs within the loss function can inspire the design of new training acceleration strategies. We conduct comprehensive experiments to evaluate the effect of our proposed internal guidance. Our IG brings significant performance improvements to both DiTs, SiTs and LightningDiT. Ultimately, we compared our generative performance with the current state-of-the-art models on SiT-XL/2 and LightningDiT-XL/1 with IG. For SiT-XL/2 training, we show the model achieves FID = 5.31 and FID = 1.75 on class-conditional ImageNet generation only using 80 and 800 training epochs (without CFG) which already exceeds the FID of the vanilla SiT-XL at 1400 epoch and REPA at 800 epochs that uses pre-trained representation model to align. Moreover, with classifier-free guidance and guidance interval, our scheme shows an improved FID at 1.46 at 800 epochs. And for LightningDiT-XL/1 model which uses selfsupervised representation-based tokenizer, it achieves FID = 2.42 using 60 epochs and further achieves FID = 1.34 at 680 epochs, which achieves large margin between all of these methods. Combined with classifier-free guidance and guidance interval, our scheme achieves state-of-the-art results of FID = 1.19 at 680 epochs. 2. Related Work We discuss with the most relevant literature here and provide more discussion in Supplementary Material. Sampling Guidance in Diffusion Models. The sampling guidance after the training has become the key to improving the quality of the visual generation. Classifier-free guidance (CFG) [24] has become the standard method which forms guided score by extrapolating from an unconditional one. While highly effective, CFG will cause the problem of reduced generation diversity when its guiding coefficient becomes larger. Several studies [4, 14, 44, 60, 75] have reduced the downsides of CFG by making the guidance weight noise level-dependent. Another research direction separates the class guidance part from the CFG and only uses weaker version of the current model for guidance. This approach improves image quality while maintaining diversity. While promising, these methods are limited by the complex design of degradation strategies [26, 27], extra training [38] or additional sampling steps [5, 29], which are not available for current large-scale image generators in practice. In this paper, our proposed internal guidance can be seamlessly integrated into the current advanced diffusion Transformer with almost no additional cost, demonstrating its potential for large-scale application. Intermediate Representation Regularization for Diffusion Transformers. Many recent works have attempted to introduce intermediate representation regularization in diffu2 sion transformer training. MaskDiT [83] and SD-DiT [85] combine MAEs [21] and IBOTs [84] training strategy into the DiTs training process. TREAD [41] designs token routing strategy to speed up Diffusion Transformers training. More recently, self-supervised learning has been introduced into diffusion training to further enhance training efficiency. These methods can be broadly categorized into: (1) aligning intermediate network representations with pre-trained self-supervised models [46, 76, 79], and (2) incorporating self-supervised representation learning as regularization mechanism during training [33, 72]. In this paper, we employ the simplest approach of introducing an auxiliary supervision loss to alleviate the gradient vanishing problem in deep networks. This design bears conceptual similarity to DeepFlow [64], but it mainly focuses on introducing second-order ODEs in the training of the diffusion Transformer. Interestingly, we observed that the convergence performance of this simple method is comparable to that achieved by incorporating self-supervised learning as form of regularization. 3. Methodology 3.1. Preliminaries Our work is based on Flow-matching [51] and Diffusion [25], [52] and [1] have proposed unified perspective to understand these two types of generative modeling methods. We first introduce the relevant preliminaries. The standard Flow-matching and Diffusion-based models both start from Gaussian noise ϵ (0, I) and gradually transform it into data samples p(x) through stochastic processes. This process can be expressed as xt = αtx + σtϵ, (1) where αt and σt are decreasing and increasing functions of time [0, ]. The difference between the generative models based on Flow-matching and Diffusion lies in that the former typically interpolates noise and data within finite time interval, while the latter defines forward stochastic differential equation (SDE), which converges to Gaussian distribution (0, I) as . The denoising network Dθ estimates the original data by minimizing the denoising diffusion objective: Expx EϵN (0,I)Dθ(xt, t) x02. (2) As the network size of generative models continues to increase, how to efficiently obtain high-quality generation results has become the focus of attention. Next, we will introduce our framework from two aspects: the loss function and the corresponding sampling guidance after training completion. Furthermore, we will discuss some related properties of the proposed sampling guidance, as well as the design of new training acceleration methods based on this. Figure 2. The overall framework of our proposed Internal Guidance. We introduce an additional auxiliary supervision loss during training, and utilize the intermediate layer outputs during sampling process to guide the final outputs. 3.2. Internal Guidance: Guiding Diffusion Model with Its Own Internal Dynamics Training Diffusion Model with Intermediate Supervision. The difficulty of training deep networks has been explored in many aspects in various fields [19, 20, 31, 71]. Recently, in the field of generation, people have introduced self-supervised representation learning as regularization method for training diffusion models [33, 72]. In this paper, we attempt the simplest solution, which is to apply an auxiliary supervision in the intermediate layers of the network. Specifically, we additionally define an output layer after the intermediate part of the network. Accordingly, the intermediate and final losses Linter and Lfinal are defined as (cid:40) Linter = Di(xt, t) x02, Lfinal = Df (xt, t) x02, (3) where Di() and Df () denote the outputs of the intermediate and final layers, respectively. The overall framework is illustrated in Figure 2. For instance, for the training of deep Diffusion Transformer, its final training loss becomes: = Lfinal + λLinter, (4) where λ > 0 is hyperparameter that controls the trade-off between the loss of the intermediate output and that of the final output. As shown in Table 3, We verified the effect of this loss on the SiT-B/2 model [52]. Although this method is very simple, it can surprisingly achieve results that are similar to or even better than those obtained by using the regularization method based on representation learning for complex designs [33, 72]. Sampling Diffusion Model with Its Internal Outputs. The sampling stage is crucial for obtaining high-quality generated results. The standard classifier-free guidance (CFG) [38] sampling strategy can effectively improve the generation quality, but will cause an undesirable reduction in diversity. An alternative line of research attempts to maintain 3 posed IG is plug and play approach that no longer requires specific training on particular bad versions for specific experimental setups or additional sampling consumption. And this sampling method can also enhance the quality of image generation while maintaining diversity. Building upon this, we conduct an extensive discussion on the properties of IG. 4. Discussion In this section, we conduct detailed analysis of the specific impacts of IG from three perspectives: (1) the compatibility between IG and CFG (2) Guidance Interval in IG and (3) Training Acceleration Inspired by IG. 4.1. The compatibility between IG and CFG Although Internal Guidance (IG) can significantly improve the generation results without additional consumption during the sampling stage, its combination with Classifier-Free Guidance (CFG) can further improve the generation quality. To further elucidate the compatibility between our proposed IG and CFG, we conducted series of visualization experiments to analyze at which levels these guidance strategies improve generative quality. We study 2D toy example like [38] where small-scale denoising network is trained to perform conditional diffusion in synthetic dataset (Figure 3). The dataset is designed to exhibit low local dimensionality (i.e., highly anisotropic and narrow support) and hierarchical emergence of local details upon noise removal. Both properties that can be expected from the actual manifold of realistic images [3, 57]. Details of the experimental setup are provided in the Supplementary Material. As shown in Figure 3(a), sampling from diffusion model without guidance produces many outliers outside the data distribution. When using CFG, this guidance pulls samples toward higher values of log [pcon(x c)/puncon(x c)] [38], where pcon and puncon are the conditional and unconditional distributions learned by the denoising diffusion model. As shown in Figure 3(b), increasing the CFG coefficient shifts samples away from the opposite class and reduces outliers, but also suppresses branches near other classes, which enhances inter-class separability while reducing diversity. In contrast, Autoguidance [38] provides class-independent gradients that point inward towards the data manifold.The trajectories of these samples generated by this well trained model generally follow the local orientation and branching of the data manifold, pushing the samples deeper into the good side concentrates them at the manifold. Furthermore, our proposed IG exhibits performance similar to that of Autoguidance (Figure 3(c)). When the IG coefficient increases, these samples produce narrower distribution than the ground truth, but in practice this does not appear to have an adverse effect on the images. However, this reduced intra-class sample distance may introduce new outliers when large Autoguidance or IG coefficient Figure 3. fractal-like 2D distribution with two classes indicated with gray and orange regions. Approximately 99% of the probability mass is inside the shown contours. (a) Conditional sampling using small denoising diffusion model generates outliers due to its limited fitting capability. (b) Classifier-free guidance (w = 2.5) eliminates the vast majority of outliers but reduces diversity by over-emphasizing the class. (c) Internal guidance (w = 2) can maintain diversity as Autoguidance [38] while allowing some outliers at the ends of the branches. (d) The combination of IG and CFG can significantly reduce outliers without reducing diversity. diversity by guiding well-trained model using bad version of itself [38]. While promising, these approaches are still limited by carefully designed degradation strategies [26, 27], extra training [38] or additional sampling steps [5, 29]. By applying an auxiliary supervision to the intermediate layer of the denoising model in the training stage, we can obtain the output of the final layer and that of its intermediate layer during the sampling stage. This naturally amounts to creating bad version corresponding to the current final layers output. In this way, we propose Internal Guidance (IG), the intermediate output Di can be used to guide the final output Df to shift away from its own poor distribution, thereby achieving better generation results. Similar to classifier free guidance (CFG), its guiding effect is achieved by extrapolating the two denoised results using factor Dw(x; c) = Di(x; c) + w(Df (x; c) Di(x; c)). (5) Unlike the original Autoguidance [38] strategy, our pro4 Figure 4. Internal guidance inspires new training acceleration methods. (a) Conditional sampling using not well-trained denoising diffusion model generates large number of outliers. (b) Outliers can gradually be eliminated with sufficient training. (c) Internal guidance can also eliminate outliers with not well-trained denoising diffusion model. (d) The loss function we proposed can accelerate convergence. In SiT-B/2 experiments, our proposed loss function demonstrates superior accelerated convergence performance compared to REPA [79]. is applied before the model is fully trained. In this case, the stronger output fails to accurately point toward highprobability regions, rendering the guidance less effective. Because CFG provides class-conditional directionality, it complements IG by further suppressing such outliers. Combining the two achieves superior generative quality across training stages (Figure 3(d)). In Section 5.2, we present detailed quantitative validation to demonstrate this effect. 4.2. Guidance Interval in Internal Guidance Meanwhile, the guidance interval [44] can also affect the performance of internal guidance. The original guidance interval encourages using large CFG coefficient for sampling within continuous interval of noise levels, which is helpful for achieving better generation performance. For our proposed internal guidance, we also redefined the guidance function of Equation 5 by replacing ω with piecewise constant function: 4.3. Training Acceleration After conducting the above analysis, we naturally came up with new question: Can the effect of this IG be replicated during the training process, so as to achieve the goal of training acceleration? We also demonstrated the denoising diffusion model training process through 2D toy example. As shown in Figure 4(a) and (b), comprehensive training can significantly reduce many outliers near the branches compared to incomplete training. With our proposed IG, since its guiding direction points towards the overall direction of the stronger model, outliers near the branches can also be effectively reduced (Figure 4c). Based on the above observations, we incorporate the guiding effect of IG into the training process of the denoising diffusion model. Specifically, we integrate the gradient vectors obtained by calculating the differences between the final and intermediate outputs during the sampling process into the loss function, enabling the model to learn the guidance signal during training. Similar to [66], we rewrite the training objectives for the intermediate and final layers during the training process Dw(x; c) = Di(x; c) + w(σ)(Df (x; c) Di(x; c)), where w(σ) = (cid:40) if σ (σlow, σhigh], 1 otherwise, (cid:40) (6) Intermediate objective: x0, Final objective: x0 + ω sg(Df (xt) Di(xt)). (7) where, σ is the coefficient that controls the intensity of the noise. In the original guidance interval, applying CFG during periods of high noise range drastically reduces the variation in the results, basically leading them towards handful of template images per class. Therefore, the guidance interval used in the CFG indicates that this guidance should only be used in the middle and low-noise ranges. For our proposed IG with the guidance interval, we observed the opposite phenomenon: the internal guidance does not need to be applied in the low-noise range, but applied in the high-noise and middle-noise ranges. In Section 5.2, we have provided detailed presentation of the generated results after applying the guidance interval to the IG sampling. Where ω > 0 is hyperparameter that controls the strength of the guiding signal for the final layers training objective. In practice, we utilize the EMA model that updates along with the model to obtain the outputs at both the intermediate and final layers during the training process. As shown in Figure 4(d), the outliers of all branches have been significantly reduced after modifying the training objective. In the subsequent SiT-B/2 experiment, our designed training loss significantly outperformed the REPA [79] using pre-trained self-supervised representation models after 400K iterations, demonstrating performance comparable to that of IG. Due to the flexibility and tunability of IG in generating images during the reasoning process, we continued to use IG in subsequent large-scale experiments. 5 Table 1. Class-conditional performance on ImageNet 256256. SiT + IG reaches an FID of 1.75 in and further achieves an FID of 1.46 with CFG. LightningDiT + IG reaches an FID of 1.34 without CFG, outperforming all prior methods by large margin, and achieves an FID of 1.19 with CFG. For fair comparison, all prior methods employ random sampling 50K samples across 1,000 class labels. Method Epochs #Params Generation@256 w/o CFG Generation@256 w/ CFG or Autoguidance FID sFID IS Prec. Rec. FID sFID IS Prec. Rec. Pixel Diffusion ADM [10] RIN [32] PixelFlow [6] PixNerd [73] SiD2 [28] Vanilla Latent Diffusion DiT [55] MaskDiT [83] SiT [52] TREAD [41] MDTv2 [15] SiT+IG (ours) 400 480 320 160 1280 1400 1600 1400 740 1080 80 800 554M 410M 677M 700M - 675M 675M 675M 675M 675M 678M 10.94 3.42 - - - 9.62 5.69 8.61 - - 5.31 1.75 Latent Diffusion with Self-supervised Representation Model 80 800 80 80 480 64 800 800 60 680 675M 675M 677M 675M 839M 678M 7.90 5.90 3.46 1. 3.40 2.20 5.14 2.17 1.60 2.42 1.34 REPA [79] REPA-E [46] REG [76] LightningDiT [78] RAE (DiTDH) [82] LightningDiT+IG (ours) 5. Experiments 5.1. Setup Implementation details. For experiments on DiT [55] and SiT [52], we strictly follow their original setup. For experiments on LightningDiT [78], we find using the recipe in LightningDiT leads to instability or abnormal training dynamics at early epochs and slow EMA model convergence at early epochs. To solve these issues, we instead use the Muon optimizer [34] instead of AdamW and change the EMA weight from 0.9999 to 0.9995. Other optimization hyperparameters are the same as vanilla LightningDiT. We use ImageNet-1K [9] for diffusion training, where each image is preprocessed to the resolution of 256256 and follow ADM for other data preprocessing protocols. Each image is then encoded into compressed vector R32324 using the Stable Diffusion VAE or R161632 using the VAVAE. Detailed experimental details and hyperparameter settings, are provided in the Supplementary Material. Evaluation protocol. We report Frechet inception distance (FID) [23], sFID [53], inception score [61], precision(Pre.) and recall(Rec.) [43] using 50000 samples. Specifically, we uniformly randomly sample the 1,000 class labels 50,000 times and generate images accordingly SD,SiT,REPA for 6 6.02 - - - - 6.85 10.34 6.32 - - 5.68 4.98 5.06 5. 4.17 - - 4.67 4.22 4.36 5.40 3.81 3.94 101.0 182.0 - - - 121.5 177.9 131.7 - - 147.7 228.6 122.6 157.8 159.8 217.3 184.1 219.1 130.2 205. 242.7 173.7 229.3 0.69 - - - - 0.67 0.74 0.68 - - 0.80 0.80 0.70 0. 0.77 - - 0.77 0.76 0.77 0.79 0.79 0.78 0.63 - - - - 0.67 0.60 0.67 - - 0.56 0.62 0.65 0.69 0.63 - - 0.66 0.62 0. 0.65 0.62 0.65 3.94 - 1.98 2.15 1.38 2.27 2.28 2.06 1.69 1.58 1.46 - 1. 1.67 1.26 1.86 1.40 2.11 1.35 1.28 1.19 6.14 - 5.83 4.55 - 4.60 5.67 4.50 4.73 4.52 4.79 - 4.70 4.12 4.11 4.49 4.24 4.16 4. 4.72 4.11 215.8 - 282.1 297.0 - 278.2 276.6 270.3 292.7 314.7 265.7 - 305. 266.3 314.9 321.4 296.9 252.3 295.3 262.9 269.0 0.83 - 0.81 0.79 - 0.83 0.80 0.82 0.81 0.79 0.80 - 0.80 0.80 0.79 0.76 0.77 0.81 0. 0.78 0.79 0.53 - 0.60 0.59 - 0.57 0.61 0.59 0.63 0.65 0.64 - 0. 0.63 0.66 0.63 0.66 0.58 0.65 0.67 0.66 fair comparison, . Sampling follows original DiT, SiT and LightningDiT that we use the SDE Euler-Maruyama sampler with 250 steps for the experiments on DiT and SiT, and use the ODE Heun sampler with 125 steps for the experiments on LightningDiT. Full evaluation protocol details are provided in the Supplementary Material. 5.2. Ablation Studies The position of intermediate supervision. We begin by examining the effect of attaching the auxiliary supervision loss to different layers, which are shown in Table 2. We find that regularizing only the first few layers (e.g., 4 in SiT-B/2 [52] at 80 epochs) in training is effective. However, placing the auxiliary supervision loss in the latter half of the network or adding multiple auxiliary supervision losses in multiple intermediate layers cannot improve the convergence speed. We hypothesize that it interferes with the training of the deep layers output. Furthermore, we are surprised to find that the convergence effect with the auxiliary supervision loss is comparable to that of methods incorporate self-supervised representation learning regularization  (Table 3)  . In largescale experiments, we apply the auxiliary supervision loss and internal guidance to the first 8 layers in large-scale diffusion transformer models. Table 2. Auxiliary supervision loss on different intermediate layers on ImageNet 256256 with internal guidance (w = 1.5). and indicate whether lower or higher values are better."
        },
        {
            "title": "Baseline",
            "content": "+IG FID IS FID IS SiT-B/2 [52] 33.02 43.71 2 4 6 8 10 2 and 6 30.45 30.60 34.15 38.05 36.37 33. 47.97 47.70 42.38 37.97 40.91 43.08 20.39 19.02 25.04 34.05 35.58 31.55 61.38 65.06 52.39 40.35 40.86 46.66 Table 3. Convergence analysis of the SiT-B/2 model on ImageNet 256256 without classifier-free guidance (CFG). and indicate whether lower or higher values are better, respectively. All models were trained for 80 epochs. Figure 5. The combination of IG and CFG can yield higher FID value, which is superior to simply apply CFG. In particular, the FID value generated by the combination of lower IG coefficient and CFG is higher than that of higher coefficient. Table 4. The IG coefficient and the corresponding guidance interval analysis on ImageNet 256256. and indicate whether lower or higher values are better, respectively. Sampling ω Guidance Interval FID IS Methods Learning Paradigms FID IS SiT-B/2 Baseline [52] SRA [33] Disperse Loss [72] Self-Supervised Self-Supervised 33.02 43.71 29.10 31.45 50.20 47.05 w/o IG Auxiliary Supervision Supervised 30.45 47.97 w/ IG 1.0 1.3 1.5 1.7 1.9 2.1 2.3 2.3 2.3 2. [0, 1) 30.60 47.70 [0, 1) [0, 1) [0, 1) [0, 1) [0, 1) [0, 1) [0.3, 1) [0.3, 0.7) [0, 0.7) 21.88 19.02 17.61 17.38 17.75 18.53 16.19 18.23 20.69 59.55 65.06 68.16 69.12 68.67 66.77 72.95 66.21 60. Compatibility between IG and CFG. We also demonstrate that the proposed IG combined with CFG can further enhance the generation quality in the quantitative experiment. We apply IG on the fourth layer of SiT-B/2 to conduct the subsequent experiments. As shown in Figure 5, after SiTB/2 combines with our IG and then applies CFG, the best generated metric FID can be reduced from the original optimal 7.30 with only CFG to 6.50. Meanwhile, we find that when combining CFG, using lower IG coefficient can yield better generation results, which is different from the optimal coefficient when only adding IG alone (Table 4 and Figure 7). And this process basically does not involve significant computational costs, which indicates that the IG can serve as an easily integrable component for modern generative models to enhance the generation quality. Guidance interval in internal guidance. Next, we examine the role of guidance interval in internal guidance. We conducted rough search for the optimal combination rules of IG and CFG. We also apply IG on the fourth layer of SiT-B/2 to conduct experiments. In the Table 4, we can see that when no guidance interval is used, the optimal FID is approximately around the IG coefficient ω of 1.9. After increasing the IG coefficient, we find it actually has positive effect on the generation quality when the low-noise range does not apply IG. This is somewhat contrary to the conclusion of guidance interval used in CFG [44]. In our rough search, the best generative results are achieved when the IG coefficient is 2.3 and the guidance interval range is [0.3, 1). Scalability. We investigate the scalability of the auxiliary supervision loss with internal guidance by varying the model sizes of the diffusion transformers. In general, the effect of achieving efficient generation results of IG becomes more significant as the diffusion transformer model increases in size. We demonstrate this by plotting FID-50K of different SiT and DiT models with IG and without CFG in Figure: IG achieves the same FID level more quickly with larger models. To facilitate comparison, we set the IG coefficient to 2.3 and the guidance interval range is [0.3, 1) in all the DiTs and SiTs models. And the IG is applied in the fourth layer of the B/2 scale model and in the eighth layer of other scales for the experiments. Effect of λ. Finally, we examine the effect of the regularization coefficient λ by training SiT-B/2 models for 400K with different coefficients 0.25 to 1.0 and comparing the performance. As shown in Table 5, the performance becomes stable after λ <= 0.5. 5.3. Comparsion with State-of-the-art Methods We use several recent diffusion-based generation methods as baselines, each employing different inputs and network archi7 Figure 6. Scalability of IG. The relative improvement of IG over the vanilla model (DiTs and SiTs) becomes increasingly significant as the model size grows. Table 5. Ablation study for λ. λ 0.25 0.5 0.75 FID IS 30.38 47.86 30.60 47.70 31.58 46.23 31.24 46.24 tectures. Specifically, we consider the following three types of approaches: (a) Pixel Diffusion: ADM [10], RIN [32], PixelFlow [6], PixNerd [73] and SiD2 [28], (2) Vanilla Latent Diffusion: DiT [55], MaskDiT [83], SiT [52], TREAD [41], MDTv2 [15], (c) Latent Diffusion with Self-supervised Representation Model: REPA [79], REPA-E [46] and REG [76], LightningDiT [78] and DiTDH [82]. We initialized our models on SiT and LightningDiT respectively to separately verify the effectiveness of the IG in the latent diffusion model with original VAE and aligned VAE that represents the latent space using the pretrained self-supervised representation model (DINOv2-B [54]). We compare the FID values between SiT-XL/2 and LightningDiT-XL/1 with IG with other state-of-the-art methods without CFG. As shown in Table 1, IG shows consistent and significant improvement across all model variants. In particular, on SiT-XL/2, using IG (1.8) leads to FID=5.31 at 80 epochs, which already exceeds the FID of the vanilla SiT-XL at 1400 epoch and REPA at 800 epochs which uses pre-trained representation model to align. Note that the performance continues to improve with longer training; for instance, with SiT-XL/2, FID becomes 1.75 at 800 epochs. On LightningDiT-XL/1, using IG (1.7) leads to FID=2.42 at 60 epochs, which also already exceeds the FID of the vanilla LightningDiT-XL/1 at 64 epochs. For longer training, the FID becomes 1.34 at 680 epochs with LightningDiT-XL/1, which achieve large margin between all of these methods. Finally, we provide quantitative comparison between SiT-XL/2 or LightningDiT-XL/1 with IG and other recent diffusion model methods using CFG. At 800 epochs, SiTFigure 7. The combination of IG and CFG can produce better visual quality. When only adding IG, better generation results can be obtained. Incorporating CFG can further reduce the inaccurate content in the generated results. XL/2 + IG (1.8) achieves FID of 1.46 with CFG scale of = 1.35 and an extra guidance interval. Meanwhile, LightningDiT/1 + IG (1.4) achieves state-of-the-art FID of 1.19 with CFG scale of = 1.45 and an extra guidance interval at 680 epochs. We provide visual results of LightningDiTXL/2 + IG in Figure 8. Moreover, we also provide more uncurated examples and experimental results on ImageNet 512512 in the Supplementary Material, we show that IG also provides significant improvements in such setup. 6. Conclusion In this paper, we have presented Internal Guidance (IG), simple but effective guidance strategy for enhancing both generation quality and training efficiency. We introduce an auxiliary supervision loss in the deep Diffusion Transformer and utilize the output of its intermediate layers to guide the output results at the deep level during the sampling stage. This plug-and-play method has significantly improved the quality of image generation. We further analyze the properties of each part of our proposed IG, including its compatibility with CFG, its own guidance interval and discuss the design of new training acceleration methods based on its insight. Our IG can significantly improve the generation performance of various scale diffusion transformers with less computational resources. Ultimately, we get the state-of-the-art FID = 1.23 result for image generation when using LightningDiT-XL/1+IG with additional CFG. We hope our work would facilitate the research on more applicable sampling methods for visual generation."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo, Nicholas Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 3 [2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 6 [3] Bradley CA Brown, Anthony Caterini, Brendan Leigh Ross, Jesse Cresswell, and Gabriel Loaiza-Ganem. Verifying the union of manifolds hypothesis for image data. arXiv preprint arXiv:2207.02862, 2022. 4 [4] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-toimage generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 2, 6 [5] Chubin Chen, Jiashu Zhu, Xiaokun Feng, Nisha Huang, Meiqi Wu, Fangyuan Mao, Jiahong Wu, Xiangxiang Chu, s2-guidance: Stochastic self guidance for and Xiu Li. training-free enhancement of diffusion models. arXiv preprint arXiv:2508.12880, 2025. 1, 2, 4, 6 [6] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. 6, [7] Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024. 6 [8] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 2 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 6, 8, [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 6 [12] Weichen Fan, Amber Yijia Zheng, Raymond Yeh, and Ziwei Liu. Cfg-zero*: Improved classifier-free guidance for flow matching models. arXiv preprint arXiv:2503.18886, 2025. 6 [13] Johannes Fischer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan Baumann, and Bjorn Ommer. Boosting latent diffusion with flow matching. arXiv preprint arXiv:2312.07360, 2023. 6 [14] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF international con9 ference on computer vision, pages 2316423173, 2023. 2, [15] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 6, 8 [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 6 [17] Google. Nano banana:, 2025. 6 [18] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation. In European Conference on Computer Vision, pages 3755. Springer, 2024. 6 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 10261034, 2015. 3 [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 3 [21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. [22] Hendrycks. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 5 [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6, 1 [24] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1, 2, 6 [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [26] Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. Advances in Neural Information Processing Systems, 37:6674366772, 2024. 1, 2, 4, 6 [27] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7462 7471, 2023. 1, 2, 4, 6 [28] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. 6, 8 [29] Junha Hyung, Kinam Kim, Susung Hong, Min-Jung Kim, and Jaegul Choo. Spatiotemporal skip guidance for enhanced video diffusion sampling. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 11006 11015, 2025. 1, 2, 4, 6 [30] Aapo Hyvarinen and Peter Dayan. Estimation of nonnormalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005. 5 [31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448456. pmlr, 2015. [32] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. 6, 8 [33] Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, and Jingdong Wang. No other representation component is needed: Diffusion transformers can provide representation guidance by themselves. arXiv preprint arXiv:2505.02831, 2025. 3, 7 [34] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. 6, 2 [35] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1012410134, 2023. 6 [36] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022. 5 [37] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 5, [38] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. 1, 2, 3, 4, 5, 6 [39] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proc. CVPR, 2024. 5, 6 [40] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. 5 [41] Felix Krause, Timy Phan, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Tread: Token routing for efficient architecture-agnostic diffusion training. arXiv preprint arXiv:2501.04765, 2025. 3, 6, 8 [42] Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Ensembling off-the-shelf models for gan training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1065110662, 2022. 6 [43] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Improved precision and recall Lehtinen, and Timo Aila. metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 6, 1 [44] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems, 37:122458122483, 2024. 1, 2, 5, 7, [45] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 6 [46] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. 3, 6, 8, 1 [47] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. In Advances in Neural Information Processing Systems, pages 125441125468. Curran Associates, Inc., 2024. 6 [48] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. Advances in Neural Information Processing Systems, 37:125441125468, 2024. 6 [49] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 1 [50] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1228612312, 2023. 6 [51] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3, [52] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 1, 3, 6, 7, 8 [53] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. 6, 1 [54] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 8, 3 [55] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 1, 6, 8 [56] Pablo Pernias, Dominic Rampas, Mats Richter, Christopher Pal, and Marc Aubreville. Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. arXiv preprint arXiv:2306.00637, 2023. 6 [57] Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. arXiv preprint arXiv:2104.08894, 2021. 4 [58] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. 1 [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [60] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann Weber. Cads: Unleashing the diversity of diffusion models through condition-annealed sampling. arXiv preprint arXiv:2310.17347, 2023. 2, 6 [61] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6, [62] Axel Sauer, Kashyap Chitta, Jens Muller, and Andreas Geiger. Projected gans converge faster. Advances in Neural Information Processing Systems, 34:1748017492, 2021. 6 [63] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 6 [64] Inkyu Shin, Chenglin Yang, and Liang-Chieh Chen. Deeply supervised flow-based generative models. arXiv preprint arXiv:2503.14494, 2025. 3 [65] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28182826, 2016. 1 [66] Zhicong Tang, Jianmin Bao, Dong Chen, and Baining Guo. Diffusion models without classifier-free guidance. arXiv preprint arXiv:2502.12154, 2025. 5 [67] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 1 [68] Yuchuan Tian, Jing Han, Chengcheng Wang, Yuchen Liang, Chao Xu, and Hanting Chen. Dic: Rethinking conv3x3 designs in diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 24692478, 2025. [69] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. 6 [70] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 6 [71] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and Svetlana Lazebnik. Training deeper convolutional networks with deep supervision. arXiv preprint arXiv:1505.02496, 2015. 3 [72] Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025. 3, 7 [73] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. 6, 8 [74] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. arXiv preprint Ddt: Decoupled diffusion transformer. arXiv:2504.05741, 2025. 1 [75] Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernandez Abrevaya, David Picard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers. arXiv preprint arXiv:2404.13040, 2024. 2, 6 [76] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. arXiv preprint arXiv:2507.01467, 2025. 3, 6, 8 [77] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 6 [78] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. 6, 8, 1, 2 [79] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 3, 5, 6, 8, 1, [80] Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 6 [81] Jinhua Zhang, Wei Long, Minghao Han, Weiyi You, and Shuhang Gu. Mvar: Visual autoregressive modeling with scale and spatial markovian conditioning. arXiv preprint arXiv:2505.12742, 2025. 1 [82] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 6, 8, 1 [83] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 3, 6, 8 [84] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021. 3 [85] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang Wen Chen. Sd-dit: Unleashing the power of self-supervised discrimination in diffusion transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84358445, 2024."
        },
        {
            "title": "Supplementary Material",
            "content": "In this file, we provide more implementation and experimental details which are not included in the main text. In Section A, we provide the details of all the evaluation metrics and further compare the generation performance under uniform sampling. In Section B, we provide detailed comparison of computational resource consumption during the sampling stage between the proposed IG and REPA. In Section C, we provide comparison of the generate results under different sampling guidance methods. In Section D, we provide the experimental results of the proposed IG on the 512 512 ImageNet dataset. In Section E, we provide more hyperparameter settings and implementation details for Diffusion Transformer experiments of various scales. In Section F, We provide hyperparameter settings and implementation details for the 2D toy example. In Section G, We provide more detailed discussion on the related work of our approach. In Section H, we provide more uncurated generation results on ImageNet 256256 from the SiT-XL+IG with CFG. Table 6. Class-conditional performance on ImageNet 256256 with balanced sampling. LightningDiT + IG reaches an FID of 1.24 without CFG, outperforming all prior state-of-the-art VQ-based and Diffusion-based methods by large margin, and achieves an FID of 1.07 with CFG. For fair comparison, all prior methods employ uniform balanced sampling 50K samples across 1,000 class labels. Method Epochs #Params w/o CFG w/ CFG or Autoguidance FID IS Prec. Rec. FID IS Prec. Rec. Autoregressive VAR [67] MAR [49] xAR [58] 350 800 2.0B 943M 1.1B 1.92 2.35 - Latent Diffusion with Self-supervised Representation Model REPA [79] DDT [74] REPA-E [46] LightningDiT [78] RAE (DiTDH) [82] LightningDiT+IG (ours) 800 400 800 800 800 680 675M 675M 675M 675M 839M 678M 5.78 6.27 1.70 2.11 1.51 1.24 323.1 227.8 - 158.3 154.7 217.3 207.0 242.9 229.6 0.82 0.79 - 0.70 0.68 0.77 0.77 0.79 0.78 0.59 0.62 - 0.68 0.69 0.66 0.65 0.63 0.66 1.73 1.55 1.24 1.29 1.26 1.15 1.28 1.13 1.07 350.2 303.7 301.6 306.3 310.6 304.0 300.5 262.6 274.1 0.82 0.81 0. 0.79 0.79 0.79 0.80 0.78 0.79 0.60 0.62 0.64 0.64 0.65 0.66 0.65 0.67 0.66 A. Evaluation Metric We strictly follow the setup and use the same reference batches of ADM [10] for evaluation, following their official implementation. We use NVIDIA A6000 pro GPUs or 4090Ti GPUs for evaluation and enable tf32 precision for faster generation. In what follows, we explain the main concept of metrics that we used for the evaluation. FID [23] measures the feature distance between the distributions of real and generated images. It uses the Inception-v3 network [65] and computes distance based on an assumption that both feature distributions are multivariate gaussian distributions. sFID [53] proposes to compute FID with intermediate spatial features of the Inception-v3 network to capture the generated images spatial distribution. IS [61] also uses the Inception-v3 network but use logit for evaluation of the metric. Specifically, it measures KL-divergence between the original label distribution and the distribution of logits after the softmax normalization. Precision and recall [43] are based on their classic definitions: the fraction of realistic images and the fraction of training data manifold covered by generated data. As stated in [82], using class-balanced sampling can lead to better results compared to random sampling because it more closely approximates the true label distribution of the training set. As the FID value approaches lower range, these subtle differences begin to have greater impact. The vast majority of VQ-based generation methods [49, 58, 67, 81] have adopt the balanced sampling. To conduct comprehensive and fair comparison with more baselines, we also adopt balanced sampling to conduct comprehensive comparison of the current state-of-the-art VQ and Diffusion-based methods, as shown in Table 6. 1 B. Computational Cost Comparison We compare the computational efficiency of SiT-XL/2+REPA [79] and SiT-XL+IG under the same model scale in Table 7. IG introduces only marginal increase in parameter count and FLOPs relative to SiT-XL, while maintaining neral identical latency. Despite the minimal computational overhead, IG yields substantial improvements in generation quality, achieving relative reduction in FID, alongside an increase in IS. These results demonstrate that IG simultaneously improves generation quality and computational efficiency, highlighting its effectiveness as general-purpose enhancement for generative models. Table 7. Computational cost and performance comparison. This table compares REPA and IG on ImageNet 256256, detailing model size, FLOPs, sampling steps, latency, and generation quality metrics. the proposed IG achieves substantially better sample quality with negligible increases in computational cost. Method SiT-XL/2 + REPA SiT-XL/2 + IG #Params FLOPs Latency (s) 675 678 (+0.44%) 114.46 114.47 (+0.01%) 6.18 6.19 (+0.16%) FID 5.90 1.75 (-70.34%) IS 157.8 228.6 (+44.87%) C. Comparison of Guidance Methods To further illustrate the advantages of our proposed IG, we compared it with the original CFG [24] and Autoguidance [38] methods. For CFG method, we use the vanilla LightningDiT-XL/1 model which is trained for 800 epochs and employed its official CFG coefficients. For the Autoguidance method, since it is difficult to determine the optimal bad version model, we initialized 12-layer LightningDiT model and trained it for 10 epochs. We roughly searched for the optimal coefficients of Autoguidance and determined it to be 1.15. As shown in Table 8, our IG method outperforms both adding only CFG or only adding Autoguidance. Moreover, further combining CFG can achieve the current state-of-the-art in image generation, and has significant improvement compared to other guidance methods. Table 8. Different guidance strategies comparison. This table compares various sampling guidance methods. We compare the results of the standard CFG, Autoguidance, as well as our proposed IG and the combination of IG and CFG. Our proposed IG is more flexible compared to Autoguidance and can significantly enhance the results when combined with CFG as plug-and-play approach. Method LightningDiT-XL/1 + CFG [78] LightningDiT-XL/1 + Autoguidance LightningDiT-XL/1 + IG (ours) LightningDiT-XL/1 + IG + CFG (ours) Epochs FID sFID 4.15 1.35 4.06 1.81 3.94 1.34 1.19 4. 800 680 680 680 IS Prec. Rec. 295.3 0.65 0.79 0.63 0.79 215.9 0.65 0.78 229.3 0.66 0.79 269.0 D. 512512 ImageNet To further validate IGs effectiveness, we conduct experiments at 512512 resolution following REPAs protocol [79] with Muon optimizer [34]. The RGB images are processed through the VAE [59] to yield 64643 latents. As demonstrated in Table, IG surpasses the performance of REPA trained for 200 epochs and SiT-XL/2 trained for 600 epochs in terms of FID at only 60 epochs, demonstrating its superior effectiveness. E. Hyperparameter and More Implementation Details More implementation details. We implement our models based on the original DiT, SiT and LightningDiT implementation. To speed up training and save GPU memory, we use mixed-precision (fp16) with gradient clipping and FlashAttention [8] operation for attention computation. We also pre-compute compressed latent vectors from raw pixels via stable diffusion VAE and VA-VAE and use these latent vectors. We do not apply any data augmentation as discussed in MaskDiT and REPA. For stable diffusion VAE, we use stabilityai/sd-vae-ft-ema for encoding images to latent vectors and decoding latent vectors to images. For the output head, we use Linear layer to map the feature to the output size. The use of Muon In our large-scale experiment with the LightningDiT-XL/1 model, we adopt the Muon optimizer [34]. We find that in the environment of PyTorch, when using the pre-trained self-supervised representation model (such as DINOv2-B 2 Figure 8. Samples on ImageNet 512512 from SiT-XL/2 + IG (1.4) using CFG with = 1.8. Table 9. Performance comparison on ImageNet 512512. Model Epochs FID sFID IS Pre. Rec. Pixel diffusion VDM++ ADM-G, ADM-U Simple diffusion (U-Net) Simple diffusion (U-ViT, L) Latent diffusion, Transformer MaskDiT DiT-XL/2 SiT-XL/2 + REPA + REPA + REPA + IG - 400 800 800 800 600 80 100 200 60 2.65 2.85 4.28 4.53 2.50 3.04 2.62 2.44 2.32 2.08 1.78 - 5.86 - - 5.10 5.02 4.18 4.21 4.16 4.19 3.92 278.1 221.7 171.0 205.3 256.3 240. 252.2 247.3 255.7 274.6 286.3 - 0.84 - - 0.83 0.84 0.84 0.84 0.84 0.83 0.81 - 0.53 - - 0.56 0.54 0.57 0.56 0.56 0.58 0.62 [54]) to align the encoder of the VAE, there will be unstable training issues in the early stage of training. After careful investigation and experimentation, we discover that replacing the AdamW optimizer with Muon could effectively alleviate such issue. Compared to AdamW, the Muon optimizer has certain acceleration convergence effect in the early stage of model training, but we observe that the gap between them gradually narrowed as the training time increased, reaching similar final convergence results. In the later stage of training, there is small probability of encountering training instability issues again. Resuming the training with full precision can solve this problem. 3 Computing resources. We use NVIDIA A6000 pro 96 GB GPUs for training largest model (LightningDiT-XL/1, SiT-XL/2); and uses NVIDIA 4090 24GB GPUs for training smaller model (L, B). When sampling, we use either NVIDIA A6000 pro 96 GB GPUs or NVIDIA RTX 4090 24GB GPUs to obtain the samples for evaluation. Table 10. Defualt hyperparameter setup. Unless other otherwise specified, we use these sets of hyperparameters for different models. In our ablation experiments, settings are also kept the same except those we point out in Table 10. Architecture Input dim. Patch size Num. layers Hidden dim. Num. heads Optimization Batch size Optimizer lr (β1, β2) EMA decay Interpolants or Denoising αt σt wt Training objective Sampler Sampling steps CFG Scale IG Auxiliary supervision position IG Scale SiT-B SiT-L SiT-XL DiT-B DiT-L LightningDiT-XL 32324 2 12 768 12 256 AdamW 0.0001 (0.9, 0.999) 0. 32324 2 24 1024 16 256 AdamW 0.0001 (0.9, 0.999) 0.9999 32324 2 28 1152 16 256 AdamW 0.0001 (0.9, 0.999) 0.9999 32324 2 12 768 12 256 AdamW 0.0001 (0.9, 0.999) 0. 32324 2 24 1024 16 256 AdamW 0.0001 (0.9, 0.999) 0.9999 32324 2 28 1152 16 1024 Muon 0.0002 (0.9, 0.95) 0.9995 1 σt - v-prediction 1 σt - v-prediction Euler-Maruyama Euler-Maruyama Euler-Maruyama 250 - 1 σt - v-prediction 250 1.35 (if used) 250 - - - - 1000 noise-prediction DDPM 250 - - - - 1000 noise-prediction DDPM 250 - 1 σt - v-prediction Heun 125 1.45 (if used) 4 2.3 8 2.3 8 1.4 4 2.3 8 2.3 8 1. F. Details of the 2D toy example In this Section, we describe the construction of the 2D toy dataset used in the analysis of Section 4, as well as the associated model architecture, training setup, and sampling parameters. Dataset. For each of the two classes c, we model the fractal-like data distribution as mixture of Gaussians Mc = (cid:0){ϕi}, {µi}, {Σi}(cid:1), where ϕi, µi, and Σi represent the weight, mean, and 22 covariance matrix of each component i, respectively. This lets us calculate the ground truth scores and probability densities analytically and, consequently, to visualize them without making any additional assumptions. The probability density for given class is given by pdata(x c) = (cid:88) iMc ϕi (x; µi, Σi), where (x; µ, Σ) = 1 (cid:112)(2π)2 det(Σ) (cid:18) exp 1 2 (x µ)Σ1(x µ) (cid:19) . (8) (9) Applying heat diffusion to pdata(xc), we obtain sequence of increasingly smoothed densities p(xc; σ) parameterized by noise level σ: p(x c; σ) = (cid:88) iMc The score function of p(xc; σ) is then given by ϕi (x; µi, Σ i,σ), where Σ i,σ = Σi + σ2I. log p(x c; σ) = (cid:80) iMc ϕi (x; µi, Σ (cid:80) i,σ) (Σ ϕi (x; µi, Σ iMc i,σ) i,σ)1(µi x) (10) (11) . 4 We construct Mc to represent thin tree-like structure by starting with one main branch and recursively subdividing it into smaller ones. Each branch is represented by 8 anisotropic Gaussian components and the subdivision is performed 6 times, decaying ϕ after each subdivision and slightly randomizing the lengths and orientations of the two resulting sub-branches. This yields 1278 = 1016 components per class and 10162 = 2032 components in total. We define the coordinate system so that the mean and standard deviation of pdata, marginalized over c, are equal to 0 and σdata = 0.5 along each axis, respectively, matching the recommendations by Karras et al. [37]. Models. We implement the denoiser models as simple multi-layer perceptrons, utilizing the magnitude-preserving design principles from EDM2 [40]. Following Autoguidance [38], we design the model interface so that for given noisy sample, each model outputs single scalar representing the logarithm of the corresponding unnormalized probability density, as opposed to directly outputting the denoised sample or the score vector. Concretely, let us denote the output of given model by Gθ(x; σ, c). The corresponding normalized probability density is then given by pθ(x c; σ) = exp(cid:0)Gθ(x; σ, c)(cid:1) (cid:46) (cid:90) exp(cid:0)Gθ(x; σ, c)(cid:1) dx. (12) By virtue of defining Gθ this way, we can derive the score vector, and by extension, the denoised sample, from Gθ through automatic differentiation: log pθ(xc; σ) = xGθ(x; σ, c) Dθ(x; σ, c) = + σ2xGθ(x; σ, c). (13) (14) Besides Equation 13, according to the description of Autoguidance, trying out the alternative formulations where the model outputs the score vector or the denoised sample directly is qualitatively more or less identical. To connect the above definition of Gθ to the raw network layers, we apply preconditioning using the same general principles as in EDM [36]. Denoting the function represented by the raw network layers as Fθ, we define Gθ as Gθ(x; σ, c) = 1 x2 2 gθ σn (cid:88) i=1 (cid:18) x; Fθ,i 1 4 (cid:19)2 log σ, , where = (cid:112)σ2 + σ2 data (15) and the sum is taken over the output features of Fθ. We scale the output of Fθ by learned scaling factor gθ that we initialize to zero. The goal of Equation 15 is to satisfy the following three requirements: The input of Fθ should have zero mean and unit magnitude. This is achieved through the division by (cid:112)σ2 + σ2 After initialization, Gθ should represent the best possible first-order approximation of the correct solution. This is achieved data. through the 1 After training, 2 x2 2 term, as well as the fact that gθ = 0 after initialization. gθ Fθ should have approximately unit magnitude. This is achieved through the division by σn. In practice, we use an MLP with one input layer and four hidden layers, interspersed with SiLU [22] activation functions and implemented using the magnitude-preserving primitives from EDM2 [39]. Additionally, we defined an output layer after 4 log σ; 1(cid:3) and the first hidden layer to produce weaker version of the output. The input is 4-dimensional vector (cid:2)x the output of each hidden layer has features, where = 64. x; y; Training. Given that we have the exact score function of the ground truth distribution readily available (Equation 10), we train the models using exact score matching [30] for simplicity and increased robustness. We thus define the loss function and the additional supervision loss as L(θi) = Eσptrain,xp(x;σ)σ2 log pθi(x; σ) log p(x; σ)2 2 , (16) L(θd) = Eσptrain,xp(x;σ)σ2 log pθd (x; σ) log p(x; σ)2 2 , where σ ptrain is realized as log(σ) (Pmean, Pstd) [37] and θi and θd represent the intermediate layer and the deep layer network respectively. In particular, for the training acceleration method depicted in Figure 4, our loss function is (17) L(θi) = Eσptrain,xp(x;σ)σ2 log pθi(x; σ) log p(x; σ)2 2 , (18) 5 L(θd) = Eσptrain,xp(x;σ)σ2 log pθd (x; σ) (x log p(x; σ) + 0.5 (x log pθd (x; σ) log pθi(x; σ)))2 2 , (19) The final training loss is = L(θd) + 0.5 L(θi) The more commonly used denoising score matching do not show any significant differences in the model behavior or training dynamics. We train these models for 4096 iterations using batch size of 4096 samples. We set Pmean = 2.3 and Pstd = 1.5, and use αref/(cid:112)max(t/tref, 1) learning rate decay schedule with αref = 0.01 and tref = 512 iterations, along with power function EMA profile [39] with σrel = 0.010. Overall, the setup is robust with respect to the hyperparameters; the phenomena illustrated in Figures 3 and 4 remain unchanged across wide range of parameter choices. Sampling. We use the standard EDM sampler [37] with = 32 Heun steps (NFE = 63), σmin = 0.002, σmax = 5, and ρ = 7. We chose the values of and σmax to be much higher than what is actually needed for this dataset in order to avoid potential discretization errors from affecting our conclusions. In Figure 3, we set = 2.5 for CFG, = 2 for autoguidance and = 2 for internal guidance (IG), and w1 = 1 and w2 = 1.5 for IG + CFG. In Figure 4, we set = 2 for internal guidance. G. More Discussion on Related Work Denoising transformers. Many recent works have tried to use transformer backbones for diffusion or flow-based model training. First, several works like U-ViT [2], MDT [15], and DiffiT [18] how transformer-based backbones with skip connections can be an effective backbone for training diffusion models. Intriguingly, DiT and SiT show pure transformer architecture can be scalable architecture for training diffusion-based models. Based on these improvements, Stable diffusion 3 [11], FLUX 1 [45] and Nano Banana [17] show pure transformers can be scaled up for challenging text-to-image generation, and Sora, CogvideoX [77] and Wan [70] demonstrate their success in text-to-video generation. Our work analyzes and improves the training of Diffusion Transformer architecture based on simple auxiliary supervision to the early layers. Sampling guidance in diffusion models. Achieving better generation results over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance [10], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifier free guidance (CFG) [24] is proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. Subsequently, several studies [4, 7, 12, 14, 44, 60, 75] have reduced the downsides of CFG by making the guidance weight noise level-dependent. Another research direction separates the class guidance part from the CFG and only uses weaker version of the current model for guidance [5, 26, 27, 29, 38], which also bears conceptual similarity to contrastive decoding [50] used in large language models to reduce the repetitiveness of generations. Our approach does not require any additional sampling steps. It can be applied to any deep diffusion Transformer network in plug-and-play manner while achieving better generation results. Efficient training of generative models. The issue of how to accelerate the convergence of generative models during the training process has also received significant attention. Previous studies have explored architectural optimization [41, 55, 68], improved flow-based theories [13, 51], optimized data [69, 80] to accelerate convergence. Meanwhile, there have been several approaches in generative adversarial network [16] that try to accelerate training with better convergence using pretrained visual encoder [35, 42, 62, 63]. Another line of work tries to exploit the pretrained visual encoders for improving diffusion model training from scratch [48, 56], usually by training two diffusion models where one model generates the pretrained representations and the other model generates the target data conditioned on the generated representation. More recently, representation learning has been introduced into generative model training to further enhance training efficiency [46, 47, 76, 78, 79, 82]. Our method not only alleviate the problem of gradient vanishing during training, but also can be directly applied in the sampling stage to achieve better generation results on models with less training consumption. H. More Qualitative Results Below we show some uncurated generation results on ImageNet 256256 from the SiT-XL+IG (1.8). We use classifier-free guidance with = 2.5. Figure 9. Uncurated visualization results of LightningDiT-XL/1 + IG (1.8) use CFG with = 2.5. 7 Figure 10. Uncurated visualization results of LightningDiT-XL/1 + IG (1.8) use CFG with = 2.5. 8 Figure 11. Uncurated visualization results of LightningDiT-XL/1 + IG (1.8) use CFG with = 2.5."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "North China Institute of Computer Systems Engineering",
        "Sun Yat-sen University",
        "University of Electronic Science and Technology of China"
    ]
}