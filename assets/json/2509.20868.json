{
    "paper_title": "StyleBench: Evaluating thinking styles in Large Language Models",
    "authors": [
        "Junyu Guo",
        "Shangding Gu",
        "Ming Jin",
        "Costas Spanos",
        "Javad Lavaei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 8 6 8 0 2 . 9 0 5 2 : r STYLEBENCH: EVALUATING THINKING STYLES IN LARGE LANGUAGE MODELS Junyu Guo University of California, Berkeley Shangding Gu University of California, Berkeley Ming Jin Virginia Tech Costas Spanos University of California, Berkeley Javad Lavaei University of California, Berkeley"
        },
        {
            "title": "ABSTRACT",
            "content": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning stylesChain-of-Thought (CoT), Tree-of-Thought (ToT), Algorithm-of-Thought (AoT), Sketch-of-Thought (SoT), and Chain-of-Draft (CoD)on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require largescale models, while concise styles (SoT, CoD) achieve radical efficiency gains on welldefined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as function of scale. Our findings offer crucial roadmap for selecting optimal reasoning strategies based on specific constraints, We open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated impressive capabilities across diverse range of tasks, including mathematical reasoning, code generation, and complex question answering (Imani et al., 2023; Wang & Chen, 2023; Tan et al., 2023). key insight from prior work is that their performance on challenging problems is not merely function of scale, but is critically dependent on the methods used to guide reasoning (Huang & Yang, 2025). This has spurred the development of sophisticated prompting techniques designed to structure the models internal reasoning process. Notable among these are Chain-of-Thought (CoT) (Wei et al., 2022), which decomposes problems into sequential steps, and more advanced paradigms like Tree-of-Thought (ToT) (Yao et al., 2023), which explores multiple reasoning paths in parallel, and Reasonflux (Yang et al., 2025b), employing high-level templates to explore potential solutions. Corresponding author: junyuguo24@berkeley.edu 1 Despite the outstanding capabilities of advanced models like GPT-4o (Agarwal et al., 2025) and DeepSeek (Guo et al., 2025), their application to specialized or highly complex problems often reveals critical limitations. Performance remains highly sensitive to prompt phrasing and frequently necessitates iterative feedback to achieve robust results (Sel et al., 2023). In response, recent work has sought to automate reasoning strategy selection. For instance, Gao et al. (2024) proposed two-stage meta-reasoning process to identify the optimal style, while Wan et al. (2025) leveraged generated instructions to guide the model. However, these approaches assume the existence of well-understood mapping between problem types and optimal reasoning strategiesa foundation that is currently lacking. The existing literature provides an incomplete picture, as evaluations are typically limited to single reasoning style, narrow set of tasks, or small selection of models. This leaves significant gap in our understanding of how these strategies generalize across different model architectures, problem domains, and computational budgets. critical challenge is the trade-off between reasoning depth and efficiency. For simple queries, we expect an LLM to provide concise, direct answer without verbose reasoning chain. For complex problems, however, more elaborate thinking procedure is necessary to achieve high accuracy. This raises the important issue of preventing LLMs from overthinking on simple tasks without compromising their ability to reason deeply on hard ones, see e.g., (Chen et al., 2024; Fang et al., 2025; Sui et al., 2025). An ideal reasoning strategy should be both effective and efficient, adapting its cognitive load to the complexity of the problem at hand. The gaps in generalization understanding and the need for adaptive efficiency motivate the central question of our benchmark: How do contemporary reasoning strategies perform across diverse suite of tasks, model scales, and architectures, and which approach offers the optimal balance between performance and computational efficiency? To address these gaps, we introduce StyleBench, rigorous and extensive benchmark for evaluating reasoning strategies in LLMs. We systematically assess several representative stylesfrom simple prompting to complex multi-path searchesacross diverse models and tasks. Our work provides clear, empirical guidance and practical roadmap for selecting the most effective reasoning strategy for given application. Our contributions are summarized as follows: Comprehensive Benchmark: We introduce large-scale benchmarking framework that systematically evaluates five reasoning styles (Chain of Thought Wei et al., 2022, Tree of Thought Yao et al., 2023, Algorithm of Thought Sel et al., 2023, Sketch of Thought Aytes et al., 2025, Chain of Draft Xu et al., 2025) across five diverse tasks, including mathematical reasoning (Cobbe et al., 2021), question answering (Liu et al., 2020; Talmor et al., 2018), and puzzle-solving. Extensive Model Coverage: Our evaluation encompasses 15 state-of-the-art open-source LLMs spanning major model families (Qwen Yang et al., 2025a, LlaMA Grattafiori et al., 2024, Mistral Jiang et al., 2024, Gemma Team et al., 2025, GPT-OSS Agarwal et al., 2025, DeepSeek Guo et al., 2025) and scales (270M to 120B parameters). ModelStyle Interaction: We demonstrate that reasoning style efficacy is highly contingent on model architecture and scale, showing that optimal strategy selection is model-dependent. TaskStyle Affinities: We identify strong correlations between task types and effective reasoning strategies. Structured multi-step reasoning (e.g., CoT) excels in mathematical tasks, while branching-based exploration (e.g., ToT, AoT) proves more effective for open-ended puzzles like Game of 24. In-context learning styles (CoD, SoT) perform best on symbolic reasoning and commonsense reasoning tasks. Scaling Laws for Reasoning: We provide the empirical analysis of how reasoning style performance scales with model size, revealing non-trivial trade-offs between accuracy, latency and efficiency. 2 Core Logic and Structural Framework Dataset Questions Reasoning Style Processing LLM Response Mathematical Reasoning e.g., Solve 2x + 5 = 15 Step-by-Step Decompose equation solving into sequential steps Common Sense QA e.g., Why is sky blue? Causal Explanation Explain light scattering mechanisms 1. Subtract 5: 2x = 10 2. Divide by 2: = 5 Answer: = 5 Blue light scattered more by atmospheric molecules due to shorter wavelength Logical Deduction e.g., Syllogism validation Rule Application Apply syllogism reasoning rules Valid syllogism: All AB, CA, therefore CB Multi-step Decision e.g., Route planning Path Planning Identify routes and connections Bus Line 1 (15min) Transfer Subway Line 3 (20min) Figure 1: Core Logic and Structural Framework for Reasoning Style Processing"
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 REASONING WITH LARGE LANGUAGE MODELS Recent advances in LLM reasoning have been driven by the development of structured thinking styles. Chain-of-Thought (CoT) prompting (Wei et al., 2022) demonstrated that step-by-step reasoning can substantially improve performance, particularly in mathematical and logical tasks. Building on this idea, Tree-ofThought (ToT) (Yao et al., 2023) introduced branching exploration strategy that allows models to consider multiple reasoning paths in parallel. Subsequent paradigms such as Algorithm-of-Thought (AoT) (Sel et al., 2023), Sketch-of-Thought (SoT) (Aytes et al., 2025), and Chain-of-Draft (CoD) (Xu et al., 2025) further extend this direction by incorporating algorithmic priors, lightweight reasoning sketches, or iterative drafting mechanisms into prompts. Other approaches leverage high-level templates or rich contextual information to equip LLMs with more structured reasoning capabilities (Gao et al., 2024; Yasunaga et al., 2023; Yang et al., 2025b). Despite these advances, most existing methods rely on fixed reasoning style determined in advance, which may be suboptimal across heterogeneous tasks. 2.2 BENCHMARKING LLM REASONING The development of comprehensive benchmarks has been crucial for evaluating the reasoning capabilities of Large Language Models (LLMs). Existing research has largely focused on specialized domains, each requiring distinct reasoning skills. Mathematical reasoning is commonly assessed using benchmarks such as GSM8K (Cobbe et al., 2021), HardMath (Fan et al., 2024) and the more challenging AIME problems. For logical reasoning, datasets like LogiQA (Liu et al., 2020) provide standardized tests, while commonsense reasoning is typically measured by benchmarks such as CommonsenseQA (Talmor et al., 2018). In the domain of code generation, Bigcodebench (Zhuo et al., 2024), HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) evaluate functional correctness and algorithmic problem-solving capabilities. More recently, many works have focused on assessing LLMs performance on puzzle-solving and constrained reasoning tasks. These include problems such as word sorting, Sudoku, and Game of 24, which require structured, multi-step deduction and explore the limits of LLMs systematic reasoning abilities."
        },
        {
            "title": "3.1 REASONING STYLES IN LARGE LANGUAGE MODELS",
            "content": "This work evaluates five distinct reasoning methodologies that represent different strategies for structuring the LLM problem-solving process: Chain of Thought (CoT) (Wei et al., 2022) guides models to decompose problems into sequential series of intermediate steps. By explicitly generating reasoning trace, this approach significantly improves performance on multi-step tasks like mathematical reasoning. Chain of Draft (CoD) (Xu et al., 2025) emphasizes brevity by constraining models to produce condensed, symbolic reasoning traces. The prompt establishes this format through few-shot examples, leading to responses like 20 12 = 8, ####8 for arithmetic problems. = 12, = 20 Sketch of Thought (SoT) (Aytes et al., 2025) uses two-stage process: trained adapter first identifies the question type, then retrieves relevant few-shot examples to augment the prompt. This encourages concise, symbolic answers while maintaining transparency. Tree of Thought (ToT) (Yao et al., 2023) frames reasoning as tree search, maintaining multiple parallel reasoning paths (nodes) and pruning less promising branches. This allows for more systematic exploration of the solution space than linear methods. Algorithm of Thought (AoT) (Sel et al., 2023) implements backtracking search, enabling the model to retreat from unproductive paths and explore alternatives, thereby mimicking algorithmic problem-solving. Example prompts for each style are provided in Table 1, with additional visualizations of each mechanism included in the Appendix. Table 1: Comparison of Different Thinking Styles for Mathematical Problem Solving Prompt: Solve the equation: 3x + 7 = 22 Thinking Style Processed Prompt CoT CoD AoT SoT ToT Let me solve this step by step Think step by step, but only keep minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after separator ####. Let me explore multiple approaches, and backtrack to the previous one when the current path is incorrect. This is symbolic reasoning question. will feed you with some examples to solve this type of question Ill construct solution tree: Root: 3x + 7 = 22. Branch through different solution paths and select the optimal approach. 3.2 THE STYLEBENCH BENCHMARK The StyleBench benchmark was created by writing the question from each dataset in the form of each thinking style, then pass the prompt to the evaluation model. This results in 500 prompts for each model and each dataset under each thinking style. Examples of prompt entries and model responses can be found in Appendix C. Each thinking styles mechanism is visualized in Appendix A."
        },
        {
            "title": "3.3 BENCHMARK CONSTRUCTION",
            "content": "StyleBench evaluates five distinct reasoning methodologies across comprehensive suite of 15 open-source language models. The selected models cover wide range of parameter scales (270M to 120B) and major architectural familiesincluding LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeekto ensure the broad applicability of our findings. To ensure reproducibility and deterministic outputs, we set the model temperature to 0 for all experiments and collected single response per model-question pair. Model performance was evaluated by automatically extracting the final answer from each generated response and comparing it against the ground truth. We categorize the evaluated models into three groups based on scale: Small-scale models (< 5B parameters) include Gemma3-270M (Google), Qwen2.5-0.5B (Alibaba), DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek AI), Gemma-2B (Google), Qwen2.5-3B (Alibaba), and Phi3-Mini-4K-Instruct-3.8B (Microsoft). This group features models optimized for efficiency, with the distilled DeepSeek model and instruction-tuned Phi-3 providing specific insights into compact reasoning capabilities. Medium-scale models (5B15B parameters) include Mistral-7B (Mistral), Qwen-7B (Alibaba), Llama3-8B (Meta), and Gemma2-9B (Google). These models strike practical balance, offering substantial reasoning capabilities while remaining feasible for real-world deployment. Large-scale models (> 15B parameters) include GPT-OSS-20B (OpenAI), Qwen2.5-32B (Alibaba), Llama3-70B (Meta), Qwen2.5-72B (Alibaba), and GPT-OSS-120B (OpenAI). This group enables the study of advanced reasoning emergence at scale, with the recently released GPT-OSS series providing valuable performance baselines for large models."
        },
        {
            "title": "4 RESULTS",
            "content": "We begin by providing high-level overview of the aggregate performance across different reasoning styles. To enable this comparison, we first compute the mean accuracy of each style over the five benchmark datasets. These scores are then averaged across all models within each of the three size-based groups (small, medium, and large). This analysis highlights several key trends regarding the interaction between model scale, task type, and reasoning strategy. The aggregated results for the three model groups are presented in Figure 2, while detailed accuracy scores for each model, style, and task are reported in Appendix B. 4.1 AGGREGATE PERFORMANCE AND SCALING TRENDS As expected, the performance of all reasoning styles improves with increasing model scale. However, the rate of improvement is not uniform. Search-based strategies like ToT and AoT demonstrate pronounced scaling law, showing their highest relative advantage on challenging tasks like AIME and Game of 24 only when using large-scale models. In contrast, their performance on small and medium models is unremarkable. CoD emerged as the most stable and robust style across all model sizes and tasks. 4.2 THE ROLE OF INNATE KNOWLEDGE VS. REASONING The results on CommonsenseQA highlight the distinction between knowledge retrieval and reasoning. For large models, all reasoning styles perform similarly well, suggesting that the models inherent knowledge is sufficient to solve the task with even minimal prompting. Conversely, for small and medium models, all styles struggle profoundly; the best-performing style (SoT for medium models) barely exceeds 6% accuracy. This stands in stark contrast to the large models, where even the worst-performing style (CoT) surpasses 30%, underscoring vast performance gap driven primarily by model scale."
        },
        {
            "title": "4.3 TASK-STYLE AFFINITIES",
            "content": "Our analysis reveals strong, task-specific affinities for certain reasoning styles, independent of model size. GSM8K. Contrary to our expectation that complex reasoning strategies would prevail, Chain-of-Thought (CoT) consistently outperformed all others across every model group. This indicates that for mathematical problems of this difficulty, straightforward, stepwise reasoning process is not only sufficient but optimal. LogiQA. SoT proved to be the unequivocally superior strategy, with significant accuracy margin over all others. We hypothesize that this is because logical reasoning tasks benefit from structured, symbolic reasoning traces. SoTs use of correlated few-shot examples with concise answers allows it to maximize the utility of constrained context window, efficiently guiding the model to the correct logical conclusion."
        },
        {
            "title": "5 DISCUSSION",
            "content": "To move beyond aggregate metrics and understand the nuanced failures and successes of different reasoning strategies, we conduct detailed qualitative analysis of model responses. This case study approach allows us to probe the underlying causes of performance difference and address three critical questions: (1) What causes specific reasoning style to fail on task where others succeed? (2) To what extent do these failures reflect fundamental misunderstanding versus minor, recoverable error? (3) What are the practical implications for selecting an optimal reasoning style for given problem? Our analysis of these failure modes and success patterns provides crucial insights into the inner workings of LLM reasoning. 5.1 KEY FINDINGS Findings: On high-difficulty tasks, small models typically do not exhaust the available token budget. Instead, they tend to produce an answer regardless of correctness. This behavior suggests that when an LLM lacks the capability to solve problem, it often defaults to guessing rather than abstaining. We analyze the average token consumption of each reasoning style across different models on two challenging tasks, AIME and Game24, as visualized in Figure 3. Contrary to what might be expected, Figure 3 shows that smaller models (with the exception of DeepseekDistill) do not consistently consume more tokens than large models (e.g., Qwen-72B or GPT-OSS-120B) in these tasks. This is particularly notable in high-difficulty tasks like Game24, which requires models to exhaustively search for combinations of elementary arithmetic operators using four given numbers to reach the target of 24. Although smaller models struggle to find correct solutions (as shown in Figure 2), they frequently complete their generation without hitting the predefined token budget limit. To investigate this further, we present examples of responses from Qwen-3B and Llama-3-8B on the AIME and Game24 datasets, respectively, in Appendix D.1. Both responses are incorrect, yet each model produced final answer and terminated generation naturally, rather than being truncated for exceeding limits. In the Qwen-3B response for AIME (see the first example), the models reasoning is correct until Equation 2. However, it makes critical error during verification: after setting = 60 and obtaining = 156, n. This illustrates that while small modit incorrectly accepts this result despite the constraint that els can sometimes generate high-level rationales, their inability to avoid subtle but decisive logical flaws 6 (a) Small Models Average Accuracy on each task (b) Medium Models Average Accuracy on each task (c) Large Models Average Accuracy on each task Figure 2: Overall Accuracy rate for each group of models across five tasks ultimately prevents them from arriving at the correct solution. The second response, from Llama-3-8B on Game24, reveals different failure mode. The model terminates after outputting the expression 12 + 12 , which violates the core rules of the task, as it uses only two of the four provided numbers. Furthermore, its intermediate reasoning steps (Step 2 and Step 3) are entirely disconnected from the final output. This suggests that for smaller models, advanced prompting instructions do not reliably lead to coherent, multi-step reasoning; instead, the models often produce superficially structured but ultimately nonsensical rationales before guessing an answer. These patterns indicate that the primary bottleneck for small and medium-sized LLMs on complex tasks is not lack of generative capacity (as they do not exhaust token budget) but fundamental deficiency in reasoning capability. This phenomenon aligns with recent research on LLM behavior, such as Fu et al. (2025) showing that LLMs can exhibit unjustified confidence in incorrect answers during reasoning processes. 7 (a) AIME token usage results. (b) Game24 token usage results. Figure 3: Token usage comparison of different models on two datasets. Findings: Following output format instructions is skill that improves with model size. Small models often ignore formatting, making evaluation harder. boxed { answer } Our case study (see D.2) reveals that this formatting issue is widespread. Even with clear instructions to use the format, smaller models frequently ignore this directive. This inconsistency creates major challenges for automated evaluation, requiring additional processing to extract answers correctly. The problem extends beyond mathematical tasks. On LogiQA and CommonsenseQA, we often find smaller models adding unexpected characters around answerslike < > or < 2 >instead of following the specified format. This behavior appears to reflect patterns learned during pretraining that smaller models lack the capacity to override when given explicit instructions. Findings: Search-based methods like AoT and ToT work well for open problems like Game24 by trying many solutions, but incur higher token usage. For clearer tasks like LogiQA, concise methods like SoT and CoD give short, accurate answers fast. The responses show clear strengths and weaknesses for each reasoning style. In the Game24 problem (D.3), AoT found solution by flexibly trying different math operations. In contrast, ToT failed because its first guess was wrong and it could not recover without using many more tokens. This shows key problem with tree-search: it needs good first guesses to work well. On the same Game24 problem, CoT, CoD, and SoT all failed. They tried random number combinations instead of searching in smart way. This shows that methods without built-in search strategy often guess poorly on open-ended puzzles. However, CoD and SoT work very well for structured tasks like LogiQA (D.4). They produced much shorter answers than CoT (16% and 94% shorter) because they work in different ways: CoD uses knowledge from its training to give more direct answers. SoT skips unnecessary steps by connecting ideas quickly. Both methods kept high accuracy while being much more efficient. This makes them very useful for realworld applications where speed and cost matter. Findings: Effective reasoning on challenging problems is strongly dependent on model scale. Large models can succeed with variety of strategies, while smaller models require specialized, advanced prompting to achieve comparable performance. This scaling effect is evident in our case studies (Appendix E.1 and E.2), which evaluate reasoning styles across the Qwen series (3B to 72B) and GPT-OSS-120B on the Game24 task. The smallest models (Qwen3B/7B) failed completely. Qwen-32B generated solution, but it violated the game rules. Only the larger models succeeded: Qwen-72B produced two valid solutions (via CoT and AoT), and GPT-OSS-120B produced one correct solution (via SoT). Critically, large models consistently generated meaningful solution attempts, whereas smaller models often failed to make progress or became stuck in unproductive loops, highlighting fundamental capability gap."
        },
        {
            "title": "5.2 KEY QUESTIONS",
            "content": "Can LLMs autonomously select the most effective reasoning style for given problem? Our findings indicate that this meta-reasoning capability remains emergent and is not readily achievable through standard supervised fine-tuning (SFT). We fine-tuned Qwen-7B model to identify optimal reasoning styles, providing explicit rationales for each selection. The model failed to develop robust selection strategy, instead defaulting to shallow memorization of the training distribution. This resulted in pathological bias toward consistently selecting Chain-of-Draft (CoD) across diverse problems, yielding no substantial advantage over using any single fixed style. This behavior aligns with known limitations of SFT, particularly model hallucination and shallow pattern matching Ren & Sutherland (2024). Crucially, these results reveal that current fine-tuning approaches enable only superficial association with style selection rather than genuine, contextual understanding of which strategy best fits given problem. Experimental details are provided in Appendix G."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced StyleBench, comprehensive benchmark for systematically evaluating five reasoning styles (CoT, ToT, AoT, SoT, CoD) across five diverse tasks and 15 language models spanning 270M to 120B parameters. Our large-scale analysis yields several crucial insights that advance our understanding of reasoning in LLMs. Our findings demonstrate that reasoning strategy effectiveness is highly contingent on both task requirements and model capabilities, with no single approach dominating across all scenarios. We identified distinct behavioral patterns across model scales: smaller models frequently disregard formatting instructions and default to guessing, while larger models exhibit more reliable instruction-following and systematic reasoning capabilities. Most significantly, we observed substantial performance gap that correlates strongly with model scale, confirming that fundamental reasoning abilitiesincluding the capacity for meta-reasoning about strategy selectionemerge primarily with increased model size. These results provide practical framework for optimal strategy selection: search-based methods (ToT, AoT) excel for complex, open-ended problems (e.g., Game24) with capable models, while concise approaches (SoT, CoD) offer superior efficiency for well-structured tasks (e.g., LogiQA, CommonsenseQA) or resource-constrained environments. However, our experiments also reveal limitations: attempts to teach style selection via supervised fine-tuning resulted in shallow memorization rather than genuine strategic understanding, highlighting the need for more sophisticated approaches to meta-reasoning. By establishing these scaling laws, task-style affinities, and the current boundaries of adaptive reasoning, StyleBench provides both valuable evaluation framework and strategic foundation for developing more efficient, robust, and self-aware reasoning systems in language models."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have made significant efforts to ensure reproducibility of our experiments. All datasets used are publicly available open-source benchmarks, and we provide full descriptions of the reasoning style definitions, data processing steps, and experimental protocols in the main manuscript and in the appendix. We also supply link to our benchmarks processed data and README file, hosted at: https://github.com/JamesJunyuGuo/Style_Bench/blob/master/README.md Implementation details, hyperparameters, and any scripts needed to reproduce our main results are referenced in the README file and supplementary materials."
        },
        {
            "title": "REFERENCES",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Simon Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Jingxuan Fan, Sarah Martinson, Erik Wang, Kaylie Hausknecht, Jonah Brenner, Danxian Liu, Nianli Peng, Corey Wang, and Michael Brenner. Hardmath: benchmark dataset for challenging problems in applied mathematics. arXiv preprint arXiv:2410.09988, 2024. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Thinkless: Llm learns when to think. arXiv preprint arXiv:2505.13379, 2025. Tairan Fu, Javier Conde, Gonzalo Martınez, Marıa Grandury, and Pedro Reviriego. Multiple choice questions: Reasoning makes large language models (llms) more self-confident even when they are wrong. arXiv preprint arXiv:2501.09775, 2025. Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, and Furu Wei. Meta reasoning for large language models. arXiv preprint arXiv:2406.11698, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 10 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Yichen Huang and Lin Yang. Gemini 2.5 pro capable of winning gold at imo 2025. arXiv preprint arXiv:2507.15855, 2025. Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. Yi Ren and Danica Sutherland. Learning dynamics of llm finetuning. arXiv preprint arXiv:2407.10490, 2024. Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. Algorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379, 2023. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Can chatgpt replace traditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family. In International Semantic Web Conference, pp. 348367. Springer, 2023. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, et al. Rema: Learning to meta-think for llms with multi-agent reinforcement learning. arXiv preprint arXiv:2503.09501, 2025. Jianxun Wang and Yixiang Chen. review on code generation with llms: Application and evaluation. In 2023 IEEE International Conference on Medical Artificial Intelligence (MedAI), pp. 284289. IEEE, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. 11 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. Reasonflux: Hierarchical llm reasoning via scaling thought templates. arXiv preprint arXiv:2502.06772, 2025b. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed Chi, and Denny Zhou. Large language models as analogical reasoners. arXiv preprint arXiv:2310.01714, 2023. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A THINKING STYLES",
            "content": "(a) Chain-of-Thought (CoT): Linear step-by-step reasoning (b) Chain-of-Draft (CoD): Iterative refinement process Figure 4: Sequential reasoning methodologies: CoT follows linear progression while CoD employs iterative refinement of drafts. 13 (a) Algorithm-of-Thought (AoT): Backtracking exploration (b) Tree-of-Thought (ToT): Branching and pruning Figure 5: Exploratory reasoning methodologies: ToT explores multiple paths with selective pruning, while AoT employs systematic backtracking to explore alternative approaches. 14 Figure 6: Sketch-of-Thought (SoT): Router-based paradigm selection with exemplar retrieval. The method classifies the input problem, retrieves relevant examples from paradigm cache, applies targeted prompts, and generates responses through structured LLM processing."
        },
        {
            "title": "B OVERALL ACCURACY SCORE",
            "content": "(a) Accuracy boxplot for small models (b) Accuracy Heatmap for medium models (c) Accuracy Heatmap for small models Figure 7: Boxplot for three groups of model accuracy. 16 Figure 8: Accuracy Heatmap for small models 17 Figure 9: Accuracy Heatmap for medium models 18 Figure 10: Accuracy Heatmap for large models"
        },
        {
            "title": "C SAMPLE PROMPTS BY REASONING STYLE",
            "content": "Here we provide the sample prompts for each thinking style and we choose the CommonsenseQA dataset for example."
        },
        {
            "title": "Sample CommonsenseQA Problem",
            "content": "Bob the lizard lives in warm place with lots of water. Where does he probably live? Options: A) rock B) tropical rainforest C) jazz club D) new mexico E) rocky places Chain of Thought (CoT) boxed < answer > { System: You are careful math solver. Think step by step and show all your work clearly. Explain your reasoning process, then provide your final answer. At the end, put your final numerical answer in User: Problem: Bob the lizard lives in warm place with lots of water. Where does he probably live? Options: A) rock B) tropical rainforest C) jazz club D) new mexico E) rocky places Lets work through this step by step, showing all calculations. Please solve this step by step, showing all your work and reasoning. Put your final numerical answer in at the very end. format. } boxed < think > { } Tree of Thoughts (ToT) boxed { System: Imagine three different experts are solving this math problem together. Each expert contributes their reasoning step by step. They discuss, check each others work, and collaborate. Show this collaborative problem-solving process with clear reasoning. At the end, put the final agreedupon answer in User: Problem: Bob the lizard lives in warm place with lots of water. Where does he probably live? Options: A) rock B) tropical rainforest C) jazz club D) new mexico E) rocky places Show how the three experts would collaborate on this problem, with each contributing their reasoning and checking each others work. Please solve this step by step, showing all your work and reasoning. Put your final numerical answer in < think > } at the very end. format. boxed < think > { } Algorithm of Thoughts (AoT) System: Use two-phase reasoning to solve this problem: Phase 1 - Forward Analysis: Think step by step through the problem from start to finish. Phase 2 - Verification: Work backward from your answer to double-check its correct. Show both phases of your reasoning clearly. At the end, put your final answer in User: Problem: Bob the lizard lives in warm place with lots of water. Where does he probably live? Options: A) rock B) tropical rainforest C) jazz club D) new mexico E) rocky places Use both forward analysis and backward verification. Show both phases of your reasoning clearly. Please solve this step by step, showing all your work and reasoning. Put your final numerical answer in < think > { at the very end. boxed format. } boxed < think > { } 20 Chain of Draft (CoD) } format. < think > < /think > 36h = 1.5 days; today: 2015/01/01 - 1.5 = 12/30/2014; 12/30/2014 + 7 = Refine workflow to solve problems step by step. Show your draft solution, < System: Use Draft then refine and improve it with detailed reasoning. At the end, put your final answer in think > User: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? Assistant: { 01/06/2015. ... [22 more similar examples with ... User: Bob the lizard lives in warm place with lots of water. Where does he probably live? Options: A) rock B) tropical rainforest C) jazz club D) new mexico E) rocky places Please solve this step by step, showing all your work and reasoning. Put your final numerical answer in boxed { < think > } format responses] at the very end. 01/06/2015 boxed } { } { } { boxed < think > { } Symbolic of Thoughts (SoT) System: Role & Objective You are reasoning expert specializing in Chunked Symbolism, cognitive reasoning technique that organizes numerical reasoning into structured steps. Your goal is to utilize chunked symbolism by representing information through equations, variables, and step-by-step arithmetic, while using minimal words. Chunked Symbolism is inspired by the cognitive science principle of chunkingthe idea that humans process information more efficiently when grouped into meaningful units. Instead of solving problems in free-form manner, Chunked Symbolism breaks down complex operations into smaller, structured steps. This method is particularly effective for: - Mathematical problems (arithmetic, algebra, physics, engineering) - Symbolic reasoning (logic-based computations, formula derivations) - Technical calculations (financial modeling, physics simulations, unit conversions) How to Apply Chunked Symbolism Step-by-Step Guide 1. Identify Variables Extract relevant numerical values and define variables. 2. Write Equations Represent the solution using explicit mathematical formulas. 3. Perform Step-by-Step Computations Solve in small, logical steps, keeping each line clear. 4. Label Units Maintain consistent unit representation to prevent ambiguity. 5. Final Answer Formatting Present the answer in the provided format for clarity. Rules & Directives 1. Use Equations & Variables - Define variables before computation. - Always use explicit equations to represent reasoning. 2. Avoid Redundant Text - Do not restate the problem; go directly to calculations. - Use minimal context only if it aids understanding. 3. Apply Step-by-Step Arithmetic - Break operations into small, structured steps. 21 { } [Final answer] - Ensure each line contains only one computation for clarity. 4. Output Format - Use the exact structured format: < think > [shorthand reasoning] < /think > - The final answer must boxed be boxed. - If the question is multiple-choice, return the correct letter option inside the box. - Use minimal words in your response. User: car accelerates at 2.5 m/s2 for 10 seconds. If its initial velocity was 15 m/s, what is its final velocity? Assistant: < think > = 2.5 m/s2 = 10 vi = 15 m/s vf = 15 + (2.5 10) vf = 40 m/s < /think > boxed { User: If product costs $120 and there is 15% discount, what is the final price? Choices: A) $10 B) $97 C) 102 Assistant:< think > op = 120 = 15% dp = 120 (15 / 100) = 18 fp = 120 - 18 = 102 < /think > boxed User: Question: circuit has voltage of 12V and resistance of 4Ω. What is the current? Assistant: < think > = 12V = 4Ω = 12 / 4 = 3A < /think > User: Bob the lizard lives in warm place with lots of water. Where does he probably live? Options: A) rock B) tropical rainforest C) jazz club D) new mexico E) rocky places Please solve this step by step, showing all your work and reasoning. Put your final numerical answer in at the very end. boxed < think > 40 } boxed } 3 { } { { }"
        },
        {
            "title": "D SAMPLE RESPONSE",
            "content": "D."
        },
        {
            "title": "INCORRECT RESPONSE EXAMPLES",
            "content": "This is response produced by Qwen2-3B model. LLM Response Question: The pages of book are numbered 1 through n. When the page numbers of the book were added, one of the page numbers was mistakenly added twice, resulting in an incorrect sum of 1986. What was the number of the page that was added twice? Ground Truth: 33 LLM Response: Heres how to solve the problem step-by-step: 1. Formula for the sum of an arithmetic series: The sum of the first natural numbers is given by the formula: = n(n + 1) 2 where is the sum of the series. 2. Setting up the equation: Let be the total number of pages in the book. The sum of the page numbers, without the duplicate, would be n(n+1) Since one page number was added twice, the incorrect sum is: n(n+1) . 2 2 + where is the page number that was added twice. We know this incorrect sum is 1986: n(n + 1) + = 1986 (1) (2) 3. Finding pattern: We need to find value for and that satisfies this equation. Notice that 1986 is close to perfect square. Lets try some values of and see if we can find solution. 4. Trial and Error: If = 60, then n(n+1) This means = 1986 2 = 60(61) 2 = 1830. 5. Verification: Lets check if our answer works: 1830 = 156. 23 Is 156 page number in the book? Yes, it could be. Therefore, the page number that was added twice is 156. 156 This is response produced by Llama3-8B model on Game24 task. LLM Response: Game of 24 Puzzle: Use the numbers 3, 3, 12, 12 exactly once each with operations +, to make 24. Ground Truth: Multiple valid solutions exist (e.g., (12 LLM Response: Heres how we can solve this puzzle: 1. Look for obvious pairs: (3 + 3) = 4 3) , , and parentheses 6 = 24) Notice that 12 Lets try 3 12 = 144, which is too big. 12 = 36, which is also too big. 2. Consider division: Since we need to get to 24, lets see if we can divide one of the larger numbers by the smaller ones: 3 = 4. 3. Build from there: Now we have 4, 3, 3, and 12. We can use the 4 and 3 to get 12: 4 4. Combine the results: We have 12 + 12 = 24. Therefore, the solution is: 12 + 12 3 = 12. D.2 INCORRECT FORMAT RESPONSE EXAMPLE LLM Response Problem: Let be the smallest integer whose cube root is of the form + r, where is positive integer and is positive real number less than 1 1000 . Find n. Ground Truth: = 19 LLM Response (Chain of Thought): Lets start by writing the equation: m1/3 = + where is positive integer and is positive real number less than 1 1000 . 24 Since is less than 1 1000 , we can write: 0 < < 1000 We can also write: + = m1/3 Subtracting from both sides gives: = m1/3 Since is positive real number less than 1 1000 , we can see that: 0 < m1/3 < 1 1000 Adding to both sides gives: < m1/3 < + 1 1000 Since is positive integer, we can see that: < m1/3 < + 1 This means that m1/3 is an integer that is one more than multiple of n. The smallest such integer is + 1, so: m1/3 = + Cubing both sides gives: = (n + 1)3 Expanding the right-hand side gives: = n3 + 3n2 + 3n + 1 We want to find the smallest integer that satisfies this equation. We can start by trying small values of n: If = 1, then = 13 + 3(1)2 + 3(1) + 1 = 8 If = 2, then = 23 + 3(2)2 + 3(2) + 1 = 27 If = 3, then = 33 + 3(3)2 + 3(3) + 1 = 56 If = 4, then = 43 + 3(4)2 + 3(4) + 1 = We can see that = 27 is the smallest integer that satisfies the equation. Therefore, = 2. Final Answer: The final answer is 2. hope it is correct. 25 D.3 COMPARISON BETWEEN DIFFERENT STYLES LLM Responses: Game of 24 - Multiple Reasoning Styles Problem: Use the numbers 2, 3, 13, 13 exactly once each with operations +, to make 24. Ground Truth: Valid solution exists: ( 2 = 24 13) 3 , , and parentheses Chain of Thought (CoT) Response: Lets solve the puzzle step by step using the numbers 2, 3, 13, and 13. [The response shows 8 different attempts, all yielding incorrect results, then concludes:] After trying various combinations, it seems that none of the expressions we have formed using the numbers 2, 3, 13, and 13 with the allowed operations and parentheses result in 24. Conclusion: Since we have exhausted all reasonable combinations and none of them yield 24, we conclude that there is no solution for this set of numbers. No solution Tree of Thoughts (ToT) Response: [Shows collaborative discussion between three experts, trying various combinations like (13 + 13) 2 Final Answer: After trying multiple combinations, it seems that we cannot form an expression that evaluates to 24 using the numbers 2, 3, 13, and 13 with the allowed operations. No solution 3) = 45.5, (13 + 13) 3) = 20, etc.] 3 = 49, 13 (13/ (2 Analysis of Thoughts (AoT) Response [CORRECT]: Phase 1 - Forward Analysis: We are given the numbers 2, 3, 13, and 13. Lets consider different combinations: 1. Multiplication and Addition: 2 = 26 3 = 23 (not 24) 13 = 13 (not 24) 13 26 26 13 39 26 2. Multiplication and Subtraction: 3 = 39 13 = 26 2 = 24 (this looks promising) Phase 2 - Verification: Lets verify the expression (13 13) 2: 3 = 39 13 13 = 26 39 2 = 24 26 ( 3 13) 2 Chain of Draft (CoD) Response: [Shows 8 draft attempts including (13+13 16, etc.] 2 3) = 20, (13 3) (2+13) = 150, (13+13) 2+3 = 26 After trying multiple combinations, it seems that it is not possible to form an expression using the numbers 2, 3, 13, 13 that evaluates to 24. No solution 0 = 0 (Not 24) 13) = 5 13) = 6 + 0 = 6 (Not 24) Structured of Thoughts (SoT) Response: = 2, = 3, = 13, = 13 Step 1: Consider possible combinations of operations. Step 2: Test (a + b) Step 3: Test (a Step 4: Test (a Step 5: Test (a + b) Step 6: Test (a + b) Step 7: Test (a Step 8: Test (a Step 9: Test (a Step 10: Test (a [... continues with systematic enumeration of 30+ more combinations ...] Step 40: Test (a Result: Response was truncated due to length limits. No final answer was provided in the original response. d) (2 + 3) (13 d) (2 3) + (13 3) + 13 (2 13 (2 + 3) (13 d) (2 + 3) 13) + (3 d) (2 13) d) (2 (3 13) + (3 c) (2 (3 13) c) (2 13) = 5 13) = 26 + 39 = 65 (Not 24) 13) = 26 39 = 13) = 26 + 39 = 65 (Not 24) 39 = 13) = 26 (c b) + (c b) + (c c) + (b c) (b d) + (b (b d) 13 = 6 + 13 13 13 = 5 1 = 5 (Not 24) 13 = 6 (Not 24) 13 = 5 (Not 24) 38.8462 (Not 24) 13) = 0.1538 39 = 13 (Not 24) 13 (Not 24) d) (2 13) (3 c) (b D.4 REASONING STYLE COMPARISON: CONCISENESS LLM Responses: Commonsense QA - Reasoning Style Comparison Problem: Reading newspaper one of many ways to practice your what? Options: A) literacy B) knowing how to read C) money D) buying Ground Truth: A) literacy E) money bank Chain of Thought (CoT) Response: [Length: 1,192 characters] This problem is not numerical or mathematical problem but rather question about understanding the context and meaning of the options provided. Lets analyze the question and the options step by step: 1. The question asks, Reading newspaper one of many ways to practice your what? 2. We need to determine what reading newspaper helps to practice. Now, lets analyze each option: A) literacy: Literacy refers to the ability to read and write. Reading newspaper is indeed way to practice and improve ones literacy skills. B) knowing how to read: This is similar to literacy but more specific to the act of reading. While reading newspaper does involve reading, the term literacy encompasses broader set of skills including reading comprehension and writing. C) money: Reading newspaper does not directly relate to practicing money management or understanding. D) buying: Reading newspaper does not directly relate to practicing the act of buying. E) money bank: This option is unrelated to reading newspaper. Given the analysis, the most appropriate answer is A) literacy, as reading newspaper is way to practice and improve ones literacy skills. 27 Tree of Thoughts (ToT) Response: [Length: 1,912 characters] Expert 1: Lets start by analyzing the question. The question asks, Reading newspaper one of many ways to practice your what? We need to identify what reading newspaper helps to practice. Reading newspaper involves understanding written text, which is key component of literacy. Expert 2: agree with Expert 1. Lets look at the options provided: [analyzes all options] Option A, literacy, directly aligns with the idea of reading and understanding written text. Expert 3: concur with both of you. Lets eliminate the options that are clearly unrelated: [eliminates options C, D, E] This leaves us with options and B. While knowing how to read (Option B) is part of literacy, the broader term that encompasses both reading and writing is literacy (Option A). Analysis of Thoughts (AoT) Response: [Length: 1,779 characters] Phase 1 - Forward Analysis: 1. Understanding the Problem: The problem asks about what reading newspaper helps to practice. 2. Analyzing Each Option: [Systematically analyzes options through E] 3. Selecting the Best Option: Both and are valid, but literacy (option A) is broader term that encompasses the ability to read and write, which is what reading newspaper helps to practice. Phase 2 - Verification: [Works backward from selected option to verify correctness] Chain of Draft (CoD) Response: [Length: 1,005 characters - 16% shorter] To solve this problem, we need to determine which option best fits the context of the question: Reading newspaper one of many ways to practice your what? Lets analyze each option: A) literacy: Literacy refers to the ability to read and write. Reading newspaper is indeed way to practice and improve ones literacy skills. B) knowing how to read: This option is somewhat similar to literacy, but it is more specific to the act of reading. While reading newspaper does involve knowing how to read, the term literacy encompasses broader set of skills, including reading comprehension and writing. C) money: Reading newspaper has no direct relation to practicing money management or understanding. D) buying: Reading newspaper does not involve practicing the act of buying. E) money bank: This option is unrelated to reading newspaper. Given the context, the most appropriate answer is A) literacy, as it encompasses the skills that are practiced when reading newspaper. Structured of Thoughts (SoT) Response: [Length: 69 characters - 94% shorter] <think> #reading newspaper #improves #literacy </think> 28 D.5 TOKEN USAGE Figure 11: Token Usage on GSM8k Figure 12: Token Usage on CommonsenseQA 29 Figure 13: Token Usage on LogiQA 30 CROSS-MODEL COMPARISON E.1 CASE STUDY 1: GAME24 Table 2: Final responses from different models and reasoning methods for Game24 problem 2, 2, 7, 10 Model CoT ToT AoT CoD SoT No solution No solution No solution No answer Qwen 3B No solution No answer No answer No answer No answer Qwen 7B No answer Invalid expr. No answer No answer Qwen 32B Invalid expr. No answer Qwen 72B Valid sol. No answer GPT-OSS 120B No answer Valid sol. No answer No answer No solution Valid sol. No answer No answer Table 3: Success rate analysis across models and methods Model Valid Solutions Correct No Solution Invalid Solutions No Answer Success Rate Qwen 3B Qwen 7B Qwen 32B Qwen 72B GPT-OSS 120B Total 0 0 0 2 1 3 4 0 0 1 5 0 0 1 1 0 2 1 5 4 1 4 15 80% 0% 0% 60% 20% 32% Table 4: Method performance across all models Method Valid Solutions Correct No Solution Invalid Solutions No Answer Success Rate CoT ToT AoT CoD SoT 2 0 2 0 1 2 2 2 0 1 1 0 0 1 0 0 3 1 4 3 80% 40% 80% 0% 40% Table 5: Valid solutions discovered Model Method Expression and Verification CoT Qwen 72B Qwen 72B AoT GPT-OSS 120B SoT (10 2 + 7) 2 = (5 + 7) 2 = 12 2 = 24 (10 2 + 7) 2 = (5 + 7) 2 = 12 2 = 24 (7 + 10/2) 2 = (7 + 5) 2 = 12 2 = 24 Table 6: Detailed final expressions from each model-method combination Method Final Expression CoT ToT AoT CoD SoT CoT ToT AoT CoD SoT CoT ToT AoT CoD SoT CoT ToT AoT CoD SoT CoT ToT AoT CoD SoT No solution No solution No solution No final boxed answer (infinite loop) No solution No final boxed answer (infinite loop) No final boxed answer (infinite loop) No final boxed answer (infinite loop) No final boxed answer (infinite loop) No final boxed answer (infinite loop) (10 + 2) 2 + 7 7 (uses 7 twice) No final boxed answer (infinite loop) No final boxed answer (infinite loop) No final boxed answer (infinite loop) No final boxed answer (infinite loop) (10 2 + 7) 2 (Valid solution) No solution (10 2 + 7) 2 (Valid solution) (2 + 2) (10 7) + 10 + 2 (number reuse) No final boxed answer (infinite loop) No final boxed answer (stream of consciousness) No final boxed answer (chaotic exploration) No final boxed answer (endless calculations) No final boxed answer (partial insight) (7 + 10/2) 2 (Valid solution) Model Qwen 3B Qwen 7B Qwen 32B Qwen 72B GPT-OSS 120B E.2 CASE STUDY 2: AIME Let = 231319. How many positive integer divisors of n2 are less than but do not divide n? Ground Truth: 589 Legend: Green = Correct (589), Orange = Near-correct (588), Red = Incorrect Model Qwen3B CoT ToT AoT CoD SoT 588 588 1816 589 32 Model Qwen7B Qwen32B Qwen72B GPT-OSS-120B Ground Truth CoT ToT AoT CoD SoT 909 588 589 588 588 588 589 908 589 589 908 589 589 588 588 589 589 589 588"
        },
        {
            "title": "F EXPERIMENT SETTINGS",
            "content": "To ensure fair and reproducible comparisons across all models, we maintained consistent prompting configurations. We set the temperature to 0 for deterministic outputs and standardized both the maximum new tokens and model context length for each dataset. Table 8: Prompting Configuration for All Models Dataset AIME CommonsenseQA Game24 GSM8K LogiQA"
        },
        {
            "title": "Max New Tokens Context Length",
            "content": "2048 512 2048 512 512 8192 8192 8192 8192 8192 Task Characteristics Complex mathematical reasoning Multiple-choice questions Mathematical puzzle solving Grade school math problems Logical reasoning tasks SFT EXPERIMENTAL SETUP (DATASET, TRAINING, AND EVALUATION) For each problem in our dataset, we generate responses using five distinct reasoning style prompts applied to capable base model. Each problem receives five different reasoning approaches while maintaining the same correct answer. G.1 SFT DATASET CONSTRUCTION PROCEDURE Style Selection Training Data Our SFT approach trains models to automatically select the most appropriate reasoning style for each problem. The dataset consists of problems paired with optimal style choices determined through empirical evaluation. G.2 TRAINING DATA FORMAT Each training example follows conversational format with system instructions, user queries, and target style selections: { \"messages\": [ { \"role\": \"system\", \"content\": \"Your task is to choose the most appropriate reasoning style for answering the users question. You must choose from: - CoT (Chain of Thought) - CoD (Chain of Draft) 33 - ToT (Tree of Thought) - SoT (Sketch of Thought) - AoT (Algorithm of Thought) The selection should follow two criteria: 1. The style must lead to the correct answer. 2. Among all styles that produce correct answers, choose the one with the most concise response.\" }, { \"role\": \"user\", \"content\": \"[Problem statement with multiple choice options]\" }, { \"role\": \"assistant\", \"content\": \"[Selected Style: CoT/CoD/ToT/SoT/AoT]\" } ] } G.3 STYLE SELECTION CRITERIA The training data is constructed using two-stage optimization process: Stage 1: Correctness Filtering For each problem, we evaluate all five reasoning styles and identify which ones produce the correct answer. Stage 2: Conciseness Selection Among the correct styles, we select the one with the most concise response based on: Token count Reasoning steps Computational complexity This is the proportion of the predicted label in the training dataset. G.4 TRAINING CONFIGURATION Model Architecture: We fine-tune base models from each scale category using the conversational format. Training Parameters: Learning rate: 2 Batch size: 16 105 Training epochs: 3 Gradient clipping: 1. Loss function: Cross-entropy loss on style classification Data Distribution: The training set maintains balanced representation across: 34 Problem types (math, reasoning, coding, puzzles) Difficulty levels Optimal style assignments G.5 EVALUATION PROTOCOL Style Selection Accuracy: Measured as the percentage of problems where the model selects the empirically optimal style. Downstream Performance: Evaluate whether automatic style selection maintains accuracy compared to human-selected styles. This methodology enables models to automatically adapt their reasoning approach based on problem characteristics, potentially improving efficiency while maintaining accuracy across diverse tasks. G.6 TRAINING DATA DISTRIBUTION AND MODEL BEHAVIOR Ground Truth Style Distribution Our training dataset of 3,000 problems exhibits an uneven distribution of optimal reasoning styles, as determined through empirical evaluation (Figure 14). At inference time, the fine-tuned model was prompted to first select the best reasoning style for new question before applying that style to generate solution. However, as shown in Figure 14, the SFT process failed to instill genuine strategic understanding. Instead, the model developed strong bias towards consistently selecting Chainof-Draft (CoD), regardless of the actual problem context. This pathological selection strategy effectively nullified any potential advantage over simply using single, fixed style across all tasks. 58.28% 80.27% 4.49% 12.59% 24.64% 1.40% 4.20% 14.13% Styles CoD CoT SoT ToT Other (a) Distribution of optimal reasoning styles in training data. (b) Model predictions after SFT training. Figure 14: Comparison of reasoning style distributions before and after training. CoD dominates both distributions, with increased prevalence after training. The dominance of Chain-of-Draft (CoD) at 58.28% suggests that for most problems in our benchmark suite, concise drafting approach provides the optimal balance between correctness and efficiency. Treeof-Thought (ToT) represents only 4.49% of optimal solutions, indicating that multi-perspective reasoning is beneficial for smaller subset of complex problems. Post-Training Model Predictions After SFT, the models prediction behavior shifts notably (Figure 14b), showing even stronger preference for CoD while developing capability to select ToT for appropriate problems. Reasoning Style Count Percentage CoD (Chain of Draft) ToT (Tree of Thought) SoT (Sketch of Thought) Other Total 242 44 11 300 80.67% 14.67% 3.67% 1.00% 100% Table 9: Distribution of reasoning styles predicted by the fine-tuned model on 300 sampled questions. The model demonstrates learned preference for CoD (80.67%) and increased selection of ToT (14.67%) compared to training distribution, suggesting the model has learned to identify problems where multiperspective reasoning provides value. G.7 TRAINING DYNAMICS We compare two fine-tuning approaches: Low-Rank Adaptation (LoRA) and full parameter fine-tuning on Qwen-7B. Figure 15: Training dynamics for Qwen-7B with LoRA fine-tuning. Left panel shows training loss convergence over steps. Right panel shows gradient norm evolution, indicating stable optimization throughout training. 36 Figure 16: Training dynamics for Qwen-7B with full parameter fine-tuning. Left panel shows training loss convergence over steps. Right panel shows gradient norm evolution, demonstrating higher gradient magnitudes compared to LoRA fine-tuning. Figures 15 and 16 demonstrate that both LoRA and full fine-tuning achieve stable convergence. LoRA finetuning exhibits more stable gradient norms, while full fine-tuning shows higher gradient magnitudes but maintains convergence, suggesting both approaches are viable for style selection training."
        }
    ],
    "affiliations": [
        "University of California, Berkeley",
        "Virginia Tech"
    ]
}