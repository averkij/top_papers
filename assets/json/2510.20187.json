{
    "paper_title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values",
    "authors": [
        "Dian Yu",
        "Yulai Zhao",
        "Kishan Panaganti",
        "Linfeng Song",
        "Haitao Mi",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Large Language Model (LLM) optimization directly with quantifiable human value signals. While Reinforcement Learning with Verifiable Rewards (RLVR) effectively trains models in objective domains using binary correctness rewards, it overlooks that not all tasks are equally significant. RLEV extends this framework by incorporating human-defined value signals directly into the reward function. Using exam-style data with explicit ground-truth value labels, RLEV consistently outperforms correctness-only baselines across multiple RL algorithms and model scales. Crucially, RLEV policies not only improve value-weighted accuracy but also learn a value-sensitive termination policy: concise for low-value prompts, thorough for high-value ones. We demonstrate this behavior stems from value-weighted gradient amplification on end-of-sequence tokens. Ablation studies confirm the gain is causally linked to value alignment. RLEV remains robust under noisy value signals, such as difficulty-based labels, demonstrating that optimizing for an explicit utility function offers a practical path to aligning LLMs with human priorities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 7 8 1 0 2 . 0 1 5 2 : r Reinforcement Learning with Explicit Human Values Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values Dian Yu1 , Yulai Zhao1,2 , Kishan Panaganti1 , Linfeng Song1 , Haitao Mi1 , and Dong Yu1 1Tencent AI Lab 2Princeton University"
        },
        {
            "title": "Abstract",
            "content": "We propose Reinforcement Learning with Explicit Human Values (RLEV), method that aligns Large Language Model (LLM) optimization directly with quantifiable human value signals. While Reinforcement Learning with Verifiable Rewards (RLVR) effectively trains models in objective domains using binary correctness rewards, it overlooks that not all tasks are equally significant. RLEV extends this framework by incorporating human-defined value signals directly into the reward function. Using exam-style data with explicit ground-truth value labels, RLEV consistently outperforms correctness-only baselines across multiple RL algorithms and model scales. Crucially, RLEV policies not only improve value-weighted accuracy but also learn value-sensitive termination policy: concise for low-value prompts, thorough for high-value ones. We demonstrate this behavior stems from value-weighted gradient amplification on end-of-sequence tokens. Ablation studies confirm the gain is causally linked to value alignment. RLEV remains robust under noisy value signals, such as difficulty-based labels, demonstrating that optimizing for an explicit utility function offers practical path to aligning LLMs with human priorities. Figure 1: RLEV overview. The verifier can be either reward model or rule-based function."
        },
        {
            "title": "Introduction",
            "content": "Aligning Large Language Models (LLMs) with human goals can follow two paradigms: implicit value learning, which infers human utility from feedback, and explicit value learning, which optimizes directly for defined utility signals. The dominant paradigm, Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al. (2020); Ouyang et al. (2022); Rafailov et al. (2023)), learns an implicit utility model from subjective pairwise preferences. While effective for non-verifiable tasks, this is often unnecessary for objective domains. For these, Reinforcement Learning with 1 Reinforcement Learning with Explicit Human Values Verifiable Rewards (RLVR) (Lambert et al. (2024); Guo et al. (2025); Su et al. (2025)) offers simpler, more direct approach, using binary reward for correctness. However, this method carries critical oversight: by assigning uniform reward (e.g., +1) to all correct answers, it treats all prompts as equally important, failing to capture the explicit and non-uniform value common in real-world scenarios. For instance, on an exam, correctly answering 10-point question is demonstrably more valuable than answering 2-point one. An LLM trained to maximize only the count of correct answers is not optimized for the total score, which is the true human objective. To bridge this gap, we introduce Reinforcement Learning with Explicit Human Values (RLEV), method that extends the RLVR framework by integrating explicit, human-assigned values into the reward function. RLEV operationalizes simple principle: the utility of response depends jointly on its correctness and the intrinsic value of its prompt. Using 100k exam-style training examples with ground-truth value labels, we show that RLEV consistently outperforms the standard RLVR baseline across multiple RL algorithms (REINFORCE++ (Hu, 2025), RLOO (Ahmadian et al., 2024), and GRPO (Shao et al., 2024)) and model scales (7B and 32B) (Team, 2024). Notably, RLEV-trained policies learn value-sensitive termination policy, generating highly concise responses for low-value prompts while remaining thorough on high-value ones. Our gradient analysis reveals this behavior stems from the value-scaled reward amplifying updates on end-of-sequence tokens, encouraging the model to terminate efficiently based on the prompts importance. Crucially, we demonstrate through ablation studies that this performance gain is causally linked to alignment with human-defined values. Baselines using randomly shuffled or uniformly scaled rewards show no significant improvement over correctness-only training. Finally, we show RLEV is robust even with noisy value signals, such as pseudo-labels from score predictor or weak labels based on question difficulty, which still outperform the baseline. These findings establish that directly optimizing for an explicit utility function is potent and effective method for aligning LLM behavior with stated human priorities. Our contributions are as follows: We propose RLEV, novel training paradigm that aligns LLMs with explicit human priorities by scaling correctness rewards with quantifiable value signals. We demonstrate empirically that RLEV consistently outperforms strong correctness-only baselines across multiple RL algorithms and model scales, leading to higher value-weighted scores and desirable property of generating more concise responses.1 Through gradient analysis and ablation studies, we provide strong evidence that these gains are causally linked to value alignment, not merely to changes in reward magnitude. We show that RLEV is robust and practical, achieving superior performance even when using noisy or approximate value signals, such as difficulty-based weak labels."
        },
        {
            "title": "2 Method: Learning from Human-Aligned Rewards",
            "content": "To align Large Language Model with human priorities, we first define utility function that captures the desired behavior. We then operationalize this function as scalar reward for reinforcement learning. 2.1 Human Utility Function for Valued and Verifiable Outcomes We begin from the principle that the value of models response depends on both its correctness and the importance of the prompt. We can formalize this by defining human utility function, U(x, y), for response to prompt x: U(x, y) = v(x) 1correct(y) (1) where: 1The RLEV dataset is available at https://huggingface.co/datasets/sarosavo/RLEV. 2 Reinforcement Learning with Explicit Human Values v(x) represents the intrinsic human-defined value or importance of the prompt x. 1correct(y) is an indicator function that is 1 if the response is verifiably correct and 0 otherwise. This utility function captures the simple, powerful idea that correct response is worth the value of the question, while an incorrect response has zero utility. The goal of our alignment process is to train policy π that maximizes the expected utility, yπ(yx)[U(x, y)]. While we instantiate this principle in exam-like settings, the same formulation applies to any domain where outcome correctness and human-assigned importance jointly determine utility, such as medical triage, tutoring, or content moderation. This product-based utility function U(x, y) is straightforward formalization of human priorities in domains where outcome correctness is verifiable and input importance is non-uniform (e.g., exams, medical triage). 2.2 Normalizing Human Values To obtain practical signal for v(x), we use ground-truth scores from human-designed tasks, such as exams. Since different exams have different total scores, we must normalize these values to create consistent scale. Let: sij be the raw score of question in exam i. Ti be the total score of exam i. We define the normalized value v(x) for given question (i.e., question in exam i) as its proportion of the exam total: v(x) = sij Ti (2) This proportional scaling naturally bounds v(x) between 0 and 1 and makes it interpretable as the relative importance of the question. 2.3 The RLEV Reward Function While U(x, y) defines our target objective, its direct use as reward can lead to unstable training. low-value but correct question could receive near-zero reward, discouraging the model from learning to answer it. To ensure stable and effective learning signal, we design practical surrogate reward function, r(x, y), that preserves the relative importance of prompts while guaranteeing minimum reward of 1 for any correct response. We achieve this by defining scaling factor s(x) based on the normalized human value v(x) that is always greater than or equal to 1: where s(x) is scaling factor based on the normalized human value v(x): r(x, y) = s(x) 1correct(y) s(x) = 1 + min(α v(x), 1) (3) (4) Here, α is scaling hyperparameter. The resulting reward r(x, y) is within the range [1, 2] for correct responses and is 0 for incorrect ones. This formulation incentivizes correctness on all questions while providing stronger bonus for correctly answering high-value ones. This additive and clipped form is chosen specifically to ensure stable learning signal by providing minimum reward for all correct answers while preventing excessively large rewards from destabilizing the training process, design choice validated in our ablation studies (Section 3.8). 3 Reinforcement Learning with Explicit Human Values 2.4 The Reinforcement Learning Objective We aim to find the optimal policy πθ that maximizes the expected cumulative reward J(θ) over dataset of prompts D, standard in REINFORCE-style RL (Williams, 1992): J(θ) = xD,yπθ (x) (cid:35) r(x, y<t, yt) , (cid:34)T1 t=0 where r(x, y<t, yt) denotes the per-step reward. In our setting, the reward is sparse and non-zero only at the final step T, thus simplifying the objective to: J(θ) = xD,yπθ (x)[r(x, y)]. The corresponding gradient is: J(θ) = xD,yπθ (x) (cid:104) r(x, y) log πθ(yx) (cid:105) . Given that the policy is autoregressive, log πθ(yx) = T1 t=0 log πθ(ytx, y<t). 2.5 Gradient Derivation To analyze the learning dynamics, we derive the policy gradients for single prompt (noting the full gradient J(θ) is the expectation over D) with respect to the parameters at single time step t. At step t, we use zk to refer to the logit at token where denotes the whole vocabulary. Note that also includes EOS symbol, denoted as e. For any token V, we use the following to represent the conditional probability that the final is correct given yt = v, where the probability is taken over the remaining rollout under the current policy pv = Pr(correct x, y<t, yt = v). Logits at step are converted to probabilities using the softmax function; then we have zk log π(yt x, y<t) = zk log (cid:1) exp(cid:0)zyt vV exp(zv) = 1{yt = k} π(k x, y<t). zk (cid:104) ytπ(x,y<t) r(x, y) (cid:0)1{yt = k} π(k x, y<t)(cid:1)(cid:105) = = E(cid:2)r(x, y) 1{yt = k}(cid:3) π(k x, y<t) E[r(x, y)] = π(k x, y<t)(cid:0)E[r(x, y) yt = k] E[r(x, y)](cid:1) where we employ the law of total expectation in the last line. Since r(x, y) = s(x) 1correct(y), as s(x) is constant for given x, we have Therefore, E[r(x, y) yt = k] = s(x) pk E[r(x, y)] = s(x) vV π(v x, y<t)pv 4 (5) (6) (7) (8) (9) Reinforcement Learning with Explicit Human Values zk = π(k x, y<t)(cid:0)E[r yt = k] E[r](cid:1) = π(k x, y<t)(cid:0)s(x) pk s(x) vV π(v x, y<t)pv (cid:1) = π(k x, y<t)s(x) (cid:0)pk vV π(v x, y<t)pv (cid:1) (10) Consider the special EOS token e, which is also in V. Below, we investigate the training dynamics of this special token. First we define pe as the averaged probability of final correctness over all non-EOS tokens: pe := 1 1 πe v=e,vV πv pv. Then the advantage for choosing EOS is E[r yt = e] E[r] = s(x)(1 πe)(cid:0)pe pe (cid:1). Thus the gradient with respect to the EOS logit is: ze = s(x) πe(1 πe)(cid:0)pe pe (cid:1). (11) (12) (13) The resulting gradient for the EOS logit (Equation 13) is driven by the difference between the expected correctness of terminating the sequence (pe) and the average expected correctness of continuing (pe), scaled by the human value factor s. EOS receives positive gradient if its correctness probability exceeds the average continuation correctness, i.e. pe > pe. continuation token receives negative gradient if pc < πv pv (the policy-weighted average). Thus, when pe > pe, most continuations are pushed down, though any with pc above the global average can still receive positive update. The human-aligned scale [1, 2] multiplies the gradient magnitude without changing these conditions. Therefore, when pe > pe (i.e. there is already sufficient information for correctness), EOS is reinforced more strongly, which accelerates the tendency to end earlier. In summary, compared to purely binary correctness reward, this scheme encourages the policy to generate more concise, more accurate completions. Moreover, because the reward is scaled by the human-defined scoring function rather than correctness alone, the resulting policy is expected to achieve higher human-defined scores in real-world use, which is supported by our experimental results in Section 3. Multiplying by the human-value factor amplifies the gradients magnitude, which more strongly reinforces the decision to terminate when correctness is already likely."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Datasets The dataset comprises question-answering pairs from multi-subject exams, with the original content predominantly in Chinese. The reference answers are written by domain experts for objective human evaluation, making them suitable for RLVR. Additionally, we extract each questions human-labeled score and the total score of the exam it originates from. Subsequently, we partition the data into 5 Reinforcement Learning with Explicit Human Values training and testing sets containing 100,000 and 8,000 examples, respectively. We split by exam to avoid leakage. To assess the generalization ability of the RLEV policies trained on Chinese data, we evaluate the out-of-distribution performance on several English and Chinese general-domain benchmarks (GPQA Diamond (Rein et al., 2024), C-Eval (Huang et al., 2023), MMLU-Pro (Wang et al., 2024), and SuperGPQA (Du et al., 2025)). As ground-truth human-defined values may be unavailable in many scenarios, in Section 3.5, we investigate the effectiveness of RLEV with two types of noisy human values. We conduct experiments using WebInstruct-verified (Ma et al., 2025), general domain English dataset with objective answers. We map each of the five difficulty category (PRIMARY SCHOOL, JUNIOR HIGH SCHOOL, SENIOR HIGH SCHOOL, UNIVERSITY, and PHD) into value scores (1, 2, 4, 6, 8), respectively. We divide the score by 100 for normalization. For each category, we randomly sample 2,000 training examples and train with the resulting 10k instances. To make this resource more accessible to the broader research community, we used GPT-4o to translate the data (with human-labeled values) into English, which will also be released. 3.2 Experimental Setup We kept the training setup consistent for all estimators. All policies were trained for one epoch on eight GPUs with learning rate of 5e-7. The rollout batch size was set to 128. The maximum length for both prompts and generated responses was capped at 1024 tokens. For evaluation, we use greedy decoding. We use base models (Qwen2.5-7B and Qwen2.5-32B (Team, 2024)) for policy initialization. 3.3 Evaluation Metrics To evaluate our method, we use set of metrics designed to capture both correctness and alignment with human-defined values: Accuracy (Acc): The standard, unweighted accuracy calculated as the percentage of total correct responses. This metric measures correctness without considering the value of each prompt. Human-Aligned Accuracy (H-Acc): The value-weighted accuracy, calculated as the ratio of achieved value from correct responses to the total possible value: H-Acc = correct responses v(x) all responses v(x) Response Length (Resp. Length): The average number of tokens in models generated response. Value Density: An efficiency metric measuring value delivered per token, calculated by dividing the H-Acc value expressed as percentage by the average response length. This is particularly relevant for tasks focused on verifiable correctness, where the primary goal is to provide the correct answer efficiently. Following previous RLVR studies for general domains (Su et al., 2025; Ma et al., 2025), we use large language model (Qwen2.5-72B-Instruct (Team, 2024)) to verify the semantic equivalence between the final answer of response and the reference answer. This automated verification method has been widely shown to have high agreement with human annotators in objective, non-adversarial, reference-based evaluation settings (Zhao et al., 2025). Importantly, focusing verification on only the final part of the response did not cause length collapse in our experiments. 6 Reinforcement Learning with Explicit Human Values 3.4 RLEV with Ground-Truth Human Values Our primary results show that RLEV consistently outperforms the correctness-only baseline across all tested configurations. This holds true for both 7B and 32B models, which see average HumanAligned Accuracy (H-Acc) gains of 2.0% and 2.8%, respectively  (Table 1)  . This improvement is driven by learned focus on high-value tasks; as detailed in the appendix  (Table 8)  , the accuracy gains are generally notably larger for high-valued prompts than for low-valued ones. key benefit is value-sensitive termination policy, which will be discussed in Section 3.6. The model learns to be concise on low-value prompts while remaining thorough on high-value ones. This leads to an overall increase in conciseness. For example, RLEV models more than halve the average response length, from 246.9 to 98.6 tokens for the 32B models. This efficiency and strategic improvement also generalize effectively. Even though trained on Chinese data, the RLEV models outperform their correctness-only counterparts on several out-ofdistribution (OOD) English and Chinese benchmarks, with the 32B model showing notable gains on tasks like GPQA Diamond and SuperGPQA  (Table 2)  . Table 1: Comparison of policies trained with RLEV (human-aligned) and baseline (correctness) rewards across 7B and 32B models. RLEV consistently improves accuracy and conciseness for both model scales. Estimator Size Reward Type Acc H-Acc Resp. Length Value Density REINFORCE++ RLOO GRPO Average 7B 32B 7B 32B 7B 32B 7B 32B correctness 63.8 human-aligned 65. correctness 67.7 human-aligned 71.0 correctness 65.9 human-aligned 66.6 70.9 correctness human-aligned 72.3 correctness 65.7 human-aligned 66.2 70.6 correctness human-aligned 71.3 correctness 65.1 human-aligned 66. correctness 69.7 human-aligned 71.5 55.0 57.0 57.6 61.9 56.7 58.9 60.9 63.3 56.0 57. 59.9 61.7 55.9 57.9 59.5 62.3 168.1 84.8 226.2 68.7 186.2 86. 345.5 78.7 251.1 100.4 169.0 148.3 201.8 90.5 246.9 98.6 0.33 0. 0.25 0.90 0.30 0.68 0.18 0.80 0.22 0.57 0.35 0.42 0.28 0. 0.26 0.71 Table 2: OOD Results across English and Chinese general-domain tasks. Model Base-7B + correctness + human-aligned Base-32B + correctness + human-aligned GPQA Diamond C-Eval MMLU-Pro SuperGPQA 31.8 31.8 31.3 33.2 39.9 43.4 60.8 76.2 76. 57.9 84.9 85.4 45.0 51.5 52.5 55.1 63.0 63.0 25.4 26.2 26.8 33.2 34.0 36.2 Reinforcement Learning with Explicit Human Values 3.5 RLEV with Other Types of Human Values Table 3: RLEV performance with imperfect value signals on the test set of WebInstruct-verified. We test two noisy value sources: weak labels derived from task difficulty and predictor-generated values from score predictor trained on our main exam dataset. Both methods consistently outperform the correctness-only baseline, showing RLEVs robustness when ground-truth values are unavailable. Model Base-7B Acc H-Acc primary junior senior university phd 18.8 17.0 38.9 28.4 24. 13.9 0.0 REINFORCE++ + correctness + weak-labeled values + predicted values 19.1 16.9 21.2 19.3 21.6 19.6 RLOO + correctness + weak-labeled values + predicted values 20.0 18.0 20.3 18.4 21.3 19.1 GRPO + correctness + weak-labeled values + predicted values 19.4 17.0 20.6 18.7 20.3 18.2 50.0 38.9 50.0 38.9 33.3 38. 44.4 50.0 50.0 27.0 28.4 32.4 28.4 29.7 32.4 33.8 31.1 25.7 27.3 29.4 27.9 28.2 28.5 30. 27.0 25.5 28.8 12.7 15.3 15.8 13.9 14.1 14.6 12.8 15.8 14.1 0.0 0.0 10.0 0.0 10.0 0. 0.0 0.0 0.0 This result  (Table 3)  demonstrates RLEVs robustness and practicality. It shows the method is effective even when precise, ground-truth scores are unavailable, making it applicable to much wider range of real-world scenarios where only heuristic value estimates (like task priority or difficulty) exist. Note that primary and phd only have 18 and 10 instances, respectively, while the total test set has 1,000 instances. We use the multi-subject exam data for training score predictor for generating the predicted values in Table 3. We discuss the training details in Appendix A.2. 3.6 Analysis of Value-Sensitive Termination Token-level Analysis: The hypothesis from the gradient analysis (value-scaling amplifies updates on the EOS token) is directly validated by our token-level analysis, though the behavior is more nuanced than simple uniform increase in EOS probability. As shown in Figure 2, the RLEV model learns sophisticated, value-sensitive termination policy For low-value prompts, the RLEV model assigns dramatically higher probability to the EOS token much earlier in the generation process compared to the baseline. Once sufficient answer is generated for these simpler prompts, the value-weighted reward strongly reinforces the decision to stop, leading to highly concise outputs. Conversely, for high-value prompts, which are often more complex, the RLEV model learns to suppress the probability of the EOS token relative to the baseline. This behavior encourages the model to generate more thorough and complete response. The gradient analysis explains this as the large value-scaling factor amplifying the signal to continue when the expected correctness from adding more tokens is higher than from stopping prematurely. This dual mechanism shows that RLEV does not merely learn to be shorter; it learns to allocate its token budget strategically, being efficient on low-stakes questions while being cautious and comprehensive on high-stakes ones to maximize the overall human-aligned score. 8 Reinforcement Learning with Explicit Human Values (a) Baseline (Top 200 valued prompts) (b) RLEV (Top 200 valued prompts) (c) Baseline (Bottom 200 valued prompts) (d) RLEV (Bottom 200 valued prompts) Figure 2: EOS probability trajectories for RLEV and the baseline, showing different termination policies for high-value (top) and low-value (bottom) prompts. 3.7 Ablation Studies: Isolating the Impact of Human Values key claim of our work is that aligning the reward signal with human-defined values is responsible for the observed performance gains. However, an alternative hypothesis is that the improvements stem from simply increasing the magnitude of the rewards for correct answers, rather than the value-alignment itself. To isolate the effect of human-aligned values, we conduct several ablation studies. Besides the correctness-only baseline, we also compare our full RLEV (human-aligned) model against two controls: Uniform Scaling: All correct responses receive constant s, where is the average reward scale calculated across the training prompts (s = E[s(x)] 1.2) (details in Appendix A.1). This control is designed to test the alternative hypothesis that general increase in reward magnitude, irrespective of its alignment with prompt value, is sufficient to cause the observed performance gains. Random Weights: The reward is scaled using the RLEV formula, but the human values v(x) are randomly shuffled across the training set before being used to calculate the scaling factor s(x). This procedure creates placebo reward signal that maintains the exact same distribution of reward magnitudes as the primary experiment but completely decouples the reward from the prompts true value. This directly tests whether the causal factor for improvement is the specific alignment between higher rewards and higher-value prompts. 9 Reinforcement Learning with Explicit Human Values Table 4: Ablation study results using the RLOO estimator. Uniformly scaling the reward degrades performance, while using random weights does not improve conciseness. Only when the reward scaling is directly correlated with human-defined values do we see meaningful increase in humanaligned accuracy (h-acc) and desirable reduction in response length. Reward Scaling Method Acc H-Acc Resp. Length Value Density correctness (baseline) uniform scaling random weights (shuffled) human-aligned (ours) 65.9 65.3 66.4 66.6 56.7 55.1 57.4 58.9 186.2 358.4 280.5 86.4 0.30 0.15 0.20 0.68 3.8 Reward Function Sensitivity and Design To validate our reward function design, we analyze its sensitivity to both the hyperparameter α and its specific mathematical form. Sensitivity to Hyperparameter α The choice of α is crucial as it determines how strongly the human value v(x) influences the final reward. We trained models using our primary reward function, r(x, y) = (1 + min(α v(x), 1)) 1correct(y), with several values of α. As shown in Table 5, while performance is robust across range of values, we found that α = 10 offered the best balance of human-aligned accuracy and response conciseness. Table 5: Sensitivity to hyperparameter α. Performance is reported across all key metrics. Hyperparameter α Acc H-Acc Resp. Length Value Density baseline 1 5 10 15 20 65. 66.4 66.1 66.6 66.3 66.1 56.7 58.1 56.8 58.9 58.1 56.8 186.2 101.5 141.0 86.4 62.4 157.9 0. 0.57 0.40 0.68 0.93 0.36 Ablation on Reward Function Form To justify our choice of an additive and clipped reward scaler, we compare it against purely multiplicative alternative: r(x, y) = (1 + α v(x)) 1correct(y). Table 6 shows that our chosen form yields superior results. There are two possible reasons: first, the mean v(x) is 0.02, and only 1.18% of the training examples have value > 0.1. This highly right-skewed distribution, which is visualized in Appendix A.1 (Figure 3), indicates that for over 98% of the data, our function acts as fine-grained linear reward scaler, preserving the original human value. Second, for the small fraction of high-value outliers shown in the distributions tail, the clipping mechanism prevents the excessively large rewards that the purely multiplicative form would generate, thus stabilizing training process and leading to better overall performance. Table 6: Comparison of different reward scaling functions. Reward Function Form Acc H-Acc Resp. Length Value Density purely multiplicative additive & clipped (ours) 66.4 66.6 57.6 58.9 201.6 86.4 0.29 0. 10 Reinforcement Learning with Explicit Human Values"
        },
        {
            "title": "4 Related Work",
            "content": "The idea of weighting learning signals according to their relative importance has deep roots in classical RL. Early methods such as importance-weighted transfer (Tirinzoni et al., 2018), rewardweighted regression (Peters & Schaal, 2007), and advantage-weighted regression (Peng et al., 2019) all adjust gradient updates to emphasize high-value samples. These studies show that non-uniform weighting can improve sample efficiency or align behavior with desired utility, but they do not consider the case where each data point (e.g., prompt or question) carries human-defined point value reflecting its real-world importance. Recent work in the LLM alignment domain has focused on RL with Verifiable Rewards (Luong et al., 2024; Lambert et al., 2024; Guo et al., 2025; Su et al., 2025), which train models using objective correctness signals. Other studies have proposed shaping or enriching verifiable rewards: for example, ConfClip (Zhang et al., 2025), rubrics as rewards (Gunjal et al., 2025), and composite reward frameworks such as RLCR with calibration rewards (Damani et al., 2025). While these approaches modify reward form or composition, they do not explicitly scale correctness rewards by human-assigned per-prompt values normalized across dataset, nor analyze the resulting gradient-level mechanisms. Our method, RLEV, integrates human-assigned per-prompt importance into the RLVR framework using clipped scaling surrogate. Through empirical tests, ablations, and gradient analysis, RLEV yields more concise and human-aligned behavior by optimizing for explicit, value-weighted utility, enabling alignment with explicitly defined human utility functions."
        },
        {
            "title": "5 Conclusions and Future Work",
            "content": "We introduced Reinforcement Learning with Explicit Human Values (RLEV), paradigm that aligns LLMs with human priorities by scaling correctness rewards with an explicit value signal. Experiments show RLEV consistently outperforms correctness-only baselines, improving valueweighted accuracy and leading to the generation of more concise responses. We trace this conciseness to value-weighted gradient amplification on end-of-sequence (EOS) tokens. Ablation studies confirm these gains are causally linked to value alignment rather than reward magnitude. Furthermore, the method proves robust, surpassing baselines even with noisy value signals derived from task difficulty. Future work could explore more dynamic value functions that are learned or adapt to user priorities. Another promising direction is to combine RLEV, for grounding in objective correctness and importance, with RLHF to fine-tune subjective qualities like style and tone. This hybrid approach could offer more holistic path to LLM alignment."
        },
        {
            "title": "6 Broader Impact and Limitations",
            "content": "Ultimately, this work demonstrates that directly optimizing for an explicit, non-uniform utility function is robust and effective method for aligning LLM behavior with human priorities. By moving beyond simple binary rewards, RLEV encourages models to develop more nuanced understanding of value, learning not just what constitutes correct answer, but also how much each correct answer matters. This represents practical step toward creating systems that are not only more capable but also more judicious in applying their capabilities to what humans deem most important. Despite its effectiveness, this work has several limitations. First, the framework formulates human value as single, pre-defined scalar quantity suited for objective domains where importance is explicitly quantified. However, human values in broader sense are often complex, multi-dimensional, and subjective. Second, applying RLEV requires explicit value labels for each prompt. While our experiments show RLEV is robust to noisy signals, this data dependency remains practical consideration. Finally, the current method relies on static value function, and future work could explore more dynamic value functions that adapt to user priorities in real-time. 11 Reinforcement Learning with Explicit Human Values"
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806, 2025. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36: 6299163010, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-Reasoner: Advancing llm reasoning across all domains. arXiv:2505.14652, 2025. URL https://arxiv.org/ abs/2505.14652. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019. Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pp. 745750, 2007. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. 12 Reinforcement Learning with Explicit Human Values Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Andrea Tirinzoni, Andrea Sessa, Matteo Pirotta, and Marcello Restelli. Importance weighted transfer of samples in reinforcement learning. In International Conference on Machine Learning, pp. 49364945. PMLR, 2018. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Bonan Zhang, Zhongqi Chen, Bowen Song, Qinya Li, Fan Wu, and Guihai Chen. Confclip: Confidence-weighted and clipped reward for reinforcement learning in llms. arXiv preprint arXiv:2509.17730, 2025. Yulai Zhao, Haolin Liu, Dian Yu, Sunyuan Kung, Meijia Chen, Haitao Mi, and Dong Yu. One token to fool llm-as-a-judge. arXiv preprint arXiv:2507.08794, 2025. 13 Reinforcement Learning with Explicit Human Values"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Data Statistics We analyze the human-defined values in 100k training instances and the 8k testing instances. These scores are normalized per-exam proportional scores defined in Equation 2 (Section 2.2). See distribution of normalized values in the training and test subsets in Figure 3 and Figure 4. Figure 3: Distribution of human-defined normalized values v(x) in training data (100k) with groundtruth values. A.2 Score Prediction Role Content Table 7: Prompt Structure for the Score Predictor system fYou are scoring assistant. Given question and its answer, output numeric score greater than 0 and up to {total_score} inclusive (decimals allowed, e.g. this problem would contribute in {total_score}-point exam. Respond with only the score, no other text. 0.5) that reflects how much user question To evaluate RLEVs performance with imperfect value signals, besides rule-based scores derived from difficulty levels, we train score predictor to generate pseudo values for datasets where ground-truth scores are unavailable. We convert the exam data into the format shown in Table 7 and train the score predictor with supervised fine-tuning for two epochs using Qwen2.5-7B. For datasets such as WebInstruct-verified, we standardize the task by setting consistent total score of 100 for all prompts. We use the same test set for evaluating the performance of the score predictor. It achieves an exact-match accuracy of 14 Reinforcement Learning with Explicit Human Values Figure 4: Distribution of human-defined normalized values v(x) in test data (8k) with ground-truth values. 79.5%. The Pearson correlation between the predicted and true scores is 0.91 (p < 0.001), indicating strong positive relationship. A.3 Detailed Accuracy Analysis Table 8: Comparison of policies trained with RLEV (human-aligned) and baseline (correctness) rewards across 7B and 32B models. We also report the accuracy on top 20% high-valued prompts and bottom 20% low-valued prompts. Estimator Size Reward Type Acc (all) Acc (high-valued) Acc (low-valued) REINFORCE++ RLOO GRPO 7B 32B 7B 32B 7B 32B correctness human-aligned correctness human-aligned correctness human-aligned correctness human-aligned correctness human-aligned correctness human-aligned 63.8 65.3 67.7 71.0 65.9 66.6 70.9 72. 65.7 66.2 70.6 71.3 54.5 58.0 57.6 62.9 57.4 58.8 60.9 62. 55.7 57.1 59.3 61.0 68.9 69.8 73.4 76.2 71.6 71.8 76.6 78. 71.9 72.4 76.8 76.6 As shown in Table 8, human-aligned (RLEV) policy achieves higher accuracy than the correctness baseline in all high-valued bins and nearly all low-valued bins. The improvement is generally more obvious for the high-valued prompts. These results show that RLEV specifically guides the model to perform better on the prompts that are defined as more valuable or important."
        }
    ],
    "affiliations": [
        "Princeton University",
        "Tencent AI Lab"
    ]
}