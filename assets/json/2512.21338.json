{
    "paper_title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
    "authors": [
        "Haonan Qiu",
        "Shikun Liu",
        "Zijian Zhou",
        "Zhaochong An",
        "Weiming Ren",
        "Zhiheng Liu",
        "Jonas Schult",
        "Sen He",
        "Shoufa Chen",
        "Yuren Cong",
        "Tao Xiang",
        "Ziwei Liu",
        "Juan-Manuel Perez-Rua"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 8 3 3 1 2 . 2 1 5 2 : r HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming Haonan Qiu1,2,, Shikun Liu1, Zijian Zhou1, Zhaochong An1, Weiming Ren1, Zhiheng Liu1, Jonas Schult1, Sen He1, Shoufa Chen1, Yuren Cong1, Tao Xiang1, Ziwei Liu2,, Juan-Manuel Perez-Rua1, 1Meta AI, 2Nanyang Technological University Work done at Meta, Joint last author High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: chunk-by-chunk strategy with fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2 faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving 107.5 acceleration over the baseline, offering compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable. Date: December 25, 2025 Project page: http://haonanqiu.com/projects/HiStream.html"
        },
        {
            "title": "1 Introduction",
            "content": "With the rapid development of video diffusion models Blattmann et al. (2023); Yang et al. (2024); Chen et al. (2024); HaCohen et al. (2024); Kong et al. (2024); Wan et al. (2025), high-resolution video generation is increasingly essential for applications in film production, virtual reality, and digital media, where visual realism and fine-grained details are critical. However, generating high-fidelity and temporally coherent videos remains extremely challenging due to the substantial spatio-temporal complexity. Although recent high-resolution generation methods Zhang et al. (2025a); Ren et al. (2025); Qiu et al. (2024, 2025a) have demonstrated impressive success, their computational efficiency is far from practical, as the inference cost scales quadratically based on the video spatial resolution and temporal length. As result, the challenge of generating realistic 1080p videos in time-efficient manner remains unsolved. To address these computational bottlenecks, several strategies have been proposed. Sampling acceleration techniques, such as timestep distillation Wang et al. (2023b); Sauer et al. (2024); Yin et al. (2024b,a), effectively reduce the number of denoising steps. However, the computational cost within each step remains high when applied to high-resolution videos. Concurrently, sparse or sliding attention mechanisms Zhang et al. (2025b); Cai et al. (2025); Huang et al. (2025) mitigate the full quadratic cost, but their cache and memory requirements still grow linearly with spatiotemporal volume. Thus, while these methods offer partial acceleration in low-resolution or short-video settings, they remain insufficient for scaling to long-duration, high-fidelity 1080p video generation. We argue that the fundamental source of this inefficiency is inherent computational redundancy across spatial, temporal, and timestep axes. Spatially, significant computation is wasted in early denoising steps, which primarily establish coarse motion and structure Liu et al. (2025b) and can be simply approximated at low-resolution. High-resolution details are only recovered in later steps. This insight directly motivates our Dual-Resolution Caching mechanism. Temporally, similar redundancy exists in autoregressive models. We find that only the initial frame (acting as persistent keyframe, akin to an attention sink Gu et al. (2024)) 1 Figure 1 HiStream delivers high-resolution (1080p) autoregressive video generation, offering up to 76.2 faster denoising than the bi-directional baseline and 2.5 faster than self-forcing, all while preserving the highest preference ratings. Furthermore, our faster variant, HiStream+, achieves remarkable 107.5 acceleration against the baseline and 3.5 faster than self-forcing. and small set of neighboring frames are crucial for temporal consistency. This observation leads to our Anchor-Guided Sliding Window strategy. Finally, we observe that the denoising trajectory differs significantly between the initial and subsequent chunks, phenomenon that motivates our Asymmetric Denoising Strategy. In this work, we propose HiStream, an efficient diffusion autoregressive framework that integrates core insights to reduce spatio-temporal complexity. HiStreams design is built on two primary optimizations. First, its dual-resolution caching performs low-resolution denoising to capture global structure, then refines high-resolution details using cached feature states, slashing redundant computation. Second, its anchor-guided sliding window generates video chunk-wise, using an anchor keyframe and small neighbor cache to maintain consistency. This design achieves fixed-size attention cache, ensuring stable inference speed regardless of video length. As third, optional optimization, we also introduce an asymmetric denoising strategy, where subsequent chunks can achieve high fidelity with as few as two denoising steps (one low-res, one high-res), enabling unprecedented efficiency. Extensive experiments on 1080p video benchmarks demonstrate that our primary HiStream model (utilizing the first two optimizations) attains state-of-the-art visual quality with negligible quality loss, while offering up to 76.2 faster denoising compared to the base video foundation model Wan2.1 Wan et al. (2025). Our faster variant, HiStream+, which additionally employs Asymmetric Denoising, pushes this acceleration to remarkable 107.5 over the baseline. This variant provides powerful speed-quality trade-off, making HiStream practical and scalable solution for both high-fidelity and ultra-fast high-resolution video generation."
        },
        {
            "title": "2 Related Work",
            "content": "Video Diffusion Models. Diffusion-based methods have recently emerged as the leading approach for video generation. VDM Ho et al. (2022) first introduced diffusion models to this domain, establishing foundation for subsequent research. Later works Blattmann et al. (2023); He et al. (2022); Guo et al. (2023); Chen et al. (2023); Wang et al. (2023a,c) built upon this framework with hierarchical latent designs, enabling the creation of longer videos with improved temporal consistency. However, these methods still rely on the UNet Ronneberger et al. (2015) architecture, which suffers from limited scalability. recent shift towards Transformer-based architectures, specifically Diffusion Transformers (DiT) Peebles and Xie (2023), has emerged in video generation. Models like CogVideoX Yang et al. (2024) and Pyramid Flow Jin et al. (2024) were among the first to demonstrate the superior scalability and strong performance of this approach. Leveraging the scalability of DiT, models like LTX HaCohen et al. (2024), Hunyuan Kong et al. (2024), and Wan Wan et al. (2025) now generate visually realistic videos from text, surpassing prior UNet-based approaches. High-Resolution Visual Generation. High resolution visual synthesis remains long-standing challenge in the generative field due to the high-resolution data scarcity and the substantial computational cost of modeling, particularly for videos. Recent efforts have diverged into two main categories: (1) Training-based methods, which fine-tune on high-resolution data directly with large-size models Teng et al. (2023); Hoogeboom et al. (2023); Ren et al. (2024); Liu et al. (2024a); Guo et al. (2024); Zheng et al. (2024); Cheng et al. (2024); Zhang et al. (2025a); Ren et al. (2025); and (2) Tuning-free methods, which modify architecture to improve resolution without additional training data Haji-Ali et al. (2024); Lin et al. (2024a); Lee et al. (2023); Jin et al. (2023); Zhang et al. (2024); Kim et al. (2024). While directly training on high-resolution data is fundamental solution, the scarcity of such data and the immense memory requirements make it impractical. Consequently, tuning-free methods have also become practical choice. However, most tuning-free approaches He et al. (2024); Du et al. (2024a); Huang et al. (2024a); Lin et al. (2024b); Liu et al. (2024b); Qiu et al. (2024) are designed for UNet-based models, focusing on mitigating receptive-field limitations that cause artifacts. DiT-based models face distinct challenge, where misaligned positional encodings can cause blurriness at high resolutions. As foundational models shift to DiT architectures, tuning-free methods Du et al. (2024b); Qiu et al. (2025a) for them are crucial. common alternative to native synthesis is two-stage approach using external super-resolution models Wang et al. (2021); Zhou et al. (2024); Wang et al. (2025). While computationally cheap, this pipeline often fails to render fine details, yielding less faithful visual quality. Efficient Visual Generation. The slow inference speed of iterative diffusion models has spurred multiple efficiency efforts. The most direct approach is reducing denoising steps, evolving from simple caching of redundant timesteps Lv et al. (2024); Liu et al. (2025a,c) to more powerful distillation techniques Luo et al. (2023); Wang et al. (2023b); Sauer et al. (2024); Yin et al. (2024b,a). Concurrently, other works have focused on reducing intra-network redundancy via sparsification Zhang and Agrawala (2025); Zhang et al. (2025b); Cai et al. (2025). more recent and relevant trend is the acceleration of temporal self-attention. By reformulating video diffusion models causally Teng et al. (2025); Chen et al. (2025); Yin et al. (2025); Huang et al. (2025), these methods enable incremental inference using KV caching. This development mirrors discoveries in LLMs, namely the attention sink phenomenon Xiao et al. (2023); Gu et al. (2024); Qiu et al. (2025b). This property was famously exploited by StreamingLLM Xiao et al. (2023) to implement local sliding-window attention with persistent anchor token, achieving stable inference with fixed-size KV cache. HiStream builds directly on this lineage of ideas, adapting them to the unique challenges of high-resolution video."
        },
        {
            "title": "3 Methodology",
            "content": "We present HiStream, an efficient high-resolution video generation framework designed to overcome the computational bottlenecks of existing diffusion models. Our complete pipeline is illustrated in Figure 2. 3 (b) Dual-Resolution Caching with Asymmetric Denoising (a) Autoregressive Inference with Anchor-Guided Sliding Window Figure 2 Pipeline details. Illustration of the synergy among our three core efficiency mechanisms. (Left) The AnchorGuided Sliding Window strategy ensures robust temporal scalability by generating the video in fixed-size chunks. Each generation step maintains constant attention context by combining tokens from the persistent content anchor (the first frame, dark green) and recent historical context (local frames, dark blue), thereby avoiding the growth of the KV cache over time. (Right) The Dual-Resolution Caching accelerates synthesis by adopting two-stage process: initial low-resolution denoising is followed by high-resolution refinement. Crucially, the final high-resolution output updates KVhigh and is subsequently downsampled to update KVlow, ensuring spatial consistency for subsequent chunks. With Asymmetric Denoising, subsequent chunks use half as many denoising steps as the first chunk."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Autoregressive Video Diffusion Models Our HiStream framework is an optimization of an autoregressive video diffusion model. This baseline architecture models the joint distribution of video frames x1:N by factorizing it autoregressively: p(x1:N ) = (cid:81)N i=1 p(xi x<i). Generation is sequential process where each frame xi is denoised from Gaussian noise, conditioned on the preceding frames x<i. In this work, our framework generates video in chunks of frames, rather than processing single frames individually. Our baseline WAN-2.1 is Diffusion Transformer (DiT) Peebles and Xie (2023) that operates in the latent space of causal 3D VAE Wan et al. (2025). The autoregressive factorization is implemented efficiently within the Transformer using causal attention masks, which restrict the model to only attend to past and current frames. This model is trained using the standard frame-wise diffusion loss, minimizing the MSE between the predicted noise ˆϵθ and the true sampled noise ϵ: LDM(θ) = Ex,t,ϵ (cid:2)ˆϵθ(xt, t, c) ϵ2 2 (cid:3) . (1) We use this architecture as the foundation for our further efficiency optimizations. Consistency Distillation key strategy for accelerating diffusion models is consistency distillation, which trains student model to produce high-quality samples in one or few steps, mimicking multi-step teacher. While traditional consistency distillation can involve complex multi-step losses, Flow Matching (FM) Lipman et al. (2022) provides more direct and stable training objective. FM trains the student model fϕ to learn conditional vector field that directly maps noisy input xt to the clean data x0. This simplifies the objective to stable L2 loss, making it robust method for few-step inference with the conditioned information c: LFM(ϕ) = Ep0(x0),pt(xt) (cid:2)fϕ(xt, t, c) (x0 xt)2 2 (cid:3) (2)"
        },
        {
            "title": "3.2 Spatial Compression",
            "content": "The computational cost of high-resolution video generation is dominated by spatial redundancy due to the quadratic complexity of the bi-directional self-attention. Traditional models operate at fixed high resolution, wasting significant computation in early denoising steps that only establish coarse layouts. To address this, we introduce Dual-Resolution Caching (DRC), mechanism designed to optimize computational efficiency by exploiting this redundancy without sacrificing visual fidelity. Progressive Upscaling. Our approach is motivated by the significant spatial redundancy we observe in few-step flow matching models. For 4-step denoising process, we find the initial two steps (Steps 1-2) primarily establish global composition, while any fine-grained details they generate are largely overridden by the final refinement steps (Steps 3-4). We validated this empirically (Fig 3): by deliberately blurring the outputs of 4 θ 0, ϵ, tj1) (0, I) tj1 Ψ(ˆxi tj ; tj, KVlow) end for for = Thigh, . . . , 1 do Algorithm 1 Autoregressive Inference with HiStream Require: KVlow and KVhigh caches Require: Denoise timesteps {t1, . . . , tT } Require: Number of generated chunk CN/M with chunk size Require: AR diffusion model Gθ (returns KVlow and KVhigh embeddings via GKV 1: Initialize: model output Xθ [ ] 2: Initialize: KV caches KVlow [ ] and KVhigh [ ] 3: for = 1, . . . , CN/M do Initialize xi 4: for = Tlow, . . . , Thigh + 1 do 5: Set ˆxi 0 Gθ(xi 6: if = Thigh + 1 then 7: 0 Up(ˆxi ˆxi 0) 8: 9: end if Sample ϵ (0, I) 10: Set xi 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: end for 32: return Xθ Xθ.append(ˆxi 0) Cache kvi high GKV θ Cache kvi low GKV if = 1 then KVhigh Concat (cid:0)KVhigh[0], kvi KVlow Concat (cid:0)KVlow[0], kvi KVhigh.append(kvi KVlow.append(kvi Sample ϵ (0, I) Set xi high[1 : S](cid:1) low[1 : S](cid:1) Set ˆxi if = 1 then (ˆxi (Down(ˆxi tj ; tj, KVhigh) tj1 Ψ(ˆxi 0); 0, KVlow) 0; 0, KVhigh) 0 Gθ(xi high) low) 0, ϵ, tj1) end for end if end if else else θ ) Dual-Resolution Caching Anchor-Guided Sliding Window Steps 1-2 (via downsampling and upsampling) before feeding them to the unmodified Steps 3-4, the final video is nearly indistinguishable from the full-resolution baseline. This confirms that the initial, structure-focused steps can be executed entirely at low resolution without compromising final quality. Based on this insight, our DRC strategy re-architects the denoising trajectory. Steps 1-2 are performed at low resolution to efficiently establish the global structure. The latent representation is then upsampled to the target high resolution, and Steps 3-4 perform high-resolution refinement. To enable single network to handle this resolution shift, we adapt techniques validated in LLMs Qiu et al. (2025a), integrating Rotary Positional Embeddings (RoPE) with NTK-inspired scaling Peng et al. (2023) for generalizing positional information across scales. This progressive upscaling approach reserves the most expensive high-resolution operations for the final, critical refinement stages. Dual KV Caching Our progressive generation structure necessitates specialized KV cache mechanism that deviates from the conventional single-resolution designs. If we cache the features from the low-resolution Steps 1-2, they will be misaligned with the final high-resolution output from Steps 3-4 (due to the iterative refinement of flow matching). Using this misaligned low-resolution cache to condition the next video chunk would introduce structural conflicts and degrade temporal consistency. To solve this, our dual-resolution caching design ensures the cache aligns with the final high-resolution"
        },
        {
            "title": "Blur Trial",
            "content": "Step 1 Step 2 Step 3 Step 4 Frame 1 Frame Frame 17 Frame 1 Frame 9 Frame 17 Figure 3 Results visualization of different timesteps. (Left) notable quality gap exists at Step 1 between frame 9 (chunk 1) and frame 17 (chunk 2). The fully denoised cache from chunk 1 provides high-value context, enabling chunk 2 to achieve near-final quality (comparable to Step 3) in just one step. (Right) Blur Trial: We also test robustness by blurring the outputs of Steps 1 and 2 (via down/upsampling) in the 4-step process. The final result remains nearly indistinguishable from the Baseline Setting. Figure 4 Visualization of temporal attention. (Right) Attention during autoregressive generation naturally focuses on the initial frame and the two most recent neighboring frames. (Left) This visualization demonstrates that dropping intermediate, less important frames has negligible impact, even without retraining. Conversely, dropping the initial frame causes catastrophic quality degradation. output. After the eventual high-resolution refinement (Step 4) of chunk is complete, we downsample this final, high-fidelity result back to the low resolution. We then use these downsampled latents to update the low-resolution KV cache. This guarantees the cache passed to the next chunk is spatially consistent with the high-resolution video just generated, maintaining spatio-temporal coherence."
        },
        {
            "title": "3.3 Temporal Compression",
            "content": "Our dual-resolution strategy does not, by itself, solve the problem of temporal complexity. In naive autoregressive structure, the KV cache still grows indefinitely, leading to memory exhaustion and slowing inference with long video generation. We therefore introduce the Anchor-Guided Sliding Window (AGSW), strategy to maintain fixed computational budget. AGSW generates video in chunks, restricting the attention context for any new chunk to constant size. This window is composed of the current chunk (M frames), local history cache (M 1 frames), and global anchor (1 frame), fixing the maximum computation length to 2M frames. 6 Temporal Decay Contribution. The primary mechanism for maintaining smooth local motion and temporal coherence is the Temporal Decay Contribution. This set consists of the 1 most recent past frames (i.e., the last two frames of the previously generated chunk, xi2:i, for = 3). These neighboring frames provide essential local historical context (Figure 4), offering the necessary motion vectors and fine-grained state information for the generation of the current chunk (xi:i+M ). The decay aspect reflects the empirical observation that fine-grained motion context is predominantly derived from immediate history, rapidly diminishing in relevance over longer temporal distances. By limiting this local context to fixed, small number of frames, we efficiently capture short-range dependencies crucial for visually fluid transitions. Temporal Attention Sink. To ensure long-range consistency, we leverage the attention sink phenomenon observed in LLMs and autoregressive generation. Efficiency-focused models like StreamingLLM Xiao et al. (2023) strategically leverage this by using fixed anchor tokens to stabilize sliding window. We observe that this phenomenon also naturally emerges in autoregressive video generation, with attention strongly concentrating on the first frame. This insight is so fundamental that, even without fine-tuning, dropping intermediate, low-value frames still yields high-quality results closely approximating full-history generation (Figure 4). Therefore, our AGSW strategy formalizes this behavior by designating the first frame as Temporal Attention Sink. The tokens from x1 are always cached and included in the attention computation for every subsequent chunk. This single-frame anchor provides stable, long-range reference point that grounds the generation, ensuring global scene and object consistency across the entire video without the computational burden of attending to the full history."
        },
        {
            "title": "3.4 Timestep Compression",
            "content": "Our analysis of the denoising trajectory (visualized in Figure 3) revealed critical and exploitable redundancy in timestep. We observed that while the first chunk (e.g., frame 9) requires the full 4-step denoising process to establish high-quality state, the second chunk (e.g., frame 17) achieves near-final quality, comparable to Step 3 output, after just one denoising step. This is because the second chunks generation is conditioned on the fully denoised and cached result of the first, providing it with high-value starting point. This insight leads to logical and highly effective optimization: applying the full 4-step denoising process to subsequent, high-quality chunks is computationally redundant. Therefore, we introduce an Asymmetric Denoising strategy. Only the initial chunk is processed using the complete 4-step denoising path to generate robust anchor cache. All subsequent chunks (Chunk 2 . . . CN/M ) are processed using an accelerated 2-step path. This simple modification drastically enhances generation efficiency by leveraging the caches power. However, as this aggressive acceleration is not strictly lossless, we introduce it as an optional, high-speed variant named HiStream+."
        },
        {
            "title": "4 Experiments",
            "content": "Implementation details. We design HiStream on top of the Wan2.1-T2V-1.3B model Wan et al. (2025), whose original resolution is 832 480. Following the Self Forcing paradigm Huang et al. (2025), we leverage the pre-trained Wan2.1-T2V-14B model as teacher to guide our student models training. Notably, HiStream requires no additional real video during this process. All training prompts are obtained from VidProM Wang and Yang (2024) as we follow Self Forcing Huang et al. (2025). We tune the model at 960 544 resolution (memory limitation), while generating content at 1920 1088 during inference. HiStream uses 4-step diffusion and implements chunk-wise autoregressive variants, generating chunks of = 3 latent frames. Evaluation metrics. We perform extensive evaluations using VBench Huang et al. (2024b) and user preference study, covering both visual fidelity and semantic alignment. We use augmented prompts from VBench as all baselines require. We also rigorously examine the runtime efficiency of our approach to validate its applicability. 7 Table 1 Quantitative comparisons with baselines. HiStream achieves the best or second-best scores for all metrics in high-resolution generation. The best results are marked in bold, and the second-best results are marked by underline. Method #Params frames Resolution (WH) Quality Score Semantic Score Total Score Per-Frame Denoising Latency (s) Low-Resolution Generation (For Reference Only) Wan2.1 Wan et al. (2025) Self Forcing Huang et al. (2025) 1.3B 1.3B 81 81 832480 832480 84.13 84.85 High-Resolution Generation Wan2.1 Wan et al. (2025) Self Forcing Huang et al. (2025) LTX HaCohen et al. (2024) FlashVideo Zhang et al. (2025a) HiStream (Ours) 1.3B 1.3B 2B 5B 1.3B 81 81 81 49 81 19201088 19201088 19201088 19201072 19201088 81.74 84.65 80.31 83.75 85.00 80.05 80. 76.39 79.97 69.54 81.62 80.97 83.32 84.06 80.67 83.71 78.16 83.32 84.20 2.04 0. 36.56 1.18 1.60 6.40 0.48 Wan2."
        },
        {
            "title": "Flash\nVideo",
            "content": "HiStream (Ours) Wan2."
        },
        {
            "title": "Flash\nVideo",
            "content": "HiStream (Ours) Figure 5 Qualitative comparisons. We visualize high-resolution videos generated by HiStream (Ours) against those by Wan2.1 Wan et al. (2025), Self Forcing Huang et al. (2025), LTX HaCohen et al. (2024), and FlashVideo Zhang et al. (2025a). The videos generated by HiStream exhibit the highest visual fidelity and the cleanest texture, free from spurious patterns or visible artifacts. Best viewed ZOOMED-IN."
        },
        {
            "title": "4.1 Comparison with Existing Baselines",
            "content": "We compare our model with relevant open-source efficient/high-resolution video generation models of similar scale. Our comparisons include three diffusion models: Wan2.1-1.3B Wan et al. (2025) (our base model 8 w/o HD Tech w/o Tuning w/o DRC w/o AGSW HiStream (Ours) Figure 6 Qualitative ablations. We perform controlled comparisons of HiStream with alternative variants. Best viewed ZOOMED-IN. structure), Self Forcing Huang et al. (2025) (our initialization weights), LTX HaCohen et al. (2024) (known for efficiency), and FlashVideo Zhang et al. (2025a) (known for high-resolution generation). Table 1 exhibits that our method achieves the best Quality Score and Total Score on VBench Huang et al. (2024b) in the high-resolution generation. While scores in high-resolution generation are generally lower than those at low resolutions, our method nonetheless achieves score that is highly competitive, approaching the best result recorded in the low-resolution setting. In terms of Per-Frame Denoising Latency, our proposed HiStream is the most efficient, achieving minimal latency of only 0.48 s. This performance represents an impressive 76.2 acceleration compared to the baseline Wan2.1-T2V-1.3B (36.56 s). Crucially, HiStream also significantly outperforms the existing efficient method Self Forcing (1.18 s), demonstrating near 2.5 speedup. As shown in Figure 5, the videos generated by HiStream exhibit the highest visual fidelity and the cleanest texture, free from spurious patterns or visible artifacts."
        },
        {
            "title": "4.2 Ablation Studies",
            "content": "We perform controlled comparisons of HiStream with alternative variants. We evaluate: (1) NTK-Rope + attention scaling (HD Tech), (2) Dual-Resolution Caching (DRC), (3) Anchor-Guided Sliding Window (AGSW), (4) Tuning, and (5) Asymmetric Denoising (HiStream+). As illustrated by the quantitative results in Table 2 and the qualitative analysis in Figure 6 (and Appendix), we draw the following conclusions regarding the contribution of each component: Foundation of HD Inference (HD Tech): The implementation of NTK-RoPE and attention scaling is the fundamental prerequisite for stable high-resolution inference. Experiments show that w/o HD Tech, the model either fails to generate coherent video content at the target resolution or exhibits severe stability issues, confirming that these techniques form the necessary quality basis. Dual-Resolution Caching (DRC): The DRC mechanism provides dual benefit. Ablating this technique (w/o DRC ) results in latency of 0.70 s. Incorporating DRC not only improves efficiency significantly (0.70 0.48 s), but also leads to superior compositional quality. This is attributed to the overall structural planning of the generated video being supervised and executed at the trained low-resolution, resulting in more coherent spatial layout. Anchor-Guided Sliding Window (AGSW): The AGSW primarily serves as an efficiency booster. Removing the AGSW mechanism (w/o AGSW ) increases the latency to 0.78 s. Crucially, AGSW accelerates the process to 0.48 with only negligible impact on the final video quality. This confirms that AGSW successfully exploits temporal redundancy without compromising video coherence. Necessity of Comprehensive Fine-Tuning: The introduction of the full HiStream framework (encompassing Table 2 Quantitative ablations. HiStream achieves the optimal overall video quality, while HiStream+ offers compelling trade-off between video fidelity and generation efficiency. Method Stable Setting w/o HD Tech w/o Tuning w/o DRC w/o AGSW HiStream (Ours) Faster Setting Naive Two Steps HiStream+ (Ours) Quality Score Semantic Score Total Score Per-Frame Denoising Latency (s) 84.58 84.63 84.46 85.15 85.00 84.68 84.69 72.99 79. 81.23 80.28 80.97 80.30 80.25 82.26 83.70 83.81 84.17 84.20 83.80 83.80 0.48 0.48 0.70 0.78 0. 0.32 0.34 HD Tech, DRC, and AGSW) creates significant gap between the models original training process and the modified inference paradigm. Our comprehensive fine-tuning is designed precisely to align the models weights with this new protocol. As demonstrated in Figure 6, the absence of this tuning (w/o Tuning) results in substantial degradation of video quality, confirming that bridging this training-inference gap is critical for achieving optimal high-fidelity synthesis. Ablation on Denoising Steps (HiStream+). We further investigate the impact of the denoising step count, comparing our 4-step main setting (0.48 s) with two accelerated 2-step variants. Interestingly, the quantitative results in Table 2 show that uniform 2-step process (applying 2 steps to all chunks) and our HiStream+ variant (4 steps for the first chunk, 2 for the rest) achieve nearly identical vbench scores. However, the qualitative results in Appendix reveal critical difference. Compared to our main HiStream setting, the uniform 2-step model suffers from severe blur and artifacts in the initial chunk. These initial errors then propagate, degrading the quality of all subsequent chunks. In contrast, HiStream+ avoids this issue by dedicating 4 steps to the initial chunk, establishing robust, high-quality cache. As result, HiStream+ exhibits far less visual degradation. This confirms that our asymmetric strategy is the superior approach, achieving significant latency reduction (0.48 0.34 s) with only minimal sacrifice in visual fidelity. When benchmarked on single H100 GPU, the per-frame denoising latency further drops to 0.21 (4.8 FPS). This result suggests that real-time 1080p video generation is now within reach, requiring only further hardware and engineering optimizations. In summary, our ablations validate the synergistic design of HiStream: HD Tech first establishes the stable high-resolution foundation. Building on this, DRC enhances both compositional quality and efficiency, while AGSW provides significant acceleration. Critically, our comprehensive Tuning then aligns the model to this modified inference protocol, bridging the training-inference gap to ensure all components work cohesively. We also demonstrate that this framework can be pushed to even higher speeds with HiStream+, where an Asymmetric Denoising strategy achieves latency of 0.34 by intelligently managing the quality-efficiency trade-off, vastly surpassing naive 2-step approach in visual quality."
        },
        {
            "title": "5 Conclusion",
            "content": "In summary, HiStream makes high-resolution video generation practical and scalable by combining DualResolution Caching with an Anchor-Guided Sliding Window. Our main framework drastically reduces redundant computation while preserving temporal consistency, achieving state-of-the-art visual quality with significantly improved inference efficiency. Furthermore, our HiStream+ variant leverages Asymmetric Denoising to push this efficiency to its limit (achieving 107.5 acceleration over the baseline), offering powerful and flexible trade-off for ultra-fast generation. This work provides robust foundation for accessible, high-fidelity cinematic content generation. However, key limitations still remain. Although our framework optimized the denoising pipeline, the VAE decoder has emerged as the new primary bottleneck; decoding 81 frames at 1080p still requires 16.45 on an A100 and 9.40 on an H100. Furthermore, high memory costs during distillation constrained our experiments to 1.3B student model, also not supervised on full 1080p data. This likely contributes to remaining flaws, 10 such as lack of physical realism and object interpenetration. Addressing these VAE and training-memory bottlenecks is the critical next step."
        },
        {
            "title": "References",
            "content": "Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In CVPR, 2024. Jiaxiang Cheng, Pan Xie, Xin Xia, Jiashi Li, Jie Wu, Yuxi Ren, Huixia Li, Xuefeng Xiao, Min Zheng, and Lean Fu. Resadapter: Domain consistent resolution adapter for diffusion models. 2024. Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising high-resolution image generation with no $$$. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61596168, 2024a. Ruoyi Du, Dongyang Liu, Le Zhuo, Qin Qi, Hongsheng Li, Zhanyu Ma, and Peng Gao. I-max: Maximize the resolution potential of pre-trained rectified flow transformers with projected flow. arXiv preprint arXiv:2410.07536, 2024b. Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. When attention sink emerges in language models: An empirical view. arXiv preprint arXiv:2410.10781, 2024. Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et al. Make cheap scaling: self-cascade diffusion model for higher-resolution adaptation. arXiv preprint arXiv:2402.10491, 2024. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Moayed Haji-Ali, Guha Balakrishnan, and Vicente Ordonez. Elasticdiffusion: Training-free arbitrary size image generation through global-local content separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66036612, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2024. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pages 1321313232. PMLR, 2023. Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: frequency perspective on training-free high-resolution image synthesis. arXiv preprint arXiv:2403.12963, 2024a. 11 Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024b. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. 2024. Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training-free diffusion model adaptation for variable-sized text-to-image synthesis. Advances in Neural Information Processing Systems, 36:7084770860, 2023. Younghyun Kim, Geunmin Hwang, Junyu Zhang, and Eunbyung Park. Diffusehigh: Training-free progressive high-resolution image synthesis through structure guidance. arXiv preprint arXiv:2406.18459, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions. Advances in Neural Information Processing Systems, 36:5064850660, 2023. Mingbao Lin, Zhihang Lin, Wengyi Zhan, Liujuan Cao, and Rongrong Ji. Cutdiffusion: simple, fast, cheap, and strong diffusion extrapolation method. arXiv preprint arXiv:2404.15141, 2024a. Zhihang Lin, Mingbao Lin, Meng Zhao, and Rongrong Ji. Accdiffusion: An accurate method for higher-resolution image generation. arXiv preprint arXiv:2407.10738, 2024b. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 73537363, 2025a. Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, Juan-Manuel Perez-Rua, and Jürgen Schmidhuber. Faster diffusion through temporal attention decomposition. Transactions on Machine Learning Research, 2025b. Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang. From reusing to forecasting: Accelerating diffusion models with taylorseers. arXiv preprint arXiv:2503.06923, 2025c. Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. 2024a. Xinyu Liu, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, et al. Hiprompt: Tuning-free higher-resolution generation with hierarchical mllm prompts. arXiv preprint arXiv:2409.02919, 2024b. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023. Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee Wong. Fastercache: Training-free video diffusion model acceleration with high quality. arXiv preprint arXiv:2410.19355, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, and Ziwei Liu. Freescale: Unleashing the resolution of diffusion models via tuning-free scale fusion. arXiv preprint arXiv:2412.09626, 2024. Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, and Ziwei Liu. Cinescale: Free lunch in high-resolution cinematic visual generation. arXiv preprint arXiv:2508.15774, 2025a. 12 Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, et al. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. arXiv preprint arXiv:2505.06708, 2025b. Jingjing Ren, Wenbo Li, Haoyu Chen, Renjing Pei, Bin Shao, Yong Guo, Long Peng, Fenglong Song, and Lei Zhu. Ultrapixel: Advancing ultra-high-resolution image synthesis to new peaks. arXiv preprint arXiv:2407.02158, 2024. Jingjing Ren, Wenbo Li, Zhongdao Wang, Haoze Sun, Bangzhen Liu, Haoyu Chen, Jiaqi Xu, Aoxue Li, Shifeng Zhang, Bin Shao, et al. Turbo2k: Towards ultra-efficient and high-quality 2k video synthesis. arXiv preprint arXiv:2504.14470, 2025. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pages 87103. Springer, 2024. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jianyi Wang, Zhijie Lin, Meng Wei, Yang Zhao, Ceyuan Yang, Chen Change Loy, and Lu Jiang. Seedvr: Seeding infinity in diffusion transformer towards generic video restoration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 21612172, 2025. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a. Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. Advances in Neural Information Processing Systems, 37:6561865642, 2024. Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023b. Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19051914, 2021. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023c. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024a. Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024b. 13 Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2296322974, 2025. Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Yuhao Chen, Yao Tang, and Jiajun Liang. Hidiffusion: Unlocking higherresolution creativity and efficiency in pretrained diffusion models. In European Conference on Computer Vision, pages 145161. Springer, 2024. Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, and Ping Luo. Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025a. Yuechen Zhang, Jinbo Xing, Bin Xia, Shaoteng Liu, Bohao Peng, Xin Tao, Pengfei Wan, Eric Lo, and Jiaya Jia. Training-free efficient video generation via dynamic token carving. arXiv preprint arXiv:2505.16864, 2025b. Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua Han, Ying Li, Songcen Xu, and Hang Xu. Any-size-diffusion: Toward efficient text-driven synthesis for any-size hd images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 75717578, 2024. Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-a-video: Temporal-consistent diffusion model for real-world video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25352545, 2024. Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, and Tianfan Xue. Flashvsr: Towards real-time diffusion-based streaming video super-resolution. arXiv preprint arXiv:2510.12747, 2025. 14 Appendix Overview. In the supplementary material, we introduce more implementation details and additional experimental results."
        },
        {
            "title": "A More Implementation Details",
            "content": "A.1 Training details We utilize the DMD2 Yin et al. (2024a) distillation framework for our training pipeline. This setup employs three distinct models based on the Wan2.1 Wan et al. (2025) architecture: frozen 14B model serves as the real diffusion model (teacher ), an updatable 1.3B model acts as the final generator (student), and second updatable 1.3B model functions as the fake diffusion model to assist gradient calculation. The fake diffusion model is updated using the standard diffusion loss. Crucially, the final generator is updated by the distribution matching gradient, which is calculated from the divergence between the real and fake diffusion models. Further technical details regarding the training process are available in the original DMD2 paper. To adapt the models for high-resolution generation, we set the timestep shift to 5 and employ NTK-RoPE with scaling factor of 2 throughout the training process. Training clips have spatial resolution of 960 544 and temporal length of 81 frames. After VAE compression, this translates to 21 latent frames, which are processed in 7 chunks of 3 latent frames each. The entire procedure was efficiently performed on 64 H100 GPUs and achieved convergence in approximately 12 hours. A.2 Inference details Our inference pipeline utilizes only the final 1.3B generator model. The process is conducted at high resolution of 1920 1088 (twice the spatial resolution used during training), maintaining 81 frames (21 latent frames) which are generated chunk-by-chunk in 7 total chunks (3 latent frames per chunk). To manage this significant resolution jump while maintaining quality, we apply the HD Tech: the timestep shift parameter is increased from 5 to 7, and the NTK-RoPE factor is increased from 2 (used in training) to 5. We also set the attention scaling factor to 2 exclusively for the initial chunk, allowing the model to focus on generating strong, detailed high-resolution content for the anchor frames. For all subsequent chunks, attention scaling is disabled (factor set to 1). This is highly effective because the high-resolution details generated in Chunk 1 are efficiently transferred and propagated to subsequent chunks, preventing the generation of redundant artifacts. For alignment with established baselines, all 1080p benchmark experiments reported in the main paper were uniformly conducted on single A100 GPU."
        },
        {
            "title": "B Additional Experimental Results",
            "content": "B.1 Comparison with Super-Resolution In contrast to conventional super-resolution (SR) methodologies, our higher-resolution generation approach is designed to exploit the latent capabilities of the pre-trained model. Therefore, the resultant performance is derived directly from the underlying base model, obviating the need for separate, dedicated SR model. We evaluate our method against standard super-resolution post-processing configuration: namely, applying FlashVSR Zhuang et al. (2025) to the Self Forcing Huang et al. (2025) output. As shown in Table 3, HiStream achieves better Quality Score and Total Score. In addition, Figure 7 shows that our native high-resolution synthesis provides better detail accuracy, capturing fine textures that are often missed or incorrectly generated by two-stage pipelines that depend on super-resolution. B.2 Qualitative Ablations for Timesteps. We further investigate the impact of the denoising step count, comparing the robust 4-step setting against accelerated 2-step variants. As depicted in Figure 8, the uniform 2-step approach suffers from catastrophic 1 Table 3 Quantitative comparisons with super-resolution. Compared to super-resolution post-processing setting Self Forcing + FlashVSR, HiStream achieves higher Quality Score and Total Score. Method Self Forcing Huang et al. (2025) + FlashVSR Zhuang et al. (2025) HiStream (Ours) Quality Score 84.71 85.00 Semantic Score 81.04 80. Total Score 83.98 84."
        },
        {
            "title": "FlashVSR",
            "content": "HiStream (Ours) Figure 7 Qualitative comparisons with super-resolution. Our native high-resolution synthesis captures fine textures with greater accuracy than two-stage super-resolution pipelines, which often miss or hallucinate details. Best viewed ZOOMED-IN. Naive Two Steps HiStream+"
        },
        {
            "title": "HiStream",
            "content": "Figure 8 Qualitative ablations for timesteps. We perform controlled comparisons of HiStream with alternative variants in different timestep strategies. failure: the initial chunk exhibits severe blur and artifacts, manifesting as pronounced ghosting on the trumpet and faint details on the motorcycles windshield. In this autoregressive system, these early-stage errors propagate through the cache, critically degrading the temporal and visual fidelity of all subsequent chunks. In stark contrast, HiStream+ mitigates this failure by investing computation strategically. By dedicating the full 4 steps to the initial chunk, it establishes robust, high-fidelity anchor cache. This high-quality initialization prevents error accumulation, resulting in minimal visual degradation across the entire video. This confirms that our asymmetric strategy is the superior acceleration approach, drastically reducing the computational load (effectively cutting steps in half for later chunks) with only minimal and acceptable sacrifice in visual fidelity. B.3 User Study In addition, we conducted user study to assess the perceptual quality of our generated results. Participants were asked to view videos produced by all comparison methods, with each example presented in randomized order to minimize potential bias. For each case, users selected the best result based on three evaluation criteria: video quality, semantic alignment, and detail fidelity. total of 21 participants took part in the study. As shown in Table 4, our method received the highest number of votes across all evaluation aspects, significantly outperforming the baseline approaches. Table 4 User study for Video Generation. Users are required to pick the best one among our proposed HiStream and the other baseline methods in terms of video quality, semantic alignment, and detail fidelity. Method"
        },
        {
            "title": "Detail\nFidelity",
            "content": "Wan2.1 Wan et al. (2025) Self Forcing Huang et al. (2025) LTX HaCohen et al. (2024) FlashVideo Zhang et al. (2025a) HiStream (Ours) 2.78% 10.71% 0.79% 12.30% 73.41% 5.95% 7.54% 1.59% 14.68% 70.24% 3.17% 8.73% 0.79% 11.51% 75.79% Figure 9 VBench scores visualization. We compare our two variants (HiStream and HiStream+) with Wan2.1 Wan et al. (2025), Self Forcing Huang et al. (2025), LTX HaCohen et al. (2024), and FlashVideo Zhang et al. (2025a) using all 16 VBench metrics. B.4 VBench Scores Across All Dimensions Figure 9 presents comprehensive evaluation of our two variants (HiStream and HiStream+) against representative models using all 16 VBench metrics. Both variants generally outperform competitors in semantic alignment, achieving top scores in dimensions such as object class, spatial relationship, and scene. They also demonstrate robust frame-wise quality, scoring high in aesthetic quality and imaging quality."
        }
    ],
    "affiliations": [
        "Meta AI",
        "Nanyang Technological University"
    ]
}