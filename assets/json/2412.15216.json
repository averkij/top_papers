{
    "paper_title": "UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency",
    "authors": [
        "Enis Simsar",
        "Alessio Tonioni",
        "Yongqin Xian",
        "Thomas Hofmann",
        "Federico Tombari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose an unsupervised model for instruction-based image editing that eliminates the need for ground-truth edited images during training. Existing supervised methods depend on datasets containing triplets of input image, edited image, and edit instruction. These are generated by either existing editing methods or human-annotations, which introduce biases and limit their generalization ability. Our method addresses these challenges by introducing a novel editing mechanism called Cycle Edit Consistency (CEC), which applies forward and backward edits in one training step and enforces consistency in image and attention spaces. This allows us to bypass the need for ground-truth edited images and unlock training for the first time on datasets comprising either real image-caption pairs or image-caption-edit triplets. We empirically show that our unsupervised technique performs better across a broader range of edits with high fidelity and precision. By eliminating the need for pre-existing datasets of triplets, reducing biases associated with supervised methods, and proposing CEC, our work represents a significant advancement in unblocking scaling of instruction-based image editing."
        },
        {
            "title": "Start",
            "content": "UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency Enis Simsar1 Alessio Tonioni3 Thomas Hofmann1 Federico Tombari2,3 1ETH Zurich 3Google Switzerland Yongqin Xian3 2Technical University of Munich https://enis.dev/uip2p 4 2 0 2 9 1 ] . [ 1 6 1 2 5 1 . 2 1 4 2 : r Figure 1. Unsupervised InstructPix2Pix. Our approach achieves more precise and coherent edits while preserving the structure of the scene. UIP2P outperforms state-of-the-art models in both real images (a. and b.) and synthetic images (c. and d.)."
        },
        {
            "title": "Abstract",
            "content": "We propose an unsupervised model for instruction-based image editing that eliminates the need for ground-truth edited images during training. Existing supervised methods depend on datasets containing triplets of input image, edited image, and edit instruction. These are generated by either existing editing methods or human-annotations, which introduce biases and limit their generalization ability. Our method addresses these challenges by introducing novel editing mechanism called Cycle Edit Consistency (CEC), which applies forward and backward edits in one training step and enforces consistency in image and attention spaces. This allows us to bypass the need for groundtruth edited images and unlock training for the first time on datasets comprising either real image-caption pairs or image-caption-edit triplets. We empirically show that our unsupervised technique performs better across broader range of edits with high fidelity and precision. By eliminating the need for pre-existing datasets of triplets, reducing biases associated with supervised methods, and proposing CEC, our work represents significant advancement in unblocking scaling of instruction-based image editing. 1. Introduction Diffusion models (DMs) have recently achieved significant advancements in generating high-quality and diverse images, primarily through breakthroughs in text-to-image generation [15, 33, 34, 36]. This led to the development of various techniques for tasks like personalized image generation [10, 35, 45], context-aware inpainting [23, 26, 49], and editing images based on textual prompts [1, 6, 13, 18, 24]. Editing images based on textual instructions [3] demonstrates the versatility of DMs as robust editing tools. 1 Figure 2. Examples of biases introduced by Prompt-to-Prompt in the InstructPix2Pix dataset. Each example shows an input image and its corresponding edited image (generated by Prompt-to-Prompt) along with the associated edit instruction. (a) Attribute-entangled edits: modifying the ladys dress also unintentionally changes the background. (b) Scene-entangled edits: transforming the cottage into castle affects surrounding elements. (c) Global scene changes: converting the image to black and white alters the entire scene. However, existing methods predominantly rely on supervised learning, which requires large datasets of triplets containing input images, edited images and edit instructions [3, 9, 50, 51]. These datasets are often generated using editing methods such as Prompt-to-Prompt [13] or human annotations. While the former solution allows better scaling of dataset size, it, unfortunately, introduces biases, such as (a) attribute-entangled or (b) scene-entangled edits that affect unintended parts of the image or (c) cause significant changes to the entire scene (see Fig. 2). On the other hand, human-annotated data, though valuable, is impractical for large-scale training due to the high cost and effort involved in manual annotation. This reliance on human-annotated or generated ground-truth edited images limits the diversity of the achievable edits and hinders the development of models capable of understanding and executing wide range of user instructions. We present UIP2P, an unsupervised model for instruction-based image editing that removes the dependency on datasets of triplets, generated or humanannotated, by introducing Cycle Edit Consistency (CEC), i.e., consistency obtained by applying forward and reverse edits. During the cyle edit, to overcome the need of explicit supervision while ensuring that edits remain consistent, we leverage the alignment between text and images in the CLIP embedding space [32]. Moreover, we explicitly enforce consistency in both the image and attention space, enabling the UIP2P model to accurately interprets and localizes user instructions and ensuring that edits are coherent and reflect the intended changes. CEC allows UIP2P to effectively maintain the integrity of the original content while making precise modifications, further enhancing the reliability of the edits. Our approach unlocks the ability to train on large real-image datasets, which was previously not possible due to the limitations of existing approaches and the high cost of human labeling. As result, our approach significantly broadens the scope and scalability of instruction-based image editing compared to previous methods. Our key contributions are as follows: We introduce an unsupervised technique for instructionbased image editing, UIP2P, that removes the requirement for ground-truth edited images during training, providing more scalable alternative to current supervised methods. We propose Cycle Edit Consistency (CEC), novel approach that ensures consistent edits when cycled across forward and reverse editing, maintaining coherence in both the image and attention space. This allows precise, high fidelity edits that accurately reflect user instructions. Our approach demonstrates scalability and versatility across various real-image datasets, enabling wide range of edits without relying on existing datasets, significantly broadening the scope of instruction-based image editing. 2. Related Work CLIP-Based Image Manipulation. StyleCLIP [29] combines StyleGAN and CLIP for text-driven image manipulation, requiring optimization for each specific edit. Similarly, StyleGAN-NADA [11] enables zero-shot domain adaptation by using CLIP guidance to modify generative models. While these approaches allow for flexible edits, they often rely on domain-specific models or optimization processes for each new task. These works illustrate the potential of CLIPs powerful semantic alignment for image manipulation, which motivates the use of CLIP in other generative frameworks, such as diffusion models. Text-based Image Editing with Diffusion Models. One common approach in image editing is to use pre-trained diffusion models by first inverting the input image into the latent space and then applying edits through text prompts [6, 13, 17, 24, 25, 27, 43, 44, 47]. For example, DirectInversion [17] edits the image after inversion using Promptto-Prompt [13], but the inversion step can lead to losing essential details from the original image. In addition, methods like DiffusionCLIP [52], CycleDiffusion [46], CycleNet [48], and DualDiffusion [40] explore domain-to-domain translation as way to improve image editing. However, 2 their focus on translating between two fixed domains makes it difficult to handle complex edits, such as the insertion or deletion of objects. In contrast, we focus on generalpurpose image editing approach that is not limited to domain translation, enabling greater flexibility in handling wider variety of edits. Another line of methods for image editing involves training models on datasets containing triplets of input image, edit instruction, and edited image [3, 50, 51]. These methods, since they directly take the input image as condition, do not require an inversion step. InstructDiffusion [12] builds on InstructPix2Pix by handling wider range of vision tasks but has difficulty with more advanced reasoning. MGIE [9] improves on this by using large multimodal language models to generate more precise instructions. SmartEdit [16] goes step further by introducing Bidirectional Interaction Module that better connects the image and text features, improving its performance in challenging editing scenarios. major challenge in image editing is the need for largescale, high-quality triplet datasets. InstructPix2Pix [3] partially addresses this by generating extensive datasets using GPT-3 [4] and Prompt-to-Prompt [13]. However, while this mitigates data scarcity, it introduces issues like model biases from Prompt-to-Prompt. MagicBrush [50] tackles the quality aspect with human-annotated datasets, but this approach is small-scale, limiting its practicality for broader use. Our approach leverages CLIPs semantic space for aligning images and text, providing more robust solution. Introducing Cycle Edit Consistency (CEC) tackles both dataset limitations and model biases, ensuring coherence between forward and reverse edits. Our approach enhances scalability and precision for complex instructions and eliminates the dependency on triplet datasets, making it applicable to any image-caption dataset of real images. Moreover, as CEC modifies only the training phase of InstructPix2Pix, it integrates seamlessly with any model extension. 3. Background 3.1. Latent Diffusion Models (LDMs) Stable Diffusion (SD) is prominent Latent Diffusion Model (LDM) designed for text-guided image generation [34]. LDMs operate in compressed latent space, typically derived from the bottleneck of pre-trained variational autoencoder, which enhances computational efficiency. Starting with Gaussian noise, the model progressively constructs images through an iterative denoising process guided by text conditioning. This process is powered by U-Netbased architecture [8], utilizing self-attention and crossattention layers. Self-attention helps refine the evolving image representation, while cross-attention integrates the textual guidance. Cross-attention mechanisms are crucial in directing image generation in LDMs. Each cross-attention layer consists of three main components: queries (Q), keys (K), and values (V ). Queries are generated from intermediate image features through linear transformation (fQ), while keys and values are extracted from the text conditioning using linear transformations (fK and fV ). The attention mechanism, formulated in Eq. (1), computes attention maps that indicate which regions of the evolving image should be modified based on the text description. We utilize those attention maps in our loss functions to localize the desired edit, enabling localized and consistent image editing. Attention(Q, K, ) = Softmax (cid:19) (cid:18) QK (1) 3.2. InstructPix2Pix (IP2P) Our method builds upon InstructPix2Pix (IP2P) [3], an LDM-based framework for text-conditioned image-toimage transformations. Like Stable Diffusion, IP2P employs U-Net architecture. The conditional framework of IP2P allows it to simultaneously utilize both an input image (I) and text instruction (T ) to guide image modifications. Classifier-free guidance (CFG) [14] is used, with coefficients (sI and sT ) controlling the influence of the text and the original image during editing. The predicted noise vectors (eθ) from the learned network are combined linearly to produce the final score estimate eθ. InstructPix2Pix is trained on dataset containing triplets of input image, edit instruction and edited image. The dataset is composed of synthetic images generate by SD on top of real captions, edit instructions generated by an LLM and edited images obtained using Prompt-to-Prompt [13]. The reliance on synthetic datasets introduces several limitations that we aim to address in this work. First, models like IP2P are trained exclusively on synthetic data, which limits their applicability during training on real-world image datasets. Second, their performance is inherently constrained by the quality of the images generated by the Prompt-to-Prompt method, which introduces biases into the editing process, as demonstrated in Fig. 2. 4. Method Differently from existing works such as InstructPix2Pix [3], which rely on paired datasets of input and edited images for instruction-based editing, instead we utilize an unsupervised technique that requires only real images and corresponding edit instructions, eliminating the need for groundtruth edited images. In nutshell, given an image and forward edit instruction (e.g., turn the sky pink), we generate an edited image. We then apply reverse instruction (e.g., turn the sky blue.) to the edited image, aiming to recover the original input. During forward-reverse edits, we 3 Figure 3. Overview of the UIP2P training framework. The model learns instruction-based image editing by utilizing forward and reverse instructions. Starting with an input image and forward instruction, the model generates an edited image using IP2P. reverse instruction is then applied to reconstruct the original image, enforcing Cycle Edit Consistency (CEC). enforce our proposed Cycle Edit Consistency (CEC) ensuring that the edits are reversible and maintain coherence in both the image and attention space. This approach allows us to scale instruction-based image editing across various real-image datasets without the limitations of synthetic or paired edited datasets. In the following sections, we describe our approach in detail, including the key components of our framework (Sec. 4.1), the loss functions used to enforce consistency, and the training data generation procedure (Sec. 4.2). 4.1. Framework 4.1.1. UIP2P At the core of our method is the concept of Cycle Edit Consistency (CEC), which ensures that edits applied to an image can be reversed back to the original input through corresponding reverse instructions. Our framework, UIP2P, introduces four key components designed to enforce CEC and maintain both semantic and visual consistency during the editing process, leveraging mechanism that effectively reuses predictions across diffusion steps to enhance the editing process (an overview is illustrated in Fig. 3): 1. Text and Image Direction Consistency: We leverage CLIP embeddings [31] to align the semantic relationship between textual instructions and the image modifications. By operating within CLIPs embedding space, our model ensures that the relationship between the input and edited images corresponds to the relationship between their respective captions. This alignment is critical for enforcing Cycle Edit Consistency (CEC), ensuring that the desired edit is applied while preserving the input images structure. 2. Attention Map Consistency: To maintain consistency throughout the editing process, we enforce that attention maps generated during both forward and reverse edits align. This guarantees that the model consistently focuses on the same regions of the image during the initial edit and its reversal. Attention Map Consistency both regularize the training objective and ensures that the learned edits are well localized. 3. Reconstruction Consistency: Central to enforcing CEC, the model must reconstruct the original input image after applying the reverse instruction. This ensures that the model can reliably undo its edits. We 4 achieve this by minimizing both pixel-wise and semantic discrepancies between the reconstructed image and the original input, ensuring coherence between the applied edit and its reversal. 4. Unified Prediction with Varying Diffusion Steps: We sample different diffusion steps (t for forward and ˆt for reverse), we independently predict ˆϵF and ˆϵR for one step of each and finally apply them across steps in the forward (F) and ˆt steps in the reverse (R) to reconstruct the image. Therefore while training we effectively perform prediction with 1 denoising step. Reusing the same prediction across steps reduces computational cost. By combining these componentsText and Image Direction Consistency, Attention Map Consistency, Reconstruction Consistency, and Unified Prediction with Varying Diffusion Stepsour framework enforces CEC and allow training across diverse real-image datasets. This ability to generalize beyond synthetic datasets underscores the versatility of our method in real-world instruction-based image editing scenarios. 4.1.2. Loss Functions To enforce Cycle Edit Consistency (CEC) and ensure both visual and semantic consistency during the editing and reconstruction process, we introduce additional loss terms to each training iteration. In our approach, training sample comprises an input image, an edit instruction in text, and its corresponding reverse instruction, along with the input and edited captions of the images. We will provide further details on how these samples are generated in later section. CLIP Direction Loss. This loss ensures that the transformations applied to the image align with the text instructions in CLIPs semantic space [11]. Given the CLIP embeddings of the input image (EIinput), edited image (EIedit), input caption (ETinput ), and edited caption (ETedit), the loss is defined as: LCLIP = 1 cos (cid:0)EIedit EIinput, ETedit ETinput (cid:1) (2) This loss aligns the direction of change in the image space with the direction of the transformation described in the text space, ensuring that the modifications reflect the intended semantic of the edits. This ensures that the model aligns transformations in the image space with the corresponding text modifications. However, ensuring spatial consistency is equally important, which we address with the Attention Map Consistency Loss. Attention Map Consistency Loss. To ensure that the same regions of the image are edited in both the forward and reverse edits, we define an attention map consistency loss. Let A(i) represent the cross-attention maps and A(i) from the i-th layer of the U-net model during the forward and reverse edits, respectively. The loss is defined as: Lattn = (cid:88) (cid:13) (cid:13)A(i) (cid:13) A(i) (cid:13) (cid:13) (cid:13)2 (3) This loss ensures spatial consistency during both the editing and reversal stages, key requirement for CEC, as it guarantees that the attention focuses on the same regions when reversing the edits. CLIP Similarity Loss. This loss encourages the edited image to remain semantically aligned with the provided textual instruction. It is calculated as the cosine similarity between the CLIP embeddings of the edited image (EIedit) and the edited caption (ETedit): Lsim = 1 cos(EIedit, ETedit) (4) This loss ensures that the generated image aligns with the desired edits in the instruction, preserving semantic coherence between the forward and reverse processesan essential aspect of CEC. Reconstruction Loss. To guarantee that the original image is recovered after the reverse edit, we employ reconstruction loss. This loss consists of two components: pixel-wise loss and CLIP-based semantic loss. The total reconstruction loss is defined as: Lrecon = Iinput Irecon2 + 1 cos(EIinput , EIrecon ) (5) This loss ensures that the model can faithfully reverse edits and return to the original image when the reverse instruction is applied, enforcing CEC by minimizing differences between the input and reconstructed images. 4.1.3. Total Loss The total loss function is applied to the single step noise prediction rather than recursively, used for training the model is weighted combination of the individual losses: LCEC = λCLIPLCLIP+λattnLattn+λsimLsim+λreconLrecon (6) where λCLIP, λattn, λsim, and λrecon are hyperparameters controlling the relative weights of each loss. 4.2. Training Data To enable CEC training on datasets with image and edit instructions [3], we leverage Large Language Models (LLMs), such as GEMMA2 [42] and GEMINI [41], to automatically generate reverse edit instructions. These LLMs provide an efficient and scalable solution for obtaining reverse instructions with minimal cost and effort [3]. We use 5 Table 1. Reverse Instruction Generation. Our method generates reverse instructions for the IP2P dataset, eliminating the need for manually edited images. Additionally, edit instructions, edited captions, and reverse instructions are generated for CC3M and CC12M datasetsdenoted as CCXM. The texts are generated by LLMs such as GEMINI Pro, and GEMMA2. Input Caption Edit Instruction Edited Caption man wearing denim jacket sofa in the living room make the jacket rain coat add pillows Person on the cover of magazine make the person cat tourist rests against concrete wall give him backpack man wearing rain coat sofa in the living room with pillows Cat on the cover of the magazine tourist with backpack rests against concrete wall 2 M Reverse Instruction make the coat denim jacket remove the pillows make the cat person remove his backpack GEMINI Pro to enrich the IP2P dataset with reverse instructions based on the input caption, edit instruction, and corresponding edited caption. To improve model performance, we employ few-shot prompting during this process, enabling the generation of reverse instructions without the need for manually paired datasets, which significantly enhances scalability. The reverse instructions generated by the LLM aim to revert the edited image to its original form (see Tab. 1 - IP2P section). Using the enriched dataset with reverse instructions (see Tab. 1, IP2P section), we fine-tune GEMMA2 [42], to generate an edit instruction, edited caption, and reverse instruction given an input caption. We use this fine-tuned model to allow training on image-caption paired datasets such as CC3M and CC12M [5, 37], generating forward and reverse edits along with corresponding edited captions (see Tab. 1, CCXM section). 5. Experiments 5.1. Experimental Setup Dataset Generation. To train our method, we generate datasets consisting of paired forward and reverse instructions, as detailed in Sec. 4.2. For the initial experiments, we use the InstructPix2Pix dataset [3], which provides generated image-caption pairs along with edit instructions. We further extend our experiments to real-image datasets. The real-image datasets include CC3M [37] and CC12M [5], for which we generate eight possible edits per image-caption pair. This increases diversity in the editing tasks, exposing the model to wide range of transformations and enhancing its ability to generalize across different types of edits and real-world scenarios. Baselines. We evaluate our method by comparing it against several models. The primary baseline is InstructPix2Pix [3], supervised method that relies on ground-truth edited images during training. To demonstrate the advantages of our unsupervised approach, we train and test both IP2P and our model on the same datasets, with our method not using the ground-truth edited images while training. We also compare our method with other instruction-based editing models, including MagicBrush [50], HIVE [51], MGIE [9], and SmartEdit [16]. These additional comparisons allow us to evaluate how effectively our unsupervised model handles diverse and complex edits without the need for existing editing methods to generate ground-truth edited images or human-annotated data. Implementation Details. Our method, UIP2P, fine-tunes SD-v1.5 model [34], without any pre-training on supervised datasets. While we retain the IP2P architecture, our approach uses different training objectives, primarily focusing on enforcing Cycle Edit Consistency (CEC). Specifically, we employ the CLIP ViT-L/14 model, integrated into SD-v1.5, to calculate the losses. By using single noise prediction across varying diffusion steps for forward and ˆt for reverse, both sampled between 0-1000 (as proposed in IP2P training), our model reduces computational overhead, respect to IP2P (please refer to Sec. 5.4), while maintaining consistency between forward and reverse edits. This reuse of the prediction enables efficient and accurate editing with fewer inference steps than IP2P, which improves both generalization and performance, as empirically demonstrated in Sec. 5.4. UIP2P is trained using the AdamW optimizer [22] with batch size of 768 over 11K iterations. The base learning rate is set to 5e-05. All experiments are implemented in PyTorch [28] and conducted on 16 NVIDIA H100 GPUs, with loss weights set as λCLIP = 1.0, λattn = 0.5, λsim = 1.0, and λrecon = 1.0. We select the best configuration based on the validation loss of LCEC. 5.2. Qualitative Results We compare UIP2P with state-of-the-art methods, including InstructPix2Pix [3], MagicBrush [50], HIVE [51], MGIE [9], and SmartEdit [16], on various datasets [3, 38, 39, 50]. The tasks include color modifications, object removal, and 6 Figure 4. Qualitative Examples. UIP2P performance is shown across various tasks and datasets, compared to InstructPix2Pix, MagicBrush, HIVE, MGIE, and SmartEdit. Our method demonstrates either comparable or superior results in terms of accurately applying the requested edits while preserving visual consistency. structural changes. UIP2P consistently produces highquality edits, applying transformations accurately while maintaining visual coherence. For example, in let the bird turn yellow, UIP2P provides more natural color change while preserving the birds shape. Similar improvements are observed in tasks like remove hot air balloons and change hat color to blue. These results demonstrate UIP2Ps ability to handle diverse edits, often matching or outperforming other methods, see Fig. 4. two methods, as suggested in SmartEdit [16], based on: (Q1) how well the edit matched the instruction and localization, and (Q2) how accurately the edit was applied to the intended region. The table summarizes the percentage of times each method was chosen as top performer for each question. UIP2P achieves the highest preference score, with MGIE and SmartEdit closely following. Unlike these methods, however, our approach introduces no latency penalty at inference time, offering both accuracy and efficiency. 5.3. Quantitative Results 5.3.1. User Study (Q1) Models Table 2. User Study. We conduct user study on Prolific Platform [30] with 52 to evaluate six participants methodsIP2P, MagicBrush, HIVE, MGIE, SmartEdit, and UIP2Pacross randomly sampled 15 image-edit instructions from various datasets [3, 38, 39, 50]. For each instruction, participants select the best 8% 12% 17% 18% 14% 13% 20% 19% 19% 18% 22% 20% IP2P MagicBrush HIVE MGIE SmartEdit UIP2P (Q2) 5.3.2. IP2P Test Dataset We evaluate our method on the IP2P test split, containing 5K image-instruction pairs. Following [3], we use CLIP image similarity for visual fidelity and CLIP text-image similarity to assess alignment with the instructions. Higher scores in both metrics indicate better performance (upper right corner) by preserving image details (image similarity) and effectively applying the edits (direction similarity). As shown in the plot, UIP2P outperforms IP2P across both In these experiments, the text scale sT is fixed, metrics. while the image scale sI varies from 1.0 to 2.2. 7 Settings Methods L1 L2 CLIP-I DINO CLIP-T Single-turn Multi-turn HIVE [51] InstructPix2Pix [3] UIP2P w/ IP2P Dataset UIP2P w/ CC3M Dataset UIP2P w/ CC12M Dataset HIVE [51] InstructPix2Pix [3] UIP2P w/ IP2P Dataset UIP2P w/ CC3M Dataset UIP2P w/ CC12M Dataset 0.1092 0.1122 0.0722 0.0680 0.0619 0.1521 0.1584 0.1104 0.1040 0. 0.0341 0.0371 0.0193 0.0183 0.0174 0.0557 0.0598 0.0358 0.0337 0.0323 0.8519 0.8524 0.9243 0.9262 0.9318 0.8004 0.7924 0.8779 0.8816 0.8857 0.7500 0.7428 0.8876 0.8924 0.9039 0.6463 0.6177 0.8041 0.8130 0. 0.2752 0.2764 0.2944 0.2966 0.2964 0.2673 0.2726 0.2892 0.2909 0.2901 (a) Zero-shot Quantitative Comparison on MagicBrush [50] test set. methods that are not fine-tuned on MagicBrush are presented. ages are iteratively edited from the initial images. Instruction-based editing In the multi-turn setting, target im- (b) Evaluation on the IP2P test dataset. UIP2P outperforms IP2P in both CLIP image similarity and CLIP text-image similarity metrics, demonstrating better visual fidelity and instruction alignment. Figure 5. Evaluation on MagicBrush and IP2P test datasets. 5.3.3. MagicBrush Test Dataset The MagicBrush test split contains 535 sessions (source images for iterative editing) and 1053 turns (individual editing steps). It uses L1 and L2 norms for pixel accuracy, CLIP-I and DINO embeddings for image quality via cosine similarity, and CLIP-T to ensure alignment with local text descriptions. As seen in Sec. 5.3.1, UIP2P perfoms the best for both singleand multi-turn settings. It is important to be noted that HIVE utilizes human feedback on edited images to understand user preferences and fine-tunes IP2P based on learned rewards, aligning the model more closely with human expectations. Section 5.3.1 also shows that increasing the number of samples in the training dataset and also training on real images provides better performance than training on the synthetic dataset, IP2P dataset. 5.4. Ablation Study Loss Functions. We conduct zero-shot evaluation on the MagicBrush test set (single-turn) to assess the effectiveness of different loss functions. Starting with the base configuration which contains LCLIP and Lrecon, we observe moderate performance across the same metrics. Adding Lsim loss allows the model to perform edits more freely, as the Base without it tends to create outputs similar to the input image. Finally, Lattn enhances the models focus on relevant regions and ensures that the region of interest remains consistent between the forward and reverse processes. Table 3. Ablation study on loss functions. Adding additional loss functions to the base loss functions enhances performance on the MagicBrush benchmark. Loss L1 L2 CLIP-I DINO CLIP-T Base 0.117 + Lsim 0.089 + Lattn 0.062 0.032 0.024 0.017 0.878 0.906 0. 0.806 0.872 0.904 0.309 0.301 0.296 Number of Steps. We analyze the effect of varying the number of diffusion steps during inference. Fewer steps reduce computational time but may affect image quality. Our experiments show that UIP2P maintains high-quality edits with as few as 5 steps, providing significant speedup without sacrificing accuracy. In contrast, IP2P requires more steps to achieve similar results. As shown in Fig. 6, UIP2P consistently outperforms IP2P in both quality and efficiency, particularly with fewer inference steps. Figure 6. Ablation study on the number of steps. UIP2P achieves high fidelity edits on the input image with fewer steps, whereas IP2P struggles to maintain quality. 6. Conclusion In this work, we present UIP2P, an unsupervised instruction-based image editing framework that leverages Cycle Edit Consistency (CEC) to ensure reversible and coherent edits without relying on ground-truth edited images. Some key components of our approach are Text and Image Direction Consistency, Attention Map Consistency, Reconstruction Consistency, and Unified Prediction with Varying Diffusion Steps, which together enforce consistency in both the image and attention space. Through experiments on real-image datasets, we show that UIP2P delivers highquality and precise edits while maintaining the structure of the original image. It performs competitively against existing methods, demonstrating the effectiveness of our approach, which scales efficiently across diverse editing tasks without the need for manually annotated datasets."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 1824, 2022, pages 1818718197. IEEE, 2022. 1 images. [2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV, pages 707723. Springer, 2022. 16 [3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 1, 2, 3, 5, 6, 7, 8, 14, 15, 16 [4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models In Advances in Neural Information are few-shot learners. Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 3 [5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 6, 15 [6] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. In The Eleventh International Conference on Learning Representations, 2023. 1, [7] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. VQGAN-CLIP: open domain image generation and editing with natural language guidance. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVII, pages 88105. Springer, 2022. 16 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 3 [9] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. 2, 3, 6, 12, 14 [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. 1 [11] Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. ACM Trans. Graph., 41(4):141:1141:13, 2022. 2, [12] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, et al. Instructdiffusion: generalist modeling interface for vision tasks. arXiv preprint arXiv:2309.03895, 2023. 3 [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. CoRR, abs/2208.01626, 2022. 1, 2, 3, 14 [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 3 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [16] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8362 8371, 2024. 3, 6, 7, 12, 14 [17] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. [18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. CoRR, abs/2210.09276, 2022. 1 [19] Krishnaram Kenthapadi, Himabindu Lakkaraju, and Nazneen Rajani. Generative ai meets responsible ai: Practical challenges and opportunities. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 58055806, 2023. 12 [20] Pavel Korshunov and Sebastien Marcel. Deepfakes: new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018. 12 [21] Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan Tran, Xiaogang Wang, and Hongsheng Li. Open-edit: Opendomain image manipulation with open-vocabulary instrucIn Computer Vision - ECCV 2020 - 16th European tions. Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI, pages 89106. Springer, 2020. 16 [22] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [23] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1145111461. IEEE, 2022. 1 [24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equa9 tions. In International Conference on Learning Representations, 2022. 1, 2, 16 [25] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. CoRR, abs/2211.09794, 2022. 2, [26] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, 2022, pages 1678416804. PMLR, 2022. 1 [27] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. 2 [28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 6 [29] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of In Proceedings of the IEEE/CVF Interstylegan imagery. national Conference on Computer Vision, pages 20852094, 2021. 2 [30] prolific. Prolific. https://www.prolific.com/, 2024. Accessed: 2024-09-24. 7, [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, 2021, pages 87488763. PMLR, 2021. 4, 15 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763, 2021. 2 [33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. 1 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pages 1067410685. IEEE, 2022. 1, 3, 6 [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition (CVPR), pages 2250022510, 2023. 1 [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. 1 [37] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. 6, [38] Jing Shi, Ning Xu, Trung Bui, Franck Dernoncourt, Zheng Wen, and Chenliang Xu. benchmark and baseline for language-driven image editing. In Computer Vision - ACCV 2020 - 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 - December 4, 2020, Revised Selected Papers, Part VI, pages 636651. Springer, 2020. 6, 7 [39] Jing Shi, Ning Xu, Yihang Xu, Trung Bui, Franck DerLearning by planning: noncourt, and Chenliang Xu. In Proceedings of Language-guided global image editing. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1359013599, 2021. 6, 7 [40] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-image translation. arXiv preprint arXiv:2203.08382, 2022. 2 [41] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 5 [42] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 5, [43] Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and Joost van de Weijer. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2 [44] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Mdp: generalized framework for text-guided image editing by manipulating the diffusion path, 2023. 2 [45] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023. 1 [46] Chen Henry Wu and Fernando De la Torre. latent space of stochastic diffusion models for zero-shot image editing and guidance. In ICCV, 2023. 2 [47] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in textto-image diffusion models. In Proceedings of the IEEE/CVF 10 Conference on Computer Vision and Pattern Recognition, pages 19001910, 2023. [48] Sihan Xu, Ziqiao Ma, Yidong Huang, Honglak Lee, and Joyce Chai. in text-guided diffusion for image manipulation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2 Cyclenet: Rethinking cycle consistent [49] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023. 1 [50] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionIn Advances in Neural Information guided image editing. Processing Systems, 2023. 2, 3, 6, 7, 8, 14, 15, 16 [51] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. HIVE: harnessing human feedback for instructional visual editing. CoRR, abs/2303.09618, 2023. 2, 3, 6, 8, 14, 16 [52] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 11 7. Appendix"
        },
        {
            "title": "Table of Contents",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . 7.1. Ethics Statement . . . . . . 7.2. Runtime Analysis . . . . . 7.3. Ablation Study on Loss Functions . . . . . 7.4. Discussion on Reduced DDIM Steps . . . . . 7.5. Additional Qualitative Results . . . . . . 7.6. Details of Competitor Methods 7.7. More Examples from Reverse Instructions . . . . . . . . . . . . 7.8. Cycle Edit Consistency Example . . . . . . . 7.9. Dataset Filtering . 7.10. Additional Quantitative Analysis on Mag- . . . . . icBrush Test . . . . . . . 7.11. User Study Setting . . . . . . 7.12. Additional Implementation Details . Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 12 12 12 13 14 14 15 15 15 16 7.1. Ethics Statement Advancements in localized image editing technology offer substantial opportunities to enhance creative expression and improve accessibility within digital media and virtual reality environments. Nonetheless, these developments also bring forth important ethical challenges, particularly concerning the misuse of such technology to create misleading content, such as deepfakes [20], and its potential effect on employment in the image editing industry. Moreover, as also highlighted by [19], it requires thorough and careful discussion about their ethical use to avoid possible misuse. We believe that our method could help reduce some of the biases present in previous datasets, though it will still be affected by biases inherent in models such as CLIP. Ethical frameworks should prioritize encouraging responsible usage, developing clear guidelines to prevent misuse, and promoting fairness and transparency, particularly in sensitive contexts like journalism. Effectively addressing these concerns is crucial to amplifying the positive benefits of the technology while minimizing associated risks. In addition, our user study follows strict anonymity rules to protect the privacy of participants. 7.2. Runtime Analysis Our method modifies the training objectives of IP2P by incorporating Cycle Edit Consistency (CEC) and additional loss functions. However, these changes do not affect the overall runtime. Inference time remains comparable to the original IP2P framework, as we retain the same architecture and model structure. Consequently, our approach introduces no additional complexity or overhead in terms of processing time or resource consumption. This gives UIP2P an advantage over methods like MGIE [9] and SmartEdit [16], which rely on large language models (LLMs) during inference in terms of runtime and resource consumption. 12 Additionally, as shown in Sec. 5.4, UIP2P requires fewer inference steps to achieve accurate edits. For instance, while IP2P typically uses more steps, e.g., from 50 to 100 steps, UIP2P can produce coherent results in as few as five steps. This reduction in steps leads to faster inference times, offering clear efficiency advantage without compromising on quality, especially in real-time or large-scale applications. 7.3. Ablation Study on Loss Functions We focused our ablation studies on Lsim and Lattn because these losses are additional components beyond the core LCLIP and Lrecon. The core losses are essential for ensuring semantic alignment and reversibility in Cycle Edit Consistency (CEC), forming the foundation of our method. Without LCLIP and Lrecon, the model risks diverging, losing its ability to preserve both the inputs structure and its semantic coherence during edits. Adding Lsim enables the model to perform edits more freely by encouraging alignment between image and textual embeddings, thereby expanding its capacity for complex and diverse transformations. On the other hand, Lattn refines the models ability to focus on relevant regions during edits, improving localization and reducing unintended changes in non-targeted areas. LCLIP is applied between the input image and the edited image to ensure semantic alignment with the edit instruction. The reconstructed image is already constrained by Lrecon, which enforces structural and semantic consistency with the input. Adding LCLIP to the reconstructed image would be redundant and could interfere with the reversibility objective. Our design does not apply LCLIP to the reconstructed image to preserve the focus on reversibility and prevent conflicting optimization objectives. 7.4. Discussion on Reduced DDIM Steps This observation is based on empirical results, as detailed in Number of Steps During Inference (Sec. 5.4). Specifically, we hypothesize that the CEC ensures strong alignment between forward and reverse edits, enabling the model to produce high-quality outputs even with fewer DDIM steps. Additionally, as shown in Algorithm 1 (Lines 4 and 8), our method uses the same denoising prediction across all timesteps to recover the image, which enhances efficiency. In contrast, IP2P does not optimize its losses in image space during training, limiting its ability to achieve comparable results with fewer DDIM steps. This reduction in DDIM steps contributes to improved scalability and makes our method more applicable in real-world scenarios where computational resources are often constrained. Figure 7. Qualitative comparison of our method with baseline models for various editing instructions. From left to right: Input image, edit instruction, and results from InstructPix2Pix, MagicBrush, HIVE, MGIE, SmartEdit, and our method. Our approach demonstrates superior fidelity and alignment with the provided instructions across diverse tasks, such as expression changes, color adjustments, object transformations, and creative edits. 7.5. Additional Qualitative Results moval, style changes, and complex scene edits. To further demonstrate the capabilities of our approach, we present additional qualitative comparisons in Fig. 7. These results showcase the performance of our method against several baseline models, including InstructPix2Pix, MagicBrush, HIVE, MGIE, and SmartEdit, across diverse set of editing instructions. These tasks range from simple edits, such as color adjustments and expression changes, to more challenging transformations, including object reThe comparison highlights that our method consistently achieves higher fidelity and better alignment with the provided instructions. For instance, when instructed to modify facial expressions, such as make the face happy, our method produces more natural and expressive results. Similarly, for color adjustments, such as make the color more green, our approach ensures vibrant and accurate edits that surpass the performance of baseline models. In more challenging scenarios, like turn the sunset into firestorm or 13 make it the Vatican, our method maintains the structural integrity of the original image while executing the desired transformations. Furthermore, in creative edits, such as put blue glitter on fingernails, our model demonstrates exceptional precision and attention to detail. 7.6. Details of Competitor Methods Our method offers significant advantages over competitors in both training and inference. Unlike supervised methods that rely on paired triplets of input images, edited images, and instructions, our approach eliminates the need for such datasets, reducing biases and improving scalability. For example, MagicBrush is fine-tuned on humanannotated dataset, while HIVE leverages Prompt-to-Prompt editing with human annotators, introducing dependency on labor-intensive processes. Furthermore, MGIE and SmartEdit rely on LLMs during inference, which significantly increases computational overhead. These distinctions highlight the efficiency and practicality of our approach, as it avoids the need for expensive human annotations and additional inference-time complexities. Like other editing methods, our approach can produce small variations for different random seeds but consistently applies the specified edit, eliminating the need for manual selection. To the best of our knowledge, the compared methods (e.g., MagicBrush, InstructPix2Pix) also do not involve manual selection. InstructPix2Pix [3] is diffusion-based model that performs instruction-based image editing by training on triplets of input image, instruction, and edited image1. The model is fine-tuned on synthetic dataset of edited images generated by combining large language models (LLMs) and Promptto-Prompt [13]. This approach relies on paired datasets, Inwhich can introduce biases and limit generalization. structPix2Pix serves as one of the key baselines for our comparison, given its supervised training methodology. HIVE [51] is an instruction-based editing model that finetunes InstructPix2Pix based on human feedback2. Specifically, HIVE learns from user preferences about which edited images are preferred, incorporating this feedback into the model training. While this approach allows HIVE to better align with human expectations, it still builds on top of InstructPix2Pix and does not start training from scratch. This limits its flexibility compared to methods like UIP2P, which are trained from the ground up. MagicBrush [50] fine-tunes the pre-trained weights of InstructPix2Pix on human-annotated dataset to improve real-image editing performance3. While this fine-tuning approach makes MagicBrush highly effective for specific tasks with ground-truth labels, it limits its generalizability compared to methods like UIP2P, which are trained from scratch. Moreover, MagicBrushs reliance on humanannotated data introduces significant scalability challenges, as obtaining such annotations is both costly and laborintensive. This dependency makes it less suited for broader datasets where large-scale annotations may not be feasible. MGIE [9] introduces large multimodal language model to generate more precise instructions for image editing4. Like InstructPix2Pix, MGIE requires paired dataset for training but uses the language model to improve the quality of the instructions during inference. However, this reliance on LLMs during inference adds computational overhead. In contrast, UIP2P operates without LLMs at inference time, reducing overhead while maintaining flexibility. SmartEdit [16] is based on InstructDiffusion, model already trained for instruction-based image editing tasks5. It introduces bidirectional interaction module to improve text-image alignment, but its reliance on the pre-trained InstructDiffusion limits flexibility, as SmartEdit does not start training from scratch. Additionally, SmartEdit depends on large language models (LLMs) during inference, increasing computational overhead. This makes SmartEdit less efficient than UIP2P in scenarios where real-time or large-scale processing is required. During evaluation, we use the publicly available implementations and demo pages of the baseline methods. Each baseline provides different approach to instruction-based image editing, and together they offer comprehensive set of methods for comparing the performance, flexibility, and efficiency of the proposed method, UIP2P. 7.7. More Examples from Reverse Instructions"
        },
        {
            "title": "Dataset",
            "content": "To demonstrate the versatility of our reverse instruction dataset, we provide examples with multiple variations of edits for two different input captions. Each caption has four distinct edits, such as color changes, object additions, object removals, and positional adjustments. This variety helps the model generalize across wide range of tasks and scenarios, as discussed in Sec. 4.2. The use of large language models (LLMs) to generate reverse instructions further enhances the flexibility of our dataset. These examples, along with others in Tab. 1, illustrate the diversity of edit types our model learns, enabling it to perform wide range of tasks across different real-image 1https : / / github . com / timothybrooks / instruct - pix2pix 2https://github.com/salesforce/HIVE 3https://github.com/OSU-NLP-Group/MagicBrush 4https://ml-mgie.com/playground.html 5https://github.com/TencentARC/SmartEdit 14 Table 4. Examples of Four Possible Edits for Two Different Input Captions. Our dataset generation process showcases the flexibility of the reverse instruction dataset by demonstrating multiple transformations for the same caption. Input Caption Edit Instruction Edited Caption Reverse Instruction dog sitting on couch car parked on the street change the dogs color to brown add ball next to the dog remove the dog move the dog to the floor change the car color to red add bicycle next to the car remove the car move the car to the garage brown dog sitting on couch dog sitting on couch with ball An empty couch dog sitting on the floor red car parked on the street car parked on the street with bicycle An empty street car parked in the garage change the dogs color back to white remove the ball add the dog back move the dog back to the couch change the car color back to black remove the bicycle add the car back move the car back to the street datasets. The reverse instruction mechanism ensures that the edits are reversible, maintaining consistency and coherence in both the forward and reverse transformations. 7.8. Cycle Edit Consistency Example We demonstrate CEC with visual example during inference. In the forward pass, the model transforms the input image based on the instruction (e.g., turn the forest path into beach). In the reverse pass, the corresponding reverse instruction (e.g., turn the beach back into forest) is applied, reconstructing the original image. This showcases the models ability to maintain consistency and accuracy across complex edits, ensuring that both the forward and reverse transformations align coherently. Additional examples, such as adding and removing objects, further emphasize UIP2Ps adaptability in diverse editing tasks. Figure 8 illustrates how our method ensures precise, reversible edits while maintaining the integrity of the original content. 7.9. Dataset Filtering We apply CLIP [31] to both the CC3M [37] and CC12M [5] datasets to calculate the similarity between captions and images, ensuring that the text descriptions accurately reflect the content of the corresponding images. Following the methodology used in InstructPix2Pix (IP2P) [3], we adopt CLIP-based filtering strategy with similarity threshold set at 0.2. This threshold filters out image-caption pairs that do not have sufficient semantic alignment, allowing us to curate dataset with higher-quality text-image pairs. For the filtering process, we utilize the CLIP ViT-L/14 model, which provides robust and well-established framework for capturing semantic similarity across text and images. By applying this filtering process, we ensure that only relevant and coherent pairs remain in the dataset, improvFigure 8. Forward and reverse edits are applied sequentially. ing the quality of training data and helping the model better generalize to real-world editing tasks. As result, the filtered CC3M dataset contains 2.5 million image-caption pairs, while the filtered CC12M dataset contains 8.5 million pairs. This careful curation of the dataset enhances the reliability of the training process without relying on human annotations, making it scalable for broader realimage datasets without the cost and limitations of humanannotated ground-truth datasets [3, 50]. 7.10. Additional Quantitative Analysis on MagicBrush Test In this section, we present the full quantitative analysis on the MagicBrush test set, including results from both global description-guided and instruction-guided models, 15 as shown in Tab. 5. While our method, UIP2P, is not fine-tuned on human-annotated datasets like MagicBrush, it still achieves highly competitive results compared to models specifically fine-tuned for the task. In particular, UIP2P demonstrates either the best or second-best performance in key metrics such as L1, L2, and CLIP-I, even outperforming fine-tuned models in several cases. This highlights the robustness and generalization capabilities of UIP2P, showing that it can effectively handle complex edits without the need for specialized training on real datasets. These results further validate that UIP2P delivers high-quality edits in variety of contexts, maintaining competitive performance against fine-tuned models on the MagicBrush dataset, which is human-annotated. Table 5. Quantitative comparison on MagicBrush [50] test set. In the multi-turn setting, target images are iteratively edited from the initial source images. Best results are in bold. Settings Methods L1 L2 CLIP-I DINO CLIP-T Global Description-guided Open-Edit [21] VQGAN-CLIP [7] SD-SDEdit [24] Text2LIVE [2] Null Text Inversion [25] 0.1430 0.2200 0.1014 0.0636 0.0749 0.0431 0.0833 0.0278 0.0169 0.0197 Single-turn Instruction-guided HIVE [51] w/ MagicBrush [50] InstructPix2Pix [3] w/ MagicBrush [50] UIP2P w/ IP2P Dataset UIP2P w/ CC3M Dataset UIP2P w/ CC12M Dataset 0.1092 0.0658 0.1122 0.0625 0.0722 0.0680 0.0619 0.0341 0.0224 0.0371 0.0203 0.0193 0.0183 0. 0.8381 0.6751 0.8526 0.9244 0.8827 0.8519 0.9189 0.8524 0.9332 0.9243 0.9262 0.9318 Global Description-guided Open-Edit [21] VQGAN-CLIP [7] SD-SDEdit [24] Text2LIVE [2] Null Text Inversion [25] 0.1655 0.2471 0.1616 0.0989 0.1057 0.0550 0.1025 0.0602 0.0284 0. Multi-turn Instruction-guided HIVE [51] w/ MagicBrush [50] InstructPix2Pix [3] w/ MagicBrush [50] UIP2P w/ IP2P Dataset UIP2P w/ CC3M Dataset UIP2P w/ CC12M Dataset 0.1521 0.0966 0.1584 0.0964 0.1104 0.1040 0.0976 0.0557 0.0365 0.0598 0.0353 0.0358 0.0337 0.0323 0.8038 0.6606 0.7933 0.8795 0.8468 0.8004 0.8785 0.7924 0.8924 0.8779 0.8816 0.8857 0.7632 0.4946 0.7726 0.8807 0.8206 0.7500 0.8655 0.7428 0.8987 0.8876 0.8924 0. 0.6835 0.4592 0.6212 0.7926 0.7529 0.6463 0.7891 0.6177 0.8273 0.8041 0.8130 0.8235 0.2610 0.3879 0.2777 0.2424 0.2737 0.2752 0.2812 0.2764 0.2781 0.2944 0.2966 0.2964 0.2527 0.3845 0.2694 0.2716 0.2710 0.2673 0.2796 0.2726 0.2754 0.2892 0.2909 0. 7.11. User Study Setting We conduct user study with 52 anonymous participants on the Prolific Platform [30], presenting them with 30 questions. Each question shows participants six edited images generated by different methods, alongside their corresponding input images and edit instructions. Participants are tasked with evaluating the effectiveness of the edits in achieving the specified outcome (Q1) and assessing the ability of the editing method to preserve the details in areas not targeted by the instruction (Q2). For example, as shown in Fig. 9, where the edit instruction is make the face happy, participants are asked to determine which of the six edited images (a-f) best satisfies the instruction while maintaining the fidelity of irrelevant details in the scene. By aggregating responses from participants, we gather insights into the preferred methods for both accurate editing and detail preservation. This feedback provides fair comparison between methods, complementing the quantitative analysis, and informs the development and refinement of more advanced image editing techniques. Figure 9. User Study Setup. The input image is shown alongside randomly ordered edited images generated by different methods (a)-(f) based on the edit instruction, make the face happy. Participants are asked to select the best two methods that match the editing effect and those that best preserve irrelevant instruction regions. 7.12. Additional Implementation Details 7.12.1. Code Implementation Overview Our UIP2P implementation with CEC builds on existing frameworks for reproducibility: Base Framework: The code is based on InstructPix2Pix6, which provides the foundation for instructionbased image editing. Adopted CLIP Losses: We adopted and modified CLIPbased loss functions from StyleGAN-NADA7 to fit CEC, improving image-text alignment for our specific tasks. 7.12.2. Algorithm Overview In this section, we explain the proposed method, UIP2P, which introduces unsupervised learning for instructionbased image editing. The core of our approach is the Cycle Edit Consistency (CEC), which ensures that edits are coherent and reversible when cycled through both forward and reverse instructions. The algorithm consists of two key processes: Forward Process: Starting with an input image and forward edit instruction, noise is first added to the image. The model then predicts the noise, which is applied to reverse the noise process and recover the edited image (see Algorithm 1, lines 2-4). Backward Process: Given the forward-edited image and reverse edit instruction, noise is applied again. The model predicts the reverse noise, which is used to undo the edits and reconstruct the original image. This ensures that the reverse edits are consistent with the original input image (see Algorithm 1, lines 6-8). CEC is applied between the original input image, the forward-edited image, and the reconstructed image, along with their respective attention maps and captions (see Algorithm 1, line 10). The LCEC function guides the models learning through backpropagation (see Algorithm 1, lines 12-13). 6https : / / github . com / timothybrooks / instruct - pix2pix 7https://github.com/rinongal/StyleGAN-nada 17 Algorithm 1 Unsupervised Instruction-Based Image Editing (UIP2P) with CEC Require: Image Iinput (input image), Forward edit instruction , Reverse edit instruction R, Noise levels (forward), ˆt (backward), Model , Loss function LCEC, Noise function , Input caption Tinput, Edited caption Tedit Ensure: Edited image Iedit, Reconstructed image Irecon 1: Forward Process: 2: zt (Iinput, t) Iinput Add noise to the input image 3: ˆϵF , Af (ztIinput, ) Model predicts forward noise ˆϵF and extracts attention map Af 4: Iedit Apply(ˆϵF , zt, t) Apply predicted noise ˆϵF to reverse the process of obtaining zt and recover Iedit 5: Backward Process: 6: zˆt (Iedit, ˆt) image Iedit Add noise ˆt to the forward-edited 7: ˆϵR, Ar (zˆtIedit, R) Model predicts reverse noise ˆϵR and extracts attention map Ar 8: Irecon Apply(ˆϵR, zˆt, ˆt) Apply predicted noise ˆϵR to reverse the process of obtaining zˆt and recover Irecon 9: Cycle Edit Consistency Loss: 10: LCEC L(Iinput, Iedit, Irecon, Af , Ar, Tinput, Tedit) Compute CEC loss using Iinput, Iedit, Irecon, attention maps Af , Ar, input text Tinput, and edited text Tedit 11: Update Model: 12: Backpropagate the loss LCEC and update the model 13: Repeat until convergence"
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Google Switzerland",
        "Technical University of Munich"
    ]
}