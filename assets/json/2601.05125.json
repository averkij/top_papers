{
    "paper_title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "authors": [
        "Ignacio de Rodrigo",
        "Alvaro J. Lopez-Lopez",
        "Jaime Boal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 5 2 1 5 0 . 1 0 6 2 : r VERSE: Visual Embedding Reduction and Space Exploration Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding Ignacio de Rodrigoa,, Alvaro J. Lopez-Lopeza, Jaime Boala aInstitute for Research in Technology, ICAI School of Engineering, Comillas Pontifical University, Calle Rey Francisco, 4, Madrid, 28008, Madrid, Spain Abstract This work introduces VERSE, methodology for analyzing and improving VisionLanguage Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral. Keywords: Visually-rich Document Understanding, Vision-Language Models, Visual Embeddings, Interpretability, Explainability Corresponding author Email addresses: iderodrigo@comillas.edu (Ignacio de Rodrigo), allopez@comillas.edu (Alvaro J. Lopez-Lopez), jboal@comillas.edu (Jaime Boal) 1. Introduction VisionLanguage Models (VLMs) have recently demonstrated remarkable capabilities in multimodal contexts, enabling them to perform complex tasks that combine visual and textual understanding. Within this family, models for Visually-rich Document Understanding (VrDU) aim to interpret structured documents by integrating layout, textual, and visual features into unified representation or embedding. Understanding how these embeddings are organized, how they relate to one another, and what latent patterns emerge brings us closer to uncovering each models underlying semantic structure [1]. Moreover, optimizing the spatial arrangement of embeddings relative to their neighbors not only enhances pattern detection and interpretability in the latent space but also leads to improved performance in downstream tasks [2]. VLMs are generally trained on large-scale, real-world multimodal corpora, but rely on synthetic data in domains where access to real samples is limited [3]. In such cases, visual quality is often evaluated from human perspective, emphasizing whether the samples appear photorealistic or plausible. However, visual realism does not necessarily imply usefulness for the model itself. From the models perspective (its semantic structure representation), what truly matters is whether synthetic sample lies within the same distribution as real samples in the visualsemantic embedding space. This perspective motivates paradigm shift in the way synthetic training data are evaluated: rather than judging visual quality from an anthropocentric point of view, data quality should be assessed through the models own internal representations (see Figure 1). To translate this idea into practical methodology, we propose VERSE (Visual Embedding Reduction and Space Exploration). VERSE is methodology designed to analyze, interpret, and leverage the structure of the visual embedding space of VLMs. VERSE serves three main purposes: (i) model interpretability, by assessing whether the visual embeddings of given model form coherent patterns aligned with the target task; (ii) explainability and model boosting, by identifying error-inducing regions in the embedding space and enriching training datasets with representative samples from these regions; and (iii) human-centered explainability, aimed at revealing the visual and structural features that drive the patterns underlying model behavior and task performance. The methodology is validated on the MERIT Dataset [4] and its real2 Figure 1: Paradigm shift proposed in this work. Traditionally, the quality of synthetic images in dataset is assessed from an anthropocentric perspective, answering the question of whether such images appear photorealistic. In contrast, this work proposes evaluating the images from the models perspective, which entails analyzing their visual embeddings to determine whether synthetic image lies within the target distribution, as perceived by the model itself. world counterpart MERIT Secret, focusing on key information extraction as sequence-generation task in Spanish. Our main contributions are as follows: We present novel methodology for visual embedding analysis and latent-space visualization, named VERSE (Visual Embedding Reduction and Space Exploration). We employ VERSE to assess models validation for specific task (VrDU in this case) by examining the structure and patterns present in its reduced embedding space (RES). For suitable models, VERSE identifies problematic clusters and helps to reveal which visual features should be modified to enhance performance within them. Applying VERSE yields measurable performance boost, enabling onpremise models to match (and in some cases surpass) the results of leading SaaS-based solutions such as GPT-4 and Pixtral [5]. The remainder of this paper is organized as follows. Section 2 reviews the main advances in VrDU, covering both model and dataset perspectives. Section 3 introduces the VERSE methodology. Section 4 presents the experimental evaluation, showing how VERSE improves performance by addressing error-inducing regions in the latent space and benchmarking on-premise 3 models against SaaS-based solutions. Finally, Section 5 discusses the results, and Section 6 concludes the paper by summarizing the contributions and outlining future directions. 2. Related Work The document analysis domain has emerged as one of the most impactful areas of applied Artificial Intelligence, attracting both industrial and academic attention. This growing interest has led to broad range of research efforts (from model architectures to dataset generation) aimed at enhancing the performance and robustness of VrDU systems. Among the most influential model families in this field is the LayoutLM [6] series, which represents OCR-dependent VLMs. The original LayoutLM model was pre-trained on tasks such as document classification and form understanding, establishing the foundations for multimodal document analysis. LayoutLMv2 [7] introduced additional pre-training objectives (text-image alignment and text-image matching) designed to improve the interaction between visual and textual modalities. Building upon this, LayoutXLM [8] extended the approach to multilingual settings using corpus covering 53 languages, and released the XFUND dataset [9] as cross-lingual benchmark. Later, LayoutLMv3 [10] refined cross-modal representations by introducing WordPatch Alignment task, promoting stronger associations between visual patches and their corresponding textual tokens. Despite the strong performance of this family, OCR dependence remains critical limitation. On one hand, OCRs struggle with challenging real-world conditions such as complex layouts or degraded text [11]; on the other, they impose rigid reading order that constrains token sequence modeling. XYLayoutLM [12] explicitly addresses this issue by proposing token reordering mechanism based on spatial coordinates. In contrast, recent approaches have moved toward eliminating the OCR dependency altogether. Donut [13] introduced an end-to-end OCR-free architecture, marking paradigm shift in document understanding. Building upon this direction, DocParser [14] enhances character-level discrimination, while UDOP [15] broadens the modeling scope by incorporating inpainting capabilities for coherent text editing and region reconstruction in document layouts. More recently, an alternative line of work has focused on instruction-tuned multimodal models. These architectures extend LLMs with visual encoders and cross-modal fusion mechanisms, enabling end-to-end reasoning across 4 text and images. PaliGemma [16] offers an efficient, open-source variant of PaLI [17], combining SigLIP [18] encoder with Gemma-based [19] decoder. LLaVA [20] integrates CLIP [21] with Vicuna [22] backbone, emphasizing conversational and general-purpose multimodal tasks. In contrast, Idefics2 [23] and Idefics3 [24] provide larger-scale, instruction-aligned systems trained on diverse imagetext corpora. Finally, all these developments in VrDU tasks have led to the emergence of toolkits such as Docling [25], developed by IBM, which illustrates the growing industrial interest in this field. This ecosystem is also accelerating the development of new ad hoc strategies for document processing (such as table tokenization [26]) and fostering the release of open-source models like Smol-Docling [3]. These advances exemplify the increasing collaboration between industry and the open-source community, highlighting the pivotal contribution of Hugging Face to the deep learning ecosystem. In parallel with the rapid development of VLMs, numerous datasets have been introduced to support progress in VrDU for multiple tasks. The NNE dataset [27] focuses on Named Entity Recognition (NER), providing finely nested labels that enrich the semantic and syntactic context of each token. FUNSD [28] is another widely used dataset for token classification, consisting of real scanned documents annotated with text, layout, and semantic information in English. To overcome the linguistic limitation of previous corpora, XFUND [9] extends this approach to include samples in up to seven languages, offering multilingual benchmark for VrDU. Domain-specific datasets have also emerged. CORD [29] and SROIE [30] target purchase receipt analysis, supporting tasks such as text localization and key information extraction. PublayNet [31] focuses on document layout analysis, providing digitally born images of scientific articles. However, since these are not scanned or photographed, domain gap arises when applying trained models to real-world documents. Similarly, DocVQA [32] and its extensions (PDF-VQA [33], SlideVQA [34], and InfographicVQA [35]) expand document understanding into visual question answering, multi-page reasoning, and numerical inference tasks, pushing VrDU toward more complex multimodal reasoning challenges. Other approaches have aimed to minimize human intervention in the dataset creation process. For instance, CRAFT [36] enables the construction of textual datasets in few-shot setting, where representative samples are used to retrieve semantically related examples from large corpora via embedding-based similarity. In cases where no suitable examples exist, con5 Figure 2: VERSE methodology components. The validation dataset MERIT secret (A) is processed by models visual encoders (B) to obtain visual embeddings. F1 scores are obtained after inference on the validation dataset with fine-tuned models on the different MERIT Dataset versions. High-dimensional visual embeddings are reduced to lowerdimensional space (D), known as the Reduced Embedding Space. This space provides better model interpretability, while overlaying the samples visual features and the F1 scores enhances model explainability (E). Sections 3.1.A to 3.1.E explain in further detail the components (A-E) involved in the methodology. trolled data generation methodologies (such as MERIT [4]) are essential for building multimodal datasets with highly structured content. 3. Methodology The methodology proposed in this paper, VERSE, builds upon the inspection, reduction, and visual analysis of visual embeddings in VLMs with three main objectives: (i) interpretability to assess models potential for executing given task (sequence generation in this case) in Subsection 3.2; (ii) from human-centered perspective, explainability to uncover the relevant features embedded in the synthetic training samples in Subsection 3.3; and (iii) explainability to identify model-specific error-inducing features and use them to construct enriched training datasets that enhance model performance in Subsection 3.4. The overall workflow of the VERSE methodology is depicted in Figure 2 and its building blocks are explained in Subsections 3.1.A-3.1.E. 6 Figure 3: Validation dataset: MERIT Secret, dataset with real and anonymized samples, with 10 categories subdivided by school. Table A.8 shows the relevant features extracted in MERIT Secret. 3.1. VERSE building blocks A. Validation dataset The validation dataset corresponds to MERIT Secret, dataset of realworld samples provided to the authors under NDA. It comprises 152 images of student transcripts of records in Spanish, captured with different devices and under varying lighting conditions. The samples originate from 10 different schools, each with its own template and formatting conventions, though minor variations may also appear within the same institution. The dataset characteristics can be grouped into two categories: (i) intrinsic document properties, such as the arrangement of elements (tables, headings, images), or the representation of grades (numeric or alphanumeric); and (ii) environmental factors, external to the document itself, including wrinkles, shadows, or artifacts introduced during the anonymization process. As Figure 2 shows, MERIT Secret samples are labelled attending to these human-observed features. Table A.8 exposes and describes them. Figure 3 illustrates the distribution of samples across schools and provides, in privacy-preserving manner, thumbnail for each subset that conveys the structural and visual characteristics of the documents. The validation dataset serves as the source for several data signals in the VERSE methodology: (i) the F1 score computed for the 152 samples; (ii) the corresponding embedding produced by models for every image; and (iii) the observable features extracted as human-interpretable labels. 7 B. Models The models used are VLMs available on Hugging Face. These models are based on the transformer architecture and incorporate visual encoder to process both text and images as input. Table 1 summarizes their main features. The visual embeddings for each model are extracted from the last hidden state of the visual block. All training runs performed with the different versions of the training dataset are conducted with the visual backbone frozen, ensuring that embedding comparisons and dimensionality reduction are carried out over consistent visual semantic structure. Otherwise, each fine-tuning process would introduce changes in the underlying visual representation. Within the VERSE methodology, the models are involved in two key stages: (i) obtaining the visual embedding for each sample in the validation set from the off-the-shelf model, and (ii) computing the F1 score for each sample using the fine-tuned model. Model/ HF repo ID Visual Encoder (VE) Is VE frozen in model P-T? Model P-T Datasets Donut [13] naver-clova-ix/donut-base Idefics2 [23] HuggingFaceM4/idefics2-8b PaliGemma [16] google/paligemma-3b-pt-224 LLaVA [20] LLaVA-hf/LLaVA-1.5-7b-hf Swin Transformer [37] False Synth-Dog [13] docs SigLIP ViT-So400M [18] SigLIP ViT-So400M [18] False, LoRA adaption False CLIP ViT-L/14 [21] True OBELICS [38]a OCR-IDL [39], PDFA. WebLI [16], CC3M-35L [40], OpenImages [41], WIT [42], VQ2A-CC3M [43]b N/A Interleaved imagetext documents from OBELICS. Authors use specific subsets, but they do not specify what their content is. Table 1: Summary of Visual Encoders (VE). Models whose visual encoders are unfrozen (Donut and Idefics2) and pre-trained (P-T) on document-like datasets tend to produce richer and more structured visual embedding spaces, as shown in Figure 6. In contrast, PaliGemma relies more heavily on general-purpose, object-centric pre-training data, which is less aligned with document-specific visual cues. C. Training dataset The training dataset corresponds to the Spanish partition of the MERIT Dataset. To analyze how the models respond to different visual stimuli, we generate different versions from MERIT vanilla (Figures 4.C and D). These data augmented versions introduce additional granularity along the vanilla spectrum of samples. Figure 4.A illustrates this expansion of dataset versions and their relative positioning (from qualitative perspective) according to 8 Figure 4: Training samples used. We employ the Spanish-language subsets of the MERIT Dataset, across its different versions (A). These versions are detailed in Table 2). Each version comprises data from seven different schools (B). New versions complement the vanilla MERIT Dataset, composed of digital document samples (C) and their renderized versions (D). the increasing amount and quality of visual information. Table 2 summarizes the characteristics of each version. Figure 4.B depicts, for given version, the subdivision of the dataset into subsets organized by school. Each subset, corresponding to specific institution, exhibits distinctive visual, layout, and textual features. These characteristics show varying degrees of alignment with those of the validation dataset (MERIT Secret). In this context, within the VERSE methodology, each model is iteratively fine-tuned using specific version or composition of the MERIT dataset as training data. D. Embedding reduction In VERSE, embeddings are extracted from the off-the-shelf (pre-trained) versions of the models. The visual encoders in VLMs process each input image by dividing it into [nm] patches, which are then progressively reduced to obtain their latent representations. This reduction may be accompanied by re-projection into the input space of the textual encoder to meet its dimensionality requirements. Conceptually, each patch is processed, resulting in [nmL] tensor. In VERSE, an average pooling operation is applied to reduce this tensor to final vector of size [1L], so that each image is 9 Version Description MERIT Dataset key Digital Paragraph Digital Line Digital Digital Rotation Digital Zoom Rendered Plain text, no structure or visual features (single paragraph). Line breaks per subjectgrade pair; minimal structure, no visuals. includes structure and Base version; visuals (headings, stamps, signatures). Rotated samples; black padding fills corners. Zoomed-out samples; black padding surrounds content. Blender-rendered samples with 3D mesh, lighting, background, textures. es-digital-paragraph-degradation-seq es-digital-line-degradation-seq es-digital-seq es-digital-rotation-degradation-seq es-digital-zoom-degradation-seq es-render-seq Table 2: Overview of the visual features incorporated in the different versions of the MERIT dataset. condensed into single embedding (thus obtaining model-specific representation for each sample). Figure 5 shows the embedding obtention process. We use the frozen off-the-shelf version of the models to produce these embeddings. The visual embeddings used in this work have L-dimensional representations on the order of several thousand. To obtain human-interpretable graphical representation, these embeddings must therefore be reduced. Figure 6 shows the Reduced Embedding Spaces (RES) for the Idefics2 [23] (A) and LLaVA [20] (B) models. Table 3 summarizes the main characteristics of the embeddings produced by each model. Model Visual Encoder Visual Output Dimensionality [L] Visual-to-Text Projection Projection Module Location Donut [13] Idefics2 [23] PaliGemma [16] LLaVA [20] Swin Transformer [37] SigLIP ViT-So400M [18] SigLIP ViT-So400M [18] CLIP ViT-L/14 [21] 1024 1152 4096 1152 1024 Linear Outside visual module MLP + Perceiver Within visual module Outside visual module Linear Outside visual module Linear Table 3: Summary of visual encoders (VE) and their embedding outputs across the evaluated models. Idefics2 [23], which incorporates its projection mechanism directly within the visual module, produces richer and more structured visual embeddings (see Figure 6). All embeddings were extracted as the last hidden state of each models visual module. E. Clustering As result of dimensionality reduction, models with sufficiently rich visual representations for the target task (sequence generation for Visuallyrich Document Understanding, VrDU) exhibit well-defined clustered regions 10 Figure 5: Qualitative illustration of the pipeline used to obtain single visual embedding per image. The procedure is encoder-specific: Donut, PaliGemma, and LLaVA employ lightweight MLP projection to align visual and textual embedding dimensions, whereas Idefics2 incorporates more advanced projection mechanism within the vision encoder. For consistency across models, we extract visual embeddings from the output of the vision block in all cases. Figure 6: Graphical representation (PC1 vs. PC2) of the Reduced Embedding Space (RES) obtained for Idefics2 [23] (A) and LLaVA [20] (B). The spaces were computed by applying PCA to the 152 embeddings from the validation dataset. VERSE enables visual assessment of the models visual-world representation quality: well-defined and separable clusters (A) indicate stronger representational capacity and higher task potential, whereas disorganized or highly entangled RES (B) reflects weaker visual discrimination and poorer downstream performance. 11 within the embedding space (Figure 6). Analyzing these clusters deepens model interpretability (Section 3.2), while overlaying complementary information (such as F1 scores or the visual and structural attributes summarized in Table A.8) enhances explainability (Sections 3.4 and 3.3). 3.2. Interpretability and model feasibility Dimensionality reduction through PCA reveals distinct clustering patterns. The degree of cluster definition varies depending on the model from which the embeddings are derived. Figure 6 illustrates the clusters projected onto the first two principal components for Idefics2 [23] (A) and LLaVA [20] (B). Since we are working within highly specific domain, we are not interested in general or isotropic embedding spaces [44], but rather in highly structured space that leverages dominant directions. To validate that the structure of the embedding space is preserved after dimensionality reduction, trustworthiness and proximity metrics were computed between the original high-dimensional embeddings and their PCAreduced projections. Table 4 reports both metrics, together with the Silhouette score (computed over the reduced embedding space) for each model. Model Trust. [0, 1] Prox. [0, 1] K-Means clusters Radius µ, [min, max] Density µ, [min, max] Silh. [1, 1] RES Idefics2 [23] Donut [13] PaliGemma [16] LLaVA [20] 0.96 0.99 0.93 0. 0.98 0.99 0.95 0.97 7 4 5 5 0.30 [0.210.47] 0.46 [0.360.58] 0.40 [0.320.57] 0.58 [0.421.02] 243 [69476] 98 [76150] 159 [19253] 51 [872] 0.63 0.50 0.38 0.35 Table 4: Quantitative analysis of the Reduced Embedding Space (RES). Trust. (Trustworthiness) and Prox. (Proximity) measure how well the local neighborhood structure is preserved after dimensionality reduction. K-Means clusters indicates the number of clusters automatically detected. Radius represents the average and range of cluster radii, providing measure of cluster compactness, while Density reflects data concentration. Silh. (Silhouette score) quantifies the degree of cluster separability: higher values indicate that samples are well matched to their own cluster and distant from others, meaning more structured and discriminative embedding space. Finally, RES summarizes the qualitative richness of the models visual embedding space. Based on these metrics (especially the Silhouette score) Idefics2 [23] and Donut [13] exhibit well-structured and semantically meaningful embedding spaces, whereas PaliGemma [16] and LLaVA [20] display weaker cluster separation and lower representational quality. VERSE proposes qualitative and quantitative inspection of the reduced embedding space to assess models potential to solve the target task. better cluster definition implies that the models can distinguish samples based 12 on the discriminative features, which is interpreted as richer understanding of the problem. Idefics2 [23] and Donut [13] are identified as the most suitable models, while PaliGemma [16] and LLaVA [20] are ruled out at this stage. 3.3. Explainability and exploration of the reduced embedding space (RES) The projection performed by PCA entails an intrinsic loss of information and high degree of entanglement among the principal components of the RES. Moving along single component does not correspond to variations in one specific feature but rather in combination of several. Nevertheless, overlaying visual features on the RES reveals clearly distinguishable clusters, with no apparent mixing of samples that exhibit different visual properties. This behavior indicates that the models (Donut [13] and Idefics2 [23]) are able to represent the visual structure of the data coherently, suggesting that their visual representations are sufficiently rich for the target task. Figure 7 shows some visual features and their distribution across the Donuts [13] RES. Most of these features are discrete and describe macrovisual properties of the documents, such as the number of columns (Figure 7.A), the vertical arrangement of information blocks (Figure 7.C), or the table position within the page (Figure 7.F and Figure 7.G). These features appear well-defined in the RES, indicating that the model can distinguish relevant structural patterns. On the other hand, for features not related to macroscopic document structure (such as zoom level, Figures 7.E and J) show higher degree of overlap between regions. Despite this, Donut [13] is still capable of identifying low-zoom areas, which later correspond to low-F1 regions (see Section 3.4). Figure 8 illustrates the same analysis for Idefics2 [23], using the same set of features considered for Donut [13]. Again, macro-level features (such as the number of columns or the vertical distribution of information, Figures 8.A and C) enable the formation of well-defined clusters, while features related to zoom exhibit more entangled behavior. Overall, despite the high degree of feature entanglement present in principal components, both reduced spaces show that each cluster is locally driven by specific combination of features. This behavior reinforces the interpretation that the model organizes its embedding space according to coherent visual patterns, mostly governed by macroscopic document layout properties. Consequently, the most influential features are those that describe the 13 Figure 7: Representation of visual features projected onto Donuts [13] RES. Each subplot shows specific document feature distribution across PC1 vs. PC2. Number of columns (A), layout type (B), number of vertical information blocks (C), table complexity (D), row height / image height ratio (zoom) (E), table vertical (F) and horizontal (G) position, vertical density (H), table grid presence (I), and row width / image width ratio (zoom)(J). Figure 8: Representation of visual features projected onto Idefics2s [23] RES. Each subplot shows specific document feature distribution across PC1 vs. PC2. Features exposed are the same as the ones in Figure 7. 14 Figure 9: Inference results for Donut [13] (A) and Idefics2 [23] (B). For each pre-trained model, the horizontal axis shows the different versions of the training dataset ordered by increasing visual richness (new versions include green mark), while the vertical axis represents the F1 score obtained for each fine-tuned version. Both models exhibit clear sensitivity to the increasing level of visual richness. macrostructural design of the document (such as column arrangement, table position, or layout type) whereas lower-level details (e.g., the presence of stamps or signatures) have lesser impact on the organization of the reduced spaces. 3.4. Explainability and model boosting VERSE first performs training sweep in which the models are finetuned using progressively richer visual versions of the MERIT dataset (see Figure 4). Figure 9 shows the corresponding F1 scores on the validation dataset (MERIT Secret), for each fine-tuned version of Donut [13] (A) and Idefics2 [23] (B), ordered from the lowest to the highest level of visual richness (perceived by human). Figure 9 shows an important first insight: models achieve competent performance when trained with samples that do not replicate human interpretation of photorrealism (rendered version) but extract relevant information from versions including simple data augmentation techniques (rotation and specially, zoom). On the other hand, although Figure 9 illustrates the sensitivity of the different Donut [13] and Idefics2 [23] versions (trained with progressively richer visual inputs) when evaluated on the validation set, it does not reveal (i) how individual samples are distributed with respect to their F1 scores, or (ii) whether specific features are associated with poor per15 Figure 10: Reduced Embedding Space (RES) for Donut [13] (PC1 vs. PC2). Higher F1 values are shown in green, while lower ones appear in red. Each subplot includes, in the lower-left corner, representative training sample used for fine-tuning and subset of training examples shown in purple. The visual richness of the training data increases progressively from to F, causing the corresponding embeddings to transition from concentrated regions to more dispersed distribution across the RES. formance. To address this, VERSE analyzes the visual embeddings and their representation within the reduced embedding space (RES). Figure 10 shows, for each fine-tuned version of Donut [13], the distribution of F1 scores over the RES. Since the embeddings were extracted from the pre-trained model and the fine-tuning process was carried out with the visual encoder frozen, the embedding map remains consistent across all versions. Figure 10 reveals the emergence of distinct clusters in Donuts [13] Reduced Embedding Space (RES) 1 This structure becomes evident once the model is fine-tuned on digital samples (Figure 10.C). Some clusters consistently achieve higher F1 scores, while others perform systematically worse than the rest of the regions. Figure 11.A highlights the two problematic regions identified for Donut [13] (referred to as cluster and cluster ). In Figure 11, cluster is characterized by vertical distribution of in1VERSE is also applied to Idefics2 [23] in Appendix B. 16 Figure 11: Analysis of Donuts [13] Reduced Embedding Space (RES). Two low-performing regions are identified: region A, characterized by small number of information blocks and high table complexity, primarily affected by an external factor (the low zoom level); and region B, driven by intrinsic document properties such as table structure and alphanumeric grading, but also affected by low zoom level. formation blocks located both at the top and bottom of the document, with reduced number of blocks (two in total). However, although these characteristics help to understand which visual features Donut [13] can distinguish in its RES, they are not the features inducing poor performance, as they are already present in the training dataset (starting from the digital version). Cluster can also be discriminated based on the row_h/image_h index (see definition in A.8), as this region corresponds to very low values of that index. We conduct the additional research in Figure 12 to analyze how zoom affects model performance. From this analysis, it is concluded that the performance degradation in cluster is associated with an external feature of the document structure (namely, the zoom level). Conversely, in addition to being located in low-zoom region, cluster is segmented by intrinsic document characteristics, such as table structure and the way grades are represented. Specifically, the samples within this region exhibit alphanumeric grading systems (combining letters and numbers) and template layout with two tables per page. 3.4.1. Extrinsic document feature analysis: zoom level We evaluate Donuts sensitivity to zoom during training using four different zoom levels: 0.25, 0.50, 0.625, and 0.75. Figure 12.A illustrates the effect of the zoom level applied to the training samples on Donuts inference performance. At low zoom levels, the model fails to learn effectively, as it cannot extract textual information from low-resolution images. As the zoom level increases, additional regions emerge in the Reduced Embedding Space (RES) that benefit from improved visual clarity. Focusing on cluster A, the best performance is achieved at zoom level of 0.625, while both lower (0.50) and higher (0.75) levels lead to performance degradation. This pattern suggests the existence of an optimal zoom range (sufficiently challenging to promote robust feature learning, yet not so extreme that the model fails to extract meaningful information). Figure 12.B presents an alternative approach. Instead of training with fixed zoom level, we train with uniform distribution of zoom values between 0.3 and 1.0. This variability introduces greater visual diversity in the training data, which proves beneficial for generalization. This strategy is particularly benefitial for clusters and B. We quantitatively compare the performance of these training strategies across three regions: the entire validation set, cluster A, and cluster B. Figure 12.C shows that Donut [13] achieves its best results across all regions when trained on samples with distributed range of zoom levels (0.31.0). This behavior suggests that significant portion of Donuts [13] strong performance on rendered samples (Figure 10.F) can be attributed to the camera movements introduced during rendering, which naturally generate images with varying zoom levels. 3.4.2. Intrinsic document features analysis We analyze the intrinsic characteristics of the documents, focusing on region of the Reduced Embedding Space (RES). This region was previously identified as problematic due to its prevalence of documents with dual tables and alphanumeric grading systems (intrinsic features), as well as low zoom levels (an extrinsic feature). To address this limitation, Donut [13] is fine-tuned using specific subset of the MERIT Dataset (referred to as the booster set) composed exclusively of samples exhibiting the two intrinsic features detected (this new MERIT split is not included in MERITs default training partition). This booster is used both independently and in combination with different base training 18 Figure 12: Effect of training zoom levels on Donuts [13] performance. Fixed zoom-level experiments reveal that excessively low zoom values prevent the model from extracting textual information, while excessively high zoom values make the training task insufficiently challenging compared to the validation data. An intermediate zoom level (0.625) yields optimal results in the challenging areas cluster and cluster (A). Training with uniform distribution of zoom values between 0.3 and 1.0 introduces visual diversity that enhances generalization (B). Comparative results across different regions (validation), cluster A, and cluster (confirm that mixed zoom levels during fine-tuning consistently improve performance over fixed-zoom configurations (C)). sets. Figure 13.A compares the results of fine-tuning Donut [13] on the digital version of MERIT (Figure 4.A) versus the booster set. The results show that the booster improves performance precisely in region (confirming that we have identified the key features governing this region). However, combining both datasets introduces certain degree of forgetting in region B, as the inclusion of less relevant features for this region dilutes its specificity (F1 decreases). Despite this, the combined model achieves level of generalization on the validation set comparable to that of the digital model, although with slightly accentuated suboptimal behavior in regions previously under control. In Figure 13.B, the same booster set is combined with the rendered training set. In this case, the combination yields the most significant improvements (both in region and across the entire validation dataset). This behavior suggests complementary effect: while the booster set contributes the dual-table and alphanumeric-grade features, the rendered dataset introduces zoom variability (as previously discussed in Section 3.4.1), which enhances generalization. As result, the model achieves more balanced learning process by jointly incorporating the three characteristics previously identified as Figure 13: Effect of training data composition on Donuts [13] performance. Fine-tuning with the booster set (a subset containing intrinsic features) improves performance in the low-performing region (A). However, combining the booster with the digital dataset induces partial forgetting in that region while maintaining comparable generalization on the validation set. When the booster is combined with the rendered dataset (B), both region and the overall validation performance improve significantly. This complementary effect arises from jointly leveraging the intrinsic features of the booster and the zoom variability introduced by rendering. sources of low performance. This combination produces the best-performing Donut [13] model across the entire validation set. 4. Results The results of VERSE can be analyzed along three main dimensions: (i) the intrinsic feasibility of each model for the target task, (ii) the performance gains achievable through clustering-guided fine-tuning, and (iii) the explainability and interpretability of datasetmodel interactions. 4.1. Model feasibility An analysis of the visual embedding space suggests that off-the-shelf models with limited clustering capabilities (reflected by lower Silhouette score values) tend to exhibit reduced performance in sequence generation tasks. Table 5 shows that PaliGemma [16] and LLaVA [20] produce less structured embeddings and achieve lower F1 scores after fine-tuning. In contrast, Donut [13] and Idefics2 [23] yield more compact and discriminative embeddings, as indicated by higher Silhouette score values, and subsequently reach higher F1 scores. 20 Model Silhouette score RES richness F1 Idefics2 [23] Donut [13] PaliGemma [16] LLaVA [20] 0.63 0.50 0.38 0.35 0.8101 0.7607 0.3028 0.0000 Table 5: Correlation between Silhouette score, RES richness, and downstream F1 for the best-performing model configurations. 4.2. Model Boosting We identify challenging clusters for suitable models (Donut [13] and Idefics [23]) and extract the corresponding features to target their underlying limitations. Both models exhibit difficulties in regions characterized by clusters containing alphanumeric scores, page layouts with two tables, and small visual attributes (e.g., zoomed-out content). Thus, we identify these features as the critical ones driving performance improvements for both models. Table 6 reports the performance gains enabled by VERSE methodology. Model Pre-VERSE F1(validation) Region F1 (region) Base dataset Added boosting features F1 (validation) F1 (region) Donut [13] Idefics2 [23] 0. Cluster 0.5989 Render Alphanumeric grading, double tables 0.7607 (+0.09) Cluster 0.4325 0.7556 Cluster 0.5211 Digital zoom Double tables 0.8101 (+0.06) Cluster 0.5054 Cluster 0.5040 0.7828 (+0.18) 0.5849 (+0.17) 0.6914 (+0.17) 0.6567 (+0.15) 0.7432 (+0.24) Table 6: Performance improvements obtained after applying the VERSE methodology. The Pre-VERSE F1 column reports baseline results obtained on the MERIT dataset, using human-centered selection criteria (render images are the most photorealistic and thus have higher potential for model performance). The Region column identifies modelspecific clusters associated with low performance detected in the Reduced Embedding Space (RES). Each experiment combines base training dataset with an additional subset containing targeted boosting features, designed to address weaknesses detected in those regions. The resulting F1 improvements (F1) are shown for both the overall validation set and the specific target regions. Results demonstrate that incorporating the identified boosting features leads to substantial performance gains in problematic clusters while preserving global generalization. VERSE enhances performance on conflictive clusters without degrading performance on already well-generalized regions (thereby improving overall 21 generalization). As result, F1 scores increase across the entire MERIT Secret validation dataset. These improvements position the fine-tuned models at level comparable to API-based solutions such as GPT4-O or Pixtral. Moreover, this gain comes with the additional benefits of working with local models, namely data privacy and governance, without compromising performance. Table 7 summarizes the results of the best fine-tuned configurations for each model. Model Deployment Fine-tuned F1 Idefics2 [23] GPT4-O Donut [13] Pixtral [5] On-premise API-Based On-premise API-Based VERSE API fine-tune VERSE N/A 0.8101 0.7821 0.7607 0.7267 Table 7: Comparison of the best-performing models. After applying the VERSE methodology, on-premise models achieve performance comparable to API-based solutions. Notably, Idefics2 [23] fine-tuned with VERSE outperforms GPT4-O, demonstrating that locally deployed models can match (or even exceed) the accuracy of proprietary cloud services when efficiently optimized. 4.3. Explainability The dimensionality reduction of the embedding space, combined with its graphical representation and the overlay of visual features, allows us to identify which document properties exert the greatest influence on the RES structure. In both Donut and Idefics2 (Figures 7 and 8), we consistently observe that the features with the strongest impact on cluster formation and positioning are those intrinsic to the documents (namely, those that describe their macro-visual structure). These include the layout type, the vertical distribution of information, the number of table columns, the vertical position of the table on the page, or the number of vertical content blocks. All of these features directly shape the global organization of the document and, consequently, its representation in the RES. In contrast, we argue that extrinsic features (those not inherent to the documents structure) do not primarily determine the location of samples within the embedding space; rather, their distribution emerges as consequence of the intrinsic characteristics that dominate each region. This is the case, for example, for zoom-related indices: although they do not govern the main spatial arrangement of samples in the RES, they remain relevant for explaining variations in performance (F1). 22 Similarly, the models do not appear to attend to visual features that are irrelevant to the table structure, such as the presence of signatures, stamps, or emblems (intrinsic but non-structural), nor to extrinsic artifacts, including shadows, creases, or other forms of noise in the document. Due to their limited relevance for the task and their weaker visual consistency, these features do not exert any observable influence on the organization of the RES. 5. Discusion In this section, we analyze the methodological implications and insights derived from applying VERSE to VrDU models, as well as its limitations and practical consequences. Reducing the visual embedding space enables us to consistently analyze how different data-augmentation strategies affect the training samples within the RES. We observe that such techniques scatter the samples away from the clusters defined by their intrinsic and structural features, expanding their coverage over the manifold where real samples lie. This dispersion correlates with substantial improvements in F1 for both Donut [13] and Idefics2 [23]. Another observation is that our results reinforce the idea that visual representation quality is critical for VrDU tasks. When the visual module fails to adequately disambiguate the documents structure, the information forwarded to the textual decoder is already degraded, ultimately harming downstream performance. This perspective slightly diverges from the emphasis suggested by the authors of Idefics2 [23], who point to the textual decoder as the component with the greatest room for improvement. In contrast, our findings are more consistent with [44], underscoring the importance of strong and task-aligned visual representations for solving multimodal tasks. While VERSE provides valuable insight into explainability, we also note significant degree of polysemanticity in the principal components. Although each PC cannot be interpreted as clean combination of features, the qualitative analysis does reveal dominant patterns. The most problematic characteristics consistently correspond to low zoom levels, double-table layouts, and alphanumeric grading systems, which define the clusters with the lowest performance. Finally, comparing models that share the same visual encoder (such as Idefics2 [23] and PaliGemma [16], both based on SigLIP-So400M [18]) reveals that the encoder alone does not determine the richness of the visual embedding space. Idefics2 [23] exhibits far greater sensitivity to VrDU-relevant fea23 tures because its pre-training is more closely aligned with document-centric tasks. Additionally, its visual module is structurally more comprehensive, as it includes an internal MLP and Perceiver projection. As shown in Table 3, this architectural difference enables Idefics2 [23] to produce richer and more task-aligned visual embeddings. 6. Conclusions and contributions In this work, we presented VERSE, methodology for reducing and exploring the visual embedding space of VLMs. By projecting the latent space and overlaying the relevant features that define the target samples, VERSE demonstrates how models internally structure visual information. We further show that photorealistic synthetic data are not strictly required to improve performance. For training purposes, what matters is not human-perceived realism but whether synthetic samples populate regions of the visualsemantic embedding space that are meaningful and useful for the model. Based on this methodology, our main contributions are: Assessment of model feasibility for VrDU tasks, identifying which VLMs exhibit sufficiently structured embedding space to handle the task effectively. Identification of error-inducing regions and determination of the visual features that must be reinforced in training data to mitigate them. Demonstration of targeted performance improvements: applying VERSE enhances low-performing regions without compromising overall generalization. As future work, we plan to invert the VERSE pipeline: once problematic regions in the reduced embedding space are identified, we aim to sample these areas directly and reconstruct images from their latent representations using generative models. This would enable the synthesis of data inherently aligned with the models latent space, providing mechanism to generate training samples that embody the visual characteristics most relevant for improving model performance. 24 Data availability Training: The MERIT Dataset [4] is available on Hugging Face. 2. Validation: The MERIT Secret Dataset is not available since it contains real samples under Non-Disclosure Agreement. Code: The repository to train and test the models, and extract the embeddings from them is available on GitHub. 3. Training Sessions: WandB links to training projects: Donut [13] 4. Idefics2 [23] 5. PaliGemma [16]6. LLaVA [20] 7. Acknowledgments The authors of this publication would like to thank the Chair for Smart Industry for providing the necessary resources to produce this research. In addition, the authors extend their gratitude to the Secretarys Office staff at Universidad Pontificia Comillas for granting access to authentic transcripts of records under the condition of Non-Disclosure Agreement, enabling the curation of MERIT Secret. Appendix A. Table A.8 describes the features extracted from the validation dataset (MERIT Secret). 2MERIT Dataset on Hugging Face: https://huggingface.co/datasets/de-Rodrigo/merit 3Code on GitHub: https://github.com/nachoDRT/VrDU-Doctor/tree/main 4Donut [13] training project: https://wandb.ai/ciclab-comillas/Donut 5Idefics2 [23] training project : https://wandb.ai/ciclab-comillas/Idefics2-patient 6PaliGemma [16] training project: https://wandb.ai/ciclab-comillas/Paligemma 7LLaVA [20] training project: https://wandb.ai/ciclab-comillas/LLaVA 25 Metadata Description Image width (px). Image height (px). Cell width (px). Cell height (px). Cell-to-image height ratio. Cell-to-image width ratio. Includes merged and split cells in column. Grading scale: Spanish (010), English (A*, AU). Missing grade(s) for subject. Multiple grades for the same subject. With/without grid lines (some only vertical). Numeric/alphanumeric, may include honors (MH, M). image_w image_h row_w row_h row_h/image_h row_w/image_w table_complexity grades_system lacking_grades retake table_grid grades orthogonal_distortion Misalignment or curved grids; excludes rotated but orthogonal tables. white_border anonymization_marks Marks outside the table to ensure anonymity. Wrinkled document. wrinkles Number of visual blocks (e.g., title, table, signature). v_info_blocks Vertical distribution of blocks (uniform, top, bottom). v_density Table type: A, A-double, B, C. layout Number of columns. columns Image acquisition: camera or scanner. source Presence of shadows. shadows School emblem at top. header_badge Signed (signature may be anonymized). signed School stamp. stamped Horizontal position: left, center, right. table_pos White border from scanning apps. Table A.8: Features extracted from samples in MERIT Secret. Appendix B. The application of VERSE to the Idefics2 [23] model reveals that it does not only exhibits more structured embedding space than Donut, but it can also obtain better results even when trained with limited levels of visual information. For instance, Idefics2 achieves strong performance in clusters composed of single-table documents, despite being trained on highly degraded versions of the trainin dataset (Figure B.14.A and B). As additional visual features are introduced in the digital version (B.14.C), the training samples (shown in violet) shift toward more clearly defined clusters, indicating good organization of the latent space. However, when data augmentation techniques are applied, the samples become more dispersed across the embedding space (a behavior that, although less pronounced, also appears in the rendered version, and correlates with better F1 scores). It is worth noting that Idefics2 performs particularly well in clusters dominated by single-table layouts. However, three low-performing regions persist, driven mainly by low zoom levels, dual-table layouts, and alphanumeric 26 Figure B.14: Reduced Embedding Space (RES) for Idefics2 [23] (PC1 vs. PC2). Higher F1 values are shown in green, while lower ones appear in red. Each subplot includes, in the lower-left corner, representative training sample used for fine-tuning and subset of training examples shown in purple. The visual richness of the training data increases progressively from to F. grading systems (Figure B.15.A). Following the VERSE methodology, the most problematic area is identified as cluster B, characterized by both low zoom level and double-table structure (Figure B.15.B). When these features are included into the training set, the combination yields significant improvements over the entire validation set and achieves solid results in the previously problematic regions (Figure B.15.C). When we analyze the independent effects of each training set on the validation set and region (Figure B.16), the booster dataset achieves slightly lower results in region than the model fine-tuned exclusively on the zoom version. However, this behavior should be interpreted as complementary learning effect rather than an overlap: each dataset contributes distinct strengths (the zoom version enhances the models ability to extract meaningful information from low-resolution tables, while the booster improves its understanding of double-table layouts). When combined, these two effects reinforce each other, resulting in better overall performance than either could achieve independently. 27 Figure B.15: Effect of training data composition on Idefics2s [23] performance. Regions A, B, and are detected in the RES as conflictive clusters. Since Regions and already achieve solid performance when the model is fine-tuned on the zoom version (C), we define region as the target cluster for the booster set. VERSE suggests including samples with low zoom level and two tables as driving features of the booster set (B). Fine-tuning the modle with the suggested combination improves performance in the problematic region while preserving consistent generalization across the validation set and the remaining conflictive regions (C). Figure B.16: Fine-tuning with the booster set improves Idefics2s [23] performance in the problematic region while preserving consistent generalization across the validation set. 28 References [1] A. F. Spies, W. Edwards, M. Ivanitskiy, A. Skapars, T. Räuker, K. Inoue, A. Russo, M. Shanahan, Transformers use causal world models in maze-solving tasks, in: ICLR 2025 Workshop on World Models: Understanding, Modelling and Scaling, 2024. [2] Z. Li, C. Guo, X. Wang, H. Zhang, L. Hu, Multi-view visual semantic embedding for cross-modal imagetext retrieval, Pattern Recognition 159 (2025) 111088. [3] A. Nassar, A. Marafioti, M. Omenetti, M. Lysak, N. Livathinos, C. Auer, L. Morin, R. T. de Lima, Y. Kim, A. S. Gurbuz, et al., Smoldocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion, arXiv preprint arXiv:2503.11576 (2025). [4] I. De Rodrigo, A. Sanchez-Cuadrado, J. Boal, A. J. Lopez-Lopez, The merit dataset: Modelling and efficiently rendering interpretable transcripts, Pattern Recognition (2025) 112502. [5] P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. De Monicault, S. Garg, T. Gervet, et al., Pixtral 12b, arXiv preprint arXiv:2410.07073 (2024). [6] Y. Xu, M. Li, L. Cui, S. Huang, F. Wei, M. Zhou, Layoutlm: Pretraining of text and layout for document image understanding, in: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, 2020, pp. 11921200. [7] Y. Xu, Y. Xu, T. Lv, L. Cui, F. Wei, G. Wang, Y. Lu, D. Florencio, C. Zhang, W. Che, et al., Layoutlmv2: Multi-modal pre-training for visually-rich document understanding, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 25792591. [8] Y. Xu, T. Lv, L. Cui, G. Wang, Y. Lu, D. Florencio, C. Zhang, F. Wei, Layoutxlm: Multimodal pre-training for multilingual visually-rich document understanding, arXiv preprint arXiv:2104.08836 (2021). 29 [9] Y. Xu, T. Lv, L. Cui, G. Wang, Y. Lu, D. Florencio, C. Zhang, F. Wei, Xfund: benchmark dataset for multilingual visually rich form understanding, in: Findings of the Association for Computational Linguistics: ACL 2022, 2022, pp. 32143224. [10] Y. Huang, T. Lv, L. Cui, Y. Lu, F. Wei, Layoutlmv3: Pre-training for document ai with unified text and image masking, in: Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 40834091. [11] R. Palacios, A. Gupta, system for processing handwritten bank checks automatically, Image and Vision Computing 26 (2008) 12971313. [12] Z. Gu, C. Meng, K. Wang, J. Lan, W. Wang, M. Gu, L. Zhang, Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 45834592. [13] G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, in: D. Han, S. Park, Ocr-free document understanding transformer, European Conference on Computer Vision, Springer, 2022, pp. 498517. [14] M. Dhouib, G. Bettaieb, A. Shabou, Docparser: End-to-end ocr-free information extraction from visually rich documents, in: International Conference on Document Analysis and Recognition, Springer, 2023, pp. 155172. [15] Z. Tang, Z. Yang, G. Wang, Y. Fang, Y. Liu, C. Zhu, M. Zeng, C. Zhang, M. Bansal, Unifying vision, text, and layout for universal document processing, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1925419264. [16] L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, et al., Paligemma: versatile 3b vlm for transfer, arXiv preprint arXiv:2407.07726 (2024). [17] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, et al., Pali: jointly-scaled multilingual language-image model, in: The Eleventh International Conference on Learning Representations, 2023. [18] X. Zhai, B. Mustafa, A. Kolesnikov, L. Beyer, Sigmoid loss for language in: Proceedings of the IEEE/CVF international image pre-training, conference on computer vision, 2023, pp. 1197511986. [19] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, et al., Gemma: Open models based on gemini research and technology, arXiv preprint arXiv:2403.08295 (2024). [20] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, Advances in neural information processing systems 36 (2023) 3489234916. [21] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transferable visual models from natural language supervision, in: International conference on machine learning, PmLR, 2021, pp. 87488763. [22] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. URL: https://lmsys.org/blog/2023-03-30-vicuna/. [23] H. Laurençon, L. Tronchon, M. Cord, V. Sanh, What matters when building vision-language models?, Advances in Neural Information Processing Systems 37 (2024) 8787487907. [24] H. Laurençon, A. Marafioti, V. Sanh, L. Tronchon, Building and better insights and future directions, understanding vision-language models: arXiv preprint arXiv:2408.12637 (2024). [25] N. Livathinos, C. Auer, M. Lysak, A. Nassar, M. Dolfi, P. Vagenas, C. B. Ramis, M. Omenetti, K. Dinkla, Y. Kim, et al., Docling: An efficient open-source toolkit for ai-driven document conversion, in: AAAI Conference on Artificial Intelligence, 2025. [26] M. Lysak, A. Nassar, N. Livathinos, C. Auer, P. Staar, Optimized table tokenization for table structure recognition, in: International Conference on Document Analysis and Recognition, Springer, 2023, pp. 3750. [27] N. Ringland, X. Dai, B. Hachey, S. Karimi, C. Paris, J. R. Curran, Nne: dataset for nested named entity recognition in english newswire, in: 31 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 2019. [28] G. Jaume, H. K. Ekenel, J.-P. Thiran, Funsd: dataset for form unin: 2019 International Conderstanding in noisy scanned documents, ference on Document Analysis and Recognition Workshops (ICDARW), volume 2, IEEE, 2019, pp. 16. [29] S. Park, S. Shin, B. Lee, J. Lee, J. Surh, M. Seo, H. Lee, Cord: consolidated receipt dataset for post-ocr parsing, in: Workshop on Document Intelligence at NeurIPS 2019, 2019. [30] Z. Huang, K. Chen, J. He, X. Bai, D. Karatzas, S. Lu, C. Jawahar, Icdar2019 competition on scanned receipt ocr and information extraction, in: 2019 International Conference on Document Analysis and Recognition (ICDAR), IEEE, 2019, pp. 15161520. [31] X. Zhong, J. Tang, A. J. Yepes, Publaynet: largest dataset ever for docin: 2019 International conference on document ument layout analysis, analysis and recognition (ICDAR), IEEE, 2019, pp. 10151022. [32] M. Mathew, D. Karatzas, C. Jawahar, Docvqa: dataset for vqa on document images, in: Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2021, pp. 22002209. [33] Y. Ding, S. Luo, H. Chung, S. C. Han, Vqa: new dataset for realworld vqa on pdf documents, in: Joint European Conference on Machine Learning and Knowledge Discovery in Databases, Springer, 2023, pp. 585601. [34] R. Tanaka, K. Nishida, K. Nishida, T. Hasegawa, I. Saito, K. Saito, Slidevqa: dataset for document visual question answering on multiple images, in: Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 2023, pp. 1363613645. [35] M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, C. Jawahar, in: Proceedings of the IEEE/CVF Winter Conference Infographicvqa, on Applications of Computer Vision, 2022, pp. 16971706. 32 [36] I. Ziegler, A. Köksal, D. Elliott, H. Schuetze, Craft your dataset: Taskspecific synthetic dataset generation through corpus retrieval and augmentation, in: NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability, 2024. [37] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in: Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 1001210022. [38] H. Laurençon, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. Rush, D. Kiela, et al., Obelics: An open web-scale filtered dataset of interleaved image-text documents, Advances in Neural Information Processing Systems 36 (2023) 71683 71702. [39] A. F. Biten, R. Tito, L. Gomez, E. Valveny, D. Karatzas, Ocr-idl: in: European Ocr annotations for industry document library dataset, Conference on Computer Vision, Springer, 2022, pp. 241252. [40] P. Sharma, N. Ding, S. Goodman, R. Soricut, Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image capin: Proceedings of the 56th Annual Meeting of the Associationing, tion for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 25562565. [41] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. PontTuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov, et al., The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale, International journal of computer vision 128 (2020) 19561981. [42] K. Srinivasan, K. Raman, J. Chen, M. Bendersky, M. Najork, Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning, in: Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, 2021, pp. 24432449. [43] S. Changpinyo, D. Kukliansy, I. Szpektor, X. Chen, N. Ding, R. Soriin: Proceedings of cut, All you may need for vqa are image captions, 33 the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies, 2022, pp. 19471963. [44] R. Balestriero, Y. LeCun, Lejepa: Provable and scalable self-supervised learning without the heuristics, arXiv preprint arXiv:2511.08544 (2025)."
        }
    ],
    "affiliations": [
        "Institute for Research in Technology, ICAI School of Engineering, Comillas Pontifical University, Calle Rey Francisco, 4, Madrid, 28008, Madrid, Spain"
    ]
}