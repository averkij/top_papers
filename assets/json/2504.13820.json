{
    "paper_title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning",
    "authors": [
        "Yang Yue",
        "Yulin Wang",
        "Chenxin Tao",
        "Pan Liu",
        "Shiji Song",
        "Gao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans can develop internal world models that encode common sense knowledge, telling them how the world works and predicting the consequences of their actions. This concept has emerged as a promising direction for establishing general-purpose machine-learning models in recent preliminary works, e.g., for visual representation learning. In this paper, we present CheXWorld, the first effort towards a self-supervised world model for radiographic images. Specifically, our work develops a unified framework that simultaneously models three aspects of medical knowledge essential for qualified radiologists, including 1) local anatomical structures describing the fine-grained characteristics of local tissues (e.g., architectures, shapes, and textures); 2) global anatomical layouts describing the global organization of the human body (e.g., layouts of organs and skeletons); and 3) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs (e.g., varying clarity, contrast, and exposure caused by collecting radiographs from different hospitals, devices, or patients). Empirically, we design tailored qualitative and quantitative analyses, revealing that CheXWorld successfully captures these three dimensions of medical knowledge. Furthermore, transfer learning experiments across eight medical image classification and segmentation benchmarks showcase that CheXWorld significantly outperforms existing SSL methods and large-scale medical foundation models. Code & pre-trained models are available at https://github.com/LeapLabTHU/CheXWorld."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 0 2 8 3 1 . 4 0 5 2 : r CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning Yang Yue1* Yulin Wang1 Chenxin Tao1 Pan Liu2 Shiji Song1 Gao Huang1 (cid:12) 1Tsinghua University 2PLA General Hospital yueyang22@mails.tsinghua.edu.cn, gaohuang@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Humans can develop internal world models that encode common sense knowledge, telling them how the world works and predicting the consequences of their actions. This concept has emerged as promising direction for establishing general-purpose machine-learning models in recent preliminary works, e.g., for visual representation learning. In this paper, we present CheXWorld, the first effort towards selfsupervised world model for radiographic images. Specifically, our work develops unified framework that simultaneously models three aspects of medical knowledge essential for qualified radiologists, including 1) local anatomical structures describing the fine-grained characteristics of local tissues (e.g., architectures, shapes, and textures); 2) global anatomical layouts describing the global organization of the human body (e.g., layouts of organs and skeletons); and 3) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs (e.g., varying clarity, contrast, and exposure caused by collecting radiographs from different hospitals, devices, or patients). Empirically, we design tailored qualitative and quantitative analyses, revealing that CheXWorld successfully captures these three dimensions of medical knowledge. Furthermore, transfer learning experiments across eight medical image classification and segmentation benchmarks showcase that CheXWorld significantly outperforms existing SSL methods and large-scale medical foundation models. Code & pre-trained models are available at https://github.com/LeapLabTHU/CheXWorld. 1. Introduction Intelligent agents like humans learn extensive background knowledge about the world [41]. This common-sense information is embedded in the agents internal models of the world, playing pivotal role in their perception, learning, *Equal contribution. (cid:12)Corresponding authors. and decision-making processes by simulating the worlds dynamics, telling them what is plausible or impossible, and predicting the outcomes of their actions. Consequently, these world models enable agents to acquire new concepts and skills with minimal demonstrations and trials [59]. Recent works [2, 22] have preliminarily verified the effectiveness of establishing visual representation learning approaches through the lens of world modeling. Pre-trained world models can produce semantically rich embeddings and adapt to various downstream tasks with limited data. Like many other research areas, the field of medical imaging is experiencing paradigm shift from task-specific models to general-purpose foundation models [21, 35, 49, 72], which are pre-trained on massive data and expected to encode meaningful medical knowledge. The idea of world modeling offers an approach to training medical foundation model by capturing common sense information (e.g. human anatomy) from medical images, which is promising yet under-explored research direction. In this paper, we present CheXWorld, the first initiative toward world modeling for self-supervised representation learning on radiographic images. We propose and integrate three world modeling tasks that capture different dimensions of medical knowledge essential for qualified radiologists, depicted in Figure 1. In particular, radiologist with strong understanding of human anatomy can identify different anatomical structures and determine the relative geometry between anatomical regions. Given the hierarchical nature of human anatomy, we introduce world modeling tasks that focus on both local and global local anatomical structure modeling levels of anatomy: aims to predict specific anatomical characteristics and such structures within localized anatomical as bones, airways, blood vessels, and lung segments; global anatomical layout modeling learns the overall arrangement and spatial relationships of various anatomical regions within the human body, understanding the relative positioning of organs and tissues such as the heart, lungs, diaphragm, and rib cage. Additionally, medical images are region, 1 Figure 1. Overview of the CheXWorld framework1. The upper part of the figure depicts three dimensions of medical knowledge that are formulated in our framework, including (a) local anatomical structures describing the fine-grained characteristics of local tissues, (b) global anatomical layouts describing the global organization of the human body and (c) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs. The middle part of the figure illustrates the world modeling tasks corresponding to these aspects of medical knowledge. (d) shows our unified pipeline that combines the merits of all three tasks. typically gathered from multiple domains with notable appearance differences due to varying equipment and acquisition techniques [56]. Experienced radiologists can recognize the physical reality behind domains with different appearances and facilitate the diagnosis process. Motivated by this, we hypothesize that radiologists quickly adapt to diverse domain changes by learning sophisticated internal world models that are capable of simulating transitions between domains. Driven by our hypothesis, we introduce the domain variation modeling task, which aims to learn an expressive feature space that encodes predictable transitions between domains. Finally, we design an integrated pipeline that performs the three world modeling tasks simultaneously, effectively incorporating medical anatomical knowledge and achieving robustness across domains. It is noteworthy that the three aspects we consider, i.e., local anatomical structure modeling, global anatomical layout modeling, and domain variation modeling, represent core elements of radiographic expertise, each contributing to comprehensive framework for capturing medical knowledge. While they may not encompass every dimen1The x-rays are taken from kenhub.com, Radiopedia [47] and ChestXray14 [65]. sion of medical understanding, they are fundamental to radiology and critical for radiograph representation learning, as extensively demonstrated in [24, 53, 73, 74]. Empirically, we validate that CheXWorld effectively captures the three dimensions of medical knowledge through series of analytical experiments, including visualizations of predictor outputs with the help of generative model and domain variation sensitivity test. Moreover, we extensively evaluate CheXWorld on eight medical image classification and segmentation benchmarks. Our model consistently outperforms competitive self-supervised learning baselines with comparable backbone capacity, pretraining data, and pre-training computational cost. 2. Related Work World Modeling. In the machine learning community, world modeling originates from model-predictive control [9, 10] and is common practice in reinforcement learning, where it predicts the future state of the environment based on the agents action [2528]. Recently, building general world models featuring profound comprehension of common sense knowledge is believed to be crucial step towards general artificial intelligence [41, 57]. Video gener2 ation models [39] trained on vast data (e.g. SoRA[51] and VideoPoet [40]) are world models that perform predictions in input space. Another line of work makes predictions in the latent space [2, 7, 29, 34, 41], which more closely resembles the world model in the human brain. World modeling is also an effective approach for visual representation learning, where the model predicts missing parts of the visual inputs [2, 4, 5, 7, 22]. Our framework builds on this foundation, where our model acquires knowledgesuch as human anatomy and domain variationsthrough learning to predict unobserved outcomes from specific actions. Our key contribution lies in providing insights into modeling medical knowledge from radiographic images and designing framework that unifies three world modeling tasks. Self-supervised learning in medical imaging. Selfsupervised learning (SSL) [19, 32, 52, 66] provides practical solution to mitigating annotation scarcity in medical imaging. large body of works incorporate restorative SSL tasks, which typically involve reconstructing the original data from certain image corruptions, such as random masking [67, 71, 72], pixel shuffling [73], and patch order shuffling [54, 62, 74]. Another line of works [3, 12, 55, 60, 70] adapt global or dense contrastive learning to medical imaging. TransVW[30] and DiRA [31, 61] explore combining discriminative and restorative methods. Additionally, learning from anatomy is common topic for medical SSL. SAM [69] enforces pixel-level anatomical correspondence between images, and Adam [33] learns part-whole correspondence that reflects the hierarchy of human anatomy. Recently, SSL-based foundation models trained on largescale datasets gained significant attention, such as RETFound [72] for retinal imaging and UNI [13] for pathology. Compared with existing works, CheXWorld is the first to introduce the concept of world modeling into medical SSL, capturing task-relevant medical knowledge through three tailored world modeling tasks. Unlike contrastivebased approaches such as Adam [33], CheXWorld takes technically distinct path by constructing equivariant [16, 17] representationswhere input transformations result in predictable changes within the embedding spacerather than invariant representations. This approach allows CheXWorld to better model medical data without discarding valuable information. Moreover, our unified SSL framework delivers state-of-the-art performance across eight benchmarks, demonstrating the significant potential of designing SSL methods grounded in the philosophy of world modeling. 3. Basic Framework of World Modeling World modeling. We begin by briefly introducing the concept of world modeling [26, 41], which forms the basis for our proposed method. The primary motivation behind world modeling is to predict the unobserved parts of world (e.g., visual environment, an image, or video) based on an observed context x. This prediction can be formulated across various dimensions, such as spatial (predicting the unseen regions of the data) and temporal (foreseeing the consequence of an action). In fact, the paradigm of world modeling takes inspiration from human visual cognition. For example, when presented with the right upper lobe region of chest radiograph as the context x, the internal world of radiologist could imagine that there are two lobes (right middle and right lower) below the visible region and two lobes on the left , forming the target y. Figure 2. basic framework of world modeling. basic framework. Here, we introduce our basic framework of instantiating world modeling, upon which we will further discuss how to establish radiology world model by formulating various medical knowledge in Section 4. In specific, our basic framework mainly follows the joint-embedding predictive architecture (JEPA) [2, 41], as shown in Figure 2. We consider two encoders: the context encoder fθ and the target encoder θ, which encode the context and target into representations hx and hy, respectively. predictor gϕ learns to predict the target embedding hy using the context embedding hx, conditioned on an additional latent variable that indicates how the target relates to the context. For example, can be the spatial locations of with respect to x, or an action that leads to the transition from context to target (e.g., image transformations). The latent variable carries the information that makes predictable based on x, effectively modeling the inherent variation and uncertainty of the real world. The primary objective of world modeling is: minimize D(hy, ˆhy) = D(hy, gϕ(hx; z)), where D(, ) denotes the prediction error. Notably, the predictions are performed in the abstract representation space, contrasting with generative world models that predict every detail of the target. This mirrors an important characteristic of intelligent agents: information filtering, i.e., eliminating the irrelevant details in the data during the perception process. For example, radiologist can infer the anatomical layout of partly missed image, but its nearly impossible for him to recover each pixel value of the unseen regions. (1) In implementation, we set the target encoder to be the exponential moving average of the context encoder following [2, 4, 7]. Moreover, we use the Vision Transformer (ViT) [18] architecture for the context/target encoder and the predictor. For the rest of this paper, the context and target feature hx, hy are sequences of patch embeddings produced by the ViTs. To perform patch-level feature predictions, we attach mask tokens to the context features and feed the combined sequence to the predictor. The output representations of the mask tokens are used as the final prediction. context feature hx, which forms the input token sequence to the predictor. The predictor output ˆhy is given by: 4. CheXWorld: World Modeling for Radiograph Representation Learning This section extends the idea of world modeling to radiology. We propose three world modeling tasks tailored for radiographs and establish unified CheXWorld framework that seamlessly integrates them. Importantly, the three tasks are designed to model three critical dimensions of medical knowledge essential for qualified radiologists. To be specific, these tasks are organized hierarchically from low-level to high-level, namely 1) local anatomical structure modeling that aims to learn the fine-grained structures of local anatomical regions, 2) global anatomical layout modeling that seeks to learn the global geometry of the human body (e.g., the layout of organs and skeletons), and 3) domain variation modeling that learns to model the transition across different appearance domains of radiographs. We first elaborate on the details of each task. Then, we present unified framework that comprehensively combines the characteristics and merits of all three tasks, yielding powerful foundation model that produces semantically rich and transferable representations. 4.1. Local Anatomical Structure Modeling At relatively low, fundamental level, various types of tissues (e.g., bones, muscles, and epithelial tissues) form the structural organization of the human body. Understanding the fine-grained characteristics (e.g., shapes, sizes, appearance, and textures) of these micro-structures is essential knowledge that radiologists must possess. To build up learning procedure that enables neural networks to acquire such local anatomical knowledge, we consider mask-andreconstruction task. The model predicts fine-grained details of masked tissue region based on its peripheral, surrounding information, as shown in Figure 1(a). This encourages the networks to develop an internal model that captures the intricate structures and local continuities of human microanatomy. The large, continuous prediction target also prevents reliance on low-level features. Specifically, the image mask is union of four randomly selected rectangular regions of the image following [2]. Let denote the set of 2D patch locations that are masked from the context input. Given an input image (crop) I, the image patches with locations contained in are dropped from the image to create the context = Mask(I, ), while the target is the entire image. The context and target are then fed to the corresponding encoders, producing representations hx = fθ(x) and hy = θ(y). Note that the context encoder exclusively processes visible patches, following [2, 32]. In the predictor, the mask tokens carrying the positional encoding of the masked locations are concatenated with the ˆhy = gϕ(hx; ) (cid:16) = gϕ (hx + px) {m + PE(u, v) }(u,v)M (2) (cid:17) , where is the concatenation operation along sequence dimension, px is the positional embedding corresponding to the context, and are image patch location indices along height and width dimensions, PE() is the sinusoidal positional encoding proposed by [63]. The prediction loss is computed only on the masked locations, which is given by: Llocal(x, y) = (cid:88) cM gϕ(fθ(x); )c θ(y)c2 2 , (3) where is the 2D image patch location that belongs to . 4.2. Global Anatomical Layout Modeling In addition to understanding local microstructures like tissues, it is also important for radiologists to possess comprehensive medical knowledge regarding how groups of tissues or systems of organs are anatomically organized and assembled into the full bodies of humans. Inspired by this, we develop the global anatomical layout modeling task (see Figure 1(b)), seeking to predict the feature of an out-of-context area based on its relative position to given context, thus facilitating understanding of the global structure of human bodies. Notably, this idea is orthogonal to the technique proposed in section 4.1 (see section 5.3 for ablation studies) as it focuses on formulating the long-range topological relationships of different tissues or organs instead of the fine-grained anatomical structures within local tissues. As illustrated in Figure 1(b), we randomly crop two anatomical areas from the original radiograph, serving as context and target y, respectively. The world model learns to predict from given their relative position information xy. In particular, we assume the context and target image crop share the same spatial size of pixels in height and pixels in width, with the left-top corner located at (ix, jx) and (iy, jy) in the pixel space. The relative position information is defined as the proportional relative displacement between the two regions: Figure 3. Formulation of global anatomical layout modeling. xy = (h, w) = (cid:18) iy ix , jy jx (cid:19) . (4) 4 To properly represent the location of the image patches in the target y, we establish coordinate system over the context image patches, with the top-left corner patch located at the origin (0, 0) and the bottom-right corner patch located at (Nh 1, Nw 1), where Nh, Nw are the number of tokens along the height and width dimensions. As demonstrated in Figure 3, the coordinates of the image patch at the u-th row and v-th column in the target image are given by: ϕxy(u, v) = (h Nh + u, Nw + v) where [0, Nh 1], [0, Nw 1]. (5) Essentially, ϕxy(u, v) describes the targets location from the contexts perspective, enabling the model to predict the target based on the context. Sinusoidal position encoding PE() is then applied to the relative coordinates ϕxy(u, v), and the mask tokens carrying the positional embeddings of all the target image patches are fed to the predictor. The predictor output is given by: ˆhy = gϕ(hx; xy) (cid:16) (cid:17) = gϕ (hx + px) {m + PE(ϕxy(u, v))}u,v , (6) where [0, Nh 1], [0, Nw 1], and the loss function is given by: Lglobal(x, y) = gϕ(fθ(x); xy) θ(y)2 2 . (7) the original image. Another transform parameterized by scalars (denoted by Rk) is further applied to the target to produce the context = Ta(y), where is an augmentation pipeline consisting of Gaussian blur, brightness, contrast, and gamma adjustments. The parameter contains the strength and other configurations of the augmentation. The world model learns to model the feature transformation from the context to the target domain (i.e. the inverse effect of Ta) conditioned on the parameter a, which can be regarded as the action of domain transition. The prediction is also performed with mask tokens to be compatible with the other two world modeling tasks. The parameter is combined with the mask tokens via lightweight policy network π. The forward pass and loss function of domain variation modeling are given by: ma,p = π(m + p, a), ˆhy = gϕ(hx; a) (cid:16) = gϕ (hx + px) (cid:8)ma,PE(u,v) (8) (cid:17) , (cid:9) u,v Ldomain(x, y) = gϕ(fθ(x); a) where is the positional embedding of an image patch, ma,p denotes the mask token carrying both spatial and domain transition information. θ(y)2 2 , 4.3. Domain Variation Modeling 4.4. The Unified Framework Thus far, we have discussed two levels of anatomical knowledge of human bodies. Beyond this, radiologists are usually capable of flexibly adapting to the specific characteristics of each radiograph and understanding them despite variations [15, 24, 53]. As shown in Figure 1(c), medical images often come from different sources (e.g., hospitals, techniques, equipment) and show diverse appearances [56] (e.g., clarity, contrast, exposure). Nevertheless, experienced radiologists can imagine the physical reality behind the radiographs with varying appearances, which facilitates the diagnosis process. We hypothesize that radiologists internal world models can simulate the variations across domains, thus obtaining an objective view of the scanned body. Inspired by this, we propose domain variation modeling task that learns how image features change across domains, enabling cross-domain adaptability. We simulate domain shifts using data augmentation and construct context-target pairs to model these transitions. The model is tasked with predicting the feature outcomes of the inverse effect of an image transformation determined by certain augmentation parameters, as shown in Figure 1(c). The learned representation space is designed to be equivariant [16, 17], i.e., input transformation leads to predictable output change, which preserves sufficient information across domains, mimicking radiologists internal models. Specifically, the target is an augmented version of Having established the individual tasks of local anatomical structure modeling, global anatomical layout modeling, and domain variation modeling, we now integrate these tasks into cohesive system. This integration is important for creating comprehensive model that can leverage multiple dimensions of medical knowledge simultaneously and maximize the combined benefits of each task. Instead of simply adding three loss functions together, more elegant solution is to design unified contexts, targets, and latent variables to perform the three world modeling tasks in single forward pass. As illustrated in Figure 1(d), the training pipeline of CheXWorld starts with sampling two different image crops from the image y1, y2, which serve as the targets. The targets are then augmented and masked with two different configurations (a1, M1) and (a2, M2), producing the contexts x1, x2: (9) xi = Mask(Tai(yi), Mi), = 1, 2. The contexts x1, x2 and targets y1, y2 are then encoded into representations hx1, hx2, hy1 , hy2 . Note that the targets hy1, hy2 are predictable from the contexts hx1, hx2 given the proper conditions. Specifically, since x1 is obtained by applying augmentation Ta1 and mask M1 to y1, predicting y1 from x1 with conditions (a1, M1) simultaneously performs local anatomical structure modeling and domain vari5 Figure 4. Visualization of the CheXWorld predictor outputs (zooming in for details). The images presented in this figure were not included in the pre-training of CheXWorld or the training of the diffusion model. Regions in red bounding boxes denote the predictor outputs that are mapped to pixel space using the RCDM [8] framework. In (a), gray areas indicate masked regions excluded from the context. In (b), the two overlapping regions alternately serve as context and target. Figure 5. CheXWorld prioritizes relevant medical features over spurious signals (e.g., lateral markers) in the image. Regions in red boxes denote the predictor outputs. Figure 6. CheXWorld learns anatomical correspondence. We compute pixel-level embeddings using RoI-pooling [23] and calculate the embedding similarity between four anatomical landmarks in reference image and each pixel in the test image to create feature similarity heatmaps. ation modeling: L11 = Llocal+domain(x1, y1) (cid:88) = cM1 gϕ(hx1 ; M1, a1)c θ(y1)c2 2 . (10) Moreover, x1 can be derived from y2 by moving the input window by x1y2 and applying augmentation Ta1, hence the predictor conditioning on (x1y2, a1) performs global anatomical structure modeling and domain variation modeling at the same time. L12 = Lglobal+domain(x1, y2) = gϕ(hx1; x1y2 , a1) θ(y2)2 2 . (11) Similarly, L22, L21 can be computed between (x2, y2) and (x2, y1), respectively. Note that the context features and target features are reused twice when computing the loss (10) and (11). Finally, the model is trained with the combination of 2 2 = 4 supervisions computed from the two pairs of context and target. = L11 + L22 + L12 + L21. (12) With this unified objective, CheXWorld showcases deep comprehension of human anatomy in different scales and pixel intensity distributions across various domains. 5. Experiments In this section, we first demonstrate CheXWorlds world modeling capability with series of analytical experiments. Then, we empirically compare CheXWorld with other selfsupervised methods as baselines on eight downstream tasks. Finally, we provide ablation studies to highlight the effect of each building block of our method. Implementation details. CheXWorld is pre-trained on 0.5M frontal chest X-rays from several public datasets. The visual backbone is ViT-Base [18]. The model is trained for 300 epochs, taking 16 hours on 8 RTX 4090 GPUs. Please refer to the appendix for more details. 5.1. CheXWorld is Strong World Predictor Visualization of anatomical modeling. To better understand the predictions made by CheXWorld, we train generative model that maps the average-pooled predictor output back to pixel space following the RCDM framework [8]. Specifically, we construct context-target pairs in the same way as in the local and global anatomical modeling tasks. Then, we feed the context to frozen pre-trained CheXWorld model to obtain the models feature prediction, on which diffusion model conditions to produce the target image in pixel space. As shown in Figure 4(a), the CheXWorld predictor perfectly recovers the appearance of bones 6 Table 1. Performance on five downstream classification benchmarks. Accuracy is reported for RSNA, and the area under the ROC curve (AUROC) is reported for the rest of the datasets. The best two results are bold-faced and underlined, respectively. Method Scratch MoCo-v3 [14] DINO [11] BEiT [6] LVM-Med [48] MAE [32, 67] SimMIM [46, 68] Adam-v2 [33] Pre-train Data Backbone VinDr-CXR ShenZhen ChestX-ray14 RSNA (CLS) CheXpert - IN-1k (1.3M) IN-1k (1.3M) IN-21k (12M) Medical (1.3M) X-rays (0.5M) IN-21k (12M) & X-rays (0.9M) IN-21k (12M) & X-rays (0.9M) ViT-B ViT-B ViT-B ViT-B ViT-B ViT-B 70.221.95 82.240.60 71.690.32 66.590. 80.780.03 87.250.63 82.891.10 85.931.98 92.851.00 90.394.29 92.871.08 88.220.44 92.760.18 94.811.32 97.630.21 79.200.30 78.370.47 79.910. 80.080.09 83.0 72.790.52 71.270.45 72.780.37 72.750.44 73.750.24 87.120.36 87.010.62 87.770.38 88.070.25 89.3 Swin-B 92.810.31 98.090.13 83.040.15 74.090.39 89.140.22 ConvNeXt-B 91.460. 97.80 83.4 73.400.88 88.900.36 CheXWorld X-rays (0.5M) Rad-DINO [55] LVD (142M) & X-rays (public+private) ViT-B ViT-B 95.240.13 98.880. 83.580.05 75.030.39 89.630.13 95.160.16 98.200.17 83.610. 74.510.46 88.940.15 Results on ImageNet pre-trained models are adopted from [46]. Results reported by the original authors [32, 33]. Rad-DINO [55] is included for reference but not for direct comparison, as it requires 2560 GPU-hours (20 times more than ours) and is trained on both public and private datasets. and tissues within various masked areas, and Figure 4(b) shows that the model makes global-level predictions that are consistent with the context. Further visualizations also show that CheXWorld filters artifacts in the image (Figure 5) and learns anatomical correspondence (Figure 6). Awareness to domain variations. We design domain sensitivity test to verify the models ability to capture the uncertainty in radiographs appearance attributes. Specifically, we first generate candidate set of targets {yi}n i=1 by applying augmentations with different configurations to the same image. Then, we construct set of context-targetlatent triplets {(xi, yi, ai)} by applying another augmentation xi = Tai(yi). The model is asked to predict the target yi given the context xi and latent ai. For each predicted output ˆyi, we calculate the top-k recall rate of the true target yi over the entire candidate set using L2 distance2. As shown in Table 4, our model achieves an average top-5 recall of 77.67 (10 times higher than random choice), demonstrating the models strong discriminative ability across domains. We also observe significant drop in the recall rate when removing the domain condition a, indicating that the model indeed makes predictions conditioned on a. 5.2. CheXWorld Produces Transferable Representations Setup and baselines. We compare CheXWorld with several competitive baselines across eight classification and segmentation tasks using the following datasets: VinDr-CXR [50], ShenZhen-CXR [37], NIH ChestX-ray14 [65], CheXpert [36], MedFMC-ChestDR [64], SIIM-ACR Pneumothorax [1] and RSNA Pneumonia [58]. 2In our experiments the candidate set size is fixed at = 64. This process is repeated across multiple images, and the final result is obtained by averaging the outcomes. Table 4. Effectiveness of domain variation modeling. Method Recall@1 Recall@3 Recall@ Random Choice w/o domain condition CheXWorld 1.56 15.87 38.49 4.68 33.12 65.90 7.81 44.37 77.67 We attach task-specific heads (linear heads and U-Net decoders) and perform full fine-tuning on the pre-trained CheXWorld encoder. The input image resolution is fixed to 2242. We compare CheXWorld with several self-supervised learning methods designed for general domain images [6, 11, 14, 32, 68] and medical images [33, 48] using basescale backbones [18, 43, 44]. We use the radiology version of these methods [46, 55, 67] if available. Classification results are shown in Table 1. Our method consistently outperforms the best alternatives on all the benchmarks. Although SimMIM and Adam-v2 are initialized from ImageNet weights and pre-trained on large dataset (0.9M X-rays) and, yet still lag behind CheXWorld with considerable margins. For instance, CheXWorld surpasses Adam-v2 by 4.04 in AUROC on VinDr-CXR and SimMIM by 0.54 on ChestX-ray14. Our approach performs comparably to Rad-DINO across all benchmarks, despite Rad-DINO requiring 20 times more computational resources and utilizing private training data. Additionally, one can observe significant contrast in performance between models trained on photometric images (ImageNet) and models trained on medical images, highlighting the importance of in-domain transfer. Segmentation results are presented in Table 2 and Figure 8. CheXWorld surpasses the leading baselines in terms of dice score and produces more accurate segmentation masks, demonstrating its superior capability in both instance-level and dense prediction settings. Few-shot learning results on the MedFMC dataset are presented in Table 2. CheXWorld demonstrates clear ad7 Table 2. Results on segmentation (left) and few-shot learning (right) tasks. The dice score and the AUROC score are reported for the segmentation and few-shot learning benchmarks respectively. Method Segmentation SIIM-ACR RSNA (SEG) Few-shot (MedFMC-ChestDR) 5-shot 1-shot 10-shot LVM-Med [48] MAE [32, 67] SimMIM [46, 68] Adam-v2 [33] 82.190.30 83.010.39 82.890.07 83.600.33 78.470.24 77.390.25 78.310.41 78.530.19 57.550.81 61.310.27 60.641.92 59.161.03 66.950.60 74.030.30 72.140.54 70.280.39 67.440.71 75.260.36 74.420.38 70.670.71 CheXWorld 84.580.34 79.020.25 64.601.00 75.190.51 76.400.25 Figure 7. Fine-tuning with 1%, 10%, and 100% training data on VinDr-CXR. Table 3. Ablation studies of the world modeling tasks and latent variables. We report the results on VinDr-CXR and RSNA with 1% proportion of training data. AUROC and accuracy are reported for the respective datasets. Local., Global., and Domain. are abbreviations for local anatomical structure modeling, global anatomical layout modeling, and domain variation modeling, respectively. Unified indicates whether our unified formulation is applied, as opposed to naive combination of losses. World Modeling Task Latent Variables Local. Global. Domain. xy Unified VinDr 1% RSNA 1% N.A. 84.711.00 88.310.88 89.730.50 90.531.01 65.890.54 67.470.51 68.271.00 70.010.33 89.120. 68.370.50 89.910.60 87.600.72 68.980.60 66.560.39 Figure 8. Visualization of segmentation masks on SIIM-ACR pneumothorax dataset. GT stands for the ground truth masks. vantage on the 1-shot learning task and consistently outperforms the second-best methods in both 5-shot and 10-shot scenarios, showcasing quick adaptation to new concepts by leveraging knowledge acquired through world modeling. Sample efficiency results.3 The superiority of our method is further highlighted by reducing the training data proportion. As shown in Figure 7, we conduct experiments on VinDr-CXR with 1%, 10%, and 100% of the training data. Notably, our method with 10% of the data significantly outperforms all baselines trained on the entire dataset, reducing the annotation cost by over 90 percent. 5.3. Ablation Study Effectiveness of the world modeling tasks is validated in the upper part of Table 3. Combining local anatomical structure modeling with either of the other two world modeling tasks yields significant performance gains. The best performance is achieved when all three world modeling tasks are integrated within our unified framework, indicating that these tasks reflect different aspects of medical knowledge that are effectively captured by CheXWorld. Role of the latent variables. To verify that the latent variables (such as relative position xy for global anatomical layout modeling and transformation parameter for domain variation modeling) contribute to transfer learning performance, we test the models pre-trained without certain latent variables, shown in the bottom part of Table 3Detailed numerical results can be found in the appendix. 3. Domain variation modeling without the condition degrades into blind image restoration task, which yields inferior results compared with the original method. Removing the condition xy from global anatomical layout modeling leads to sharp decrease in performance because it confounds the model with erroneous target spatial locations. 6. Conclusion In this paper, we proposed CheXWorld, self-supervised world modeling framework for radiographic images that simultaneously encodes local and global anatomical knowledge and appearance domain variations. By integrating three world modeling tasks into unified pipeline, our model effectively captures the spatial and domain uncertainties of the image world, as verified by qualitative and quantitative analytical experiments. Extensive transfer learning experiments on eight medical image analysis benchmarks demonstrate the state-of-the-art performance of our model. Our work offers new insights into representing and extracting knowledge from medical images, paving the way toward general-purpose foundation model for medical vision. Acknowledgements. The work is supported in part by the National Key R&D Program of China under Grant 2024YFB4708200 and National Natural Science Foundation of China under Grant 62321005."
        },
        {
            "title": "References",
            "content": "[1] Zawacki Anna, Wu Carol, Shih George, Elliott Julia, Fomitchev Mikhail, Hussain Mohannad, Lakhani Paras, Culliton Phil, and Bao Shunxing. Siim-acr pneumothorax segmentation. 2019. 7, 1 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. 1, 3, 4, 2 [3] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, et al. Big self-supervised models advance medical image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pages 34783488, 2021. 3 [4] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: general framework for self-supervised learning in speech, vision and lanIn International Conference on Machine Learning, guage. pages 12981312. PMLR, 2022. 3 [5] Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-supervised learning with contextualized target representations for vision, speech and language. In International Conference on Machine Learning, pages 1416 1429. PMLR, 2023. 3 [6] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. 7, 1 [7] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning. 2023. 3 [8] Florian Bordes, Randall Balestriero, and Pascal Vincent. High fidelity visualization of what your self-supervised representation knows about. arXiv preprint arXiv:2112.09164, 2021. 6, 2 [9] Arthur Earl Bryson. Applied optimal control: optimization, estimation and control. Routledge, 2018. 2 [10] Eduardo Camacho, Carlos Bordons, Eduardo Camacho, and Carlos Bordons. Constrained model predictive control. Springer, 2007. [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 7, 1 [12] Krishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender Konukoglu. Contrastive learning of global and local features for medical image segmentation with limited annotations. Advances in neural information processing systems, 33:1254612558, 2020. 3 [13] Richard Chen, Tong Ding, Ming Lu, Drew FK Williamson, Guillaume Jaume, Andrew Song, Bowen Chen, Andrew Zhang, Daniel Shao, Muhammad Shaban, et al. Towards general-purpose foundation model for computational pathology. Nature Medicine, 30(3):850862, 2024. 3 [14] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96409649, 2021. 7, 1 [15] Peng Cui, Yang Yue, Zhijie Deng, and Jun Zhu. Confidencebased reliable learning under dual noises. Advances in Neural Information Processing Systems, 35:3511635129, 2022. [16] Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Cheung, Pulkit Agrawal, and Marin Soljaˇcic. Equivariant contrastive learning. arXiv preprint arXiv:2111.00899, 2021. 3, 5 [17] Alexandre Devillers and Mathieu Lefort. Equimod: An equivariance module to improve visual instance discrimination. In International Conference on Learning Representations, 2023. 3, 5 [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 3, 6, 7, 1 [19] Chaoqun Du, Yulin Wang, Shiji Song, and Gao Huang. Probabilistic contrastive learning for long-tailed visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3 [20] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. learned representation for artistic style. arXiv preprint arXiv:1610.07629, 2016. 2 [21] Cong Gao, Benjamin Killeen, Yicheng Hu, Robert Grupp, Russell Taylor, Mehran Armand, and Mathias Unberath. Synthetic data accelerates the development of generalizable learning-based algorithms for x-ray image analysis. Nature Machine Intelligence, 5(3):294308, 2023. 1 [22] Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes, Laurent Najman, and Yann LeCun. Learning and leveraging world models in visual representation learning. arXiv preprint arXiv:2403.00504, 2024. 1, [23] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580587, 2014. 6 [24] Hao Guan and Mingxia Liu. Domain adaptation for medical image analysis: survey. IEEE Transactions on Biomedical Engineering, 69(3):11731185, 2021. 2, 5 [25] David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018. 2 [26] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 3 [27] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. 9 [28] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. 2 [29] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. 3 [30] Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Zongwei Zhou, Michael Gotway, and Jianming Liang. Transferable visual words: Exploiting the semantics of anatomical patterns for self-supervised learning. IEEE transactions on medical imaging, 40(10):28572868, 2021. 3 [31] Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Michael Gotway, and Jianming Liang. Dira: Discriminative, restorative, and adversarial learning for self-supervised In Proceedings of the IEEE/CVF medical image analysis. Conference on Computer Vision and Pattern Recognition, pages 2082420834, 2022. 3 [32] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 3, 4, 7, 8, 1, 2 [33] Mohammad Reza Hosseinzadeh Taher, Michael Gotway, and Jianming Liang. Representing part-whole hierarchies in foundation models by learning localizability, composability, and decomposability from anatomy via self-supervision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. 3, 7, 8, 1, [34] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. 3 [35] Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas Montine, and James Zou. visuallanguage foundation model for pathology image analysis using medical twitter. Nature medicine, 29(9):23072316, 2023. 1 [36] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, pages 590597, 2019. 7, 1, 2 [37] Stefan Jaeger, Sema Candemir, Sameer Antani, Y`ı-Xiang Wang, Pu-Xuan Lu, and George Thoma. Two public chest xray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery, 4(6): 475, 2014. 7, 1 [38] Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Roger Mark, and Steven Horng. Mimic-cxr, deidentified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. 1, 2 [39] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. 3 [40] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [41] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1), 2022. 1, 2, 3 [42] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In European conference on computer vision, pages 280296. Springer, 2022. 3 [43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: In Hierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 7, 1, 2 [44] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. 7, 1 [45] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2018. [46] DongAo Ma, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Nahid UI Islam, Fatemeh Haghighi, Michael Gotway, and Jianming Liang. Benchmarking and boosting In MICCAI transformers for medical image classification. Workshop on Domain Adaptation and Representation Transfer, pages 1222. Springer, 2022. 7, 8, 1, 2 [47] Phillip Marsh. Case courtesy of phillip marsh, radiopaehttps : / / radiopaedia . org / cases / dia.org. 58938, 2023. Case ID: rID: 58938. 2 [48] Duy MH Nguyen, Hoang Nguyen, Nghiem Diep, Tan Ngoc Pham, Tri Cao, Binh Nguyen, Paul Swoboda, Nhat Ho, Shadi Albarqouni, Pengtao Xie, et al. Lvm-med: Learning largescale self-supervised vision models for medical imaging via second-order graph matching. Advances in Neural Information Processing Systems, 36, 2024. 7, 8, 1, 2 [49] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan Krumholz, Jure Leskovec, Eric Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259265, 2023. 1 [50] Ha Nguyen, Khanh Lam, Linh Le, Hieu Pham, Dat Tran, Dung Nguyen, Dung Le, Chi Pham, Hang TT Tong, Diep Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologists annotations. Scientific Data, 9(1):429, 2022. 7, Video generation models as world simula- [51] OpenAI. https : / / openai . com / index / video - tors. generation - models - as - world - simulators/, 2024. Accessed: Feb, 2024. 3 [52] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. 10 Transactions on Machine Learning Research Journal, pages 131, 2024. 3, 2 [53] Cheng Ouyang, Chen Chen, Surui Li, Zeju Li, Chen Qin, Wenjia Bai, and Daniel Rueckert. Causality-inspired singlesource domain generalization for medical image segmentation. IEEE Transactions on Medical Imaging, 42(4):1095 1106, 2022. 2, 5 [54] Jiaxuan Pang, Fatemeh Haghighi, DongAo Ma, Nahid Ul Islam, Mohammad Reza Hosseinzadeh Taher, Michael Gotway, and Jianming Liang. Popar: Patch order prediction and appearance recovery for self-supervised medical image analysis. In MICCAI Workshop on Domain Adaptation and Representation Transfer, pages 7787. Springer, 2022. 3 [55] Fernando Perez-Garcıa, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel Castro, Anton Schwaighofer, Matthew Lungren, et al. Rad-dino: Exploring scalable medical arXiv preprint image encoders beyond text supervision. arXiv:2401.10815, 2024. 3, 7, 1, [56] Eric Postal. Why dont radiology textbooks have imperfect images? https://www.diagnosticimaging.com/ view/why-dont-radiology-textbooks-haveimperfect-images, 2019. Accessed: Feb, 2024. 2, 5 [57] RunwayML. Introducing general world models. https: / / research . runwayml . com / introducing - general-world-models, 2023. Accessed: May, 2023. 2 [58] George Shih, Carol Wu, Safwan Halabi, Marc Kohli, Luciano Prevedello, Tessa Cook, Arjun Sharma, Judith Amorosa, Veronica Arteaga, Maya GalperinAizenberg, et al. Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. Radiology: Artificial Intelligence, 1(1): e180041, 2019. 7, 1 [59] Ben Sorscher, Surya Ganguli, and Haim Sompolinsky. Neural representational geometry underlies few-shot concept learning. Proceedings of the National Academy of Sciences, 119(43):e2200800119, 2022. 1 [60] Hari Sowrirajan, Jingbo Yang, Andrew Ng, and Pranav Rajpurkar. Moco pretraining improves representation and In Medical Imaging transferability of chest x-ray models. with Deep Learning, pages 728744. PMLR, 2021. 3 [61] Mohammad Reza Hosseinzadeh Taher, Fatemeh Haghighi, Michael Gotway, and Jianming Liang. Caid: selfsupervised learning framework for empowering instance discrimination in medical imaging. Proceedings of Machine Learning Research, 3, 2022. 3 [62] Xing Tao, Yuexiang Li, Wenhui Zhou, Kai Ma, and Yefeng Zheng. Revisiting rubiks cube: self-supervised learning with volume-wise transformation for 3d medical image segmentation. In Medical Image Computing and Computer Assisted InterventionMICCAI 2020: 23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part IV 23, pages 238248. Springer, 2020. [63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4, 2 [64] Dequan Wang, Xiaosong Wang, Lilong Wang, Mengzhang Li, Qian Da, Xiaoqiang Liu, Xiangyu Gao, Jun Shen, Junjun He, Tian Shen, et al. real-world dataset and benchmark for foundation model adaptation in medical image classification. Scientific Data, 10(1):574, 2023. 7, 1 [65] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. Chestxray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20972106, 2017. 2, 7, 1 [66] Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, and Gao Huang. Efficienttrain: Exploring generalized curriculum learning for training visual backbones. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58525864, 2023. 3 [67] Junfei Xiao, Yutong Bai, Alan Yuille, and Zongwei Zhou. Delving into masked autoencoders for multi-label thorax disIn Proceedings of the IEEE/CVF Winease classification. ter Conference on Applications of Computer Vision, pages 35883600, 2023. 3, 7, 8, 1, 2 [68] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96539663, 2022. 7, 8, 1, [69] Ke Yan, Jinzheng Cai, Dakai Jin, Shun Miao, Dazhou Guo, Adam Harrison, Youbao Tang, Jing Xiao, Jingjing Lu, and Le Lu. Sam: Self-supervised learning of pixel-wise anatomical embeddings in radiological images. IEEE Transactions on Medical Imaging, 41(10):26582669, 2022. 3 [70] Hong-Yu Zhou, Shuang Yu, Cheng Bian, Yifan Hu, Kai Ma, and Yefeng Zheng. Comparing to learn: Surpassing imagenet pretraining on radiographs by comparing image representations. In Medical Image Computing and Computer Assisted InterventionMICCAI 2020: 23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part 23, pages 398407. Springer, 2020. 3 [71] Lei Zhou, Huidong Liu, Joseph Bae, Junjun He, Dimitris Samaras, and Prateek Prasanna. Self pre-training with masked autoencoders for medical image classification and segmentation. In 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI), pages 16. IEEE, 2023. 3 [72] Yukun Zhou, Mark Chia, Siegfried Wagner, Murat Ayhan, Dominic Williamson, Robbert Struyven, Timing Liu, Moucheng Xu, Mateo Lozano, Peter WoodwardCourt, et al. foundation model for generalizable disease detection from retinal images. Nature, 622(7981):156163, 2023. 1, 3 [73] Zongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael Gotway, and Jianming Liang. Models genesis. Medical image analysis, 67:101840, 2021. 2, 3 [74] Ziyu Zhou, Haozhe Luo, Jiaxuan Pang, Xiaowei Ding, Michael Gotway, and Jianming Liang. Learning anatomically consistent embedding for chest radiography. arXiv preprint arXiv:2312.00335, 2023. 2, 3 CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Datasets and Baselines A.1. Datasets Our experiments are based on eight chest X-ray datasets, including MIMIC-CXR [38] for pre-training; CheXpert [36] and NIH ChestX-ray14 [65] for both pre-training and fine-tuning; VinDr-CXR [50], ShenZhen-CXR [37], RSNA Pneumonia [58], MedFMC-ChestDR [64], and SIIM-ACR Pneumothorax [1] for fine-tuning. Detailed information on these datasets is provided below. MIMIC-CXR [38] is one of the largest X-ray datasets, containing over 370k radiograph images from over 220,000 patient studies with paired radiology reports. We gather non-lateral scans from this dataset (about 230k images) and use this dataset for self-supervised pre-training. CheXpert [36] contains about 218k images with 14 disease labels automatically extracted from radiology reports. We use this dataset for pre-training and conduct multi-label classification experiments on five conditions: atelectasis, cardiomegaly, consolidation, edema, and effusion. We report the performance on the official validation set (200 patients) with held-off subset from the training set for model selection. The mean AUROC score over the five classes is reported for this dataset. NIH ChestX-ray14 [65] contains about 112k frontalview chest radiographs, with annotations on 14 thoracic diseases: atelectasis, cardiomegaly, consolidation, edema, effusion, emphysema, fibrosis, hernia, infiltration, mass, nodule, pleural thickening, pneumonia, and pneumothorax. We use the training split of this dataset for pretraining and conduct disease classification experiments on the 14 classes. We follow the official split with 86k images for training and 25k for testing. The mean AUROC score over the 14 classes is reported for this dataset. VinDr-CXR contains 18,000 radiographs with expert annotations. Each radiograph is associated with 22 local findings and 6 global findings. We consider the multilabel classification task on the 6 global labels, including lung tumor, pneumonia, tuberculosis, other diseases, COPD, and no finding. We adopt the official split with 15,000 images for training and 3,000 images for testing. The mean AUROC score over the 6 classes is reported for this dataset. ShenZhen-CXR defines binary classification problem where each radiograph is labeled with the presence of tuberculosis. We follow the data split provided by [46] with the train/val/test split containing 463/65/134 images, respectively. The AUROC score is reported for this dataset. RSNA Pneumonia [58] consists of over 26k radiographs, each categorized into one of three classes: normal, lung opacity, or no opacity but not normal. Additionally, expert-annotated bounding boxes highlight areas of lung opacity. This dataset is used for both classification and segmentation tasks. For classification, we frame it as three-class problem, reporting top-1 accuracy. For segmentation, the bounding boxes are converted into segmentation masks and the mean dice score is reported. We follow the data split provided by [46] with train/val/test split containing 21295/2680/2709 images, respectively. MedFMC-ChestDR [64] is dataset tailored for fewshot adaptation. Each radiograph is associated with 19 common thoracic disease labels. The official competition consists of 1-shot, 5-shot, and 10-shot tracks, each with five different train/val splits. To ensure consistency, we use the first split in each track and report the mean performance averaged over five random seeds. The mean AUROC score is reported over the 19 classes for this dataset. SIIM-ACR Pneumothorax [1] comprises 12,047 radiographs with pixel-level annotations for pneumothorax. We perform binary segmentation on this dataset, with the mean dice score reported as the evaluation metric. A.2. Baselines We compare CheXWorld with several self-supervised learning methods developed for general-domain and medical images, including MoCo-v3 [14], DINO [11], BEiT [6], MAE [32, 67], SimMIM [46, 68], LVM-Med [48], Adamv2 [33], and Rad-DINO [55]. When possible, we leverage radiology-specific adaptations of these methods. For fair comparison, all methods utilize models of comparable sizes, such as ViT-B [18], Swin-B [43], and ConvNeXt-B [44]. Below, we provide brief overview of each approach: MoCo-v3 [14] is contrastive learning framework that employs momentum encoder to create dynamic dictionary for stable and effective representation learning. It explores additional training techniques to optimize vision transformer performance. DINO [11] pre-trains vision transformers with selfdistillation objective. Techniques like distribution centering and sharpening are incorporated to stabilize the training process. BEiT [6] is masked image modeling (MIM) approach inspired by masked language modeling in natural language processing. The model predicts masked token in1 dices generated by discrete variational autoencoders. MAE [32] is an encoder-decoder framework for MIM, predicting raw pixel values for masked patches. Only visible patches are passed to the encoder to improve computational efficiency. We use its radiology-adapted version introduced by [67]. SimMIM [68] is another MIM approach based on the Swin Transformer [43]. It employs random masking with moderately large patch size and uses simple linear decoder head. The radiology-adapted version from [46] is used in our experiments. LVM-Med [48] leverages graph-matching formulation for contrastive learning, building versatile model that integrates diverse medical image modalities and datasets. Adam-v2 [33] focuses on learning anatomical structures in X-ray images hierarchically, using pre-training objectives that promote localizability, composability, and decomposability. Rad-DINO [55] extends DINOv2 [52] by performing continuous pre-training on radiology datasets. B. Implementation Details B.1. Pre-training Data. CheXWorld is pre-trained on the combination of three datasets: MIMIC-CXR [38], NIH ChestX-ray14 [65], and CheXpert [36] (following [67]). We only use the frontal scans for pre-training, resulting in 0.5M radiographs in total. We exclude the validation/test split of the NIH ChestXray14 and CheXpert datasets from the pre-train dataset to avoid data leakage to the downstream tasks. Architecture and optimization. The context encoder is ViT-Base with patch size of 16 16. The target encoder is the exponential moving average of the context encoder with an initial ratio equal to 0.996 that gradually increases to 1.0 following cosine schedule. The predictor is 6 layers deep with 384-dimensional embeddings. We use sinusoidal functions [63] to encode the image patch positions following [32]. We use the AdamW optimizer [45] with β1 = 0.9, β2 = 0.999 with an initial learning rate of 2 104 and weight decay set to 0.05. Gradient clipping is set to 1.0 throughout our experiments. The learning rate schedule follows linear warmup for 40 epochs and cosine annealing afterward. L2 loss is computed between the raw predictor outputs and the layer-normalized target encoder outputs. The model is trained from scratch for 300 epochs with batch size of 2048, taking 16 hours on machine with 8 RTX 4090 GPUs, each with 24 GB memory. Local anatomical structure modeling. We adopt block-wise masking strategy [2]. The image mask is the union of four rectangular blocks with the scale (0.15, 0.2). We further shrink the contexts visible area by maximal factor of 0.25, which we found beneficial. The context encoder only processes unmasked patches, while the entire image takes the entire image as input. In the predictor, mask tokens corresponding to the masked locations are padded to the context. The loss is computed on masked locations. Global anatomical structure modeling. We sample two random crops with the same spatial size with their scales in (0.3, 1.0) and aspect ratios in (0.75, 1.33). The relative position information xy is obtained in pixel space and then used to determine the location of target image patches in the contexts coordinate system. Note that the sinusoidal encoding function PE() supports fractional inputs. Thus, the target patch locations ϕxy(u, v) can be encoded in the same way as the context patch locations. We compute prediction loss on all target patches. Domain variation modeling. We simulate domain transitions with set of augmentations, including brightness, contrast, gamma transform, and Gaussian blur. Given an input image (or an image crop), the target is obtained by applying brightness and contrast adjustment to the original image. Then, we apply another augmentation consisting of bright, contrast, gamma transform, and Gaussian blur, with the configurations of the augmentation stored in the parameter a. In particular, consists of four scalars: the factor for brightness enhancement in the range (0.6, 1.4), the factor for contrast adjustment in the range (0.6, 1.4), the factor for gamma transform in the range (0.5, 2.0) and the kernel size of the Gaussian blur in the range (0.05, 2.0). Essentially, the context is obtained by augmenting the original image twice, where the second augmentation is modeled by CheXWorld. Domain variation modeling is implemented along with local or global anatomical modeling. The parameter is concatenated with the mask token Rd along the feature dimension, resulting in vector of length + 4, which is then fed into the policy network π. The policy network π is three-layer MLP with an input dimension of + 4 and an output dimension of d. B.2. Analytical Experiments Anatomical modeling visualization. We utilize the RCDM framework [8] to showcase the anatomical modeling capabilities of CheXWorld. Specifically, we train diffusion model to predict target pixel values conditioned on the output representation ˆhy of the world model. This guiding representation is first projected to 512-dimensional vector, which is then injected into the diffusion model via conditional batch normalization layers [20] within each residual block. For local anatomical structure modeling, the diffusion model individually predicts four rectangular masked regions, guided by spatially pooled predictor outputs corresponding to each location. For global anatomical layout modeling, the model predicts the entire target region using spatially pooled outputs from the predictor. Figure 5 is built upon local anatomical modeling, focusing on with layer-wise decay rate of 0.8 and drop path rate of 0.1. The data preprocessing pipeline for training involves random brightness contrast, shifting, and scaling. Due to the varying sizes of the datasets, we employ different batch sizes and epochs across benchmarks. The input size of the image is set to 224 224 pixels. 10% of the training data is used for validation. Each experiment is conducted five times. C. Numerical Results Figure 7 illustrates the fine-tuning performance of CheXWorld on the VinDr-CXR dataset using varying proportions of the training data, which highlights CheXWorlds ability to enhance data efficiency. Here, we present the corresponding numerical results in Table 5. Table 5. Fine-tuning with 1%, 10%, and 100% training data on VinDr-CXR."
        },
        {
            "title": "Method",
            "content": "1% 10% 100% LVM-Med Adam-v2 MAE 76.413.79 77.901.14 78.071.66 SimMIM 83.851.62 85.850.59 88.260.48 90.630.16 92.150. 88.220.44 91.460.33 92.760.18 92.810."
        },
        {
            "title": "CheXWorld",
            "content": "90.531.01 94.710.10 95.240.12 masked regions with visible artifacts. The diffusion model is trained using the validation split of the NIH ChestXray14 dataset, while the visualizations are generated from the test split. This separation ensures that there is no information leakage between the different stages of the experimentCheXWorld pre-training, diffusion model training, and visualization. Anatomical Correspondence Visualization. We input the entire radiograph into the CheXWorld encoder to obtain image patch embeddings. Then we calculate per-pixel feature embeddings using RoI pooling over 2x2 window centered on the pixel location. To illustrate anatomical correspondence, we focus on four key anatomical landmarks: the aortic arch, right hilum, left ventricle, and clavicle. The final similarity map is computed by measuring the L2 distance between the landmark embeddings of the reference image and the pixel embeddings of the test image. For improved visualization, the similarity values are rescaled. Domain sensitivity test. To evaluate how effectively CheXWorld handles domain variations, we construct test dataset using different augmentation configurations applied to the same base image. Specifically, we sample = 64 augmentation parameters evenly from predefined range and apply these augmentations to generate candidate set of target images {yi}n i=1. For each target yi, we further apply randomly sampled augmentation to obtain the corresponding context xi = Tai (yi), resulting in set of contexttarget-latent triplets {(xi, yi, ai)}. The models task is to predict the target yi given the context xi and latent ai. The prediction error is defined as: L(y; x, a) = g(fθ(x); a) θ(y)2. (13) Ideally, the prediction error L(yi, xi, ai) should be smaller than L(yj, xi, ai) for any = i. For the i-th case, we rank the errors {L(yj, xi, ai)}n j=1 across the candidate set and compute the top-k recall rate of the true target yi. This procedure is repeated across 50 different images, and the final result is the averaged outcome over these trials. B.3. Fine-tuning For classification, we employ mean pooling over all the output tokens to obtain global feature representation of the image. Subsequently, task-specific linear head is attached to the model for fine-tuning. We utilize the AdamW optimizer with default learning rate of 1 104, with layerwise decay set to 0.75 and drop path rate of 0.6. For the CheXpert benchmark, we adopt learning rate of 1 102 and drop path of 0.1. The data augmentation pipeline involves random resized cropping and color jittering. For segmentation, we connect U-Net decoder with the pre-trained backbone with SimpleFPN [42] adapter. The U-Net decoder has four stages with number of channels 8, 16, 32, and 64. The initial learning rate is set to 2"
        }
    ],
    "affiliations": [
        "PLA General Hospital",
        "Tsinghua University"
    ]
}