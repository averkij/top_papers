{
    "paper_title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
    "authors": [
        "Sixiao Zheng",
        "Zimian Peng",
        "Yanpeng Zhou",
        "Yi Zhu",
        "Hang Xu",
        "Xiangru Huang",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 1 3 5 7 0 . 2 0 5 2 : r VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation SIXIAO ZHENG, Fudan University, China ZIMIAN PENG, Zhejiang University, China YANPENG ZHOU, Huawei Noahs Ark Lab, China YI ZHU, Huawei Noahs Ark Lab, China HANG XU, Huawei Noahs Ark Lab, China XIANGRU HUANG, Westlake University, China YANWEI FU, Fudan University, China Fig. 1. VidCRAFT3 is high-quality image-to-video generation model that supports large object motion, view changes, and strong lighting effects. It offers user-friendly control over camera motion (a trajectory in blue), object motion (sparse trajectories in red), and lighting direction. VidCRAFT3 can take any combination of supported control signals and deliver fine-grained and faithful generation results. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2025 Association for Computing Machinery. 0730-0301/2025/2-ART $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, novel framework for precise imageto-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in symmetric way. Since most ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. 2 Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu real-world video datasets lack lighting annotations, we construct highquality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/. CCS Concepts: Computing methodologies Artificial intelligence; Image manipulation. Additional Key Words and Phrases: Diffusion models, Image-to-video generation, Motion Control, Lighting Control ACM Reference Format: Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu. 2025. VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation. ACM Trans. Graph. 1, 1 (February 2025), 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "INTRODUCTION",
            "content": "Image-to-video (I2V) generation is powerful technique that brings still images to life, enabling widespread applications in the fields of content creation, advertising, and art animation. Typically, I2V generation aims to animate reference image according to userprovided control signals, such as text descriptions, object motion, and camera motion, while maintaining the quality and natural dynamics of the generated video. Thanks to the availability of largescale web data and the development of highly expressive neural architectures, I2V has made significant progress in recent years. Early research work has demonstrated their ability to generate temporally consistent videos [Chen et al. 2023b; Guo et al. 2024, 2023; Xing et al. 2025]. More recently, researchers have been working to incorporate wider range of control signals into I2V models, building on the successes in the field of image manipulation [Geng et al. 2024; He et al. 2024b; Li et al. 2024; Wang et al. 2024d]. Training I2V generation models require high-quality video datasets that are annotated with all forms of control signals. However, such datasets are difficult to obtain as additional control signals are considered. To mitigate this issue, previous approaches [Li et al. 2024; Mou et al. 2024; Wang et al. 2024d] propose using powerful perception models to generate accurate annotations of some forms of control signals from real-world videos. However, such powerful models may not be available for other forms of control signals. For example, predicting the lighting from real-world videos is an ill-posed problem. Lighting is intrinsically tied to an objects appearance model, specifically how much light each object scatters, reflects, and transmits [Pharr et al. 2023]. For lighting control, techniques like DILightNet [Zeng et al. 2024a] and NeuLighting [Li et al. 2022], while effective for static scenes, struggle to generalize to dynamic videos due to their reliance on precomputed radiance fields or datasets that lack temporal lighting annotations. This limitation is further compounded by recent work on lighting control in I2V generation [Huynh et al. 2021; Lin et al. 2024; Qiu et al. 2024; Zhang et al. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. 2021], which primarily focuses on specific object categories, such as human faces, resulting in limited generalization to open-domain objects. To overcome these challenges, we propose VidCRAFT3, which integrates three core technical advancements. First, the Image2Cloud module leverages DUSt3R [Wang et al. 2024b] to reconstruct 3D point cloud from single reference image, enabling precise camera motion control by rendering the point cloud along user-defined camera trajectories. Second, ObjMotionNet encodes sparse object motion trajectories by extracting multi-scale motion features from Gaussian-smoothed optical flow maps to guide realistic object motion. Third, the Spatial Triple-Attention Transformer integrates lighting direction control by integrating lighting embedding with imagetext embeddings through parallel cross-attention layers, ensuring consistent illumination effects. The capabilities of these modules are illustrated in Fig. 1, showcasing VidCRAFT3s control over camera motion, object motion, and lighting direction. To address data scarcity, we construct the VideoLightingDirection (VLD) dataset, synthetic video dataset annotated with per-frame lighting directions, capturing complex light interactions such as reflections. threestage training strategy progressively trains the model, from camera motion control to sparse trajectory adaptation, without requiring joint multi-element annotations. The main contributions of this work can be summarized as follows: (1) VidCRAFT3 is the first I2V model to achieve simultaneous control over camera motion, object motion, and lighting direction through disentangled architecture combining 3D point cloud rendering, trajectory-based motion encoding, and lightingaware attention mechanisms. (2) We introduce the VLD dataset, providing temporally consistent lighting annotations for diverse objects and scenes, overcoming the limitations of category-specific or static lighting datasets. (3) three-stage training strategy enables robust multi-element control without requiring real-world videos annotated with camera, object, and lighting signals. (4) Extensive experiments demonstrate that VidCRAFT3 achieves state-of-theart performance across diverse scenarios, outperforming existing methods in control precision and visual realism."
        },
        {
            "title": "2 RELATED WORK\n2.1",
            "content": "Image-to-video Generation Image-to-video (I2V) generation aims to animate static images into dynamic videos while preserving visual content and introducing realistic motion. Recent advances in diffusion models [Ho et al. 2020; Sohl-Dickstein et al. 2015] have revolutionized video synthesis by extending pre-trained T2I models like AnimateDiff [Guo et al. 2023] to incorporate temporal dimensions for motion generation. These methods integrate the input image as condition, either through CLIP-based [Radford et al. 2021] image embeddings or by concatenating the image with noisy latent representations. For example, VideoCrafter1 [Chen et al. 2023b], DynamiCrafter [Xing et al. 2025], and I2V-Adapter [Guo et al. 2024] use dual cross-attention layers to fuse image embeddings with noisy frames, ensuring spatial-aligned guidance. Similarly, Stable Video Diffusion (SVD) [Blattmann et al. 2023] replaces text embeddings with CLIP image embeddings, maintaining semantic consistency in an image-only manner. Another line of work, exemplified by SEINE [Chen et al. 2023a], DynamiCrafter [Xing et al. 2025] and PixelDance [Zeng et al. 2024b], expands the input channels of diffusion models to concatenate the static image with noisy latents, effectively injecting image information into the model. However, these methods preserve input image fidelity while generating dynamic videos but often struggle with fine-grained details due to reliance on global conditions."
        },
        {
            "title": "2.2 Motion-controlled Video Generation",
            "content": "Motion-controlled video generation is key research direction in video synthesis, focusing on creating high-fidelity videos that follow user-defined motion dynamics. Existing methods fall into three categories: (1) camera motion control, (2) object motion control, and (3) joint motion control. For camera motion control, MotionCtrl [Wang et al. 2024d] directly inputs the camera extrinsics, while CamI2V [Zheng et al. 2024], CameraCtrl [He et al. 2024b], and CamCo [Xu et al. 2024] encode camera parameters into Plücker embeddings for more discriminative motion representation. ViewCrafter [Yu et al. 2024] and I2VControl-Camera [Feng et al. 2024a] use 3D representations of reference images to render point clouds and generate partial frames. For object motion control, existing methods employ diverse strategies to guide object movements in video synthesis. DragNUWA [Yin et al. 2023], Image Conductor [Li et al. 2024], DragAnything [Wu et al. 2025], and MotionBridge [Tanveer et al. 2024] utilize sparse optical flow to control object motion, while MOFA-Video [Niu et al. 2025] and Motion-I2V [Shi et al. 2024] predicts dense motion fields to control the video generation. Alternatively, Boximator [Wang et al. 2024e] and Direct-A-Video [Yang et al. 2024] rely on bounding box trajectories to guide object movements. LeviTor [Wang et al. 2024c] introduces an approach by combining depth with K-means clustered points, achieving precise 3D trajectory control. Current research on Joint Motion Control, which aims to simultaneously control both camera and object motions, remains relatively limited. MotionCtrl designs specialized modules for camera and object motion, enabling precise control over both. I2VControl [Feng et al. 2024b] conceptualizes video as motion units, using trajectory functions as control signals. Motion Prompting [Geng et al. 2024] uses motion tracks as conditioning signals for flexible control, while Perception-as-Control [Chen et al. 2025] leverages 3D-aware motion representations for fine-grained control. Despite these advances, existing methods struggle to fully disentangle camera and object motion or maintain temporal consistency in complex scenes. Our approach solves this with point cloud-based camera motion control, decoupling the two motions effectively."
        },
        {
            "title": "2.3 Lighting-controlled Visual Generation",
            "content": "Lighting-controlled visual generation aims to manipulate illumination while preserving scene geometry and materials. Recent advances in diffusion models have significantly enhanced lighting control quality and flexibility. Methods like DiLightNet [Zeng et al. 2024a] and GenLit [Bharadwaj et al. 2024] achieve fine-grained and realistic relighting through radiance hints and SVD, respectively. Facial relighting methods, including DifFRelight [He et al. 2024a] and DiFaReli [Ponglertnapakorn et al. 2023], produce high-quality portrait images, while frameworks like NeuLighting [Li et al. 2022] VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation 3 focus on outdoor scenes using unconstrained photo collections. Additionally, Lite2Relight [Rao et al. 2024] and GSR [Poirier-Ginter et al. 2024] combine diffusion models with neural radiance fields for realistic 3D-aware relighting. Extending lighting control to video introduces challenges like temporal consistency and dynamic lighting effects. Recent techniques leverage 3D-aware generative models for temporally consistent relighting, as seen in EdgeRelight360 [Lin et al. 2024] and ReliTalk [Qiu et al. 2024]. Neural rendering approaches [Cai et al. 2024; Zhang et al. 2021] use datasets like dynamic OLAT for high-quality portrait video relighting, while reflectance fieldbased methods [Huynh et al. 2021] infer lighting from exemplars. Despite these advancements, most prior works focus on portraits and HDR lighting conditions. In contrast, we target general scenes with directional lighting, allowing users to interactively adjust lighting directions in fast and intuitive manner."
        },
        {
            "title": "3 METHOD\n3.1 Overview",
            "content": "We present VidCRAFT3, novel image-to-video generation model that, for the first time, enables simultaneous control over camera motion, object motion, and lighting direction. VidCRAFT3 comprises three core components: Image2Cloud, which converts the input image into 3D point cloud for accurate camera motion control; ObjMotionNet, which encodes sparse object motion trajectories to guide object movements; and Spatial Triple-Attention Transformer, which integrates lighting direction with image and text features to enable fine-grained lighting control. Given reference image 𝐼 and text description, image-to-video generation aims to generate sequence of video frames 𝐼 = {𝐼1, 𝐼2, . . . , 𝐼𝐹 }. In this paper, users can additionally provide three types of control signals: camera motion, object motion, and lighting direction. Specifically, camera motion is represented by sequence of camera extrinsics 𝐸 = {𝐸1, 𝐸2, . . . , 𝐸𝐹 }; object motion is represented by sparse trajectories of image pixels {𝑠1 𝐹 }, . . .; the lighting direction is , . . . , 𝑠2 , 𝑠2 , . . . , 𝑠1 3 1 simply unit 3d vector 𝐿 𝑹3. 𝐹 }, {𝑠2 1 , 𝑠1 2 , 𝑠2 2 , 𝑠"
        },
        {
            "title": "3.2 Model Architecture",
            "content": "Camera Motion Control via Point Cloud Rendering. To achieve precise camera motion control in image-to-video generation, we first reconstruct high-quality 3D representation of the scene from single input image. This step ensures spatial consistency and dynamic variation in the generated video. We use the Image2Cloud module, specifically DUSt3R, to convert the image into 3D point cloud and estimate camera parameters. DUSt3R, an unconstrained stereo 3D reconstruction model, performs monocular and binocular reconstruction via point regression and uses global alignment strategy for multi-view reconstruction. The input image is duplicated to create paired inputs, allowing for point cloud and camera parameter estimation. The reconstructed point cloud is then rendered along the input camera motion trajectory to produce point cloud renderings 𝑅 = 𝑟1, 𝑟2, . . . , 𝑟𝐹 . However, due to the limitations of point cloud representation and 3D cues from single image, artifacts such as missing regions, occlusions, and distortions may affect rendering quality. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. 4 Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu Fig. 2. Architecture of VidCRAFT3 for controlled image-to-video generation. The model builds on Video Diffusion Model (VDM) and consists of three main components: the Image2Cloud module generates high-quality 3D point cloud renderings from reference image; the ObjMotionNet module encodes object motion represented by sparse trajectories, the output is integrated into the UNet encoder by element-wise addition; the Spatial Triple-Attention Transformer module integrates image, text, and lighting information via parallel cross-attention modules. The model generates video by conditioning on camera motion, object motion, and lighting direction, ensuring realistic and consistent outputs across different modalities. To improve the quality of point cloud rendering from the cameras perspective, we train VDM conditioned on the input reference image, text description, and point cloud renderings, generating highquality and temporally consistent video outputs. As illustrated in Fig. 2, our model is based on the open-source image-to-video generation model DynamiCrafter, which consists of VAE encoder and decoder for image compression, UNet architecture with spatial and temporal layers, and CLIP Text Encoder, CLIP Image Encoder. We adopt dual-stream injection approach to input both point cloud renderings and the reference image into UNet. Specifically, the reference image is encoded using the CLIP Image Encoder, and the encoded features are injected into UNet via Image Cross-Attention. Meanwhile, we replace the first frame of the point cloud rendering with the reference image, encode it using the VAE Encoder, and then concatenate the encoded image with noise along the channel dimension before feeding it into UNet. Object Motion Control through Trajectory Learning. Object motion is represented as set of sparse spatial trajectories that capture the movement of objects across frames. Each trajectory consists of pixel coordinates (𝑥, 𝑦) over 𝐹 frames for up to 𝑁 objects, 𝑓 𝑛 = (𝑥 𝑓 𝑛 ) 𝑛 = 1, 2, . . . , 𝑁 ; 𝑓 = formally defined as = {s 𝑓 𝑛 denotes the position of the 𝑛-th object in the 𝑓 - 1, 2, . . . , 𝐹 }, where th frame. Then, we compute the optical flow between consecutive 𝑓 𝑓 𝑛 , the optical flow vector frames. For each trajectory point 𝑛 is 𝑓 𝑓 𝑛 = (𝑥 𝑓 +1 𝑛 𝑥 𝑓 𝑛 𝑦 𝑓 𝑛 ). These sparse 𝑛 = computed as motion vectors are assigned to their corresponding pixel positions in the optical flow map 𝑓 (𝑥, 𝑦): 𝑛 , 𝑦 𝑓 +1 𝑓 +1 𝑛 𝑛 , 𝑦 𝑓 𝑓 (𝑥, 𝑦) = (cid:40) 𝑓 𝑛 (0, 0) if (𝑥, 𝑦) = (𝑥 𝑓 +1 otherwise. 𝑛 , 𝑦 𝑓 +1 𝑛 ), (1) ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. The optical flow for the first frame is initialized as zero tensor V1 (𝑥, 𝑦) = (0, 0), (𝑥, 𝑦) [𝐻,𝑊 ]. After processing all frames, the resulting optical flow map R𝐹 𝐻 𝑊 2, where 𝐻 and 𝑊 are the height and width of the map. To transform these optical flow into dense motion representations, we apply Gaussian smoothing to the optical flow maps. The ObjMotionNet is an object motion encoder specifically designed to process the smoothed optical flow tensor R𝐹 𝐻 𝑊 2. It extracts multi-scale motion features Φ( V) through series of convolutional layers with downsampling operations, enabling the network to capture both local and global motion dynamics effectively. The extracted features Φ( V) are integrated into the UNet encoder by element-wise addition to its convolutional inputs. By focusing on encoder-level integration, ObjMotionNet ensures balance between precise motion control and high visual fidelity. Lighting Direction Control with Spatial Triple-Attention Transformer. The lighting direction is represented as per-frame 3D vector 𝐿 = (𝑙𝑥 , 𝑙𝑦, 𝑙𝑧), describing the orientation of the light source in Cartesian coordinates. To effectively encode this directional information into high-dimensional feature space, we employ Spherical Harmonic (SH) Encoding. SH encoding captures the angular characteristics of the lighting using basis functions up to degree 4, resulting in 16 coefficients. The resulting SH-encoded vector 𝐿SH R16 is projected into the feature space of the UNet using multi-layer perceptron (MLP). Elight = MLP(𝐿SH), where Elight is lighting embedding aligned with the dimensionality of text embedding. To incorporate the lighting embedding into the UNet, we propose the lighting cross-attention, which integrates the encoded lighting embedding Elight into the model. This attention mechanism modulates the spatial features based on the input lighting direction. The operation is defined as: Attention(𝑄, 𝐾, 𝑉 ) = Softmax (cid:19) (cid:18) 𝑄𝐾 𝑑 𝑉 , (2) where 𝑄 (Query) comes from the self-attention output of the UNet, 𝐾 (Key) and 𝑉 (Value) are derived from Elight. To enhance multimodal interactions, the lighting cross-attention is combined with two existing cross-attention layers, image cross-attention and text cross-attention. The proposed Spatial Triple-Attention Transformer integrates these three attention modules in parallel, with the outputs summed to produce fused feature representation = Oimage + Otext + Olight where Oimage, Otext, and Olight are the outputs of the image, text, and lighting cross-attention modules, respectively. This novel mechanism ensures the generated videos maintain consistency across lighting, textual, and image-based conditions."
        },
        {
            "title": "3.3 Training Strategy",
            "content": "We introduce three-stage training strategy to enable precise control over camera motion, object motion, and lighting direction in image-to-video generation. This approach fine-tunes the model progressively, balancing global and local feature learning while optimizing computational efficiency. Stage 1: Camera Motion Control Training. We fine-tune the entire UNet architecture using the Camera Motion Control Dataset, which provides camera poses and point cloud renderings. This enables the model to capture both spatial structures and temporal dynamics across video frames. The model is trained for 40,000 iterations during this stage. Stage 2: Dense Object Trajectories and Lighting Mixed Finetuning. We combine the Object Motion Control and Lighting Direction Control Datasets, annotating camera motion, object motion, and lighting direction. This mixed training approach helps the model jointly learn control over these aspects, enhancing video realism and consistency. Dense object motion trajectories provide rich spatial-temporal information, while lighting directions and camera poses enable the model to understand lighting interactions with objects and scenes. In this stage, we freeze the temporal layers to preserve the models ability to capture time-dependent dynamics, while fine-tuning the spatial layers (e.g., 2D convolutions and spatial transformer) to optimize spatial details like object contours, shadows, and highlights. This approach ensures the model learns both complex motion patterns and fine-grained spatial features. This stage is trained for 20,000 iterations, balancing computational efficiency with model performance. Stage 3: Sparse Object Trajectories and Lighting Mixed Finetuning. We fine-tune the model with sparse object trajectories to adapt to real-world inputs, where motion guidance is limited. Sparse trajectories, consisting of key points, require the model to generalize, enhancing its robustness in generating realistic motion effects with minimal input. To improve stability, we smooth trajectories with Gaussian filter, handling noise while preserving key features. This stage, trained for 20,000 iterations, strengthens the models ability to infer complex motion patterns and improves generalization. VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation 5 Fig. 3. Illustrations of examples from the Video Lighting Direction (VLD) Dataset, showcasing samples with 3D models and backgrounds. (a) Poly Haven-based VLD Samples with HDR backgrounds. (b) BOP-based VLD Samples with textured backgrounds. Each set includes video frames of two samples under two different lighting conditions."
        },
        {
            "title": "4 DATASET CONSTRUCTION",
            "content": "To enable fine-grained control over camera motion, object motion, and lighting direction in I2V generation, detailed annotations for camera poses, object trajectories, and lighting directions are essential. Existing datasets lack such comprehensive annotations, hindering model training for simultaneous control. To address this, we construct three specialized datasets:"
        },
        {
            "title": "4.1 Camera Motion Control Dataset",
            "content": "We constructed Camera Motion Control Dataset from RealEstate10K [Zhou et al. 2018], creating 62,000 25-frame clips with smooth camera transitions. DUSt3R was used to generate globally aligned point clouds, and the first frames point cloud was rendered along the ground truth camera trajectory to visualize camera motion. Since RealEstate10K lacks captions, we sampled four frames per clip and used Qwen2-VL-7B-Instruct [Wang et al. 2024a] to generate comprehensive descriptions. Although focused on indoor scenes, the dataset provides diverse camera trajectories for fine-grained motion control in video generation."
        },
        {
            "title": "4.2 Object Motion Control Dataset",
            "content": "To enable precise control over object motion in image-to-video generation, we constructed an Object Motion Control Dataset from WebVid-10M [Bain et al. 2021], consisting of 60,000 video clips with high-quality object motion trajectories. (1) Video Preprocessing: We used PySceneDetect to remove clips with abrupt scene changes, sampling 25-frame sequences with temporal intervals from 1 to 16 frames. (2) Motion Estimation: Optical flow was computed using MemFlow [Dong and Fu 2024], filtering out the bottom 25% of clips with low motion scores. (3) Cropping and Tracking: The clips are center-cropped to 320 512 25 dimensions. We then apply CoTrackerV3 [Karaev et al. 2024] to generate dense trajectories on 16 16 grid, retaining only those trajectories longer than the average length to ensure meaningful object motion. (4) Sampling and Gaussian Filtering: To enhance usability, we sample 1 to 8 ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. 6 Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu Table 1. Quantitative comparison with SOTA methods on RealEstate10K. Table 3. Ablation of training strategy on WebVid-10M. Method FID FVD CLIPSIM PSNR SSIM LPIPS CamMC Method FID FVD CLIPSIM PSNR SSIM LPIPS ObjMC CameraCtrl CamI2V MotionCtrl Ours 97.99 98.54 103.82 75.62 96.11 85.03 188.93 49.77 29.41 30.37 30.18 32.32 14.94 14.53 12.81 18. 0.56 0.51 0.47 0.63 0.25 0.28 0.35 0.18 4.19 4.24 4.23 4.07 Table 2. Quantitative comparison with SOTA methods on WebVid-10M. Dense Sparse Dense+Sparse 92.05 91.54 87. 143.44 123.15 120.65 30.78 30.93 32.99 18.21 18.47 18.48 0.59 0.59 0.60 0.20 0.20 0.19 4.39 4.05 3. Table 4. Ablation of lighting embedding integration strategies on VLD. Method FID FVD CLIPSIM PSNR SSIM LPIPS ObjMC Image Conductor Motion-I2V Ours 150.26 128.35 87.12 242.01 171.35 120.65 29.69 30.92 32. 15.18 16.20 18.48 0.501 0.53 0.60 0.30 0.26 0.19 12.96 3.96 3.51 Method FID FVD CLIPSIM PSNR SSIM LPIPS CamMC 121.95 Text Cross-Attention 123.31 Time Embedding Lighting Cross-Attention 100.83 117.69 111.08 101.71 22.77 22.70 23.70 18.14 19.07 19.49 0.72 0.73 0. 0.13 0.12 0.11 5.31 5.21 5.00 sparse trajectories from the dense set, with probability proportional to trajectory length. Optical flow is computed between adjacent frames to capture motion intensity and direction, and the sparse matrix is smoothed with Gaussian filter for stable training."
        },
        {
            "title": "4.3 Lighting Direction Control Dataset",
            "content": "As shown in Fig. 3, we introduce the VideoLightingDirection (VLD) Dataset, synthetic dataset created in Blender with artist-designed 3D models, HDR environments, and procedural scene generation for precise lighting control. (1) Scene Creation: We create diverse scenes using 3D models and backgrounds from Poly Haven and the BOP dataset. Poly Haven assets pair 3D models with HDR environments, while BOP scenes use rooms of six planes with randomly placed 3D models, ensuring varied and dynamic environments. (2) Camera Trajectory Sampling: To simulate realistic camera motion, we define spherical region (0.71.3 meters radius) around each 3D model. Cameras are randomly placed on the sphere, pointing toward the model center, with smooth trajectories for natural orbital motion, ensuring stable scene representation. (3) Lighting Direction Sampling: To sample light sources, we define hemisphere centered on the model, with its base-plane normal aligned to the cameras viewing direction. Sixteen points are uniformly sampled on the hemisphere as 2kW spotlights, each with radius of 1 and oriented toward the model center. The sampled positions are mapped into the coordinate systems of all camera views and normalized to calculate lighting directions for each view. (4) Rendering and Annotation: We render 25 frames at resolution of 320 512 using Cycles, path-tracing renderer in Blender known for its photorealistic output. For each scene, we render 16 videos corresponding to the 16 sampled light positions, while maintaining consistent camera trajectory. This results in total of 57,600 videos from 3,600 unique scenes. Each frame is annotated with the corresponding camera pose and lighting direction, providing precise control signals for training."
        },
        {
            "title": "5 EXPERIMENTS\n5.1 Experimental Setup",
            "content": "Implementation Details. Our model is built upon the DynamiCrafter and ViewCrafter [Yu et al. 2024] architecture, leveraging its pretrained weights for initialization. During both training and inference, we set the resolution to 320 512 and process video clips containing 25 frames. To optimize the model, we employ the Adam optimizer with learning rate of 1 105 and batch size of 96. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. The training is conducted on 8 NVIDIA H100 GPUs, with each GPU handling batch size of 6. For inference, we adopt the DDIM sampler along with classifier-free guidance to enhance generation quality and multimodal consistency. Evaluation Datasets. To evaluate the performance of our model in camera motion control, object motion control, and lighting direction control, we construct three domain-specific evaluation datasets and one generalized testing dataset. For camera motion control, we sample 1,000 samples from the RealEstate10K test set. For object motion control, we sample 1,000 samples from the WebVid10M test set. Lighting direction control is evaluated on 1,000 examples sampled from the proposed VLD dataset, covering wide range of lighting directions. To provide broader evaluation, we create generalized testing dataset consisting of 80 videos sourced from copyright-free websites like Pixabay and Pexels, as well as videos generated by T2V models. This dataset spans categories such as human activities, animals, vehicles, indoor scenes, artworks, natural landscapes, and AI-generated images. Evaluation Metrics. We evaluate VidCRAFT3 using three types of metrics. Video quality is assessed with FID, FVD, and CLIPSIM, measuring visual quality, temporal coherence, and semantic alignment, respectively. Motion control performance is quantified with CamMC and ObjMC, computed as the Euclidean distance between predicted and ground truth camera poses and object trajectories. DUSt3R and CoTrackerV3 are used for camera pose estimation and trajectory extraction. Lighting direction control is evaluated with LPIPS, SSIM, and PSNR, comparing generated frames against ground truth images for perceptual quality and structural fidelity."
        },
        {
            "title": "5.2 Comparisons with Other State-of-the-Art Methods",
            "content": "Camera Motion Control. VidCRAFT3 demonstrates superior performance in camera motion control compared to state-of-the-art methods like CameraCtrl, CamI2V, and MotionCtrl on the RealEstate 10K dataset. As shown in Table 1 and Fig. 4, VidCRAFT3 excels in camera motion control, as evidenced by its CamMC score of 4.07, which is lower than CameraCtrl (4.19), CamI2V (4.24), and MotionCtrl (4.23). VidCRAFT3 also achieves better results in other key metrics indicate that VidCRAFT3 not only controls camera motion more accurately but also generates videos with higher visual quality, temporal coherence, and semantic alignment. Qualitatively, VidCRAFT3 generates smoother and more realistic camera movements with fewer artifacts, particularly in complex scenes. This success stems from the Image2Cloud module, which enable precise 3D scene reconstruction and fine-grained camera control. VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation 7 Fig. 4. Qualitative comparisons with SOTA methods on RealEstate10K. Fig. 5. Qualitative comparisons with SOTA methods on WebVid-10M. Object Motion Control. VidCRAFT3 demonstrates exceptional performance in object motion control, as highlighted in Table 2. The model achieves an ObjMC score of 3.51, which is lower than Image Conductor (12.96) and Motion-I2V (3.96). This indicates that VidCRAFT3 more accurately aligns the generated object trajectories with the ground truth, resulting in more realistic and faithful object motion. Additionally, VidCRAFT3 outperforms in other key metrics, showcasing superior visual quality, temporal coherence, and semantic alignment. Qualitatively, as shown in Fig. 5, VidCRAFT3 generates more realistic and consistent object movements compared to Image Conductor and Motion-I2V. The model effectively captures the dynamics of object motion, ensuring smooth transitions and natural interactions within the scene. These results highlight VidCRAFT3s robust object motion control, driven by its advanced ObjMotionNet, which effectively captures and controls complex motion patterns."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "Training Strategy. We compare training with Dense, Sparse, and Dense+Sparse trajectories. As shown in Table 3, dense training (ObjMC: 4.39) provides rich motion information but struggles during inference due to sparse inputs. Sparse training (ObjMC: 4.05) improves inference alignment but lacks motion detail. The Dense+Sparse approach achieves the best results (ObjMC: 3.51), leveraging dense trajectories for robust learning and fine-tuning with sparse trajectories for better generalization. This hybrid strategy enhances object motion control and video quality, as shown by superior FID (87.12) Table 5. Ablation of representation of lighting direction on VLD. Light Representation FID FVD CLIPSIM PSNR SSIM LPIPS CamMC"
        },
        {
            "title": "Fourier Embedding\nSH Encoding",
            "content": "107.40 121.89 100.83 117.69 21.71 23.70 17.48 19.49 0.70 0.74 0.14 0.11 5.03 5. and FVD (120.65) scores. The qualitative results in Fig. 9 further demonstrate more accurate object trajectories and better alignment with the ground truth, confirming the effectiveness of the combined approach. Lighting Embedding Integration Strategies. We compare different lighting embedding integration strategies. As shown in Table 4, Lighting Cross-Attention achieves the best results, with an PSNR of 19.49, SSIM of 0.74, and LPIPS of 0.11, outperforming other methods. The qualitative results in Fig. 10 further demonstrate that Lighting Cross-Attention produces more realistic lighting effects and better alignment with the reference image and lighting direction. These findings confirm the effectiveness of the proposed approach for precise lighting control in image-to-video generation. Representation of Lighting Direction. We compare Fourier Embedding [Mildenhall et al. 2021] and SH Encoding for representation of lighting direction. As shown in Table 5, SH Encoding outperforms Fourier Embedding, achieving better PSNR (19.49), SSIM (0.74), and LPIPS (0.11). The qualitative results in Fig. 11 further demonstrate that SH Encoding produces more realistic lighting effects and better alignment with the reference image and lighting direction. These results confirm SH Encodings effectiveness for precise lighting control in image-to-video generation. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. 8 Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu"
        },
        {
            "title": "6 CONCLUSIONS AND LIMITATIONS",
            "content": "This paper presents VidCRAFT3, framework for high-quality image-to-video generation with simultaneous control over camera motion, object motion, and lighting direction. Combining 3D point cloud rendering, sparse trajectory encoding, and lighting-aware attention mechanisms, VidCRAFT3 achieves fine-grained, disentangled control. The Spatial Triple-Attention Transformer ensures temporal consistency and visual realism by integrating lighting, text, and image conditions. To address data limitations, we introduce the VLD dataset and three-stage training strategy, removing the need for joint multi-element annotations. Experiments show VidCRAFT3 outperforms SOTA methods in control precision, video quality, and generalization. However, it struggles with large human motion, physical interactions, or significant lighting changes due to limited training data and challenges in understanding physics and 3D spatial relationships."
        },
        {
            "title": "REFERENCES",
            "content": "Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. 2021. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision. 17281738. Shrisha Bharadwaj, Haiwen Feng, Victoria Abrevaya, and Michael Black. 2024. GenLit: Reformulating Single-Image Relighting as Video Generation. arXiv preprint arXiv:2412.11224 (2024). Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). Ziqi Cai, Kaiwen Jiang, Shu-Yu Chen, Yu-Kun Lai, Hongbo Fu, Boxin Shi, and Lin Gao. 2024. Real-time 3D-aware portrait video relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 62216231. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. 2023b. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512 (2023). Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. 2023a. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations. Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. 2025. Perceptionas-Control: Fine-grained Controllable Image Animation with 3D-aware Motion Representation. arXiv preprint arXiv:2501.05020 (2025). Qiaole Dong and Yanwei Fu. 2024. MemFlow: Optical Flow Estimation and Prediction with Memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1906819078. Wanquan Feng, Jiawei Liu, Pengqi Tu, Tianhao Qi, Mingzhen Sun, Tianxiang Ma, Songtao Zhao, Siyu Zhou, and Qian He. 2024a. I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength. arXiv preprint arXiv:2411.06525 (2024). Wanquan Feng, Tianhao Qi, Jiawei Liu, Mingzhen Sun, Pengqi Tu, Tianxiang Ma, Fei Dai, Songtao Zhao, Siyu Zhou, and Qian He. 2024b. I2VControl: Disentangled and Unified Video Motion Synthesis Control. arXiv preprint arXiv:2411.17765 (2024). Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. 2024. Motion Prompting: Controlling Video Generation with Motion Trajectories. arXiv preprint arXiv:2412.02700 (2024). Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, et al. 2024. I2v-adapter: general imageto-video adapter for diffusion models. In ACM SIGGRAPH 2024 Conference Papers. 112. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2023. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023). Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. 2024b. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101 (2024). Mingming He, Pascal Clausen, Ahmet Levent Taşel, Li Ma, Oliver Pilarski, Wenqi Xian, Laszlo Rikker, Xueming Yu, Ryan Burgert, Ning Yu, et al. 2024a. DifFRelight: ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. Diffusion-Based Facial Performance Relighting. In SIGGRAPH Asia 2024 Conference Papers. 112. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Loc Huynh, Bipin Kishore, and Paul Debevec. 2021. new dimension in testimony: Relighting video with reflectance field exemplars. arXiv preprint arXiv:2104.02773 (2021). Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. 2024. CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos. In Proc. arXiv:2410.11831. Quewei Li, Jie Guo, Yang Fei, Feichao Li, and Yanwen Guo. 2022. Neulighting: Neural lighting for free viewpoint outdoor scene relighting with unconstrained photo collections. In SIGGRAPH Asia 2022 Conference Papers. 19. Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. 2024. Image conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339 (2024). Min-Hui Lin, Mahesh Reddy, Guillaume Berger, Michel Sarkis, Fatih Porikli, and Ning Bi. 2024. EdgeRelight360: Text-Conditioned 360-Degree HDR Image Generation for Real-Time On-Device Video Portrait Relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 831840. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1 (2021), 99106. Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. 2024. ReVideo: Remake Video with Motion and Content Control. arXiv preprint arXiv:2405.13865 (2024). Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. 2025. Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model. In European Conference on Computer Vision. Springer, 111128. Matt Pharr, Wenzel Jakob, and Greg Humphreys. 2023. Physically based rendering: From theory to implementation. MIT Press. Yohan Poirier-Ginter, Alban Gauthier, Julien Phillip, J-F Lalonde, and George Drettakis. 2024. Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis. In Computer Graphics Forum, Vol. 43. Wiley Online Library, e15147. Puntawat Ponglertnapakorn, Nontawat Tritrong, and Supasorn Suwajanakorn. 2023. DiFaReli: Diffusion face relighting. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2264622657. Haonan Qiu, Zhaoxi Chen, Yuming Jiang, Hang Zhou, Xiangyu Fan, Lei Yang, Wayne Wu, and Ziwei Liu. 2024. Relitalk: Relightable talking portrait generation from single video. International Journal of Computer Vision (2024), 116. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. Pramod Rao, Gereon Fox, Abhimitra Meka, Mallikarjun BR, Fangneng Zhan, Tim Weyrich, Bernd Bickel, Hanspeter Pfister, Wojciech Matusik, Mohamed Elgharib, et al. 2024. Lite2Relight: 3D-aware Single Image Portrait Relighting. In ACM SIGGRAPH 2024 Conference Papers. 112. Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. 2024. Motioni2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers. 111. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning. PMLR, 22562265. Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, and Nanxuan Zhao. 2024. MotionBridge: Dynamic Video Inbetweening with Flexible Controls. arXiv preprint arXiv:2412.13190 (2024). Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. 2024c. LeviTor: 3D Trajectory Oriented Image-toVideo Synthesis. arXiv preprint arXiv:2412.15214 (2024). Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. 2024e. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566 (2024). Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191 (2024). Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. 2024b. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2069720709. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. 2024d. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers. 111. VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation 9 Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. 2025. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision. Springer, 331348. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. 2025. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision. Springer, 399417. Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. 2024. CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation. arXiv preprint arXiv:2406.02509 (2024). Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. 2024. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers. 112. Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. 2023. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089 (2023). Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. 2024. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048 (2024). Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. 2024a. Dilightnet: Fine-grained lighting control for diffusion-based image generation. In ACM SIGGRAPH 2024 Conference Papers. 112. Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. 2024b. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 88508860. Longwen Zhang, Qixuan Zhang, Minye Wu, Jingyi Yu, and Lan Xu. 2021. Neural video portrait relighting in real-time via consistency modeling. In Proceedings of the IEEE/CVF international conference on computer vision. 802812. Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. 2024. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957 (2024). Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. 2018. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817 (2018). ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. 10 Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu Fig. 6. Additional experimental results on camera motion control + lighting direction control. Fig. 7. Additional experimental results on camera motion control. Fig. 8. Additional experimental results on camera motion control + object motion control. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025. VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation 11 Fig. 9. Qualitative results of the ablation study on training strategies with WebVId-10M. Fig. 10. Qualitative results of the ablation study on lighting embedding integration strategies on VLD. Fig. 11. Qualitative results of the ablation study on the representation of lighting direction on VLD. Fig. 12. Additional experimental results on object motion control. Fig. 13. Additional experimental results on lighting direction control. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: February 2025."
        }
    ],
    "affiliations": [
        "Fudan University, China",
        "Huawei Noahs Ark Lab, China",
        "Westlake University, China",
        "Zhejiang University, China"
    ]
}