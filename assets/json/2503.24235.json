{
    "paper_title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models",
    "authors": [
        "Qiyuan Zhang",
        "Fuyuan Lyu",
        "Zexu Sun",
        "Lei Wang",
        "Weixu Zhang",
        "Zhihan Guo",
        "Yufei Wang",
        "Irwin King",
        "Xue Liu",
        "Chen Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 5 3 2 4 2 . 3 0 5 2 : r What, How, Where, and How Well? Survey on Test-Time Scaling in Large Language Models Qiyuan Zhang1* , Fuyuan Lyu2* , Zexu Sun3 , Lei Wang5 , Weixu Zhang2 , Zhihan Guo4 , Yufei Wang6 , Irwin King4, Xue Liu2, Chen Ma1 1City University of Hong Kong, 2McGill University & MILA, 3Gaoling School of Artificial Intelligence, Renmin University of China, 4Chinese University of Hong Kong, 5Salesforce AI Research, 6Macquarie University qzhang732-c@my.cityu.edu.hk, fuyuan.lyu@mail.mcgill.ca"
        },
        {
            "title": "Abstract",
            "content": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS)also referred to as test-time computinghas emerged as prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for comprehensive survey offering systemic understanding. To fill this gap, we propose unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs)(Brown et al., 2020; OpenAI, 2024a) have emerged in recent years as transformative milestone toward artificial general intelligence (AGI)(Goertzel, 2014; Bubeck et al., 2023). These models remarkably learn general intelligence by training-time scaling, where the models ingest more data and parameters (Kaplan et al., 2020; Hoffmann et al., 2022). However, core challenge remains: how can we fully elicit the intelligence encoded in LLMs at inference time to maximize their real-world effectiveness (Wei et al., 2022; Ouyang et al., 2022)? Human cognition may suggest clue. When faced with complex problems, people tend to engage in deeper, more deliberate thinking, often producing better outcomes (Kahneman, 2011, 2003; Evans, 1984). Inspired by this principle, recent research (Wei et al., 2022; Wang et al., 2023) has introduced methods that allocate additional computation during inference to boost task performance. Notably, some studies (Brown et al., 2024b; Wu et al., 2024c) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements. This family of methods, referred to as test-time scaling (TTS), progressively elicits the models intelligence in the test-time. The remarkable successes of reasoning models, such as o1 (OpenAI, 2024b) and R1 (DeepSeek-AI, 2025), have further amplified interest in TTS, highlighting its potential as key driver of LLM reasoning and utility. However, despite this surge in research activity, the field currently lacks unified and systematic framework to synthesize insights, compare techniques, or identify consistent trends in TTS . To address this gap, we present comprehensive survey of TTS, offering hierarchical and extensible framework to analyze methods, map research efforts, and guide future progress. While prior efforts have examined TTS from specific lensessuch as input modification and output verification (Snell et al., 2024), or through the lens of System 2 AI and Long Chain-of-Thought (CoT) (Li et al., 2025i; Ji * Core contribution Significant contribution Taxonomy designer 1 Figure 1: Illustration of pre-train scaling and test-time scaling. et al., 2025; Chen et al., 2025b)they often treat methods as isolated instances or abstract reasoning ideals. In contrast, our work emphasizes fine-grained, decomposition-based understanding of TTS. We analyze the full pipeline, from scaling formulations and algorithmic mechanisms to task domains and performance dimensions. To our knowledge, this is the first survey to comprehensively examine TTS across multiple orthogonal dimensions, offering structured perspective for both theoretical inquiry and practical deployment. Our framework dissects TTS into four key dimensions: what to scale, how to scale, where to scale, and how well to scale. It provides structured foundation that allows future research to be seamlessly integrated into our taxonomy, making it easier to understand their contributions. Specifically, what to scale (Sec. 2) is what being scaled at the inference stage. How to scale (Sec. 3) is how they are implemented, We categorize various techniques, recognizing that single approach may involve multiple techniques. For instance, complex search strategy can be used to generate long CoTs, which are then refined through supervised fine-tuning (SFT) to enhance LLM imitation. Where to scale (Sec. 4) covers the tasks and datasets where these techniques are applied. And how well to scale (Sec. 5) refer to evaluating the different attributions of TTS methods. We further provide fine-grained subcategories under each axis and systematically map representative works to highlight their contributions and trade-offs (Sec. 6). From this structured analysis, we extract major trends in TTS development and offer hands-on guidance (Sec. 7) for real-world deployment. Grounded in our multi-dimensional taxonomy, we also identify persistent challenges and promising research directions (Sec. 8): advancing test-time scalability, clarifying the fundamental essence of the effectiveness of different techniques in TTS, broadening generalization to wider range of downstream tasks, and optimizing TTS methods along additional dimensions such as efficiency. Our contributions are threefold: 1. Unified, Multi-Dimensional Taxonomy. We propose four-axis taxonomywhat to scale, how to scale, where to scale, and how well to scalethat supports structured classification, comparison, and extensibility for TTS methods. 2. Systematical Literature Organization and Pragmatic Analysis. Using our taxonomy, we survey the TTS landscape, analyze representative methods, and present guidelines for research application and deployment. 3. Challenges, Insights, and Forward Directions. Building on our organized perspective, we uncover critical challengesranging from advancing scaling to clarifying essenceand outline promising research directions that could shape future progress. Our unified framework facilitates the mapping of these open questions to concrete dimensions of TTS, enabling more targeted and impactful advancements. We plan to continuously update our taxonomy to reflect ongoing progress and provide an evolving foundation for organizing future developments in TTS research."
        },
        {
            "title": "2 What to Scale",
            "content": "What to scale refers to the specific form of TTS that is expanded or adjusted to enhance an LLMs performance during inference. When applying TTS , researchers typically choose specific what to scale based on an empirical hypothesis, aiming to achieve performance gains. For example, some researchers hypothesize that longer CoTs improve complex reasoning, leading them to enforce longer outputs from LLMs. Others leverage the self-consistency principle, assuming that generating multiple solutions to reasoning task increases the likelihood of reaching the correct answer. 2 Parallel Scaling (2.1) Self-Consistency (Brown et al., 2024b; Irvine et al., 2023; Song et al., 2024; Snell et al., 2024; Wang et al., 2023; Nguyen et al., 2024) (Chen et al., 2024d; Wu et al., 2025b), Multi-Agents (Jiang et al., 2023), PlanSearch (Wang et al., 2024a), CCE (Zhang et al., 2025e) Sequential Scaling (2.2) Self-Refine (Madaan et al., 2023; Chen et al., 2024e; Gou et al., 2024; Zhang et al., 2024d), Sequential Revision (Lee et al., 2025), ReAct (Yao et al., 2023c), Budget-aware (Kimi, 2025; Muennighoff et al., 2025; Han et al., 2025), RecurrentBlock (Geiping et al., 2025), STaR (Yuan et al., 2023; Singh et al., 2024), Meta-STaR (Xiang et al., 2025), PlanningToken (Wang et al., 2024g), RaLU (Li et al., 2025c) What to Scale (2) Hybrid Scaling (2.3) MoA (Wang et al., 2025a), Tree of Thoughts (Yao et al., 2023b; Zhang et al., 2024b), Graph of Thoughts (Besta et al., 2024), Tree-Search (Chen et al., 2024g), SoS (Gandhi et al., 2024), REBASE (Wu et al., 2024c), OAIF (Guo et al., 2024), Beam-Search (Guo et al., 2024),MCTS(Tian et al., 2024; Zhang et al., 2024e; Gao et al., 2024b; Wan et al., 2024; Chen et al., 2024a), Journey Learning(Qin et al., 2024),AdaptiveAlloc(Snell et al., 2024; Ong et al., 2025), METAL(Li et al., 2025a), rStar-Math(Guan et al., 2025a),AtomThink(Xiang et al., 2024) Internal Scaling (2.4) DeepSeek-R1 (DeepSeek-AI, 2025), OpenAI-o1&o3 (OpenAI, 2024b, 2025), Gemini Flash Thinking (Google, 2024), QwQ (Qwen, 2024), K1.5 (Kimi, 2025), 3SUM (Pfau et al., 2024), OAIF (Guo et al., 2024), LIMO (Ye et al., 2025), T1 (Hou et al., 2025), Distilled-o1 (Huang et al., 2024b), RedStar (Xu et al., 2025a), SKY-T1 (NovaSky, 2025),s1 (Muennighoff et al., 2025), ITT (Hao et al., 2024) Supervised Finetuning (3.1.1) Distillation (Muennighoff et al., 2025; Huang et al., 2024b; Xu et al., 2025a; NovaSky, 2025; Bespoke, 2025) (Munkhbat et al., 2025; Ye et al., 2025), Synthesized Long CoT (Hou et al., 2025; Yeo et al., 2025), Learning Reasoning Structure (Li et al., 2025f), Long CoT warmup (Kimi, 2025) , CFT (Wang et al., 2025d) Tuning (3.1) Reinforcement Learning (3.1.2) Reward model-free Rule-Based (DeepSeek-AI, 2025), cDPO (Lin et al., 2024), Focused-DPO (Zhang et al., 2025b), Selective DPO (Gao et al., 2025b), CPL (Wang et al., 2024f), OREO (Wang et al., 2024b), DAPO (Liu et al., 2024b), RFTT (Zhang et al., 2025c), SimPO (Meng et al., 2024), DQO (Ji et al., 2024), DAPO (Yu et al., 2025), VC-PPO (Yuan et al., 2025), Light-R1 (Wen et al., 2025), etc. Reward model-based PPO (Schulman et al., 2017), RLOO (Ahmadian et al., 2024a), GRPO (Shao et al., 2024), REINFORCE++ (Hu, 2025), DVPO (Huang et al., 2025a), PRIME (Cui et al., 2025), SPPD (Yi et al., 2025), etc. Prompt Strategy Hint-infer (Li et al., 2025b), Dipper (Lau et al., 2024), EVA (Ye et al., 2024), EvalPlan(Saha et al., 2025), ReasonFlux (Yang et al., 2025a), Hong et al. (2024) How to Scale (3) Decode Strategy Filler Tokens (Pfau et al., 2024), Budget Forcing (Muennighoff et al., 2025), LTV (Kong et al., 2025), AFT (Li et al., 2025g), Predictive-Decoding (Ma et al., 2025a) Stimulation (3.2.1) Self-Repetition Self-Consistency (Wang et al., 2023), Self-Refine (Madaan et al., 2023), DeCRIM (Ferraz et al., 2024), CCE (Zhang et al., 2025e), TreeBoN (Qiu et al., 2024) Mixture-of-Model MoA (Wang et al., 2025a), RR-MP (He et al., 2025), BRAIN (Chen et al., 2024f) Outcome Process Output Verification (Cobbe et al., 2021), Generative Verifier (Zhang et al., 2025d), Self-Reflection Feedback (Li et al., 2025h), Discriminator (Chen et al., 2024g), OVM (Yu et al., 2024b), Heuristic (DeepSeek-AI, 2025), Bandit (Sui et al., 2025), Functional (Lee et al., 2025), XoT (Liu et al., 2023b), WoT (Zhang et al., 2024c) State Evaluator (Yao et al., 2023b; Zhang et al., 2024b), SIaM (Yu et al., 2024a), Deductive Verification (Ling et al., 2023), Self-Evaluator (Xie et al., 2023), V-STaR (Hosseini et al., 2024), Tool (Li et al., 2025b), PoT (Chen et al., 2023a) Inference (3.2) Verification (3.2.2) Search (3.2.3) TreeSearch (Yao et al., 2023b; Chen et al., 2024g),GraphSearch (Besta et al., 2024),C-MSTS (Lin et al., 2025), MCTS (Tian et al., 2024; Zhang et al., 2024e; Gao et al., 2024b; Wan et al., 2024; Chen et al., 2024a), SPaR (Cheng et al., 2025), REBASE (Wu et al., 2024c), SoS (Gandhi et al., 2024), CoAT (Pan et al., 2025a),BeamSearch (Guo et al., 2024; Xie et al., 2023), Lookahead-Search (Snell et al., 2024; Zhang et al., 2023b), etc. Selection Aggregation (3.2.4) Majority Voting(Wang et al., 2023; Chen et al., 2024d), BOND(Sessa et al., 2024), Filter Vote(Chen et al., 2024d), Length-filtered Vote(Wu et al., 2025b), Best-of-N (Irvine et al., 2023; Song et al., 2024), Rejection Sampling (Kimi, 2025), etc. Fusion BoN (weighted) (Brown et al., 2024b), Synthesize (Wang et al., 2025a), etc. Math Code Science Reasoning (4.1) AIME (Google, 2025; Guan et al., 2025b), CNMO (CMS, 2025), NuminaMATH (LI et al., 2024), OmniMath (Gao et al., 2025a), MATH (Cobbe et al., 2021; Hendrycks et al., 2021; Guan et al., 2025b), s1-prob-teasers (Muennighoff et al., 2025), GSM8K (Guan et al., 2025b; Zhang et al., 2024a), MATH500(Zhang et al., 2024a), AMC (Guan et al., 2025b), College Math (Guan et al., 2025b), FrontierMath (Glazer et al., 2024), etc. USACO (Shi et al., 2024), LiveCodeBench (Jain et al., 2025), CodeContests (Li et al., 2022), Aider-Polyglot (aider, 2025),SWE-bench(Jimenez et al., 2024),Codeforces(codeforce, 2025),CodeMind (Liu et al., 2024a), etc. OlympicArena (Huang et al., 2024a), OlympiadBench (He et al., 2024a; Guan et al., 2025b), TheoremQA (Chen et al., 2023b), JEEBench (Arora et al., 2023), GPQA (Rein et al., 2024), SciEval (Sun et al., 2024), Miverva (Lewkowycz et al., 2022), SciBench (Zhang et al., 2024a), HLE (Phan et al., 2025), etc. Where to Scale (4) Game & Strategy SysBench (Google, 2025), Points24 (Yao et al., 2023b; Zhai et al., 2024), TravelPlan (Xie et al., 2024), etc. Medical SysBench, JMLE-2024 (Nori et al., 2024),Medbullets (Chen et al., 2025a),MedQA (Jin et al., 2020), etc. Basics Agents AGIEval (Zhong et al., 2024), MMLU-Pro (Wang et al., 2024h), Gaokao (NCEE, 2025; Guan et al., 2025b), Kaoyan (GSEE, 2025), CMMLU (Li et al., 2024), LongBench (Bai et al., 2024), ARC-AGI (Chollet, 2019), etc. WebShop (Yao et al., 2023a), WebArena (Zhou et al., 2023c), SciWorld (Wang et al., 2022), WebVoyager (He et al., 2024b), TextCraft(Prasad et al., 2024), TAU-bench (Yao et al., 2024), BCFL (Yan et al., 2024), etc. General-Purpose (4.2) Knowledge SimpleQA (Wei et al., 2024a), C-SimpleQA (He et al., 2024c)),FRAMES (Krishna et al., 2025), etc. Open-Ended AlpacaEval2.0 (Dubois et al., 2024), ArenaHard (Li et al., 2024b), IF-Eval (Zhou et al., 2023b), Chatbot Arena (Zheng et al., 2023b), C-Eval (Huang et al., 2023), FollowBench (Jiang et al., 2024b), etc. How Well to Scale (5) Accuracy (5.1) Efficiency (5.2) Controllability (5.3) Scalability (5.4) Multi-Modal MMMU (Yue et al., 2024), MATH-Vision (Wang et al., 2024d), MathVista (Lu et al., 2024), LLAVA-Wild (Liu et al., 2023a), MM-Vet (Yu et al., 2024d), MMBench (Liu et al., 2024c), MMMU (Yue et al., 2024), CVBench (Tong et al., 2024), MMStar (Chen et al., 2024c), CHAIR (Rohrbach et al., 2018), etc. Pass@1(DeepSeek-AI, 2025; Kimi, 2025), Pass@k(Chen et al., 2021; Brown et al., 2024a), WinRate(DeepSeek-AI, 2025; Hou et al., 2025) Cons@k (DeepSeek-AI, 2025; Zeng et al., 2025c), , etc. Token Cost (Welleck et al., 2024; Aytes et al., 2025), FLOPs-based Efficiency Analysis (Kaplan et al., 2020; Snell et al., 2024), KV Cache size (Hooper et al., 2025), Underthinking score (Wang et al., 2025e), etc. Control Metric (Muennighoff et al., 2025),Length Deviation (Aggarwal and Welleck, 2025a),k-ϵ Controllability (Bhargava et al., 2024), etc. Scaling Metric (Muennighoff et al., 2025),Scaling Curves (Accuracy vs. Compute) (Aggarwal and Welleck, 2025a; Teng et al., 2025), etc. l e - T Figure 2: Taxonomy of research in Test-time Scaling that consists of what, how, where, and how well to scale. 2.1 Parallel Scaling 2.1 Parallel Scaling LLMs typically generate single response per query. Parallel scaling improves test-time performance by generating multiple outputs in parallel and then aggregating them into final answer. Formally, consider problem set and collection of models {1, . . . , }. Each model generates km candidate responses for given problem P, producing set of sampled solutions S: = {sm,i M, km}, (ˆs) ˆs = A(s1,1, . . . , sM,kM ) is correct. (1) Here, is the aggregation function that derives final response from the set S. The effectiveness of parallel scaling depends on both coveragethe likelihood of generating at least one correct responseand aggregation quality, which determines whether correct response is successfully identified. This approach is supported by both theory and intuition: cognitive science research (Stanovich and West, 2000) suggests that complex problems often allow multiple valid solution paths, and increasing the number of generated responses improves the chance of finding correct one (Li et al., 2025d). Empirically, this relationship is often log-linear with respect to compute (Brown et al., 2024b). We categorize parallel scaling into two common forms based on different sources of coverage: (1) repeated sampling from single model and (2) sampling across multiple models. Furthermore, there are some additional techniques to enhance solution diversity and reliability, such as hyperparameter adjustments (e.g., sampling temperature (Renze, 2024) to control output variability) and input modifications (e.g., prompt rephrasing (Lambert et al., 2025) to elicit diverse responses). 2.2 Sequential Scaling Sequential scaling involves explicitly directing later computations based on intermediate steps. Unlike parallel methods, sequential scaling updates intermediate states iteratively. We denote the partial solution states (subproblem results, or initial drafts) by n1, n2, . . . , nT , with each new state nt+1 = R(nt, p) incorporating both the previous state and the problem context. Because many problems require deliberation rather than immediate pattern matching, single-pass System 1 (Yu et al., 2024c)-style generation often fails on complex reasoning tasks. Iterative methods emulate System 2 approach, breaking down and refining the solution step by step. Early work like chain-of-thought prompting (Wei et al., 2022) motivated solve the problem step-by-step, nt+1 = AppendStep(nt, new reasoning step), leading to approaches that refine responses (Madaan et al., 2023), nt+1 = Refine(nt), or break down problems systematically (Zhou et al., 2023a; Zelikman et al., 2022), nt+1 = (cid:0)nt, solution to next subproblem(cid:1). Subsequent studies show that iterative revision (Chen et al., 2023c; Gou et al., 2024; Chen et al., 2025c; Snell et al., 2024) triggers self-correction, improving accuracy on challenging tasks In practice, real-world tasks often demand more flexible and potentially non-linear reasoning paths, suggesting that purely sequential approaches, while effective, may be only one part of broader solution. 2.3 Hybrid Scaling Hybrid scaling exploits the complementary benefits of parallel and sequential scaling. Parallel scaling mitigates the risk of the model missing the correct line of thought by casting wide net, while sequential scaling allows deep exploration of line of reasoning once it seems promising. Formally, let Ft be the set of candidate solutions at iteration t. Each iteration expands these candidates in parallel with an expansion function and sequentially filters them with selection function S: Ft+1 = E(s) ., (1) (cid:16) (cid:91) (cid:17) sFt After iterations, an aggregator selects the final solution ˆs FT . From cognitive standpoint, such combination mirrors how human problem-solvers generate multiple hypotheses (divergent thinking) and then refine/evaluate them (convergent thinking). Classic search algorithms (e.g., iterative deepening (Chen et al., 2025c) and beam search (Snell et al., 2024)) embody this strategy by balancing exploration and exploitation. Recent work expands on this idea. Tree-of-Thoughts (ToT) (Yao et al., 2023b) branches at decision points, exploring multiple reasoning paths before pruning to single sequence. Follow-up methods, such as Graph-ofThoughts (Besta et al., 2024), Algorithm-of-Thought (Sel et al., 2024), Forest-of-Thought (Bi et al., 2024), Monte Carlo Tree Search (MCTS) (Lin et al., 2025), and multi-agent reasoning (Wang et al., 2025a), leverage similar but more complex hybrid patterns. For instance, multiple LLMs can debate or verify each others answers (Liang et al., 2024; Schaul, 2024), while journey learning and tool-augmented reasoning (Li et al., 2025b) emphasize capturing full reasoning trajectories. 4 2.4 Internal Scaling 2.4 Internal Scaling Internal scaling elicits model to autonomously determine how much computation to allocate for reasoning during testing within the models internal parameters instead of depending on external human-guided strategies. Formally, we update an initial model M0 to new model M1 via training procedure, Φ : (M0, D) (cid:55) M1, on data that includes multi-step reasoning tasks (e.g., long CoT examples produced by external scaling (Qin et al., 2024)). Surprisingly, employing outcome-oriented reward modeling (DeepSeek-AI, 2025; OpenAI, 2024b) for RL enables the model to extend its reasoning process autonomously. At test time, M1 generates sequence of internal states z1, z2, . . . , zT via zt+1 = fθ(zt), stop(zt) = πθ(zt). (2) The models learned policy πθ controls when to halt. This internal feedback loop can lead to emergent behaviorssuch as more detailed reasoning chains or self-evaluation stepswithout any external prompts or multi-call orchestration. In practice, internal scaling often rivals or surpasses standard techniques, thanks to its ability to focus computational effort on single, coherent reasoning trajectory."
        },
        {
            "title": "3 How to Scale",
            "content": "3.1 Tuning-based Approaches To activate models ability to devote cost at test time, directly tuning its parameters is an effective strategy. This includes two approaches: 1) Supervised Finetuning (SFT): Training an LLM via next-token prediction on synthetic or distilled long CoTs enables it to imitate and internalize structured reasoning patterns, effectively learning to think through complex problems. By mimicking extended rationales, SFT reduces the reliance on explicit prompting at inference time. 2) Reinforcement Learning (RL): By leveraging feedback from reward model on inference tasks, the policy model is automatically updated. Although no supervised data is introduced, the model autonomously generates long CoT reasoning while ensuring reliable answers. We divide the RL for internal scaling works into two perspectives. The reward model-based methods and the reward model-free methods. 3.1.1 Supervised Finetuning (SFT) Training an LLM via next-token prediction on synthetic or distilled long CoTs enables it to internalize structured reasoning patterns and effectively think through complex problems. By mimicking extended rationales, SFT reduces the reliance on explicit prompting at inference time. This will include two subsections: (1) Imitation, describing techniques like MCTS used to generate CoT-style demonstrations for fine-tuning, and (2) Distillation, summarizing how student models are trained using outputs from stronger models (e.g., o1, R1). Imitation prominent approach to enhancing LLM reasoning via SFT is to generate long CoT demonstrations using test-time planner algorithms and then fine-tune the model to imitate those demonstrations. For example, STaR (Zelikman et al., 2022) uses the model itself to generate step-by-step solutions for given problem and filters for correct outcomes, treating the verified solutions as new demonstrations to fine-tune. More structured search has been applied to generate even higher-quality traces: ReST-MCTS (Zhang et al., 2024a) integrates an MCTS planner (guided by learned value model) to explore the space of possible reasoning steps; the model is subsequently fine-tuned on these search-generated traces, i.e., it learns to imitate the successful reasoning trajectories discovered by the planner. Distillation While the imitation approach uses models own intermediate outputs for improvement, distillation techniques aim to transfer the capabilities of stronger model (or ensemble of models) into target model via supervised learning. As reported by Muennighoff et al. (2025); Li et al. (2025f), 32B model trained on curated sample set generated by top-tier reasoner was able to solve competition-level math problems nearly as well as the teacher, indicating successful distillation of reasoning. 3.1.2 Reinforcement Learning (RL) Reward model-free. Recent advancements in RL and preference optimization have significantly enhanced the performance of large language models, particularly in reasoning and problem-solving tasks. key innovation in this domain is the introduction of RL with verifiable reward by DeepSeek R1 (DeepSeek-AI, 2025), which leverages rule-based reward mechanisms to optimize models efficiently and reliably. This approach has sparked growing interest among researchers working on large models, as it addresses challenges such as sparse rewards and training instability by providing dense feedback for policy optimization. Several methods have been developed to improve exploration and accuracy in reasoning tasks through preference optimization. For instance, cDPO (Lin et al., 2024), CPL (Wang et al., 2024f), Focused-DPO (Zhang et al., 2025b), DAPO (Liu et al., 2024b), and RFTT (Zhang et al., 2025c) prioritize critical or error-prone areas, enhancing internal scaling and reasoning accuracy. Additionally, 5 3.2 Inference-based Approaches Selective DPO (Gao et al., 2025b) emphasizes the importance of aligning data difficulty with model capacity by filtering out overly challenging examples, further refining the training process. VC-PPO (Yuan et al., 2025) investigates the failure of PPO for the long CoT task and uses pre-trained value model to achieve better results. Light-R1 (Wen et al., 2025) proposes curriculum training framework for increasing data difficulty combined with multi-staged post-training. SimPO (Meng et al., 2024) uses the average log probability of sequence as the implicit reward and removes the reference model in DPO. In the realm of mathematical problem-solving, DQO (Ji et al., 2024) and OREO (Wang et al., 2024b) propose novel value function optimization techniques, demonstrating improvements in model performance. DAPO (Yu et al., 2025) leverages dynamic sampling for large-scale RL systems. These advancements are complemented by range of open-source training frameworks that have equipped researchers and developers with tools to optimize training and enhance inference. Early frameworks like SimpleRL (Zeng et al., 2025b) and DeepScaler (Luo et al., 2025) quickly replicated the technology stack of DeepSeek R1. Furthermore, SimpleRL-Zoo (Zeng et al., 2025a) presents more experimental details about SimpleRL. Others, such as X-R1 (X-R1Team, 2025) and TinyZero (Pan et al., 2025b), focus on delivering an intuitive and cost-effective user experience. Notably, Open-Reasoner-Zero (Hu et al., 2025) replicated the DeepSeek R1-zero training scheme using 32B model, achieving comparable performance. Further advancements in RL for internal scaling have been facilitated by frameworks like OpenR (Wang et al., 2024c), OpenRLHF (Hu et al., 2024), OpenR1 (HuggingFace, 2025), Logic-RL (Xie et al., 2025) and AReaL(AntResearchRL-Lab, 2025). These frameworks have enhanced the replication of internal scaling and, through open-source sharing, accelerated academic research progress. The above developments not only address key challenges in RL but also pave the way for more efficient and reliable model training and deployment. Reward model-based. With Bradley-Terry model (Zheng et al., 2023c) optimized by human preference as the reward model, PPO (Schulman et al., 2017) stands as one of the most influential algorithms with its efficiency and stability and is widely used for internal scaling. Building upon PPO, ReMax (Li et al., 2023b) introduces variance reduction techniques along with REINFORCE (Sutton et al., 1999) and RLOO (Ahmadian et al., 2024b) methods. This eliminates the need for additional value models in PPO, reduces over four hyperparameters, lowers GPU memory usage, and speeds up the training process. GRPO (Shao et al., 2024) replaces traditional value models with improved sampling strategies. This significantly accelerates the learning process and achieves performance comparable to GPT-4 in mathematics. REINFORCE++ (Hu, 2025) further simplifies GRPO and enhances its training. DVPO (Huang et al., 2025a) presents streamlined framework, substituting the reward model with pre-trained global value model and removing the dependency between the actor and critic. PRIME (Cui et al., 2025) integrates the SFT model as PRM within unified RL framework, allowing online updates through policy rollouts and outcome labels via implicit process rewards. SPPD (Yi et al., 2025) utilizes process preference learning with dynamic value margin for self-training. Recently, several works have focused on other challenges of existing reward model-based methods. UGDA (Sun et al., 2025) leverages the uncertainty and influence of samples during PPO training and iteratively refines the reward model. VinePPO (Kazemnejad et al., 2024) exploits the flexibility of language environments to compute unbiased Monte Carlo-based estimates, avoiding the need for large value networks. LCPO (Aggarwal and Welleck, 2025a) focuses on optimizing accuracy and adherence to user-specified length constraints for reasoning tasks. Rest-MCTS* (Zhang et al., 2024a) uses tree-search-based RL to bypass per-step manual annotation typically required for training process rewards. These advancements and refinements in algorithms continue to drive the field of reinforcement learning for internal scaling, offering more effective tools and methods for solving complex problems. 3.2 Inference-based Approaches Unlike training-based approaches, which adjust the models parameters offline, inference-based approaches dynamically adjust computation during deployment. This paradigm includes four essential components: (i) Stimulation, which encourages the model to generate longer or multiple candidate outputs; (ii) Verification, which filters or scores outputs based on correctness or other criteria; (iii) Search, which systematically explores the sample space; and (iv) Aggregation, which consolidates multiple outputs into the final output. These four components are often used in combination to allocate test-time computation more effectively and boost performance on complex reasoning tasks. In the following sections, we provide detailed discussions of each component. 3.2.1 Stimulation Stimulation techniques are the first step in encouraging the model to allocate more computation to thinking. It basically stimulates the LLM to generate (i) longer samplers and (ii) more samples instead of generating single and short samples via naive prompting. This includes several key approaches: Prompt Strategy. Instead of allowing the model to generate an answer directly, one way to stimulate the scaling of LLM during test time is through the prompt. This behavior requires the backbone LLMs ability to follow 3.2 Inference-based Approaches Figure 3: Illustration of how to scale for various approaches. instructions. For instance, prompts can guide the model toward step-by-step reasoning. Simple modifications such as adding explicit instructions (e.g., Please think step by step.) can improve the models ability to break down complex problems into intermediate steps (Lightman et al., 2023). This strategy ensures more deliberate and structured thought generation by shaping the reasoning process at the input level. Other techniques such as (Wei et al., 2022; Ranaldi et al., 2025) also rely on explicitly stating the requirements in the prompt to stimulate samples during the TTS . Decode Strategy Rather than passively accepting the models default output behavior, this approach modifies the decoding process to encourage LLM to generate longer, more detailed samples adaptively. Techniques such as injecting filler token (Pfau et al., 2024), adaptively injecting predefined injection phrase (Jin et al., 2020), forcing scaling budget (Muennighoff et al., 2025), enforcing intermediate generation (Li et al., 2025g), enforcing prior distribution (Kong et al., 2025) or predictive decoding (Ma et al., 2025a) allow the model to modify its distribution progressively. Enforcing extended reasoning at the output level enables the model to think longer and generate more comprehensive solutions without requiring additional external guidance. Self-Repetition Strategy Apart from generating longer samples, another way to stimulate the LLM is to generate multiple samples instead of individual ones. One commonly adopted strategy is to prompt the LLM repeatedly during the decoding stage, commonly known as self-repetition (Wang et al., 2023). Another strategy is to prompt the LLM sequentially, in order to mimic refinement process (Madaan et al., 2023) or correlation under constraint (Ferraz et al., 2024). Mixture-of-Model Strategy Gathering the wisdom of the crowd can move beyond repeated sampling from single model to coordinated sampling across multiple models. These LLMs can play either homogeneous roles (Wang et al., 2025a) or heterogeneous roles (Chen et al., 2024f; He et al., 2025) during the process. By harnessing diverse perspectives, such multi-model strategy not only increases the coverage of possible solutions but also improves the systems overall robustness. 3.2.2 Verification Verifying the correctness and consistency of LLM during the test-time scaling is also crucial. The verification process plays an important role in the test-time scaling, as solid verification process can be adapted to: directly selects the output sample among various ones, under the Parallel Scaling paradigm; guides the stimulation process and determines when to stop, under the Sequential Scaling paradigm; serves as the criteria in the search process, which we will discuss in Section 3.2.3; determines what sample to aggregate and how to aggregate them, e.g., weights, which we will discuss in Section 3.2.4. Usually, there are two types of verifications, as shown below: 7 3.2 Inference-based Approaches Category Approach Approach Description Prompt Decode CoT (Wei et al., 2022) Step-by-step (Lightman et al., 2023) QuaSAR (Ranaldi et al., 2025) CoD (Xu et al., 2025b) Hint-infer (Li et al., 2025b) Think (Li et al., 2025b) Contains series of intermediate reasoning steps in prompts Stimulate step-by-step thinking via prompt Decompose CoT into Quasi-Symbolic Language Generate concrete representations and distill into concise equation Inserting artificially designed hints in the prompt Prompt LLM with Think before response Filler-token (Pfau et al., 2024) Budget-forcing (Muennighoff et al., 2025) LTV (Kong et al., 2025) AFT (Li et al., 2025g) Predictive-Decoding (Ma et al., 2025a) Adaptive Injection (Jin et al., 2025) uses arbitrary, irrelevant filler tokens before answering suppress the generation of the end-of-thinking token decode according to latent thought following prior model iteratively aggregating proposals and aggregate for future proposals re-weight decoding distribution given evaluation of foresight Injecting predefined injection phrase under certain condition Self-Repetition Self-Repetition (Wang et al., 2023) Self-Refine (Madaan et al., 2023) DeCRIM (Ferraz et al., 2024) prompt LM in parallel Naively prompt LM to iteratively refine answer Self-correlation for multi-constrained instruction following Mixture-of-Model MoA (Wang et al., 2025a) RR-MP (He et al., 2025) BRAIN (Chen et al., 2024f) Prompt different models in parallel and iteratively improve Propose Reactive and Reflection agents to collaborate Propose frontal & parietal lobe model to inspire brain Table 1: Summary of Certain Stimulation Techniques. Outcome Verification. Outcome verification plays crucial role in ensuring the correctness and consistency of generated outputs. Common approaches include using separate verifier model to score multiple candidate answers (e.g.,Cobbe et al. (2021)), employing self-consistency, voting mechanisms (Wang et al., 2023) and discriminator LM (Chen et al., 2024g) and leveraging tool-assisted (Gou et al., 2024) or heuristic checks (DeepSeek-AI, 2025) in domains such as math and code generation. For specific task problems, such as trip planning, functional scoring (Lee et al., 2025) is also adopted for verifying the proposed plans. Instead of formulating the outcome verification as classification problem, Zhang et al. (2025d) exploits the generative ability of LLM and proposes to reformulate the outcome verification process as next-token prediction task. Li et al. (2025h) formulate the feedback utilization as an optimization problem and adaptive propagate information between samples. Apart from single criteria, certain outcome verification approaches verify the quality of the simulated samples from multiple perspectives. Liu et al. (2023b) conducts both (i) passive verification from external tools and (ii) active verification via rethinking mechanism to justify each sample. Zhang et al. (2024c) follows similar idea and proposes to verify each sample from three aspects: Assertion, Process, and Result. Lifshitz et al. (2025) further extends the number of verification agents to an arbitrary number and decouples the semantic criteria with verification agents. Parmar et al. (2025) and Saad-Falcon et al. (2024) also propose verification agent to score each sample considering various factors, respectively. Saad-Falcon et al. (2024) additionally proposes unit test-based verification approach. We provide detailed technical categorization in the Appendix A. Process Verification. Process verification approaches verify the sample outcomes and the process of obtaining such an outcome. They are commonly adopted in tasks with formal, deductive processes, such as reasoning, coding, or mathematics. They are also known as the process reward model (PRM) or state verification. Lightman et al. (2023) processes to train PRM as step-level verification on mathematical tasks. Yao et al. (2023b) processes an LM-based state verifier as guidelines for searching the samples under the tree structure. Zhang et al. (2024b) further tunes these preference data into LLM and enables CoT structure during test time. Instead of training an external verifier, Xie et al. (2023) prompts the same LM to evaluate the current step given all previous ones. Hosseini et al. (2024) proposes to train the verifier with both accurate and inaccurate generated data. Although LM-based process verifiers can be easily integrated, they may yield unreliable verification, especially for complex problems with long processes. Ling et al. (2023) decomposes the verification process in deductive manner. Hence, the verifier only needs to verify few statements within the long thought chain. Yu et al. (2024a) is based on similar intuition but instead focuses on code-aided mathematical reasoning tasks with the critic model iteratively. Li et al. (2025b) instead relies on the external toolbox, such as code interpreters, to verify the process. 3.2.3 Search Search is also frequently used component during the test-time scaling. LLMs pre-trained on vast amounts of online data, can be viewed as compression of real-world knowledge. However, standard inference tends to underutilize their capacity. Search, being classic yet working technique in retrieving relevant information from vast databases, can be utilized to fully exploit the capability of LLMs by exploring their potential options in structured manner. Existing test-time scaling approaches based on search techniques demonstrate significant performance increases 8 3.2 Inference-based Approaches Category Approach Approach Description Outcome Naive ORM (Cobbe et al., 2021) OVM (Yu et al., 2024b) Heuristic (DeepSeek-AI, 2025) Functional (Lee et al., 2025) Bandit (Sui et al., 2025) Generative Verifier (Zhang et al., 2025d) Self-Reflection Feedback (Li et al., 2025h) Discriminator (Chen et al., 2024g) Unit Test (Saad-Falcon et al., 2024) XoT (Liu et al., 2023b) WoT (Zhang et al., 2024c) Multi-Agent Verifiers (Lifshitz et al., 2025) Multi-Perspective Verification without explicit semantic meanings Naively process to train solution-level and token-level verifiers on labeled-dataset Train value model under outcome supervision for guided decoding Heuristic check for domain-specific problems Functional scoring for task-specific problems Train bandit algorithm to learn how to verify Exploit the generative ability of LLM-based verifiers via reformulating the verification formulate the feedback utilization as an optimization problem and solve during test-time SFT domain-specific LM as discriminator Verify each sample as unit tests Passive verification from external tools and Activate verification via re-thinking Multi-Perspective Verification on three aspects: Assertion, Process, and Result Process Naive PRM (Lightman et al., 2023) State Verifier (Yao et al., 2023b) Deductive PRM (Ling et al., 2023) Self-Evaluation (Xie et al., 2023) PoT (Chen et al., 2023a) Tool (Li et al., 2025b) V-STaR (Hosseini et al., 2024) SFT an LM as PRM on each reasoning step over mathematical tasks SFT an LM as state verifier and evaluate states either independently or jointly Deductively verify few statements in the process Prompting the same LM to evaluate the current step given previous ones delegate computation steps to an external language interpreter Relies on external toolbox for verification Verifier trained on both accurate and inaccurate self-generated data Table 2: Summary of Certain Verification Techniques. over complex tasks, such as complex mathematics, etc. Yao et al. (2023b) explores the potential of search by decomposing the output samples into multiple thoughts and organizing them in tree structure. Based on only Naive tree search algorithms, such as depth-first search and breath-first search, it demonstrates superior performance on reasoning tasks. Monte-Carlo Tree Search (Coulom, 2006), being classical and powerful search algorithm, also shines its light on better exploiting the hidden knowledge of LLMs. Chaffin et al. (2022) adopts MCTS during the decoding stage guided by discriminators for constrained textual generation. Zhang et al. (2023b) further extends the MCTS to enhance the planning ability in code generation via looking ahead. Tian et al. (2024) incorporates the MCTS as critical component in the self-improving framework for LLM. Wan et al. (2024) tailors the search algorithm to tackle problems requiring long-horizon planning and deep tree structure for searching. Chen et al. (2024g) further identifies that discriminators are the key bottleneck in search-enhanced planning. Gandhi et al. (2024) systematizes the search process in unified language and proposes to train an LLM with data and feedback from the search process. Wu et al. (2024c) empirically analyzes various search algorithms and designs reward-balanced search algorithm toward Paretooptimal test-time scaling. Edward Beeching (2024) further extends the beam search by incorporating diversity consideration. Apart from searching within the tree structure, Besta et al. (2024) models the output as graph search problem. Xie et al. (2023) proposes stochastic beam search solution based on self-evaluation for reasoning tasks. Pan et al. (2025a) enhances MCTS with proposed associative memory to dynamically update its knowledge base. Li et al. (2025c) proposes to solve the reasoning process as constructing control flow graph with each node indicating logic unit. 3.2.4 Aggregation Aggregation techniques consolidate multiple solutions into final decision to enhance the reliability and robustness of model predictions at test time. Based on how the final output is generated, we empirically categorize them into two key classes: (i) Selection, which selects the best-performed sample among all candidates, where the selection criteria may vary across different approaches; and (ii) Fusion, which fuse multiple samples into one though tricks like weighting or generation. Selection In this category, the aggregation process can be viewed as selection problem. One well-known example is to select the most consistent answer, commonly known as self-consistency. Wang et al. (2023) improves accuracy by leveraging statistical redundancyif different reasoning paths converge to the same conclusion, the answer is more likely to be correct. Self-consistency effectively reduces variance in model outputs and mitigates occasional hallucinations. However, as the final output is voted based on consistency, inaccurate and low-quality samples would inevitably influence the output quality. Therefore, various approaches are proposed to filter the candidates before voting. Chen et al. (2024d) incorporates an LM as filter, while Wu et al. (2025b) proposes Length-filtered vote, where prediction uncertainty is adopted as proxy to filter reliable CoT length. Best-of-N (Irvine et al., 2023) follows the same process but replaces the self-consistency criteria with scalar scores generated by external verifiers. Song et al. (2024) further demonstrates that best-of-N on small LLMs can yield competitive performance against SOTA propriety models. Munkhbat et al. (2025) attaches few-conditioning filtering before the best-of-N selection. This aims to alleviate its sample inefficiency and achieves significant length 9 reduction. Motivated by particle filtering, Puri et al. (2025) proposes to consider filtering upon the samples. Sessa et al. (2024) went one step further in reducing sample inefficiency. It tunes the best-of-N results into the LM via RLHF. With the blooming of the agentic approach, Parmar et al. (2025) proposes selection agent considering complex factors with both historical and current status. Apart from selecting samples from one single LM, Ong et al. (2025) views the selection of samples generated by weak and strong LLMs as routing problem and proposes constraints on computation costs. Fusion Directly selecting the final output sample among candidates may yield unsatisfactory results, especially when the sample quality of candidates is low. Fusion approaches propose to merge multiple samples into one to solve such problem. Brown et al. (2024b) and Li et al. (2023a) extend the idea from Best-of-N and weigh each sample by its score from external verifiers. Jiang et al. (2023), on the other hand, directly prompts another LLM as summarizer to merge multiple selected samples. Li et al. (2025k) shares similar intuition by replacing the majority voting in self-consistency (Wang et al., 2024e) with generative self-aggregation. Li et al. (2025c) also adopts LLM as the synthesizer, given the intermediate consideration in previous steps. Category Approach External Verifier Approach Description Selection Fusion Majority Voting (Wang et al., 2023) Best-of-N (Irvine et al., 2023) Few-shot BoN (Munkhbat et al., 2025) Agentic (Parmar et al., 2025) Weighted BoN (Li et al., 2023a) Synthesize (Jiang et al., 2023) Ensemble Fusion (Saad-Falcon et al., 2024) Select the most common sample Select the highest scored sample BoN with few-shot conditioning agent considering both current and previous status Weight each sample by its score Fuse the selected samples via GenAI Conduct ensemble before fusion Also Utilized in (Chen et al., 2024d) (Song et al., 2024) (Brown et al., 2024b) (Wang et al., 2025a; Li et al., 2025c) Table 3: Summary of Certain Aggregation Techniques. BoN stands for Best-of-N."
        },
        {
            "title": "4 Where to Scale",
            "content": "TTS can substantially enhance LLMs performance across diverse real-world scenarios. We systematically categorize these scenarios into representative domains, detailing the characteristic challenges, critical evaluation criteria, and representative benchmarks that illustrate the practical value of TTS. Here, we also list brief summary of various benchmarks in Table 4. 4.1 Reasoning-intensive Tasks Reasoning-intensive tasks require structured, explicit, multi-step reasoning, precision, and rigorous correctness verification. These tasks challenge LLMs ability to systematically decompose problems, iteratively refine solutions, and verify intermediate reasoning steps. Mathematical Reasoning Mathematical tasks involve complex computations, logical inference, and iterative verification. Key challenges for TTS methods include generating accurate step-by-step solutions, effectively verifying intermediate steps, and handling intricate reasoning logic. Representative benchmarks include AIME 2024 (Google, 2025), MATH-500 (Zhang et al., 2024a), AMC 2023 (Guan et al., 2025b), and OlympiadBench (He et al., 2024a). These datasets span advanced competition-level math problems, emphasizing precise and explicit reasoning skills. Programming & Code Generation Coding tasks demand syntactic accuracy, executable correctness, and iterative debugging. Challenges for TTS methods lie in generating correct implementations, debugging code iteratively, and efficiently exploring multiple coding solutions. Representative datasets include Codeforces (codeforce, 2025), SWE-bench (Jimenez et al., 2024), and LiveCodeBench (Jain et al., 2025), each providing expert-level coding challenges that require rigorous logical thinking and implementation accuracy. Game Playing and Strategic Reasoning Strategic reasoning tasks involve adaptive planning, interactive decisionmaking, and complex multi-round reasoning. TTS methods must efficiently perform iterative search, adaptive inference, and dynamic interactions. representative benchmark is SysBench (Google, 2025), which evaluates models strategic reasoning in interactive tasks. Scientific Reasoning Scientific problems typically require multi-domain knowledge integration across physics, chemistry, biology, and other disciplines. TTS methods must demonstrate broad knowledge synthesis, multi-step reasoning, and accurate factual verification. Notable benchmarks include GPQA Diamond (Rein et al., 2024) and MR-Ben (Zeng et al., 2024), focusing on advanced scientific reasoning and integrated domain knowledge. 10 4.2 General-purpose Tasks Medical Reasoning Medical tasks involve diagnostic decision-making, clinical reasoning, and precise medical knowledge. The key challenge for TTS here is ensuring reliable, accurate reasoning that mimics medical professionals decision logic. Representative datasets include JAMA Clinical Challenge (Chen et al., 2025a), Medbullets (Chen et al., 2025a), and MedQA (Jin et al., 2020). These benchmarks critically assess reasoning LLMs capabilities in diagnosis, treatment planning, and medical decision accuracy. 4.2 General-purpose Tasks These tasks require broad, general-purpose reasoning capabilities, creativity, and subjective evaluation of outputs. Basics To achieve general objectives, many efforts have collected numerous official, public datasets that are challenging for humans but are not exclusive to any particular domain. Representative benchmarks include AGIEval (Zhong et al., 2024), MMLU-Pro (Wang et al., 2024d), and Gaokao (Guan et al., 2025b). Open-Ended Tasks TTS methods must enhance output diversity, quality, and coherence, balancing creativity and correctness. Representative benchmarks include AlpacaEval2.0 (Dubois et al., 2024), ArenaHard (Li et al., 2024b), IF-Eval (Zhou et al., 2023b), and C-Eval (Huang et al., 2023), which collectively evaluate subjective, open-ended, and general-purpose reasoning. Agentic Tasks Agentic tasks involve realistic and interactive environments, requiring complex planning, iterative reasoning, and effective tool utilization. TTS methods face challenges such as optimal stepwise planning, adaptive decision-making, tool integration, and iterative refinement. Representative benchmarks include WebShop (Yao et al., 2023a), WebArena (Zhou et al., 2023c), SciWorld (Wang et al., 2022), and TextCraft (Prasad et al., 2024). These datasets provide realistic interactive scenarios, emphasizing iterative decision-making and effective tool usage. Knowledge-intensive Tasks Knowledge-intensive tasks require LLMs to retrieve and synthesize factual knowledge from external sources, ensuring accuracy and reducing hallucinations. TTS challenges center around effective retrieval-augmented reasoning, iterative verification, and multi-source aggregation. Representative benchmarks include SimpleQA (Wei et al., 2024a), C-SimpleQA (He et al., 2024c), and FRAMES (Krishna et al., 2025), emphasizing factual correctness and retrieval-based reasoning. Multimodal Tasks Multimodal reasoning tasks demand effective cross-modal integration, iterative reasoning between modalities, and robust verification across visual and textual inputs. TTS methods face challenges in modality fusion, iterative multimodal reasoning, and handling ambiguity across modalities. Representative benchmarks include MMMU (Yue et al., 2024), MathVista (Lu et al., 2024), MathVision (Wang et al., 2024d), CMMaTH (Li et al., 2025j), and PGPS9K (Zhang et al., 2023a), each testing multimodal reasoning across visual and textual modalities."
        },
        {
            "title": "5 How Well to Scale",
            "content": "In this section, we classify the metrics used in evaluating the test-time scaling methods into four high-level dimensions: Performance, Controllability, Scalability, and Efficiency. Each dimension captures an essential aspect critical to assessing test-time scaling approaches. 5.1 Performance Performance metrics assess the correctness of generated solutions. Pass@1. Pass@1 is one of the most widely used metrics for evaluating the correctness of models first output attempt (DeepSeek-AI, 2025; Li et al., 2025e; Snell et al., 2024; Xie et al., 2025; Kimi, 2025; Yang et al., 2025b,a; Hou et al., 2025). It measures the proportion of problems where the models first generated solution is correct. correct solution means the one that exactly matches the ground-truth answer or passes all required validation checks, such as the exact answer match in mathematical benchmarks and private unit tests in coding tasks. Pass@1 is frequently used in tasks such as mathematical reasoning and coding benchmarks. In mathematical reasoning tasks such as AIME 2024 (Google, 2025) and MATH-500 (Zhang et al., 2024a), Pass@1 measures the percentage of exact matches between the models answer and the ground truth. In coding benchmarks such as LiveCodeBench (Jain et al., 2025) and HumanEval-Mul, Pass@1 evaluates the code correctness against hidden test cases. Pass@k (Coverage). Pass@k extends Pass@1 by measuring whether at least one of the models sampled outputs is correct (Brown et al., 2024a; Snell et al., 2024; Li et al., 2025e). Formally, Pass@k can be estimated using the unbiased estimator from Chen et al. (2021): Pass@k = 1 (cid:88) i=1 (cid:32) 1 (cid:33) (cid:1) , (cid:0)N Ci (cid:0)N (cid:1) 11 5.1 Performance Table 4: Summary of Benchmarks Benchmark Size Evaluation Criteria Example Task Key Features Type Reasoning-intensive Tasks FrontierMath (Glazer et al., 2024) MATH (Cobbe et al., 2021) NuminaMath (LI et al., 2024) OmniMath (Gao et al., 2025a) GSM8K (Zhang et al., 2024a) rStar-Math (Guan et al., 2025b) ReST-MCTS (Zhang et al., 2024a) s1 (Muennighoff et al., 2025) USACO (Shi et al., 2024) AlphaCode (Li et al., 2022) LiveCodeBench (Jain et al., 2025) SWE-bench (Jimenez et al., 2024) GPQA (Rein et al., 2024) OlympicArena (Huang et al., 2024a) OlympiadBench (He et al., 2024a) TheoremQA (Chen et al., 2023b) MedQA (Jin et al., 2020) AGIEval (Zhong et al., 2024) MMLU-Pro (Wang et al., 2024h) C-Eval (Huang et al., 2023) Gaokao (NCEE, 2025) Kaoyan (GSEE, 2025) CMMLU (Li et al., 2024) LongBench (Bai et al., 2024) IF-Eval (Zhou et al., 2023b) ArenaHard (Li et al., 2024b) Chatbot Arena (Zheng et al., 2023b) AlpacaEval2.0 (Dubois et al., 2024) WebShop (Yao et al., 2023a) WebArena (Zhou et al., 2023c) SciWorld (Wang et al., 2022) TextCraft (Prasad et al., 2024) SimpleQA (Wei et al., 2024a) C-SimpleQA (He et al., 2024c) FRAMES (Krishna et al., 2025) MMMU (Yue et al., 2024) MathVista (Lu et al., 2024) MATH-Vision (Wang et al., 2024d) LLAVA-Wild (Liu et al., 2023a) MM-Vet (Yu et al., 2024d) MMBench (Liu et al., 2024c) CVBench (Tong et al., 2024) MMStar (Chen et al., 2024c) CHAIR (Rohrbach et al., 2018) 448 11.1K 8.4K 1.3K 8K 12K 13.9K Varied Varied Varied Varied 541 500 Varied 805 1.18M Varied 30 tasks Varied 4.3K 3K 824 11.5K 6.1K 3K Varied Varied 3.2K Varied 1.5K Varied Hundreds 12.5K 860K 4.4K 8.5K 747K Varied 1K 307 Thousands 511 2.3K Exact match Exact match Exact match, CoT Accuracy Accuracy Pass@1 accuracy Accuracy Accuracy Pass@1 Solve rate Pass@1 Resolution rate Accuracy Accuracy Accuracy Accuracy Accuracy Algebraic geometry AMC/AIME-style Olympiad-level math Math Olympiads Grade-school math Competition math Multi-step reasoning Math/science tasks Olympiad coding Competitive coding Real-time coding GitHub issues High complexity Structured reasoning Annotated reasoning Advanced reasoning Natural-language solutions Iterative refinement Reward-guided search Controlled compute Creative algorithms Complex algorithms Live evaluation Multi-file edits Graduate STEM Multidisciplinary tasks Math/Physics Olympiads Theorem-based STEM Domain expertise Multimodal reasoning Expert multimodal tasks Theoretical application Math Code Science Clinical diagnostics Medical accuracy Medical General-purpose Tasks Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy College exams Multidisciplinary tests Chinese exams Chinese college exams Graduate entry exams Multi-task Chinese eval Bilingual multi-task eval Human-centric reasoning Deep reasoning complexity Multidisciplinary reasoning Broad knowledge Specialized knowledge Comprehensive coverage Long-form reasoning Accuracy Human preference Human alignment Win rate Task success Task completion Task-specific scores Success rate Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy GPT-4 score GPT-4 evaluation Accuracy Accuracy Accuracy Hallucination rate Instruction adherence Open-ended creativity Chatbot quality Chatbot responses Online shopping Web navigation tasks Scientific experiments Task decomposition Short queries Chinese queries Multi-hop queries Objective evaluation Human alignment User-aligned responses Debiased evaluation Real-world interaction Adaptive decision-making Interactive simulation Iterative planning Factual correctness Cultural relevance Source aggregation Multimodal expert tasks Multidisciplinary integration Visual math reasoning Visual math problems Visual QA Integrated multimodal Diverse multimodal Vision tasks Vision-critical QA Image captioning Visual-math integration Multimodal math reasoning Complex visuals Multi-capability eval Fine-grained eval High-quality eval Visual reliance Object hallucination Basic Open-ended Agentic Knowledge Multimodal where is the number of problems, is the total number of samples per problem, and Ci is the number of correct samples for the i-th problem. Pass@k is widely adopted in program synthesis and formal theorem-proving tasks, such as CodeContests (Li et al., 2022) and SWE-bench Lite (Jimenez et al., 2024). Cons@k (Consensus@k). Cons@k measures the majority vote correctness from independently sampled outputs (DeepSeek-AI, 2025; Zeng et al., 2025c). Given responses generated by model for given problem, the majority-voted prediction is the most frequent answer. The answer is then compared against the ground truth. Cons@k is frequently used alongside pass@1 to assess the benefit of leveraging multiple samples. Larger values of (e.g., 16, 64) typically improve answer stability and accuracy but at the cost of increased compute. This metric is especially valuable in tasks where single generations may be noisy or uncertain, and ensemble strategies can improve robustness. Cons@k has been widely adopted in mathematical reasoning benchmarks such as AIME 2024 (Google, 2025) and MATH-500 (Zhang et al., 2024a). Arena-based Evaluation (Pairwise Win Rate). In addition to accuracy-oriented metrics, some studies adopt pairwise comparison metrics, where model outputs are compared against baselines using human or LLM-based judges (DeepSeek-AI, 2025; Hou et al., 2025). For instance, LC-Winrate (Dubois et al., 2024) adjusts win rates to control for response length, while ArenaHard GPT-4 Judge (Li et al., 2024b) uses GPT-4-Turbo to score outputs 12 5.2 Efficiency from open-ended tasks. These pairwise evaluation methods are especially common in generation tasks where qualitative assessments (e.g., fluency, coherence) matter. Task-Specific Metrics. Certain domains employ specialized metrics. For example, Codeforces Percentile and Elo Rating are used to measure coding capabilities under competitive programming settings (DeepSeek-AI, 2025; Kimi, 2025). Percentile indicates how well model performs relative to other participants, while Elo Rating reflects relative skill under tournament-based evaluations. 5.2 Efficiency Efficiency metrics assess the computational and resource cost, offering insights into the practical deployment of test-time scaling methods. Token Cost. Token cost measures the total number of tokens generated during inference, including intermediate reasoning steps and final outputs (Welleck et al., 2024; Brown et al., 2024a; Hou et al., 2025; Yang et al., 2025b; Xu et al., 2025c; Wang et al., 2025c; Aytes et al., 2025). This metric is especially important, as verbose reasoning typically leads to higher token consumption. Reducing token cost while maintaining performance is crucial for inference efficiency, particularly when operating under fixed computational budgets or API pricing constraints. In addition, inference efficiency metrics such as latency and throughput are critical in real-world applications, especially for high-throughput systems (Welleck et al., 2024). FLOPs-based Efficiency Analysis. FLOPs-based compute analysis has been widely adopted to quantify computational cost (Kaplan et al., 2020; Snell et al., 2024; Wu et al., 2024b; Teng et al., 2025). Several recent works (Snell et al., 2024; Wu et al., 2024b) benchmark test-time scaling strategies, such as adaptive revisions and verifier-based search, against model scaling by plotting accuracy versus total inference FLOPs. This FLOPs-based evaluation can be used to determine whether inference-time methods outperform larger models under equivalent compute budgets. Underthinking score. The underthinking score (Wang et al., 2025e) quantifies the inefficiency of model when it initially generates correct thought but fails to follow through to correct final answer. It measures how early in the response the first correct thought appears, relative to the total length of the response, in cases where the final answer is incorrect. Formally, the underthinking score ξUT is defined as: ξUT = 1 N (cid:88) i=1 (cid:32) 1 (cid:33) ˆTi Ti (3) : Number of incorrect responses in the test set. Ti: Total number of tokens in the i-th incorrect response. ˆTi: Number of tokens from the beginning of the response up to and including the first correct thought. If no correct thought exists in the response, then ˆTi = Ti, indicating the model failed to meaningfully engage with the problem, and the score for that instance is zero (i.e., not underthinking). high ξUT value indicates greater inefficiency, where useful insights appear early but are not pursued, reflecting strong underthinking behavior. KV Cache Size. The KV cache size (Hooper et al., 2025) refers to the total memory footprint required to store the Key-Value cache across all trajectories and time steps during the inference-time search process. As each unique generation path requires its own KV cache, methods with low KV sharing across trajectories tend to consume significantly more memory and incur higher latency. By promoting KV cache sharing among trajectories, ETS reduces the total KV cache size, thereby improving throughput. For instance, ETS achieves up to 1.8 KV cache reduction compared to REBASE, leading to 1.4 faster inference on NVIDIA H100 GPUs, without compromising accuracy. 5.3 Controllability Controllability metrics evaluate whether inference-time methods can consistently adhere to pre-defined resource constraints such as compute budgets or output length targets. 13 5.4 Scalability . Muennighoff et al. (2025) propose Control as formal metric to quantify adherence to Control Metric specified compute budget range. It measures the fraction of test-time compute values that stay within given upper and lower bounds: Control = I(amin amax), 1 (cid:88) aA where is the set of observed compute values such as thinking tokens, and I() is the indicator function. score of 100% denotes perfect adherence to the compute budget across all tasks. Additionally, Hou et al. (2025) and Yang et al. (2025b) report experiments where models are evaluated under fixed token budgets, e.g., 1024, 2048, 4096, to examine how well models meet pre-specified length or token constraints during reasoning. Moreover, Xie et al. (2025) and Teng et al. (2025) impose explicit constraints on maximum output lengths to ensure inference-time stability and prevent output truncation. Length Deviation Metrics. Mean Deviation from Target Length and RMSE of Length Deviation are introduced to quantify models ability to control output length (Aggarwal and Welleck, 2025a): Mean Deviation from Target Length quantifies the average relative difference between the generated output length and the target length: Mean Deviation = ExD (cid:20) ngenerated ngold ngold (cid:21) , where ngenerated is the models output length and ngold is the target length. Root Mean Squared Error (RMSE) of Length Deviation captures the variance in length control: RMSE = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) i=1 (cid:18) ngenerated,i ngold,i ngold,i (cid:19)2 . Lower values for both metrics indicate more stable and precise length control across samples. kϵ Controllability. Bhargava et al. (2024) propose kϵ controllability as formal metric to characterize the prompt-based steerability of language models. Unlike metrics focused on compute or length constraints, this metric quantifies whether model can be guided to produce target output within bounded prompt length and allowable deviation. Formally, model is said to be (k, ϵ)-controllable for target output if there exists prompt with such that the model outputs with probability at least 1 ϵ: Pr[LLM(p) = y] 1 ϵ. By evaluating across different values of and ϵ, one can map out the controllability landscape of model. In practice, Bhargava et al. (2024) measures this property on tasks such as next-token prediction in WikiText, finding that over 97% of targets are reachable with prompt of at most 10 tokens and an error tolerance ϵ 0.05. This metric provides theoretical lens for quantifying how easily models outputs can be controlled via prompt design. While not directly tied to resource constraints, kϵ controllability offers valuable insight into the models test-time responsiveness and has been used to compare inherent steerability across model families and sizes. 5.4 Scalability Scalability metrics measure how effectively test-time scaling methods can leverage increased compute (e.g., token budgets, samples, inference steps) to improve performance. Scaling Metric Muennighoff et al. (2025) propose the Scaling metric, capturing the average slope of performance gains as compute increases: Scaling = 1 (cid:1) (cid:0)A 2 (cid:88) a,bA b>a (b) (a) . This metric quantifies how effectively models improve accuracy or pass rates with additional computation. Scaling Curves (Accuracy vs. Compute). Scaling curves are used to visualize how metrics such as accuracy, pass rate, or EM improve as token budgets, iteration depth, or the number of samples increase (Aggarwal and Welleck, 2025a; Teng et al., 2025; Wu et al., 2024b). These plots help reveal diminishing returns and performance saturation at higher compute budgets."
        },
        {
            "title": "6 Organization and Trends in Test-time scaling",
            "content": "Building on our taxonomy, we decompose the existing literature along multiple dimensions  (Table 5)  . As shown in Figure 4, these works, with different technical innovations, follow broadly consistent path. From 2022 to 2023, researchers emphasized structured inference to guide LLMs in generating more complex solutions. In 2024, methods like PRM and MCTS enabled the automatic supervision of intricate reasoning trajectories, yielding richly annotated data for fine-tuning and improving TTS performance. Subsequent approaches, such as o1 and R1, demonstrated that pure RL can also elicit comprehensive, logically sound reasoning. Figure 4: From Emergence to the Next Frontier, the Evolutionary Path of Test-Time Scaling. Method WHAT HOW WHERE HOW WELL SFT RL STIMULATION SEARCH VERIFICATION AGGREGATION Beam Search, LookAhead Search Verifier (Weighted) Best-of-N, Stepwise Aggregation Math DSC (Snell et al., 2024) MAV (Lifshitz et al., 2025) Mind Evolution (Lee et al., 2025) Meta-Reasoner (Sui et al., 2025) START (Li et al., 2025b) AID (Jin et al., 2025) CoD (Xu et al., 2025b) rStar-Math (Guan et al., 2025b) (Liu et al., 2025a) Tree of Thoughts (Yao et al., 2023b) MindStar (Kang et al., 2024) REBASE (Wu et al., 2025a) RaLU (Li et al., 2025c) PlanGen (Parmar et al., 2025) Puri et al. (2025) Archon (Saad-Falcon et al., 2024) AB-MCTS (Misaki et al., 2025) TPO (Wu et al., 2024a) SPHERE (Singh et al., 2025) MA-LoT (Wang et al., 2025b) OREO (Wang et al., 2024b) DeepSeek-R1 (DeepSeek-AI, 2025) s1 (Muennighoff et al., 2025) o1-Replication (Qin et al., 2024) AFT (Li et al., 2025g) Meta-CoT (Xiang et al., 2025) ReasonFlux (Yang et al., 2025a) l1 (Aggarwal and Welleck, 2025b) Marco-o1 (Zhao et al., 2024) Parallel, Sequential Parallel Sequential Sequential Parallel, Sequential Sequential Sequential Hybrid Parallel, Hybrid Hybrid Hybrid Hybrid Hybrid Parallel, Hybrid Hybrid Hybrid Hybrid Internal, Parallel Internal, Hybrid Internal, Sequential Internal, Sequential Internal Internal Internal Internal, Parallel Internal, Hybrid Internal, Sequential Internal Internal, Hybrid Rejection Sampling imitation imitation warmup distillation imitation imitation imitation distillation, imitation DPO DPO OREO GRPO, Rule-Based meta-RL PPO, Trajectory GRPO, Length-Penalty Self-Repetition Self-Refine CoT + Self-Repetition Hint-infer Adaptive Injection Decoding Chain-of-Draft Propose prompt Self-Repetition Self-Refine MoA MoA, Self-Repetition Mixture-of-Model Think Diversity Generation MoA Budget Forcing MCTS DVTS, Beam Search Tree Search LevinTS Reward Balanced Search Control Flow Graph Multiple-Agent Verifiers Functional Bandit Tool PRM PRM Self-Evaluate PRM RM Best-of-N Best-of-N Math, Code, General Open-Ended Game,Sci, Math Math, Code Math, Logical, Commonsense Math, Symbolic, Commonsense MATH Math GAME, Open-Ended MATH Math Self-Evaluate Prompt Synthesis MATH, Code Verification agent Selection Agent PRM+SSM Particle filtering Particle-based Monte Carlo AB-MCTS-(M,A) MCTS Verification agent, Unit Testing Judge models Self-Reflect Tool Beam Search Value Function (Ensemble) Fusion Math, General, Finance MATH Math, Code, Open-Ended Code Math, Code, Sci Math, Sci Pass@1, FLOPsMatched Evaluation BoN-MAV (Cons@k), Pass@1 Success Rate, Token Cost Accuracy, Token Cost Pass@1 Accuracy Accuracy, Latency, Token Cost Pass@1 Pass@1, Pass@k, Majority, FLOPS Success Rate, LLM-as-a-Judge Accuracy, Token Cost Test Error Rate, FLOPs Pass@1 Accuracy, F1 Score Pass@1, Budget vs. Accuracy Pass@1, Win Rate Pass@1, RMSLE, ROC-AUC Pass@1, cons@64, Percentile, Elo Rating, Win Rate Pass@1, Control, Scaling Accuracy Win Rate Win Rate Pass@1 Pass@1, Length Error Pass@1, Pass@k Open-Ended Win Rate Math Math Pass@ Pass@k Math, Agent Pass@1, Success Rate Journey Learning PRM, Critique Multi-Agents Math Think MCTS,A* Thought Template Retrieve PRM Reflection Prompt MCTS Self-Critic Fusion Math, Open-Ended Math, Open-Ended Math Math Math Table 5: Commonly-used combinations in existing literature when conducting inference scaling. Crucially, these techniques are complementary rather than mutually exclusive: for instance, R1 necessitates an SFT-based warmup via rejection sampling. Therefore, achieving more powerful scaling requires systematically integrating these methods. Even within RL frameworks, practitioners should continue to leverage synthesized CoT approaches and incorporate structured inference strategies to tackle increasingly complex scenarios effectively. 15 Researchers found that there does not exist one simple scaling solution that works for all problems. Increasingly, researchers tend to focus on optimal-scaling solutions (Wu et al., 2024c; Snell et al., 2024). The boundary between inference-based and tuning-based approaches is blurring. Consequentially, the target of scaling (what to scale) changes between different stages. Certain papers, such as Li et al. (2025g); Munkhbat et al. (2025), tune the inference-based capability into the LLM by synthesizing high-quality data from inference-based approaches as the tuning data. Others, such as Wan et al. (2024), are proposing various techniques that better exploit the LLMs capability during both the training and inference stages."
        },
        {
            "title": "7 A Hand-on Guideline for Test-time Scaling",
            "content": "In this section, we shift from theoretical categorizations to providing practical, hands-on guideline for TTS. Our goal is to offer clear, actionable instructions and technical pathways to facilitate effective SST deployment. (cid:17) Hands-on Guidelines: Common Problems ﬁ Q: What kind of task does TTS help? A: Almost any task! While traditional reasoning taskssuch as Olympiad-level mathematics, complex coding, and game-based challengeshave been shown to significantly improve with TTS , community observations suggest that TTS can also enhance performance in open-ended tasks, such as comment generation or evaluation. However, due to the long-form nature of outputs and the lack of centralized, objective benchmarks, these tasks are inherently more difficult to evaluate quantitatively, making it harder to draw conclusive claims. Beyond that, more realistic, complex, and long-horizon scenarios, like medical reasoning and law, have also shown promising gains through TTS strategies. ﬁ Q: If want to quickly implement TTS pipeline, what are the essential paths should consider? How can beginners use TTS at minimal cost? A: Broadly speaking, there are three essential technical pathways for test-time scaling: i) Deliberate reasoning procedure at inference time, ii) imitating complex reasoning trajectories, and iii) RL-based incentivization. If your goal is to get quick sense of the potential upper bound that strong TTS can bring to your task at minimum cost, you can directly utilize model that has been trained with (iii). If you want to develop TTS baseline at minimum cost, you can start with (i). Once (i) yields result that meets expectations, you can apply (ii) to further verify and generalize the outcome. ﬁ Q: Are these pipelines mutually exclusive? How should design frontier-level TTS strategy? A: These pipelines are by no means mutually exclusivethey can be seamlessly integrated. For instance, R1 inherently necessitates SFT through rejection sampling as preliminary warmup step. When employing RL, practitioners should continue leveraging synthesized CoT methods and introduce additional structured inference strategies to tackle increasingly complex scenarios effectively. ﬁ Q: What are some representative or widely-used TTS methods that can serve as baselines? A: ParallelSelf-Consistency, Best-of-N; SequentialSTaR, Self-Refine, PRM; HybridMCTS, ToT; InternalDistilled-R1, R1. ﬁ Q: Is there an optimal go-to solution so far? A: No free lunch. Optimal computing is often dependent on the hardness and openness of the question. ﬁ Q: How should we evaluate the performance of TTS method? In addition to standard accuracy, what other aspects should we pay attention to? A: The evaluation is largely task-aware, but metrics like accuracy remain the most critical indicators. In addition, efficiency (the trade-off between performance and cost) is another key concern in practical settings. As TTS becomes more general-purpose strategy, researchers have also begun evaluating range of secondary attributes, including robustness, safety, bias, and interpretability, to better understand the broader impacts of TTS . ﬁ Q: Is there any difference when tuning other scaling formats into internal scaling, compared with directly using the original scaling format? A: Yes, one intuitive difference lies in the efficiency aspect. Internal scaling tends to yield higher efficiency as it only prompts the LM once, while other scaling techniques usually require multiple trials. However, internal scaling requires non-neglectable resources for tuning, making it less available for practitioners."
        },
        {
            "title": "8 Challenges and Opportunities",
            "content": "8.1 More Scaling is the Frontier Pushing AI toward more general intelligence, especially for complex tasks, test-time scaling has emerged as one of the most promising methodologies in the post-pretraining era. Given its transformative impact on reasoningintensive tasksas seen in models like OpenAIs o1 and DeepSeek-R1it is increasingly clear that realizing the full promise of test-time scaling remains central pillar in advancing AGI. However, to push the frontier further, we need new and more effective strategies. There are some several promising research directions: Parallel Scaling. Parallel scaling improves solution reliability by generating multiple responses and selecting the best answer. Despite its effectiveness, parallel scaling remains has diminishing returns when coverage reaches saturation. key challenge is how to enhance coverage, shifting from brute-force coverage expansion to more guided, efficient process. Possible future advancements include: 1. Smart Coverage Expansion: Instead of naive best-of-N sampling, model could intelligently generate diverse reasoning paths, ensuring each sampled response explores meaningfully different approach; 2. Verifier-Augmented Parallel Scaling: Integrating real-time verification mechanisms could allow parallel samples to be filtered dynamically. Sequential Scaling. Sequential scaling faces unique challenges, particularly in maintaining coherence and preventing error accumulation. key issue is optimizing stepwise reasoning to avoid diminishing returns or reinforcing incorrect steps. Instead of naive iterative refinement, future advancements should focus on more adaptive and structured approaches to ensure each reasoning step meaningfully improves the final outcome. Possible directions include: 1. Structured Self-Refinement: Rather than blindly refining the entire response, models could learn to target specific parts of their reasoning that require adjustment. 2. Verification-Enhanced Iterative Scaling: Introducing real-time validation steps within the sequential reasoning process could prevent models from propagating early mistakes. This could involve running self-verification checks between iterations (e.g., checking consistency with known facts, comparing intermediate results to prior context, or re-computing specific logical steps). By selectively verifying before proceeding, models can ensure high-quality stepwise improvements instead of compounding errors. By addressing these challenges, sequential scaling can evolve beyond simple iterative refinement, becoming highly adaptive, self-correcting reasoning paradigm that enables models to engage in goal-directed, long-horizon thinking. Hybrid Scaling. Hybrid scaling blends parallel and sequential methods, making it more adaptive and practical for real-world applications. Current test-time scaling methods are often highly specialized, limiting their generalizability. To address these limitations, hybrid scaling can be improved in several ways: 1. Generalized Hybrid Scaling Architectures: research should focus on unifying test-time scaling mechanisms into single framework that dynamically chooses the best strategy for different query types. 2. Multi-Agent & Interactive Scaling: Expanding hybrid scaling beyond single-agent reasoning process could allow multiple model instances to engage in structured debate, argumentation, or negotiation, improving solution reliability. While current hybrid scaling is mostly studied in controlled benchmarks, future work must consider its role in real-world applications. Internal Scaling. this paradigm has demonstrated promising results, it also introduces unique challenges. Internal scaling allows on-the-fly computation modulation without external intervention. While 1. Effective Compute Allocation: Ensuring that internal scaling allocates extra reasoning steps only where necessary is critical. If the model overthinks simple tasks or fails to extend reasoning on complex ones, the benefits of dynamic computation are lost. 2. Stability and Consistency: As models extend their own reasoning paths, they risk logical drift, hallucination, or over-complication. Unlike sequential scaling, which can incorporate external verification, internal scaling must maintain self-consistency without external guidance. 17 8.2 Clarifying the Essence of Techniques in Scaling is the Foundation 3. Interpretability and Controllability: Internal scaling happens implicitly, making it difficult to diagnose failures or regulate inference costs. Unlike parallel scaling (which provides multiple explicit outputs) or sequential scaling (which follows structured iterations), internal scaling lacks clear intermediate checkpoints, posing challenges for debugging and efficiency management. By addressing these challenges, internal scaling has the potential to maximize efficiency, enhance model adaptability, and push AI systems toward more autonomous, self-regulating reasoning. 8.2 Clarifying the Essence of Techniques in Scaling is the Foundation While what to scale continues to evolve and techniques further developing internally, such as PPO transitioning to GRPO, we observe that the core categories of scaling techniques remain relatively stable. For example, SFT and RL remain two of the most common approaches, though their roles and interactions have shifted over time. This raises an urgent need to deepen our understanding of how these fundamental techniques contribute to test-time scaling. Here, we raise some potential directions for further investigation: 1. Theoretical Gaps in Scaling Techniques: How do core techniques (SFT, RL, reward modeling) contribute to test-time scaling? how should SFT and RL be optimally combined? 2. Re-evaluating Reward Modeling: whether PRMs actually improve multi-step inference? Does the classic reward model incorporate noise and unnecessary complexity? 3. Mathematical Properties of Test-Time Scaling: How does performance scale with increased inference steps? Is there an optimal stopping criterion? Are there fundamental constraints on how much test-time scaling can improve reasoning performance? 4. Chain-of-Thought Reasoning Priorities: which aspects of chain-of-thought are most crucial for effective test-time scaling? 5. Adaptive Test-Time Scaling: How can we make model automatically adjust its inference process based on the problem at hand? As empirical observations on certain property models (xAI, 2025) show blindly scaling over test-time may lead to over-thinking. 8.3 Optimizing Scaling is the Key As new TTS methods proliferate, systematic evaluation and optimization become critical. We must comprehensively measure how different strategies perform regarding task accuracy and consider efficiency, robustness, bias, safety, interpretability, and more. Optimizing these aspects of TTS is gradually emerging (Zhang et al., 2025a; Huang et al., 2025b) and will become an important part of future developments. 8.4 Generalization across Domains is the Mainstream We anticipate wave of research extending test-time scaling into wider range of domains, such as medicine and finance, where complex decision-making and structured reasoning are critical. This expansion is both inevitable and promising, as test-time scaling offers powerful mechanism to enhance reasoning depth, adapt computation dynamically, and improve accuracy without requiring costly retraining. Beyond these fields, we can expect widespread applications in law, AI evaluation, open-domain QA, and other high-stakes or knowledge-intensive areas. Despite its potential, scaling test-time reasoning across domains presents several key challenges: 1. Balancing Cost and Accuracy: Unlike general NLP tasks, specialized domains often require strict computational efficiency and reliability; 2. Ensuring Domain-Specific Interpretability: In fields like medicine and law, outputs must be transparent and justifiable; 3. Integrating External Knowledge & Real-World Constraints: Many domains require retrieval-augmented generation, real-time data analysis, or interactive query refinement; 4. Future research must identify generalizable test-time scaling strategies that are robust across diverse reasoning tasks. By addressing these challenges, test-time scaling can become foundational AI capability, enabling models to extend their own reasoning dynamically, adapt to real-world constraints, and generalize across specialized fields. This shift represents paradigm change, where AI systems dont just memorize knowledgethey actively scale their intelligence at inference to meet the demands of diverse, evolving tasks."
        },
        {
            "title": "9 Conclusion",
            "content": "This is the first survey to decompose TTS through hierarchical taxonomy, offering structured perspective that aids both conceptual understanding and the identification of individual contributions. Emphasizing practical utility, we introduce hands-on guideline aligned with each taxonomy dimension, which we plan to expand over time. Based on this framework, we outline key trends, challenges, and opportunities shaping the future of TTS research."
        },
        {
            "title": "Author Contributions",
            "content": "Below, we list the individual author contributions: Qiyuan Zhang and Fuyuan Lyu are core contributors who coordinate and finalize the full paper. Zexu Sun, Lei Wang, Weixu Zhang and Zhihan Guo are significant contributors who are responsible for certain chapters of this paper. Yufei Wang provides the overall structures of the taxonomy and provides close supervision during the process. Irwin King, Xue Liu and Chen Ma provide high-level suggestions on this survey overall."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. 2025a. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697. Pranjal Aggarwal and Sean Welleck. 2025b. L1: Controlling how long reasoning model thinks with reinforcement learning. In arXiv. Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. 2024a. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1224812267. Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. 2024b. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1224812267. aider. 2025. Aider. AntResearch-RL-Lab. 2025. Areal: Ant reasoning rl. https://github.com/inclusionAI/AReaL. Daman Arora, Himanshu Gaurav Singh, and Mausam . 2023. Have LLMs advanced enough? challenging In Conference on Empirical Methods in Natural problem solving benchmark for large language models. Language Processing. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations. Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang. 2025. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. In arXiv. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional ai: Harmlessness from ai feedback. In arXiv. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. Longbench: bilingual, multitask benchmark for long context understanding. In arXiv. Bespoke. 2025. Bespoke-stratos: The unreasonable effectiveness of reasoning www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation. cessed: 2025-01-22. distillation. Ac19 References Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large language models. AAAI Conference on Artificial Intelligence, page 1768217690. Aman Bhargava, Cameron Witkowski, Shi-Zhuo Looi, and Matt Thomson. 2024. Whats the magic word? control theory of llm prompting. In arXiv. Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. 2024. Forest-of-thought: Scaling test-time compute for enhancing llm reasoning. arXiv preprint arXiv:2412.09078. Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. 2025. Forest-of-thought: Scaling test-time compute for enhancing llm reasoning. In arXiv. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. 2024a. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. 2024b. Large language monkeys: Scaling inference compute with repeated sampling. In arXiv. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In arXiv. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. In arXiv. Antoine Chaffin, Vincent Claveau, and Ewa Kijak. 2022. Ppl-mcts: Constrained textual generation through In NAACL 2022-Conference of the North American Chapter of the discriminator-guided mcts decoding. Association for Computational Linguistics: Human Language Technologies, pages 115. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024a. Alphamath almost zero: Process supervision without process. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. 2025a. Benchmarking large language models on answering and explaining challenging medical questions. In arXiv. Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. 2024b. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. In arXiv. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. 2024c. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024d. Are more LLM calls all you need? towards the scaling properties of compound AI systems. In Conference on Neural Information Processing Systems. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025b. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. Weizhe Chen, Sven Koenig, and Bistra Dilkina. 2025c. Iterative deepening sampling for large language models. In arXiv. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. 2023a. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research. 20 References Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023b. TheoremQA: theorem-driven question answering dataset. In Conference on Empirical Methods in Natural Language Processing, pages 78897901. Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. 2023c. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations. Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. 2024e. Teaching large language models to self-debug. In International Conference on Learning Representations. Yezeng Chen, Zui Chen, and Yi Zhou. 2024f. Brain-inspired two-stage approach: Enhancing mathematical reasoning by imitating human thought processes. In arXiv. Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu Su, and Huan Sun. 2024g. When is tree search useful for LLM planning? it depends on the discriminator. In Annual Meeting of the Association for Computational Linguistics, pages 1365913678. Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, and Minlie Huang. 2025. Spar: Self-play with tree-search refinement to improve instruction-following in large language models. In arXiv. Francois Chollet. 2019. On the measure of intelligence. In arXiv. Sanjiban Choudhury. 2025. Process reward models for llm agents: Practical framework and directions. In arXiv. CMS. 2025. Chinese national high school mathematics olympiad. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. codeforce. 2025. Codeforces. Remi Coulom. 2006. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pages 7283. Springer. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. 2025. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. In arXiv. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. In arXiv. Sasha Rush Edward Beeching, Lewis Tunstall. 2024. Scaling test-time compute with open models. Jonathan Evans. 1984. Heuristic and analytic processes in reasoning. British Journal of Psychology, 75(4):451468. Yu Feng, Phu Mon Htut, Zheng Qi, Wei Xiao, Manuel Mager, Nikolaos Pappas, Kishaloy Halder, Yang Li, Yassine Benajiba, and Dan Roth. 2024. Diverseagententropy: Quantifying black-box llm uncertainty through diverse perspectives and multi-agent interaction. In arXiv. Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, and Nanyun Peng. 2024. Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 77737812. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D. Goodman. 2024. Stream of search (sos): Learning to search in language. In arXiv. Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, and Baobao Chang. 2024a. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. In arXiv. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Chenghao Ma, Shanghaoran Quan, Liang Chen, Qingxiu Dong, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Ge Zhang, Lei Li, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. 2025a. Omni-MATH: universal olympiad level mathematic benchmark for large language models. In International Conference on Learning Representations. 21 References Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, and Zhiqiang Xu. 2025b. Principled data selection for alignment: The hidden risks of difficult examples. arXiv preprint arXiv:2502.09650. Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. 2024b. Interpretable contrastive monte carlo tree search reasoning. In arXiv. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. 2025. Scaling up test-time compute with latent reasoning: recurrent depth approach. In arXiv. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Jarviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. 2024. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai. Ben Goertzel. 2014. Artificial general intelligence: Concept, state of the art, and future prospects. Journal of Artificial General Intelligence, pages 148. Google. 2024. Gemini 2.0 flash thinking. Google. 2025. Aime problems and solutions. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: Large language models can self-correct with tool-interactive critiquing. In International Conference on Learning Representations. GSEE. 2025. Chinese graduate school entrance examinations. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025a. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. In arXiv. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025b. rstarmath: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. 2024. Direct language model alignment from online ai feedback. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2025. Token-budgetaware llm reasoning. In arXiv. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. In arXiv. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024a. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Annual Meeting of the Association for Computational Linguistics, pages 38283850. Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, and Huimin Ma. 2025. Enhancing llm reasoning with multi-path collaborative reactive and reflection agents. In arXiv. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024b. Webvoyager: Building an end-to-end web agent with large multimodal models. In arXiv. Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Zhuoran Lin, Xuepeng Liu, Dekai Sun, Shirong Lin, Zhicheng Zheng, Xiaoyong Zhu, Wenbo Su, and Bo Zheng. 2024c. Chinese simpleqa: chinese factuality evaluation for large language models. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. In arXiv. 22 References Ruixin Hong, Xinyu Pang, and Changshui Zhang. 2024. Advances in reasoning by prompting large language models: survey. Cybernetics and Intelligence, pages 115. Coleman Hooper, Sehoon Kim, Suhong Moon, Kerem Dilmen, Monishwaran Maheswaran, Nicholas Lee, Michael W. Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. 2025. Ets: Efficient tree search for inference-time scaling. In arXiv. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. V-star: Training verifiers for self-taught reasoners. In First Conference on Language Modeling. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. 2025. Advancing language model reasoning through reinforcement learning and inference scaling. Jian Hu. 2025. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. 2025. Open-reasonerzero: An open source approach to scaling reinforcement learning on the base model. https://github. com/Open-Reasoner-Zero/Open-Reasoner-Zero. Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, and Qi Zhang. 2025a. Lean and mean: Decoupled value policy optimization with global value guidance. arXiv preprint arXiv:2502.16944. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. 2025b. Efficient test-time scaling via self-calibration. In arXiv. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, jiayi lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, and Pengfei Liu. 2024a. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent AI. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024b. O1 replication journey part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? In arXiv. HuggingFace. 2025. Open r1: fully open reproduction of deepseek-r1. Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, and William Beauchamp. 2023. Rewarding chatbots for real-world engagement with millions of users. In arXiv. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2025. Livecodebench: Holistic and contamination free evaluation of large language models for code. In International Conference on Learning Representations. Kaixuan Ji, Guanlin Liu, Ning Dai, Qingping Yang, Renjie Zheng, Zheng Wu, Chen Dun, Quanquan Gu, and Lin Yan. 2024. Enhancing multi-step reasoning abilities of language models through direct q-function optimization. arXiv preprint arXiv:2410.09302. Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, and Min Zhang. 2025. Test-time computing: from system-1 thinking to system-2 thinking. arXiv preprint arXiv:2501.02497. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. 2024a. Tigerscore: Towards building explainable metric for all text generation tasks. In arXiv. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. In Annual Meeting of the Association for Computational Linguistics, pages 1416514178. References Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024b. Followbench: multi-level fine-grained constraints following benchmark for large language models. In arXiv. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In International Conference on Learning Representations. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020. What disease does this patient have? large-scale open domain question answering dataset from medical exams. In arXiv. Hyunbin Jin, Je Won Yeom, Seunghyun Bae, and Taesup Kim. 2025. well, keep thinking: Enhancing llm reasoning with adaptive injection decoding. In arXiv. D. Kahneman. 2011. Thinking, Fast and Slow. Farrar, Straus and Giroux. Daniel Kahneman. 2003. Maps of bounded rationality: Psychology for behavioral economics. The American Economic Review, 93(5):14491475. Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, Jianye Hao, and Jun Yao. 2024. Mindstar: Enhancing math reasoning in pre-trained llms at inference time. In arXiv. Zhewei Kang, Xuandong Zhao, and Dawn Song. 2025. Scalable best-of-n selection for large language models via self-certainty. In arXiv. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. In arXiv. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679. Kimi. 2025. Kimi k1.5: Scaling reinforcement learning with llms. In arXiv. Deqian Kong, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si, Chuan Li, Jianwen Xie, Sirui Xie, and Ying Nian Wu. 2025. Scalable language models with posterior inference of latent thought vectors. In arXiv. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2025. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing frontiers in open language model post-training. In arXiv. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Rewardbench: Evaluating reward models for language modeling. In arXiv. Gregory Kang Ruey Lau, Wenyang Hu, Diwen Liu, Jizhuo Chen, See-Kiong Ng, and Bryan Kian Hsiang Low. 2024. Dipper: Diversity in prompts for producing large language model ensembles in reasoning tasks. In arXiv. Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. 2025. Evolving deeper llm thinking. In arXiv. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In Conference on Neural Information Processing Systems. Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, and Nanyun Peng. 2025a. METAL: multi-agent framework for chart generation with test-time scaling. In arXiv. Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. 2025b. START: Self-taught reasoner with tools. In arXiv. References Cheryl Li, Tianyuan Xu, and Yiwen Guo. 2025c. Reasoning-as-logic-units: Scaling test-time reasoning in large language models through logic unit alignment. In arXiv. Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E. Gonzalez, and Ion Stoica. 2025d. S*: Test time scaling for code generation. Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph Gonzalez, and Ion Stoica. 2025e. S*: Test time scaling for code generation. arXiv preprint arXiv:2502.14382. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. 2025f. Llms can easily learn to reason from demonstrations structure, not content, is what matters! In arXiv. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2024. Cmmlu: Measuring massive multitask language understanding in chinese. In arXiv. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann [https://github.com/ Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numinamath. project-numina/aimo-progress-prize](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf). Minzhi Li, Zhengyuan Liu, Shumin Deng, Shafiq Joty, Nancy F. Chen, and Min-Yen Kan. 2024a. Dna-eval: Enhancing large language model evaluation through decomposition and aggregation. In arXiv. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024b. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. In arXiv. Yafu Li, Zhilin Wang, Tingchen Fu, Ganqu Cui, Sen Yang, and Yu Cheng. 2025g. From drafts to answers: Unlocking llm potential via aggregation fine-tuning. In arXiv. Yanyang Li, Michael Lyu, and Liwei Wang. 2025h. Learning to reason from feedback at test-time. In arXiv. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023a. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with alphacode. Science, pages 10921097. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. 2025i. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419. Zhongzhi Li, Ming-Liang Zhang, Pei-Jie Wang, Jian Xu, Rui-Song Zhang, Yin Fei, Zhi-Long Ji, Jin-Feng Bai, Zhen-Ru Pan, Jiaxin Zhang, and Cheng-Lin Liu. 2025j. CMMaTH: Chinese multi-modal math skill evaluation benchmark for foundation models. In International Conference on Computational Linguistics, pages 26902726. Zichong Li, Xinyu Feng, Yuheng Cai, Zixuan Zhang, Tianyi Liu, Chen Liang, Weizhu Chen, Haoyu Wang, and Tuo Zhao. 2025k. Llms can generate better answer by aggregating their own responses. In arXiv. Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. 2023b. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv preprint arXiv:2310.10505. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. Encouraging divergent thinking in large language models through multi-agent debate. In Conference on Empirical Methods in Natural Language Processing, pages 1788917904. Shalev Lifshitz, Sheila A. McIlraith, and Yilun Du. 2025. Multi-agent verification: Scaling test-time compute with goal verifiers. In Workshop on Reasoning and Planning for Large Language Models. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. 25 References Qingwen Lin, Boyan Xu, Zijian Li, Zhifeng Hao, Keli Zhang, and Ruichu Cai. 2025. Leveraging constrained monte carlo tree search to generate reliable long chain-of-thought for mathematical reasoning. In arXiv. Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. 2024. Critical tokens matter: Token-level contrastive estimation enhence llms reasoning capability. arXiv preprint arXiv:2411.19943. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. 2023. Deductive verification of chain-of-thought reasoning. In Advances in Neural Information Processing Systems, volume 36, pages 3640736433. Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, and Reyhaneh Jabbarvand. 2024a. Codemind: framework to challenge large language models for code reasoning. In arXiv. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916. Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, and Yahui Zhou. 2024b. Improving multi-step reasoning abilities of large language models with direct advantage policy optimization. In arXiv. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. 2025a. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703. Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. 2023b. Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts. In Conference on Empirical Methods in Natural Language Processing. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023c. G-eval: Nlg evaluation using gpt-4 with better human alignment. In arXiv. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2025b. Pairjudge rm: Perform best-of-n sampling with knockout tournament. In arXiv. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2024c. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2024. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In arXiv. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/ DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog. Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. 2025a. Non-myopic generation of language models for reasoning and planning. In International Conference on Learning Representations. Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. 2025b. Inference-time scaling for diffusion models beyond scaling denoising steps. In arXiv. Yiran Ma, Zui Chen, Tianqiao Liu, Mi Tian, Zhuo Liu, Zitao Liu, and Weiqi Luo. 2025c. What are step-level reward models rewarding? counterintuitive findings from mcts-boosted mathematical reasoning. In arXiv. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Conference on Neural Information Processing Systems. Tarek Mahmud, Bin Duan, Corina Pasareanu, and Guowei Yang. 2025. Enhancing llm code generation with ensembles: similarity-based selection approach. In arXiv. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 2024. Llm critics help catch llm bugs. In arXiv. 26 References Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235. Kou Misaki, Yuichi Inoue, Yuki Imajuku, So Kuroki, Taishi Nakamura, and Takuya Akiba. 2025. Wider or deeper? scaling llm inference-time compute with adaptive branching tree search. In arXiv. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. In arXiv. Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. 2025. Self-training elicits concise reasoning in large language models. In arXiv. NCEE. 2025. Chinas national college entrance examination. Alex Nguyen, Dheeraj Mekala, Chengyu Dong, and Jingbo Shang. 2024. When is the consistent prediction likely to be correct prediction? In arXiv. Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2024. Next: Teaching large language models to reason about code execution. In arXiv. Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen tau Yih, Sida I. Wang, and Xi Victoria Lin. 2023. Lever: Learning to verify language-to-code generation with execution. In arXiv. Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, and Eric Horvitz. 2024. From medprompt to o1: Exploration of run-time strategies for medical challenge problems and beyond. arXiv preprint arXiv:2411.03590. NovaSky. 2025. Sky-t1: Train your own o1 preview model within $450. https://novasky-ai.github.io/posts/sky-t1. Accessed: 2025-01-09. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, Waleed Kadous, and Ion Stoica. 2025. RouteLLM: Learning to route LLMs from preference data. In International Conference on Learning Representations. OpenAI. 2024a. Gpt-4 technical report. In arXiv. OpenAI. 2024b. Openai o1 system card. In arXiv. OpenAI. 2025. Openai o3-mini system card. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Jianfeng Pan, Senyou Deng, and Shaomang Huang. 2025a. Coat: Chain-of-associated-thoughts framework for enhancing large language models reasoning. In arXiv. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. 2025b. Tinyzero. https://github.com/Jiayi-Pan/TinyZero. Accessed: 2025-01-24. Mihir Parmar, Xin Liu, Palash Goyal, Yanfei Chen, Long Le, Swaroop Mishra, Hossein Mobahi, Jindong Gu, Zifeng Wang, Hootan Nakhost, Chitta Baral, Chen-Yu Lee, Tomas Pfister, and Hamid Palangi. 2025. Plangen: multi-agent framework for generating planning and reasoning trajectories for complex problem solving. In arXiv. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. In arXiv. Jacob Pfau, William Merrill, and Samuel R. Bowman. 2024. Lets think dot by dot: Hidden computation in transformer language models. In Conference on Language Modeling. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, et al. 2025. Humanitys last exam. arXiv preprint arXiv:2501.14249. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2024. Adapt: As-needed decomposition and planning with language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 42264252. 27 References Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, and Akash Srivastava. 2025. probabilistic inference approach to inference-time scaling of llms using particle-based monte carlo methods. In arXiv. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. 2024. O1 replication journey: strategic progress report part 1. In arXiv. Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang. 2024. Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling. arXiv preprint arXiv:2410.16033. Qwen. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Leonardo Ranaldi, Marco Valentino, Alexander Polonsky, and Andr`e Freitas. 2025. Improving chain-of-thought reasoning via quasi-symbolic abstractions. In arXiv. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Matthew Renze. 2024. The effect of sampling temperature on problem solving in large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 73467356. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156. Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, E. Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Re, and Azalia Mirhoseini. 2024. Archon: An architecture search framework for inference-time techniques. In arXiv. Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang. 2025. Learning to plan & reason for evaluation with thinking-llm-as-a-judge. In arXiv. Alireza Salemi and Hamed Zamani. 2024. Towards search engine for machines: Unified ranking for multiple retrieval-augmented large language models. In arXiv. Tom Schaul. 2024. Boundless socratic learning with language games. In Language Gamification-NeurIPS 2024 Workshop. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. Bilgehan Sel, Ahmad Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. 2024. Algorithm of thoughts: Enhancing In International Conference on Machine Learning, pages exploration of ideas in large language models. 4413644189. PMLR. Pier Giuseppe Sessa, Robert Dadashi, Leonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Rame, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos, Amelie Heliou, Aliaksei Severyn, Matt Hoffman, Nikola Momchev, and Olivier Bachem. 2024. Bond: Aligning llms with best-of-n distillation. In arXiv. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Ben Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024. Can language models solve olympiad programming? In Conference on Language Modeling. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alexander Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2024. Beyond human data: Scaling self-training for problem-solving with language models. Transactions on Machine Learning Research. Joykirat Singh, Tanmoy Chakraborty, and Akshay Nambi. 2025. Self-evolved preference optimization for enhancing mathematical reasoning in small language models. In arXiv. 28 References Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Yifan Song, Guoyin Wang, Sujian Li, and Bill Yuchen Lin. 2024. The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism. In arXiv. Keith E. Stanovich and Richard F. West. 2000. Advancing the rationality debate. Behavioral and Brain Sciences, page 701717. Yuan Sui, Yufei He, Tri Cao, Simeng Han, and Bryan Hooi. 2025. Meta-reasoner: Dynamic guidance for optimized inference-time reasoning in large language models. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: multi-level large language model evaluation benchmark for scientific research. In AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence. Zexu Sun, Yiju Guo, Yankai Lin, Xu Chen, Qi Qi, Xing Tang, and Ji-Rong Wen. 2025. Uncertainty and influence aware reward model refinement for reinforcement learning from human feedback. In The Thirteenth International Conference on Learning Representations. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Principle-driven self-alignment of language models from scratch with minimal human supervision. In Advances in Neural Information Processing Systems, pages 25112565. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12. Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, and Gal Yona. 2025. Confidence improves self-consistency in llms. In arXiv. Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. 2025. Atom of thoughts for markov llm test-time scaling. arXiv preprint arXiv:2502.12018. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Lei Han, Haitao Mi, and Dong Yu. 2024. Toward self-improvement of LLMs via imagination, searching, and criticizing. In Conference on Neural Information Processing Systems. Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, and Dawn Song. 2025. Codehalu: Investigating code hallucinations in llms via execution-based verification. In arXiv. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. 2024. Cambrian-1: fully open, visioncentric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcome-based feedback. In arXiv. Juraj Vladika and Florian Matthes. 2024. Improving health question answering with reliable and time-aware evidence retrieval. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 47524763. David Wan, Justin Chih-Yao Chen, Elias Stengel-Eskin, and Mohit Bansal. 2025. Mamm-refine: recipe for improving faithfulness in generation with multi-agent collaboration. In arXiv. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2024. Alphazero-like tree-search can guide large language model decoding and training. In Forty-first International Conference on Machine Learning. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. 2024a. Planning in natural language improves llm search for code generation. In arXiv. Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi Wu. 2024b. Offline reinforcement learning for llm multi-step reasoning. arXiv preprint arXiv:2412.16145. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. 2024c. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671. 29 References Junlin Wang, Jue WANG, Ben Athiwaratkun, Ce Zhang, and James Zou. 2025a. Mixture-of-agents enhances large language model capabilities. In International Conference on Learning Representations. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. 2024d. Measuring multimodal mathematical reasoning with MATH-vision dataset. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024e. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Annual Meeting of the Association for Computational Linguistics, pages 94269439. Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, and Tong Zhang. 2025b. Ma-lot: Multi-agent lean-based long chain-of-thought reasoning enhances formal theorem proving. In arXiv. Ruoyao Wang, Peter Jansen, Marc-Alexandre Cˆote, and Prithviraj Ammanabrolu. 2022. Scienceworld: Is your agent smarter than 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1127911298. Tianlong Wang, Junzhe Chen, Xueting Han, and Jing Bai. 2024f. Cpl: Critical plan step learning boosts llm generalization in reasoning tasks. arXiv preprint arXiv:2409.08642. Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, and Kan Li. 2025c. Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning. In arXiv. Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. 2024g. Guiding language model reasoning with planning tokens. In Conference on Language Modeling. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024h. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Yubo Wang, Xiang Yue, and Wenhu Chen. 2025d. Critique fine-tuning: Learning to critique is more effective than learning to imitate. In arXiv. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025e. Thoughts are all over the place: On the underthinking of o1-like llms. In arXiv. Zhao Wang, Sota Moriyama, Wei-Yao Wang, Briti Gangopadhyay, and Shingo Takamatsu. 2025f. Talk structurally, act hierarchically: collaborative framework for llm multi-agent systems. In arXiv. Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Re, Irina Rish, and Ce Zhang. 2024. Redpajama: an open dataset for training large language models. In arXiv. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024a. Measuring short-form factuality in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. 2024b. Long-form factuality in large language models. In arXiv. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. 2025. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460. References Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. Qurating: Selecting high-quality data for training language models. In arXiv. Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024a. Thinking llms: General instruction following with thought generation. In arXiv. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024b. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024c. Scaling inference computation: Compute-optimal inference for problem-solving with language models. In Workshop on Mathematical Reasoning and AI at NeurIPS24. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2025a. Inference scaling laws: An empirical analysis of compute-optimal inference for llm problem-solving. In The Thirteenth International Conference on Learning Representations. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. 2025b. When more is less: Understanding chain-of-thought length in llms. In arXiv. Zengqing Wu and Takayuki Ito. 2025. The hidden strength of disagreement: Unraveling the consensus-diversity tradeoff in adaptive multi-agent systems. In arXiv. X-R1Team. 2025. X-r1. https://github.com/dhcode-cpp/X-R1. Github. xAI. 2025. Grok 3 beta - the age of reasoning agents. Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Jianhua Han, Lanqing Hong, Hang Xu, and Xiaodan Liang. 2024. Atomthink: slow thinking framework for multimodal mathematical reasoning. In arXiv. Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, and Chelsea Finn. 2025. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: benchmark for real-world planning with language agents. In International Conference on Machine Learning, pages 5459054613. PMLR. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023. Self-evaluation guided beam search for reasoning. In Thirty-seventh Conference on Neural Information Processing Systems. Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, and Debing Zhang. 2025a. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? In arXiv. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025b. Chain of draft: Thinking faster by writing less. In arXiv. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Conference on Empirical Methods in Natural Language Processing. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. 2025c. Softcot: Soft chain-of-thought for efficient reasoning with llms. In arXiv. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_ berkeley_function_calling_leaderboard.html. Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. 2025a. Reasonflux: Hierarchical llm reasoning via scaling thought templates. In arXiv. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. 2025b. Towards thinking-optimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080. 31 References Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing the power of large language models in automated code translation. In arXiv. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2023a. Webshop: Towards scalable real-world web interaction with grounded language agents. In arXiv. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. τ -bench: benchmark for tool-agentuser interaction in real-world domains. In arXiv. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023b. Tree of thoughts: Deliberate problem solving with large language models. In Conference on Neural Information Processing Systems. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023c. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. LIMO: Less is more for reasoning. In arXiv. Ziyu Ye, Rishabh Agarwal, Tianqi Liu, Rishabh Joshi, Sarmishta Velury, Quoc V. Le, Qijun Tan, and Yuan Liu. 2024. Evolving alignment via asymmetric self-play. In arXiv. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chain-of-thought reasoning in llms. In arXiv. Hao Yi, Qingyang Li, Yulan Hu, Fuzheng Zhang, Di Zhang, and Yong Liu. 2025. Sppd: Self-training with process preference learning using dynamic value margin. arXiv preprint arXiv:2502.13516. Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, and Dong Yu. 2024a. Siam: Self-improving code-assisted mathematical reasoning of large language models. In arXiv. Fei Yu, Anningzhe Gao, and Benyou Wang. 2024b. Ovm, outcome-supervised value models for planning in mathematical reasoning. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 858875. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. 2024c. Distilling system 2 into system 1. In The First Workshop on System-2 Reasoning at Scale, NeurIPS24. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2024d. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International Conference on Machine Learning, pages 5773057754. PMLR. Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. 2025. Whats behind ppos collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In arXiv. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. 2025a. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 2025b. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https: //hkust-nlp.notion.site/simplerl-reason. Notion Blog. 32 References Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. 2025c. Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities? arXiv preprint arXiv:2502.12215. Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, and Jiaya Jia. 2024. MR-ben: meta-reasoning benchmark for evaluating system-2 thinking in LLMs. In Conference on Neural Information Processing Systems. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. 2024. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In arXiv. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024a. ReST-MCTS*: LLM selftraining via process reward guided tree search. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. 2025a. Lightthinker: Thinking step-by-step compression. In arXiv. Kechi Zhang, Ge Li, Jia Li, Yihong Dong, and Zhi Jin. 2025b. Focused-dpo: Enhancing code generation through focused preference optimization on error-prone points. arXiv preprint arXiv:2502.11475. Kongcheng Zhang, Qi Yao, Baisheng Lai, Jiaxing Huang, Wenkai Fang, Dacheng Tao, Mingli Song, and Shunyu Liu. 2025c. Reasoning with reinforced functional token tuning. arXiv preprint arXiv:2502.13389. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2025d. Generative verifiers: Reward modeling as next-token prediction. In arXiv. Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. 2023a. multi-modal neural geometric solver with textual clauses parsed from diagram. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 33743382. Qiyuan Zhang, Yufei Wang, Yuxin Jiang, Liangyou Li, Chuhan Wu, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, and Chen Ma. 2025e. Crowd comparative reasoning: Unlocking comprehensive evaluations for llm-as-a-judge. In arXiv. Qiyuan Zhang, Yufei Wang, Tiezheng YU, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, and Chen Ma. 2025f. Reviseval: Improving LLM-as-a-judge via response-adapted references. In International Conference on Learning Representations. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. 2023b. Planning with large language models for code generation. In International Conference on Learning Representations. Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. 2024b. Chain of preference optimization: Improving chain-of-thought reasoning in LLMs. In Conference on Neural Information Processing Systems. Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng Lu, and Libo Qin. 2024c. Wrong-of-thought: An integrated reasoning framework with multi-perspective verification and wrong information. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 66446653. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. 2024d. Small language models need strong verifiers to self-correct reasoning. In ACL (Findings). Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. 2024e. o1-coder: an o1 replication for coding. In arXiv. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards open reasoning models for open-ended solutions. In arXiv. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems, volume 36, pages 4659546623. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. In arXiv. 33 CONTENTS Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. 2023c. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2024. AGIEval: human-centric benchmark for evaluating foundation models. In Findings of North American Chapter of the Association for Computational Linguistics, pages 22992314. Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2023a. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023b. Instruction-following evaluation for large language models. In arXiv. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023c. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854. Contents 1 Introduction 2 What to Scale 2.1 Parallel Scaling . . 2.2 Sequential Scaling . . 2.3 Hybrid Scaling . . Internal Scaling . 2.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 How to Scale 3.2 3.1 Tuning-based Approaches . . . . . . . 3.1.1 Supervised Finetuning (SFT) . 3.1.2 Reinforcement Learning (RL) . . . Inference-based Approaches . . . . 3.2.1 Stimulation . . . 3.2.2 Verification . . . . . . . 3.2.3 . Search . . . . . 3.2.4 Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . 4 Where to Scale 4.1 Reasoning-intensive Tasks . 4.2 General-purpose Tasks 5 How Well to Scale . 5.1 Performance . 5.2 Efficiency . . . 5.3 Controllability . . 5.4 Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Organization and Trends in Test-time scaling 7 Hand-on Guideline for Test-time Scaling 8 Challenges and Opportunities 8.1 More Scaling is the Frontier . . . . . 8.2 Clarifying the Essence of Techniques in Scaling is the Foundation . . . . . 8.3 Optimizing Scaling is the Key . . . . 8.4 Generalization across Domains is the . . . . . Mainstream . . . . . . . . . 9 Conclusion 16 17 17 18 18 18 19 Detailed Outcome Verification Methods 35 A.1 Verifier Model-Based Scoring . . . . 35 A.2 Self-Consistency and Voting Mechanisms 35 A.3 Tool-Assisted and Heuristic Verification 35 . . . Representative Methods . . . . . . . . . B.1 Best-of-N . . . . . . . . . . . B.2 Majority Voting . . . . . . . . . . . . B.3 Process Reward Model . . . . . . . . . B.4 MCTS . . . . . . . . . . . . . . B.5 Self-Refine . . B.6 Tree-of-Thought . . . . . . . . . B.7 Reinforcement Learning . . . . . . . . . . . . . . . . 35 35 36 36 36 37 37 1 2 4 4 4 5 5 5 5 5 6 6 7 8 9 10 10 11 11 11 13 13"
        },
        {
            "title": "A Detailed Outcome Verification Methods",
            "content": "This appendix expands on the outcome verification techniques employed at test time in LLMs. Unlike training-time methods (e.g., RL fine-tuning), these techniques operate on the fly during inference, often by generating multiple solutions and using proposerverifier framework. A.1 Verifier Model-Based Scoring The verifier, which is typically trained using human feedback or supervised data (e.g., as in (Cobbe et al., 2021; Lambert et al., 2024)), scores each candidate based on its expected correctness or quality. Variants include i) pairwise comparison verifiers (Liu et al., 2025b), where candidates are compared against each other to determine winner, ii) weighted voting systems (Wettig et al., 2024; Li et al., 2024a) that use the verifiers scores to combine outputs, iii) LLM-based verifiers that prompt LLM to perform evaluation instruction, like LLM-as-a-Judge (Zheng et al., 2023a; Zhang et al., 2025e,f), LLM-based Evaluator (Liu et al., 2023c; Xu et al., 2023; Jiang et al., 2024a), Critic-based Model (Gao et al., 2024a; McAleese et al., 2024). A.2 Self-Consistency and Voting Mechanisms Self-consistency techniques generate multiple independent reasoning chains and choose the final answer based on majority voting (Wang et al., 2023). The underlying assumption is that if several chains converge on the same answer, that answer is more likely to be correct. Some approaches (Taubenfeld et al., 2025; Mahmud et al., 2025) also incorporate confidence scores or soft-voting schemes to mitigate noise in individual outputs. In place of multiple samples from one model, one can also have multiple models (Wan et al., 2025; Wu and Ito, 2025; Wang et al., 2025f; Feng et al., 2024; Chen et al., 2024b): if majority (or consensus) of these agents agree on an answer, trust it; if they diverge, it may trigger further scrutiny. This is effectively an ensemble vote. A.3 Tool-Assisted and Heuristic Verification In domains like code generation or mathematical problem-solving, outcome verification can be implemented via direct execution or rule-based checks. For example, candidate programs are executed on sample test cases to ensure they produce correct results, while in math tasks, answers can be validated by plugging them back into the original equations. These approaches serve as an external check on the LLMs internal reasoning. Execution-Based Verification. In programming tasks, the ultimate test of correctness is running the code (Tian et al., 2025; Ni et al., 2024; Yang et al., 2024; Ni et al., 2023). For math problems, simple heuristic is to verify the answer by plugging it back into the original equation or problem constraints. Similarly, if puzzle answer must satisfy certain conditions, those can be programmatically checked. Fact-Checking via Retrieval. In open-domain QA or tasks that risk factual errors, search engines or knowledge bases serve as powerful verifiers (Wei et al., 2024b; Vladika and Matthes, 2024; Asai et al., 2023; Peng et al., 2023). An LLM may draft an answer, but then the system issues search queries (based on the answers claims) to find supporting evidence. If the retrieved documents contradict the LLMs answer, the answer is likely incorrect and can be rejected or revised. Some frameworks generate answers in closed-book fashion, then do post-hoc retrieval to validate facts. This idea overlaps with Retrieval-Augmented Generation (Salemi and Zamani, 2024), but the focus is on post-generation validation essentially checking if the answer aligns with external truth. Rule-Based Filters. In some applications, simple heuristic filters (Bai et al., 2022; Sun et al., 2023; Weber et al., 2024) can automatically reject bad outputs. For dialogue system, one might have list of forbidden answers (certain unsafe or nonsensical replies) and if the model outputs one, the system can either regenerate or adjust it. These arent outcome-based in terms of correctness, but they verify the output against predefined rules of form and content."
        },
        {
            "title": "B Representative Methods",
            "content": "B.1 Best-of-N The Best-of-N strategy is TTS approach in which model generates candidate outputs for given input and then selects the best one according to chosen evaluation metric (Wu et al., 2024c). Mathematically, given an input and model , one draws independent outputs y1, . . . , yN (x) (e.g., via different random seeds or sampling strategies) and chooses the result ˆy = arg maxN i=1 (yi), where is quality scoring function. At the cost of additional inference compute, increasing raises the probability of obtaining high-quality outcome (for example, if each attempt succeeds with probability p, then best-of-N run succeeds with probability 1 (1 p)N ). This technique leverages extra computation to boost performance (Kang et al., 2025) and has been applied in real-world settings ranging from complex reasoning and code generation with LLMs to enhancing image synthesis quality in diffusion models (Ma et al., 2025b). 35 B.2 Majority Voting B.2 Majority Voting Majority voting is fundamental ensemble strategy for TTS that aggregates multiple independent predictions to make final decision. In this approach, each model or inference (voter) casts vote for predicted outcome, and the output chosen is the one with the highest number of votes (i.e., the mode of the predictions). Formally, given an ensemble of models h1, h2, . . . , hM each producing prediction hm(x) for input x, the majority vote outcome is defined as ˆy = arg max (cid:88) m=1 1{ hm(x) = }, where 1{} is the indicator function and ranges over all possible classes or outputs. This test-time inference technique leverages additional computing at inference to improve reliability without retraining models, and it is widely used in real-world applications, such as combining votes of decision trees in random forest, consolidating crowd-sourced annotations, or enhancing the consistency of answers from LLMs by selecting the most frequent response. B.3 Process Reward Model Process Reward Model (PRM) (Uesato et al., 2022; Pfau et al., 2024) is reward model designed to evaluate an entire reasoning trajectory on step-by-step basis. Formally, given an input problem and sequence of reasoning steps z1, z2, . . . , zT leading to final output y, we can represent this full reasoning trace as: ST = (x, z1, z2, . . . , zT , y), and define the PRM as function that assigns real-valued score: : ST R, mapping possible reasoning process ST to reward score (Choudhury, 2025; Ma et al., 2025c). Intuitively, r(ST ) is higher when the reasoning process is logical, valid, and leads to correct solution, and lower (or negative) when the reasoning is flawed. PRMs are typically trained on human or algorithmic annotations for each step, internalizing notion of partial credit to evaluate correctness and relevance at each stage. PRMs play crucial role in TTS strategies such as stepwise beam search and self-consistency verification. They have been successfully applied in mathematical reasoning, code generation, automated theorem proving, and decision-making tasks. By leveraging PRMs, models can optimize not only for correctness but also for process coherence, making AI systems more transparent and robust. B.4 MCTS Monte Carlo Tree Search (MCTS) is simulation-based decision-making algorithm for sequential decision problems, often formalized as Markov Decision Process (MDP). It incrementally builds search tree by sampling many possible future trajectories (playouts) and using their outcomes to estimate the value of decisions. Unlike brute-force search, MCTS selectively explores the most promising actions by balancing exploration (trying unexplored or uncertain moves) and exploitation (favoring moves with high estimated reward). Each iteration of MCTS consists of four phases: 1. Selection Recursively select child actions that maximize heuristic value until reaching leaf node. common selection strategy is the Upper Confidence Bound for Trees (UCT): UCT(a) = wa na + (cid:114) ln na , where wa is the total simulation reward, na is the visit count for action a, is the total simulations from the parent state, and > 0 is an exploration constant. 2. Expansion Once leaf state is reached, new child nodes are created by simulating unexplored actions. 3. Simulation (Rollout) Perform Monte Carlo simulation by selecting actions to simulate full episode to the end, providing an estimate of the nodes value. 4. Backpropagation Propagate the simulation result back up the tree, updating the statistics of each node along the path. 36 B.5 Self-Refine MCTS is well-suited for TTS because its anytime nature allows flexible computation budgets. At test time, running MCTS for longer or with more rollouts leads to deeper search and better decisions. Notably, AlphaGo used MCTS at runtime to refine moves, significantly improving performance without additional training. Researchers are leveraging MCTS to enhance test-time reasoning in other AI domains. MCTS-Judge improves code correctness evaluation by systematically exploring reasoning paths, raising verification accuracy significantly. Similarly, hybrid approaches integrate MCTS into generative model inference for problem-solving, such as solving Sudoku puzzles through sequential search. By repeating these steps, MCTS concentrates simulations on the most promising branches. In the limit, MCTS value estimates converge to the optimal values in certain perfect-information games. B.5 Self-Refine Self-Refine (Madaan et al., 2023) is an advanced TTS technique that enables an LLM to iteratively improve its own outputs through self-generated feedback. Introduced by Madaan et al. (2023), the Self-Refine framework is inspired by how humans revise draft: The model first produces an initial answer, then critiques or evaluates that answer, and finally uses the critique to refine the answer. This feedback-refinement loop can be repeated multiple times, progressively polishing the output. Notably, Self-Refine requires no additional training data or fine-tuning the same pre-trained model acts as the initial answer generator, the feedback provider, and the refiner. For sufficiently powerful models, this self-iteration yields significantly better results, presumably because it is easier for model to identify and fix errors in given solution than to produce perfect solution in one attempt. In essence, Self-Refine leverages test-time compute to let the model think twice (or more) about its answer, leading to higher-quality and more reliable outputs. Formally, consider an input and language model Mθ with parameters θ, defining conditional distribution Pθ(y x) over possible outputs y. The Self-Refine procedure generates sequence of outputs y(0), y(1), . . . , y(T ) as follows: 1. Initial Output Generation: The model first produces an initial response: y(0) = Mθ(x). (4) 2. Feedback Generation: At each refinement step = 1, 2, . . . , , the model evaluates the previous output and generates feedback: (t) = Mθ (cid:0)x, y(t1); feedback-prompt(cid:1). 3. Refinement Step: Using the generated feedback, the model updates its output: y(t) = Mθ (cid:0)x, y(t1), (t); refine-prompt(cid:1). (5) (6) This feedback-refinement loop continues iteratively until stopping condition is met, such as reaching predefined number of iterations or detecting convergence in the output quality. The Self-Refine approach enhances model reliability by progressively improving its responses without requiring additional training. B.6 Tree-of-Thought Complex reasoning problems often require exploring different lines of thought before arriving at correct solution. CoT prompting was first step in this direction: CoT guides the model to produce single sequence of intermediate reasoning steps (a linear chain) leading to the answer. This improves the models performance on tasks requiring multi-step logic by breaking the problem into step-by-step narrative. However, CoT still follows single path if the model makes wrong turn in the reasoning chain, it cannot recover because it doesnt revisit earlier decisions. Tree-of-Thought (Yao et al., 2023b), by contrast, generalizes CoT to branching search. At each reasoning step, the model can generate multiple candidate thoughts instead of one, forming tree of possibilities. It evaluates these candidates (using heuristics or self-evaluation prompts) and selects the most promising branch(es) to continue expanding (Bi et al., 2025). This test-time exploration allows the model to consider alternative approaches and scale up inference computation as needed much like how human might try different reasoning avenues for hard problem. Researchers have categorized ToT and similar strategies (e.g., graph-of-thought) as X-of-Thought (XoT) reasoning methods, which significantly improve LLM reasoning by introducing iterative, structured inference without additional training. ToT can be modeled as search process through state space of partial solutions, where each state encodes the sequence of thoughts (intermediate steps) explored so far. Let be the set of all possible reasoning states for given problem. The initial state s0 contains the problem statement, and goal state represents complete solution. B.7 Reinforcement Learning Thought Generation (State Transitions): At each step, the language model serves as thought generator function G. Given the current state (context) s, the model generates set of next-step thoughts: G(s) {t1, t2, . . . , tb} (7) where each ti represents candidate next reasoning step. Each thought extends the current reasoning path, yielding new state: si = ti (8) where denotes concatenation of the thought to the sequence. State Evaluation (Heuristic Function): To guide the search, ToT uses an evaluation function (s) that estimates the quality of partial state s: : (9) This function may be implemented by the model itself using self-evaluation prompt or scoring heuristic. Search Algorithm (Tree Expansion): ToT can employ different search strategies, including: Breadth-First Search (BFS): Expands all plausible thoughts at each depth, keeping the top best states based on (s). Depth-First Search (DFS): Follows the most promising thought path deeply, backtracking if necessary. Each strategy allows ToT to control computational budgets by limiting depth (number of steps) and branching factor (number of candidates per step). Solution Extraction: state is considered valid solution if it satisfies the problem constraints. The search continues until: 1. goal state is reached. 2. The computational budget (depth or number of states evaluated) is exhausted. This framework formalizes ToT as an organized search over the space of reasoning sequences, allowing models to iteratively refine and explore multiple potential solutions during test-time inference. B.7 Reinforcement Learning Reinforcement learning can play pivotal role in unlocking effective TTS for language models. The process of inference itself can be formulated as sequential decision-making problem: at each step in generating solution, e.g., each token in reasoning chain or each attempt at an answer, the model (agent) must decide whether to continue reasoning, which direction to explore, or when to stop and output an answer. By training the model with RL, we can explicitly reward outcomes that lead to correct or high-quality answers, thereby encouraging the model to make better use of the extra inference steps available. This addresses key challenge in TTS : simply allowing model to think longer doesnt guarantee better answers unless the model knows how to productively use that extra time (it could otherwise repeat mistakes or terminate too early). RL provides feedback-driven way to learn such behaviors. In fact, prior approaches to improve reasoning in LLMs often relied solely on imitation learning (learning from observed human or AI reasoning traces), which can limit model to mimicking given patterns (Hou et al., 2025). By contrast, RL enables self-exploration: the model can try diverse reasoning paths and learn from trial-and-error which strategies yield the highest reward (for example, reaching correct solution). This means an RL-trained language model can learn dynamic inference policiessuch as when to double-check an intermediate result or how to backtrack and correct itself if the reasoning seems to be going astray. Recent research indeed shows that combining chain-of-thought reasoning with reinforcement learning techniques leads to improved inference-time performance."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong",
        "City University of Hong Kong",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Macquarie University",
        "McGill University & MILA",
        "Salesforce AI Research"
    ]
}