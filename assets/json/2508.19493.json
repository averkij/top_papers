{
    "paper_title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents",
    "authors": [
        "Zhixin Lin",
        "Jungang Li",
        "Shidong Pan",
        "Yibo Shi",
        "Yue Yao",
        "Dongliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPA-Bench."
        },
        {
            "title": "Start",
            "content": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents Zhixin Lin1, Jungang Li2,3, Shidong Pan4, Yibo Shi5, Yue Yao1,, Dongliang Xu1, 1Shandong University 2Hong Kong University of Science and Technology (Guangzhou) 3Hong Kong University of Science and Technology 4Columbia University 5Xian Jiaotong University 5 2 0 2 7 2 ] . [ 1 3 9 4 9 1 . 8 0 5 2 : r Abstract Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users personal information during this operation. To gain thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utilityprivacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPABench. Introduction With the rapid advancement of multimodal large language models (MLLMs) (Bai et al. 2025b; Zhu et al. 2025; Xun et al. 2025; Dang et al. 2024) and smartphone agents (Jiang et al. 2025; Ma, Zhang, and Zhao 2024; Dai et al. 2025; Wang et al. 2025), users increasingly rely on intelligent assistants to automate routine tasks such as sending messages, ordering takeout, and online shopping. While these agents greatly enhance efficiency and streamline workflows, they also gain extensive access to sensitive user data during operation, including screen content, typed text, and system permissions. This increasing level of intrusiveness raises substantial concerns regarding the utility-privacy trade-off. Existing evaluations mainly focus on the capability of agents, employing metrics such as task completion rate (Xu Corresponding authors. Figure 1: Motivation of SAPA-Bench. Left: Current agents often execute sensitive commands (e.g., entering PIN) without privacy checks, posing risks. Middle: We redefine this process by introducing privacy-aware module that detects sensitive input, warns the user, and proceeds only upon confirmation shown in Right. et al. 2025), interaction latency (Wang et al. 2024b), or resource consumption (Deng et al. 2024; Dai et al. 2025), but they lack systematic, quantitative assessment of the privacy awareness of agents. Benchmarks such as Android-inthe-Wild (Rawles et al. 2023) and GUI Odyssey (Lu et al. 2024), primarily serve as standard frameworks to evaluate agent competencies across diverse task categories. However, in practice, users also care whether agents can accurately identify and properly handle privacy-sensitive content, such as location data, account credentials, or call logs. Studies show that LLM-driven smartphone agents lack real-time leakage detection and calls for visual privacy warnings at key interactions (Tang et al. 2025). Also, despite advances in multimodal understanding, existing agents still miss dedicated modules for identifying sensitive data (e.g., location, contacts) or requesting user confirmation (Liu et al. 2025). As shown in Figure 1, the absence of unified benchmark and dedicated metrics makes it difficult to compare the privacy awareness of agents and obstructs privacy-driven agent design. To address this gap, we introduce SAPA-Bench, the firstever large-scale benchmark specifically designed to evaluate the privacy awareness of smartphone agents. SAPA-Bench comprises 7,138 real-world scenarios, and each scenario is annotated for privacy presence, leakage modality (image or instruction), privacy category, risk severity, and the expected risk prompt. Building on this dataset, we define five specialized evaluation metrics-Privacy Recognition Rate (PRR), Privacy Localization Rate (PLR), Privacy Level Awareness Rate (PLAR), Privacy Category Awareness Rate (PCAR), and Risk Awareness (RA) to quantify an agents capabilities in privacy recognition, localization, classification, severity estimation, and risk response, respectively. We conduct comparative evaluation of seven mainstream representative smartphone agents, including those driven by open-source and closed-source models. Our results reveal that most existing agents perform poorly in privacy awareness, with performance remaining below 60% even with implicit hints; overall, closed-source models slightly outperform open-source ones, and there exists notable positive correlation between models privacy sensitivity and scenario sensitivity. Furthermore, our results indicate that augmenting inputs with targeted prompt signals substantially improves the ability of the models to detect sensitive content to privacy. The main contributions of this work are: We construct SAPA-Bench, dedicated benchmark for privacy-aware smartphone agents that unlike prior security benchmarks such as MobileSafetyBench(Lee et al. 2024) covers the full privacy perception pipeline: recognition, localization, classification, severity estimation, and risk warning evaluation. We propose five specialized privacy metrics (PRR, PLR, PLAR, PCAR, RA), enabling the first quantitative, evaluation of agents privacy understanding and response capabilities. We evaluate mainstream smartphone agents to reveal key privacy awareness bottlenecks and highlight trade-offs between performance and privacy, show that models with greater scenario sensitivity detect privacy more effectively, and demonstrate that adding targeted prompt hints can improve detection while maintaining usability. We envision that SAPA-Bench will serve as an extensible, privacy-focused evaluation platform, guiding the community toward smarter, safer smartphone agents that strike an optimal balance between functionality and privacy protection."
        },
        {
            "title": "Related Work",
            "content": "Smartphone Agent powered by MLLM Existing mainstream research on mobile agents are mainly powered by MLLMs (Liu et al. 2025; Wu et al. 2024a). To better adapt to diverse tasks such as UI parsing, multi-step action planning, or cross-app reasoning, these systems often dynamically switch or fine-tune different MLLM backbones (e.g., GPT-4o, Gemini, or customized vision-language models). MLLM enables mobile agents to jointly understand and reason over both visual (e.g., UI screenshots) and textual (e.g., instructions) inputs, allowing for more flexible, generalizable, and human-aligned interaction. Specifically, early systems like AppAgent (Li et al. 2024b) pioneered two-phase explorationdeployment pipeline: during exploration it passively observes UI elements to build knowledge base. Mobile-Agent (Wang et al. 2024a) followed with fully vision-driven framework that uses only screenshots as input, achieving high precision multi-step operations and introducing the Mobile-Eval benchmark. Subsequent methods Show-UI (Lin et al. 2025) with visual-token selection and streaming inference, and SpiritSight Agent (Huang et al. 2025) with universal block parsing further improved UI localization and cross-platform understanding efficiency. However, none of these single-agent approaches incorporates mechanisms for detecting privacy-sensitive operations or issuing risk warnings. While these frameworks significantly advance task success rates and robustness, they commonly neglect to add modules to particularly response to potential privacy risks. Existing Privacy Evaluation Frameworks Existing standards and guidelines offer general frameworks for privacy impact assessment, but they have not been directly adapted to smartphone agents. One study (Iwaya et al. 2024) surveyed privacy impact assessment (PIA) methodologies and emphasized the need to cover spectrum of risks from low to high in real-world settings. Similarly, another paper (Sangaroonsilp et al. 2023) introduced three-tier taxonomy (high/medium/low) for classifying privacy requirements in issue reports, providing reference for multi-level risk assessment. Such multi-level privacy annotations could serve as valuable standard for systematically evaluating and benchmarking the privacy awareness of smartphone agents. Motivation Existing benchmarks, such as SPA-bench (Chen et al. 2024a) and GUI-odyssey (Lu et al. 2024), have attempted to diversify task types, increase task volume and complexity, and introduce more sophisticated scenarios to challenge the problem-solving capabilities of smartphone agents. As security and privacy becomes increasing concern when using smartphone agents, SIUO (Wang et al. 2024c) and related works concentrate on hazardous behaviors, criminal activities, and other security domains. Other benchmarks, such as MobileSafetyBench (Lee et al. 2024), focus on behavioral safety in benign versus harmful tasks, penalizing agents for overstepping predefined boundaries. However, existing benchmarks overlook crucial issue: When the model fails to recognize that an operation involves personal privacy information, no notice is raised. The Fair Information Practice Principles (FIPP), first introduced by the U.S. Department of Health, Education, and Welfare in 1973, emphasize transparency through user notification and informed choice (Pan et al. 2024). These principles later evolved into the notice-and-choice privacy framework, forming the conceptual basis of contemporary privacy regulations, such as the European General Data Protection Regulation (GDPR). In both common deployment architectures, i.e., on-device agents and cloud-based end-to-end agents, sensitive actions (e.g., reading password from the clipboard or uploading users contact list) execute automatically without prompting users, thus deny users of the opportunity to potentially intervene. When confronted with privacy-related requests, an agent must not only actions. Specifically, high-sensitivity content refers to operations involving rich personal information, such as chat messages containing ID numbers or account credentials, precise location sharing, device-level permission grants, or entry of financial credentials. Medium-sensitivity content covers moderately private data commonly encountered in daily life, such as coarse location sharing, meeting links, or casual chat messages. In contrast, low-sensitivity content includes routine behavioral traces like browsing history, cart additions, viewing task status, or sharing public content, which rarely reveal identifiable personal data. Benchmark Characteristics Our benchmark, namely as SAPA-Bench (Smartphone Agent Privacy Awareness Benchmark), comprises suite of 7,138 scenarios drawn from 50 popular English-language smartphone applications, e.g., Instagram and Google map, from various categories. Each scenario is based on UI Screenshot, which is snapshot of the interface where privacy leak may occur. For the sake of consistency and comparativeness, all UI screenshots are sampled from the GUIOdyssey (Lu et al. 2024) and OS-Atlas (Wu et al. 2024b) task suites. To comprehensively evaluate the agents capabilities on privacy awareness, for each scenario, we further annotate the following privacy-related information: Instruction & Response: Instruction: the users prompt to describe the operation (e.g., Enter password P@sswOrd to log in). Response: the models expected response (e.g., This action may send your message history and leak private information. Are you sure you want to proceed?). Exposure Modality & Location: whether the privacy exposure occurs in the screenshot or in the instruction. Privacy Sensitivity Level: three tiersLow, Medium, and Highreflecting the degree of sensitivity. Privacy Type: eight categories (e.g., Account Credentials, Financial and Payment etc.). Dataset Statistics. SAPA-Bench comprises 7,138 privacy-sensitive tasks across eight main categories and 27 sub-categories, two privacy exposure modality, three sensitivity levels, and thousands tailored instruction & response pairs. Figure 2 shows the number of sub-category per category broken down by risk level (Low, Medium, High) and cases in SAPA-Bench. The Personal Information and Communication Content categories exhibit the largest High-Risk proportions, each exceeding 40% of their totals, whereas Device Permissions and Operation and Financial and Payment are dominated by Medium-Risk or Low-Risk tasks. This uneven distribution mirrors real-world differences in privacy sensitivity: for example, entering account credentials or pasting an email address is inherently high-risk, while browsing history or setting behavioral preferences generally poses lower or medium risk. Thus, SAPA-Bench functions as multi-level, multidimensional benchmark on evaluating GUI agents ability to warn and protect users across varying privacy sensitivity contexts. Figure 2: Five-stage annotation pipeline of SAPA-Bench. GPT-4o and human annotators collaboratively label privacysensitive ground truth. Specifically, stage 0 cleans and samples raw screenshots; stages 1 and 3 automatically generate privacy-sensitive ground truth; stages 2 and 4 conduct human verification. recognize the private nature and sensitivity of the content but also proactively alert users before execution; only then can agent be deemed to possess robust privacy-handling capabilities. To address this gap, we propose the SAPA-Bench, designed to systematically evaluate agents on comprehension and handling of privacy-sensitive operations. Specifically, an agent should accurately identify privacy-related requests and provide appropriate mitigation strategies. For example, when user asks the agent to paste password from the clipboard into an applications login field, the agent should recognize this as privacy-sensitive operation rather than boldly executing the command. Simultaneously, the agent should prompt the user: This operation may cause unnecessary consequences or troubles; for security and privacy reasons, do you wish to proceed? Achieving this requires deep understanding of contextual scenarios and nuanced privacy semantics. We specifically introduce the SAPA-Bench and its construction in the subsequent section. Smartphone Agent Privacy Awareness: SAPA-Bench Privacy in Smartphone Inspired by large-scale user perspective on mobile app privacy (Nema et al. 2022) and the structured privacy taxonomies adopted in Apple and Googles official privacy label frameworks (Ali et al. 2024; Khandelwal et al. 2023), we classify privacy leakage into eight categories by jointly considering the operation type and the app category including: 1. Account Credentials (AC), 2. Personal Information (PI), 3. Financial and Payment (FP), 4. Communication Content (CC), 5. Location and Environment (LE), 6. Device Permissions (DPO), 7. Media and Files (MF), 8. Behavior and Browsing (BPBH). Based on previous studies (Chen et al. 2024a; Li et al. 2024a), we further categorize these eight categories into three privacy-sensitivity levels from low, medium, and high, to enable fine-grained evaluation on agents. This stratification is grounded in the degree to which an action may expose sensitive user information in real-world mobile interFigure 3: Overview of SAPA-Bench, illustrating its task taxonomy, representative examples, and risk-level distribution. (a) Hierarchical classification of privacy-leakage tasks, organized into eight top-level categories and their corresponding subtasks. (b) Example cases from categories Account Credentials (high risk) and Personal Information (mid risk), each with system prompt and expected agent warning. (c) Task counts by risk level (High/Mid/Low) across the eight privacy categories (represented by their initials), showing real-world imbalanced distribution of sensitivity. Annotation Pipeline We combine MLLM-driven automatic generation with rigorous human verification in five stages: Stage 0: Data Cleaning & Sampling. We apply GPT-4o to the raw GUI-Odyssey and OS-Atlas corpora ( 80, 000 screenshots) to automatically filter those likely to contain privacy-sensitive content. From the filtered set, we then randomly sample 400 screenshots for quick manual spotchecking to validate filter precision. We divide manual annotation into two parts: first, we verify the accuracy and consistency of the InstructionResponse pairs to ensure clear, mistake-free dialog content; then, using those validated dialogs along with the original screenshots, we structurally annotate the remaining fields (e.g., sensitivity level, leakage location, privacy category). This two-part approach reduces cognitive load and improves annotation quality and consistency. Stage 1: Automatic Generation of Instruction & Response. We leverage GPT-4o model to automatically generate privacy-sensitive instruction and corresponding response for each example. The prompts are constructed to simulate realistic user intents that may trigger privacyrelated concerns.A template of the prompt format is provided in Appendix. Combined with Stage 0 (data filtering and sampling), this stage took approximately 40 humanhours to complete, including generation, basic validation, and API processing time. Stage 2: Initial Human Review. An initial human review is conducted on every InstructionResponse pair by four graduate and three undergraduate annotators trained in privacy annotation. Annotators verify that instructions are concise, unambiguous, and signaled potential privacy risk, and that responses conform to the standardized warning template This action may result in [privacy leakage type]. Please confirm before proceeding. Only pairs passing this quality check are advanced to the next stage. Stage 3: Automatic Annotation of Remaining Fields. In this stage, we employ GPT-4o to complete the remaining fields in the ground-truth structure, including the privacy leakage position, privacy sensitivity level, and privacy category. For each sample, the model is prompted with the previously verified InstructionResponse pair, and asked to infer the additional fields in structured output format. The generation process is single-pass and fully automated. We also check that each annotation followed consistent format. To complete all annotation, this stage takes approximately 10 hours in total. All outputs from this step are forwarded to Stage 4 for the final human verification. Stage 4: Final Human Verification. To ensure the consistency and accuracy of the automatically generated annotations, we conduct final round of human verification. This process is carried out by seven trained annotators (three undergraduate students and four graduate students from STEM background), all of whom have participated in prior stages and received annotation training. The primary focus of this stage is to verify the correctness of three critical fields. After the initial pass, we adopt cross-validation strategy that each sample is independently reviewed by two annotators. sample is only considered verified if both reviewers agreed with no objections. Each annotator spends approximately 4 hours per review round, resulting in total of 50 hours dedicated to this stage. This rigorous two-pass review process Model #Size SR PRR PLR Image Instruction Overall PLAR PCAR RA(EH) Score 2B 8B Smartphone Agent Show-UI SpiritSight Agent General Vision-Language Model Qwen2.5-VL InternVL 2.5 LLaVA-NeXT Close-source Model Gemini 2.0-flash GPT-4o 7B 8B 7B 25.71% 34.17% 29.68% 43.00% 32.75% 29.04% 17.51% 28.39% 27.00% 25.29% 35.79% 29.56% 35.95% 79.72% 10.59% 52.37% 41.03% 10.16% 4.33% 33.87% 15.23% 11.78% 38.70% 23.12% 3.43% 16.63% 25.06% 5.74% 4.03% 16.50% 11.38% 19.68% 13.61% 13.90% 2.58% 18.77 27.25 40.23 51.66 36.94 48.12% 75.62% 18.96% 29.16% 31.64% 80.16% 74.42% 15.85% 24.06% 26.45% 35.08% 67.14 45.14% 31.66% 27.78% 55.03 Table 1: Evaluation results for each model: PRR, PLR, PLAR, PCAR, and RA measure the models privacy capabilities on SAPA-Bench, while SR assesses their task completion performance. Figure 4: Visualization of performance across six evaluation metrics (PRR, SR, PCAR, PLAR, RA, PLR) for each evaluated model. results in high-quality and reliable annotation labels across all 7,138 samples."
        },
        {
            "title": "Experiments",
            "content": "Experiment setting All experiments are conducted on our proposed SAPABench dataset. Each sample consists of multimodal inputs (i.e., instruction and screenshot) with fine-grained privacy annotations, including whether privacy is involved, the modality of the privacy exposure (screenshot or instruction), the category and severity level of the privacy content, and the expected response. This setup enables comprehensive evaluation across multiple privacy understanding dimensions. In addition to our five privacy-oriented metrics (PRR, PLR, PLAR, PCAR, and RA), we also report the Success Rate (SR) on the GUI-Odyssey benchmark to investigate how the agents handle the privacy-utility trade-off. By analyzing the interplay between privacy awareness and task execution success, we gain critical insights into potential trade-offs and their implications for agent design and deployment. Most of existing smartphone agents utilize mainstream Figure 5: Scatter plot of SR (%) versus RA for each evaluated model, illustrating how different agents trade off task completion performance against privacy-sensitive response capability. MLLMs as their backbone architectures. Thus, evaluating these foundational models directly allows us to infer the capabilities and limitations of broader range of smartphone agents. To this end, we evaluate three representative categories of agents/models: (1) Smartphone agent, including SpiritSight Agent (Huang et al. 2025), Show-UI (Lin et al. 2025); (2) Generalist Vision-Language Models, including Qwen2.5-VL (Bai et al. 2025a), InternVL 1.5 (Chen et al. 2024b) and LLaVA-NeXT (Liu et al. 2024); and (3) Closedsource Models, including Gemini 2.0-flash (Reid et al. 2024) and GPT-4o (Hurst et al. 2024). To ensure consistency in parameter scale, all open-source models are limited to the 7B8B range, except Show-UI(2B). Model deployment and inference are carried out on server equipped with 8NVIDIA RTX 3090 GPUs. Detailed hyperparameter settings are provided in the Appendix. Mid High Overall RA(NH) RA(IH) RA(EH) Overall Low Model Smartphone Agent Show-UI SpiritSight Agent General Vision-Language Model Qwen2.5-VL InternVL 2.5 LLaVA-NeXT Close-source Model Gemini 2.0-flash GPT-4o 31.52% 34.57% 36.42% 34.17% 34.16% 33.03% 31.05% 32.75% 17.16% 29.91% 38.11% 28.39% 24.26% 37.58% 45.54% 35.79% 78.30% 80.86% 80.00% 79.72% 55.94% 80.12% 90.81% 75.62% 67.66% 83.62% 89.19% 80.16% 15.59 21.63 Model Smartphone Agent Show-UI SpiritSight Agent General Vision-Language Model Qwen2.5-VL InternVL 2.5 LLaVA-NeXT Close-source Model Gemini 2.0-flash GPT-4o 11.75 14.88 16.74 18.77 15. 23.69 27.76 22.67 28.70 36.83 27.37 29.40 18.77 27.25 40.23 51.66 36.94 67.14 55. 19.35 25.55 24.88 31.75 30.17 37.76 33.34 Table 2: Detailed PRR evaluation results across three sensitivity levels (Low, Mid, High) and the overall average for each models. Table 3: Detailed RA evaluation results across three prompting conditions: No Hint (NH), Implicit Hint (IH), and Explicit Hint (EH), as well as the overall average. Evaluation Matrics Conventional evaluation metrics commonly use in classification tasks, such as Accuracy and F1-score, failing to capture the multi-dimensional requirements of privacy understanding in this work. Beyond identifying the presence of privacy-related content, our task further requires agents to localize which modality (screenshot or instruction) contains privacy exposure information, determine the specific privacy category, determine its severity level, and generate appropriate responses with privacy awareness. To facilitate these needs, we propose five privacy-oriented evaluation metrics that collectively assess agents ability to perceive and respond to privacy-sensitive content: Privacy Recognition Rate (PRR) reflects the proportion of all samples that the agent flags as privacy-related. Privacy Localization Rate (PLR) measures, among those samples the agent identifies as privacy-related, how often it correctly pinpoints the locationscreen or instruction. Privacy Level Awareness Rate (PLAR) evaluates whether, once sample is marked private, the model assigns it to the correct risk tier (Low, Medium, or High). Privacy Category Awareness Rate (PCAR) assesses how accurately the agent identifies the category of privacysensitive information (e.g., Account Credentials). Risk Awareness (RA) denotes the fraction that the agent produces reasonable, risk-aware response for the privacy-related scenarios. In addition to these privacy-oriented metrics, we also report the Success Rate (SR) on the GUI-Odyssey benchmark to explore how privacy handling correlates with the overall task completion capability. For PRR, PLR, PLAR, and PCAR, results are compared against human-annotated ground-truth labels. For RA, which involves natural language outputs, we employ an LLM to perform semantic alignment between the agents response and reference risk prompt. Details of the scoring procedure are provided in the appendix. Results and Discussion The benchmarked smartphone agents demonstrate relatively poor performance in safeguarding sensitive user information, revealing insufficient privacy awareness in practice. To comprehensively evaluate the privacy understanding capabilities of different agents, we report quantitative results across all proposed metrics in Table 1. This table includes agents from three distinct categories: Smartphone Agent, General Vision-Language Model, and commercial closed-source model. Each metric reflects key aspect, including recognition (PRR), localization (PLR), severity awareness (PLAR), category classification (PCAR), response quality (RA) and task completion(SR). This evaluation setup enables multi-perspective comparison of models capabilities in identifying, interpreting, and responding to privacy-sensitive content. Furthermore, Figure 4 visualizes each models six-metric profile as radar chart, making it easy to spot strengths and weaknesses at glance. Additionally, Figure 5 reveals the relationship between each models SR and RA, showing how privacy handling correlates with overall task completion. As shown in Table 1, the experimental results reveal that contemporary smartphone agents exhibit markedly inadequate privacy safeguards. First, PRR for all tested models fall below 85%, with open-source systems such as Show- (34.17%), SpiritSight Agent (32.75%), Qwen2.5-VL UI (28.39%), and InternVL2.5 (35.79%) languishing around the 30% markindicating that the vast majority of sensitive scenarios go undetected. Second, PLR is likewise poor: even GPT-4o, powerful MLLM, correctly attributes privacy exposure to the instruction stream only 74.42% of the time, while most models score under 30% in both the image and instruction modalities. Third, the agents show almost no fine-grained sensitivity: PLAR and PCAR hover in the single to low-double-digit range (535%), demonstrating an inability either to distinguish risk severity or to classify leak types (e.g., location, identity, credentials). Finally, RA scores remain severely constrained Gemini 2.0-flash, the best of the lot, achieves only 67.14, while open-source models score substantially lower (Show-UI 18.77; SpiritSight Agent 27.25; Qwen2.5-VL 40.23) showing that even when model detects privacy threat, it cannot generate sufficiently effective mitigation prompts. Collectively, these findings underscore pronounced gap in current smartphone agent capabilities: robust, specialized privacy training, tighter alignment strategies, and dedicated evaluation benchmarks are urgently needed to elevate practical privacy protection. Compared with open-source model, closed-source model dominates privacy awareness capability. As similar with other tasks, closed-source models consistently outperform their open-source counterparts across all privacy-oriented metrics. Specifically, in Table 1, in terms of PRR, Gemini 2.0-flash and GPT-4o achieve approximately 7580%, outpacing open-source models by over ten percent. On other measures, GPT-4o attains PLAR of 31.66% and PCAR of 27.78%, whereas open-source systems rarely exceed 20%. This demonstrates that the closed-source agents not only detect the presence of sensitive content more reliably, but also more accurately assess its severity and type. Finally, in terms of RA matrices, GPT-4o and Gemini score 55.03% and 66.14%, respectively higher than the best open-source model, InternVL2.5, at 51.66%. We attribute this superiority chiefly to extensive Reinforcement Learning from Human Feedback(RLHF) (Hurst et al. 2024; Reid et al. 2024) based fine-tuning on large, high-quality datasets and rigorous internal safety alignment, whereas open-source models remain primarily optimized for general functionality without specialized privacy calibration. Dataset bias may result in model bias in privacy protection. Despite its strong performance on standard multimodal benchmarks such as Optical Character Recognition (OCR) and Visual Question Answering (VQA) (Bai et al. 2025a), Qwen2.5-VL exhibits pronounced baseline gap on our privacy-sensitive dataset: its privacy-recognition and risk-awareness metrics fall markedly below those of its pretraining its open-source peers. We propose that and instruction-tuning corpora lack sufficient exposure to privacy-critical scenarios and thus fail to generalize to real-world privacy detection tasks. In contrast, InternVL2.5 and LLaVA-NeXT demonstrate substantially more robust privacy perception. InternVL2.5 incorporates extensive real world interaction exemplars during its multimodal alignment phase and leverages Chain-of-Thought prompting to sharpen its identification and annotation of sensitive content (Zhu et al. 2025), while LLaVA-NeXT combines systematic harmful-content filtering with CoT training strategies to heighten its sensitivity to risk cues (Liu et al. 2024). These targeted data curation and alignment strategies enable both models to detect, localize, and classify privacy-leakage scenarios with better reliability. As the level of privacy sensitivity decreases, the agents ability to detect privacy-sensitive content correspondingly deteriorates. We analyzed the data by privacy level. From table 2, we observe that PRR increase systematically with sensitivity level. In low-sensitivity scenarios, opensource models such as Show-UI, Qwen2.5-VL, and InternVL2.5 attain only 31.52%, 17.16%, and 24.26%, respectively, while even the closed-source models GPT-4o and Gemini achieve merely 67.66% and 59.94%, indicating that the vast majority of low-risk operations remain undetected. Under medium sensitivity, PRR rises by approximately ten percentage points for most agents, demonstrating that Privacy content that stands out more is easier to detect. Although in high-sensitivity conditions LLaVA-NeXT, GPT-4o, and Gemini reach 80%, 89%, and 91% , these figFigure 6: Each models Risk Awareness (RA) under three prompting conditionsNo Hint, Implicit Hint, and Explicit Hint, illustrating how the level of prompt detail affects agents risk-response performance. ures still fall short of deployment grade reliability thresholds and performance at low and medium sensitivity levels remains uniformly inadequate. Collectively, these results expose pronounced deficiency in current smartphone agents privacy detection capabilities across all sensitivity levels, so theres an urgent need to improve them with specialized training data and more precise tuning strategies. Employing more salient prompt cues can effectively enhance the agents RA capabilities. We evaluated RA across three prompting conditions no hint, implicit hint, and explicit hintand observed marked differences in model performance. As shown in table 3, We can conclude that under the no-hint condition, all agents achieved low RA scores. Introducing implicit hints yielded consistent 515 percentage-point upliftfor example, Qwen2.5-VL improved from 11.75% to 22.67% and InternVL2.5 from 14.88% to 28.70%demonstrating that even subtle cueing can activate latent risk-sensitivity. As shown in Figure 6, with explicit prompting, RA reached its peak: GPT-4o rose to 55.03% and Gemini to 67.14%. These results underscore that prompts with explicite hints can substantially enhance multimodal agents risk-response capability, highlighting the critical importance of designing and embedding appropriate prompt frameworks for secure deployment. Takeaways: There are substantial limitations in current smartphone agents privacy-awareness capabilities, particularly in open-source models, emphasizing the necessity for specialized privacy-focused training and evaluation. Integrating carefully designed prompts can effectively improve the privacy awareness. Conclusion In this work, we present SAPA-Bench, the first large-scale benchmark for evaluating privacy awareness in smartphone agents, comprising 7,138 real-world scenarios and five dedicated metrics. Our experiments reveal that both openand closed-source agents struggle to reliably detect, localize, and classify privacy risks particularly in low and medium sensitivity settings. Through SAPA-Bench, we advocate for enhanced privacy-awareness capabilities in smartphone agents, emphasizing that the pursuit of efficiency and accuracy must not compromise essential user privacy protections. Acknowledgment This work was supported in part by the Key Research and Development Program of Shandong Province under Grant No. 2025CXGC010901. References Ali, M. M.; Balash, D. G.; Kodwani, M.; Kanich, C.; and Aviv, A. J. 2024. Honesty is the Best Policy: On the Accuracy of Apple Privacy Labels Compared to Apps Privacy Policies. arXiv:2306.17063. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; Zhong, H.; Zhu, Y.; Yang, M.-H.; Li, Z.; Wan, J.; Wang, P.; Ding, W.; Fu, Z.; Xu, Y.; Ye, J.; Zhang, X.; Xie, T.; Cheng, Z.; Zhang, H.; Yang, Z.; Xu, H.; and Lin, J. 2025a. Qwen2.5-VL Technical Report. CoRR, abs/2502.13923. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025b. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Chen, J.; Yuen, D.; Xie, B.; Yang, Y.; Chen, G.; Wu, Z.; Yixing, L.; Zhou, X.; Liu, W.; Wang, S.; et al. 2024a. Spabench: comprehensive benchmark for smartphone agent In NeurIPS 2024 Workshop on Open-World evaluation. Agents. Chen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong, W.; Hu, K.; Luo, J.; Ma, Z.; Ma, J.; Wang, J.; Dong, X.; Yan, H.; Guo, H.; He, C.; Shi, B.; Jin, Z.; Xu, C.; Wang, B.; Wei, X.; Li, W.; Zhang, W.; Zhang, B.; Cai, P.; Wen, L.; Yan, X.; Dou, M.; Lu, L.; Zhu, X.; Lu, T.; Lin, D.; Qiao, Y.; Dai, J.; and Wang, W. 2024b. How far are we to GPT-4V? Closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12). Dai, G.; Jiang, S.; Cao, T.; Li, Y.; Yang, Y.; Tan, R.; Li, M.; and Qiu, L. 2025. Advancing Mobile GUI Agents: Verifier-Driven Approach to Practical Deployment. Dang, Y.; Gao, M.; Yan, Y.; Zou, X.; Gu, Y.; Liu, A.; and Hu, X. 2024. Exploring response uncertainty in mllms: An empirical evaluation under misleading scenarios. arXiv preprint arXiv:2411.02708. Deng, S.; Xu, W.; Sun, H.; Liu, W.; Tan, T.; Liu, J.; Li, A.; Luan, J.; Wang, B.; Yan, R.; and Shang, S. 2024. MobileBench: An Evaluation Benchmark for LLM-based Mobile In Proceedings of the 62nd Annual Meeting of Agents. the Association for Computational Linguistics (Long Papers) (ACL 2024), 88138831. Association for Computational Linguistics. Huang, Z.; Cheng, Z.; Pan, J.; Hou, Z.; and Zhan, M. 2025. Spiritsight agent: Advanced gui agent with one look. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2949029500. Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Iwaya, L. H.; Alaqra, A. S.; Hansen, M.; and FischerHübner, S. 2024. Privacy Impact Assessments in the Wild: Scoping Review. CoRR, abs/2402.11193. Jiang, W.; Zhuang, Y.; Song, C.; Yang, X.; Zhou, J. T.; and Zhang, C. 2025. AppAgentX: Evolving GUI Agents as Proficient Smartphone Users. Khandelwal, R.; Nayak, A.; Chung, P.; and Fawaz, K. 2023. Comparing privacy labels of applications in android and In Proceedings of the 22nd Workshop on Privacy in iOS. the Electronic Society, 6173. Lee, J.; Hahm, D.; Choi, J. S.; Knox, W. B.; and Lee, K. 2024. Mobilesafetybench: Evaluating safety of autonomous agents in mobile device control. arXiv preprint arXiv:2410.17520. Li, Q.; Hong, J.; Xie, C.; Tan, J.; Xin, R.; Hou, J.; Yin, X.; Wang, Z.; Hendrycks, D.; Wang, Z.; et al. 2024a. Llmpbe: Assessing data privacy in large language models. arXiv preprint arXiv:2408.12787. Li, Y.; Zhang, C.; Yang, W.; Fu, B.; Cheng, P.; Chen, X.; Chen, L.; and Wei, Y. 2024b. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824. Lin, K. Q.; Li, L.; Gao, D.; Yang, Z.; Wu, S.; Bai, Z.; Lei, S. W.; Wang, L.; and Shou, M. Z. 2025. Showui: One visionlanguage-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1949819508. Liu, G.; Zhao, P.; Liu, L.; Guo, Y.; Xiao, H.; Lin, W.; Chai, Y.; Han, Y.; Ren, S.; Wang, H.; Liang, X.; Wang, W.; Wu, T.; Li, L.; Wang, H.; Xiong, G.; Liu, Y.; and Li, H. 2025. LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects. CoRR, abs/2504.19838. Liu, H.; Li, C.; Li, Y.; Li, B.; Zhang, Y.; Shen, S.; and Lee, Y. J. 2024. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. Lu, Q.; Shao, W.; Liu, Z.; Meng, F.; Li, B.; Chen, B.; Huang, S.; Zhang, K.; Qiao, Y.; and Luo, P. 2024. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451. Ma, X.; Zhang, Z.; and Zhao, H. 2024. CoCo-Agent: Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation. Nema, P.; Anthonysamy, P.; Taft, N.; and Peddinti, S. T. 2022. Analyzing user perspectives on mobile app privacy at scale. In Proceedings of the 44th international conference on software engineering, 112124. Pan, S.; Tao, Z.; Hoang, T.; Zhang, D.; Li, T.; Xing, Z.; Xu, X.; Staples, M.; Rakotoarivelo, T.; and Lo, D. 2024. NEW HOPE: Contextual Privacy Policies for Mobile Applications and An Approach Toward Automated Generation. In 33rd USENIX Security Symposium (USENIX Security 24), 5699 5716. Philadelphia, PA: USENIX Association. ISBN 9781-939133-44-1. Rawles, C.; Li, A.; Rodriguez, D.; Riva, O.; and Lillicrap, T. 2023. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36: 5970859728. Reid, M.; Savinov, N.; Teplyashin, D.; et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530. Sangaroonsilp, P.; Dam, H. K.; Choetkiertikul, M.; Ragkhitwetsagul, C.; and Ghose, A. 2023. Taxonomy for Mining and Classifying Privacy Requirements in Issue Reports. Information and Software Technology, 157: 107162. Tang, F.; Xu, H.; Zhang, H.; Chen, S.; Wu, X.; Shen, Y.; Zhang, W.; Hou, G.; Tan, Z.; Yan, Y.; Song, K.; Shao, J.; Lu, W.; Xiao, J.; and Zhuang, Y. 2025. Survey on (M)LLMBased GUI Agents. CoRR, abs/2504.13865. Wang, J.; Xu, H.; Ye, J.; Yan, M.; Shen, W.; Zhang, J.; Huang, F.; and Sang, J. 2024a. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158. Wang, J.; Xu, H.; Zhang, X.; Yan, M.; Zhang, J.; Huang, F.; and Sang, J. 2025. Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration. CoRR, abs/2502.17110. Wang, L.; Deng, Y.; Zha, Y.; Mao, G.; Wang, Q.; Min, T.; Chen, W.; and Chen, S. 2024b. MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents. CoRR, abs/2406.08184. Wang, S.; Ye, X.; Cheng, Q.; Duan, J.; Li, S.; Fu, J.; Qiu, X.; and Huang, X. 2024c. Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Model. arXiv preprint arXiv:2406.15279. Wu, B.; Li, Y.; Fang, M.; Song, Z.; Zhang, Z.; Wei, Y.; and Chen, L. 2024a. Foundations and Recent Trends in Multimodal Mobile Agents: Survey. CoRR, abs/2411.02006. Wu, Z.; Wu, Z.; Xu, F.; Wang, Y.; Sun, Q.; Jia, C.; Cheng, K.; Ding, Z.; Chen, L.; Liang, P. P.; and Qiao, Y. 2024b. OSATLAS: Foundation Action Model for Generalist GUI Agents. CoRR, abs/2410.23218. Xu, Y.; Liu, X.; Sun, X.; Cheng, S.; Yu, H.; Lai, H.; Zhang, S.; Zhang, D.; Tang, J.; and Dong, Y. 2025. AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Long Papers) (ACL 2025), 21442166. Association for Computational Linguistics. Xun, S.; Tao, S.; Li, J.; Shi, Y.; Lin, Z.; Zhu, Z.; Yan, Y.; Li, H.; Zhang, L.; Wang, S.; et al. 2025. RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video. arXiv preprint arXiv:2505.02064. Zhu, J.; Wang, W.; Chen, Z.; Liu, Z.; Ye, S.; Gu, L.; Tian, H.; Duan, Y.; Su, W.; Shao, J.; et al. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479."
        }
    ],
    "affiliations": [
        "Columbia University",
        "Hong Kong University of Science and Technology",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Shandong University",
        "Xian Jiaotong University"
    ]
}