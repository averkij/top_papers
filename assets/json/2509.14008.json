{
    "paper_title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale",
    "authors": [
        "Hasan Abed Al Kader Hammoud",
        "Mohammad Zbeeb",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong AR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the \"nano\" ($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 8 0 0 4 1 . 9 0 5 2 : r HALA Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale Hasan Abed Al Kader Hammoud Mohammad Zbeeb"
        },
        {
            "title": "King Abdullah University of Science and Technology",
            "content": "In Arabic, (Hala) conveys sweetness and beauty - qualities long associated with the language itself. In this spirit, we call our models Hala. We present HALA, family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress strong AREN teacher to FP8 (yielding 2 higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing million-scale corpus tailored to instruction following. We train HALA models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, HALA achieves state-of-the-art results within both the nano (2B) and small (79B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP. Models & Data: hf.co/collections/Hala Code: github.com/Hala"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have rapidly advanced the state-of-the-art across general-purpose NLP, demonstrating strong capabilities in few-shot learning, instruction following, and multistep reasoning. Early milestones such as GPT-3 [1] catalyzed this progress, while more recent families (e.g., Gemini [2], Claude 3) continue to expand the frontier of capability and reliability. Open-weight counterparts, including DeepSeek [3], LLaMA 3 [4], Qwen [5], Gemma [6], and Kimi K2 [7], have Equal Contribution. Correspondence to hasanabedalkader.hammoud@kaust.edu.sa Figure 1: Cross-lingual translation and fine-tuning pipeline for Liquid 1.2B. In the teacher phase, the Cohere model with FP8 inference is used to translate the Orca dataset, which is then used to fine-tune Liquid 1.2B. In the bootstrapped translator phase, Liquid 1.2B translates datasets, producing group of arabic dataset. Liquid models and FANAR were then further fine-tuned on the combined translated datasets, yielding the final instruction-tuned models. ( is orca in arabic) enabled broad experimentation and downstream applications, accelerating community research into scaling, alignment, and efficient deployment. Multilingual modeling at scale. Alongside raw capability, major thrust in recent work targets multilinguality: building models and resources that operate across many languages. Dataset efforts range from broad-coverage sentence-aligned corpora such as Tatoeba [8] to large-scale conversational resources such as MASSIVE [9]. Engineering pipelines (e.g., warc2text extraction and parallel translation) have been used to derive multilingual corpora from web archives [10]. Beyond data, analyses probe whether models preserve knowledge and answer consistency across languages [11]. Model design has also embraced multilinguality from the ground up: BLOOM [12] supports 46 languages, while Baichuan-2 [13] and other families emphasize improved performance on nonEnglish tasks. Despite this breadth-first progress, per-language depth and cultural alignment remain uneven, especially for underrepresented languages. Arabic LLMs and the instruction-data bottleneck. Arabic poses distinct challenges due to diglossia, rich morphology, and wide dialectal variation. growing line of Arabic-centric work [14] spans monolingual pre-training (e.g. AraBERT [15]), foundation and chat models (e.g. JAIS [16], FANAR [17], PEACOCK [18], ACE-GPT [19], ALLAM [20]) and broader sovereign AI efforts such as Falcon [21]. Benchmarks, including Arabic-MMLU [22] , provide initial evaluation scaffolding, although coverage and difficulty remain limited relative to English. persistent bottleneck is the scarcity of high-quality Arabic instruction data, which constrains both instruction tuning and scaling. Previous works document the underrepresentation of non-English languages in pretraining corpora and their impact on downstream performance [2325]. In parallel, the community has explored the paradigms of AI trains AI, e.g., self-instruction and synthetic supervision, to overcome data scarcity [2629]. However, in Arabic, the volume and fidelity of the instruction data still lag behind. Language-centric vs. multilingual. We adopt the term language-centric to denote models whose primary optimization target is depth of capability in specific language (here, Arabic), rather than uniform breadth across many languages. language-centric approach can better capture linguistic nuance (e.g. morphology, orthography)[30], dialectal variation, and cultural/safety alignment, while still benefiting from cross-lingual transfer when appropriate. In practice, this requires (i) reliable translation pipelines to convert strong English supervision into Arabic without eroding instruction fidelity, and (ii) training strategies that scale across model sizes while preserving Arabic fluency and task competence. 2 LLMs as translators: opportunities for Arabic data bootstrapping. LLMs have recently emerged as strong machine translation engines [31], capable of long-document and stylistic translation, interactive workflows, and even domain-preserving scientific translation [32]. Creative strategies, such as searching for keywords / topics with multiple generations of candidates and selection [33], further improve quality. Broad evaluations in 120+ languages [34] suggest that carefully managed LLMs can serve as reliable translators. These developments make translation-first bootstrapping especially attractive for Arabic instruction tuning: If we can (1) compress capable translator for efficient, scalable inference and (2) preserve instruction semantics during translation, we can unlock large Arabic corpora suitable for high-quality tuning. In this report, we introduce HALA, family of Arabic languageOur approach and contributions. centric instruction and translation models built around an efficient translate-and-tune pipeline. Our contributions are as follows: Lightweight AREN translator. We compress strong multilingual translator to FP8 with dynamic scaling using LLM Compressor [35] and fine-tune LiquidAI/LFM2-1.2B to serve as fast, robust AREN engine. This translator is used to construct Arabic instruction data at scale while maintaining fidelity to the source instructions. Million-scale bilingual supervision. We build 1.25M AREN bilingual corpus by pairing translated and original texts (e.g., from Open-Orca [27]) and filtered subset of OPUS-100 [36], enabling stable training of lightweight translation models and consistency checks. Large Arabic instruction corpus. Using our translation stack, we convert several highquality English instruction datasets into Arabic, including Hermes 3 [37], SCP-116K [38], ReAlign-Alpaca [39], LaMini [40], Tulu 3 [41], and Synthetic Instruct-GPT-J Pairwise [42], alongside Open-Orca [27]. The resulting Arabic corpus (millions of pairs) emphasizes instruction following, reasoning, and alignment. Arabic-centric models across scales. We release HALA models at 350M, 700M, and 1.2B parameters (based on Liquid checkpoints) as well as 9B model built on the FANAR architecture [17]. To combine complementary strengths from Englishand Arabic-tuned checkpoints, we employ MergeKit [43] with spherical linear interpolation. Open releases and recipes. We release models, data, and training/evaluation scripts to facilitate reproducibility and further research on Arabic instruction tuning. Summary. By coupling an efficient AREN translator with million-scale data construction, HALA advances Arabic instruction tuning under constrained compute budgets. Our results (Section 3) indicate that HALA models achieve competitive performance within their parameter classes on Arabic-centric benchmarks [22], supporting the view that language-centric modeling is practical and effective complement to breadth-first multilingual scaling."
        },
        {
            "title": "2.1 Quantizing the main translator to FP8",
            "content": "We begin with high-capacity multilingual translator (CohereLabs/command-a-translate-08-2025) and compress it to FP8 [44] with dynamic scaling using LLM Compressor [35], releasing the FP8 artifact as hammh0a/command-a-translate-FP8-Dynamic. The FP8 conversion reduces memory footprint and improves inference throughput (empirically 2 faster than the non-quantized counterpart) while preserving translation quality on our evaluation sets. We follow the official llm-compressor recipe (per-tensor dynamic scaling and post-conversion validation) to ensure stability."
        },
        {
            "title": "2.2 Bootstrapping bilingual supervision from Open-Orca",
            "content": "To construct high-quality AREN supervision aligned with instruction-tuning style data, we translate the first 405K instructionresponse pairs from Open-Orca/OpenOrca [27] into Arabic, covering both the user questions and assistant responses. The quantized FP8 translator is prompted with minimal instruction: 3 Translate from English to Arabic: {x} For each example, we pair the Arabic translations with their original English counterparts, yielding bilingual tuples of the form instren, instrar, respen, respar. This produces an instruction-focused bilingual set mirroring the semantics and difficulty of Open-Orca, with substantial coverage of reasoning-heavy queries."
        },
        {
            "title": "2.3 Quality filtering of OPUS-100 with a strict bilingual judge",
            "content": "We augment the above with large parallel corpus drawn from the Helsinki-NLP/opus-100 [36] ar-en subset. From 1M candidate pairs, we filter for fidelity using compact judge model (Qwen2.5-3B-Instruct [5]) prompted to emit binary verdict (accept/reject): prompt = f\"\"\" You are strict bilingual judge. You will be given translation pair. Arabic: {ar_text} English: {en_text} If the English is correct and natural translation of the Arabic, output only: accept Otherwise, output only: reject \"\"\" Pairs marked accept are retained; this procedure yields 439,592 accepted pairs out of 1M candidates, providing clean AREN signal complementary to the instruction-style data above."
        },
        {
            "title": "2.4 Training a lightweight AR↔EN translator",
            "content": "We combine the translated Open-Orca set (405K 2 = 810K) with the filtered OPUS-100 pairs (440K), totaling 1.26M bilingual examples, and fine-tune LiquidAI/LFM2-1.2B into fast, stable AREN translator specialized for instruction-style inputs (instructions and responses). We use simple chat-style prompting during training (for EA) and standard supervised fine-tuning with cross-entropy. This lightweight translator serves as workhorse for the construction of large-scale Arabic data in the next stage."
        },
        {
            "title": "2.5 Building the Arabic instruction corpus via translation",
            "content": "Using the above translator, we convert multiple high-quality English instruction datasets into Arabic, preserving formatting and answer style: Open-Orca/OpenOrca [27]: 405K (first subset), covering multi-step, reasoning-heavy queries. NousResearch/Hermes-3-Dataset [37]: filtered to remove all code-related samples to avoid translation artifacts. EricLu/SCP-116K [38]: instructional and conversational pairs. GAIR/ReAlign-Alpaca [39]: realigned version of Alpaca instructions. Dahoas/synthetic-instruct-gptj-pairwise [42]: synthetic paired preference-style instructions. MBZUAI/LaMini-instruction [40]: lightweight instruction data, translated fully. allenai/tulu-3-sft-mixture [41]: we keep only English subsets and translate them. The resulting corpus emphasizes instruction following, reasoning, and alignment, providing broad coverage for Arabic-centric instruction tuning. We collect total of roughly 4.5M samples."
        },
        {
            "title": "2.6 Arabic instruction fine-tunes and model merging",
            "content": "We fine-tune models across scales on the translated Arabic instruction mix, then apply merging to balance Arabic gains with base-model strengths: 350M: fine-tune LiquidAI/LFM2-350M, then merge with its base to obtain HALA-350M. 700M: fine-tune LiquidAI/LFM2-700M, then merge with its base to obtain HALA-700M. 1.2B: fine-tune LiquidAI/LFM2-1.2B, then merge with its base to obtain HALA-1.2B. 9B: fine-tune on top of QCRI/Fanar-1-9B-Instruct [17], then merge with its base to obtain HALA-9B. Merging is performed with MergeKit [43] using spherical linear interpolation (slerp) at t=0.5, which we found to preserve general capability while boosting Arabic instruction-following performance. The overall translateandtune pipeline is illustrated in Fig. 1."
        },
        {
            "title": "3 Evaluation",
            "content": "Benchmarks and protocol. We evaluate on suite of Arabic-centric tasks following the OpenArabic-LLM-Leaderboard (OALL) task selection where feasible. Concretely, we report results on: AlGhafa [45], AraTrust [46], ArabicMMLU [22], ArbMMLU-HT [22], EXAMS [47], and MadinahQA [22]. We exclude Alrage (present in some OALL variants) because it requires an LLM-as-a-judge setup. All evaluations are conducted with LightEval [48] using vLLM [49] as the backend for efficient, reproducible inference. We will release exact LightEval command lines, task definitions, and configuration files in the accompanying GitHub repository ( ). Model families. To contextualize HALA within the broader landscape, we include models spanning both multilingual and Arabic-centric families: LLaMA [4], Qwen [5], Gemma [6], SILMA [50, 51], Saka, FANAR [17], Yehia [52], ALLAM, Command-R, and LiquidAI. We report our HALA models at 350M, 700M, 1.2B, and 9B parameters alongside their corresponding bases (LiquidAI checkpoints and FANAR), and representative competitive baselines (e.g., Command-R-7B Arabic). Main results. The aggregated results across the six benchmarks are summarized in Table 1. In the nano regime (2B), HALA-1.2B improves substantially over its base (LiquidAI/LFM2-1.2B), achieving the best average within the size bucket (cf. Table 1). Similarly, HALA-350M and HALA700M consistently outperform their Liquid bases across most tasks, indicating that our translateand tune pipeline yields consistent Arabic gains even at very small scales. In the small regime (9B), HALA-9B consistently outperforms the previous state-of-the-art QCRI/Fanar-1-9B-Instruct baseline on the average metric, while maintaining competitive scores on individual tasks. These trends support our central claim: language-centric tuning on high-fidelity Arabic instruction data improves Arabic capability across scales, and merging [53] (slerp, t=0.5) preserves general competence while enhancing Arabic instruction-following. Translator quality: ENAR MMLU question translation. We assess translation fidelity in an instruction-style regime by constructing controlled, reference-based evaluation using cais/mmlu (English questions) and openai/mmmlu (Arabic questions). We uniformly sample n=500 English questions from cais/mmlu with fixed random seed, translate each to Arabic using the system under test, and align it to its ground-truth Arabic counterpart from the openai/mmmlu Arabic subset (same subject and item ID). We report BLEU (SacreBLEU, 13a tokenization), ROUGEL (F1, rouge-score), and chrF++ (SacreBLEU) between the system output and the reference Arabic question. Exact sampling seeds, preprocessing, and metric commands will be released in the accompanying repository. Prompting setup (fairness control). To ensure comparability across systems, we use fixed prompts: LiquidAI/LFM2-1.2B (specialized translator) prompt You are professional translation engine. Translate English to Modern Standard Arabic. Reply ONLY with the Arabic translationno quotes, notes, or explanations. 5 Table 1: Arabic-centric evaluation across six benchmarks following the OALL task suite (excluding Alrage); higher is better. Columns 49 report task scores (%). Average is the unweighted mean across the six tasks. Best Average within each size bucket is bold; second-best is underlined. All runs use LightEval with vLLM; exact commands are released in the repo. Size Model Name Params AlGhafa ArabicMMLU EXAMS MadinahQA AraTrust ArbMMLU-HT Arabic-centric Benchmarks (%) Average Nano (2B) 2B 2B 2B 2B 2B 2B 2B 2B 2B 2B 2B 2B 2B 2B meta-llama/Llama-3.2-1B Qwen/Qwen2-1.5B-Instruct Qwen/Qwen2.5-1.5B-Instruct Sakalti/Saka-1.5B Qwen/Qwen3-1.7B-Base Qwen/Qwen1.5-1.8B silma-ai/SILMA-Kashif-2B-Instruct-v1.0 google/gemma-2-2b-it LiquidAI/LFM2-350M HALA-350M LiquidAI/LFM2-700M HALA-700M LiquidAI/LFM2-1.2B HALA-1.2B Small (7B9B) 7B9B CohereForAI/c4ai-command-r7b-arabic-02-2025 7B9B JasperV13/Yehia-7B-DPO-Reasoning-preview 7B9B Navid-AI/Yehia-7B-preview 7B9B JasperV13/Yehia-7B-Reasoning-preview 7B9B ALLaM-AI/ALLaM-7B-Instruct-preview 7B9B Qwen/Qwen2-7B-Instruct 7B9B Qwen/Qwen3-8B-Base 7B9B QCRI/Fanar-1-9B-Instruct 7B9B HALA-9B 1B 1.5B 1.5B 1.5B 1.7B 1.8B 2B 2B 350M 350M 700M 700M 1.2B 1.2B 7B 7B 7B 7B 7B 7B 8B 9B 9B 33.9 53.1 48.4 51.4 56.8 32.7 59.7 34.1 39.0 51. 50.1 55.5 53.8 59.2 74.8 75.1 70.8 75.2 69.5 73.2 74.8 76.4 78.3 26.5 49.2 43.5 40.0 49.7 26.7 45.6 30.1 35.2 41. 38.3 45.9 45.2 48.6 59.3 66.3 64.9 66.3 64.9 60.0 65.0 65.8 65.6 21.2 35.2 31.8 31.3 38.2 23.8 33.1 23.6 30.9 36. 34.3 40.6 35.0 43.4 65.0 51.8 52.1 52.7 51.6 47.3 52.5 52.7 53.8 25.7 45.5 38.2 31.5 40.0 26.0 38.8 20.1 28.3 34. 32.5 34.7 34.7 41.6 63.8 54.9 54.4 55.0 54.2 59.5 52.2 73.3 70.4 37.1 68.9 70.8 47.5 75.6 31.5 73.3 31.2 43.3 52. 56.3 65.2 65.6 71.7 80.5 81.9 87.5 80.8 86.9 82.8 83.4 88.3 89.6 23.9 37.4 35.9 33.5 43.9 23.6 35.8 23.4 29.1 35. 37.2 39.4 43.4 44.2 50.1 55.1 53.4 55.2 52.8 51.3 61.5 58.6 61.4 28.0 48.2 44.8 39.2 50.7 27.4 47.7 27.1 34.3 41.9 (+7.6) 41.4 46.9 (+5.5) 46.3 51.4 (+5.1) 65.6 64.2 63.9 64.2 63.3 62.4 64.9 69.2 69.9 (+0.7) Translate everything that follows into Arabic: {text} All other models (teacher FP16/FP8 and baselines) prompt Translate everything that follows into Arabic: {text} Here, {text} is replaced verbatim by the English question from cais/mmlu. Outputs are evaluated directly against the paired Arabic reference from openai/mmmlu without post-processing beyond each metrics built-in normalization. Table 2: ENAR translation quality on 500 sampled MMLU questions. References come from the Arabic subset of openai/mmmlu. Higher is better. Values in ( ) denote absolute deltas vs. the reference system within each block (FP8 vs. FP16 for the teacher translator; HALA vs. LFM2-1.2B base for the lightweight translator). Prompts are fixed as specified above."
        },
        {
            "title": "Teacher translator",
            "content": "BLEU ROUGE-L chrF++ CohereLabs/command-a-translate-08-2025 (FP16) hammh0a/command-a-translate-FP8-Dynamic 53.1 53.5 (+0.3) 26.0 26.0 (+0.0) 68.6 68.9 (+0.3) Lightweight translator (LFM2-1.2B family) LiquidAI/LFM2-1.2B (base) HALA LFM2-1.2B Translator (ours) 16.0 48.2 (+32.1) 19.3 25.1 (+5.9) 43.2 64.2 (+21.0) Compute and cost. All models were trained within budget of $1,000 on 8H100-SXM GPUs, and dataset translation was performed on 12A100 GPUs at an additional cost of roughly $500."
        },
        {
            "title": "4 Conclusion",
            "content": "We presented HALA, family of language-centric Arabic models that leverage an efficient translate andtune pipeline: compress capable AREN translator to FP8, bootstrap million-scale Arabic instruction data from high-quality English sources, and fine-tune compact and small models with slerp-based merging. HALA delivers consistent improvements over base Liquid and FANAR checkpoints, achieving state-of-the-art averages in both the nano (2B) and small (9B) categories on diverse Arabic benchmark suite. We release models, data, and recipes to catalyze further research on Arabic instruction tuning and to encourage language-centric approaches as complement to breadth-first multilingual scaling."
        },
        {
            "title": "5 Acknowledgements",
            "content": "The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940."
        },
        {
            "title": "References",
            "content": "[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, Language models are few-shot learners, in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 18771901. [Online]. Available: https://proceedings.neurips.cc/ paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf [2] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [3] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. [4] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, ..., X. Ma, X. Wang, and Y. Adi, The Llama 3 herd of models, 2024, author list abbreviated. [Online]. Available: https://arxiv.org/abs/2407.21783 [5] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu, Qwen3 technical report, 2025. [Online]. Available: https://arxiv.org/abs/2505. [6] Gemma Team, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. Ramé, M. Rivière, L. Rouillard, T. Mesnard, J.-b. Grill, G. Cideron, S. Ramos, E. Yvinec, M. Casbon, E. Pot, I. Penchev, G. Liu, F. Visin, K. Kenealy, L. Beyer, X. Zhai, A. Tsitsulin, R. Busa-Fekete, A. Feng, N. Sachdeva, B. Coleman, Y. Gao, B. Mustafa, I. Barr, and ..., Gemma 3 technical report, 2025, author list abbreviated. [Online]. Available: https://arxiv.org/abs/2503.19786 [7] K. Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen et al., Kimi k2: Open agentic intelligence, arXiv preprint arXiv:2507.20534, 2025. [8] J. Tiedemann, The tatoeba translation challenge realistic data sets for low resource and multilingual MT, in Proceedings of the Fifth Conference on Machine Translation, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, 7 M. Fishel, A. Fraser, Y. Graham, P. Guzman, B. Haddow, M. Huck, A. J. Yepes, P. Koehn, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, and M. Negri, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 11741182. [Online]. Available: https://aclanthology.org/2020.wmt-1.139/ [9] J. FitzGerald, C. Hench, C. Peris, S. Mackie, K. Rottmann, A. Sanchez, A. Nash, L. Urbach, V. Kakarala, R. Singh, S. Ranganath, L. Crist, M. Britan, W. Leeuwis, G. Tur, and P. Natarajan, MASSIVE: 1M-example multilingual natural language understanding dataset with 51 typologically-diverse languages, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 42774302. [Online]. Available: https://aclanthology.org/2023.acl-long.235/ [10] O. de Gibert, G. Nail, N. Arefyev, M. Bañón, J. van der Linde, S. Ji, J. Zaragoza-Bernabeu, M. Aulamo, G. Ramírez-Sánchez, A. Kutuzov, S. Pyysalo, S. Oepen, and J. Tiedemann, new massive multilingual dataset for high-performance language technologies, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, and N. Xue, Eds. Torino, Italia: ELRA and ICCL, May 2024, pp. 11161128. [Online]. Available: https://aclanthology.org/2024.lrec-main.100/ [11] M. Ifergan, L. Choshen, R. Aharoni, I. Szpektor, and O. Abend, Beneath the surface of consistency: Exploring cross-lingual knowledge representation sharing in LLMs, 2024. [Online]. Available: https://arxiv.org/abs/2408.10646 [12] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon et al., Bloom: 176b-parameter open-access multilingual language model, arXiv preprint arXiv:2211.05100, 2022. [13] A. M. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang, F. Deng, F. Wang, F. Liu, G. Ai, G. Dong, H. Zhao, H. Xu, H.-L. Sun, H. Zhang, H. Liu, J. Ji, J. Xie, J. Dai, K. Fang, L. Su, L. Song, L. Liu, L. Ru, L. Ma, M. Wang, M. Liu, M. Lin, N. Nie, P. Guo, R. Sun, Z. Tao, T. Li, T. Li, W. Cheng, W. Chen, X. Zeng, X. Wang, X. Chen, X. Men, X. Yu, X. Pan, Y.-B. Shen, Y. Wang, Y. Li, Y. Jiang, Y. Gao, Y. Zhang, Z. Zhou, and Z. Wu, Baichuan 2: Open large-scale language models, ArXiv, vol. abs/2309.10305, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:261951743 [14] S. Al-Khalifa, N. Durrani, H. Al-Khalifa, and F. Alam, The landscape of Arabic large language models (ALLMs): new era for Arabic language technology, 2025. [Online]. Available: https://arxiv.org/abs/2506. [15] W. Antoun, F. Baly, and H. Hajj, AraBERT: Transformer-based model for Arabic language understanding, in Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with Shared Task on Offensive Language Detection, H. Al-Khalifa, W. Magdy, K. Darwish, T. Elsayed, and H. Mubarak, Eds. Marseille, France: European Language Resource Association, May 2020, pp. 915. [Online]. Available: https://aclanthology.org/2020.osact-1.2/ [16] N. Sengupta, S. K. Sahu, B. Jia, S. Katipomu, H. Li, F. Koto, W. Marshall, G. Gosal, C. Liu, Z. Chen, O. M. Afzal, S. Kamboj, O. Pandit, R. Pal, L. Pradhan, Z. M. Mujahid, M. Baali, X. Han, S. Mahmoud Bsharat, A. F. Aji, Z. Shen, Z. Liu, N. Vassilieva, J. Hestness, A. Hock, T. Baldwin, P. Nakov, and E. Xing, Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models, 2023. [Online]. Available: https://arxiv.org/abs/2308.16149 [17] F. Team, U. Abbas, M. S. Ahmad, F. Alam, E. Altinisik, E. Asgari, Y. Boshmaf, S. Boughorbel, S. Chawla, S. Chowdhury, F. Dalvi, K. Darwish, M. Elfeky, A. Elmagarmid, M. Eltabakh, M. Fatehkia, A. Fragkopoulos, M. Hasanain, M. Hawasly, M. Husaini, S.-G. Jung, J. K. Lucas, W. Magdy, S. Messaoud, A. Mohamed, T. Mohiuddin, B. Mousi, H. Mubarak, A. Musleh, Z. Naeem, M. Ouzzani, D. Popovic, A. Sadeghi, H. T. Sencar, M. Shinoy, O. Sinan, Y. Zhang, A. Ali, Y. El Kheir, X. Ma, and C. Ruan, Fanar: An Arabic-centric multimodal generative AI platform, 2025. [Online]. Available: https://arxiv.org/abs/2501.13944 8 [18] F. Alwajih, E. M. B. Nagoudi, G. Bhatia, A. Mohamed, and M. Abdul-Mageed, Peacock: family of Arabic multimodal large language models and benchmarks, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), L.-W. Ku, A. Martins, and V. Srikumar, Eds. Bangkok, Thailand: Association for Computational Linguistics, Aug. 2024, pp. 12 75312 776. [Online]. Available: https://aclanthology.org/2024.acl-long.689/ [19] H. Huang, F. Yu, J. Zhu, X. Sun, H. Cheng, S. Dingjie, Z. Chen, M. Alharthi, B. An, J. He, Z. Liu, J. Chen, J. Li, B. Wang, L. Zhang, R. Sun, X. Wan, H. Li, and J. Xu, AceGPT, localizing large language models in Arabic, in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), K. Duh, H. Gomez, and S. Bethard, Eds. Mexico City, Mexico: Association for Computational Linguistics, Jun. 2024, pp. 81398163. [Online]. Available: https://aclanthology.org/2024.naacl-long.450/ [20] M. S. Bari, Y. Alnumay, N. A. Alzahrani, N. M. Alotaibi, H. A. Alyahya, S. AlRashed, F. A. Mirza, S. Z. Alsubaie, H. A. Alahmed, G. Alabduljabbar, R. Alkhathran, Y. Almushayqih, R. Alnajim, M. Al Mansour, M. Alrubaian, A. Alammari, Z. Alawami, A. Al-Thubaity, A. Abdelali, J. Kuriakose, A. Abujabal, N. Al-Twairesh, A. Alowisheq, and H. Khan, ALLaM: Large language models for Arabic and english, 2024. [Online]. Available: https://arxiv.org/abs/2407.15390 [21] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, É. Goffinet, D. Hesslow, J. Launay, Q. Malartic, D. Mazzotta, B. Noune, B. Pannier, and G. Penedo, The falcon series of open language models, 2023. [Online]. Available: https://arxiv.org/abs/2311.16867 [22] F. Koto, H. Li, S. Shatanawi, J. Doughman, A. B. Sadallah, A. Alraeesi, K. Almubarak, Z. Alyafeai, N. Sengupta, S. Shehata, N. Habash, P. Nakov, and T. Baldwin\", Arabicmmlu: Assessing massive multitask language understanding in arabic, in Findings of the Association for Computational Linguistics: ACL 2024, 2024. [23] X. V. Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig, M. Ott, N. Goyal, S. Bhosale, J. Du, R. Pasunuru, S. Shleifer, P. S. Koura, V. Chaudhary, B. OHoro, J. Wang, L. Zettlemoyer, Z. Kozareva, M. Diab, V. Stoyanov, and X. Li, Few-shot learning with multilingual language models, 2022. [Online]. Available: https://arxiv.org/abs/2112.10668 [24] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, mT5: massively multilingual pre-trained text-to-text transformer, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021, pp. 483498. [Online]. Available: https://aclanthology.org/2021.naacl-main.41/ [25] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, Llama 2: Open foundation and fine-tuned chat models, 2023. [Online]. Available: https://arxiv.org/abs/2307. [26] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang, Wizardlm: Empowering large language models to follow complex instructions, arXiv preprint arXiv:2304.12244, 2023. [27] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah, Orca: Progressive learning from complex explanation traces of GPT4, 2023. [Online]. Available: https://arxiv.org/abs/2306.02707 9 [28] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [29] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, Self-instruct: Aligning language models with self-generated instructions, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 13 48413 508. [Online]. Available: https://aclanthology.org/2023.acl-long.754/ [30] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, Unsupervised cross-lingual representation learning at scale, in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, Eds. Online: Association for Computational Linguistics, Jul. 2020, pp. 84408451. [Online]. Available: https://aclanthology.org/2020.acl-main.747/ [31] C. Lyu, Z. Du, J. Xu, Y. Duan, M. Wu, T. Lynn, A. F. Aji, D. F. Wong, and L. Wang, paradigm shift: The future of machine translation lies with large language models, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, and N. Xue, Eds. Torino, Italia: ELRA and ICCL, May 2024, pp. 13391352. [Online]. Available: https://aclanthology.org/2024.lrec-main.120/ [32] H. C. Kleidermacher and J. Zou, Science across languages: Assessing LLM multilingual translation of scientific papers, arXiv preprint arXiv:2502.17882, 2025. [33] Z. He, T. Liang, W. Jiao, Z. Zhang, Y. Yang, R. Wang, Z. Tu, S. Shi, and X. Wang, Exploring human-like translation strategy with large language models, Transactions of the Association for Computational Linguistics, vol. 12, pp. 229246, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.13/ [34] W. Zhu, H. Liu, Q. Dong, J. Xu, S. Huang, L. Kong, J. Chen, and L. Li, Multilingual machine translation with large language models: Empirical results and analysis, in Findings of the Association for Computational Linguistics: NAACL 2024, K. Duh, H. Gomez, and S. Bethard, Eds. Mexico City, Mexico: Association for Computational Linguistics, Jun. 2024, pp. 27652781. [Online]. Available: https://aclanthology.org/2024.findings-naacl.176/ [35] R. H. AI and vLLM Project, LLM Compressor, 8 2024. [Online]. Available: https://github.com/vllm-project/llm-compressor [36] B. Zhang, P. Williams, I. Titov, and R. Sennrich, Improving massively multilingual neural machine translation and zero-shot translation, in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, Eds. Online: Association for Computational Linguistics, Jul. 2020, pp. 16281639. [Online]. Available: https://aclanthology.org/2020.acl-main.148 [37] R. Teknium, J. Quesnelle, and G. Chen, Hermes 3 technical report, 2024. [Online]. Available: https://arxiv.org/abs/2408.11857 [38] D. Lu, X. Tan, R. Xu, T. Yao, C. Qu, W. Chu, Y. Xu, and Y. Qi, Scp-116k: high-quality problem-solution dataset and generalized pipeline for automated extraction in the higher education science domain, arXiv preprint arXiv:2501.15587, 2025. [39] R.-Z. Fan, X. Li, H. Zou, J. Li, S. He, E. Chern, J. Hu, and P. Liu, Reformatted the Association for Computational Linguistics: EMNLP alignment, in Findings of 2024, Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 574597. [Online]. Available: https://aclanthology.org/2024.findings-emnlp.32/ 10 [40] M. Wu, A. Waheed, C. Zhang, M. Abdul-Mageed, and A. F. Aji, LaMini-LM: diverse herd of distilled models from large-scale instructions, in Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), Y. Graham and M. Purver, Eds. St. Julians, Malta: Association for Computational Linguistics, Mar. 2024, pp. 944964. [Online]. Available: https://aclanthology.org/2024.eacl-long.57/ [41] N. Lambert, J. Morrison, V. Pyatkin, S. Huang, H. Ivison, F. Brahman, L. J. V. Miranda, A. Liu, N. Dziri, S. Lyu, Y. Gu, S. Malik, V. Graf, J. D. Hwang, J. Yang, R. L. Bras, O. Tafjord, C. Wilhelm, L. Soldaini, N. A. Smith, Y. Wang, P. Dasigi, and H. Hajishirzi, Tülu 3: Pushing frontiers in open language model post-training, https://arxiv.org/abs/2411.15124, 2024. [42] A. Havrilla, synthetic-instruct-gptj-pairwise (revision cc92d8d), 2023. [Online]. Available: https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise [43] C. Goddard, S. Siriwardhana, M. Ehghaghi, L. Meyers, V. Karpukhin, B. Benedict, M. McQuade, and J. Solawetz, Arcees MergeKit: toolkit for merging large language models, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, F. Dernoncourt, D. Preotiuc-Pietro, and A. Shimorina, Eds. Miami, Florida, US: Association for Computational Linguistics, Nov. 2024, pp. 477485. [Online]. Available: https://aclanthology.org/2024.emnlp-industry.36 [44] A. Kuzmin, M. Van Baalen, Y. Ren, M. Nagel, J. Peters, and T. Blankevoort, Fp8 quantization: The power of the exponent, Advances in Neural Information Processing Systems, vol. 35, pp. 14 65114 662, 2022. [45] E. Almazrouei, R. Cojocaru, M. Baldo, Q. Malartic, H. Alobeidli, D. Mazzotta, G. Penedo, G. Campesan, M. Farooq, M. Alhammadi, J. Launay, and B. Noune, AlGhafa evaluation benchmark for Arabic language models, in Proceedings of ArabicNLP 2023, H. Sawaf, S. El-Beltagy, W. Zaghouani, W. Magdy, A. Abdelali, N. Tomeh, I. Abu Farha, N. Habash, S. Khalifa, A. Keleg, H. Haddad, I. Zitouni, K. Mrini, and R. Almatham, Eds. Singapore (Hybrid): Association for Computational Linguistics, Dec. 2023, pp. 244275. [Online]. Available: https://aclanthology.org/2023.arabicnlp-1.21 [46] E. A. Alghamdi, R. I. Masoud, D. Alnuhait, A. Y. Alomairi, A. Ashraf, and M. Zaytoon, Aratrust: An evaluation of trustworthiness for llms in arabic, arXiv preprint arXiv:2403.09017, 2024. [47] M. Hardalov, T. Mihaylov, D. Zlatkova, Y. Dinkov, I. Koychev, and P. Nakov, EXAMS: multi-subject high school examinations dataset for cross-lingual and multilingual question answering, in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 54275444. [Online]. Available: https://aclanthology.org/2020.emnlp-main.438 [48] N. Habib, C. Fourrier, H. Kydlíˇcek, T. Wolf, and L. Tunstall, Lighteval: lightweight framework for llm evaluation, 2023. [Online]. Available: https://github.com/huggingface/ lighteval [49] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica, Efficient memory management for large language model serving with pagedattention, in Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [50] silma-ai, Silma 9b instruct v1.0, https://huggingface.co/silma-ai/SILMA-9B-Instruct-v1.0, 2024. [51] SILMA-AI, Silma SILMA-Kashif-2B-Instruct-v1.0, 2025. kashif 2b instruct v1.0, https://huggingface.co/silma-ai/ [52] Navid-AI, Yehia 7b preview, https://huggingface.co/Navid-AI/Yehia-7B-preview, 2025. [53] S. Sanyal, A. Neerkaje, J. Kaddour, A. Kumar, and S. Sanghavi, Early weight averaging meets high learning rates for llm pre-training, 2023. [Online]. Available: https://arxiv.org/abs/2306."
        }
    ],
    "affiliations": [
        "KAUST"
    ]
}