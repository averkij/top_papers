{
    "paper_title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation",
    "authors": [
        "Edoardo Bianchi",
        "Antonio Liotta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment."
        },
        {
            "title": "Start",
            "content": "BIANCHI, LIOTTA: SKILLFORMER 1 SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation Edoardo Bianchi edbianchi@unibz.it Antonio Liotta antonio.liotta@unibz.it 5 2 0 Free University of Bozen-Bolzano Bozen-Bolzano, IT 3 1 ] . [ 1 5 6 1 6 8 0 . 5 0 5 2 : r Abstract Assessing human skill levels in complex activities is challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-ofthe-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment."
        },
        {
            "title": "Introduction",
            "content": "Automatically estimating human expertise from video is key challenge in computer vision, especially in domains like sports, rehabilitation, and skill training. Yet, unlike action recognition, this task requires capturing subtle differences in motion quality and precision, which are often visible only through combination of viewpoints. To address this, we propose SkillFormer, lightweight and efficient architecture for multi-view skill assessment. SkillFormer processes synchronized egocentric and exocentric video streams using TimeSformer backbone, enhanced by novel CrossViewFusion module that leverages cross-attention and learnable gating. To enable efficient fine-tuning, it incorporates Low-Rank Adaptation (LoRA). This design achieves competitive overall accuracy while substantially improving computational efficiency through reduced trainable parameters and faster training convergence. This work introduces the following key contributions: CrossViewFusion module that fuses multiple views via multi-head cross-attention and learnable gating, allowing the model to focus on the most informative aspects of each view. 2025. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. BIANCHI, LIOTTA: SKILLFORMER self-calibration mechanism that aligns view-specific embeddings using learnable statistics. LoRA-based fine-tuning strategy applied to the TimeSformer backbone, enabling efficient adaptation with minimal overhead. Compared to state-of-the-art benchmarks, SkillFormer demonstrates strong overall classification accuracy in multi-view settings, which are the primary focus of our work. While our Ego-only configuration achieves comparable results to specialized single-view methods (45.9% vs 46.8%), the real strength of our approach emerges in multi-view scenarios. SkillFormer significantly improves accuracy in exocentric settings by 14% (from 40.6% to 46.3%) and in combined ego+exo setups by 16.4% (from 40.8% to 47.5%). The benefits are particularly pronounced in structured activities, where our multi-view approach achieves improvements for Basketball (77.88%), Cooking (60.53%), and Bouldering (33.52%) by effectively integrating complementary spatial and temporal features from synchronized perspectives. These substantial gains highlight the effectiveness of our integrated cross-attention fusion approach, particularly in scenarios where multiple viewpoints and fine-grained motion cues are crucial for expertise assessment."
        },
        {
            "title": "2 Background and Related Works",
            "content": "Action Quality Assessment (AQA) and Proficiency Estimation focus on evaluating the quality of human actions, with applications across sports, rehabilitation, and skill training [22]. Traditional approaches relied on single-modality inputs, using handcrafted features or 3D CNNs like C3D and I3D to extract spatiotemporal representations [5, 14, 15]. More recent methods adopt transformer-based models such as TimeSformer, Video Swin, and ActionFormer to capture long-range dependencies and fine-grained motion dynamics [1, 12, 21]. Specialized architectures like GDLT decouple grade-aware features via decoder attention [18], while CoRe introduces contrastive regression for more precise skill scoring [19]. Nonetheless, skill classification remains underexplored, particularly in multi-view settings. Multi-modal AQA improves robustness by integrating complementary signalsRGB, flow, audio, depth, or pose. Skating-Mixer [17] and PAMFN [20] incorporate audio-visual cues via staged fusion. Others combine RGB and depth for industrial tasks [2], or rely on skeletons to capture motion kinematics [3, 6, 7]. Datasets like EgoExoLearn [11] and EgoExo4D [9] emphasize the importance of multi-perspective inputs, but existing fusion strategies often rely on heuristic or late fusion, limiting generalization. In EgoExo4D [9], baseline models are modality-specificseparate architectures are trained for first-person and third-person views, with predictions later combined via ensembling. These models also rely on heterogeneous pretraining datasets across modalities, making comparison and deployment under unified constraints more challenging. Recent work on efficient adaptation, such as LoRA [10], enables fine-tuning with fewer trainable parameters. Knowledge distillation also supports low-resource adaptation for vision tasks [13, 16]. Our work advances the state-of-the-art by addressing key limitations in prior methodssuch as reliance on modality-specific models, late fusion, and heavy pretraining. By introducing unified architecture for multi-view skill classification, we simultaneously enhance classification accuracy and computational efficiency, enabling robust skill assessment under realistic resource constraints. BIANCHI, LIOTTA: SKILLFORMER 3 Figure 1: Overview of the SkillFormer architecture. Multi-view video inputs (one egocentric and up to four exocentric) are processed through shared TimeSformer backbone fine-tuned with LoRA. Features are fused using the CrossViewFusion module and passed to classification head."
        },
        {
            "title": "3 Proposed Methodology",
            "content": "We present SkillFormer, novel architecture for unified multi-view proficiency estimation. Our model builds upon the TimeSformer [1] framework, incorporating parameter-efficient fine-tuning through LoRA [10] and specialized CrossViewFusion module for multi-view integration. Figure 1 provides an overview of our approach."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "Given set of synchronized videos from multiple viewpoints (e.g., first-person and thirdperson cameras) capturing person performing an activity, our goal is to classify their expertise level into one of predefined categories (Novice, Early Expert, Intermediate Expert, or Late Expert). Formally, for each sample, we have set of video sequences {V1,V2, ...,Vn} from different views, and we aim to predict label {0, 1, 2, 3} indicating the expertise level."
        },
        {
            "title": "3.2 Video Feature Extraction",
            "content": "We use TimeSformer backbone [1], pretrained on Kinetics-600 [5], to extract spatiotemporal features by modeling spatial and temporal dependencies via divided attention. For efficient adaptation, we apply LoRA [10] to key componentsincluding attention projections, feed-forward layers, and temporal modulesallowing us to fine-tune only small 4 BIANCHI, LIOTTA: SKILLFORMER Figure 2: Architecture of the CrossViewFusion module. Input features from multiple views (B,V, input_dim) are first normalized and integrated using multi-head attention. The fused representation passes through feed-forward network with residual connections and learnable gating mechanism. Final projections and normalization are followed by adaptive selfcalibration using learned mean and variance parameters, producing output features of shape (B, out_dim). number of additional parameters. For multi-view inputs, we flatten the batch and view dimensions, extract features in parallel using the shared backbone, and reshape the output before fusing it with the CrossViewFusion module. This design maintains per-view identity while enabling efficient joint processing. To support variable-length inputs while preserving alignment with the original 8-frame pretraining, we interpolate both the input frames and the models temporal positional embeddings. In particular, we apply linear interpolation to the learned time embeddings, enabling the model to generalize across different temporal resolutions without degrading temporal consistency."
        },
        {
            "title": "3.3 CrossViewFusion for Unified Multi-View Integration",
            "content": "We introduce CrossViewFusion, lightweight yet expressive module for fusing information from multi-view video streams. It takes as input view-specific features from egocentric and exocentric sequences and applies sequence of operations: normalization, cross-view attention, gated transformation, and adaptive self-calibration. These steps produce unified embedding tailored for downstream expertise classification. An overview of the module is shown in Figure 2. Below, we describe each component of the CrossViewFusion module, aligned with its actual implementation. View-Wise Normalization: To account for statistical disparities across viewpoints, we apply layer normalization independently to each views features. This operation ensures consistent feature scaling and centering, which is especially critical when merging modalities with inherently different distributions, such as first-person and third-person video streams. Multi-Head Cross-View Attention: Rather than relying on simple concatenation or averaging, we employ multi-head cross-attention mechanism, where each view can attend to and contextualize information from all other views. This enables the model to learn interview dependencies and adaptively prioritize view-specific cues. For instance, in cooking scenarios, egocentric hand movements may dominate attention, whereas in dance sequences, global body posture from exocentric views may become more salient. Gated Feature Transformation The aggregated features are passed through feedforward network that includes learnable gating mechanism. This consists of linear layer BIANCHI, LIOTTA: SKILLFORMER followed by sigmoid activation that produces gating weights for each hidden dimension. The gate acts as dynamic filter, performing an element-wise modulation of the hidden representation to selectively amplify informative features while suppressing irrelevant ones. Such modulation is crucial for expertise assessment, where fine-grained differences in motion and controlnot just coarse action labelsdrive prediction quality. Adaptive Feature Self-Calibration To further align the fused representations with the target classification space, we introduce an adaptive self-calibration block that learns perfeature affine parameters (µlearn, σlearn). This departs from traditional fixed-statistics normalization and allows the model to reshape feature distributions in data-driven manner, improving its ability to resolve fine-grained skill variations across diverse activities. Classification Head Finally, the calibrated features are fed into classifier to predict discrete expertise levels."
        },
        {
            "title": "4.1 Dataset",
            "content": "We evaluate on the Ego-Exo4D dataset [9], large-scale multimodal benchmark comprising over 1,200 hours of synchronized egocentric and exocentric video from 740 participants recorded across 123 real-world environments. Egocentric videos are first-person recordings captured from the demonstrators viewpoint, while exocentric videos are thirdperson views recorded from static cameras placed in the environment. The dataset spans eight domainscooking, health, bike repair, music, basketball, bouldering, soccer, and dancecaptured in naturalistic settings to reflect authentic task execution. For the demonstrator proficiency estimation benchmark, only six domains are used: cooking, music, basketball, bouldering, soccer, and dance. The health and bike repair domains are excluded due to highly imbalanced or incomplete label distributions. Each sample includes one egocentric video, recorded using Project Aria glasses [8], and up to four time-synchronized exocentric views captured via static GoPro cameras. Expert annotators assign one of four discrete proficiency levelsNovice, Early Expert, Intermediate Expert, or Late Expertserving as ground truth for classification. The label distribution is notably imbalanced, skewed toward intermediate and late experts due to targeted recruitment of skilled participants. We use all videos with available proficiency estimation labels, following the official benchmarks training and validation splits. As in prior work [4], we train on the official training set, reserving 10% for validation, and evaluate on the full, held-out official validation set. This setup ensures direct and fair comparability with the accuracy reported in the dataset paper. The datasets rich multi-view structure and diverse activity settingsranging from kitchens and dance studios to climbing gyms and soccer fieldsmake it strong testbed for realworld, multi-view skill assessment."
        },
        {
            "title": "4.2 Implementation Details",
            "content": "All models were initialized from TimeSformer backbone [1], pre-trained on Kinetics-600 [5], and fine-tuned for 4 epochs using AdamW (weight decay = 0.01). We used cosine 6 BIANCHI, LIOTTA: SKILLFORMER Table 1: Comparison with EgoExo4D proficiency estimation baselines (from [9]). We report accuracy (%) for egocentric (Ego), exocentric (Exos), and combined views (Ego+Exos). SkillFormer outperforms the baselines in the Exos and Ego+Exos settings with significantly fewer trainable parameters and fewer epochs. Bold denotes the best accuracy; underlined values indicate second-best. Pretrain Ego Exos Ego+Exos Params Epochs Method Random Majority-class TimeSformer TimeSformer TimeSformer TimeSformer TimeSformer - - - K400 24.9 31.1 42.3 42.9 HowTo100M 46.8 44.4 45.9 EgoVLP EgoVLPv2 24.9 31.1 40.1 39.1 38.2 40.6 38. - 46.3 - 24.9 31.1 40.8 38.6 39.7 39.5 37.8 - - 47.5 - - 121M 121M 121M 121M 121M 14M 20M 27M - - 15 15 15 15 4 4 4 SkillFormer-Ego SkillFormer-Exos SkillFormer-EgoExos K600 K600 K600 45.9 - - annealing learning rate schedule with 1 epoch of linear warmup, fixed batch size of 16, and 16 attention heads. Training was conducted on single 80GB NVIDIA A100 GPU. For each video, fixed number of frames was sampled uniformly from random temporal window using frame sampling rate of (e.g., every n-th frame), ensuring temporal coverage within variable-length clips. The selected frames were resized such that the shortest edge is 224 pixels, center-cropped to 224 224, rescaled to the [0, 1] range, and normalized with mean [0.45, 0.45, 0.45] and standard deviation [0.225, 0.225, 0.225]. Hyperparameters such as the number of input frames, projector hidden size, LoRA rank and scaling, and learning rate were tuned based on the number and type of input views. All other settings were held fixed across experiments (Refer to Section 5.2 and Table 2)."
        },
        {
            "title": "5 Results",
            "content": "We evaluate SkillFormer on the EgoExo4D [9] proficiency estimation benchmark using classification accuracy as the primary evaluation metric, in line with the official proficiency demonstrator benchmark protocol. We consider three input configurations: egocentric (Ego, 1 view), exocentric (Exos, 4 views), and multi-view (Ego+Exos, 5 views), enabling us to isolate the contribution of each modality (Sections 5.1 and 5.2). Additionally, we analyze per-domain accuracy to evaluate generalization across diverse real-world tasks (Section 5.3). SkillFormer achieves consistent improvements in both accuracy and efficiency, outperforming multi-view baselines with fewer parameters and significantly lower training cost."
        },
        {
            "title": "5.1 Overall Accuracy and Efficiency",
            "content": "Table 1 compares overall accuracy between SkillFormer and baseline models from the EgoExo4D benchmark [9]. In contrast to baselineswhich train separate models for egocentric and exocentric inputs and perform late fusion at inferenceSkillFormer uses single unified model BIANCHI, LIOTTA: SKILLFORMER 7 Table 2: Training hyperparameters and overall accuracy for SkillFormer across view configurations. Fixed across all runs: epochs = 4, batch size = 16, output dim = 768, attention heads = 16. Hid denotes the dimensionality of the intermediate representation used in the CrossViewFusion module. It controls the capacity of the feed-forward transformation and the gating mechanism. Views Frames LoRA-r LoRA-a Hid LR Params Acc (%) Ego Exos Ego+Exos 32 24 16 32 48 64 64 96 128 1536 2048 5e-5 3e-5 2e-5 14M 20M 27M 45.9 46.3 47.5 for each configuration (Ego, Exos, Ego+Exos). Furthermore, we do not apply multi-crop testing, simplifying inference and reducing computational overhead. SkillFormer achieves state-of-the-art classification accuracy in both Exos (46.3%) and Ego+Exos (47.5%) settings, outperforming the best TimeSformer baseline by up to 16.4%. Simultaneously, SkillFormer demonstrates increased computational efficiency by using 4.5x fewer trainable parameters (27M vs. 121M) and requiring 3.75x fewer training epochs (4 vs. 15). These results highlight SkillFormers accuracy and compute-efficiency. Random and majority-class baselines perform significantly worse, underscoring the inherent complexity of the task. It is worth noting that the proficiency label distribution is notably imbalanced, skewed toward intermediate and late experts due to targeted recruitment of skilled participants. This may bias overall accuracy by underrepresenting novice classes."
        },
        {
            "title": "5.2 Efficiency and Configuration Analysis",
            "content": "Table 2 details the hyperparameter configurations adopted across different multi-view setups. As the number of views increases, our design prioritizes efficiency without compromising accuracy. To this end, we strategically reduce the number of frames per viewpreserving the temporal span with fewer sampled tokenswhile proportionally increasing the LoRA rank and hidden dimension in the CrossViewFusion module. This trade-off ensures that the model retains sufficient capacity to capture inter-view dependencies, even under tighter per-view bandwidth. Our choice of LoRA rank and fusion dimensionality is not arbitrary. Higher ranks allow the adapter to express richer transformations across views, compensating for the reduced token budget. Empirically, we found that increasing these parameters moderately yields significant gains in accuracy while maintaining training efficiency within tractable computational budgets. For example, SkillFormer-Ego+Exos achieves 47.5% accuracy with only 27M trainable parametersa 4.5x reduction compared to full fine-tuning of the 121M parameter TimeSformer backbonedemonstrating the effectiveness of low-rank adaptation and targeted fusion. This design reflects key motivation behind SkillFormer: enabling scalable, parameterefficient skill recognition across multi-view egocentric and exocentric inputs. 8 BIANCHI, LIOTTA: SKILLFORMER Table 3: Per-scenario accuracy (%) for the majority-class baseline, the baseline models from [9], and SkillFormer across different view configurations. Bold indicates the best-performing method per scenario, while underlined values represent the second-best. SkillFormer consistently outperforms other models in multi-view setups, particularly in Basketball, Cooking, and Bouldering. Scenario Majority Baseline SkillFormer Ego Exos Ego+Exos Ego Exos Ego+Exos Basketball Cooking Dancing Music Bouldering Soccer 36.19 50.00 51.61 58.97 0.00 62.50 51.43 45.00 55.65 46.15 25.31 56.25 52.30 35.00 42.74 69.23 17.28 75.00 55.24 35.00 42.74 56.41 17.28 75.00 69.03 31.58 20.51 72.41 30.77 70.83 70.80 47.37 15.38 68.97 33.52 66. 77.88 60.53 13.68 68.10 31.87 66."
        },
        {
            "title": "5.3 Per-Domain Accuracy Analysis",
            "content": "Table 3 reports per-scenario accuracy across all models and configurations. SkillFormer consistently outperforms baselines in the Ego+Exos setting for structured and physically grounded activities such as Basketball (77.88%), Cooking (60.53%), and Bouldering (31.87%). These domains benefit from synchronized egocentric and exocentric perspectives, which enable better modeling of spatial layouts and temporally extended actions. The fusion of multi-view signals allows SkillFormer to exploit cross-perspective cues, such as object-hand interactions or full-body movement trajectories, which are often ambiguous or occluded in single-view inputs. Interestingly, in Music, the Ego-only configuration achieves the highest accuracy (72.41%), suggesting that head-mounted views are sufficient to capture detailed instrument manipulation, while additional views may introduce redundant or misaligned information. This highlights the flexibility of SkillFormer to adapt its fusion mechanism to view-specific signal quality. Subjective domains like Dancing reveal limitations: SkillFormers Ego+Exos accuracy (13.68%) falls significantly below both the majority baseline (51.61%) and the baseline Ego model (55.65%). This indicates that tasks with high intra-class variability and weak structure may not benefit as much from multi-view fusion, or may require additional modalities (e.g., audio) or supervision cues to disambiguate subtle skill indicators. These trends underscore that SkillFormer is particularly effective in domains requiring precise spatial-temporal reasoning and multi-view integration, aligning with its architectural design focused on efficient yet expressive fusion across diverse camera perspectives. However, this per-domain evaluation is also affected by class imbalance: domains with few novice samples may exhibit inflated or unstable accuracy due to limited representation of early-stage performers."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "Despite strong accuracy and efficiency, SkillFormer has several limitations suggesting future improvements. BIANCHI, LIOTTA: SKILLFORMER 9 Fixed frame sampling may miss critical temporal cues in variable activities, limitation that could be addressed through adaptive sampling with attention-based keyframe detection. SkillFormer also underperforms significantly in subjective domains like Dancing, suggesting challenges with stylistic variability that domain adaptation strategies such as curriculum learning could improve. Additionally, the class imbalance in EgoExo4D affects evaluation accuracy, which could be mitigated through class-weighted loss functions or synthetic data generation. Finally, SkillFormer lacks semantic understanding of skill components, limiting interpretabilitya challenge that could be addressed through multi-task learning with pose estimation or textguided supervision. These enhancements would improve SkillFormers accuracy in challenging domains while maintaining its computational efficiency."
        },
        {
            "title": "7 Conclusions",
            "content": "We introduced SkillFormer, lightweight architecture for multi-view skill assessment that leverages synchronized egocentric and exocentric video inputs. Our approach combines TimeSformer backbone with novel CrossViewFusion module, featuring cross-attention, learnable gating, and parameter-efficient LoRA fine-tuning. SkillFormer makes dual contribution to multimodal video understanding. First, it achieves state-of-the-art overall classification accuracy on the EgoExo4D proficiency estimation benchmarkparticularly in Exos (46.3%) and Ego+Exos (47.5%) settingsoutperforming previous baselines by up to 16.4%. Second, it demonstrates strong computational efficiency, using 4.5x fewer trainable parameters and requiring 3.75x fewer training epochs compared to prior approaches. Our per-domain analysis reveals that SkillFormer excels in structured physical activities that benefit from complementary viewpoints, achieving significantly higher accuracy in Basketball (77.88%), Cooking (60.53%), and Bouldering (33.52%). These improvements are most pronounced when both egocentric and exocentric perspectives provide complementary information about spatial layouts and temporally extended actions. These results demonstrate that parameter-efficient adaptation and strategic multi-view fusion can substantially enhance skill assessment capabilities while reducing computational demands. SkillFormers unified architecture for integrating egocentric and exocentric signals presents significant advancement in efficient multi-view skill assessment, with particular promise for resource-constrained deployment scenarios."
        },
        {
            "title": "References",
            "content": "[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding?, 2021. URL https://arxiv.org/abs/2102. 05095. [2] Edoardo Bianchi and Oswald Lanz. Egocentric video-based human action recognition in industrial environments. In Franco Concli, Lorenzo Maccioni, Renato Vidoni, and Dominik T. Matt, editors, Latest Advancements in Mechanical Engineering, pages 257 267, Cham, 2024. Springer Nature Switzerland. ISBN 978-3-031-70465-9. 10 BIANCHI, LIOTTA: SKILLFORMER [3] Edoardo Bianchi and Oswald Lanz. Gate-shift-pose: Enhancing action recognition in sports with skeleton information. In Proceedings of the Winter Conference on Applications of Computer Vision (WACV) Workshops, pages 12571264, February 2025. [4] Björn Braun, Rayan Armani, Manuel Meier, Max Moebus, and Christian Holz. egoppg: Heart rate estimation from eye-tracking cameras in egocentric systems to benefit downstream vision tasks, 2025. URL https://arxiv.org/abs/2502.20879. [5] João Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 47244733, 2017. doi: 10.1109/CVPR.2017.502. [6] Yong Du, Wei Wang, and Liang Wang. Hierarchical recurrent neural network for skeleton based action recognition. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11101118, 2015. doi: 10.1109/CVPR.2015.7298714. [7] Haodong Duan, Yue Zhao, Kai Chen, Dahua Lin, and Bo Dai. Revisiting skeletonbased action recognition. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 29592968, 2022. doi: 10.1109/CVPR52688.2022. 00298. [8] Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith, Cheng Peng, Chris Sweeney, Cole Wilson, Dan Barnes, Daniel DeTone, David Caruso, Derek Valleroy, Dinesh Ginjupalli, Duncan Frost, Edward Miller, Elias Mueggler, Evgeniy Oleinik, Fan Zhang, Guruprasad Somasundaram, Gustavo Solaira, Harry Lanaras, Henry Howard-Jenkins, Huixuan Tang, Hyo Jin Kim, Jaime Rivera, Ji Luo, Jing Dong, Julian Straub, Kevin Bailey, Kevin Eckenhoff, Lingni Ma, Luis Pesqueira, Mark Schwesinger, Maurizio Monge, Nan Yang, Nick Charron, Nikhil Raina, Omkar Parkhi, Peter Borschowa, Pierre Moulon, Prince Gupta, Raul Mur-Artal, Robbie Pennington, Sachin Kulkarni, Sagar Miglani, Santosh Gondi, Saransh Solanki, Sean Diener, Shangyi Cheng, Simon Green, Steve Saarinen, Suvam Patra, Tassos Mourikis, Thomas Whelan, Tripti Singh, Vasileios Balntas, Vijay Baiyya, Wilson Dreewes, Xiaqing Pan, Yang Lou, Yipu Zhao, Yusuf Mansour, Yuyang Zou, Zhaoyang Lv, Zijian Wang, Mingfei Yan, Carl Ren, Renzo De Nardi, and Richard Newcombe. Project aria: new tool for egocentric multi-modal ai research, 2023. URL https: //arxiv.org/abs/2308.13561. [9] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid BIANCHI, LIOTTA: SKILLFORMER 11 Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C.V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, and Michael Wray. Egoexo4d: Understanding skilled human activity from firstand third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1938319400, June 2024. [10] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. [11] Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Dong Lu, Yali Wang, Limin Wang, and Yu Qiao. Egoexolearn: dataset for bridging asynchronous egoand exo-centric view of procedural activities in real world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [12] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer, 2021. URL https://arxiv.org/abs/2106.13230. [13] Laura Meneghetti, Edoardo Bianchi, Nicola Demo, and Gianluigi Rozza. Kd-ahosvd: Neural network compression via knowledge distillation and tensor decomposition. In Jordane Lorandel and Ahmed Kamaleldin, editors, Design and Architecture for Signal and Image Processing, pages 8192, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-87897-8. [14] Paritosh Parmar and Brendan Tran Morris. What and how well you performed? multitask learning approach to action quality assessment. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 304313, 2019. doi: 10.1109/CVPR.2019.00039. [15] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks, 2015. URL https: //arxiv.org/abs/1412.0767. [16] Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual intelligence: review and new outlooks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(6):30483068, 2022. doi: 10.1109/TPAMI.2021. 3055564. [17] Jingfei Xia, Mingchen Zhuge, Tiantian Geng, Shun Fan, Yuantai Wei, Zhenyu He, and Feng Zheng. Skating-mixer: Long-term sport audio-visual modeling with mlps, 2022. URL https://arxiv.org/abs/2203.03990. [18] Angchi Xu, Ling-An Zeng, and Wei-Shi Zheng. Likert scoring with grade decoupling for long-term action assessment. In 2022 IEEE/CVF Conference on Computer Vision BIANCHI, LIOTTA: SKILLFORMER and Pattern Recognition (CVPR), pages 32223231, 2022. doi: 10.1109/CVPR52688. 2022.00323. [19] Xumin Yu, Yongming Rao, Wenliang Zhao, Jiwen Lu, and Jie Zhou. Group-aware contrastive regression for action quality assessment, 2021. URL https://arxiv. org/abs/2108.07797. [20] Ling-An Zeng and Wei-Shi Zheng. Multimodal action quality assessment. IEEE Transactions on Image Processing, 2024. [21] Chen-Lin Zhang, Jianxin Wu, and Yin Li. Actionformer: Localizing moments In Computer Vision ECCV 2022: 17th Euroof actions with transformers. pean Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part IV, page 492510, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-197710. doi: 10.1007/978-3-031-19772-7_29. URL https://doi.org/10.1007/ 978-3-031-19772-7_29. [22] Kanglei Zhou, Ruizhi Cai, Liyuan Wang, Hubert P. H. Shum, and Xiaohui Liang. comprehensive survey of action quality assessment: Method and benchmark, 2024. URL https://arxiv.org/abs/2412.11149."
        }
    ],
    "affiliations": [
        "Free University of Bozen-Bolzano Bozen-Bolzano, IT"
    ]
}