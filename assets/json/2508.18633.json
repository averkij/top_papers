{
    "paper_title": "ROSE: Remove Objects with Side Effects in Videos",
    "authors": [
        "Chenxuan Miao",
        "Yutong Feng",
        "Jianshu Zeng",
        "Zixiang Gao",
        "Hantang Liu",
        "Yunfeng Yan",
        "Donglian Qi",
        "Xi Chen",
        "Bin Wang",
        "Hengshuang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is https://rose2025-inpaint.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 3 3 6 8 1 . 8 0 5 2 : r ROSE: Remove Objects with Side Effects in Videos Chenxuan Miao1, Yutong Feng2, Jianshu Zeng3, Zixiang Gao3, Hantang Liu2, Yunfeng Yan1, Donglian Qi1, Xi Chen4, Bin Wang2, Hengshuang Zhao4 1Zhejiang University, 2KunByte AI, 3Peking University, 4The University of Hong Kong {weiyuchoumou526, fengyutong.fyt, zengjianshu.AI, gzx2401210062 }@gmail.com {liuhantang77, chauncey0620, binwang393}@gmail.com {yyff, qidl}@zju.edu.cn hszhao@cs.hku.hk"
        },
        {
            "title": "Abstract",
            "content": "Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, framework that systematically studies the objects effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage 3D rendering engine for synthetic data generation. We carefully construct fully-automatic pipeline for data preparation, which simulates largescale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is https://rose2025-inpaint.github.io/."
        },
        {
            "title": "Introduction",
            "content": "Removing objects in visual contents represents valuable technique with widespread applications in both daily and industrial scenarios. This task targets to re-fill the masked region of objects via reasonable and consistent content, regarding the context in surrounding environment. Prior works [15, 37, 14, 3] towards either image or video object removal have explored to leverage flowbased pixel propagation to restore the masked region with neighboring information [43], or adopt the inpainting paradigm to directly generate the masked content [32, 23]. Powered by the significant capability of large-scale models [27, 8, 19, 23] on generalized visual creation, the inpainting-based methods exhibit satisfying erasing performance in diverse scenarios of image and video. Despite the advanced performance, however, existing works are still restricted due to the lack of paired training samples that follows real-world physical rules. The paired samples represents data Project Leader Corresponding Author Preprint. Under review. Figure 1: Video object removal results generated by ROSE (zoom in for better view). Every two lines are an example where the above is input video with mask and the bottom is inference result. We sequentially show cases of various side effects studied in this paper. with and without the object, where the objects influence on the environment is correspondingly changed, such as its shadow on the ground. Most works leverage the segmentation dataset, e.g., DAVIS [26] and YouTube-VOS [39], to construct artificial pairs, either directly pasting an object from another sample, or masking the object with zero value. While simple and scalable, these strategies fail to reflect the side effects of object, e.g., shadows, reflections and lighting changes. Therefore, models supervised by the artificial pairs typically generate unnatural outputs with side effects left in environment. To tackle this, OmniEraser [37] manages to filter out such image pairs from the sequential frames in videos with static camera motion. However, when confronting with video object removal, it is impractical to leverage higher dimensional data to construct the paired dataset. To address these problems, we propose to prepare the paired video samples via 3D rendering. Recent advancements on the rendering engines [7] make it practical to generate high-qualified and strictly-aligned synthetic video pairs. We design fully-automatic data preparation pipeline to create large-scale video set for object removal. More concretely, we collect batch of base environments and split them into multiple scenes containing various objects. Following that, the pipeline automatically generates cameras focusing on the objects to be removed, and apply random camera trajectories. The rendering engine enables us to activate or disable the object, and also precisely render the object masks. Thus, we could obtain list of triples consisting of the original video, edited video with object removed, and the corresponding mask video, which contain perfectly simultaneous temporal contents. Furthermore, we systematically study the various types of side effects in videos, including light source, mirror, reflection, shadow, and translucency. Equipped with the data preparation pipeline, we efficiently construct comprehensive dataset including all the above side effects on diverse scenes. To fully utilize the synthetic data, we present ROSE, an efficient framework based on video inpainting to remove object in videos with their side effects. To help distinguish the object-interacted region 2 in environment, we directly feed the whole video into the model, in contrast to previous works that fills the object area with zero mask. The complete video serves as powerful reference guidance on model, to localize the side effects concerning the intrinsic attributes of the object. We also apply random augmentation strategies on the mask to cope with various input in inference. Furthermore, we introduce an additional supervision to explicitly predict the difference mask between edited and original videos. We implement this by injecting mask predictor based on the hidden representations of the inpainting model. The aforementioned architectures of ROSE are observed to enhance the models capability to attend and erase the side effects in videos. To facilitate comprehensive evaluation on the object removal results with side effects, we construct new benchmark, named ROSE-Bench, consisting of both realistic and synthetic video data. Through extensive experiments, we demonstrate that ROSE achieves state-of-the-art performance on video object removal, and effectively adapts to real-world scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion Transformers for Video Generation. Recent diffusion models [10, 28, 30, 31] have shown strong performance in text-to-video generation. By integrating transformers [33], diffusion transformers (DiTs)[25] improve video quality and temporal consistency. State-of-the-art methods leverage large-scale video-text datasets[2, 40] and hybrid architectures for efficiency and fidelity. Recent DiT-based latent diffusion models, such as Wan2.1 [34] and MAGI-1 [1], excel in long video generation: Wan2.1 uses causal 3D VAEs with 1:256 compression and flow matching for real-time synthesis, while MAGI-1 employs an autoregressive DiT for chunk-wise generation with strict causality. These advances underscore DiTs strength in balancing quality, efficiency, and control. Video Inpainting. Early video inpainting methods primarily used 3D CNNs [5, 35, 12] to model spatial-temporal features, but their limited receptive fields hindered long-range propagation. Subsequently, optical flow [16, 22, 45] and homography [21, 4] were introduced to guide pixel propagation. To improve efficiency and accuracy, Zhou et al.[43] proposed ProPainter, combining optical flow and attention mechanisms. Recently, with the rise of diffusion models[10, 28], diffusionbased video inpainting has emerged [29, 20, 38, 44]. Li et al.[23] proposed DiffuEraser, extending the image inpainting model BrushNet[15] to videos via two-stage training scheme."
        },
        {
            "title": "3.1 Paired Erasing Videos Preparation using 3D data",
            "content": "Acquiring paired data samples that depict scenes with and without objects and their side effects represents significant challenge in object removal task. Though recent work explores generating such image pairs from videos with static camera motion [37], it is impossible to obtain video pairs in higher dimension using this technique. To tackle this problem, we propose to utilize the adequate 3D data together with advanced game engine, i.e., the Unreal Engine [7], to synthesize the paired video data. As illustrated in Fig. 2, we present an automatic data preparation pipeline as follows: Scene and Object Sampling. We begin by collecting large-scale virtual environments from public 3D asset platforms such as Fab [6]. Each environment is sufficiently complex and diverse, covering wide range of indoor and outdoor scenes, including urban settings, natural landscapes, and artificial constructions. We manually subdivide these base environments into smaller scenes, each containing one or more candidate objects for removal. In total, we collect 28 high-quality environments and split them into 450 unique scenes. The selected scenes include wide variety of object typesboth static and dynamicincluding vehicles, animals, plants, and more. This ensures diverse training corpus that enhances the generalization ability of the inpainting model. Multi-view Generation with Object Masks. Given sampled object in scene, we randomly assign multiple camera views with varying angles and distances within predefined ranges. key advantage of using 3D engine is the ability to generate accurate object masks via programmable post-process shaders, avoiding reliance on segmentation models [17]. For each object, we apply custom shader that renders the object in white and masks the rest in black, producing precise binary masks. Per-frame mask videos are automatically generated through scripting. 3 Figure 2: Paired video preparation pipeline using 3D data, which can be divided into: scene and object sampling, multi-view generation with masks, valid view filtering and video data rendering. Valid View Filtering. To ensure the quality of videos and avoid object-occlusion cases, we further filter out views by calculating the ratio of foreground pixels in the mask. Ratios lower than threshold suggest videos with insufficient mask coverage, e.g., due to occlusion or mislabeling. Such videos are discarded to avoid introducing noisy supervision into the training set. Video Pair Rendering. After filtering, we render both the unedited (original) and edited (objectremoved) video sequences by toggling the visibility of the selected objects in the engine. The camera moving is sampled from pre-defined set with random disturbing, e.g., zooming in and out. All video pairs are rendered at resolution of 1920 1080 and frame length of 90 frames (6 seconds). Since the camera trajectories and object placement are determined via scripted generation, the original video, the corresponding mask video, and the edited video remain spatially and temporally aligned on per-frame basis. Such an alignment is critical for enabling pixel-wise supervised learning."
        },
        {
            "title": "3.2 Categorize Side Effect in Videos",
            "content": "To improve the generalization ability of the model and its robustness under various complex realworld conditions, we deliberately construct the dataset composed of six distinct categories. These categories are carefully designed to simulate typical yet challenging side effects that commonly occur in practical scenarios, such as object-light interactions, mirror reflections, and translucent materials. By explicitly injecting such variations into the training process, we aim to equip the model with the capacity to understand and handle diverse object-environment relationships beyond trivial inpainting cases. We summarize the definition of side effects on the environment as follows: Common: Objects with minimal interaction with surrounding context, representing typical inpainting cases. Their removal causes little disruption to spatial layout or visual semantics. Light Source: This category includes objects that function as light emitters. Their removal changes the global illumination, affecting shadows, reflections, and overall scene appearance. Figure 3: Illustration of the various side-effect categories studied in the dataset of ROSE. 4 Figure 4: The framework of ROSE. We concatenate the noisy latents with the original input video and masks, consumed by video inpainting model. An additional difference mask predictor is introduced to predict the correlated area in video, automatically computed from the input video pairs. Mirror: Objects reflected in mirrors require spatial reasoning and semantic understanding to inpaint both the object and its mirrored counterpart, ensuring visual consistency. Reflection: Compared to the Mirror category, it emphasizes reflective surfaces like water, requiring the model to infer and complete indirect visual cues from reflections. Shadow: Shadows linked to objects require joint removal, making inpainting sensitive to lighting and spatial structure to ensure coherence across both object and shadow regions. Translucent: Semi-transparent objects expose the background with blending or refraction. Inpainting must recover both visible cues and hidden structures for realistic restoration."
        },
        {
            "title": "4.1 Overview",
            "content": "In this section, we elaborate the model architecture of ROSE for conducting video object removal, as illustrated in Fig. 4. In brief, ROSE is implemented as an inpainting model continued from the foundation video generative models [34, 18] (the Wan2.1 model [34] in this paper). Following the general architecture in diffusion-based inpainting models [19, 3], we extend the model input with the original input video together with object masks. Distinguished from the typical setting that multiply the mask onto input video, we directly feed the whole video to assist the understanding on environment. The input masks, with precise boundary generated by 3D engine, are further augmented to enhance model robustness. To better supervise the model to localize the subtle object-environment interactions, we introduce an additional difference mask predictor to explicitly predict the side effect areas. We present the detail of ROSE in the following sub-sections."
        },
        {
            "title": "4.2 Reference-based Object Erasing",
            "content": "We start by formulate the video inpainting task in ROSE. Given an input video and binary mask sequence M, where the area of object to be removed is filled with value 1, the target is to generate an object-erased video ˆV. For the video condition consumed by the model, most prior methods [23, 5, 43, 35] follow mask-and-inpaint paradigm, feeding the network with only the non-object area (1 M), where indicates point-wise product. Suppose the noisy latents of diffusion model as X, then the model input can be regarded as [X; (1 M); M]. Such manner explicitly eliminate the object from input, and is friendly for model convergence. However, when confronting the side effect removal, isolating the object from the model makes it challenging to localize the object-related region. In contrast, recent work on image modality has explored to guide the model with the masked region for reference [14]. In this paper, we adapt such reference-based erasing, modifying the model input as [X; V; M]. Experimental results suggest introducing the whole video as guidance significantly increase the performance. We attribute the advancement that the inner 5 Figure 5: Visualization of various mask augmentation strategies adopted in training. Figure 6: Comparison between the previous paradigm and our reference-based paradigm. attention mechanism is effective for seeking the inter-region correlations in videos. Given the object region as input, the model thereby leverage it prior knowledge to localize the side effect regions, thus outperforms the model with masked video input. Furthermore, the complete video as input serves to enhance the temporal consistency of output video, for introducing the original object-environment interactions. The visualization comparisons between the two paradigms are shown in Fig. 6."
        },
        {
            "title": "4.3 Mask Augmentation",
            "content": "In real-world applications, user-provided masks often vary in precision, size, and shaperanging from accurate segmentation maps to coarse bounding boxes or sparse point annotations. Since the masks generated by 3D engine is perfectly accurate, training solely on such ideal masks can lead to performance gap at deployment. To mitigate this, we introduce set of mask augmentation strategies that simulate diverse mask types likely to appear in practice. As shown in Fig. 5, we adopt five variants: (i) Original mask, precise binary map from ground-truth annotations; (ii) Point-wise mask, an extremely sparse point simulating minimal user input; (iii) Bounding box mask, coarse rectangular region enclosing the target; (iv) Dilated mask, obtained via morphological dilation to simulate loose annotations; and (v) Eroded mask, generated by erosion to mimic under-segmentation. These variants are randomly sampled during training, which exposes the model to diverse, imperfect masks and improves its generalization to real-world inputs."
        },
        {
            "title": "4.4 Explicit Supervision via Difference Mask Prediction",
            "content": "Beyond the diffusion loss targeting reconstruct the regions of object and its side effect, we introduce an additional supervision into ROSE. Specifically, we inject difference mask predictor into the framework, predicting binary masks indicating all the areas to be modified in video. The core idea is to leverage the complementary information in the video pairs for training. When an object is removed from scene, it often leaves behind subtle but semantically significant side effects, such as shadows, reflections, and occlusions. To explicitly guide the model in attending to these regions, we compute binary difference mask by comparing the original video x0 Rcf hw and its edited counterpart x0. The difference mask d0 {0, 1}f hw is defined as: (cid:40) d(t,h,w) 0 = if 1, (cid:13) (cid:13)x(t,h,w) (cid:13) 0, otherwise 0 x(t,h,w) 0 (cid:13) (cid:13) (cid:13) > δ (1) where δ > 0 is fixed threshold (δ = 0.09 in this paper). The resulting binary mask highlights pixel-level differences induced by object removal and is downsampled to match the latent resolution, yielding the ground-truth difference mask dt {0, 1}f h/sw/s. Difference Mask Predictor. To guide the model in identifying regions influenced by object removal, we design difference mask predictor Dθ, which takes as concatenated token features as input, extracted from multiple transformer blocks. Let RBLDtotal denote the fused feature sequence, where = Fp Hp Wp represents the total number of tokens and Dtotal is the aggregated channel dimension after selecting and concatenating multiple transformer layers. The difference mask predictor consists of two-layer MLP that reduces Dtotal to scalar prediction per token. Its 6 Figure 7: Qualitative comparison between our method and existing approaches on real-world samples. Our model demonstrates superior ability and effectively handles complex object-environment interactions, including shadows, reflections, and illumination changes. output is then reshaped into 3D spatio-temporal grid with the same shape of video latents: ˆdt = Interpolate (Reshape(Dθ(x)), size = (F, H, )) , (2) where the predicted mask ˆdt [0, 1]B1F HW is upsampled via trilinear interpolation from coarse patch-level grid (Fp, Hp, Wp) to the full resolution (F, H, ). The module is trained under MSE loss supervision against the ground-truth difference mask dt described in Eq. (3). It functions as an auxiliary self-localization signal to encourage the model to be sensitive to subtle visual effects introduced by object edits. Then the training objective of ROSE consists of two terms: the standard diffusion denoising loss and the auxiliary mask prediction loss: = Et,z0,ϵ ϵ ˆϵ2 2 + λˆdt dt2 , (3) (cid:104) (cid:105) where λ balances the two objectives. This formulation enables the difference mask predictor to guide the model in localizing and identifying regions where object-environment interactions occur."
        },
        {
            "title": "5.1 Experiment Settings",
            "content": "Training Data. Our dataset contains 16,678 synthetic video pairs rendered in Unreal Engine, each 6 seconds (90 frames) at 19201080 resolution. It features diverse urban, rural, and natural scenes with dynamic weather, lighting, and interactive objects. 7 oTable 1: Quantitative comparison on the synthetic paired benchmark (PSNR / SSIM / LPIPS)"
        },
        {
            "title": "Category",
            "content": "Metric ROSE(Ours) DiffuEraser [23] ProPainter [43] FuseFormer [24] FloED [9] FGT [41] Common Shadow Light Source Reflection Mirror Translucent Mean PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 36.5998 0.9517 0.0413 33.7876 0.9225 0.0626 30.0739 0.9209 0.0862 27.7344 0.8715 0.1129 28.3498 0.9381 0. 31.4264 0.9470 0.0598 31.1221 0.9170 0.0772 30.9326 0.9204 0.0825 28.9976 0.9220 0.1119 22.6541 0.8832 0.1403 26.2914 0.8619 0. 22.1228 0.8855 0.1751 28.4520 0.9259 0.1036 26.5024 0.8981 0.1284 31.9972 0.9466 0.0515 30.2427 0.9353 0.0619 23.4291 0.8924 0. 26.9373 0.8763 0.1072 22.1206 0.8994 0.1447 29.8910 0.9397 0.0722 27.1991 0.9148 0.0946 31.2325 0.9154 0.0658 28.5520 0.8972 0. 22.8571 0.8630 0.1410 25.7707 0.8345 0.1437 22.3175 0.8671 0.1596 28.1712 0.9168 0.0914 26.2566 0.8795 0.1208 29.8932 0.9066 0. 27.8932 0.8834 0.1208 22.3125 0.8596 0.1589 25.1018 0.8413 0.1651 21.3789 0.8678 0.1845 27.3924 0.8956 0.1134 25.4847 0.8697 0. 28.4331 0.8819 0.0832 27.5809 0.8547 0.1172 21.4579 0.8433 0.1347 24.3986 0.8421 0.1520 22.6013 0.8594 0.1653 27.4802 0.9034 0. 25.2353 0.8641 0.1289 Evaluation Benchmark and Metrics. Existing benchmarks in the video inpainting domain mainly suffer two limitations. First, most of them lack access to paired edited videos following real-world physical rules, which restricts quantitative evaluation due to the absence of ground-truth. Second, they overlook the side effects induced by object-environment interactions hat are critical for assessing the semantic correctness and realism of inpainting. Consequently, these benchmarks fail to capture fine-grained challenges that frequently arise in real-world applications. To address these gaps, we construct ROSEBench, comprehensive evaluation benchmark on video object removal, consisting of following subsets: (i) Synthetic paired benchmark tailored for evaluation under diverse physical interaction effects. Using the same simulation approach described in Sec. 3, the benchmark consists of 6 representative categories: common, light source, mirror, reflection, shadow, and translucent, each modeling specific class of object-environment interaction. Every category contains 10 highquality triplets of video sequences, i.e., original, edited, and mask videos, offering precise and controllable evaluation of model behavior under different side-effect conditions. Table 2: Ablation study on ROSE-Bench (PSNR / SSIM / LPIPS ) Category Metric Base +MRG +MA +DMP Common Shadow Light Reflect. Mirror Translucent PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 32.58 0.937 0.053 30.65 0.914 0.081 24.99 0.894 0.112 25.39 0.836 0.131 22.63 0.905 0.142 27.43 0.925 0.087 35.24 0.950 0.040 33.29 0.920 0.061 30.37 0.923 0.074 27.71 0.843 0.109 28.45 0.941 0.076 30.98 0.949 0.052 35.37 0.948 0.041 33.62 0.921 0.061 25.80 0.893 0.107 27.08 0.841 0.122 27.37 0.921 0.107 30.12 0.947 0.056 36.60 0.952 0.041 33.79 0.923 0.063 30.07 0.921 0.086 27.73 0.872 0.113 28.35 0.938 0.088 31.43 0.947 0.060 Mean (ii) Realistic paired benchmark constructed using copy-and-paste strategy based on the video segmentation dataset dataset DAVIS [26]. We copy masked object from one video into another. The resulting video with inserted object is treated as input, while the original unaltered video serves as the ground-truth. This process allows us to construct realistic and diverse test cases that mirror practical editing scenarios while preserving access to ground-truth supervision. For quantitative evaluation on paired benchmark, we compute PSNR [11], SSIM [36], and LPIPS [42] across both synthetic and real-world test sets. These metrics capture both low-level structural fidelity and perceptual similarity, assessing the model performance under various side-effect challenges. PSNR SSIM LPIPS 31.12 0.917 0.077 29.89 0.912 0.082 27.28 0.902 0.101 30.84 0.918 0.071 (iii) Realistic unpaired benchmark containing real videos with masks. Different from the second subset, we directly feed real-world videos into model, which are also sampled from DAVIS [26]. To conduct evaluation without ground-truth, we select related metrics from the VBench [13], widely-adopted benchmark on text-to-video generation, for evaluating the quality of output videos on motion smoothness, background consistency and temporal flickering. Implementation Details. In the training process, we resize all the video pairs into the resolution of 720 480 and use 81 frames for training. The backbone model is controllable generation variant of Wan2.1 1.3B version [34]. We fully train the model together with the difference mask predictor in 80000 optimization steps with 0.00002 learning rate on 4 NVIDIA H800 GPUs."
        },
        {
            "title": "5.2 Comparisons with Previous Methods",
            "content": "Quantitative Evaluation. For quantitative evaluation, we compare our method with flowbased transformers (ProPainter [43], FuseFormer [24], FGT [41]) and diffusion-based methods (DiffuEraser [23], FLoED [9]). We evaluate all methods on the three components of ROSE-Bench: synthetic paired benchmark (Tab. 1), realistic paired benchmark (Tab. 3), and real-world videos (Tab. 4). Our model achieves superior performance in object removal, as measured by PSNR [11], SSIM [36], and LPIPS [42], and excels in maintaining motion smoothness, background consistency, and subject consistency in Tab. 4. Table 3: Quantitative comparison on realistic paired benchmark. Method PSNR SSIM LPIPS ROSE(Ours) DiffuEraser [23] ProPainter [43] FuseFormer [24] FloED [9] FGT [41] 31.34 29.97 32.81 26.52 28.48 27.53 0.923 0.901 0.917 0.885 0.881 0.874 0.092 0.128 0.122 0.151 0.147 0.135 Table 4: VBench-based evaluation on the realistic unpaired benchmark. (Best scores are bolded)."
        },
        {
            "title": "Method",
            "content": "DiffuEraser ProPainter FuseFormer FloED FGT ROSE (Ours) Motion Smoothness Background Consistency Temporal Flickering Subject Consistency Imaging Quality 0.972 0.975 0.971 0.973 0.971 0.975 0.902 0.917 0.905 0.904 0.897 0.923 0.931 0.932 0.938 0.932 0.933 0.936 0.891 0.903 0.892 0.889 0.895 0.908 0.658 0.626 0.625 0.618 0.614 0.630 Qualitative Evaluation. For qualitatitve evaluation, we compare our method with ProPainter [43], FuseFormer [24] and DiffuEraser [23]. Qualitative visualization results can be seen in Fig. 7. In the Fig. 7, we demonstrate cases with various different side effects like shadows, reflection and lumination changes and we can obviously find that our model shows superior performance over other methods. The side effects areas that previous works fail to fill in have been framed in red boxes."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "We perform ablation studies to demonstrate the effectiveness of our designs. We keep training settings same as in Sec. 5.1 to ensure the fairness of comparisons and we evaluate our methods on the synthetic paired benchmark. We set the baseline with the following settings: use the \"mask-andinpaint\" paradigm, without mask augmentation and difference mask predictor. And in Tab. 2, MRG stands for mask region guidance, MA stands for mask augmentation and DMP stands for difference mask predictor. In Tab. 2, we have shown that our primary designs are effective and useful."
        },
        {
            "title": "6 Discussion",
            "content": "This paper introduces ROSE, unified framework for video object removal that addresses both target objects and their side effects, such as shadows, reflections, and lighting distortions. By leveraging synthetic data from 3D rendering pipeline, we alleviate real-world data scarcity while ensuring diverse scenes and camera motions. Our diffusion transformer architecture excels in 9 object localization and side effect removal via differential mask supervision. The proposed ROSEBench offers systematic evaluation for object-environment interactions, addressing key gap in video inpainting. Extensive experiments show that ROSE significantly outperforms prior methods and generalizes well to real-world videos. These contributions advance video editing and set new benchmarks for handling complex visual artifacts. Future work will explore real-time optimization and broader environmental effects to further bridge synthetic and real-world domains. Despite its strengths, ROSE has limitations: (1) It may produce flickering artifacts under large motion, as shown in Tab. 4; (2) Inference time grows with video length, reducing efficiency on long sequences."
        },
        {
            "title": "References",
            "content": "[1] Sand AI. Magi-1: Autoregressive video generation at scale. https://github.com/SandAI-org/ MAGI-1, 2025. [2] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: Learning joint representations for vision and text using cross-modal contrastive learning. In NeurIPS, 2021. [3] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Any-length video inpainting and editing with plug-and-play context control. arXiv preprint arXiv:2503.05639, 2025. [4] Jiayin Cai, Changlin Li, Xin Tao, Chun Yuan, and Yu-Wing Tai. Devit: Deformed vision transformers in video inpainting. In ACM MM, 2022. [5] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston Hsu. Free-form video inpainting with 3d gated convolution and temporal patchgan. In ICCV, 2019. [6] Epic Games. Fab. https://www.fab.com/, 2024. [7] Epic Games. Unreal engine 5.3. https://www.unrealengine.com/, 2024. [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [9] Bohai Gu, Hao Luo, Song Guo, and Peiran Dong. Advanced video inpainting using optical flow-guided efficient diffusion. arXiv preprint arXiv:2412.00857, 2024. [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [11] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In ICPR, 2010. [12] Yuan-Ting Hu, Heng Wang, Nicolas Ballas, Kristen Grauman, and Alexander G. Schwing. Proposal-based video completion. In ECCV, 2020. [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. [14] Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, and Houqiang Li. Smarteraser: Remove anything from images using masked-region guidance. In CVPR, 2025. [15] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In ECCV, 2024. [16] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Deep video inpainting. In CVPR, 2019. [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In CVPR, 2023. [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. [19] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [20] Minhyeok Lee, Suhwan Cho, Chajin Shin, Jungho Lee, Sunghun Yang, and Sangyoun Lee. Video diffusion models are strong video inpainter. arXiv preprint arXiv:2408.11402, 2024. [21] Sungho Lee, Seoung Wug Oh, DaeYeun Won, and Seon Joo Kim. Copy-and-paste networks for deep video inpainting. In ICCV, 2019. [22] Ang Li, Shanshan Zhao, Xingjun Ma, Mingming Gong, Jianzhong Qi, Rui Zhang, Dacheng Tao, and Ramamohanarao Kotagiri. Short-term and long-term context aggregation network for video inpainting. In ECCV, 2020. 11 [23] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. Diffueraser: diffusion model for video inpainting. 2025. [24] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. Fuseformer: Fusing fine-grained information in transformers for video inpainting. In ICCV, 2021. [25] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [26] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In CVPR, 2016. [27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [29] Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, and Limin Wang. Bivdiff: training-free framework for general-purpose video synthesis via bridging image and video diffusion models. In CVPR, 2024. [30] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015. [31] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. [32] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. arXiv preprint arXiv:2109.07161, 2021. [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [34] Wan Video. Wan: Open and advanced large-scale video generative models. https://github.com/ Wan-Video/Wan2.1, 2025. [35] Chuan Wang, Haibin Huang, Xiaoguang Han, and Jue Wang. Video inpainting by jointly learning temporal structure and spatial details. In AAAI, 2019. [36] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: From error visibility to structural similarity. TIP, 2004. [37] Runpu Wei, Zijin Yin, Shuo Zhang, Lanxiang Zhou, Xueyi Wang, Chao Ban, Tianwei Cao, Hao Sun, Zhongjiang He, Kongming Liang, and Zhanyu Ma. Omnieraser: Remove objects and their effects in images with paired video-frame data. arXiv preprint arXiv:2501.07397, 2025. [38] Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, et al. Towards language-driven video inpainting via multimodal large language models. arXiv preprint arXiv:2401.10226, 2024. [39] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. Youtube-vos: Sequence-to-sequence video object segmentation. In ECCV, 2018. [40] Yue Xu, Jiabo Ye, Yifan Xu, Hang Zhou, Wayne Wu, and Ziwei Liu. Video-llava: Learning multimodal video instruction-following. In EMNLP, 2023. [41] Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-guided transformer for video inpainting. In ECCV, 2022. [42] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [43] Shangchen Zhou, Chongyi Li, Kelvin C. K. Chan, and Chen Change Loy. Propainter: Improving propagation and transformer for video inpainting. In ICCV, 2023. 12 [44] Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Kam-Fai Wong, and Lei Zhang. Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility. In AAAI, 2025. [45] Xueyan Zou, Linjie Yang, Ding Liu, and Yong Jae Lee. Progressive temporal feature alignment network for video inpainting. In CVPR, 2021."
        }
    ],
    "affiliations": [
        "KunByte AI",
        "Peking University",
        "The University of Hong Kong",
        "Zhejiang University"
    ]
}