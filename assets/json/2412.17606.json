{
    "paper_title": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images",
    "authors": [
        "Risa Shinoda",
        "Kuniaki Saito",
        "Shohei Tanaka",
        "Tosho Hirasawa",
        "Yoshitaka Ushiku"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building a large-scale figure QA dataset requires a considerable amount of work, from gathering and selecting figures to extracting attributes like text, numbers, and colors, and generating QAs. Although recent developments in LLMs have led to efforts to synthesize figures, most of these focus primarily on QA generation. Additionally, creating figures directly using LLMs often encounters issues such as code errors, similar-looking figures, and repetitive content in figures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic Figures), a dataset for pre-training figure QA. Our proposed pipeline enables the creation of chart figures with complete annotations of the visualized data and dense QA annotations without any manual annotation process. Our stage-by-stage pipeline makes it possible to create diverse topic and appearance figures efficiently while minimizing code errors. Our SBSFigures demonstrate a strong pre-training effect, making it possible to achieve efficient training with a limited amount of real-world chart data starting from our pre-trained weights."
        },
        {
            "title": "Start",
            "content": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images Risa Shinoda12, Kuniaki Saito2, Shohei Tanaka2, Tosho Hirasawa2, Yoshitaka Ushiku2 1Kyoto University, Japan 2OMRON SINIC Corp., Japan {risa.shinoda, kuniaki.saito, shohei.tanaka, tosho.hirasawa, yoshitaka.ushiku}@sinicx.com 4 2 0 2 3 2 ] . [ 1 6 0 6 7 1 . 2 1 4 2 : r Abstract Building large-scale figure QA dataset requires considerable amount of work, from gathering and selecting figures to extracting attributes like text, numbers, and colors, and generating QAs. Although recent developments in LLMs have led to efforts to synthesize figures, most of these focus primarily on QA generation. Additionally, creating figures directly using LLMs often encounters issues such as code errors, similar-looking figures, and repetitive content in figures. To address this issue, we present SBS Figures (Stage-by-Stage Synthetic Figures), dataset for pre-training figure QA. Our proposed pipeline enables the creation of chart figures with complete annotations of the visualized data and dense QA annotations without any manual annotation process. Our stageby-stage pipeline makes it possible to create diverse topic and appearance figures efficiently while minimizing code errors. Our SBS Figures demonstrate strong pre-training effect, making it possible to achieve efficient training with limited amount of real-world chart data starting from our pre-trained weights. Our code is available at https://github.com/omronsinicx/SBSFigures."
        },
        {
            "title": "Introduction",
            "content": "Building models that understand figures is essential for automating document understanding, given that numerous documents incorporate figures for data visualization. Understanding figures necessitates two key abilities of models: (i) precise interpretation of visualized information, encompassing numerical data, labels, and plot positions, and (ii) reasoning to return accurate responses based on the visualized information and user queries. densely annotated dataset with many graph figures is imperative to train such models effectively. Given the requirements above, these annotations need to (i) offer exhaustive details about each figure, e.g., the values and labels of plots along with the graph title, and (ii) include question-answer pairs to build well-performing QA model (Masry et al. 2022). However, collecting such dataset is not easy. First, collecting figures itself needs careful selection and parsing of diverse documents or websites (Siegel et al. 2016), and its cost will increase if we care about their copyright. Second, exhaustive labeling demands lot of effort for annotators as they must first comprehend the visualization and then translate it into texts (Hoque, Kavehzadeh, and Masry 2022). To reduce the extensive annotation effort, prior approaches have Figure 1: SBS Figures (Stage-by-Stage Synthetic Figures). We create SBS Figures, dataset for pre-training figure QA. Our stage-by-stage synthetic dataset creation enables strong pre-training effect for real-world chart data. utilized template-based QA augmentation or LLMs (Kahou et al. 2018; Methani et al. 2020; Carbune et al. 2024). While this reduces the cost of QA generation, the number of figures remains limited. Recently, LLM-based figure generation has been proposed (e.g., ChartLlama (Han et al. 2023)) , where an LLM generates both visualization target and code to render the target for each figure. However, this pipeline is inefficient in terms of the number of queries required, as each figure demands separate LLM query. Moreover, the synthesized code often contains errors without refinement. To address this challenge, we propose novel stage-bystage pipeline for generating both figures and their annotations, designed to progressively transform the seed to synthesize figure. Specifically, our pipeline divides the figure generation process into three modules, visualization target data generation, figure rendering via Python code, and QA pair generation, with each module progressively transforming seed data. This stage-by-stage approach offers three key advantages over generating figures all at once: (i) the generated seed data, such as data point to be visualized, and figure rendering code, can be stored and reused, reducing the cost of querying LLMs multiple times, (ii) the figures appearance can be easily diversified, as components like chart type, data content, and font can be controlled at each stage, and (iii) figure rendering is more reliable because, rather than having the LLM generate rendering code for each data point, we create the figure-rendering code separately for each figure type, using same data structure. Using our scalable dataset generation pipeline, we introduce new dataset called SBS Figures (Stage-By-Stage Synthetic Figures), which comprises 1 million figure images, each paired with annotations of the accurate visualized data and QA pairs. Our model, pre-trained on SBS Figures, demonstrates strong performance on real-world figure QA datasets, enabling efficient learning on real-world charts. Our SBS Figures pre-training shows generality to models and fine-tuning dataset. Beyond the pipeline and dataset, we also investigate key factors influencing figure QA pretraining. Specifically, we explore the impact of various dataset components on pre-training with synthetic datasets, including figure appearance, QA quality, and task prompts, which have not been fully explored in previous studies. We will make the entire pipeline, including the dataset, code, prompts for LLM, and models, publicly available, allowing future research to use our SBS Figures for more efficient training. In summary, our main contributions are as follows: We propose new pipeline that enables the efficient creation of diverse topics, visually distinct figures, and dense QAs while minimizing code errors. Our proposed dataset, entirely synthesized, demonstrates strong pre-training effect on real-world chart data. This dataset enables effective pre-training even with limited amount of real-world chart data. We will make the entire pipeline, including the dataset, code, prompts, and models, publicly available."
        },
        {
            "title": "2 Related Work\nSynthetic figure generation To overcome the limited\nnumber of annotated real-world figures, some existing\ndatasets create figures in two main ways. The first way\nis gathering real-world data and converting them into\nfigures: datasets such as LEAF-QA (Chaudhry et al.\n2020), LEAFQA++ (Singh and Shekhar 2020), and\nPlotQA (Methani et al. 2020) have been introduced. How-\never, collecting real-world data suitable for synthetic figure\ngeneration remains a costly process, therefore, the number\nof figure images is still limited (e.g., LEFQA / LEAFQA++:\n250k, PlotQA: 224k). To address the costly process of\ndata curation, previous works have attempted to synthe-\nsize data for visualizing figures, leading to the develop-\nment of datasets like FigureQA (Kahou et al. 2018) and\nDVQA (Kafle et al. 2018). FigureQA uses fixed labels based\non bar color, while DVQA randomly selects words from vo-\ncabularies in its two dataset splits. However, this creates a\nbig gap from real-world figure data, as the data itself lacks\nmeaningful content due to the use of random words and fixed\nvocabularies.",
            "content": "Recently, ChartLlama (Han et al. 2023) 1 created figures by providing data topics and trends to GPT-4, resulting in the generation of 160k synthetic figures. Differently from this work, we generate figures synthetically from data topics, enabling us to create as many figures as needed. Additionally, we use pre-defined Python code to ensure error-free and efficient figure generation, enabling the large-scale production of synthetic figures. Synthetic figure QA Generating QA is the next step from generating figures. While precise human annotation (Masry et al. 2022) enables complex task evaluation, creating massive QA datasets is also important for developing strong models. Initially, FigureQA (Kahou et al. 2018) created template-based Yes/No questions. To more complicated QA pairs, fixed vocabulary template-based datasets have been developed (Chaudhry et al. 2020; Singh and Shekhar 2020; Kafle et al. 2018). LEAFQA 1 (Chaudhry et al. 2020) and LEAFQA++ 1 (Singh and Shekhar 2020) use 35 and 75 templates each, and DVQA uses 26 templates. While template-based QA reduces the need for extensive annotation in QA generation, more diverse methods for generating QA pairs have been explored. PlotQA (Methani et al. 2020) synthesizes open-vocabulary QA using 74 annotated template-based question generations. For more diverse QA generation without templates, ChartQA-Synth 1 was recently generated, adding 544k QA pairs to the ChartQA dataset (Masry et al. 2022) using LLMs (Carbune et al. 2024). Similarly, Li et al. created synthetic QA pairs for existing 312k images from the ChartQA dataset (Masry et al. 2022) with LLMs (Li et al. 2024). In contrast, our approach involves using LLMs to generate both the figures and the QA pairs from scratch, allowing us to greatly expand the dataset with large number of figures and corresponding presence data and QA pairs. Models for figure understanding Multi-modal models that combine visual and text modalities have been developed (Raffel et al. 2020; Cho et al. 2021; Kim et al. 2022; Lee et al. 2023). For robust visual language models (VLMs), pre-training on synthetic datasets is an effective strategy. For instance, Pix2Struct (Lee et al. 2023) trained ViT-based models by converting images to HTML representations, while Donut (Kim et al. 2022) utilized synthetic datasets to extract and read textual information from images during pretraining. These pre-training strategies enhance the models performance. Building VLM for Figure QA is well-researched area. These tasks involve both OCR and reasoning over data points, requiring specialized models. One effective way to build strong Figure QA model is by training it with figurerelated data. UniChart (Masry et al. 2023), building on Donut (Kim et al. 2022), is fine-tuned using figure-specific datasets to improve performance. Similarly, Matcha (Liu et al. 2023b) is figure-focused model pre-trained using figure-related tasks and builds on Pix2Struct (Lee et al. 2023). For the combination of LLM, Deplot (Liu et al. 2023a) used hybrid approach by converting figures into 1We dont compare with this because it is concurrent and the dataset is not openly available yet. Figure 2: Generation pipeline of SBS Figures. SBS Figures was created using fully synthetic method. First, we generate the visualization data, represented in JSON format, containing complete numbers, text, and colors. Next, we produce figure images from this data using pre-defined, error-free Python scripts. Finally, we generate dense and accurate QA pairs from visualization data without the need for OCR. JSON-style data for further reasoning with large language models (LLMs). Our proposed pipeline enhances this model training process by generating synthetic figures and their corresponding data points, providing tailored datasets for pre-training figure-specific VLM models."
        },
        {
            "title": "3.1 Data Generation\nHere, we describe how to generate data points that will be\nconverted to figures and QAs as shown in the left of Fig-",
            "content": "ure 2. We propose two-step data generation approach to enhance the diversity of the output. In the first step, data topic is generated, followed by the data content in the second step. Since the data topic acts as seed for generating the visualized data, this method allows for greater variability in the results. Data topic We generate various data topics to be visualized using an LLM. Specifically, we synthesize figure topics given chart type from the pre-defined commonly used ten figure types: Diverging bar chart, Vertical / Horizontal bar chart, Vertical / Horizontal grouped bar chart, Vertical / Horizontal stacked bar chart, Line chart, Scatter plot, and Pie chart (Figure 2 a). To cover diverse topics, we generate topics and construct over 100k unique topics for each chart type. We query the prompt shown in Figure 3 (data topic) multiple times and delete duplicates to ensure the variety of the data to be visualized. Data content We pre-defined the JSON format for each chart type. Given the data topics, we ask LLM to create figure data to illustrate each topic (Figure 2 b). We adopt JSON-style data representation as in previous works (Liu et al. 2023a), which can easily include title, axis, and corresponding colors along with data points. To ensure datastyle consistency, we adopt few-shot prompting using example JSONs for each chart type. To enhance the diversity of data styles, we create around 10 examples per chart type by varying the number of data points and data trends, then randomly select them for few-shot prompting. We finally obtain JSON files, which include the title, x, y-axis, data labels, data point numbers, and corresponding colors. This two-step Figure 3: Prompt templates used in the generation pipeline of SBS Figures. We adopt few-shot prompting to ensure consistent formatting for both JSON data and QA generation. To improve efficiency, our pipeline includes code that repeatedly adjusts the context and prompts during the generation process. data generation process ensures the diversity of topics in our dataset and consistency of the data formatting."
        },
        {
            "title": "3.2 Figure generation\nWe adopt a step-by-step data-to-figure rendering process be-\ncause LLMs often generate code with errors when generat-\ning data and code simultaneously.\nCode generation Firstly, we create one chart generation\ncode per chart type instead of having one code per JSON file.\nWe also pre-defined the JSON format for each chart type in\nthe data generation process, allowing us to create a generic\nfigure generation code that can handle any data following\nthe defined data structure. This approach avoids code errors\nand does not require LLM to generate code for each chart\nindividually, making the process more efficient and suitable\nfor large-scale figure generation. For the creation of the pre-\ndefined Python code (Figure 2 c), we use GPT-4 due to its\nsuperior coding capabilities than GPT-3.",
            "content": "Then, we feed the JSON data to the code to generate the figures (Figure 2 d). We randomized following components: Fonts: We randomly select fonts used in figures from seven commonly used types. We also use various font sizes. Title: We randomly position the title at the center, middle, or right. Additionally, since real-world figures often lack titles, we also randomly create no-title figures. Legend: We randomize the presence of legends if the chart type does not necessarily require legend information. We also randomly select the legend location from six positions. Marker: We randomly choose the marker style from nine types. Spine: We randomly select the presence of spines. Numbers: We randomly select whether to show the represented numbers. Convert data to the figure images We randomize the chart properties in the generation code to create variety of figures, e.g., title position, colors, fonts, marker style, the presence of gridlines, and whether to include numbers on the figures. This randomness results in around 2,000 unique combinations per chart type, greatly enhancing the variety in the appearance of the generated figures."
        },
        {
            "title": "3.4 Statistics\nWe finally obtained 1M images of SBS Figures. SBS Fig-\nures consists of 10 types of figures and includes 4.2M dense\nQA pairs with complete JSON format data, including title,\naxis, data, and colors. Each type of figure is generated with\naround 2,000 combinations of appearance variations, which\nare defined within the Python code. Our SBS Figures is free",
            "content": "Figure 4: Example of SBS Figures QA pairs. The figures show diverse visual variations, with each data content containing around 2,000 combinations of visual components. Additionally, our pipeline generates dense and precise QA pairs, requiring complex reasoning skills to address the questions. from copyright issues. We illustrate the example images and QAs in Figure 4. To show the distribution of our SBS Figures themes, we randomly selected 10 figures from each type and manually categorized them using two hierarchical levels. As shown in Figure 5, SBS Figures demonstrates wide range of themes. The largest category is business, covering wide range of topics such as marketing and finance. Although users can control the data topics in their prompts by specifying the topic or modifying the provided examples, our default prompt allows for the generation of figures on diverse topics."
        },
        {
            "title": "4.1 Experimental Settings\nDataset and Evaluation Here, we describe the dataset de-\ntails used in our experiments.",
            "content": "ChartQA (Masry et al. 2022) is standard and challenging dataset for evaluating document understanding models. This dataset contains two splits of QA pairs: 1) human split (human), which is annotated by humans, and 2) augmented split (aug.), which is created by the T5 (Raffel et al. 2020) model. The human split has 9.6k QA pairs, the augmented split has 23.1k pairs, and the total number of figure images is 21k. PlotQA (Methani et al. 2020) is synthesized dataset created by converting real-world data into figures. QA pairs are generated using templates created by human annotators. There are two splits of the dataset: v1, which has around 8M QA pairs, and v2, which is an extended version with 20M QA pairs. Both splits share the same set of figure images, totaling 224k images. FigureQA (Kahou et al. 2018) is synthesized dataset created using fixed vocabularies representing 100 colors. The QAs are generated using 15 templates, and the answer type is limited to yes (1) or no (0) questions. The dataset contains 2.3M QA pairs and 180k figure images. DVQA (Kafle et al. 2018) is synthesized dataset created using 1K fixed nouns. The QAs are generated using 26 templates, and the answers come from closed vocabularies. The dataset contains 3.4M QA pairs and total of 300k figure images. We dont compare with LEAFQA (Chaudhry et al. 2020), LEAFQA++ (Singh and Shekhar 2020), ChartQASynth (Carbune et al. 2024), and ChartLlama (Han et al. 2023), because they are not openly available yet. We primarily evaluate on the ChartQA dataset (Masry et al. 2022). Following previous works (Masry et al. 2023; Liu et al. 2023b), Dataset human aug. avg Scratch FigureQA (Kahou et al. 2018) DVQA (Kafle et al. 2018) PlotQA (Methani et al. 2020) SBS Figures (Ours) 31.28 13.44 26.88 30.56 39. 77.76 9.36 72.16 74.00 82.24 54.42 11.40 49.52 52.28 60.84 Table 1: Comparison of the pre-training effect of SBS Figures with other synthetic datasets. All datasets were trained using the Donut model. Model human aug. avg VisionTaPas (Masry et al. 2022) T5 (Raffel et al. 2020) VL-T5 (Cho et al. 2021) Donut (Kim et al. 2022) Donut+SBS Figures (Ours) Pix2Struct (Lee et al. 2023) Pix2Struct+SBS Figures (Ours) 29.60 25.12 26.24 31.28 39. 35.92 41.84 61.44 56.96 56.88 77.76 81.20 85.92 87.20 45.52 41.04 41.56 54.42 60. 60.92 64.52 Table 2: Comparison of the model pre-trained on our SBS Figures to other models. fine-tuning on the PlotQA (Methani et al. 2020) and FigureQA (Kahou et al. 2018) datasets, we train for 1 epoch on each training split and test on subset of 10k QA pairs from the test split."
        },
        {
            "title": "4.2 Main Results\nDataset Comparison We evaluate the pre-training effective-\nness of our SBS Figures compared to other synthetic fig-\nure QA datasets by conducting pre-training on each dataset\nand fine-tuning each model on ChartQA. 2 Pre-training was\nconducted for 3 epochs, except for PlotQA, which was pre-\ntrained for 1 epoch due to the difference in QA numbers. Af-\nter pre-training of each dataset, all models were fine-tuned\nfor 20 epochs for ChartQA. Pre-training and fine-tuning\nwere performed using the Donut (Kim et al. 2022) model.\nIn this context, ”scratch” refers to fine-tuning on ChartQA\ndataset without any pre-training, starting from the vanilla\nDonut-base weights. As shown in Table 1, only SBS Fig-\nures demonstrates the improvements by pre-training. Other\ndatasets, which contained only template-based QA, did not\nshow a pretraining effect. PlotQA, which is based on real-\nworld data, achieved superior results compared to the others.\nWe further assess the contributions of the components of our\nSBS Figures by investigation results section.\nModel Comparison We compare the model trained on our\nSBS Figures dataset with previous SOTA models that are\nnot pre-trained on real-world charts and have parameter\nsizes less than 1 billion, the same as ours. For the Donut\nand Pix2struct models, we show the re-implementation re-",
            "content": "2We dont compare with LeafQA, LeafQA++, and ChartLlama, because these datasets are not openly available yet. Figure 5: Theme distribution of SBS Figures. We randomly select 10 questions from each figure type and manually analyze the topic of the figure. experiments we use exact match accuracy with numerical tolerance for 5%. Model Our are mainly based on the Donut (Kim et al. 2022). We also use Pix2Struct (Lee et al. 2023) to show our dataset generality. Donut (Kim et al. 2022): an openly available OCR framework. The encoder is built on the Swin Transformer (Liu et al. 2021) architecture, and the text decoder is BART (Lewis et al. 2020). We initialized the model weight with the donut-base model. The model parameter count is 201M. implementations, we Pix2Struct (Lee et al. 2023): an image-encoder-textdecoder based on ViT (Dosovitskiy et al. 2021). This model is pre-trained by parsing masked screenshots of web pages into HTML. We initialized the model weight with the pix2struct-textcaps-base model. The model parameter count is 282M. the Donut model For follow UniChart (Masry et al. 2023) for hyperparameters. In the pre-training phase, we use batch size of 80 and learning rate of 1e-4. In the fine-tuning phase, we use batch size of 24 and learning rate of 5e-5. The input image resolution is set to 960 960. We apply cosine scheduler with warm-up step of 100. For the Pix2Struct model, the learning rate and batch size are the same as those of Donut. The only changes we make are adjusting the image resolution to 640 640 and decreasing the pre-training batch size to 40, due to the larger model size and higher computational resource requirements. For computational resources, we use A100 GPUs, with minimum of 4 GPUs and maximum of 8 GPUs, depending on the batch size. Excluding the noted parts, we conduct pre-training for 3 epochs for each dataset and fine-tune the model for 20 epochs on the ChartQA (Masry et al. 2022) dataset. For Randomize JSON QA Template Gemma GPT-3.5 GPT-4o human aug. 33.44 80.16 35.92 80.48 human aug. 31.44 79.12 35.92 80.48 human aug. 30.00 77.92 31.52 79.04 35.92 80. 34.56 81.84 Table 3: (F1) Appearance. Table 4: (F2) Pre-training task. Table 5: (F3) QA quality. <chartqa> <synthetic qa> <chartqa><synthetic qa> #Images 50k human aug. 35.92 80.48 34.72 79.20 33.12 79.68 human aug. 35.92 80.48 500k 36.48 81.12 1M 39.20 81.20 Table 6: (F4) Prompt. Table 7: (F5) Number of images. sult. Our SBS Figures shows the pre-training effect on both Donut and Pix2Srtruct models. Our SBS Figures improves performance on both human and augmented splits. The human split, which includes more complex questions and answers, shows particularly notable improvement. Chart domain QA typically relies on costly annotated datasets. Our pipeline-generated dataset improves model performance without real-world figures, demonstrating the effectiveness of synthetic data for pre-training in figure understanding tasks. We present qualitative comparisons for ChartQA finetuning between the Donut model (fine-tuned from donutbase model) and our model (the Donut model pre-trained with our SBS Figures) in Figure 6. This shows that our pretrained model can answer questions requiring multi-step reasoning."
        },
        {
            "title": "4.3\nWe conduct ablation studies to further investigate how our\nproposed pipeline impacts the model pre-training. In this\nphase, we used 50k images of our SBS Figures for efficient\ncomputational resources. We fine-tuned both the human and\naugmented (aug.) splits of the ChartQA training dataset, and\ntested on the Chart QA test split. Pre-training is conducted\nfor 3 epochs, and fine-tuning for 20 epochs. We investigate\nthe following five factors.\n(F1) Appearance. We investigate whether the variation in\nthe figure’s appearance enhances the pre-training impact. To\nexplore this, we compare figures created by: 1) our origi-\nnal proposed pipeline, which introduces randomness, such\nas fonts, text placement, and number presence, and 2) a mod-\nified pipeline that removes the augmentation phase, produc-\ning similar-looking figures. Both splits use the same JSON\ndata points for a fair comparison. As shown in Table 3,\nour proposed pipeline, which randomizes the component ap-\npearance of the figure, achieves superior results in both test\nsubsets. This indicates that a diverse range of figure struc-\ntures enhances the model’s generality and performance dur-\ning pre-training.\n(F2) Pre-training task. We investigate the effectiveness of\npre-training tasks by comparing QA-based pre-training with\nJSON parsing-based pre-training. For JSON parsing pre-\ntraining, we use annotated JSON generated during the data\ncreation stage, which includes all data points, axes, and color\ninformation. Then, we train the model to perform JSON-",
            "content": "style data extraction from the reference figure. As shown in Table 4, QA type pre-training outperforms JSON parsing. To answer the QA, the model needs to extract data points along with their corresponding colors and perform reasoning based on the extracted information. This complexity is potential reason for the superior pre-training effect. (F3) QA quality. We investigate how the quality of question-answer (QA) pairs affects model performance during pre-training. QAs are generated using two methods: (1) modified pipeline utilizing 26 predefined templates with various reasoning techniques, and (2, 3, 4) our original pipeline, which uses large language model (LLM) for few-shot prompting with sample QAs. For (2), we use the Gemma7b model (Team et al. 2024), an open-source model based on the same technology as Gemini. In (3), we use GPT3.5 Turbo, the same as our standard pipeline. For (4), we use GPT-4o mini, one of the latest models from OpenAI. In Table 4, the model trained on QAs generated by LLMs outperforms the one trained on template-based QAs in both test splits. GPT-3.5 Turbo achieves the highest scores on the ChartQA human split, while GPT-4o mini achieves the best result on the augmented split. These results suggest that leveraging LLMs, especially more advanced models, can significantly enhance the quality of QA pairs, leading to improved model performance. (F4) Prompt. We explore the necessity of different prompts during the pre-training phase of synthetic dataset. We used three types of input prompts: A: <chartqa>question <s answer>(same as in the fine-tuning phase), B: <synthetic qa>question <s answer>, and C: <chartqa><synthetic qa>question <s answer>. The same figure images and QA sets were used consistently throughout the three training processes. Table 6 shows that prompt achieved the best results, indicating that synthetic pretraining does not need special prompt, even for real-world chart fine-tuning. (F5) Number of images. We examine the effectiveness of the large number of images in the pre-training phase. Table 7 shows that increasing the number of images enhances accuracy in both test sets. This indicates that our proposed pipeline, which can generate large number of figures and QAs, plays crucial role in figure understanding pretraining. Furthermore, the continued improvement in accuracy as the number of images increases indicates the diverPlotQA FigureQA Pre-train V1 V1 V2 Scratch SBS Figures 65.40 73.15 26.16 42.48 52.04 84. 52.46 83.64 Table 8: Evaluation of the pre-training effect of SBS Figures on the PlotQA and FigureQA tasks. All pre-training and fine-tuning were conducted using the Donut model. Fine-tuning steps on UniChart QA Pre-train 10k 30k 50k Scratch SBS Figures 31.2078.96 40.0082.16 33.6081.12 40.4082.96 34.5678.82 41.3683.76 Table 9: Evaluation of the pretraining effect of our SBS Figures for the UniChart reasoning training based on steps. We evaluate on ChartQA dataset (humanaug.). many figure-specific models have trained on figure reasoning during pre-training (Masry et al. 2023; Liu et al. 2023b). Here, we evaluated whether SBS Figures remains effective for pre-training figure-specific models, which are trained on real-world figures. For this experiment, we selected UniChart QA, model trained on datasets of realworld charts. We conducted the training of the UniChart QA reasoning dataset in two ways: (1) training from scratch, and (2) using model pre-trained with SBS Figures. As shown in Table 9, starting with SBS Figures pre-training proves to be effective for training figure QA models. This suggests that pre-training with SBS Figures can efficiently enhance the performance of existing figure QA models."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced SBS Figures, synthesized dataset generated by our proposed pipeline. Our pipeline enables the creation of diverse topic figures with completely accurate presence data annotations and dense QA pairs without any manual annotation. The model pre-trained on our dataset demonstrates high pre-training effect for real-world figure datasets, allowing for efficient training. We make our model, dataset, and step-by-step generation pipelines code and prompts publicly available."
        },
        {
            "title": "6 Limitations",
            "content": "There is room for improvement by generating more than 1M images, which we havent tried due to computational resource constraints. Additionally, further hyper-parameter tuning for pre-training on synthesized figure data could enhance performance."
        },
        {
            "title": "7 Acknowledgment",
            "content": "This work was supported by JST Moonshot R&D Program, Grant Number JPMJMS2236. Figure 6: Qualitative Comparison. Our pre-trained Donut model on SBS Figures demonstrates its ability to answer complex reasoning questions. Incorrect answers are highlighted in red, while correct answers are highlighted in green. sity within our dataset, contributing to more robust model training."
        },
        {
            "title": "4.4 Explorative Study\nHere, we conduct further experiments. We conduct further\nexperiments to evaluate the generality of our SBS Figures in\nfine-tuning for other datasets. We also investigate whether\nSBS Figures still shows the pre-training effect with real-\nworld figure pre-training datasets. We use 1M images of\nSBS Figures .\nEffectiveness for other datasets. We further investigate the\neffectiveness of pre-training on our SBS Figures for fine-\ntuning to other datasets. For PlotQA, one million training\nQA pairs were extracted. A 5% relaxed accuracy is reported\nfor 10,000 examples from each of the v1 and v2 subsets of\nthe PlotQA test set as well as the FigureQA validation set.\nWe use Donut base model for both dataset fine-tuning. We\nconduct one epoch of fine-tuning for both datasets. Here,\n’scratch’ refers to starting the fine-tuning from the donut-\nbase model. As shown in Table 8, SBS Figures demon-\nstrates generality for fine-tuning datasets by showing the\npre-training effect for both datasets.\nPre-training effect for figure-specific models. Recently,",
            "content": "References Carbune, V.; Mansoor, H.; Liu, F.; Aralikatte, R.; Baechler, G.; Chen, J.; and Sharma, A. 2024. Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs. arXiv:2403.12596. Chaudhry, R.; Shekhar, S.; Gupta, U.; Maneriker, P.; Bansal, P.; and Joshi, A. 2020. LEAF-QA: Locate, Encode & Attend for Figure Question Answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). Cho, J.; Lei, J.; Tan, H.; and Bansal, M. 2021. Unifying In ProVision-and-Language Tasks via Text Generation. ceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, 19311942. PMLR. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR. Han, Y.; Zhang, C.; Chen, X.; Yang, X.; Wang, Z.; Yu, G.; Fu, B.; and Zhang, H. 2023. ChartLlama: Multimodal LLM for Chart Understanding and Generation. arXiv:2311.16483. Hoque, E.; Kavehzadeh, P.; and Masry, A. 2022. Chart question answering: State of the art and future directions. In Computer Graphics Forum, volume 41, 555572. Wiley Online Library. Kafle, K.; Price, B.; Cohen, S.; and Kanan, C. 2018. DVQA: Understanding Data Visualizations via Question Answering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 56485656. Kahou, S. E.; Michalski, V.; Atkinson, A.; Kadar, A.; Trischler, A.; and Bengio, Y. 2018. FigureQA: An Annotated Figure Dataset for Visual Reasoning. arXiv:1710.07300. Kim, G.; Hong, T.; Yim, M.; Nam, J.; Park, J.; Yim, J.; Hwang, W.; Yun, S.; Han, D.; and Park, S. 2022. OCRIn European Free Document Understanding Transformer. Conference on Computer Vision (ECCV). Lee, K.; Joshi, M.; Turc, I.; Hu, H.; Liu, F.; Eisenschlos, J.; Khandelwal, U.; Shaw, P.; Chang, M.-W.; and Toutanova, K. 2023. Pix2Struct: screenshot parsing as pretraining for visual language understanding. In Proceedings of International Conference on Machine Learning (ICML). Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.; Stoyanov, V.; and Zettlemoyer, L. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and CompreIn Proceedings of the Association for Computahension. tional Linguistics (ACL), 78717880. Online: Association for Computational Linguistics. Li, Z.; Jasani, B.; Tang, P.; and Ghadar, S. 2024. Synthesize Step-by-Step: Tools Templates and LLMs as Data GeneraIn Proceedings of tors for Reasoning-Based Chart VQA. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1361313623. Liu, F.; Eisenschlos, J.; Piccinno, F.; Krichene, S.; Pang, C.; Lee, K.; Joshi, M.; Chen, W.; Collier, N.; and Altun, Y. 2023a. DePlot: One-shot visual language reasoning by plot-to-table translation. In Findings of the Association for Computational Linguistics (ACL), 1038110399. Toronto, Canada: Association for Computational Linguistics. Liu, F.; Piccinno, F.; Krichene, S.; Pang, C.; Lee, K.; Joshi, M.; Altun, Y.; Collier, N.; and Eisenschlos, J. 2023b. MatCha: Enhancing Visual Language Pretraining with Math In Proceedings of Reasoning and Chart Derendering. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1275612770. Toronto, Canada: Association for Computational Linguistics. Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin Transformer: Hierarchical ViIn Proceedings sion Transformer using Shifted Windows. of the IEEE/CVF International Conference on Computer Vision (ICCV). Masry, A.; Kavehzadeh, P.; Do, X. L.; Hoque, E.; and Joty, S. 2023. UniChart: Universal Vision-language Pretrained In ProModel for Chart Comprehension and Reasoning. ceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 1466214684. Singapore: Association for Computational Linguistics. Masry, A.; Long, D.; Tan, J. Q.; Joty, S.; and Hoque, E. 2022. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In Findings of the Association for Computational Linguistics (ACL), 22632279. Dublin, Ireland. Methani, N.; Ganguly, P.; Khapra, M. M.; and Kumar, P. In The 2020. PlotQA: Reasoning over Scientific Plots. IEEE Winter Conference on Applications of Computer Vision (WACV). Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the Limits of Transfer Learning with Unified Text-toText Transformer. Journal of Machine Learning Research, 21(140): 167. Siegel, N.; Horvitz, Z.; Levin, R.; Divvala, S. K.; and Farhadi, A. 2016. FigureSeer: Parsing Result-Figures in Research Papers. In European Conference on Computer Vision. Singh, H.; and Shekhar, S. 2020. STL-CQA: Structurebased Transformers with Localization and Encoding for In Proceedings of the 2020 Chart Question Answering. Conference on Empirical Methods in Natural Language Processing (EMNLP), 32753284. Online: Association for Computational Linguistics. Team, G.; Mesnard, T.; Hardin, C.; Dadashi, R.; Bhupatiraju, S.; Pathak, S.; Sifre, L.; Rivi`ere, M.; Kale, M. S.; Love, J.; Tafti, P.; Hussenot, L.; Sessa, P. G.; Chowdhery, A.; Roberts, A.; Barua, A.; Botev, A.; Castro-Ros, A.; Slone, A.; Heliou, A.; Tacchetti, A.; Bulanova, A.; Paterson, A.; Tsai, B.; Shahriari, B.; Lan, C. L.; ChoquetteChoo, C. A.; Crepy, C.; Cer, D.; Ippolito, D.; Reid, D.; Buchatskaya, E.; Ni, E.; Noland, E.; Yan, G.; Tucker, G.; Muraru, G.-C.; Rozhdestvenskiy, G.; Michalewski, H.; Tenney, I.; Grishchenko, I.; Austin, J.; Keeling, J.; Labanowski, J.; Lespiau, J.-B.; Stanway, J.; Brennan, J.; Chen, J.; Ferret, J.; Chiu, J.; Mao-Jones, J.; Lee, K.; Yu, K.; Millican, K.; Sjoesund, L. L.; Lee, L.; Dixon, L.; Reid, M.; Mikuła, M.; Wirth, M.; Sharman, M.; Chinaev, N.; Thain, N.; Bachem, O.; Chang, O.; Wahltinez, O.; Bailey, P.; Michel, P.; Yotov, P.; Chaabouni, R.; Comanescu, R.; Jana, R.; Anil, R.; McIlroy, R.; Liu, R.; Mullins, R.; Smith, S. L.; Borgeaud, S.; Girgin, S.; Douglas, S.; Pandya, S.; Shakeri, S.; De, S.; Klimenko, T.; Hennigan, T.; Feinberg, V.; Stokowiec, W.; hui Chen, Y.; Ahmed, Z.; Gong, Z.; Warkentin, T.; Peran, L.; Giang, M.; Farabet, C.; Vinyals, O.; Dean, J.; Kavukcuoglu, K.; Hassabis, D.; Ghahramani, Z.; Eck, D.; Barral, J.; Pereira, F.; Collins, E.; Joulin, A.; Fiedel, N.; Senter, E.; Andreev, A.; and Kenealy, K. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv:2403.08295. Implementation details We provide the implementation details in the main paper. Here, we offer additional implementation specifics. Our work primarily utilizes the Donut model (Kim et al. 2022), as described in the main paper, and we also use Pix2Struct (Lee et al. 2023) to demonstrate the generality of our approach. The training details are as follows; Donut (Kim et al. 2022): Following the UniChart (Masry et al. 2023), which is based on Donut, we adopt the same hyper-parameters. The batch size is set to 80 during pretraining and 24 during fine-tuning. The learning rate is 5e-5 for pre-training and 1e-4 for fine-tuning. Additionally, the image resolution is fixed at 960 960. Pix2Struct (Lee et al. 2023): We adopt the same hyperparameters as UniChart (Masry et al. 2023), with two differences: the pre-training batch size is set to 40 due to the difference in model parameter size, and the image resolution is adjusted to 640 640. In Table 2, we present the re-implementation results for Donut and Pix2Struct, allowing fair comparison between training from scratch and using SBS Figures under the same settings. For VisionTaPas (Masry et al. 2022), T5 (Raffel et al. 2020), and VL-T5 (Cho et al. 2021), we report the official results from their respective publications. Dataset details We provide the dataset details in the main paper. In Table 10, we present the dataset details used for pre-training in the main paper for clearer understanding. The number of images in SBS Figures varies across experiments (e.g., 50k images in Tables 36, variable number in Table 7, and 1M images in the other tables). We use v1 split of PlotQA (Methani et al. 2020). We also present the statistics of our SBS Figures QAs. random sample of 100 QAs was extracted and manually analyzed based on their hierarchy of categories. As illustrated in Figure 7, our SBS Figures effectively generates diverse range of QAs. Additional figure images and examples of QAs are provided in Figure 8. Dataset FigureQA DVQA PlotQA SBS Figures (Ours) #Images 180k 300k 224k 50k - 1M 2.6k - 4.2M #QAs 2.3M 3.4M 8M Table 10: Comparison of the pre-training effect of SBS Figures with other synthetic datasets. All datasets were trained using the Donut model. Figure 7: QA distribution of SBS Figures. We randomly selected 100 QAs and manually analyzed their QA types. Figure 8: Examples of SBS Figures figure images and QA pairs."
        }
    ],
    "affiliations": [
        "Kyoto University, Japan",
        "OMRON SINIC Corp., Japan"
    ]
}