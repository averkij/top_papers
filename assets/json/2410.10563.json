{
    "paper_title": "MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks",
    "authors": [
        "Jiacheng Chen",
        "Tianhao Liang",
        "Sherman Siu",
        "Zhengqing Wang",
        "Kai Wang",
        "Yubo Wang",
        "Yuansheng Ni",
        "Wang Zhu",
        "Ziyan Jiang",
        "Bohan Lyu",
        "Dongfu Jiang",
        "Xuan He",
        "Yuan Liu",
        "Hexiang Hu",
        "Xiang Yue",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 2 3 6 5 0 1 . 0 1 4 2 : r Preprint. Work in progress. MEGA-BENCH EVALUATION TO OVER 500 REAL-WORLD TASKS : SCALING MULTIMODAL Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang Yue, Wenhu Chen MEGA-Bench Team https://tiger-ai-lab.github.io/MEGA-Bench/ Figure 1: MEGA-BENCH contains 505 multimodal tasks with diverse data sources, input/output formats, and skill requirements. The taxonomy tree guides and calibrates the annotation process."
        },
        {
            "title": "ABSTRACT",
            "content": "We present MEGA-BENCH, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for set of high-quality data samples that cover highly diverse and rich set of multimodal tasks, while enabling costeffective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace wide range of output formats like numbers, phrases, code, LATEX, coordinates, * Core Contributors, Contributed equally. See the Author Contribution Statement for details. (cid:0) jca348@sfu.ca; wenhuchen@uwaterloo.ca 1 Preprint. Work in progress. JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-BENCH offers fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate wide variety of frontier vision-language models on MEGA-BENCH to understand their capabilities across these dimensions."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large foundation models (OpenAI, 2023; 2024a; Anthropic, 2024a; Google, 2023; Meta, 2024; Alibaba, 2024) have dramatically transformed the landscape of artificial intelligence by showcasing exceptional capabilities across various tasks and domains. Originating in the realm of natural language processing, these models have progressively expanded to perceive and interpret multimodal information, including single images, multiple images, and videos. Previously, multimodal models were mainly used for standardized tasks like image captioning (Lin et al., 2014), video captioning (Wang et al., 2019), and visual question answering (Antol et al., 2015; Goyal et al., 2017; Xiao et al., 2021). With the recent progress on multimodal alignment, these models have shown great potential to solve any desired task with well-designed prompt. As result, people have applied them to assist with many realistic tasks like web navigation (Koh et al., 2024), game playing (Valevski et al., 2024), travel planning (Xie et al., 2024), visual navigation (Wang et al., 2023a), sports analysis (Xia et al., 2024), visual entity recognition (Hu et al., 2023), visual quality assessment (Ku et al., 2024), and more. These efforts have significantly increased the utility of multimodal models. An important challenge is identifying how to accurately gauge the abilities of these vision-language models (VLMs) across wide range of tasks. Most existing benchmarks are designed to cover only one or few similar tasks, making them inadequate for evaluating the models overall capabilities. The status quo is to evaluate the model on many existing benchmarks to showcase their all-round abilities. For example, Qwen2-VL1 was evaluated on 27 image and video benchmarks in total. Although this massive evaluation effort provides valuable insights into how well these models handle specialized tasks, it also introduces significant overhead and several challenges: - Limited Output Diversity: The existing multi-task benchmarks like MMMU (Yue et al., 2024a), MMT-Bench (Ying et al., 2024) rely heavily on multiple-choice questions to lower the burden of evaluation. This fails to evaluate the generative abilities of these multimodal models. - Lack of Task Coverage: The existing benchmarks are often sporadic and lack systematic design to cover the multimodal task space. Certain abilities are not well covered in the current ecosystem. Consequently, even exhaustively testing all the available benchmarks would not be sufficient. - Expensive Inference Cost: The full evaluation process is expensive in the computation cost, the time elapsed, and API price in $. Since many examples or tasks are highly similar in the capabilities that they assess, overly repetitive evaluation at large scale causes waste of resources. - Unmanageable Setups: Each benchmark has its own complexities when setting up the evaluation. For example, VQA (Goyal et al., 2017) has four splits including val, dev-test, std-test, and test. It is hard to track the exact setup of different baseline models to ensure fair comparison. To address these challenges, we advocate for unified protocol that scales up multi-modal evaluation to maximize the task coverage and the diversity in model outputs while optimizing the inference cost. As an initial attempt, we propose MEGA-BENCH, which is designed to provide comprehensive and systematic assessment of multimodal foundation models. To build MEGA-BENCH, we first construct task taxonomy tree that organizes different multimodal tasks based on the application type (Figure 1), with significant effort spent adjusting and refining the taxonomy tree to ensure sufficient coverage and diversity. The task taxonomy tree then serves as the guiding principle to ensure all relevant tasks and skills are covered and appropriately balanced. To help the annotators create their tasks, we build an annotation GUI to simplify the process of creating the task JSON files and web tool to visualize the results of the VLMs responses alongside the ground truth. We also review each task contribution when it is first submitted, after evaluating the models on the new tasks, and periodically throughout the annotation process to ensure that all of the 1 https://github.com/QwenLM/Qwen2-VL 2 Preprint. Work in progress. tasks are novel and high-quality. This collaborative effort resulted in the compilation of 505 realistic tasks, effectively covering (almost) the entire multimodal capability space at manageable inference cost. To facilitate nuanced and precise evaluation, we also developed 45 highly-customized metrics tailored to these tasks during the annotation process. Unlike existing benchmarks that often provide single score, MEGA-BENCH offers fine-grained capability report based on multiple dimensions such as the input type, input format, output format, and required skills. This interactive and visualizable report enables users to identify the models performance across several orthogonal dimensions, uncovering strengths and weaknesses that might be obscured in aggregate scores. Such detailed analysis is invaluable for researchers and developers aiming to enhance foundation models and optimize them for specific downstream applications. Based on the MEGA-BENCH, comprehensive studies on existing flagship and efficiency multimodal foundation models (covering most open-source software and proprietary model APIs) have discovered the following findings: 1. Among flagship models, Claude 3.5 Sonnet (1022) and GPT-4o (0513) currently lead in performance across wide range of multimodal tasks, with less than 0.1% difference in their overall scores. Detailed breakdown analyses show Claude 3.5 Sonnet excels in planning and math, while GPT-4o leads in information extraction and knowledge-intensive tasks. 2. Among open-sourced models, Qwen2-VL performs the best, with its performance near the top close-sourced flagship models, and outperforms the second best open-source model by 10%. 3. Among efficiency models, Gemini 1.5 Flash is the strongest model overall, except for the tasks related to handling User Interfaces and Documents. 4. Proprietary models can effectively leverage Chain-of-Thought (CoT) prompting to improve their performance, while open-source models hardly produce helpful reasoning processes. In our evaluation results, 11 out of 16 open-source models get worse results with CoT prompting."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multimodal benchmarks. Benchmarking in vision-language models has been long-standing research problem. Prior to large multimodal models, most benchmarks were focused on specific tasks or skills. Some benchmarks like VQA (Antol et al., 2015), GQA (Hudson & Manning, 2019), and ViswizVQA (Gurari et al., 2018) focus on photograph or natural images. ChartQA (Masry et al., 2022), InfoVQA (Mathew et al., 2022), DocVQA (Mathew et al., 2021), and OCR-VQA (Mishra et al., 2019) focus more on documents, infographics, and other similar media. Later on, there was trend to build more well-rounded benchmarks to cover wider range of skills including MMMU (Yue et al., 2024a), MMMU-Pro (Yue et al., 2024b), MMT-Bench (Ying et al., 2024), MMBench (Liu et al., 2023b), ScienceQA (Lu et al., 2022), and more. However, due to the diversity of these different tasks, most benchmarks simply use multiple-choice questions for all of the problems. Therefore, these benchmarks are not able to reflect the generational abilities of multimodal models. Complementary to this, LMsys arena (Chiang et al., 2024) and WildVision arena (Lu et al., 2024) have proposed to use user voting and Elo-ranking to benchmark multimodal models. Our benchmark is the first to scale up the tasks by signficant magnitude. Also, our benchmark provides breakdown report to show the models abilities across multiple dimensions. Sensitivity of large model leaderboards to input format. Creating reliable leaderboards poses substantial challenge for evaluating large models. Previous studies have noted that LLMs exhibit sensitivity to minor input modifications, including prompts and in-context examples in few-shot settings (Sclar et al., 2024; Chang & Jia, 2023). To mitigate input sensitivities, researchers have developed specialized prompt design and prompting-based training approaches (Liu et al., 2023a; Jain et al., 2024b). Nonetheless, for benchmarks that only allow multiple-choice format (Wang et al., 2024d), studies by Zheng et al. (2024) and Robinson et al. (2023) find the option sequencing can significantly alter model rankings on the leaderboard. Recently, Alzahrani et al. (2024) explores the advantage of hybrid scoring method to stabilize models leaderboard rankings over input format. Though MEGA-BENCH does not include hybrid scoring for each individual task, the overall use of diverse and hybrid scoring methods and output formats across more than 500 tasks demonstrates the robustness of the benchmark. 3 Preprint. Work in progress. Figure 2: MEGA-BENCHs four keyword dimensions and the task-level statistics. The diversity along various dimensions enables fine-grained capability analysis."
        },
        {
            "title": "3 MEGA-BENCH",
            "content": "MEGA-BENCH is comprehensive multimodal benchmark that spans 7 input formats, 6 output formats, 10 different types of skills, and varying number of visual inputs, whether single-image, multi-image, or from video, as shown in Figure 2. Our benchmark covers 8 distinct subject areas in hierarchical taxonomy to evaluate the ability of VLMs to tackle variety of different tasks. 3.1 BENCHMARK CONSTRUCTION PROCESS Preparation. Figure 3 illustrates our annotation process. In the conceptualization stage, we propose draft task taxonomy tree with the top two levels of Figure 1. The first level consists of the task scopes like Perception, Planning, Reasoning, etc. The second level consists of more concrete categories like Document Understanding, App Understanding, Logic Reasoning, and so forth. We add few exemplars under each second-level node and write detailed descriptions of the tasks we expect from the annotators. Figure 3: The annotation process of MEGA-BENCH. We first propose draft taxonomy tree, and then distribute the meta-nodes to different annotators. We allow the annotators to gradually refine the tree structure as they add new tasks. Each task consists of many examples and has shared tasklevel instruction, several per-example questions, and several per-question ground truth answers. We then distribute the second-level nodes in draft taxonomy tree to the annotators based on their preferences. This top-down annotation framework can help minimize the overlaps of tasks contributed by different annotators. To ensure consistent commitment throughout the project, we call up over 16 designated expert annotators. Our annotators are all graduate students or above from different majors like Computer Science, Electronics, Bio-statistics, and Finance. The annotators can 1) refine the draft taxonomy by adding/deleting nodes, 2) add task group nodes and then add series of tasks under that, and 3) directly add tasks under the second-level meta-task node. To simplify the task contribution, we develop tools to facilitate the annotation process, including 1) an interactive annotation tool that defines the annotation format and automatically unifies all annotations as JSON files (Please see B.1 for our annotation format), 2) GitHub repository to coordinate the task submission, reviewing, and discussion process, which was inspired by BIG-bench (Srivastava et al., 2022), and 3) visualization tool that allows annotators to browse the existing tasks and the evaluation results of representative vision language models (VLMs). We coordinate all the annotators to ensure they understand our expectations and continuously improve our tools. Preprint. Work in progress. Task annotation. The annotation process contains two rounds. The annotators submit tasks to the benchmark by creating pull requests (PR) to the main branch of our GitHub repository. In the first round of the annotation process, we ask the annotators to contribute 20 tasks following the principles below to ensure the quality of the task: Data source and answer format: Creative tasks with diverse data sources and answer formats are encouraged. If the data was collected from existing datasets, we ask annotators to adapt the original annotation into more specific questions and design more diverse answer formats. Number of examples: Each task should have at least 15 examples. Exceptions are allowed for some complicated tasks where the data are scarce. Documentation: Each task should be accompanied by documentation that indicates the source of the data, the capabilities the task tries to evaluate, and the evaluation metric to be used. Our core contributors review each PRs carefully to provide feedback, and the accepted PRs are merged into the main branch. We periodically run the evaluation with commercial VLMs (e.g., GPT4o) and update the results of existing tasks on our visualization page, which allows the annotators to better understand the difficulty of their tasks and catch potential glitches in the annotation. We found that this helps significantly improve the annotation quality. Before the second round of annotations, the core contributors gathered with the annotators to review all the contributed tasks in the taxonomy tree and discover the biases in the task distribution. We then host another brainstorming session to propose new meta-tasks to balance the distribution and maximize the coverage. We then distribute the new meta-tasks to our annotators in the second round and follow the same guidelines as before to finish the rest of the annotations. After this round, each annotator contributes at least another 10 tasks to meet the requirements (total 30). Quality control and refinement. We leverage state-of-the-art VLMs to help the annotators better examine the quality of the tasks. Concretely, we gather the results of GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro and compute an average score on each task. Tasks with almost 1.0 scores often have trivial questions (based on manual inspection) and can hardly distinguish the ability of different models. We ask the corresponding annotators to investigate and augment those tasks to increase the difficulty. For tasks with almost zero scores, the task reviewers audit them carefully and remove them if the zero score comes from an incorrect annotation or insufficient question context. Finally, the benchmark contains total 505 tasks with roughly 8,200 examples, which is large enough to minimize the sample variance within each high-level taxonomy node. Please refer to 4.3 for an in-depth analysis of the number of examples per task. 3.2 METRICS FOR ANSWERS IN DIVERSE OUTPUT FORMATS To properly evaluate the tasks with different output formats, we develop set of highly-customized evaluation metrics in parallel with the benchmark construction process (3.1). Figure 4 shows several examples of the model outputs along with the tasks associated metrics. When new tasks are submitted to our GitHub repository, we implement any new metrics specified by the task authors. We use two types of metrics: rule-based metrics and LLM-assisted metrics. Rule-based metrics. When there is unique answer under the question context or the correctness of the answer can be verified by rules (e.g., if the generated story/poetry meets the desired formats, or if the generated code can pass the given test cases), we implement rule-based metrics for evaluation. To satisfy the needs of all tasks submitted by our annotators, we end up with suite of over 40 rulebased metrics. Robust string parsing is also implemented to extract the answer from the models response. We conduct sanity check to ensure the correct implementation of all rule-based metrics. Specifically, we create an oracle model that always returns the ground truth annotation. We then compute its score over all tasks evaluated with rule-based metrics. The sanity check is passed when the oracle model gets 1.0 score. See D.4 for details. LLM-assisted metrics. For open-ended tasks that do not have unique answer, we instead employ an LLM-assisted metric (Zheng et al., 2023; Li et al., 2023a). We design per-task evaluation prompt template and fill in the tailored evaluation criteria for each task. The LLM is instructed to compare the model response with the reference answer and assign score from 1 to 10. The score is then normalized into [0, 1] to be consistent with the other metrics. See D.3 for details. 5 Preprint. Work in progress. Figure 4: Representative examples for MEGA-BENCHs diverse output formats and the corresponding customized metrics. The outputs are the real responses from GPT-4o (OpenAI, 2024a). We implement robust parsing to extract the final answer from raw responses. We divide the tasks into two subsets based on the different evaluation processes. The Core Set is evaluated with rule-based metrics to make the evaluation fast and cost-free. The Open-Ended Set is evaluated with metrics that use an LLM-as-a-judge, where the evaluation pipeline calls proprietary LLM over an API. Specifically, we use GPT-4o-0806 (OpenAI, 2024a) as the judge LLM. The Core and Open-Ended sets contain 440 and 65 tasks, respectively. 3.3 MULTI-DIMENSIONAL KEYWORDS FOR FINE-GRAINED ANALYSIS Existing multi-task multimodal benchmarks analyze models according to dimensions like the image type and academic discipline (Yue et al., 2024a), ability (Liu et al., 2023b), or meta-task (Ying et al., 2024). MEGA-BENCH offers broad and diverse range of coverage across all these dimensions, and extends even further beyond them. As explained in 3.1, the taxonomy tree divides the tasks into general application scenarios, the most manageable dimension for distributing the annotation efforts to different annotators. After we collected all tasks and finished the quality control process, we grouped all tasks based on four extra dimensions: input visual type, input visual number, output format, and required skills (Figure 2). Each dimension has 6 to 10 keywords, enabling fine-grained analysis and comparison. Interactive visualization tools can then be developed based on our evaluation results, which allows model developers to delve deep into different aspects of model, and compare different models comprehensively like how scout compares athletes. 3.4 DATASET STATISTICS AND COMPARISON WITH OTHER BENCHMARKS MEGA-BENCH contains 505 real-world tasks with 8,186 manually annotated or repurposed samples. Even for repurposed data, considerable effort is needed to convert the original annotations into specific task descriptions, diverse output formats, and additional instructions to include auxiliary information and about the formatting. Figure 2 shows the task distribution of all five dimensions, and the detailed breakdown statistics are in C.2. Table 1 compares MEGA-BENCH to existing multimodal benchmarks. The key feature of our benchmark is the diversity across all aspects in the table. MMMU (Yue et al., 2024a;b) focuses on 6 Preprint. Work in progress. Table 1: comparison between MEGA-BENCH and existing works. MEGA-BENCH has greater diversity in data sources, input/output format, the number of metrics, and the number of tasks. Dataset Annotation Source Input Output #Metrics #Tasks VQA-v2 (Antol et al., 2015) GQA (Hudson & Manning, 2019) VizwizVQA (Gurari et al., 2018) ChartQA (Masry et al., 2022) AI2D (Kembhavi et al., 2016) GeoQA (Chen et al., 2021) NLVR2 (Suhr & Artzi, 2019) InfoVQA (Mathew et al., 2022) DocVQA (Mathew et al., 2021) OCR-VQA (Mishra et al., 2019) MathVista (Lu et al., 2023) MMBench (Liu et al., 2023b) MME (Yin et al., 2023) MMStar (Chen et al., 2024a) MMVet (Yu et al., 2024b) ScienceQA (Lu et al., 2022) MMMU (Yue et al., 2024a) MUIRBench (Wang et al., 2024a) MileBench (Song et al., 2024) VideoMME (Fu et al., 2024a) MVBench (Li et al., 2024e) MMTBench (Ying et al., 2024) New New New New New New New New New New Phrase/Bool/Num Phrase/Bool/Num Phrase/Bool/Num Bool/Num Bool Phrase/Bool/Num Phrase/Bool/Num Phrase Free-form (FF) Photo Photo Photo Chart Diagram Geometry Photo Infographics Document Book covers 1 Image 1 Image 1 Image 1 Image 1 Image Multi-choice (MC) 1 Image Multi-choice (MC) 2 Images 1 Image 1 Image 1 Image 1 Image 1 Image Multi-choice (MC) 1 Image Multi-choice (MC) 1 Image Multi-choice (MC) 1 Image Multi-choice (MC) K12 Books 1 Image Multi-choice (MC) 1 Image Multi-choice (MC) > 1 Image Multi-choice (MC) > 1 Image Video Video MC or FF Multi-choice (MC) Multi-choice (MC) Image/Videos Multi-choice (MC) Repurposed Existing Repurposed Existing Repurposed Existing Repurposed Existing Repurposed Existing New New Hybrid Diverse Existing Repurposed Existing Youtube Repurposed Existing Repurposed Existing New 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 MEGA-BENCH New Unrestricted Image/Videos Unrestricted 45 1 1 1 1 1 1 1 1 1 1 31 20 14 18 16 26 30 12 28 30 20 162 college-level exam questions with various discipline and image formats, testing visual and textual fusion with advanced knowledge. All questions are single-image and answered in multiple-choice format. MMT-Bench (Ying et al., 2024) covers 162 concrete sub-tasks, enabling in-depth analysis based on their taskonomy and diverse input forms. However, all of the tasks MMT-Bench are from existing datasets and these tasks are mostly to under the Perception sub-tree in our taxonomy, and all outputs are in multiple-choice form like MMMU. To maximize task coverage and the diversity in model outputs with cost-effective inference, MEGA-BENCH includes much broader range of task types and output formats, while having fewer total samples compared to existing benchmarks."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate 22 VLMs with multi-image support on MEGA-BENCH. 4.1 describes the models being assessed and the evaluation pipeline. 4.2 presents the quantitative evaluation results with fine-grained analytical breakdown. 4.3 discusses the impact of the number of examples per task on the evaluation results. 4.4 provides error analysis based on GPT-4o. Detailed inspections of model behaviors with concrete error examples can be found in of the Appendix. 4.1 EVALUATION SETTINGS Evaluated models. We evaluate diverse range of large multimodal models. The proprietary models assessed include GPT-4o (0513) and GPT-4o mini (OpenAI, 2024a), Claude-3.5-Sonnet (0620 and 1022) (Anthropic, 2024a;b), Gemini-1.5-Pro (002) and Gemini-1.5-Flash (002) (Google, 2024a). For open-source models, we mainly focus on large flagship (>70B parameters) and small-to-medium efficiency models. The large models include Qwen2-VL-72B (Alibaba, 2024), InternVL2-Llama3-76B (Chen et al., 2024d), LLaVA-OneVision-72B (Li et al., 2024a), and NVLM (Dai et al., 2024). The medium-scale models comprise Qwen2-VL-(7B/2B) (Alibaba, 2024), Pixtral 12B (Mistral, 2024), Aria (Li et al., 2024d), InternVL2-(8B/2B) (Chen et al., 2024d), Phi3.5-Vision (Abdin et al., 2024), MiniCPM-V2.6 (Yao et al., 2024), LLaVA-OneVision-7B (Li et al., 2024a), Llama-3.2-11B Meta (2024), Idefics3-8B-Llama3 (Laurencon et al., 2024), and Aquila-VL2B-llava-qwen (Gu et al., 2024). 7 Preprint. Work in progress. Table 2: The main results of different models on the Core and Open-ended subset of MEGABENCH, with 440 and 65 tasks, respectively. We report the macro mean scores across all tasks in each set. The overall score is the weighted average of the Core and Open-ended scores. When computing the overall score, we use the higher Core score from w/o CoT and w/ CoT. Some open-source models perform more reasonably when the input contains only single image. Please see for results and analyses of our single-image setting. Model Eval Tier Open Source Core (rule eval) Open-ended (GPT eval) w/o CoT w/ CoT Overall Claude-3.5-Sonnet (1022) (Anthropic, 2024b) Flagship GPT-4o (0513) (OpenAI, 2024a) Flagship Claude-3.5-Sonnet (0620) (Anthropic, 2024a) Flagship Flagship Gemini-1.5-Pro-002 (Google, 2024b) Gemini-1.5-Flash-002 (Google, 2024b) GPT-4o mini (OpenAI, 2024b) Qwen2-VL-72B (Alibaba, 2024) InternVL2-Llama3-76B (Chen et al., 2024d) LLaVA-OneVision-72B (Li et al., 2024a) NVLM-72B (Dai et al., 2024) Efficiency Efficiency Flagship Flagship Flagship Flagship Efficiency Qwen2-VL-7B (Alibaba, 2024) Efficiency Pixtral-12B (Mistral, 2024) Efficiency Aria-MoE-25B (Li et al., 2024d) Efficiency InternVL2-8B (Chen et al., 2024d) Efficiency Phi-3.5-Vision-4B (Abdin et al., 2024) Efficiency MiniCPM-V2.6-8B (Yao et al., 2024) Efficiency LLaVA-OneVision-7B (Li et al., 2024a) Efficiency Qwen2-VL-2B (Alibaba, 2024) Efficiency Llama-3.2-11B (Meta, 2024) Efficiency Aquila-VL-2B-llava-qwen (Gu et al., 2024) InternVL2-2B (Chen et al., 2024d) Efficiency Idefics3-8B-Llama3 (Laurencon et al., 2024) Efficiency No No No No No No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 49.20 52.03 48.80 46.99 41.90 39.85 46.41 35.02 31.99 24. 34.80 31.91 30.49 25.96 23.27 22.88 22.41 16.45 10.04 16.32 9.09 11.12 52.59 52.65 50.41 48.22 41.89 40.77 45.42 35.63 29.74 21.59 32.93 31.36 28.90 24.09 23.00 22.96 21.36 20.88 16.00 16.00 13.14 8.96 65.63 64.78 63.74 58. 56.91 58.65 56.40 51.93 45.99 34.78 43.96 45.66 51.03 39.79 39.48 41.73 33.98 31.54 31.73 24.57 23.86 32.11 54.27 54.21 52.13 49.55 43.82 43.07 47.70 37.73 33.79 25. 35.98 33.68 33.13 27.74 25.36 25.38 23.90 22.25 18.02 17.38 14.52 13.82 Evaluation pipeline. MEGA-BENCH has diverse and flexible formats. To ensure the models have clear instructions on the output format, we provide all evaluated VLMs with one-shot in-context example. For each query, we fill in pre-defined prompt template with the task instructions written by the task annotators, the 1-shot example, and the concrete query question. Since this one-shot examples primary purpose is to illustrate the output format, we allocate only tiny portion of the total image budget for it. For each model, we conduct experiments with and without Chain-ofThought (CoT) prompting (Wei et al., 2022) for the Core tasks (the one-shot example of Open-ended tasks already contains CoT demonstrations). The prompt templates and other evaluation details (e.g., the frame sampling strategy for video inputs) are in D. Single-image setting. Our evaluation pipeline focuses on models with multi-image support. To properly evaluate models trained mainly for single-image use cases, we create single-image setting using the single-image tasks of MEGA-Bench. See for the detailed results and analyses. 4.2 MAIN RESULTS WITH BREAKDOWN ANALYSIS Table 2 presents the main evaluation results, with Figure 5 and Figure 6 being the accompanying fine-grained breakdowns enabled by MEGA-BENCHs multi-dimensional diversity. We discuss some important findings below and leave full breakdown of the results in of the Appendix. For the sake of careful comparison, we organize the results into two tiers: (1) The Flagship Model Tier compares the strongest performing models from each models organization, (believed) with #params 70B. (2) The Efficiency Model Tier compares efficiency models from each models organization, (believed) with #params 20B. Flagship models. Unlike the results on recent benchmarks like MMMU-Pro (Yue et al., 2024b) where GPT-4o (0513) and Claude-3.5-Sonnet (0620) get close scores, GPT-4o (0513) outperforms Claude-3.5-Sonnet (0620) with clear margin on MEGA-BENCH (> 2%). Investigating the break8 Preprint. Work in progress. Figure 5: Fine-grained breakdown analysis of flagship models on four dimensions. From top-left to bottom-right: input format, output format, skills, and application. down results, we observe that GPT-4o (0513) wins in most applications/skills except for coding, math, and planning-related tasks, where the answers are typically in structured output format, as shown in Figure 5. The recent update of Claude-3.5-Sonnet (1022) makes consistent improvements across almost all dimensions, especially in planning-related tasks and those with infographics/UI/photographs inputs, and slightly surpasses GPT-4o (0513) in the overall score (< 0.1%). The Planning application keyword contains tasks like symbolic planning (Zhu et al., 2024), navigation (Ku et al., 2020), chess games (Fu et al., 2024b), puzzle games (e.g., maze, Sudoku, etc.), and even the best models get low scores. One typical observation of Claude-3.5-Sonnet models is that they tend to be meticulous and refuse to answer routine knowledge or commonsense questions, such as the name and nationality of famous actors. The bottom radar maps show that they fall behind in knowledge, information extraction, and commonsense reasoning compared to GPT-4o, partially because of this refusal behavior. The evaluation results suggest that Qwen2-VL performs particularly well amongst open-source models of similar parameter sizes. In Figure 5, Qwen2-VL-72B gets similar score to closed-source models in the general perception category and outperforms Gemini-1.5-Pro-002 on information extraction tasks. Llava-OneVision-72B scores very low when the visual inputs are in UI related and Document formats while performing well on video inputs. This suggests lack of OCR and language parsing abilities, which can be confirmed with its skills radar plot. Efficiency models. Figure 6 analyzes the results on efficiency models. In general, Gemini-1.5flash-002 has the best performance with exceptional scores in Science and Metrics applications. The Metrics keyword contains tasks such as rating the quality of GenAI results (He et al., 2024; Jiang et al., 2024b) and requires deep multimodal reasoning and commonsense. However, its performance on UI-related inputs and information extraction tasks falls behind GPT-4o mini. Chain-of-Thought. An interesting finding is that the CoT prompt (See D) effectively guides all proprietary models to generate detailed reasoning process, and flagship-tier proprietary models all 9 Preprint. Work in progress. Figure 6: Fine-grained analysis of efficiency models on input format (left) and application (right). obtain better performance on the Core set. However, it has almost no effect on most open-source models. For example, the Qwen2-VL, InternVL2, and LLaVA-OneVision models rarely produce reasoning when given CoT instruction, and sometimes get confused about the required format after generating the reasoning process, leading to lower score on the Core set. Some open-source models get comparatively low scores for their parameter count. Llama-3.2-11B has difficulty leveraging the one-shot example to understand the correct output format and tends to generate long descriptive sentence instead. This issue is alleviated under the CoT setting because the prompt provides extra instructions on the output format beyond the one-shot example, requesting the model to strictly separate the reasoning process from the final answer. Idefics3 frequently repeats the example answer from the one-shot demonstration. We suspect the reason for this problem is the poor support for multi-image (as our query contains at least two images, including the one-shot example) since it can generate reasonable responses with single-image input. in the Appendix presents single-image setting of MEGA-BENCH and conducts further analyses. 4.3 ANALYSIS ON THE NUMBER OF SAMPLES PER TASK As discussed in 1, one of MEGA-BENCHs goals is to optimize the inference cost while still producing detailed multi-dimensional breakdown analysis. Therefore, we prioritize expanding the number of tasks over adding many examples per task in the benchmark construction process. To understand the robustness of the benchmark score with around 15 examples per task, we obtained bootstrap distributions (Efron & Tibshirani, 1994; Hesterberg, 2011) of the model scores for our Core set with the CoT prompting. We did this by taking random subset of the models responses of size (n = 3, 5, . . . , 13, 15) with replacement for each task and calculating the task-level macromean scores. To ensure the bootstrap distribution was numerically stable, we ran 10,000 Monte Carlo simulations. Figure 7 (left) shows that the variance in model scores rapidly narrows as the number of examples per task increases. As the number of examples per task increases beyond 7, the marginal return in variance reduction diminishes. 4.4 ERROR ANALYSIS To understand the limitations of state-of-the-art VLMs, we analyze the GPT-4o (0513) results by manually identifying the error types over subset of 255 tasks from the Core set. We use the CoT setting since the reasoning process helps determine the error type. Figure 7 (right) presents the error distribution. For GPT-4o, the lack of various reasoning capabilities (e.g., symbolic reasoning for planning/coding tasks, spatial or temporal reasoning for complex perception tasks, etc.) is the dominating failure mode on MEGA-BENCH. Please refer to for the full definition of error types and detailed example-wise inspection results with different models. Preprint. Work in progress. Figure 7: (Left) The bootstrap distribution of benchmark scores of subset of models as we gradually increase the bootstrap sample size of the number of examples per task. We use the results with the CoT prompting. Claude 3.5 Sonnet (1022) and Gemini 1.5 Pro (002) are used in this figure. (Right) The task-wise error distribution of GPT-4o (0513) over subset of 255 Core tasks."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper presents MEGA-BENCH, comprehensive benchmark that scales multimodal evaluation to over 500 real-world tasks but at manageable inference cost. By systematically organizing tasks across dimensions like skill, output format, and input type, we enable fine-grained analysis of multimodal models. Our evaluation of state-of-the-art VLMs revealed significant performance variations between models that previously seemed similar. MEGA-BENCH provides new standard for multimodal evaluation, offering robust analysis tool for model development."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. ArXiv preprint, abs/2404.14219, 2024. URL https://arxiv.org/abs/2404.14219. Alibaba. Qwen2-vl: To see the world more clearly. https://qwenlm.github.io/blog/qwen2-vl/ , 2024. Norah Alzahrani, Hisham Alyahya, Yazeed Alnumay, Sultan AlRashed, Shaykhah Alsubaie, Yousef Almushayqih, Faisal Mirza, Nouf Alotaibi, Nora Al-Twairesh, Areeb Alowisheq, Saiful Bari, and Haidar Khan. When benchmarks are targets: Revealing the sensitivity of large language model leaderboards. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1378713805. Association for Computational Linguistics, August 2024. Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024a. URL https://www.anthropic.com/news/claude-3-5-sonnet. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, 2024b. URL https://www.anthropic.com/news/3-5-models-and-computer-use. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: visual question answering. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 24252433. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.279. URL https://doi.org/10.1109/ ICCV.2015.279. 11 Preprint. Work in progress. Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, et al. Coig-cqia: Quality is all you need for chinese instruction fine-tuning, 2024. Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1162111631, 2020. Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. Ting-Yun Chang and Robin Jia. Data curation alone can stabilize in-context learning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 81238144. Association for Computational Linguistics, July 2023. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024a. Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, et al. Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai. arXiv preprint arXiv:2408.03361, 2024b. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. Tabfact: large-scale dataset for table-based fact verification. arXiv preprint arXiv:1909.02164, 2019. Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Guicourse: From general vision language models to versatile gui agents, 2024c. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. ArXiv preprint, abs/2404.16821, 2024d. URL https://arxiv.org/abs/2404.16821. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. Tai-Yin Chiu, Yinan Zhao, and Danna Gurari. Assessing image quality issues for real-world problems. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020. Francois Chollet. On the measure of intelligence, 2019. URL https://arxiv.org/abs/ 1911.01547. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint, 2024. 12 Preprint. Work in progress. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision (ECCV), pp. 720736, 2018. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 326335, 2017. Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, and Preslav Nakov. Exams-v: multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. arXiv preprint arXiv:2403.10378, 2024. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. Li Deng. The mnist database of handwritten digit images for machine learning research, 2012. Bradley Efron and Robert Tibshirani. An introduction to the bootstrap. Chapman and Hall/CRC, 1994. Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, and Xin Eric Wang. Muffin or chihuahua? challenging large vision-language models with multipanel vqa. arXiv preprint arXiv:2401.15847, 2024. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024a. Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. IsoBench: Benchmarking multimodal foundation models on isomorphic representations. In First Conference on Language Modeling (COLM), 2024b. First four authors contributed equally. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. ArXiv preprint, abs/2404.12390, 2024c. URL https://arxiv.org/abs/ 2404.12390. Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. arXiv preprint arXiv:2401.18085, 2024. Zahra Gharaee, ZeMing Gong, Nicholas Pellegrino, Iuliia Zarubiieva, Joakim Bruslund Haurum, Scott Lowe, Jaclyn McKeown, Chris Ho, Joschka McLeod, Yi-Yun Wei, et al. step towards worldwide biodiversity assessment: The bioscan-1m insect dataset. Advances in Neural Information Processing Systems, 36, 2024. Google. Gemini: family of highly capable multimodal models. ArXiv preprint, abs/2312.11805, 2023. URL https://arxiv.org/abs/2312.11805. and more. Google. Updated production-ready gemini models, reduced 1.5 pro pricing, increased rate https://developers.googleblog.com/en/updated-production-ready-geminiURL https: limits, models-reduced-15-pro-pricing-increased-rate-limits-and-more/, //developers.googleblog.com/en/updated-production-ready-geminimodels-reduced-15-pro-pricing-increased-rate-limits-and-more/. 2024a. Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024b. URL https://arxiv.org/abs/2403.05530. 13 Preprint. Work in progress. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 63256334. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.670. URL https://doi.org/10.1109/CVPR.2017.670. Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Infinity-mm: Scaling multimodal performance Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. Haisu Guan, Huanxin Yang, Xinyu Wang, Shengwei Han, Yongge Liu, Lianwen Jin, Xiang Bai, and Yuliang Liu. Deciphering oracle bone language with diffusion models, 2024. URL https: //arxiv.org/abs/2406.00684. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. Xuan He, Dongfu Jiang, Ge Zhang, Max W.F. Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, Kai Wang, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Yuchen Lin, and Wenhu Chen. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. ArXiv, abs/2406.15252, 2024. URL https://api.semanticscholar.org/CorpusID: 270688037. Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, and Patrick Schramowski. Llavaguard: Vlm-based safeguards for vision dataset curation and safety assessment. arXiv preprint arXiv:2406.05113, 2024. Jack Hessel, Ana Marasovic, Jena Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor understanding benchmarks from the new yorker caption contest. arXiv preprint arXiv:2209.06293, 2022. Tim Hesterberg. Bootstrap. Wiley Interdisciplinary Reviews: Computational Statistics, 3(6):497 526, 2011. Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo, and Xueqi Cheng. Visual reasoning: From state to transformation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):11352 11364, 2023. Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1206512075, 2023. Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 67006709. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00686. HuggingFaceM4. Docmatix dataset, 2024. URL https://huggingface.co/datasets/ HuggingFaceM4/Docmatix. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024a. URL https://arxiv.org/abs/ 2403.07974. Neel Jain, Ping yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. NEFTune: Noisy embeddings improve instruction finetuning. In The Twelfth International Conference on Learning Representations, 2024b. 14 Preprint. Work in progress. Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: dataset for form understanding in noisy scanned documents. In 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), volume 2, pp. 16. IEEE, 2019. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. ArXiv preprint, abs/2405.01483, 2024a. URL https://arxiv.org/abs/2405.01483. Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024b. Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, and Srijan Kumar. Mm-soc: Benchmarking multimodal large language models in social media platforms. ArXiv preprint, abs/2402.14154, 2024. URL https://arxiv.org/abs/2402.14154. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29012910, 2017. Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 56485656, 2018. Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251. Springer, 2016. Mohammad Abdullah Matin Khan, Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. xcodeeval: large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval, 2023. URL https://arxiv.org/abs/ 2303.03004. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. VisualWebArena: Evaluating multimodal agents on realistic visual web tasks. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 881905, Bangkok, Thailand, 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.50. Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the navgraph: Vision-and-language navigation in continuous environments, 2020. URL https:// arxiv.org/abs/2004.02857. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Conference on Empirical Methods for Natural Language Processing (EMNLP), 2020. Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. arXiv preprint arXiv:2310.01596, 2023. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. VIEScore: Towards exIn Lun-Wei Ku, Andre Martins, plainable metrics for conditional image synthesis evaluation. and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1226812290, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.663. URL https://aclanthology.org/2024.acl-long.663. 15 Preprint. Work in progress. Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 95104, 2018. Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding vision-language models: insights and future directions. ArXiv preprint, abs/2408.12637, 2024. URL https://arxiv.org/abs/2408.12637. Benjamin Charles Germain Lee, Jaime Mears, Eileen Jakeway, Meghan Ferriter, Chris Adams, Nathan Yarasavage, Deborah Thomas, Kate Zwaard, and Daniel Weld. The newspaper navigator dataset: Extracting headlines and visual content from 16 million historic newspaper pages in chronicling america. In Proceedings of the 29th ACM international conference on information & knowledge management, pp. 30553062, 2020. Jiyoung Lee, Seungryong Kim, Sunok Kim, Jungin Park, and Kwanghoon Sohn. Context-aware In Proceedings of the IEEE/CVF international conference on emotion recognition networks. computer vision, pp. 1014310152, 2019. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. ArXiv preprint, abs/2408.03326, 2024a. URL https://arxiv.org/abs/2408.03326. Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension, 2024b. URL https://arxiv.org/abs/2404.16790. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1329913308, 2024c. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024d. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024e. Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, and Qi Liu. Red teaming visual language models. arXiv preprint arXiv:2401.12915, 2024f. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca eval, 5 2023a. Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1496314973, 2023b. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? Conference on Language Modeling, 2024a. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9), January 2023a. ISSN 0360-0300. doi: 10.1145/3560815. URL https://doi.org/10.1145/3560815. 16 Preprint. Work in progress. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? ArXiv preprint, abs/2307.06281, 2023b. URL https://arxiv.org/abs/2307.06281. Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, and Jie Zhou. Points: Improving your vision-language model with affordable strategies. arXiv preprint arXiv:2409.04828, 2024b. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pp. 37303738, 2015. Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. ArXiv preprint, abs/2310.02255, 2023. URL https: //arxiv.org/abs/2310.02255. Yujie Lu, Dongfu Jiang, Wenhu Chen, William Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision arena: Benchmarking multimodal llms in the wild, February 2024. URL https: //huggingface.co/spaces/WildVision/vision-arena/. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv:2306.05424, 2023. Xiaofeng Mao, Yuefeng Chen, Yao Zhu, Da Chen, Hang Su, Rong Zhang, and Hui Xue. Coco-o: benchmark for object detectors under natural distribution shifts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 63396350, 2023. U-V Marti and Horst Bunke. full english sentence database for off-line handwriting recognition. In Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR99 (Cat. No. PR00318), pp. 705708. IEEE, 1999. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: bencharXiv preprint mark for question answering about charts with visual and logical reasoning. arXiv:2203.10244, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2021. Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV JawaIn Proceedings of the IEEE/CVF Winter Conference on Applications of Infographicvqa. har. Computer Vision, 2022. Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. URL https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/, 2024. https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edgemobile-devices/. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual In 2019 international conference on document question answering by reading text in images. analysis and recognition (ICDAR), pp. 947952. IEEE, 2019. Mistral. Announcing pixtral 12b. https://mistral.ai/news/pixtral-12b/, 2024. URL https:// mistral.ai/news/pixtral-12b/. Preprint. Work in progress. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, et al. Fetaqa: Free-form table question answering. Transactions of the Association for Computational Linguistics, 2022. Teng Niu, Shiai Zhu, Lei Pang, and Abdulmotaleb El-Saddik. Sentiment analysis on multi-view social data. In MultiMedia Modeling, pp. 1527, 2016. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. Docci: Descriptions of connected and contrasting images. arXiv preprint arXiv:2404.19753, 2024. OpenAI. Gpt-4v(ision) system card, 2023. URL https://cdn.openai.com/papers/ GPTV System Card.pdf. OpenAI. Hello gpt4-o. https://openai.com/index/hello-gpt-4o/, 2024a. URL https:// openai.com/index/hello-gpt-4o/. OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. https://openai.com/index/gpt-4o-miniadvancing-cost-efficient-intelligence/, 2024b. URL https://openai.com/index/gpt4o-mini-advancing-cost-efficient-intelligence/. Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, et al. Vibe-eval: hard evaluation suite for measuring progress of multimodal language models. arXiv preprint arXiv:2405.02287, 2024. Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305, 2015. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. Obioma Pelka, Sven Koitka, Johannes Ruckert, Felix Nensa, and Christoph Friedrich. Radiology objects in context (roco): multimodal image dataset. In Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3, pp. 180189. Springer, 2018. Sai Raj Kishore Perla, Yizhi Wang, Ali Mahdavi-Amiri, and Hao Zhang. Easi-tex: Edge-aware mesh texturing from single image. ACM Trans. Graph., 2024. Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, and Bernhard Scholkopf. Can large language models understand symbolic graphics programs?, 2024. URL https://arxiv.org/abs/ 2408.08313. Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. arXiv preprint arXiv:2407.06581, 2024. Jonathan Roberts, Kai Han, and Samuel Albanie. Satin: multi-task metadataset for classifying satellite imagery using vision-language models. arXiv preprint arXiv:2304.11619, 2023. Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language In The Eleventh International Conference on models for multiple choice question answering. Learning Representations, 2023. Stefan Romberg, Lluis Garcia Pueyo, Rainer Lienhart, and Roelof Van Zwol. Scalable logo recognition in real-world images. In Proceedings of the 1st ACM international conference on multimedia retrieval, pp. 18, 2011. 18 Preprint. Work in progress. David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. Cvqa: Culturally-diverse multilingual visual question answering benchmark. arXiv preprint arXiv:2406.05967, 2024. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models sensitivity to spurious features in prompt design or: How learned to start worrying about prompt formatting. In The Twelfth International Conference on Learning Representations, 2024. Silvia Sellan, Yun-Chun Chen, Ziyi Wu, Animesh Garg, and Alec Jacobson. Breaking bad: dataset for geometric fracture and reassembly. Advances in Neural Information Processing Systems, 35: 3888538898, 2022. Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua. Annotating objects and relations in user-generated videos. In Proceedings of the 2019 on International Conference on Multimedia Retrieval, pp. 279287. ACM, 2019. Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. arXiv preprint arXiv:2409.06662, 2024. Haojun Shi, Suyu Ye, Xinyu Fang, Chuanyang Jin, Layla Isik, Yen-Ling Kuo, and Tianmin Shu. Muma-tom: Multi-modal multi-agent theory of mind, 2024. URL https://arxiv.org/ abs/2408.12574. Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. arXiv preprint arXiv:2404.18532, 2024. Khurram Soomro, Amir Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. ArXiv, abs/1212.0402, 2012. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Alane Suhr and Yoav Artzi. Nlvr2 visual bias analysis. arXiv preprint arXiv:1909.10411, 2019. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. ArXiv preprint, abs/2406.16860, 2024. URL https://arxiv.org/abs/2406.16860. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multiimage understanding. ArXiv preprint, abs/2406.09411, 2024a. URL https://arxiv.org/ abs/2406.09411. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research, 2023a. Jing Wang, Weiqing Min, Sujuan Hou, Shengnan Ma, Yuanjie Zheng, Haishuai Wang, and Shuqiang Jiang. Logo-2k+: large-scale logo dataset for scalable logo classification. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020. 19 Preprint. Work in progress. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024b. Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, and Jing Ma. Mfc-bench: arXiv preprint Benchmarking multimodal fact-checking with large vision-language models. arXiv:2406.11288, 2024c. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023b. Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 45814591, 2019. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. ArXiv preprint, abs/2406.01574, 2024d. Zhengqing Wang, Jiacheng Chen, and Yasutaka Furukawa. Puzzlefusion++: Auto-agglomerative 3d fracture assembly by denoise and verify. arXiv preprint arXiv:2406.00259, 2024e. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. T. Weyand, A. Araujo, B. Cao, and J. Sim. Google Landmarks Dataset v2 - Large-Scale Benchmark for Instance-Level Recognition and Retrieval. In Proc. CVPR, 2020. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in neural information processing systems, 34:2241922430, 2021. Yang Wu, Shilong Wang, Hao Yang, Tian Zheng, Hongbo Zhang, Yanyan Zhao, and Bing Qin. An early evaluation of gpt-4v (ision). arXiv preprint arXiv:2310.16534, 2023. xAI. Grok-1.5 vision preview, 2024. URL https://x.ai/blog/grok-1.5v. Haotian Xia, Zhengbang Yang, Yun Zhao, Yuqing Wang, Jingxi Li, Rhys Tracy, Zhuangdi Zhu, Yuan-fang Wang, Hanjie Chen, and Weining Shen. Language and multimodal models in sports: survey of datasets and applications. arXiv preprint arXiv:2406.12252, 2024. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionIn Proceedings of the IEEE/CVF conference on answering to explaining temporal actions. computer vision and pattern recognition, pp. 97779786, 2021. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts, 2024. URL https://arxiv.org/abs/2407.04973. Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa: Towards surprising video comprehension. arXiv preprint arXiv:2306.14899, 2023. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents. In Forty-first International Conference on Machine Learning, 2024. Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: novel task for finegrained image understanding. arXiv preprint arXiv:1901.06706, 2019. Preprint. Work in progress. Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: face detection benchmark. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 55255533, 2016. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. ArXiv preprint, abs/2408.01800, 2024. URL https://arxiv.org/abs/2408.01800. Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. ArXiv preprint, abs/2306.13549, 2023. URL https: //arxiv.org/abs/2306.13549. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, and Dacheng Tao. Ap-10k: benchmark for animal pose estimation in the wild. arXiv preprint arXiv:2108.12617, 2021. Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024a. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-vet: Evaluating large multimodal models for integrated capabilities. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 5773057754. PMLR, 2024b. URL https://proceedings.mlr.press/v235/yu24o.html. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynetqa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019. Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, and Xiang Bai. arXiv preprint Syntax-aware network for handwritten mathematical expression recognition. arXiv:2203.01601, 2022. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, Hao Zhang, and Jindong Chen. Photochat: human-human dialogue dataset with photo sharing behavior for joint image-text modeling. arXiv preprint arXiv:2108.01453, 2021. Hanlei Zhang, Hua Xu, Xin Wang, Qianrui Zhou, Shaojie Zhao, and Jiayan Teng. Mintrec: In Proceedings of the 30th ACM International new dataset for multimodal intent recognition. Conference on Multimedia, pp. 16881697, 2022. Yiming Zhang, ZeMing Gong, and Angel Chang. Multi3drefer: Grounding text description to In Proceedings of the IEEE/CVF International Conference on Computer multiple 3d objects. Vision, pp. 1522515236, 2023. 21 Preprint. Work in progress. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/ abs/2306.05685. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):14521464, 2017. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, 2021. Wang Zhu, Ishika Singh, Robin Jia, and Jesse Thomason. Language models can infer action semantics for classical planners from environment feedback. arXiv preprint arXiv:2406.02791, 2024. 22 Preprint. Work in progress."
        },
        {
            "title": "Table of Contents in Appendix",
            "content": "A Single-image Setting: Results and Analyses Details of Annotation Protocols B.1 The unified annotation format . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 General task collection and creation guidelines . . . . . . . . . . . . . . . . . . . B.3 Tools for coordinating annotation and quality control . . . . . . . . . . . . . . . . Taxonomy Tree and Multi-dimensional Keywords C.1 Details of the Taxonomy Structure . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Statistics of each keyword dimension . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation Details D.1 Prompt template . . . D.2 Model query details . . . D.3 LLM-assisted metrics . D.4 Rule-based metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Answer extraction from model response . . . . . . . . . . . . . . . . . . . . . . . Complete Multi-dimensional Breakdown Results E.1 Breakdown results on the skill dimension . . . . . . . . . . . . . . . . . . . . . . E.2 Breakdown results on the input format dimension . . . . . . . . . . . . . . . . . . E.3 Breakdown results on the output format dimension . . . . . . . . . . . . . . . . . E.4 Breakdown results on the application dimension . . . . . . . . . . . . . . . . . . . E.5 Breakdown results on the visual input number dimension . . . . . . . . . . . . . . Detailed Inspection of Model Behaviours on MEGA-BENCH Detailed Task Information 24 26 26 28 29 29 33 35 35 36 37 38 40 41 42 43 44 45 Author Contribution Statement 111 23 Preprint. Work in progress. SINGLE-IMAGE SETTING: RESULTS AND ANALYSES Table 2 in the main paper focuses on models with multi-image support. However, some open-source models are only trained with single images. To provide feasible evaluation setting for these models, we create single-image (SI) setting using the single-image tasks in MEGA-BENCH, containing 273 and 42 tasks from the Core and Open-ended sets, respectively. Evaluation setup. The Chain-of-Thought (CoT) prompting is used for Core SI tasks. To make the entire query contain only one image as needed by some single-image models, we drop the image input of the 1-shot demonstration example ( demo im column in the table). In this case, the 1-shot example only demonstrates the output format, which is necessary for inferring the correct answer. For those models already evaluated in Table 2, we calculate the task-level average scores on single-image tasks to obtain the demo im results. Compared to Table 2, 3 single-image models are evaluated and added: Molmo-72B-0924 (Deitke et al., 2024), Molmo-7B-D-0924 (Deitke et al., 2024), and POINTS-Qwen2.5-7B (Liu et al., 2024b). Evaluation results. Table 3 presents the evaluation results of the SI setting. The Core and Openended scores of the standard setting (with CoT prompting) are also in the table for reference. Some observations from the table are listed below: Single-image tasks are easier than multi-image tasks in general, and all models get higher scores in the SI setting than in the standard setting. GPT-4o has the best overall SI score, slightly higher than Claude 3.5 Sonnet (1022). Interestingly, GPT-4o mini overtakes Gemini-1.5-Flash-002 under the SI setting, suggesting that Gemini1.5-Flash has pretty stable performance across different numbers of image inputs. NVLM-72B (Dai et al., 2024) has much better scores in the SI setting than in the standard setting, suggesting its training data might only contain single or few images. Comparing the demo im and demo im results of open-source models, the image input in the 1-shot demonstration example is not well utilized by the models to better understand the task logic. Including the image input in the demonstration example makes the results much worse for models like Llama-3.2-11B. More detailed breakdown results are available on our project page and the leaderboard (hosted with Hugging Face Spaces) 24 Preprint. Work in progress. Table 3: The single-image (SI) setting results of MEGA-BENCH. The Core set evaluation uses Chain-of-Thought (CoT) prompting. The demo img means the image input of the 1-shot demonstration example. The demo im directly takes the single-image subset average from the full results in Table 2. The demo im means the 1-shot demonstration example only demonstrates the output format, and the entire query has single image. We report demo im alone for the proprietary models because they have good multi-image support. For open-source models, we do additional evaluations with the demo im setting and use it to compute the overall score. Model Core Core SI demo im demo im Open Open SI demo im demo im Overall SI - - - - - - 47.31 39.99 36.48 32.99 31. 35.39 34.37 31.79 27.65 25.61 25.51 23.23 23.68 22.78 20.98 20.79 20.77 12.07 8.94 64.78 65.63 63.74 58.58 58.65 56.91 56.40 51.93 - 34.78 45.99 43.96 45.66 51.03 39.79 39.48 - 41.73 33.98 31.54 - 31.73 24.57 23.86 32.11 66.00 67.64 64.80 58. 59.56 57.87 58.50 55.33 - 48.67 46.12 45.87 44.03 50.92 40.94 44.61 - 42.54 36.44 30.59 - 34.29 24.58 24.28 33.25 - - - - - - 55.10 55.47 44.66 44.69 44. 45.17 44.17 51.37 39.39 42.72 30.32 43.61 38.71 35.09 35.70 38.61 31.47 28.52 32.31 56.73 56.36 53.73 50.34 46.32 45.40 48.34 42.05 37.58 34.55 32.99 36.69 35.68 34.40 29.21 27.89 26.15 25.95 25.69 24.43 22.95 23.17 22.20 14.26 12.06 GPT-4o (0513) (OpenAI, 2024a) 52.65 Claude-3.5-Sonnet (1022) (Anthropic, 2024b) 52.59 Claude-3.5-Sonnet (0620) (Anthropic, 2024a) 50.41 48.22 Gemini-1.5-Pro-002 (Google, 2024b) GPT-4o mini (OpenAI, 2024b) Gemini-1.5-Flash-002 (Google, 2024b) Qwen2-VL-72B (Alibaba, 2024) InternVL2-Llama3-76B (Chen et al., 2024d) Molmo-72B-0924 (Deitke et al., 2024) NVLM-72B (Dai et al., 2024) LLaVA-OneVision-72B (Li et al., 2024a) Qwen2-VL-7B (Alibaba, 2024) Pixtral-12B (Mistral, 2024) Aria-MoE-25B (Li et al., 2024d) InternVL2-8B (Chen et al., 2024d) Phi-3.5-Vision-4B (Abdin et al., 2024) POINTS-Qwen2.5-7B (Liu et al., 2024b) MiniCPM-V2.6-8B (Yao et al., 2024) LLaVA-OneVision-7B (Li et al., 2024a) Qwen2-VL-2B (Alibaba, 2024) Molmo-7B-D (Deitke et al., 2024) Llama-3.2-11B (Meta, 2024) Aquila-VL-2B-llava-qwen (Gu et al., 2024) InternVL2-2B (Chen et al., 2024d) Idefics3-8B-Llama3 (Laurencon et al., 2024) 40.77 41.89 45.42 35.63 - 21.59 29.74 32.93 31.36 28.90 24.09 23.00 - 22.96 21.36 20.88 - 16.00 16.00 13.14 8. 55.30 54.63 52.03 49.14 44.31 43.48 47.31 39.32 - 31.19 31.77 35.04 34.87 31.67 27.19 25.72 - 23.82 22.70 24.16 - 17.34 16.98 13.83 9.13 25 Preprint. Work in progress."
        },
        {
            "title": "B DETAILS OF ANNOTATION PROTOCOLS",
            "content": "This section presents additional details of our task annotation pipeline and protocols, providing complete details for 3.1 of the main paper. B.1 THE UNIFIED ANNOTATION FORMAT Figure 8 presents the annotation format designed and used in our annotation process. All annotated tasks share this unified structure, including task instruction, optional global media to provide context to all the questions (typically used in retrieval-related tasks). Additionally, each specific example contains distinct media path(s), concrete question, and an answer with single or multiple answer fields. Multi-field answers are organized as JSON structures. Figure 8: The structure of our task annotation format, which helps coordinate all task annotators and standardize the annotation format. Our evaluation pipeline follows this format to convert the task information into concrete queries and feed them to the evaluated model. Based on this format, we establish an interactive annotation tool to ensure the tasks submitted by all annotators have the correct and unified format. Figure 9 demonstrates the GUI of the annotation tool. B.2 GENERAL TASK COLLECTION AND CREATION GUIDELINES This subsection provides more detailed annotation guidelines for our annotators, complementing the descriptions in 3.1. Data source of each task. There is no restriction on the data source as long as the annotator follows the copyright and license requirements of the original data. Below are three typical task types and their data sources: (1). The task is designed entirely by the annotator, and the annotator looks for the image or video resources from the Internet or even using code/simulator; 26 Preprint. Work in progress. Figure 9: screenshot of our GUI annotation tool. (2). The task is inspired by existing benchmarks or datasets. The annotator collects the raw image/video data from existing datasets but does not use the original annotation. The annotator redesigns/repurposes the data by writing concrete task descriptions and creating new questions and answers, or using scripts to re-process the data for the designed task. (3). The task is directly converted from existing benchmarks or datasets. The annotator randomly samples subset from the existing benchmark, directly using its image/video and the annotation without redesign. In our annotation process, the first two task types are encouraged. The task reviewers strictly control the number of the third type and reject the task if an annotator submits many tasks of the third type. Table 18 shows the detailed data source of all tasks in MEGA-BENCH. Output format and answer uniqueness. We aim to cover diverse output formats in MEGABENCH. Therefore, we always require the task annotators to consider adapting the original datasets answer format, especially avoiding unnecessary multiple-choice questions (many MCQs are unnatural and mainly for evaluation convenience). Notably, the annotator must provide sufficient context in the task description and per-example question so that the range of the correct answer is manageable and the task can be evaluated with clearly defined metric. Metric specification. When creating task, the annotator must specify the corresponding evaluation metric. Since the metric implementation is in parallel to the task construction process, as described in 3.2, our GUI annotation tool (Figure 9) allows annotators to choose from existing metrics for each answer field of the task and assigns different weights to each field. When the desired metric is unavailable, the annotator chooses an unsupported metric type and writes down detailed metric specifications in the pull request. Our core contributors periodically check the needs of new metrics and implement them. Documentation. When submitting the pull request, the annotator must write README documentation for each task. If the desired metric has not been implemented, the documentation should contain the specification described in the last point. Furthermore, the doc should record the data source (e.g., the Web, an existing dataset, etc.) and brief descriptions of the task. These descriptions are instrumental in helping the core contributors assign various keywords to the task and creating Table 18 to show the details of all tasks. 27 Preprint. Work in progress. B.3 TOOLS FOR COORDINATING ANNOTATION AND QUALITY CONTROL As described in 3.1, we have two additional tools for coordinating the annotation process and maintaining the data. We present the details in this subsection. The GitHub repository for task organization. We created private GitHub repository for constructing MEGA-BENCH. The repositorys main branch is protected, and all task submissions must go through pull requests (PRs). The core contributors serve as the task reviewers and discuss with task annotators in the pull request forum to ensure the task conforms to our data collection guidelines (B.2). The code of our evaluation pipeline, including the model query and score computation, is maintained in the same repository. The core contributors submit pull requests to support different VLMs and add new evaluation metrics, and these PRs are cross-reviewed by other core contributors. We also actively use the repositorys Issues forum to report bugs in annotation or metric implementation so the corresponding contributors can get notified and work on the fix. At the end of the annotation process, our repository has 685 pull requests and 40 issues. 277 out of the 685 PRs are for task submission, indicating that many annotators submit task groups with more than one task in each PR. Other PRs are mainly for the evaluation pipeline and bug fixing. Figure 10: Illustrations of our task visualization page. Task visualization web page. We developed simple visualization web page and periodically synchronized the evaluation results of existing tasks on the page. The page provides several benefits: 1) it allows the core contributors to keep track of the overall annotation process, 2) it helps the annotators understand the capability of state-of-the-art VLMs, so that they can adjust the task difficulty accordingly, and 3) it facilitates the checking of the potential annotation glitches or metric bugs, significantly improving the overall quality of MEGA-BENCH. Figure 10 shows screenshots of the visualization page taken during the benchmark construction process. Note that the task names in the figure might not align with the final names in the paper. In our project page, we will provide similar visualization page for users to interactively inspect the behaviors of different VLMs. 28 Preprint. Work in progress. TAXONOMY TREE AND MULTI-DIMENSIONAL KEYWORDS This section presents the full details of our application-based taxonomy tree and the multidimensional keywords. C.1 DETAILS OF THE TAXONOMY STRUCTURE Table 4 shows the detailed structure of our application-driven task taxonomy. The first level defines the broad scope of use cases. At the second level, tasks are categorized into more specific domains. These first two levels guide the annotation process of our benchmark and are gradually updated/refined in the annotation process. The third level lists the concrete names of tasks or task groups. If the third-level node is task group, the number of concrete tasks under this group is shown in the parenthesis. Table 4: Details of taxonomy of MEGA-BENCH. Level-2 Tasks Leaf Tasks (at Level-3 or deeper) # Tasks Coding Code Debugging Code Generation Code Translation Stackoverflow Debug Qa, Code Error Line Identification Document Conversion (8 tasks), Programming Problems (4 tasks), Visualization With Code Code Translation Easy, Code Translation Python, Code Translation Hard, Code Translation Advanced Code Understanding Symbolic Graphics Programming (2 tasks), Webpage Code Understanding, Code Add Tag, Code Match (5 tasks), Code Output (3 tasks) App Function Understanding"
        },
        {
            "title": "Compound Search\nand Calculate",
            "content": "Detailed Manual Understanding Multimodal QA Information Extraction App Layout Understanding Leetcode, App Layout Understanding Youtube, App Layout Understanding Amazon, App Layout Understanding Word, App Layout Understanding Notes, App Layout Understanding Ppt, App Layout Understanding Alipay, App Layout Understanding Instagram, App Layout Understanding Zoom, App Layout Understanding Excel, App Layout Understanding Iphone Settings, App Layout Understanding Tiktok, App Layout Understanding Twitter Cheapest Flight Identification, Weather Info Retrieval, Stock Info Retrieval, Game Platform Support Identification, Top Rated Hotel Identification, Movie Info Retrieval, Top Video Creator Identification, Highest Discount Game Price Identification, Newspaper Page Parse And Count, Remaining Playback Time Calculation Multi Lingual Manual Explanation Scooter Spanish, Multi Lingual Manual Explanation Scooter Arabic, Multi Lingual Manual Explanation Scooter French, Multi Lingual Manual Explanation Scooter Chinese, Multi Lingual Manual Explanation Scooter Russian Multilingual News Qa, Product Ocr Qa, Large Image (3 tasks), Gui Chat (2 tasks), Realworld Qa En2cn, Star Object Interaction Video, Video Qa (7 tasks) 2 13 4 12 10 5 16 29 Preprint. Work in progress. Level-2 Tasks Leaf Tasks (at Level-3 or deeper) # Tasks Table 4 continued from previous page"
        },
        {
            "title": "Structured Parsing",
            "content": "8 16 Coco Ood Global Image Retrieval By Query Property, Places365 Similar Scene Retrieval, Booking Web Recommendation, Game Info Retrieval, Media Homepage Profile, Movie Retrieval By Actor, Music Info Retrieval, Tv Show Retrieval By Character Multilingual Movie Info Parsing, Movie Info Parsing, Stock Info Parsing, Music Info Parsing, Multilingual Game Info Parsing, Ocr Article Authors, Youtube Video Info Parsing, Tv Show Info Parsing, Ocr Resume School Plain, Image Translation En2cn, Booking Web Rating, Weather Info Parsing, Game Info Parsing, Weather Map Climate Type Temperature Parsing, Hotel Booking Confirmation Parsing, Entertainment Web Game Style Summarization Video Summary, Video Short Title, Video2notes, Video Content Reasoning 4 Arts Poetry Generation (7 tasks), Ascii Art 30 Knowledge Fact Checking Human and Culture World Knowledge Algebra Calculus Functions Background Change, Out Of Context, Text Entity Replace, Text Style, Face Attribute Edit, Face Swap, Interpret Force Perspective Illusion, Clip Stable Diffusion Generate, Unusual Images, Forensic Detection Of Different Images, Veracity, Distinguish Ai Generated Image Cultural Vqa, Human Relationship Reasoning, Sign Language, Ishihara Test, Safety And Norm (13 tasks), Video Content Follow Up, Emotion And Intent Understanding (9 tasks), Theory Of Minds (2 tasks), Hashtag Recommendation Dish Ingredient Match, Music (6 tasks), Insect Order Classification, Signage Navigation, Song Title Identification From Lyrics, Logo And Sign (3 tasks), Chinese Idiom Recognition, Ruozhiba (6 tasks), Font Recognition, Traffic Accident Analysis, Multiple State Identification (4 tasks), Worldle, Location Vqa, Daily (2 tasks), Ancient Map Understanding, Rocks Samples Compare, Painting (2 tasks), Memorization (4 tasks), Soccer Offside, Deciphering Oracle Bone, Actor Character And Famous People (3 tasks), Landmark And Buliding (3 tasks), Defeasible Reasoning Mathematics Algebra Scibench Calculus Wo Solution Math Parity, Math Breakpoint, Math Convexity Value Estimation General Math Exams V, Theoremqa, Math 8 12 30 47 1 3 3 30 Preprint. Work in progress. Level-2 Tasks Leaf Tasks (at Level-3 or deeper) # Tasks Table 4 continued from previous page 11 10 3 10 4 3"
        },
        {
            "title": "Graph Theory",
            "content": "Geometry Reasoning Count Line Intersections, Geometry Length, Geometry Reasoning Nested Squares, Geometry Transformation, Geometry Reasoning Overlapped Circle, Geometry Area, Geometry Reasoning Grid, Polygon Interior Angles, Geometry Solid, Geometry Analytic, Geometry Descriptive Graph Shortest Path Kamada Kawai, Graph Shortest Path Planar, Graph Connectivity, Graph Theory, Graph Isomorphism, Graph Hamiltonian Cycle, Graph Hamiltonian Path, Graph Chordless Cycle, Topological Sort, Graph Maxflow"
        },
        {
            "title": "Counterfactual Arithmetic",
            "content": "Numeric Reasoning Clevr Arithmetic, Iconqa Count And Reasoning, Number Comparison Metrics Generated Eval Image Autorater Artifact, Autorater Control, Autorater Artifact Reason, Autorater Aesthetics, Autorater Unmask, Autorater Subject, Autorater 3d Model Texturing, Autorater Semantics, Autorater Motion Guided Editing, Autorater Mask Generated Eval Video Video Eval Visual Pref, Generated Video Artifacts, Video Eval Factual Pref, Video Eval Dynamic Pref Paper Review Paper Review Writing, Paper Review Rating, Paper Review Acceptance Quality Assessment Vizwiz Quality Accessment For Blind Reward Models Reward Models T2i Reward, Reward Models I2t Reward 3D understanding Perception Adapted Cvbench Depth, Relative Depth Of Different Points, Visual Prediction Rater Depth Estimation, Visual Prediction Rater Novel View Synthesis, Pokemon 3d Recognition, Av View Identification, Multiview Reasoning Camera Moving, 3d Indoor Scene Text Bbox Prediction, Google Streetview Circle Reasoning, Google Streetview Direction Understanding, Video Motion Matching Real 3d, Video Motion Matching 3d Real, Visual Prediction Rater 3d Assembled Quality Understanding, Visual Prediction Rater Surface Normal Estimation, Visual Prediction Rater Plane Segmentation, 3d Indoor Scene Text Bbox Selection, Google Streetview Circle Sorting Counting Diagram and Document Understanding Ad Count Detection, Adapted Cvbench Count, Av Vehicle Multiview Counting, Counting Multi Image, Av Human Multiview Counting, Shape Composition Shapes, Counting Single Image, Clevrer Video Moving Object Count, Shape Composition Colours 9 Diagram (23 tasks), Document (9 tasks), Table Qa (6 tasks) 31 Preprint. Work in progress. Level-2 Tasks Leaf Tasks (at Level-3 or deeper) # Tasks Table 4 continued from previous page 3 5 10 10 10"
        },
        {
            "title": "Image\ntion",
            "content": "SegmentaVisual Prediction Rater Openable Part Segmentation, Visual Prediction Rater Panoptic Segmentation, Visual Prediction Rater Semantic Segmentation"
        },
        {
            "title": "Multimodal\ntioning",
            "content": "CapMultimodal Constrained Captioning"
        },
        {
            "title": "Object and Scene\nUnderstanding",
            "content": "Video Detail Description, Guess Image Generation Prompt, Docci Image Description Long, Tweets Captioning, Image Captioning With Additional Requirements Contain Contain Images, Contain Repeat Length, Multi Contain Repeat Position Only Length, Contain Length, Contain Position Images, Contain Position Length, Xor Images, Multi Contain Repeat, Contain Contain Length, Multi Contain Position Only Autonomous Driving Scene Analysis, Super Clevr Scene Understanding, Functionality Matching In Different Objects, Visual Dialog Image Guessing, Nlvr2 Two Image Compare Qa, Egocentric Analysis Single Image, Clevrer Object Existence Video, Snli Ve Visual Entailment, Ocr Open Ended Qa, Semantic Matching Of Two Images Physical standing UnderPhysical Reasoning (8 tasks), Lighting And Shading (2 tasks) Spatial Understanding Temporal Understanding Visual Recognition Adapted Cvbench Relation, Visual Correspondance In Two Images, 2d Image Jigsaw Puzzle Easy, Geometry Plot Position Relationship, Adapted Cvbench Distance, Video Grounding Spatial, Egocentric Spatial Reasoning Video To Camera Trajectory Retrieval, Sceneqa Scene Transition Video, Video Segments Reordering, Video Action Recognition, Action Sequence Understanding, Google Streetview Line Sorting, Next Action Prediction, Perception Test Video Action Count, Google Streetview Line Reasoning, Video Camera Motion Description, Video Grounding Temporal, Web Action Prediction, Cam Traj To Video Selection, Sta Action Localization Video Face Identity Matching, Rocks Samples Identify, Animal Pose Estimation, License Plate Recognition, Image Style Recognition, Long String Letter Recognition, Coco Object Detection By Query Property, Widerface Face Count And Event Classification, Handwritten Math Expression Extraction, Geometry Reasoning Circled Letter, Av Multicamera Tracking Predict Bbox, Ascii Art Understanding, Face Keypoint Detection, Extract Webpage Headline, Waldo, Geographic Remote Sensing Land Cover, Signboard Identification, Long String Number Recognition, Waybill Number Sequence Extraction, Single Person Pose Estimation, Coco Person Detection, Places365 Scene Type Classification Planning 32 Preprint. Work in progress. Level-2 Tasks Leaf Tasks (at Level-3 or deeper) # Tasks Table 4 continued from previous page Agents and Planning Wikihow Complex Task Completion, Navigation (6 tasks), Gui Operation (18 tasks), Calendar Schedule Suggestion, Symbolic Planning (13 tasks)"
        },
        {
            "title": "Puzzles and Games",
            "content": "Logical Reasoning Find Odd One Out, Logical Reasoning Fit Pattern, Perception Test Object Shuffle Video, Board Games (12 tasks), Bongard Problem, Number Puzzle Kakuro 5x5, Mensa Iq Test, Arc Agi, Mnist Pattern, Number Puzzle Sudoku, Move Pos To Pos Hanoi 4 Pole, Pictionary (5 tasks), Annoying Word Search, Logical Reasoning 2d Views Of 3d Shapes, Maze 2d 8x8, Crossword Mini 5x5, Rebus, Icon Arithmetic Puzzle, Iq Test Open Ended, Ball Cup Swap 3, Logical Reasoning 2d Folding"
        },
        {
            "title": "Reordering",
            "content": "Perception Test Video Character Order, Comic Page Ordering, Recipe Image Ordering 3 Chemistry Chemistry Exams V, Science Molecule Chemistry Life Sciences Biology Exams V, Medical (15 tasks) Science Physics STEM Circuit Diagram Understanding, Mmmu Physics Chemistry Selected, Science Basic Physics, Physics Exams Mmmu Pro Exam Screenshot, Scibench Solution Open Ended, Arxiv Vqa, Tqa Textbook Qa, Question Solution Solving, Quizlet Question Solving, Scibench Fundamental Wo Solution 16 4 7 C.2 STATISTICS OF EACH KEYWORD DIMENSION Figure 2 of the main paper presented the overall keyword distribution. As complement, Table 5 provides more detailed statistics. Each of the five dimensions contains multiple keywords, and for each keyword, we explicitly show the number of related tasks and the total number of samples. Preprint. Work in progress. Table 5: Number of tasks and samples across the five dimensions, with detailed breakdown into each keyword. Dimension Keywords (number of tasks, num of samples) Skills Object Recognition (303, 4755), OCR (137, 2239), Language Parsing & Gen. (154, 2509), Scene & Event Understanding (154, 2467), Math & Logical Reasoning (109, 1910), Commonsense & Social Reasoning (51, 855), Ethical & Safety Reasoning (15, 245), Domain-Specific Knowledge/Skills (77, 1387), Spatial & Temporal Reasoning (152, 2437), Planning & Decision Making (37, 577) Input Format User Interface (93, 1517), Text-rich Image & Doc (82, 1294), Diagrams & Visualizations (101, 1718), Videos (43, 698), Artistic & Creative (32, 542), Photographs (143, 2248), 3D Related (11, 169) Output Format Contextual Formatted (98, 1514), Structured (110, 1714), Exact (83, 1279), Numerical (49, 862), Open-ended (80, 1454), Multiple Choice (85, 1363) Input Number 6-8 images (21, 314), 9-image+ (41, 623), 1-image (315, 5228), Video (43, 698), 4-5 images (34, 520), 2-3 images (51, 802) Application Information Extraction (72, 1124), Planning (78, 1239), Coding (31, 474), Perception (145, 2313), Metrics (20, 309), Science (29, 574), Knowledge (97, 1605), Mathematics (33, 547) 34 Preprint. Work in progress."
        },
        {
            "title": "D EVALUATION DETAILS",
            "content": "This section details our evaluation settings, including the prompt template design, model query details, and evaluation metrics. Figure 11: The prompt template structure without Chain-of-Thought (CoT). D.1 PROMPT TEMPLATE We provide the concrete prompt template in Figure 11 and Figure 12. All the information organized by the prompt template is serialized by our evaluation pipeline before sending queries to the evaluated model. The non-CoT prompt instructs the VLM to strictly follow the one-shot example, directly producing the answer without additional text. In contrast, the CoT prompt instructs the VLM to output step-bystep reasoning before providing the final answer, and the model must strictly separate the reasoning process from the final answer. Note that our prompt sets different formats for single-field and multi-field outputs. Single-field answers must be explicitly indicated by the Answer: ... format so that our output parser can robustly locate and extract the models answer. Multi-field answers are in JSON format, and our JSON parser can robustly extract the JSON-style answer from the entire response without the Answer: ... format. D.2 MODEL QUERY DETAILS Since the evaluated VLMs have different context windows, we must tailor the number of query images or video frames for each model. We implement an image/video pre-processing pipeline that follows the settings listed in Table 6 to sub-sample the input images and videos. We allocate different budgets for in-context examples and the query. Since the in-context examples (we use 35 Preprint. Work in progress. Figure 12: The prompt template structure for the Chain-of-Thought (CoT) setting one-shot example) mainly help models understand the task logic and the output format, we reserve most of the image budget for the query. Images or video frames surpassing the budget are discarded. To make sure the open-source models can run smoothly, we implement fallback strategy, which reduces the image budget to decrease the number of input tokens if the models maximum context length is exceeded. For images or video frames with longer side larger than 1000 pixels, we resize the longer side to 1000 without changing the aspect ratio before sending them to the evaluated model. Each D.3 LLM-ASSISTED METRICS The LLM-assisted metric instructs multimodal LLM to evaluate VLMs response by providing detailed evaluation prompt. When submitting task with open-ended answers that cannot be evaluated by rule-based metrics, the annotator is asked to write down detailed evaluation prompt for the LLM judge following the prompt format in Figure 13. Concretely, the task annotator decides if the LLM judge should consider the questions visual input when evaluating the models response. If yes, then the query media (images or videos) will be passed to the LLM as well (we use GPT-4o-0806 as multimodal judge model). For most tasks, the LLM judge can do proper evaluation by comparing the models response with the reference answer, and the visual media is not needed. The task annotator also writes thorough evaluation criteria, explaining to the judge model the meaning of each score range, which is important to get reliable evaluation results. 36 Preprint. Work in progress. Table 6: The maximum number of images and the budget for the in-context example per model. Model Max # of images In-context example budget GPT-4o (0513) (OpenAI, 2024a) Claude-3.5-Sonnet (1022) (Anthropic, 2024a) Claude-3.5-Sonnet (0620) (Anthropic, 2024a) Gemini-1.5-Pro-002 (Google, 2024b) Gemini-1.5-Flash-002 (Google, 2024b) GPT-4o Mini (OpenAI, 2024b) Qwen2-VL-72B (Alibaba, 2024) InternVL2-Llama3-76B (Chen et al., 2024d) NVLM-72B Dai et al. (2024) Molmo-72B-0924 (Deitke et al., 2024) LLaVA-OneVision-72B (Li et al., 2024a) Qwen2-VL-7B (Alibaba, 2024) Pixtral-12B (Mistral, 2024) Aria-MoE-25B (Li et al., 2024d) POINTS-Qwen2.5-7B (Liu et al., 2024b) InternVL2-8B (Chen et al., 2024d) Phi-3.5-Vision (Abdin et al., 2024) MiniCPM-V2.6 (Yao et al., 2024) Molmo-7B-D (Deitke et al., 2024) LLaVA-OneVision-7B (Li et al., 2024a) Llama-3.2-11B (Meta, 2024) Idefics3-8B-Llama3 (Laurencon et al., 2024) Qwen2-VL-2B (Alibaba, 2024) InternVL2-2B (Chen et al., 2024d) Aquila-VL-2B-llava-qwen (Gu et al., 2024) 64 64 64 128 128 64 24 24 32 1 28 18 48 32 1 18 16 64 1 20 32 20 16 18 8 8 8 8 16 16 8 2 4 4 0 2 6 4 0 2 2 8 0 4 4 2 2 2 1 Figure 13: The prompt template structure for LLM-Assisted Metrics At the end of the prompt, pre-defined scoring format instruction is attached, ensuring the judge model outputs score between 1 and 10 and an explanation for the score. D.4 RULE-BASED METRICS We have over 40 highly customized rule-based metrics to evaluate the Core set of MEGA-BENCH. Basic metrics like extract string match and simple string match (which ignores punctuation and special characters) are first added to the supported metric set. New metrics are implemented when our task annotators submit new tasks requiring uncovered metrics. In the end, we get 45 customized tasks, as shown in Table 7. The usage distribution is long-tail because many metric implementations are triggered by single novel task. Preprint. Work in progress. Table 7: All metrics used in MEGA-BENCH. Metric Name Usage Count (# tasks) Exact String Match GPT-4o as Judge Simple String Match Multi Reference Phrase Evaluation Constrained Generation Set Equality Sequence Equality General Single Numerical Match Exact String Match Case Insensitive Sequence Accuracy Case Insensitive Symbolic Planning Test String Set Equality Comma Normalized RMSE Program Judge Set Precision Dictionary Equality String Set Equality Line Break Sequence Coordinates Similarity LaTeX Expression Equality Jaccard Index Case Insensitive Jaccard Index Normalized Bounding Box IOU Tuple Number Relative Difference Ratio XML Bounding Box IOU Dictionary Exact String Match Aggregate Recall Boxed Single Numerical Match Positive Integer Match Chess Move List Jaccard Index Code Result Exact String Match Normalized Bounding Box IOU Single Normalized Bounding Box IOU Sequence Normalized Similarity Damerau-Levenshtein Near String Match XML Normalized Point Distance Dictionary Precision Text with LaTeX Expression Equality Angle Sequence Float RMSE XML Normalized Point in Bounding Box Longest Common List Prefix Ratio Sequence Equality Case Insensitive Set Equality Case Insensitive GLEU (Chinese) ASCII Art GPT-4O Judge Dictionary Jaccard Aggregate Jaccard Dictionary Normalized Bounding Box IOU Tuple Aggregate Jaccard 198 64 61 25 18 15 15 14 14 13 13 9 8 8 5 4 4 3 3 3 3 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 D.5 ANSWER EXTRACTION FROM MODEL RESPONSE For Core tasks, our rule-based evaluation metrics compare the models answer with ground-truth answer or some ground-truth constraints. Therefore, an answer extraction step is necessary to separate the final answer from the reasoning process and other irrelevant texts. We implement robust extraction logic for different types of outputs based on the format specified in the prompt template: Single-field answer. We first reduce the answer by the Answer: ... pattern. If this pattern does not exist, we take the entire response. Since many VLMs do not strictly follow the format instructions, we have specific and extra processing for different output formats to improve robustness. Some typical examples are: 1) For multiple-choice outputs, we locate the exact letter or index choice using sophisticated regular expressions, which excludes any potential parenthesis or accompanying texts; 2) For code outputs, we extract the code from the potential code blocks; 3) For structured 38 Preprint. Work in progress. outputs, we parse the structural data into the proper Python data structures (list, set, dictionary, etc.), with tolerance on minor syntax errors (e.g., we automatically fix wrong quotes). Multi-field answer. Since the prompt requires the model to output the final answer in JSON format, we implement robust JSON parser to locate the JSON structure in the raw response and convert the JSON structure into the corresponding Python data structure. If our comprehensive answer extraction fails to obtain any meaningful final answer from the model response, we consider the model as fail to follow instructions. 39 Preprint. Work in progress. COMPLETE MULTI-DIMENSIONAL BREAKDOWN RESULTS This section provides the full breakdown results over the five dimensions of MEGA-BENCH, complementing section 4 of the main paper. E.1 BREAKDOWN RESULTS ON THE SKILL DIMENSION Table 8: Average scores for each model on the skill dimension. The best-performing model in each category is in-bold, and the second best is underlined. Model CASR DKAS EASR LUAG MALR ORAC PADM SAEU SATR TR Claude-3.5-Sonnet (1022) (Anthropic, 2024b) GPT-4o (0513) (OpenAI, 2024a) Claude-3.5-Sonnet (0620) (Anthropic, 2024a) Gemini-1.5-Pro-002 (Google, 2024b) Gemini-1.5-Flash-002 (Google, 2024b) GPT-4o mini (OpenAI, 2024b) Qwen2-VL-72B (Alibaba, 2024) InternVL2-Llama3-76B (Chen et al., 2024d) LLaVA-OneVision-72B (Li et al., 2024a) NVLM-72B (Dai et al., 2024) Qwen2-VL-7B (Alibaba, 2024) Pixtral 12B (Mistral, 2024) Aria-MoE-25B (Li et al., 2024d) InternVL2-8B (Chen et al., 2024d) Phi-3.5-Vision (Abdin et al., 2024) MiniCPM-V2.6 (Yao et al., 2024) LLaVA-OneVision-7B (Li et al., 2024a) Qwen2-VL-2B (Alibaba, 2024) Llama-3.2-11B (Meta, 2024) Aquila-VL-2B-llava-qwen (Gu et al., 2024) InternVL2-2B (Chen et al., 2024d) Idefics3-8B-Llama3 (Laurencon et al., 2024) 59.1 63.5 57.6 57.5 55.9 55.7 56.8 52.6 47.8 40.9 49.4 41.9 49.4 39.7 36.8 40.7 36.8 31.3 32.3 26.6 24.0 19. 54.9 55.1 52.8 51.4 44.8 41.9 46.3 33.3 31.7 25.8 33.3 32.8 32.8 27.1 24.1 23.7 24.5 20.8 17.7 18.6 14.8 17.9 65.7 68.0 69.7 69.8 63.8 69.0 60.5 57.8 60.1 45.6 52.2 56.9 58.1 47.0 46.7 48.8 45.0 41.4 42.6 35.2 34.2 28.6 60.8 61.6 57.5 55.3 49.9 51.7 53.9 43.7 36.7 29.4 40.3 38.3 40.0 32.0 28.7 30.0 25.6 25.7 19.6 17.9 16.9 17.3 48.9 44.2 47.7 42.6 34.4 34.1 37.8 29.8 29.5 26.4 28.2 28.3 27.6 24.1 21.7 18.3 19.0 17.6 13.3 16.8 13.9 13.3 56.9 56.3 54.1 52.0 46.3 44.9 49.8 38.2 36.2 24.0 37.1 34.6 32.6 28.2 25.5 26.0 25.2 22.2 19.1 18.4 14.5 14.5 29.1 22.9 23.8 23.9 19.0 19.4 22.0 17.0 13.9 6.7 14.7 10.6 11.9 8.3 8.9 8.7 6.7 6.2 6.6 4.5 1.7 4. 55.1 58.2 54.5 54.7 51.0 46.7 50.9 42.7 42.1 22.8 41.1 37.8 37.8 32.6 30.5 31.8 30.0 26.5 22.4 22.0 18.5 14.7 43.2 39.4 40.8 38.5 34.5 29.4 35.1 29.5 29.6 15.7 27.6 26.8 24.8 23.2 21.5 19.7 21.8 17.3 15.4 16.2 13.0 10.2 62.2 62.2 60.8 50.2 43.4 49.0 54.4 41.3 28.3 32.2 40.2 37.8 35.7 28.1 24.8 25.0 19.1 23.7 14.3 12.4 12.1 11.6 The abbreviations used in the table above are explained in the following table: Table 9: Abbreviation list of the keywords in the skill dimension. Abbreviation Skill CASR DKAS EASR LUAG MALR ORAC PADM SAEU SATR TR Commonsense and Social Reasoning Domain-Specific Knowledge and Skills Ethical and Safety Reasoning Language Understanding and Generation Mathematical and Logical Reasoning Object Recognition and Classification Planning and Decision Making Scene and Event Understanding Spatial and Temporal Reasoning Text Recognition (OCR) 40 Preprint. Work in progress. E.2 BREAKDOWN RESULTS ON THE INPUT FORMAT DIMENSION Table 10: Average scores for each model on the input format dimension. The best-performing model in each category is in-bold, and the second best is underlined. Model 3MAAI AACC DADV TIAD UIS Claude-3.5-Sonnet (1022) (Anthropic, 2024b) GPT-4o (0513) (OpenAI, 2024a) Claude-3.5-Sonnet (0620) (Anthropic, 2024a) Gemini-1.5-Pro-002 (Google, 2024b) Gemini-1.5-Flash-002 (Google, 2024b) GPT-4o mini (OpenAI, 2024b) Qwen2-VL-72B (Alibaba, 2024) InternVL2-Llama3-76B (Chen et al., 2024d) LLaVA-OneVision-72B (Li et al., 2024a) NVLM-72B (Dai et al., 2024) Qwen2-VL-7B (Alibaba, 2024) Pixtral 12B (Mistral, 2024) Aria-MoE-25B (Li et al., 2024d) InternVL2-8B (Chen et al., 2024d) Phi-3.5-Vision (Abdin et al., 2024) MiniCPM-V2.6 (Yao et al., 2024) LLaVA-OneVision-7B (Li et al., 2024a) Qwen2-VL-2B (Alibaba, 2024) Llama-3.2-11B (Meta, 2024) Aquila-VL-2B-llava-qwen (Gu et al., 2024) InternVL2-2B (Chen et al., 2024d) Idefics3-8B-Llama3 (Laurencon et al., 2024) 44.2 47.8 44.3 42.9 38.5 29.4 36.2 28.7 23.9 5.7 26.2 24.0 19.6 10.9 15.4 7.6 13.0 13.4 6.4 10.1 11.9 4.0 55.6 56.4 57.0 55.8 50.5 47.6 50.8 45.0 44.0 34.7 34.8 37.5 36.1 29.4 27.9 31.0 32.0 24.9 25.2 19.7 14.9 18.4 55.6 50.0 52.6 48.7 40.1 38.9 42.1 34.7 34.6 30.3 32.2 32.2 32.4 28.0 26.1 21.6 24.2 19.6 16.9 19.4 16.3 16.2 54.3 56.1 51.0 55.0 51.7 46.5 49.8 42.9 42.5 32.6 40.7 37.1 37.3 33.9 34.1 31.8 32.6 28.8 24.9 24.6 20.1 14.9 48.9 49.1 48.0 42.9 36.0 36.2 42.9 31.4 21.3 21.7 29.0 28.8 27.8 20.1 17.5 18.6 13.3 16.3 11.5 11.4 10.5 11. 60.5 60.8 56.9 46.3 38.7 47.2 54.0 36.3 23.4 23.9 38.2 30.7 28.3 22.8 18.7 21.2 14.7 19.1 11.9 7.5 5.7 10.1 49.5 53.2 50.9 50.3 49.0 45.5 49.9 39.6 44.5 0.0 41.1 41.0 42.9 34.8 24.7 35.3 31.0 25.2 21.2 21.4 19.0 16.2 The abbreviations used in the table above are explained in the following table: Table 11: Abbreviation list of the keywords in the input formats dimension. Abbreviation Input Format 3MAAI AACC DADV TIAD UIS 3D Models and Aerial Imagery Artistic and Creative Content Diagrams and Data Visualizations Photographs Text-Based Images and Documents User Interface Screenshots Videos 41 Preprint. Work in progress. E.3 BREAKDOWN RESULTS ON THE OUTPUT FORMAT DIMENSION Table 12: Average scores for each model on the output format dimension. The best-performing model in each category is in-bold, and the second best is underlined. Model Claude-3.5-Sonnet (1022) (Anthropic, 2024b) GPT-4o (0513) (OpenAI, 2024a) Claude-3.5-Sonnet (0620) (Anthropic, 2024a) Gemini-1.5-Pro-002 (Google, 2024b) Gemini-1.5-Flash-002 (Google, 2024b) GPT-4o mini (OpenAI, 2024b) Qwen2-VL-72B (Alibaba, 2024) InternVL2-Llama3-76B (Chen et al., 2024d) LLaVA-OneVision-72B (Li et al., 2024a) NVLM-72B (Dai et al., 2024) Qwen2-VL-7B (Alibaba, 2024) Pixtral 12B (Mistral, 2024) Aria-MoE-25B (Li et al., 2024d) InternVL2-8B (Chen et al., 2024d) Phi-3.5-Vision (Abdin et al., 2024) MiniCPM-V2.6 (Yao et al., 2024) LLaVA-OneVision-7B (Li et al., 2024a) Qwen2-VL-2B (Alibaba, 2024) Llama-3.2-11B (Meta, 2024) Aquila-VL-2B-llava-qwen (Gu et al., 2024) InternVL2-2B (Chen et al., 2024d) Idefics3-8B-Llama3 (Laurencon et al., 2024) 51.9 53.9 50.7 44.9 38.7 41.2 44.7 36.3 28.7 22.9 34.3 30.8 30.9 25.1 21.8 23.5 20.3 16.2 12.4 11.9 11.3 14.0 53.9 59.9 52.8 51.5 44.8 44.2 51.0 39.4 37.1 27.9 35.2 36.4 29.3 27.4 25.7 25.5 25.4 20.0 15.8 18.5 15.5 7. 57.8 54.5 54.6 55.4 47.8 39.9 52.0 38.8 39.9 18.5 39.9 30.1 32.8 30.3 26.0 29.3 28.0 25.7 19.3 22.1 21.3 11.6 48.2 44.6 44.9 46.9 37.0 36.3 40.3 29.2 30.7 23.3 32.7 32.1 30.9 22.4 21.4 20.8 22.0 22.0 15.0 19.9 16.0 9.8 62.4 62.7 58.4 55.8 54.5 57.1 51.6 45.8 42.9 32.2 39.1 41.7 45.2 35.4 36.5 36.5 31.3 30.2 30.0 23.3 21.4 29. 50.7 48.0 49.7 44.4 39.9 39.1 45.0 34.8 25.9 27.9 34.3 31.9 30.4 25.2 21.4 17.8 18.3 21.0 16.4 12.3 5.7 10.6 The abbreviations used in the table above are explained in the following table: Table 13: Abbreviation list of keywords in the output formats dimension. Abbreviation Output Format N Contextual Formatted Text Exact Text Multiple Choice Numerical Data Open-ended Output Structured Output 42 Preprint. Work in progress. E.4 BREAKDOWN RESULTS ON THE APPLICATION DIMENSION Table 14: Average scores for each model on the application dimension. The best-performing model in each category is in-bold, and the second best is underlined. Model Claude-3.5-Sonnet (1022) (Anthropic, 2024b) GPT-4o (0513) (OpenAI, 2024a) Claude-3.5-Sonnet (0620) (Anthropic, 2024a) Gemini-1.5-Pro-002 (Google, 2024b) Gemini-1.5-Flash-002 (Google, 2024b) GPT-4o mini (OpenAI, 2024b) Qwen2-VL-72B (Alibaba, 2024) InternVL2-Llama3-76B (Chen et al., 2024d) LLaVA-OneVision-72B (Li et al., 2024a) NVLM-72B (Dai et al., 2024) Qwen2-VL-7B (Alibaba, 2024) Pixtral 12B (Mistral, 2024) Aria-MoE-25B (Li et al., 2024d) InternVL2-8B (Chen et al., 2024d) Phi-3.5-Vision (Abdin et al., 2024) MiniCPM-V2.6 (Yao et al., 2024) LLaVA-OneVision-7B (Li et al., 2024a) Qwen2-VL-2B (Alibaba, 2024) Llama-3.2-11B (Meta, 2024) Aquila-VL-2B-llava-qwen (Gu et al., 2024) InternVL2-2B (Chen et al., 2024d) Idefics3-8B-Llama3 (Laurencon et al., 2024) 51.7 50.3 51.9 43.5 40.4 34.6 43.7 29.5 23.2 23.9 32.7 25.7 28.5 24.7 21.9 15.3 15.2 17.0 5.8 13.3 11.3 9.1 65.9 70.6 66.6 54.2 46.6 56.7 58.1 43.1 30.8 22.8 42.7 43.0 38.3 29.1 22.4 26.7 19.3 25.2 17.3 9.5 8.7 14.7 56.6 61.4 55.1 57.2 51.2 54.0 51.7 46.3 43.6 37.2 42.8 38.1 41.0 33.9 33.3 33.2 32.7 26.6 28.1 24.1 21.2 17.6 M2 47.6 44.0 47.5 41.2 33.7 32.9 31.2 28.7 31.6 24.5 25.6 24.2 26.2 22.1 17.6 16.5 22.1 16.4 13.9 20.7 11.0 13.2 61.2 61.0 58.1 58.2 60.1 51.8 49.7 47.4 48.1 18.9 42.5 50.2 39.7 40.0 39.5 37.8 36.0 31.0 25.4 29.3 33.3 14.6 55.6 55.1 53.2 52.5 48.0 43.6 53.6 42.2 38.4 30.2 40.0 38.9 37.8 32.1 31.6 29.2 28.5 27.6 19.9 20.7 17.0 14. P2 39.9 33.2 33.8 33.4 25.2 24.2 31.2 21.3 18.2 8.0 20.0 13.6 14.3 12.2 8.9 11.7 9.8 7.0 8.1 5.9 4.1 5.4 55.1 52.8 51.3 51.2 45.7 35.5 44.9 30.0 31.7 24.9 29.9 31.3 29.7 24.6 21.9 25.7 23.7 21.1 16.3 21.1 16.9 22.7 The abbreviations used in the table above are explained in the following table: Table 15: Abbreviation list of keywords in the applications dimension ."
        },
        {
            "title": "Abbreviation Application",
            "content": "C M2 P2 Coding Information-Extraction Knowledge Mathematics Metrics Perception Planning Science 43 Preprint. Work in progress. E.5 BREAKDOWN RESULTS ON THE VISUAL INPUT NUMBER DIMENSION Table 16: Average scores for each model on the visual input number dimension. The bestperforming model in each category is in-bold, and the second best is underlined. Model Claude-3.5-Sonnet (1022) (Anthropic, 2024b) GPT-4o (0513) (OpenAI, 2024a) Claude-3.5-Sonnet (0620) (Anthropic, 2024a) Gemini-1.5-Pro-002 (Google, 2024b) Gemini-1.5-Flash-002 (Google, 2024b) GPT-4o mini (OpenAI, 2024b) Qwen2-VL-72B (Alibaba, 2024) InternVL2-Llama3-76B (Chen et al., 2024d) LLaVA-OneVision-72B (Li et al., 2024a) NVLM-72B (Dai et al., 2024) Qwen2-VL-7B (Alibaba, 2024) Pixtral 12B (Mistral, 2024) Aria-MoE-25B (Li et al., 2024d) InternVL2-8B (Chen et al., 2024d) Phi-3.5-Vision (Abdin et al., 2024) MiniCPM-V2.6 (Yao et al., 2024) LLaVA-OneVision-7B (Li et al., 2024a) Qwen2-VL-2B (Alibaba, 2024) Llama-3.2-11B (Meta, 2024) Aquila-VL-2B-llava-qwen (Gu et al., 2024) InternVL2-2B (Chen et al., 2024d) Idefics3-8B-Llama3 (Laurencon et al., 2024) 1 56.4 56.7 53.7 50.3 44.3 46.3 49.2 41.5 34.8 36.8 37.7 37.1 35.8 30.1 27.8 26.3 25.5 25.0 19.6 18.2 15.2 14.8 2I 48.8 49.1 49.3 45.5 42.0 37.0 45.2 31.5 34.2 23.3 33.0 31.0 27.3 25.3 28.5 22.3 24.1 21.3 18.6 23.3 15.8 12. 4I 48.3 45.0 44.2 48.9 42.3 24.7 36.7 24.4 25.0 3.8 26.4 25.8 19.8 17.7 20.2 17.9 17.8 17.4 13.5 19.0 17.7 12.2 6I 9OM 46.3 47.5 46.3 39.1 33.7 33.6 31.0 20.3 20.7 0.0 19.4 19.7 21.1 15.4 12.5 14.0 14.8 7.7 14.6 11.1 3.7 10.1 59.1 53.4 54.1 53.7 43.7 43.1 54.7 34.8 28.1 0.0 37.5 16.6 27.1 19.9 14.3 23.6 13.8 10.5 7.3 1.2 5.8 9. 49.5 53.2 50.9 50.3 49.0 45.5 49.9 39.6 44.5 0.0 41.1 41.0 42.9 34.8 24.7 35.3 31.0 25.2 21.2 21.4 19.0 16.2 The abbreviations used in the table above are explained in the following table: Table 17: Abbreviation list of keywords in the visual input number dimension."
        },
        {
            "title": "Input Number",
            "content": "1 2I 4I 6I 9OM 1-image 2-3 images 4-5 images 6-8 images 9-image or more video 44 Preprint. Work in progress. DETAILED INSPECTION OF MODEL BEHAVIOURS ON MEGA-BENCH To complement 4.4 of the main paper, this section presents case study analysis of the error types of different models on different tasks in MEGA-BENCH. We use similar error categories as in MMMU (Yue et al., 2024a) and MMT-Bench (Ying et al., 2024): Perception Error: VLMs fail to recognize or perceive the content of interest in the query image(s). Perception errors indicate the Lack of Knowledge: VLMs lack the domain-specific knowledge to answer specialized questions, such as identifying the taxonomic order of an insect. Lack of (Reasoning) Capability: VLMs lack the necessary capabilities to solve the task, mainly related to various reasoning abilities, such as logical reasoning, counting, spatial or temporal reasoning, symbolic reasoning for code or various programs, and so on. This is broad type that covers many errors. One typical case for this error type is that the models can accurately follow instructions and perceive the visual inputs but struggle with the required reasoning process, leading to incorrect answers. Refuse to Answer: VLMs refuse to answer questions that they believe to involve sensitive content. Fail to Follow Instructions: VLMs fail to correctly understand instructions and provide wrong answers. The tasks in MEGA-BENCH usually have more instructions on the answer format compared to previous benchmarks. typical error pattern is not comprehending the required format, thus providing answers with incorrect formats or generating irrelevant responses. This error type is much more common in open-source models. Figure 14 to Figure 33 shows the case study for samples from different tasks. We use distinct colors to highlight the tags in each task sample. We borrow the error case analysis template from MMMU (Yue et al., 2024a) while adding the keywords information of MEGA-BENCH. We mainly focus on the flagship proprietary models. The Claude-3.5 in these figures refers to the Claude-3.5Sonnet (0620) model."
        },
        {
            "title": "14 Coding - Code Debugging: Error Case . . . . . . . . . . . . . . . . . . . . . . . .\n15 Coding - Code Understanding: Error Case 1 . . . . . . . . . . . . . . . . . . . . .\n16 Coding - Code Understanding: Error Case 2 . . . . . . . . . . . . . . . . . . . . .\nInformation Extraction - Multimodel QA: Error Case . . . . . . . . . . . . . . . .\n17\nInformation Extraction - Detailed Manual Understanding: Error Case\n18\n. . . . . . .\n19\nInformation Extraction - Search by Attribute without Calculate: Error Case . . . .\n20 Knowledge - World Knowledge: Error Case 1 . . . . . . . . . . . . . . . . . . . .\n21 Knowledge - World Knowledge: Error Case 2 . . . . . . . . . . . . . . . . . . . .\n22 Knowledge - Art: Error Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23 Mathematics - Graph Theory: Error Case\n. . . . . . . . . . . . . . . . . . . . . .\n24 Metrics - Generated Image Eval: Error Case 1 . . . . . . . . . . . . . . . . . . . .\n25 Metrics - Generated Image Eval: Error Case 2 . . . . . . . . . . . . . . . . . . . .\nPlanning - Puzzles and Games: Error Case 1 . . . . . . . . . . . . . . . . . . . . .\n26\nPlanning - Puzzles and Games: Error Case 2 . . . . . . . . . . . . . . . . . . . . .\n27\nPlanning - Reordering: Error Case . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nPerception - Multimodal Constrained Captioning: Error Case . . . . . . . . . . . .\n29\nPerception - Visual Recognition: Error Case 1 . . . . . . . . . . . . . . . . . . . .\n30\nPerception - Visual Recognition: Error Case 2 . . . . . . . . . . . . . . . . . . . .\n31\nPerception - Visual Recognition: Error Case 3 . . . . . . . . . . . . . . . . . . . .\n32\nScience - STEM: Error Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33",
            "content": "47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 46 Preprint. Work in progress. Figure 14: sample error case of Coding (subfield: Code Debugging). Source:Web Back to List of Figures Preprint. Work in progress. Figure 15: sample error case of Coding (subfield: Code Understanding). Source:Web Back to List of Figures 48 Preprint. Work in progress. Figure 16: sample error case of Coding (subfield: Code Understanding). Source:Web Back to List of Figures 49 Preprint. Work in progress. Figure 17: sample error case of Information Extraction (subfield: Multimodel QA). Source: MVBench (Li et al., 2024e) and STAR (Wu et al., 2024) Back to List of Figures 50 Preprint. Work in progress. Figure 18: sample error case of Information Extraction (subfield: Detailed Manual Understanding).Source:Web Back to List of Figures 51 Preprint. Work in progress. Figure 19: sample error case of Information Extraction (subfield: Search by Attribute without Calculate).Source:Web Back to List of Figures 52 Preprint. Work in progress. Figure 20: sample error case of Knowledge (subfield: World Knowledge). Source: BIOSCAN-1M (Gharaee et al., 2024) Back to List of Figures Preprint. Work in progress. Figure 21: sample error case of Knowledge (subfield: World Knowledge). Source: Web Back to List of Figures 54 Preprint. Work in progress. Figure 22: sample error case of Knowledge (subfield: Art). Source: Web Back to List of Figures 55 Preprint. Work in progress. Figure 23: sample error case of Mathematics (subfield: Graph Theory). Source:Web Back to List of Figures 56 Preprint. Work in progress. Figure 24: sample error case of Metrics (subfield: Generated Image Eval). Source:Motion Guidance (Geng & Owens, 2024) Back to List of Figures Preprint. Work in progress. Figure 25: sample error case of Metrics (subfield: Generated Image Eval). Source: EASI-Tex (Perla et al., 2024)"
        },
        {
            "title": "Back to List of Figures",
            "content": "58 Preprint. Work in progress. Figure 26: sample error case of Planning: (subfield: Puzzles and Games). Source: Web Back to List of Figures Preprint. Work in progress. Figure 27: sample error case of Planning (subfield: Puzzles and Games). Source:Web Back to List of Figures 60 Preprint. Work in progress. Figure 28: sample error case of Planning (subfield: Reordering). Source:Perception Test (Patraucean et al., 2024) Back to List of Figures 61 Preprint. Work in progress. Figure 29: sample error case of Perception (subfield: Multimodal Constrained Captioning). Source: Web Back to List of Figures 62 Preprint. Work in progress. Figure 30: sample error case of Perception (subfield: Visual Recognition). Source:Web Back to List of Figures 63 Preprint. Work in progress. Figure 31: sample error case of Perception (subfield: Visual Recognition). Source: COCO (Lin et al., 2014) Back to List of Figures 64 Preprint. Work in progress. Figure 32: sample error case of Perception (subfield: Visual Recognition). Source: CelebA (Liu et al., 2015) Back to List of Figures Preprint. Work in progress. Figure 33: sample error case of Science (subfield: STEM). Source: SciBench (Wang et al., 2023b) Back to List of Figures 66 Preprint. Work in progress."
        },
        {
            "title": "G DETAILED TASK INFORMATION",
            "content": "In Table 18, we list data source details for every task in our benchmark. We also list the output format and metrics to help better understand each tasks form. Table 18: Detailed task information description of MEGA-BENCH. Task Name Source Description Output Format Metrics Information Extraction"
        },
        {
            "title": "Flight",
            "content": "Weather Info Retrieval Stock Info Retrieval Game Support cation Platform IdentifiTop Rated Hotel Identification Movie Info Retrieval Top Video Creator Identification Highest Discount Game Price Identification Newspaper Page Parse And Count Remaining Playback Time Calculation Screenshots were taken by the human annotator on Google Flights. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on Microsoft Weather. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on Yahoo Finance. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Steam store. Questions and answers were created by the annotator. Screenshots were taken by the human on Booking.com. Questions and answers were created by the annotator. annotator Screenshots were taken by the human annotator on the Amazon Prime Video webpage. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on YouTube. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Steam store. Questions and answers were created by the annotator. Data collected from the Newspaper Navigation Dataset (Lee et al., 2020). Questions and answers were created by the annotator. Screenshots were taken by the human annotator on YouTube. Questions and answers were created by the annotator."
        },
        {
            "title": "Contextual",
            "content": "String Set Equality Comma Contextual Set Equality Structured Exact String Match, Set Equality Contextual String Set Equality Comma Contextual String Set Equality Comma Exact Numerical Exact Exact Exact Match Exact Match Exact Match String String String Exact Match String 67 Preprint. Work in progress. Task Name"
        },
        {
            "title": "Multi\nManual\nnation\nSpanish",
            "content": "Lingual ExplaScooter"
        },
        {
            "title": "Multi\nManual\nnation\nArabic",
            "content": "Lingual ExplaScooter"
        },
        {
            "title": "Multi\nManual\nnation\nFrench",
            "content": "Lingual ExplaScooter Multi Manual nation Chinese Lingual ExplaScooter Multi Manual nation Russian Lingual ExplaScooter Video Summary Video Short Title Video2notes Video Reasoning Content Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Open",
            "content": "taken located from at Screenshots manual user https://fcc.report/FCCID/2A33E5LCHG11U/6288539.pdf. Questions and answers created by human annnotator."
        },
        {
            "title": "Open",
            "content": "taken located from at Screenshots user manual https://fcc.report/FCCID/2A33E5LCHG11U/6288539.pdf. Questions and answers created by human annnotator."
        },
        {
            "title": "Open",
            "content": "taken located from at Screenshots user manual https://fcc.report/FCCID/2A33E5LCHG11U/6288539.pdf. Questions and answers created by human annnotator. Open taken located from at Screenshots user manual https://fcc.report/FCCID/2A33E5LCHG11U/6288539.pdf. Questions and answers created by human annnotator. Open taken located from at Screenshots user manual https://fcc.report/FCCID/2A33E5LCHG11U/6288539.pdf. Questions and answers created by human annnotator. Metrics GPT-4o as Judge GPT-4o as Judge GPT-4o as Judge GPT-4o as Judge GPT-4o as Judge Videos taken from WikiHow or YouTube. Questions and answers created by human annnotator. Videos taken from YouTube. Questions and answers created by human annnotator. WikiHow or YouTube. Questions and answers created by human annnotator. Videos and annotations were taken from the HME100k (Yuan et al., 2022) dataset. Questions and answers were adapted by human annotator. Open GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Contextual Simple Match String COCO OOD Global Image Retrieval By Query Property Images were from COCOO (Mao et al., 2023). Questions and answers were re-designed by the annotator manually Structured Jaccard Index 68 Preprint. Work in progress. Task Name Places365 Similar Scene Retrieval"
        },
        {
            "title": "Game\ntrieval",
            "content": "Info ReMedia Homepage Profile Movie Retrieval By Actor Music Info Retrieval Tv Show Retrieval By Character App Layout Understanding Leetcode Layout App Understanding Youtube App Layout Understanding Amazon Table 18 continued from previous page Source Description Output Format and labels from the were Images Places365 taken dataset (Zhou et al., 2017) and adapted and answers by human annotator. questions into MC Metrics"
        },
        {
            "title": "Contextual",
            "content": "String Set Equality Comma Structured Jaccard Case Insensitive Index Contextual String Set Equality Comma Contextual String Set Equality Comma Contextual String Set Equality Comma Exact Exact Exact Exact Match Exact Match Exact Match String String String Images and labels come from the SEED-Bench (Li et al., 2024b) dataset. Some images are from Yelp. Questions and annotations were adapted by human annotator. Screenshots were taken by the human annotator on the Epic Games Store. Questions and answers were created by the annotator. Most images and labels come from the SEED-Bench (Li et al., 2024b) dataset, while one came from screenshot taken by human annotator. Questions and annotations were adapted by human annotator. Screenshots were taken by the human annotator on the Amazon Prime Video webpage. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Spotify Web Player. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Amazon Prime Video webpage. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on Leetcode. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on YouTube. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on Amazon. Questions and answers were created by the annotator. 69 Preprint. Work in progress. Table 18 continued from previous page Source Description Output Format Task Name App Layout Understanding Word App Layout Understanding Notes App Layout Understanding Ppt Layout App Understanding Alipay App Layout Understanding Instagram App Layout Understanding Zoom App Layout Understanding Excel Layout App Understanding Iphone Settings Layout App Understanding Tiktok App Layout Understanding Twitter Multilingual News Qa"
        },
        {
            "title": "Exact",
            "content": "Exact Exact Exact Exact Exact Exact Exact Screenshots were taken by the human annotator on Microsoft Word. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Google Notes app. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on Microsoft PowerPoint. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Alipay app. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Instagram app. Questions and answers were created by the annotator. Screenshots were taken by the human on Zoom. Questions and answers were created by the annotator. annotator Screenshots were taken by the human annotator on Microsoft Excel. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the iPhone. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the TikTok app. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the (formerly Twitter) app. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on (formerly Twitter). Questions and answers were created by the annotator. 70 Metrics"
        },
        {
            "title": "String",
            "content": "Exact Match Exact Match Exact Match Exact Match Exact Match Exact Match Exact Match String String String String String String String Contextual Multi Ref Phrase Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Exact",
            "content": "Metrics"
        },
        {
            "title": "String",
            "content": "Preprint. Work in progress. Task Name"
        },
        {
            "title": "Research Website\nParsing Blogpost",
            "content": "Research Website Parsing Homepage Research Website Parsing Publication Gui Chat Easy Gui Chat Hard Realworld En2cn Qa Star Object Interaction Video Funqa pected Magic Video UnexAction Activitynetqa Images were taken from various websites. Questions and answers were created by the annotator. Screenshots were taken of various ML research websites. Questions and answers were created by the annotator. Screenshots were taken of various ML research websites. Questions and answers were created by the annotator. Screenshots were taken of various ML research websites. Questions and answers were created by the annotator. Images and annotations were adapted from the GUI Chat dataset (Chen et al., 2024c) by the human annotator into an open-ended question. Images and annotations were adapted from the GUI Chat dataset (Chen et al., 2024c) by the human annotator into an open-ended question. Images and annotations were adapted from the RealWorldQA benchmark (xAI, 2024) by the human annotator into an openended question. The translation requirement was added by the human annotator. Videos and annotations were adapted from the STAR benchmark (Wu et al., 2024) by the human annotator into questions and answers. Videos and annotations were adapted from the FunQA benchmark (Xie et al., 2023) by the human annotator into being an open-ended question. Images and annotations were adapted from the ActivityNetQA benchmark (Yu et al., 2019) by the human annotator into being an open-ended question."
        },
        {
            "title": "Multi Ref Phrase",
            "content": "Open GPT-4o as Judge Open GPT-4o as Judge Contextual Multi Ref Phrase Contextual Multi Ref Phrase Open GPT-4o as Judge Open GPT-4o as Judge Preprint. Work in progress. Task Name"
        },
        {
            "title": "Funqa\npected\nCreative Video",
            "content": "UnexAction"
        },
        {
            "title": "Video Qa",
            "content": "Nextqa Oe Funqa pected Humor Video UnexAction Multilingual Movie Parsing Info Movie Info Parsing Stock Info Parsing Music Info Parsing Multilingual Game Info Parsing Ocr Article Authors Table 18 continued from previous page Source Description Output Format Metrics"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge MC"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Structured Exact Match, String Match String Simple Structured Exact Match String Structured Structured Exact Match Exact Match String String Structured Exact Match String Structured Simple Match String Videos and annotations were adapted from the FunQA benchmark (Xie et al., 2023) by the human annotator into being an open-ended question. Images and annotations were adapted from the NExTQA benchmark (Xiao et al., 2021) by the human annotator into questions and answers. Videos taken from YouTube. Questions and answers created by human annnotator. Images and annotations were adapted from the NExTQA benchmark (Xiao et al., 2021) by the human annotator into being an open-ended question. Videos and annotations were adapted from the FunQA benchmark (Xie et al., 2023) by the human annotator into being an open-ended question. Screenshots were taken by the human annotator on the Amazon Prime Video webpage. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Amazon Prime Video webpage. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on Yahoo Finance. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Spotify Web Player. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Epic Games Store. Questions and answers were created by the annotator. Screenshots taken of various academic papers. Questions and answers created by human annotator. 72 Preprint. Work in progress. Task Name"
        },
        {
            "title": "Resume",
            "content": "Image Translation En2cn Booking Rating Web Weather Parsing Info Game Info Parsing Weather Map Climate Type Temperature Parsing Booking Hotel Confirmation Parsing Table 18 continued from previous page Source Description Output Format Metrics Videos taken from YouTube. Questions and answers created by human annnotator. Screenshots were taken by the human annotator on the Amazon Prime Video webpage. Questions and answers were created by the annotator. Resumes taken from various personal websites. Questions and answers were created by the annotator. Images were collected from various sources, including academic papers, news articles, shopping receipts, etc. The annotations are obtained by GPT-4o translation followed by human check. Images and labels come from the SEED-Bench (Li et al., 2024b) dataset. Some images are from Yelp. Questions and annotations were adapted by human annotator. Images were collected from the Microsoft Weather by taking screenshots. Questions and answers were designed by the annotator. Screenshots were taken by the human annotator on the Epic Games Store. Questions and answers were created by the annotator. One of the examples comes from the SEED-Bench 2 Plus benchmark (Li et al., 2024b). The rest of the images were collected from various online websites. Questions and annotations were adapted by human annotator. Screenshots were taken by the on Bookhuman ing.com. Questions and answers were created by the annotator. annotator"
        },
        {
            "title": "Contextual",
            "content": "String Set Equality Line Break"
        },
        {
            "title": "Gleu Cn",
            "content": "Structured Exact Match String Structured Exact Match String Structured Exact Match String Structured Exact Match String Structured Exact Match String 73 Preprint. Work in progress. Task Name"
        },
        {
            "title": "Entertainment\nWeb Game Style",
            "content": "Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Structured",
            "content": "Some of the examples come from the SEED-Bench 2 Plus benchmark (Li et al., 2024b). The rest of the screenshots were taken on the Steam store. Questions and annotations were adapted by human annotator. Planning Metrics Exact Str Match Case Insensitive, Exact String Match Wikihow Complex Task Completion Data collected from website, and the questions and answers are designed by human annotator"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Identify",
            "content": "Vln English Next Step Vlnqa Egocentric Navigation Video Vln Identify Location Vln Tegulu Next Step Vln Hindi Next Step InteracOperations App tive Instagram InteracOperations App tive Leetcode collected from RxR Data dataset (Ku et al., 2020), the question and answer are adapted to select the robot that should execute the instruction collected from RxR Data the dataset (Ku et al., 2020), question and answer are adapted by human annotator al., collected from VLNData CE (Krantz 2020) et and the task is adapted from MVBench (Li et al., 2024e), the question and answer are adapted by human annotator"
        },
        {
            "title": "String",
            "content": "Contextual Simple Match String Contextual Simple Match String collected from RxR Data dataset (Ku et al., 2020), the question and answer are adapted by human annotator collected from RxR Data dataset (Ku et al., 2020), the question and answer are adapted by human annotator collected from RxR Data dataset (Ku et al., 2020), the question and answer are adapted by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Structured Exact Match String Structured Simple Match String Contextual Simple Match String MC MC Exact Match Exact Match String String Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Structured",
            "content": "Metrics String Exact Xml Match, Nbbox Iou Single Preprint. Work in progress. Task Name"
        },
        {
            "title": "Gui Act Mobile\nSwipe",
            "content": "App Interactive Operations Excel Gui Act Mobile Tap App Interactive Operations Alipay Gui Act Web Single InteracOperations App tive Twitter App Interactive Operations Word InteracOperations App tive Iphone Settings App Interactive Operations Tiktok Data collected from webpage screenshots by human annotator, and the questions and answers bounding boxes are annotated by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from application screenshots by human annotator, and the questions and answers bounding boxes are annotated by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from application screenshots by human annotator, and the questions and answers bounding boxes are annotated by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from webpage screenshots by human annotator, and the questions and answers bounding boxes are annotated by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator 75 MC"
        },
        {
            "title": "Xml Norm Point\nDistance",
            "content": "MC Exact Match String Numerical Xml Norm Point In Bbox MC Exact Match String Numerical Xml Nbbox Iou Single MC MC MC MC Exact Match Exact Match Exact Match Exact Match String String String String Preprint. Work in progress. Task Name"
        },
        {
            "title": "App\nInteractive\nOperations Zoom",
            "content": "InteracOperations"
        },
        {
            "title": "Action",
            "content": "InteracOperations App tive Youtube Calendar Schedule Suggestion Planning Visual Barman Planning Visual Floortile Planning Visual Storage Planning Screenshot Grippers Planning Visual Blocksworld Table 18 continued from previous page Source Description Output Format Metrics Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from VisualWebBench (Liu et al., 2024a), and the questions and answers are adapted by human annotator Data collected from application screenshots by human annotator, and the questions and answers are designed by human annotator Data collected from Google Calendar by human annotator, and the questions and answers are designed by human annotator to identify all possible starting times for meeting within specified time range and duration Data collected from Planning Domain Definition Language (PDDL) of Barman, and the are questions adapted to match the transitions from init state to goal state answers and MC MC MC MC MC"
        },
        {
            "title": "Exact\nMatch",
            "content": "Exact Match"
        },
        {
            "title": "String",
            "content": "String Contextual Set Equality Structured Symbolic ning Test PlanData collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Structured Symbolic ning Test PlanStructured Symbolic ning Test PlanStructured Symbolic ning Test PlanStructured Symbolic ning Test Plan76 Preprint. Work in progress. Task Name Planning Screenshot Barman Planning Screenshot Termes Planning Screenshot Floortile Planning Screenshot Blocksworld Planning Screenshot Storage Planning Visual Termes Planning Screenshot Tyreworld Planning Visual Grippers Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Structured",
            "content": "Data collected from Planning Domain Definition Language (PDDL) of Barman, and the questions are adapted to match the transitions from init state to goal state answers and Metrics"
        },
        {
            "title": "Symbolic\nning Test",
            "content": "PlanData collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state Data collected from website, and the questions and answers are adapted to match the transitions from init state to goal state"
        },
        {
            "title": "Symbolic\nning Test",
            "content": "Plan-"
        },
        {
            "title": "Symbolic\nning Test",
            "content": "PlanStructured Symbolic ning Test PlanStructured Symbolic ning Test PlanStructured Symbolic ning Test PlanStructured Symbolic ning Test PlanStructured Symbolic ning Test PlanLogical Reasoning Find Odd One Out Data collected from website, and the questions and answers are adapted to match strings Structured Dict Equality, Exact String Match Logical Reasoning Fit Pattern Perception-Test Object Video Shuffle collected Data from LogicVista (Xiao et al., 2024), and the questions and answers are adapted by human annotator al., collected from VLNData 2020) et CE (Krantz and the task is adapted from MVBench (Li et al., 2024e), the question and answer are adapted into single choice by human annotator MC MC Exact Match String Simple Match String 77 Preprint. Work in progress. Task Name"
        },
        {
            "title": "Puzzles",
            "content": "Table 18 continued from previous page Source Description Output Format Metrics Data collected from Lichess, and the questions and answers are adapted to match strings Data collected from Lichess, and the questions and answers are adapted to match strings"
        },
        {
            "title": "Bridge Strategies\nExpert",
            "content": "Data and answer are collected from Bridge Master"
        },
        {
            "title": "Puzzles",
            "content": "Chess Puzzle Single Step Chess Find Legal Moves Data collected from Lichess, and the questions and answers are adapted to match strings Data collected from Lichess, and the questions and answers are adapted to match strings Data collected from game positions of games in the 2024 FIDE Candidates tournament, and the questions and answers are adapted to match strings Bridge Strategies Advanced Data and answer are collected from Bridge Master 2000 Chess Identification Winner from collected Data IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator Bridge Strategies Worldclass Data and answer are collected from Bridge Master Mahjong Chess Endgames Sygyzy Data collected from website and screenshot of MajSoul, and the answer are annotated by human annotator Endgames created by human annotator collected and data from https://syzygy-tables.info, and the questions and answers are adapted to match Jaccard index"
        },
        {
            "title": "Exact",
            "content": "Exact Open Exact Open Exact Exact Go Capture Stone Data collected from Exact https://online-go.com/learnto-play-go/capture https://forums.onlinego.com/t/capture-goproblems/31531/9, questions adapted to match strings and answers and and the are GPT-4o as Judge"
        },
        {
            "title": "String",
            "content": "Chess Move List Index, Jaccard Exact String Match GPT-4o as Judge Exact Match String GPT-4o as Judge Exact Match String Chess Move List Index, Jaccard String Exact Match Exact Match String Bongard Problem Data collected from Contextual https://www.oebp.org/welcome.php and https://www.foundalis.com/res/bps/bpidx.htm String Set Equality Comma 78 Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Exact",
            "content": "Metrics"
        },
        {
            "title": "String",
            "content": "Preprint. Work in progress. Task Name Number Kakuro 5x"
        },
        {
            "title": "Arc Agi",
            "content": "Mnist Pattern Number Sudoku Puzzle Move Pos To Pos Hanoi 4 Pole Pictionary toon Guess CarDrawing Pictionary Chinese Food Img2en Pictionary Doodle Guess Pictionary Skribbl Io Pictionary Genai Output Chinese Annoying Word Search collected Data https://krazydad.com/kakuro/, and the questions and answers are adapted to match strings from Data collected from website, and the questions and answers are adapted to match dict equality collected from Data https://arcprize.org/play and the task is adapted from Intelligence (Chollet, 2019), the question and answer are adapted into grid of digits by human annotator collected from Data MNIST (Deng, and the questions and answers are adapted to match strings 2012), Data collected from puzzles.ca, and the questions and answers are adapted to match strings Shortest paths derived from diagram found on website and the questions and answers are created to match strings and the longest common move prefix Data collected from An early evaluation of gpt-4v (ision) (Wu et al., 2023), the question and answer are adapted to match strings by human annotator Data collected from website, and the questions and answers are adapted to match strings Data collected from website, and the questions and answers are adapted to match strings Data collected from screenshots collected by human annotator on skribbl.io and the questions and answers are adapted to match strings Data collected from screenshot of website, and the questions and answers are adapted to match strings Data collected from various websites, and the answers are annotated by human annotator"
        },
        {
            "title": "String",
            "content": "Numerical Exact Match String Contextual Simple Match String Structured Exact Exact Exact Exact Exact Exact Match, Common Prefix Ratio String Longest List Exact Str Match Case Insensitive Exact Str Match Case Insensitive Exact Match String Exact Str Match Case Insensitive Exact Match String Contextual Dict Jaccard Agg Jaccard Preprint. Work in progress. Task Name Table 18 continued from previous page Source Description Output Format Metrics Logical Reasoning 2d Views Of 3d Shapes Data collected from website, and the questions and answers are adapted to match strings"
        },
        {
            "title": "Dict Equality",
            "content": "Maze 2d 8x8 Crossword Mini 5x"
        },
        {
            "title": "Rebus",
            "content": "Icon Arithmetic Puzzle Data from generated https://www.mazegenerator.net/, and the questions and answers are adapted to match strings Data collected from website, and the questions and answers are adapted to match strings Data collected from website, and the questions and answers are adapted to match strings Data collected from An early evaluation of gpt-4v (ision) (Wu et al., 2023), the question and answer are adapted to match strings by human annotator Test Open Iq Ended Data and answers are collected from website Ball Cup Swap 3 Screenshots taken from video and edited together using images found online, and the questions and answers are adapted to match strings Structured Open MC Logical Reasoning 2d Folding Data collected from website, and the questions and answers are adapted to match strings MC Perception Test Video Character Order Contextual Data collected from Perception Test (Patraucean et al., 2024) and the task is adapted from MVBench (Li et al., 2024e), the question and answer are adapted into single answer string by human annotator"
        },
        {
            "title": "String",
            "content": "String Exact Match, Sequence Equality GPT-4o as Judge Exact Match String Exact Match Simple Match String String Comic Page Ordering Recipe Image Ordering Data collected from website Contextual Data collected from website MC Sequence Equality Sequence Equality Coding Code Translation Easy Data and test cases are collected from Pintia Code Translation Python Data collected from xCodeEval split (Khan et al., 2023), and test cases are annotated by human Code Translation Hard Data and test cases are collected from Pintia Structured Program Judge Structured Program Judge Structured Program Judge Preprint. Work in progress. Task Name Table 18 continued from previous page Source Description Output Format Metrics"
        },
        {
            "title": "Program Judge",
            "content": "Data and answer are collected from SGP-Bench (Qiu et al., 2024) Data and answer are collected from SGP-Bench (Qiu et al., 2024)"
        },
        {
            "title": "Multi Ref Phrase",
            "content": "Data are collected from website, and the question and answer are adapted for string match MC"
        },
        {
            "title": "String",
            "content": "Symbolic Graphics Programs Computer Aided Design Symbolic Graphics Programs Scalable Vector Graphics"
        },
        {
            "title": "Code",
            "content": "Code Add Tag Media mend Stackoverflow RecomSolutions Flowchart Code Generation Code Compare Solution Code Match Problem Data collected from xCodeEval (Khan et al., 2023), the question and answer are adapted to match code tag Data are collected from Stack Overflow Website, and the question and answer are adapted to match string Data are collected from website, and the question and answer are designed by human annotator collected Data from SGPBench (Qiu et al., 2024), and the question and answer are adapted for string match collected Data from SGPBench (Qiu et al., 2024), and the question and answer are adapted to match code Code ization Understanding VisualOutput Data are collected from website, and the question and answer are designed by human annotator Code Output Result Code Execution Code Retrieval Data are collected from SanFoundry MCQs, and the question and answer are designed by human annotator Data collected from executionv2 (Jain et al., 2024a), the question and answer are adapted to match string collected Data from SGPBench (Qiu et al., 2024), and the question and answer are adapted to match string 81 Contextual Set Equality MC MC Exact Exact MC Exact Exact Match Exact Match Exact Match Exact Match String String String String String Set Equality Comma Code Result Exact Str Match Contextual Simple Match String Exact Exact Match String Preprint. Work in progress. Task Name Table2latex Complex"
        },
        {
            "title": "Table",
            "content": "To"
        },
        {
            "title": "Ocr\nMarkdown",
            "content": "To Ocr Math Text Latex Ocr Math Equation Latex plex Convertion ComFormula Ocr Table To Latex Ocr Table To Csv Table 18 continued from previous page Source Description Output Format Metrics collected Data from SGPBench (Qiu et al., 2024), and the question and answer are adapted for LLM Judge Data are collected from website, and the question and answer are designed by human annotator Data are collected from website, and the question and answer are designed by human annotator Data are collected from website, and the question and answer are designed by human annotator to match text with LATEX Data are collected from website, and the question and answer are designed by human annotator to match LATEX Data are collected from latexformulas and TexTeller, and the question and answer are designed by human annotator Data are collected from website, and the question and answer are designed by human annotator Data are collected from website, and the question and answer are designed by human annotator"
        },
        {
            "title": "Structured",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Text With Latex\nExpr Equality",
            "content": "Contextual Latex Expr Equality Structured Latex Expr Equality Structured Structured Simple Match Simple Match String String Code ming Test Easy ProgramData and test cases are collected from Pintia Code ming Test Hard ProgramData and test cases are collected from Pintia Structured Program Judge Structured Program Judge Code Programming Test Advanced Code Programming Extremely Hard Visualization With Code Stackoverflow Debug Qa Data and test cases are collected from Pintia Structured Program Judge Data and test cases are collected from Pintia Structured Program Judge Structured GPT-4o as Judge Data are collected from website, and the question and answer are designed by human annotator Data are collected from Stack Overflow Website, and the question and answer are adapted to match string Structured Exact Str Match Case Insensitive, Exact String Match Exact Match String Code Error Line Identification Data collected from Pintia, and the question and answer are adapted to match string MC Preprint. Work in progress. Task Name Table 18 continued from previous page Source Description Output Format Metrics Visual Correspondence In Two Images 2D Image Jigsaw Puzzle Easy"
        },
        {
            "title": "Adapted Cvbench\nDistance",
            "content": "Perception Images are from BLINK (Fu et al., 2024c). Annotator manually added one more reference point per sample and designed structured answers Images created by playing the online Jigsaw simulator and taking screenshots Data collected from CV-Benchs distance split (Tong et al., 2024), and adapted into exact text answer Plot Geometry Position Relationship Data collected from Internet. Question and answers were designed by the annotator Video Grounding Spatial Adapted Cvbench Relation Egocentric Spatial Reasoning Trance Reasoning Basic Physics CLEVER Moving Direction Video Trance Reasoning Event Physics Videos collected dOR (Shang et Re-designed answers for this specific task from Vial., 2019). and questions Data collected from CV-Benchs relation split (Tong et al., 2024), and adapted into exact text answer Data are collected from EpicKitchen (Damen et al., 2018) and the Internet. Questions and answers are adapted for contextual formatted output collected are (Hong et from Data Trance al., 2023) by specifically picking up samples with the easiest settings. Questions are re-designed for this specific task answers and Video data are collected from MVBench (Li et al., 2024e). Questions are contextual for adapted formatted output format answers and the selecting collected are (Hong et from Data al., 2023) Trance settings where by objects are moved. Questions and answers are re-designed for indicating changed objects"
        },
        {
            "title": "Exact",
            "content": "Exact Contextual"
        },
        {
            "title": "String",
            "content": "Exact Match Simple Match String String Exact Exact Match String Contextual Multi Ref Phrase Exact Exact Match String Contextual Multi Ref Phrase MC Set Equality Fragments 3D Understanding We write rendering scripts to produce the data from the assets of the Break Bad dataset (Sellan et al., 2022) Numerical Simple Match String 83 Preprint. Work in progress. Task Name"
        },
        {
            "title": "ClEVRER\nPhysics",
            "content": "ClEVRER Video ObMoving ject Property Recognition Trance Reasoning View Physics Table 18 continued from previous page Source Description Output Format Metrics"
        },
        {
            "title": "Multi Ref Phrase",
            "content": "MC Set Equality Images are collected from the Internet, questions and answers are designed by annotator are collected Images from CLEVRER (Yi et al., 2019), are questions re-designed the for understanding of physical status answers testing and The videos are collected from MVBench (Li et al., 2024e), the questions and answers are adapted to test the understanding of physical property and dynamics collected are (Hong et from Data Trance al., 2023) by selecting the most challenging settings (objects are moved, and two states are captured by different cameras). Questions and answers are re-designed for indicating changed objects Photoshop Operation ReRelative Of flectance Different Regions Images are collected from the Web, questions and answers designed by annotator Images come from BLINK (Fu the annotator et al., 2024c), added one more point per image and converted the task into reflectance sorting task Structured Jaccard Index Structured Sequence Equality Autonomous Driving Analysis Scene Images are collected from the Internet, questions and answers are designed by annotator Exact Exact Str Match Case Insensitive Functionality In Matching Different Objects NLVR2 Two Image Compare QA Egocentric Analysis Single Image ClEVR Existence Video Object images come from The al., 2024c). BLINK (Fu et The annotator manually added one ref point per image to augment the task Structured Dict Equality MC Multi Ref Phrase Exact MC Exact Match Insensitive String Case Simple Match String are collected from Images NLVR2 (Suhr & Artzi, 2019). Questions redesigned by the annotator answers and The images are collected from Epic-Kitchens (Damen et al., 2018). Questions and answers are re-designed by the annotator are collected Videos from MVBench (Li et al., 2024e). Questions are and slightly adapted answers 84 Preprint. Work in progress. Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Exact",
            "content": "Metrics"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Multi Ref Phrase",
            "content": "MC"
        },
        {
            "title": "String",
            "content": "Structured Dict Equality Contextual Simple Match String Open GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Numerical Normalized RMSE Numerical Normalized RMSE Task Name SNLI-VE Visual Entailment OCR Open-ended QA"
        },
        {
            "title": "Super\nScene\nstanding",
            "content": "ClEVR UnderVisual Dialog Image Guessing Semantic Matching Of Two Images Recover Masked Word In Figure Graph Interpretation Science Explanation Figure Bar Chart pretation InterElectricity Load Estimate Plot Average Humidity Estimate Plot Data are collected and converted (Xie from SNLI-VE dataset et al., 2019) Images collected from the Internet. Questions and answers made up by the annotator for the openended output format Images are collected from SuperCLEVR (Li et al., 2023b). Questions and answers are redesigned by the annotator Images are collected from Visual Dialog dataset (Das et al., 2017). Questions and answers are designed by the annotator come Images from BLINK dataset (Fu et al., 2024c). The annotator augmented the data by adding one more ref point and re-designed the answer The annotator took screenshots from few public papers on arXiv and designed the questionanswer pairs The images of line/dot graphs are collected from the Internet, and the annotator created the question and open-ended reference answer The images of science figures are collected from the Internet, and the annotator created the question and open-ended reference answer The images of bar graphs are collected from the Internet, and the annotator created the question and open-ended reference answer and AutoFormer The temporal data were collected (Zhou et al., from Informer 2021) (Wu et al., 2021). The annotator re-processed the data to design more specific task The temporal data were collected from AutoFormer (Wu et al., 2021). The annotator reprocessed the data to design more specific task 85 Preprint. Work in progress. Task Name"
        },
        {
            "title": "Rate",
            "content": "Road Map Find Highway Between Two Place Transit Map Intersection Points Panel Images Single Question Knowledge Graph Understanding Panel Multi Question Images Mindmap ments Parsing EleDvqa Figureqa Map Diagram Qa Chart Vqa Table 18 continued from previous page Source Description Output Format Metrics"
        },
        {
            "title": "Structured",
            "content": "MC Exact String Match, Sequence Equality Case Insensitive Exact Match String Contextual Set Equality Structured Exact Match String Structured Set Equality Case Insensitive Numerical Multi Ref Phrase MC Multi Ref Phrase Contextual Simple Match String Numerical General Single Numerical Match The temporal data were collected from Lai et al. (2018) and AutoFormer (Wu et al., 2021). The annotator re-processed the data to design more specific task The road map images were collected from Seed-Bencn (Li et al., 2024c) and the Internet. Questions and answers are designed by the annotator The transit map images were collected from Seed-Bencn (Li et al., 2024c) and the Internet. Questions and answers are designed by the annotator Panel images were collected from (Fan et al., 2024). Questions and answers were designed by the annotator The large knowledge graph image was collected from the Internet. Questions and answers were designed by the annotator Panel images were collected from (Fan et al., 2024). Questions and answers were designed by the annotator Mindmap images were collected from Seed-Bencn (Li et al., 2024c) and the Internet. Questions and answers are designed by the annotator Images were collected from Dvqa dataset (Kafle et al., 2018). Questions and answers were re-designed by the annotator Images were collected from FigureQA dataset (Kahou et al., 2017). Questions and answers were re-designed by the annotator Images were collected from MapQA dataset (Chang et al., Questions and an2022). swers were re-designed by the annotator Data were collected from MathVista (Lu et al., 2023) (statistics subset) and converted into more specific task 86 Preprint. Work in progress. Table 18 continued from previous page Source Description Output Format Task Name Photo Sharing Image Retrieval"
        },
        {
            "title": "Stock Price Future\nPrediction",
            "content": "Traffic Future Prediction From Line Plot Electricity Plot Future Prediction Ili Ratio Future Prediction Paper Vqa Doc Vqa FunSD Document Qa OCR Article Journal IAM Line Ocr And Locate Images were from the Phoal., 2021) toChat dataset. Questions and answers are designed by the annotator (Zang et and AutoFormer The temporal data were collected (Zhou et al., from Informer 2021) (Wu et al., 2021). The annotator re-processed the data to design more specific task The annotator downloaded data from Yahoo! Finances API, and processed data to design this task The temporal data were collected from AutoFormer (Wu et al., 2021). The annotator reprocessed the data to design more specific task The temporal data were collected from AutoFormer (Wu et al., 2021). The annotator reprocessed the data to design more specific task The temporal data were collected from AutoFormer (Wu et al., 2021). The annotator reprocessed the data to design more specific task took annotator The highresolution screenshots of few papers on arXiv, and designed the questions and answers Data and pairs were DocMatix 2024) open-ended QA converted from (HuggingFaceM4, Images were collected from FunSD (Jaume et al., 2019). Questions and answers were designed by annotator The article screenshots were taken from various websites. Questions and answers were created by the annotator were collected Images from the IAM handwritten database (Marti & Bunke, 1999). Questions and answers were re-designed by the annotator 87 MC MC Metrics"
        },
        {
            "title": "Sequence\ncuracy\nInsensitive",
            "content": "AcCase"
        },
        {
            "title": "Normalized Rmse",
            "content": "Numerical Normalized Rmse Numerical Normalized Rmse Numerical Normalized Rmse Contextual Simple Match String Open GPT-4o as Judge Contextual Simple Match String Contextual Simple Match String Structured Exact String Match, Normalized Similarity Damerau Levenshtein Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Contextual",
            "content": "Metrics String Set Equality Line Break Preprint. Work in progress. Task Name OCR Resume Experience Plain"
        },
        {
            "title": "Resume",
            "content": "OCR Employer Plain Resume Finance Table Understanding Monthly Weather Days Count Table Understanding Complex Question Answering Table Understanding Fetaqa Table Understanding Fact Verification Electricity Future Prediction From Table The resume screenshots were taken from various websites. Questions and answers were created by the annotator"
        },
        {
            "title": "Newspaper",
            "content": "collected from Images were Navigator The Dataset (Lee et al., 2020). Questions and answers were adapted by the annotator into simple string answer format. The article screenshots were taken from various websites. Questions and answers were created by the annotator The article screenshots were taken from various websites. Questions and answers were created by the annotator Images were collected from MMMU (Yue et al., 2024a). Questions and answers were adapted by the annotator into direct numerical output format Images were collected from the Microsoft Weather by taking screenshots. Questions and answers were designed by the annotator. collected Tables were from WikiTableQuestions (Pasupat & Liang, 2015) and TabFact (Chen et al., 2019). Questions and answers were designed by the annotator Data were collected and converted from FetaQA (Nan et al., 2022) collected Tables were from WikiTableQuestions (Pasupat & Liang, 2015) and TabFact (Chen et al., 2019). Questions and answers were designed by the annotator The temporal data were collected from AutoFormer (Wu et al., 2021). The annotator reprocessed the data to design more specific task"
        },
        {
            "title": "Contextual",
            "content": "String Set Equality Line Break Contextual String Set Equality Line Break Numerical Exact Match String Structured Exact Match String Contextual Simple Match String Open GPT-4o as Judge Contextual Dict Precision Numerical Normalized Rmse Preprint. Work in progress. Task Name Video Detail Description Guess Image Generation Prompt Table 18 continued from previous page Source Description Output Format Metrics Video and description data were collected from VideoDetailCaption (Maaz et al., 2023) and converted into specific task Examples were collected from various online text-to-image generation demos"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge Docci Image Description Long Data were DOCCI (Onoe et al., 2024) collected from"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge Tweets Captioning"
        },
        {
            "title": "Image Captioning\nWith Additional\nRequirements",
            "content": "Ad Count Detection Adapted Cvbench Count Av Vehicle Multiview Counting Counting Multi Image Av Human Multiview Counting Shape Composition Shapes Counting Single Image"
        },
        {
            "title": "The annotator collected the data\nfrom X by taking screenshots and\nand the texts",
            "content": "Images were collected from various sources on the Web. The annotator used Claude 3.5 Sonnet to generate reference answers and manually polished them Image were collected from various websites by taking screenshots. Questions and answers created by the annotator Data were collected from CVBenchs counting split (Tong et al., 2024) and adapted into specific task by rewriting the question-answer pairs Images were collected from the nuScenes (Caesar et al., 2020) dataset. The annotator designed the questions and implemented script to generate the answers from the raw annotation Data were collected from Mantis (Jiang et al., 2024a) and adapted into direct numerical answer Images were collected from the nuScenes (Caesar et al., 2020) dataset. The annotator designed the questions and implemented script to generate the answers from the raw annotation Images were made by the annotator using Canva. Questions and answers were created by the annotator Data were collected from Mantis (Jiang et al., 2024a) and adapted into direct numerical answer"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge Numerical Numerical Exact Match Exact Match String String Numerical Exact Match String Numerical Numerical Exact Match Exact Match String String Structured Positive Int Match Numerical Exact Match String Preprint. Work in progress. Task Name"
        },
        {
            "title": "CLEVRER Video\nObject\nMoving\nCount",
            "content": "Shape Composition Colours"
        },
        {
            "title": "Identity",
            "content": "Rocks Identify Samples Animal Pose Estimation License Recognition Plate Image Recognition Style Long String Letter Recognition COCO Object Detection By Query Property Widerface Face Count And Event Classification Handwritten Math Expression Extraction Geometry soning Letter ReaCircled Table 18 continued from previous page Source Description Output Format Video data are collected from MVBench (Li et al., 2024e). Questions are adapted for the direct numerical output answers and"
        },
        {
            "title": "Numerical",
            "content": "Metrics"
        },
        {
            "title": "Set Equality",
            "content": "Contextual Simple Match String Numerical Sequence Coords Similarity Exact Exact Exact Numerical Structured Exact Str Match Case Insensitive Exact Str Match Case Insensitive Exact Match String Exact String Match, Nbbox Iou Tuple Exact Match, String Match String Simple Contextual Latex Expr Equality Structured Exact String Match, Sequence Equality Images were created by the annotator using Canva. Questions and answers were created by the annotator collected from Images were CelebA (Liu et al., 2015). Questions and answers re-designed by the annotator for this specific task Images, questions, and answers were collected from the Web by the annotator Images were collected from AP10K (Yu et al., 2021). The annotator implemented script to produce the answer from raw annotations for this task Images were collected from the Web. Questions and answers were created by the annotator Images were collected from the Web. Questions and answers were created by the annotator Data were designed by the annotator and generated automatically with code were from MSImages COCO (Lin 2014). al., et Questions and answers were re-designed by the annotator and adapted manually Images were collected from WiderFace (Yang et al., 2016). Questions and answers were designed and produced by the annotator Data were HME100K (Yuan et al., 2022) collected from Image were collected from Rahmanzadehgervi et al. (2024) are manually created. Questions and answers were re-designed by the annotator Preprint. Work in progress. Task Name"
        },
        {
            "title": "Av Multicamera\nTracking Predict\nBbox",
            "content": "Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Numerical",
            "content": "Images were collected from the nuScenes (Caesar et al., 2020) dataset. The annotator designed the questions and implemented script to generate the answers from the raw annotation Metrics Nbbox Iou Sequence ASCII Art Understanding Data and annotations were collected and created by the annotator from various online resources MC"
        },
        {
            "title": "Keypoint",
            "content": "Extract Webpage Headline Waldo Raw data were from CelebA (Liu et al., 2015). The annotator wrote script to produce the answers for this task collected from Images were VisualWebBench (Liu et al., 2024a). Questions and answers were adapted by the annotator Images and annotations were collected and created by the annotator using various resources on the Web Geographic ReSensing mote Land Cover Images and annotations were collected and converted from SATIN (Roberts et al., 2023) Signboard Identification Long String Number Recognition Images were collected from the Internet. The annotator created the question-answer pairs Data were designed by the annotator and generated automatically with code"
        },
        {
            "title": "Sequence Coords\nSimilarity",
            "content": "Contextual Simple Match String Structured Dict Nbbox Iou Tuple Agg Jaccard Contextual Sequence Equality Contextual Exact Waybill ber Extraction NumSequence Images were collected from the Internet. The annotator created the question-answer pairs Contextual Single Pose Estimation Person COCO Detection Person Places365 Scene Type Classification Visual Prediction Rater Openable Part Segmentation hello, this is Source Description Structured were from MSImages COCO (Lin 2014). al., et Questions and answers were re-designed by the annotator and adapted with script Images were collected from Places365 (Zhou et al., 2017). Questions and answers were re-designed and generated by the annotator Images were collected using screenshots from arXiv papers qualitative results. Questions and answers were created by the annotator Numerical Exact MC 91 Simple Match Exact Match Simple Match String String String Sequence Coords Similarity Exact String Match, Nbbox Iou Tuple Exact Match String Sequence Equality Preprint. Work in progress. Task Name"
        },
        {
            "title": "Visual Prediction\nRater\nSemantic\nSegmentation",
            "content": "Video To Camera Trajectory Retrieval Sceneqa Transition Video Scene Video Segments Reordering Action Sequence Understanding Video Recognition Action Table 18 continued from previous page Source Description Output Format Images were collected using screenshots from qualitative results from the arXiv papers. Questions and answers were created by the annotator Images were collected using screenshots from the qualitative results of the arXiv papers. Questions and answers were created by the annotator Data were collected from the project page of VD3D (Bahmani et al., 2024). Questions and answers designed and created by the annotator MC MC MC Metrics"
        },
        {
            "title": "Sequence\ncuracy\nInsensitive",
            "content": "AcCase"
        },
        {
            "title": "Sequence\ncuracy\nInsensitive",
            "content": "AcCase"
        },
        {
            "title": "String",
            "content": "Video data are collected from MVBench (Li et al., 2024e). Questions are adapted by the annotator into open-ended format answers and data come Raw from UCF101 (Soomro et al., 2012). The annotator designed the task and re-organized the data to produce question-answer pairs the Open GPT-4o as Judge Structured Sequence Equality collected from Data were MileBench (Song et al., 2024). Questions and answers were designed and created by the annotator Exact Exact Match String data come Raw from UCF101 (Soomro et al., 2012). The annotator designed the task and re-organized the data to question-answer produce pairs the Structured Exact Match String Google Streetview Line Sorting The data were taken from Google Maps. Questions and answers were created by the annotator Structured Sequence Equality Next Action Prediction Perception Video Count Test Action collected Data were from MileBench (Song et al., 2024). Questions and answers were designed and created by the annotator MC Exact Match String Video data are collected from MVBench (Li et al., 2024e). are Questions adapted by the annotator into direct numerical output format answers and Numerical Exact Match String Preprint. Work in progress. Task Name Table 18 continued from previous page Source Description Output Format Metrics"
        },
        {
            "title": "Google\nStreetview Line\nReasoning",
            "content": "The data were taken from Google Maps. Questions and answers were created by the annotator Video Camera Motion Description"
        },
        {
            "title": "Video Grounding\nTemporal",
            "content": "Videos were collected from VidOR (Shang et al., 2019). Questions and answers re-designed and created by the annotator Videos were collected from VidOR (Shang et al., 2019). Questions and answers re-designed and created by the annotator MC"
        },
        {
            "title": "Exact",
            "content": "MC Web Action Prediction Data were collected from VisualWebBench (Liu et al., 2024a) MC Contextual"
        },
        {
            "title": "Exact\nMatch",
            "content": "Simple Match"
        },
        {
            "title": "String",
            "content": "String Cam Traj Video Selection To Sta Action Localization Video Contain Contain Images Contain Length Repeat Multi Contain Repeat Position Only Length Contain Length Contain Position Images Data were collected from the project page of VD3D (Bahmani et al., 2024). Questions and answers designed and created by the annotator Video data are collected from MVBench (Li et al., 2024e). Questions and answers are repurposed for the contextual formatted output format Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Contextual Simple Match String Open Open Open Open Open Constrained Generation Constrained Generation Constrained Generation Constrained Generation Constrained Generation Metrics Constrained Generation Constrained Generation Constrained Generation Constrained Generation Constrained Generation Exact Match String Sequence curacy Insensitive AcCase Sequence Equality Preprint. Work in progress. Table 18 continued from previous page Source Description Output Format Task Name"
        },
        {
            "title": "Xor Images",
            "content": "Multi Contain Repeat Contain Contain Length Multi Contain Position Only Relative Depth Of Different Points Visual Prediction Rater Depth Estimation Visual Prediction Rater Novel View Synthesis Pokemon Recognition 3d Av View Identification"
        },
        {
            "title": "Open",
            "content": "Open Open MC MC MC Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer Images were collected from the Web. Questions and constraints are designed by the annotator. This task has no reference answer collected from Images were al., 2024c). BLINK (Fu et The annotator augmented each sample by adding one more reference point manually and adjusted the answers Images were collected by taking screenshots from depth estimation papers on arXiv. Questions and answers were created by the annotator Images were collected by taking screenshots from novel view synthesis papers on arXiv. Questions and answers were created by the annotator Images were created by the annotator from the Pokemon Go game. Questions and answers were designed by the annotator Images were collected from the nuScenes (Caesar et al., 2020) dataset. Questions and answers were designed and created by the annotator 94 Structured Exact Match String Contextual Sequence curacy Insensitive AcCase Preprint. Work in progress. Task Name Multiview ReaCamera soning Moving 3d Indoor Scene Text Bbox Prediction Table 18 continued from previous page Source Description Output Format collected from Images were al., 2024c). BLINK (Fu et Questions and answers were re-designed and augmented by the annotator"
        },
        {
            "title": "Exact",
            "content": "Metrics"
        },
        {
            "title": "String",
            "content": "data adapted (Zhang et from is The Multi3DRefer al., 2023). Questions and answers were designed by the annotator and dataset annotation."
        },
        {
            "title": "Simple\nMatch",
            "content": "Exact Match Exact Match Exact Match"
        },
        {
            "title": "String",
            "content": "String String String Sequence Equality Sequence curacy Insensitive AcCase Exact Match String Sequence curacy Insensitive Exact Match AcCase String"
        },
        {
            "title": "Google\nStreetview Circle\nReasoning",
            "content": "The data were taken from Google Maps. Questions and answers were created by the annotator Google Streetview Direction derstanding UnMotion Video Matching Real 3d Video Motion Matching 3d Real Visual Prediction Rater 3d Assembled Quality Understanding Visual Prediction Rater Surface Normal Estimation Adapted Cvbench Depth The data were taken from Google StreetView. Questions and answers were created by the annotator Videos were collected from the project page of Shen et al. (2024). Questions and answers were created by the annotator Videos were collected from the Shen et al. project page of (2024). Questions and answers were created by the annotator Data were collected from the project page of Wang et al. (2024e). Questions and answers were designed and created by the annotator Images were collected by taking screenshots from surface normal estimation papers on arXiv. Questions and answers were created by the annotator Images were collected from CVBench (Tong et al., 2024). Answers were adapted by the annotator into exact text Visual Prediction Rater Plane Segmentation Images were collected by taking screenshots from plane segmentation papers on arXiv 3d Indoor Scene Text Bbox Selection Images were collected by taking screenshots from 3D scene understanding papers on arXiv. Questions and answers were designed and generated by the annotator MC Exact MC MC MC MC Exact MC MC 95 Preprint. Work in progress. Task Name Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Google\nStreetview Circle\nSorting",
            "content": "The data were taken from Google Maps. Questions and answers were created by the annotator"
        },
        {
            "title": "Structured",
            "content": "Metrics Sequence Equality Metrics"
        },
        {
            "title": "Review",
            "content": "Data collected from OpenReviews public paper reviews"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge Data collected from OpenReviews public paper reviews"
        },
        {
            "title": "Number Rel Diff\nRatio",
            "content": "Paper Review Acceptance Data collected from OpenReviews public paper reviews"
        },
        {
            "title": "Autorater Artifact",
            "content": "Autorater Control Autorater Artifact Reason Autorater thetics AesAutorater mask UnAutorater Subject Autorater Model Texturing 3d Autorater Semantics Images were collected from ImagenHub (Ku et al., 2023). Questions and answers adapted by the annotator Images were collected from ImagenHub (Ku et al., 2023). Questions and answers adapted by the annotator Images were collected from ImagenHub (Ku et al., 2023). The created openended reference answer manually annotator Images were collected from ImagenHub (Ku et al., 2023). Questions and answers adapted by the annotator Images were collected from ImagenHub (Ku et al., 2023). Questions and answers adapted by the annotator Images were collected from ImagenHub (Ku et al., 2023). Questions and answers adapted by the annotator Resources are collected from the user study of Perla et al. (2024). Questions and answers were designed and created by the annotator Images were collected from ImagenHub (Ku et al., 2023). Questions and answers adapted by the annotator"
        },
        {
            "title": "Exact",
            "content": "MC Exact Open Exact Exact Exact"
        },
        {
            "title": "Exact\nMatch",
            "content": "Exact Match"
        },
        {
            "title": "String",
            "content": "String Constrained Generation Exact Match Exact Match Exact Match String String String Contextual Sequence Equality Exact Exact Match String Autorater Motion Guided Editing Images were collected by taking screenshots from image generation papers on arXiv MC Sequence Equality Preprint. Work in progress. Task Name"
        },
        {
            "title": "Generated Video\nArtifacts",
            "content": "Video Eval Factual Pref Video Eval Dynamic Pref Vizwiz Accessment Blind Quality For Reward Models T2i Reward Reward Models I2t Reward Table 18 continued from previous page Source Description Output Format Metrics Images were collected from ImagenHub (Ku et al., 2023). Questions and answers adapted by the annotator Video frames were collected from ImagenHub (He et al., 2024). Questions and answers adapted by the annotator Videos were collected by running various text-to-video diffusion models online. Open-ended reference answers were written by the annotator manually Video frames were collected from ImagenHub (He et al., 2024). Questions and answers adapted by the annotator Video frames were collected from ImagenHub (He et al., 2024). Questions and answers adapted by the annotator Images were collected from Chiu et al. (2020). Questions and answers were adapted and redesigned by the annotator Images were collected from (Yu et al., RLAIF-V dataset 2024a). Questions and answers were adapted by the annotator Images were collected from (Yu et al., RLAIF-V dataset 2024a). Questions and answers were adapted by the annotator Science"
        },
        {
            "title": "Exact",
            "content": "MC"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge MC MC Exact Match Exact Match String String Contextual Set Equality Exact Exact Exact Match Exact Match String String Biology Exams Data collected from EXAMSV (Das et al., 2024) and MMMUPro (Yue et al., 2024b), and the questions and answers are adapted to match strings Pmc Vqa Medical Image Qa Medical Content Based Retrieval Radiology Data collected from NLVR2 dataset (Suhr et al., 2018), and the questions and answers are adapted to match strings collected Data from ROCO dataset (Pelka et al., 2018), and the questions and answers are adapted to match strings"
        },
        {
            "title": "String",
            "content": "Contextual Simple Match String MC Exact Match String 97 Preprint. Work in progress. Task Name AbMedical MRI domen Organ Recognition Medical Multi Organ Segmentation Rater"
        },
        {
            "title": "Cell",
            "content": "Medical age Indentification ImArtifacts Blood Medical Vessels Recognition Healthcare Judgement Info Table 18 continued from previous page Source Description Output Format Data collected from GMAIMMBench (Chen et al., 2024b), and the questions and answers are adapted to match sequence accuracy Data collected from pdf screenshot, and the questions and answers are adapted to match strings collected from GMAIData MMBench (Chen et al., 2024b), and the questions and answers are adapted to match strings collected from GMAIData MMBench (Chen et al., 2024b), and the questions and answers are adapted to match strings collected from GMAIData MMBench (Chen et al., 2024b), and the questions and answers are adapted to match strings Data collected from GMAIMMBench (Chen et al., 2024b), and the questions and answers are adapted to match strings"
        },
        {
            "title": "Contextual",
            "content": "MC"
        },
        {
            "title": "Exact",
            "content": "Exact Structured MC Metrics"
        },
        {
            "title": "Sequence\ncuracy\nInsensitive",
            "content": "AcCase"
        },
        {
            "title": "Exact\nMatch",
            "content": "Exact Match Exact Match Exact Match"
        },
        {
            "title": "String",
            "content": "String String String Electrocardiogram Data collected from MMMU (Yue et al., 2024a), and the answers are open-ended Open GPT-4o as Judge Medical Segmentation Single Rater Polyp Object Medical Abdomen Endscopy Organ Recognition Medical words Retrieval Radiology KeyBased Non Medical Parasite Detection Medical Retrieval Surgeon Given Activity Data collected from pdf screenshot, and the questions and answers are adapted to match sequence equality Data collected from GMAIMMBench (Chen et al., 2024b), and the questions and answers are adapted to match sequence accuracy collected Data from ROCO dataset (Pelka et al., 2018), and the questions and answers are adapted to match strings Data collected from pdf screenshot, and the questions and answers are adapted to match set equality Data collected from GMAIMMBench (Chen et al., 2024b), and the questions and answers are adapted to match strings Structured Sequence Equality Contextual Sequence curacy Insensitive AcCase Exact Exact Match String Structured Set Equality MC Exact Match String 98 Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Numerical",
            "content": "Metrics"
        },
        {
            "title": "String",
            "content": "Preprint. Work in progress. Task Name"
        },
        {
            "title": "Science Molecule\nChemistry",
            "content": "Mmmu Pro Exam Screenshot Scibench Solution Open Ended arXiv Vqa Tqa Textbook Qa Data collected from GMAIMMBench (Chen et al., 2024b), and the questions and answers are adapted to match strings Data collected from EXAMSV (Das et al., 2024) and MMMUPro (Yue et al., 2024b), and the questions and answers are adapted to match strings from collected Data IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator Data collected from MMMUPro (Yue et al., 2024b), and the questions and answers are adapted to match strings from collected Data Scibench (Wang et al., 2023b), and the answers are open-ended Data collected from screenshots by human annotator, and the questions and answers are adapted to match strings Data collected from Dvqa (Kafle et al., 2018), and the questions and answers are refractered from the original TQA dataset MC"
        },
        {
            "title": "String",
            "content": "MC Open MC Exact Match String GPT-4o as Judge, General Single Numerical Match Exact Match String Contextual Multi Ref Phrase General Single Numerical Match General Single Numerical Match General Single Numerical Match Exact Match String Exact Match Simple Match String String Question Solution Solving Data collected from webpage screenshots by human annotator Contextual Quizlet Question Solving Data collected from webpage screenshots by human annotator Contextual Scibench Fundamental Wo Solution Mmmu Chemistry Mcq Physics Data Scibench (Wang et al., 2023b) collected from Numerical Data from collected MMMU (Yue et al., 2024a), and the questions and answers are adapted to match strings Exact Circuit Diagram Understanding Data collected from webpage screenshots by human annotator Numerical Science Physics Basic from collected Data IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator Contextual Preprint. Work in progress. Metrics"
        },
        {
            "title": "String",
            "content": "Exact Match String Exact Match String Exact Match String Exact Match String Exact Match Exact Match String String Task Name"
        },
        {
            "title": "Out Of Context",
            "content": "Text Entity Replace Text Style Face Edit Attribute Face Swap Interpret Perspective sion Force IlluClip Stable Diffusion Generate Unusual Images Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Contextual",
            "content": "MC MC MC MC MC MC Exact MC Data collected from EXAMSV (Das et al., 2024) and MMMUPro (Yue et al., 2024b), and the questions and answers are adapted to match strings Knowledge Images and labels come from the MFCBench (Wang et al., 2024c) dataset. Questions and annotations were adapted by human annotator. Images and labels come from the MFCBench (Wang et al., 2024c) dataset. Questions and annotations were adapted by human annotator. Images and labels come from the MFCBench (Wang et al., 2024c) dataset. Questions and annotations were adapted by human annotator. Images and labels come from the MFCBench (Wang et al., 2024c) dataset. Questions and annotations were adapted by human annotator. Images and labels come from the MFCBench (Wang et al., 2024c) dataset. Questions and annotations were adapted by human annotator. Images and labels come from the MFCBench (Wang et al., 2024c) dataset. Questions and annotations were adapted by human annotator. Images come from various websites. Questions and annotations were created by human annotator. Images and labels come from the MFCBench (Wang et al., 2024c) dataset. Questions and annotations were adapted by human annotator. Images come from various websites. Questions and annotations were created by human annotator. 100 Open GPT-4o as Judge Preprint. Work in progress. Table 18 continued from previous page Source Description Output Format MC MC Metrics"
        },
        {
            "title": "String",
            "content": "Contextual Multi Ref Phrase Contextual Simple Match String Contextual Multi Ref Phrase Structured Set Precision Structured Exact Match String Open GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Task Name Forensic Detection Of Different Images"
        },
        {
            "title": "Distinguish\nAI\nGenerated Image",
            "content": "Cultural Vqa Human Relationship Reasoning Sign Language Ishihara Test Llavaguard Red Racial Teaming Red Captcha Teaming Red Teaming Politics Images and labels come from the BLINK benchmark (Fu et al., 2024c). Questions and annotations were adapted by human annotator. Images and labels come from the MFCBench (Wang et al., 2024c) dataset. Questions and annotations were adapted by human annotator. come Images from various websites and image generators. Questions and annotations were created by human annotator. CulturalVQA Images and labels come from the benchmark (Romero et al., 2024). Questions and annotations were adapted by human annotator. Images come from various websites. Questions and annotations were created by human annotator. Videos come from Dr. Bill Vicars Signs YouTube channel. Questions and annotations were created by human annotator. Images come from various websites. Questions and annotations were created by human annotator. Images and labels come from the LlavaGuard benchmark (Helff et al., 2024). Questions were created by human annotator. Images and labels come from the Red Teaming benchmark (Li et al., 2024f). Questions were created by human annotator or generated by GPT-4. Images and labels come from the Red Teaming benchmark (Li et al., 2024f). Questions were created by human annotator or generated by GPT-4. Images and labels come from the Red Teaming benchmark (Li et al., 2024f). Questions were created by human annotator or generated by GPT-4. Table 18 continued from previous page Source Description Output Format MC Metrics"
        },
        {
            "title": "String",
            "content": "Preprint. Work in progress. Task Name Mmsoc Hatefulmemes Red Teaming Visual Order B"
        },
        {
            "title": "Teaming",
            "content": "Mmsoc Memotion Mmsoc Misinformation Politifact Red Teaming Jailbreak Red Teaming Visual Order Mmsoc Misinformation Gossipcop Red Teaming Visualmisleading Video Follow Up Content Meme Explain Images and labels come from the MMSoc benchmark (Jin et al., 2024). Questions and answers were adapted by human annotator. Images and labels come from the Red Teaming benchmark (Li et al., 2024f). Questions were created by human annotator or generated by GPT-4. Images and labels come from the Red Teaming benchmark (Li et al., 2024f). Questions were created by human annotator. or generated by GPTImages and labels come from the MMSoc benchmark (Jin et al., 2024). Questions and answers were adapted by human annotator. Images and labels come from the MMSoc benchmark (Jin et al., 2024). Questions and answers were adapted by human annotator. Images and labels come from the Red Teaming benchmark (Li et al., 2024f). Questions were created by human annotator or generated by GPT-4. Images and labels come from the Red Teaming benchmark (Li et al., 2024f). Questions were created by human annotator or generated by GPT-4. Images and labels come from the MMSoc benchmark (Jin et al., 2024). Questions and answers were adapted by human annotator. Images and labels come from the Red Teaming benchmark (Li et al., 2024f). Questions were created by human annotator. Videos taken from YouTube. Questions and answers created by human annnotator. Images come from various websites. Questions were created by human annotator."
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge Structured Exact Match String MC Exact Match String Open GPT-4o as Judge Open GPT-4o as Judge MC Exact Match String Open GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Preprint. Work in progress. Task Name"
        },
        {
            "title": "Funny Image Title",
            "content": "Emotion Recognition Image Humor Understanding Humor Explanation Mvsa Sentiment Classification Video Recognition Intent Humor stand Match UnderCaption Figurative Speech Explanation Muma Theory Of Mind Social Goal Muma Theory Of Mind Belief Of Goal Hashtag Recommendation Table 18 continued from previous page Source Description Output Format Metrics"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge MC Exact Match String Contextual Simple Match String Exact Exact Match String Open GPT-4o as Judge Contextual Simple Match String Contextual Simple Match String Structured Set Precision Images come from various websites. Questions were created by human annotator. Videos and labels come from the CAER dataset (Lee et al., 2019). Questions and answers were adapted by human annotator. Images come from various websites. Questions were created by human annotator. Images and labels come from Humor Understanding benchmark derived from the New Yorker Caption Contest (Hessel et al., 2022). Questions were created by human annotator. Images and labels come from the MVSA dataset (Niu et al., 2016). Questions and answers were adapted by human annotator Video and labels come from the MIntRec dataset (Zhang et al., 2022). Questions and answers were adapted by human annotator. Images and labels come from Humor Understanding benchmark derived from the New Yorker Caption Contest (Hessel et al., 2022). Questions and answers were adapted by human annotator. Images come from various websites. Questions were created by human annotator. Images and labels come from the MuMA-ToM dataset (Shi et al., 2024). Questions and answers were adapted by human annotator. Images and labels come from the MuMA-ToM dataset (Shi et al., 2024). Questions and answers were adapted by human annotator. Images and hashtags come from various social media websites. Questions were created by human annotator. 103 Preprint. Work in progress. Task Name"
        },
        {
            "title": "Dish\nMatch",
            "content": "Music Sheet Sentiment Music Sheet Author"
        },
        {
            "title": "Music Sheet Note\nCount",
            "content": "Music Sheet Format Qa Orchestra Recognition Score Music Name Sheet Insect Order Classification Signage Navigation Song Title Identification From Lyrics Table 18 continued from previous page Source Description Output Format Metrics Images and labels come from the HelloFresh website. Questions were created by human annotator. Images are music sheets posted to Noteflight. Questions and answers were created by human annotator. Images are music sheets posted to Noteflight. Questions and answers were created by human annotator. Images are music sheets posted to Noteflight. Questions and answers were created by human annotator. Images are music sheets posted to Noteflight. Questions and answers were created by human annotator. Images come from various websites. Questions were created by human annotator. Images are music sheets posted to Noteflight. Questions and answers were created by human annotator. Images and labels come from the BIOSCAN-1M dataset (Gharaee et al., 2024). Questions and answers were adapted by human annotator. Images come from various websites. Questions and answers were created by human annotator. Screenshots were taken by the human annotator on the Spotify Web Player. Questions and answers were created by the annotator. MC"
        },
        {
            "title": "Numerical",
            "content": "Numerical Structured Exact"
        },
        {
            "title": "Exact\nMatch",
            "content": "Exact Match"
        },
        {
            "title": "String",
            "content": "String Exact Match, String Match String Simple Exact Match String Contextual Simple Match String Exact Structured Exact Match Exact Match String String Knowledge Sign Recognition Images come from various websites. Questions were created by human annotator. MC String Set Equality Comma Brand Logo Recognition And Elaboration Images come from the FlickrLogo (Romberg et al., 2011) dataset and various websites. Questions were created by human annotator. Structured Multi Ref Phrase 104 Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Structured",
            "content": "Metrics Exact Str Match Case Insensitive, Set Equality Preprint. Work in progress. Task Name Logo2k Same Type Logo Retrieval"
        },
        {
            "title": "Idiom",
            "content": "Lingual Multi Ruozhiba Explanation French Multi Lingual Ruozhiba Explanation Arabic Multi Lingual Ruozhiba Explanation Spanish Multi Lingual Ruozhiba Explanation English Multi Lingual Ruozhiba Explanation Japanese Lingual Multi Ruozhiba Explanation Russian Font Recognition Traffic Accident Analysis Multiple Identify Asia States Images come from the Logo2K+ (Wang et al., 2020) dataset dataset and various websites. Questions were created by human annotator. Images come from various websites. Questions and answers were created by human annotator. Some images and labels are from the COIG-CQIA dataset (Bai et al., 2024) and some images are from Baidu Tieba and annotated by human annotator. Some images and labels are from the COIG-CQIA dataset (Bai et al., 2024) and some images are from Baidu Tieba and annotated by human annotator. Some images and labels are from the COIG-CQIA dataset (Bai et al., 2024) and some images are from Baidu Tieba and annotated by human annotator. Some images and labels are from the COIG-CQIA dataset (Bai et al., 2024) and some images are from Baidu Tieba and annotated by human annotator. Some images and labels are from (Bai the COIG-CQIA dataset et al., 2024) and some images are from Baidu Tieba and annotated by human annotator. Some images and labels are from the COIG-CQIA dataset (Bai et al., 2024) and some images are from Baidu Tieba and annotated by human annotator. Images and labels are taken from Identifont. Questions are created by human annotator. Images and labels are taken from Jia Kao Bao Dian. Questions are created by human annotator. Images come from various websites and were edited by the annotator. Questions and answers were created by human annotator."
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Open GPT-4o as Judge Exact Exact Match String Open GPT-4o as Judge Contextual Sequence curacy Insensitive AcCase Preprint. Work in progress. Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Contextual",
            "content": "Metrics"
        },
        {
            "title": "Sequence\ncuracy\nInsensitive",
            "content": "AcCase"
        },
        {
            "title": "Sequence\ncuracy\nInsensitive",
            "content": "AcCase"
        },
        {
            "title": "Sequence\ncuracy\nInsensitive",
            "content": "AcCase Structured Exact Match String Exact Exact Match String Contextual Multi Ref Phrase Open GPT-4o as Judge Exact Exact Match String Contextual Simple Match String Exact Exact Match String Open GPT-4o as Judge Task Name"
        },
        {
            "title": "States",
            "content": "Worldle Location Vqa Vibe Eval Open Vibe Eval Phrase Ancient Map Understanding Rocks Compare Samples Painting Qa Art Explanation Images come from various websites and were edited by the annotator. Questions and answers were created by human annotator. Images come from various websites and were edited by the annotator. Questions and answers were created by human annotator. Images come from various websites and were edited by the annotator. Questions and answers were created by human annotator. Images and labels are taken from Worldle Daily, free Geoguessr alternative. Questions and answers are created by human annotator. Images and labels come from various websites. Questions were created by human annotator. Images and labels come from the Vibe-Eval dataset Padlewski et al. (2024). Questions were created by human annotator. Images and labels come from the Vibe-Eval dataset Padlewski et al. (2024). Questions were created by human annotator. Images and labels come from various websites. Questions were created by human annotator. Images and labels come from ChinaNeolithic.coms online Questions were rock store. created by human annotator. Images and labels come from the MMMU benchmark Yue et al. (2024a). Questions and answers were adapted by human annotator. Images come from various websites. Questions were created by human annotator. 106 Preprint. Work in progress. Task Name"
        },
        {
            "title": "Memorization\nChinese Celebrity",
            "content": "Memorization Papers Memorization Famous Treaty Memorization Indian Celebrity Soccer Offside Deciphering Oracle Bone Kvqa Knowledge Aware Qa Character Recognition Tv Shows In Actor tion In Movie RecogniLandmark Recognition And Qa Famous Building Recognition Table 18 continued from previous page Source Description Output Format Metrics Images and labels come from Questions various websites. were created by human annotator. Images and labels come from Questions various websites. were created by human annotator. Images and labels come from Questions various websites. were created by human annotator. Images and labels come from Questions various websites. were created by human annotator. Images come from various websites. Questions were created by human annotator. Images and labels come from the Deciphering Oracle Bone Language with Diffusion Models paper (Guan et al., 2024). Questions were created by human annotator. Images and labels come from the MapQA dataset (Chang et al., 2022). Questions and answers were adapted by human annotator. Screenshots were taken by the human annotator on the Amazon Prime Video webpage. Questions and answers were created by the annotator. Screenshots were taken by the human annotator on the Amazon Prime Video webpage. Questions and answers were created by the annotator. Images and labels come from the Landmark v2 dataset (Weyand et al., 2020). Questions and answers were adapted by human annotator. Images and labels come from various websites. Questions were created by human annotator."
        },
        {
            "title": "Structured",
            "content": "MC Exact Exact String Match, Multi Ref Phrase Exact String Match, Multi Ref Phrase Exact Match Exact Match String String Contextual Simple Match String Contextual Set Equality Exact Exact Match String Structured Structured Exact String Match, Multi Ref Phrase, Near Str Match Exact Str Match Case Insensitive, String Exact Match 107 Table 18 continued from previous page Source Description Output Format"
        },
        {
            "title": "Structured",
            "content": "Metrics"
        },
        {
            "title": "Exact Str Match\nCase Insensitive",
            "content": "Preprint. Work in progress. Task Name"
        },
        {
            "title": "Landmark Check\nTwo Images",
            "content": "Defeasible Reasoning"
        },
        {
            "title": "Poetry Limerick",
            "content": "Poetry spearean Sonnet ShakeCustom Poetry Rhyming Scheme Poetry Acrostic Alliteration Poetry Haiku Poetry chian Optional Meter PetrarSonnet Poetry Acrostic Ascii Art 30 Graph Path Kawai Shortest Kamada Images and labels come from the Landmark v2 dataset (Weyand et al., 2020). Questions and answers were adapted by human annotator. Images and labels come from various websites. Questions were created by human annotator. Images come from various websites. Questions and evaluation constraints were created by human annotator. Images come from various websites. Questions and evaluation constraints were created by human annotator. Images come from various websites. Questions and evaluation constraints were created by human annotator. Images come from various websites. Questions and evaluation constraints were created by human annotator. Images come from various websites. Questions and evaluation constraints were created by human annotator. Images come from various websites. Questions and evaluation constraints were created by human annotator. Images come from various websites. Questions and evaluation constraints were created by human annotator. Images come from various webReference ASCII art sites. images were created using the ASCII Art Archives Image to ASCII Art tool. Mathematics collected Data from Visual Graph Arena Dataset by human annotator, and the questions and answers are adapted to match strings"
        },
        {
            "title": "Open",
            "content": "GPT-4o as Judge"
        },
        {
            "title": "Open",
            "content": "Open Open Open Open Open Open Constrained Generation Constrained Generation Constrained Generation Constrained Generation Constrained Generation Constrained Generation Constrained Generation Contextual ASCII Art GPT4o Judge Numerical Exact Match String Preprint. Work in progress. Table 18 continued from previous page Source Description Output Format Task Name"
        },
        {
            "title": "Shortest",
            "content": "Graph Connectivity"
        },
        {
            "title": "Graph Theory",
            "content": "Graph phism IsomorGraph Hamiltonian Cycle Graph Hamiltonian Path Graph Chordless Cycle Topological Sort Graph Maxflow Scibench Calculus Wo Solution Clevr Arithmetic Iconqa Count And Reasoning collected Data from Visual Graph Arena Dataset by human annotator, and the questions and answers are adapted to match strings from collected Data IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator Data collected from MathVision (Wang et al., 2024b), and the questions and answers are adapted by human annotator from collected Data IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator collected from Visual Data Graph Arena Dataset by human annotator, and the questions and answers are adapted to match set precision collected from Visual Data Graph Arena Dataset by human annotator, and the questions and answers are adapted to match set precision collected Data from Visual Graph Arena Dataset by human annotator, and the questions and answers are adapted to match strings Data collected from screenshots by human annotator from collected Data IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator Data Scibench (Wang et al., 2023b) collected from Data collected from Clevr (Johnson et al., 2017) Data collected from IConQA (Lu et al., 2021), with annotation refractered from the original IConQA dataset"
        },
        {
            "title": "Exact",
            "content": "MC Structured Structured Metrics"
        },
        {
            "title": "Exact\nMatch",
            "content": "Exact Match"
        },
        {
            "title": "String",
            "content": "String String Exact Match, Set Precision String Exact Match, Set Precision Numerical Exact Match String Structured Set Equality Numerical Exact Match String Numerical Numerical Single General Numerical Match Exact Match String Numerical Multi Ref Phrase Number Comparison Data collected from screenshots by human annotator Numerical Exact Match String Preprint. Work in progress. Task Name"
        },
        {
            "title": "Math Breakpoint",
            "content": "Math Convexity Value Estimation ReaGeometry Count soning Line Intersections Geometry Length Geometry soning Squares ReaNested Geometry Transformation Geometry soning lapped Circle ReaOverGeometry Area Table 18 continued from previous page Source Description Output Format Data collected from MMMUPro (Yue et al., 2024b), and the questions and answers are adapted to match numerical data MC Metrics"
        },
        {
            "title": "Numerical",
            "content": "Boxed Single Numerical Match Boxed Single Numerical Match Data from collected IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator Data from collected IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator Data from collected IsoBench (Fu et al., 2024b), and the questions and answers are adapted by human annotator Data collected from Vision language models are blind (Rahmanzadehgervi et al., 2024), and the questions and answers are adapted by human annotator Data collected from MathVision (Wang et al., 2024b), and the questions and answers are adapted by human annotator Data collected from Vision language models are blind (Rahmanzadehgervi et al., 2024), and the questions and answers are adapted by human annotator Data collected from MathVision (Wang et al., 2024b), and the questions and answers are adapted by human annotator Data collected from Vision language models are blind (Rahmanzadehgervi et al., 2024), and the questions and answers are adapted by human annotator Data collected from MathVision (Wang et al., 2024b), and the questions and answers are adapted by human annotator MC"
        },
        {
            "title": "Numerical",
            "content": "Structured"
        },
        {
            "title": "String",
            "content": "Exact String Match, Number Rel Diff Ratio Structured Exact Match String Contextual Single General Numerical Match Structured Exact Match String"
        },
        {
            "title": "General\nSingle\nNumerical Match",
            "content": "Structured Exact Match String Numerical Exact Match String 110 Preprint. Work in progress. Task Name"
        },
        {
            "title": "Geometry\nsoning Grid",
            "content": "ReaTable 18 continued from previous page Source Description Output Format Data collected from Vision language models are blind (Rahmanzadehgervi et al., 2024), and the questions and answers are adapted by human annotator"
        },
        {
            "title": "Geometry Solid",
            "content": "Geometry Analytic Geometry scriptive DeData collected from MathVision (Wang et al., 2024b), and the questions and answers are adapted by human annotator Data collected from MathVision (Wang et al., 2024b), and the questions and answers are adapted by human annotator Data collected from MathVision (Wang et al., 2024b), and the questions and answers are adapted by human annotator Metrics"
        },
        {
            "title": "General\nSingle\nNumerical Match",
            "content": "Contextual General Single Numerical Match Counterfactual Arithmetic Data collected from screenshots by human annotator Numerical Exact Match String Algebra Data collected from MathVision (Wang et al., 2024b), and the questions and answers are adapted by human annotator Contextual Single General Numerical Match"
        },
        {
            "title": "H AUTHOR CONTRIBUTION STATEMENT",
            "content": "All authors contributed at least 30 tasks to MEGA-BENCH and participated in task brainstorming, annotation, and validation. They also engaged in discussions on data annotation and provided feedback on the project. The following authors made additional contributions to various aspects of the project: Jiacheng Chen co-designed the project with Wenhu Chen, led the benchmark construction process, and coordinated collaboration among all contributors. Jiacheng Chen created and maintained the annotation GUI tool, GitHub repository, draft task taxonomy tree, and results visualization page to facilitate data annotation and improve data quality. Jiacheng Chen led the development and maintenance of the benchmark evaluation pipeline, including querying various VLMs and customizing evaluation metrics, and conducted the main experiments and analyses. Jiacheng Chen also led the writing of the paper, coordinating core contributors and incorporating their input into the manuscript. Tianhao Liang co-led the benchmark data organization, implemented most of the model query pipelines under consistent and unified framework, and conducted major experiments and analyses with Jiacheng Chen. Tianhao Liang helped maintain the evaluation pipeline and implemented the code execution metric. Tianhao Liang made significant efforts in data quality control, error analysis, and creating figures and tables for the paper. Sherman Siu made major contributions to the benchmark construction process, including task reviewing, annotator coordination, data quality control, metric implementation, and code maintenance. Sherman Siu contributed and designed bunch of complex and novel planning tasks. Sherman Siu also analyzed the number of examples per task to investigate the variance of the benchmark score and contributed to writing the main paper. 111 Preprint. Work in progress. Zhengqing Wang contributed around 40 tasks, including several complex traditional computer vision tasks. Zhengqing Wang organized the benchmark statistics for the Appendix, participated in error case analysis, and developed the interactive functionalities of the project page. Kai Wang contributed around 40 tasks with diverse data sources and output formats. Kai Wang helped organize the benchmark construction process and actively checked the annotation quality of other annotators. Yubo Wang assisted with the experiments of open-source models. Yuansheng Ni helped organize the Appendix and polished to F. Wang Zhu implemented the evaluation metric for symbolic planning tasks and helped with the paper writing. Hexiang Hu participated in discussions of the projects initial idea and continuously provided thoughts and resources for diverse tasks to facilitate the benchmark construction process. Hexiang Hu wrote significant portion of the main paper, advised on experimental design, and helped present tables and figures. Xiang Yue discussed the high-level directions and goals of the project with Jiacheng Chen and Wenhu Chen. Xiang Yue provided insightful thoughts for multi-dimensional results analysis and assisted with the experiments of open-source models. Wenhu Chen proposed the initial concepts of the project, continuously advised on project progress while refining its strategic scope and direction, and called up and organized all contributors. Wenhu Chen contributed approximately 50 diverse tasks, generated ideas for new tasks, and distributed them to other annotators. Wenhu Chen wrote the initial draft of the main paper to establish the high-level structure and guided the organization and analysis of the results."
        }
    ],
    "affiliations": []
}