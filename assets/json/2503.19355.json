{
    "paper_title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models",
    "authors": [
        "Dohwan Ko",
        "Sihyeon Kim",
        "Yumin Suh",
        "Vijay Kumar B. G",
        "Minseo Yoon",
        "Manmohan Chandraker",
        "Hyunwoo J. Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio-temporal reasoning, we present ST-VLM, a VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: https://ikodoh.github.io/ST-VLM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 5 5 3 9 1 . 3 0 5 2 : r ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models Dohwan Ko1 Sihyeon Kim1 Yumin Suh2 Vijay Kumar B.G2 Minseo Yoon1 Manmohan Chandraker2,3 Hyunwoo J. Kim4 1Korea University 2NEC Labs America"
        },
        {
            "title": "3 UC San Diego",
            "content": "4KAIST {ikodoh, sh bs15, cooki0615}@korea.ac.kr {yumin, vijay.kumar, manu}@nec-labs.com hyunwoojkim@kaist.ac.kr (a) (b) Figure 1. Spatio-temporal reasoning in dynamic videos of moving objects. (a) challenging case with complex trajectory, asking the model to predict the total traveled distance using only the video, without additional modalities such as 3D point clouds, depth map, or camera poses. The video features basketball player moving erratically across the court, making it more difficult for the model to predict. (b) An emerging capability of ST-VLM, asking the model to solve multi-step reasoning questions that require integrating spatio-temporal understanding with its existing abilities (e.g., commonsense knowledge, logical reasoning, arithmetic computation). Since GPT-4V lacks spatio-temporal reasoning ability, it fails to generate accurate answers. In (a), it responds that spatial analysis software is required, while in (b), it assumes hypothetical speed of 30 mph, leading to highly inaccurate answer compared to the ground truth. In contrast, ST-VLM, equipped with spatio-temporal reasoning with the proposed STKit dataset, consistently provides accurate answers in both cases."
        },
        {
            "title": "Abstract",
            "content": "Spatio-temporal reasoning is essential in understanding real-world environments in various fields, e.g., autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like travWork partly done during an internship at NEC Labs America. Equal contribution. Currently at Atmanity Inc. Corresponding author. eled distance and speed of moving objects. To bridge this gap, we construct spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, interobject distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio1 Figure 2. Task examples from the proposed STKit-Bench along with predictions from ST-VLM. temporal reasoning, we present ST-VLM, VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (e.g., ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: https://ikodoh.github.io/STVLM . 1. Introduction Spatio-temporal reasoning is the ability to infer spatial and temporal relationships within dynamic environments. For example, when analyzing video of two cars driving on the road, spatio-temporal reasoning allows us to predict which car is moving faster or to estimate the movement direction and speed of specific vehicle. Incorporating this ability into Vision-Language Models (VLMs) enables complex reasoning that integrates spatio-temporal reasoning with multi-step inference. Fig. 1b illustrates that solving such problems involves accessing relevant knowledge (distances between cities), estimating kinematics (speed), understanding logical relationships (time = distance/speed), and performing calculations to derive the answer. This high-level reasoning capability is essential in various applications, including autonomous driving, sports analytics, augmented/virtual reality, and embodied AI. However, advanced AI models find it challenging to perform spatiotemporal reasoning; for instance, GPT-4V produces incorrect answers when asked to estimate the moving distance of basketball player in the short video, as shown in Fig. 1a. Recent studies [6, 8, 42] have explored ways to improve the spatial reasoning capabilities of image-based VLMs, through large-scale data curation pipelines that annotate images with 3D spatial information, including object sizes and locations. While effective for spatial reasoning, these approaches still fall short in spatio-temporal reasoning for video. VLMs trained solely on spatial reasoning datasets struggle with tasks requiring temporal understanding, as they are limited to analyzing spatial relationships in images and cannot properly recognize temporal dynamics like kinematic quantities. Building on the success of spatial reasoning in images through data curation pipelines [6, 8], we hypothesize that video datasets, especially those with dynamic object motion, annotated with kinematic spatio-temporal information is effective in enhancing the spatio-temporal reasoning abilities of VLMs. In this paper, we propose framework that goes beyond spatial reasoning in image-based VLMs to tackle spatiotemporal challenges in video-based VLMs. We first introduce STKit and STKit-Bench, spatio-temporal reasoning datasets and evaluation benchmarks designed for kinematic instruction tuning. We define seven fundamental tasks in kinematic instructions, e.g., traveled distance and traveling speed (see Fig. 2 for task examples). We then present pipeline for generating kinematic instruction tuning dataset based on spatio-temporal grounding framework in dynamic videos with 3D annotations, including driving videos [3, 37] with LiDAR-based point clouds and sports videos [15] with SLAM-based estimated point clouds from AR devices [12]. Given the difficulty in acquiring point cloud-labeled videos, we also propose pseudolabeling pipeline based on 4D reconstruction for unlabeled videos [23, 40, 48] at real-world scale. By fine-tuning LLaVA-OneVision [19] on both labeled and pseudo-labeled kinematic instruction data, we develop ST-VLM which outperforms recent baseline VLMs on multiple spatio-temporal benchmarks. Furthermore, equipped with spatio-temporal reasoning abilities based on kinematic understanding, our ST-VLM extends its capability to multi-step reasoning by integrating learned reasoning with VLMs existing abilities. In summary, our contributions are threefold:"
        },
        {
            "title": "Traveled Distance",
            "content": "Predict the total traveled distance of the object given the timestamp. e.g., Can you calculate the total distance the object traveled between [START] and [END] seconds?"
        },
        {
            "title": "Traveling Speed",
            "content": "Predict the average traveling speed of the object given the timestamp. e.g., Tell me the objects average speed throughout the video."
        },
        {
            "title": "Movement Direction",
            "content": "Predict the movement direction of the object at the end of the video. e.g., What direction does the object travel at the end of the video?"
        },
        {
            "title": "Direction Timestamp",
            "content": "Predict the timestamp when the object moves in the given direction. e.g., Describe the timestamp when the object moves in the [DIRECTION] oclock direction."
        },
        {
            "title": "Traveled Distance\nComparison",
            "content": "Compare which object has traveled farther (or less). e.g., Which object travels greater distance in the video?"
        },
        {
            "title": "Traveling Speed\nComparison",
            "content": "Compare which object has traveled faster (or slower). e.g., Which object moves faster throughout the video?"
        },
        {
            "title": "Movement Direction\nComparison",
            "content": "Compare whether objects are moving in the same direction or not. e.g., Is object moving in the same direction as object in the video? Table 1. Overview of kinematic instructions. For each task, common prompt is prepended to provide additional information about the video timestamp and bounding box color of the object, e.g., The video lasts for seconds, and frames are uniformly sampled from it. These frames are located at t1, t2, . . . , tn seconds. There are objects annotated with [COLOR] bounding boxes in the video. We introduce new dataset and benchmark, STKit and STKit-Bench, to endow VLMs kinematic understanding in dynamic videos, e.g., traveled distance and movement direction of the object, for spatio-temporal reasoning. As video datasets with 3D annotations remain limited, we propose pseudo-labeled data generation pipeline by leveraging 4D reconstruction on unlabeled videos. We present ST-VLM, which significantly surpasses GPT4V by 31.3% on STKit-Bench with strong spatiotemporal reasoning. Our in-depth analyses demonstrate that ST-VLM excels in complex reasoning regarding object kinematics in various scenarios. TVQA+ [18] is video question answering dataset that requires localized spatial and temporal rationale to support the predicted answers, while VidSTG [49] targets the spatio-temporal grounding given query sentence. Several benchmarks have been introduced to evaluate videobased VLMs spatio-temporal reasoning abilities in the general domain [21], embodied AI [44], and autonomous driving [34, 51]. However, these datasets and benchmarks do not explicitly take into account kinematics in dynamic videos. In contrast, we focus on enhancing the spatiotemporal reasoning abilities of the video-based VLM with kinematics instruction tuning data. 2. Related work 3. Method Vision-Language Models (VLMs). Recent VLMs have demonstrated remarkable perception and reasoning capabilities across wide range of tasks for both images [19, 24, 33, 38] and videos [9, 21, 29, 35, 47], leveraging strong Large Language Models. However, VLMs often fall short in understanding 3D geometry [26]. To overcome this issue, image-based spatial-aware VLMs [5, 5, 6, 8, 27, 39] have been proposed to improve spatial reasoning capabilities. For example, SpatialCoT [27] introduced chain-ofthought (CoT) spatial grounding, which exhibits advanced spatial reasoning. For video-based VLMs, recent studies seek to enhance spatio-temporal capabilities [9, 20] for applications like autonomous driving [28, 34, 51] and embodied AI [5, 17]. In this paper, we propose novel video-based VLM with high-level spatio-temporal reasoning capabilities regarding the object kinematics by directly estimating the traveled distance and movement direction. Spatio-temporal reasoning datasets. Several datasets have been proposed in the literature [18, 49] to improve the video-based VLMs spatio-temporal reasoning ability. We aim to infuse VLMs with spatio-temporal reasoning capabilities through kinematic instruction tuning data called STKit. First, in Sec. 3.1, we introduce seven tasks to categorize kinematic instructions in STKit. We then present spatio-temporal grounding framework to generate QA pairs in STKit with dynamic videos annotated with 3D point clouds in Sec. 3.2. Since 3D annotation is not always available, we propose pseudo-labeling pipeline based on 4D reconstruction for unlabeled videos in Sec. 3.3. Finally, we develop ST-VLM by fine-tuning LLaVA-OneVision [19] with STKit based on labeled and unlabled data in Sec. 3.4. 3.1. Kinematic instructions We introduce STKit, new kinematic instruction tuning dataset designed to enhance the spatio-temporal reasoning capabilities of VLMs. Kinematic instructions include measuring kinematic quantities of objects in dynamic videos, such as their trajectories, traveled distances, and movement directions. We categorize kinematic instructions into seven tasks to comprehensively cover different aspects of spatio-"
        },
        {
            "title": "Dataset",
            "content": "# QA pairs # Videos"
        },
        {
            "title": "Domain",
            "content": "3D annotation NuScenes [3] Argoverse2 [37] Ego-Exo4D [15] BDD100K [40] LLaVA-Video [48] MultiSports [23] 13K 8K 6K 86K 2K 1K 4K 0.6K 0.8K 15K 0.3K 0.2K autonomous driving autonomous driving sports autonomous driving general sports LiDAR LiDAR VIO/SLAM - - - Table 2. Data composition of STKit. We extract 21K dynamic videos and generate total of 116K kinematic instructions from six datasets. For videos without 3D annotations, we generate pseudolabels via 4D reconstruction using Monst3R [43]. Hence, we establish reference direction for each object based on its initial movement direction, calculated from the first two frames in which it appears, i.e., P(i) . Subsequent movement directions are computed as relative angles to this reference vector as: s+1 P(i) (cid:32) θt = arccos (P(i) P(i) t+1 P(i) t+1 P(i) ) (P(i) P(i) s+1 P(i) ) s+1 P(i) (cid:33) . (1) However, describing directions with angles is not intuitive, as humans typically do not use exact degrees. To make this more accessible for both humans and VLMs, we convert the calculated angles θt into discretized clockwise directions. In detail, we set the initial reference direction as 12 oclock, with subsequent directions expressed relative to this reference, as shown in Fig. 3. Due to the highly complex 3D trajectories in the sports domain, we exclude the movement direction category from that domain (see Fig. 1a for trajectory example). 3.3. Pseudo-labeling for unlabeled dynamic videos Existing datasets with 3D annotations mostly cover selfdriving scenarios, limiting applicability to broader domains. Thus, we propose pseudo-labeling pipeline based on 4D reconstruction to extend STKit to different domains of unlabeled video datasets like BDD100K [40], LLaVAVideo [48], and MultiSports [23] as in Tab. 2. Leveraging recent advances in geometric reconstruction and semantic understanding, we reconstruct 4D scenes by lifting segmented objects from 2D frames into 3D space. By applying 4D reconstruction, we extend the spatio-temporal grounding in Sec. 3.2 to various domains, enabling accurate estimation of object kinematics. See Fig. 4 for the pipeline. Geometric reconstruction branch. For the 4D reconstruction given the unlabeled video, we utilize MonST3R [43] which proposes 4D reconstruction framework that estimates scene geometry including depth and camera intrinsic/extrinsic, even in dynamic videos containing moving objects. However, the reconstructed space by MonST3R is not aligned with the real-world scale, since it lacks fixed reference for depth, resulting in reconstructions that are accurate in shape but arbitrary in size. This scale ambiguity poses significant problem for our spatio-temporal reasoning tasks such as traveled distances and traveling speed. To Figure 3. Movement directions as clockwise directions. temporal reasoning. These tasks are grouped into two main categories: Single Objects and Multiple Objects. Each is further divided into two subcategories: Distance and Direction. The seven spatio-temporal reasoning tasks are summarized in Tab. 1. The tasks encourage the model to understand both the absolute distance and direction of an objects movement, as well as the relative distance and direction by comparing multiple objects. Successfully managing these tasks requires the model to infer spatial information (e.g., object location) and temporal information (e.g., object movement), enabling complex spatio-temporal reasoning abilities built upon the prior knowledge of LLMs. 3.2. Spatio-temporal grounding of dynamic videos Generating QA pairs for STKit requires grounding the kinematic quantities of objects in dynamic videos. Videos with substantial object movement are most suitable for these tasks. Thus, we focus on diverse dynamic scenarios, such as autonomous driving and outdoor sports (e.g., football and basketball). Specifically, we use autonomous driving datasets, such as Argoverse2 [37] and NuScenes [3], which provide precise LiDAR-based 3D object coordinates represented in real-world scale at each timestamp. We also incorporate sports videos in Ego-Exo4D [15], captured from wearable AR devices [12] that provide both RGB images and IMU, enabling accurate 3D trajectory estimation via Visual-Inertial Odometry (VIO) and SLAM. These estimates are treated as ground-truth in practice. For each object annotated in the video, we have access to its 3D center and 3D bounding box coordinates in world space for each timestamp. Utilizing the 3D center coordinate P(i) of i-th object at seconds, we first construct the trajectories by sampling the center at 0.5-second intervals over 40-frames videos, covering up to 20 seconds. Then, we calculate the traveled distance of the i-th object between and seconds as the cumulative sum of distances between two consecutive frames, i.e., (cid:80)e1 2. The traveling speed is also calculated by dividing the total traveled distance by the duration s. t+1 P(i) t=s P(i) Calculating the movement direction for each object is more challenging than computing distance, as an absolute direction cannot be defined across all objects in the video. 4 Figure 4. Pseudo-label generation pipeline. For the geometric reconstruction branch, canonicalized 4D scene is reconstructed using MonST3R [43] and Metric3Dv2 [16]. For the semantic understanding branch, the object bounding boxes, segmentation masks, and trajectories are extracted using Grounded-SAM2 [32]. Finally, by integrating each branch, 2D object masks are lifted to 3D and trajectories are computed by tracking 3D barycenters in the 4D scene. address the scale ambiguity, we integrate Metric3Dv2 [16] to obtain the absolute metric depth at the real-world scale. Specifically, we canonicalize the reconstructed 4D scene by rescaling the original depth estimates from MonST3R to the metric depth from Metric3Dv2. Semantic understanding branch. We extract bounding boxes, segmentation masks, and trajectories of selected objects based on the open-vocabulary video semantic understanding model, Grounded-SAM2 [32]. We focus on classes of moving objects such as automobiles (e.g., cars, buses, trucks, motorcycles, bicycles) and humans. To ensure the reliability of pseudo-labels, we filter detected objects based on confidence scores and bounding box sizes. Spatio-temporal grounding in canonicalized 4D scene. By integrating the outputs from the geometric reconstruction branch and the semantic understanding branch, the 2D segmentation mask of the selected objects is lifted into 3D point cloud within the canonicalized 4D reconstructed scene. We calculate the traveled distance, speed and movement direction for each object in the 3D space by tracking the barycenter of 3D object coordinates across video frames. Due to inherent limitations in monocular 4D reconstruction such as partial visibility and viewpoint constraints, we empirically develop filtering and smoothing strategies by comparing calculated trajectories against ground-truth trajectories in labeled datasets [3]. This validation allows us to establish effective criteria for filtering and smoothing, increasing the reliability of generated pseudo-labels for unlabeled datasets [23, 40, 48]. See the supplement for details. Hence, we scale up our spatio-temporal reasoning dataset with the pseudo-label generation pipeline. 3.4. Kinematic instruction tuning Based on spatio-temporal groundings obtained by 27K point cloud labels (Sec. 3.2) and 89K 4D reconstructionbased pseudo-labels (Sec. 3.3), we construct STKit for kinematic instruction tuning. Specifically, with the distance and direction information, we adopt template-based approach to construct QA pairs for our instruction-following dataset. Tab. 1 provides example templates for task categories. To facilitate the models understanding, we introduce visual prompt by overlaying bounding box on each frame, highlighting the object of interest (see Fig. 1). We also provide common prompt including the timestamp and the color of the bounding box. The generated QA pairs and the videos with bounding boxes are fed into the model for training. We develop ST-VLM by fine-tuning LLaVA-OneVision 7B [19], which can accept various forms of visual inputs such as single images, multiple images, and videos, using our STKit dataset. However, fine-tuning solely on STKit degrades performance on other generic benchmarks, implying overfitting to this specific task. Hence, we blend STKit with subset of general supervised fine-tuning (SFT) datasets, LLaVA-Video-178K [48], which is common practice in recent VLM training [8, 24, 25]. By blending these datasets, we empirically observe that our model can handle complex reasoning instructions that are not present in the predefined templates. Detailed analyses are provided in Sec. 6.3. Furthermore, we adopt OpenSpatialDataset [8] to enhance the models spatial reasoning ability which is potentially advantageous in spatio-temporal reasoning. 4. STKit-Bench Since no benchmark exists for assessing the spatio-temporal reasoning capabilities of general VLMs, particularly in object kinematics, we introduce STKit-Bench, benchmark Models Traveled Distance Traveling Speed Movement Direction Direction Timestamp Traveled Distance Comparison Traveling Speed Comparison Movement Direction Comparison Average Acc MAE (m) Acc MAE (km/h) Acc MAE (clock) Acc IoU Acc Acc Acc Acc Single Object Multiple Objects closed-source models GPT-4o GPT-4V 4.5 4.5 autonomous driving-specific models Dolphins [28] ELM [51] 16.0 11.5 open-source models VideoLLaMA3-7B [41] Qwen2.5-VL-7B [1] InternVL2.5-8B [7] InternVideo2.5-8B [36] VideoChat-Flash-7B [22] LLaVA-Video-Qwen2-7B [48] LLaVA-OneVision-7B [19] ST-VLM-7B (Ours) 11.0 4.5 7.5 4.0 3.0 7.5 8. 49.5 31.2 33.1 241.9 43.1 54.9 206.4 177.3 487.8 72.8 74.0 60.2 25.1 7.5 4. 2.5 8.5 9.0 7.0 2.5 0.5 9.0 10.5 7.5 42.0 33.8 29.9 32.0 14.1 20.0 26.1 35.1 32.3 26.0 24.2 23. 11.6 11.0 15.5 5.0 7.0 8.0 3.0 8.0 2.5 2.5 13.5 4.5 32.0 2.7 2. 3.0 2.8 2.1 2.8 2.2 2.9 3.2 2.3 2.0 1.7 13.5 9.5 1.5 15.5 5.0 2.0 11.5 1.5 4.0 2.0 23. 69.0 0.20 0.11 0.07 0.15 0.11 0.04 0.10 0.10 0.10 0.05 0.22 0.61 49.0 50. 37.0 51.5 40.5 47.5 50.5 30.0 49.0 50.5 43.0 75.5 56.0 58.5 19.0 48.5 39.5 47.5 52.0 36.0 45.0 45.5 52. 76.5 46.0 57.0 41.0 51.0 23.5 46.0 50.0 34.5 46.5 44.5 53.0 74.0 26.8 28. 17.4 27.6 19.5 22.5 26.0 15.6 22.7 24.9 27.4 59.8 Table 3. Results on STKit-Bench. The average accuracy is reported in the last column. racy (correct if IoU(y, ˆy) 0.5) and IoU. (5) Traveled Distance Comparison, (6) Traveling Speed Comparison, and (7) Movement Direction Comparison: Accuracy. Comparison with other benchmarks. Recently, several video benchmarks have been proposed in the literature [13, 21, 31, 50]. For example, MLVU [50] aims to assess video-based VLMs for long-form video understanding, and VideoMME [13] focuses on the comprehensive perception ability of the model on wide range of domains. To tackle the problem that most VLMs overlook the temporal information, MVBench [21] has been proposed by covering diverse temporal understanding tasks, e.g., action sequence understanding, action prediction, and counterfactual inference. More recently, several works [11, 30, 34, 51] have been introduced as video spatio-temporal understanding benchmark for autonomous driving scenes. In contrast, our STKit-Bench covers general scenes (e.g., sports) not limited to autonomous driving scenarios. 5. Experiments In this section, we compare ST-VLM with baselines on STKit-Bench across diverse experimental settings to ensure robust evaluation. 5.1. Experimental settings Baselines. To validate the spatio-temporal reasoning ability of ST-VLM, compared to strong proprietary models, we choose GPT-4V and GPT-4o as baselines. Various Opensource video-based VLMs, including VideoLLaMA3 [41], Qwen2.5-VL [1], InternVL2.5 [7], InternVideo2.5 [36], LLaVA-Video-Qwen2 [48], and LLaVA-OneVision [19] are selected as baselines as well. Implementation details. For pseudo-labeling data, we construct 89K QA pairs from 15.5K dynamic videos sourced from BDD100K [40], LLaVA-Video [48], and MultiSports [23]. 4D scene reconstruction with MonST3R requires approximately 400 seconds per video on single Figure 5. Statistics of STKit-Bench. We balance the number of samples for each label to prevent biased results. Red and green bars indicate the number of samples before/after balancing. comprising four datasets spanning the autonomous driving and sports domains. For autonomous driving, we incorporate NuPlan [4] for robust evaluation on an unseen dataset with NuScenes [3] and Argoverse2 [37], which all provide LiDAR-based annotations. For sports, we use EgoExo4D [15] with SLAM-based point clouds. We adopt the official validation splits to construct STKit-Bench, with dataset proportions of 74.8% (NuPlan), 12.5% (NuScenes), 5.6% (Argoverse2), and 7.1% (Ego-Exo4D). Each task in STKit-Bench includes 200 QA pairs, resulting in total of 1,400 QA pairs. However, directly adopting generated QA pairs for the benchmark exhibits long-tail label distribution. The red-colored bars in Fig. 5 highlight the unbalanced label distribution in both distance and direction categories. Therefore, to prevent biased evaluation results in STKitBench, we balance the number of samples for each label, as shown by the green-colored bars. For evaluation, we use GPT-4 to extract the prediction from the response in natural language. Then, we compare the prediction and the ground-truth answer and measure the performance by adopting the following metrics: Given the ground-truth answer and the prediction ˆy, (1) Traveled Distance and (2) Traveling Speed: Accuracy (correct if 0.75 ˆy 1.25) and MAE (y ˆy). (3) Movement Direction: Accuracy (correct if = ˆy in the clockwise direction) and MAE (min(y ˆy, 12 ˆy) in the clockwise direction). (4) Direction Timestamp: Accu6 -shots geo. TD TS MD DT TDC TSC MDC Avg Label Pseudo-label TD TS MD DT TDC TSC MDC Avg Models GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V ST-VLM (Ours) 0 1 3 0 1 3 - - 4.5 4.0 8.0 5.0 3.5 6.5 4.5 3.5 5.5 4.0 4.5 6. 15.5 14.0 16.0 9.5 9.0 16.0 9.5 8.5 49.5 9.0 7.5 52.5 50.0 52.5 55.0 51.5 47.5 42. 58.5 55.5 49.0 58.5 49.5 49.5 49.5 42.0 32.0 69. 75.5 76.5 57.0 55.5 46.5 52.5 47.5 44.0 74.0 28.5 27.6 32. 27.1 24.1 31.0 59.8 Table 4. Comparison with improved baselines. We provide few-shot examples and additional geometric contexts (denoted as geo.), i.e., camera extrinsics and depth maps, to GPT-4V and report the accuracy for each task and the average accuracy."
        },
        {
            "title": "Models",
            "content": "TD"
        },
        {
            "title": "Avg",
            "content": "GPT-4o GPT-4V LLaVA-OneVision 6.5 5.0 6.5 8.5 7.5 5.0 9.0 12.0 4.5 11.0 12.5 16.0 46.5 50.5 55. 56.0 55.5 58.0 ST-VLM (Ours) 46.0 41.5 33.5 67. 76.0 72.5 51.5 48.0 44.5 70.5 27.0 27.3 27.1 58. Table 5. Results on paraphrased STKit-Bench. A6000 GPU. We train our model for 1 epoch with batch size of 128. The cosine learning rate scheduler is adopted with learning rate of 1e-5 and the total training takes three days with 8 A6000 GPUs. For instruction tuning data composition, we blend STKit (117K) with subsets of 100K samples from LLaVA-Video-178K [48] and 20K samples from OpenSpatialDataset [8], respectively. 5.2. Results on STKit-Bench Tab. 3 presents results on STKit-Bench, comparing STVLM to baseline VLMs. Proprietary models, GPT-4V and GPT-4o, struggle with spatio-temporal reasoning on STKitBench. In the Traveled Distance (TD) category, GPT-4V achieves an accuracy of only 4.5% with mean absolute error (MAE) of 33.1, indicating an average gap of 33.1 from the ground-truth distance. Open-source models also face challenges with spatio-temporal reasoning. For instance, LLaVA-OneVision [19] demonstrates poor performance with an average accuracy of 27.4%. Furthermore, we evaluate the performance of baseline models in autonomous driving, including Dolphins [28] and ELM [51], both of which are trained on road scene videos. However, we observe that both Dolphins and ELM are specifically trained with instructions for autonomous driving scenarios, making them unable to generalize to new types of instructions, resulting in poor performance with average accuracies of only 17.4% and 27.6%, respectively. Our ST-VLM surpasses all baselines across seven tasks by significant margin, e.g., achieving 32.4% higher average accuracy than LLaVAOneVision, introducing new spatio-temporal reasoning capabilities which were absent in previous models. - - - 10.0 28.0 49.5 15.0 31.0 42.0 4.5 33.5 32.0 20.5 65.5 69.0 43.5 65.0 75.5 53.0 64.5 76. 48.5 70.0 74.0 27.9 51.1 59.8 Models VILA-40B [24] PLLaVA-34B [38] LLaVA-N-Video-34B [47] LongVA-7B [46] IXC-2.5-7B [45] LLaVA-N-Video-32B [47] LLaVA-OneVision-7B [19] ST-VLM-7B (Ours) Table 6. Ablation on data composition. PerceptionTest MVBench VideoMME MLVU NExT-QA Avg. acc w/o & w/ subtitle m-avg test test val 54.0 - 51.6 - 34.4 59. 57.1 64.5 - 58.1 - - 69.1 - 56.7 57.4 60.1 & 61.1 - & - 52.0 & 54.9 52.6 & 54.3 55.8 & 58.8 60.2 & 63.0 58.2 & 61.5 60.5 & 63.3 - - - 56.3 37.3 65. 64.7 66.9 67.9 - 70.2 68.2 71.0 77.3 79.4 80.8 60.8 58.1 57.2 77.1 54.4 65.1 62.9 65.6 Table 7. Results on comprehensive video benchmarks. Figure 6. Comparison on LLaVA-OneVision and ST-VLM for spatio-temporal understanding. mIoU is multiplied by 100. 5.3. Robust evaluation across diverse settings To ensure robust and fair evaluation, we present two experimental settings. First, we augment the baselines with contextual support for spatio-temporal reasoning and compare them to our method. Specifically, we provide GPT-4V with QA pairs in few-shot manner, and additionally give geometric context as inputs (e.g., camera extrinsics as textual prompts and depth maps as extra image inputs) to match the intrinsic geometric knowledge implicitly learned by STVLM from 3D point clouds. As shown in Tab. 4, increasing the number of few-shot examples indeed improves performance; however, giving extra geometric contexts leads to performance degradation. Nevertheless, GPT-4Vs performance still remains significantly lower than that of STVLM. Second, we verify the robustness of ST-VLM to variations in questions on STKit-Bench by paraphrasing questions using GPT-4. In Tab. 5, ST-VLM maintains robust performance across paraphrased questions, outperforming other methods. This indicates that ST-VLMs strong performance arises from its inherent kinematic understanding rather than overfitting to specific question templates. 6. Analysis In this section, we provide in-depth analyses to answer the following research questions: Q1. How effective is our instruction tuning dataset, STKit? (a) (b) Figure 7. Qualitative results on emerging capabilities of ST-VLM with multi-step reasoning questions. Q2. Does the spatio-temporal reasoning ability of ST-VLM generalize across various domains and tasks? Q3. Does ST-VLM exhibit emergent capabilities, combining spatio-temporal reasoning (learnt ability) with LLMs knowledge (existing ability) within multi-step reasoning? 6.1. Data composition analysis of STKit We present the ablations on STKit introduced in Sec. 3.2 and 3.3 to discuss Q1. In Tab. 6, we compare the baseline performance of LLaVA-OneVision [19] against models fine-tuned with different data configurations. Incorporating data made with labeled videos significantly improves the average performance from 27.9% to 51.1%. Furthermore, using pseudo-labeled data generated from unlabeled videos further enhances performance by 8.7% improvement, highlighting the effectiveness of our 4D reconstruction-based pseudo-labeling pipeline. 6.2. Generalized spatio-temporal understanding We assess the generalization capacity of ST-VLMs spatiotemporal reasoning by evaluating it across various tasks and domains in zero-shot setting for Q2. First, Tab. 7 shows that ST-VLM trained with STKit outperforms LLaVAOneVision by 2.7% in average accuracy even on general video understanding benchmarks, and it further surpasses LLaVA-N-Video-32B [47] on average, which is the larger model than ours. Notably, ST-VLM achieves 7.4% improvement on PerceptionTest [31], which is benchmark that thoroughly evaluates the comprehension of fundamental physical properties in videos, by leveraging kinematic priors from STKit. See the supplement for further qualitative analyses. Furthermore, Fig. 6 compares the performance of our ST-VLM with LLaVA-OneVision on various spatiotemporal reasoning tasks other than proposed STKit-Bench. For spatial reasoning, we design region-aware VideoQA task by combining Referring Expression Generation and VideoQA. We reformulate existing VideoQA datasets, TVQA+ [18] and DramaQA [10], by providing specific region and posing questions related to that region. For temporal reasoning, we conduct temporal grounding tasks on TVQA+, ActivityNet [2], and Charades-STA [14], requiring the model to predict timestamps for query actions. Notably, TVQA+ involves both spatial and temporal reasoning, as it requires answers with supporting timestamps based on the question and specified region. In Fig. 6, on temporalaware tasks, ST-VLM demonstrates improved performance in both ActivityNet and Charades-STA datasets, measured by mean Intersection over Union (mIoU) and Recall at IoU 0.3 (R@0.3). For instance, R@0.3 on ActivityNet increases by 17.0%. Additionally, in the spatial-aware task, ST-VLM outperforms LLaVA-OneVision in DramaQA by margin of 4.6% in accuracy. Finally, in TVQA+, both spatial and temporal grounding abilities of ST-VLM are improved compared to LLaVA-OneVision. 6.3. Emerging capabilities of ST-VLM Finally, we answer Q3 through qualitative analyses in Fig. 7 and Fig. 1b, showcasing ST-VLMs emerging multi-step reasoning capabilities involving spatio-temporal reasoning. Despite not being explicitly trained for complex reasoning, ST-VLM effectively integrates kinematic reasoning with VLMs existing abilities, such as knowledge retrieval, logical reasoning, and arithmetic computation. For example, in Fig. 7a, to answer, How much slower is this objects speed compared to normal airplane?, model must (1) know the average speed of normal airplane, (2) estimate the objects speed from the video, and (3) perform arithmetic to compare them. Leveraging kinematic reasoning, ST-VLM provides an accurate answer (36.58 times slower than normal plane), while the LLaVA-OneVision offers less precise response (10 times slower than normal plane). Similarly, in Fig. 7b, identifying the faster player requires recognizing teams by jerseys and estimating player speeds. ST8 VLM correctly does so, whereas the baseline fails. These examples illustrate the effectiveness of our STKit-trained ST-VLM in multi-step reasoning that incorporates spatiotemporal understanding. 7. Conclusion We present ST-VLM, VLM with enhanced spatiotemporal reasoning capabilities, achieved through an enhanced understanding of kinematics in dynamic videos. To this end, we introduce STKit and STKit-Bench, which define seven fundamental tasks using 3D-annotated video data. Additionally, our 4D reconstruction-based data generation pipeline effectively mitigates the scarcity of 3Dannotated data, leading to improved performance on STKitBench. Extensive analyses reveal that ST-VLM exhibits strong generalization across various video benchmarks and enables multi-step reasoning as an emerging capability, leveraging the combination of LLMs pretrained knowledge and newly acquired kinematic understanding."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [2] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In CVPR, benchmark for human activity understanding. 2015. 8 [3] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In CVPR, 2020. 2, 4, 5, 6 [4] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuplan: closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021. 6 [5] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. In ICLR, 2025. 3 [6] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 2, [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 6 [8] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. In NeurIPS, 2024. 2, 3, 5, 7 [9] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 3 [10] Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang, Minsu Lee, and Byoung-Tak Zhang. Dramaqa: Character-centered video story understanding with hierarchical qa. In AAAI, 2021. 8 [11] Xinpeng Ding, Jianhua Han, Hang Xu, Xiaodan Liang, Wei Zhang, and Xiaomeng Li. Holistic autonomous driving understanding by birds-eye-view injected multi-modal large models. In CVPR, 2024. 6 [12] Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith, et al. Project aria: new tool for egocentric multi-modal ai research. arXiv preprint arXiv:2308.13561, 2023. 2, 4 [13] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [14] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, 2017. 8 [15] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In CVPR, 2024. 2, 4, 6 [16] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. 5 [17] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In CoRL, 2024. 3 [18] Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. Tvqa+: Spatio-temporal grounding for video question answering. In ACL, 2020. 3, 8 [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 3, 5, 6, 7, 8 [20] Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, and Si Liu. Llava-st: multimodal large language model for fine-grained spatialtemporal understanding. In CVPR, 2025. [21] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. CVPR, 2024. 3, 6 [22] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, 9 Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 6 [23] Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gangshan Wu, and Limin Wang. Multisports: multi-person video dataset of spatio-temporally localized sports actions. In ICCV, 2021. 2, 4, 5, 6 [24] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. 3, 5, [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 5 [26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. 3 [27] Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, et al. Spatialcot: Advancing spatial reasoning through coordinate alignment arXiv and chain-of-thought for embodied task planning. preprint arXiv:2501.10074, 2025. 3 [28] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal language model for driving. In ECCV, 2024. 3, 6, 7 [29] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In ACL, 2023. 3 [30] Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, and Li Zhang. Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving. In ECCV, 2024. [31] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adri`a Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Joao Carreira. Perception test: diagnostic benchmark for multimodal video models. In NeurIPS, 2023. 6, 8 [32] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 5 [33] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [34] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose M. Alvarez. OmniDrive: holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. arXiv preprint arXiv:2405.01533, 2024. 3, 6 [35] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation modarXiv preprint els for multimodal video understanding. arXiv:2403.15377, 2024. 3 [36] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [37] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, et al. Argoverse 2: Next generation datasets for self-driving perception and forecasting. In NeurIPS Datasets and Benchmarks, 2023. 2, 4, 6 [38] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 3, 7 [39] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In CVPR, 2025. 3 [40] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: diverse driving dataset for heterogeneous multitask learning. In CVPR, 2020. 2, 4, 5, 6 [41] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 6 [42] Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee. Countercurate: Enhancing physical and semantic visiolinguistic compositional reasoning via counterfactual examples. In ACL Findings, 2024. [43] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. In ICLR, 2025. 4, 5 [44] Min Zhang, Jianye Hao, Xian Fu, Peilong Han, Hao Zhang, Lei Shi, Hongyao Tang, and Yan Zheng. Mfe-etp: comprehensive evaluation benchmark for multi-modal foundaarXiv preprint tion models on embodied task planning. arXiv:2407.05047, 2024. 3 [45] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 7 [46] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 7 [47] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava10 next: strong zero-shot video understanding model, 2024. 3, 7, [48] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2, 4, 5, 6, 7 [49] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In CVPR, 2020. 3 [50] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 6 [51] Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu, Minyi Guo, Yu Qiao, and Hongyang Li. Embodied understanding of driving scenarios. In ECCV, 2024. 3, 6,"
        }
    ],
    "affiliations": [
        "KAIST",
        "Korea University",
        "NEC Labs America"
    ]
}