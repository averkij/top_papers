{
    "paper_title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
    "authors": [
        "Kechen Li",
        "Wenqi Zhu",
        "Coralia Cartis",
        "Tianbo Ji",
        "Shiwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 4 5 0 2 . 2 0 5 2 : r SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers Kechen Li1 Wenqi Zhu3 Coralia Cartis3 Tianbo Ji2 Shiwei Liu3 1Nanjing University of Aeronautics and Astronautics 2School of Transportation and Civil Engineering, Nantong University 3Mathematical Institute, University of Oxford kechenli@nuaa.edu.cn, jitianbo@ntu.edu.cn {wenqi.zhu, cartis, shiwei.liu}@maths.ox.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. et al., 2023; Yoshikawa et al., 2023), their ability to reason has emerged as central topic of interest (Wei et al., 2022; Huang and Chang, 2022). Among these, mathematical reasoning stands out as one of the most rigorous and demanding (Kant, 1908; In this work, we investigate fundamental yet Hendrycks et al., 2021; Ahn et al., 2024; Liu et al., computationally intractable problem: determin2024). As result, the ability of LLMs to solve ing whether given multivariate polynomial is nonnegative. This problem, closely related to Hilberts Seventeenth Problem, plays crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, meticulously curated dataset of approximately 1,000 polynomials, along with research-level mathematical problems is not only critical benchmark for evaluating their reasoning capabilities but also has the potential to transform mathematical research and practice. Demonstrated by the success of OpenAI o1 (OpenAI, 2024) and DeepSeek-R1 (Guo et al., expert-designed reasoning instructions based 2025), test-time scaling has emerged as promison five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline (50%). However, high-quality reasoning instructions significantly improve accuracyboosting performance up to 81%. Furing technique for enhancing LLMs performance in mathematical reasoning (Snell et al., 2024a; Welleck et al., 2024). This approach involves prompting LLMs to generate more reasoning steps, either sequentially (Snell et al., 2024b; Hou et al., 2025; Lee et al., 2025) or in parallel (Brown et al., thermore, our 7B model, SoS-7B, fine-tuned on 2024; Xin et al., 2024), to increase the accuracy SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems. Code is available at https://github.com/Joe-2002/SoS1."
        },
        {
            "title": "Introduction",
            "content": "of their final answers. However, the communitys primary focus has been limited to relatively simple levels of mathematics, ranging from high school and Olympiad-level problems to early undergraduate topics (of America, 2024; Hendrycks et al., 2021). Whether the promise of test-time scaling extends to research-level mathematics remains an open question. In this paper, we investigate fundamental yet With Large Language Models (LLMs) reaching formally well-posed problem in mathematics: dehuman-level proficiency across diverse range of termining whether given multivariate polynomial tasks (Brown et al., 2020; Singhal et al., 2023; Cai is nonnegative. This question is closely related to 1 Hilberts Seventeenth Problem, which was posed challenging criteria: polynomial degree, nonnegaby David Hilbert in 1900 as part of his famous 23 tivity of the leading search direction, identification problems presented at the International Congress of special structures, assessment of square-form of Mathematicians (ICM) (Hilbert, 1893), and it expressions, and matrix decomposition into the remains central to global polynomial optimization. quadratic form of monomials. Our comprehenMany key challenges in applied and computational sive evaluation of multiple SOTA LLMs, includmathematics can be reframed as deciding the noning DeepSeek-R1, DeepSeek-V3, GPT-4o, OpenAI negativity of certain polynomials including control o1-mini, Qwen2.5 series, and QwQ-32B-Preview, theory (Parrilo, 2000), quantum computation demonstrate the following interesting findings: (Doherty et al., 2002), polynomial games (Gvozdenovic and Laurent, 2007), tensor methods (Zhu and Cartis, 2024; Ahmadi et al., 2023a) and combinatorial optimization (Gvozdenovic and Laurent, 2007). Testing whether general polynomial is nonnegative is provably NP-hard, even for polynomials of relatively low degrees or with small number of variables. People usually seek special cases of polynomials where the challenging nonnegativity constraints can be replaced with more manageable conditions. For instance, the sum of squares (SoS) condition, mathematical technique in polynomial optimization where polynomial is expressed as sum of squared polynomials, provides sufficient criterion for polynomial nonnegativity. Classical solvers, such as SoSTOOLS, YALMIP, and Gloptipoly have been developed to verify these When presented with plain question, all SOTA LLMs bluntly fail to solve SoS with most models achieving only around 60% accuracy, just slightly above the random guess baseline of 50%.1 When prompted with high-quality reasoning traces, we consistently observe significant accuracy boost across all models, up to 21%. And models perform better with higher-quality reasoning traces. Reasoning-focused LLMs generally outperform general-purpose LLMs, regardless of prompt quality. Higher-capacity models require fewer thinking tokens to make correct predictions, while lowercapacity models need more reasoning steps to SoS conditions (Prajna et al., 2002). However, achieve their optimal performance. significant limitation of these approaches lies in the typically large size of the resulting semidefinite programming (SDP) problem. Specifically, for polynomial with variables and degree of 2d, the (cid:1), makSDPs dimension is given by = (cid:0)n+2d 2d ing it challenging to scale this approach to larger problems. We further demonstrate that supervised finetuning (SFT) of pre-trained 7B model on SoS1K for just 4 hours using 2 A100 GPUs significantly improves accuracy from 54% to 70% with significantly faster response times. Specifically, SoS-7B requires only 1.8% and 5% of the computation time needed for DeepSeek-V3 and GPTTo evaluate if state-of-the-art (SOTA) reason4o-mini, respectively. The resulting SoS-7B suring LLMs like Openai o1 and DeepSeek-R1 can passes much larger models, including the 671B solve large-scale SoS programming problems, we DeepSeek-V3 and GPT-4o-mini. More interestintroduce SoS-1K, meticulously curated dataset ingly, when prompted with high-quality reasonof approximately 1,000 polynomialsalong ing, the models demonstrate an understanding of with expert-designed, SoS-specialized, reasoning1Since SoS is binary classification problem, random guiding instructions based on five progressively guessing yields 50% accuracy. 2 Figure 1: Demonstration of SoS Plain (left), SoS Simple (mid), and SoS Reasoning (right). research-level questions. For instance, Qwen2.514B-Instruct-1M leverages the Motzkin polynomial to generate new, previously unseen counterexam2001). The development of numerical solvers began in 2009, with tools like GloptiPoly and SoSTOOLS introduced in (Henrion et al., 2009; Paples to Hilberts 17th problem (Motzkin, 1967). pachristodoulou et al., 2013). More details on Such examples are nontrivial, as the first counterexthe literature review for SoS can be found in Apample to Hilberts 17th problem was discovered pendix A. 27 years after Hilbert originally posed it (Hilbert, The first attempt to tackle SoS-related mathemat1893). These findings suggest that LLMs exhibit ical difficulty via AI was presented in (Alfarano reasoning patterns, expanding the boundaries of et al., 2024). The authors trained Transformers to solving NP-hard problems. address long-standing open problem in mathematOur work serves as pilot study on leveraging ics: discovering Lyapunov function that guaranreasoning LLMs for solving SoS problems, paving tees the global stability of dynamical system. The the way for tackling large-scale research-level quesglobal stability of polynomial Lyapunov systems tions in mathematics using AI. 1.1 Related Work is closely tied to SoS framework. Their approach was tested on relatively small systems (with at most five equations for polynomial systems) and demonThe study of SoS and nonnegative polynomials strated promising results compared to state-of-thehas rich history spanning over 120 years, with art conventional solvers. numerous scholars contributing to this field. Classical methods for characterizing an SoS polynomial"
        },
        {
            "title": "2 SoS-1K Dataset",
            "content": "involve expressing the polynomial as quadratic In this section, we first provide the definition of SoS form of monomials and then reformulating the polynomials. We describe the expert-annotated reaproblem as semidefinite program (SDP) (Lasserre, soning traces based on five criteria of increasing 2000, 2001). Classic techniques, such as the difficulty in Section 2.1, which lead to the generaLasserre hierarchy, date back to 2001 (Lasserre, tion of the SoS-1K dataset, detailed in Section 2.2. 3 Definition 2.1 SoS polynomial (Ahmadi et al., Specifically, we provide logical reasoning trace 2023a; Ahmadi and Parrilo, 2013; Kojima, 2003) 2d-degree multivariate polynomial q(x) : Rn where = [x1, . . . , xn] Rn is sum of squares (SoS) if there exist polynomials q1, . . . , qr : Rn R, for some N, such that q(x) = (cid:80)r j=1 qj(x)2 for all Rn (Ahmadi et al., 2023a, Def. 1). Further details on the theory for SoS polynomials can be found in Appendix B. 2.1 Expert-Designed Reasoning Instructions for SoS Instructions play crucial role in guiding LLMs toward better reasoning and problem-solving (Zhang et al., 2023). The way an LLM processes problem can be significantly improved with carefully crafted instructions that provide structure, constraints, and logical flow. To evaluate the capacity of SOTA LLMs on SoS reasoning, we create three sets of reasoningguiding instructions with increasing quality that can be applied across multiple SoS problem types: (1) plain question (SoS Plain); (2) simple SoS inbased on proofs and theorems, offering necessary and sufficient conditions for identifying SoS polynomials. large number of positive and negative examples accompany each set of theorems, helping the model recognize special structures, symmetries, and mathematical forms inherent to SoS polynomials. Additionally, SoS Reasoning introduces intermediate steps and incorporates key and challenging reasoning processes, such as the matrix and the squared form ps, enabling deeper reasoning and iterative refinement. Below is an illustration of SoS Reasoning. The full version is provided in Appendix D. Step 1. Check the Degree: An SoS polynomial must have an even highest degree. Step 2. Check for Non-negativity: SoS polynomials are nonnegative for all real inputs. We verify this by examining the constant term, the coefficients of the leading term, and performing grid-based numerical check. Step 3. Check for Well-known Special Cases: Any nonnegative quadratic polynomial and any nonnegative quartic polynomial in one or two varistruction (SoS Simple); (3) reasoning-guiding SoS ables is SoS. instruction (SoS Reasoning). SoS Plain simply asks LLMs: Please analyze if this polynomial can be expressed as Sum of Squares (SOS). SoS Simple classifies SoS polynomials into five distinct groups, each defined by concise, one-line criterion. The full instruction set for SoS Simple contains 78 words and 647 characters, with complete details provided in Appendix C. SoS Reasoning is structured five-step framework designed to identify SoS polynomials. Unlike SoS Simple, which provides only basic classification criteria, SoS Reasoning encourages the model follow step by step mathematical verification process. The framework introduces progressively more detailed reasoning steps to guide the Step 4. Check for Square Form: By Definition 2.1, an SoS polynomial can be expressed as: ps(x) = (cid:80) qi(x)2, where each qi(x) is polynomial. Step 5. Check for Matrix Decomposition: Based on Theorem B.1, we express the polynomial as p(x) = yQy, where is symmetric matrix2. We then check whether is positive semidefinite. 2.2 Construction of SoS-1K Test Set Building on the above expert design, we construct SoS-1K, dataset comprising five subsets of polynomials, each corresponding to polynomials filtered out at steps 15. Appendix provides comprehensive summary of the SoS-1K test set and its model in verifying whether polynomial is SoS. 2Q and are provided in Appendix 4 subsets. Since most LLMs struggle with very long mance boost when prompted with high-quality polynomials, we have ensured that the majority rereasoning-guiding instructions. We consistently main within length of 4,000. Approximately half observe substantial accuracy improvement across of the polynomials are SoS, while the other half are all models. With SoS Simple, QwQ-32B-Preview not. The number of variables and the polynomial achieves 71% accuracy, while DeepSeek-R1 with degree both range from 2 to 10. For all test subsets the highest-level SoS Reasoning reaches the high- (except Set 1), we provide two corresponding sets: est accuracy (81%). It suggests that while LLMs one containing SoS polynomials and another conmay possess the underlying knowledge to solve taining non-SoS polynomials. Each polynomial is SoS problems, they require clear and structured labeled as either SoS or non-SoS, with accompanyinstructions to effectively retrieve and apply it. ing justifications and difficulty levels. For certain Furthermore, it is worth mentioning that SoS polynomial classes, we provide theoretical proofs Reasoning plays crucial role in achieving these confirming their SoS status, while the remaining improvements. The improvement from SoS Simple polynomials are labeled based on results from stanover the baseline (SoS Plain) is relatively small, dard solvers."
        },
        {
            "title": "3 Evaluation of LLMs on SoS",
            "content": "In this section, we compare the performance of SOTA LLMs using SoS Plain, SoS Simple, and SoS Reasoning on subset of SoS-1K of approximately 340 randomly chosen from all sub-classes of test problems (see Table 3). These test samples are drawn such that the number of samples in each subtest is approximately equal. We test across subclasses and report results per test subclasses. The models evaluated include reasoning-purpose models like DeepSeek-R1, OpenAI o1-mini, and QwQ32B-Preview, as well as general-purpose models such as DeepSeek-V3, GPT-4o, and Qwen2.5 series. The full results for each model on each test set are summarized in Table 1. We summarize our main observations below: OB1. All LLMs fail when presented with plain question. When given SoS-plain, all LLMs exhibit poor accuracy, ranging from 50% to 60%, with QwQ-32B-Preview being the sole exception, averaging 5% increase in the accuracy of valid samples and 6% increase in the accuracy of valid samples, whereas SoS Reasoning improves performance by 17% and 11%, respectively over the same baseline. OB3. Reasoning-purpose LLMs benefit more from high-quality instructions than generalpurpose LLMs. Overall, reasoning-focused LLMs such as DeepSeek-R1, OpenAI o1-mini, and QwQ-32B-Preview achieve higher average accuracy (79.0%) compared to general-purpose LLMs (72.9%). This result suggests that stronger reasoning capabilities contribute to improved performance in solving SoS problems. OB4. Many LLMs struggle to consistently provide valid answers to SoS questions. Most LLMs fail to consistently provide valid answers, often encountering timeout issues. Nevertheless, GPT-4o, DeepSeek-V3, and o1-mini demonstrate robustness in this regard, consistently producing effective and reliable answers."
        },
        {
            "title": "4 Further Analysis",
            "content": "achieving 64% valid accuracy. This result suggests In this section, we outline several research questhat, despite being trained on vast amounts of mathtions that the authors find particularly intriguing. ematical data, SOTA LLMs struggle to solve SoS Q1: Does the model follow truly mathematical problems without explicit prompting. step by step verification process? OB2. LLMs have significant perforWe find that LLMs are able to generate answers Model Accuracy on Valid Samples Accuracy on Total Samples Response Time (s) Instruction Type SoS Plain SoS Simple SoS Reasoning SoS Plain SoS Simple SoS Reasoning SoS Plain SoS Simple SoS Reasoning Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct-1M Qwen2.5-14B-Instruct Qwen2.5-14B-Instruct-1M Qwen2.5-32B-Instruct DeepSeek-V GPT-4o-mini GPT-4o QwQ-32B-Preview DeepSeek-R1 OpenAI o1-mini Average 55% 54% 55% 56% 56% 54% 59% 60% 64% 62% 58% 57% 61% 64% 66% 60% 58% 60% 67% 61% 71% 62% 61% 63% General-purpose LLMs 52% 54% 52% 56% 55% 54% 59% 59% 59% 64% 66% 59% 58% 60% 67% 61% Reasoning-purpose LLMs 44% 55% 57% 54% 54% 55% 61% 60% 76% 75% 74% 74% 67% 70% 72% 75% 79% 81% 77% 75% 62% 63% 69% 67% 62% 69% 69% 75% 52% 56% 76% 65% 22.4 5.6 12.9 12. 13.0 29.6 10.8 14.6 105.7 514. 8.3 68.2 31.2 8.4 23.1 20. 18.0 39.8 15.4 16.2 101.8 565. 18.1 78.0 68.5 35.2 48.3 52. 37.4 95.0 53.1 27.8 100.0 492. 34.9 95.0 Table 1: Accuracy comparison across various SOTA LLMs on subset of SoS-1K with 340 samples. Results are divided into Valid Samples and Total Samples, as we found that LLMs sometimes suffer from timeout issues. that are both logically and mathematically correct, step-by-step, following our SoS Reasoning instrucunder SoS Plain, SoS Simple, and SoS Reasoning.3 We observe that the simplest test set, Test Set 1 tion. For instance, we demonstrate o1-minis re- (corresponding to Step 1), unsurprisingly achieves sponse in Appendix where we can observe that 100% accuracy across all prompting methods. For the responses are logically and mathematically corthe more challenging test sets, Test Sets 2a, 3.1a, rect, and the model stops naturally once it derives 5.1a5.4a, we observe continuous improvement an answer, rather than blindly going through all from SoS Plain to SoS Simple and further to SoS possible steps. Q2: Can LLMs effectively retrieve critical information from long-context polynomials? Unlike standard text input, polynomials are complex algebraic expressions consisting of variables, coefficients, exponents, and terms. Thus, it is crucial for LLMs to effectively interpret and extract critical information from such structured formats. Our analysis reveals that while QwQ-32B-Preview struggles with questions exceeding 4K tokens in length, most SOTA LLMs can successfully extract the necessary coefficients from 4K-length polynomials for evaluation, producing correct answers. Q3: At which of the Steps 1 through 5 in SoS Reasoning does the accuracy improve? In Figure 2, we illustrate the accuracy improveReasoning. This improvement is attributed to Steps 2 and 5 in SoS Reasoning, where series of mathematical verification methods for non-negativity are introduced, including constant coefficient check, grid evaluation, leading order and dominant terms comparison, finding minima, matrix decomposition, and finding symmetry and translation. Q4. Will LLMs become lazy (take shortcuts) during reasoning? Yes, another interesting phenomenon observed under the SoS Reasoning prompt is that the model tends to be lazy in Step 5. Specifically, instead of fully executing Step 5, it often avoids matrix decomposition or semidefinite programming (SDP) due to complexity and instead guesses an answer based on prior steps. This behavior is particularly ment across different test sets for the o1-mini model 3Similar patterns are observed for other models. 6 Figure 2: Accuracy of different test sets using o1-mini. prevalent for long inputs and complex polynomials, such as those in Test Set 5.4a. For simpler probFigure 3: Number of correct samples with various response lengths. Table 2: Accuracy Comparison on SoS Reasoning Benchmark, where \"\" denotes the undisclosed model size. Accuracy is measured on full evaluation samples. lems, reasoning models such as o1-mini (which Model Size Acc. (%) had the shortest runtime of 17s) and larger modClosed Source els like QwQ-32B-Preview tend to take shortcuts, skipping Step 5 and inferring the answer from earGPT-4o o1-mini lier, simpler steps. In contrast, DeepSeek-V3 does Open Source not take shortcuts and instead spends significantly more time solving all steps properly (40s). Q5: How does reasoning length affect accuracy? Figure 3 shows that higher-capacity models generally require fewer thinking tokens to make correct predictions, whereas lower-capacity models need more reasoning steps to reach optimal performance. For instance, DeepSeek-R1 and o1-mini achieve the highest number of correct predictions with 1K2K response length, whereas the Qwen2.5 series require 3K4K tokens to produce correct answers. 7B 32B 32B 671B 671B 7B 75 76 63 62 52 69 56 70 Qwen2.5-7B-Instruct-1M Qwen2.5-32B-Instruct QwQ-32B-Preview DeepSeek-V3 DeepSeek-R1 SoS-7B (Ours) in cases where the quadratic form of the polynomial involves low-rank matrix decomposition."
        },
        {
            "title": "5 Performing SFT on SoS-1K",
            "content": "Q6: Do SOTA LLMs have any limitations? We further conduct supervised fine-tuning (SFT) Though we demonstrate that SoS Reasoning efwith Qwen2.5-7B-Instruct-1M on SoS-1K using fectively improves accuracy, it is subject to the LLaMA-Factory (Zheng et al., 2024). The trainfollowing limitations. Firstly, for long input cases, ing process was performed on 2 NVIDIA A100 invalid samples occur. For example, in DeepSeekGPUs for 4 hours. The resulting model, SoS-7B, R1, only 234 out of 340 samples were valid. Secestablishes SOTA total accuracy of 70%, outondly, when handling complex problems, \"taking performing 671B DeepSeek-V3 (69%), while reshortcuts\" may save time; however, stopping prequiring only 1.8 seconds response time compared maturely at difficult steps and guessing an answer to DeepSeek-V3s 100 seconds. While o1-mini can negatively impact prediction accuracy. Thirdly, achieves higher accuracy (75%), it is acceptable while these LLMs excel on small-sized polynomias our model is only trained with 1K dataset and enals (achieving accuracy close to 90%), they struggle joys much faster response time, i.e., 1.8s vs 34.9s."
        },
        {
            "title": "6 Model’s Understanding of SoS and",
            "content": "However, when the same research question was"
        },
        {
            "title": "Nonnegativity",
            "content": "One might ask: Did the model merely learn to classify, or has it truly developed the ability to think and construct new proofs and examples? When faced with research questions in SoS or polynomial optimization, can the model generate mathematically meaningful insights? To explore this, we designed series of researchdriven questions (Appendix H) to test the models ability to understand the mathematical concepts behind Sum of Squares (SoS) and nonnegativity properties. Finding nonnegative polynomials that are not Sum of Squares (NNSoS) is fundamental and ongoing research problem in real algebraic geometry and polynomial optimization (Ahmadi et al., 2023b, 2024; Ahmadi and Zhang, 2022). This problem is closely connected to Hilberts 17th problem, semidefinite programming (SDP), and positivity certificates in polynomial optimization. We test the models ability to create and analyze unseen mathematical examples. We asked the following question to Qwen-7B-1M and Qwen14B-1M: Can you provide new NNSoS that was never found in the literature? posed to the model with SoS reasoning, the model correctly identified that pa is not valid solution to our question. This improvement can be attributed to Step 4 of SoS Reasoning (Appendix D), where the trained model recognized that pa(x) is nonnegative quartic polynomial in two variables and, therefore, cannot be NNSoS. Moreover, using SoS reasoning, Qwen-14B-1M derived new valid example for NNSoS, qa(x) = x4 1x 2x2 3 + x2 1x4 2x2 3 + x4 3 + 1 3x 1x2 2x2 3. We cross-checked this polynomial using the classic solver YALMIP and confirmed that qa is indeed NNSoS. The trained models approach to constructing this example is particularly interesting. It began 1x2 1x 2+x2 with the well-known example of NNSoS such as 2+13x2 pm(x) = x4 2, Then, the model then introduced new variable and slightly modified the coefficients to generate qa. This demonstrates that the trained model not only recognizes 1x2 existing patterns in polynomial optimization but also generalizes and constructs novel cases, providing valuable mathematical insights. Interestingly, when prompted with SoS plain,"
        },
        {
            "title": "7 Conclusion and Discussion",
            "content": "Qwen-14B-1M can only give examples that are well-known in the literature and Qwen-7B-1M returned an incorrect example: pa(x) = x4 1 + x4 2 + 1 1 x2 2 x2 1x2 2. Although this example is incorrect, it is nontrivial, as classic solvers such as YALMIP also fail to extract global optimality4. The reason this example is challenging is that it has four global minima at (1, 1), This paper investigates the capacity of LLMs on research-level mathematical problem: determining whether given multivariate polynomial is SoS. This problem, closely related to Hilberts Seventeenth Problem, plays crucial role in various fields. We first introduce SoS-1K, dataset of approximately 1,000 polynomials, along with set of SoSspecific reasoning-guiding instructions. Our results show that with our expert-designed, current SOTA (1, 1), (1, 1), and (1, 1), with global minLLMs is able to achieve up to 81% accuracy. Furimum value of 0. The number of global minima thermore, an in-depth analysis of model responses exceeds the rank of the moment matrix, making it reveals several intriguing insights. This study highdifficult to extract global optimality certificates. lights the potential of AI in tackling large-scale 4YALMIP returns status 0, indicating that although the SDP is solvable, rank conditions cannot ensure global optimality. open problems in mathematics, paving the way for future advancements."
        },
        {
            "title": "8 Limitation and Potential Risks",
            "content": "One limitation of this study is that we constrain the context length of SoS question within 4K, as some LLMs tend to fail with longer sequence length. Due to this reason, most SoS polynomials are within the capacity of traditional solvers. In the future, we will extend our dataset, targeting more challenging questions with scale that is beyond traditional SoS solvers. One potential risk of this study is that LLMs have the chance to make wrong decisions, which might be misleading, and therefore we need to use traditional solvers to verify."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by Hong Kong Innovation and Technology Commission (InnoHK Project CIMDA). S. Liu is supported by the Royal Society with the Newton International Fellowship. References Amir Ali Ahmadi, Grigoriy Blekherman, and Pablo Parrilo. 2024. Convex ternary quartics are sosconvex. arXiv preprint arXiv:2404.14440. Amir Ali Ahmadi, Abraar Chaudhry, and Jeffrey Zhang. 2023a. Higher-order newton methods with arXiv preprint polynomial work per iteration. arXiv:2311.06374. Amir Ali Ahmadi, Cemil Dibek, and Georgina Hall. 2023b. Sums of separable and quadratic polynomials. Mathematics of Operations Research, 48(3):1316 1343. Amir Ali Ahmadi and Anirudha Majumdar. 2019. Dsos and sdsos optimization: more tractable alternatives to sum of squares and semidefinite optimization. SIAM Journal on Applied Algebra and Geometry, 3(2):193 230. Amir Ali Ahmadi, Alex Olshevsky, Pablo Parrilo, and John Tsitsiklis. 2013. Np-hardness of deciding convexity of quartic polynomials and related problems. Mathematical programming, 137:453476. Amir Ali Ahmadi and Pablo Parrilo. 2013. complete characterization of the gap between convexity and sos-convexity. SIAM Journal on Optimization, 23(2):811833. Amir Ali Ahmadi and Jeffrey Zhang. 2022. On the complexity of finding local minimizer of quadratic function over polytope. Mathematical Programming, 195(1-2):783792. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157. Alberto Alfarano, François Charton, and Amaury Hayat. 2024. Global lyapunov functions: long-standing open problem in mathematics, with symbolic transformers. arXiv preprint arXiv:2410.08304. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023. Large language models as tool makers. arXiv preprint arXiv:2305.17126. Octavia Camps and Mario Sznaier. 2017. The interplay between big-data and sparsity in systems identification. In Jean-Paul Laumond, Nicolas Mansard, and Jean-Bernard Lasserre, editors, Geometric and Numerical Foundations of Movements, number 117 in Springer Tracts in Advanced Robotics, pages 133 159. Springer, New York. Andrew Doherty, Pablo Parrilo, and Federico Spedalieri. 2002. Distinguishing separable and entangled states. Physical Review Letters, 88(18):187904. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Nebojša Gvozdenovic and Monique Laurent. 2007. Semidefinite bounds for the stability number of graph via sums of squares of polynomials. Mathematical Programming, 110:145173. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Didier Henrion, Jean-Bernard Lasserre, and Johan Löfberg. 2009. Gloptipoly 3: moments, optimization and semidefinite programming. Optimization Methods & Software, 24(4-5):761779. David Hilbert. 1893. Über ternäre definite formen. Journal. 9 Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. 2025. Advancing language model reasoning through reinforcement learning and inference scaling. Preprint, arXiv:2501.11651. Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403. Immanuel Kant. 1908. Critique of pure reason. 1781. Modern Classical Philosophers, Cambridge, MA: Houghton Mifflin, pages 370456. Masakazu Kojima. 2003. Sums of squares relaxations of polynomial semidefinite programs. Inst. of Technology. Jean Lasserre. 2001. Global optimization with polynomials and the problem of moments. SIAM Journal on optimization, 11(3):796817. Jean Bernard Lasserre. 2000. Optimisation globale et théorie des moments. Comptes Rendus de lAcadémie des Sciences-Series I-Mathematics, 331(11):929934. Jean-Bernard Lasserre. 2006. Convergent sdprelaxations in polynomial optimization with sparsity. SIAM Journal on Optimization, 17(3):822843. JeanB Lasserre, Kim-Chuan Toh, and Shouguang Yang. 2017. bounded degree sos hierarchy for polynomial optimization. EURO Journal on Computational Optimization, 5(1-2):87117. Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. 2025. Evolving deeper llm thinking. Preprint, arXiv:2501.09891. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Zhi-Quan Luo and Shuzhong Zhang. 2010. semidefinite relaxation scheme for multivariate quartic polynomial optimization with quadratic constraints. SIAM Journal on Optimization, 20(4):17161736. Daniel Molzahn and Ian Hiskens. 2015. Sparsityexploiting moment-based relaxations of the optimal power flow problem. IEEE Transactions on Power Systems, 30:31683180. Theodore Samuel Motzkin. 1967. The arithmeticInequalities (Proc. Symgeometric inequality. pos. Wright-Patterson Air Force Base, Ohio, 1965), 205:54. Katta Murty and Santosh Kabadi. 1987. Some npcomplete problems in quadratic and nonlinear programming. Mathematical Programming: Series and B, 39(2):117129. Mathematical Association of America. 2024. Aime. OpenAI. 2024. Learning to reason with llms. Antonis Papachristodoulou, James Anderson, Giorgio Valmorbida, Stephen Prajna, Pete Seiler, Pablo Parrilo, Matthew Peet, and Declan Jagt. 2013. Sostools version 4.00 sum of squares optimization toolbox for matlab. arXiv preprint arXiv:1310.4716. Pablo Parrilo. 2000. Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization. Ph.D. thesis, California Institute of Technology. Pablo Parrilo and Bernd Sturmfels. 2001. Minimizing polynomial functions. arXiv preprint math/0103170. Stephen Prajna, Antonis Papachristodoulou, and Pablo Parrilo. 2002. Introducing sostools: general purpose sum of squares programming solver. In Proceedings of the 41st IEEE Conference on Decision and Control, 2002., volume 1, pages 741746. IEEE. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172180. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024a. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024b. Scaling llm test-time compute optimally can be more effective than scaling model parameters. Preprint, arXiv:2408.03314. Hayato Waki, Sunyoung Kim, Masakazu Kojima, and Masakazu Muramatsu. 2006. Sums of squares and semidefinite program relaxations for polynomial optimization problems with structured sparsity. SIAM Journal on Optimization, 17(1):218242. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Tillmann Weisser, Jean Lasserre, and Kim-Chuan Toh. 2018. Sparse-bsos: bounded degree sos hierarchy for large scale polynomial optimization with sparsity. Mathematical Programming Computation, 10:132. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. 2024. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333. Naruki Yoshikawa, Marta Skreta, Kourosh Darvish, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen, Andrew Zou Li, Yuchi Zhao, Haoping Xu, Artur Kuramshin, et al. 2023. Large language models for chemistry robotics. Autonomous Robots, 47(8):10571086. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792. Yang Zheng, Giovanni Fantuzzi, and Antonis Papachristodoulou. 2019. Sparse sum-of-squares (sos) optimization: bridge between dsos/sdsos and sos optimization for sparse polynomials. In 2019 American Control Conference (ACC), pages 55135518. IEEE. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Wenqi Zhu and Coralia Cartis. 2024. Global convergence of high-order regularization methods with arXiv preprint sums-of-squares taylor models. arXiv:2404.03035."
        },
        {
            "title": "A Literature Review for SoS",
            "content": "Due to the computational intractability of the general problem, we seek special cases of polynomials where the challenging nonnegativity constraints can be replaced with more manageable conditions. The sum of squares (SoS) condition, mathematical technique in polynomial optimization where polynomial is expressed as sum of squared polynomials, provides sufficient criterion for polynomial nonnegativity. The SoS property is particularly useful because it allows the nonnegativity problem to be reformulated as semidefinite programming (SDP) problem, for which efficient algorithms, such as interior-point methods, exist. In certain special cases, nonnegativity and SoS are equivalent; for example, any nonnegative quadratic polynomial or any nonnegative even-degree univariate polynomial can always be expressed as sum of squares (Hilbert, 1893; Ahmadi and Parrilo, 2013; Ahmadi et al., 2023b). For more complex polynomials, the Lasserre hierarchy provides systematic way to approximate nonnegativity using sequence of SoS relaxations (Lasserre, 2001). This method constructs sequence of SDP problems that yield increasingly tighter approximations to nonnegativity. Many large-scale problems exhibit structured sparsity patterns, enabling the application of The problem of determining whether multivarisparsity-adapted hierarchy of SDP relaxations ate polynomial is nonnegative is inherently linked (Camps and Sznaier, 2017; Lasserre, 2006; to the task of finding its global minimuma funMolzahn and Hiskens, 2015; Waki et al., 2006). damental challenge in the optimization commuAdditional techniques for addressing large-scale nity (Ahmadi et al., 2023a; Zhu and Cartis, 2024; problems include Structured DSoS and SDSoS Lasserre, 2000; Parrilo and Sturmfels, 2001). Testprogramming, as well as Bounded Degree SoS ing whether general polynomial is nonnegative (BSoS) (Ahmadi and Majumdar, 2019; Lasserre is provably NP-hard, even for polynomials of relaet al., 2017; Waki et al., 2006; Weisser et al., 2018; tively low degrees or with small number of variZheng et al., 2019). These approaches take advanables. For instance, it has been shown that findtage of the structure of the problem (sparsity) to ing the global minimum of general even-degree generate smaller SDPs. There are also methods to polynomials of degree at least four is NP-hard (Ahreformulate the original optimization problem to remadi et al., 2023a, 2013; Ahmadi and Zhang, 2022; duce the size of the optimization. For instance, the Murty and Kabadi, 1987). optimization of multivariate fourth-order (quar11 tic) homogeneous polynomial under quadratic connegative values. straints can be relaxed into quadratic SDP (Luo Step 3: Consider these special properties: and Zhang, 2010). In contrast to the SoS approach, which gives matrix variable of size at least , the quadratic SDP system has size of only. The resulting quadratic SDP can be well approximated in polynomial time in some cases, but it remains NP-hard. Yet, these methods primarily depend on the specific structure of the problem, and generally, the scalability of characterizing polynomial nonnegativity remains significant challenge in the literature."
        },
        {
            "title": "B Mathematical Background for SoS",
            "content": "Definition 2.1 implies that the degree of an SoS polynomial has to be even and that the maximum degree of each qj is deg[q] . Therefore, we denote 2 the degree of the SoS polynomial as = 2d where is positive integer. Definition B.1 (Total degree polynomial space) Let > 0 be the dimension and = [x1, . . . , xn]T be the variables. We denote Pp[x] as the general representation of the polynomial spaces, where represents the highest degree each entry can take. The associated multi-index set as α = [α1, α2, . . . , αn] [Z[0, p]]n where each αr is an integer between 0 to inclusively and the indices satisfy (cid:80)n r=1 αr p. The number of monomial bases are := (cid:0)n+p (cid:1). Theorem B.1 (From (Parrilo, 2000)) For variable = [x1, . . . , xn]T and an even integer = 2d, let ϕd(x) be the vector of all monomials of degree at most in xj for 1 n. polynomial ˆp : Rn of degree is SoS if and only if there exists symmetric matrix such that (i) ˆp(x) = ϕd(x)T Qϕd(x) for all Rn, (ii) 0."
        },
        {
            "title": "C SoS Simple",
            "content": "Step 1: Examine if the highest degree is odd or even. Step 2: For the even highest degree d, examine the Properties of quadratic polynomials Properties of quartic polynomials in 1-2 variables. Properties of quartic homogeneous polynomials in 1-3 variables. Properties of even-degree univariate polynomials. Step 4: Try direct sum of squares representation. Step 5: Consider matrix methods if needed."
        },
        {
            "title": "D SoS Reasoning",
            "content": "Step 1. Degree: An SoS polynomial must have an even degree (i.e., its highest-degree term must have an even exponent). Any odd-degree polynomial cannot be expressed as sum of squared polynomials. This is the simplest criterion and should always be checked first. the highest-degree univariate term (i.e., If 1, . . . , xd xd n) has negative coefficient, then the polynomial is not SoS. Otherwise, we cannot determine whether it is SoS and proceed to the next step. 2 + x4 1 x4 Example 1: p(x) = 2. Since the highest-degree univariate term has negative coefficient (namely, x4 2), by letting x2 , it is clear that p(x) becomes negative. Therefore, it is 3 + x2 1x2 not SoS. 2+x4 32x Example 2: p(x) = x4 1+x4 2+x1x2. All the highest-degree univariate terms have nonnegative coefficients (i.e., x4 3). Thus, we cannot determine whether it is SoS, and we move 2, x4 1, x4 1x2 to the next step. 1 + x4 2 2x2 Example 3: p(x) = x4 2. This polynomial is SoS because it can be rewritten as: 2)2. Note that negative coefficient p(x) = (x2 in the highest-degree cross term is allowed. For in1 x2 1x coefficients of highest-degree terms. Check for any stance, in this case, we have the negative coefficient 12 1x2 cross term 2x2 univariate terms are positive (i.e., x4 2. However, the highest-degree 1, x4 2). Test Set Construction: Test Set 1 is constructed such that the highest-degree term is odd, thereby ensuring that the polynomials are not SoS. Step 2. Non-negativity: SoS polynomials are nonnegative for all real inputs. For example, if polynomial p(x) has negative constant term, then p(0) < 0, proving it is not SoS. Similarly, if horizontally translated and scaled polynomial q(x) = cp(x + d) (for any and Rn ) satisfies q(0) < 0, then p(x) cannot be SoS. common method for checking SoS. (x2 Example: Consider p(x) = (x1 x1x2)2 + 1)2, which is an SoS polynomial. How2 x4 ever, polynomials are sometimes given in their expanded form. For instance, the same polynomial can be written as: p(x) = 2x2 1 2 + x4 2x4 2. On the other hand, consider p(x) = (x1 x1x2)2 + (x2 1)2 20. This polynomial is not SoS. To determine whether an 1x2 + x2 2 x4 1 + x8 2 + x2 1x2 1x expanded polynomial can be expressed in SoS form with negative constant, one should analyze the symmetries of the terms and the structure of the To determine whether polynomial is nonnegacross terms. tive, please use the following approaches: Constant coefficient check: If the constant coefficient is negative, then p(0) < 0. For instance, p(x) = x4 + x3 1, p(x) = x2 2 0.1. are no SoS polynomials. 2 + x4 1 + x2 1x2 Grid evaluation: Try finding the minimum value of the polynomial over selected evaluation grid. It is crucial to perform this step. Substitute multiple values of x, such as (1, 0, 0, . . . ), (0, 1, 0, . . . ), (0, 0, 1, . . . ), etc., to check whether the polynomial Test Set Construction: Examples and counterexamples are provided in Test Set 2. Step 4. Special Structures and Cases: a) Any nonnegative quadratic polynomial is sum of squares (SoS). 2 2x1x2, p(x) = 1 + x2 Examples: p(x) = x2 2 + 4x2 1 + x2 x2 Counterexamples: p(x) = x2 p(x) = 2 + 4x2 1 + x2 1+x2 3 5x2x3. 3 3x2x3. These are SoS. 22x1x21, evaluates to negative value. b) Any nonnegative quartic polynomial in one or Leading order and dominant terms: Analyze two variables is SoS. the highest-degree terms and explore symmetries among cross terms. Evaluate the magnitude of negative coefficients relative to positive coefficients. Finding minima: Attempt to find the local or global minimum of the polynomial to determine if it is negative. Finding Symmetry and Translation: Example 1: Consider horizontally translated and scaled polynomial: p(x) = 1.8x2 2 + 4.8x2 + 20.82. Rewriting, p(x) = 1.8(x1 + 3)2 + 1.2(x2 + 2)2 0.18. Since p(3, 2) < 0, the polynomial is still not SoS. 1 + 10.8x1 + 1.2x Step 3. Square Form: An SoS polynomial p(x) can be written as p(x) = (cid:80) qi(x)2, where each qi(x) is polynomial. Examples and counterexamples are provided in Test Set 2, as this is the most 13 1x2 2x2 1 + x2 2 Example: p(x) = x4 2x2 + 1 = (x 1 + 2x2 1 + x2 1)2. Counterexample: p(x) = x4 2 2x2 = (x2 x2 1 + 2x2 1 + x2 1)2 1. 1x2 2x2 1 + c) Any nonnegative quartic homogeneous polynomial in one, two, or three variables is SoS. d) Any nonnegative even-degree univariate polynomial is SoS. Example: p(x) = x6 + 3x4 + 2x2, which is nonnegative and SoS. Counterexample: p(x) = x6 + 3x4 + 2x, which takes negative values and is not SoS. e) Any nonnegative polynomial with quadratic term and quartic regularization is SoS. Therefore, if polynomial meets one of the above subject to the constraint criteria and is nonnegative, it is an SoS polynomial. Nonnegativity can be verified by determining the global minimum, checking the descent direction, or performing grid search. Test Set Construction: Examples are provided in Test Set 3.1, 3.2, and 4, while counterexamples are constructed as polynomials that take negative y10 y01 y20 y11 y02 y10 y01 y20 y11 y11 y02 y30 y21 y21 y12 y12 y03 y20 y11 y02 y30 y21 y12 y21 y12 y03 y40 y31 y22 y31 y22 y13 y22 y13 y04 0. values. If γ 0, then is SoS; otherwise, it is not. Step 5. Matrix Decomposition and Check for Test Set Construction: Examples and counterexSymmetric Positive Definite Q: If the above amples are provided in Test Set 5.1-5.4. The exchecks fail, we can use the following theoretical amples are constructed using different positive reasoning: a) For an even degree 2d polynomial in [x1, . . . , xn], construct monomial basis using canonical ordering: := (x 2x 1, . . . , 3, . . . , (x 1)2, . . . , n, (x 1)2d, . . . , (x 1x n, n)2d). This vector has length (cid:0)n+2d 2d (cid:1). b) Express the polynomial as p(x) = yQy, where is symmetric matrix of size (cid:1). Note that this representation (cid:0)n+2d 2d is not unique; there are multiple valid forms (cid:1)(cid:0)n+2d 2d of Q. c) Check whether is positive definite. This can be done by finding its smallest eigenvalue. If such exists, then p(x) is sum of squares (SoS). Otherwise, p(x) is very likely not SoS. semidefinite (PSD) matrices Q, such as sparse PSD Q, low-rank PSD Q, and ill-conditioned PSD Q. The counterexamples are generated using an indefinite and the corresponding polynomials are likely to not be SoS. However, to conclusively prove that polynomial is not SoS, all possible must be examined. In our test set, we crosschecked the results with classic solvers to confirm the \"Not SoS\" classification for Test Sets 5.15.4."
        },
        {
            "title": "E Details for Test Subsets",
            "content": "We provide the details of test subsets in Table 3. Sample Models Response under SoS"
        },
        {
            "title": "Reasoning Prompt",
            "content": "F.1 o1-mini response for simple polynomial under SoS Reasoning Prompt To determine whether the given polynomial is sum of squares (SoS), we will follow the provided steps: If all the above tests fail, we can try Semidefinite Step 1: Check the Highest Degree Programming (SDP), which is the test used by existing solvers (e.g., YALMIP) to verify whether polynomial is SoS. For example, to determine if p(x) = x4 1 4x 1x2 +7x2 1x2 2 4x1x3 2 4x1x2 +x4 2 is SoS. We convert the problem to the following. We solve the SDP The highest degree of polynomial is the largest total degree among its terms. When expanding these squared terms, the highest degree terms are 1, x3 of the form x3 2, etc.), which are degree 3 terms. (e.g., x3 Since 3 is an odd degree, according to Step 1: If the highest degree of polynomial is odd, it is not SoS. Therefore, the polynomial cannot be γ = min = y404y31+7y224y134y11+y04 expressed as sum of squares. 14 Polynomial Type length< length>4000 Total Is it SoS? Difficulty Test Set 1: Odd Degree Polynomial Test Set 2a: SoS (Expanded Form) Test Set 2b: Negative (Expanded Form) Test Set 2.1a: SoS (Squared Form) Test Set 2.1b: Negative (Squared Form) Test Set 3.1a: Nonnegative Quadratic Quartic Test Set 3.1b: Negative Quadratic Quartic Test Set 3.2a: Nonnegative Quartic with 2 variables Test Set 3.2b: Negative Quartic Test Set 4a: Nonnegative Quadratic Quartic Test Set 4b: Negative Quartic Test Set 5.1a: PSD Test Set 5.1b: Non-PD Test Set 5.2a: PSD Spare (Sparsity 0.1) Test Set 5.2b: Non-PD Spare (Sparsity 0.1) Test Set 5.3a: PSD Low Rank (rank 3) Test Set 5.3b: Non-PD Low Rank (rank 3) Test Set 5.4a: PSD Ill-Conditioned (λ = 1 1012) Test Set 5.4b: Non-PD Ill-Conditioned 150 69 105 38 100 100 100 100 100 80 80 56 42 28 20 40 50 40 15 25 0 0 0 0 0 16 16 16 18 12 15 30 120 63 120 63 100 100 100 100 100 96 72 72 60 40 35 NO YES NO YES NO YES NO YES NO YES NO YES NO YES NO YES NO YES NO Easy Hard Hard Easy Easy Medium Medium Medium Medium Medium Medium Hard Hard Hard Hard Hard Hard Hard Hard Table 3: Summary of SoS-1K Test Sets. F.2 o1-mini response for difficult polynomial the SoS property yet and move to the next step. under SoS Reasoning Prompt Step 3: Check for Negative Values To determine whether the given polynomial is sum of squares (SoS), we follow the outlined steps meticulously. Step 1: Check the Highest Degree Testing Specific Points: Point (1, 1, 1, 0, 0), Evaluated p(1, 1, 1, 0, 0) = 66.89 (Positive). Point (0, 0, 0, 1, 1), Evaluated p(0, 0, 0, 1, 1) = 6.52 (Positive). Point (1, 1, 1, 1, 0), Evaluated The highest total degree among the terms is 4, p(1, 1, 1, 1, 0) = 2.037 (Positive). which is even. Since the highest degree is even, we Conclusion: At tested points, the polynomial proceed to the next step. remains nonnegative. While this doesnt guarantee Step 2: Check Highest Degree Univariate its SoS, it suggests the possibility. Terms for Negative Coefficients Step 4: Special Cases Analysis: The polynoUnivariate Terms of Degree 4: 7.9x4 2 (Coefficient: 4.1) 0.91x4 4 (Coefficient: 2.0) 0.92x4 1 (Coeffi3 (Coeffi5 (Coefcient: 7.9) 4.1x4 cient: 0.91) 2.0x4 ficient: 0.92) mial doesnt clearly fit into any of the special cases (a to f) outlined for immediate determination. Step 5: Advanced Methods - Matrix Decomposition and SDP: Given the complexity of the Conclusion: All highest degree univariate terms polynomial, performing matrix decomposition or have positive coefficients. We cannot determine semidefinite programming (SDP) manually is im15 practical. However, based on the evaluations in Note that the first three polynomials are wellStep 3 and the positive coefficients in Step 2, theres known examples of nonnegative but not suma strong indication that the polynomial does not of-squares (SoS) polynomials. The first is the take negative values. Motzkin polynomial, classic example of nonFinal Conclusion: Based on the evaluations and negative polynomial that is not SoS. The second the absence of negative terms in critical compois the Robinson polynomial, counterexample to nents, the polynomial is sum of squares. Hilberts 17th problem. The last polynomial is recent result from Ahmadis work (Ahmadi et al., 2023b, Thm 3.3). Finding nonnegative polynomials that are not sum of squares (SoS) is significant ongoing research problem in real algebraic geometry and polynomial optimization (Ahmadi et al., 2023b, 2024; Ahmadi and Zhang, 2022). It connects to Hilberts 17th problem, semidefinite programming (SDP), and positivity certificates in polynomial optimization."
        },
        {
            "title": "SOTA LLMs",
            "content": "Model # Total Samples # Valid Samples Instruction Type SoS Plain SoS Simple SoS Reasoning DeepSeek-R QWQ-32b o1-mini Qwen2.5-7b GPT-4o Qwen2.5-7b-1m Qwen2.5-14b Qwen2.5-14b-1m GPT-4o-mini DeepSeek-V1 Qwen2.5-32b Average 340 340 340 340 340 340 340 340 340 340 233 338 323 336 340 340 339 340 334 323 259 340 332 338 339 336 339 340 339 328 225 337 277 337 286 309 327 332 315 300 Table 4: # Total Samples and Valid Samples of Different Models."
        },
        {
            "title": "Questions",
            "content": "1. Can you comment on the sum of squares (SoS) and nonnegativity properties of the following polynomials? 2 + x2 p(x) = x4 1x2 p(x) = x6 1 + x6 3 x4 2x2 1 x4 2x2 x4 1 + x4 p(x) = (x4 1x4 1x2 2 + 1 3x2 2, 2 + x6 1x2 1x2 3 x4 2 x4 3 3x2 2x2 1x2 2 + 3x2 3x2 1 x4 3, 1 + x2 2 + x4 3) + 2(x2 2 + 3) + 8(x1x2 + x1x3 + x2x3) + 9 x2 4 . 2. Can you construct general formula for nonnegative polynomials that are not SoS? 3. Can you provide new nonnegative polynomial that is not SoS and has never been found in the literature?"
        }
    ],
    "affiliations": [
        "Mathematical Institute, University of Oxford",
        "Nanjing University of Aeronautics and Astronautics",
        "School of Transportation and Civil Engineering, Nantong University"
    ]
}