{
    "paper_title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
    "authors": [
        "Duygu Altinok"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets."
        },
        {
            "title": "Start",
            "content": "Introducing TrGLUE and SentiTurca: Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis"
        },
        {
            "title": "Duygu Altinok",
            "content": "Independent Researcher, Berlin, Germany. Contributing authors: duygu@turkish-nlp-suite.com; Abstract Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as fundamental criterion for assessing model capabilities. it is essential to establish benchmarks that enable thorough evaluaThus, tion and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, comprehensive benchmark encompassing variety of NLU tasks for Turkish. In addition, we present SentiTurca, specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields scalable, reproducible workflow. With TrGLUE, our goal is to establish robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets. Keywords: Turkish NLU, Turkish NLU benchmark, Turkish NLI, Turkish sentiment analysis, Turkish sentiment analysis datasets 5 2 0 2 6 2 ] . [ 1 0 0 1 2 2 . 2 1 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Given the advancements in transformer models, such as BERT [1] and large language models (LLMs) [2], it has become essential to benchmark these models from various perspectives. Evaluating natural language understanding (NLU) is particularly important, and having benchmark for evaluating and analyzing NLU abilities is crucial. The General Language Understanding Evaluation (GLUE) benchmark [3] has been widely used for English. However, benchmarks for languages other than English have also been developed, including CLUE for Chinese [4], FLUE for French [5], JGLUE for Japanese [6], and KLUE for Korean [7]. In the case of Turkish, several datasets have been created for tasks like text classification, sentiment analysis [8], and hate speech [9]. However, there is lack of datasets for paraphrase, similarity, and inference tasks, with the exception of Semantic Textual Similarity (STS) dataset translated by Beken et al. [10]. As result, there is currently no standard NLU benchmark for Turkish, and the existing datasets are scattered and not comprehensive. To address this gap, we have developed centralized and easily accessible benchmark called TrGLUE, which is hosted on Hugging Face. Despite the availability of transformer models like BERTurk [11] and even some LLMs, there is scarcity of training and evaluation datasets in Turkish. Most models are trained on OSCAR [12, 13] and/or mC4 corpora [14], which lack variability, and the situation is even worse for evaluation datasets. BERTurk has been evaluated in limited areas such as sequence classification, named entity recognition (NER), partof-speech (POS) tagging, and question answering. However, there is little information available about the construction process or corpus statistics of the question answering dataset used by BERTurk. Existing evaluations in the field lack comprehensive coverage and reproducibility, either focusing superficially on few dimensions or relying on local datasets that are not shared. Moreover, the absence of standardized benchmarking dataset for Turkish creates challenges in comparing results across different models. Another issue is using translations instead of building from natural text data, which introduces low-quality translations, cultural biases as well as not considering agglutinative nature of Turkish, also introducing evaluation metric defects. To address these issues, we propose the creation of the Turkish General Language Understanding Evaluation (TrGLUE) benchmark. Similar to the widely used GLUE benchmark for English, TrGLUE aims to establish standardized evaluation framework for Turkish NLP models. It comprises diverse tasks of text classification and sentence pair classification, carefully designed to assess various aspects of natural language understanding in Turkish. We also introduce SentiTurca, sentiment analysis focused benchmark. SentiTurca encompasses datasets from different domains and covers wide range of sentiments, including the Turkish Hate Map (TuHaMa), the most extensive Turkish hate speech dataset available. These resources enable researchers to explore sentiment analysis in diverse contexts and address the challenges of hate speech in the Turkish language. During the construction of TrGLUE, we mirrored the task design and domain coverage of the original GLUE-style datasets. Source texts were collected from native 2 Turkish resources, including Wikipedia (tr), news outlets, social platforms, and public forums, to ensure linguistic naturalness and domain comparability. For STS-B, whose task formulation is language-agnostic, we opted for translation followed by multiple human review passes to adapt cultural references and idiomatic usage for Turkish. Aside from STS-B, we avoided direct translations and relied on Turkish-native corpora. To scale annotation while maintaining quality, we adopted semi-automated, disagreement-driven pipeline. For each task, we first assembled large unlabeled Turkish instance pools and translated the corresponding English task definitions into Turkish guidelines. We trained lightweight sentence-transformer[15] classifier to generate preliminary labels and, in parallel, prompted permissively licensed, state-ofthe-art LLM, Snowflake Arctic [16] to produce label hypotheses for the same instances. We then reconciled the two signals: examples with classifier-LLM disagreement were prioritized for human annotation, while agreement cases underwent stratified spot checks. This triage concentrates expert effort on ambiguous or diﬀicult items, preserves scalability for clear-cut cases, and yields measurable quality control. We document all prompts, model versions, and decision thresholds to ensure reproducibility and to facilitate re-annotation or extension to new domains. Additionally, we provide detailed documentation and guidelines for dataset creation in non-English languages, facilitating the development of benchmarks for other languages. By sharing the dataset creation process and relevant prompts, we encourage researchers to contribute to the expansion of NLU benchmarks for wide range of languages. Our contributions can be summarized as follows: We introduce TrGLUE, an open-source NLU benchmarking dataset specifically designed for evaluating Turkish language models. It serves as comprehensive and standardized benchmark, addressing the lack of resources for evaluating Turkish NLP across various tasks. Each dataset included in TrGLUE is unique and carefully crafted to ensure quality and diversity, enabling robust evaluations and comparisons of Turkish language models. SentiTurca provides large-scale and diverse sentiment analysis dataset for the Turkish language, including the extensive Turkish Hate Map (TuHaMa), facilitating research on hate speech challenges in Turkish. To facilitate the use of TrGLUE and SentiTurca, we provide fine-tuning and evaluation scripts, simplifying the adaptation and evaluation of models on these benchmarks. By utilizing our tools, we conducted assessments on various Turkish transformer models as well as popular open-source and proprietary LLMs. As result, our research also delves into exploring the Turkish NLU proficiencies of certain wellestablished LLMs. Synthetic data quality is verified through semi-automated pipeline combining cross-model agreement checks and targeted human validation, keeping the process cost-eﬀicient and reproducible. 3 Both TrGLUE and SentiTurca are freely available through their respective repositories on Hugging Face1 and GitHub. 2 Table 1: All tasks involve classifying single sentences or sentence pairs, except for STS-B, which is task involving regression. MNLI consists of three classes, while the remaining classification tasks have two classes. For TrSTS-B, no test set is released due to the limited dataset size; all evaluations are conducted on the development split. QA stands for question answering. Corpus Train Dev Test Task Metrics Single-Sentence Tasks TrCoLA TrSST7.9K 60K 1K 8.9K 1K acceptability 8.9K sentiment Matthews corr. acc./F1 Similarity and Paraphrase Tasks TrMRPC 3.18K 2.46K TrSTS-B 309K TrQQP 1.0K 0.6K 30K 1.0K paraphrase - sentence similarity 30K paraphrase acc./F1 Pearson/Spearman corr. acc./F1 Inference Tasks TrMNLI TrQNLI TrRTE 165K 18.4K 18.4K NLI 120K 3.78K 10K QA/NLI 1.0K NLI 10K 1.0K matched/mismatched acc. acc. acc."
        },
        {
            "title": "2 Related Work",
            "content": "The GLUE benchmark, consisting of nine datasets, initially paved the way for evaluating NLU models. It includes tasks such as sentence classification and sentence pair classification, with focus on natural language inference (NLI). GLUEs success has inspired the creation of similar benchmarks in various languages, such as CLUE [4], FLUE [5], KLUE [7], IndicGLUE [17], ALUE [18], JGLUE [6], Napolab [19], UINAUIL [20], SuperGLEBer [21] and CLUB [22], targeting Chinese, French, Korean, Indian languages, Arabic, Japanese, Portuguese, Italian, German and Catalan, respectively. The development of multilingual benchmarks has seen significant progress with the emergence of resources such as XGLUE [23], XTREME [24], and XTREMER [25], which incorporate datasets spanning multiple languages. However, Turkish remains underrepresented in these benchmarks, offering only limited and relatively small datasets. This underrepresentation has highlighted the growing need for comprehensive and large-scale natural language understanding (NLU) benchmarks tailored specifically to the Turkish language. 1TrGLUE: https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE SentiTurca: https://huggingface.co/datasets/turkish-nlp-suite/SentiTurca 2TrGLUE: https://github.com/turkish-nlp-suite/TrGLUE SentiTurca: https://github.com/turkish-nlp-suite/SentiTurca"
        },
        {
            "title": "2.1 Constructing NLI datasets for Turkish",
            "content": "One of the key efforts in creating cross-lingual natural language inference (NLI) datasets is the XNLI benchmark [26], which includes data for 15 languages, including Turkish. This benchmark was constructed by translating 7,500 instances from the MultiNLI dataset [27] into each target language using crowd-sourced human translators. For evaluation, the authors employed back-translation approaches and multilingual sentence encoders. Specifically, the multilingual sentence encoders were trained to align sentence embeddings and word embeddings across languages. For Turkish, the back-translation approach performed the best. While the XNLI benchmark provides accurate translations due to human involvement, the costs associated with human translation were significant consideration. notable effort focused specifically on Turkish NLI datasets is the work by [28]. In this study, the authors translated both the SNLI and MultiNLI datasets into Turkish using the Amazon Translate engine. To ensure dataset quality, the authors employed human annotators to evaluate the translations and verify that semantic relations were preserved. total of 500 example sets were distributed to four annotators, who assessed both the translation quality and label correctness. Their findings indicated that the datasets were of high quality, demonstrating that machine translation is feasible and eﬀicient way to construct Turkish NLI datasets. These datasets also offer commercial license, making them accessible for various applications. Our work builds on these efforts with three key advancements. First, we minimize translation: with the exception of TrSTS-B, all datasets are sourced from native Turkish text. Second, we leverage recent advances in LLMs to scale annotation while maintaining quality. Third, we retain full provenance and licensing information for all sources to ensure transparency and reuse. Concretely, we sample inputs (premises, sentences, questions, reviews) from licensed Turkish sources across multiple registers and genres. We adapt the English task definitions into Turkish annotation guidelines and train lightweight sentence transformer to assign coarse labels. In parallel, our instruction-tuned, permissively licensed LLM of choice produces label hypotheses via prompting. Instances with classifier-LLM disagreement are routed to human annotators; agreement cases undergo stratified spot checks. This disagreement-driven triage concentrates human effort on ambiguous items and keeps the process cost-effective. Human annotators act as gatekeepers: they reject non-native or awkward phrasing, correct labels where necessary, and allow only minor orthographic fixes. To protect evaluation integrity, we balance lexical overlap across labels, cap trivial negation or heuristic patterns, diversify distractors, and rigorously deduplicate to prevent near-duplicates and train-dev-test leakage. In practice, we pre-clean inputs and, when relevant, stratify by register/genre. Prompting budgets, temperatures, and overlap/length constraints bound variance, and double-annotated subset with adjudication is used to calibrate quality. We enforce class balance per split and, when relevant, per genre; we also track rejection and relabeling rates. Task-specific exceptions apply: TrCoLA is entirely hand-crafted by linguists without LLM generation; TrRTE hypotheses are written by humans; TrMNLI hypotheses are generated by the LLM conditioned on labels; TrSST-2 labels are derived directly from review ratings with human spot checks; and TrWNLI is omitted (see Section 3.2.7 for Turkish-specific rationale). Table 2: Comparison of sizes between the original GLUE datasets and our translated datasets. The last column indicates the percentage of data retained after translation, reflecting some data loss during the process. Corpus Original size CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE 10.7K 70K 5.8K 8.63K 795K 432K 116K 5.77K TR size 9.92K 78K 5.18K 3.06K 369K 202K 140K 5.78K Size percent. 0.92 1.11 0.89 0.35 0.46 0.46 1.20 1. Finally, we provide size comparison to situate TrGLUE relative to GLUE  (Table 2)  . Unlike translation-based pipelines that often lose substantial volume (especially for sentence similarity), our native-first, semi-automated approach produces high-quality datasets quickly and at lower cost while preserving linguistic naturalness. Beyond NLI, we cover the full range of GLUE-style taskssingle-sentence and sentence-pair classification, and regression for TrSTS-Byielding complete, reproducible benchmark for Turkish NLU."
        },
        {
            "title": "3 TrGLUE Benchmark",
            "content": "TrGLUE covers single-sentence and sentence-pair classification tasks, summarized in Table 1. We provide Turkish counterparts for all GLUE tasks except WNLI (omitted; see Section 3.2.7). Where possible, we mirror the construction philosophy of GLUE while avoiding translation: inputs come from native, human-written Turkish sources; labels are produced via an LLM-assisted, human-verified workflow. Professional verification and spot-checks were conducted by Co-one (Istanbul)3 with team of ten native Turkish annotators (balanced by gender) holding advanced degrees in language-related fields. Task-specific procedures are provided in the subsequent subsections, with the following task-level exceptions: TrRTE: hypotheses are fully human-written under linguist-designed guidelines, with linguists serving as lead annotators and adjudicators. TrCoLA: hand-crafted by linguists; no LLM generation. TrSST-2: labels derived directly from review ratings with human spot-checks. TrSTS-B: constructed via translate-then-editmachine translations were corrected by humans to ensure natural Turkish usage and cultural fit. 3Co-one, https://co-one.co/ 6 To maximize tool compatibility, our Hugging Face release preserves GLUE-style split names and metadata, enabling drop-in use with the oﬀicial GLUE metric4 and common evaluation pipelines."
        },
        {
            "title": "3.1.1 TrCoLA",
            "content": "The TrCoLA dataset is Turkish replica of the original CoLA dataset [29] and was compiled in similar manner. The sentences in TrCoLA were sourced from Turkish textbooks and linguistic books, just as in the original dataset. Like CoLA, TrCoLA consists of sentences that exhibit morphological, syntactic, and semantic violations. Each instance in the dataset is pair (sentence, label), where the label indicates whether the sentence is acceptable or unacceptable. Table 3: The linguistic textbooks utilized for TrCoLA sentences are sourced from Anadolu Universitesis Open and Distance Education System publications https://www.anadolu.edu.tr/en, which are freely accessible on the internet. Source Türk Dili-I Türkçe Sözlü Anlatım 20 Verbal expression Türkçe Cümle Bilgisi Türkçe Ses Bilgisi Syntax Phon(etics/ology) % Topic 25 Overview 30 15 We collected total of 3,630 sentences from publicly available Turkish textbooks and linguistic books, as shown in Table 3. Next, we tasked Snowflake Arctic with generating three variations for each sentence, each containing violation. This process resulted in 10,890 variations. However, not all of these variations were usable due to known issues of hallucinations in generative models [30]. Therefore, we sought human review for these variations. The review process was conducted by our annotation-data company Co-one. For each variation, we provided the original sentence, the variation itself, the type of violation, and the human annotators decided whether to keep or discard the instances. After the review, the final dataset size was reduced to 6,686 instances, highlighting the importance of human oversight despite the advancements in generative models. The final dataset size is 9,916 instances, which were then divided into train, development, and test splits. Each instance in the dataset was annotated by 4 annotators, and only instances with at least 3 agreeing votes were accepted. Krippendorffs Alpha [31] was computed for the dataset, yielding high agreement score of 0.91, indicating strong interannotator reliability. Despite their expertise, annotators reported that the dataset was challenging, and significant portion of instances required multiple readings to make accurate judgments. The complete annotation guidelines are made publicly available in the TrGLUE GitHub repository. 4https://huggingface.co/spaces/evaluate-metric/glue 7 To the best of our knowledge, this dataset is the first of its kind for the Turkish language. We also provide standalone version of this dataset in its Hugging Face repository5, with slightly different format than the original CoLA dataset. In the standalone version, we provide the original sentence, the variation, the variation type, and the label, allowing for more in-depth research and analysis."
        },
        {
            "title": "3.1.2 TrSST-2",
            "content": "The TrSST-2 dataset, utilized for sentiment analysis of film critiques, was constructed by aggregating data from online sources, specifically Sinefil.com and Beyazperde.com. Sinefil.com contributed around 40K reviews, while Beyazperde.com provided approximately 38K reviews, resulting in combined total of about 78K reviews. Initially, the dataset comprised pairs of (sentence, star) entries, where star denoted rating from 0 to 10 with precision of 1. To align with the binary classification structure of the original SST-2[32], reviews rated with 6 stars were excluded due to their mixed sentiments, with some leaning negative and others positive; subsequently, reviews with stars below 5 were categorized as negative, and those with stars above 5 were classified as positive. The final dataset comprised 78K entries, with 67K for training and 8.9K each for validation and testing. For those seeking the complete challenge, the dataset is available with all 10 classes in its dedicated Hugging Face repository.6 The distribution of stars within the dataset, illustrated in Figure 1, reveals that significant portion of reviews fall within the >7 star range, indicating that the majority of viewers enjoyed the movies they reviewed. However, this also presents challenge due to the skewed distribution of classes. To address this class imbalance, we utilize the both binary accuracy and F1-score, whereas the original GLUE task employs binary accuracy. Analysis of review lengths, as shown in Figure 1, indicates that most reviews contain 0-50 words, with an average length of 48 words. The reviews are generally detailed, offering substantial context, and may include emoticon characters. (a) Distribution of star ratings in the dataset. (b) Binary classification labels of the dataset. (c) Distribution of the number of words in review texts. Fig. 1: Statistical information about the movie reviews dataset. 5https://huggingface.co/datasets/turkish-nlp-suite/TrCoLA 6https://huggingface.co/datasets/turkish-nlp-suite/BuyukSinema 8 It is worth mentioning that there is another Turkish movies dataset by [8], which contains 10K instances consisting of 5K positive and 5K negative reviews. However, this dataset only includes single-sentence reviews."
        },
        {
            "title": "3.2.1 TrMRPC",
            "content": "The Microsoft Research Paraphrase Corpus (MRPC) [33] is dataset consisting of sentence pairs extracted from online news sources. These pairs are annotated by humans to indicate whether the sentences are semantically equivalent. The task associated with this dataset is to classify the sentence pairs as either equivalent or not equivalent. We construct Turkish paraphrase corpus in the style of MRPC using the Havadis news corpus (745K articles; 2.88 GB; 315M tokens).7 Articles are stored by newspaper, which facilitates eﬀicient filtering by outlet and author metadata when available [34]. We follow three-phase harvesting pipeline to obtain sentence pairs that include both likely paraphrases and plausible near-miss non-paraphrases. Our phases mirror the original MRPC methodology, with Turkish-specific adaptations to handle rich morphology. We describe these phases in detail as follows: Phase 1: Documentand string-based retrieval Each article is sentencesegmented. We generate initial candidate links using (i) normalized Levenshtein distance over sentence strings; and (ii) sentence-position proximity within the same document. To limit topical drift in cross-document candidates, we further restrict pairs to articles from the same outlet and (when available) the same author. Phase 2: Lexical/length filtering with lemmas We adapt MRPCs word-level heuristics by operating on lemmas rather than surface forms to account for Turkish agglutination. pair (s1, s2) is retained iff all of the following hold: 1. 5 lenwords(s1), lenwords(s2) 40; 2. 3 shared lemmas between s1 and s2; 3. min(lenwords(s1), lenwords(s2)) 0.666 max(lenwords(s1), lenwords(s2)); 4. Bag-of-lemmas edit distance 8. For lemmatization we used spaCy Turkish models [35], which include statistical morphosyntactic component (the Morphologizer). After Phase 2, we obtain 40,000 candidate pairs. Phase 3: Model-assisted triage We translate the original MRPC to Turkish and fine-tune sentence transformer8 classifier on the translated set (used only as weak semantic filter; we do not rely on translated labels for evaluation). We score all Phase 2 candidates with this classifier and apply temperature scaling for probability calibration. Pairs with high confidence (e.g., 0.9 paraphrase, 0.1 nonparaphrase) are provisionally auto-labeled subject to random audits; the remainder proceed to manual annotation. 7https://huggingface.co/datasets/turkish-nlp-suite/Havadis 8sentence-transformers/all-MiniLM-L6-v2 9 LLM disagreement sampling To better surface borderline cases, we additionally obtain labels from our LLM Snowflake Arctic on the provisionally selected set. We do not expose model or LLM labels to annotators to avoid anchoring. Instead, we route to humans only those instances where the LLM and the classifier disagree or both exhibit low confidence. In our build, 1.5k instances met this criterion and were prioritized for human review. Human annotation and adjudication Annotators receive only the raw sentence pairs and task guidelines; no pre-labels are shown. We use double annotation with adjudication on substantial subset to monitor inter-annotator agreement (IAA). Final labels are derived from majority vote with expert adjudication on disagreements. Outcome and class balance The final Tr-MRPC comprises 5.18k sentence pairs, with class distribution similar to MRPC: 1.6k paraphrase (entailment) and 3.5k non-paraphrase (non-entailment). Labeling policy difference from English MRPC One issue in constructing Turkish MRPC-style corpus is that our labeling rules necessarily diverge from English MRPC. In Turkish news, exact sentence-level equivalence across outlets is uncommonheadlines are highly compressed, body sentences vary stylistically, and agglutinative morphology amplifies surface variation. To obtain usable number of positive pairs without rewarding mere topical overlap, we adopt relaxed but principled criterion: two sentences are paraphrases if they assert the same main claim, even when one adds non-contradictory detail. We keep the core MRPC exclusions: changes in truth conditions, polarity, modality strength, or key factual quantities remain non-paraphrase. This policy is applied consistently throughout TrMRPC. We treat the following as paraphrase (label 1) when the main claim is shared. Headline detailed sentence: short headline can pair with longer sentence if both communicate the same event/claim; extra who/where/how detail is acceptable. Ex: Pariste ilk: Sinek alarmı harekete geçirdi. Pariste sağlık yetkilileri, kaplan sivrisinekleri nedeniyle ilk kez bölgeleri dezenfekte etti. 1 Attribution frames: Constructions like farkı, etkisi are acceptable if the underlying result matches. Ex: Fenerbahçede İsmail Kartal farkı! Sağlam savunma Fenerbahçede artık savunma daha sağlam. 1 Wording/structure changes: Order, phrasing, synonyms, activepassive, or minor detail that does not change truth conditions. Ex: Kaza sabah saatlerinde oldu. Kaza sabah 8de meydana geldi. 1 We treat the following as non-paraphrase (label 0). Topic overlap only: Same subject, different claims. Ex: İnsanlar kumruları beslemek için simit atıyor olabilir. Kumru güvercine benzer. 0 Mismatched certainty: Hedge vs. confirmation changes the claim. Ex: Şirketin iflas ettiği iddia edildi. Şirketin iflas ettiği doğrulandı. 0 Contradictory numbers/dates/names/places: Factual quantity changes the event. Ex: Bakanlık 500 öğretmen atadı. Bakanlık 600 öğretmen atadı. 0 Negation or polarity change. Ex: Sözleşme imzalandı. Sözleşme imzalanmadı. 0 Role swap or incompatible coreference. Ex: Ali, Veliyi yendi. Veli, Aliyi yendi. 0 Headline mismatch: Same context, different asserted claims. Ex: Milli Piyango sonuçları kazananları gösterecek. Sıralı tam liste cnnturk.comda olacak. 0 Annotators apply substitution test: if one sentence can replace the other in news story without changing the intended main claim, assign label 1; if the substitution changes the claim, assign label 0. When uncertain, assign label 0. We retain Turkish orthography and do not penalize benign morphological or word-order variation provided the main claim is preserved."
        },
        {
            "title": "3.2.2 TrSTS-B",
            "content": "The Semantic Textual Similarity Benchmark (STS-B) [36] contains sentence pairs drawn from news headlines, and from image/video captioning and NLI sources, each labeled by humans with similarity score from 1 to 5. Turkish translation exists [10], but it was created in 2021 via Google Translate without human quality control. In contrast, our TrSTS-B follows translate-then-edit pipeline with human verification: we translate English pairs and then apply targeted edits to ensure natural, idiomatic Turkish and cultural appropriateness, followed by annotator spot-checks and format validation. We choose translation for STS-B because large fraction of the dataespecially captions and simple event descriptionsis effectively language-agnostic (e.g., man is riding horse), so faithful translation preserves task semantics. Nonetheless, some phrases are culturally specific or uncommon in Turkish, such as mechanical bull, for which literal form (mekanik boğa) would be opaque or unnatural. In these cases we substitute culturally appropriate counterparts that preserve the underlying scene/event while keeping diﬀiculty comparable; for example, mechanical bull becomes çarpışan araba (bumper cars). Similarly, where flora, fauna, or place names are unfamiliar in Türkiye (e.g., lemur, Delaware), we replace them with natural Turkish equivalents that maintain semantic roles (e.g., lemur sincap, Delaware İzmir). We documented these interventions and counted approximately 40 such substitutions. Proper-noun conventions are also normalized; for instance, Alexander (the Great) is rendered as the conventional Turkish İskender. Turkishs lack of articles and different casing/inflectional conventions cause some distinct English sentences to collapse into the same Turkish string. For example, The woman is playing the flute. and woman is playing flute. both translate to Bir kadın flüt çalıyor. We remove pairs that collapse to identical sentences because they no longer provide meaningful similarity gradient. Different English pairs can also map to the same Turkish pair, creating duplicates; we detect and remove these via normalized exact matching and MinHash-based near-duplicate filtering. Translation was produced with our LLM Snowflake Arctic, under constrained output format. Human annotators then performed targeted edits and spot-checks, correcting literalisms, proper-noun handling, and culturally odd renderings. We filter 11 outputs that violate the expected format or exhibit hallucinations/unfaithful content; the translation prompt and format constraints are provided in Appendix A.1.4. Finally, we run consistency pass to ensure that numeric scales, named entities, and temporal expressions remain aligned across pairs. Original STS-B has 8.63K English pairs. After cultural substitutions, collapsedpair removal and deduplication (approximately 3.7K instances), and filtering of malformed/hallucinated outputs and low-quality translations, TrSTS-B contains 3.06K pairs. We withhold test set due to small size; evaluation is reported on the development split"
        },
        {
            "title": "3.2.3 TrQQP",
            "content": "The Quora Question Pairs2 (QQP) dataset9 is collection of question pairs obtained from the community-driven question-answering platform, Quora. The task is to determine whether pair of questions are semantically equivalent, and the labels include duplicate and not duplicate. We construct Turkish QQP-style resource (TrQQP) by harvesting question pairs from multiple Turkish Q&A and opinion platforms that span both formal and informal registers. Sources and sizes are summarized in Table 4. Table 4: Sizes and source of QQP sentence pairs. Num of pairs Website 58K SETA 15.5K Yetkin Report 58.1K Türkiye Raporu 73.6K Ekşi Duyuru 73.7K Sorucevap 89.6K KızlarSoruyor Topics politics, economy, society politics, economy, society politics, economy, society life life life Data are gathered from several websites with heterogeneous metadata. Some platforms expose explicit similar question links/tags, enabling guided harvesting of likely duplicates; others lack such signals and require unsupervised retrieval. We unify these sources in multi-stage pipeline that mixes metadata-driven pairing, text similarity, and model-assisted triage. As with TrMRPC, automatic labels are not shown to annotators. We consulted our LLM, Snowflake Arctic for disagreement sampling and prioritization, not as ground truth. Our phases of compiling QQP are as follows: Phase 1: Crawling and normalization We crawl publicly accessible question pages and threads across the listed websites, preserving (i) question text, (ii) timestamps, (iii) topic/tags where available, (iv) intra-site similar question edges when present. We apply light normalization only (whitespace/punctuation normalization and Unicode NFC); we do not correct casing, slang, typos, or emojis. Phase 2: Guided pairing via site metadata For sites that expose similar/related question links, we form positive candidate pairs by materializing each such edge as pair and adding 1-hop transitive neighbors within thread/topic scopes 9https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 12 (while avoiding trivial duplicates by ID). We deduplicate within-site pairs by hashing normalized text and keep provenance for audit. Phase 3: Unguided retrieval for sites without links For platforms without similar question tags, we retrieve candidate pairs using two-pass approach: (i) lexical BM25 over question index to obtain top-k neighbors per query (k [10, 50] depending on site size), then (ii) semantic reranking with sentence embedding model to select the top-m nearest neighbors. We additionally enforce simple length and overlap constraints to avoid trivial matches: 90% of the tokens per question, at least two shared content lemmas after lemmatization, and minimum characterlevel similarity, namely Jaro-Winkler similarity, threshold tuned per site. Phase 4: Negative sampling To construct non-duplicate pairs with realistic topical overlap, we sample hard negatives by: (i) picking high lexical/semantic similarity neighbors below duplicate threshold, (ii) mixing cross-topic pairs within the same site section to prevent overly easy negatives, and (iii) ensuring that negatives do not coincide with any known metadata-linked duplicate. Phase 5: Model-assisted triage (no annotator anchoring) We train weak duplicate detector by translating English QQP to Turkish and fine-tuning Turkish sentence-pair classifier on the translated set. This model is used solely for triage. We calibrate its probabilities with temperature scaling and score all candidate pairs. High-confidence ends (p 0.9 duplicate; 0.1 non-duplicate) are provisionally auto-labeled and sampled for random audit; the remaining uncertain cases are routed to human annotation. LLM-based disagreement sampling We obtain independent judgments from Snowflake Arctic on provisionally labeled or uncertain items. Pairs where Arctic and the classifier disagree, or where both are low-confidence, are prioritized for human review. Annotators never see model or LLM labels or rationales. Human annotation Annotators receive only raw question pairs and task guidelines (duplicate vs. not duplicate). We use double annotation with expert adjudication on disagreements and track inter-annotator agreement. We sent 35K uncertain/borderline pairs to annotators and audited an additional 5K high-confidence ends, totaling 40K human-judged items. Descriptive statistics and construction notes are as follows: Label distribution The resulting label distribution is skewed toward nonduplicates, similar to English QQP: 219,329 not-duplicate (0) and 149,465 duplicate (1), i.e., 63% vs. 37%. Emoji prevalence and orthography Emojis are frequent in the informal platforms; we count 15.9K emoji occurrences with 1,998 distinct symbols. We retain emojis, slang, abbreviations, and capitalization noise to preserve real-world diﬀiculty (contrast: Quora prohibits emojis). Redundancy and uniqueness Across all English QQP pairs, there are 980,004 unique questions; 240,623 (24.55%) repeat at least once, with maximum frequency of 391. Counts are heavy-tailed: 739,381 appear once, 135,202 twice, 47,086 thrice, and only handful exceed 100 repetitions. Across all Turkish TrQQP pairs, there are 360,965 unique questions; 49,831 (13.80%) repeat at least once, with maximum 13 frequency of 470. Compared to English, Turkish is smaller in scale (361k vs. 980k unique) and less repetitive overall (13.80% vs. 24.55%), but exhibits higher single-question maximum (470 vs. 391), indicating few highly popular questions. De-duplication and split hygiene We canonicalize questions by Unicode normalization and lightweight token rules to compute equivalence classes of near-duplicates for split hygiene. Train/dev/test splits are created by clustering questions and ensuring clusters do not straddle splits, preventing leakage via repeated questions."
        },
        {
            "title": "3.2.4 TrMNLI",
            "content": "The Multi-Genre Natural Language Inference Corpus (MNLI) [37] is crowd-sourced dataset consisting of sentence pairs with annotations for textual entailment. The task involves predicting whether given premise sentence entails, contradicts, or is neutral towards given hypothesis sentence. The premise sentences are sourced from ten different genres, including transcribed speech, fiction, and government reports. MNLI was designed as an ambitious benchmark to capture the full complexity of modern English, extending beyond single-domain or short-caption datasets. It draws from ten distinct genres of written and spoken American English, selected to approximate the linguistic and stylistic diversity of contemporary usage. Five of these genresfiction, government, slate, telephone, and travelare included in the training set, while the remaining five9/11 reports, letters, Oxford University Press non-fiction, verbatim speech, and face-to-face conversationappear only in evaluation. This split enables systematic assessment of both in-domain (matched) and cross-domain (mismatched) generalization in natural language inference models. Dataset compilation TrMNLI provides large-scale, genre-balanced Turkish NLI benchmark. Premises are sampled from the BellaTurca collection [34], drawing on subcorpora that span distinct communicative styles (e.g., AkademikDerlem/AcademicCrawl10, ÖzenliDerlem/CraftedCrwal11, ForumSohbetleri12). To mirror MNLI, we curate eight genres and organize them into matched and mismatched splits. The matched portion (used for training and in-domain evaluation) includes: Folk tales (MasalMasal / ÖzenliDerlem): narrative and descriptive storytelling; Product complaints (Serzenişler / ÖzenliDerlem): semi-formal, problem-oriented discourse; Travel blogs (GeziNotları / ÖzenliDerlem): informal, experiential writing; Forum content (ForumSohbetleri): conversational online communication. The mismatched portion (held out for out-of-domain evaluation) contains four genres with distinct tone and structure (all from Özenli Derlem): Movie reviews (PerdeArkasıYorumlar): subjective and evaluative prose; Fashion magazines (SüslüTrendler): semi-formal, trend-focused articles; 10https://huggingface.co/datasets/turkish-nlp-suite/AkademikDerlem 11https://huggingface.co/datasets/turkish-nlp-suite/OzenliDerlem 12https://huggingface.co/datasets/turkish-nlp-suite/ForumSohbetleri/ForumChats Cultural magazines (KültürHaritası): formal expository pieces on heritage and events; Viral media content (ViralMedya): casual, humorous, and stylistically varied online text. While MNLI uses ten genres (five matched, five mismatched), suitable Turkish sources with clear licensing and genre delineation were more limited. Accordingly, TrMNLI comprises eight genres: four matched (used for training and in-domain evaluation) and four mismatched (evaluation only). This design preserves the core matched/mismatched generalization test while maintaining genre diversity across narrative, evaluative, expository, and conversational registers. Construction methodology We adopt an LLM-assisted, human-verified procedure. For each sampled premise sentence, our LLM Snowflake Arctic was prompted to generate three hypotheses one entailment, one neutral, and one contradictionunder three prompting styles (factuality-focused, linguistic-variation, and free style; prompts in Appendix A.1.6). Human annotators then conducted manual validation pass with the following rules: (i) do not edit model text for content; (ii) discard the pair if the hypothesis is ungrammatical or non-native Turkish; (iii) correct the label if it is incorrect; and (iv) optionally fix minor typos/orthography (no semantic edits). Annotators did not see model scores or rationales. We double-annotated subset with expert adjudication to calibrate quality. To limit artifacts, we balanced lexical overlap across labels, capped overt-negation contradictions, and deduplicated near-duplicates via MinHash and embedding screening. We retain genre tags and provenance (corpus/subcorpus IDs) for audit, and release only premise-hypothesis-label triples with split and genre metadata. Table 5: Sizes and sources of TrMNLI sentence pairs across genres. Genre Train Dev Test Matched Genres Folk tales Product complaints Travel blogs Forum content 23.5K 47.1K 47.1K 47.1K 1.3K 2.62K 2.62K 2.62K 1.3K 2.62K 2.62K 2.62K Mismatched Genres Movie reviews Fashion magazines Cultural magazines Viral media content TrMNLI overall 2.3K 2.3K 2.3K 2.3K 2.3K 2.3K 2.3K 2.3K 165K 18.4K 18.4K Diversity of linguistic phenomenon Although the original MNLI study reports quantitative counts of predefined syntactic or semantic phenomena, we provided diversity in such phenomena during corpus creation. Namely, we prompted Arctic with specific instructions focusing in three fields, 15 factuality, linguistic variations and free style. Appendix A.1.6 presents the prompts used for this purpose. We also provide few illustrative examples below. The examples below illustrate typical sentence-pair relations in Turkish along two primary dimensions: (i) factuality and (ii) linguistic variation. Many instances require evaluating whether the hypothesis is grounded in objective information expressed in the premise. Entailment: Premise: Türkiyenin başkenti Ankaradır. Hypothesis: Ankara Türkiyenin başkentidir. (The hypothesis repeats an objectively true fact stated in the premise.) Contradiction: Premise: Türkiyenin başkenti Ankaradır. Hypothesis: Türkiyenin başkenti İstanbuldur. (The hypothesis contradicts an objective fact from the premise.) Neutral: Premise: Türkiyenin başkenti Ankaradır. Hypothesis: Ankara İç Anadolu Bölgesinde yer alır. (The hypothesis introduces new factual information that is not entailed by the premise.) variety of transformations highlighting sentence-level lexical and structural diversity in Turkish included in the dataset. Paraphrasing: Premise: Toplantı iptal edildi. Hypothesis: Toplantıyı yapmamaya karar verdiler. (Same meaning expressed with different lexical choices.) Summarization: Premise: Konferans, birçok uzmanın katıldığı uzun bir tartışmayla sonuçlandı. Hypothesis: Konferans tartışmalarla bitti. (Condensed version preserving the main meaning.) Lexical Transformation: Premise: Çocuk hızlı koştu. Hypothesis: Çocuk seri adımlarla ilerledi. (Synonym and lexical substitution.) Syntactic Restructuring: Premise: Doktor hastayı muayene etti. Hypothesis: Hasta doktor tarafından muayene edildi. (Active-passive alternation preserving entailment.) Focus Shifting: Premise: Öğrenci öğretmene bir mektup verdi. Hypothesis: Öğretmen öğrenciden bir mektup aldı. (Same event with shifted focus.) Negation: Premise: Kadın tiyatroya gitti. Hypothesis: Kadın tiyatroya gitmedi. (Contradiction created by negation.) Addition of Unrelated Details: Premise: Adam parka yürüdü. Hypothesis: Adam parka yürüyüp arkadaşlarıyla kahve içti. (Neutral example by adding unrelated new information.) Overall, these patterns demonstrate that TrMNLI contains wide range of natural linguistic phenomena beyond simple lexical overlap, including factual reasoning, rephrasing, and structural variation, reflecting the expressive morphology and flexible word order characteristic of Turkish."
        },
        {
            "title": "3.2.5 TrQNLI",
            "content": "The Stanford Question Answering Dataset (QNLI) [38] is question-answering dataset comprising question-paragraph pairs. Each paragraph is extracted from Wikipedia and contains the answer to the corresponding question, which is generated by an 16 annotator. The benchmark authors convert the task into sentence pair classification by creating pairs between each question and each sentence in the corresponding context, filtering out pairs with low lexical overlap. The objective is to determine whether the context sentence contains the answer to the question. For TrQNLI, we construct native Turkish counterpart using Turkish Wikipedia. We sample approximately 24K paragraphs and prompt our LLM Snowflake Arctic to generate question-answer pairs, requesting exactly 10 question-answer-answer-index triplets per paragraph. While we do not require span extraction at evaluation time, we still guide the LLM to respect Turkish morphology when proposing answers. In particular, the prompt requests both lemma and surface forms where relevant and enforces case morphology when it is semantically required by the question (e.g., genitive question such as Kimin ? expects genitive-marked answer). The full prompt appears in Appendix A.1.7. We obtain about 240K raw triplets and apply automatic filtering to remove malformed outputs, format violations, and likely hallucinations, resulting in roughly 150K remaining instances. Human annotators then review stratified subset and discard low-quality items (approximately 10K), yielding around 140K validated question-answer-context triplets. To convert to the QNLI format, we split each context into sentences, identify the answer-bearing sentence for each triplet, and form question-sentence pairs labeled as entails (the sentence contains the answer) or not. For each positive pair we sample matched negative from the same paragraph to control topical overlap, producing 1:1 class balance. Train/dev/test splits are stratified to maintain label balance and to avoid paragraph leakage across splits."
        },
        {
            "title": "3.2.6 TrRTE",
            "content": "The Recognizing Textual Entailment (RTE) datasets are derived from series of annual entailment challenges. The benchmark combines RTE1 [39], RTE2 [40], RTE3 [41], and RTE5 [42], with examples built from news and Wikipedia. For consistency, all datasets are converted to two-class setting by collapsing neutral and contradiction into single not entailment class. RTE was compiled within the PASCAL challenges to provide an application-driven benchmark for inference. Curators sourced short texts (premises, ) from realistic pipelinesIR, IE, QA, MTprimarily from newswire and web documents. For each text, trained annotators authored or edited single-sentence hypothesis (H) that was either licensed by the text (entailed) or deliberately plausible yet unsupported/contradicted (not entailment), under guidelines targeting diverse phenomena (negation, quantification, temporal relations, coreference, lexical inference). Candidate pairs were double-annotated and adjudicated to produce gold binary labels. The datasets are small but carefully curated (e.g., RTE-1: 567 dev, 800 test), with short, self-contained texts and hypotheses, balanced across application categories and labels, and standardized as (P, H, label) pairs. 17 We construct Turkish RTE corpus in similar spirit by sampling short, self-contained premises from licensed newspapers and academic texts, and instructing native-speaker annotators to generate single-sentence hypotheses under three complementary styles. (i) Linguistic: annotators target specific phenomenon (e.g., negation, quantifiers, comparatives, modality, coreference, word order/topicalization, agreement/case, derivational/inflectional morphology) and author minimal, fluent hypotheses that isolate that phenomenon while preserving truth conditions. (ii) Free: open-ended, natural paraphrasing that stays on topic while varying lexical choice and syntax to reduce superficial overlap artifacts. (iii) Factuality: minimal edits to factual attributes (entities, dates, locations, quantities, roles) to yield clear entailment/not-entailment cases. For each premise, annotators draft candidates for both labels (entailed, not entailment) using any styles, with the constraint that at least one pair per premise uses the linguistic style to ensure systematic coverage. separate validation pass with different annotators labels the pairs; disagreements are adjudicated. We maintain 50/50 label balance within each split (train/dev/test). To promote consistency, we provide concise labeling guidelines: an entailment hypothesis must follow from the premise under common background knowledge without adding new specific details; not-entailment hypothesis is either plausible but unwarranted (neutral) or incompatible (contradiction). Turkish-specific cautions include using both clausal (değil, -ma/-me) and morphological (-maz, yok) negation without over-reliance on any single cue; careful treatment of quantifiers and determiners (tüm/bazı/hiçbir) and their scopes; correct agreement and case marking; attention to evidentiality and modality (-miş vs. -di; olabilir, muhtemelen), especially for neutrality; and leveraging SOV flexibility and focus particles (bile, sadece) without altering truth conditions. We control artifacts by (i) balancing lexical overlap across labels, (ii) limiting the fraction of pairs where overt negation alone flips the label, (iii) diversifying contradiction mechanisms (number, date, role, entity swaps; world knowledge), (iv) prohibiting verbatim copying of premises, and (v) deduplicating near-duplicates via MinHash and embedding screening. Quality assurance includes two independent labels per pair, majority-vote gold with no-consensus bucket excluded from evaluation for auditability."
        },
        {
            "title": "3.2.7 Absence of TrWNLI: Why WNLI Does Not Transfer to Turkish",
            "content": "We argue that Turkish adaptation of WNLI (Winograd-style NLI) is linguistically ill-posed and largely redundant. The core ambiguity exploited by WNLI in English reference resolution for third-person pronouns under minimal lexical cuesdoes not naturally arise in Turkish due to (i) pervasive subject and object omission (prodrop/zero anaphora), (ii) rich case and agreement morphology, (iii) productive voice and reflexive morphology, and (iv) flexible word order that encodes information structure. Literal translations of WNLI items either (a) suppress the relevant ambiguity by distributing information across morphology and structure, or (b) yield stilted, 18 non-native uses of overt pronouns, thereby testing translation artifacts rather than reasoning. In other words, the construct measured by WNLI in English is anchored in properties of the English pronominal system (limited morphology, obligatory overt pronouns, and fixed SVO word order). Turkish departs from these properties along several dimensions: pronouns are frequently omitted when recoverable, argument roles are morphologically marked, and discourse prominence is signaled by word order and particles. As result, when the same scenarios are expressed idiomatically in Turkish, the intended ambiguity is either eliminated by overt morpho-syntactic cues or becomes unnatural to maintain. This misalignment undermines both the face validity (native acceptability) and the construct validity (is the benchmark still testing pronoun resolution?) of direct TrWNLI port. WNLI descends from Winograd Schemas: minimally contrasted sentence pairs in which pronoun (he/she/it/they) admits two plausible antecedents, and where subtle lexical or world-knowledge cue determines the correct resolution. In English, the pronominal system provides limited morpho-syntactic disambiguation: third-person pronouns lack rich case morphology (beyond nominative/accusative), have limited agreement features, and rely heavily on syntactic position and semantics for interpretation. Consequently, the benchmark meaningfully probes pragmatic reasoning and commonsense under tight surface controls. Crucially, the English design assumes that the pronoun must be overt and that replacing it with full noun phrase would change the distribution of cues the model sees, weakening the diagnostic. The minimality constraints (few function word changes, no explicit re-mention of antecedents) are tailored for English. When transported to languages where such constraints are not naturalbecause pronouns are optional or because case and agreement overtly encode rolesthe same minimal edits no longer isolate pragmatic knowledge; instead, they repackage or erase it via grammar. In Turkish, the same functional space is occupied and often obviated by morphological and structural mechanisms. First, pro-drop and zero anaphora are pervasive: null subjects and (to lesser extent) null objects are licensed when recoverable from context. Overt third-person is optional and stylistically marked; native renderings prefer dropping it. This removes the surface pronoun token that WNLI manipulates and shifts interpretation to verbal morphology and discourse coherence. Practically, contexts designed to hinge on it or she often have no overt pronoun at all in Turkish. The reference is resolved through the agreement morphology on the verb and through discourse structure. Forcing an overt to mimic English typically sounds marked or literary and introduces unnaturalness that contaminates the evaluation signal. Second, case and postpositions play central role. Productive case marking accusative (- (y)I ), dative (- (y)A), locative (-DA), ablative (-DAn)and postpositions encode grammatical/semantic roles, often forcing unique reading that English leaves ambiguous. Ambiguity that hinges on bare it in English is resolved by havuçta (LOC) vs. havucun (GEN) vs. havucu (ACC), etc. Case morphology does not merely add redundancy; in many minimal pairs, it deterministically signals affectedness, definiteness/specificity, and thematic roles. Thus, tiny morphological choices (e.g., 19 locative vs. genitive) collapse the ambiguity without appeal to world knowledge. This shifts the task from pragmatic inference to morphological parsing. Third, agreement, voice, and reflexives further constrain interpretation. Verbal inflection marks person/number and often identifies the elided subject. Voice alternations (passive, causative, reflexive, reciprocal) restructure argument prominence, while reflexive kendi/kendisi and possessive suﬀixes (-sI ) disambiguate coreference that English encodes with bare pronouns. For example, reflexive morphology encodes selfdirected actions that English indicates with himself/herself or even bare him/her under certain constraints. In Turkish, choosing between kendi(ni) and onu is categorical signal of binding, not subtle cue. Similarly, passive voice can suppress the agent, altering salience and reference in ways that make Winograd-style ambiguity poorly formed. Fourth, flexible word order and information structure allow speakers to signal intended referents via placement and particles (e.g., sadece, bile) rather than through pronominal choice. Native phrasing tends to encode the disambiguating cue structurally. Information-structural choices are not ornamental: fronting constituent or marking focus can make one antecedent prominent and the other backgrounded, which in turn guides pronoun resolution. English relies more on prosody and fixed order; Turkish openly uses syntactic flexibility. Consequently, literal Winograd item translated word-for-word often becomes trivial for native readers, because the right antecedent is foregrounded by structure. Consider two canonical WNLI-style examples and their Turkish realizations. Carrot/pin (English): stuck pin through carrot. When pulled the pin out, it had hole. Hypothesis: The carrot had hole. Literal Turkish: Bir havucun içinden iğne geçirdim. İğneyi çıkardığımda bir delik vardı. / Havucun bir deliği vardı. Natural Turkish: Bir havucun içinden iğne geçirdim. İğneyi çıkardığımda havuçta bir delik vardı. / Havuçta bir delik vardı. In English, the pronoun it is ambiguous between pin and carrot; world knowledge resolves it in favor of carrot. In Turkish, the natural formulation avoids an overt pronoun entirely and uses the locative havuçta, which encodes the affected location. The morphological marking makes the intended referent explicit, so the Winograd-style ambiguity evaporates without invoking commonsense. Tatyana/mother (English): When Tatyana reached the cabin, her mother was sleeping. She was careful not to disturb her, undressing and climbing back into her berth. Literal Turkish: Tatyana kulübeye vardığında annesi uyuyordu. Onu rahatsız etmemeye dikkat ederek soyunup ranzasına geri döndü. Natural Turkish: Tatyana kulübeye vardığında annesi uyuyordu. Annesi onu rahatsız etmemeye dikkat ederek soyunup ranzasına geri döndü. / Annesi Tatyanayı rahatsız etmemeye dikkat ederek soyunup ranzasına geri döndü. The English item leverages two female referents and ambiguous she/her. Turkish resolves this by reintroducing the subject noun phrase (annesi) and case-marking the object (Tatyanayı). These choices are natural and preferred; keeping bare pronouns (o/onu) is either marked or unclear. The resulting sentences no longer rely on delicate pronoun cues; reference is disambiguated morpho-syntactically. Attempting to port WNLI to TrWNLI raises methodological issues. First, translation artifacts are unavoidable if one tries to preserve English-like ambiguity. Translators must either overuse overt pronouns or suppress normal case marking, 20 both of which produce sentences that native speakers judge unnatural. Systems trained on such data risk learning to prefer translations rather than mastering Turkish discourse. Second, there is label instability under minimal edits. Edits that are pragmatically inert in Englishsuch as introducing locative or switching bare to accusative objectscan deterministically fix reference in Turkish and flip labels. This violates the minimal-pair design principle and makes evaluation brittle with respect to morpho-syntactic variation. Third, there are construct validity concerns: when disambiguation is carried by obligatory or strongly preferred morphology, the benchmark ceases to isolate commonsense or pragmatic reasoning and instead measures whether models detect overt markers. This conflates the target competence and can inflate scores through shallow cues. Finally, TrWNLI would be redundant with Turkishnative phenomena that we already evaluate elsewhere (e.g., in our linguistic-style tasks): zero anaphora resolution, reflexive vs. non-reflexive alternations (kendi/kendisi vs. onu), possessive disambiguation (annesi vs. annesini), case-driven specificity (ACC vs. bare), and voice alternations (passive/causative) that reshape salience. We do not provide Turkish WNLI (TrWNLI) split in TRGLUE. As argued above, Winograd-style pronoun disambiguation does not transfer to Turkish due to pro-drop, rich case/agreement morphology, and flexible information structure. Direct translations either eliminate the ambiguity or yield translationese, undermining construct validity. Accordingly, TrWNLI is omitted from TRGLUE. For transparency and comparability, we include short note in the benchmark documentation and tables clarifying that omission is due to linguistic inapplicability rather than resource constraints. Although we omit TrWNLI, we still assess reference-related reasoning in Turkish via more appropriate tasks (e.g., anaphora and morphology-sensitive inference) described elsewhere in this paper. direct TrWNLI port misconstrues the locus of ambiguity in Turkish and risks measuring conformity to Anglophone pronominal patterns rather than genuine inference. morphologyand discourse-aware evaluation regimewithout WNLI-style splitoffers valid and linguistically grounded path for Turkish NLI. Accordingly, we omit TrWNLI from TRGLUE and document the linguistic rationale to guide future cross-lingual benchmark design."
        },
        {
            "title": "3.3 Comparison of corpus statistics",
            "content": "English and Turkish differ substantially in linguistic structure. Turkish is agglutinative with rich inflectional morphology and relatively free word order [43]. For instance, single Turkish token such as gideceksin corresponds to multiple English words (you will go). These properties affect parsing and tokenization: in Kediyi köpek kovaladı, the dependencies (Kediyi obj kovaladı, köpek nsubj kovaladı, kovaladı root ROOT) differ from The dog chased the cat (The det dog, dog nsubj chased, chased root ROOT, the det cat, cat obj chased). Case marking and flexible constituent order yield different headdependent configurations, often reducing orthographic token counts in Turkish while increasing the diversity of surface forms per lemma. Consistent with this, Turkish 21 translations may contain fewer words on average, as reflected in the Avg. words column of Table 6. Table 6 presents vocabulary statistics for each dataset in both English and Turkish. For each dataset, we computed the total word count, unique word count (vocabulary), and distinct lemma count. While lemma and suﬀix counts are less meaningful for English, they are significant for Turkish due to its morphosyntactic complexity. These statistics were computed using spaCy Turkish models, recognized for their industrialgrade accuracy and speed. The computation scripts are available in the TrGLUE GitHub repository. Table 6: Comparison of vocabulary statistics between the English and Turkish GLUE datasets for each individual dataset. We note that original QQP and MNLI are twice the size of our datasets, STS-B is almost 3 times size. Dataset CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE English Turkish Tot wrds Uniq wrds Lemmas Avg words Tot wrds Uniq wrds Lemmas Avg words 83.6K 646K 226.6K 148K 17.9M 12M 4.3M 300K 6.3K 16.2K 18.5K 6.4K 175K 40K 89K 26.5K 4.3K 13.5K 15.2K 5K 157K 25K 78K 21.8K 7.81 9.22 19.5 9.26 11.25 13.85 18.5 25.95 123.9K 3.6M 170K 37.8K 5.5M 4.2M 4.2M 141K 17.6K 297K 37.5K 3.8K 112K 273.8K 211K 35.8K 12.1K 233K 16.1K 1.9K 42K 124K 110.4K 13.9K 12.48 47.3 16.3 6.17 7.52 21.13 15.9 12. The figures in Table 6 reflect systematic cross-lingual effects rather than preprocessing artifacts. Turkish splits typically have fewer words per instance but many more unique surface forms relative to lemma counts, consistent with agglutinative morphology and productive suﬀixation. This yields higher type-token ratios on the Turkish side (e.g., SST-2, MNLI) even when total word counts are comparable or smaller (e.g., MRPC, RTE). Pairwise tasks (QQP, MRPC, STS-B) compress more aggressively in Turkish, reducing average length while preserving core semantics. These properties informed our design choices: lemma-aware preprocessing for retrieval and filtering, relaxed equivalence for TrMRPC to handle headline-body asymmetries, and sentence-level segmentation for TrQNLI with attention to morphology. Practically, subword tokenization will fragment Turkish words more, potentially increasing sequence lengths despite fewer orthographic tokens; morphologically informed tokenizers and normalization choices therefore have outsized impact. We release scripts and configuration details (model versions, seeds, and normalization settings) to facilitate exact replication and ablation."
        },
        {
            "title": "3.4 Linguistic diversity in TrGLUE",
            "content": "Turkish features agglutinative morphology, relatively free word order, and pervasive pro-drop, posing modeling challenges that differ from English GLUE. We quantify TrGLUEs coverage of these core phenomena and report their per-task distributions. All measurements below are derived with spaCys Turkish pipeline, which jointly predicts POS, morphological features, NER, and dependency parses [44]. Because spaCys components are coupled (morphological analyses inform POS and vice versa, and both condition the dependency parse), our counts reflect integrated morpho-syntactic analyses rather than isolated taggers. Where relevant, we exclude punctuation and follow standard UD conventions for token-head relations and feature inventories."
        },
        {
            "title": "3.4.1 Morphological richness and coverage",
            "content": "We measure (i) morphemes-per-token, (ii) distinct inflectional feature bundles, and (iii) derivational suﬀix productivity using spaCy-based tagging and the Zeyrek finitestate analyzer13. TrGLUE contains on average 2.25 morphemes/token (median: feature bundles 2; 95th pct: 5; 99th pct: 8), with 1,095 distinct inflectional (POSCaseNumberPersonTenseAspectMoodPolarityEvidentiality). Verbal negation via -ma/-me appears in 11.75% of finite verbs, evidential/mirative marking (-miş) in 9.21%, and derivational morphology is frequent (e.g., -lI, -sIz, -CI, -lIk). Across datasets, the morphemes-per-token histograms exhibit consistent core with heavy tail characteristic of Turkish agglutination (Figure 3). The overall median is 2 and the mean is 2.25, with the 95th and 99th percentiles at 5 and 8, respectively. Most tokens fall in the 1-4 range, while meaningful tail (5 morphemes) reflects stacked case, agreement, derivation, and polarity/evidential markers. For readability, the rightmost mass is aggregated into 10+ bin, and we annotate the median and p95 to emphasize the tail. Per-dataset shapes broadly mirror the overall distribution, with NLI/QA skews slightly heavier (more verbs and possessed/case-marked nominals) and sentiment lighter. These histograms contextualize token-level complexity behind length expansion and help explain performance sensitivity in morphologically rich segments."
        },
        {
            "title": "3.4.2 Syntax and word order variation",
            "content": "Across TrGLUE, Turkish displays the expected SOV-dominant profile with limited but measurable flexibility: among clauses with overt subjects and objects, non-canonical permutations (OSV/OVS/VSO/VOS) account for 3.07%, consistent with topicalization and information-structure-driven reordering under generally head-final preference. The pooled average dependency distance is 3.26 tokens (non-punctuation arcs), slightly higher than typical English estimates and compatible with largely right-branching written register in morphologically rich language with postverbal clausal complements and freer constituent placement. Subjects are omitted in 73.64% of finite clauses, underscoring robust pro-drop even in formal task data; this 13https://github.com/obulat/zeyrek 23 rate remains high under conservative coordination-based subject recovery, indicating that null subjects are pervasive rather than parsing artifact. Overall, TrGLUE preserves core typological properties of TurkishSOV alignment, flexible alternations, and extensive subject dropwith dependency lengths in reasonable range for mixed-genre corpora. For English GLUE, we calculated average dependency distance as 2.5 tokens; noncanonical S/O/V orders occur in 1.1% of clauses with overt subjects and objects; and subject omission is 0.5% of finite clauses. Relative to this baseline, Turkish TrGLUE shows longer dependencies (3.26 vs. 2.5), higher rate of non-canonical orders (3.07% vs. 1.1%), and dramatically more subject omission (73.64% vs. 0.5%), highlighting Englishs rigid SVO, non-pro-drop profile versus Turkishs flexible word order and robust pro-drop. We compute average dependency distance as pooled mean over non-punctuation dependency arcs: for each token-head pair within sentence, distance is the absolute difference in token indices, and the corpus average is the sum of distances divided by the number of counted arcs. Clause word order is estimated from dependency parses by locating, for each finite verb (VerbForm=Fin), the nearest overt subject and object dependents within the same sentence. We consider subjects labeled nsubj/csubj (including passive variants) and, for objects, obj/iobj. Orders are assigned by the linear positions of S, O, and V; non-canonical rates are computed among clauses where both and are present."
        },
        {
            "title": "3.4.3 Named entities",
            "content": "Using unified NER pipeline and tokenization regime, TrGLUE exhibits high overall density of named entities. Aggregating across all subsets, we counted 35,635,200 tokens and 3,278,439 entity tokens, yielding an average of 0.092 entity tokens per token (9.2%). In broad terms, this pooled rate indicates that entity-bearing content is frequent across TrGLUEconsistent with the prevalence of newsy or informational text in several tasksso models repeatedly encounter person, organization, location, and miscellaneous entities, making NER coverage and entity-aware tokenization consequential for downstream performance. Because Turkish morphology packs case and agreement onto proper nouns, entity recognition and normalization are sensitive to segmentation choices; nevertheless, the aggregate density suggests ample signal for evaluating how models handle named entities. Totals are computed by summing non-overlapping entity spans and token counts over all instances, using the same lowercasing, no-punctuation counting, and digit-collapsing conventions as elsewhere for consistency"
        },
        {
            "title": "3.4.4 Task-relevant phenomena",
            "content": "Across subsets, the morphemes-per-token profiles share common Turkish core but differ in tail weight and inventory breadth in ways that track genre and task design. The pooled distribution is centered at 2 morphemes with heavy tail (p95=5, p99=8), yet means span from lighter CoLA (2.14) and QQP/SST-2/MRPC/QNLI (2.37-2.42) to heavier MNLI/STS-B/RTE (2.50-2.58). Heavier subsets exhibit more mass in 35 and occasional 6 morpheme bins, driven by stacked case/possessive/agreement on 24 (a) Pooled average dependency distance by task. Bars show the mean token-head distance over all dependency edges within each dataset (excluding punctuation), with the pooled Overall summary on the right; higher values indicate looser, more rightbranching structure. (b) Subject-drop rate (%) in finite clauses. Bars report the proportion of finite clauses lacking an overt subject (null subject), highlighting pervasive pro-drop across tasks; Overall gives the pooled estimate. (d) Entity density by subset and overall. Each panel reports the proportion of tokens that are part of named entities (entity tokens/tokens, %) (c) Non-canonical word order rates (%) by task. All = share of OSV/OVS/VSO/VOS among all finite clauses; S+O = restricted to clauses where both subject and object are overt. Fig. 2: Cross-task syntactic and named entity profiles for TrGLUE. Subfigures (a)-(c) report (a) average dependency distance (tokens), (b) subject-drop rate (%), and (c) non-canonical word order rates (%), with Overall computed over all finite clauses and S+O restricted to clauses where both subject and object are overt. Together they illustrate SOV-dominant syntax with pervasive pro-drop, modest but consistent word-order flexibility, and task-specific variation. nominals and tense-aspect-mood-polarity/evidential layering on verbs; lighter subsets concentrate in 1-3, reflecting shorter, template-like prompts and fewer derivational chains. Inflectional feature diversity follows suit: bundle counts are smallest in CoLA (340) and largest in the pooled Overall set (1,095), with intermediate diversity for MNLI/RTE (745-812) and broad coverage for QQP/QNLI/MRPC/SST-2 (902-1,072), 25 indicating that genre mixture and corpus breadthnot just sizeexpand the space of inflectional combinations. Discourse-layer morphology also varies systematically. Negation and evidentiality are most pronounced in the pooled Overall aggregate (11.75% and 9.21% of finite verbs), and are comparatively subdued in MNLI/RTE (3-6% negation, 2-6% evidentiality) and QNLI (2.76% negation, 8.30% evidentiality), while QQP shows moderate negation but low evidentials, consistent with user queries that favor imperative/instructional moods over source-of-knowledge marking. These differences matter for modeling: datasets with heavier MPT tails and richer bundle inventories tend to induce longer effective contexts and more parameter stress in subword segmentation, whereas lighter, templatic subsets emphasize lexical paraphrase and local morphosyntactic cues. Overall, the per-subset histograms (Figure 3) reveal stable agglutinative backbone with task-specific shifts in tail mass and functional morphology that plausibly underlie observed performance variation across TrGLUE tasks. Across tasks, we see consistent Turkish-typological behavior with task-specific nuances. In CoLA, pro-drop is high and full clauses are mostly canonical SOV, fitting short acceptability stimuli with local phenomena. MNLI shows strong pro-drop and modest non-canonicality across mixed genres, with many partial SV/OV observations likely from ellipsis between premise and hypothesis. MRPC mirrors newsy brevity with high subject omission, modest non-canonical rates, and relatively short dependencies. QNLIs interrogative origins yield elevated partial orders and slightly longer dependencies, while QQPs terse, template-like user questions produce the shortest dependencies despite substantial pro-drop. RTE displays notable OSV/OVS tails driven by topicalized objects and clausal complements, captured by the relaxed object heuristic, alongside moderate dependency length and frequent (zero-)copular structures. SST-2 comprises short evaluative snippets with strong subject drop and longer dependencies due to stacked modifiers, and STS-B (train+dev) shows similarly high pro-drop and somewhat more word-order variability reflecting heterogeneous sentence sources. Overall, Overall aggregated over all tasks (excluding STS-B test) confirm pervasive pro-drop, SOV dominance with occasional flexible alternations, and dependency lengths in reasonable range for mixed-genre Turkish text. Coming to the named entities, entity density and role vary notably by subset, aligning with each tasks operational demands. TrQNLI shows the highest entity concentration (16.3% entity tokens/token), reflecting its news-derived question-passage pairs where salient cues often hinge on named persons, organizations, and locations; resolving entailment frequently requires tracking coreferent entities across sentences and disambiguating near-synonymous organization or place names. TrMRPC likewise exhibits high entity load (15.0%), and paraphrase decisions regularly turn on whether headlines and leads refer to the same named actor, event, or geopolitical unit; subtle surface variations (e.g., honorifics, transliterated names, or suﬀixed proper nouns) can flip labels. RTE (9.2%) features compact premise-hypothesis pairs in which veracity depends on fine-grained entity attributes (titles, nationalities, aﬀiliations), making entity linking and appositive parsing disproportionately influential. TrMNLIs moderate density (8.4%) spans genres; entity cues vary widely by domain (fictional names vs. newswire), challenging models to maintain consistent entity representations across 26 (a) CoLA (b) MNLI (c) MRPC (d) QNLI (e) QQP (f) RTE (g) SST-2 (h) STS-B Fig. 3: Morphemes-per-token histograms by subset. Each panel shows the distribution with capped 10+ bin; vertical markers indicate the subset median and p95. Subsets differ mainly in tail weight and bundle diversityheavier tails (e.g., MNLI/STS-B/RTE) reflect stacked inflection and discourse morphology, while lighter tails (e.g., CoLA/QQP) reflect shorter, templatic inputs. The pooled Overall summary reproduces the overall median 2 and heavy tail (p95=5, p99=8). (i) Overall styles. TrQQP is comparatively sparse (6.4%): user queries often generalize over entity slots (how to get visa for X), so robustness hinges on handling entity placeholders and slot-filling paraphrases rather than rich, document-grounded references. TrCoLA is the second sparsest (6.3%), as acceptability judgments depend primarily on morphosyntax; when entities do occur, they are often inert with respect to the label, though proper-noun morphology can still stress tokenization. TrSTS-B (2%) contains few entity tokens, reflecting its caption-based origin from video and image descriptions. We note that the observed entity-token rates, while seemingly high due to spaCys broad label inventory (including CARDINAL/DATE/TIME/MONEY), are consistent with newsand information-centric text. Taken together, these profiles suggest that entity-aware modeling offers the greatest payoff in TrQNLI and TrMRPC, with measurable but smaller gains in TrMNLI, TrSTS-B, and TrRTE, while TrQQP and TrCoLA primarily test generalization beyond explicit named references."
        },
        {
            "title": "3.4.5 Tokenization behavior",
            "content": "Using BERTurk (32k vocab) on TrGLUE, we observe an average of 1.58 subwords per token, versus 1.29 for BERT ( 28.9k vocab) on GLUE. This gap reflects Turkishs agglutinative morphology, which induces more wordpiece splits despite slightly larger vocabulary. Practically, higher fragmentation increases effective sequence length (and thus compute) for Turkish inputs under the same max length, while improving coverage of rare forms. The magnitudes ( 1.3 for English vs. 1.5-1.6 for Turkish) align with prior reports for comparable WordPiece settings."
        },
        {
            "title": "4 Evaluation using TrGLUE",
            "content": "We utilized the created benchmark to evaluate the widely used pretrained model BERTurk. For the evaluation process, we employed the benchmarking GLUE script from the HuggingFace repository14, making minimal modifications. As mentioned earlier, we maintained the original dataset format to leverage existing resources. Table 7 reports the performances of BERT-base and RoBERTa-base on GLUE alongside BERTurk on TrGLUE. As expected, all models struggle on CoLA, which is both small and linguistically challenging. BERTurk notably outperforms BERT on RTE, while underperforming on STS-B relative to corresponding GLUE baselines. The RTE dataset sizes in TrGLUE and GLUE are nearly identical, suggesting that scale alone does not explain the difference. In contrast, TrGLUEs STS-B is roughly one third the size of the original, which may contribute to the gap observed. Overall, the results indicate broadly balanced performance profile across tasks. Table 7: Performances of BERT-base and RoBERTa-base on GLUE vs. BERTurk on TrGLUE. Note that the original GLUE uses only accuracy for SST-2; we report accuracy and F1-score due to class imbalance. Corpus CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE BERT 52.1 91.6/91.9 78.4/85.4 87.2/86.9 90.1/86.7 84.6/83.7 90.5 67.8 RoBERTa 59.8 94.2/94.3 88.9/92.0 90.5/90.2 90.8/88.7 86.9/87.3 92.3 75. BERTurk 42 87.4/91.3 74.4/72.7 71.3/69.9 95.4/94.3 87.9/90.8 90.6 92.2 14https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_ glue.py 28 To assess whether the constructed training sizes are suﬀicient, we trained with fractions of the available data (0.4-1.0) and plotted learning curves as fraction of training data versus fraction of full-data score. Figure 4 shows that most tasks approach saturation well before using all available training examples, suggesting the current dataset sizes are adequate for the evaluated model class and setup. (a) Learning curves as function of training fraction. Metrics are normalized per task by each tasks full-data score, enabling comparison across heterogeneous metrics. (b) TrRTE vs. RTE. TrRTE saturates by 0.8with higher absolute accuracy, while English RTE shows limited gains between 0.6-1.0and seed-level fluctuations with overlapping confidence bands. Fig. 4: Per-task learning dynamics under fractional training. To assess whether the constructed training sizes are suﬀicient, we trained with fractions of the available data (0.4-1.0) and plotted learning curves as fraction of training data versus fraction of full-data score. As shown in Figure 4, classification tasks (QQP, QNLI, MNLI, SST-2, MRPC) reach geq95% of their full-data scores by 0.6-0.8with narrow confidence bands, and RTE similarly plateaus with overlapping bands beyond 0.6. In contrast, TrSTS-B improves more gradually and continues to trend upward at 1.0, indicating residual headroom at current scale. In addition, we examined TrRTE versus original RTE under the same fractionaldata protocol. Figure 4 overlays their normalized curves. TrRTE reaches its plateau by roughly 0.8of the training set, whereas English RTE varies only modestly from 0.6-1.0, with the shaded confidence bands largely overlapping. These trends indicate that TrRTE is not data-limited in this range and that the English curve exhibits only small, seed-driven fluctuations rather than systematic gains. TrRTE vs RTE As seen in Table 7, TrRTE achieves strong result (0.92 at full data), while English RTE hovers around 0.67 under the same recipe. The fractional-data curves and the overlay in Figure 4 show early saturation for TrRTE (0.6-0.8) with minor seed-level variability thereafter. By contrast, English RTE shows limited sensitivity to additional training data beyond 0.6: mean accuracy across 0.6-1.0 varies within roughly 2-3 points with small dip around 0.8, and confidence bands largely overlap, suggesting fluctuations are seed-driven rather than systematic. We attribute the gap to dataset adequacy rather than size or translation effects. TrRTE is handcrafted natively in Turkish: premises are sampled from licensed news and academic sources, and hypotheses are authored under three styles (linguistic, free, factual) to ensure both systematic coverage (e.g., negation, quantification, agreement/case, modality/evidentiality, word order) and natural variation. We enforce controls to reduce superficial cues (balanced lexical overlap across labels; limits on overtnegation flips; diversified contradiction mechanisms such as number/date/role/entity changes; no verbatim copying), deduplicate near-duplicates, and maintain 50/50 label balance within each split. Each pair receives independent labels with adjudication, and ambiguous items are excluded. These design and QA choices yield clearer supervision per example, leading to higher absolute accuracy and earlier saturation with respect to training fraction (Figure 4). STS-B Despite careful translate-then-edit construction, TrSTS-B yields lower absolute correlation than the English STS-B under the same recipe Table 7. The fractional-data curves in Figure 4 show slower approach to saturation than classification tasks: gains from 0.4to 0.9are steady but modest, and the curve continues to climb at 1.0, suggesting residual headroom rather than early plateau. We attribute this gap to measurement granularity and scale, not task mismatch. First, our Turkish set is roughly one third the size of STS-B, which increases estimator variance for continuous scores and makes fine-grained rank ordering harder to learn; this is visible in the wider confidence band for STS-B/TrSTS-B compared to discrete-label tasks. Second, Turkish surface variation (article absence, richer morphology, productive compounding) introduces more lexical-syntactic paraphrase diversity per meaning unit, so sentence-level similarity depends more on morphology-aware cues that BERTurk underutilizes relative to large English baselines. Third, our cultural substitutions (about 40 targeted edits) reduce literal lexical overlap by design; while they preserve event semantics, they attenuate n-gram and entity matching signals that correlation-tuned objectives exploit. Finally, de-duplication and collapsed-pair filtering remove many near-identical pairs that in English act as easy anchors at the top of the scale; their absence yields sharper distribution and fewer trivial high-similarity examples, lowering absolute correlation without indicating poorer semantic competence. Together, these factors explain the lower score alongside learning curve that has not fully saturated at 1.0, pointing to expected improvements with larger Turkish data or models with stronger morphology-sensitive similarity modeling."
        },
        {
            "title": "4.1 Investigation of the results",
            "content": "We now turn our focus to the CoLA dataset, which posed significant challenges for BERT-based model, resulting in poor performance and limited success."
        },
        {
            "title": "4.1.1 CoLA",
            "content": "The CoLA dataset has proven to be exceptionally challenging, even for some state-of-the-art multilingual closed and open-source LLMs known for their English performance. Human evaluators also found the task daunting, with all annotators expressing diﬀiculty. Despite possessing substantial experience in linguistics, particularly Turkish linguistics, the author encountered challenges when analyzing sentences from this dataset. Some sentences had complex semantics, necessitating multiple readings for comprehension. In our experimentation, we engaged with Claude 3 Sonnet [45], Gemini 1.0 Pro, GPT-4 Turbo [46], LLaMa 3 70B [47], and Qwen2-72B [48]. These interactions occurred on the Poe.com platform. With each LLM, we initiated conversations with greeting, followed by an informative sentence about the task and the input-output format. Subsequently, we presented the test sentences in segments and obtained the corresponding labels. Details about the LLM evaluation process can be found in Appendix B. Notably, all results presented are zero-shot results. Table 8: 0.0 correlation coeﬀicient being the score for random guessing, it was surprising that LLaMa 3 70B initially achieved nearly 0.0. Evidently, this model lacked understanding of Turkish grammar and semantics. However, with coherent chain of thought, its performance notably improved. Claude 3 Sonnet, Gemini Pro, and GPT-4 did not fare well either. Only Qwen2-72B managed to produce significant results and demonstrated proficiency in Turkish. Model Gemini 1.0 Pro GPT-4 Turbo Claude 3 Sonnet LLaMa 3 70B LLaMa 3 70B CoT Qwen2-72B BERTurk Matthews corr. 0.21 0.28 0.14 0.05 0.35 0.47 0.42 Table 8 displays the success of various closedand open-source LLMs on TrCoLA, alongside BERTurk for comparison. Surprisingly, despite the impressive reputation, Claude 3 Sonnet, GPT-4, and Gemini Pro showed lackluster performance. Conversely, LLaMa 3 70B and Qwen2-72B emerged as clear frontrunners. Dissecting the results and examining the confusion matrices depicted in Figure 5, we observe that Gemini failed completely in the task. It struggled to differentiate between acceptable and unacceptable sentences, often mislabeling each sentence as acceptable. Gemini predominantly misclassified syntactic variation sentences as true negatives, while struggling with semantic, morphological, and some syntactic variations. GPT-4 Turbo also exhibited subpar performance, albeit showing some ability to distinguish unacceptable sentences to certain extent. However, the poorest performance was seen in Claude 3 Sonnet. 31 Fig. 5: Confusion matrices of the LLMs utilized in our study on the CoLA dataset. Regarding LLaMa 3 70B, the results reveal two distinct outcomes. Initially, Matthews correlation coeﬀicient of 0.05 was observed, almost akin to random guessing. However, when prompted with chain of thought task to provide label for each sentence along with brief justification, LLaMa made remarkable leap from nearly 0 to 0.35, delivering significant improvement. Similar to Gemini, LLaMa struggled primarily with false positives and faced challenges in discerning unacceptable sentences. While the results of LLaMa 3 70B and Qwen2-72B may appear similar, they are indeed quite different. Figure 6 showcases the responses from these two LLMs to the test sentence Kardeşiniz buradası mı?, correctly predicted by BERTurk, LLaMa, and Qwen2; and incorrectly predicted by Gemini and GPT-4. This sentence is grammatically incorrect as bura+da+sı combines the locative suﬀix -(D)A with the possessive suﬀix -(s)I in morphologically invalid manner according to Turkish grammar rules. When queried about this sentence, LLaMa responded that the sentence is not grammatically incorrect but rather that buradası is not valid word in Turkish, based purely on the word statistics it encountered during training. Conversely, Qwen2 provided the correct answer along with grammatical explanation, showcasing its deep understanding of Turkish morphology and syntax. When examining the responses of Gemini Pro in Figure 7, it appears that the model attempts to evaluate the sentence within the context of Turkish grammar or semantics. However, critical error was made concerning the role of the possessive suﬀix -(s)I, 32 (a) Qwen2-72Bs response to the provided test sentence. (b) LLaMa 3 70Bs response to the provided test sentence. Fig. 6: Justification of top performing LLMs for their responses to the chosen test sentence. (a) Gemini 1.0 Pros response to the provided test sentence. (b) Follow-up question to Gemini Pro and its response. (c) GPT-4 Turbos response to the provided test sentence. (d) Claude 3 Sonnets response to the provided test sentence. (e) Follow-up question to Claude 3 Sonnet and its response. (f) Continuation answer. of Claudes Fig. 7: Justifications of underperforming LLMs for their responses to the chosen test sentence. incorrectly claiming its correct usage twice. Even when prompted specifically about 33 this suﬀix to provide clue, Gemini Pro did not improve its understanding. On the other hand, GPT-4 provided correct answer but did not specify the nature of the mistake. In the case of Claude 3 Sonnet, it correctly labeled the sentence as unacceptable; however, the rationale behind this decision was solely based on corpus statistics, without any mention of morphology or the incorrect usage of the possessive suﬀix -(s)I. When explicitly questioned about the -(s)I suﬀix, Claude 3 Sonnet failed to recognize it as possessive suﬀix, offering erroneous claims about Turkish grammar. This indicates lack of consciousness regarding Turkish morphology and the provision of inaccurate information to users. It is evident that all LLMs exhibit some understanding of corpus statistics, possess certain syntactic knowledge, deduce syntactic rules, and comprehend semantics to some extent. However, when delving into the intricate mechanisms of the Turkish language, particularly its morphology and syntax in detail, they fall short, with the exception of Qwen2-72B. Qwen2-72B stands out as the only model that truly embodies Turkish language knowledge at its core. Regarding BERTurk, the model demonstrated proficiency in identifying morphological errors effectively. Unsuccessful classifications primarily stemmed from syntactic and semantic variations. For both the CoLA task, LLMs do not exhibit the same level of success as their English dataset counterpart. recent study by [49] discovered that the performance of ChatGPT, also known as GPT-3.5 [50], on the Japanese WNLI task does not correlate with its performance on the English WNLI task. The study evaluated GPT3.5s performance on tasks involving world knowledge and commonsense reasoning relative to human performance in Japanese. Their findings indicate that ChatGPT achieves lower accuracy compared to humans. The study highlighted that variations in training data across different languages can impact LLM performance, suggesting that success in English may not necessarily generalize to other languages, notion supported by our own research."
        },
        {
            "title": "5 SentiTurca",
            "content": "The SentiTurca dataset is tailored for sentiment analysis tasks, encompassing three distinct domains: movie reviews, customer reviews from e-commerce websites, and newly introduced hate speech dataset. The movie and customer reviews were initially rated by reviewers, whereas the hate speech dataset has been annotated by human annotators from the data company Co-one. Examples from each dataset can be found in Appendix C. Table 9: Size of SentiTurca datasets. Train Dev 60K 73K 42K Test Metric 8.9K 8.9K acc./F1 acc./F1 15K acc./F1 5K Dataset Movies Customer reviews Turkish Hate Map 15K 5K"
        },
        {
            "title": "5.1 Movie reviews",
            "content": "This dataset consists of movie reviews, the same as TrSTS-2 dataset, detailed in Section 3.1.2. We incorporated it into the SentiTurca benchmark to ensure thorough coverage. It is reiterated in this section for completeness."
        },
        {
            "title": "5.2 Customer reviews",
            "content": "This dataset was collected by scraping two distinct e-commerce platforms, Hepsiburada and Trendyol, amassing total of 103K reviews. Each data instance in the dataset comprises review text along with star rating ranging from 1 to 5. These reviews encompass wide array of product categories, including apparel, food items, baby products, and books. The size breakdown of each segment of the dataset is detailed in Table 9. Initially, all reviews and ratings for each product were crawled. However, not all reviews were directly utilized due to mismatches between review texts and star ratings. Certain users indicated in their reviews that they granted 5-star rating to boost visibility, followed by negative critique. To filter out such reviews, semantic grouping and pattern matching techniques were applied. Outlier candidates were identified by clustering 5-star reviews based on semantic similarities, common patterns were extracted among these candidates, and list of patterns signaling such review texts was compiled. These reviews were then filtered out using the compiled patterns list. (a) Representation of star distribution within the dataset. (b) Illustration of word count distribution in customer reviews. Fig. 8: Data insights from the corpus of customer reviews dataset. Upon data analysis, it was noted that 60% of the ratings were either 4 stars or 5 stars, indicating that customers tended to leave reviews primarily when satisfied with the product. This trend is depicted in Figure 8. Figure 8 showcases the word count distribution within the dataset. On average, each review consists of 9.39 words and 70.26 characters, which aligns with the concise nature of product reviews. Moreover, notable portion of reviews includes emoticons. 35 The significance of this dataset lies in its scale, being the most substantial collection of customer reviews published for Turkish to date. Previous studies, such as [51] with 2K reviews and [52] with 150K reviews, fall short in terms of scale and quality; the former dataset is notably smaller, and the latter offers only three classesnegative, positive, and neutral. Although [53] focused on categorizing around 15K e-commerce reviews, they did not publicly release the dataset. This dataset is an integral part of SentiTurca and has its dedicated repository on Hugging Face as well15."
        },
        {
            "title": "5.3 Turkish Hate Map",
            "content": "This dataset is component of SentiTurca and is also accessible independently on Hugging Face.16 Comprising 52K entries distributed across 13 distinct categories such as misogyny, political animosity, animal aversion, vegan antipathy, ethnic group hostility, and more, these entries were sourced from the collaborative hypertext compendium, Ekşi Sözlük. Within this platform, users contribute texts on myriad of subjects spanning life, politics, and everyday concerns. Each entry is associated with its own webpage on the site, facilitating users in exploring grouped texts based on the entry. Details regarding the target categories and the corresponding entry counts are outlined in Table 10. Table 10: Hate Target Categories: Ethnic groups encompass variety of ethnicities within Turkey, including Kurds, Laz, Circassians, and even Turks. Sects denote various branches of Islam, such as Shiism, Hanafism, Sunnism, and Shafiism. The Animals category encompasses both domestic pets and stray animals. Target group Animals Cities Ethnic groups LGBT Misogyny Occupations Politic Political orientation Refugees Religion Sects Vegans-vegetarians Total Size 1.2K 1.2K 4.4K 1.1K 19.9K 0.8K 12.6K 3.4K 2.1K 2.1K 1.5K 1.3K 52K Four labels were employed for annotation: offensive, hate, neutral, and civilized. The civilized classification signifies that text conveys an opinion in manner conducive to civilized discussion with the author of the text. The breakdown of 15https://huggingface.co/datasets/turkish-nlp-suite/MusteriYorumlari 16https://huggingface.co/datasets/turkish-nlp-suite/TurkishHateMap 36 labels for each target group is illustrated in Figure 9, with sample instances from each label and category detailed in Appendix C. Fig. 9: The distribution of labels across different target groups indicates that politics, political orientation, refugees, and misogyny demonstrate elevated levels of hate and offensiveness. These subjects frequently ignite heated discussions and draw substantial levels of animosity, especially concerning refugees and women. Notably, stray dogs have also been the subject of notable amount of animosity in recent times. When analyzing the distribution of labels across various categories, it becomes evident that the majority of the text falls into either the offensive or neutral categories. Hate speech and civilized expressions represent minority, with one notable exception being the category of refugees. Within this category, hate speech predominates, closely followed by offensive language. There are notably fewer instances of neutral and civilized comments. This skew in distribution could be attributed to the substantial influx of migrants to Turkey since 2010, encompassing both irregular and regular migrations, commonly referred to as the migrant crisis.17 Consequently, the authors of the text exhibit pronounced reaction to this particular issue. The LGBT category contains the smallest proportion of civil discourse, while categories like animals, ethnicity, and vegans/vegetarians exhibit the highest percentage of civilized discussions. The remaining categories demonstrate similar pattern, primarily comprising neutral and offensive content. In terms of significance, our dataset stands out for its scale and inclusivity across broad spectrum of target demographics. For instance, [9] provided collection of 60K tweets spanning five distinct categories, while [54] contributed dataset comprising 17https://en.wikipedia.org/wiki/Turkish_migrant_crisis 37 10K tweets. [55] presented hate speech dataset focusing on misogyny and refugee hate, albeit with relatively modest sizes of 1,206 and 1,278 instances, respectively. Additionally, [56] introduced corpus containing 36K tweets featuring offensive language. Notably, these datasets, primarily Twitter-based, are comparatively smaller than their English counterparts and are constrained by the platforms character limit, often hindering the depth of semantic analysis. (a) Distribution of labels in the dataset. (b) Distribution of the number of words in review texts. Fig. 10: Statistical information about Turkish Hate Map dataset. In contrast, our dataset boasts substantial size of 52K instances encompassing diverse categories. The source material closely mirrors everyday conversational language, enhancing its authenticity and applicability. Each instance within our dataset contains an average of 57 words and 453 characters, nearly doubling the length of typical tweet. This expanded length allows for more comprehensive exploration of language nuances and contextual intricacies, setting our dataset apart in terms of depth and breadth of content representation. Misogyny, political hate, ethnic tensions, refugee sentiments, and political aﬀiliations represent pivotal groups that underpin the prevalence of hate within contemporary societies. Consequently, our Turkish Hate Map dataset holds profound significance not only within the realm of NLP research but also within the broader scope of social sciences. By encapsulating these critical themes, our dataset serves as valuable resource offering profound insights into the societal dynamics and prevailing sentiments. It effectively captures the pulse of society, shedding light on the intricate interplay of hatred and bias across various dimensions, thereby facilitating deeper understanding of societal attitudes and behaviors."
        },
        {
            "title": "5.4.1 Data compilation",
            "content": "The dataset was constructed through focused approach. Candidate headlines were first curated to represent categories such as misogyny, political hate, and other forms of hostility. Subsequently, text entries associated with these headlines were scraped and compiled. The collected texts underwent randomization, and representative sample was extracted to form the dataset. As previously mentioned, Ekşi Sözlük is hypertext-based platform similar to Reddit, where users contribute to topical headlines by writing entries. However, unlike Reddit, users cannot directly quote each other but may reference specific entries by their unique numbers. The platform is open to everyone, attracting individuals from diverse educational, cultural, religious, and economic backgrounds. While anyone can join, becoming published author requires an initial contribution of certain number of entries. Notably, Ekşi Sözlük lacks moderation, allowing unrestricted expression, which often results in the presence of profanity, slang, and offensive language. This absence of moderation, combined with the platforms diversity, frequently leads to debates, quarrels, and hostility in discussions on societal topicsmaking it an ideal source for collecting hate and offensive language data. The platforms linguistic style closely mirrors colloquial spoken Turkish, with authors often using everyday language, idioms, and popular expressions. Furthermore, lengthy and structured arguments are common, enriching the dataset with semantic depth. On average, instances in the dataset contain 57 words and 453 characters, reflecting higher degree of textual complexity compared to traditional social media datasets."
        },
        {
            "title": "5.4.2 Choice of target groups",
            "content": "The inclusion of diverse target groups such as animals, cities, ethnic groups, LGBT individuals, misogyny, occupations, political groups/orientations, refugees, religion, sects, and vegans/vegetarians in hate speech dataset is crucial for understanding the multifaceted nature of hate speech in Turkeys unique sociocultural and political context. These groups were chosen because they represent the most frequent and culturally significant targets of hate speech in Turkish society. For example, discrimination against ethnic groups (such as Kurds) and refugees (particularly Syrians) is deeply entrenched in the societal and political discourse, as noted in studies on ethnic tensions in Turkey [57, 58]. Similarly, hate speech targeting religious groups and sects, such as Alevi communities, reflects long-standing sectarian divides and requires careful contextual understanding due to their sensitivity in Turkish society [59]. The inclusion of political orientations and political groups in the dataset is justified by Turkeys highly polarized political environment, where political identity often fuels online aggression, as discussed by [60]. Hate speech against LGBT individuals and women (misogyny) is another critical focus, as these groups frequently face societal discrimination and hostility in Turkey, particularly on social media platforms [61]. The targeting of groups like vegans and vegetarians or occupational groups (e.g., 39 public servants or healthcare workers) represents emerging hate phenomena driven by lifestyle or professional conflicts, which are increasingly visible in online discourse. Uniquely, the inclusion of animals as target group highlights the societal neglect and abuse of stray animals, significant issue in Turkey, as documented by Haytap (Animal Rights Federation) [62]. Cities as target group are also relevant since interregional stereotyping and rivalry, such as between Istanbul and Ankara, or urban and rural populations, fuel hate speech based on geographical prejudice. By including these diverse groups, the dataset becomes highly representative of Turkeys complex and evolving hate speech landscape. This breadth enables the development of hate detection models that are both culturally relevant and capable of addressing underexplored issues, such as animal hate or occupational discrimination, alongside more traditional forms of hate speech. Such datasets also support broader societal interventions to combat hate speech and promote equality, inclusivity, and coexistence."
        },
        {
            "title": "5.4.3 Choice of labels",
            "content": "labelshate, offensive, neutral, and neither The dataset adopts traditional commonly used in prior studies (e.g., [6366]). Additionally, we introduce novel label, civilized, which represents distinct subgroup within the neutral class. This label stems from two observations: (1) clustering and visualizing texts using sentence encoder revealed semantic subgroup within neutral instances, and (2) empirical observations identified that certain texts in this group are longer, more structured, and offer meaningful information to readers. We define the civilized label as texts that promote constructive dialogue and provide insights, making them distinct from standard neutral texts."
        },
        {
            "title": "5.4.4 Data annotation",
            "content": "The dataset was annotated by team of 10 human annotators (5 male and 5 female) from professional data annotation company (Co-one, also responsible for TrCoLA annotations). Initially, we explored Snowflake Arctic for annotation automation but found its performance unsatisfactory, as it misclassified numerous offensive instances as neutral. During the first round of annotation, each instance was labeled by three annotators. However, the interrater reliability, measured via Intraclass Correlation Coefficient (ICC), was only 61.2, indicating poor agreement. detailed investigation revealed significant flaws in the annotations. Many offensive texts, particularly in categories like political hate and misogyny, were mislabeled as neutral, despite containing profanity, disrespect, and mockery toward targeted groups. This mislabeling appeared to stem from annotators incorporating their personal opinionsagreeing with texts content often led them to dismiss its offensive tone. Consequently, all first-round annotations were discarded. For the second round, we developed more comprehensive and objective annotation guideline, including numerous examples to clarify edge cases, such as texts with light or heavy profanity, disrespect, or humiliation targeting specific groups (e.g., 40 women or refugees). This revised process resulted in an ICC of 91.2, indicating excellent agreement. While annotators found the task challenging, the improved guidelines ensured higher-quality labels. Both annotation guidelines from the first and second rounds are publicly available in the SentiTurca GitHub repository."
        },
        {
            "title": "6.1 Movie reviews",
            "content": "As previously indicated in Section 3.1.2, the evaluation metrics are binary accuracy and F1-score. BERTurk attains satisfactory score of 87.4/91.3 in these metrics. The obtained results appear acceptable, as illustrated by the confusion matrix depicted in Figure 11. Notably, no specific measures were taken to address the class imbalance issue. Enhancing this baseline performance is left as an open challenge for researchers seeking to delve deeper into this area. Fig. 11: Assessment of BERTurks eﬀicacy in the task of sentiment classification for movie reviews."
        },
        {
            "title": "6.2 Customer reviews",
            "content": "The evaluation criteria employed for this task encompass accuracy and macro F1score. BERTurk achieved respective scores of 0.66 and 0.64. Upon examining the confusion matrix generated by BERTurk, as depicted in Figure 12, instances where the model confuses 1-star with 2-star and 4-star with 5-star reviews are observed, phenomenon that can be reasonably understood. Despite the conventional nature of the customer reviews dataset, typically considered non-complex and akin to standard text classification task, the LLMs yielded unexpected outcomes. Some prominent LLMs exhibited inferior performance compared to BERTurk, as detailed in Table 11, all within single-shot setting. Among the models, Gemini Pro emerged as the most proficient, achieving perfect accuracy score by accurately classifying all reviews. We believe this could be because 41 Table 11: Achieved accuracy and macro F1-scores on the test set of customer reviews for each model. Model Gemini 1.0 Pro GPT-4 Turbo Claude 3 Sonnet LLaMa 3 70B Qwen2-72B BERTurk accuracy/F1 1.0/1.0 0.64/0.63 0.57/0.53 0.58/0.55 0.53/0.50 0.66/0.64 Gemini Pro may have encountered examples from this website during its pretraining. Following closely is BERTurk, whereby confusion primarily arose within the subset of 4-star and 5-star reviews. Notably, BERTurks performance closely rivaled that of GPT-4. While GPT-4 excelled in categorizing 1-star, 2-star, and 3-star reviews, it exhibited more confusion within the 4-star and 5-star categories. Subsequently, the accuracy declined with LLaMa 3, demonstrating notable success with 1-star and 5star reviews but frequently misclassifying 4-star reviews as 5 stars. The performance on 2-star and 3-star reviews was notably lacking. Claudes performance mirrored that of LLaMa 3, with similar confusion matrices. LLaMa excelled in 1-star reviews, while Claude demonstrated proficiency in 3-star reviews. Contrary to expectations given its performance on the CoLA dataset, Qwen2 emerged as the least effective performer. Particularly, Qwen2 exhibited poor performance within the 1-star and 5-star classes, traditionally considered the simplest due to the polarity of the reviewsrepresenting the most negative and positive sentiments. This underscores that despite their exceptional proficiency in tasks such as writing, editing, and translation, LLMs still have substantial progress to make in the realm of Turkish NLU."
        },
        {
            "title": "6.3 Turkish Hate Map",
            "content": "The Turkish Hate Map dataset presents significant challenges stemming from two key factors: semantic intricacies and cultural references embedded within the text. Primarily, instances of hateful and offensive language may lack explicit insults or profanity, often camouflaged within everyday language. Conversely, seemingly neutral text may contain mild or sporadically placed curse words. This dataset frequently features the use of curse words in non-offensive contexts, necessitating models to possess profound comprehension of Turkish semantics. The second challenge arises from cultural references present in the text, encompassing allusions to contemporary popular culture events, concepts, icons, as well as historical references. Consequently, models must adeptly navigate and interpret Turkish cultural nuances. In Appendix C.3, instances illustrating these inherently complex scenarios from the dataset are showcased. Moving on to the performance of the models, it is crucial to note that this dataset exhibits significant imbalance, with limited instances of hate speech and civilized text. Consequently, we opted for balanced accuracy score and macro F1-score metrics for 42 Fig. 12: Confusion matrices of LLMs on customer reviews set. evaluation purposes. Table 12 presents detailed overview of the performance of each model in this context. Table 12: Balanced accuracy and F1-scores achieved by each model on Turkish Hate Map test set. Model Gemini 1.0 Pro GPT-4 Turbo Claude 3 Sonnet LLaMa 3 70B Qwen2 72B BERTurk accuracy/F1 0.33/0.29 0.38/0.32 0.16/0.29 0.55/0.35 0.70/0.35 0.61/0.58 When analyzing the results of BERTurk, as depicted in Figure 13, we observe notable challenge within the offensive category where instances are misclassified as neutral, while some are erroneously placed in the civilized category. Those that are classified as civilized often present lengthy texts containing information and viewpoints, yet manage to convey an offensive tone within few sentences. An example illustrating this phenomenon is provided in Appendix D.1.1. Upon scrutinizing instances misclassified as neutral, distinct narrative emerges. These inherently challenging examples, devoid of explicit curse words or insults yet Fig. 13: Confusion matrix of BERTurk on the test set of the Turkish Hate Map. carrying an underlying offensive tone, are referred to as polite hate within our framework, as showcased in the same section of the Appendix D.1.1. Similar complexities are encountered within the hate class, where certain instances are inaccurately assigned to the neutral category due to the inherent intricacies of the task. When considering the neutral class, BERTurk demonstrated considerable success in its classification. The number of misclassified instances was notably lower compared to the offensive class. Upon examining some of these misclassifications, they may not appear overly challenging, yet the model still encountered confusion, particularly with the offensive class, which proved more intricate to discern. Within the civilized class, instances were directed towards either the civilized or neutral categories. Those categorized as neutral often comprised lengthy texts lacking substantial intellectual content. Conversely, instances misclassified as offensive contained adequate intellectual material but featured sentences with an offensive connotation. These texts predominantly consisted of intellectual and courteous content, with small segment containing curse words or insults. Overall, BERTurk exhibited satisfactory performance. Subsequently, attention turns to the LLMs, where the success metrics, as depicted in Figure 12, may appear discouraging. When it comes to LLMs, distinct narrative unfolds. Initially, closed-source LLMs exhibited substantial resistance in sharing their perspectives on the potential harms of hate speech, refraining from directly returning the labels. Consequently, sending review text via Python script was not feasible. As an alternative approach, inputs were manually provided, enabling the submission of 100 reviews to each model. Upon examining the confusion matrices depicted in Figure 14, it becomes evident that Gemini Pro and Claude exhibited similar performance. Notably, both models misclassified some neutral instances as offensive or hate, as well as mislabeled offensive instances as neutral. In rather considerate manner, these models tended to categorize offensive instances as hate. Such instances can be referenced in Appendix D.1.2. This behavior is reasonable since these models, alongside GPT-4, are recognized for their safety filters and classifiers, demonstrating their awareness of harmful content. Consequently, they displayed cautious approach towards offensive and hateful instances, 44 as reflected in their labeling decisions. Recent research, as highlighted in [67], supports these observations by indicating that Gemini Pro and GPT-4 are inclined to produce responses that counter hate speech, underscoring their familiarity with and sensitivity to hateful content. In the context of misclassifying offensive and hateful content as neutral or civilized, distinct pattern emerges. Instances of this nature often involve nuances such as subtle expressions of hate or references to cultural contexts, rendering them inherently intricate to categorize accurately. In such cases, the models under scrutiny exhibited notable shortcomings, particularly in discerning these subtleties. Similarly, instances where neutral content was incorrectly labeled as offensive revealed challenges related to the comprehension of complex sentence semantics and cultural references. While the models demonstrated satisfactory performance with neutral instances, their effectiveness waned considerably when faced with offensive and hateful content, primarily due to the demanding nature of the classification task. Transitioning to the subsequent models, namely GPT-4 and LLaMa 3, marked improvement in performance was observed across both offensive and neutral categories. The underlying rationales for success and failure resembled those observed with Claude and Gemini, with misclassifications in offensive/hateful categories often stemming from nuanced expressions of hate or offense, while challenges in the neutral class were linked to complexities in sentence semantics and cultural references. Noteworthy examples illustrating the classification outcomes of these models can be found in Appendix D.1.3. Consequently, the behavioral trends of these models parallel those of Claude and Gemini, albeit with an overall enhancement in performance. Upon examination of Qwen2, notable decline in performance was noted, particularly following its commendable performance on CoLA. Despite proficient handling of offensive and hateful instances, Qwen2 exhibited tendency to misclassify substantial number of neutral instances as offensive, resulting in significant degradation in overall performance. The reasons underpinning these misclassifications mirror those encountered by preceding LLM pairs, albeit with heightened frequency of errors. Appendix D.1.4 showcases instances where Qwen2 misclassified neutral content that was correctly identified by other models, highlighting specific area of weakness. These instances, while not inherently complex, were nonetheless mishandled by Qwen2. In conclusion, the Turkish Hate Map dataset presents formidable challenge owing to the intricate nature of the classification task and the complexities inherent in hate speech. Even robust language models exhibit limitations when confronted with certain instances within this dataset. It is envisaged that this dataset will serve as valuable resource for researchers seeking to engage with challenging tasks in the domain of hate speech classification."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper detailed the process of creating TrGLUE, general language understanding benchmark for the Turkish language, and SentiTurca, sentiment analysis benchmark for Turkish. Our aim is for TrGLUE to be utilized as comprehensive evaluation tool for pretrained models and to facilitate the development of more challenging natural 45 Fig. 14: Confusion matrices from LLMs evaluated on the test set of the Turkish Hate Map. language understanding datasets, such as Adversarial GLUE [68]. Moving forward, our future plans involve expanding the collection of Turkish datasets, particularly focusing on instruction fine-tuning datasets. We also encourage all researchers to contribute and share their data for the Turkish language."
        },
        {
            "title": "8 Legal and Ethical Concerns",
            "content": "As previously discussed, we aimed to offer dataset that is both commercially viable and completely free, which led us to select Snowflake Arctic, an open-source LLM with commercial usability for derivative works. Consequently, our translated datasets are now accessible for commercial use under permissive licensing terms. Another important consideration was the acquisition of data through crawling. We consistently adhered to websites robots.txt files and only scraped limited amounts of data, ensuring we did not overwhelm any website servers. Our approach was always respectful, aligning with legal and ethical standards, maintaining polite and considerate manner throughout. All text translations and LLM assessments were conducted on Poe.com, all within the scope of two-month subscription costing $40. All dataset instance lengths complied with the limitations of Poe.com and the individual model constraints."
        },
        {
            "title": "Appendix A TrGLUE",
            "content": "A.1 Prompting and human annotations A.1.1 TrCoLA We used this prompt for Arctic: Im building dataset similar to CoLA for Turkish. Ill input sentence and Id like you to output 3 variations with Morphological Violation, Syntactic Violation and Semantic Violation. Each resulting variation and the variation type should be split by character and given in newline. No explanations or justifications please, only the output format. INPUT: A.1.2 Translation prompting for sentence-pair tasks Prompting for original dataset translation was done same for all the sentence-pair tasks translation. Heres our prompt, where DATASET is place holder name for the dataset names in inference and similarity-paraphrase tasks. Prompt for Arctic is as follows: Hello, Id like to translate DATASET dataset to Turkish. Ill input each pair as Sentence1 Sentence2. Ill split different pairs with ## characters. Please translate each pair and provide the output per pair separated by new line. No explanations, or greetings, only the output please. Each output must be of the form Sentence1 Sentence2, no other output formats. Im sending the pairs right now. INPUT: A.1.3 TrMRPC We used the following prompt to generate labels for Arctic: 47 Purpose You are assisting in creating relaxed Microsoft Research Paraphrase Corpus (MRPC) dataset for Turkish. Task Input: Each line contains pair in the format: sentence1 sentence2 Output: One label per line, no extra text: 1 = paraphrase (same main claim) 0 = non-paraphrase (different main claim or different truth conditions) Exactly 10 input lines exactly 10 output lines. Guidelines Label 1 (Paraphrase / Same Main Claim) Sentences are 1 if they assert the same central event, fact, or claim, even if one has extra non-contradictory detail. Allowed differences: 1. Headline vs. detailed sentence short headline is 1 with longer sentence if both communicate the same event/claim. Extra detail about who / where / how is fine. Ex: Pariste ilk: Sinek alarmı harekete geçirdi. Pariste sağlık yetkilileri, kaplan sivrisinekleri nedeniyle ilk kez bölgeleri dezenfekte etti. 1 48 2. Attribution frames Phrases like farkı, etkisi are OK if the underlying result matches. Ex: Fenerbahçede İsmail Kartal farkı! Sağlam savunma Fenerbahçede artık savunma daha sağlam. 1 3. Wording / structure changes Differences in order, phrasing, synonyms, activepassive, or minor detail that does not change truth conditions. Ex: Kaza sabah saatlerinde oldu. Kaza sabah 8de meydana geldi. 1 Label 0 (Non-paraphrase) Sentences are 0 if they do not assert the same claim, even if topically related. Cases that require label 0: 1. Topic overlap only Same subject, different claims. Ex: İnsanlar kumruları beslemek için simit atıyor olabilir. Kumru güvercine benzer. 0 2. Mismatched certainty Hedge vs confirmation changes the claim. Ex: Şirketin iflas ettiği iddia edildi. Şirketin iflas ettiği doğrulandı. 0 3. Contradictory numbers/dates/names/places If the factual quantity changes the event. Ex: Bakanlık 500 öğretmen atadı. Bakanlık 600 öğretmen atadı. 0 4. Negation or polarity change Ex: Sözleşme imzalandı. Sözleşme imzalanmadı. 0 5. Role swap or incompatible coreference Ex: Ali, Veliyi yendi. Veli, Aliyi yendi. 0 6. Headline mismatch Headline and sentence talk about the same context but assert different claims. Ex: Milli Piyango sonuçları kazananları gösterecek. Sıralı cnnturk.comda olacak. 0 Decision Rule Substitution test: If you can swap one sentence for the other in news story without changing the intended main claim, label 1. If substitution changes the claim 0. When in doubt 0. tam liste Formatting Input: sentence1 sentence2 Output: exactly one character per line: 1 or 0 No spaces, no extra text, no explanations. A.1.4 TrSTS-B TrSTS-B is direct translate, hence for the translation we used the prompt in A.1.2. Post-translate annotation guideline for the human annotators is as follows: 49 Purpose Edit Turkish sentence pairs to be grammatical, idiomatic, and culturally appropriate. Preserve (do not change) the provided semantic similarity label for each pair. If necessary, minimally adjust one or both sentences so that their meaning aligns with the given label. Input and output You will receive: batches of Turkish sentence pairs with fixed similarity label (score provided). You will deliver: for each pair, the revised sentences formatted as: Sentence Sentence Do not edit the score; the score is already provided and must remain unchanged. General Principles Use minimal edits to make sentences natural and clear in Turkish. Maintain the pairs semantic relationship so it matches the given score. Prefer idiomatic Turkish over literal translation; ensure cultural appropriateness. Avoid introducing new facts that would shift the intended similarity. Editing Rules (Fit-to-Label) 1. Naturalness and correctness. Fix grammar, morphology, and word choice to sound native, without changing the underlying meaning more than needed to match the label. 2. Label preservation. Do not alter the provided similarity label. If the current pair does not fit the label, adjust wording (paraphrase, specify, generalize) so that it does. 3. Semantic tuning by label range. Label 4.55.0 (near-identical): Make the two sentences semantically equivalent; vary only style or minor surface form. Label 3.04.4 (closely related): Keep the same core event/entity; allow small detail or phrasing differences. Label 2.02.9 (loosely related): Keep shared topic/frame but introduce notable difference in detail, stance, or entailment strength. Label 1.01.9 (unrelated): Remove unintended links; ensure sentences refer to different events/entities/topics. 4. Cultural adaptation Replace culturally odd items with natural Turkish alternatives only if this does not change the intended similarity level. 5. Morphosyntax Correct case marking (accusative, dative, etc.), agreement, and clitics; prefer idiomatic verb-noun combinations. 6. Avoid trivial-only contrasts. If the pair differs only by tense/aspect but the label implies stronger/weaker relation, adjust lexical content minimally to reach the target similarity. 7. Removal policy Remove pair only if it is irreparably nonsensical and cannot 50 be edited to match the given label without inventing content. Otherwise, edit to fit. Quality Checklist Both sentences are grammatical and idiomatic. The semantic relationship matches the given label. Cultural references are appropriate for Turkish. Edits are minimal and do not introduce new facts that skew similarity. Formatting Output exactly one line per pair: Sentence Sentence B. Do not output the score; it is fixed and known. Use sentence-final punctuation only when natural. Notes If tiny edit suﬀices to fit the label, prefer the smallest change. For borderline cases, prioritize clarity and idiomatic usage while keeping the label intact. A.1.5 TrQQP We use the following prompt for generating TrQQP labels: Hello, Im creating QQP dataset equivalent for Turkish. collected some questions from Turkish news website. The dataset consist of triplet, Question1, Question2, and Label. Ill feed pair of the form Question1Label and wanna get Question2 from you. Label can be either 0 - non-duplicate or 1 duplicate. If the given label is 1, duplicate (a) you can paraphrase question1 (b) make similar sentence with extra few words (c) or make sentence with some extra words. One example is lets say question1 is Yolumuz evrim yolu mu, uzay yolu mu?. Possible duplicates are Bilimin yeni yonu evrimi incelemk mi, yoksa yuzumuzu uzaya donmek mi?, Bulutlarin otesi mi, yoksa damarlarimizdaki sonsuzluk mu, Istikbal goklerde mi yoksa DNAmizda mi?, Yeni yonumuz evrim mi, uzaya cikmak mi?. As you see you should bias towards Turkish idioums, sayings and Turkish culture during generation. If the label is 0 - non-duplicate you can include totally irrelevant sentences or sentences using same/similar words but semantically different. For the example sentence above some examples might be Uzay arastirmalari icin butcemiz yeterli mi?, yeni Turk uzay istasyonlari acilacak mi, Trump Erdogan gorusmesi nasil gecti?, ABD-Turkiye iliskileri bundan sonra nasil olacak?. All the question2s you should generate look like news headings. Ill feed 10 pairs of the form Question1label , question1 and label is separated by character and pairs are separated by newlines. Youll output 10 triplets of the form Question1LabelQuestion2, same delimiter char and pairs are separated by newline. You can see the same questions1 and label in consecutive rows, please generate different question2s for the same question1 in that case. Output only your output, no greetings, explanations or suggestions. Here is my 10 pairs. INPUT: 51 A.1.6 TrMNLI For generating hypotheses and labels, we used three different prompts for three different areas of generation, factual, linguistic and free style. Here are the three prompts in order: Factual You are creating Multi-Genre Natural Language Inference (MNLI) dataset focused on **factuality**. will provide sentence (the premise) and label (entailment, contradiction, or neutral). Your task is to generate second sentence (the hypothesis) that corresponds to the given label while adhering to the following rules: 1. The hypothesis must be grounded in **factual information**: **Entailment**: The hypothesis must be an objectively true statement that is logically inferred from the premise. **Contradiction**: The hypothesis must directly contradict an objective fact stated in the premise. **Neutral**: The hypothesis must introduce new factual information that is unrelated to or not implied by the premise. 2. Ensure the tone and style of the hypothesis match the premise (e.g., formal, descriptive, or conversational). 3. Avoid generating speculative, subjective, or nonsensical hypotheses. 4. If the premise is unsuitable (e.g., spam, nonsensical, or not factual), exclude it from the output. will provide 24 premise-label pairs separated by the character. You should output 24 triplets in the form: premise your hypothesis label. Separate each triplet by newline. Example Inputs and Outputs: **Input 1**: Premise: Türkiyenin başkenti Ankaradır. Entailment **Output 1**: Türkiyenin başkenti Ankaradır. Ankara, Türkiyenin başkentidir. Entailment **Input 2**: Premise: Dünya, Güneşin etrafında döner. Contradiction **Output 2**: Dünya, Güneşin etrafında döner. Güneş, Dünyanın etrafında döner. Contradiction **Input 3**: Premise: Ay, Dünyanın uydusudur. Neutral **Output 3**: Ay, Dünyanın uydusudur. Mars, Güneş Sistemindeki dördüncü gezegendir. Neutral 52 Now, process the following inputs: INPUT: Linguistic You are creating Multi-Genre Natural Language Inference (MNLI) dataset focusing on **linguistic variations**. will provide sentence (the premise) and label (entailment, contradiction, or neutral). Your task is to generate second sentence (the hypothesis) by applying **diverse linguistic transformations** based on the label. Follow these rules: 1. The hypothesis must reflect **linguistic relationship** with the premise: **Entailment**: Use linguistic transformations such as paraphrasing, summarization, or syntactic restructuring while preserving the meaning of the premise. **Contradiction**: Introduce linguistic variations that directly contradict the meaning of the premise (e.g., antonyms, negation, or reversing relationships). **Neutral**: Create hypothesis that introduces new, unrelated factual information or is linguistically unrelated to the premise. 2. Use **variety of linguistic transformations**, including: **Paraphrasing**: Rephrase the premise using synonyms or alternative expressions. **Summarization**: Condense the premise into shorter statement while retaining its core meaning. **Lexical transformations**: Replace words with synonyms, antonyms, or hypernyms/hyponyms. **Syntactic restructuring**: Change the sentence structure (e.g., active to passive voice, clause order rearrangement, etc.). **Focus shifting**: Emphasize different parts of the premise while maintaining its meaning. **Negation**: Use negation to create contradiction or shift meaning. **Addition of unrelated details**: For neutral, introduce new information that is unrelated to the premise. 3. Maintain the tone and style of the premise (e.g., formal, conversational, or descriptive). 4. If the premise is unsuitable (e.g., spam, nonsensical, or not linguistically relevant), exclude it from the output. will provide 24 premise-label pairs separated by the character. You should output 24 triplets in the form: premise your hypothesis label. Separate each triplet by newline. 53 Example Inputs and Outputs: **Input 1**: Premise: Türkiyenin başkenti Ankaradır. Entailment **Output 1**: Türkiyenin başkenti Ankaradır. Ankara, Türkiyenin başkentidir. Entailment **Input 2**: Premise: Dünya, Güneşin etrafında döner. Contradiction **Output 2**: Dünya, Güneşin etrafında döner. Dünya, Güneşin etrafında dönmez. Contradiction **Input 3**: Premise: Ay, Dünyanın uydusudur. Neutral **Output 3**: Ay, Dünyanın uydusudur. Venüs, Güneş Sistemindeki en sıcak gezegendir. Neutral **Input 4**: Premise: İstanbul, Türkiyenin en kalabalık şehridir. Entailment **Output 4**: İstanbul, Türkiyenin en kalabalık şehridir. Türkiyenin en yoğun nüfuslu şehri İstanbuldur. Entailment **Input 5**: Premise: Türkiye, dört mevsimi birden yaşayan bir ülkedir. Contradiction **Output 5**: Türkiye, dört mevsimi birden yaşayan bir ülkedir. Türkiyede kış mevsimi hiç yaşanmaz. Contradiction **Input 6**: Premise: Mısır Piramitleri, dünyanın en eski yapılarındandır. Neutral **Output 6**: Mısır Piramitleri, dünyanın en eski yapılarındandır. Çin Seddi dünyanın en uzun yapılarından biridir. Neutral Now, process the following inputs: INPUT: 54 Free You are assisting in creating Multi-Genre Natural Language Inference (MNLI) dataset. will provide sentence (the premise) and label (entailment, contradiction, or neutral). Your task is to generate second sentence (the hypothesis) based on the given label and ensure the following: 1. The hypothesis must align with the label: **Entailment**: The hypothesis must be logically inferred from the premise. **Contradiction**: The hypothesis must directly contradict the premise. **Neutral**: The hypothesis must be unrelated to or not fully supported by the premise. 2. The hypothesis should match the premise in tone and style (e.g., formal or informal). 3. If the premise is unsuitable (e.g., spam, nonsensical, or advertisement-like), exclude it from the output. will provide 24 premise-label pairs separated by the character. You should output 24 triplets in the form: premise your hypothesis label. Separate each triplet by newline. Example Inputs and Outputs: **Input 1**: Premise: Başbakan Bülent Ecevit Karaoğlan adıyla bilinirdi. Entailment **Output 1**: Başbakan Bülent Ecevit Karaoğlan adıyla bilinirdi. Bülent Ecevitin lakabı Karaoğlandı. Entailment **Input 2**: Premise: Türkiyenin en iyi tatil beldeleri arasında Çeşme de yer alıyor. Contradiction **Output 2**: Türkiyenin en iyi tatil beldeleri arasında Çeşme de yer alıyor. Türkiyenin popüler tatil beldelerinden biri değildir. Contradiction **Input 3**: Premise: İstanbul, Türkiyenin en kalabalık şehridir. Neutral **Output 3**: İstanbul, Türkiyenin en kalabalık şehridir. İstanbul, 1453 yılında fethedilmiştir. Neutral Now, process the following inputs: INPUT: Çeşme, 55 A.1.7 TrQNLI For generating TrQNLI, fist we generated SQuAD style context-question-answer triplets, then generated TrQNLI from these triplets. For generating the triplets we used the following prompt: Hello, Im generating question answering corpus for Turkish in the same fashion of SQuAD. Ill send paragraph as input and Id like you to output question and answer pairs. The original dataset has answers with numerical entities, proper nouns, noun phrases, adjectives and adverbs, please make sure we have them as well. Make sure to use when, who, how, how many, how much etc. in the questions. Also use paraphrasing in questions. Additionally, wanna include verb phrase answers , specific to Turkish syntax e.g. Atatürk bu ülkeyi büyük zorluklarla savaşarak kurdu. - Atatürk Türkiyeyi ne şekilde kurdu? - büyük zorluklarla. Ill input the context, you output 10 question-answer-index triplets. Keep in mind that SQuAD is span extractive type question answering, hence the answer must come from the paragraph and the index should mark the start of the answer span. Please keep the casing of answer as in the paragraph. When giving the answers, take Turkish morphology in the account. The answer may or may not contain suﬀix depending on the question. Separate output triplets with newline, also use the character as separator between the triplet. Like this: CONTEXT: Cumhuriyet Atatürk ve silah arkadaşları tarafından büyük mücadelelerden sonra 1923te ilan edildi. OUTPUT: Cumhuriyet ne zaman ilan edildi? 1923te 78 Türkiye Cumhuriyetinin kurulma tarihi nedir? 1923 78 Cumhuriyeti kimler ilan etti? Atatürk ve silah arkadaşları 11 Cumhuriyet ne şekilde kuruldu? büyük mücadelelerden sonra 50 INPUT: A.1.8 RTE annotation guidelines For creating RTE hypothesis sentences, we gave the following guideline to the annotators: Scope Create Turkish hypothesis sentences that are labeled as Entailment, Neutral, or Contradiction with respect to given premise. Annotators use three generation styles per premise, with at least one linguistic-style item per premise. 56 Generation Styles Linguistic: Target specific phenomenon (negation; quantifiers; comparatives; modality; coreference/anaphora; word order/topicalization; case/agreement; derivational/inflectional morphology; tense/aspect; monotonicity; temporal reasoning; prepositions/spatial). Produce minimal, fluent hypothesis that foregrounds the phenomenon. Free style: Natural paraphrasing that stays on topic while varying wording/syntax; avoid verbatim copying of the premise. Factuality: Minimal factual edits (entities, dates, locations, quantities, roles) to yield clear E/N/C cases; avoid trivial cues (e.g., relying solely on değil). Label Definitions Entailment: Hypothesis necessarily follows from the premise under common background knowledge; avoid adding new specific details. Neutral: Plausible given the premise but not supported; does not contradict the premise. Contradiction: Incompatible with the premise under ordinary interpretation; prefer minimal edits (polarity, role swaps, number/date/location changes). Turkish-Specific Guidance Negation: Use clausal (değil, -ma/-me) and morphological (-maz, yok) forms; avoid overusing single cue. Quantifiers/Scope: Vary tüm/bazı/hiçbir; handle partitives and scope carefully. Agreement/Case: Ensure correct case marking and verbal agreement; control for pro-drop ambiguity. Evidentiality/Modality: Distinguish -miş vs. -di; use modal markers (olabilir, muhtemelen) for neutral cases. Word Order/Focus: Leverage SOV flexibility and particles (bile, sadece) without unintentionally changing truth conditions. Artifact Controls Balance lexical overlap across labels; avoid label-specific surface cues (e.g., constant negation tokens for contradictions). Prohibit verbatim copying of premises; require minimal but substantive edits. Diversify contradiction mechanisms and covered phenomena. Deduplicate near-duplicates (e.g., via MinHash/embedding screening). Quality and Validation Draft candidates for all labels per premise using any styles; require at least one linguistic-style item. Obtain 23 independent labels per pair; use majority vote; mark no-consensus items and exclude them from evaluation. Maintain 5050 label balance (0/1) within each split (train/dev/test). Record metadata: premise provenance, genre, style used, annotator IDs, and adjudication notes. 57 Linguistic Task Menu 1. Proto-Roles: Focus on event participants and their roles (agent, patient, instrument). 2. Anaphora Resolution: Resolve pronouns and nominal references to their correct antecedents. 3. Compositionality: Test meaning composition from modifiers, clauses, or operators. 4. Prepositions: Target spatial relations (üstünde, altında, içinde, yanında). 5. Comparatives: Reformulate or simplify comparisons (daha, daha az, kadar, en). 6. Quantification: Reason over numbers/quantifiers (bazı, tüm, hiçbir, çoğu); prefer monotone-preserving transformations for entailment. 7. Spatial Expressions: Paraphrase spatial descriptions beyond simple prepositions (sağında/solunda, kuzeyinde, yakınında). 8. Negation: Handle explicit/implicit negation (değil, -ma/-me, -maz, yok); avoid over-reliance on negation as the sole cue. 9. Tense & Aspect: Vary tense/aspect/evidentiality (-di, -miş, -yor, -ecek) while preserving intended temporal interpretation. 10. Monotonicity: Test upward/downward entailments under quantifiers, negation, and comparisons. 11. Implicatures: Use scalar/conversational inferences; ensure labels reflect cancellability. 12. Temporal Reasoning: Reason about event order and intervals (önce/sonra, sırasında). 13. Lexicosyntactic Inference: Leverage lexical relations (synonymy, hypernymy) and syntactic alternations (active/passive, causative). Illustrative Examples Premise: Ali topu attı ve Elif yakaladı. Task: Proto-Roles Hypothesis: Elif topu yakaladı. Premise: Ahmet sınava girmedi. Task: Negation Hypothesis: Ahmet sınava girdi. Premise: Bahçede 15 tane ağaç var. Task: Quantification Hypothesis: Bahçede 10dan fazla ağaç var. Premise: Deprem, kurtarma ekipleri olay yerine ulaşmadan önce meydana geldi. Task: Temporal Reasoning Hypothesis: Kurtarma ekipleri depremden sonra olay yerine ulaştı. 58 A.2 Samples from datasets A.2.1 CoLA Ayrıca düz ünlülerden sonra düz, yuvarlak ünlülerden sonra dar yuvarlak veya düz geniş ünlüler gelebilir. acceptable Ali Şir Nevayi, şüphesiz bütün Türk edebiyatı için son derece önemli bir şahsiyet değildir. unacceptable Osmanlı halkı çoğunluksa Türkçe yanışır. unacceptable A.2.2 TrSST-2 insana Çok tatlı bir film. Birbirimizle nasıl ve neden iletişim kuruyoruz? İnsanı bağlayan ya da insanı insandan ayıran-kopararan nedir? Bunları düşünmek üzere. Sosyal bilimlerde araştırma metotları üzerine de oldukça eleştirel bir film, alandan insanlar özellikle keyif alacaklardır zannediyorum. Yönetmen Bent Hamerin diğer filmlerini de izleme listeme ekledim bu filmden sonra. 8,5/10 8 stars MATRIX sinema tarihi için bir milattır. Kuşkusuz sinema tarihinin de en iyi filmidir. Hem senaryosu hem oyunculukları hemde halen çözülmeyi bekleyen gizemiyle. Sadece bunlar değil teknik ve görsellik anlamında da çığır açmış bir başyapıttır. TOP 3 listemde ilk sırada olan bir film. MatrıxV for vendettaSavaş tanrısı 9 stars ilk film için fena olmasa da oyunculuk, olay örgüsü oldukça vasatın altındaydı. konu ve filmin ilk ve son yarısı haricinde pek bir şey veremedi. 4 stars A.2.3 TrMRPC Hamaney iddiaları yalanladı: Hamas saldırısının arkasında İran yok, yapanların ellerinden öperiz. / İran Dini Lideri Ayetullah Ali Hamaney, Hamasın cumartesi günü İsraile yönelik gerçekleştirdiği saldırıda Tahranın rolü olduğuna yönelik iddiaları yalanladı. equivalent Ankaradaki önceki yürüyüşe de katılmıştım; aslında hem imza kampanyaları hem de yürüyüşler söz konusu olduğunda çok hevesli değilim. / Yanılmıyorsam etkin katıldığım son imza kampanyası 1996da, meslektaşımız Metin Göktepeyi katledenlerin bulunması içindi; 600 gazeteciden topladığımız imzaları, Başbakan Tansu Çillere sunanlardan biri de bendim. not equivalent A.2.4 TrSTS-B Bir uçak havalanıyor. / Bir uçak kalkıyor. score: 5.0 Bir adam bisiklet üzerinde gidiyor. / Bir maymun bir bisiklet sürüyor. score: 3.0 A.2.5 TrQQP Gül almak için sevgili yapmaya gerek var mı? / Gül almak için bir sevgili bulmak şart mı?duplicate Yeni vergi paketi: 226 milyar ek gelir nereden elde edilecek? / Uluslararası yatırımcılar Türkiye ekonomisini yakından izliyor, ne bekleniyor? not duplicate A.2.6 TrMNLI İsrail, pandemi nedeniyle Mart 2020den bu yana kapılarını turistlere kapatmıştı. / İsrail, Mart 2020den itibaren pandemi yüzünden turist kabul etmedi. entailment Şu an sektörde Steamde eski oyunlarına zam yapmayan birkaç firma kaldı ve bunlar paragöz olarak bildiğimiz EA, Activision ve Rockstar. / SSteamde eski oyunların tamamına zam yapıldı. contradiction Bu dev yatırım Türkiyeye yakışır niteliktedir. / Almanyanın başkenti Berlindir. neutral A.2.7 TrQNLI Fransızların başkente dair niyetleri neydi? / Fransızlar kendi iradelerini ülkeye dayatmak için zora başvurdular. 29 Mayıs 1945te Fransız komutasındaki Senegalli askerler, tutuklanmaları emredilen hükûmet üyelerini aramak için parlamento binasını bombaladı ve baskın düzenledi. not entailment ASELSAN hangi tür batarya için çalışmalar yaptı? / 2005 yılı itibarıyla ASELSAN 4400 serisi telsizler için ihtiyaç duyulan lityum iyon batarya için yeni bir teknoloji olan lityum iyon batarya konusunda çalışmalar başlatmıştır. entailment A.2.8 TrRTE Yayımlanmış çok sayıda eseri ile sayısız ödüle layık görüldü. / Yazar birçok ödül almıştır. entailment Tarih boyunca Antalya, Akdeniz kıyısında gelişmiş bir turizm merkezidir. / Van Gölü, Türkiyenin en derin gölüdür ve doğal güzellikleriyle ünlüdür. not entailment"
        },
        {
            "title": "Appendix B Evaluation using TrGLUE",
            "content": "Evaluation of CoLA dataset with LLMs As we said before, all evaluations of LLMs took place on the Poe.com platform. After saying some greetings manually, we sent test set instances in chunks of 10. Figure B1 exhibits initiation of evaluation chat with GPT-4 Turbo. We used similar prompt for WNLI, with minimal prompt engineering, just described the task plainly. Our prompt is: have WNLI task for Turkish. wanna 60 Fig. B1: Initiation of the chat with GPT-4 Turbo model. label my sentences as entailment and not entailment. Ill send my sentence pairs in chunks of 10, separated by characters. Please output only the label per sentence, 0 for not entailment and 1 for entailment."
        },
        {
            "title": "Appendix C SentiTurca",
            "content": "Content Advisory: Certain text may be unsettling or distressing due to the presence of hate speech. C.1 Customer reviews Çok kalıtesız ahşabı cok kıymık vardı, malesef iade. 1 star dandik ama iş görüyor 2 stars Fiyat kalite performansı idare eder 3 stars Maliyetine göre iyi bir çanta beğendim 1 puanı kumaşı biraz ince diye kırdım 4 stars Çok sağlam ve kaliteli bir ürün. Beklediğimdende büyük. 5 stars 61 C.2 Turkish Hate Map C.2.1 Animals heading: kopek, text: daha önce yazılmış mı bilmiyorum, evde köpek besleyen insanlar için bi bilgi; eğer köpeğiniz kokuyorsa mamasını değiştirin. kalitesiz mamalar, köpeğin kokmasına neden olur. ağzı da kokar, tüyleri de. eğer maddi olarak gücünüz kaliteli mamaya yetmiyorsa evde kendiniz de yaptığınız gıdalarla besleyebilirsiniz minnoşunuzu. kasaptan aldığınız kemikleri suda kaynatın, içine bulgur koyun mesela. bayılıyorlar. sırf kokuyo diye sürekli banyo yaptırmak beyhude. olay mamada bitiyor. ,neutral heading: kopektapar, text: su kitledir; (bkz: ) zeka seviyeleri ise soyledir. 13 milyon hayvanin kisirlastirilabileceginin teknik olarak mumkun oldugunu dusunurler, bak finansal yanini soylemedim daha bir de dusunun bunlara sirket falan emanet ettiginizi..., label:offensive C.2.2 Cities heading: izmir-denince-akla-gelenler, text: çocukluğum. ilk gençliğim.. ha bir de şevketi bostan. neutral heading: erzurum, text:aşırı dinci ve aşırı ırkçı barbar yuvası. aşırı sağ ete kemiğe bürünüp bir il olsa bu erzurum olurdu, hate C.2.3 Ethnic groups heading: arnavut-denilince-akla-gelenler, text: börek. çocukken babaannemin , halamların evine gittiğimde içeriye girdiğimde burnuma gelen koku.. hala unutamam. annem arnavut olmamasına rağmen, öğrenip bizlere kokuyu hiiiiç unutturmadı sağ olsun! arnavut böreği inanılmaz güzeldir. kesinlikle denemelisiniz. neutral heading: kurtler, text: adam olsalardı bu kadar küfürü hakareti kendilerine ettirmezlerdi. dünyanın hiçbir yerinde bu kadar aşağılanan ve bunu sineye çeken bir topluluk yoktur, yap bakalım bunu amerikada bir siyahiye seni neyin üstüne oturturlar görürsün. tanım: 20 milyonuz diye böbürleniyorlar ama analarına bacılarına kadınlarına bile küfür edilmesine ses çıkarmayan kağıttan kaplan bir millet. yok böyle cesurdurlar, yok böyle yiğittiler filan hikaye, ödleğin tekidirler. bu kadar belli ki şişman irisi ergenlerin bile ağzına çocuk oyuncağı olmuşlardır., hate C.2.4 LGBT heading: lgbt, text: kimse kimsenin cinsel yönelimini değiştiremez. nasıl ki kişisi her sabah yatağından kalktığı zaman hetero bir bireyse, kişisi de yatağından her sabah gey bir birey olarak kalkıyor. kimse heyecan aradığından bugün de gey, lezbiyen, biseksüel takılayım diye yeni güne uyanmıyor, kimse de kimsenin yönelimini değiştirmeye (nasıl olacaksa) çalışmıyor. medyada, internette, orada burada lgbti odaklı filmler, yazılar, diziler ve şarkılar vs. görmeleriniz çoğalmışsa belki sizin cinsel yöneliminizi 62 değiştirmek için değil de, insanlar artık seslerini çıkarmak isteyip bir köşede oturmayıp dizilerini, filmlerini çekip, eserlerini yayınlayıp, yazılarını yazıp daha görünür olmayı seçtiklerinden olmasın?, civilized heading: lgbt-haklari-insan-haklaridir, text: devlet yakaladığı lgbtliyi aynı bölgeye atıp karantinaya alsa ne güzel olurdu. dış dünyadan izole etmek lazım ki gelecek nesillere kötü örnek olmasınlar., hate C.2.5 Misogyny heading: beyaz-tenli-kadin-iticiligi, text: pardon ? tanim : on numara kadinlardir. neutral heading: kadinlari-itici-yapan-detaylar, text: kuaför önünde sohbet edip, sigara içmeleri. bunu zaten keko kızlar yapıyor sanırım. aman diyim, aman., offensive C.2.6 Occupations heading: avukat, text: kendilerine basit bir konu danışmak istediğim meslek grubu. konu; ürün iade - tüketici hakem heyeti., neutral heading: emlakci, text: bedavadan hiçbir iş yapmadan para kazanan, üzerine insanı rezil eden yalancılıkta doktora yapmış nazlarından yanına yaklaşılmayan, meslek grubu., hate C.2.7 Politics heading: iyi-parti, text: tartışma programlarında konuşmak için bahadır erdem, yavuz ağıralioğlu ve ümit dikbayırdan daha başka isimler bulması gereken parti., neutral heading: turkiye-isci-partisi, text: geçtiğimiz seçimde oy vermeyi düşünüp son anda vazgeçtiğim parti. son iki günde görüyorum ki iyi ki vazgeçmişim. zira hamasa destek veren bir partiye oy vermiş olsaydım hayatım boyunca bunun pişmanlığından uyku uyuyamazdım., neutral C.2.8 Political orientation heading: sekuler, text: konuyu dinden ayri ,din etkisi olmadan inceleme durumu., neutral heading: solcularin-cok-zeki-olup-iktidar-olamamasi, text: bu halkın yönelimi sağ ayrıca solcular da tam bir gerizekalı., offensive 63 C.2.9 Refugees heading: suriyeli-siginmacilar, text: bir an önce göndermezsek, yakın gelecekte artık çözülemeyecek sorunlar doğuracakları açık. 2023 seçimleri bu anlamda da son şansımız., neutral heading: suriyeli-siginmacilar, text: savaşın bitip suriyelilerin dönmesini en çok istanbulda kiraları şişiren o****u çocuğu ev sahipleri batsın diye istiyorum., hate C.2.10 Sects heading: alevilerin-genelde-iyi-insanlar-olusu, text: insan bu niye din seçimine göre iyi ya da kötü olsun. sadece uğradıkları zulümlerden dolayı dinden soğumuşlar, bu iyi insan olmaya faktör mü? türk, fransız, rus milliyetçiliğe göre iyi insan kötü insan genellemesini anlarım. maalesef din-mezhep seçimine göre böyle genelleme yapılamaz., neutral heading: alevilerin-genelde-iyi-insanlar-olusu, text: adı ile başlayan 1 tanesi haindir ben onu bilirim. gerisini de tanımam, sen alevi misin de demem. herkesin inancı kendine. ama var ya en azılı haindir., offensive C.2.11 Vegans-vegetarians heading: vegan-cevreci-hayvansever-lezbiyen-solcu-feminist, text: vegan dışında beni tanımlamakta. ( 1 tanesi daha uymuyor ama onu söylemeyeceğim), neutral heading: vegan, text: farklı bir yaşam biçimidir. b*kunu çıkarıp mavi hap kırmızı hap kafasına girmemek lazım. zaten bu ülkede ve büyük ihtimal tüm dünyada, insanları bu akımdan en fazla soğutan şey bu akımda olan diğer kişiler., offensive C.3 Turkish Hate Map C.3.1 Diﬀicult instances Polite hate/offensive instances heading: recep-tayyip-erdogan, text: zincirin son halkası. (bkz: iyi uykular türkiye), offensive heading: arap, text: sınır denen birşeyin olmadığını sanıp yol alanlar var nasıl geliyorlar anlamıyorum, hate Cultural reference including instances heading: arap, text: mevcut iktidar tarafından, ait olmadığımız için sinir olunan ırk, hate heading: bakalım muharrem, dhont erkan, seçim sonrası için meral, offensive rtenin-2023-secimiyle-ilgili-rahatliginin-sebebi, text: kukla kemal, gel"
        },
        {
            "title": "Appendix D Evaluation using SentiTurca",
            "content": "D.1 Turkish Hate Map We present some misclassified instances per model here. Predicted labels are given in bold. D.1.1 BERTurk True label: offensive fatih altaylının da dediği gibi kaçmıştır, yükü taşıyamamıştır. eski sunucu yeni çiftçi. offensive artık kendi ülkelerine geri dönme vakitleri gelen araplar hate kukla kemal, gel bakalım muharrem, dhont erkan, seçim sonrası için meral. neutral dinin siyasete alet edilmesinden daha iğrençtir laikliğin bu ülkede allahı inkar propagandasının aracı haline getirilmiş olması.. bu sebepledir bu ülke insanının din diye allah diye cemaat ve tarikatlara meyletmesi, rte gibi bir adama sarılması. işte bu yüzden sevilmez doğu perinçek gibiler de. ve işte bu yüzden doğu perinçek ile rte arasında bir fark yoktur. biri devrimcidir diğeri müslüman, ikisi de dünya çapında bir değişime öncülük ediyorlardır kendilerince. lakin gerçekte ikisi de ikiyüzlülüğün kitabını yazmışlardır. civilized True label: hate böyle üremeye devam ederlerse 2030 da sığınmacı biz türkler oluruz bu gidişata göre.. benim akp ye ve oy verenlere (birisi öz abim misal) kafayı takma sebebim bu. başka hiçbir şeyi dert etmeyebilirim,tolere edebilirim akp icratları ile alakalı ama bu konuyu tolere edemiyorum aq. savaş bitti.. akp nin mütahitleri suriye ye ev mi yapar, toki binaları mı inşa eder bilemiyorum ama bir an önce s@@tir olsun gitsinler ülkemizden! offensive yarın sabah itibarı ile hpnginin anasını sikip ülkelerine geri yoLLaMak lazım s@@tirip gidip ülkelerini savunsunlar, burada çöpün yanında yaşayip cayır cayır sikişip tavşan gibi doğuracaklarına s@@tirip gidip ülkeleri için canlarını versinler. halepmiş şammış elbabmış neresiyse geldikleri yer s@@tirip gitsinler. bu orospu cocuklarına şahsım adına verdiğim 1 kuruş bile vergi gidiyorsa verenlerin soyu kurusun, ölüleri mezarda ters dönsün, ciğerleri patlasın, kötürüm kalsın. hate almanyada olanları almanya ve belçikada yaşanan sel felaketinin bıraktığı enkazı temizleme çalışmalarına çatır çatır da almanca konuşuyorlar. türkiyedeyse 10 yıldır türkiyede yaşayıp hiç türkçe bilmeyen ve sel olsa gidip yağmalayacak tipler var ne yazık ki (elbette hpngi değil ama önemli bir bölümü). neutral 65 True label: neutral iş çıkışı bir dolmuş durdurdum, tam yaklaşırken de pişman oldum ulan bu dolmuşa nasıl binicez diye. sol elim komple alçının içinde, kırdık işte bir şekilde. duşta falan poşet sarıp yıkandık ama dolmuş sıkıntılı. her neyse artık çok geç, dolmuşçu gözlerimin içine bakmakta insan kalabalığı içindeki ufacık aralıktan. ulan bundan sonraki de dolu gelicek hem adam durdu artık.. diye düşünürken içerde buldum kendimi. bi tane demir tuttum ama dengem yerinde değil, hass@@tir nasıl vericez lan dolmuş parasını cüzdan sol cepte sağ elle de tutunup hayatta kalmaya çalışıyorum, oha ya hava bu kadar sıcak mı? acayip ter bastı lan. işte tam anda 2 tane türk kızı kalktı buyrun oturun zorlamayın kendinizi diye, birinin yerine oturdum mahçup bir şekilde bir de mırıldandım niye zahmet ettiniz diye. diğeri yerine tekrar oturdu. ayaktakine teşekkür edip paramı öne uzattıktan sonra buyrun oturun parayı yolladım gerisini hallederim dedim. oturun oturun sorun değil dedi ve işgüzar dolmuşçunun insanlık dışı doldurduğu dolmuşta ayakta yolculuğuna devam edip işgüzar dolmuşçunun isteği üzerine de polis kandırmaca niyetli biraz çömelebilir miyiz arzusunu da yerine getirdi. baya bir sevap kazanan tatlı türleri de vardır. not: ben yakışıklı değildim kız güzel değildi. offensive önce ülkedeki kaçakları sınırdışı edeniz. sonra türklere medeniyet dersi verirsiniz. hate gebze(artık çayırova) şekerpınarda bir sürüsüne rastladığı, bir sürüsünün çocuğunu okuttuğum, temiz, kibar, dürüst insanlar. neutral mango meselesi biz veganlar arasinda da uzun yillardir tartisiliyor. genel kani su sekilde; 1a) kurdu gorunce bir anlik korkuyla canina okuduysan gun veganligin dusuyor; ay sonundan itibaren 60 gun raw veganlik yapmak zorundasin. gun veganligin dustugu icin cekirge kizartmasi yiyebiliyorsun. 1b) hic farketmeden kurdun canina okuduysan ve sonra farkettiysen veganligin devam ediyor. ancak mangoyu şaman esliginde buyuk bir saygiyla ormanda gommen gerekiyor. 2) bu gibi durumlarda markaya ait mango suyunu sokaklara dokup visne suyu iciyoruz. civilized True label: civilized agnostik gorusun bir tik ilerisini savunan insandir kanimca. agnostiklere gore daha kibirli buluyorum acikcasi bu grubu. dinlerin sorgusuz sualsiz bir sekilde tanrinin varligini savunmasinda elestirdigim nokta kuskuya hicbir acik kapi birakmamasi. ama ateistler de bunun tam zittini yaparak tanri kesinlikle yoktur demekten geri kalmiyorlar. boyle bakinca agnostik gorus candir dostlar. temennim herkesin agnostik olmasi yonundedir. sonucta tanrinin varligini ispatlamak icin sunulan verilerin yetersizligi ortada. baya bir zorlaya zorlaya mucize falan ornekleri veriyolar ama bu cagda bunlarin yetersiz kaldigi acik. ama ayni zamanda tanrinin olmadigini ispatlamak icin de yeterli kanit yok. uc bes tane madem tanri var;... formatindaki sacma sapan soru benim acimdan tanrinin yoklugunu da kanitlamiyor. dolayisiyla, agnostik gorusun temelini olusturan tanrinin varligi ya da yoklugu bilinemez dusuncesi bana tam uyuyor. ama ateistler bu lafim da size: birakin bu atarli ergen ayaklarini. kucuk, orta veya buyuk olcekte bir dag yaratmisliginiz yok. kendinizi bir bok sanmayin. dunyayi degistiren 66 bilim adamlarinin ateist olmasini ornek gostererek de bu insanlardan nemalanmayi birakin. hadi bakalim; opuyorum hepinizi. offensive anlamamakta ısrarcı olanlar için şöyle kısa bi açıklama yapmak şart olmuş; şöyle ağız dolusu kadın diyemediğiniz için -bayandiyosunuz. çünkü kadın demek ayıp. tıpkı vajina diyememek gibi. ha bu arada hitap ederken kullandığınız bayana bişey demiyoruz. derdimiz erkekler tuvaleti diyebiliyoken kadınlar tuvaleti diyememenizle. bayanlar tuvaleti demenizle. kendi kendinize evde uygulayabileceğiniz test: kadın kelimesini de erkek kelimesini kullandığınız rahatlıkla kullanabiliyo musunuz? neutral insana uygulanan şiddeti, kadın diye bir alt sınıfa ayırmak gibi cinsiyetçi yaklaşımları bırakmaktır. gören de sanacak sadece kadına uygulanan şiddet cezasız kalıyor. erkeklerin birbirine uyguladıkları şiddet çok daha fazla ve gereken cezalar verilmiyor. ancak medyanın ve insanların kadın şiddeti üzerinden demagoji yapmak daha çok işlerine geldiği için işin bu boyutunu görmek istemiyorlar. bir kadın şiddet gördüğünde ana akım medyada hemen yer alıp erkekler yerden yere vururken erkek şiddet gördüğünde kimsenin umurunda dahi olmuyor. civilized D.1.2 Gemini 1.0 Pro and Claude 3.5 Sonnet True label: offensive suriyeli sağmalcılar olarak okunası başlık. ayrıca bazı ilçelere asıl sığınmaya çalışanlar bizim insanlarımız olmuş, oluyor, olmaya devam ediyor. offensive fatih altaylının da dediği gibi kaçmıştır, yükü taşıyamamıştır. eski sunucu yeni çiftçi. hate fiziksel güçten bahsetmiyorum, kafa biraz gidik oluyor bir mevkiye geldiyse güç zehirlenmesi sanırım:p neutral suratına birkaç saniyeden fazla baktığınızda hemen asabileşen, özellikle kızlardan pek bir nefret eden, korkulası topluluk. civilized True label: hate suriyeli sığınmacılar ülkesine girdi diye mutluluktan göbek atan insanların işleri elinden alınacak diye yine kemalistlere gezicilere giydirmesine sebebiyet vermiş sığınmacılardır. suriyeli sığınmacıların işinizi elinizden alacak olması sevgili boşbakanınızın icraatıdır, gelip de bize niye mutlu oluyorsunuz? lan diye ağlamayın... neutral by Gemini Pro and offensive by Claude True label: neutral laz olmak türk olmaya engel olmadığı için çelişki içermeyen durum. offensive neye göre güzel olacağımı kestirebilsem bir hak iddia edebileceğim durum olurdu. malum herkesin güzellik algısı çok farklı. hate 67 dünyada onca sahipsiz çocuk varken insan kendi çocuğunu doğurmak zorunda mı? doğurmak istemeyebilir. neutral hayvanların sırf zevk olsun diye kapalı kutu evlere hapsedilmesine şiddetle karşı bir insan olarak sünnilikle ilişkili midir bilemediğim sorunsaldır. civilized True label: civilized patrondan aldığınız maaşı net brüt diye ayırıp kafanızı karıştırıyorlar. sizin bir tane maaşınız var. sizin işverene filan yükünüz yok. işverenin senin bana maliyetin dediği şey aslında senin maaşın ve hakkın olan servis yemek vs. aslında senin maaşının tamamını cebine atıp sonra vergi dairesine gidip ayki vergini vermen gerekiyor. ama böyle yaparlarsa sen işe uyanırsın ve devlete hesap sormaya başlarsın. devlet senin gelirinden alacağı vergiyi direkt patronundan alıyor. sana kalan maaşı veriyor. sen de işe ayıkmıyorsın. ya da kafanda bilsen bile para eline geçmediği için ve sen olayı fiziki olarak görmediğin için ciddiyetini anlamıyorsun. devlet sizin maaşınızdan daha alır almaz vergi alıyor. sonra da ekstra ev/araba/su/elektrik/iletişim vs gibi şeyler için ekstra vergi alıyor. daha sonra da siz elinizde kalan parayı harcarken yine ekstra kdv/ötv gibi vergiler veriyorsunuz. bunların üzerine yollarda giderken yine geçtiğiniz yolun parasını veriyorsunuz. herhangi bir doğal afette yine pamuk eller cebe denerek para toplanıyor. futbol kulüplerinin ve büyük şirketlerin vergi borçları siliniyor. siz bir telefon/oyun konsolu/pc/araba alırken paramız yurtdışına akmasın döviz açığı olmasın diye devlete inanılmaz vergiler ödüyorsunuz ekstradan. ama futbol kulüpleri tek bir oyuncuya milyonlarca euro verebiliyor bu oyuncular 0 katkı sağlasa bile kimse hesap vermiyor üstelik. ayrıca vergi borçları da siliniyor. bütün bunlar bana inanılmaz geliyor. gerçekten bunlar yaşanıyor bu ülkede. offensive D.1.3 GPT-4 Turbo and LLaMa 3 70B True label: offensive koltuğunu bir an önce bırakması gereken kişidir.bırak başkası geçsin amk yapamıyon işte olmuyo devletçiğim bırak gitsin ölecen yakında koltukta offensive suriyeli sağmalcılar olarak okunası başlık. ayrıca bazı ilçelere asıl sığınmaya çalışanlar bizim insanlarımız olmuş, oluyor, olmaya devam ediyor. hate fatih altaylının da dediği gibi kaçmıştır, yükü taşıyamamıştır. eski sunucu yeni çiftçi. neutral fiziksel güçten bahsetmiyorum, kafa biraz gidik oluyor bir mevkiye geldiyse güç zehirlenmesi sanırım:p civilized by GPTTrue label: hate suriyeli sığınmacılar ülkesine girdi diye mutluluktan göbek atan insanların işleri elinden alınacak diye yine kemalistlere gezicilere giydirmesine sebebiyet vermiş sığınmacılardır. suriyeli sığınmacıların işinizi elinizden alacak olması sevgili boşbakanınızın 68 icraatıdır, gelip de bize niye mutlu oluyorsunuz? lan diye ağlamayın... neutral by GPT-4, hate by LLaMa 3 True label: neutral lütfü türkkanı acilen ihraç etmesi gereken parti. offensive zeki kadınlara tapılır. zeki kadın değerlidir. başlığı açan yazar yanılıyor. (bkz: sarhoş ellam) hate by GPT-4, correctly classified by LLaMa 3 numaranı versene, arıyım seni? + tabii. 0212 43... - (bağlantı kesildi) komple kabloyu sökerek kaçmış. neutral gerekli olandır. yönetime din karıştırılırsa ilerleme beklemek mümkün değildir; inanç konusu tamamen kişiye özelken yönetim hepimizi ilgilendirir, bu da başlıkla ilgili, 5 yaşındaki bir çocuğun anlayabileceği bir açıklamadır. civilized True label: civilized patrondan aldığınız maaşı net brüt diye ayırıp kafanızı karıştırıyorlar. sizin bir tane maaşınız var. sizin işverene filan yükünüz yok. işverenin senin bana maliyetin dediği şey aslında senin maaşın ve hakkın olan servis yemek vs. aslında senin maaşının tamamını cebine atıp sonra vergi dairesine gidip ayki vergini vermen gerekiyor. ama böyle yaparlarsa sen işe uyanırsın ve devlete hesap sormaya başlarsın. devlet senin gelirinden alacağı vergiyi direkt patronundan alıyor. sana kalan maaşı veriyor. sen de işe ayıkmıyorsın. ya da kafanda bilsen bile para eline geçmediği için ve sen olayı fiziki olarak görmediğin için ciddiyetini anlamıyorsun. devlet sizin maaşınızdan daha alır almaz vergi alıyor. sonra da ekstra ev/araba/su/elektrik/iletişim vs gibi şeyler için ekstra vergi alıyor. daha sonra da siz elinizde kalan parayı harcarken yine ekstra kdv/ötv gibi vergiler veriyorsunuz. bunların üzerine yollarda giderken yine geçtiğiniz yolun parasını veriyorsunuz. herhangi bir doğal afette yine pamuk eller cebe denerek para toplanıyor. futbol kulüplerinin ve büyük şirketlerin vergi borçları siliniyor. siz bir telefon/oyun konsolu/pc/araba alırken paramız yurtdışına akmasın döviz açığı olmasın diye devlete inanılmaz vergiler ödüyorsunuz ekstradan. ama futbol kulüpleri tek bir oyuncuya milyonlarca euro verebiliyor bu oyuncular 0 katkı sağlasa bile kimse hesap vermiyor üstelik. ayrıca vergi borçları da siliniyor. bütün bunlar bana inanılmaz geliyor. gerçekten bunlar yaşanıyor bu ülkede. hate by GPT-4, offensive by LLaMa 3 D.1.4 Qwen2-72B Misclassified neutral instances benim için ters olan, ama gel gelelim mıknatıs gibi çektiğim, hayır dediğimde de türlü türlü laflar yediğim olay tanım : bir çeşit namüsait durum. en büyük hatanız insanları genellemeniz. zeki olsalar adamları başlarında tutarlar mıydı? kimden bahsettim hadi bilin, sağ? sol? bilemeyecek kadar ince artık çizgi gezegenimizde en az otuz yıl geçirmiştir 69 tersi uygulamalar da mevcuttur. anadoluda sünniler alevilerden kız alırlar ama vermezler. gerekçesi: aleviler sünnileri yezid taraftarı olarak görürler. halbuki tüm sünniler emevi ve yezid fanı değildir. sünniler de alevileri islam dışı olarak görürler. halbuki islam dışı olmak için kitabı ve peygamberi inkar etmek gerekir. bu söylediğim genel profildir. bu şekilde düşünmeyenler de elbette vardır."
        },
        {
            "title": "References",
            "content": "[1] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: Burstein, J., Doran, C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186. Association for Computational Linguistics, Minneapolis, Minnesota (2019). https://doi.org/10. 18653/v1/N19-1423 . https://aclanthology.org/N19-1423 [2] Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., Gao, J.: Large Language Models: Survey (2024). https://arxiv.org/abs/2402. 06196 [3] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.: GLUE: multitask benchmark and analysis platform for natural language understanding. In: Linzen, T., Chrupała, G., Alishahi, A. (eds.) Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks For NLP, pp. 353355. Association for Computational Linguistics, Brussels, Belgium (2018). https://doi.org/10.18653/v1/W18-5446 . https://aclanthology.org/W185446 [4] Xu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., Lan, Z.: CLUE: Chinese language understanding evaluation benchmark. In: Scott, D., Bel, N., Zong, C. (eds.) Proceedings of the 28th International Conference on Computational Linguistics, pp. 47624772. International Committee on Computational Linguistics, Barcelona, Spain (Online) (2020). https://doi.org/10.18653/v1/2020. coling-main.419 . https://aclanthology.org/2020.coling-main.419 [5] Le, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux, B., Allauzen, A., Crabbé, B., Besacier, L., Schwab, D.: FlauBERT: Unsupervised Language Model Pre-training for French (2019) [6] Kurihara, K., Kawahara, D., Shibata, T.: JGLUE: Japanese general language understanding evaluation. In: Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., Piperidis, S. (eds.) Proceedings of 70 the Thirteenth Language Resources and Evaluation Conference, pp. 2957 2966. European Language Resources Association, Marseille, France (2022). https://aclanthology.org/2022.lrec-1.317 [7] Park, S., Moon, J., Kim, S., Cho, W.I., Han, J., Park, J., Song, C., Kim, J., Song, Y., Oh, T., Lee, J., Oh, J., Lyu, S., Jeong, Y., Lee, I., Seo, S., Lee, D., Kim, H., Lee, M., Jang, S., Do, S., Kim, S., Lim, K., Lee, J., Park, K., Shin, J., Kim, S., Park, L., Oh, A., Ha, J.-W., Cho, K.: KLUE: Korean Language Understanding Evaluation (2021). https://arxiv.org/abs/2105.09680 [8] Demirtas, E., Pechenizkiy, M.: Cross-lingual polarity detection with machine translation. In: Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining. WISDOM 13. Association for Computing Machinery, New York, NY, USA (2013). https://doi.org/10.1145/ 2502069.2502078 . https://doi.org/10.1145/2502069.2502078 [9] Toraman, C., Şahinuç, F., Yilmaz, E.H.: Large-scale hate speech detection with cross-domain transfer. In: Proceedings of the Language Resources and Evaluation Conference, pp. 22152225. European Language Resources Association, Marseille, France (2022). https://aclanthology.org/2022.lrec-1. [10] Beken Fikri, F., Oflazer, K., Yanikoglu, B.: Semantic similarity based evaluation for abstractive news summarization. In: Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pp. 24 33. Association for Computational Linguistics, Online (2021). https://doi.org/ 10.18653/v1/2021.gem-1.3 . https://aclanthology.org/2021.gem-1.3 [11] Schweter, S.: BERTurk - BERT Models for Turkish. https://doi.org/10.5281/ zenodo.3770924 . https://doi.org/10.5281/zenodo.3770924 [12] Ortiz Suárez, P.J., Sagot, B., Romary, L.: Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. In: Bański, P., Barbaresi, A., Biber, H., Breiteneder, E., Clematide, S., Kupietz, M., Lüngen, H., Iliadi, C. (eds.) Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pp. 916 (2019). https: //doi.org/10.14618/ids-pub-9021 [13] Ortiz Suárez, P.J., Romary, L., Sagot, B.: monolingual approach to contextualized word embeddings for mid-resource languages. In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J. (eds.) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 17031714. Association for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020. acl-main.156 . https://aclanthology.org/2020.acl-main.156 [14] Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., Raffel, C.: mT5: massively multilingual pre-trained text-to-text transformer. In: Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., 71 Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., Zhou, Y. (eds.) Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 483498. Association for Computational Linguistics, Online (2021). https://doi.org/10.18653/ v1/2021.naacl-main.41 . https://aclanthology.org/2021.naacl-main.41 [15] Reimers, N., Gurevych, I.: Making monolingual sentence embeddings multilingual using knowledge distillation. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, ??? (2020). https://arxiv.org/abs/2004.09813 [16] Arctic: Arctic Assistant. Snowflake AI Research (2024). https://www.snowflake. com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/ [17] Kakwani, D., Kunchukuttan, A., Golla, S., N.C., G., Bhattacharyya, A., Khapra, M.M., Kumar, P.: IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In: Cohn, T., He, Y., Liu, Y. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 49484961. Association for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.findings-emnlp.445 . https://aclanthology.org/2020.findings-emnlp.445 [18] Seelawi, H., Tuffaha, I., Gzawi, M., Farhan, W., Talafha, B., Badawi, R., Sober, Z., Al-Dweik, O., Freihat, A.A., Al-Natsheh, H.: Alue: Arabic language understanding evaluation. In: Proceedings of the Sixth Arabic Natural Language Processing Workshop, pp. 173184. Association for Computational Linguistics, Kyiv, Ukraine (Virtual) (2021). https://www.aclweb.org/anthology/2021.wanlp-1. [19] Chaves Rodrigues, R., Tanti, M., Agerri, R.: Natural Portuguese Language Benchmark (Napolab). https://doi.org/10.5281/zenodo.7781848 . https: //github.com/ruanchaves/napolab [20] Basile, V., Bioglio, L., Bosca, A., Bosco, C., Patti, V.: UINAUIL: unified benchmark for Italian natural language understanding. In: Bollegala, D., Huang, R., Ritter, A. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 348356. Association for Computational Linguistics, Toronto, Canada (2023). https://doi. org/10.18653/v1/2023.acl-demo.33 . https://aclanthology.org/2023.acl-demo.33/ [21] Pfister, J., Hotho, A.: SuperGLEBer: German language understanding evaluation benchmark. In: Duh, K., Gomez, H., Bethard, S. (eds.) Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 79047923. Association for Computational Linguistics, Mexico City, Mexico (2024). https://doi.org/10.18653/v1/2024.naacl-long.438 . https://aclanthology.org/2024.naacl-long.438/ 72 [22] Penagos, C.R., Armentano-Oller, C., Villegas, M., Melero, M., Gonzalez, A., Bonet, O.D.G., Carrino, C.P.: The catalan language CLUB. CoRR abs/2112.01894 (2021) 2112.01894 [23] Liang, Y., Duan, N., Gong, Y., Wu, N., Guo, F., Qi, W., Gong, M., Shou, L., Jiang, D., Cao, G., Fan, X., Zhang, R., Agrawal, R., Cui, E., Wei, S., Bharti, T., Qiao, Y., Chen, J.-H., Wu, W., Liu, S., Yang, F., Campos, D., Majumder, R., Zhou, M.: XGLUE: new benchmark dataset for cross-lingual pre-training, understanding and generation. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 60086018. Association for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.emnlp-main.484 . https://aclanthology.org/2020.emnlp-main. [24] Hu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., Johnson, M.: XTREME: massively multilingual multi-task benchmark for evaluating cross-lingual generalization. CoRR abs/2003.11080 (2020) 2003.11080 [25] Ruder, S., Constant, N., Botha, J.A., Siddhant, A., Firat, O., Fu, J., Liu, P., Hu, J., Neubig, G., Johnson, M.: XTREME-R: towards more challenging and nuanced multilingual evaluation. CoRR abs/2104.07412 (2021) 2104.07412 [26] Conneau, A., Rinott, R., Lample, G., Williams, A., Bowman, S., Schwenk, H., Stoyanov, V.: XNLI: Evaluating cross-lingual sentence representations. In: Riloff, E., Chiang, D., Hockenmaier, J., Tsujii, J. (eds.) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 24752485. Association for Computational Linguistics, Brussels, Belgium (2018). https://doi.org/10.18653/v1/D18-1269 . https://aclanthology.org/D18-1269/ [27] Williams, A., Nangia, N., Bowman, S.: broad-coverage challenge corpus for sentence understanding through inference. In: Walker, M., Ji, H., Stent, A. (eds.) Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 11121122. Association for Computational Linguistics, New Orleans, Louisiana (2018). https://doi.org/10.18653/v1/N18-1101 . https://aclanthology.org/N18-1101/ [28] Budur, E., Özçelik, R., Gungor, T., Potts, C.: Data and Representation for Turkish Natural Language Inference. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 82538267. Association for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.emnlp-main.662 . https://aclanthology.org/2020.emnlp-main.662/ [29] Warstadt, A., Singh, A., Bowman, S.R.: Neural network acceptability judgments. arXiv preprint arXiv:1805.12471 (2018) 73 [30] McKenna, N., Li, T., Cheng, L., Hosseini, M.J., Johnson, M., Steedman, M.: Sources of Hallucination by Large Language Models on Inference Tasks (2023). https://arxiv.org/abs/2305.14552 [31] Krippendorff, K.: Content Analysis: An Introduction to Its Methodology, 2nd edn. Sage Publications, Thousand Oaks, CA (2004) [32] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A., Potts, C.: Recursive deep models for semantic compositionality over sentiment treebank. In: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 16311642. Association for Computational Linguistics, Seattle, Washington, USA (2013). https://www.aclweb.org/anthology/D13- [33] Dolan, W.B., Brockett, C.: Automatically constructing corpus of sentential paraphrases. In: Proceedings of the Third International Workshop on Paraphrasing (IWP2005) (2005). https://aclanthology.org/I05-5002 [34] Altinok, D.: Bella turca: large-scale dataset of diverse text sources for turkish language modeling. In: Nöth, E., Horák, A., Sojka, P. (eds.) Text, Speech, and Dialogue, pp. 196213. Springer, Cham (2024) [35] Altinok, D.: diverse set of freely available linguistic resources for Turkish. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1373913750. Association for Computational Linguistics, Toronto, Canada (2023). https://doi.org/10.18653/v1/2023.acl-long.768 . https://aclanthology.org/2023.acl-long.768/ [36] Cer, D.M., Diab, M.T., Agirre, E., Lopez-Gazpio, I., Specia, L.: Semeval-2017 task 1: Semantic textual similarity - multilingual and cross-lingual focused evaluation. CoRR abs/1708.00055 (2017) 1708.00055 [37] Williams, A., Nangia, N., Bowman, S.R.: broad-coverage challenge corpus for sentence understanding through inference. CoRR abs/1704.05426 (2017) 1704.05426 [38] Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: Squad: 100, 000+ questions for machine comprehension of text. CoRR abs/1606.05250 (2016) 1606. [39] Dagan, I., Glickman, O., Magnini, B.: The pascal recognising textual entailment challenge. In: Quiñonero-Candela, J., Dagan, I., Magnini, B., dAlché-Buc, F. (eds.) Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pp. 177190. Springer, Berlin, Heidelberg (2006) [40] Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., Szpektor, I.: The second pascal recognising textual entailment challenge. (2006). 74 https://api.semanticscholar.org/CorpusID:13385138 [41] Giampiccolo, D., Magnini, B., Dagan, I., Dolan, B.: The third PASCAL recognizing textual entailment challenge. In: Sekine, S., Inui, K., Dagan, I., Dolan, B., Giampiccolo, D., Magnini, B. (eds.) Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 19. Association for Computational Linguistics, Prague (2007). https://aclanthology.org/W07-1401 [42] Bentivogli, L., Magnini, B., Dagan, I., Dang, H.T., Giampiccolo, In: D.: The fifth PASCAL recognizing Proceedings of the Second Text Analysis Conference, TAC 2009, Gaithersburg, Maryland, USA, November (2009). https://tac.nist.gov/publications/2009/additional.papers/RTE5_overview.proceedings.pdf 2009. NIST, entailment challenge. textual 16-17, ??? [43] Oflazer, K.: Two-level description of turkish morphology. Literary and Linguistic Computing 9(2), 137148 (1994) https://doi.org/10.1093/llc/9.2.137 [44] Honnibal, M., Montani, I., Van Landeghem, S., Boyd, A.: spaCy: Industrialstrength Natural Language Processing in Python (2020) https://doi.org/10.5281/ zenodo.1212303 [45] Anthropic: Claude AI Assistant. Accessed on October, 2024 (2024). https://www. anthropic.com [46] OpenAI: GPT-4 Turbo. Accessed on October, 2024 (2024). https://www.openai. com [47] Team, L.: The Llama 3 Herd of Models (2024). https://arxiv.org/abs/2407.21783 [48] Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Yang, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Liu, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Guo, Z., Fan, Z.: Qwen2 Technical Report (2024). https://arxiv.org/abs/2407.10671 [49] Reese, M.L., Smirnova, A.: Comparing chatgpt and humans on world knowledge and common-sense reasoning tasks: case study of the japanese winograd schema challenge. In: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems. CHI EA 24. Association for Computing Machinery, New York, NY, USA (2024). https://doi.org/10.1145/3613905.3650975 . https://doi.org/10.1145/3613905.3650975 [50] OpenAI: GPT-3.5. Accessed on October, 2024 (2024). https://www.openai.com 75 [51] Gözükara, F., Özel, S.A.: Türkçe ve İngilizce yorumların duygu analizinde doküman vektörü hesaplama yöntemleri için bir deneysel İnceleme. Çukurova Üniversitesi Mühendislik-Mimarlık Fakültesi Dergisi 31(2), 464482 (2016) https: //doi.org/10.21605/cukurovaummfd.310341 [52] Aydoğan, M., Kocaman, V.: Trsav1: new benchmark dataset for classifying user reviews on turkish e-commerce websites. Journal of Information Science 49(6), 17111725 (2023) https://doi.org/10.1177/01655515221074328 https://doi.org/10.1177/01655515221074328 [53] Toprak, B.M., Güney, S.: Classification of turkish e-commerce product reviews. In: 2023 46th International Conference on Telecommunications and Signal Processing (TSP), pp. 2932 (2023). https://doi.org/10.1109/TSP59544.2023. 10197752 [54] Mayda, .I., Demir, Y.E., Dalyan, T., Diri, B.: Hate speech dataset from turkish tweets. In: 2021 Innovations in Intelligent Systems and Applications Conference (ASYU), pp. 16 (2021). https://doi.org/10.1109/ASYU52992.2021.9599042 [55] Beyhan, F., Çarık, B., Arın, İ., Terzioğlu, A., Yanikoglu, B., Yeniterzi, R.: Turkish hate speech dataset and detection system. In: Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., Piperidis, S. (eds.) Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 41774185. European Language Resources Association, Marseille, France (2022). https://aclanthology.org/2022.lrec-1. [56] Çöltekin, Ç.: corpus of Turkish offensive language on social media. In: Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., Piperidis, S. (eds.) Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 61746184. European Language Resources Association, Marseille, France (2020). https://aclanthology.org/2020.lrec-1.758 [57] Konda Research and Consultancy: Konda Barometer: Social and Political Trends in Turkey. https://www.konda.com.tr/ [58] Erdoğan, E.: Dimensions of Polarization in Turkey. https://ipc.sabanciuniv.edu/ Content/Images/CKeditorImages/20200317-13105885.pdf [59] Göl, A.: The identity of turkey: Muslim and secular. Third World Quarterly 30(4), 795811 (2009) https://doi.org/10.1080/01436590902867381 [60] Cagaptay, S.: Islam, Secularism, and Nationalism in Modern Turkey: Who Is Turk? Routledge, New York, NY (2006) [61] Kaos GL: Annual Reports on LGBTQ+ Rights in Turkey. Reports on hate speech 76 and discrimination against LGBTQ+ individuals in Turkey. (various). https:// kaosgl.org/ [62] Haytap: At turkey-passes-massacre-law-aimed-at-street-dogs-2024-august Massacre Turkey (2024). Passes Street Dogs Aimed https://www.haytap.org/tr/ Law [63] Davidson, T., Warmsley, D., Macy, M., Weber, I.: Automated hate speech detection and the problem of offensive language. Proceedings of the 11th International AAAI Conference on Web and Social Media (ICWSM), 512515 (2017) [64] Olid, D., Serrano, J., Plank, B., Ponzetto, S.P., Rosso, P.: hierarchical approach to detect abusive language: Combining different features and classifiers. In: Proceedings of the 2018 International Conference on Computational Linguistics (COLING), pp. 622633 (2018). https://aclanthology.org/C18-1053/ [65] Mandl, T., Modha, S., Majumder, P., Patel, D., Dave, M., Mandlia, C., Patel, A.: Overview of the hasoc track at fire 2019: Hate speech and offensive content identification in indo-european languages. In: Proceedings of the 11th Forum for Information Retrieval Evaluation (FIRE), pp. 1417. ACM, ??? (2019). https: //doi.org/10.1145/3368567. [66] Mathew, B., Saha, P., Yimam, S.M., Biemann, C., Goyal, P., Mukherjee, A.: Hatexplain: benchmark dataset for explainable hate speech detection. Proceedings of the AAAI Conference on Artificial Intelligence 35(17), 1486714875 (2021) [67] Piot, P., Parapar, J.: Decoding Hate: Exploring Language Models Reactions to Hate Speech (2024). https://arxiv.org/abs/2410.00775 [68] Wang, B., Xu, C., Wang, S., Gan, Z., Cheng, Y., Gao, J., Awadallah, A.H., Li, B.: Adversarial GLUE: multi-task benchmark for robustness evaluation of language models. CoRR abs/2111.02840 (2021) 2111."
        }
    ],
    "affiliations": [
        "Independent Researcher, Berlin, Germany"
    ]
}