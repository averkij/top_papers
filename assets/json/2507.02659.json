{
    "paper_title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding",
    "authors": [
        "Ramchalam Kinattinkara Ramakrishnan",
        "Zhaocong Yuan",
        "Shaojie Zhuo",
        "Chen Feng",
        "Yicheng Lin",
        "Chenzheng Su",
        "Xiaopeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \\textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 9 5 6 2 0 . 7 0 5 2 : r OmniDraft: Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding Ramchalam Kinattinkara Ramakrishnan, 1 Zhaocong Yuan, 2, Shaojie Zhuo3, Chen Feng4, Yicheng Lin5, Chenzheng Su6, Xiaopeng Zhang7 Qualcomm AI Research {1rkinatti, 2zhaocong, 3shaojiez, 4chenf, 5yichengl, 6chenzhen, 7xiaopeng} @qti.qualcomm.com"
        },
        {
            "title": "Abstract",
            "content": "Speculative decoding generally dictates having small, efficient draft model that is either pretrained or distilled offline to particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, unified framework that enables single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the one drafter for all paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup."
        },
        {
            "title": "Introduction",
            "content": "Unlike traditional auto-regressive generation in LLMs, speculative decoding (SpD) [25, 9] offers unique advantage to accelerate LLM inference by decoupling the generation phase and verification phase. Speculative decoding generally requires small but efficient draft model and large target model. The draft model generates sequence of proposed tokens to be verified by the target in one shot, amortizing the target models memory bottleneck in batch inference and attaining better tokens per second throughput. The speedup factor relies not only on the predictability of the generated text like commonly occurring phrases, but also on the alignment between draft and target model. As such, common practice is to utilize draft and target models from the same model family given their consistency in pretrained data, tokenization and training configurations. Alternatively, one might consider distilling target model into smaller model to serve as the drafter [53, 30], which still follows the same principle of better alignment leading to greater speedup. The tight coupling of draft and target models limits flexibility of model selection and creates additional overhead for draft model distillation and maintenance, especially when deploying LLMs at scale Contributed equally Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Preprint. Under review. Figure 1: Overview of the OmniDraft framework: during cross-vocabulary speculative decoding, the drafter (Llama-68M) generates multiple tokens di with corresponding distributions qi. Crossvocabulary translator then converts the drafter tokens into tokens in the vocabulary of the target model (Llama3-8B). In this example, token d0(Snow) and d4(is) are directly mapped to target tokens t0 and t2, while token d1(f), d2(la) and d3(ke) are merged into single target token t1 (flake), since there is mapping item in the n-gram cache. The translated proposal ti along with combined probabilities is verified by the target model, resulting in t0 and t1 being accepted while t2 being rejected and replaced by 2. The target outputs tokens and their probabilities pi are translated into drafter tokens and sent back to drafter for next round of drafting. The n-gram cache is updated by inserting new unseen item (st,amps->stamps). Meanwhile, the accepted and corrected tokens from the target model are used to align the drafter through online cross-vocabulary distillation. across diverse hardware platforms and updating target models overtime. It is compelling to use universal lightweight model on-device to draft tokens for broad range of targets models. This universality greatly simplifies deployment, facilitates easy optimization, and allows for rapid model updates. Furthermore, the heterogeneity of on-device and cloud hardware presents an opportunity for hybrid speculative decoding. This enables users to choose between running any target model locally or on the cloud, balancing model performance, inference cost, and privacy concerns. However, building universal drafter for speculative decoding presents several unique challenges. Firstly, different family of target models might use tokenizers with different vocabularies. This is natural since target models are typically trained with massive pretrained data and hence larger vocabulary is needed to include higher-order n-grams or BPE [7] merges. As result, the vocabulary mismatch breaks the speculative decoding formulation where the draft and target model need to evaluate probabilities over the same set of tokens. previous work UAG [45] addresses vocabulary mismatch with an intermediate translation layer, but it primarily deals with tokens within the intersection of the drafter and target vocabularies, which might \"falsely\" reject good tokens from the drafter. Secondly, independent training of the drafter and target models can result in misaligned predicted token distributions, which reduces the acceptance rate during verification. This misalignment diminishes the efficiency benefits of speculative decoding. Although existing research [53, 30, 34] has explored techniques to improve alignment between the drafter and target, the alignment is often done offline with assumption of fixed target model. In practice, the target model may change due to user personalization or tasks switching, further complicating the alignment issue. Additionally, edge devices usually have limited memory, compute capacity, and power budget. The drafting process must therefore be efficient to maximize the benefits of speculative decoding. To address the challenges, we propose scalable speculative decoding framework, named OmniDraft, centered on an on-device universal drafter that generates draft tokens for wide variety of target models. An overview of OmniDraft is shown in Figure 1. To tackle the vocabulary mismatch issue and enable cross-vocabulary speculative decoding, we introduce an n-gram cache to store cross-vocabulary mappings from draft tokens to target tokens. By integrating the n-gram cache into the speculative decoding algorithm, we alleviate vocabulary mismatches and achieve higher acceptance rate for future queries. We further employ online knowledge distillation to improve alignment between the drafter and target. hybrid distillation loss, combining token-level and distribution level objectives, updates the drafter using the targets accepted and corrected outputs. This enables continuous alignment during speculative decoding, even when the target model changes due to personalization or task switching. To further improve runtime efficiency, we incorporate online adaptive drafting, where the drafter dynamically adjust the number of tokens it proposes 2 based on predicted confidence. The adaptive drafting balances generation cost and acceptation rate, maximizing throughput under device constraints. Overall, our OmniDraft framework enables robust, efficient and flexible speculative decoding system with universal drafter for on-device applications. Through extensive experiments, we show that single Llama-68M draft model can be paired with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for cross-vocabulary speculative decoding and provides up to 1.5-2x speedup on reasoning, coding and text generation tasks. Contributions (1) We propose cross-vocabulary speculative decoding via online n-gram cache that translates between drafter and target vocabularies, enabling speculative decoding across models with different tokenizers; (2) We introduce online knowledge distillation with hybrid alignment loss, which updates the drafter using accepted and corrected outputs from the target model to improve alignment and acceptance rate over time; (3) We integrate alignment training with adaptive drafting, allowing the drafter to dynamically adjust draft length based on alignment confidence for improved efficiency and speedup."
        },
        {
            "title": "2 Related work",
            "content": "Speculative decoding The idea of speculative decoding is proposed and formalized in the pioneer works of [25, 9]. Subsequent works have centered around optimizing different components of the speculative decoding framework. Some have highlighted tree attention to facilitate simultaneous verification of multiple draft sequences such as SpecInfer [32], Medusa [8] and Sequoia [11]. There is also focus on more efficient drafting by using retrieval-based methods as in Lookahead [16], REST [19], NEST [26], RASD [38], or by using dynamic length drafting as in BiLD [22], DISCO [31], AdaEDL [2], SpecDec++ [21], EAGLE2 [28]. More recent works explore speculative decoding in the context of vocabulary adaptation like UAG [45] and AdaptiVocab [35], long-context tasks like LongSpec [50], MagicDec [41], or more efficient draft models as in EAGLE [27], Speculative Streaming [5], and self speculative decoding as in [51], [14], [48]. Distillaton Model distillation in LLMs is crucial for speculative decoding since quality of the draft model dictates the final speed-up. Early works on sequence model distillation include [3, 23, 46]. There are also recent works that have specific focus on LLM distillation such as MiniLLM [18] and GKD [1]. Most closely aligned to our setting are the works of DistillSpec [53] and OSD [30] that aim towards training better draft model to given target. Another related line of works is distillation on different tokenizers as in [6, 33, 34]. Online adaptation Compared to model finetuning on fixed offline training set over multiple epochs, online adaptation focuses on continual learning or few-shot generalization to new data. Frameworks such as ProtoNet [42] and MAML [15] address the few-shot learning problem. Robotics and reinforcement learning also offer insights with works like DAGGER [40], RLˆ2 [13] and PEARL [39] for continuous adaptation to new tasks. There are also works that emphasize online adaptation for LLMs as in [20, 43, 29]. Lastly in speculative decoding, OSD [30] explicitly optimizes for draft model online adaptation which is closest to ours."
        },
        {
            "title": "3 Methodology",
            "content": "Notation Assume small draft model Mq and large target model Mp, let p(ytx, y<t) and q(ytx, y<t) be the distributions of next-token predictions at time step for Mp and Mq respectively, where represents the prompt prefix and x, y<t represents the context at time step t. For convenience, we use p(yt) and q(yt) as shorthands for p(ytx, y<t) and q(ytx, y<t) for the remaining sections. denotes the number of tokens proposed by the drafter Mq and p(yt+i) = norm(max(0, p(yt+i) q(yt+i))) is the residual distribution for the resampling, as per the original SpD algorithm [25, 9] [53, 30, 1] shows that better alignment between the draft and target model gives higher acceptance rate and hence higher speedup in speculative decoding. To align them, some common distillation losses include supervised finetuning (FT) or sequence level Knowledge Distillation (KD) LSF (θ) = 3 E(x,y)(X,Y )[ log qθ(yx)], and supervised KD LSD(θ) = E(x,y)(X,Y )[D(MpM θ selected choice of divergence metric D. )(yx)] with 3.1 Cross-vocabulary N-gram Cache Normal speculative decoding is infeasible when the draft vocabulary Vq and target vocabulary Vp are different, since the rejection scheme relies on acceptance ratios evaluated on the same token min(1, p(yt+i)/q(yt+i)) and the residual distribution norm(max(0, p(yt+i)q(yt+i))) also requires per-token probability differences. This foreshadows two issues: 1) tokens without direct mapping between the drafter vocabulary and target vocabulary cannot be handled i.e. the drafter can propose tokens not recognized by the target or vice-versa. 2) target can falsely reject good tokens from the drafter. This occurs when the drafter proposes sequence of (sub-)tokens that constitute merged token/n-gram in target, but target rejects the sequence since it prioritizes the merged token over the prefix sub-token. This is byproduct of the tokenization process that optimizes for the longest token in the vocabulary or sequentially applies merge rules to get the longest possible token ([47, 24, 7]). UAG [45] addresses the first mismatch with translation layer between the drafter and target and derives the intersection of draft and target vocabularies, where tokens have direct mappings. During proposal stage, UAG suppresses tokens outside of the intersection and converts drafter token ids to target ids with the mapping. After verification, if token without direct mapping is sampled, the translation layer will invoke the target and draft tokenzier to map the target token to sub-tokens in draft vocabulary. However, UAG cannot solve the second mismatch meaning it only guarantees feasibility of cross-vocabulary speculative decoding but lacks in optimality. To overcome this, we propose to build cache of n-grams that tracks the instances of target-draft token translations. Denote target tokens ti Vp and drafter tokens di Vq, then the n-grams cache is = {(ti, [di j]j=1:n)}, where each element represents mapped n-gram instance given the matching context so far ctxq, di = tokenizeq(detokenizep(ctxp, ti)). In inference time, we add postprocessing (pp) stage at the translation layer, where we scan over the proposed draft tokens dt, , dt+k1 and merge sub-tokens that hit the n-gram cache. The resulting new sequence tt, , tt+m and their draft probabilities q(tt), , q(tt+m) will be under the target vocabulary space and follow the mapping rule 2, , di 1, di ti, q(ti) = (cid:40) di, q(di) lookup([di j]j=1:n, C), (cid:81) q(di j) otherwise if direct mapping, ti di (1) This cross-vocabulary mapping translates the draft sequence to the target vocabulary space, ensuring speculative decoding still functions well with per-token acceptance ratios min(1, p(tt+i)/q(tt+i)). From the perspective of the final matched text, p(ti) and (cid:81) j) would be the probability of producing that specific chunk of text in their respective tokenization space. q(di However, in the correction stage we require the full distribution for the residual distribution instead of point-wise evaluation of probabilities, which is infeasible since p(), q() work on different space. Hence we enhance the mapping rule 1 as Vp, q(t) = (cid:81) q(di q(t) q(di j) 1) (cid:81) if n-gram mapped, [di j]j=1:n j) prefix sub-token of n-gram, = di q(di (2) otherwise We use the mapped probability for the selected n-gram but adjust the prefix sub-token probability by subtracting the n-gram probability. This can be seen as approximately re-allocating the original probability mass assigned to prefix sub-token di 1 under the draft distribution q(), between the new n-gram token and the prefix sub-token under the modified draft distribution q(). This is also related to the known problem of tokenization bias [37]. Using mapping 2, we at least ensure point-wise, approximate correctness of the n-gram token for the residual distribution norm(max(0, p(tt+i) q(tt+i))). Note that we can apply 2 to the n-grams sampled and matched from the current speculative round. Evaluating all other n-grams require re-running drafter at the step of rejection which is impractical. We summarize the modified speculative decoding with our proposed mappings in Algorithm 1. 4 Algorithm 1 Cross-vocabulary Speculative Decoding for = 1 : do 1: Given draft model q(), target model p(), n-gram cache 2: Given draft length k, max length , prompt 3: Initialize 0, ctxq, ctxp 4: while < do 5: 6: 7: 8: 9: 10: 11: 12: tt, , tt+m1, q(tt), , q(tt+m1) In parallel, compute target probabilities on mapped tokens Sample draft auto-regressively dt+i q(dt+ictxq, d<t+i) p(tt), , p(tt+m) end for Apply translation mapping 12 to get proposed tokens and draft probabilities in target space Apply rejection sampling with acceptance ratios min(1, p(tt+i)/q(tt+i)) and correction residual distributions norm(max(0, p(tt+i) q(tt+i))) to get accepted tokens tt, , tt+n1 for some accepted length if == then 13: 14: 15: 16: 17: 18: 19: 20: 21: end while 22: Return results Sample free token tt+m p(tt+m) and add to accepted tokens, + 1 end if Apply reverse translation to get accepted tokens in draft space ctxq, dt, , dt+p1 = tokenizeq(detokenizep(ctxp, tt, , tt+n1)) Add to n-gram cache if there exists unseen n-gram instance + n, update ctxq, ctxp 3.2 Cross-vocabulary Distillation Using the n-gram cache with the approximate distribution mapping helps to draft and verify n-gram tokens as if operating under the target vocabulary directly. To extend it for online adaptation, we propose hybrid distillation framework that progressively aligns the draft and target model on both direct mapping tokens and n-gram tokens. Given the online setting, we have limited access to the target model so we distill on the draft model generated data, or simply on-policy data similar to GKD [1]. We employ reverse KL on direct mapping tokens for richer supervision signals, but use maximum log-likelihood (NLL) on n-gram tokens since we only have reliable point-wise evaluation of probabilities on those tokens. Overall, our proposed hybrid distillation loss is Lcross_vocab_distill(θ) = LDM(θ) + λLN-gram(θ) = xX,diq(), ti,qmapping(di,q) (cid:20) DKL(q θp)(tix)IDM(di) λ log qθ(dix)IN-gram(di) (cid:21) (3) (4) where IDM, IN-gram are the indicator functions to identify if current token is part of direct mapping or n-gram; this is implemented as binary masks in practice. Note that the KL loss term corresponds to direct mapping token ti di, and divergence is computed in the target vocabulary space given q() is elevated to q() via the translation mapping. The NLL loss term however operates in drafter vocabulary space to increase likelihoods of drafter tokens which constitute an n-gram accepted by the target during inference. The parameter λ can either be hyperparameter to account for ratio imbalance between direct mapping tokens and n-gram tokens, or can be dynamic weight such as the verified target probability of the n-gram. The latter leads to loss term of λLN-gram(θ) = p(ti) log qθ(dix), which can be treated as the point-wise KL evaluated on the n-gram token. Moreover, it is possible to extend the NLL or point-wise KL loss to an approximate KL loss using mapping 2 as LN-gram = DKL(q θ(tix)p(tix)). This is equivalent to using KL on the intersection tokens plus the n-gram and its first sub-token. Due to the additional components on the intersection tokens and the fist sub-token, this approximate KL loss can provide richer learning signal. Empirically we only observed minimal improvement from the approximate KL loss, as shown in section 4.3.2. 5 3.3 Online Adaptive Drafting One observation in performing cross-vocabulary speculative decoding is that we are implicitly shortening the proposal draft length, since multiple sub-tokens map to single n-gram token. We then explicitly incorporate adaptive drafting to gain even better speedup for on-device speculative decoding focusing on the same vocab setting. We adopt the framework in SpecDec++ [21] where lightweight head network fϕ() predicts the acceptance rate of the current proposed token. The acceptance prediction head takes the embedding of the proposed token ei as input, and is trained using weighted BCE loss Ladapt with acceptance ratios min(1, p(yi)/q(yi)) as labels. It then controls if to early exist based on the cumulative probability of at least one proposed token getting rejected and given stopping threshold γ. (yi acceptedy<i accepted) = sigmoid(fϕ(ei)) (1 k, s.t. yi rejected) = 1 (cid:89) i=1 (yi acceptedy<i accepted) (1 k, s.t. yi rejected) > γ exit (5) (6) (7) However in online adaptation, the labels are subject to change since the draft model is continuously finetuned with distillation loss Ldistill to align with the target, which could cause distribution shift for dynamic drafting. We propose two variants of online adaptive drafting. The first one performs draft model alignment and acceptance prediction head training jointly at each update step Ljoint = Ldistill + Ladapt. The second one interleaves two trainings such that we perform multiple acceptance prediction updates per draft model alignment update, aiming to mimic slowly moving labels to reduce distribution shift. Unlike the first variant that performs both updates jointly using the online data batch, for the second variant we keep larger buffer for the acceptance prediction updates which includes data from previous batches. This helps to enhance training stability and adaptation speed for the acceptance prediction."
        },
        {
            "title": "4 Results",
            "content": "Models To show the efficacy of OmniDraft on the setting of single drafter for multiple targets, we fix the drafter to be Llama-68M [32] and the target model to be Llama3-8B [17], Qwen2-7B [49] for the cross vocabulary results, as well as Vicuna-7B [52] for the same family vocabulary evaluation. Tasks We perform online distillation across 4 tasks: GSM8K [12], Alpaca [44], XSum [36]and combined MBPP+HumanEval [4][10] datasets. Each task has dedicated train and test set or we slice out the portion of the train set as the test set. For the MBPP+HumanEval, we combine the two datasets to add some more diversity to the data for the coding tasks. The training is conducted for specific number of steps (<1 epoch) across each of the tasks as per general online adaptation setting. All the tokens will contribute to the loss calculation including the tokens that were accepted by the target as they would provide option for improved alignment between the two models. Moreover, all experiments are performed with temperature 0.01, unless specified otherwise. We also include the setting for online adaptation of the drafter using LoRA across all the tasks. Using dynamic adapter switching we can pair the same drafter with any target across any of the task which is the ideal scenario for on-device online speculative decoding. Evaluation Metrics Speedup - Walltime acceleration rate, measures the improvement in tokens-per-second throughput. We follow Medusas [8] convention. Acceptance Rate - Ratio of accepted tokens to proposed tokens averaged over speculative decoding steps, measures alignment between draft and target model. Notation We refer to SpDDM as the baseline speculative decoding which uses direct mapping between vocabularies. N-gram postprocessing (pp) refers to using the N-gram cache as postprocessing technique without directly training on it as we proposed in Algorithm 1. N-gram hit refers to the average number of successful N-gram cache lookups that were accepted by the target per speculative decoding step. 6 4.1 Cross-vocabulary Online Distillation Table 1 shows the results on the test set after training. Across all the tasks, LDM + LN-gram approach perform better than training only on LDM. Overall, this indicates that the additional LN-gram plays significant role in improving the acceptance rate. Moreover, LDM + LN-gram with LoRA finetuning performs reasonably well across all of the tasks when compared to the baseline. The largest speedup is obtained for the GSM8K dataset for both the target models, with XSum being the least improved task. We observe XSum could also be improved by increasing the number of training samples as scaling effect. Figure 2 portrays the training dynamics across all tasks for both the acceptance rate and speedup metrics. For all the experiments, we can see the metrics improve as training proceeds. The instability seen in some of the LoRA curves could indicate that training hyper-parameters are not fully optimized or that training plateaus quicker for certain tasks, which could also explain why there is still small gap between the LoRA and the full finetuned model performance. Table 1: Performance on Cross-vocabulary Distillation with Llama-68M and two different targets XSum MBPP+HumanEval GSM8K Alpaca Target Method Acc Rate Speedup Acc Rate Speedup Acc Rate Speedup Acc Rate Speedup Llama3-8B Qwen2-7B SpDDM LDM LDM +λLN-gram LDM +λLN-gram + LoRA SpDDM LDM LDM +λLN-gram LDM +λLN-gram + LoRA 0.10 0.32 0.42 0.37 0.14 0.33 0.37 0.31 0.94x 1.58x 1.70x 1.59x 1.04x 1.50x 1.61x 1.41x 0.09 0.22 0.27 0.19 0.09 0.22 0.26 0.21 1.03x 1.26x 1.33x 1.28x 0.91x 1.29x 1.36x 1.21x 0.09 0.16 0.20 0.17 0.13 0.17 0.20 0. 0.96x 1.25x 1.30x 1.21x 1.01x 1.25x 1.30x 1.25x 0.11 0.20 0.24 0.23 0.12 0.19 0.22 0.22 0.91x 1.20x 1.24x 1.21x 0.96x 1.16x 1.22x 1.21x Figure 2: Cross-vocabulary SpD online distillation on Llama-68M with Qwen2-7B as target 4.2 Online Adaptive Drafting Table 2 shows the test results of online adaptive drafting with ablations to the different training variants. Across all tasks we observe consistent increase in acceptance rate. This is the combined effect of better model alignment due to distillation, and also the acceptance prediction head learning to exit early when there is higher chance for proposal rejection In terms of speedup, we can also see improvement in most tasks except on GSM8K where the distill-only baseline outperforms both adaptive drafting variants. This could be due to the latency reduction over the number of the proposed tokens are not sufficient in comparison to the increase in successful accepted tokens with longer draft length. We hypothesize it could be due the difficulty in training the acceptance prediction head in an online setting. This involves optimizing with respect to 7 changing labels and training only on incoming data without stable corrective feedback compared to offline training. By end of the one epoch training, the acceptance prediction head might not have converged well to align with the current distilled drafter, leading to sub-optimal performance. Finally, we also observe the interleaved variant has better speedup than the joint variant on average (similar in Alpaca and Xsum, larger increment in GSM8K and MBPP+HumanEval). But the joint variant has higher acceptance rates over all tasks, indicating it might be underestimating the acceptance probability, leading to wrongful early exit. This could be direct consequence of the previously mentioned training challenges, which is alleviated with the interleaved variant with more stable training. Table 2: Performance on Online Adaptive Drafting with Llama-68M and Vicuna-7B. Target Method GSM8K MBPP+HumanEval Alpaca XSum Acc Rate Speedup Acc Rate Speedup Acc Rate Speedup Acc Rate Speedup Vicuna-7B SpD Adapt Only Distill Only Joint Distill + Adapt Interleaved Distill + Adapt 0.21 0.38 0.42 0.61 0.52 1.44x 1.50x 2.20x 2.08x 2.15x 0.14 0.28 0.35 0.51 0. 1.22x 1.34x 1.92x 1.91x 1.94x 0.20 0.38 0.25 0.44 0.41 1.44x 1.52x 1.57x 1.61x 1.60x 0.20 0.38 0.23 0.42 0.38 1.42x 1.51x 1.53x 1.59x 1.58x 4.3 Ablations Studies 4.3.1 Effectiveness of N-gram Cache In this section, we focus on the impact of the N-gram cache to different training variants as per Table 3. We perform ablation on subset of GSM8K dataset (4k samples) across multiple techniques. It can be seen that the baseline SpDDM performs poorly which indicates the mismatch between the drafter and the target model alignment for the pretrained models. We also capture all the possible N-gram matches during the SpDDM baseline for the train set, and then use the cache as lookup for an improved baseline in the SpDDM + N-grampp. As such there is cache hit of 0.87, which indicates that the baseline model without any pretraining can still benefit with the N-gram cache. We also train the model to improve the alignment using LDM and although without the N-gram, we can still see large improvement over the baseline. Moreover. when we train with N-grampp, there is further improvement on the overall speedup. Finally, we train using both LDM + λLN-gram, which provides the best results across all techniques on different draft length k. Table 3: Effect of n-grams cache with Llama-68M and Llama3-8B on GSM8K (subset) Metrics SpDDM SpDDM + N-grampp LDM LDM + N-grampp LDM + λLN-gram Acc Rate Speedup Avg n-gram hit Acc Rate Speedup Avg n-gram hit 0.16 1.04x 0 0.12 1.01x 0 = = 4 0.40 1.59x 0 0.32 1.51x 0 0.20 1.16x 0.87 0.16 1.11x 1.49 0.42 1.61x 0. 0.35 1.54x 0.75 0.46 1.66x 2.40 0.41 1.61x 3.66 4.3.2 Distillation Loss Comparisons The different loss variants also provide different levels of performance on the test set as shown in Table 4. When trained only on the LN-gram, the training is very unstable. This could either be due to the number of n-grams being substantially smaller than the direct mapping tokens within most datasets, or the training requires additional constraints to direct towards minima. Consequently, training on the combined loss provides better performance metrics across temperatures as well. The final LDM KL + λLN-gram KL, 2, provides slightly better results on temperature = 1 indicating the impact of the additional KL over the intersection vocabulary which is beneficial for the sampling process. However, we noticed tuning the scaling factor λ becomes critical and hence we use Equation 1 for all of the experiments since it was much more stable across all tasks. We fix λ = 0.2 for all the tasks, across all experiments. Table 4: Training Loss comparisons with Llama-68M and Llama3-8B on GSM8K (subset) Metrics LN-gram NLL LDM NLL + LN-gram NLL LDM KL +LN-gram NLL LDM KL + LN-gram KL Acc Rate Speedup Acc Rate Speedup 0.090 0.86x 0.070 0.79x Temperature = 0.01 0.368 1.51x 0.271 1.23x Temperature = 0.375 1.57x 0.273 1.27x 0.376 1.57x 0.275 1.31x 4.3.3 Adaptive Drafter Initialization and Thresholds Two additional aspects in training and using adaptive drafting are the acceptance prediction head initialization and the stopping threshold for early exit. Before the joint training of model distillation and adaptive drafting, we can pretrain the acceptance prediction head on an offline dataset, while keeping the drafter fixed. This should ideally capture some priors of the draft-target alignment and provide better initialization for the online joint training. To verify this, we pretrain the acceptance prediction head on the Alpaca dataset for 1 and 3 epochs respectively, and use the two checkpoints as initialization for online training. We observe mixed results where the pretrained initializations harm performance on GSM8K and XSum, and only improve on MBPP+HumanEval by small margins. We suspect MBPP+HumanEval has closer affinity in data distribution to Alpaca while the other two do not, which leads to negative transfer with pretraining. The stopping threshold γ in adaptive drafting also plays key role for speedup performance. In Alpaca and XSum we observe conservative threshold of γ = 0.3 is sufficient to attain better speedup than baselines, while in GSM8K and MBPP+HumanEval we need more relaxed threshold of γ = 0.7 to achieve comparable speedup. We also observe applying relaxed threshold on model trained on stricter threshold improves speedup performance, indicating adaptive drafting could be prone to underestimating the maximum acceptance draft length. It also suggests the actual stopping threshold to be used for inference should be chosen based on the task and then adjusted based on online performance. 4.4 Limitations While most of our results indicate the potential of our methodology and the OmniDraft framework, some potential limitations still require additional research. 1) Although we are training on incoming data for online adaptation of the drafter, since it is limited to single iteration of the data stream, there is still potential for instability on new unseen data. 2) We currently use the full n-gram cache per task per target model, however, should memory become bottleneck, it would require optimized cache eviction policy to cater to edge devices. 3) Special tokens that do not have direct mapping currently would require some additional effort to handle. Consequently, this would make it less seamless to integrate multi-modal tasks. As part of our future plan of action, although cross-vocabulary speculative decoding already implicitly includes the adaptive proposal length due to n-gram merge, we are working towards incorporating an explicit adaptive head in the cross-vocabulary setting."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work we propose the OmniDraft framework that leverages n-gram cache and hybrid distillation loss to enable cross-vocabulary speculative decoding. We show how the draft model can be aligned to different target models with different vocabulary space via online adaptation, and we further showcase online adaptive drafting to get additional speedup. Our empirical results also show good performance across all metrics. Overall, OmniDraft shows great potential and could pave way to new on-device LLM applications."
        },
        {
            "title": "References",
            "content": "[1] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations, 2024. [2] Sudhanshu Agrawal, Wonseok Jeon, and Mingu Lee. Adaedl: Early draft stopping for speculative decoding of large language models via an entropy-based lower bound on token acceptance probability. In NeurIPS Efficient Natural Language and Speech Processing Workshop, pages 355369. PMLR, 2024. [3] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: In Findings of the An imitation learning perspective of error accumulation in language generation. Association for Computational Linguistics: ACL 2022, pages 700710, 2022. [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and Mahyar Najibi. Speculative streaming: Fast llm inference without auxiliary models. In NeurIPS Efficient Natural Language and Speech Processing Workshop, pages 395413. PMLR, 2024. [6] Nicolas Boizard, Kevin El Haddad, Céline Hudelot, and Pierre Colombo. Towards cross-tokenizer distillation: the universal logit distillation loss for llms. arXiv preprint arXiv:2402.12030, 2024. [7] Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining. arXiv preprint arXiv:2004.03720, 2020. [8] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. In International Conference on Machine Learning, pages 52095235. PMLR, 2024. [9] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. [10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. [11] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, robust, and hardware-aware speculative decoding. CoRR, 2024. [12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [13] Yan Duan, John Schulman, Xi Chen, Peter Bartlett, Ilya Sutskever, and Pieter Abbeel. Rlˆ 2: Fast reinforcement learning via slow reinforcement learning. 2016. [14] Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et al. Layerskip: Enabling early exit inference and self-speculative decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1262212642, 2024. [15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 11261135. PMLR, 2017. [16] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. In International Conference on Machine Learning, pages 1406014079. PMLR, 2024. 10 [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations. [19] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and Di He. Rest: Retrieval-based speculative decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 15821595, 2024. [20] Nathan Hu, Eric Mitchell, Christopher Manning, and Chelsea Finn. Meta-learning online adaptation of language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 44184432, 2023. [21] Kaixuan Huang, Xudong Guo, and Mengdi Wang. Specdec++: Boosting speculative decoding via adaptive candidate lengths. In Workshop on Efficient Systems for Foundation Models II@ ICML2024. [22] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 3923639256, 2023. [23] Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 conference on empirical methods in natural language processing, pages 13171327, 2016. [24] Taku Kudo and John Richardson. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018. [25] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [26] Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. Nearest neighbor speculative decoding for llm generation and attribution. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [27] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language models with dynamic draft trees. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 74217432, 2024. [28] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: speculative sampling requires rethinking feature uncertainty. In Proceedings of the 41st International Conference on Machine Learning, pages 2893528948, 2024. [29] Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang. Online training of large language models: Learn while chatting. arXiv preprint arXiv:2403.04790, 2024. [30] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. Online speculative decoding. In Forty-first International Conference on Machine Learning. [31] Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, and Roy Schwartz. Dynamic speculation lookahead accelerates speculative decoding of large language models. In NeurIPS Efficient Natural Language and Speech Processing Workshop, pages 456467. PMLR, 2024. [32] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification. arXiv preprint arXiv:2305.09781, 2023. [33] Benjamin Minixhofer, Edoardo Maria Ponti, and Ivan Vulic. Zero-shot tokenizer transfer. arXiv preprint arXiv:2405.07883, 2024. [34] Benjamin Minixhofer, Edoardo Maria Ponti, and Ivan Vulic. Cross-tokenizer distillation via approximate likelihood matching. arXiv preprint arXiv:2503.20083, 2025. [35] Itay Nakash, Nitay Calderon, Eyal Ben David, Elad Hoffer, and Roi Reichart. Adaptivocab: Enhancing llm efficiency in focused domains through lightweight vocabulary adaptation. arXiv preprint arXiv:2503.19693, 2025. 11 [36] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. ArXiv, abs/1808.08745, 2018. [37] Buu Phan, Brandon Amos, Itai Gat, Marton Havasi, Matthew Muckley, and Karen Ullrich. Exact bytelevel probabilities from tokenized language models for fim-tasks and model ensembles. arXiv preprint arXiv:2410.09303, 2024. [38] Guofeng Quan, Wenfeng Feng, Chuzhan Hao, Guochao Jiang, Yuewei Zhang, and Hao Wang. Rasd: Retrieval-augmented speculative decoding. arXiv preprint arXiv:2503.03434, 2025. [39] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on machine learning, pages 53315340. PMLR, 2019. [40] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings, 2011. [41] Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, and Beidi Chen. Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding. arXiv preprint arXiv:2408.11049, 2024. [42] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. [43] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard Schwarz. Online adaptation of language models with memory of amortized contexts. arXiv preprint arXiv:2403.04317, 2024. [44] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023. [45] Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Gaurav Jain, Roy Schwartz, Moshe Wasserblat, and David Harel. Accelerating llm inference with lossless speculative decoding algorithms for heterogeneous vocabularies. 2025. [46] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level knowledge distillation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1081710834, 2023. [47] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [48] Heming Xia, Yongqi Li, Jun Zhang, Cunxiao Du, and Wenjie Li. Swift: On-the-fly self-speculative decoding for llm inference acceleration. arXiv preprint arXiv:2410.06916, 2024. [49] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [50] Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, and Bo An. Longspec: Long-context speculative decoding with efficient drafting and verification. arXiv preprint arXiv:2502.17421, 2025. [51] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft& verify: Lossless large language model acceleration via self-speculative decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1126311282, 2024. [52] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [53] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. In The Twelfth International Conference on Learning Representations."
        },
        {
            "title": "A Implementation Details",
            "content": "This appendix provides detailed information about the implementation and training setup used in our experiments. A.1 Model and Hardware Details Throughout our work, we use the environment setup with NVIDIA A100 GPU (40/80GB), PyTorch 2.1.0 framework, CUDA version 12.1, and Ubuntu 22.04 LTS. Our models used include Llama-68M [32], Llama3-8B [17], Qwen2-7B [49] and Vicuna-7B [52]. We train the Llama-68M model in our experiments. Their model details are listed in the following. Table 5: Model hyperparameters Hyperparameter model layers hidden size attention heads activation function Value Llama-68M 2 768 12 SiLU Table 6: Model latency and vocabulary statistics Model Llama-68M Qwen2-7B Llama3-8B Vicuna-7B Wall-time per Step (s) Vocabulary Size 0.00203 0.02667 0.02846 0.02683 32000 152064 128256 32000 Intersection with Llama-68M 32000 22275 22416 32000 A.2 Training Details Our experiments use GSM8K [12], Alpaca [44], XSum [36] and combined MBPP+HumanEval [4][10] datasets. We also show the major hyperparameters used across the experiments. For evaluation, we perform three runs with different seeds and report the average in the main results sections. Table 7: Dataset details Dataset GSM8K MBPP+HumanEval Alpaca XSum 4K/8K 100 train test 8K 200 8K 100 1K Table 8: Training hyperparameters Hyperparameter batch size learning rate (LR) LR scheduler optimizer β1 β2 weight decay epochs mixed precision LoRA rank temperature Value 8 1e-4/2e-5 constant AdamW 0.9 0.999 0.01/0 1 (online) FP16 32 0."
        },
        {
            "title": "B Additional Experiment Details",
            "content": "B.1 Speedup Metric We follow the convention in Medusa [8] to define the speedup metric. Given acceleration rate as the average number of tokens decoded per decoding step and overhead as the average per step latency of the proposed model divided by that of the vanilla model, speedup refers to the wall-time acceleration rate and can be computed with speedup = acceleration rate/overhead. B.2 Adaptive Drafting Training We show the training curves for adaptive drafting across all tasks. It can be seen that our proposed variants of joint or interleaved align + adapt training have the best performances in both average acceptance rate ans speedup. Figure 3: Online Adaptive Drafting training plot of Llama-68M vs Vicuna-7B B.3 Adaptive Drafting LoRA Training To showcase the idea of one drafter for all, we further show results of online adaptive drafting with LoRA used in distillation in Table 9. We observe slightly lower performances across tasks compared to distillation with full fine-tuning as expected. But each of the distillation or distillation with adaptive drafting baseline with LoRA still outperforms normal speculative decoding. And distillation with adaptive drafting outperforms the distillation only baseline, except on GSM8K where the gap is minimal. By demonstrating the compatibility of LoRA with our proposed online distillation and adaptive drafting training, we empower single draft model to serve as the backbone and selectively fine-tune LoRA modules and acceptance prediction heads for any potential target model. This would strike good balance to keep minimal memory overhead while retaining the benefits of greater speedup and flexibility, which are ideal for on-device applications. Table 9: Performance on Online Adaptive Drafting using LoRA with rank 32 with Llama-68M and Vicuna-7B. Target Method GSM8K MBPP+HumanEval Alpaca XSum Acc Rate Speedup Acc Rate Speedup Acc Rate Speedup Acc Rate Speedup Vicuna-7B SpD LoRA Distill Only Joint LoRA Distill + Adapt Interleaved LoRA Distill + Adapt 0.21 0.37 0.49 0.49 1.44x 1.95x 1.87x 1.94x 0.14 0.24 0.39 0.38 1.22x 1.52x 1.59x 1.61x 0.20 0.25 0.39 0.42 1.44x 1.54x 1.58x 1.58x 0.20 0.23 0.41 0.39 1.42x 1.49x 1.54x 1.55x 14 B.4 Cross-vocabulary SpD online distillation on Llama-68M with Llama3-8B as target Figure 4 showcases the training dynamics of Llama-68M with Llama3-8B as the target model. Similar to the previous Figure 2, the pattern is very much matched. Overall, across all the tasks, our methods are able to improve upon the SpD baseline significantly as training progresses. Figure 4: Cross-vocabulary SpD online distillation on Llama-68M with Llama3-8B as target B.5 Rank ablation for Cross-vocabulary online distillation of LoRA We also ablate on the rank for the LoRA experiments, specifically focussed on the LDM +λLN-gram + LoRA method. As shown in the Table 10, it is clear that as the rank increases, performance also improves, but beyond rank 32, there is diminishing return in terms of overall improvement. As such we choose conservative rank of 32 across all our experiments, tasks and methodologies. Moreover, for the one drafter for all setting, folding of the LoRA weights is infeasible and as such having lower rank would benefit the overall memory and compute required for the drafter model on-device. Table 10: Effect of rank for LDM +λLN-gram + LoRA for Llama-68M and Llama3-8B on GSM8K Metrics LoRA Rank 8 16 32 128 Acc Rate Speedup 0.30 1.478x 0.34 1.543x 0.37 1.595x 0.38 1.625x 0.38 1.632x"
        },
        {
            "title": "C Additional Algorithm Details",
            "content": "We show the detailed algorithm for cross-vocabulary distillation in Algorithm 2. The online distillation training leverages our proposed cross-vocabulary speculative decoding in inference or data collection, and uses the hybrid distillation loss to update the draft model. 15 Algorithm 2 Cross-vocabulary Distillation S, + 1 Get response with cross-vocabulary speculative decoding as in Algorithm 1 Append (x, y) to if mod == 0 then 1: Given target p(), drafter qθ(), online data stream S, data buffer Q, update interval I. 2: i, 0, {} 3: while True do 4: 5: 6: 7: 8: 9: 10: 11: end while Update qθ on with hybrid distillation loss Lcross_vocab_distill(θ) {} end if We show the detailed algorithms for the two variants of online adaptive drafting training in Algorithm 3 and 4. The distillation loss Ldistill(θ) uses KL divergence on generated response tokens. The adaptive drafting loss Ladapt(ϕ) is weighted BCE loss between the acceptance prediction head outputs and acceptance ratio labels constructed from the target model and current draft model qθ. Algorithm 3 Online Adaptive Drafting Joint Training 1: Given target p(), drafter qθ(), acceptance prediction head fϕ(), online data stream S, data buffer Q, update interval I. 2: i, 0, {} 3: while True do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end while end if S, + 1 Get response with speculative decoding using adaptive drafting Push (x, y) to if mod == 0 then Update qθ on with distillation loss Ldistill(θ) Compute labels = min(1, p(y)/qθ(y)) on (x, y) Update fϕ on {(x, y, l)}Q with adaptive drafting loss Ladapt(ϕ) {} Algorithm 4 Online Adaptive Drafting Interleaved Training 1: Given target p(), drafter qθ(), acceptance prediction head fϕ(), online data stream S, distillation data buffer Q, adaptive drafting data buffer with max size , batch size B, update interval I. 2: i, Q, 0, {}, {} 3: while True do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end while end if {} end if else S, + 1 Get response with speculative decoding using adaptive drafting Push (x, y) to if mod == 0 then Update qθ on with distillation loss Ldistill(θ) Push to if > then Evict early data from Sample batch RB Compute labels = min(1, p(y)/qθ(y)) on (x, y) RB Update fϕ on {(x, y, l)}B with adaptive drafting loss Ladapt(ϕ) 16 Cross-vocabulary N-gram Cache Ablations D.1 N-gram distributions To showcase the captured n-grams during our cross-vocabulary speculative decoding, we collect the n-gram cache across tasks after online training with the hybrid distillation losses. We plot the distribution of the n-gram counts with respect to the frequencies they are encountered in Figure 5. We include n-grams with maximum frequency of 3000 and use log-scale of the n-gram counts for cleaner visualization. N-grams with higher frequencies over 3000 typically represents group of delimiters such as spaces and newline characters that are used for formatting. Besides these, we can observe clear long-tail distribution for more frequent n-grams. Although the majority of n-grams have low frequencies which are not guaranteed to re-appear in future data stream, the long-tail frequency n-grams still have non-trivial contribution and can be utilized to speed up the cross-vocabulary speculative decoding process. We also notice the frequent n-grams constitute higher percentage in the MBPP+HumanEval domain, despite having smaller dataset size. It indicates the n-gram cache technique for cross-vocabulary speculative decoding can be more effective in well-structured domain like coding. Figure 5: Cross-vocabulary N-gram Distributions of Llama-68M and Qwen2-7B D.2 N-gram inference examples Next in Table 11, we show examples of the n-grams being used from the cache during inference in different tasks. The n-gram examples are extracted from actual samples in the datasets with Llama-68M and Qwen2-7B, given draft length of 4 each proposal or speculative decoding step. Pink tokens represent tokens with direct mapping between the drafter vocabulary and target vocabulary. Yellow tokens represent sub-tokens in the drafter space and the corresponding n-gram tokens in the target space, which are mapped via cache lookup. Lime tokens represent those that are accepted by the target model after verification. Note that even with the mapping, the proposed n-gram tokens are not guaranteed to be accepted and require cross-vocabulary distillation to further align their distributions with the target. 17 Proposed Tokens (Drafter Vocab) Mapped Tokens (Target Vocab) Accepted Tokens GSM8K There , fore , appoint , ments , , , there . , Since Therefore , appointments , , , there . , Since of , qu , izz , es distance , tra , ve , led _ , to , _ , sw asc , ending , order , . = , , ounter , ( test , _ , sol , ution of , quizzes distance , traveled MBPP+HumanEval _to , _sw ascending , order , . = , Counter , ( test , _solution Alpaca - appointments - distance , traveled - ascending , order = , Counter , ( - Future , of , , of , Int , ellig , ent most , common , , oun Imp , act , : , ** Future , of , AI of , Intelligent most , common , noun Impact , :** - - most , common , noun Impact . . an , off , ender , is , on , going , Ins , pect , or , David and , thank , ed , the XSum . an , offender , . is , ongoing , Inspector , David and , thanked , the . . an , offender , is , ongoing , Inspector - Table 11: Examples of N-gram Cache Across Tasks. Each row represents one cross-vocabulary speculative decoding step/proposal extracted from the inference results on the data test set (no sequential order between rows). Drafter proposes 4 tokens each time, which are mapped to the target space either as direct mapping tokens or as merged n-gram tokens . Target model then verifies the mapped tokens to get the final accepted tokens , plus any correction token from the residual distribution or sampled free token if all mapped tokens are accepted. We also denote no token being accepted as -. 18 D.3 N-gram learning Figure 6: Evolution of n-gram tokens learning with Llama-68M and Llama3-8B on MBPP+HumanEval dataset. N-gram hit refers to the number of n-grams proposed by drafter in each response, averaged over query samples. Acceptance ratio refers to the quantity min(1, p/q) in speculative decoding, it is averaged over proposal steps with an n-gram token and is indicative of the distribution alignment between the drafter and target. These two metrics show cross-vocabulary distillation learns to slowly align n-gram distributions to the target and utilize them more in inference. From Figure 6, it can be seen that during the training of LDM +λLN-gram on MBPP+HumanEval dataset, the average acceptance ratio for the n-grams is steadily improving, showing the impact of the loss function for better alignment of the n-grams. Furthermore, the impact is clearly visible even across different temperatures. Similarly as training progresses, the average number of successful cache hit for n-gram tokens per query is also improving."
        }
    ],
    "affiliations": [
        "Qualcomm AI Research"
    ]
}