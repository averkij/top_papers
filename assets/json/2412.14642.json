{
    "paper_title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation",
    "authors": [
        "Jiatong Li",
        "Junxian Li",
        "Yunqing Liu",
        "Dongzhan Zhou",
        "Qing Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on TOMG-Bench. Our codes and datasets are available through https://github.com/phenixace/TOMG-Bench."
        },
        {
            "title": "Start",
            "content": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation Jiatong Li1 Junxian Li2 Yunqing Liu1 Dongzhan Zhou3 Qing Li1 1The Hong Kong Polytechnic University 2Shanghai Jiao Tong University 3Shanghai AI Lab jiatong.li@connect.polyu.hk, lijunxian0531@sjtu.edu.cn, yunqing617.liu@connect.polyu.hk zhoudongzhan@pjlab.org.cn, csqli@comp.polyu.edu.hk 4 2 0 2 9 1 ] . [ 1 2 4 6 4 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMGBench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the opensource general LLMs, even surpassing GPT3.5-turbo by 46.5% on TOMG-Bench. Our codes and datasets are available through https: //github.com/phenixace/TOMG-Bench."
        },
        {
            "title": "Introduction",
            "content": "Molecule discovery plays pivotal role in various scientific research fields, from pharmaceuticals (Keiser et al., 2010) to materials science (Higuchi et al., 2023). Normally, molecule discovery is trial and error process (Ekins, 2024), which requires repetitive experimentation and data analysis (Mattern and Grosser, 2023). Due to the inefficiency of the traditional techniques, it usually takes more than 10 years to bring new drug candidate into the market (Lee et al., 2018). With the development of machine learning techniques and the advent of Graph Neural Networks Equal Contribution. Figure 1: Comparison of Text-Based Targeted Molecule Generation (a) v.s. Text-Based Open Molecule Generation (b). (GNNs) (Wang et al., 2023), there has been significant step forward. As molecules could be represented as graphs, GNN-based methods can capture the structural patterns of the molecule and make accurate predictions. With the assistance of GNNs, researchers could analyse the properties of molecules (Cai et al., 2022) and generate new molecule candidates (Jin et al., 2018). However, challenges still exist. GNN-based methods struggle to generalize to different tasks (Chen et al., 2024), necessitating costly data collection and preparation for different downstream tasks. Moreover, these methods are constrained in their capacity to generate molecules with specific, customized properties, limiting their flexibility in molecular design (Li et al., 2024c). In Contrast, Large Language Models (LLMs) have shown their great generalization capability (Achiam et al., 2023) and could be easily adapted to different research fields. For instance, Cascella et al. (2023) utilizes ChatGPT for supporting clinical practice and Zhen et al. (2024) adopts LLMs as assistants for task planning in the field of Civil Engineering, showing the great potential of LLMs in scientific discovery. As molecules can be represented as texts by Simplified Molecular Input Line Entry System (SMILES), linear notation that encapsulates the structure of chemical compound (Weininger, 1988), they can be processed and understood by LLMs, bridging the gap between molecules and natural languages. With advanced reasoning and in-context learning capabilities, LLMs are particularly adept at generalizing to the molecule domain (Li et al., 2024b). The generalization capability makes LLMs viable option in molecule discovery. Furthermore, by aligning molecules with textual data, LLMs can serve as powerful assistants to chemists (Zhang et al., 2024; Li et al., 2024d). They could help interpret and generate chemical knowledge, suggest modifications to molecular structures, and even predict the properties and behaviours of compounds, which would potentially streamline the molecule discovery process, leading to breakthroughs in diverse research areas. While the integration of LLMs into molecule discovery holds immense promise, the process of aligning molecules with textual data is challenging (Li et al., 2024a). significant challenge lies in the availability and diversity of datasets and benchmarks necessary for training and evaluation. Although the task of molecule-caption translation (Edwards et al., 2022) is crucial for bridging the gap between the molecular and textual domains, it still has several limitations that need to be addressed: On the one hand, there is concern about the generalization of the molecule-caption translation task (Li et al., 2024b). In real-world scenarios, molecule captions that describe molecular structures can be highly ambiguous, with multiple correct interpretations, while the current molecule-caption translation is actually targeted generation task. In this case, these models often struggle to generalize to customized molecules, even for seemingly simple examples (Li et al., 2024b). This suggests fundamental mismatch between the molecular and textual spaces, raising questions about whether this task could truly guide LLMs well. On the other hand, critical issue is the inability to propose new molecule structures. The ultimate goal of molecule discovery is not just to understand and describe existing chemical compounds but to innovate and discover new ones, particularly in the context of drug discovery, indicating that the current molecule-caption translation task and the corresponding evaluation metrics fall short in this regard. Therefore, addressing these challenges is essential for harnessing the full potential of LLMs in molecule discovery. In our efforts to bridge the gap between the natural language and the molecular spaces and to further facilitate LLMs as chemist assistants in molecule discovery, we propose novel benchmark, Text-based Open Molecule Generation Benchmark (TOMG-Bench). TOMG-Bench is designed to evaluate the open-domain generative capabilities of LLMs in the molecular domain through series of structured instructions for molecule design and operations. As shown in Figure 1, different from the previous targeted molecule generation tasks, Text-based Open Molecule Generation does not set specific target or enables LLMs to generate an exact matched molecule. Instead, we adopt chemical toolboxes like RDKit (Landrum, 2013) to test whether the generation meets the requirements. In other words, there can be multiple correct answers for single question, and LLMs are only required to generate one of them. Notably, TOMG-Bench is meticulously categorized into three primary tasks, i.e., molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each category contains three subtasks, and each subtask is composed of 5,000 test samples, providing comprehensive and robust assessment of whether LLMs truly grasp the molecular space. Meanwhile, we also propose different set of evaluation metrics to evaluate and rank the performance of LLMs, which considers both the accuracy and quality of the generated molecules. Moreover, we propose an instruction-tuning dataset, OpenMolIns, by extracting and reformatting molecules from an existing molecule database. OpenMolIns is structured across five distinct data levels (i.e., light, small, medium, large, and extra-large) to tailor different training purposes. To encapsulate our contributions, they are primarily threefold: Firstly, we are the first to propose the task of Text-based Open Molecule Generation, along with new set of evaluation metrics to benchmark the performance of LLMs in this task. Secondly, we have benchmarked suite of 25 LLMs, shedding light on the limitations of the targeted molecule generation. Thirdly, we introduce an instruction tuning set, OpenMolIns, which helps Llama-3.1-8BInstruct to outperform all the open-source general LLMs in TOMG-Bench."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we briefly review related work about developing Artificial Intelligences (AIs) in Molecule Discovery and, more specifically, in textbased molecule generation tasks. 2.1 Development of AIs in Molecule Discovery Molecule discovery plays pivotal role across numerous scientific fields, driving advancements in the development of drug discovery and material design (Du et al., 2022). Thus, integrating artificial intelligence into molecule discovery has marked transformative shift in the pharmaceutical landscape, significantly enhancing the efficiency and effectiveness of identifying and developing new therapeutic molecules. Recent advancements in machine learning (ML), deep learning (DL), and natural language processing (NLP) have enabled AI systems to analyze complex biological and chemical data more effectively than traditional methods (Wigh et al., 2022; Wu et al., 2018; Zhou et al., 2023). For instance, MolReGPT (Li et al., 2024b) leverages large language models (LLMs) like ChatGPT to learn molecule SMILES strings representation for molecule-caption translation tasks. Moreover, existing studies have explored advanced methods that utilize various AI techniques to further enhance molecule discovery processes, including Convolutional Neural Networks (CNNs) (Peng and Zhao, 2019; Le et al., 2019), Recurrent Neural Networks (RNNs) (Grisoni et al., 2020; Popova et al., 2019), Graph Neural Networks (GNNs) (Wang et al., 2023; Sun et al., 2022), and Transformerbased networks (Xia et al., 2023; Balaji et al., 2023; Edwards et al., 2022)."
        },
        {
            "title": "2.2 Text-based Molecule Generation",
            "content": "Text-based Molecule Generation (Text2Mol) (Edwards et al., 2021) has recently emerged as transformative approach to molecule discovery. This task centres on retrieving molecules using natural language descriptions as search queries, requiring the creation of paired datasets of molecules and their corresponding textual representations. This enables the learning of shared semantic embedding space for efficient retrieval. Early approaches leveraged transformer-based models like MolT5 (Edwards et al., 2022), employing selfsupervised learning on large datasets to generate high-quality Simplified Molecular Input Line Entry System (SMILES) strings from textual inputs. Subsequent advancements, such as KVPLM (Zeng et al., 2022), MoMu (Su et al., 2022), and BioT5 (Pei et al., 2023), integrated molecular graphs and biochemical text to improve both understanding and generation capabilities. 3DMoLM (Li et al., 2024e) further enhanced this by incorporating spatial configurations, leading to more accurate and geometrically valid molecular representations. The application of large language models (LLMs) like MolReGPT (Li et al., 2024b) and ICMA (Li et al., 2024a) as in-context learners has also shown significant promise. These models demonstrate the ability of LLMs to adaptively generate molecules by retrieving and leveraging relevant examples from the provided context. Most recently, MolReFlect (Li et al., 2024c) underscored the importance of fine-grained alignment between molecular structures and their textual descriptions, utilizing teacher-student training paradigm to capture these nuanced relationships effectively. Unlike this targeted generation task, in this paper, we propose Text-based Open Molecule Generation task to enable LLMs to generate an exactly matched molecule rather than set specific target."
        },
        {
            "title": "3 TOMG-Bench",
            "content": "In this section, we propose TOMG-Bench to comprehensively assess the performance of LLMs in molecular space. Specifically, the benchmark is composed of three basic tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). To ensure the integrity and effectiveness of TOMGBench, we have developed robust set of evaluation metrics for different tasks. Additionally, we have created OpenMolIns, an instruction-tuning dataset aimed at enhancing the performance and adaptability of LLMs to the challenges presented by this benchmark."
        },
        {
            "title": "3.1 Dataset Categorization",
            "content": "The categorization of the TOMG-Bench dataset initially considers the inherent characteristics of molecule SMILES representation and the role of LLMs serving as the chemists assistant, namely helping the chemists to edit, optimize and cusFigure 2: Data construction workflow and evaluation process of TOMG-Bench. tomize molecules as they want. Following the difficulty of the tasks, we demonstrate the content of the three basic tasks as well as their corresponding subtasks: MolEdit emerges as the most straightforward task among the three domains, as an existing molecule is already provided, and LLMs are only required to make modifications to it, which tests the molecular structure knowledge of LLMs. In this case, we have crafted three subtasks for MolEdit: AddComponent, DelComponent, and SubComponent. In AddComponent, LLMs are instructed to add specific functional group to the given molecule, and DelComponent challenges LLMs to remove specified functional group from the provided molecule, while SubComponent is hybrid of the previous two subtasks, requiring LLMs to first remove designated functional group and then introduce new one as specified to the molecule. MolOpt challenges LLMs to not only edit molecules but also to discern whether the modification will steer the molecule towards desired optimization target. To assess this capability, we concentrate on three pivotal properties that are vital for molecule discovery: LogP (Octanol-water partition coefficient, metric of lipophilicity), MR (molecular refractivity, proxy for the molar refractive index), and QED (Quantitative Estimate of Druglikeness, an assessment of drug-like characteristics). These metrics offer critical information about the potential pharmacological attributes of the molecule, which could help chemists filter molecules as viable drug candidates. MolCustom is the most challenging task, where we have established three subtasks: AtomNum, BondNum, and FunctionalGroup. For AtomNum, LLMs are tasked with generating molecules that adhere to specified count and type of atoms. BondNum involves the creation of molecules with defined number and type of bonds. In FunctionalGroup, LLMs must generate molecule that includes functional groups as specified. These subtasks may appear straightforward, yet they are deceptively challenging. They demand that LLMs have sophisticated understanding of molecular syntax to precisely generate molecules that meet the complex criteria set forth."
        },
        {
            "title": "3.2 Dataset Construction",
            "content": "Previously, molecule-related datasets and tasks have been hindered by the scarcity of human annotations. For instance, ChEBI-20 (Edwards et al., 2022), dataset for the molecule-caption translation task, contains only 33,000 samples for training, whereas image captioning datasets like MS COCO (Chen et al., 2015) have over 1,500,000 annotated captions on more than 330,000 images. Meanwhile, the annotation of molecules demands expertise from chemists and can sometimes require wet lab experiments, which are both expensive and time-consuming. In contrast, TOMG-Bench, as an open-domain generation task, does not rely on human annotations for construction. Instead, we focus on basic molecule structural properties and basic molecule operations that could be validated by chemical toolboxes to construct our dataset. For MolCustom, we randomly generate 5,000 prompts as requests for each subtask, requiring different numbers and collections of atoms, bonds, and functional groups. For MolEdit and MolOpt, we sample molecules from specific molecule database. Specifically, we select two molecule databases for this work: Zinc-250K (Sterling and Irwin, 2015), and PubChem (Kim et al., 2019). Zinc250K has 250,000 molecules, which is smaller than PubChem, which has 10 million molecules. To facilitate the fast calculation of metrics mentioned in Section 3.5, we choose Zinc-250K for sampling the test molecules in TOMG-Bench. Each subtask is allocated 5,000 test samples. After sampling, we utilize RDKit (Landrum, 2013), molecular informatics toolbox, to collect basic molecule statistics. There are functions available to calculate the required characteristics, especially the structural patterns and chemical properties like LogP, MR, and QED values, which can then be integrated into our pre-defined task prompts. Further details will be provided in the Appendix A. 3.3 Metric Design The evaluation of the TOMG-Bench is facilitated through set of carefully designed metrics tailored to the specific tasks within the benchmark. For the MolCustom task, which includes subtasks such as AtomNum, BondNum, and FunctionalGroup, the following metrics are employed: Accuracy: This metric measures the proportion of generated molecules that successfully meet the criteria. high accuracy indicates that the LLM is effectively generating molecules according to the request. Novelty: Novelty is critical metric for MolCustom tasks, as it assesses the ability of LLMs to discover new molecular structures. This is particularly important for potential drug or material candidates, where innovation can lead to significant advancements. Validity: This metric ensures that the generated molecules are chemically valid and follow the rules of molecular syntax. For the MolEdit and MolOpt tasks, the following metrics are adopted: Accuracy: Similar to the MolCustom category, accuracy here measures the proportion of edited or optimized molecules that could pass the test process. Similarity: Similarity between the original molecule and the edited molecule is an important point to concern. Since there are multiple valid solutions, only considering the accuracy is not sufficient for verification. For implementation, high similarity indicates that the generated molecule can be modified from the original molecule via as few as possible steps. For instance, when removing specific functional group, the edited molecule should still be similar to the original molecule. This process should not transform the molecule into an entirely new one nor inadvertently remove additional components. Validity: Similarly, validity is still crucial metric to confirm that the edited or optimized molecules adhere to the principles of the molecule textual representation. Notably, the calculation of novelty and similarity metrics only considers valid molecules. Specifically, for the novelty metric, we assess the similarity between the generated molecule and those within the Zinc-250K database. To comprehensively evaluate the average performance of LLMs on TOMG-Bench, we introduce weighted average accuracy to rank the performance of LLMs. Considering that the novelty scores for MolCustom and similarity scores for MolEdit and MolOpt are also crucial metrics for evaluating the performance, especially the similarity scores, which help identify correct molecule editing operations. In this case, we adopt the novelty and similarity scores as the weights, and the weighted average accuracy can be computed as follows: wAcc = 1 ( 9 (cid:88) (nt Acct)+ (cid:88) (st Acct)), tMolCustom t{MolEdit,MolOpt} (1) where nt represents the novelty score for the MolCustom tasks and st is the similarity score for the MolEdit and MolOpt tasks. Acct is the accuracy for each subtask t. This weighted average wAcc, provides balanced measure of accuracy, performance that considers both the accuracy and quality of the generation."
        },
        {
            "title": "3.4 OpenMolIns: Instruction Tuning Dataset",
            "content": "In this section, we introduce OpenMolIns, specialized dataset derived from the PubChem database to help LLMs get familiar with text-based open molecule generation via instruction tuning. This instruction tuning dataset is meticulously designed to ensure that the molecules it contains do not overlap with those in the Zinc-250K dataset to promote the generation of more novel molecular structures and avoid any potential data leakage that could compromise the integrity of the model performance. We collect the instruction tuning dataset by introducing the samples of the nine subtasks in equal amounts. We still apply the RDKit toolbox to construct the instruction tuning dataset. In the MolCustom domain, constructing the instruction tuning samples is rather straightforward. We calculate the molecular statistics and encapsulate them within prompts. For example, for AtomNum, we count all the atoms we would consider in the molecule, including their types and numbers. Then, we wrap these statistics with random pre-defined prompt to construct the training sample. This method allows the generated molecules to better fit the distribution of molecular space. For the MolEdit and MolOpt domains, we randomly select functional group from the original molecule for addition or removal, utilizing the RDKit toolbox to execute these operations. Then, similarly, we wrap the original molecule and the edited molecule with random pre-defined prompt to construct the training sample for MolEdit. In the case of MolOpt, we also evaluate the direction of the desired property changes by the functions in RDKit to determine whether the operation improves or decreases the property value, which is then altogether wrapped by the pre-defined prompts. To investigate the impact of data scales on the performance of LLMs, we have established five distinct data levels tailored for different training purposes: light, small, medium, large, and xlarge, as illustrated in Section 3.5. Each level represents different quantity of data, shown in Table 1, allowing us to analyze how the amount of training data influences the models ability to learn and generate or edit molecules effectively."
        },
        {
            "title": "Data Size",
            "content": "TOMG-Bench subtask OpenMolIns light small medium large xlarge 5,000 4,500 18,000 45,000 90,000 1,200,000 Table 1: Statisics of TOMG-Bench and OpenMolIns."
        },
        {
            "title": "3.5 Statistics",
            "content": "In this section, we outline the basic statistics of TOMG-Bench along with the OpenMolIns dataset. Table 1 shows the details of the data size in TOMGBench as well as the OpenMolIns datset. For TOMG-Bench, we have three main tasks with nine subtasks in total, where each subtask contains 5,000 test samples. More details can be found in Appendix A. For OpenMolIns, we have five distinct data scales: light, small, medium, large, and xlarge, ranging from 4,500 to 1,200,000 examples, which helps us investigate the data scaling law of applying LLMs to the Text-based Open Molecule Generation task. Notably, the nine subtasks within the TOMGBench are uniformly distributed in the OpenMolIns dataset."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present the experiment setup and results. Then, we illustrate our findings based on the observations. 4.1 Setup 4.1.1 Models The models benchmarked are categorized into four groups: proprietary models, open-source general LLMs, open-source ChEBI-20 fine-tuned LLMs, and OpenMolIns fine-tuned LLMs. Proprietary Models. This category includes LLMs that are only accessible via commercial API services. In this work, we benchmark GPT4o, GPT-4-turbo, GPT-3.5-turbo (Achiam et al., 2023), Claude-3.5 (Anthropic, 2024b), Claude-3 (Anthropic, 2024a), and Gemini-1.5-pro (Deepmind, 2024). Open-source General LLMs. This group contains open-source LLMs that are tuned with the instruction following capability, which can be used for wide range of tasks and applications. Specifically, we benchmark Llama-3-70B-Instruct, Llama-38B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-1BInstruct (Dubey et al., 2024), Mistral-7B-Instructv0.2 (Jiang et al., 2023), Qwen2-7B-Instruct (Yang et al., 2024), yi-1.5-9B (Young et al., 2024), and chatglm-9B (GLM et al., 2024). Open-source ChEBI-20 Fine-tuned LLMs. LLMs fine-tuned on the ChEBI-20 dataset can grasp some extent of text-based molecule generation capability. In this case, our experiments also cover LLMs like MolT5-small, MolT5-base, MolT5-large (Edwards et al., 2022), and BioT5base (Pei et al., 2023). OpenMolIns Fine-tuned LLMs We further adopt LLMs like Galactica-125M (Taylor et al., 2022), Llama3.2-1B-Instruct, and Llama-3.1-8B-Instruct on OpenMolIns dataset for instruction tuning. We specifically include the experiments on five distinct data sizes of OpenMolIns for Galactica-125M. 4.1.2 Implementation Details We implement various scripts to facilitate the testing of the aforementioned models. For proprietary models, we adopt the OpenAI API 1 framework. For open-source general LLMs, we utilize both the VLLM 2 framework and the OpenAI framework. For the remaining LLMs, we adopt the Hugging Face transformers library 3 for inference. Detailed hyper-parameters are demonstrated in Appendix B. Furthermore, it is important to note that BioT5 is designed to use SELFIES as input instead of SMILES. Consequently, we convert the molecule SMILES strings into SELFIES format on BioT5. 4.2 Results Figure 3 presents the performances and model sizes of different models benchmarked on TOMGBench, as well as the instruction-tuning performance of Galactica-125M on the five distinct data levels of OpenMolIns. More precise details are further illustrated in Appendix C."
        },
        {
            "title": "4.3 Findings",
            "content": "Based on the above results, we observe the following key findings: Text-based open molecule generation is challenging for LLMs. As illustrated in Figure 3, we have calculated the weighted average accuracy across all the nine subtasks. Among the LLMs benchmarked, Claude-3.5 stands out as the top performer, achieving weighted average accuracy of 35.92%. Gemini-1.5-pro follows closely with weighted average accuracy of 34.80%. These results underscore the considerable scope for improvement, even among the most advanced proprietary LLMs. It is also worth noting that while the most advanced LLMs like Claude-3.5 and GPT-4o exhibit relatively strong performance in the MolEdit and MolOpt tasks, the more challenging MolCustom task still remains challenge. In MolCustom tasks, no LLM has managed to achieve an accuracy exceeding 25% for single subtask. This observation indicates that the generation of molecules from scratch demands deep understanding of the molecular structural space, an area where current models are still striving to make significant strides. Most powerful open-source general LLMs can already outperform GPT-3.5-turbo. In the 1https://platform.openai.com/docs/ 2https://docs.vllm.ai 3https://huggingface.co/docs/transformers/ TOMG-Bench, Llama-3-70B-Instruct has achieved an impressive weighted average accuracy of 23.93%, notably outperforming GPT-3.5-turbo, which scored 18.58%. Despite previously lagging behind proprietary models, open-source general LLMs have rapidly bridged the gap. The evolution of the Llama series, in particular, has been remarkable, finally surpassing GPT-3.5-turbo and demonstrating the fast development of open-source general LLMs. More powerful LLMs inherit better performance in TOMG-Bench. Across all the LLMs we benchmarked, clear trend emerged: the more powerful the LLM is, the higher performance it can achieve on the TOMG-Bench. For instance, the GPT series has consistently demonstrated improved performance with each new iteration from GPT-3.5-turbo to GPT-4o. Similarly, within the Llama-3 series, we could also observe that larger models tend to achieve superior results on the TOMG-Bench. These findings underscore strong correlation between an LLMs capabilities and its performance in our benchmark. However, we encountered an unexpected anomaly with certain open-source LLMs. Notably, Qwen2-7B-Instruct, despite its impressive ability to solve mathematical problems and its size of 7 billion parameters, underperformed models with as few as 1 billion parameters. This result is particularly striking and suggests that the TOMG-Bench offers unique and comprehensive evaluation that current races for LLMs may not have adequately addressed. This discovery also highlights the significance of the TOMG-Bench as new benchmark for LLMs. It provides broader and more diverse assessment that exposes potential blind spots in the development of LLMs. ChEBI-20 dataset is insufficient for LLMs to master molecular structures and editing operations. The ChEBI-20 dataset and the associated molecule-caption translation task are designed to bridge the gap between molecular structures and textual descriptions. Despite this intention, LLMs trained on ChEBI-20 have demonstrated limited effectiveness in our TOMG-Bench benchmark. For instance, BioT5-base, which is claimed as the state-of-the-art (SOTA) model for text-based molecule generation on the ChEBI-20 dataset, only achieves weighted average accuracy of 4.21% on the TOMG-Bench. In the MolEdit and MolOpt tasks, these models are unable to execute correct Figure 3: The performance of models benchmarked in TOMG-Bench. In TOMG-Bench, LLMs are divided into 4 categories: Proprietary Models, Open-source General LLMs, Open-source ChEBI-20 Fine-tuned LLMs, and OpenMolIns Fine-tuned LLMs. Models whose parameters are known are plotted as dots, while models of unknown parameters are denoted as horizontal lines. operations on provided molecules, resulting in disappointing similarity scores. Similarly, in the MolCustom task, which closely mirrors the textbased molecule generation task, the performance remains unsatisfactory, with no model achieving score above 5% in single subtask. This performance shortfall highlights critical limitation of the ChEBI-20 dataset, as it lacks the data quantity and diversity necessary to effectively align molecules with textual descriptions. In contrast, TOMG-Bench offers more comprehensive and intricate evaluation framework for textto-molecule generation. With larger and more varied set of test examples, TOMG-Bench could robustly assess the capabilities of language models in translating textual descriptions into molecular structures. As such, it represents significant advancement in the evaluation of text-based molecule generation. OpenMolIns can enable LLMs to achieve better performance than the most powerful open source general LLMs. We have also developed OpenMolIns, an instruction-tuning dataset, to enhance LLMs proficiency in the tasks outlined in the TOMG-Bench. Across five distinct data scales, we observed pronounced data scaling law: as the size of the corpus increases, the performance of LLMs also improves. In particular, for Galactica-125M, we assessed its capabilities comprehensively on both five distinct data scales. As shown in Figure 3, the outcomes were remarkable: Galactica-125M achieved weighted average score of 25.73% on OpenMolIns-xlarge, surpassing even the 70B Llama-3-70B-Instruct and GPT-3.5-turbo, with only 125 million parameters. Meanwhile, the results of Galactica-125M show clear data scaling law, denoting that LLMs are hungry for more molecule corpora to achieve better performance. Notably, OpenMolIns-large has also enabled Llama3.1-8B-Instruct to outperform all the existing open-source general LLMs in TOMGBench, showing the effectiveness of the dataset."
        },
        {
            "title": "5 Conclusion",
            "content": "In this study, we introduce TOMG-Bench, the first benchmark designed to assess the capabilities of Large Language Models (LLMs) in the realm of open-domain molecule generation. Benchmarking 25 LLMs, TOMG-Bench highlights the limitations of existing targeted molecule generation tasks and demonstrates the potential of general LLMs in this domain. Additionally, through instruction tuning on our proposed OpenMolIns, LLMs exhibit significant potential on the TOMG-Bench, matching the performance of GPT-3.5-turbo. Our contributions not only lie in the development of novel benchmark for molecular discovery but also provide diverse indication of the capabilities of LLMs. clinical and research scenarios. Journal of medical systems, 47(1):33."
        },
        {
            "title": "6 Limitations",
            "content": "Although TOMG-Bench is carefully designed and well-validated through our experiments, we still observe several limitations: Prompt Diversity. Prompt diversity helps relieve the over-fitting of the instructions. While we adopt several different prompt templates and randomly choose from them, we still find the number of prompt templates is not enough to satisfy the prompt diversity. Data Distribution. In our data construction process, we allocate distributions to atoms, bonds, and functional groups with the aim of making our benchmark more reflective of real-world distributions. Nevertheless, the distribution we use is largely empirical and may not be sufficiently accurate to reconstruct real-world scenarios accurately. This could potentially mask the true performance capabilities of LLMs in these specific tasks."
        },
        {
            "title": "7 Acknowledgments",
            "content": "We thank all the reviewers for their insightful comments. We also thank GitHub Copilot for coding assistance and ChatGLM-4 for polishing the writing."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Anthropic. 2024a. Claude-3. Anthropic. 2024b. Claude-3.5. Suryanarayanan Balaji, Rishikesh Magar, Yayati JadGpthav, and Amir Barati Farimani. 2023. molberta: Gpt molecular features language model for molecular property prediction. arXiv preprint arXiv:2310.03030. Hanxuan Cai, Huimin Zhang, Duancheng Zhao, Jingxing Wu, and Ling Wang. 2022. Fp-gnn: versatile deep learning architecture for enhanced molecular property prediction. Briefings in bioinformatics, 23(6):bbac408. Marco Cascella, Jonathan Montomoli, Valentina Bellini, and Elena Bignami. 2023. Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. 2024. Exploring the potential of large language models (llms) in learning on graphs. ACM SIGKDD Explorations Newsletter, 25(2):4261. Google Deepmind. 2024. Gemini-1.5-pro. Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. 2022. Molgensurvey: systematic survey in machine learning models for molecule design. arXiv preprint arXiv:2203.14500. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. Translation between molecules and natural language. arXiv preprint arXiv:2204.11817. Carl Edwards, ChengXiang Zhai, and Heng Ji. 2021. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 595607. Sean Ekins. 2024. The lab of the future: Self-driving labs for molecule discovery. GEN Biotechnology, 3(2):8386. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793. Francesca Grisoni, Michael Moret, Robin Lingwood, and Gisbert Schneider. 2020. Bidirectional molecule generation with recurrent neural networks. Journal of chemical information and modeling, 60(3):1175 1183. Akon Higuchi, Tzu-Cheng Sung, Ting Wang, QingDong Ling, Suresh Kumar, Shih-Tien Hsu, and Akihiro Umezawa. 2023. Material design for nextgeneration mrna vaccines using lipid nanoparticles. Polymer Reviews, 63(2):394436. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for 2018. molecular graph generation. In International conference on machine learning, pages 23232332. PMLR. Michael Keiser, John Irwin, and Brian Shoichet. The chemical basis of pharmacology. 2010. Biochemistry, 49(48):1026710276. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin Shoemaker, Paul Thiessen, Bo Yu, et al. 2019. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47(D1):D1102D1109. Greg Landrum. 2013. Rdkit documentation. Release, 1(1-79):4. Nguyen Quoc Khanh Le, Edward Kien Yee Yapp, YuYen Ou, and Hui-Yuan Yeh. 2019. imotor-cnn: Identifying molecular functions of cytoskeleton motor proteins using 2d convolutional neural network via chous 5-step rule. Analytical biochemistry, 575:17 26. Jeong Hee Lee, Tae-Eung Sung, Eungdo Kim, and Kwangsoo Shin. 2018. Evaluating determinant priority of license fee in biotech industry. Journal of Open Innovation: Technology, Market, and Complexity, 4(3):30. Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, and Qing Li. 2024a. Large language models are in-context molecule learners. arXiv preprint arXiv:2403.04197. Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and Qing Li. 2024b. Empowering molecule discovery for molecule-caption translation with large language models: chatgpt perspective. IEEE Transactions on Knowledge and Data Engineering. Jiatong Li, Yunqing Liu, Wei Liu, Jingdi Le, Di Zhang, Wenqi Fan, Dongzhan Zhou, Yuqiang Li, and Qing Li. 2024c. Molreflect: Towards in-context fine-grained arXiv alignments between molecules and texts. preprint arXiv:2411.14721. Process Research & Development, 27(11):1992 2009. Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan. 2023. Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. arXiv preprint arXiv:2310.07276. Shi-Ping Peng and Yi Zhao. 2019. Convolutional neural networks for the design and analysis of non-fullerene acceptors. Journal of Chemical Information and Modeling, 59(12):49935001. Mariya Popova, Mykhailo Shvets, Junier Oliva, and Olexandr Isayev. 2019. Molecularrnn: Generating realistic molecular graphs with optimized properties. arXiv preprint arXiv:1905.13372. Teague Sterling and John Irwin. 2015. Zinc 15 ligand discovery for everyone. Journal of chemical information and modeling, 55(11):23242337. Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and JiRong Wen. 2022. molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481. Ruoxi Sun, Hanjun Dai, and Adams Wei Yu. 2022. Does gnn pretraining help molecular representation? Advances in Neural Information Processing Systems, 35:1209612109. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: large language model for science. arXiv preprint arXiv:2211.09085. Yuyang Wang, Zijie Li, and Amir Barati Farimani. 2023. Graph neural networks for molecules. In Machine Learning in Molecular Sciences, pages 21 66. Springer. David Weininger. 1988. Smiles, chemical language and information system. 1. introduction to methodJournal of chemical ology and encoding rules. information and computer sciences, 28(1):3136. Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Yaotian Yang, Xinrui Xiong, et al. 2024d. Chemvlm: Exploring the power of multimodal large language models in chemistry area. arXiv preprint arXiv:2408.07246. Daniel Wigh, Jonathan Goodman, and Alexei Lapkin. 2022. representation in the age of machine learning. Wiley Interdisciplinary Reviews: Computational Molecular Science, 12(5):e1603. review of molecular Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and Qi Tian. 2024e. Towards 3d molecule-text interpretation in language models. arXiv preprint arXiv:2401.13923. Zhenqin Wu, Bharath Ramsundar, Evan Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh Pappu, Karl Leswing, and Vijay Pande. 2018. Moleculenet: benchmark for molecular machine learning. Chemical science, 9(2):513530. Keith Mattern and Shane Grosser. 2023. Automated end-to-end workflow for volumetric masstransfer coefficient (k a) characterization in smallmolecule pharmaceutical development. Organic Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Li. 2023. Mole-bert: Rethinking pre-training graph neural networks for molecules. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652. Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2022. deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature communications, 13(1):862. Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, et al. 2024. Chemllm: chemical large language model. arXiv preprint arXiv:2402.06852. Yue Zhen, Sheng Bi, Shuo Tang, Xing-tong Lu, Weiqin Pan, Hai-peng Shi, Zi-rui Chen, Yi-shu Fang, and Xin-meng Wang. 2024. Llm-project: Automated engineering task planning via generative ai and wbs integration. In 2024 IEEE 14th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER), pages 605610. IEEE. Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. 2023. Uni-mol: universal 3d molecular representation learning framework."
        },
        {
            "title": "A Data Construction",
            "content": "In this section, we introduce the construction details of TOMG-Bench and OpenMolIns dataset, as well as the prompt templates. A.1 MolEdit For the molecule editing (MolEdit) task, we consider the common operations on modifying functional groups in given molecule (i.e., add, drop, and substitute), which are simple tasks for human experts but challenging to LLMs. In this case, we further develop three corresponding subtasks: AddComponent, DelComponent, and SubComponent. Prompt templates for MolEdit are shown in Table 2. However, there are different kinds of functional groups, and some functional groups can play an important role in the molecule structure, such as connecting two separate parts of the molecule, which makes them unsuitable for these operations above as these operations will entirely change the structure of the molecule. In this case, we aim to make slight change in the molecule structure and limit most of the functional groups we choose within the end groups. Prompt Templates for MolEdit AddComponent Please add {} to the molecule {}. Modify the molecule {} by adding {}. Add {} to the molecule {}. DelComponent Please remove {} from the molecule {}. Modify the molecule {} by removing {}. Remove {} from the molecule {}. SubComponent Please substitute {} in the molecule {} by {}. Modify the molecule {} by replacing {} by {}. Replace {} in the molecule {} by {}. Please replace {} in the molecule {} with {}. Modify the molecule {} by substituting {} with {}. Substitute {} in the molecule {} with {}. Table 2: Prompt Templates for MolEdit Table 3 presents the functional groups that are taken into account for AddComponent and DelComponent, along with their respective selection weights. To reflect the distribution of these functional groups in real-world scenarios, we have implemented weighted random selection process for AddComponent, which ensures that less common functional groups are assigned lower probability to be chosen, thereby refining the selection mechanism to better mirror practical occurrences. Functional Group Weights benzene ring 15 hydroxyl 15 aldehyde 5 carboxyl amide 10 Functional Group Weights amine 5 nitro 5 halo 5 nitrile thiol 1 Table 3: Functional Groups that are considered in AddComponent and DelComponent, as well as their weights to be selected in AddComponent. For SubComponent, our focus is exclusively on end groups, which include hydroxyl, aldehyde, carboxyl, nitro, halo, nitrile, and thiol, which ensures that the editing operations are confined to substituting the existing functional group with another from this list, thereby maintaining the integrity of the molecules overall structure without altering it fundamentally. A.2 MolOpt Molecule optimization (MolOpt), designed to optimize molecular properties through the refinement of molecule structures, is not brand-new task. Previously, GNN-based methods have been widely adopted in this task, while these methods can only help with one specific subtask at time. In contrast, TOMG-Bench requires one single LLM to optimize molecules with different metrics and directions. In this work, we specifically focus on enhancing specific characteristics that are crucial for drug discovery and chemical synthesis, including LogP, MR, and QED. The prompt templates for MolOpt are illustrated in Table 4. Prompt Templates for MolOpt LogP Please optimize the molecule {} to have lower/higher LogP value. Modify the molecule {} to decrease/increase its LogP value. Optimize the molecule {} to have lower/higher LogP value. Please modify the molecule {} to decrease/increase its LogP value. Modify the molecule {} to have lower/higher LogP value. MR Please optimize the molecule {} to have lower/higher MR value. Modify the molecule {} to decrease/increase its MR value. Optimize the molecule {} to have lower/higher MR value. Please modify the molecule {} to decrease/increase its MR value. Modify the molecule {} to have lower/higher MR value. QED Please optimize the molecule {} to have lower/higher QED value. Modify the molecule {} to decrease/increase its QED value. Optimize the molecule {} to have lower/higher QED value. Please modify the molecule {} to decrease/increase its QED value. Modify the molecule {} to have lower/higher QED value. Table 4: Prompt Templates for MolOpt LogP refers to the logarithm of the partition coefficient, which is measure of molecules hydrophilicity or lipophilicity. It is an important factor in determining compounds bioavailability and membrane permeability. Molecular Refractivity (MR) is measure of the molar refractive index, which provides insight into the molecular size and the degree of molecular branching. It is used to assess the overall shape and bulk of molecule. Quantitative Estimation of Drug-Likeness (QED) is computational metric that evaluates the drug-likeness of molecule based on set of predefined rules. higher QED score suggests greater likelihood that the molecule will have favourable pharmacological properties. A.3 MolCustom To enable customized design of molecules, we think of three fundamental features for describing the molecule, including atoms, bonds, and functional groups. Given the specified category and number of atoms, bonds, and functional groups, LLMs should generate the molecule as we request. The prompt templates for MolCustom are shown in Table 5. Below, we present the construction details of the three subtasks for MolCustom: AtomNum. Table 6 shows the atoms we consider in AtomNum, as well as their weights to be selected. Notably, carbon, as the basic unit in organic chemicals, is mandatory option. The number of carbon atoms ranges from 1 to 40, while the number of other selected atoms ranges from 1 to 5. This setting relieves the difficulty for generation, as LLMs could generate carbon backbone first and attach the remaining atoms to the backbone one by one. BondNum. Similarly, we select five different kinds of chemical bonds: single, double, triple, rotatable, Prompt Templates for MolCustom AtomNum Please generate molecule with {} atom(s). Please generate molecule composed of {} atom(s). Please generate molecule consisting {} atom(s). The molecule has {} atom(s). The molecule is composed of {} atom(s). The molecule consists of {} atom(s). There is molecule with {} atom(s). There is molecule composed of {} atom(s). There is molecule consisting of {} atom(s). The molecule contains {} atom(s). BondNum Please generate molecule with {} bond(s). Please generate molecule composed of {} bond(s). Please generate molecule consisting {} bond(s). The molecule has {} bond(s). The molecule is composed of {} bond(s). The molecule consists of {} bond(s). There is molecule with {} bond(s). There is molecule composed of {} bond(s). There is molecule consisting of {} bond(s). The molecule contains {} bond(s). FunctionalGroup Please generate molecule with {} group(s). Please generate molecule composed of {} group(s). Please generate molecule consisting {} group(s). The molecule has {} group(s). The molecule is composed of {} group(s). The molecule consists of {} group(s). There is molecule with {} group(s). There is molecule composed of {} group(s). There is molecule consisting of {} group(s). The molecule contains {} group(s). Table 5: Prompt Templates for MolCustom Atom Weights carbon [Mandatory] oxygen 5 nitrogen sulfur 3 fluorine 2 chlorine 2 bromine 2 iodine 2 phosphorus Atom Weights boron 1 silicon 1 selenium tellurium arsenic 1 1 antimony 1 bismuth 1 polonium 1 Table 6: Atoms that are considered in AtomNum, as well as their weights to be selected."
        },
        {
            "title": "Bond\nWeights",
            "content": "single 5 double 4 triple 3 rotatable 1 aromatic 1 Table 7: Chemical bonds that are considered in BondNum, as well as their weights to be selected. and aromatic, as shown in Table 7. For the single bond, if selected, the number can vary from 1 to 50. For the aromatic bond, the number follows the rules of the formation of aromatic bonds, varying from 5 to 20. Moreover, the number of these remaining bonds, if selected, is specified from 1 to 5. Functional Group Weights benzene ring 15 hydroxyl 15 anhydride 2 aldehyde ketone 5 carboxyl 10 ester 5 amide 5 amine 5 nitro Functional Group Weights halo 2 thioether 1 nitrile 1 thiol 1 sulfide disulfide 1 sulfoxide 1 sulfone 1 borane 1 Table 8: Functional Groups that are considered in FunctionalGroup, as well as their weights to be selected. FunctionalGroup. Lastly, we also specify functional groups in the molecule structure. Table 8 shows the range of functional groups and their weights that are taken into consideration. Notably, in MolCustom, if not specified, LLMs can generate any number of these atoms, bonds, and functional groups. However, for these specified items, LLMs should strictly follow the requirements."
        },
        {
            "title": "B Hyper Parameters",
            "content": "In this section, we illustrate the detailed parameters adopted in this work, as shown in Table 9."
        },
        {
            "title": "Value",
            "content": "Generation temperature top_p num_beams max_new_tokens Instruction Tuning batchsize lr cutoff_len Lora Settings α dropout 0.75 0.85 1 512 32 3e-4 1024 64 128 0.1 Table 9: Hyper-parameters"
        },
        {
            "title": "C Detailed Results",
            "content": "In this section, we first show the leaderboard of TOMG-Bench in Table 10, where Claude-3.5 achieves first place with weighted average accuracy of 35.92%. Notably, via instruction tuning on our OpenMolIns dataset, Llama-3.1-8B achieves 6th place, which outperforms all the existing open-source LLMs and is just behind Cladue-3. Then, we present the detailed results of all the subtasks in Table 11, 12, and 13. Model #Parameters (B) Acc (%) wAcc(%) Claude-3.5 (Anthropic, 2024b) Gemini-1.5-pro (Deepmind, 2024) GPT-4-turbo (Achiam et al., 2023) GPT-4o (Achiam et al., 2023) Claude-3 (Anthropic, 2024a) OpenMolIns-large (Llama-3.1-8B) OpenMolIns-xlarge (Galactica-125M) Llama3-70B-Instruct (Int4) (Dubey et al., 2024) OpenMolIns-large (Galactica-125M) OpenMolIns-medium (Galactica-125M) GPT-3.5-turbo (Achiam et al., 2023) OpenMolIns-small (Galactica-125M) Llama3.1-8B-Instruct (Dubey et al., 2024) Llama3-8B-Instruct (Dubey et al., 2024) chatglm-9B (GLM et al., 2024) OpenMolIns-light (Galactica-125M) OpenMolIns-large (Llama3.2-1B) yi-1.5-9B (Young et al., 2024) Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) BioT5-base (Pei et al., 2023) MolT5-large (Edwards et al., 2022) Llama-3.1-1B-Instruct (Dubey et al., 2024) MolT5-base (Edwards et al., 2022) MolT5-small (Edwards et al., 2022) Qwen2-7B-Instruct (Yang et al., 2024) - - - - - 8 0.125 70 0.125 0.125 - 0.125 8 8 9 0.125 1 9 7 0.25 0.78 1 0.25 0.08 7 Table 10: Leaderboard of TOMG-Benchmark. 51.10 52.25 50.74 49.08 46.14 43.1 44.48 38.54 39.28 34.54 28.93 24.17 26.26 26.40 18.50 20.95 14.11 14.10 11.17 24.19 23.11 3.95 11.11 11.55 0.18 35.92 34.80 34.23 32.29 30.47 27.22 25.73 23.93 23.42 19.89 18.58 15.18 14.09 13.75 13.13(7) 13.13(6) 8.10 7.32 4.81 4.21 2.89 1.99 1.30(0) 1.29(9) 0.15 Models GPT-4o (Achiam et al., 2023) GPT-4-turbo (Achiam et al., 2023) GPT-3.5-turbo (Achiam et al., 2023) Claude-3.5 (Anthropic, 2024b) Claude-3 (Anthropic, 2024a) Gemini-1.5-pro (Deepmind, 2024) Llama3-70B-Instruct (Int4) (Dubey et al., 2024) Llama3-8B-Instruct (Dubey et al., 2024) Llama3.1-8B-Instruct (Dubey et al., 2024) Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) Qwen2-7B-Instruct (Yang et al., 2024) Yi-1.5-9B (Young et al., 2024) Chatglm-9B (GLM et al., 2024) Llama-3.2-1B-Instruct (Dubey et al., 2024) MolT5-small (Edwards et al., 2022) MolT5-base (Edwards et al., 2022) MolT5-large (Edwards et al., 2022) BioT5-base (Pei et al., 2023) OpenMolIns-large (Llama-3.2-1B) OpenMolIns-large (Llama-3.1-8B) OpenMolIns-light (Galactica-125M) OpenMolIns-small (Galactica-125M) OpenMolIns-medium (Galactica-125M) OpenMolIns-large (Galactica-125M) OpenMolIns-xlarge (Galactica-125M) AddComponent DelComponent Accuracy Similarity Validity Accuracy Similarity Validity Accuracy Similarity Validity SubComponent 0.6188 0.699 0.5832 0.6832 0.6766 0.7058 0.5198 0.3914 0.2992 0.1868 0.001 0.1742 0.2932 0.0374 0.122 0.1354 0.2834 0.3462 0.1756 0.5822 0.3786 0.3472 0.4736 0.5866 0.5842 0.6782 0.6936 0.6545 0.7017 0.684 0. 0.6801 0.6649 0.6088 0.6251 0.2527 0.417 0.7622 0.5343 0.1027 0.1066 0.1084 0.1567 0.5676 0.6541 0.5958 0.6172 0.5682 0.5876 0.5859 0.7412 0.7934 0.798 0.4414 0.818 0.8254 0.5922 0.5374 0.4954 0.376 0.0036 0.4216 0.5686 0.1982 0.449 0.4686 0.9282 0.3216 0.673 0.3786 0.5356 0.7442 0.8228 0.8438 0.7012 0.7244 0.3082 0.5414 0.5556 0.759 0.6122 0.4348 0.4336 0.2018 0.0006 0.2858 0.2956 0.0768 0.1598 0.1562 0.2228 0.1668 0.1816 0.5104 0.2062 0.3258 0.4886 0.6078 0.6526 0.6038 0.5735 0.7797 0.6678 0.6408 0. 0.5637 0.5058 0.5257 0.3774 0.4024 0.5936 0.7494 0.575 0.1125 0.1144 0.1201 0.1597 0.4963 0.5074 0.6521 0.6025 0.5184 0.5577 0.5084 0.8474 0.906 0.8468 0.796 0.8984 0.9158 0.7182 0.57 0.591 0.359 0.0012 0.4909 0.6914 0.3028 0.4504 0.4472 0.9198 0.2466 0.6896 0.7048 0.5758 0.7488 0.7934 0.8286 0.7992 0.7778 0.2918 0.8104 0.655 0.7148 0.5094 0.2602 0.3401 0.0602 0.0004 0.137 0.1498 0.0102 0.0708 0.0584 0.1692 0.0684 0.0844 0.544 0.3102 0.2692 0.3282 0.3438 0.1872 0.7225 0.7323 0.6333 0.731 0.7159 0. 0.717 0.6841 0.6424 0.6227 0.2895 0.4619 0.715 0.3671 0.1029 0.1028 0.0932 0.1576 0.5415 0.6258 0.5879 0.6181 0.5975 0.6491 0.6024 0.9368 0.916 0.6822 0.9588 0.9184 0.8684 0.6822 0.4838 0.5076 0.355 0.0068 0.4368 0.5084 0.1468 0.4876 0.4426 0.941 0. 0.2958 0.84 0.6674 0.5692 0.6958 0.8438 0.8538 Table 11: Results on MolEdit. For each task, we highlight the best accuracy and underline the second best accuracy. Models GPT-4o (Achiam et al., 2023) GPT-4-turbo (Achiam et al., 2023) GPT-3.5-turbo (Achiam et al., 2023) Claude-3.5 (Anthropic, 2024b) Claude-3 (Anthropic, 2024a) Gemini-1.5-pro (Deepmind, 2024) Llama3-70B-Instruct (Int4) (Dubey et al., 2024) Llama3-8B-Instruct (Dubey et al., 2024) Llama3.1-8B-Instruct (Dubey et al., 2024) Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) Qwen2-7B-Instruct (Yang et al., 2024) Yi-1.5-9B Chatglm-9B Llama-3.2-1B-Instruct (Dubey et al., 2024) MolT5-small MolT5-base MolT5-large BioT5-base OpenMolIns-large (Llama-3.2-1B) OpenMolIns-large (Llama-3.1-8B) OpenMolIns-light (Galactica-125M) OpenMolIns-small (Galactica-125M) OpenMolIns-medium (Galactica-125M) OpenMolIns-large (Galactica-125M) OpenMolIns-xlarge (Galactica-125M) MR Accuracy Similarity Validity Accuracy Similarity Validity Accuracy Similarity Validity LogP QED 0.719 0.7662 0.4048 0.797 0.7984 0.7712 0.5984 0.4642 0.399 0.222 0 0.2884 0.3666 0. 0.2158 0.2074 0.4244 0.5158 0.2898 0.8054 0.3202 0.4172 0.5904 0.6454 0.7362 0.6586 0.6984 0.6327 0.7124 0.6067 0.7022 0.6028 0.3658 0.4235 0.4501 0.2923 0.5461 0.6902 0.5055 0.1052 0.1051 0.1015 0.1526 0.5951 0.6678 0.6547 0.642 0.5812 0.5927 0. 0.8796 0.9048 0.854 0.9422 0.9096 0.9274 0.6482 0.6086 0.5122 0.2802 0.0004 0.4927 0.4736 0.1664 0.4302 0.4168 0.8156 1 0.385 0.872 0.6416 0.5568 0.789 0.8198 0.8902 0.6864 0.7388 0.412 0.6962 0.6094 0.7876 0.5684 0.4332 0.4164 0.1908 0.0002 0.205 0.3514 0. 0.2316 0.1856 0.4496 0.506 0.2644 0.7122 0.3508 0.3958 0.5874 0.6388 0.7124 0.642 0.6821 0.6263 0.7112 0.6398 0.6744 0.6032 0.4793 0.483 0.2578 0.4123 0.3724 0.682 0.441 0.1011 0.1073 0.1072 0.1597 0.5956 0.6548 0.6435 0.6452 0.5873 0.5973 0. 0.8352 0.8848 0.8486 0.911 0.9062 0.8926 0.6272 0.5704 0.5238 0.3795 0.0004 0.4126 0.5 0.1604 0.442 0.3796 0.8678 1 0.3678 0.8514 0.6358 0.5338 0.7384 0.8028 0.8612 0.3952 0.3946 0.3316 0.5361 0.4678 0.4704 0.2774 0.2568 0.2655 0.121 0 0.1064 0.1832 0. 0.2214 0.2358 0.4654 0.5068 0.1996 0.5224 0.269 0.2956 0.4608 0.495 0.5786 0.618 0.6587 0.5635 0.7042 0.5855 0.6077 0.4828 0.4547 0.4499 0.3244 0 0.6596 0.6506 0.4757 0.1031 0.1054 0.119 0.158 0.5849 0.6398 0.6521 0.6385 0.5859 0.5962 0. 0.857 0.905 0.8354 0.8604 0.9044 0.9484 0.634 0.6112 0.6158 0.2532 0 0.4526 0.4342 0.1796 0.4326 0.4536 0.9214 1 0.349 0.8802 0.638 0.5376 0.7768 0.81 0.8626 Table 12: Results on MolOpt. For each task, we highlight the best accuracy and underline the second best accuracy. Models GPT-4o (Achiam et al., 2023) GPT-4-turbo (Achiam et al., 2023) GPT-3.5-turbo (Achiam et al., 2023) Claude-3.5 (Anthropic, 2024b) Claude-3 (Anthropic, 2024a) Gemini-1.5-pro (Deepmind, 2024) Llama3-70B-Instruct (Int4) (Dubey et al., 2024) Llama3-8B-Instruct (Dubey et al., 2024) Llama3.1-8B-Instruct (Dubey et al., 2024) Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) Qwen2-7B-Instruct (Yang et al., 2024) Yi-1.5-9B (Young et al., 2024) Chatglm-9B (GLM et al., 2024) Llama-3.2-1B-Instruct (Dubey et al., 2024) MolT5-small (Edwards et al., 2022) MolT5-base (Edwards et al., 2022) MolT5-large (Edwards et al., 2022) BioT5-base (Pei et al., 2023) OpenMolIns-large (LLama-3.2-1B) OpenMolIns-large (LLama-3.1-8B) OpenMolIns-light (Galactica-125M) OpenMolIns-small (Galactica-125M) OpenMolIns-medium (Galactica-125M) OpenMolIns-large (Galactica-125M) OpenMolIns-xlarge (Galactica-125M) BondNum Accuracy Novelty Validity Accuracy Novelty Validity Accuracy Novelty Validity FunctionalGroup AtomNum 0.1998 0.1702 0.107 0.1928 0.1044 0.1742 0.1404 0.0242 0.0228 0.0078 0.011 0.0392 0.0002 0.004 0.0006 0.0008 0.015 0.0118 0.0144 0.0136 0.0044 0.0146 0.0294 0.0464 0.1862 0.6703 0.6991 0.5054 0.6926 0.6833 0. 0.6675 0.6649 0.702 0.6732 0.9061 0.6848 0.7483 0.6807 0.6586 0.6868 0.7103 0.8353 0.649 0.6634 0.6054 0.6568 0.6553 0.6729 0.6899 0.5852 0.4904 0.6947 0.6548 0.591 0.6774 0.5474 0.3812 0.3862 0.2986 0.2622 0.617 0.2131 0.185 0.661 0.756 0.8412 0. 0.5616 0.7582 0.793 0.8424 0.8698 0.9116 0.9308 0.065 0.0774 0.0518 0.1058 0.1042 0.0708 0.067 0.026 0.0395 0.0102 0.001 0.0208 0.0254 0.008 0.0064 0.007 0.0118 0.0078 0.035 0.0544 0.0216 0.053 0.0622 0.0716 0.1656 0.6336 0.6301 0.6871 0.6584 0.6598 0. 0.6478 0.6303 0.6541 0.6309 0.8645 0.6407 0.7189 0.7465 0.598 0.6509 0.5611 0.6667 0.615 0.6614 0.5724 0.6365 0.6473 0.6695 0.6887 0.8564 0.9068 0.5522 0.886 0.8696 0.8688 0.7378 0.57 0.6387 0.4524 0.0796 0.7072 0.4682 0.2226 0.6202 0.8422 0.8916 0. 0.6186 0.7456 0.7596 0.7926 0.7474 0.7374 0.7952 0.233 0.218 0.1136 0.2364 0.1816 0.2486 0.1752 0.0848 0.13 0.0048 0.0022 0.0126 0 0.0008 0.0114 0.013 0.0382 0.0476 0.0252 0.1344 0.0244 0.057 0.0882 0.0996 0.2006 0.6513 0.6605 0.6585 0.6582 0.9158 0. 0.6576 0.6167 0.6274 0.6012 0.8601 0.6945 0.6908 0.7461 0.5287 0.5464 0.6088 0.6792 0.6373 0.6396 0.5756 0.5954 0.6091 0.6276 0.6445 0.859 0.8778 0.8686 0.8892 0.6644 0.924 0.765 0.7216 0.6905 0.402 0.0622 0.6521 0.5926 0.2818 0.8354 0.8382 0.9406 0. 0.4412 0.6435 0.8442 0.8874 0.8932 0.8966 0.9162 Table 13: Results on MolCustom. For each task, we highlight the best accuracy and underline the second best accuracy."
        }
    ],
    "affiliations": [
        "Shanghai AI Lab",
        "Shanghai Jiao Tong University",
        "The Hong Kong Polytechnic University"
    ]
}