{
    "paper_title": "Representational Stability of Truth in Large Language Models",
    "authors": [
        "Samantha Dies",
        "Courtney Maynard",
        "Germans Savcisens",
        "Tina Eliassi-Rad"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\\leq 8.2\\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone."
        },
        {
            "title": "Start",
            "content": "Representational Stability of Truth in Large Language Models Samantha Dies1, Courtney Maynard1, Germans Savcisens1, and Tina Eliassi-Rad1,2,3 1Khoury College of Computer Sciences, Northeastern University, 440 Huntington Ave, #202, Boston, MA 02115 USA 2Network Science Institute, Northeastern University, 177 Huntington Ave, #1010, Boston, MA 02115 USA 3Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, NM 87501 USA dies.s@northeastern.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) are widely used for factual tasks such as What treats asthma? or What is the capital of Latvia?. However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLMs veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training linear probe on an LLMs activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgments in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ( 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) are increasingly used as sources of information, yet their behavior often blurs the line between knowledge and plausibility [1, 2, 3]. People expect experts to distinguish between True, False, and Neither statements, yet it remains unclear whether LLMs form similarly structured internal representations. The stability of internal veracity representations in LLMs, i.e., how consistently they encode truth and falsity across related statements, is crucial for reliability and safety [4, 5, 6]. When this representational structure is unstable, LLMs often exhibit undesirable behaviors such as hallucinations [3, 7]. LLMs can appear factually competent even when their internal veracity representations are weakly separated or inconsistent. Such instability is one explanation for why small prompt or context changes can affect an LLMs answers [6, 8, 9], with recent work suggesting that epistemic familiarity also shapes an LLMs confidence and self-evaluation [10]. Prior work follows two paths: representation-based probing, which examines whether true and false statements form separable clusters in activation space [11, 12, 13], and in-context analyses, which test how output varies under persuasion [14, 15], phrasing [2, 16], or jailbreak attacks [17]. However, we lack unified approach for identifying which kinds of statements disrupt an LLMs latent factual representation [18]. We address this gap by analyzing representational stability: the consistency of an LLMs veracity representations under controlled perturbations of probes training data (see Figure 1). Inspired by Leitgebs notion of - stability [19], property of belief systems requiring stability under small evidential changes, we treat statements embedding as belief state and use representation-based probes to identify truth directions. We analyze three factual domains, City Locations, Medical Indications, and Word Definitions, and five statement types (True, False, Fictional, Synthetic, and Noise). Fictional and Synthetic statements represent distinct Neither cases. Fictional statements originate from familiar imaginary worlds likely present in training corpora, whereas Synthetic statements are automatically generated to ensure unfamiliarity. We train probe on activations from sixteen open-source LLMs to learn baseline True vs. Not True direction. For our probe, we use sAwMIL: max-margin, multiple-instance probe designed to incorporate Neither statements [12]. We then retrain the same probe under label perturbations (e.g., treating Fictional statements as True) to quantify shifts in inferred belief boundaries. Across our experiments, LLMs maintain well-separated True and False representations, but both familiar and unfamiliar Neither statements occupy contextdependent regions. Unfamiliar Synthetic Neither statements induce the largest rotations and flip rates, showing 5 2 0 D 8 ] . [ 2 6 6 1 9 1 . 1 1 5 2 : r Figure 1. Overview of Representational Stability Evaluation. toy example demonstrating how we assess representational stability by (a) training True vs. Not True probe on LLM activations with True (blue), False (green) and Neither (purple) veracity values and (b) retraining the probe with perturbed labels (i.e., redefining the operational definition of truth to include the Neither statements). We compare the similarity between the original (solid) and perturbed (dashed) decision boundaries and identify how many True statements flip to Not True after the perturbation (epistemic retractions) or, conversely, how many Not True statements flip to True (epistemic expansions). Stable veracity representations should have well-clustered activations that minimize the number of epistemic retractions and expansions. that unfamiliar content disrupts an LLMs veracity structure more than familiar Fictional content. Together, these analyses provide systematic and principled way to evaluate the robustness of LLM veracity representations under different semantic assumptions, an essential step toward diagnosing and mitigating factual inconsistency. 4. Stability: We show that the unfamiliar Synthetic statements produce the largest rotations in the truth directions and the highest prediction flip rates (up to 40% in Word Definitions), indicating that previously unseen yet semantically factual content most strongly destabilizes veracity geometry. Contributions"
        },
        {
            "title": "2 Related Work",
            "content": "1. Data: We introduce new dataset of fictional statements across three factual domains, enabling controlled comparisons between familiar (Fictional) and unfamiliar (Synthetic) Neither content. 2. Method: We introduce and study representational stability in LLMs by combining activation-based probes with controlled label perturbations that vary the operational definition of truth. 3. Representational Structure: We show across sixteen open-source LLMs that True and False activations form tightly aligned clusters, while Neither statements (familiar and unfamiliar) occupy distinct regions, reflecting differences in training familiarity rather than superficial linguistic form. Understanding how LLMs encode veracity touches on three research threads: (1) representation-based probing, (2) in-context stability, and (3) epistemic distinctions between belief, knowledge, and fact. We connect these strands by examining how familiar and unfamiliar Neither statements perturb the latent veracity geometry, thereby assessing how stable an LLMs representations are under shifts in semantic assumptions. Representation-based probing methods determine which properties are linearly recoverable from hidden states, revealing what models represent beyond inputoutput behavior [20, 21, 22]. Much of this work focuses on linguistic or syntactic recoverability, but recent studies have examined geometric structure, specifically, whether True and False statements form separable or directionally aligned clusters in activation space [11, 13]. Of 2/ Table 1. Summary of datasets and statement types. Number of affirmative (A) and negated (N) statements for each type across the three datasets, along with examples. Each dataset includes True, False, Synthetic, and Fictional statements, while Noise consists of randomly generated Gaussian activation vectors matched in dimensionality and distribution to the real statement embeddings. Synthetic statements serve as Neither statements that were not seen during LLM training, while Fictional statements are familiar Neither statements. Dataset True False Synthetic Fictional Noise Examples City Locations A: 1392 N: 1376 A: 1358 N: 1374 A: 876 N: 876 A: 350 N: 350 Medical Indications A: 1439 N: A: 1523 N: 1419 A: 478 N: 522 A: 402 N: 402 795 771 Word Definitions A: 1234 N: 1235 A: 1277 N: 1254 A: 1747 N: 1753 A: 1224 N: 1224 1095 (T) The city of Surat is located in India. (Fa) The city of Palembang is located in the Dominican Republic. (S) The city of Norminsk is located in Jamoates. (Fi) The city of Bikini Bottom is located in the Pacific Ocean. (T) Pentobarbital is indicated for the treatment of insomnia. (Fa) Vancomycin is not indicated for the treatment of lower respiratory tract infections. (S) Alumil is indicated for the treatment of reticers. (Fi) The Trump Virus is indicated for the treatment of Xenovirus Takis-B. (T) Hoagy is synonym of an italian sandwich. (Fa) Decalogue is an astronomer. (S) Dostab is scencer. (Fi) Snozzberry is type of berry. version of this table without the Fictional and Noise columns can be found in [12]. particular relevance is the sAwMIL framework [12], which uses multiple-instance learning and conformal prediction to classify statements as True, False, or Neither. Hallucination-detection studies likewise suggest that hidden states encode strong veracity signals even when outputs are incorrect [3]. These empirical approaches complement philosophical work clarifying when neural components should count as representations [18]. separate line of research demonstrates that LLM outputs are highly sensitive to prompting and context. Models are vulnerable to jailbreaks [17], sycophancy [23], word variation [8], and multi-turn drift [9]. These studies diagnose behavioral brittleness rather than instability in the underlying representations, though recent evidence on epistemic familiarity suggests that some output-level failures may trace back to deeper weaknesses in internal epistemic structure [10]. LLMs also struggle to distinguish between belief, knowledge, and fact. Suzgun et al. [5] show that LLMs often fail to track agents beliefs when those beliefs are mistaken, underscoring weaknesses in their epistemic structure. Uncertainty-focused analyses reveal similar failures under epistemic ambiguity, i.e., when information admits multiple plausible interpretations [6]. Theoretical work also argues that the study of LLM beliefs lacks unified standards, with Herrmann and Levinstein proposing criteria for when internal states should count as belieflike [24]. Formal epistemology offers complementary perspective: Leitgebs theory of -stability links rational belief to stable truth assignments under small contextual changes [19]. This connection motivates our focus on representational stability as an epistemic property of model activations rather than model outputs. We bridge probing and in-context work by measuring how controlled label perturbations reshape inferred truth directions, thereby identifying which types of familiar and unfamiliar Neither statements most strongly disrupt an LLMs latent encoding of veracity."
        },
        {
            "title": "3 Methodology",
            "content": "We treat an LLMs internal activations as proxy for its belief structure and use the decision boundary learned by linear probe as geometric diagnostic of how truth is encoded in that space. We define representational stability as the consistency of this boundary under controlled perturbations of what counts as True (see Fig. 1). The notation introduced in this section is summarized in Supplementary Table A1. Statements and Labels. We begin with collection of statements = {si}N i=1 drawn from factual domains. Each statement has ground-truth veracity label yi {True, False, Neither}. The Neither category includes familiar Fictional statements (e.g., Gotham City is in New Jersey) and unfamiliar Synthetic fact-like statements (e.g., The city of Norminsk is located in Jamoates). Activation Extraction. Each statement si is passed through an LLM to obtain its internal activation z(l) layer l, chosen empirically to maximize linear separability between True and Not True statements [11, 12, 13]. These activations constitute the models veracity representations. We construct the dataset = {(z(l) , yi)}N i=1, and partition it into training, calibration, and test splits: Dtrain, Dcal, and Dtest. Baseline Probe Training. We train max-margin, multiple-instance probe (namely, sAwMIL [12]) that learns linear decision boundary (z) = + b, where represents the truth direction. Labels are encoded as {+1, 1}: = ( +1, 1, if yi = True if yi = Not True 3/25 Table 2. Label configurations for experiments. Each row defines the composition of the True and Not True classes used when retraining the probe under different perturbation conditions. The baseline (Original) probe is trained on True statements versus all others, while perturbed probes redefine the True class to include additional statement types to test representational stability under shifts in the operational definition of truth. Fictional(T) denotes fictional truth and Fictional(F) denotes fictional falsehood. Original Perturbed Perturbation Type Statements Considered True Statements Considered Not True Baseline Synthetic Fictional Fictional(T) Noise False + Synthetic + Fictional + Noise False + Fictional + Noise False + Synthetic + Noise False + Synthetic + Fictional(F) + Noise False + Synthetic + Fictional True True + Synthetic True + Fictional True + Fictional(T) True + Noise Training on Dtrain yields the baseline classifier. The set of statements that the probe, and by extension the LLMs latent geometry, represents as True is Following -stability theory [19], retractions indicate stronger instability because they withdraw beliefs, whereas expansions reflect milder over-inclusiveness. Btrue = { si z(l) Dtest, yi = True, ˆyi = +1 }. We interpret Btrue as the models baseline belief set under the original definition of truth. Label Perturbations and Retraining. To assess representational stability, we systematically vary which Neither statements are treated as True. Let denote all Neither statements and partition it into two subsets N1 and N0, such that = N1 N0, N1 N0 = . Relabeling N1 as True and N0 as Not True yields the perturbed labels = ( +1, 1, if yi {True} N1, if yi {False} N0. Using the same data splits, we retrain the probe to obtain ( w, b) and define the perturbed belief set as true = { si z(l) Dtest, yi = True, ˆy = +1 }. Comparing Btrue and true reveals how the probes truth assignments shift under modified semantic assumptions while the underlying LLM activations remain fixed. Quantifying Representational Stability. Stability is quantified in two complementary ways. First, we measure geometric stability by comparing the decision boundaries ( w, b) and ( w, b). Cosine similarity between and captures rotational changes in the truth direction, while b captures translational shifts of the hyperplane. Second, we evaluate prediction stability by comparing belief sets: true Btrue Btrue B true true Btrue (stable truths), (epistemic retractions), (epistemic expansions). Together, these measures assess how reliably the LLMs veracity representations support stable truth assignments under shifts in semantic boundaries. The probes decision boundary thus serves not as an end task, but as geometric lens on how belief, falsity, and plausibility are structured within the LLMs activation space."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Data Our experiments draw on the three factual domains introduced in [12]: City Locations, Medical Indications, and Word Definitions. Although all three domains contain factual assertions, they differ in how sharply truth and falsehood are delineated. Statements in City Locations are objective and stable. Statements in Medical Indications are factual but context-dependent. Statements in Word Definitions are more interpretive due to polysemy and variation in usage. This range provides diverse testbed for studying how LLMs encode veracity across domains. Full dataset construction and validation details appear in Supplementary Section B. We analyze five types of statements: True, False, Synthetic, Fictional, and Noise  (Table 1)  . We take the True, False, and Synthetic statements directly from [12]. Synthetic statements are grammatically coherent but semantically meaningless constructions built from generated entity names. Because these entities cannot have appeared in training corpora, LLMs lack the background needed to assign them truth value. They therefore serve as unfamiliar Neither statements for which calibrated models should suspend belief. Fictional statements lack real-world truth value.1 Unlike unfamiliar Synthetic statements, many fictional entities (e.g., Gotham City, Xenovirus Takis-B) are likely present in training corpora. Fictional statements test 1We have released the fictional statements at https: //huggingface.co/datasets/samanthadies/representational_ stability. 4/25 Figure 2. Character bigram distributions of statements. Rankfrequency plots of normalized character bigram counts for True (green), False (red), Synthetic (yellow), and Fictional (blue) statements in the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. For each dataset, we compute per-type bigram frequencies, normalize within type, sort bigrams by their frequency under True statements, and plot log-normalized frequency with moving-average smoothing. Across datasets, the True, False, and Synthetic distributions are nearly indistinguishable, whereas the Fictional distribution decays more slowly, marking it as structurally distinct. whether LLMs distinguish between recognition and factual commitment: model may have rich associations with these entities while still treating them as nonfactual. Finally, Noise serves as non-semantic control. We sample random activation sequences matched to the mean, variance, and sequence-length distribution of the real activations. This ensures that observed representational effects arise from semantic content and context rather than from activation-space statistics alone. 4.2 Stability We measure stability to discuss the probe, the activations, and the perturbations. Probe. We use the sparse-aware multiple-instance learning probe (sAwMIL) [12], multiclass probing method designed to extract reliable and transferable veracity directions from LLM activations. Unlike simpler probes such as the Mean Difference classifier [13], which assumes that truth and falsehood lie along single axis, sAwMIL models True, False, and Neither as distinct directions and aggregates token-level representations using multiple-instance learning. It also incorporates conformal prediction [25] to calibrate uncertainty. As maxmargin method, sAwMIL yields stable decision boundaries, making differences across perturbations more reflective of genuine structure in the LLMs geometry than probe noise. For comparison, results from the Mean Difference probe appear in Supplementary Section E. Generating and Characterizing Activations. We consider sixteen open-source LLMs spanning the Gemma, Llama, Mistral, and Qwen families, with both base and chattuned variants (see Supplementary Section C). For each dataset, LLM pair, we extract token-level activations from the layer that maximizes linear separability between True and Not True statements [12] (see Supplementary Table A3). We then record the sequence of hidden states z(l) for each statement si. For descriptive, model-agnostic analyses, we reduce each sequence to single vector by selecting the final non-padding token. We then characterize patterns at both the linguistic and representation levels. At the linguistic level, we compute rankfrequency curves over character bigrams aggregated across entity names for each statement type. At the representation level, we compute pairwise 1-D Wasserstein distances between activation distributions across all dimensions, averaged across LLMs. These metrics reveal similarities between statement types at the linguistic level and in latent space before any supervised probing. Perturbations. For each dataset, LLM pair, we train one baseline probe and four perturbed probes as listed in Table 2.2 In the Baseline condition, True statements are contrasted with all others. The Synthetic pertur2We also evaluated perturbations in which each Neither type was added separately to either the True or False class. We observed qualitatively similar results. 5/ Figure 3. Average Wasserstein distance between activations. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements, averaged over sixteen LLMs for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Across datasets, Synthetic activations lie closest to the True and False activations, while Fictional and Noise activations are farther from all others, indicating that unseen but fact-like statements (Synthetic) resemble factual structure, whereas Fictional statements form distinct representational clusters. bation treats True and Synthetic as True, testing how unfamiliar fact-like content shifts the decision boundary. The Fictional perturbation treats True and Fictional as True, testing the influence of linguistically familiar but nonfactual content. The Fictional(T) perturbation further separates Fictional statements by canonical truth (e.g., Smallville is located in Kansas vs. Smallville is located in Virginia). Finally, the Noise perturbation treats True and Noise as True, serving as sanity check in which the added truths are random Gaussian activations rather than semantic content. Experimental Details. Activation sequences, data splits, hyperparameters, and preprocessing steps are held fixed. Token-level representations are scaled using standard scaler fit on the training set; bags are truncated to fixed maximum size; and we perform small grid search over the regularization parameter using three-fold crossvalidation with mean average precision as the criterion. Approximately 55% of the data is used for training, 20% for calibration, and 25% for testing (see Supplementary Table A2). Holding all training conditions constant ensures that changes in the learned truth direction arise solely from label perturbations."
        },
        {
            "title": "5 Results",
            "content": "differ in linguistic-level features and in activation space. Second, we measure probe-level changes by assessing how the learned True vs. Not True direction shifts under controlled label perturbations. Third, we analyze outputlevel changes in the probes predicted labels, quantifying how truth-value assignments respond to modified semantic assumptions. 5.1 Representations of True, False, and Neither Statements We first characterize the probe inputs. At the linguistic level, True, False, and Synthetic statements exhibit nearly identical normalized bigram distributions across all three domains (Figure 2). This confirms that the generation procedure for Synthetic statements3 preserves low-level linguistic structure. Fictional statements, however, show slower rankfrequency decay, reflecting stylistic patterns characteristic of narrative text. This divergence is most visible in Word Definitions (Fig. 2(c)) and least visible in City Locations (Fig. 2(a)), where some fictional cities align with real countries (e.g., Brigadoon is located in Scotland). We next test whether these linguistic-level differences correspond to differences in latent space by computing pairwise Wasserstein distances between activation distributions for all statement types (Fig. 3; per-model We present results at three levels of analysis. First, we examine the input representations: how statement types 3See Supplementary B.1.1 for details on the procedure that generates Synthetic statements. 6/25 Figure 4. Changes in the probe decision boundary under perturbations. Cosine similarity (left column) and bias difference (right column) between the baseline True vs. Not True probe and probes retrained under label perturbations for the (a,b) City Locations, (c,d) Medical Indications, and (e,f) Word Definitions datasets. Each heatmap shows results for sixteen LLMs (columns) and five perturbation conditions (rows). LLMs with leading underscores are chat models, while those without are base models. Higher cosine similarity indicates smaller rotations of the learned decision boundary, while bias difference reflects shifts in intercept. Across datasets, probes retrained with the Synthetic perturbation show the largest deviation from the original, particularly in cosine similarity. 7/25 Table 3. Flipped predictions under label perturbations. Counts (and percentages) of predictions that remain stable or flip between True and Not True across Synthetic, Fictional, Fictional(T), and Noise perturbations for each dataset. City Locations is the most stable, with only 4.8% of statements flipping in the worst case, followed by Medical Indications (12.2%) and Word Definitions (40.6%). Across all domains, Synthetic perturbations produce the most flips, suggesting that unseen but fact-like statements most strongly distort the learned veracity boundaries. Dataset City Locations Medical Locations Word Definitions Perturbation True to True Synthetic Fictional Fictional (T) Noise Synthetic Fictional Fictional (T) Noise Synthetic Fictional Fictional (T) Noise 9153 (91.5) 9326 (93.3) 9330 (93.3) 9183 (91.8) 7413 (72.6) 7808 (76.4) 7815 (76.5) 7779 (76.2) 3682 (37.2) 6507 (65.8) 6795 (68.7) 6653 (67.3) Not True to Not True 360 (3.6) 568 (5.7) 576 (5.8) 532 (5.3) 1556 (15.2) 2284 (22.4) 2269 (22.2) 2009 (19.7) 2188 (22.1) 2494 (25.2) 2520 (25.5) 2251 (25.4) Not True to True 274 (2.7) 66 (0.7) 58 (0.6) 102 (1.0) 786 (7.7) 58 (0.6) 73 (0.7) 333 (3.3) 785 (7.9) 479 (4.1) 453 (4.6) 462 (4.7) True to Not True Total Flips 213 (2.1) 40 (0.4) 36 (0.4) 183 (1.8) 460 (4.5) 65 (0.6) 58 (0.6) 94 (0.9) 3233 (32.7) 408 (4.1) 120 (1.2) 262 (2.6) 460 (4.8) 106 (1.1) 94 (1.0) 285 (2.8) 1246 (12.2) 131 (1.3) 427 (1.3) 427 (4.2) 4018 (40.6) 887 (8.2) 573 (5.8) 724 (7.3) Figure 5. Stability of probe predictions under label perturbations for City Locations data. Bar plots show, for each of the sixteen LLMs (x-axis), how often the sAwMIL probes predicted label changes when retrained under four perturbations: (a) Synthetic, (b) Fictional, (c) Fictional(T), and (d) Noise. Green bars indicate True to Not True flips, while purple bars indicate Not True to True flips. The left y-axis reports the number of statements with flipped predictions, and the right y-axis reports the corresponding proportions. The Synthetic perturbation leads to the most instability, and the base models exhibit more True to Not True flips than the chat models. 8/25 Figure 6. Stability of probe predictions under perturbations for Medical Indications data. Bar plots show, for each of the sixteen LLMs (x-axis), how often the sAwMIL probes predicted label changes when retrained under four perturbations: (a) Synthetic, (b) Fictional, (c) Fictional(T), and (d) Noise. Green bars indicate True to Not True flips, while purple bars indicate Not True to True flips. The left y-axis reports the number of statements with flipped predictions, and the right y-axis reports the corresponding proportions. The Synthetic perturbation leads to the most instability, and the Fictional and Fictional(T) perturbations result in almost no flips. heatmaps appear in Supplementary Figures A1A16). Across all sixteen LLMs, True and False representations lie close together, with mean distances of 0.40, 0.29, 0.14 for City Locations, Medical Indications, and Word Definitions, respectively. Synthetic statements remain only modestly farther from True, with mean distances of 0.58, 0.44, 0.33 for City Locations, Medical Indications, and Word Definitions, respectively. This is consistent with their linguistic similarity despite their unfamiliarity. By contrast, Fictional and Noise statements lie much farther from True. For Fictional, the mean distances from True are 2.28, 1.30, 1.41 for City Locations, Medical Indications, and Word Definitions, respectively. For Noise, the mean distances from True are 1.37, 1.30, 1.09 for City Locations, Medical Indications, and Word Definitions, respectively. The mean distances between Fictional and Noise are 2.38, 1.93, 1.58 for City Locations, Medical Indications, and Word Definitions, respectively. Fictional statements form their own representational cluster shaped not only by lexical differences but by their appearance in non-factual contexts during training. Taken together, these findings show clear decoupling between linguistic and latent-space similarity. Word Definitions exhibit substantial linguistic divergence between factual and fictional content, yet only moderate activation-space divergence. Conversely, City Locations show minimal linguistic differences but large representational separation. The geometry of LLM activations thus reflects both linguistic form and epistemic context. 5.2 Probe-level Changes under Label Perturbations We assess representational stability by retraining the probe with expanded definitions of True (i.e., by treating Synthetic, Fictional, Fictional(T), or Noise as True). Because the LLM activations remain fixed, shifts in the learned boundary reflects how the underlying veracity structure supports (or resists) linear reclassification. 9/25 Figure 7. Stability of probe predictions under label perturbations for Word Definitions data. Bar plots show, for each of the sixteen LLMs (x-axis), how often the sAwMIL probes predicted label changes when retrained under four perturbations: (a) Synthetic, (b) Fictional, (c) Fictional(T), and (d) Noise. Green bars indicate True to Not True flips, while purple bars indicate Not True to True flips. The left y-axis reports the number of statements with flipped predictions, and the right y-axis reports the corresponding proportions. The Synthetic perturbation leads to the most instability, with some LLMs retracting over 50% of their originally True statements. Figure 4 reports cosine similarity and intercept shifts between baseline and perturbed classifiers. Cosine similarity captures rotations of the True vs. Not True direction, while bias differences capture translational shifts. Across the datasets, Synthetic perturbations induce the largest boundary rotations, often nearing orthogonality in Word Definitions (Fig. 4(e),(f)). Fictional and Noise perturbations yield considerably smaller deviations. These results indicate that the baseline True vs. Not True direction is generally stable, but that adding unfamiliar yet semantically factual statements forces substantial reorientation. Synthetic statements therefore reveal where the veracity structure is most brittle. Although the global pattern is consistent, different LLM families exhibit different degrees of susceptibility to these perturbations. Chat-tuned variants (denoted by leading underscores) tend to exhibit somewhat larger rotations and bias shifts than their base models. An exception is gemma-7b, which shows unusually large shifts in the Word Definitions domain. Importantly, such rotations do not necessarily imply failures of predictive accuracy: large re-orientations can reflect either weak separation or greater dispersion within the underlying latent geometry (e.g., models with larger inter-type Wasserstein distances). Overall, across models and domains, Synthetic perturbations consistently produce the strongest probe-level instability. 5.3 Changes in Predicted Labels We examine how shifts in the truth boundary affect predicted veracity labels. Table 3 reports the percentage of statements whose predicted labels remain stable versus those that flip between True and Not True. City Locations is most stable (maximum flip rate 4.8%), Medical Indications shows intermediate instability (12.2%), and Word Definitions is substantially less stable (up to 40.6%). In all domains, Synthetic perturbations yield the highest flip rates, reinforcing that unfamiliar yet semantically fact-like statements are most disruptive to the learned veracity boundary. Figures 57 show flip patterns by model family. 10/ Chat-tuned models tend to produce more Not True to True flips, suggesting mild tendency toward overinclusiveness when the True class expands. Base models tend to show the opposite pattern. From -stability perspective, True to Not True flips represent stronger epistemic instability, as they retract previously assigned beliefs. However, these tendencies do not hold uniformly, and overall differences across LLM families are smaller than differences across perturbation types. These output-level results reveal consistent hierarchy of representational stability. Domains richly represented in training corpora (e.g., geography) exhibit the strongest representational stability; specialized but widely discussed domains (e.g., medicine) show moderate stability; and semantically flexible domains (e.g., definitions) are most vulnerable. Across all cases, Synthetic perturbations remain the dominant source of instability, indicating that unfamiliar yet factually plausible statements impose the greatest challenge for generating well-organized truth representations in LLMs."
        },
        {
            "title": "6 Discussion",
            "content": "Our results show that LLMs encode coherent but sometimes brittle separation of veracity in their internal activations. True, False, and familiar Fictional statements form well-defined clusters, while unfamiliar Synthetic statements sit near the boundary and disrupt it when relabeled as True. Because the activations remain fixed, these disruptions reflect genuine weaknesses in the underlying veracity geometry rather than artifacts of training or optimization. The mismatch between linguistic similarity and activation-space similarity across domains further indicates that representational stability is driven by epistemic familiarity, not linguistic-level features. central implication is that representational stability depends strongly on training-induced familiarity. Familiar Fictional statements appear in narrative contexts the LLMs have likely repeatedly encountered, enabling them to form stable, internally coherent clusters. Unfamiliar Synthetic statements lack any such anchoring: they resemble factual claims but violate the LLMs learned priors. As result, they induce the largest rotations and label flips, mirroring how humans show greater belief instability when confronted with novel but superficially plausible assertions. In this way, the probes decision boundary serves as diagnostic of the models epistemic landscape, revealing which statements the model encodes as grounded, merely recognized, or unsupported. Domain differences reinforce this picture. City Locations, which are richly represented and highly regular in training data, shows the tightest clustering and highest stability. Medical Indications, which are factual but context-sensitive, shows moderate stability. Word Definitions, which are semantically flexible and often usagedependent, show the weakest structure. This reflects broader patterns in factual generalization: LLMs inherit the uneven epistemic structure of their training corpora, and their capacity to encode veracity varies accordingly. Beyond characterizing these patterns, our method helps diagnose the representational sources of factual inconsistency. By perturbing the labeling of truth rather than modifying LLM parameters, we isolate which semantic shifts the underlying geometry will tolerate. This complements output-based factuality metrics: rather than asking whether model states the truth, we evaluate whether it represents truth in stable, well-organized fashion. Such diagnostics could guide data curation, finetuning objectives, or auditing procedures focused on improving epistemic reliability rather than linguistic-level accuracy. Finally, our results connect to philosophical accounts of -stability [19], which hold that rational beliefs should remain stable under small evidential changes. In our setting, reclassifying Neither statements that are logically consistent with the True statements should preserve the inferred truth boundary if the models epistemic representation is robust. Instead, Synthetic perturbations cause substantial reorientation. This highlights deeper challenge: distributional similarity alone does not guarantee that truth, falsity, and indeterminacy are encoded in forms that support stable inference. Addressing this may require training objectives or architectures that explicitly distinguish these epistemic categories. Future Work. Our analysis focuses on representational (and not behavioral) stability. We evaluate how truth directions emerge from fixed activations rather than how LLMs revise beliefs. Extending the framework to dynamic settings, such as after fine-tuning, reinforcement learning, or conversational interaction, may reveal how representational shifts propagate to behavior. Although sAwMIL [12] provides strong linear diagnostic, nonlinear or causal probes could uncover subtler dependencies among veracity, uncertainty, and linguistic form. Finally, our datasets center on factual and quasi-factual content; applying the method to disputed, opinion-based, counterfactual, or evolving claims would broaden the range of epistemic ambiguity and deepen our understanding of how LLMs encode belief."
        },
        {
            "title": "7 Conclusion",
            "content": "This study introduces representational stability as an approach for examining how LLMs internally encode and preserve distinctions between True, False, and Neither statements. By combining controlled label perturbations with representation-based probe, we show that LLMs display coherent but uneven geometry of truth. Factual representations are generally well structured, but plausible yet not True statements unseen during training (i.e., unfamiliar Synthetic Neither statements) most readily 11/25 disrupt that structure. These effects are consistent across architectures and domains, revealing that epistemic familiarity of the LLM with the content, rather than linguistic similarity or model scale, determines the stability of veracity representations. These findings underscore the importance of evaluating not only what LLMs output, but also the reliability of the veracity representations. Understanding and reinforcing this internal coherence offers path toward models that are not only accurate in response but also epistemically stable in representation."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Hannes Leitgeb and Branden Fitelson for discussions on -stability and how it might be related to epistemic uncertainty in LLMs. We also thank Zohair Shafi and Moritz Laber for their feedback and discussions on methodological and empirical portions of this work."
        },
        {
            "title": "Funding",
            "content": "This material was sponsored by the Government of the United States under Contract Number FA8702-15-D0002. The view, opinions, and/or filings contained in this material are those of the author(s) and should not be construed as an official position, policy, or decision of the Government of the United States or Carnegie Mellon University or the Software Engineering Institute unless designated by other documentation."
        },
        {
            "title": "Competing interests",
            "content": "The authors declare no competing interests."
        },
        {
            "title": "Data availability",
            "content": "True, False, and Synthetic statements are available at https://huggingface.co/datasets/carlomarxx/ trilemma-of-truth. Fictional statements are available https://huggingface.co/datasets/ The samanthadies/representational_stability. code used to generate the Noise activations can be found at https://github.com/samanthadies/ representational_stability. at"
        },
        {
            "title": "Code availability",
            "content": "The code used to generate activations, generate the Noise activations, and run the stability experiments can be found at https://github.com/samanthadies/ representational_stability."
        },
        {
            "title": "References",
            "content": "2. Turpin, M., Michael, J., Perez, E. & Bowman, S. Language models dont always say what they think: Unfaithful explanations in Chain-of-Thought prompting. Adv. Neural Inf. Process. Syst. 36, 74952 74965 (2023). 3. Han, J. et al. Simple factuality probes detect hallucinations in long-form natural language generation. In Christodoulopoulos, C., Chakraborty, T., Rose, C. & Peng, V. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2025, 1620916226. https://doi.org/10.18653/v1/ 2025.findings-emnlp.880 (Association for Computational Linguistics, 2025). 4. Liu, Y. et al. Trustworthy LLMs: survey and guideline for evaluating large language models alignment. In Socially Responsible Language Modelling Research. https://doi.org/10.48550/arXiv.2308.05374 (2023). 5. Suzgun, M. et al. Language models cannot reliably distinguish belief from knowledge and fact. Nat. Mach. Intell. 111. https://doi.org/10.1038/ s42256-025-01113-8 (2025). 6. Abbasi Yadkori, Y., Kuzborskij, I., György, A. & Szepesvari, C. To believe or not to believe your LLM: Iterative prompting for estimating epistemic uncertainty. Adv. Neural Inf. Process. Syst. 37, 5807758117 (2024). 7. Huang, L. et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Inf. Syst. 43, 155. https://doi.org/10.1145/3703155 (2025). 8. Elazar, Y. et al. Measuring and improving consistency in pretrained language models. Transactions Assoc. for Comput. Linguist. 9, 10121031. https://doi.org/10.1162/tacl_a_00410 (2021). 9. Li, Y., Miao, Y., Ding, X., Krishnan, R. & Padman, R. Firm or fickle? evaluating large language models consistency in sequential interactions. arXiv preprint arXiv:2503.22353 https://doi.org/10.48550/arXiv. 2503.22353 (2025). 10. Kadavath, S. et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221 (2022). 11. Bürger, L., Hamprecht, F. A. & Nadler, B. Truth is universal: Robust detection of lies in LLMs. Adv. Neural Inf. Process. Syst. 37, 138393138431 (2024). 1. AlKhamissi, B., Li, M., Celikyilmaz, A., Diab, M. & Ghazvininejad, M. review on language models as knowledge bases. arXiv preprint arXiv:2204.06031 https://doi.org/10.48550/arXiv.2204.06031 (2022). 12. Savcisens, G. & Eliassi-Rad, T. Trilemma of truth in large language models. In Mechanistic Interpretability Workshop at NeurIPS 2025 (2025). https: //openreview.net/forum?id=z7dLG2ycRf. 12/25 13. Marks, S. & Tegmark, M. The geometry of truth: Emergent linear structure in large language model representations of True/False datasets. arXiv preprint arXiv:2310.06824 https://doi.org/10. 48550/arXiv.2310.06824 (2024). 14. Wilie, B., Cahyawijaya, S., Ishii, E., He, J. & Fung, P. Belief revision: The adaptability of large language models reasoning. arXiv preprint arXiv:2406.19764 https://doi.org/10.48550/arXiv.2406.19764 (2024). 15. Xu, R. et al. The earth is flat because...: Investigating LLMs belief towards misinformation via persuasive conversation. In Ku, L.-W., Martins, A. & Srikumar, V. (eds.) Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1625916303. https: //doi.org/10.18653/v1/2024.acl-long.858 (2024). 16. Lu, Y., Bartolo, M., Moore, A., Riedel, S. & Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 80868098. https: //doi.org/10.18653/v1/2022.acl-long.556 (2022). 17. Wei, A., Haghtalab, N. & Steinhardt, J. Jailbroken: How does LLM safety training fail? Adv. Neural Inf. Process. Syst. 36, 8007980110 (2023). 18. Harding, J. Operationalising representation in natural language processing. Br. J. for Philos. Sci. https://doi.org/10.1086/728685 (2023). 19. Leitgeb, H. The stability theory of belief. Philos. review 123, 131171. https://doi.org/10.1215/ 00318108-2400575 (2014). 20. Conneau, A., Kruszewski, G., Lample, G., Barrault, L. & Baroni, M. What you can cram into single vector: Probing sentence embeddings for linguistic properties. arXiv preprint arXiv:1805.01070 https: //doi.org/10.48550/arXiv.1805.01070 (2018). 21. Hewitt, J. & Manning, C. D. structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 41294138. https://doi.org/10.18653/v1/N19-1419 (2019). 22. Tenney, I., Das, D. & Pavlick, E. Bert rediscovers the classical nlp pipeline. In Association for Computational Linguistics. https://doi.org/10.48550/arXiv. 1905.05950 (2019). 23. Sharma, M. et al. Towards understanding sycophancy in language models. In The Twelfth International Conference on Learning Representations. https://doi.org/10.48550/arXiv.2310.13548 (2024). 24. Herrmann, D. A. & Levinstein, B. A. Standards for belief representations in LLMs. Minds Mach. 35, 5. https://doi.org/10.1007/s11023-024-09709-6 (2024). 25. Angelopoulos, A. N., Bates, S. et al. Conformal prediction: gentle introduction. Foundations Trends Mach. Learn. 16, 494591. https: //doi.org/10.1561/2200000101 (2023). 26. Wikipedia contributors. List of fictional settlements (2025). https://en.wikipedia.org/wiki/List_ of_fictional_settlements. 27. Wikipedia contributors. List of fictional city-states in literature (2025). https://en.wikipedia.org/wiki/ List_of_fictional_city-states_in_literature. 28. Fandom NeoEncyclopedia. List of fictional diseases. https://neoencyclopedia.fandom.com/wiki/ List_of_fictional_diseases. 29. Fandom NeoEncyclopedia. List of fictional toxhttps://neoencyclopedia.fandom.com/wiki/ ins. List_of_fictional_toxins. 30. Chemeurope Encyclopedia. List of fictional medicines and drugs. https://www.chemeurope. com/en/encyclopedia/List_of_fictional_ medicines_and_drugs.html. 31. Tomasula, S. The Thackery T. Lambshead pocket guide to eccentric & discredited diseases. The Rev. Contemp. Fiction 24 (2004). 32. Almaden, S. A. 103 words made-up Dahl dictionary: list by Roald Dahl https://beelinguapp.com/blog/Dahl% of (2023). 20Dictionary:%20A%20List%20of%20103% 20Words%20Made-up%20By%20Roald%20Dahl. 33. Schleitwiler, P. & Shuflin, G. Dothraki initial text. https://conlang.org/language-creation-conference/ lcc5/1-dothraki-initial-text/. 34. Dict-Navi.com Online Dictionary. wordlist substantive (noun). https://dict-navi.com/en/dictionary/ list/?type=classification&ID=1. 35. Wolf, T. et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 38 45. https://doi.org/10.18653/v1/2020.emnlp-demos. 6 (2020). 13/"
        },
        {
            "title": "A Notation",
            "content": "We summarize the mathematical notation used throughout the manuscript in Table A1. Table A1. Notation. Summary of symbols used throughout the manuscript. Symbol Description given LLM. Number of statements in the dataset. = {si}N i=1 Set of natural-language statements. yi Ground-truth label of si: {True, False, Neither}. z(l) , yi)}N i=1 Full activationlabel dataset. Index of the LLM layer used for activation extraction. Activation vector for statement si at layer l. = {(z(l) Dtrain, Dcal, Dtest Train, calibration, and test splits of D. {+1, 1} Baseline binary label: +1 for True, and 1 for False/Neither. (z) = + Linear decision function of the baseline probe. Learned truth direction for the baseline probe. Bias term of the baseline probe. ˆyi Predicted label of the baseline probe for si. Btrue = {si z(l) Dtest, yi = True, ˆyi = +1} True statements classified as true (baseline belief set). Set of all Neither statements. N1, N0 Partition of : those re-labeled as True (N1) versus Not True (N0). {+1, 1} Perturbed binary label under given N1/N0 split. w, Truth direction and bias learned by the perturbed probe. true = {si z(l) Dtest, yi = True, ˆy ˆy Predicted label of the perturbed probe. = +1} True statements classified as true under the perturbed probe."
        },
        {
            "title": "B Data",
            "content": "B.1 Data Generation We use statements from the City Locations, Medical Indications, and Word Definitions datasets introduced in [12]. City statements take the form The city of [city] is (not) located in [country], (omitting The city of when redundant). Medical statements follow [drug] is (not) indicated for the treatment of [disease/condition]. Word Definition statements draw from three templates: [word] is (not) [instanceOf], [word] is (not) type of [typeOf], and [word] is (not) synonym of [synonym]. B.1.1 True, False, and Synthetic Statements We take the True, False, and Synthetic statements from the datasets introduced in [12]. All statements are constructed with both affirmative and negated forms. Synthetic entities are generated using Markov-chainbased name generator (namemaker4) and undergo multi-stage filtering, including database checks, model tagging, and web-search validation, to ensure no accidental overlap with real entities. Validated names are then paired to form grammatically coherent but semantically meaningless statements that follow each template. Because Synthetic entities do not exist and cannot have appeared in training corpora, LLMs have no basis for assigning them truth value. Accordingly, these statements function as Neither cases: unknown claims for which belief should be suspended rather than confidently classified as true or false. B.1.2 Fictional Statements In addition to Synthetic statements, which represent unseen and unknown claims, we construct new sets of Fictional statements for all three domains. Fictional statements also function as Neither statements in our experiments as they reference entities that do not exist in the real world and therefore lack real-world truth value. However, unlike Synthetic statements, many Fictional entities are likely to have appeared in LLM training corpora.5 As such, they represent complementary form of Neither: claims that an LLM may recognize, but that still lie outside the truefalse axis relevant to factual grounding. 4https://github.com/Rickmsd/namemaker 5For later analyses, we additionally annotate fictional statements with their within-universe factual status (Fictional True or Fictional False), but this labeling is not used in the primary True vs. Not True classification tasks. 14/ To ensure that Fictional statements remain genuinely non-factual, all terms were validated to exclude any realworld overlap, and fictional lexical items appearing in any natural language were excluded to prevent misinterpretation by multilingual LLMs. Fictional statements were then constructed using the same templates as the True, False, and Synthetic statements, including both affirmative and negated forms. Fictional City Locations. Fictional cities and countries are sourced from [26, 27], spanning literature, film, radio, television, comics, animation, and games. Each city, location pair is included only when an identifiable enclosing region exists. When multiple spatial resolutions are available, we select the most specific (e.g., Quahog, Rhode Island rather than Quahog, United States). Fictional Medical Indications. Fictional drug and disease statements are drawn from (1) NeoEncyclopedia Wiki [28, 29]; (2) ChemEuropes List of Fictional Medicines and Drugs [30]; and (3) The Thackery T. Lambshead Pocket Guide to Eccentric & Discredited Diseases [31]. Drugdisease pairs are included when treatment relationship exists within the fictional source. Fictional Word Definitions. Fictional lexical items are compiled from (1) Gobblefunk [32]; (2) Dothraki [33]; and (3) Navi [34]. Dothraki and Navi have formal linguistic structure, whereas Gobblefunk is playful neologistic extension of English. B.1.3 Noise The Noise statements contains no linguistic content. We generate nnoise = 0.10 random activation sequences by sampling from multivariate Gaussian with per-feature mean, standard deviation, and sequence-length distribution matched to the LLM activations. These distributionally consistent but non-semantic sequences serve as control, allowing us to test whether observed representational differences arise from semantic content or from statistical variation in activation space. B.2 Data Splits for Probing Experiments Table A2. Dataset splits. The number of statements used in training, calibration, and testing of the probe. The proportion of total statements is reported in parentheses. Dataset City Locations Medical Indications Definitions Train 4746 (0.54) 4636 (0.55) 6488 (0.54) Calibration 1772 (0.20) 1721 (0.20) 2514 (0.21) Test 2229 (0.25) 2121 (0.25) 3041 (0.25) Total 8747 (1.00) 8478 (1.00) 12043 (1.00) Table A2 summarizes the dataset partitions used for all probing experiments. Each dataset is split exclusively into training, calibration, and test sets to prevent data leakage. Approximately 55% of statements are used for training, 20% for calibration, and 25% for testing. We use identical splits across all probes and all LLMs, enabling direct comparison of representational stability under matched data conditions."
        },
        {
            "title": "C LLMs",
            "content": "Table A3 lists the sixteen open-source LLMs used in our stability experiments. The set spans four major model families, Gemma, Llama, Mistral, and Qwen, with between about 3 billion to about 15 billion parameters and release dates between February and September 2024. For each family, we include both base (pre-trained) and chat-tuned variants to capture differences introduced by instruction fine-tuning. Together, these models provide representative cross-section of current decoder-only architectures varying in scale, origin, and training objectives. Representations of True, False, and Neither by LLM Supplementary Figures A1-A16 show the pairwise activation distance matrices for all sixteen LLMs. Three general representational patterns emerge. The first, observed in _gemma-2-9b (Fig. A1) and gemma-2-9b (Fig. A10), shows Fictional and Synthetic statements clustering near True and False statements, with Noise forming distinct outlier. The second, present in _gemma-7b (Fig. A2), gemma-7b (Fig. A10), _qwen-2.5-14b (Fig. A8), qwen-2.5-14b (Fig. A15), and _qwen-2.5-7b (Fig. A9), exhibits Synthetic statements close to True and False, Fictional statements clearly separated, and Noise positioned slightly closer to the True/False/Synthetic cluster. The third, seen in the remaining nine models, features Synthetic statements aligned with True and False, while 15/25 Table A3. LLMs used in the stability experiments. We list the official names of the LLMs according to the HuggingFace repository [35]. We further specify the shortened name we use to refer to each of the models, whether it is the base, pre-trained model or chat-tuned version, the nubmer of decoders, the number of parameters, the release date, and the source of the model. Finally, we report the layers with the best separation between True and Not True statements for the City Locations (C), Medical Indications (M), and Word Definitions (W) datasets. The LLMs are publicly available through HuggingFace [35]. Official Name Gemma-7b Gemma-2-9b Llama-3-8b Llama-3.2-3b Mistral-7B-v0.3 Qwen2.5-7B Qwen2.5-14B Gemma-7b-it Gemma-2-9b-it Llama-3.2-3b-Instruct Llama-3.1-8b-Instruct Llama3-Med42-8b Bio-Medical-Llama-3-8b Mistral-7b-Instruct-v0.3 Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Type # Decoders # Parameters Short Name Base gemma-7b Base gemma-2-9b Base llama-3-8b Base llama-3.2-3b Base mistral-7B-v0.3 Base qwen-2.5-7b Base qwen-2.5-14b Chat _gemma-7b Chat _gemma-2-9b Chat _llama-3.2-3b Chat _llama-3.1-8b Chat _llama-3-8b-med Chat _llama-3-8b-bio _mistral-7B-v0.3 Chat Chat _qwen-2.5-7b Chat _qwen-2.5-14b 8.54 9.24 8.03 3.21 7.25 7.62 14.80 8.54 9.24 3.21 8.03 8.03 8.03 7.25 7.62 14.80 28 26 32 28 32 28 38 28 26 28 32 32 32 32 28 38 Release Date Source Google Feb 21, 2024 Google Jun 27, 2024 Jul 23, 2024 Meta Sep 25, 2024 Meta Best Layer C: 14, M: 19, W: 17 C: 24, M: 25, W: 23 C: 18, M: 17, W: 17 C: 16, M: 17, W: 15 C: 18, M: 17, W: 18 May 22, 2024 Mistral AI C: 18, M: 19, W: 17 C: 30, M: 31, W: 30 C: 19, M: 19, W: 17 C: 27, M: 26, W: 25 C: 16, M: 19, W: 18 C: 18, M: 19, W: 18 C: 18, M: 16, W: 15 C: 18, M: 19, W: 18 C: 19, M: 21, W: 18 May 22, 2024 Mistral AI C: 19, M: 21, W: 18 C: 31, M: 34, W: 30 Sep 19, 2024 Sep 19, 2024 Feb 21, 2024 Jul 27, 2024 Sep 25, 2024 Meta Jul 23, 2024 Meta Aug 12, 2024 M42 Health Aug 11, Aug 18, 2024 Aug 18, 2024 Alibaba Cloud Alibaba Cloud Google Google Alibaba Cloud Alibaba Cloud Contact Doctor Figure A1. Wasserstein distance between activations for _gemma-2-9b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Noise has distinct representations, but Fictional and Synthetic statements are represented similarly to True and False statements and each other. both Fictional and Noise statements occupy distinct and distant regions. Except for _qwen-2.5-7b (which follows the second pattern; Fig. A9) and qwen-2.5-7b (the third; Fig. A16), base and chat versions of each model display qualitatively similar representational structures. 16/25 Figure A2. Wasserstein distance between activations for _gemma-7b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements are represented distinctly from all other statements. Figure A3. Wasserstein distance between activations for _llama-3-8b-med. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. 17/25 Figure A4. Wasserstein distance between activations for _llama-3-8b-bio. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. Figure A5. Wasserstein distance between activations for _llama-3.1-8b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. 18/25 Figure A6. Wasserstein distance between activations for _llama-3.2-3b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. Figure A7. Wasserstein distance between activations for _mistral-7B-v0.3. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. 19/25 Figure A8. Wasserstein distance between activations for _qwen-2.5-14b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements are represented distinctly from all other statements. Figure A9. Wasserstein distance between activations for _qwen-2.5-7b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements are represented distinctly from all other statements. 20/25 Figure A10. Wasserstein distance between activations for gemma-2-9b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Noise has distinct representations, but Fictional and Synthetic statements are represented similarly to True and False statements and each other. Figure A11. Wasserstein distance between activations for gemma-7b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements are represented distinctly from all other statements. 21/25 Figure A12. Wasserstein distance between activations for llama-3-8b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. Figure A13. Wasserstein distance between activations for llama-3.2-3b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. 22/25 Figure A14. Wasserstein distance between activations for mistral-7B-v0.3. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. Figure A15. Wasserstein distance between activations for qwen-2.5-14b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements are represented distinctly from all other statements. 23/25 Figure A16. Wasserstein distance between activations for qwen-2.5-7b. Pairwise Wasserstein distances between activation distributions of True, False, Synthetic, Fictional, and Noise statements for the (a) City Locations, (b) Medical Indications, and (c) Word Definitions datasets. Synthetic statements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements."
        },
        {
            "title": "E Exploring Additional Probes",
            "content": "We repeated the label perturbation experiments using the Mean Difference probe proposed by Marks and It estimates truth direction by taking the vector difference between the mean activation Tegmark [13]. of True statements and that of False statements, optionally scaled by the inverse covariance matrix of the data. This approach is inherently sensitive to differences in the centroids and covariance structure of the data, which leads to strong instability in the learned decision boundary when Neither statements are included alongside true and false examples. The Mean Difference probe show considerably greater variability across LLMs than sAwMIL (Fig. A17). While sAwMIL yields consistent decision boundary rotation corresponding to specific perturbations, particularly the Synthetic perturbation, the Mean Difference probe exhibits near-orthogonal boundary shifts for certain LLMs regardless of perturbation. In addition, Table A4 shows that, unlike with sAwMIL, the Fictional perturbation produces the largest number of prediction flips across all three datasets, and the Word Definitions dataset exhibits the fewest total flips. We interpret these discrepancies as artifacts of the Mean Difference probes reliance on dataset centroids: when statement activations are well separated, as with Fictional statements, class-label perturbations can induce disproportionately large changes in the estimated decision boundary. This instability reflects probe sensitivity rather than genuine representational instability in the LLMs. Accordingly, the Mean Difference probe is less well suited for quantifying representational stability than sAwMIL. Table A4. Flipped predictions under label perturbations for the Mean Difference Probe. Counts (and percentages) of predictions that remain stable or flip between True and Not True across Synthetic, Fictional, Fictional(T), and Noise perturbations for each dataset. Word Definitions is the most stable, followed by City Locations and Medical Definitions. The Fictional perturbation leads to the most instability. Dataset City Locations Medical Locations Word Definitions Perturbation True to True Synthetic Fictional Fictional (T) Noise Synthetic Fictional Fictional (T) Noise Synthetic Fictional Fictional (T) Noise 9724 (97.2) 7386 (73.9) 9680 (96.8) 9653 (96.5) 9457 (86.8) 4694 (43.1) 9718 (89.2) 8955 (82.1) 8468 (85.6) 7035 (71.1) 8282 (83.8) 8208 (83.0) Not True to Not True 167 (1.7) 221 (2.2) 184 (1.8) 201 (2.0) 894 (8.2) 977 (9.0) 1069 (9.8) 1026 (9.4) 500 (5.1) 1175 (11.9) 1083 (11.0) 1178 (11.9) Not True to True 63 (0.6) 9 (0.1) 46 (0.5) 29 (0.3) 221 (2.0) 138 (1.3) 46 (0.4) 89 (0.8) 695 (7.0) 20 (0.2) 112 (1.1) 17 (0.2) True to Not True Total Flips 46 (0.5) 2384 (23.8) 90 (0.9) 117 (1.2) 324 (3.0) 5087 (46.7) 63 (0.6) 826 (7.6) 225 (2.3) 1658 (16.8) 411 (4.2) 485 (4.9) 109 (1.1) 2393 (23.9) 136 (1.4) 146 (1.5) 545 (5.0) 5225 (48.0) 109 (1.0) 915 (8.4) 920 (9.3) 1678 (17.0) 523 (5.3) 502 (5.1) 24/25 Figure A17. Change in Mean Difference decision boundaries under perturbations. Cosine similarity (left column) and bias difference (right column) between the baseline True vs. Not True probe and probes retrained under label perturbations for the (a,b) City Locations, (c,d) Medical Indications, and (e,f) Word Definitions datasets. Each heatmap shows results for sixteen LLMs (columns) and five perturbation conditions (rows). LLMs with leading underscores are chat models, while those without are base models. Higher cosine similarity indicates smaller rotations of the learned decision boundary, while bias difference reflects shifts in intercept. Certain LLMs lead to near orthogonal perturbed decision boundaries across all perturbation types, suggesting that, unlike sAwMIL, the probe is highly sensitive to differences in the distributions of the underlying activations. 25/"
        }
    ],
    "affiliations": [
        "Khoury College of Computer Sciences, Northeastern University",
        "Network Science Institute, Northeastern University",
        "Santa Fe Institute"
    ]
}