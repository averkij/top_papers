{
    "paper_title": "Diversity Has Always Been There in Your Visual Autoregressive Models",
    "authors": [
        "Tong Wang",
        "Guanyu Yang",
        "Nian Liu",
        "Kai Wang",
        "Yaxing Wang",
        "Abdelrahman M Shaker",
        "Salman Khan",
        "Fahad Shahbaz Khan",
        "Senmao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR."
        },
        {
            "title": "Start",
            "content": "Tong Wang1,2 Guanyu Yang1 Nian Liu2 Kai Wang3 Yaxing Wang4 Abdelrahman M. Shaker2 Fahad Shahbaz Khan2 Salman Khan2 Senmao Li4,2 1Southeast University 2MBZUAI 3City University of Hong Kong 4Nankai University 5 2 0 2 1 2 ] . [ 1 4 7 0 7 1 . 1 1 5 2 : r Figure 1. Multiple generation samples from the vanilla VAR models (1st and 3rd rows) and our DiverseVAR (2nd and 4th rows). While vanilla VAR models suffer from the diversity collapse, our method generates more diverse outputs while maintaining imagetext alignment. The text prompts used are as follows: man in clown mask eating donut, cat wearing Halloween costume, Golden Gate Bridge at sunset, glowing sky, ..., palace under the sunset, cool astronaut floating in space, and cat riding skateboard down hill."
        },
        {
            "title": "Abstract",
            "content": "Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. https://github.com/wangtong627/DiverseVAR 1 1. Introduction Autoregressive (AR) models [14] have emerged as powerful next-token prediction paradigm [3, 5], achieving competitive results in visual generation but suffering from substantial generation latency caused by their numerous sequential decoding steps. To alleviate the latency, pioneering studies investigated various representation patterns [6 8] and parallel decoding [9, 10]. Recently, unlike conventional next-token prediction in AR models, visual autoregressive (VAR) models adopt next-scale prediction paradigm [11, 12], achieving efficient and high-quality image generation within approximately ten scale steps. However, although VAR models perform inference with far fewer steps than traditional AR models, they suffer from the diversity collapse problem under numerous scenarios (Fig. 1, 1st, 3rd and 5th rows). In this work, we address the diversity collapse problem in VAR models by unlocking their inherent generative diversity. Through detailed analysis of the coarse-to-fine next-scale prediction process in pretrained VAR models, we arrive at the following key observations. (a) Structural Formation in Early Scales: As shown in Figs. 2 and 3, structural formation predominantly occurs at early scales, while later scales mainly refine and stabilize existing structures. This finding directs our attention toward the early-stage pre- (b) Diversity Governed by the Pivotal diction dynamics. Token: As demonstrated in Figs. 4 and 5, the pivotal token dominates the formation of structure while the auxiliary token carries both semantic information and image fidelity. Based on the these insights, we propose DiverseVAR, training-free framework that effectively unleashes the inherent generative diversity of VAR models. Without any additional training, DiverseVAR strategically intervenes during the inference process, guided by our analytical findings. Our approach consists of two complementary sampling steps: (a) Soft-Suppression Regularization: We identify the pivotal component of the model input that governs diversity formation and apply soft-suppression strategy to attenuate components contributing to structural redundancy, thereby mitigating diversity collapse. (b) SoftAmplification Regularization: To further promote controlled diversity, we enhance the pivotal component of the model output through soft-amplification mechanism, ensuring that the expanded diversity remains consistent textimage alignments. Together, these two regularizations enable DiverseVAR to unleash the inherent generative potential of VAR models while preserving high image fidelity and faithful semantic alignment. Our main contributions are summarized as follows: We conduct systematic analysis to identify the scale factors that govern the diversity collapse problem in VAR models. Our findings reveal that the VAR model diversity is predominantly influenced by the pivotal component at early scales. Based on our observations, we introduce DiverseVAR, simple yet effective training-free method to enable the emergence of the inherent generative diversity of VAR It primarily incorporates two regularization models. terms during VAR inference: soft-suppression regularization and soft-amplification regularization. Experiments on the COCO, GenEval, and DPG benchmarks show that our method DiverseVAR unleashes the VAR model diversity while maintaining the sampled image fidelity, with slight influences on the generation performance according to numerous evaluation metrics. 2. Related Wrok 2.1. Visual Autoregressive Generation Traditional autoregressive (AR) methods [1, 3, 4] follow the next-token prediction paradigm, which requires large number of iterative steps to generate high-quality images. Recently, Visual Autoregressive (VAR) modeling [11] has adopted next-scale prediction paradigm, enabling progressive generation across different resolutions to produce highquality images. In contrast to AR methods that require massive steps to produce high-resolution image [1, 3, 4], VAR can achieve comparable quality within only around ten steps [1114]. Despite the promising performance of VAR models, current VAR models remain exhibit notable gap: their generated samples for given text prompt tend to be highly similar. Such indicated limited diversity is termed as diversity collapse problem in this paper. 2.2. Alternative Visual Generation Paradigms Diffusion models [15, 16] trained on large-scale text-image datasets [17] are able to generate high-quality and diverse images, but they face time-consuming challenge due to multi-step sampling. To address this challenge, distillation techniques [1821] have been applied to diffusion models, enabling few-step sampling in distilled student models. Despite the improved sampling speed, the resulting images often exhibit diversity collapse [22]. The exploration of diverse generation has recently attracted increasing attention in the context of few-step diffusion models. For example, Diffusion2GAN [22] aligns its generation trajectory with that of the teacher diffusion model, effectively reducing diversity collapse. Loopfree [23] employs 4-step parallel decoder in the UNet architecture combined with Variational Score Distillation (VSD) [24] to generate images that exhibit both high diversity and fidelity. Hybrid [25] uses the base model only for the first step to seed diversity, and then switches to the distilled model for subsequent generation. C3 [26] enhances generative diversity by amplifying internal feature representations using automatically selected amplification factors. Despite improving diversity, 2 Figure 2. Visualization of samples across all scales (1st row) and their associated DINO features (2nd row). models to achieve higher generative diversity while maintaining generation fidelity (Sec. 3.3). 3.1. Preliminaries Visual Autoregressive (VAR) modeling [11] reformulates the conventional autoregressive (AR) framework [14] to the visual domain by transitioning from next-token to nextscale prediction. Under this paradigm, each autoregressive step produces token map corresponding to specific resolution scale, instead of predicting tokens one by one. Given an image RHW 3, continuous image feature map Rhwd is obtained through visual tokenizer. VAR then quantizes this feature map into multi-scale token maps = (R1, R2, . . . , RK), where each token map corresponds to predefined spatial resolution (hk, wk) for = 1, . . . , K. Here, (hK, wK) denotes the final scale, which is identical to the spatial size (h, w) of the feature map . This sequence of residuals progressively approximates the continuous feature map as: = (cid:88) i=1 up(Ri, (h, w)), (1) where up() denotes the upsampling operation. The transformer autoregressively predicts the next-scale conditioned on the previously generated residuals, with the overall likelihood formulated as: p(R1, . . . , RK) = (cid:89) k=1 p(Rk R1, . . . , Rk1). (2) The transformer block in VAR takes the text embeddings at the first scale as input to predict R1. In the subsequent k-th scale, the feature map from the previous (k-1)-th scale is downsampled to match the spatial resolution of the input at the k-th scale: (cid:101)F k1 = down(F k1, (hk, wk)), (3) where down() denotes the downsampling operation, and the transformer block receives the downsampled feature (cid:101)F k1 as input to predict Rk. (Left) Statistics of structure evolution on all scale Figure 3. steps. (Right) The relative log amplitude of frequency components across different scales. the requirement for training remains bottleneck [22, 23], the simultaneous use of both teacher and student models further increases memory consumption [25], and depend on creativity-oriented prompts to stimulate diverse outputs [26]. By contrast, AR models often demonstrate strong generative diversity [27]. Recent researches [28, 29] focus on enhancing the balance between content quality and diversity, rather than specifically addressing diversity in AR models. In contrast to conventional AR models, VAR performs next-scale prediction to enable efficient generation, but suffers from diversity collapse. Nevertheless, prior efforts addressing diversity in diffusion and AR models are not directly transferable to VAR, and exploration of diversity in VAR remains scarce. In this work, we conduct systematic analysis to identify which scales govern diversity in VAR models. Building on these observations, we further characterize the properties of these scales that are closely associated with diversity. Finally, leveraging these properties, we propose diversity enhancement technique for VAR inference that preserves both semantic consistency and visual fidelity. 3. Method VAR revisits the conventional next-token prediction paradigm in AR image generation and introduces coarseto-fine next-scale prediction framework (Sec. 3.1), which substantially accelerates sampling while generating highquality images, yet suffers from diversity collapse. In this section, we explore how the mechanisms underlying the coarse-to-fine generation process affect diversity (Sec. 3.2). Building on these insights, we propose our method DiverseVAR that fosters the emergence of diversity, enabling VAR 3 Figure 4. Visualization of samples when zeroing out the pivotal (1st row) or auxiliary (2nd row) tokens across all scales except the 1st scale (1st12th columns), along with the vanilla generation results (last column). mation. The curve then converge at the remaining scales, implying that the structure has already stabilized. We also employ LPIPS [35] and DISTS [36] to evaluate structural formation. Fig. 3 (Left) shows that the curves of both LPIPS and DISTS exhibit similar trend to that of the DINO structure distance. Furthermore, we perform frequency-domain analysis of the DINO features to evaluate the progression of structural formation (Fig. 3 (Right)). The frequency components decrease sharply at the early scales and gradually converge at the later scales, indicating that they become more stable and consistent as the scale increases. In conclusion, the early scales can be exploited to modify the structure, thereby promoting the emergence of diversity. Observation 2: Diversity Governed by Pivotal Token. The above observation and analysis indicate that structural formation primarily occurs at early scales. This finding motivates us to further examine the components that influence this formation. Thus, we investigate the internal composition of the feature map (cid:101)F k1 at scale (Eq. (3)) by dividing it into pivotal and auxiliary components. Specifically, we follow [37] and define the pivotal score sk,i = (cid:101)F k1,i k12 using the L2 norm, where k1 represents the mean feature map obtained by averaging (cid:101)F k1 across scale dimensions, and (cid:101)F k1,i denotes each token along the scale dimension. Based on their pivotal scores, all tokens in the feature map are ranked and then categorized into pivotal and auxiliary tokens, with the partition determined by the elbow point computed using the Maximum Distance to Chord (MDC) method [38] (See Suppl. for details). We simply regard the pivotal and auxiliary tokens as the pivotal and auxiliary components, and refer to this straightforward partition approach as Naive Component Partition (NCP). We observe that the pivotal token primarily contribute structural formation, whereas the auxiliary token carry both semantic information and image fidelity. For example, when provided with the text prompt bird perched on tree branch in the rain, VAR generates an image consistent with the given description (Fig. 4 (last column)). When zeroing out the pivotal token at one of the early scales (e.g., scale 4 within scales 28), the generated image exhibits noticeable structural changes, while its semantic meaning remains consistent (Fig. 4 (1st row, 1st-4th columns). In conFigure 5. Structural (Left) and semantic (Right) evaluation when pivotal and auxiliary tokens are zeroed out. 3.2. Motivation In this subsection, we present the motivation for our proposed method, based on two key observations1 derived from vanilla text-to-image generative models [12]. First, the structural formation in vanilla next-scale models occurs at early scales, motivating us to explore the mechanisms that influence model diversity during these early scales (Figs. 2 and 3). Second, model diversity is primarily influenced by the pivotal token at early scales, presenting an opportunity to further promote the emergence of diversity while preserving generation fidelity (Figs. 4 and 5). Observation 1: Structural Formation in Early Scales. We experimentally observe that the structural formation emerges at early scales, whereas later scales exhibit already stabilized structures (Figs. 2 and 3). More specifically, given pretrained VAR model, it progressively produces intermediate predictions Rk and constructs the corresponding feature maps (Eq. (1)) as the scale gradually increases (Fig. 2 (1st row)). Motivated by [3133], we adopt DINO features [34] as representation of the structural characteristics of the feature maps (Fig. 2 (2nd row)). As illustrated in Fig. 2 (2nd row), the structural formation of the image persists up to specific scale (i.e., 12), after which the structure stabilizes. Here, we further utilize DINO structure distance [31, 32] to measure whether the feature maps exhibit well-formed overall structure comparable to that of the final scale  (Fig. 3)  . As illustrated in Fig. 3 (Left), the curve quickly fall below 0.2 at early scales (i.e., 12), indicating ongoing structural for1The observations are drawn by statistical experiments on generated images, using 100 random text prompts from the COCO dataset [30]. 4 Figure 6. The dominant singular values correspond to the pivotal component influence generative diversity trast, zeroing out the auxiliary token at one of these early scales severely degrades both semantic information and image fidelity (Fig. 4 (2nd row, 1st-4th columns). Furthermore, we employ DISTS [36] and SigLIP [39] to quantitatively evaluate the modified images in comparison with the vanilla VAR outputs. As shown in Fig. 5, DISTS and SigLIP display consistent trends at early scales: zeroing out pivotal token (solid lines) results in slight degradation, while zeroing out auxiliary token (dashed lines) results in drastic deterioration in both metrics. For example, when zeroing out the pivotal token, the variances of DISTS and SigLIP are below 0.3 and 0.1, respectively, whereas the corresponding values for the auxiliary token exceed 0.5 and 0.4. Finally, when zeroing out either the pivotal or auxiliary token at one of the later scales (e.g., scales 16-64), both the structural and semantic characteristics remain consistent with those of the vanilla generation (Fig. 4 (6th-12th columns) and Fig. 5). These results indicate that the models generative diversity is predominantly governed by the pivotal token at early scales. However, using the Naive Component Partition (NCP) to identify the pivotal component and apply its suppression (i.e., zeroing out) can promote the emergence of generative diversity, yet this naive approach may also induce unexpected variations (Fig. 4, 1st row, 2nd and 3rd columns). 3.3. Diverse Visual Generation for VAR Based on all the above observations, we propose DiverseVAR for VAR to trigger the emergence of generative diversity while preserving generation fidelity in the VAR inference (Fig. 8 and Algorithm 1 (See Suppl.)). As shown in Fig. 8 (Left), the vanilla VAR inference processes the intermediate feature (cid:101)F k1 through VAR block to produce the output feature as well as its predicted logits, which is subsequently quantized into Rk (The predicted logits, the quantization step, and Rk are omitted in Fig. 8 for brevity). Our method operates at the block level (e.g., 8 blocks in the Infinity backbone). Diversity emergence via Pivotal Component. For the Figure 7. (Top) The generated images often fail to reflect the number described in the text prompt when using only SSR. For example, given the prompt hot air balloon floating above the clouds, SSR fails to generate the correct quantity of the hot air balloon (2nd row). (Bottom) The logits distribution under different samplings of vanilla model (Left), SSR (Middle), and SSR+SAR (Right). pivotal component, it is difficult to disentangle it within the pivotal token, as the self-attention mechanism in Transformer-based VAR models inherently entangles information across tokens [12, 40] in the feature map (cid:101)F k1. Inspired by [41], we assume that the dominant singular values of the feature map (cid:101)F k1 correspond to the fundamental information, i.e., the pivotal component. To avoid directly using the pivotal token as the pivotal component, we instead regard the dominant singular values as determining the pivotal component. Specifically, given the feature map (cid:101)F k1 RSkD at the k-th (early) scale 2, we decompose it using Singular Value Decomposition (SVD): (cid:101)F k1 = UΣVT , where Σ = diag(σ1, , σn), the singular values σ1 σn, = min(Sk, D). Then, our objective is to suppress the pivotal component, thereby triggering the emergence of generative diversity. To suppress the pivotal component of the feature map (cid:101)F k1, we introduce the Soft-Suppression Regularization (SSR) for each singular value, which is formulated as: ˆσ = αeβσ σ. (4) Here, represents the exponential function, while α and β are parameters constrained to be positive. For brevity, the subscript of σ is omitted. The feature map is then reconstructed as ˆF k1 = ˆΣVT , with ˆΣ = diag(ˆσ1, . . . , ˆσn) 2i.e., Sk = hk wk, = 2048 in the Infinity model [12] 5 Figure 8. The overall framework of DiverseVAR. We explore the diversity emergence of VAR models at the early scales, while retaining the original VAR inference process at the later scales. denoting the updated singular values. The feature map ˆF k1 is passed through block to generate the output ˆF k. In special case, we reset the top-K singular values to 0 (here, K=2). Interestingly, as shown in Fig. 6 (2nd row), removing the dominant components leads to improved generative diversity but reduces image fidelity. This supports our assumption that the dominant singular values of the feature map correspond to the pivotal component that influence generative diversity (Fig. 6 (2nd row)), and also shows that our method suppresses the pivotal component more effectively than directly zeroing it out (Fig. 6 (3rd row)). Guided Diversity Formation. Soft-Suppression Regularization facilitates diversity emergence via the pivotal component. However, while enhancing diversity, it weakens the alignment with the text semantics, especially in cases involving numerical descriptions (Fig. 7 (Top (2nd row))). Motivated by [28], we analyze the distribution of the predicted logits for the output feature to understand the underlying cause of this phenomenon. We observe that the logits distributions under different samplings are similar in the vanilla VAR (Fig. 7 (Bottom (Left))), while SoftSuppression Regularization leads to more dispersed logits distributions across different samplings (Fig. 7 (Bottom (Middle))). Specifically, the logits distributions observed in the vanilla VAR reveal that the probability peaks of different samplings nearly coincide (Fig. 7 (Bottom (Left))), leading to highly similar samples and lack of diversity In contrast, incorporating Softin the generated images. Suppression Regularization yields more dispersed logits distributions (Fig. 7 (Bottom (Middle))), with peaks varying significantly in both position and magnitude, thereby promoting the emergence of diversity in the generated images. However, we observe that independent and very high probability peaks can distort the representation of numerical attributes, leading to suboptimal image-text alignment. Here, we aim to further improve image-text alignment, especially reducing inaccuracies in numerical attributes, while maintaining generative diversity. Based on the above analysis, we introduce an augmentation for the output feao ture ˆF to further guide the formation of diversity. Specifically, we perform singular value decomposition (SVD) on ˆF k, yielding singular values ˆσ1 ˆσn. We then augment each singular value according to the following rule, referred to as Soft-Amplification Regularization (SAR): σ = ˆαe ˆβ ˆσ ˆσ, (5) where ˆα and ˆβ are positive parameters controlling the scaling strength. The recovered feature = (cid:101)ΣVT , where (cid:101)Σ = diag(σ1, , σn). The Soft-Augmentation Regularization, by encouraging more dispersed logit distributions and avoiding the formation of isolated peaks across different samplings, can further guide the formation of diversity (Fig. 7 (Top (3rd row) and Bottom (Right))). 4. Experiment 4.1. Experimental Setup Evaluation Datasets and Metrics. We evaluate DiverseVAR on the text-to-image VAR models Infinity-2B and Infinity-8B [12], generating images at resolution of 10241024. Following the setup of the vanilla Infinity-2B and Infinity-8B models [12], we evaluate DiverseVAR on two widely used benchmarks [12, 42, 43]: GenEval [44] and DPG [45]. We further evaluate the generative diversity of our method in comparison with the vanilla model on the zero-shot text-to-image benchmarks COCO 2014 and COCO 2017 [30]. On the COCO 2014 benchmark, we adopt the conventional evaluation protocol [15, 4648], using the 30K text prompts selecte by GigaGAN [49] to generate 30K corresponding images. On the COCO 2017 benchmark, we generate 5K images from the 5K provided text prompts. Following prior works [26, 50], we evaluate the generative diversity of the synthesized images using Frechet Inception Distance (FID) [51], Recall [52], and Coverage (Cov.) [53]. To assess the text-image alignment, we em-"
        },
        {
            "title": "Dataset",
            "content": "COCO2014-30K COCO2017-5K"
        },
        {
            "title": "AFHQ",
            "content": "CelebA-HQ Method Recall Cov. FID CLIP Recall Cov. FID CLIP Method Recall Cov. FID CLIP Recall Cov. FID CLIP +Ours Infinity-2B 0.316 0.651 28.48 0.313 0.408 0.832 39.01 0.313 0.385 0.690 22.96 0.313 0.480 0.860 33.39 0.313 Infinity-8B 0.451 0.740 18.79 0.319 0.563 0.892 29.47 0.319 0.497 0.748 14.26 0.315 0.585 0.892 25.01 0.316 +Ours Infinity-2B 0.0 +Ours Infinity-8B 0.0 0.0 +Ours 0.007 91.85 0.276 0.003 0.007 81.16 0.259 0.012 0.020 78.88 0.278 0.017 0.128 62.07 0.257 0.0 149.95 0.241 0.013 139.73 0.235 0.001 126.47 0.271 0.005 109.73 0. 0.0 0.0 Table 1. Quantitative comparison between the vanilla model and our DiverseVAR on COCO2014 (30K prompts) and COCO2017 (5K prompts), evaluated using FID, Recall, and Coverage (Cov.) for generative diversity, and CLIPScore (CLIP) for textimage alignment. ploy CLIPScore (CLIP) [54], where ViT-B/32 is used as the backbone for feature extraction. Implementation Details. Following our observations that diversity formation primarily occurs at early scales, we apply our method at scales {4, 6} to unleash diversity, while retaining the vanilla inference process in the remaining scales {1, 2, 8, 12, 16, 20, 24, 32, 40, 48, 64} to maintain fidelity. Our operation is applied at the block level, spanning all 8 blocks of the Infinity backbone. We set the parameters as α = 1.0 and β = 0.01 in Eq. (4), and ˆα = 1.0 and ˆβ = 0.001 in Eq. (5). All experiments are performed on single NVIDIA A100 GPU equipped with 40 GB of memory. See Suppl. for details. 4.2. Main Results Diversity Comparison. To demonstrate the generative diversity of our DiverseVAR, we report quantitative evaluations using Recall, Coverage, and FID. Generally, Recall and Coverage are commonly used to assess the diversity of generated samples by comparing the support of real and generated data distributions [50, 53]. As reported in Tab. 1, our method achieves higher Recall, Coverage, and FID than the vanilla Infinity on both benchmarks, while maintaining comparable CLIP scores. Fig. 1 shows generations from the vanilla Infinity and our method, qualitatively demonstrating that our approach produces more diverse outputs. Furthermore, to evaluate the diversity under multiple samplings, we use booth AFHQ [55] and CelebA-HQ [56]. We use three prompts corresponding to the three categories of AFHQ and two prompts corresponding to CelebA-HQ, each generating approximately 5,000 images. During evaluation, the training sets of both AFHQ and CelebA-HQ are used as the ground truth. As reported in Tab. 2, compared with the vanilla Infinity-2B and Infinity-8B, our method achieves superior scores in terms of Recall, Coverage, and FID on both datasets, except for Recall on the Infinity-8B model. Fig. 9 illustrates that our generated results (3rd row) more closely resemble the real images from AFHQ (1st row). See Suppl. for details. Overall Comparison. We evaluate the performance of our Table 2. Quantitative comparison between the vanilla model and our DiverseVAR under multiple sampling runs on AFHQ and CelebA-HQ. Figure 9. The vanilla model tends to produce results with similar styles under the same prompt, leading to limited diversity, while our results are more consistent with those from AFHQ. method against the vanilla VAR model [12], diffusion-based models [16, 58, 60], and AR models [3, 14, 57, 61]. Tab. 3 summarizes the comparison results on the GenEval [44] and DPG [45] benchmarks. As shown in Tab. 3, our method surpasses most competing approaches except, except DALL-E 3 on the DPG benchmark, while maintaining performance on par with the vanilla VAR model. Specifically, both our method and Infinity achieve over 0.7 on GenEval and around 83.0 on DPG, outperforming most competing methods except DALL-E 3, which demonstrates strong overall performance and consistency with the vanilla Infinity. These results indicate that our method generates images with significantly higher diversity while maintaining textimage alignment and visual fidelity compared to the vanilla Infinity model. See Suppl. for additional results. 4.3. Additional Analysis Ablation Study. Based on our analysis and observation, DiverseVAR applies Soft-Suppression Regularization (i.e., SSR) to the input feature (cid:101)F k1, and Soft-Augmentation Regularization (i.e., SAR) to the output feature (i.e., 5 in both Tabs. 4 and 5). We perform an ablation study on these components, exploring alternative designs and comparing performance. The ablated designs include: 1 SSR: The SAR is removed, and only the SSR is applied to the input feature (cid:101)F k1. 2 SAR+SSR: Apply the SAR to the input feature (cid:101)F k1, and the SSR to the output feature k."
        },
        {
            "title": "Metrics",
            "content": "Steps Params"
        },
        {
            "title": "Method",
            "content": "SDXL [16] LlamaGen [3] Show-o [57] PixArt-Sigma [58] SD3-medium [59] DALL-E 3 [60] Emu3 [61] HART [14] Infinity-2B [12] + Ours Infinity-8B [12] + Ours 40 1024 1024 20 28 - 14 13 13 13 13 2.6B 0.8B 1.3B 0.6B 2.0B 8.5B 0.7B 2.0B 2.0B 8.0B 8.0B Two Obj. Position Color Attri. Overall Global Relation Overall 0.74 0.34 0.80 0.62 0.74 0.81 0.62 0.84 0.85 0.90 0.89 0.15 0.07 0.31 0.14 0.34 0.49 0.13 0.45 0.41 0.61 0. 0.23 0.04 0.50 0.27 0.36 0.45 0.18 0.55 0.53 0.68 0.66 0.55 0.32 0.68 0.55 0.62 0.67 0.66 0.51 0.73 0.70 0.79 0.76 83.27 86.89 - 90.97 84.80 85.11 85.10 84. 86.76 86.59 - 90.58 93.04 92.26 94.50 94.93 74.65 65.16 67.48 80.54 - 83.50 81.60 80.89 82.97 83.02 86.60 86.78 Table 3. Quantitative comparisons of perceptual quality on GenEval and DPG benchmarks."
        },
        {
            "title": "DPG",
            "content": "Method 1 SSR 2 SAR+SSR 3 SSR+SAR 4 SAR+SSR 5 SSR+SAR (Ours) Two Obj.Pos.Color Attri. OA GlobalRelation OA 0.81 0.84 0.83 0.81 0.39 0.40 0.41 0.41 0.85 0. 0.53 0.53 0.51 0.51 0.53 0.68 83.89 92.45 83.01 0.69 84.19 92.57 82.79 0.68 82.67 92.80 82.77 0.68 82.97 91.37 82.72 0.70 85.11 92.26 83.02 Table 4. Ablation study of each component in DiverseVAR, evaluating perceptual quality on the GenEval and DPG benchmarks. Components include SSR and SAR."
        },
        {
            "title": "Dataset",
            "content": "COCO2014-30K COCO2017-5K Method 1 SSR Recall Cov. FID CLIP Recall Cov. FID CLIP 0.375 0.689 23.77 0.313 0.440 0.858 34.65 0.313 2 SAR+SSR 0.352 0.686 24.98 0.313 0.456 0.857 35.36 0.313 3 SSR+SAR 0.380 0.694 23.18 0.313 0.461 0.861 33.88 0.313 4 SAR+SSR 0.374 0.688 23.14 0.313 0.471 0.871 33.85 0.313 5 SSR+SAR (Ours) 0.385 0.690 22.96 0.313 0.480 0.860 33.39 0. Table 5. Ablation study of each component in DiverseVAR, evaluating perceptual quality on the COCO2014 (30K prompts) and COCO2017 (5K prompts) benchmarks. Components include SSR and SAR."
        },
        {
            "title": "3 SSR†+SAR†: First, apply SSR and then SAR to the in-\nput feature (cid:101)F k−1. 4 SAR†+SSR†: First, apply SAR and\nthen SSR to the input feature (cid:101)F k−1. Here, † indicates that\nthe operation is applied to the input feature (cid:101)F k−1, while ‡\nindicates that the operation is applied to the output feature\nF o\nk. Performance results and comparisons are presented\nin Tabs. 4 and 5. We observe that our method (i.e., 5 ) out-\nperforms other ablated designs (i.e., 1 - 4 ) in both percep-\ntual quality and diversity, except for the Relation on DPG\nand the Coverage metric on COCO2017. See Suppl. for\nadditional ablation.\nAdditional Results. The Infinity [12] model originally\nsupports image generation with varying aspect ratios, and\nour method, DiverseVAR, preserves this property. As\nshown in Fig. 10, when combined with DiverseVAR, it con-",
            "content": "Figure 10. Qualitative comparison between the vanilla model and our method. Our DiverseVAR facilitates generation with diverse aspect ratios. See Suppl. for the text prompts used. tinues to facilitate efficient image generation, demonstrating that our proposed method can be easily extended to generate images with diverse aspect ratios. 5. Conclusion In this work, we explore the factors that influence diversity in text-to-image VAR models and find that the pivotal component plays critical role in shaping diversity formation at early scales. Building on this insight, we introduce DiverseVAR, simple yet effective framework that unleashes the intrinsic generative diversity of VAR models while preserving image fidelity, through input-level suppression and output-level augmentation of the pivotal component. We conduct extensive experiments and demonstrate that our approach effectively improves generative diversity while preserving image quality and textimage alignment."
        },
        {
            "title": "References",
            "content": "[1] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1152311532, 2022. 2, 3 [2] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [3] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 7, 8 [4] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 2, 3 [5] Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. 2 [6] Zhihao Huang, Xi Qiu, Yukuo Ma, Yifu Zhou, Junjie Chen, Hongyuan Zhang, Chi Zhang, and Xuelong Li. Nfig: Autoregressive image generation with next-frequency prediction. arXiv preprint arXiv:2503.07076, 2025. [7] Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis E. H. Tay, Ser-Nam Lim, Harry Yang, and Li Yuan. Next patch prediction for autoregressive visual generation, 2024. [8] Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, and Eunho Yang. Lantern: Accelerating visual autoregressive modarXiv preprint els with relaxed speculative decoding. arXiv:2410.03355, 2024. 2 [9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 2 [10] Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating autoregressive text-to-image generation with training-free speculative jacobi decoding. arXiv preprint arXiv:2410.01699, 2024. 2 [11] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 2, 3 [12] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis, 2024. 2, 4, 5, 6, 7, 8, 1 [13] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-toimage generation via auto-regressive representations. arXiv preprint arXiv:2406.10797, 2024. [14] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. HART: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. 2, 7, 8 [15] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 6 [16] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion modarXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 7, 8 [17] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 2 [18] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2 [19] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. [20] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation, lightning: 2024. [21] Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher. In European Conference on Computer Vision, pages 176192. Springer, 2024. 2 [22] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. In European Conference on Computer Vision, pages 428447. Springer, 2024. 2, 3 [23] Senmao Li, Lei Wang, Kai Wang, Tao Liu, Jiehang Xie, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, and Jian Yang. One-way ticket: Time-independent unified encoder for distilling text-to-image diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, [24] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2 [25] Rohit Gandikota and David Bau. sity and control arXiv:2503.10637, 2025. 2, 3 in diffusion models. Distilling diverarXiv preprint [26] Jiyeon Han, Dahee Kwon, Gayoung Lee, Junho Kim, and Enhancing creative generation on stable Jaesik Choi. In Proceedings of the Computer diffusion-based models. Vision and Pattern Recognition Conference, pages 28609 28618, 2025. 2, 3, 6 [27] Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, et al. Autoregressive models in vision: survey. arXiv preprint arXiv:2411.05902, 2024. 3 [28] Xiaoxiao Ma, Feng Zhao, Pengyang Ling, Haibo Qiu, Zhixiang Wei, Hu Yu, Jie Huang, Zhixiong Zeng, and Lin Ma. Towards better & faster autoregressive image generation: From the perspective of entropy, 2025. 3, 6 [29] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. 3 [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 4, 6 [31] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1074810757, 2022. [32] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19211930, June 2023. 4 [33] Geon Yeong Park, Jeongsol Kim, Beomsu Kim, Sang Wan Lee, and Jong Chul Ye. Energy-based cross attention for bayesian context update in text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 4 [34] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 4 [35] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 4 [36] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, abs/2004.07728, 2020. 4, 5 [37] Hang Guo, Yawei Li, Taolin Zhang, Jiangshan Wang, Tao Dai, Shu-Tao Xia, and Luca Benini. Fastvar: Linear visual autoregressive modeling via cached token pruning. arXiv preprint arXiv:2503.23367, 2025. 4, 1 [38] David Douglas and Thomas Peucker. Algorithms for the reduction of the number of points required to represent digitized line or its caricature. Cartographica: the international journal for geographic information and geovisualization, 10(2):112122, 1973. 4, 1 [39] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 5 [40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 5 [41] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with applicaIn Proceedings of the IEEE contion to image denoising. ference on computer vision and pattern recognition, pages 28622869, 2014. 5 [42] Kunjun Li, Zigeng Chen, Cheng-Yen Yang, and Jenq-Neng Hwang. Memory-efficient visual autoregressive modeling arXiv preprint with scale-aware kv cache compression. arXiv:2505.19602, 2025. [43] Zhuokun Chen, Jugang Fan, Zhuowei Yu, Bohan Zhuang, and Mingkui Tan. Frequency-aware autoregressive modeling for efficient high-resolution image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 6 [44] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 5213252152. Curran Associates, Inc., 2023. 6, 7, 1 [45] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 6, 7, 1 Ella: [46] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis. International Conference on Machine Learning, 2023. 6 [47] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up GANs for Text-to-Image Synthesis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep lan10 Improving image generation with Lee, Yufei Guo, et al. better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 7, 8 [61] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 7, 8 [62] Minguk Kang, Joonghyuk Shin, and Jaesik Park. Studiogan: taxonomy and benchmark of gans for image synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 1 [63] Minguk Kang, Woohyeon Shim, Minsu Cho, and Jaesik Park. Rebooting acgan: Auxiliary classifier gans with stable training. In Neural Information Processing Systems, 2021. [64] Mingu Kang and Jaesik Park. Contragan: Contrastive learning for conditional image generation. Advances in Neural Information Processing Systems, 2020. 1 [65] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 1 guage understanding. Advances in Neural Information Processing Systems, 2022. 6 [49] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 6 [50] MinGuk Kang, Joonghyuk Shin, and Jaesik Park. StudioGAN: Taxonomy and Benchmark of GANs for Image Synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023. 6, 7 [51] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6, [52] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. CoRR, abs/1904.06991, 2019. 6 [53] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity metrics for generative models. In International Conference on Machine Learning, pages 71767185. PMLR, 2020. 6, 7, 1 [54] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, 2021. 7, 1 [55] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 7, 1 [56] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 7, 1 [57] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 7, 8 [58] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-Sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. 7, [59] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 8 [60] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Overview This supplementary material provides additional implementation details, algorithmic descriptions, and extended experiments to support the main paper. Specifically, it includes: Implementation Details (Sec. B): Detailed configuration of datasets, metrics, and baseline implementations used in our experiments. Algorithm Detail of DiverseVAR (Sec. C): Complete pseudocode and procedural explanation of our proposed method for reproducibility. Additional Analysis (Sec. D): Ablation studies on different design choices, in-depth analysis of scale and block configurations, and additional qualitative comparisons demonstrating image generation quality and diversity. B. Implementation Details B.1. Configure Datasets. We utilize the AFHQ [55] and CelebA-HQ [56] datasets to evaluate diversity under multiple sampling scenarios, as they contain diverse animal and human face images, respectively. We employ text prompts in the following formats: face of <cat/dog/wild animal> and face of <man/woman>. Metrics. We leverage the code from the popular GitHub repository StudioGAN [6264]3 to compute two metrics: Recall [65] and Coverage [53]. For FID [51] and CLIP Score [54], we employ the official evaluation code from GigaGAN [47]4 We use the official evaluation code for GenEval [44]5 (also provided in the official Infinity code) and DPG [45]6 Baseline Implementations. We use the official implementation of Infinity-2B and Infinity-8B [12]7. For Infinity-2B, we use all the hyperparameters at their default settings. For Infinity-8B, due to computational resource limitations, using A100 40GB GPUs leads to out-of-memory (OOM) errors. Therefore, we conduct both qualitative and quantitative evaluations on the generation results at scale 48. B.2. The Pivotal and Auxiliary Token In our Observation 2, we employ the defined pivotal score and the Maximum Distance to Chord (MDC) to identify the 3https : / / github . com / POSTECH - CVLab / PyTorch -"
        },
        {
            "title": "StudioGAN",
            "content": "4https://github.com/lucidrains/gigagan-pytorch 5https://github.com/djghosh13/geneval 6https://github.com/TencentQQGYLab/ELLA 7https://github.com/FoundationVision/Infinity pivotal and auxiliary token. Specifically, we follow [37] and define the pivotal score sk,i = (cid:101)F k1,i k12 using the L2 norm, where k1 represents the mean feature map obtained by averaging (cid:101)F k1 across scale dimensions, and (cid:101)F k1,i denotes each token along the scale dimension. Then, to determine the boundary between pivotal and auxiliary tokens, we follow the MDC method [38]. Given the sorted sequence {sk,in }L n=1 in descending order, we construct chord connecting the endpoints (1, sk,i1) and (L, sk,iL), where = Sk is the scale dimension. The perpendicular distance of each intermediate point (in, sk,in ) to this chord is computed as din = (cid:12) (cid:12)(sk,iL sk,i1 ) in (L 1) sk,in + sk,i1 sk,iL (cid:112)(sk,iL sk,i1 )2 + (L 1)2 (cid:12) (cid:12) . The index (6) = arg maxin din corresponding to the maximum distance is defined as the elbow point, which separates the tokens into Tpivotal = {1, . . . , n}, Tauxiliary = {i + 1, . . . , L}. (7) Tokens before the elbow point exhibit higher deviation from the mean feature and are thus considered pivotal, while those after are treated as auxiliary. B.3. Text Prompts We list the text prompts used for image generation in this paper below. Fig. 1 : man in clown mask eating donut, cat wearing Halloween costume, Golden Gate Bridge at sunset, glowing sky, dramatic perspective, vivid colors, palace under the sunset, cool astronaut floating in space, and cat riding skateboard down hill. Fig. 2 : green train is coming down the tracks. Fig. 4 : bird perched on tree branch in the rain. Fig. 6 : bear fishing with stick by calm river. Fig. 7 : hot air balloon floating above the clouds. Fig. 9 : The face of cat. Fig. 10 : dog running through water surrounded by flowers and cozy house surrounded by autumn trees. Fig. S1 : very cute cat near bunch of birds, cat standing on hill, photo of cute rabbit holding cup of coffee in cafe, cinematic shot of little pig priest wearing sunglasses, dog covered in vines, and Cute grey cat, digital oil painting by Monet. Fig. S2 : Editorial photoshoot of an old woman, high fashion 2000s fashion, An astronaut riding horse on the 1 moon, oil painting by Van Gogh, Full body shot, French woman, photography, French streets, boy and girl fall in love, An abstract portrait of pensive face, rendered in cool shades of blues, purples, and grays, and Cute boy, hair looking up to the stars, snow, beautiful lighting, painting style by Abe Toshiyuki. Fig. S3 : table with light on over it, library filled with warm yellow light, villa standing on hill, train crossing bridge over canyon, bridge stretching over calm river, and temple surrounded by flowers. C. Algorithm detail of DiverseVAR Algorithm 1 : DiverseVAR Input : Scales {S1, S2, , SK }. Scales for DiverseVAR consists of scales {l1, , lm}. Scales for the vanilla process {S1, S2, , SK } {l1, , lm}. The VAR model ϕ, the image decoder D, and the quantizer Q. Output: The final diverse output I. 0 = 0 ; (cid:101)F 0 = SOS R11d ; for = 1, , do // The start token [12] if Sk {S1, S2, , SK } {l1, , lm} then"
        },
        {
            "title": "F o",
            "content": "k = ϕ( (cid:101)F k1) ; // Fig. 8 (Left) end else ˆF k1 (cid:101)F k1 ; = ϕ( ˆF k1) ; ˆF ˆF ; // SSR: Eq. (4) // Fig. 8 (Right) // SAR: Eq. (5) // Eq. (1) // Eq. (3) end Rk = Q(F = k1 + up(Rk, (hK , wK )) ; (cid:101)F = down(F k, (hk, wk)) ; k) ; end = D(F ) Return The final generated image D. Additional Analysis D.1. Ablation Study of Scales Based on our observation and analysis, early scales have significant impact on the structural formation. Specifically, we apply our method to the VAR at scales 2 and 4 (i.e., li {4, 6}). To quantitatively investigate the impact of scales on image generation quality and diversity, we conducted experiments on GenEval and DPG to compare generation quality, and on COCO 2014 and COCO 2017 to evaluate generation diversity. As shown in Tab. S1 (1st4th rows), applying DiverseVAR with single early scale leads to consistent improvements in Recall, Coverage, and FID over the vanilla on COCO 2014 and COCO 2017, demonstrating enhanced diversity. Meanwhile, the performance 2 on GenEval and DPG shows only marginal degradation, indicating that image generation quality is well preserved. To further stimulate model diversity, we apply DiverseVAR to multiple early scales and their various combinations. As illustrated in Tab. S1 (5th10th rows), increasing the number of applied scales leads to higher diversity but causes rapid degradation in generation quality. Notably, applying DiverseVAR to four early scales (i.e., li {2, 4, 6, 8}) yields the highest diversity metrics but the lowest generation quality, with GenEval decreasing by more than 0.1. Therefore, to achieve good trade-off between generation quality and diversity, we apply DiverseVAR at scales 4 and 6 (i.e., li {4, 6}), which are used as the default settings. D.2. Ablation Study of Blocks the block level, Our method operates at i.e., on 8 blocks bi within the Infinity backbone, where {0, 1, 2, 3, 4, 5, 6, 7}. Specifically, DiverseVAR is applied across all 8 blocks. To further assess the contribution of different blocks, we perform comparative experiments by selectively applying DiverseVAR to specific subsets of blocks. As illustrated in Tab. S2 (3rd7th rows), applying DiverseVAR to only partial subsets of blocks results in significant degradation in either image generation quality or diversity compared to applying it to all blocks. For example, applying DiverseVAR to blocks bi with {0, 1, 2, 3, 4, 5, 6} yields the best diversity performance achieving superior Recall, Coverage, and FID on COCO 2014 and COCO 2017 but results in severe degradation of image quality (Tab. S2 (3rd row)). While applying DiverseVAR to blocks bi with {4, 5, 6, 7} maintains the generation quality, the resulting diversity metrics remain suboptimal (Tab. S2 (6th row)). Therefore, to achieve good trade-off between generation quality and diversity, we apply DiverseVAR at all 8 blocks, which are used as the default settings. For more comprehensive comparison, we further apply the method at the model level. As illustrated in Tab. S2 (last row), this configuration maintains the generation quality (evaluated on GenEval and DPG), with GenEval remaining at 0.73, but yields almost no improvement in diversity (evaluated on COCO 2014 and COCO 2017). D.3. Ablation Study on logits We observe that the logits distributions under different samplings are similar in the vanilla VAR, causing diversity collapse. Our proposed method mitigates this issue by introducing variation into the distributions, thereby encouraging diverse generations while maintaining faithful textimage alignment and high visual quality. To further examine whether DiverseVAR is more effective at the block level than at the logits level, we performed comparative experiments by applying it to the logits (Tab. S3 (2nd-3rd rows)). Dataset GenEval DPG COCO2014-30K COCO2017-5K Metrics Scales li = (Vanilla) li {2} li {4} li {6} li {8} li {2, 4} li {4, 6} (Ours) li {6, 8} li {2, 4, 6} li {4, 6, 8} li {2, 4, 6, 8} Two Obj. Position Color Attri. Overall Global Relation Overall Recall Cov. FID CLIP Recall Cov. FID CLIP 0. 0.85 0.83 0.85 0.84 0.83 0.85 0.80 0.77 0.75 0.70 0.45 0.48 0.44 0.42 0.45 0.44 0.41 0.40 0.40 0.34 0.35 0.55 0.52 0.58 0.52 0.52 0.52 0.53 0.47 0.48 0.42 0.35 0. 84.80 93.04 82.97 0.316 0.651 28.48 0.313 0.408 0.832 39.01 0.313 0.733 0.716 0.711 0.725 0.712 0.70 0.677 0.669 0.622 0.593 83.28 83.28 85.41 83.89 82.06 85.11 83.89 83.58 83.58 82. 92.06 92.49 92.72 92.49 92.22 92.26 92.45 92.26 92.37 92.45 83.018 83.256 83.254 82.891 82.859 83.02 82.996 82.388 81.630 80.749 0.321 0.674 27.26 0.313 0.417 0.843 37.80 0.313 0.333 0.660 27.11 0.314 0.430 0.833 37.64 0.313 0.337 0.667 26.64 0.314 0.415 0.844 36.87 0.313 0.329 0.662 27.35 0.314 0.422 0.838 37.92 0.313 0.340 0.680 25.43 0.313 0.435 0.851 35.92 0.312 0.385 0.690 22.96 0.313 0.480 0.860 33.39 0.313 0.366 0.693 23.24 0.313 0.452 0.855 33.84 0.313 0.432 0.710 20.05 0.312 0.513 0.876 30.79 0.311 0.499 0.700 15.85 0.310 0.567 0.878 26.35 0.310 0.544 0.687 14.70 0.306 0.611 0.857 25.67 0.304 Table S1. Ablation study of different scales li on image generation quality (GenEval and DPG) and diversity (COCO 2014 and COCO 2017). GenEval DPG COCO2014-30K COCO2017-5K Two Obj. Position Color Attri. Overall Global Relation Overall Recall Cov. FID CLIP Recall Cov. FID CLIP Dataset Metrics Blocks = {0, 1, 2, 3, 4, 5, 6, 7} (Ours) 0.84 0.45 0.85 0. {1, 2, 3, 4, 5, 6, 7} {0, 1, 2, 3, 4, 5, 6} {2, 3, 4, 5, 6, 7} {4, 5, 6, 7} {1, 2, 3} Model-level 0.80 0.80 0.85 0.82 0.82 0.85 0.39 0.38 0.42 0.42 0.39 0. 0.55 0.53 0.50 0.45 0.56 0.56 0.47 0.56 0.73 84. 93.04 82.97 0.316 0.651 28.48 0.313 0.408 0.832 39.01 0.313 0.70 85.11 92. 83.02 0.385 0.690 22.96 0.313 0.480 0.860 33.39 0.313 0.678 0.657 0.715 0.722 0.682 85.41 83.89 83.58 84.19 84.49 92.57 92.72 92.49 92.26 92.45 0. 84.19 92.80 82.93 82.60 82.86 83.03 83.29 83.03 0.370 0.691 23.77 0.314 0.456 0.867 34.38 0.313 0.411 0.699 21.06 0.314 0.485 0.868 31.92 0.313 0.350 0.676 25.78 0.314 0.432 0.851 36.27 0.314 0.338 0.659 27.22 0.314 0.427 0.846 37.15 0.314 0.352 0.677 24.08 0.313 0.439 0.855 34.17 0.312 0.317 0.651 28.29 0.314 0.402 0.828 38.98 0. Table S2. Ablation study of different blocks bi on image generation quality (GenEval and DPG) and diversity (COCO 2014 and COCO 2017). Dataset Metrics Method Vanilla SSR in logits SSR+SAR in logits SSR+SAR in blocks (Ours) GenEval DPG COCO2014-30K COCO2017-5K Two Obj. Position Color Attri. Overall Global Relation Overall Recall Cov. FID CLIP Recall Cov. FID CLIP 0. 0.86 0.86 0.85 0.45 0.45 0.45 0.41 0. 0.82 0.54 0.53 0.73 0.72 0.73 0.70 84.80 93. 83.28 91.99 83.89 92.99 82.97 83.41 83.12 0.316 0.651 28.48 0.313 0.408 0.832 39.01 0.313 0.320 0.604 28.95 0.313 0.419 0.838 38.91 0.313 0.331 0.580 29.06 0.313 0.402 0.828 39.22 0.313 85.11 92. 83.02 0.385 0.690 22.96 0.313 0.480 0.860 33.39 0.313 Table S3. Ablation study on logits for image generation quality (GenEval and DPG) and diversity (COCO 2014 and COCO 2017). As shown in Tab. S3, when DiverseVAR is applied to the logits, the generation quality is well preserved as indicated by GenEval score of 0.72 and 0.73 on GenEval while the diversity metrics on COCO 2014 and COCO 2017 show only slight improvement. E. Additional Results E.1. Diversity Comparison As shown in Figs. S1 to S3, we present additional qualitative results for diversity comparison. We observe that the vanilla model tends to produce samples with limited diversity across different text prompts, although it generates high-quality results. In contrast, our method can generate diverse images from multiple samples under different text prompts, while maintaining good textimage alignment, indicating its advantage over the baselines. 3 Figure S1. Additional diversity comparison. Our results demonstrate superior diversity. 4 Figure S2. Additional diversity comparison. Our results demonstrate superior diversity. Figure S3. Additional diversity comparison. Our results demonstrate superior diversity."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "MBZUAI",
        "Nankai University",
        "Southeast University"
    ]
}