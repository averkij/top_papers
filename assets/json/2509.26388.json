{
    "paper_title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models",
    "authors": [
        "Kai-Wei Chang",
        "En-Pei Hu",
        "Chun-Yi Kuan",
        "Wenze Ren",
        "Wei-Chih Chen",
        "Guan-Ting Lin",
        "Yu Tsao",
        "Shao-Hua Sun",
        "Hung-yi Lee",
        "James Glass"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, a framework to systematically assess these temporal capabilities. Inspired by how humans learn a language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals a clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides a foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website https://ga642381.github.io/Game-Time."
        },
        {
            "title": "Start",
            "content": "GAME-TIME: EVALUATING TEMPORAL DYNAMICS IN SPOKEN LANGUAGE MODELS Kai-Wei Chang1, En-Pei Hu2, Chun-Yi Kuan2, Wenze Ren2, Wei-Chih Chen2, Guan-Ting Lin2, Yu Tsao3, Shao-Hua Sun2, Hung-yi Lee2, James Glass1 1 Massachusetts Institute of Technology, USA 2 National Taiwan University, Taiwan 3 Academia Sinica, Taiwan 5 2 0 2 0 3 ] . e [ 1 8 8 3 6 2 . 9 0 5 2 : r ABSTRACT Conversational Spoken Language Models (SLMs) are emerging as promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, framework to systematically assess these temporal capabilities. Inspired by how humans learn language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website1. Index Terms Spoken Language Models, Temporal Dynamics, Full-Duplex Speech, Conversational AI, Benchmark 1. INTRODUCTION In the pursuit of human-like conversation with machines, the research frontier is moving beyond text-based Large Language Models (LLMs). The next challenge lies in mastering conversational dynamics in real-time speech, which has given rise to the field of conversational Spoken Language Models (SLMs) [1, 2, 3, 4, 5, 6, 7]. This marks critical shift from rigid turn-by-turn dialogues to fluid spoken interactions. Achieving such dynamics requires SLMs to operate in real-time full-duplex manner [8, 9], where models must listen and speak simultaneously while producing seamless responses. This is inherently difficult, demanding synchronous speech generation, continual intent recognition, and precise control over both what to respond and when to respond. further challenge lies in modeling the temporal dynamics of spoken interaction, where contemporary systems often fail to capture the fine-grained timing that is essential for advanced conversational fluency. For instance, they struggle to process user speech while planning coherent reply that aligns with user-specified timing and tempo. This limitation underscores fundamental deficiency: the lack of time-awareness. Co-first authors 1https://ga642381.github.io/Game-Time Fig. 1. Overview of the Game-Time Benchmark, evaluating temporal dynamics in conversational Spoken Language Models (SLMs). Existing benchmarks for SLMs focus on content and style generation quality [10, 11], mimicking human dialogue behaviors (e.g. back-channeling) [12], and turn-taking [13]. However, they lack the focus on temporal dynamics in the conversation. To address this critical gap, we introduce the Game-Time Benchmark, which is designed to assess the temporal capabilities of SLMs, focusing on their ability to perceive, predict, and produce speech in sync with the user. This work is inspired by childhood language acquisition [14]. Children learn to talk through language activities that require not only understanding the meaning of vocabulary, such as counting numbers and naming objects, but also acquiring sense of timing and tempo in coordination games [15, 16]. For example, playing rock-paper-scissors relies on sharing common tempo and acting precisely on specific cue (e.g., shoot). This inspiration led us to design the Game-Time Benchmark, which contains two categories of tasks. The Basic Tasks evaluate an SLMs foundational ability to follow simple instructions, challenge that can still prove difficult for modern SLMs. The Advanced Tasks build upon this foundation by augmenting the core instructions with temporal constraints. Here, the SLM must perform the basic tasks while fulfilling requirements for timing and synchronicity. The Game-Time Benchmark provides framework for evaluating whether model can move beyond mere content generation to acquire the temporal dynamics of conversational fluency. It proposes novel perspective on evaluation, focusing not just on what to say, but critically, on when to say. In this paper, we evaluate various SLMs with different design philosophies, including Moshi [17], Unmute [18], FreezeOmni [19], Gemini-Live [20], and GPT-realtime [21]. Our results Table 1. Game-Time Task Families. Basic Task contains fundamental tasks. Advanced Task contains temporal dynamics paired with Basic Task. : Number of subtasks. *The game of Rock paper scissors is itself an advanced task. Category Task Family Subtask / Paired Basic Tasks Description Basic Adv. 1 - Sequence 2 - Repeat 3 - Compose 4 - Recall 5 - Open-Ended 6 - Role-Play Number; Alphabet; Spell 3 2 Word; Sentence 2 Word; Scenario 3 2 2 Vocabulary; Letter; Rhyme Empathy; QA Scenario; Persona Generate sequential items in order from nstart to nend. Repeat user-provided content C. Compose response including target word or fitting scenario S. Name items that satisfy property ϕ. Provide helpful and contextually appropriate content. Act within an imagined scenario or play given persona P. - Time-Fast - Time-Slow - Time-Silence - Tempo-Interval - Tempo-Adhere - Simul.-Shadow - Simul.-Cue 10 7 4 4 4 1 1 [Multiple Basic Tasks] [Multiple Basic Tasks] Repeat; Recall; Open-Ended Sequence; Recall Sequence; Recall Repeat Rock paper scissors* Complete the task quickly, within specified duration τfast. Perform the task slowly, taking at least the specified duration τslow. Insert silent interval of seconds before the response. Follow specified tempo with δ-second space between each word. Adhere to the tempo specified by the users spoken example Ctempo. Repeat each word with immediate, word-by-word overlap. Overlap with the user by speaking at designated timing or cue. show performance disparity even on Basic Tasks: while stateof-the-art models generally excel, many contemporary SLMs still struggle with fundamental instruction-following. Furthermore, the performance of nearly all models degrades significantly when temporal constraints are introduced. Our findings indicate that models especially struggle with tasks requiring time awareness and real-time full-duplex capability, revealing critical gap in the capabilities of current systems. For reproducibility, datasets and results are available on our website. 2. RELATED WORKS 2.1. Full-duplex Spoken Language Models Recent work has explored how SLMs can move beyond turn-based interaction toward full-duplex conversation [12, 22, 23, 24, 25, 26], where listening and speaking occur simultaneously. Two main modeling strategies have emerged to achieve full-duplex capability [3]: (1) Dual-channel SLMs [22, 17, 27, 28] use two input channels: listening channel for user speech and speaking channel for the models own output. At each time step, the model simultaneously processes both inputs and generates output speech, which may be either voiced or explicitly silent. Although this architecture significantly increases modeling complexity, it naturally supports real-time listening and speaking concurrently. (2) Time-multiplexing SLMs [29, 19, 18] include state prediction mechanism [30] that decides whether to speak or remain silent. During user speech, the model withholds output and monitors for an appropriate moment to take the turn and respond. During its own speech, the model generates in an autoregressive manner, and this generation is often interrupted by incoming user speech. In the Game-Time benchmark, we evaluate both model designs and commercial voice agent APIs, comparing their ability to manage timing and overlap, and discussing the trade-offs of each design. 2.2. Benchmarks for Conversational Spoken Language Models number of benchmarks have been proposed to evaluate SLMs. Some focus on the spoken language understanding and paralinguistics generation [10, 31, 32, 33, 34, 35]. Others, including FullDuplex-Bench [12], Talking Turns [13], and FD-Bench [36] primarily assess dialogue behaviors including latency, back-channeling, and turn-taking, with an emphasis on the conversational naturalness. However, these benchmarks do not directly test models ability to comprehend and act upon explicit temporal requirements. GameTime complements these existing works by shifting the focus from what to say to when to say, introducing framework for evaluating timing, tempo, and simultaneous speaking within conversation. 3. GAME-TIME BENCHMARK We introduce the Game-Time Benchmark to evaluate SLMs on their understanding of time, tempo, and timely simultaneously speaking. In this section, we define the task families, describe how the benchmark is constructed, and outline the evaluation protocol. 3.1. Task Families Inspired by how humans learn language with language activities and games, the Game-Time benchmark comprises two categories: Basic Tasks, testing fundamental speech capabilities, and Advanced Tasks, which paired temporal constraints with suitable Basic Tasks to assess the models time-awareness. The full taxonomy of families, subtasks, and temporal requirements is summarized in Table 1. Game-Time Basic Tasks: The 6 Basic Task families reflect fundamental capabilities of spoken interaction. They are inspired by the kinds of activities through which humans first practice language: reciting ordered sequences (Sequence), repeating spoken content (Repeat), composing sentences that meet specific criteria (Compose), recalling items from memory (Recall), responding helpfully and openly in conversation (Open-ended), and navigating hypothetical scenarios or adopting specific personas (Role-play). Together, they capture spectrum from structured behaviors (e.g., counting) to open-ended and social behaviors (e.g., Empathy and Role-Play). Game-Time Advanced Tasks: Advanced tasks introduce constraints that move beyond what to say and focus on when to say it. Here, the philosophy is to test temporal and interactive fluency skills that come naturally to humans but remain underexplored in SLMs. Time tasks examine whether models can modulate overall duration of speaking time, which require coordinating not only the speaking rate but also the content; Tempo tasks probe their ability to sustain rhythmic consistency or synchronize with an external beat; SimulSpeak tasks challenge them to overlap with the users speech, listening and synchronizing with the user in real time. These constraints are abstractions of conversational dynamics such as timing, tempo and coordination, which are central to human conversation. Table 2. Comparison of SLMs in Game-Time Benchmark. Model Full-Duplex Method Open Frozen LLM Freeze-Omni Unmute Moshi Gemini-Live GPT-realtime Time-Multiplexing Time-Multiplexing Dual Channel SSML-LLM Non-causal Completion - - Fig. 2. Dual-channel Evaluation with LLM-as-a-judge. for the Advanced tasks (31 subtasks, 25 samples each). 3.2. Task Formalization We formalize our tasks within an Instruction-Following (IF) framework [37, 38]. Each IF instance is specified by base task and set of constraints C. Performing an IF task requires the model to perform base task while satisfying all constraints in C. Each constraint is predicate over variables that are typically numeric or symbolic. Example: Consider the user instruction, Please count from one to ten in 10 seconds. Here, the base task is sequential generation tseq. Also, this instruction implies two constraints: (i) range constraint crangewith variables (nstart, nend) = (1, 10) requiring the spoken sequence to be 1, 2, . . . , 10, and (ii) duration constraint cdurwith variable τfast = 10s requiring the task performing time to be less than 10 seconds. For this instance, the model should perform the base task tseq while satisfying = {crange, cdur}. Building upon this formalization, we can systematically generate the dataset by creating natural language templates, instantiating them with diverse variables, and synthesizing the resulting instructions into speech. 3.3. Dataset Construction Pipeline The Game-Time benchmark dataset is constructed through fourstage pipeline designed to generate diverse and high-quality set of spoken instructions. (i) Seed Instruction Creation: We begin by manually writing set of seed instructions for Basic Tasks, defining the base task and corresponding variables. (ii) Linguistic Diversification: An LLM paraphrases seed instructions to create variety of linguistic templates. We then populate these templates by varying the defined variables, resulting in large and diverse set of instruction texts for the Basic Tasks. The Advanced Tasks are derived by augmenting subset of Basic Tasks with temporal constraints C. This controlled approach ensures that corresponding basic and Advanced Tasks share the same underlying base task t, allowing us to focus on the performance variation when imposing constraints. (iii) Speech Synthesis: These text-based instructions are synthesized into audio using TTS system with multiple voices to ensure vocal diversity 2. (iv) Quality Control: Finally, we use an ASR model to transcribe the synthesized audio. Instructions whose transcriptions do not closely match the original text are filtered out. This automated check is supplemented by manual listening verification on majority of the samples to ensure high perceptual quality. In the end we have total of 1,475 test instances: 700 samples for the Basic tasks (14 subtasks, 50 samples each) and 775 samples 2We primarily use CosyVoice [39] for speech synthesis. For tasks requiring precise tempo control, the audio was edited manually. Google TTS was used for the Rock, Paper, Scissors task, as we found it produced higherquality output in this case. 3.4. Dual-channel Evaluation Our evaluation protocol, illustrated in Figure 2, leverages an LLM to score model performance. For each dialogue, we first transcribe the dual-channel audio (user and model) to obtain time-aligned text. This transcription is then provided to an LLM judge, which assesses the models performance on the criteria of instruction following 3. We also explored alternative methods: using an audio-LLM-asa-judge [40] and employing rule-based automatic metrics. We found the audio-LLM approach is also effective but more costly and less aligned with humans evaluation (discussed in Sec.5.2). Meanwhile, rule-based metrics are often too rigid for the interpretive nature of spoken dialogue. For instance, rigid script would penalize model for including natural conversational preamble (e.g., Okay, Ill start now...) in time-constrained task. In contrast, an LLM can perform reasoning to recognize and evaluate the core speech embedded in the whole dialogue and give reasonable evaluation4. Overall, we find this text-based LLM judge to be unified, simple, yet effective method. To validate this approach, we conduct subjective human evaluations and confirm that its assessments align with human preferences. Furthermore, this method can effectively evaluate other behaviors such as turn-taking. Due to space limitations, these additional results are available on our project website. 4. EXPERIMENTAL SETUP We evaluate various SLMs on the Game-Time Benchmark with different full-duplex strategies (see Table 2). This includes TimeMultiplexing models (Freeze-Omni [19], Unmute [18]) which use modular pipeline of streaming encoder, frozen LLM, and streaming decoder; and Dual-channel model (Moshi [17]) where fine-tuned LLM directly processes and generates speech. We also include commercial voice agents: Gemini-Live5 and GPT-realtime6 Oracle System: We introduce SSML-LLM as our benchmarks oracle topline. This is non-streaming and non-causal system that operates with future knowledge. It receives the full word-level alignments of the users utterance as input, and an LLM then generates dialogue counterpart with timing that is precisely controlled and synchronized with the users speech via Speech Synthesis Markup Language (SSML). This SSML is then synthesized into 3For Open-Ended Basic Tasks, which lack an explicit instruction, the LLM judge evaluates response appropriateness instead. 4While we could have designed instructions to be unequivocally precise, doing so would result in unnatural conversations. Our approach prioritizes evaluating models ability to interpret natural instructions and time-related cues, rather than its ability to follow overly constrained commands. 5Gemini-Live [20]. API model name: gemini-live-2.5-flash-preview 6GPT-realtime [21]. API model name: gpt-realtime Fig. 3. Game-Time benchmark scores evaluated with LLM-as-a-judge. Top: results on Basic Tasks. Bottom: results on Advanced Tasks. Fig. 4. Human evaluation on Game-Time Advanced Tasks. audio by TTS system. For example, after processing the users speech Rock paper scissors shoot!, it generates <ssml><break time=\"6.9s\"/>scissors!</ssml> to make the response Scissors! overlap with users Shoot!. Although not feasible in real-time, SSML-LLM helps calibrate our LLM judge and human evaluations by providing theoretical performance ceiling. We use Gemini 2.5 Pro [41] with reasoning and Google TTS. Dual-channel Evaluation: Our LLM-as-a-judge framework requires time-stamped data for its analysis. To obtain word-level alignments, we use the Whisper-medium model. Based on preliminary study, we selected Gemini 2.5 Pro as the LLM judge, which we found superior to other open-source and commercial models at processing these time-stamped transcripts, leveraging its strong reasoning capabilities for evaluating complex temporal behaviors. 5. RESULTS 5.1. Main Results Basic Tasks: As shown in Fig. 3 (Top), the oracle topline consistently achieves the best performance across all tasks. GPT-realtime shows strong performance on most Basic Tasks, and it is worth noting that in Repeat, it is the only model that delivers reasonable performance. On the other hand, we observe that time-multiplexing models (Freeze-Omni and Unmute), which rely on frozen LLM, generally outperform the dual-channel model (Moshi). This suggests that fine-tuning text LLM to model speech signals remains challenging in spoken conversation scenarios. Overall, although Basic Tasks can be handled by the most advanced model (GPT-realtime), there is still room for improvement in modern academic models. Advanced Tasks: As shown in Fig. 3 (Bottom), introducing temporal constraints results in substantial drop in performance. Among the Advanced Tasks, models perform comparatively better on Time-Fast and Time-Slow, but fail on Time-Silence, suggesting that they can adjust their speaking rate in response to user instructions but still fail to grasp precise temporal requirements. Similarly, adhering to tempos (Tempo) and synchronizing speech with users (SimulSpeak) remains difficult for modern SLMs, even for SOTA commercial voice agents such as GPT-realtime. This performance disparity suggests that current SLMs do not possess time-awareness, highlighting the need to focus on this capability in future research. Table 3. Correlation between human judge with LLM and ALLM judge, both using Gemini 2.5 Pro. Spearmans ρ Pearsons Human - LLM Human - ALLM 0.677 0. 0.675 0.625 5.2. Human Evaluation Fig. 4 shows the result of the human evaluation. For each task, there are 20 samples, with each evaluated by three human judges via Prolific. We observe similar trend in the performance of SLMs as in LLM dual-channel evaluation. Table 3 presents the correlation between LLM-as-a-judge scores and human evaluations for Advanced Tasks. We also list the Audio LLM judge score for reference. Across 4 models 35 data scores, we observe reasonably high correlation (Spearmans ρ = 0.677, Pearsons = 0.675) for our dual-channel evaluation method. These results suggest that the LLM-as-a-judge is reliable and well aligned with human evaluation. We also find that for tasks requiring precise measurements like maintaining tensecond silence, the LLM may be more objective than humans, as it can leverage time-stamped alignment data for evaluation. 6. CONCLUSION This paper introduced the Game-Time Benchmark to address critical gap in the evaluation of the temporal dynamics of conversational Spoken Language Models (SLMs). We evaluated various SLMs with series of tasks testing temporal capabilities of timing, tempo, and simultaneous speaking. Our results reveal clear gap, with some models able to handle basic instructions, but nearly all failing once temporal constraints are introduced. This widespread inability to manage precise timing or synchronize with user reveals persistent lack of time-awareness in current SLMs, even in the most advanced systems. Our dual-channel evaluation utilizing an LLM for reasoning was shown to be reliable method, offering unified and scalable way to measure these complex behaviors. We hope the Game-Time Benchmark will motivate the community to build the next generation of diverse and time-aware SLMs. 7. ACKNOWLEDGMENT We are grateful to Yi-Cheng Lin and Cheng-Han Chiang for their valuable discussions on evaluation methods, and to Shih-Yun Shan Kuan for assistance with commercial API usage. 8. REFERENCES [1] Shengpeng Ji et al., Wavchat: survey of spoken dialogue models, arXiv preprint arXiv:2411.13577, 2024. [2] Wenqian Cui et al., Recent advances in speech language models: survey, in ACL (1). 2025, pp. 1394313970, Association for Computational Linguistics. [3] Siddhant Arora et al., guage models: comprehensive survey, arXiv:2504.08528, 2025. On the landscape of spoken lanarXiv preprint [4] Haibin Wu et al., Towards audio language modelingan overview, arXiv preprint arXiv:2402.13236, 2024. [5] Siddique Latif et al., Sparks of large audio models: survey and outlook, arXiv preprint arXiv:2308.12792, 2023. [6] Ke Hu et al., Efficient and Direct Duplex Modeling for in Interspeech 2025, Speech-to-Speech Language Model, 2025, pp. 27152719. [7] Chaoyou Fu et al., real-time vision and speech interaction, arXiv:2501.01957, 2025. Vita-1.5: Towards gpt-4o level arXiv preprint [8] Ziyang Ma et al., Language model can listen while speaking, in Proceedings of the AAAI Conference on Artificial Intelligence, 2025, vol. 39, pp. 2483124839. [9] Qian Chen, Yafeng Chen, Yanni Chen, et al., MinMo: multimodal large language model for seamless voice interaction, arXiv preprint arXiv:2501.06282, 2025. [10] Ruiqi Yan et al., URO-Bench: comprehensive benchmark for end-to-end spoken dialogue models, arXiv preprint arXiv:2502.17810, 2025. [11] Chih-Kai Yang et al., Towards holistic evaluation of large arXiv audio-language models: comprehensive survey, preprint arXiv:2505.15957, 2025. [19] Xiong Wang et al., Freeze-omni: smart and low latency speech-to-speech dialogue model with frozen llm, in Fortysecond International Conference on Machine Learning, 2025. [20] Google, Gemini live: more helpful, natural and visual assistant, Aug. 2025. [21] OpenAI, Introducing gpt-realtime and realtime api updates for production voice agents, Aug. 2025. [22] Tu Anh Nguyen et al., Generative spoken dialogue language modeling, Transactions of the Association for Computational Linguistics, vol. 11, pp. 250266, 2023. [23] Chen Chen, Ke Hu, Chao-Han Huck Yang, Ankita Pasad, et al., Reinforcement learning enhanced full-duplex spoken dialogue language models for conversational interactions, in Second Conference on Language Modeling, 2025. [24] Wenyi Yu, Siyin Wang, et al., Salmonn-omni: standalone speech llm without codec injection for full-duplex conversation, arXiv preprint arXiv:2505.17060, 2025. [25] Jin Xu et al., Qwen2. 5-omni technical report, arXiv preprint arXiv:2503.20215, 2025. [26] Qinglin Zhang et al., OmniFlatten: An end-to-end GPT model for seamless voice conversation, in Proc. ACL, 2025. [27] Qichao Wang et al., NTPP: Generative speech language modeling for dual-channel spoken dialogue via next-token-pair prediction, in Forty-second ICML, 2025. [28] Anne Wu et al., Aligning spoken dialogue models from user interactions, in Forty-second ICML, 2025. [29] Xinrong Zhang et al., Beyond the turn-based game: Enabling real-time conversations with duplex models, arXiv preprint arXiv:2406.15718, 2024. [30] Peng Wang et al., full-duplex speech dialogue scheme based on large language model, Advances in Neural Information Processing Systems, vol. 37, pp. 1337213403, 2024. [31] Junyi Ao, Yuancheng Wang, et al., SD-Eval: benchmark dataset for spoken dialogue understanding beyond words, Advances in Neural Information Processing Systems, 2024. [32] Yiming Chen et al., Voicebench: Benchmarking llm-based voice assistants, arXiv preprint arXiv:2410.17196, 2024. [12] Guan-Ting Lin et al., Full-duplex-bench: benchmark to evaluate full-duplex spoken dialogue models on turn-taking capabilities, arXiv preprint arXiv:2503.04721, 2025. [33] Kuofeng Gao et al., Benchmarking open-ended audio dialogue understanding for large audio-language models, in ACL (1). 2025, Association for Computational Linguistics. [13] Siddhant Arora et al., Talking turns: Benchmarking audio foundation models on turn-taking dynamics, in The Thirteenth International Conference on Learning Representations, 2025. [14] Jerome Bruner, Childs talk: Learning to use language, Child Language Teaching and Therapy, vol. 1, pp. 111114, 1985. [15] Nancy Ratner and Jerome Bruner, Games, social exchange and the acquisition of language, Journal of child language, vol. 5, no. 3, pp. 391401, 1978. [16] David Whitebread et al., The role of play in childrens development: review of the evidence, LEGO Fonden Billund, Denmark, 2017. [34] Heyang Liu, Yuhao Wang, et al., Vocalbench: Benchmarking the vocal conversational abilities for speech interaction models, arXiv preprint arXiv:2505.15727, 2025. [35] Jian Zhang et al., dio llms in natural speech conversation, arXiv:2506.21875, 2025. Wildspeech-bench: Benchmarking auarXiv preprint [36] Yizhou Peng et al., FD-Bench: Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems, in Interspeech 2025, 2025, pp. 176180. [37] Jeffrey Zhou et al., Instruction-following evaluation for large language models, arXiv preprint arXiv:2311.07911, 2023. [17] Defossez et al., Moshi: speech-text foundation model for [38] Valentina Pyatkin et al., Generalizing verifiable instruction real-time dialogue, arXiv preprint arXiv:2410.00037, 2024. following, arXiv preprint arXiv:2507.02833, 2025. [18] Neil Zeghidour et al., learning with delayed streams modeling, arXiv:2509.08753, 2025. Streaming sequence-to-sequence arXiv preprint [39] Zhihao Du et al., Cosyvoice 2: Scalable streaming speech arXiv preprint synthesis with large language models, arXiv:2412.10117, 2024. [40] Cheng-Han Chiang et al., models as judges for speaking styles, arXiv:2506.05984, 2025. Audio-aware large language arXiv preprint [41] Gheorghe Comanici et al., Gemini 2.5: Pushing the fronlong context, arXiv preprint tier with advanced reasoning, multimodality, and next generation agentic capabilities, arXiv:2507.06261, 2025."
        }
    ],
    "affiliations": [
        "Academia Sinica, Taiwan",
        "Massachusetts Institute of Technology, USA",
        "National Taiwan University, Taiwan"
    ]
}