{
    "paper_title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
    "authors": [
        "Shufan Li",
        "Konstantinos Kallidromitis",
        "Akash Gokul",
        "Yusuke Kato",
        "Kazuki Kozuka",
        "Aditya Grover"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld"
        },
        {
            "title": "Start",
            "content": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents Shufan Li1, Konstantinos Kallidromitis2, Akash Gokul3 Yusuke Kato2, Kazuki Kozuka 2, Aditya Grover1 1 UCLA 2Panasonic AI Research 3Salesforce AI Research *Equal Contribution Correspondence to jacklishufan@cs.ucla.edu 5 2 0 2 6 1 ] . [ 1 4 1 0 4 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld 1. Introduction World models are capable of predicting future states of system given current observations, making them particularly useful in wide range of applications such as robotics, physical simulations, video games, and autonomous driving [18, 19, 32, 49, 52]. The most common approach for world modeling is action-conditioned causal video modeling [7, 10, 18], where neural network is employed to predict pixels in future video frames based on current and past frames, and action inputs. The predictions of these video world models can be naturally integrated into modelbased policies, which have demonstrated strong utility both in real-world tasks and in simulated environments [10, 36]. Despite their success in many domains, world models for graphical user interface (GUI) agents remain underexplored. key challenge in applying naive next-frame Figure 1. Model Performance on MobileWorldBench. We introduce MobileWorldBench, world modeling benchmark that tests vision language models (VLMs) capability to serve as world models for mobile agents. We also introduce MobileWorld, 1.4M dataset that can be used to improve VLMs world modeling capability. Starting with Qwen3-VL-8B-Instruct as the base model (Base), finetuning on MobileWorld leads to considerable performance gain (+SFT) on the next-state-generation task. prediction to GUI world modeling lies in the intrinsic complexity of the pixel-level prediction task. Lets use news app, e.g. the New York Times app, as an example  (Fig. 2)  . Suppose user is browsing list of news articles, and will perform the action scroll to the top. To predict the next state, pixel-space world model needs to: 1) Understand the high-level semantics of the action, namely that the scroll action will likely result in additional articles showing up, while keeping the majority of the UI layout unchanged (e.g. bottom navigation bar, top tabs, subscribe button), 2) Predict the visual layout of the future page: e.g. Does the new app present thumbnails and abstracts? Or does it only show headlines?, 3) Predict the exact content, which involves generating plausible news, 4) Render all texts and icons based on the predicted content and layout. Even if world model masters all four capabilities, it remains highly unlikely that the predicted next state will be 1 the current screen, MobileWorldBench evaluates VLMs ability to predict future states from the current screen and given user action. Notably, MobileWorldBench involves two tasks: Next-State-Generation and Next-State-QA. In the Next-State-Generation task, the model produces freeform text describing predicted state transitions, which are then evaluated by VLM judge that compares the description with the ground-truth screenshots of the next state. In Next-State-QA, the model answers series of yes-no questions about future states, and its performance is quantified using an accuracy metric that directly measures world modeling ability. Second, to facilitate the training of semantic world models for GUI agents, we curate MobileWorld, large-scale world modeling dataset consisting of the triplets: current state, user action, and future state. Future states are represented in three forms: pixels of screenshots, QA pairs, and natural language descriptions of state transitions. We construct MobileWorld by sourcing existing trajectory data and leveraging advanced VLMs to generate QA pairs and annotate state transitions using ground-truth next states. Finally, we analyze the effectiveness of semantic world models by finetuning an open-source VLM using MobileWorld. Experimental results show that mobile agents that use semantic world models perform better on the AndroidWorld [34] benchmark (+7.4% increase in success rate). 2. Related Works 2.1. World Modeling The predominant approach to world modelling is to generate future observations at the pixel level. This includes models that simulate GUI screens for desktops (NeuralOS [37]), predict video game scenes (MineWorld [19]), or generate controllable egocentric videos (GEM [20], Cosmos [18], Genie [7]). These pixel-level approaches, while highfidelity, are computationally intensive. Other methods improve efficiency by predicting in latent space rather than reconstructing pixels. Models like V-JEPA [3, 5] learn by predicting features of future video, enabling downstream tasks and latent-space planning. Recent approaches have moved beyond pixels to high-level semantics. This is seen in SWM [6], which reframes world modeling as VQA problem to predict textual descriptions of future states. Similarly, WMA [8] improves planning by predicting natural language descriptions of state differences in web navigation. The most relevant work in this space is ViMo (ViMo [30]), which uses diffusion models to generate pixel predictions of future screen in mobile apps. Unlike these works, we posit that pixel-level prediction is unnecessarily challenging and not essential for decision-making. We provide more detailed discussions about our contribution in relation to these works in the Appendix. Figure 2. Advantages of Semantic World Modeling. Pixel-space world modeling is particularly challenging as the model needs to identify changes, come up with the correct app content, and render them accurately. By contrast, Semantic World Modeling only focuses on abstracting relevant changes in GUI semantics, while being useful for decision-making. In the example shown, the frontier visual generative model GPT Image 1 struggles to accurately render GUI states, while frontier VLMs (GPT-4o [22]) can accurately describe the expected GUI changes in text. visually similar to the actual next state, since it is improbable that the predicted news is the same as the real news at the given time. Similarly, it is also hard to predict if new articles include thumbnails, short summaries, or are just titles. In general, 2), 3), and 4) are exceptionally hard tasks that are challenging for state-of-the-art visual generative models. Moreover, these details are typically not essential for model-based policies. For instance, if an agent wants to perform different action such as subscribe or navigate to view sports news, rendering the exact content of new articles that will show up is unnecessary. Rather than relying on high-dimensional pixel-level predictions, we hypothesize that GUI agents can achieve far more efficient and generalizable world modeling by representing state changes as structured, semantically meaningful textual representations. Notably, traces of this idea appear in the chain-of-thought behavior of top-performing large language models (LLMs), which often verbalize predictions about future states in their reasoning. Our contribution is to formalize this phenomenon as an explicit modeling framework, treating textual state descriptions as world models. In doing so, we transform what was previously byproduct of reasoning into principled mechanism for planning and control. Concretely, this work introduces three key contributions to facilitate the paradigm of semantic world modeling for mobile agents: First, we propose MobileWorldBench, comprehensive benchmark that explicitly evaluates VLMs world modeling capabilities. Unlike existing GUI understanding and grounding tasks, which focus on interpreting elements on 2.2. Mobile Agents Building agents capable of operating mobile devices has gained considerable interest in recent years. Early works like DroidBot-GPT [43] and AutoDroid [44] used structural data such as UI trees to represent mobile interfaces and leveraged LLMs for decision-making. More recent multimodal agents operate directly from visual inputs, using VLMs to interpret screenshots (e.g. AppAgent [50]). For decision-making, dominant paradigm is to use pretrained VLMs in zero-shot or few-shot manner. Works like Mobile-Agent-v2 [42] and AppAgent v2 [27] introduce additional constructs, such as memory or role-based decomposition, for improved performance. For training based approaches, several works build specialized architectures for enhanced GUI perception and grounding (e.g. CogAgent [21], CoCo-Agent [31], SeeClick [12]). Other works employ supervised finetuning (SFT) (e.g. GUI Odyssey [29]) or reinforcement learning (RL) (e.g. DigiRL [4], AutoGLM [28]) to improve navigation and decision-making capabilities. To select an action, these models typically choose from list of labeled UI elements as the target (e.g., DroidBotGPT [43]), or directly predict the exact pixel coordinates for an action (e.g. Mobile-Agent-v2 [42], AppAgent v2 [27]). Most of these agents operate in reactive loop, with cyclic observation-reason-action-observation processes. Figure 3. The Semantic World Model Paradigm. (Top): we factorize the classic pixel world models into two components. We call the first component the semantic world models. It predicts the latent distribution p(zt+1Xt, at) encoding high-level semantics. zt+1 can be queried via p(yz) to produce text descriptions, or through p(oiqi, z) to produce yes-no answers. (Bottom). To use semantic world models for decision-making, we employ modelbased policy framework that combines semantic WM with an action proposal model and value model. 2.3. GUI Datasets & Benchmarks 3. Method There are many GUI-focused datasets and benchmarks. Rico [16] collected static mobile GUI screens, while Screen2Words [41] provides single-screen text summaries. AITW (Android in the Wild) [34] introduced large-scale dataset of human demonstration trajectories on real Android devices. GUICourse [11] introduced series of datasets to progressively enhance an agents core OCR, grounding, and GUI knowledge. Among existing GUI benchmarks, Mind2Web [17], WebArena [53] and Mmina [39] evaluate web agents on wide range of tasks. Spotlight [25] and Ferret-UI [48] test visual grounding and understanding on mobile UIs. OmniACT [24] introduced benchmark for generating executable scripts across desktop and web applications. AgentStudio [51] consolidates existing benchmarks to better evaluate agents abilities in GUI grounding, learning, and success detection. OSWorld [45] and AndroidWorld [35] offer online evaluations in real-world environment across Ubuntu, Windows, macOS, and Android. GUI-World [9] provides rich benchmark based on video recordings of human demonstrations. Unlike existing datasets and benchmarks that focus either on decision-making or understanding observed GUI states, MobileWorld and MobileWorldBench exclusively focus on the ability to predict future states given current observations and user actions. 3.1. Semantic GUI World Modeling Given current GUI state Xt at timestep and user action input at, classic world modeling p(Xt+1Xt, at) directly predicts the high-dimensional pixel representation of the future state Xt+1. We break down this process into two steps. Given the state-action pairs (Xt, at), we first predict the high-level changes that will occur, such as the search bar will appear, or new word is typed in the input box. We represent these changes as latent variable zt+1. Then we can render the pixel-level details based on the predicted changes zt+1 and previous screen Xt. Formally, we factorize the classic world modeling objective p(Xt+1Xt, at) as: p(Xt+1Xt, at) = (cid:88) zt+ p(Xt+1zt+1, Xt, at)p(zt+1Xt, at) (1) (2) where p(Xt+1zt+1, Xt, at) is assumed to be inthe action at and can be written as dependent of p(Xt+1zt+1, Xt), i.e. once we know the changes that will occur, we can use it to determine the next state without knowing the action. We refer to the distribution p(zt+1 Xt, at) as the semantic world model. 3 Figure 4. Data Pipeline for MobileWorldBench. Our data pipeline consists of 5 steps: 1) Curating raw trajectories, 2) Using VLM to convert low-level actions to high-level actions and annotate state-change descriptions for reference, 3) We generate QA candidates for each state transition sampled, 4) We use VLM to filter these QA pairs through self-check and relevance metrics, 5) For the filtered data, we additionally use human verification to further filter the data based on its correctness and relevance. In practice, we use natural language descriptions to represent actions. We assume zt+1 lies in the hidden representation of VLM. To extract interpretable information from latent z, we define two queries on the latent variable. The first query p(yz) generates text description that summarizes the GUI changes encoded in z. Additionally, we also provide series of statements {qi}i=1,2..N about the semantics of the next state. The second query p(oiqi, z) can be used to obtain the likelihood of qi being true in the next GUI state. These two queries allow us to evaluate the quality of and meaningfully utilize them in decision-making. These setups are illustrated in Fig. 3 (Top). To utilize semantic world models for decision-making, we adopt model-based policy approach shown in Fig. 3 (Bottom). Given current state Xt, high-level goal G, and action proposals a1...ak, we first use the world model to t+1...zk predict z1 t+1, and then use value model to obtain corresponding scores v1 t+1. Finally, we use arg max to select the action at. In our setup, the action proposal model and value model are implemented using VLMs. For the value model, we first query text descriptions p(yz) and pass alongside high-level goal to obtain value scores. t+1...vk 3.2. MobileWorldBench We can evaluate semantic world model by measuring how well the latent zt+1 captures the high-level semantics of the state transition Xt Xt+1. This can be assessed in two ways: (1) how well the text p(y zt+1) describes the observed state transition in pixel space, and (2) how accurately the model estimates the likelihood p(oi t+1 qi, zt+1) that statements qi hold in the next state. These two aspects are measured using two separate tasks: Next-StateGeneration and Next-State-QA. 3.2.1. Task Definitions Next-State-Generation tasks prompt semantic world model to generate text descriptions yt+1 describing the expected state changes given inputs (Xt, at). We then use VLM judge model (GPT-4o) to evaluate these predictions. The judge model is provided with inputs Xt, at, yt+1, and ground-truth next state Xt+1. It then rates the quality of yt+1 based on three key metrics: Accuracy. Does the prediction accurately describe the changes in GUI states from Xt to Xt+1? model is penalized on this metric if it outputs objectively false statements (e.g. predicting that checkbox will be disabled when in reality it is not). Relevance. Does the prediction involve changes that are relevant to the action being performed? While statements like the system time will advance, are indeed accurate descriptions of the state transitions, they are often not relevant to the action. model is penalized on this metric if it produces many of these accurate but trivial descriptions. Note that we do not blindly discourage statements 4 Figure 5. Examples from MobileWorld Training Set. We show qualitative examples of Next-State-Generation and Next-State-QA tasks in the training set of MobileWorld, highlighting the effectiveness of our data pipeline. which assert that something, e.g. GUI element, relevant to the users action is unchanged after the state transition. Completeness. Does the prediction involve detailed description of all relevant changes? For example, if the action is to open the cart page, detailed description would be the action will navigate to cart page, which shows the list of items that are currently in the cart. It is very likely that there are buttons allowing the user to add new items, change the count of existing items, and delete existing items. There will also be button that will direct the user to checkout page. The completeness metric assigns high score to detailed descriptions. For each metric, the judge assigns score in the range of 0-5 for each output yt+1. The overall score for NextState-Generation is the sum of all three metrics, resulting in numerical score in the range of 0-15. Next-State-QA is visual question answering (VQA) task that poses yes-or-no questions about the future state Xt+1 given current state Xt and action at. This setup differs from existing GUI understanding and grounding tasks, which focus only on the observed GUI state or user interactions. We report answer accuracy as the evaluation metric for this task. 3.2.2. Data Generation and Filtering Pipeline Our pipeline for creating MobileWorldBench involves three steps: trajectory sourcing, VLM annotation, and qualitybased filtering. Trajectory Sourcing. To build MobileWorldBench, we source trajectories from the test split of the AndroidControl dataset [26], which contains human demonstration trajectories with (Xt, at, Xt+1) triplets. The actions at come in the form of natural language descriptions, e.g. Click on the OK option. We also source trajectories from Android in the Wild (AiTW) [34], which contains low-level user actions such as Click (233, 324). In total, 250 (Xt, at, Xt+1) triplets are sampled for the Next-State-Generation and 500 triplets are sampled for Next-State-QA. VLM Annotation. Since some trajectories only contain low-level actions, we employ frontier VLM (Qwen3-VL235B-A22B ) to convert these low-level actions to highlevel action descriptions. We find that naively passing in pixel coordinates leads to low performance, as the model struggles to correctly interpret these coordinate values. Instead, we create visualizations of actions by overlaying markers indicating the action on the screen [47]. We provide additional processing details in the Appendix. After we obtain high-level actions for all samples, we proceed to create QA proposal. We provide Xt, at, Xt+1 to GPT-4o and prompt the model to propose 8 questionanswer pairs based on the observed differences between Xt and Xt+1, this leads to 4,000 question-answer candidates. Quality-based Filtering. We first conduct human verification of the quality of high-level action descriptions. We find that these descriptions are of high quality and there are no obvious errors in all generated examples. After this step, the 250 samples for Next-State-Generation task can be used without further filtering, since the trajectories are sampled from real human demonstrations and the quality of action annotations has been checked. For Next-State-QA tasks, we heavily filter the machine-generated QA pairs to ensure only meaningful, non-trivial questions are retained. The filtering process consists of three steps. First, we prompt GPT-4o to answer its own questions by providing it with (Xt, at, Xt+1) triplets, and removing the questions that it fails to answer even with ground truth images (Self-Check). Second, we use GPT-4o as judge to filter out questions 5 Figure 6. Distribution of Tasks, Apps, and Actions in MobileWorldBench. MobileWorldBench covers wide range of task categories, Apps, and action types. We visualize their distributions for reference. about irrelevant GUI elements such as system time, battery level, etc. (unless the element is directly relevant to the user action). Finally, we perform human verification on the remaining question-answer pairs. Human evaluators are instructed to provide ground truth answer for the question based on (Xt, at, Xt+1), and asked to determine whether the question is relevant. We remove all instances where human answers disagree with the GPT-4o annotation and remove examples that are deemed irrelevant by annotators. The final filtered QA dataset contains 1,787 questions. 3.2.3. Coverage of Tasks, Apps, and Actions. We followed the task distribution of AiTW and Android Control to ensure fair coverage. This is visualized in Fig. 6. Specifically, each (Xt, at, Xt+1) comes from multi-turn human demonstration whose tasks fall into four broad categories: Google Apps (e.g. Mail, News, Calendar), System (e.g. install), Web Shopping (e.g. Amazon) and Other thirdparty applications. The sampled screenshots Xt can also come from 22 distinct apps, including Google Apps such as Google Maps and third-party apps like HBO Now, Skype, Twitter (X). We note that there is no strict mapping between task categories and Apps, and the distinction of Apps is not subdivision of more general task categories. For example, web shopping accounts for 38.8% of the tasks, but Google Chrome accounts for 54.4% of the screens, and contains examples of visiting non-shopping websites. We also visualize the distribution of user actions at, which include common mobile GUI interactions such as scroll, click, wait, etc. 3.3. MobileWorld Our second contribution is MobileWorld, large foundational dataset for training semantic world models, which consists of 1.4M samples. It includes state transition triplets Xt, at, Xt+1 sourced from human demonstrations, and text descriptions yt+1 describing the changes between Xt and Xt+1 resulting from action at, as well as question-answer pairs about Xt+1. We source Trajectory Sourcing. triplets of (Xt, at, Xt+1) from human demonstrations in AiTW and the Android Control dataset largely following the same pipeline as MobileWorldBench described in 3.2. Unlike MobileWorldBench, we source from the training split of these datasets. Annotations. We obtain raw text descriptions of state changes by prompting VLM to describe the observed differences between Xt and Xt+1. We also obtain questionanswer pairs by prompting the LLM to generate them based on observed state changes. We generate 3 text descriptions and 8 question-answer candidates per state transition. Unlike the MobileWorldBench benchmark, we do not use GPT-4o as our VLM due to cost concerns. Instead, we use strong open-sourced model Qwen3-VL-235B-A22B and Qwen3-VL-8B to generate annotations. 90% of the data is annotated using the 8B model, while 10% of the data is annotated using the 235B model. We denote these splits as pretraining and finetuning split. Post-processing. For each state transition Xt Xt+1, we use VLM-as-a-judge to pick the best of three candidate text descriptions using the same criteria (accuracy, completeness, relevance) as MobileWorldBench. We also apply the same VLM filtering techniques for question-answer pairs. The final filtered dataset consists of 543k questionanswer pairs and 942k state change descriptions. Due to cost concerns, the training set is not filtered by humans. Task Coverage. The task coverage and distribution of categories is similar to that of MobileWorldBench  (Fig. 6)  , spanning variety of tasks, apps, and user actions. We provide the full distribution of our training set in the Appendix. 4. Experiments We conduct extensive experiments to evaluate our benchmark, training dataset, and the proposed semantic world modeling paradigm. Evaluation of Frontier Models on World Modeling. We evaluate state-of-the-art VLMs on the Next-StateTable 1. Quantitive Results on Next-State-Generation Tasks. We report the performance of frontier VLMs, Qwen3-VL-8B-Instruct Baseline, and our finetuned model on Next-State-Generation tasks. We also reported the performance of running our training data annotation pipeline, denoted as Annotator. Name General Google Apps System Web Shopping Accuracy Completeness Relevance Overall Per Category Score Breakdown Score Qwen3-VL-235B-A22B-Annotator Qwen3-VL-8B-Instruct-Annotator 13.17 13.58 Intern-VL3-78B [54] Qwen3-VL-235B-A22B [46] Gemini-2.5-Flash [15] Gemini-2.5-Pro [15] Claude-Sonnet-4.5 [2] Claude-Sonnet-4 [1] GPT-4o [22] Qwen3-VL-8B-Instruct [46] +SFT (Ours) % (vs Baseline) 12.92 13.08 13.08 12.92 12.75 13.25 12.75 12.83 12.83 +0.0% With GT Next State Image 13.30 13.09 12.34 12.60 13.12 13.09 Without GT Next State Image 11.70 12.23 11.91 12.16 12.06 12.03 11.77 11.26 11.09 10.77 11.23 11.03 11.20 11.23 11.96 12.34 12.56 11.96 12.43 12.26 12. 4.45 4.42 4.02 4.11 4.14 4.04 4.14 4.07 4.11 4.02 4.02 3.41 3.53 3.48 3.49 3.49 3.59 3.44 4.61 4.61 4.38 4.51 4.45 4.46 4.48 4.41 4. 13.08 13.05 11.81 12.16 12.08 11.98 12.11 12.07 12.00 12.26 12.40 +1.1% 10.80 11.63 +7.7% 11.64 12.61 +8.3% 4.04 4.19 +3.7% 3.42 3.70 +8.2% 4.38 4.50 +2.7% 11.84 12.39 +4.7% Table 2. Quantitive Results on Next-State-QA Tasks. We report the performance of frontier VLMs, Qwen3-VL-8B-Instruct Baseline, and our finetuned model on Next-State-QA tasks. We also reported the results of human evaluation. Name Human Intern-VL3-78B [54] Qwen3-VL-235B-A22B [46] Gemini-2.5-Flash [15] Gemini-2.5-Pro [15] Claude-Sonnet-4.5 [2] Claude-Sonnet-4 [1] GPT-4o [22] Qwen3-VL-8B-Instruct [46] +SFT (Ours) % (vs Baseline) Params. Acc - 78B 235B - - - - - 8B 8B - 83.15 61.00 65.10 76.94 79.13 71.74 70.29 66.03 67.32 71.40 +4.08 Generation and Next-State-QA tasks, including closedsource models (Gemini 2.5 Flash, Gemini 2.5 Pro, Claude Sonnet 4.5, Claude Sonnet 4, GPT-4o [1, 2, 15, 33]) and open-source models (Qwen3-VL-235B-A22B, Intern-VL378B [46, 54]). For the Next-State-Generation task, we report the VLM judge score, using GPT-4o as the judge, measured in terms of accuracy, completeness, relevance metrics (as discussed in Sec. 3.2.1), and the overall score. For the Next-State-QA task, we report QA accuracy. Evaluation of Training Data Quality. To test the quality of the data pipeline of the pretraining data and finetuning data, we run these pipelines on the test set of NextState-Generation task. Unlike standard evaluation, our data pipeline also allows the model to access ground truth next state images Xt+1. We report the same benchmark scores, including accuracy, completeness, and relevance metrics. Model Training. To further evaluate if the proposed training data can be used to improve model performance, we finetune Qwen3-VL-8B-Instruct using the MobileWorld dataset. Further details can be found in the Appendix. 4.1. Main Results on MobileWorldBench We report evaluation results for Next-State-Generation (Tab. 1) and Next-State-QA (Tab. 2) tasks, and highlight key insights below. Existing models have considerable room for improvement. We observe significant performance gap between the best and worst models on both benchmarks. Notably, we find the best-performing model on the Next-State-QA benchmark, Gemini-2.5-Pro, is also one of the worst performing models on the Next-State-Generation benchmark. After careful inspection, we find that Gemini-2.5-Pro tends to generate long outputs with highly detailed, hallucinated descriptions of future states, leading to low accuracy scores. However, Gemini-2.5-Pro also has one of the highest relevance (4.48) and completeness (3.49) scores. This finding suggests that there are tradeoffs between different metrics, and Gemini-2.5-Pro prioritizes completeness and relevance over accuracy. We hypothesize that this behavior might emerge from its reasoning finetuning, as Gemini-2.5-Flash from the same family achieves much higher generation score (+0.10) while maintaining strong QA accuracy. Training Data Pipelines Yield High-Quality Annotations. In Tab. 1, both the pretraining data pipeline, which uses Qwen3-VL-8B-Instruct as the annotator, and the finetuning pipeline, which uses Qwen3-VL-235B-A22B as the 7 Table 3. Online Evaluation on AndroidWorld. We use M3A agent setup[35] using Qwen3-VL-235B-A22B backbone as baseline. We compare the performance of the baseline setup, using Qwen3-VL-235B-A22B as zero shot world model, and using our finetuned Qwen3-VL-8B-Instruct as the world model. Model AndroidWorld SR M3A+Qwen3-VL-235B-A22B +Semantic WM (Zero Shot) +Semantic WM (Ours) 46.9 50.8 54. annotator, outperforms all existing models, suggesting that the pipeline was able to utilize the provided next-state observations Xt+1 to generate high-quality descriptions of state transitions. These improvements in terms of accuracy and completeness scores are more pronounced than the improvements in relevance score, which is to be expected. Most notably, the gap between the pretraining and finetuning data pipeline is relatively small, suggesting that Qwen3VL-8B-Instruct has strong enough GUI understanding capabilities to serve as high-quality annotator. Finetuning on MobileWorld improves performance on both tasks. We find that our finetuned Qwen3-VL-8BInstruct improves considerably on both generation and QA tasks, highlighting the utility of the MobileWorld dataset. Notably, on generation tasks, the finetuned model achieves the highest accuracy (4.19) score and overall score (12.39). 4.2. Online Evaluation on AndroidWorld. Recent mobile GUI agents often rely on intricate scaffolds (e.g. memory, RAG) to maximize performance. To isolate the contribution of semantic world models, we conduct simple experiment comparing agents with and without semantic world model. We use M3A [35] as the base agent with Qwen3-VL-235B-A22B as the VLM. We evaluate three setups: (1) no world model, (2) Qwen3-VL-235BA22B as both policy and world model, and (3) our finetuned Qwen3-VL-8B-Instruct as the world model. For experiments using world models, we implement model-based GUI agent using the semantic world modeling framework discussed in Sec. 3.1. We report task success rates in Table 3. We find that using either world model outperforms the M3A baseline, with our fine-tuned world model leading to the best performance. 4.3. Human Evaluation. To further verify the performance of our finetuned model, we conduct an LM-Arena-style [14] human evaluation over 3,000 randomly sampled matches between models. Each match pairs outputs from two different models for the same question from the Next-State-Generation task of MobileWorldBench. We ask human judge to pick winner based Figure 7. Model ELO ratings from human evaluation. We perform user study that asks human evaluators to pick between the outputs of two models for the Next-State-Generation task. Finetuning on MobileWorld significantly improves performance. on how helpful the model outputs are for decision-making. The human judge also has access to ground truth next state images Xt+1. We report ELO scores which assigns numerical score to each model based on match results, and is widely used to evaluate LLMs, VLMs, image generators, and other types of generative AI models [13, 14, 23]. These results are shown in Fig. 7. Overall, human evaluation shows considerable performance gain after finetuning on MobileWorld. 5. Conclusion and Future Works. In this work, we advocate for adopting semantic world models that predict state transitions at higher level of abstraction than pixel-based world models. To support this paradigm, we introduce MobileWorldBench, high-quality, human-verified benchmark that evaluates semantic world models ability to accurately predict future states. We also present MobileWorld, large-scale dataset for training semantic world models, and demonstrate that training on this dataset leads to substantial performance gains. Finally, we showcase that semantic world models can be effectively integrated into mobile agents through simple model-based policy, seamlessly translating world-modeling capabilities into task success in real-world environments. However, our work has limitations. For instance, both MobileWorld and MobileWorldBench consist solely of human demonstrations on Android, as there is currently no large-scale collection of iOS demonstrations comparable to AiTW, nor is there an easy-to-use environment like AndroidWorld for benchmarking agents without real devices. We plan to extend our efforts to iOS and other GUI environments in future work 6. Acknowledgement AG would like to acknowledge the support from Schmidt Sciences and NSF Career Award #2341040. SL is in part 8 supported by Amazon Fellowship."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing claude 4.0, 2024. Blog post announcement. 7 [2] Anthropic. Introducing claude sonnet 4.5, 2025. Blog post announcement. 7 [3] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Selfsupervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 2 [4] Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-thewild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. [5] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learnarXiv preprint ing visual representations from video. arXiv:2404.08471, 2024. 2 [6] Jacob Berg, Chuning Zhu, Yanda Bao, Ishan Durugkar, and Abhishek Gupta. Semantic world models. arXiv preprint arXiv:2510.19818, 2025. 2 [7] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 1, 2 [8] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024. 2 [9] Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, et al. Gui-world: video benchmark and dataset for multimodal gui-oriented understanding. arXiv preprint arXiv:2406.10819, 2024. 3, 1 [10] Taiye Chen, Xun Hu, Zihan Ding, and Chi Jin. Learning world models for interactive video generation. arXiv preprint arXiv:2505.21996, 2025. 1 [11] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language model In Proceedings of the 63rd Annual to versatile gui agent. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2193621959, 2025. 3 [12] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93139332, 2024. [13] Wayne Chi, Valerie Chen, Anastasios Nikolas Angelopoulos, Wei-Lin Chiang, Aditya Mittal, Naman Jain, Tianjun Zhang, Ion Stoica, Chris Donahue, and Ameet Talwalkar. Copilot arena: platform for code llm evaluation in the wild. arXiv preprint arXiv:2502.09328, 2025. 8 [14] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. 8, 6 [15] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 7 [16] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building datadriven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pages 845854, 2017. 3, 1 [17] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. 3 [18] Jinwei Gu. Cosmos world foundation models for physical ai. In Proceedings of the 3rd International Workshop on Rich Media With Generative AI, pages 3939, 2025. 1, [19] Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, and Jiang Bian. Mineworld: real-time and open-source interactive world model on minecraft. arXiv preprint arXiv:2504.08388, 2025. 1, 2 [20] Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Pedro Rezende, Yasaman Haghighi, David Bruggemann, Isinsu Katircioglu, Lin Zhang, Xiaoran Chen, Suman Saha, et al. Gem: generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2240422415, 2025. 2 [21] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14281 14290, 2024. 3 [22] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 7 [23] Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. Advances in Neural Information Processing Systems, 37:7988979908, 2024. 8, 6 [24] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. 9 In European Conference on Computer Vision, pages 161 178. Springer, 2024. 3 conversational agents on mobile gui. arXiv:2205.11029, 2022. 1 arXiv preprint [25] Gang Li and Yang Li. Spotlight: Mobile ui understanding using vision-language models with focus. arXiv preprint arXiv:2209.14927, 2022. 3, 1 [26] Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37: 9213092154, 2024. [27] Yanda Li, Chi Zhang, Wenjia Jiang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824, 2024. 3 [28] Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. 3 [29] Quanfeng Lu, Wenqi Shao, Zitao Liu, Lingxiao Du, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, and Ping Luo. Guiodyssey: comprehensive dataset for cross-app gui navigation on mobile devices. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2240422414, 2025. 3 [30] Dezhao Luo, Bohan Tang, Kang Li, Georgios Papoudakis, Jifei Song, Shaogang Gong, Jianye Hao, Jun Wang, and Kun Shao. Vimo: generative visual gui world model for app agents. arXiv preprint arXiv:2504.13936, 2025. 2, 7 [31] Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. Coco-agent: comprehensive cognitive mllm agent for smartphone gui automation. arXiv preprint arXiv:2402.11941, 2024. 3 [32] Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, and Sai Rajeswar. Multimodal foundation In Multiworld models for generalist embodied agents. modal Foundation Model meets Embodied AI Workshop@ ICML2024, 2024. 1 [33] OpenAI. Hello gpt 4o, 2024. Blog post announcement. 7 [34] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: largescale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. 2, 3, 5, 1 [35] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. 3, 8, [36] Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, and Xiaodan Liang. Surfer: Progressive reasoning with world models for robotic manipulation. arXiv preprint arXiv:2306.11335, 2023. 1 [37] Luke Rivard, Sun Sun, Hongyu Guo, Wenhu Chen, and Yuntian Deng. Neuralos: Towards simulating operating systems via neural generative models. arXiv preprint arXiv:2507.08800, 2025. 2 [38] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multi-modal [39] Shulin Tian, Ziniu Zhang, Liang-Yu Chen, and Ziwei Liu. Mmina: Benchmarking multihop multimodal internet In Findings of the Association for Computational agents. Linguistics: ACL 2025, pages 1368213697, 2025. 3 [40] Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan. Ugif: Ui grounded instruction following. arXiv preprint arXiv:2211.07615, 2022. 1 [41] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology, pages 498510, 2021. 3, [42] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:26862710, 2024. 3 [43] Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. Droidbot-gpt: Gpt-powered ui automation for android. arXiv preprint arXiv:2304.07061, 2023. 3 [44] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Autodroid: Llm-powered task automation in android. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, pages 543557, 2024. 3 [45] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. 3 [46] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 7, 4 [47] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [48] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. In European Conference on Computer Vision, pages 240255. Springer, 2024. 3, 1 [49] Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, and Guanghui Ren. Ewmbench: Evaluating scene, motion, and semanarXiv preprint tic quality in embodied world models. arXiv:2505.09694, 2025. 1 [50] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 120, 2025. 3 10 [51] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan. Agentstudio: toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024. 3 [52] Yupeng Zheng, Pengxuan Yang, Zebin Xing, Qichao Zhang, Yuhang Zheng, Yinfeng Gao, Pengfei Li, Teng Zhang, Zhongpu Xia, Peng Jia, et al. World4drive: End-to-end autonomous driving via intention-aware physical latent world model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2863228642, 2025. [53] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 3, 6 [54] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 7 11 MobileWorldBench: Towards Semantic World Modeling For Mobile Agents"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Additional Technical Details 7.1. Dataset Statistics Comparison with Other Datasets. We compare the size and task coverage in Tab. 4. As shown, MobileWorld is the first dataset focusing on next-state prediction tasks. Its scale is also larger than most of existing datasets. We emphasize that while some datasets such as GUI-World also involves questions about multi-step trajectories, they still fall in the category of GUI understanding tasks and are fundamentally different from our world-modeling next-state prediction tasks. As concrete examples, GUI-World contains questions such as After moving the Steam window to the center, what did the user do next in the Edge browser? model is expected to answer this question based on list of video frames from screen recording, including all past and future actions. This is different setup than world modeling, where future states are not provided to the model. Dataset Size Sem. Task Coverage Rico [16] Screen2Words [41] MetaGUI [38] UGIF [40] AITW [34] Ferret-UI [48] Spotlight [25] GUI-WORLD [9] MobileWorld (Ours) 72,219 22,417 1,125 523 Low High Low High 715,142 High 123,702 2.5M 12,379 1.4M UI Code/Layout Generation UI Summarization Mobile Navigation GUI Parsing & Understanding Action Selection Low GUI Grounding & Understanding Low Both Both GUI Understanding GUI Understanding Next-State Prediction Table 4. Overview of existing mobile GUI datasets. Sem. Semantic level on instructions. Distribution of MobileWorld. We provided the distribution of apps and actions of MobileWorldBench in Figure 6 of the main paper. The distribution of apps and actions of MobileWorld is similar to that of MobileWorldBench since they are sampled from the same source of human demonstrations. For completeness, we also include detailed account of these distributions in Fig. 8. major difference is that there are more click operations in the training data, which can be attributed to the differences in the train and test split of source data. Figure 8. Distribution of Tasks, Apps, and Actions in MobileWorld. MobileWorld covers wide range of task categories, Apps, and action types. We visualize their distributions for reference. Figure 9. Action Annotation via Visual Overlay. To better allow VLM annotators to understand the action being performed, in addition to integer coordinates, we provide additional visual annotations on the coordinates of user gestures visually. 7.2. Data Pipeline 7.2.1. VLM Annotation In this section, we provide details of the VLM annotation process. Recall that the VLM annotators serve three goals. First, they convert low level actions such as Click (200,312) to high-level ones such as Click the return button. Second, they generate text descriptions of the visual differences in GUI state before and after an action is performed. Finally, they generate candidate QA pairs. Action Representation. Instead of only providing low level actions as text, we also annotate the image visually because we find VLMs struggle to map screen coordinates to GUI elements. An example is shown in Fig. 9, where we annotate click actions with cross and swipe or scroll actions with an arrow. High-level Action and Change Description. We provide total of three images in this annotation process. Image 1 and Image 2 are screenshots of mobile phones before certain action. Image 2 is Image One plus the action visualization. Image 3 is the screenshot after the action is performed. The full prompt is documented as below Annotation Prompt for High-level Action and Change Description You are given three images which are screenshots of mobile phones before and after certain action. Image 1 and Image 2 are states before the action. Image 2 is image one with an action annotated. The action coordinate is marked with blue cross. If the action is swipe, the swipe direction is marked with blue arrow. The end of the arrow indicates the end point of the swipe. Image 3 is the screenshot after the action is performed. The input images have the dimension of [[Height]]x[[Width]] (Height Width). The action will be provided in the prompt. The overall goal will also be provided. There are two tasks: You should also provide natural language description of the action based on the images. For example, if the raw action provided is Tap at (200, 1112), and the image shows that the user tapped on the Settings icon, you should describe the action as Tap on the Settings icon to open the setting menu. If action is already descriptive enough, (e.g, Input text usb-c to usb-a) without any coordinate, you can just repeat it. Feel free to add additional context based on the image to make the action more understandable, such as Input the text usb-c to usb-a in the search box to search for usb-c to usb-a adapters. Your task is to describe the difference between the two images in detail, focusing on the changes that occurred as result of the action. Please provide comprehensive description of the differences, including any visual changes, layout modifications, color alterations, text changes, or any other noticeable differences. The overall goal is for reference only, do not include it in your answer or rely on it too much. It may not be fully accurate. Limit your response to one small paragraph less than 200 worlds. Only desribe the changes, no need to say which things are not changed. Also feel free to ignore system status such as wifi,battery, etc. If action is not successful or nothing is changed, just say No changes observed after the action. And explain wht it fails The action is: {low level action } 2 The goal is {goal } Your respone should be in the following format: Action Description: [Your description of the action based on the images.] Change Description: [Your description of the changes between Image 1 and Image 3.] QA proposals. We sample 8 question-answer (QA) proposals for each (Xt, at, Xt+1) triplet. The prompts are documented as follows Annotation Prompt for Creating QA Candidates You are given two images representing the before and after states of an Android device screen. The action being performed is {action}. Now generate 5 QA pairs that test models understanding of the changes between these two images. Specifically, you should provide yes/no questions about UI states after the change, as well as ground truth answers based on the changes observed. Do you include changes that are not directly related to the action, such as system time, battery level, etc. For each QA pair, format it as: Q: [question] A: [answer] Provide 8 such QA pairs in total. Provida balanced mix of true and false answers. Use exactly the format provided, do not add any extra text or formats (e.g. bold) Given 500 state transitions, the total number of generated QA candidates are 4,000. 7.2.2. VLM Filtering We perform additional filtering to 1) remove questions whose answer does not accurately reflect state-changes 2) remove irrelevant question-answer pairs relating to system time, battery level, etc. In particular, we find 2) is necessary even though we have instructed the model not to produce this kind of questions during the generation process, as we find that the model will occasionally ignore these instructions and create trivial or irrelevant questions regardless. To achieve 1) removing questions whose answer does not accurately reflect state-changes, we perform self-check verification where the model is provided with the current state, user action, and ground-truth next state to answer question. The prompt is documented as follows: Prompt for VLM Self-Check You are an intelligent GUI agent capable of understanding GUIs and actions on mobile devices. Given the current GUI screenshot and input action, answer the following questions based on your predictions of the changes that will occur on the next screen after the action is performed. The action is {action} Answer with yes or no. The question is {question}. You will also be given ground truth next state image as image 2. To achieve 2) removing irrelevant question-answer pairs, we employ the following prompt. 3 Prompt for Relevance Filter You are given two images representing the before and after states of an Android device screen. The action being performed is {action}. question-answer proposal is given as follows: Q: {question} A: {Answer} Please decide if the question asks about expected state changes that are relevant to the action. Non-relevant actions include system time, battery level, Wi-Fi status, signal strength, etc., unless these aspects are directly related to the current action (e.g. enabling airplane mode will disable cellular signal). Note that even if question asks about some UI elements that did not actually change, it may still be relevant. For example, when the user is typing the city column of an address form, it is ok to ask if the actions will result in any changes in the street address column. Answer with simple yes-or-no. After the filtering process, we are left with 2,458 QA pairs out of 4,000 proposals. 7.2.3. Human Filtering For the remaining 2,458 QA pairs, we conduct additional human filtering to further ensure the quality of the benchmark. We employ Amazon Mechanical Turk for this process. To ensure the quality of the worker, the authors annotated small 100-sample test set manually and only employ workers with high performance on this test set to work on the full set. The human annotation process consists of the following two steps: Filtering Incorrect or Irrelevant QA pairs. The purpose of this step is similar to the VLM filtering process. We ask the human annotator to judge 1) if the ground truth answer is consistent with the observed state changes observed in screenshots, 2) If the question is relevant to the action being performed. We list the prompt in Figure 10a. Ambiguity. While question can be correct and relevant, it may be ambiguous or under-defined if it asks about something that is impossible to reasonable predict. For example, when opening the sports tab of new app, it is impossible to predict what the headline will be. Hence, we additionally ask human annotators to judge if the questions can be reasonably answered by an average user. The interface is shown in Fig. 10b After this process, we are left with 1,787 QA pairs as our benchmark. 8. Additional Experiment Details and Results 8.1. Training Setup We finetune the base model Qwen3-VL-8B-Instruct [46] on the proposed MobileWorld dataset for 2 epochs, with learning rate of 2 106 on 8 A6000 GPUs. The detailed training setup is listed in Tab. 5 Table 5. Training configurations on MobileWorld. We report the relevant hyperparameters for training, including the learning rate (LR), number of training steps, optimizer setup, image resolution for understanding and generation tasks. Learning Rate (LLM Backbone) Learning Rate (Vision Encoder) Epochs 1 2 Optimizer Image resolution Batch Size Warmup LR Schedule SFT 2 106 2 107 2 0.99 0.999 AdamW 1280px 128 6% Cosine 4 (a) Correctness and Relevance Filter. (b) Ambiguity Filter. Figure 10. Example Screenshots of the User Interface for Human Annotators Used in the Data Filtering Process. (a) Interface for applying correctness and relevance filters. (b) Interface for detecting ambiguous responses. 8.2. Evaluation Setup 8.2.1. Model Choice and Sampling Parameters In this section, we document the exact model version used to reproduce our main results, as well as the sampling parameters. The model ids of closed source models are listed in Tab. 6. For open-sourced models, we use their Hugging Face releases. For next-state-generation tasks, we use the default sampling parameters of respective models and do not pass in extra parameters. For next-state-QA tasks, we set temperature to 0.0 for reproducibility. We noticed that setting the temperature to 0.0 will lead to text repetitions in some models for next-state-generation tasks. Hence, we set temperature to 0.0 only for short QA tasks. Table 6. Closed-Source Model Version Mapping. We list the precise model identifiers corresponding to each name used in Table 1. Name in Table Specific Version / Model ID Gemini-2.5-Flash Gemini-2.5-Pro Claude-Sonnet-4 Claude-Sonnet-4.5 GPT-4o gemini-2.5-flash (stable) gemini-2.5-pro (stable) claude-sonnet-4-20250514 claude-sonnet-4-5-20250929 gpt-4o-2024-08-06 (default) 8.2.2. Automatic VLM Evaluation We employ GPT-4o as the judge model. We employ the following prompt to obtain the scores for next-state-generation results 5 Prompt for Automatic Evaluation You are judge to judge VLMss ability to understand the action on mobile device GUI. Given the current GUI screenshot and input action, the model will describe the changes that will occur on the next screen after the action is performed. The action is {action}. You are provided with the models response, an input image of the current state (first image), as well as ground truth next state image (second image). You are also given reference text that describes the actual changes that happened after performing the action. Your task is to evaluate the models response based on the following criteria: 1. Accuracy: Does the models description accurately reflect the changes that occurred in the GUI after the action? 2. Completeness: Does the model mention all significant changes that are visible in the next state image? 3. Relevance: Are all the changes mentioned by the model relevant to the action performed, or are there extraneous details? For each criterion, assign score from 1 to 5, where 1 is poor and 5 is excellent. After evaluating all three criteria, provide an overall score out of 15, along with brief justification for your scores. The models response is {response} The reference changes are {changes} Format your evaluation as follows: -Begin of responseAccuracy: [score] Completeness: [score] Relevance: [score] Overall Score: [total score] -End of responseuse the exact format without any additional text. 8.2.3. Human Evaluation In human evaluation, we find that forcing human evaluators to give numerical score leads to inconsistent behavior across different annotators, whose score range varies. To address this, we instead ask human evaluators to compare outputs from two models. We illustrated the human evaluation interface in Fig. 11. In total, we create 3,000 matches by randomly selecting benchmark entries and model pairs. We compute ELO scores following the setup of LM-Arena and Gen-AI-Arena [14, 23, 53] 8.2.4. AndroidWorld For our AndroidWorld baseline, we use M3A implemented via the official AndroidWorld codebase [35]. For our model-based policy, value model is needed to evaluate the predicted next states. We use the following prompt to obtain value scores for future states: Prompt for Value Model [...Omitted Background Description Copied From Action Selection Prompt of M3A ...] Now based on the goal and expected state changes caused by the proposed 8 actions, reason about the value of the resulting states from these actions and score each of them in the range of 1-10. Higher score means the resulting state is more likely on the right trajectory to reach the final goal. Think carefully before giving the final answer. Your Final answer should be the following format (Expected Change and Action should be copied from above) Action 1: {{action type:...}} Expected Change: ... Score Reason:... Score: ... Action 2: {{action type:...}} Expected Change: ... Score Reason:... Score: ... For better presentation, we omit the long background description that describes the current environment, past action history, overall goal, and some hard-coded tips like Sometimes you may need to navigate the phone to gather information needed to complete the task. Note that this portion is directly copied from M3A agents. 6 8.3. Additional Results 8.3.1. More Qualitative Examples. In Fig. 12, we provide additional qualitative examples on screenshots, user actions, and model outputs on MobileWorld, as well as the GPT judge score. Model performance varies across different samples. Our finetuned model is among the top performing models for most of the samples. 8.3.2. Model-Scaling To further analyze the difficulty of the proposed tasks and investigate the model scaling behavior, we conducted an additional experiment by finetuning smaller model Qwen3-VL-2B-Instruct following the same setup. We report results in Tab. 7. We observe that while the relative improvements of finetuning is larger for the 2B model (+5.4) versus the 8B model (+4.1), the 2B model significantly underperforms the 8B model. We believe this gap is largely caused by limited GUI understanding capabilities, of the 2B model. We verify this hypothesis by additionally evaluate the models performance when provided with the ground truth next-state image. This process coverts next-state-QA to standard VQA tasks where the model only needs to compare the differences in the two provided images. We find that GPT4o, Human annotator, and Qwen3-VL-8BInstruct achieves high scores in this setup (> 90%), while Qwen3-VL-2B-Instruct only achieves 70.0% accuracy, indicating its limited capacity. These findings indicate that the proposed world modeling task is challenging and is beyond the capacity of small VLMs. Table 7. Model Scaling Results. We provide qualitative results on next-state-QA tasks of finetuned Qwen3-VL-2B-Instruct and Qwen3VL-8B-Instruct. Model With GT Next State Image GPT-4o Human Qwen3-VL-8B-Instruct Qwen3-VL-2B-Instruct Acc 100.0 100.0 94.9 70.0 Without GT Next State Image Qwen3-VL-8B-Instruct Qwen3-VL-8B-Instruct +SFT(Ours) Qwen3-VL-2B-Instruct Qwen3-VL-2B-Instruct +SFT(Ours) 67.3 71.4 (+4.1) 60.6 66.0 (+5.4) 8.3.3. Combining Semantic World Models to Pixel-Space World Models While we argue that semantic world models are conceptually more useful and less expensive to train than pixel world models, we emphasize that conceptually semantic world models can be considered as sub-process of the classic world models, as noted in Sec. 3.1. To validate this paradigm, we can pass the output of our semantic world model to state-of-the-art pixel renderer to achieve pixel world models. We illustrate this approach in Figure 13, where we combine our finetuned VLM with frontier image generators such as Nano-Banana [15] to produce pixel outputs. We find that accurate text prediction can be used to create highly plausible next-state screenshots. We emphasize that this works focus on the semantic level, we left more explorations in pixel world modeling for future works. 9. Reproducibility Statements All artifacts, including fine-tuned model weights, evaluation dataset, and training dataset will be open-sourced. We will also release the evaluation codebase for Next-State-Generation, Next-State-QA, and AndroidWorld experiments. 10. Additional Discussion with Related Works. We note that line of literature tangent to this work, notably ViMo [30] focuses on building pixel-space GUI world models by addressing challenges such as text rendering and GUI consistency. We are unable to compare with ViMo because the authors did not release the model checkpoints, training data, or evaluation data. 7 Figure 11. Human Evaluation Interface of Comparing Two Models. 8 Figure 12. Qualitative Examples of MobileWorld Evaluation Results. 9 Figure 13. Qualitative Results of Pixel Modelling We combine the output of semantic world models with pixel renders to obtain image predictions. Gemini Nano Banana Pro achieves the best qualitative results. We demonstrate that using text as guidance in the generation process results in prediction much closer to the ground truth."
        }
    ],
    "affiliations": [
        "Panasonic AI Research",
        "Salesforce AI Research",
        "UCLA"
    ]
}