{
    "paper_title": "BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via Adaptive Block-Based Gaussian Splatting",
    "authors": [
        "Yongchang Wu",
        "Zipeng Qi",
        "Zhenwei Shi",
        "Zhengxia Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian"
        },
        {
            "title": "Start",
            "content": "BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via Adaptive Block-Based Gaussian Splatting Yongchang Wu, Zipeng Qi, Zhenwei Shi and Zhengxia Zou 1 5 2 0 2 5 1 ] . [ 2 8 4 0 9 0 . 4 0 5 2 : r Fig. 1: BlockGaussian reconstructs city-scale scenes from massive multi-view images and enables high-quality novel view synthesis from arbitrary viewpoints, as illustrated in the surrounding images. Compared to existing methods, our approach reduces reconstruction time from hours to minutes while achieving superior rendering quality in most scenes. AbstractThe recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, novel framework incorporating content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality largescale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the groundtruth supervision, which enhances the reconstruction quality. Furthermore, we propose pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on largescale scenes demonstrate that our approach achieves stateof-the-art performance in both reconstruction efficiency and rendering quality, with 5 speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian. Index TermsLarge-scale Scene Reconstruction, Gaussian Splatting, Novel View Synthesis. I. INTRODUCTION ARGE-scale scene high-fidelity and real-time novel view synthesis is essential for many applications, including autonomous driving [1][3], virtual reality [4], [5], remote sensing photogrammetry [6], [7], and embodied intelligence. Recently, prominent novel view synthesis approaches have fallen into two main categories: Neural Radiance Fields (NeRF)-based methods [8][11] and Gaussian Splatting-based techniques [12][14]. Neural Radiance Fields (NeRF) [8], due to their capability in high-fidelity rendering through 2 implicit representation, have been extended to large-scale scene reconstruction tasks [15][17]. Although Block-NeRF [17] achieved large-scale reconstruction of San Francisco neighborhoods, the scene representation using MLP networks as the smallest unit lacks flexibility and struggles with slow rendering speeds. 3D Gaussian Splatting [12], as an alternative, demonstrates more significant potential, particularly with its fast rendering speed. Explicit point clouds scene representation makes it more scalable for large-scale scenes [18][21]. The divide-and-conquer paradigm [15], [19], [20] has become the mainstream of large-scale scene novel view synthesis constrained by hardware VRAM resources. By dividing the scene into subregions, the reconstruction speed has been significantly improved in multi-GPU parallel. This paradigm consists of three key stages: scene partitioning, individual block optimization, and fusion of block reconstruction results. These stages exhibit strictly sequential dependency, meaning the input of each subsequent stage entirely relies on the output of the preceding phase. The quality of the final reconstruction result is contingent upon the effectiveness of each stage. Although existing research methods have established baseline, challenges remain in these steps, including imbalanced reconstruction complexity across blocks, supervision mismatch in block-wise optimization, and quality degradation in fusion results. The imbalanced reconstruction complexity across blocks arises from unreasonable scene partitioning, which reduces the efficiency of large-scale scene reconstruction, especially on multi-GPU devices. As illustrated in Fig.2 (a), evenly dividing the scene into grids ignores the content disparity of different regions. When partitioning the scene, two critical factors must be considered: the granularity of block division and the computational loads across blocks. The former requires attention to the complexity of different scene regions, with higher granularity for areas of interest or greater complexity. The latter aims to balance the computational loads across blocks, thereby reducing the training time of the entire scene in multi-GPU. VastGaussian [19] introduced progressive data partitioning strategy, which divides the scene based on camera positions. DOGS [21] improved VastGaussians partitioning method by proposing recursive method to balance computational loads across blocks. However, scene partitioning based on camera positions is limited by the spatial distribution of cameras and struggles to generalize to scenes with more complex viewpoint distributions. CityGaussian [20] firstly trains coarse Gaussian as the scene prior, using it to partition the scene into grids. Nevertheless, this partitioning method requires pre-training coarse Gaussian model, which does not fully decouple scene scale from the optimization process. The supervision mismatch in block-wise optimization leads to artifacts within individual blocks, degrading scene reconstruction quality. NeRF/Gaussian-based scene representations employ an end-to-end optimization pipeline, where the parameters of the scene representation are optimized by constructing an objective function that compares ground-truth images with rendered images. However, under the divide-andconquer paradigm for large-scale scene reconstruction, the lack of entire scene representation leads to visibility problems during individual block optimization. As illustrated in Fig.2 (b), after scene partitioning, the content of training view may be distributed across multiple blocks, indicating that single block only corresponds to portion of the image scope. When reconstructing the concerned block, mismatch arises between the rendered image and the training view during loss calculation. This mismatch stems from two factors: a) the image rendering process from the concerned block ignores occlusion relationships between blocks; b) it is difficult to calculate the accurate boundary with under-optimized block representations. Noisy supervision confuses the gradients of Gaussian parameters during end-to-end optimization, leading to degradation reconstruction results. Seamless scene fusion avoiding quality degradation is another critical challenge in large-scale scene reconstruction. Optimizing individual blocks tends to produce floaters in the airspace due to the lack of accurate geometric supervision, leading to degenerate solutions. This significantly degrades the rendering quality after block fusion. As shown in Fig.2 (c), floaters in airspace may fit the training-views well, but cause artifacts in novel views, especially at block boundaries. Therefore, adequate airspace supervision is vital for the scene training process. VastGaussian [19] tries to solve this problem by introducing more training views and designs an airspaceaware visibility calculation method that selects viewpoints based on the proportion of the projected polygon of the blocks boundary. However, this approach has two limitations: it ignores occlusion relationships between blocks, and the selected viewpoints tend to introduce additional regions outside the block, raising contradiction between viewpoint selection and adequate supervision. To address these challenges, we propose BlockGaussian, new framework for large-scale novel view synthesis. In the scene division stage, we propose spatial-based scene partitioning method termed Content-Aware Scene Partition, which dynamically and finely divides the scene based on sparse point cloud output from the prior Structure from Motion [22] process while comprehensively considering the computational loads across multiple blocks. To relieve the supervision mismatch issue during individual block reconstruction, we remodel the single-block optimization problem and propose visibility-aware optimization algorithm. During the optimization process, auxiliary point clouds are introduced to adaptively represent the invisible regions of the training view to relieve the supervision mismatch issue. The experimental results demonstrate the effectiveness of the auxiliary point cloud. For airspace supervision, given the complexity of occlusion relationships in the scene, directly selecting viewpoints which can provide adequate airspace supervision for the current block is challenging. Unlike VastGaussian [19], we design Pseudo-View Geometry Constraint to supervise airspace without introducing regions outside the concerned block. Specifically, we perturb training camera poses to generate pseudo viewpoints. With the rendered depth maps, we warp the ground truth images from the original viewpoints and compute the loss corresponding to the images rendered from the pseudo viewpoints. This constraint significantly improves block fusion quality, especially for interactive rendering. Fig. 2: Existing challenges in large-scale scene novel view synthesis task under divide-and-conquer paradigm. a) Imbalanced reconstruction complexity across blocks: The intensity of content in different scene regions exhibits significant differences. Areas with dense content require finer subdivision granularity to ensure reconstruction fidelity, while sparser-content regions benefit from coarser partitioning to enhance computational efficiency. b) Supervision mismatch in block-wise optimization: The content of training view may be divided into multiple blocks after scene partitioning. Due to visibility constraints, the entire training view image does not match the ideal supervision when optimizing the individual block. c) Quality degradation in fusion results: Floater in airspace is an important reason for the quality degradation of fusion results. Since each block is optimized individually, these floaters fit well in the training perspective but degrade the quality of the synthesized novel views, especially in the boundary region. Experimental results demonstrate that our proposed method effectively addresses the challenges in large-scale scene reconstruction. As shown in Fig.1, in terms of both reconstruction quality and speed, BlockGaussian achieves state-of-the-art (SOTA) performance across multiple scenes, with 5 speedup in optimization and an average PSNR improvement of 1.21 dB. Regarding hardware requirements, BlockGaussian can be executed sequentially on single 24GB VRAM GPU or parallel across multiple GPUs. Furthermore, our method exhibits strong generalization capabilities, performing well in aerial-view scenes and street-view scenes. Our contributions are summarized as follows: 1) We propose BlockGaussian for large-scale scene novel view synthesis, with spatial-based scene partitioning paradigm that dynamically balances the granularity of block division and the computational loads across blocks. 2) We remodel the training process of individual blocks and propose visibility-aware block optimization method by introducing auxiliary point clouds to address the mismatch between rendered images and supervised viewpoints. 3) We introduce novel pseudo-view geometry constraint to supervise the airspace. This constraint can effectively mitigate rendering quality degradation caused by airspace floaters during block fusion, ensuring seamless and high-quality scene fusion. II. RELATED WORK A. Novel View Synthesis Novel view synthesis (NVS) is fundamental problem in computer vision and graphics, aiming to synthesize photorealistic images of scene from viewpoints that were not captured initially. Early image-based rendering techniques, such as light field rendering [23] and view morphing [24], use collections of images to synthesize novel views by interpolating between captured viewpoints. These methods often assume dense scene sampling, limiting their practicality in real-world scenarios. Depth-based methods, such as 3D warping [25], use depth maps to project pixels from source images to the target viewpoint. While effective, these approaches are sensitive to depth estimation errors and often produce artifacts in regions with occlusions or complex geometry. The advent of deep learning has revolutionized NVS, enabling data-driven approaches that learn to synthesize novel views directly from images without explicit 3D reconstruction. DeepStereo [26] uses deep network to predict novel views from sparse set of input images by learning to interpolate between them. Similarly, Multi-plane images(MPI) [27] represents scenes as layered depth images and use neural networks to refine and synthesize novel views. Differentiable rendering techniques have significantly propelled progress in novel view synthesis methods. Neural Radiance Fields [8] stands as landmark approach, representing 4 scene as continuous volumetric function parameterized by neural network. Subsequent works have improved upon vanilla NeRF in various aspects, including reconstruction and rendering efficiency [28][30], anti-aliasing [9], [10], sparse input [31], appearance consistency [32], [33], and scene generalizability [34][36]. Gaussian Splatting [12], achieving real-time novel view synthesis through efficient rasterization, has emerged as another milestone in novel view synthesis. SparseGS [37] focuses on few-shot novel view synthesis and introduces depthregularization to remove floater artifacts. LightGaussian [38] and Compact3d [39] manage to reduce the model size and remove redundant 3D Gaussians through exhaustive quantization. The explicit representation inherent in 3D Gaussian Splatting facilitates its application in downstream tasks, such as autonomous driving [2], human body representation [40], navigation [41][43] and so on. B. Large Scale Scene Reconstruction Significant progress has been made in large-scale scene reconstruction in recent years. Traditional scene reconstruction pipelines [22], [44] typically involve several sequential steps: feature extraction and matching, camera parameter estimation, dense reconstruction, meshing, and texture mapping, which collectively recover the geometry and appearance of the scene. Structure from Motion (SfM) encompasses feature extraction, matching, camera parameter estimation, outputting camera parameters and sparse point clouds of the scene. Due to its stability and robustness, SfM based on feature points and bundle adjustment remains the mainstream framework for pose estimation and sparse reconstruction. Traditional appearance reconstruction pipelines take the camera parameters as input, generate dense depth maps through multi-view stereo (MVS) methods [45][49], and then produce mesh-based scene representation via meshing [50] and texture mapping [51], [52]. In recent years, with the development of differentiable rendering techniques, end-to-end optimization-based reconstruction methods, such as Neural Radiance Fields [8] and 3D Gaussian Splatting [12], have achieved superior reconstruction results compared to traditional step-by-step approaches. For large-scale scenes, the divide-and-conquer strategy is widely adopted solution for handling massive datasets. This approach divides the scene into grids, optimizes each grid separately, and then merges the reconstruction results to obtain complete scene representation. NeRF-based methods have successfully reconstructed street and aerial views of largescale scenes. For instance, Block-NeRF [17] reconstructs San Francisco neighborhoods from 2.8 million street-view images and accounts for transient objects and appearance variations by modifying the underlying NeRF architecture. Mega-NeRF [15] introduces sparse and spatially aware network structure to represent aerial scenes and makes valuable attempts at interactive rendering. Switch-NeRF [16] designs learnable scene decomposition based on sparse large-scale NeRF representations. Grid-NeRF [53] integrates MLP-based NeRF with feature grids to encode local and global scene information. Although its two-branch design achieves high visual fidelity in rendering, it remains constrained by the slow training and rendering speeds inherent to NeRF. On the other hand, due to significant advantages in rendering speed, gaussianbased methods are increasingly being explored for large-scale scene applications. Hierarchy-GS [18] proposes hierarchical representation and optimizes chunk parameters in parallel, leveraging the divide-and-conquer strategy. Scaffold-GS [54] combines explicit and implicit representations, achieving more compact scene representation while maintaining high-quality view synthesis. Octree-GS [55] introduce Level-of-detail (LOD) to 3D Gaussian Splatting, using novel octree structure to organize anchor Gaussians hierarchically to achieve real-time rendering. Concurrent works with our method include VastGaussian [19], CityGaussian [20], and DOGS [21]. These three methods all adopt paradigm of scene partitioning, viewpoint assignment, parallel optimization, and scene fusion to reconstruct large-scale scenes. VastGaussian designs progressive data partitioning strategy to divide the scene and allocate training views. DOGS refines the scene partitioning process with recursive approach, aiming to split the scene into blocks with more balanced distribution of cameras. However, both methods rely on the camera position distribution for block partitioning, neglecting the misalignment between the scene content distribution and the camera distribution. This limitation poses challenges for downstream tasks, such as dynamic map loading. In CityGaussian, global coarse Gaussian model is first trained to guide scene partitioning and viewpoint allocation, which becomes difficult to implement under limited computational resources. Unlike VastGaussian and DOGS, which partition the scene based on camera position, our method proposes spatial-based block partitioning strategy. This approach ensures more flexible and adaptive division of the scene and balances the computational loads between blocks. Additionally, we remodel the individual block optimization problem, propose visibility-aware optimization algorithm, and design pseudo-view geometry constraint during optimization. These innovations effectively mitigate rendering inconsistencies and improve the quality of individual block reconstructions, facilitating seamless block merging and improving the fidelity of synthesized views. III. PRELIMINARY This section briefly introduces the vanilla 3D Gaussian Splatting on which our BlockGaussian is based. 3D Gaussian Splatting utilizes discrete Gaussian primitives in 3D space, denoted as = {Gk}, where each gaussian primitive Gk consists of learnable attributes, including position xk, rotation Rk, opacity ok, scales sk, and spherical harmonics(SH) [56] coefficients fk. During the rendering process, each 3D Gaussian primitive is projected onto the image plane as 2D Gaussian. Volume rendering [12], [32] is then performed to compute the final RGB values for each pixel. The rendering process can be formulated as follows: = (cid:88) i=1 i1 (cid:89) αici (1 αj) j=1 (1) where denotes the pixel color, while ci represents the RGB color of the Gaussian primitive computed based on spherical 5 Fig. 3: Overview of our proposed method. We first divide the entire scene and allocates viewpoints with Content-Aware Scene Partition, which jointly considering the complexity of scene content and the computational load distribution across blocks. Subsequently, we optimize each block independently, which is executable either sequentially on single GPU or in parallel across multiple GPUs. During block optimization, we introduce auxiliary point clouds (aux pts) to address supervision mismatch issues. Pseudo-View Geometry Constraint is conducted to supervise airspace regions and mitigate floater artifacts. Finally, the optimized results from all blocks are integrated to construct comprehensive Gaussian Representation of the entire scene, enabling interactive novel view synthesis. harmonics (SH) features fi. α refers to the transparency weight derived from the projected 2D Gaussian covariance and the Gaussian opacity ok. Similarly, the depth map can be computed pixel-by-pixel following the alpha blending process [37]. Here, di represents the depth value of the gaussian primitives center point in the camera space. D(u, v) = (cid:88) i=1 i1 (cid:89) αidi (1 αj) j=1 (2) The reconstruction and optimization of the scene commence with the known viewpoints = {(I gt , Ri, ti)} and the sparse point cloud. Here, gt , Ri, and ti denote the ground truth image, camera orientation, and camera position for the i-th viewpoint, respectively. The camera poses (Ri, ti) along with the sparse point cloud P, are estimated through the Structure from Motion (SfM) process. For each training view, the rendered image is computed following = R(Ri, ti, G). The parameters of the 3D Gaussian are optimized by minimizing the loss between the rendered image and the ground truth image. L(I gt , i) = (1 λ)L1(I i, gt ) + λLSSIM(I i, gt ) (3) λ represents weighting hyperparameter. To enhance the reconstruction quality of fine details, the densification process is conducted concurrently along with optimization, which supplements the scene representation with additional optimizable variables based on gradient information. IV. METHOD Large-scale scenes present significant challenges due to the extensive area and massive amounts of data. We adopt the divide-and-conquer paradigm similar to previous work [15], [19][21]. The overview of our method is shown in Fig.3. Given collection of captured images, we first calculate the camera poses and sparse point cloud for each viewpoint with Structure from Motion. Then, we iteratively partition the scene into blocks and assign supervised views to each block with our Context-Aware Scene Partition module, as detailed in Section IV-A. Subsequently, blocks are trained separately under Visibility-Aware Block Optimization, as discussed in Section IV-B. Section IV-C elaborates the Pseudo-View Geometry Constraint provides airspace supervision. Finally, we seamlessly integrate all the blocks to obtain unified scene representation in Section IV-D. A. Content-Aware Scene Partition Scene partitioning and view assignment are critical steps in reconstructing large-scale scenes. When partitioning the scene, it is essential to balance the trade-off between the granularity of the blocks and the speed of parallel optimization. Intuitively, higher granularity in block partitioning can improve reconstruction quality, but it often leads to slow reconstruction speed. Conversely, lower granularity reduces the time cost of the reconstruction process but at the expense of decreased reconstruction precision. Therefore, during scene partitioning and view assignment, two primary objectives must be satisfied: 1) Adaptive partitioning based on scene complexity: The partitioning should adapt to the complexity of the spatial scene structure, applying different granularity levels to regions with varying importance and complexity. 2) Balanced computational load: The partitioning should ensure an even distribution of computational load across blocks, which is crucial for minimizing the time required for multiGPU scene reconstruction. The density distribution of the sparse point cloud in scene can serve as an estimator of the scenes content complexity. Based on this assumption, we recursively partition the scene into multiple blocks. Specifically, given the sparse point cloud Ps of the scene, we estimate the normal direction of the ground plane using the Manhattan world assumption [57] and align it with the y-axis. The sparse point clouds are projected onto the x-z plane, and bounding rectangle is manually defined as the region of interest (RoI) for reconstruction. Subsequently, we partition the reconstruction area into multiple blocks in binary tree structure. Given the maximum depth of the binary tree and the maximum number of points in leaf node, the initial RoI region of the scene is taken as the root node for recursive partitioning. if the current nodes depth < and the number of contained point clouds Nb > , it is bisected along the longest edge of the corresponding block to generate two child nodes. Otherwise, the partitioning terminates, marking the node as leaf node. This process is iteratively executed until all nodes meet the termination condition, thereby achieving spatially adaptive scene partitioning. The view assignment process aims to select appropriate supervisory training views for each block. We score the relevancy between each training view and the blocks. The number of visible key points Nv for each training view can be obtained from the Structure from Motion results. For each block, we can count the number of points Nb within the boundary. Training views with ratio /Nv greater than thresh(0.3) are selected as supervised views for the current block. The detailed scene partitioning and view assignment procedures are outlined in Algorithm 1. B. Visibility-Aware Block Optimization Thanks to well-designed scene partition strategy, the optimization between blocks is completely independent and can be trained in parallel on multiple GPUs. For single block optimization, the Gaussian primitives within the block can be represented as Gb = {Gn }, termed block Gaussians. The supervised views associated with the current block are denoted as {I gt }, and the corresponding camera poses are represented as {Ri, ti}. Rendering images = R(Ri, ti, Gb) only based on the Gaussians within the current block often fails to cover the entire region of training views. This limitation introduces erroneous supervision when computing the loss with the ground truth image gt . To address this issue, we introduce auxiliary Gaussian primitives Ga = {Gm }, termed auxiliary Gaussians, which model the scene regions outside the current block for the supervised views. Given the camera pose (Ri, ti) of supervised view, the image rendering process is now expressed as = R(Ri, ti, Gb, Ga) (4) The initialization of block Gaussians follows the same procedure as in vanilla 3D Gaussian Splatting. Specifically, points that locate in the spatial bounds of the block are initialized as Gb. Auxiliary Gaussians Ga are initialized from the sparse point clouds associated with the supervised views of the current block but out of the block range. 6 Algorithm 1 Scene Partitioning and View Assignment Require: Scene ROI bounding box Bs; Max tree depth ; ; Scene Sparse Pointcloud Block Point Num Threshold Ps; View assignment ratio threshold ratiot. Ensure: Leaf blocks boxes and assigned view. 1: function ASSIGNVIEWFORBLOCK(B, Ps) 2: 3: 4: 5: Vb {} Pb select points in block from Ps. for each view in train views do count visible points in Pb. Nv the number of points visible in . if /Nv < ratiot then Vb Vb {V }. 6: 7: 8: 9: 10: 11: 12: end function end if end for return Vb, Nb. 13: function PARTITION(Bc, d) 14: 15: 16: Vb, Nb ASSIGNVIEWFORBLOCK(Bc, Ps). if Nb > and < then Bc1, Bc2 split Bc along longer edge. PARTITION(Bc1, + 1). PARTITION(Bc2, + 1). else 17: 18: 19: 20: 21: 22: end function end if {Bc}. 23: Execution: Initialize with PARTITION(Bs, 0). For each view, the photometric loss is the same as that of 3DGS, as mentioned in Section III. In addition, we use depth maps as priors during optimization following the prior works [37], [58]. Depth maps De are estimated by DepthAnythingV2 [59] and calculate absolute error with rendered depths Dr in inverse space after scale alignment. Ldepth(De, Dr) = L1( 1 De , 1 Dr ) (5) Given supervised views for block, the objective function for the optimization process can be formulated as follows: argmin Ga,Gb (cid:88) i=1 (L(I gt , i) + Ldepth(De , Dr i)) (6) It is evident that the optimization of auxiliary Gaussians Ga suffers from insufficient supervision. Directly applying the same optimization paradigm as in 3D Gaussian Splatting (3DGS) would lead to the degradation of auxiliary Gaussians, which in turn adversely affects the optimization of the primary focus Gb. To mitigate this issue, we introduce mini-batch optimization strategy to enhance the stability of the optimization process. Given collection of supervised perspectives, mini-batch optimization is employed to increase the stability of the gradients. We accumulate the gradients from several training views to update the properties of the 3D representation Gb, Ga. 7 disparity space. Given hyperparameter representing the disparity perturbation in the horizontal direction, the positional disturbance is computed as = [ median(Dr ref) , 0, 0]T , (7) where median(Dr ref, represents the x-axis focal length of ref-view camera. The camera parameters of the pseudo-view are then expressed as: ref) refers to the median value of Dr (Kpse, Rpse, tpse) = (Kref, Rref, tref + t) (8)"
        },
        {
            "title": "We follow the process below to warp the image I r",
            "content": "pse from the pseudo-view to the ref-view perspective based on the rendered depth map Dr pse. For pixel in the pseudo-view rendered image pse(upse, vpse), its corresponding depth value zpse = Dr pse(upse, vpse) is used to compute its projected position in the ref-view. We firstly restore the position of each pixel in pseudo-view camera space with the following equation. xpse ypse zpse = K1 pse upse vpse 1 zpse (9) Next, these points are reprojected into the world coordinate system. = R1 pse xw yw zw xpse ypse zpse R1 pse tpse (10) The obtained world coordinates are then transformed into the ref-view camera space using the extrinsic parameters of the reference camera. xref yref zref = Rref xw yw zw + tref (11) Subsequently, we obtain the corresponding pixel coordinates in the reference-view image space by applying the intrinsic camera transformation. zref = Kref uref vref 1 xref yref zref (12) By mapping each pixel position individually, we obtain the warped image warp and the corresponding validity mask . The pseudo-view geometry loss Lpse is then formulated as follows: Lpse = L1(I gt ref, warp) (13) The Pseudo-View Geometry Constraint achieves indirect supervision of the rendered depth, which significantly helps in removing floaters in airspace. D. Scene Merging and Rendering Once all block optimization processes are completed, we merge the block reconstruction results to obtain the entire scene representation. Thanks to the well-designed block optimization process and the pseudo-view geometry constraint, we can directly merge the scene after cropping the auxiliary Gaussians Ga. When rendering the novel view, BlockGaussian follows the Fig. 4: Illustration of the Pseudo-View Geometry Constraint. Typically, artifacts in the airspace can fit RGB images well with inaccurate depth. To address this, we impose constraints on depth to suppress floaters generated in the airspace. For each training view, we generate pseudo-view by applying slight perturbations to the camera pose. Then we warp the pseudo-view rendered image pse utilizing rendered depth map Dr warp and train-view ground-truth gt warp. The loss calculated between ref provides depth supervision. pse to train-view During densification, we only densify the Gaussian points in the block, which effectively reduces the redundancy in optimization process. C. Pseudo-view Geometry Constraint As mentioned in VastGaussian [19], supervising the airspace during the end-to-end optimization process is crucial. We observe that floaters in the airspace cause the degradation of rendered image quality after scene fusion. To address this issue, we propose pseudo-view geometry constraint loss to effectively supervise the airspace without introducing additional views to the concerned block. The process is illustrated in Fig.4. Specifically, for training view denoted as the reference view (ref-view), whose camera parameters are (Kref, Rref, tref) and whose rendered depth is denoted as Dr ref, we perturb the pose of the ref-view to obtain the camera pose of the pseudo-view. To finely control the magnitude of the perturbation, the disturbance is calculated in same differentiable rendering pipeline as the original 3D Gaussian Splatting framework [12]. Given the target camera pose and intrinsic parameters, the scene representationcomposed of all blocks Gaussian primitivesis projected onto the image plane. These Gaussians are then alpha-blended in depth-ordered manner to synthesize the view following the Eqn.1. V. EXPERIMENTS A. Experiments Setup Datasets. We conducted comprehensive evaluations of our proposed methods on three benchmark datasets: Mill19 [15], UrbanScene3D [60], and MatrixCity [61]. The Mill19 and UrbanScene3D datasets comprise aerial imagery captured through real-world drones, with each scene containing thousands of high-resolution images. We maintained consistent dataset partitioning with Mega-NeRF in the training and testing phases. To facilitate fair comparison across all experiments, we uniformly applied 4 downsampling to each image following previous approaches [19], [20]. Metrics. To quantitatively evaluate the quality of novel view synthesis, we employed three widely recognized metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [62]. Considering the inherent photometric variations in scene imagery data, we implemented color correction consistent with VastGaussian to the rendered images for metric computation. To quantitatively evaluate the efficiency, we report the optimization time consumption, allocated VRAM and the number of Gaussian points of each scene. Compared Methods. We conducted extensive comparative experiments against state-of-the-art large-scale scene reconstruction methods, which can be categorized into NeRF-based approaches and 3D Gaussian Splatting based methods. The NeRF-based baselines encompass Mega-NeRF [15] and SwitchNeRF [16], while the 3DGS-based methods include VastGaussian [19], CityGaussian [20], DOGS [21], and modified 3DGS [12]. It is noteworthy that for CityGaussian, the coarse-stage training proved infeasible within 24GB VRAM constraints for certain scenes. Thus, we directly adopted the metrics reported in their original publication. Additionally, it should be noted that DOGS employs 6 downsampling strategy during image preprocessing, which may bring advantages in quantitative metric evaluation. For the efficiency evaluation, we implement the algorithms on 8 RTX4090 GPUs platform. Notably, cause different methods may have different number of blocks due to the scene partition strategy, we report the total reconstruction time consumption of each scene, ignoring the case that the number of blocks is less than 8. Implementation Details. Thanks to the complete independence of individual block optimization, the training process can be efficiently parallelized across multiple GPUs or sequentially executed block-by-block on single GPU. In our proposed method, each block is trained up to 40000 / 60000 iterations (BlockGaussian-40K / BlockGaussian-60K), with densification performed every 200 iterations. During optimization, PseudoView Geometry Constraint is conducted from 10k iteration, with loss weight gradually increases logarithmically from 0.1 to 1.0, and the depth regularization weight gradually decreases from 1.0 to 0.1. 8 B. Comparison with Other Methods Reonstruction Quality. We present the average PSNR, SSIM, and LPIPS metrics of BlockGaussian across multiple scenes from Mill19, UrbanScene3D, and MatrixCity datasets in Tab.I and TabIII. Compared to existing methods, BlockGaussian-40K achieves comparable performance after 40k training iterations and BlockGaussian-60K outperforms existing methods in most scenes, particularly regarding SSIM and LPIPS metrics, indicating that the synthesized novel views exhibit superior perceptual details. Compared to NeRF-based methods, BlockGaussian achieves better rendering results. As shown in Fig.5, due to implicit scene representation with multilayer perceptron networks, NeRF-based methods tend to produce blurry and overly smooth results. BlockGaussian reconstructs more accurately in high-frequency regions of the scene, which is attributed to point-based representation. Vanilla 3D Gaussian Splatting (3DGS) struggles with areas rich in details (1st row in Fig.5) due to insufficient points. The reconstruction results of 3DGS often exhibit numerous floaters in airspace, which are detrimental to interactive rendering. In contrast to Gaussianbased methods, BlockGaussian excels in reconstructing edge and high-frequency regions (1st row in Fig.6) as well as structurally repetitive areas (2nd row in Fig.6). In addition to aerial scenes, we also evaluate our method on street-view scene MatrixCity-Street. Without any scene-specific tuning, our method demonstrates significant improvements over existing methods, achieving substantial leads in PSNR (+3.87dB), SSIM (+0.169), and LPIPS (-0.377) metrics as shown in Tab.III and Fig.7. Efficiency and Consumption. As shown in Tab. II, we compare optimization time, final point counts, and VRAM consumption across methods. Mega-NeRF, Switch-NeRF, VastGaussian, DOGS, and BlockGaussian are trained on 8 RTX 4090 GPUs, while 3DGS uses single RTX 4090 GPU. The hyperparameter Batchsize of BlockGaussian is set to 1 to match other methods. For CityGaussian, we evaluated metrics using the published checkpoints. Traditional NeRF-based methods, Mega-NeRF and Switch-NeRF, exhibit significantly longer optimization times (over 19 hours) and require substantial computational resources. In contrast, Gaussian-based approaches demonstrate considerably lower optimization times and more efficient memory usage. Notably, BlockGaussian achieves the fastest optimization times, completing optimization in minutes rather than hours. Increasing the number of optimization iterations from 40K to 60K slightly raises both optimization time and VRAM usage but remains computationally feasible. Our method demonstrates significantly faster optimization while generating more points for scene representation, justifying its superior reconstruction quality. This comes at the cost of higher VRAM usage during rendering, but is still much faster than NeRF-based methods. TABLE I: Quantitative comparison of novel view synthesis results on Mill19 [15] and UrbanScene3D [60] dataset. The best, the second best, and the third best results are highlighted in red , orange and yellow . 9 Scenes building PSNR SSIM LPIPS 0.454 0.547 20.92 0.397 0.579 21.54 0.289 0.735 20.23 0.225 0.728 21.80 0.246 0.778 21.55 0.204 0.759 22.73 BlockGaussian-40K 21.72 0.222 0.762 BlockGaussian-60K 22.05 0.206 0.775 Mega-NeRF Switch-NeRF 3DGS VastGaussian CityGaussian DOGS rubble SSIM LPIPS 0.508 0.553 0.478 0.562 0.253 0.755 0.264 0.742 0.228 0.813 0.257 0.765 0.213 0.816 0.200 0.824 PSNR 24.06 24.31 25.24 25.20 25.77 25.78 26.18 26.33 residence PSNR SSIM LPIPS 0.401 0.628 22.08 0.352 0.654 22.57 0.232 0.791 21.21 0.261 0.699 21.01 0.211 0.813 22.00 0.244 0.74 21.94 0.196 0.821 22.63 0.182 0.838 23.25 sci-art PSNR SSIM LPIPS 0.312 0.770 25.60 0.271 0.795 26.51 0.245 0.821 21.21 0.261 0.761 22.64 0.230 0.837 21.39 0.219 0.804 24.42 0.208 0.848 24.69 0.171 0.881 25. TABLE II: Quantitative comparison of novel view synthesis results on Mill19 [15] and UrbanScene3D [60] dataset. We present the optimization time OptTime (hh:mm), the number of final points (106) and the allocated VRAM (GB) during evaluation. Scenes Mega-NeRF Switch-NeRF 3DGS VastGaussian CityGaussian DOGS BlockGaussian-40K BlockGaussian-60K building Points OptTime VRAM OptTime 19:49 24:46 11:26 03:26 - 03:51 00:32 01:09 - - 5.71 5.6 13.30 6.89 13.6 17.96 5.84 5.84 3.82 3.07 8.80 3.39 9.10 11.12 30:48 38: 08:22 02:30 - 02:25 00:25 00:52 rubble Points - - 3.97 4.71 9.60 4.74 10.43 12. VRAM OptTime residence Points VRAM OptTime sci-art Points VRAM 5.88 5. 2.95 2.74 6.55 2.54 6.80 7.76 27:20 35:11 11:31 03:12 - 04:33 00:29 01:01 - - 5.86 6.26 10.80 7.64 11.29 12.94 5.99 5.94 3.78 3.67 7.89 6.11 8.30 10.05 27:39 34: 10:32 02:33 - 04:23 00:27 00:51 - - 4.77 4.21 5.37 5.67 4.58 5.40 5.97 5. 3.65 3.54 3.49 3.53 3.35 3.60 TABLE III: Quantitative comparison on MatrixCity [61]. The best results are highlighted in bold. Scenes 3DGS VastGaussian CityGaussian DOGS BlockGaussian MatrixCity-Aerial MatrixCity-Street PSNR SSIM LPIPS OptTime 27.83 28.33 27.46 28.58 29.32 16:21 05:53 - 06:34 01:42 0.229 0.22 0.204 0.219 0.112 0.821 0.835 0.865 0.847 0.908 Points 11.5 12.5 23.7 10.3 36. PSNR SSIM LPIPS OptTime 20.92 - - 21.61 25.48 10:22 - - 02:33 00:53 0.624 - - 0.649 0.272 0.655 - - 0.652 0.821 Points 3.56 - - 2.37 5.99 C. Ablation Study We conduct ablation experiments to evaluate the individual contributions of three key components in our proposed framework: Content-Aware Scene Partition, Visibility-aware Block Optimization, and Pseudo-view Geometry Constraint. In addition, we investigate the impact of key hyper-parameters on model performance. The results validate the necessity of each component and provide insights for future improvements and potential simplifications of the framework. 1) Content-Aware Scene Partition: The results of the scene partition are presented in Tab.IV and Fig.8. Nblocks denotes the total number of blocks partitioned. The terms mean views and max views represent the average and maximum number of views per block, respectively. mean indicate the average and maximum number (106) of initial sparse point clouds within each block. Through Content-Aware Scene Partitioning, we have managed to control the number of initial point clouds within each block to similar range. This is because the quantity of sparse point clouds roughly reflects the complexity of the and max pts pts scene content in that area. As can be observed from Tab.IV, when the number of point clouds within blocks is similar, the variation in the number of views across different scene blocks is considerable, particularly in urban scenes such as residence and MatrixCity-Aerial scenes. This suggests weak correlation between the complexity of scene content and the number of views. As illustrated in Fig.8, our proposed strategy enables adaptive scene partitioning based on the distribution of sparse point clouds, thereby balancing the computation complexity across different blocks. The overall reconstruction speed of the scene is positively correlated with the number of partitioned blocks and the complexity of reconstructing single block. Since the reconstruction process for each block is entirely independent, block optimization can be performed sequentially on single GPU or in parallel across multiple GPUs. Here, we report the optimization time of the most time-consuming block denoted as tmax and the total execution time when processed opt sequentially on single GPU denoted as ttotal opt with the hyperparameter Batchsize=1. The relationship between these times 10 Fig. 5: Qualitative Results on Mill19 and UrbanScene3D Datasets. Fig. 6: Qualitative Results on MatrixCity Dataset. approximately follows ttotal opt , which indicates that the partitioning strategy effectively balances the computational load across all blocks. opt 0.75 Nblocks tmax 2) Visibility-aware Block Optimization: As illustrated in Tab.V and Fig.10, we report the ablation study results of Visibility-Aware block Optimization in the rubble scene. Aux and Bopt refer to auxiliary points and mini-batch strategy during block optimization. From the 1st and 3rd row of Tab.V, the quality of scene reconstruction is significantly improved by incorporating auxiliary points. We visualize the rendering results of supervised view rendering with block points Gb and auxiliary points Ga. After scene partitioning, the content of the training view is divided into two blocks, and this view simultaneously supervises the reconstruction process of both blocks. When optimizing block_1, the auxiliary points accurately fills the invisible regions of the supervised view (3rd row of Fig.10), which indicates that we have achieved an accurate match between the current block and the visible area of the supervised image. Thereby, in the optimization results of block_1 and block_2, no floaters are generated (2nd row of Fig.10). Lines 1-2 of Tab.V demonstrate that the mini-batch optimization strategy improves scene reconstruction quality. In addition, we observe that in scenes with significant lighting variations, optimizing the scene with mini-batch effectively mitigates the generation of floaters in airspace regions, which benefit from more stable gradients during densification. By combining the mini-batch optimization strategy and auxiliary 11 Fig. 7: Qualitative Results on MatrixCity Street Scene. TABLE IV: Scene partition results of multiple datasets. Nblocks mean Scenes building rubble residence sciart MC-Street MC-Aerial 8 6 7 8 7 16 views max views mean pts 0.44 707 660 0.35 711 434 0.35 1078 622 0.32 940 342 0.27 1129 874 0.26 838 496 max pts 0.47 0.59 0.57 0.42 0.36 0.37 tmax opt 01:09 00:52 01:01 00:51 00:53 00: ttotal opt 06:28 04:11 04:57 06:21 04:42 10:52 TABLE V: Ablation experiments of our method. Aux Bopt Lpse PSNR 24.60 24.87 25. 26.23 26.33 SSIM 0.787 0.807 0. 0.823 0.824 LPIPS 0.240 0.216 0. 0.205 0.200 TABLE VI: Ablation experiments of the number of blocks. Nblock 2 NGPU 2 6 8 4 6 8 PSNR 26.99 26.09 26.33 26.16 SSIM 0. 0.827 0.824 0.819 LPIPS 0.199 0. 0.200 0.207 TABLE VII: Ablation experiments of optimization hyperparameters. Batchsize PSNR SSIM LPIPS Points OptTime 1 2 3 4 25.89 26.12 26.18 26. 0.810 0.816 0.822 0.824 0.211 0.208 0.206 0. 12.23 12.76 12.90 13.10 00:52 01:30 02:09 02: points, our method achieves notable improvement of +1.6 dB in PSNR, along with corresponding enhancements in SSIM and LPIPS metrics. 3) Pseudo-view Geometry Constraint: As shown in Line 6 of Tab.V, the Pseudo-view Geometry Constraint contributes to measurable improvement in the metrics on the test views, indicating that BlockGaussian can reconstruct more accurate and consistent geometry. This effect becomes even more pronounced when wandering through the scene, as illustrated in Fig.9. By supervising the airspace region, the floaters are effectively mitigated, significantly enhancing the image quality of interactive view synthesis. 4) Effect of Batchsize: In Tab. VII, we investigate the effect of hyper-parameter Batchsize to the reconstruction quality. Increasing the Batchsize leads to steady improvement in PSNR and SSIM while reducing LPIPS, indicating better reconstruction quality. Specifically, PSNR increases from 25.89 to 26.33, SSIM improves from 0.810 to 0.824, and LPIPS decreases from 0.211 to 0.200, demonstrating that larger batch sizes contribute to enhanced perceptual and structural fidelity. This improvement can be attributed to enhanced gradient stability, which effectively facilitates the densification process. Meanwhile, this improvement comes at the cost of increased optimization time. The trade-off between performance and computational cost should be considered when selecting an appropriate batch size for practical applications. 5) Number of blocks: We investigate the effect of number of blocks in BlockGaussian. By adjusting the hyper-parameter 12 Fig. 8: The visualization of scene partition result. The scene is divided into blocks of different sizes according to the density distribution of the sparse point cloud. And the computational load is balanced among multiple blocks. 13 VI. DISCUSSION Although BlockGaussian demonstrates impressive optimization speed and view synthesis quality, several limitations remain. First, similar to the original 3D Gaussian representation, BlockGaussian requires substantial number of points to represent intricate scene details. Enhancing the compactness of point cloud representation, as exemplified by LightGaussian [38], represents promising direction for improvement. Furthermore, to achieve interactive rendering for large-scale scenes, integrating Level-of-Detail (LoD) techniques [55] with dynamic map loading becomes essential. Such integration would enable better compatibility between large-scale scene 3D Gaussian representation and existing rendering pipelines. VII. CONCLUSION This paper introduces BlockGaussian, framework for novel view synthesis for large-scale scenes. The proposed ContentAware Scene Partition strategically divides the scene while jointly considering the complexity of scene content and the reconstruction computational loads distribution across blocks. Our Visibility-Aware Block Optimization effectively addresses the challenges posed by invisible regions in supervised views during the reconstruction of individual blocks. The PseudoView Geometry Constraint suppresses the generation of floaters in airspace, facilitating the interactive rendering. Notably, our algorithm can be implemented sequentially on single GPU or in parallel across multiple GPUs. BlockGaussian achieves stateof-the-art performance in view synthesis quality across multiple large-scale scene datasets. In addition, we plan to further explore efficient representations of Gaussian and interactive rendering with dynamic scene map loading in future work. REFERENCES [1] Z. Yang, Y. Chai, D. Anguelov, Y. Zhou, P. Sun, D. Erhan, S. Rafferty, and H. Kretzschmar, Surfelgan: Synthesizing realistic sensor data for autonomous driving, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11 11811 127. [2] Y. Yan, H. Lin, C. Zhou, W. Wang, H. Sun, K. Zhan, X. Lang, X. Zhou, and S. Peng, Street gaussians: Modeling dynamic urban scenes with gaussian splatting, in European Conference on Computer Vision. Springer, 2024, pp. 156173. [3] Y. Chen, J. Zhang, Z. Xie, W. Li, F. Zhang, J. Lu, and L. Zhang, S-nerf++: Autonomous driving simulation via neural reconstruction and generation, IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 119, 2025. [4] H. Cho, J. Kim, and W. Woo, Novel view synthesis with multiple 360 images for large-scale 6-dof virtual reality system, in 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEE, 2019, pp. 880881. [5] M. Wang, W.-T. Shu, Y.-J. Li, and W. Li, Can get there? negotiated userto-user teleportations in social vr, IEEE Transactions on Visualization and Computer Graphics, 2025. [6] Y. Wu, Z. Zou, and Z. Shi, Remote sensing novel view synthesis with implicit multiplane representations, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 113, 2022. [7] Y. Xu, T. Wang, Z. Zhan, and X. Wang, Mega-nerf++: An improved scalable nerfs for high-resolution photogrammetric images, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. 48, pp. 769776, 2024. [8] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, Nerf: Representing scenes as neural radiance fields for view synthesis, Communications of the ACM, vol. 65, no. 1, pp. 99106, 2021. Fig. 9: The visualization of Pseudo-View Geometry Constraint ablation experiment. Artifacts are marked within red circles. Fig. 10: Visualization of block optimization results. Top: local scene partition result. Middle: rendered by points in block. Bottom: rendered by block and auxiliary points, ground-truth image. maximum tree depth and block point number threshold in Context-Aware Scene Partition stage, we realize the scene partition of different block numbers. As shown in Table VI, increasing the number of blocks brings variation in PSNR metric. Meanwhile, this introduces slight variations in perceptual metrics: SSIM and LPIPS fluctuate within narrow range. We attribute this PSNR decline to inter-block illumination inconsistencies, where localized lighting variations affect PSNR more significantly than perceptual metrics. 14 [9] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan, Mip-nerf: multiscale representation for anti-aliasing neural radiance fields, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 58555864. [10] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, Mip-nerf 360: Unbounded anti-aliased neural radiance fields, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 54705479. [11] K. Gao, Y. Gao, H. He, D. Lu, L. Xu, and J. Li, Nerf: Neural radiance field in 3d vision, comprehensive review, arXiv preprint arXiv:2210.00379, 2022. [12] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., vol. 42, no. 4, pp. 1391, 2023. [13] B. Huang, Z. Yu, A. Chen, A. Geiger, and S. Gao, 2d gaussian splatting for geometrically accurate radiance fields, in ACM SIGGRAPH 2024 conference papers, 2024, pp. 111. [14] G. Chen and W. Wang, survey on 3d gaussian splatting, arXiv preprint arXiv:2401.03890, 2024. [15] H. Turki, D. Ramanan, and M. Satyanarayanan, Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 12 92212 931. [16] M. Zhenxing and D. Xu, Switch-nerf: Learning scene decomposition with mixture of experts for large-scale neural radiance fields, in The Eleventh International Conference on Learning Representations, 2022. [17] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar, Block-nerf: Scalable large scene neural view synthesis, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 82488258. [18] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and G. Drettakis, hierarchical 3d gaussian representation for real-time rendering of very large datasets, ACM Transactions on Graphics (TOG), vol. 43, no. 4, pp. 115, 2024. [19] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, S. Xu, Y. Yan et al., Vastgaussian: Vast 3d gaussians for large scene reconstruction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 51665175. [20] Y. Liu, C. Luo, L. Fan, N. Wang, J. Peng, and Z. Zhang, Citygaussian: Real-time high-quality large-scale scene rendering with gaussians, in European Conference on Computer Vision. Springer, 2024, pp. 265282. [21] Y. Chen and G. H. Lee, Dogs: Distributed-oriented gaussian splatting for large-scale 3d reconstruction via gaussian consensus, Advances in Neural Information Processing Systems, vol. 37, pp. 34 48734 512, 2025. [22] J. L. Schonberger and J.-M. Frahm, Structure-from-motion revisited, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 41044113. [23] M. Levoy and P. Hanrahan, Light field rendering, in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 441452. [24] S. M. Seitz and C. R. Dyer, View morphing, in Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, 1996, pp. 2130. [25] L. McMillan and G. Bishop, Head-tracked stereoscopic display using image warping, in Stereoscopic Displays and Virtual Reality Systems II, vol. 2409. SPIE, 1995, pp. 2130. [26] J. Flynn, I. Neulander, J. Philbin, and N. Snavely, Deepstereo: Learning to predict new views from the worlds imagery, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 55155524. [27] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, Stereo magnification: Learning view synthesis using multiplane images, arXiv preprint arXiv:1805.09817, 2018. [28] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and A. Kanazawa, Plenoxels: Radiance fields without neural networks, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 55015510. [29] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, Tensorf: Tensorial radiance fields, in European conference on computer vision. Springer, 2022, pp. 333350. [30] T. Müller, A. Evans, C. Schied, and A. Keller, Instant neural graphics primitives with multiresolution hash encoding, ACM transactions on graphics (TOG), vol. 41, no. 4, pp. 115, 2022. [31] Y.-J. Yuan, Y.-K. Lai, Y.-H. Huang, L. Kobbelt, and L. Gao, Neural radiance fields from sparse rgb-d images for high-quality view synthesis, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 7, pp. 87138728, 2022. [32] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Dosovitskiy, and D. Duckworth, Nerf in the wild: Neural radiance fields for unconstrained photo collections, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 7210 7219. [33] D. Verbin, P. Hedman, B. Mildenhall, T. Zickler, J. T. Barron, and P. P. Srinivasan, Ref-nerf: Structured view-dependent appearance for neural radiance fields, IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 112, 2024. [34] A. Chen, Z. Xu, F. Zhao, X. Zhang, F. Xiang, J. Yu, and H. Su, Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 14 12414 133. [35] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser, Ibrnet: Learning multi-view image-based rendering, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 4690 4699. [36] P. Nguyen-Ha, L. Huynh, E. Rahtu, J. Matas, and J. Heikkilä, Cascaded and generalizable neural radiance fields for fast view synthesis, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 5, pp. 27582769, 2023. [37] H. Xiong, SparseGS: Real-time 360 sparse view synthesis using Gaussian splatting. University of California, Los Angeles, 2024. [38] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. Xu, Z. Wang et al., Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps, Advances in neural information processing systems, vol. 37, pp. 140 138 140 158, 2024. [39] K. Navaneet, K. P. Meibodi, S. A. Koohpayegani, and H. Pirsiavash, Compact3d: Compressing gaussian splat radiance field models with vector quantization, arXiv preprint arXiv:2311.18159, vol. 4, 2023. [40] M. Kocabas, J.-H. R. Chang, J. Gabriel, O. Tuzel, and A. Ranjan, Hugs: Human gaussian splats, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 505515. [41] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, Splatam: Splat track & map 3d gaussians for dense rgb-d slam, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 21 35721 366. [42] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, Gaussian splatting slam, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 03918 048. [43] X. Lei, M. Wang, W. Zhou, and H. Li, Gaussnav: Gaussian splatting for visual navigation, IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 114, 2025. [44] S. Agarwal, Y. Furukawa, N. Snavely, I. Simon, B. Curless, S. M. Seitz, and R. Szeliski, Building rome in day, Communications of the ACM, vol. 54, no. 10, pp. 105112, 2011. [45] M. Bleyer, C. Rhemann, and C. Rother, Patchmatch stereo-stereo matching with slanted support windows. in Bmvc, vol. 11, no. 2011, 2011, pp. 111. [46] S. Galliani, K. Lasinger, and K. Schindler, Massively parallel multiview stereopsis by surface normal diffusion, in Proceedings of the IEEE international conference on computer vision, 2015, pp. 873881. [47] R. Chen, S. Han, J. Xu, and H. Su, Visibility-aware point-based multi-view stereo network, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 10, pp. 36953708, 2021. [48] Z. Liang, Y. Guo, Y. Feng, W. Chen, L. Qiao, L. Zhou, J. Zhang, and H. Liu, Stereo matching using multi-level cost volume and multi-scale feature constancy, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 1, pp. 300315, 2021. [49] Q. Xu, W. Kong, W. Tao, and M. Pollefeys, Multi-scale geometric consistency guided and planar prior assisted multi-view stereo, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 49454963, 2023. [50] F. Lafarge, R. Keriven, M. Brédif, and H.-H. Vu, hybrid multiview stereo algorithm for modeling urban scenes, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 517, 2013. [51] J. L. Schönberger and J.-M. Frahm, Structure-from-motion revisited, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 41044113. [52] L. Han, S. Gu, D. Zhong, S. Quan, and L. Fang, Real-time globally consistent dense 3d reconstruction with online texturing, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 3, pp. 15191533, 2022. [53] L. Xu, Y. Xiangli, S. Peng, X. Pan, N. Zhao, C. Theobalt, B. Dai, and D. Lin, Grid-guided neural radiance fields for large urban scenes, 15 in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 82968306. [54] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, Scaffold-gs: Structured 3d gaussians for view-adaptive rendering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20 65420 664. [55] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians, arXiv preprint arXiv:2403.17898, 2024. [56] R. Ramamoorthi and P. Hanrahan, An efficient representation for irradiance environment maps, in Proceedings of the 28th annual conference on Computer graphics and interactive techniques, 2001, pp. 497500. [57] J. M. Coughlan and A. L. Yuille, Manhattan world: Compass direction from single image by bayesian inference, in Proceedings of the seventh IEEE international conference on computer vision, vol. 2. IEEE, 1999, pp. 941947. [58] J. Chung, J. Oh, and K. M. Lee, Depth-regularized optimization for 3d gaussian splatting in few-shot images, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 811820. [59] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, arXiv:2406.09414, 2024. [60] L. Lin, Y. Liu, Y. Hu, X. Yan, K. Xie, and H. Huang, Capturing, reconstructing, and simulating: the urbanscene3d dataset, in ECCV, 2022, pp. 93109. [61] Y. Li, L. Jiang, L. Xu, Y. Xiangli, Z. Wang, D. Lin, and B. Dai, Matrixcity: large-scale city dataset for city-scale neural rendering and beyond, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 32053215. [62] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, The unreasonable effectiveness of deep features as perceptual metric, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586595."
        }
    ],
    "affiliations": []
}