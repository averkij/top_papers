{
    "paper_title": "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs",
    "authors": [
        "Junying Chen",
        "Zhenyang Cai",
        "Ke Ji",
        "Xidong Wang",
        "Wanlong Liu",
        "Rongsheng Wang",
        "Jianye Hou",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 5 2 9 8 1 . 2 1 4 2 : r HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs Junying Chen1, Zhenyang Cai1, Ke Ji1, Xidong Wang1, Wanlong Liu1 Rongsheng Wang1, Jianye Hou1, Benyou Wang1,2 1 The Chinese University of Hong Kong, Shenzhen 2 Shenzhen Research Institute of Big Data https://github.com/FreedomIntelligence/HuatuoGPT-o"
        },
        {
            "title": "Abstract",
            "content": "The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through two-stage approach: (1) using the verifier to guide the search for complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains."
        },
        {
            "title": "Introduction",
            "content": "The release of OpenAI o1 has marked significant milestone in large language model (LLM) development, showcasing impressive capabilities [13]. This breakthrough highlights the potential of scaling Chain-of-Thought (CoT) and reinforcement learning to enhance LLM performance [46]. While subsequent research efforts attempt to replicate these advancements, they often remain limited to mathematical reasoning tasks [79, 6]. The application of o1-like methods to specialized fields, such as medicine, remains largely underexplored. Medical tasks often involve complex reasoning [1012]. In real-world medical diagnoses or decisions, doctors often deliberate carefully. Such life-critical field necessitates meticulous thinking to ensure more reliable answers [13, 14]. Additionally, the medical domain offers unique advantages: compared to general domains, the medical domain is generally narrower in scope and easier to verify. Furthermore, medical reasoning closely resembles real-world applications in fields like finance, law, education, and security, making advancements in this area readily transferable [15, 16]. Despite these advantages, key challenge in medical reasoning is verifying the thought process, which often lacks clear steps. Inspired by mathematical problems that allow verification through their outcomes, we construct 40K verifiable medical problems reformatted from challenging, closed-set medical exam questions. These verifiable problems are characterized as open-ended with unique, Benyou is the corresponding author with email: wangbenyou@cuhk.edu.cn Preprint. Under review. objective ground-truth answers that allow an LLM verifier to check solution correctness. This enables two-stage approach for advancing medical complex reasoning: Stage 1: Learning Complex Reasoning We construct complex reasoning trajectories through strategy-based searches guided by verifier feedback (True or False). The LLM first initializes CoT. If the verifier rejects the current CoT, the model extends the CoT by applying strategy sampled from Backtracking, Exploring New Paths, Verification, and Correction until correct answer is provided. Successful reasoning trajectories are then used to fine-tune the LLM, enabling it develop complex reasoning skills that embody iterative reflection. Stage 2: Enhancing Complex Reasoning with RL After acquiring complex reasoning skills, reinforcement learning (RL) further refine this ability. Specifically, sparse rewards provided by the verifier guide self-improvement using the Proximal Policy Optimization (PPO) algorithm. Using this approach, we present HuatuoGPT-o1, medical LLM capable of producing long CoT to recognize its mistakes, try different strategies and refine the answer. Experiments demonstrate that our method (using only 40K data points) yields an 8.5-point improvement on medical benchmarks with an 8B model. Furthermore, our 70B model outperforms other open-source general and medical-specific LLMs across multiple medical benchmarks. The experiments further reveal that complex reasoning enhances medical problem-solving and boosts RL performance compared to standard or non-CoT methods. Our contributions are as follows: To the best of our knowledge, this is the first work to advance medical complex reasoning in LLMs using verifiable medical problems and medical verifier. With verifiable medical problems, we propose two-stage training approach, combining search strategies to construct reasoning pathways for fine-tuning, and further enhanced by RL with verifier feedback. Using the proposed method, we developed HuatuoGPT-o1, the first medical LLM capable of complex reasoning. HuatuoGPT-o1 exhibits superior performance compared to the open-source general and medical-specific baselines. Our experiments reveal that complex reasoning is effective for medical problem-solving and benefits RL enhancements."
        },
        {
            "title": "2 Verifiable Medical Problems",
            "content": "Figure 1: Left: Constructing verifiable medical problems using challenging close-set exam questions. Right: The verifier checks the models answer against the ground-truth answer. Inspired by mathematical problems that enable verification of the solution process through the final result, we aim to create verifiable medical problems that allow reasoning verification through outcomes. These verifiable problems are characterized as open-formal with unique, objective groundtruth answers, as illustrated in Figure 1. Sourcing from Medical Exam Questions To achieve this, we utilize closed-set real-world exam questions for two key reasons: 1) large number of medical exam questions are available; and 2) these exam questions are typically objective and accurate. Specifically, we collected 192K medical multiple-choice exam questions from the training sets of MedQA-USMLE [17] and MedMcQA [18]. 2 Transforming to Verifiable Medical Problems However, these medical questions are closed-set, meaning they provide limited options to choose from. This makes it easy for models to guess the correct answer without proper reasoning. Additionally, some questions are not suitable due to they may lack unique correct answer for verification or are too simple to require reasoning. To address this, we select and process the questions as follows: 1. Selecting Challenging Questions We removed questions that three small LLMs (Gemma29B [19], LLaMA-3.1-8B [20], Qwen2.5-7B [21]) all answered correctly and discarded short questions to retain those requiring deeper reasoning. 2. Ensure Unique Answers: We excluded questions asking for incorrect options or with multiple correct answers. LLM (GPT-4o) is further employed to remove questions where the correct answer might not be unique or could be ambiguous. 3. Reformatting to Open-Ended Formal: Using LLMs (GPT-4o), We reformatted each closed-set question into open-ended problem an open-ended problem and ground-truth answer y, as shown in Figure 1. The prompt used for filtering and processing can be found in Appendix B. After this filtering and processing, we ultimately constructed dataset of 40K verifiable medical questions denoted as = {(x, y)}, where is verifiable problem and the ground-truth answer. Developing Medical Verifier With these verifiable problems, we propose verifier to assess the correctness of model outputs. Given medical verifiable problem x, the model generates Chainof-Thought (CoT) and result y. The verifier checks against the ground-truth answer and provides binary feedback as: Verifier(y, y) {True, False} This feedback is essential for building correct reasoning trajectory and improving reasoning performance. We use GPT-4o [22] as the verifier, prompting it to perform validation with the detailed prompt provided in Appendix C. Given the prevalence of aliases in the medical domain, exact match methods [8, 23] commonly applied in mathematics are impractical. Experiments in Section 4.2 confirm this and demonstrate the reliability of the LLM-based verifier."
        },
        {
            "title": "3 Methodology",
            "content": "Figure 2: Demonstration of developing and improving LLMs for medical complex reasoning. Left (Stage1): Searching for correct reasoning trajectories to fine-tune LLMs for complex reasoning. Right (Stage2): Using the verifier to enhance complex reasoning via reinforcement learning. In this section, we present the method for training LLMs to performing medical complex reasoning to identify errors, and refine answers using deep thinking. As shown in Figure 1, the method has two stages: Stage One: master complex reasoning, and Stage Two, enhance complex reasoning with reinforcement learning (RL). 3 Figure 3: Example of data synthesis. Left: strategy search on medical verifiable problems until the answer is validated. Right: Merging the entire search process into efficient complex CoTs, facilitating effective deep reasoning to refine answers. The complex CoTs and responses are used to train the model to adopt thinks-before-it-answers behavior akin to o1. 3.1 Stage One: Learning Complex Reasoning Searching for Correct Trajectories Given verifiable medical problem as tuple (x, y), i.e. (question, ground-true answer), the LLM (e.g., GPT-4o) generates an initial CoT e0 and answer y0: e0, y0 = LLMinit(x) The verifier checks if y0 matches y. If incorrect, the model iteratively refines the answer by applying randomly selected search strategy on prior thoughts [e0, y0, . . . , ei1, yi1], producing new reasoning ei and new answer yi: ei, yi = LLMki(x, [e0, y0, . . . , ei1, yi1]) where denotes the i-th iteration. We define four search strategies to guide the refinement process: Exploring New Paths The LLM explores new approach ei , distinct from prior e0, . . . , ei1, to derive new answer yi. Backtracking The LLM revisits previous reasoning process ej, yj, where < 1, and continues reasoning from there. Note that Backtracking is sampled only if 2. Verification The LLM evaluates the current reasoning ei1and result yi1, providing validation process ei and the verified result yi. Corrections The LLM critiques and corrects the current reasoning ei1, yielding revised reasoning ej and answer yi. The process iterates until yi is verified as correct. If the maximum iteration count = 3 are reached, the search restarts. Each data point (x, y) is given up to = 3 attempts; if all fail, the data point is discarded. The prompts for search reasoning trajectories can be found in Appendix D. Constructing SFT Training Data When successful trajectory [e0, y0, . . . , ei, yi] is found, it is reformatted into coherent, natural language reasoning process ˆe (Complex CoT): 4 Algorithm 1: Training LLMs for Medical Complex Reasoning Require: Medical Verifiable Problems = {(x, y)}, Verifier, an LLM (GPT-4o) for synthesizing reasoning trajectories, search strategies K, max search depth , max search attempts , and initial policy πθ. DSearch, DRL Split(D) DSFT // Stage One: Learning Complex Reasoning for (x, y) DSearch do for 1 to do e0, y0 LLMinit(x) for 1 to do ki ei, yi LLMki (x, [e0, y0, ..., ei1, yi1]) if Verifier(yi, y) then ˆe LLMReformat([e0, y0, ..., ei, yi]) ˆy LLMResponse(ˆe) DSFT DSFT {(x, ˆe, ˆy)} break if Verifier(yi, y) then break // SFT for (x, ˆe, ˆy) DSF do LSFT(θ) log πθ(ˆe, ˆy x) θ UpdateParameters(LSF (θ), θ) // Stage Two: Enhance Reasoning with RL πref πθ for (x, y) DRL do ˆe, ˆy πθ(x) // Reward Rule (Verifier (ˆy, y)) βKL (πθ ( x) πref ( x)) θ UpdateParameters (LRL (x, ˆe, ˆy, r, πref , πθ) , θ) return πθ ˆe = LLMReformat([e0, e1, . . . , ei, yi]) As shown in Figure 3, this reformatting avoids rigid structures, using smooth transitions (e.g., hmm, also, wait) to streamline reasoning and reduce token usage.The model then generates formal response ˆy for question using the conclusion of ˆe: The prompt used for constructing SFT data can be found in Appendix E. ˆy = LLMResponse(x, ˆe) Supervised Fine-Tuning (SFT) We synthesize 20K SFT data points DSFT = {(x, ˆe, ˆy)} from the verifiable problem set = {(x, y)} using GPT-4o. DSFT is used to fine-tune LLMs to generate complex CoT ˆe followed by formal response ˆy. This fine-tuning process teaches the model to think before answering, encouraging Stream-of-Search (SoS) [23] way where the model deeply explores and refines its reasoning before answering. 3. Stage Two: Enhance Complex Reasoning with RL In this stage, we further enhance the complex reasoning skills using reinforcement learning (RL). While the LLM learned successful reasoning trajectories in stage 1, these paths, derived via search, may not be optimal. On-policy learning in stage 2 aims to refine the model for better complex CoT reasoning. 5 Rewards of RL Rewards play crucial role in guiding the RL training target. Given verifiable problem and the generated response (ˆe, ˆy), the reward is assigned as: r(x, ˆy, y) = 1 0.1 0 if verifier(ˆy, y) = True if verifier(ˆy, y) = False if ˆy = null Following [24, 25, 8], correct answers receive reward of 1, incorrect answers receive 0.1, and responses that lack think-before-answering behavior receive 0. Additionally, following related works, the total reward combines this function score with the Kullback-Leibler (KL) divergence between the learned RL policy πθ and the initial policy πref, scaled by coefficient β: to stabilize training with sparse rewards [8]. r(x, ˆy, y) = r(x, ˆy, y) + βKL(θ) Reinforcement Learning For RL, We use the Proximal Policy Optimization (PPO) [26] algorithm with clipped objective. The fine-tuned model serves as the policy model πθ. Training is conducted on the remaining verifiable medical problems DRL = {(x, y)}. The policy samples responses (ˆe, ˆy) for input x, computes the reward, and updates parameters θ. The full training process for both stages is summarized in Algorithm 1."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Training Data Finally, We constructed 40K medical verification dataset = {(x, y)} from the training sets of MedQA-USMLE [17] and MedMCQA [27]. Of this, 20K is used for SFT in stage 1 and 20K for RL in stage 2. Additionally, 4K unconverted data (close-set questions with option answers) from are included to enhance generalization. In line with prior work that integrates general-domain data to support medical adaptation [15, 28], we add 5K general verification questions sourced from MMLU-Pro [29] outside the medical-related tracks. All data were strictly screened to avoid contamination with the evaluation data using the filtering method of Med-PaLM2 [30] (filtering overlaps of 64 consecutive characters). Model Training Using the proposed method, we train our models HuatuoGPT-o1-8B and HuatuoGPT-o1-70B based on LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct [20], respectively. In Stage 1, the models are fine-tuned on the DSFT for 3 epochs with learning rate of 5e-6 and batch size of 128. In Stage 2, we employ PPO for RL with learning rate of 5e-7, batch size of 128, and β set to 0.03. The PPO parameters are set as: 3 PPO epochs, discount factor 1.0, value coefficient 1.0, and clip range 0.2. Baselines We compare our models with two type of LLMs: 1) General LLMs: Qwen-2.5 [31], LLaMA-3.1 [20], Gemma 2 [19], Yi [32], Mistral [33]; and 2) Medical-Specific LLMs: UltraMedical [28], OpenBioLLM [34], and BioMistral [35]. Benchmarks We evaluate on standard medical benchmarks: MedQA (USMLE test set) [17], MedMCQA (validation set) [18], and PubMedQA (test set) [36]. Aditionally, we evaluated the medical sections of some challenging LLM benchmarks, including the health and biology tracks of MMLU-Pro [29], and the genetics and molecular biology tracks of GPQA [37]. Due to the limited number of GPQA questions, we ran this evaluation 5 times and averaged the results. 4.2 Experimental Results Main Results We evaluated various open-source LLMs on medical tasks, as shown in Table 1. The results indicate that prior medical-specific LLMs, like UltraMedical, excel on traditional medical benchmarks (MedQA, MedMCQA, PubMedQA) but struggle on the newer, more challenging datasets, 6 MedQA MedMCQA PubMedQA Health Biology Genetics Molecular Biology Avg. MMLU-Pro GPQA BioMistral-7B OpenBioLLM-8B UltraMedical-8B Mistral-7B-Instruct Yi-1.5-9B-Chat LLaMA-3.1-8B-Instruct GLM-4-9B-Chat Qwen2.5-7B-Instruct Gemma2-9B HuatuoGPT-o1-8B w/o Stage2 (RL) UltraMedical-70B OpenBioLLM-70B DeepSeek-67B-Chat Yi-1.5-34B-Chat Gemma2-27B Qwen2.5-72B-Instruct QwQ-32B-Preview Llama-3.1-70B-Instruct HuatuoGPT-o1-70B w/o Stage2 (RL) 45.0 57.7 71.1 48.2 50.8 58.7 58.9 57.0 61.8 72.6 69.0 82.2 76.1 57.1 59.5 65.4 72.7 72.3 78.4 83.3 80.3 8B Large Language Models 27.4 66.9 38.4 74.1 55.1 77.4 33.7 59.5 43.4 69.8 52.7 75.2 45.5 73.5 50.6 72.7 55.1 63.3 58.7 79.2 53.5 77. 40.2 54.1 58.3 44.6 48.7 56.0 49.8 55.6 55.9 60.4 57.9 > 10B Large Language Models 64.8 78.4 68.8 79.2 46.9 76.1 52.8 74.3 61.1 72.6 65.3 71.7 62.0 73.7 68.2 78.5 71.0 80.6 70.2 78.6 71.8 74.7 51.7 56.7 60.2 66.2 65.6 72.5 73.6 70.1 49.2 52.4 66.7 53.6 65.6 64.6 65.4 70.2 74.9 68.2 66.1 71.1 76.7 66.2 71.0 76.2 78.8 78.1 80.8 82.8 79.8 28.6 43.7 41.2 30.0 42.5 33.8 53.8 36.2 35.0 48.8 41. 33.8 38.8 40.0 32.5 32.5 41.2 37.5 52.5 56.2 54.2 38.5 39.6 48.4 46.1 48.1 46.8 41.6 49.7 57.4 59.7 53.5 62.9 54.8 51.0 56.8 61.6 56.8 64.5 61.6 66.5 63.9 42.3 51.4 59.7 45.1 52.7 55.4 55.5 56.0 57.6 63.9 59.8 66.4 67.0 55.6 57.7 61.4 64.7 64.8 70.3 73.4 71.0 Table 1: Main Results on Medical Benchmarks. LLMs with domain, and Within each segment, bold highlights the best scores, and underlines indicate the second-best. are specifically trained for the medical indicates LLMs training for long chain-of-thought reasoning. \"w/o\" means \"without\". even when the questions are medically related. This may suggest that MMLU-Pro and GPQA require not only medical knowledge but also stronger reasoning capabilities. Our model, HuatuoGPT-o1, performs exceptionally across all datasets. The 8B version outperforms the base model (LLaMA-3.1-8B-Instruct) by 8 points in overall evaluation. Furthermore, our 70B model surpasses other comparable open-source LLMS, including QwQ-32B, which are also developed specifically for advanced reasoning capabilities. These results demonstrate the effectiveness of our approach. Additionally, compared to only fine-tuning (w/o RL), the two-stage training strategy significantly improves performance, benefiting from the verifiable medical problems. Ablation Study We conducted an ablation study on the 8B model to analyze the impact of ComplexCoT and RL The results, shown in Table 2, reveal the following insights: 1. Simple Multiple-Choice Training Is Ineffective: We compared the performance of models trained solely on the original medical multiple-choice questions of dataset D. Specifically, we used multiple-choice questions as inputs and the correct option as output for fine-tuning. The results indicate that raining solely on multiple-choice questions (the fine-tuned baseline) yields minimal improvement over the base model (LLaMA-3.1-8B-Instruct). This suggests that learning correct answers alone does not improve problem-solving ability. 2. Effectiveness of Complex CoTs: We further examined the impact of different types of Chain-ofThought (CoT) reasoning. The results show that direct learning of response (ˆy) performs the worst, while simple CoT (y0, e0) offers only little benefit. In contrast, Complex CoT (ˆy0, ˆe) significantly improves performance by an average of 4.3 points. This demonstrates the importance of teaching models to refine their answers with reflection. 3. Complex CoT Boosts RL: We compared the RL enhancements under different CoT strategies, as shown in Table 3. The results indicate that Complex CoT, which involves much longer CoT (an average of 712 tokens), yields significantly greater gain (3.6 points) compared to simple CoT (2.6 points) and no CoT (1.1 points), as detailed in Table 3. This may suggests that longer self-play reasoning paths provide richer thought processes and feedback, enabling the model to discover higher-reward solutions. 7 MedQA MedMCQA PubMedQA MMLU-Pro (Med ) GPQA (Med ) LLaMA-3.1-8B-Instruct Baseline LLMs 58.7 56.0 Fine-Tuned Baseline SFT w/ Original Exam Data of 60. 55.5 75.2 74.1 Effectiveness of Complex Chain-of-Thought (CoT) CoT (only ˆy) SFT w/o (cid:24)(cid:24) SFT w/ Simple CoT (x0, y0) SFT w/ Complex CoT (ˆx, ˆy) 65.2 66.6 69.0 58.1 59.2 57.9 CoT + RL w/ PPO SFT w/o (cid:24)(cid:24) SFT w/ Simple CoT + RL w/ PPO SFT w/ Complex CoT + RL w/ PPO Effectiveness of RL 66.4 68.7 72. 58.6 58.4 60.4 Comparison of Different RL Algorithms SFT w/ Complex CoT + RL w/ DPO SFT w/ Complex CoT + RL w/ RLOO SFT w/ Complex CoT + RL w/ PPO 72.2 71.1 72.6 58.4 60.1 60.4 75.4 75.4 77. 76.3 77.5 79.2 77.3 78.1 79.2 58.2 54.3 58.5 57.0 59.4 60.1 60.2 63. 60.4 60.9 63.1 44.1 46.9 48.7 46.7 51.0 49.8 53.1 57.5 52.5 58.2 57. Table 2: The results of ablation experiments on HuatuoHPT-o1-8B. (Med ) indicates that only the medical-related parts are evaluated. \"w/o\" and \"w/\" denote \"without\" and \"with\". \"Original Exam Data\" refers to original multiple-choice questions used for medical verifiable problems D. Bold highlights the best scores in each segment. 4. PPO Yields the Best Performance: Using the same reward function, we further compared different RL-related algorithms, including the preference learning algorithm DPO [38] and the REINFORCE-style algorithm RLOO [39]. Detailed implementation information is provided in Appendix F. Comparing PPO, RLOO, and DPO, we find PPO performs best, followed by RLOO and DPO. The weaker performance of DPO likely results from its off-policy nature, while PPO benefits from its use of value models, despite higher memory consumption. # Avg. Generated Tokens Avg. Gain from RL Direct Response (only ˆy) Simple CoT (x0, y0) Complex CoT (ˆx, ˆy) 82 281 1.1 2.6 3.6 Table 3: Comparison of models trained with different reasoning strategies. \"# Avg. Tokens\" indicates the average number of tokens generated per question. represents the performance improvement from RL, as detailed in Table 1. Reliability of the Verifier The verifier plays crucial role in guiding path search and reinforcement learning (RL). In our approach, GPT-4o serves as the verifier to assess model outcomes against ground-truth answers. To assess its reliability, we manually verified 200 scoring instances sampled from Stage 1 and Stage 2. As shown in Figure 4, GPT-4o achieved 96.5% accuracy in Stage 1 and 94.5% in Stage 2, demonstrating its reliability. In contrast, the Exact Match method [8], which uses regular expressions to determine whether the correct answer is present in the response, performed significantly worse, with accuracies of only 70.5% in Stage 1 and 74.5% in Stage 2. This highlights the critical role of LLM-based verifiers. Additionally, we fine-tuned an 8B verifier based on LLaMA3.1-8B with 20,000 scoring samples. The fine-tuned verifier also demonstrated feasibility, achieving over 90% accuracy. Domain Compatibility To verify domain compatibility, we extra applied our method to the Chinese medical domain. We constructed dataset of 40,000 verifiable Chinese questions from the CMBexam training set. We then trained HuatuoGPT-o1-7B-zh using our two-stage approach based on Qwen2.5-7B-Instruct. As shown in Table 4, HuatuoGPT-o1-7B-zh outperformed other Chinese LLMs 8 Figure 4: Accuracy of verifiers. Accuracy is based on 200 manually annotated samples. MedQA (Chinese) CMB (Exam) CMExam CMMLU (Med ) HuatuoGPT2-7B Yi-1.5-9B-Chat Qwen2.5-7B GLM-4-9B-chat HuatuoGPT-o1-7B-zh w/o Stage2 (RL) 73.7 75.8 71.4 75.2 79.8 76.5 63.6 66.2 70.7 70.0 73.0 70.8 67.4 68.1 70.4 70.5 74.1 72.3 58.4 64.2 70.5 67.6 74.5 70. Table 4: Results on Chinese medical benchmarks. (Med ) indicates that only the medical portion is evaluated. MedQA (Chinese) refers to the Chinese test set of MedQA (MedQA-MCMLE). of similar size, demonstrating the methods adaptability to new domains. For more experimental details, refer to Appendix G."
        },
        {
            "title": "5 Related Work",
            "content": "Research on o1 Recent studies have extensively analyzed the roadmap and core techniques of OpenAIs o1 [4, 6, 5], offering foundational insights into its architecture and methodology. Extensions such as LLaMA-Berry [9], LLaVA-o1 [40], o1-Coder [41], and Marco-o1 [42] have explored o1-like reasoning in various domains, including mathematics, vision-language integration, and open-ended problem-solving. However, these efforts have yet to address applications in medical or other highly specialized fields. In contrast, research focused on medicine [2, 43, 14] highlights o1s potential for deliberate, chain-of-thought reasoning in healthcare contexts. Meanwhile, several o1-inspired models, such as DeepSeek-R1-Lite-Preview [44], QwQ [7], and Gemini-2.0 Flash Thinking [45], have emerged. Despite their promise, most of these models remain closed-source, leaving substantial opportunities for further exploration and application of o1s capabilities across diverse fields. Medical LLMs The success of generalist LLMs has spurred interest in developing medical-specific LLMs to excel in the medical domain. Notably, the MedPaLM series [46, 30] achieved over 60% accuracy on the MedQA benchmark, reportedly surpassing human experts. Previous medical LLMs typically follow two main approaches [28]: (1) Prompting Generalist LLMs [47, 10, 48, 22, 12]: This method employs task-specific prompts to adapt generalist models for medical applications. While efficient and training-free, it is inherently limited by the capabilities of the original LLMs. (2) Further Training with Medical Data [4952, 34, 35, 5358]: This involves training LLMs on medical pretraining corpora or medical instructions to embed medical knowledge and expertise. However, this always requires significant computational resources, such as the 1.4 billion and 3 billion training tokens used for Meditron [59] and HuatuoGPT-II [15]. In contrast, our approach emphasizes enabling LLMs to excel in medical reasoning, offering distinct solution. Enhancing Reasoning in LLMs Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of LLMs [60, 61], but scaling expert-labeled reasoning paths remains costly, especially for complex problems [62, 63]. To mitigate this, model-generated reasoning paths filtered through external supervision offer partial solution [64, 65], yet scalability challenges persist [66, 67]. 9 Reinforcement learning-based methods leveraging reward models or oracle functions show potential but often suffer from slow processing, high costs, and supervision bottlenecks [68, 69]. Complex Reasoning Developing models with reflective abilities like critique and self-correction has shown success in reasoning, planning, and coding tasks [23, 7074], though underexplored in specialized domains like medicine. While prompting techniques can generate self-critical reasoning [75, 70], they struggle without reliable reward functions or verifiers, particularly in complex domains [76, 77]. Fine-tuning and reinforcement learning methods offer solutions but require extensive human annotations or intricate reward designs [7881]. Additionally, self-training methods present promising direction for developing self-correction capabilities [72, 82, 83]."
        },
        {
            "title": "6 Conclusion",
            "content": "This study advances the medical reasoning capabilities of LLMs. Firstly, we construct the medical verifiable problems and medical verifier. This enabled two-stage training process: (1) learning complex reasoning and (2) enhancing it through RL. We developed HuatuoGPT-o1, medical LLM with thinks-before-it-answers behavior, achieving outstanding performance in medical benchmarks. Experiments show that complex reasoning improves medical problem-solving and benefits obviously from RL. Additional validation in Chinese medical contexts shows the methods adaptability to other fields. We believe our approach can enhance domain-specific reasoning beyond mathematics."
        },
        {
            "title": "Acknowledgment",
            "content": "by supported the Shenzhen Science This work was and Technology Program (JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS20221008093330065), Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608), Shenzhen Key Laboratory of Cross-Modal Cognitive Computing (grant number ZDSYS20230626091302006), and Shenzhen Stability Science Program 2023. GPU devices are all supported by the university."
        },
        {
            "title": "References",
            "content": "[1] Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Heylar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, and Amelia Glaese. Deliberative alignment: Reasoning enables safer language models. OpenAI Blog, 2024. 1 [2] Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, and Yuyin Zhou. preliminary study of o1 in medicine: Are we closer to an ai doctor? arXiv preprint arXiv:2409.15277, 2024. 9 [3] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al. Evaluation of openai o1: Opportunities and challenges of agi. arXiv preprint arXiv:2409.18486, 2024. 1 [4] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. 1, 9 [5] Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, and Xipeng Qiu. Scaling of search and learning: roadmap to reproduce o1 from reinforcement learning perspective. arXiv preprint arXiv:2412.14135, 2024. 9 [6] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024. 1, [7] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. 1, 9 [8] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. 3, 6, 8 [9] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024. 1, 9 [10] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416, 2024. 1, 9 [11] Vimla Patel, José Arocha, and Jiajie Zhang. Thinking and reasoning in medicine. The Cambridge handbook of thinking and reasoning, 14:727750, 2005. [12] Junying Chen, Chi Gui, Anningzhe Gao, Ke Ji, Xidong Wang, Xiang Wan, and Benyou Wang. Cod, towards an interpretable medical agent using chain of diagnosis. arXiv preprint arXiv:2407.13301, 2024. 1, 9 [13] Shaochen Xu, Yifan Zhou, Zhengliang Liu, Zihao Wu, Tianyang Zhong, Huaqin Zhao, Yiwei Li, Hanqi Jiang, Yi Pan, Junhao Chen, et al. Towards next-generation medical agent: How o1 is reshaping decision-making in medical scenarios. arXiv preprint arXiv:2411.14461, 2024. 1 [14] Mohamad-Hani Temsah, Amr Jamal, Khalid Alhasan, Abdulkarim Temsah, and Khalid Malki. Openai o1-preview vs. chatgpt in healthcare: new frontier in medical ai reasoning. Cureus, 16(10):e70640, 2024. 1, 9 11 [15] Junying Chen, Xidong Wang, Ke Ji, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, et al. Huatuogpt-ii, one-stage training for medical adaption of llms. arXiv preprint arXiv:2311.09774, 2023. 1, 6, 9, [16] Daixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations, 2023. 1 [17] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. 2, 6, 23 [18] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning, pages 248260. PMLR, 2022. 2, 6 [19] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 3, 6 [20] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3, 6 [21] Qwen Team. Qwen2.5: party of foundation models, September 2024. [22] OpenAI. Gpt-4 technical report, 2023. 3, 9 [23] Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024. 3, 5, 10 [24] Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Wiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving sparse reward tasks from scratch. In International conference on machine learning, pages 43444353. PMLR, 2018. 6 [25] Alexander Trott, Stephan Zheng, Caiming Xiong, and Richard Socher. Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards. Advances in Neural Information Processing Systems, 32, 2019. 6 [26] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [27] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning, pages 248260. PMLR, 2022. 6 [28] Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in biomedicine. arXiv preprint arXiv:2406.03949, 2024. 6, 9 [29] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. 6 [30] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023. 6, 9 [31] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 6 [32] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 6 [33] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 6 [34] Malaikannan Sankarasubbu Ankit Pal and Malaikannan Sankarasubbu. Openbiollms: Advancing open-source large language models for healthcare and life sciences, 2024. 6, 9 [35] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. Biomistral: collection of open-source pretrained large language models for medical domains. arXiv preprint arXiv:2402.10373, 2024. 6, 9 [36] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. [37] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. 6 [38] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 8 [39] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. 8 [40] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. 9 [41] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. [42] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024. 9 [43] Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, and Eric Horvitz. From medprompt to o1: Exploration of run-time strategies for medical challenge problems and beyond. arXiv preprint arXiv:2411.03590, 2024. 9 [44] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. 9 [45] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 9 [46] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. 9 [47] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452, 2023. [48] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent hospital: simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957, 2024. 9 13 [49] Ming Xu. Medicalgpt: Training medical gpt model. https://github.com/shibing624/ MedicalGPT, 2023. 9 [50] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge, 2023. [51] Tianyu Han, Lisa Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno Bressem. Medalpacaan open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023. [52] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmcllama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, page ocae045, 2024. 9 [53] Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, and Zhongyu Wei. Disc-medllm: Bridging general large language models and real-world medical consultation, 2023. 9 [54] Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling Yan, Yixin Liu, Zhengliang Liu, Lifang He, Brian Davison, Xiang Li, et al. Biomedgpt: unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks. arXiv e-prints, pages arXiv2305, 2023. [55] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. [56] Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, and Benyou Wang. Apollo: Lightweight multilingual medical llms towards democratizing medical ai to 6b people. arXiv preprint arXiv:2403.03640, 2024. [57] Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, and Benyou Wang. Efficiently democratizing medical llms for 50 languages via mixture of language family experts. arXiv preprint arXiv:2410.10626, 2024. [58] Clément Christophe, Praveen Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie Rajan, Ahmed Al-Mahrooqi, Avani Gupta, Muhammad Umar Salman, Gurpreet Gosal, et al. Med42evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient approaches. arXiv preprint arXiv:2404.14779, 2024. 9 [59] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. 9 [60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 9 [61] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 9 [62] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1104811064. Association for Computational Linguistics, 2022. 14 [63] Yisheng Song, Ting Wang, Puyu Cai, Subrota K. Mondal, and Jyoti Prakash Sahoo. comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities. ACM Comput. Surv., 55(13s):271:1271:40, 2023. 9 [64] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 9 [65] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 10511068. Association for Computational Linguistics, 2023. 9 [66] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross J. Anderson. The curse of recursion: Training on generated data makes models forget. CoRR, abs/2305.17493, 2023. 9 [67] Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G. Baraniuk. Self-consuming generative models go MAD. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [68] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 10 [69] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. CoRR, abs/2308.09583, 2023. 10 [70] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 10 [71] Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training with natural language feedback. CoRR, abs/2303.16749, 2023. [72] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 10 [73] Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Jia Liu, Tao Gui, Qi Zhang, and Xuanjing Huang. Self-polish: Enhance reasoning in large language models via problem refinement. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 11383 11406. Association for Computational Linguistics, 2023. [74] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. REFINER: reasoning feedback on intermediate representations. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St. Julians, Malta, March 17-22, 2024, pages 11001126. Association for Computational Linguistics, 2024. 10 15 [75] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. 10 [76] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 10 [77] Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Wang. Pride and prejudice: LLM amplifies self-bias in self-refinement. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1547415492. Association for Computational Linguistics, 2024. 10 [78] Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane DwivediYu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: critic for language model generation. CoRR, abs/2308.04592, 2023. [79] Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Junyang Lin, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, and Baobao Chang. LLM critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. CoRR, abs/2406.14024, 2024. [80] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [81] Alexander Havrilla, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. Glore: When, where, and how to improve LLM reasoning via global and local refinements. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 10 [82] Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, and Le Sun. Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic, 2024. 10 [83] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal M. P. Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning. CoRR, abs/2409.12917, 2024. 10 [84] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et al. Cmb: comprehensive medical benchmark in chinese. arXiv preprint arXiv:2308.08833, 2023. [85] Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, et al. Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset. Advances in Neural Information Processing Systems, 36, 2024. 23 [86] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023."
        },
        {
            "title": "A Ethical Statement",
            "content": "Although the proposed model is medical LLM with complex reasoning capabilities, it may still produce content that includes hallucinations or inaccuracies. Therefore, the current model is not suitable for real-world applications. Consequently, we will impose strict limitations on the use of our model. The models are not permitted for use in clinical or other industry applications where such inaccuracies could lead to unintended consequences. We emphasize the ethical responsibility of users to adhere to these restrictions in order to safeguard the safety and integrity of their applications."
        },
        {
            "title": "B Constructing Medical Verifiable Problems",
            "content": "To construct Medical Verifiable Problems, we begin by employing small models and rule-based methods to identify challenging questions. Subsequently, we leverage GPT-4o to perform data filtering, isolating questions that have been suitably transformed. The prompt used for this data filtering process is illustrated in Figure 5. After selecting appropriate data, we reformat multiplechoice medical exam questions into open-ended verifiable problems using the prompt provided in Figure 6. The prompt for filtering Multiple-choice Questions <Multiple-choice Question> {Question} {Options} Correct Answer: {Answer} </Multiple-choice Question> You are an expert in filtering and evaluating multiple-choice questions for advanced reasoning tasks. Your job is to evaluate given question and determine whether it meets the following criteria: 1. **Depth of Reasoning:** The question should require deeper reasoning. If the question appears too simple, mark it as \"Too Simple.\" 2. **Unambiguous Correct Answer:** The question must have unique and unambiguous correct answer. If the question asks for \"incorrect options\" or allows for multiple correct answers, mark it as \"Ambiguous Answer.\" 3. **Open-Ended Reformulation Feasibility:** The question should be suitable for reformatting into an open-ended format. If the question cannot be easily reformulated into an open-ended problem and clear ground-truth answer, mark it as \"Not Reformulatable.\" For each question, provide one of the following evaluations: - \"Pass\" (The question meets all the criteria.) - \"Too Simple\" - \"Ambiguous Answer\" - \"Not Reformulatable\" Figure 5: The prompt for filtering Multiple-choice Questions. Here, {Question} and {Options} represents the multiple-choice question and options, and {Answer} represents the correct option for the multiple-choice question. The prompt for reformatting multiple-choice questions to open-ended verifiable problems will provide you with multiple-choice question, and your task is to rewrite it into an open-ended question, along with standard answer. The requirements are: 1. The question must be specific, targeting the point being tested in the original multiple-choice question. Ensure it is open-ended, meaning no options are provided, but there must be definitive standard answer. 2. Based on the correct answer from the original question, provide concise standard answer. The answer should allow for precise matching to determine whether the models response is correct. Here is the multiple-choice question for you to rewrite: <Multiple-choice Question> {Question} {Options} 17 Correct Answer: {Answer} </Multiple-choice Question> Please output the result in the following JSON format: json {{ \"Open-ended Verifiable Question\": \"...\", \"Standard Answer\": \"...\" }} Figure 6: The prompt for reformatting multiple-choice questions to open-ended verifiable problems. Here, {Question} and {Options} represents the multiple-choice question and options, and {Answer} represents the correct option for the multiple-choice question."
        },
        {
            "title": "C The Prompt of Verifier",
            "content": "GPT-4o serves as the verifier to assess the correctness of model-generated outputs. Using the prompt depicted in Figure 7, we present GPT-4o with both the models output and the ground-truth answer to evaluate the correctness of the response. The verifier returns Boolean value: True if the response is accurate and False otherwise. The Prompt for Verifier <Model Response> {Model Response} </Model Response> <Reference Answer> {Ground-true Answer} </Reference Answer> You are provided with model-generated response (<Model Response>) and reference answer (<Reference Answer>). Compare the model response with the reference answer and determine its correctness. Your task is to simply output \"True\" if the response is correct, and \"False\" otherwise. Figure 7: The prompt for the GPT-4o verifier. {Model Response} represents the output of the model to be verified. {Ground-true Answer} represents the ground-truth answer for medical verifiable problems."
        },
        {
            "title": "D Prompts for Searching Trajectories",
            "content": "This section outlines the prompts used for constructing complex Chain-of-Thought (CoT) reasoning pathways. Initially, question is presented to GPT-4o, which generates an initial CoT response using the prompt shown in Figure 8. If the verifier determines the response to be incorrect, GPT-4o employs one of several search strategies to iteratively refine the output until it is accurate. The prompts for these four search strategies Backtracking, Exploring New Paths, Correction, and Verification are detailed in Figures 10, 10, 11, and 12, respectively. The prompt for initial CoT <question> {Question} </question> Please respond to the above question <question> using the Chain of Thought (CoT) reasoning method. Your response should consist of multiple steps, each of which includes three types of actions: **\"Inner Thinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**: 18 - **Inner Thinking**: This is the step where thinking is done. Note that multiple Inner Thinking steps are required to describe thorough reasoning. Each step should first generate brief title. - **Final Conclusion**: At this stage, you summarize the correct reasoning from previous Inner Thinking steps and provide the final answer. No title is required here. - **Verification**: At this stage, you verify the conclusion from the \"Final Conclusion\" step. If the conclusion holds, end the process. If not, return to \"Inner Thinking\" for further reasoning. No title is required here. The output format must strictly follow the JSON structure below: json { \"CoT\": [ {\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"}, ..., {\"action\": \"Final Conclusion\", \"content\": \"...\"}, {\"action\": \"Verification\", \"content\": \"...\"} ] } Figure 8: The prompt for initial CoT. {Question} represents the input question, i.e., the question of the medical verifiable problems. The Prompt for Backtracking Breask Search Strategy <question> {Question} </question> <previous reasoning> {Previous_CoT} <previous reasoning> <response requirements> Your response must include the following steps, each composed of three types of actions: **\"Inner Thinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**: 1. **Inner Thinking**: Break down the reasoning process into multiple concise steps. Each step should start with brief title to clarify its purpose. 2. **Final Conclusion**: Summarize the correct reasoning from all previous Inner Thinking steps and provide the final answer. No title is needed for this section. 3. **Verification**: Verify the accuracy of the \"Final Conclusion\". If it holds, conclude the process. Otherwise, return to \"Inner Thinking\" for further refinement. </response requirements> <question> represents the question to be answered, and <previous reasoning> contains your prior reasoning. Your task is to continue from the current Verification step. have manually reviewed the reasoning and determined that the **Final Conclusion** is false. Your Verification results must align with mine. Proceed to refine the reasoning using **backtracking** to revisit earlier points of reasoning and construct new Final Conclusion. ### Output Format Strictly follow the JSON structure below. You do not need to repeat your previous reasoning. Begin directly from the next Verification stage. json { \"CoT\": [ {\"action\": \"Verification\", \"content\": \"...\"}, {\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"}, ..., {\"action\": \"Final Conclusion\", \"content\": \"...\"}, {\"action\": \"Verification\", \"content\": \"...\"} 19 ] } Figure 9: The prompt for Backtracking search strategy. Here, {Question} represents the problem of the medical verifiable problems, and {Previous_CoT} represents the previous chain of thought process, i.e., [e0, y0, . . . , ei1, yi1]."
        },
        {
            "title": "The Prompt for Exploring New Paths Breask Search Strategy",
            "content": "<question> {Question} </question> <previous reasoning> {Previous_CoT} <previous reasoning> <response requirements> Your response must include the following steps, each composed of three types of actions: **\"Inner Thinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**: 1. **Inner Thinking**: Break down the reasoning process into multiple concise steps. Each step should start with brief title to clarify its purpose. 2. **Final Conclusion**: Summarize the correct reasoning from all previous Inner Thinking steps and provide the final answer. No title is needed for this section. 3. **Verification**: Verify the accuracy of the \"Final Conclusion\". If it holds, conclude the process. Otherwise, return to \"Inner Thinking\" for further refinement. </response requirements> <question> represents the question to be answered, and <previous reasoning> contains your prior reasoning. Your task is to continue from the current Verification step. have manually reviewed the reasoning and determined that the **Final Conclusion** is false. Your Verification results must align with mine. Proceed to refine the reasoning by exploring new approaches to solving this problem and construct new Final Conclusion. ### Output Format Strictly follow the JSON structure below. You do not need to repeat your previous reasoning. Begin directly from the next Verification stage. json { \"CoT\": [ {\"action\": \"Verification\", \"content\": \"...\"}, {\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"}, ..., {\"action\": \"Final Conclusion\", \"content\": \"...\"}, {\"action\": \"Verification\", \"content\": \"...\"} ] } Figure 10: The prompt for Exploring New Paths search strategy. Here, {Question} represents the problem of the medical verifiable problems, and {Previous_CoT} represents the previous chain of thought process, i.e., [e0, y0, . . . , ei1, yi1]. The Prompt for Correction Breask Search Strategy <question> {Question} </question> <previous reasoning> {Previous_CoT} <previous reasoning> <response requirements> Your response must include the following steps, each composed of three types of actions: **\"Inner Thinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**: 1. **Inner Thinking**: Break down the reasoning process into multiple concise steps. Each step should start with brief title to clarify its purpose. 2. **Final Conclusion**: Summarize the correct reasoning from all previous Inner Thinking steps and provide the final answer. No title is needed for this section. 3. **Verification**: Verify the accuracy of the \"Final Conclusion\". If it holds, conclude the process. Otherwise, return to \"Inner Thinking\" for further refinement. </response requirements> <question> represents the question to be answered, and <previous reasoning> contains your prior reasoning. Your task is to continue from the current Verification step. have manually reviewed the reasoning and determined that the **Final Conclusion** is false. Your Verification results must align with mine. Proceed to refine the reasoning by making precise **corrections** to address prior flaws and construct new Final Conclusion. ### Output Format Strictly follow the JSON structure below. You do not need to repeat your previous reasoning. Begin directly from the next Verification stage. json { \"CoT\": [ {\"action\": \"Verification\", \"content\": \"...\"}, {\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"}, ..., {\"action\": \"Final Conclusion\", \"content\": \"...\"}, {\"action\": \"Verification\", \"content\": \"...\"} ] } Figure 11: The prompt for Correction search strategy. Here, {Question} represents the problem of the medical verifiable problems, and {Previous_CoT} represents the previous chain of thought process, i.e., [e0, y0, . . . , ei1, yi1]."
        },
        {
            "title": "The Prompt for Verification Breask Search Strategy",
            "content": "<question> {Question} </question> <previous reasoning> {Previous_CoT} <previous reasoning> <response requirements> Your response must include the following steps, each composed of three types of actions: **\"Inner Thinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**: 1. **Inner Thinking**: Break down the reasoning process into multiple concise steps. Each step should start with brief title to clarify its purpose. 2. **Final Conclusion**: Summarize the correct reasoning from all previous Inner Thinking steps and provide the final answer. No title is needed for this section. 21 3. **Verification**: Verify the accuracy of the \"Final Conclusion\". If it holds, conclude the process. Otherwise, return to \"Inner Thinking\" for further refinement. </response requirements> <question> represents the question to be answered, and <previous reasoning> contains your prior reasoning. Your task is to continue from the current Verification step. have manually reviewed the reasoning and determined that the **Final Conclusion** is false. Your Verification results must align with mine. Proceed to refine the reasoning by conducting thorough **validation** process to ensure validity and construct new Final Conclusion. ### Output Format Strictly follow the JSON structure below. You do not need to repeat your previous reasoning. Begin directly from the next Verification stage. json { \"CoT\": [ {\"action\": \"Verification\", \"content\": \"...\"}, {\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"}, ..., {\"action\": \"Final Conclusion\", \"content\": \"...\"}, {\"action\": \"Verification\", \"content\": \"...\"} ] } Figure 12: The prompt for Verification search strategy. Here, {Question} represents the problem of the medical verifiable problems, and {Previous_CoT} represents the previous chain of thought process, i.e., [e0, y0, . . . , ei1, yi1]."
        },
        {
            "title": "E Prompts for Constructing SFT Training Data",
            "content": "When successful trajectory [e0, y0, . . . , ei, yi] is found, it is reformatted into coherent, natural language reasoning process ˆe (Complex CoT) using the prompt shown in Figure 13. This reformatting avoids rigid structures, using smooth transitions (e.g., hmm, also, wait) to streamline reasoning and reduce token usage. The model then generates formal response ˆy for for question using the conclusion of ˆe with the prompt in Figure 13. The prompt for reformatting reasoning trajectory to complex CoT <Thought Process> {Thought_Process} </Thought Process> <Question> {Question} </Question> The <Thought Process> above reflects the models reasoning based on the <Question>. Your task is to rewrite the <Thought Process> to resemble more human-like, intuitive natural thinking process. The new version should: 1. Be presented as step-by-step reasoning, with each thought on new line separated by line break. 2. Avoid structured titles or formatting, focusing on natural transitions. Use casual and natural language for transitions or validations, such as \"hmm,\" \"oh,\" \"also,\" or \"wait.\" 3. Expand the content, making the reasoning richer, more detailed, and logically clear while still being conversational and intuitive. Return directly the revised natural thinking in JSON format as follows: json { 22 \"NaturalReasoning\": \"...\" } Figure 13: The prompt for reformatting reasoning trajectory to complex CoT ˆe. Here, {Thought_Process} represents the successful reasoning trajectory of [e0, y0, . . . , ei, yi], and {Question} represents the question x."
        },
        {
            "title": "The prompt for generating a formal response with complex CoT",
            "content": "<Internal Thinking> {Complex_CoT} </Internal Thinking> <Question> {Question} </Question> The <Internal Thinking> represents your internal thoughts about the <Question>. Based on this, generate rich and high-quality final response to the user. If there is clear answer, provide it first. Ensure your final response closely follows the <Question>. The response style should resemble GPT-4s style as much as possible. Output only your final response, without any additional content. Figure 14: The prompt for generating formal response ˆy with complex CoT ˆe. Here, {Complex_CoT} represents the complex CoT ˆe, and {Question} represents the question x."
        },
        {
            "title": "F Settings of other RL training",
            "content": "we further compared different RL-related algorithms with PPO. Specifically, we employed the preference-learning algorithm DPO and the REINFORCE-style algorithm RLOO. DPO For DPO, we had the model generate five answers for each question offline and used verifier to identify pairs of one correct and one incorrect answer. If no such pairs were found, the data was discarded. Verified correct answers were used as positive examples, while failed verifications served as negative examples for training DPO. The hyperparameters for DPO training were set as follows: learning rate of 1e-6, batch size of 128, and regularization parameter of 1. RLOO For RLOO, we used the same reward function as PPO. The parameters were also identical to those of PPO, with an additional parameter rloo_k set to 2."
        },
        {
            "title": "G Chinese Medical Model",
            "content": "Model Training For the Chinese medical domain, we replaced the exam questions from the CMB training set for Chinese medical verifiable problems. Based on the same training process as the English version of HuatuoGPT-o1, we developed HuatuoGPT-o1-7B-zh, built on the Qwen2.5-7B-Instruct model. Chinese Medical Evaluation To assess the Chinese medical capabilities, we evaluated the model on three Chinese medical benchmarks, including the Chinese test set from MedQA (MCMLE) [17], the test set from CMB-Exam [84], and the test set from CMExam [85]. Additionally, we evaluated the model on the medical section of the Chinese general evaluation benchmark CMMLU [86], covering tracks of clinical knowledge, agronomy, college medicine, genetics, nutrition, Traditional Chinese Medicine, and virology. Comparison Models We compared the performance of three general Chinese models: Qwen2.5, GLM-4, and Yi. Additionally, we included comparison with Chinese medical model, HuatuoGPT2-7B [15]."
        }
    ],
    "affiliations": [
        "Shenzhen Research Institute of Big Data",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}