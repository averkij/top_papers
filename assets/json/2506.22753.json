{
    "paper_title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography",
    "authors": [
        "Jianing Zhang",
        "Jiayi Zhu",
        "Feiyu Ji",
        "Xiaokang Yang",
        "Xiaoyun Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, a lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside \\textit{pseudo} data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/."
        },
        {
            "title": "Start",
            "content": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography"
        },
        {
            "title": "Xiaoyun Yuan",
            "content": "5 2 0 2 8 2 ] . [ 1 3 5 7 2 2 . 6 0 5 2 : r Figure 1. metasurface-based ultra-compact camera system empowered by large-model-driven image restoration algorithm. Compared to state-of-the-art image restoration methods, our approach achieves superior reconstruction quality and enhanced visual perception."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce DegradationModeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms stateof-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/. The demand for compact, high-performance imaging systems has grown rapidly with advancements in biomedical imaging, implantable vision systems, point-of-care diagnostics, and augmented/virtual reality (AR/VR) [29, 31, 43, 60]. However, conventional compound lens systems rely on multi-element optics to correct aberrations, making it challenging to achieve both miniaturization and high imaging quality. Metalenses, class of engineered optical interfaces composed of subwavelength-scale nanostructures, enable precise optical wavefront control by manipulating phase, amplitude, and polarization within an ultrathin form factor. Combined with computational restoration methods, their unique capabilities offer new opportunities for developing compact, high-performance imaging systems. [16, 23, 29, 31, 43, 60]. While metalenses present promising solution for ultracompact imaging systems, their practical implementation is constrained by optical limitations and computational complexities. Specifically, images captured by metalenses suffer from spatially varying degradations due to aberrations. Conventional model-based restoration techniques, such as Wiener filtering [55] and iterative deconvolution methods [8, 25, 58], typically assume uniform distortions and struggle to correct the highly localized aberrations caused by the metalens. Although dividing images into small patches can mitigate this issue, it leads to reduced robustness and stability in the algorithms. Deep learning-based approaches [1, 33, 51, 61] have demonstrated more significant potential for restoration, offering powerful capabilities. However, these approaches typically require large datasets of pixelaligned image pairs for training, which are often difficult to obtain for computational imaging systems. As result, recent research has shifted toward employing large-scale models pretrained on extensive datasets as priors for image super-resolution and restoration tasks, leading to significant improvements in robustness, generalization, and overall performance. Among the various techniques, generative diffusion models (DMs) have demonstrated exceptional performance [2, 18, 27, 40, 56]. These models, pretrained on vast collections of image and text data, possess powerful natural image priors that allow them to synthesize vivid and authentic details more effectively than conventional GANbased methods [15]. However, despite their impressive capabilities, DMs are not devoid of limitations. Their restoration process is inherently prone to generating hallucinated details that do not exist in the original image, due to the lack of fine-grained control over the restoration process during inference. In this manuscript, we propose degradation-modeled multipath diffusion model for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. We first introduce the spatially varying degradation-aware attention (SVDA) module, which quantifies degradation by analyzing both the optical point spread function (PSF) of the metalens and the quality of image patches. This allows it to effectively account for degradation from both the metalens and image sensors. The degradation characterization results are further employed by the attention network to guide the LoRA finetuning process, enabling the diffusion model to adaptively handle region-specific degradations. We then propose the multipath diffusion model comprising three distinct pathways: positive-prompt path for generating high-frequency details, neutral-prompt path for restoring structural fidelity, and negative-prompt path for mitigating metalensspecific degradation. The negative-prompt path also facilitates the generation of pseudo input-output paired samples, which expand the training dataset and enhance generalization. To enable smooth control over the reconstruction results, we introduce tunable decoder that adaptively merges the latent codes from the positive and neutral paths, allowing users to adjust the balance between reconstruction fidelity and perceptual quality. Finally, as demonstrated in Fig. 1, we design and build micro metalens-based camera (MetaCamera) to validate our algorithm. MetaCamera achieves size of approximately 1 mm3. The experimental results demonstrate that our algorithm successfully reconstructs images with both high fidelity and sharpness, outperforming state-of-the-art approaches. Collectively, the contributions of this manuscript are as follows: Multipath Diffusion Model. We propose multipath diffusion model incorporating positive, neutral, and negative-prompt paths to balance high-frequency details and structural fidelity while mitigating metalens-specific degradation. Additionally, we introduce an instantly tunable decoder that enables smooth trade-off between perceptual quality and reconstruction fidelity. Spatially Varying Degradation Aware Attention (SVDA) Module. We introduce the SVDA module, which quantifies degradation from both metalens aberrations and image sensors, guiding the LoRA process to fine-tune the diffusion network for adaptive handling of spatially varying degradations. Hardware verification. We design and build millimeter-scale metalens-based camera (MetaCamera) to verify our algorithm, demonstrating superior image reconstruction results with high fidelity and sharpness, outperforming state-of-the-art methods. 2. Related work 2.1. Flat Computational Cameras Researchers have explored various approaches to reduce the height and complexity of conventional compound camera optical systems. One such approach simplifies the typical multi-element optical stack into single refractive element [17, 28, 42, 44], effectively minimizing both geometric and chromatic aberrations. Another approach uses thin monolithic sensor array [49] with color-filtered single-lens element array to mitigate chromatic aberrations, converting the deconvolution problem into color light field reconstruction challenge. However, solving this challenge without introducing artifacts remains difficult, which limits the potential for thin camera designs. Lensless cameras [2, 22, 38, 54] replace the entire optical stack with an amplitude mask or diffuser, perturbing the incident wavefront. While this allows for thin cameras just few millimeters thick, it complicates the recovery of high-quality images. 2.2. Metasurface Optics Recent advancements in nanofabrication techniques have enabled the development of ultrathin metasurfaces composed of subwavelength scatterers [12, 14, 33, 36]. Each scatterer within the metasurface can be independently engineered to precisely control the amplitude, phase, and polarization of incident wavefronts, capability that has significantly advanced research in planar optical imaging [3, 5, 14]. For instance, Tseng et al. [47] proposed differentiable design framework to achieve full-color imaging with large aperture of 0.5 mm. Moreover, Chakravarthula et al. [3] introduced metalens array design that enhances image quality across the full broadband spectrum over 100 FoV without increasing the back focal length, offering significant potential for miniaturizing wafer-level multielement compound lens cameras. Recently, Lee et al. [26] developed computational framework to correct aberrations in metalens-captured images, offering promising solution to overcome the limitations of current meta-optical imaging systems. 2.3. Image Restoration Image restoration aims to reconstruct high-quality images from degraded inputs. Traditional techniques like Wiener deconvolution [55] tackle this by solving inverse problems but struggle with noise and complex degradations, limiting real-world use. Deep learning has revolutionized the field, with models like SRCNN [10] and SwinIR [32] achieving state-of-the-art results in tasks such as denoising [4, 13, 30, 48], deblurring [24, 34, 53], and super-resolution [6, 7, 35]. However, major drawback of existing methods, including SwinIR, is their dependence on simplified degradation models, which often fail to reflect real-world complexities. Therefore, recent studies have increasingly focused on generative models for image restoration, which can be broadly categorized into model-driven and priordriven methods. Model-driven methods, such as SR3 [40] and SRDiff [27], adapt diffusion models to tasks like superresolution by directly learning the mapping from degraded to high-quality images. In contrast, prior-driven methods leverage pre-trained models as priors to enhance the performance of image restoration tasks. For instance, DiffBIR [2] and SeeSR [56] can significantly reduce computational overhead while maintaining high-quality restoration results. More recently, OSEDiff [57] and S3Diff[62] introduced efficient one-step diffusion networks using trainable LoRA layers. Despite these promising advancements, significant limitation of these methods is their reliance on welldefined degradation models, which restricts their applicability to computational imaging tasks under realistic and uncontrolled conditions. 3. Method To tackle the challenges of metalens-based imaging, we propose degradation-modeled multipath diffusion framework that leverages pretrained large-scale generative diffusion models for tunable metalens photography. Our approach addresses three key challenges: complex metalens degradations, limited paired training data, and hallucinations in generative models. With the powerful natural image priors from the base generative diffusion model, our method reconstructs vivid and realistic images using small training dataset. To further enhance restoration, we propose Spatially Varying Degradation-Aware (SVDA) attention module, which quantifies optical aberrations and sensor-induced noise to guide the restoration process. Additionally, we introduce Degradation-modeled Multipath Diffusion (DMDiff) framework, incorporating positive, neutral, and negativeprompt paths to balance detail enhancement and structural fidelity while mitigating metalens-specific distortions. Finally, we design an instantly tunable decoder, enabling dynamic control over reconstruction quality to suppress hallucinations. We begin in Section 3.1 with description of our DMDiff with LoRA-based fine-tuning, which serves as the foundation of our approach. In Section 3.2, we introduce the SVDA module, explaining how it characterizes and leverages spatially varying degradations for adaptive restoration. Finally, in Section 3.3, we detail the multi-prompt training and inference of DMDiff, as well as the instantly tunable decoder. 3.1. Degradation-modeled multipath diffusion framework The network architecture DMDiff is presented in Fig. 2, consisting of VAE encoder, latent diffusion UNet, VAE decoder, and our spatially varying degradation aware attention module. Here, we employ SD-Turbo [41], distilled version of Stable Diffusion [39], as the base encoder, UNet and decoder. Given an input image captured by our MetaCamera, the model encodes into the latent space via the VAE encoder E, producing the latent coded image z. Next, the UNet iteratively denoises the latent coded image for times, leveraging the given text prompt and spatially varying degradation cues from our SVDA module. This step ensures that the denoised latent coded image aligns with natural image priors while mitigating metalens-specific degradation. the VAE decoder converts the refined latent coded image back into high-quality reconstructed image. In this manuscript, we set = 1 to realize efficient one-step reconstruction. To adapt the powerful pre-trained model for metasurface photography, we incorporate trainable LoRA [19] layers for fine-tuning the VAE encoder and latent diffusion UNet. LoRA is an efficient fine-tuning method that updates model parameters by adding low-rank adaptation matrices to specific layers of pretrained model. In Transformer-based architectures, LoRA decomposes the original weight matrix Rdk into two trainable low-rank matrices Rdr and Rrk, where min(d, k), yielding the modified weight representation: = + AB. (1) During training, only and are updated, while remains frozen. This approach drastically reduces the number of trainable parameters, making the fine-tuning process more computationally efficient. At inference, the transformed weight is integrated into the model without Figure 2. Degradation-modeled multipath diffusion framework (DMDiff). (a) The network architecture of DMDiff. The core of DMDiff is diffusion-based large model, where LoRA fine-tuning is applied to the encoder and UNet. (b) To address the spatially varying degradation, spatially varying degradation aware attention (SVDA) module is proposed to guide the LoRA fine-tuning process. adding computational overhead. 3.2. Spatially Varying Degradation Aware Attention Image degradation in metalens-based imaging systems primarily arises from two sources: optical aberrations and sensor-induced electronic noise, especially in miniaturized sensors. Conventional correction methods rely on physicsbased models, which require precise aberration characterization. While optical aberrations can be simulated, fabrication errors and unknown illumination often lead to discrepancies between simulated and real-world distortions. Calibrating the point spread function (PSF) is also tedious and inaccurate, as it depends heavily on illumination (see Supplementary Material for calibration results). To overcome these limitations, we propose the Spatially Varying Degradation Aware (SVDA) attention module, which enables the network to learn degradation characteristics and correct them adaptively. Instead of depending on an accurate PSF, SVDA quantifies degradation by analyzing optical degradation from simulated PSFs and incorporating quality assessments (QA) from metalens-captured images. Meanwhile, degradation introduced by the electronic sensor is also quantified using QA results. This degradation representation is then incorporated into the LoRA fine-tuning process via an attention mechanism, allowing the diffusion model to focus on region-specific degradations and enhance restoration accordingly. By leveraging SVDA, our method eliminates the need for precise aberration calibration while enabling more robust and data-driven image reconstruction. To quantify optical aberrations, we use the Full Width at Half Maximum (FWHM) as metric to assess the degradation. The FWHM is defined as the width of distribution, specifically the distance between the points where the function reaches half of its maximum value. The image is first divided into patches, assuming the point spread function (PSF) is constant within each patch. The PSF at the center of each patch is then simulated using Fourier optics based on the designed metalens parameters, and its FWHM is calculated by fitting 2D Gaussian distribution to the corresponding PSF image: min µx,µy,σx,σy IPSF G(µx, µy, σx, σy)2 2, (cid:113) FWHM = (2.35σx)2 + (2.35σy)2, (2) where IPSF denotes the PSF image, and G() represents 2D Gaussian distribution with mean (µx, µy) and standard deviations (σx, σy). Given the radial symmetry of our metalens, the PSF image can be rotated to align its principal axes with the xand y-directions before fitting. The FWHM of Gaussian distribution function is 2 2 ln 2σ 2.35σ. To quantify the remaining degradation, we introduce NoReference Image Quality Assessment (NR-IQA) using the transformer-based MUSIQ metric[21] to measure the degradation directly from the captured image patches. Similarly, the image is divided into patches, and MUSIQ generates an image quality score distribution to capture spatial variations. Combining FWHM with NR-IQA enables comprehensive assessment of spatially varying degradation. Finally, to incorporate degradation awareness into fine-tuning, we introduce an attention mechanism that utilizes FWHM and NR-IQA scores. The adaptation LoRA process is formulated as: = NA(Sf , Si) = + AQB (3) where the Attention Network (NA) takes the FWHM score (Sf ) and NR-IQA score (Si) as input and generates an attention matrix Q. This mechanism allows the fine-tuning process to be guided by spatially varying degradation characteristics. By dynamically adjusting adaptation weights, Figure 3. (a) Multi-prompt paths training algorithm for DMDiff, including three paths with positive, neutral, and negative prompts. (b) In the inference step of DMDiff, the latent coded images of the neutral and positive paths are obtained. (c) Instantly tunable decoding of DMDiff, images with varying diffusion intensities can be generated quickly from the latent coded images. the model better preserves high-quality details while correcting degraded regions, improving robustness against spatial quality variations in our MetaCamera. 3.3. Multipath Diffusion Training The large-scale generative diffusion model provides powerful natural image priors for detail enhancement but also carries high risk of undesirable hallucinated artifacts. To address this, we propose multi-prompt training and inference algorithm for DMDiff that balances detail enhancement with fidelity preservation while mitigating metalensspecific degradation. Specifically, our training algorithm consists of three paths with positive, neutral, and negative prompts, as illustrated in Fig. 3(a). The neutral and positive paths take the MetaCamera-captured image as input, with the positive path learning to reconstruct the high-quality groundtruth image and the neutral path learning to reconstruct filtered groundtruth image. Here, an edge-preserving low-pass filter is applied to the groundtruth image to remove highfrequency details while preserving strong structural conIn contrast, the negative path takes the high-quality tent. groundtruth image as input and learns to generate degraded version that mimics MetaCamera-captured image. The three paths are conditioned on different text prompts, which are provided in the supplementary material. The neutral-prompt path enables dynamic control between subjective perceptual quality and objective accuracy (detailed in Section 3.4). The negative-prompt path helps the network learn and avoid metalens-style degradation. Additionally, since capturing pixel-aligned training pairs with real MetaCamera is time-consuming, this reverse diffusion process can generate pseudo input-ground truth pairs, effectively expanding the dataset and improving generalization. The effectiveness of this path is validated in the ablation study. The training process follows unified framework, expressed as: DMDiffneg(Igt, Pneg) Iin, DMDiffpos(Iin, Ppos) Igt, DMDiffneu(Iin, Pneu) G(Igt), if = 1, if = 2, if = 3, (4) where Pneg, Ppos, Pneu denote the negative, positive, and Iin represents the input neutral prompts, respectively. image, which can be MetaCamera capture or generated pseudo input image, while Igt is the high-quality groundtruth image. The function G() applies the edgepreserving low-pass filter. The variable follows categorical distribution: Cat(p1, p2, p3), (M = k) = pk. (5) At each training step, one of the three paths, negative, positive, or neutral, is randomly selected based on the probabilities (p1, p2, p3). 3.4. Instantly Tunable Decoding As shown in Fig. 3(b), in the inference step, we forward the positive and neutral-prompt paths to obtain two latent coded image zneu and zpos. While in the decoding step, the two features are combined using tunable scale and decoded by the VAE decoder to output the restored high-quality image: = (α zpos + (1 α) zneu) (6) where represents the VAE decoder, zpos and zneu denote the positive and neutral latent coded images, respectively. The parameter α is weighting factor, which controls the balance between subjective perception and objective accuracy. In practical applications, the neural-coded images are stored as the final imaging results. When pixel-level reconstruction is needed, series of images with varying diffusion intensities can be generated very quickly. 4. MetaCamera Design and Implementation Figure 4. (a) Schematic of the metalens unit cell, consisting of Si3N4 nano-pillar on SiO2 substrate. The pillar diameter varies radially to modulate the optical phase. (b) Diameter distribution map of the metalens. (c) Top: Optical and SEM images of the fabricated metalens. Bottom: Optical image of the fully integrated MetaCamera, demonstrating its ultra-compact size. Our MetaCamera integrates metalens with miniaturized CMOS image sensor (OV6946, OmniVision Technologies) featuring resolution of 400 400 pixels. The entire imaging module has an overall footprint of approximately 1 1 1 mm3, enabling ultra-compact imaging. The metalens is composed of dense array of subwavelength-scale nano-pillars, where the local phase modulation is dictated by the varying diameters of the cylindrical structures. To optimize the metalens design, we employ combination of Finite-Difference Time-Domain (FDTD) simulations and Neural Nano-Optics approach [47], ensuring precise phase control across the aperture. More details are provided in the supplementary material. 5. Experiment 5.1. Experimental Setup and Dataset To obtain paired training data, we displayed images on screen and captured them using our MetaCamera. Initial alignment was achieved through mechanical adjustments, followed by precise pixel-level correction via homography transformation. We also measured the MetaCameras PSFs using an objective lens (details in the supplementary material). To augment the dataset, we incorporated three datasets (DIV2K [45], Flickr2K [46], and WED) with multi-scale cropping, resulting in 7,800 training images and 3,000 test images. Additionally, we generated 10,000 pseudo images via the negative diffusion process to enhance training. Beyond these datasets, we also captured real-world scene images for qualitative evaluation. 5.2. Implementation details During training, we employed L2 Loss and LPIPS Loss as reconstruction losses, which can be expressed as = L2 + λ LLPIPS, (7) where the hyper-parameter λ is set to 2.5. And the patch number in SVDA is set to 7. The model was trained on four NVIDIA A100 80G GPUs for two days with batch size of 16. For fair comparison, all baseline methods were trained using the same dataset. More details are delivered in the supplementary material. 5.3. Image quality Assessment We compare our method with several baselines, including PSF-based non-blind denoising (Wiener deconvolution, neural nano-optics[47]), kernel-based blind denoising (Two-step PSF correction[11]), transformer-based restoration (SwinIR [32]), and diffusion-based methods. Specifically, we evaluate SeeSR, which leverages ControlNet for structure-aware restoration, and OSEDiff, single-step denoising model based on LoRA fine-tuning. All methods are trained on the same dataset. For evaluation, we use both reference and non-reference metrics. PSNR and SSIM [52] measure fidelity, while LPIPS [63], and DISTS [9] assess perceptual quality. We also include NIQE [37], MANIQA [59], MUSIQ [20], and CLIP-IQA [50] for nonreference evaluation. Table 1 shows that our method surpasses all baselines across all metrics, demonstrating its effectiveness. The blind denoising method and simple Wiener deconvolution struggle to restore the image due to sensor noise and chromatic aberration. We also performed two qualitative comparisons, with one using dataset images, as shown in Fig. 5, and the other using real-world images, as shown in Fig. 6. From the qualitative and quantitative results, it can be observed that non-diffusion-based methods, such as SwinIR, produce blurry images lacking high-frequency details due to the absence of priors. However, this also preserves accurate color tones and low-frequency structures, leading to higher PSNR and SSIM but poorer subjective perception scores. In contrast, diffusion-based methods generate richer highfrequency details and more natural images by leveraging priors. Yet, their reliance on priors and lack of degradation modeling introduces errors in color tones, low-frequency structures, and sometimes incorrect details. Our method combines the strengths of both, achieving accurate color tones, rich details, and enhanced realism, while the instantly tunable decoding strategy enables users to adjust the diffusion style as needed dynamically. Notably, due to its robust modeling of optical characteristics, our method maintains high performance even at the edges of images, where other methods typically degrade significantly. Furthermore, the strong chromatic dispersion of Figure 5. Qualitative comparisons of different methods on our unseen test dataset, zoom in for details. Our method achieves high imaging quality with sharp details and maintains robust performance even in severely degraded edge regions. Figure 6. Qualitative comparisons of different methods on real-world images captured by our system. Reference views captured by smartphone from similar perspective are provided. Note that smartphone software processing may introduce slight color discrepancies. Real-world images present greater challenges, highlighting the strong robustness of our method. metasurfaces poses challenges for diffusion models incorporating text semantics, often leading to instability. In contrast, our approach relies solely on imaging quality descriptions in the prompt, avoiding dependence on scene content and ensuring consistent performance. The real-world dataset used in Fig. 6 presents more complex lighting conditions and noise characteristics. Despite these challenges, our method exhibits remarkable robustness, whereas other approaches struggle to maintain effectiveness in practical applications. 5.4. Ablation Study The results in Table 2 and Fig. 7 validate the effectiveness of our multipath diffusion and instantly tunable decoding strategies. Experiments with various injection ratios reveal that higher diffusion injection strengths enhance subjective perception metrics such as LPIPS, MANIQA, and MUSIQ, producing images that are more vivid and detailed, albeit with some excessive detail, whereas lower injection strengths favor the generation of conservative lowfrequency structures, thereby improving PSNR and SSIM. Methods PSNR SSIM LPIPS DISTS NIQE MANIQA MUSIQ CLIP-IQA Wiener deconvolution[55] Two-step PSF correction[11] Neural nano-optics[47] SwinIR[32] SeeSR-s50[56] OSEDiff-s1[57] Ours-s1-α0.75 Ours-s1-α1.05 16.06 16.68 29.25 29.46 23.95 19.69 30.31 29.75 0.5727 0.6251 0.8624 0.8786 0.8340 0. 0.8731 0.8598 0.6706 0.6266 0.2001 0.2462 0.2315 0.2643 0.1705 0.1485 0.4393 0.3651 0.1765 0.2111 0.1673 0.1868 0.1499 0.1356 20.9859 8.1287 6.8027 8.2019 6.6028 7. 7.2751 6.3073 0.0931 0.2018 0.1901 0.2208 0.2633 0.2256 0.2782 0.3138 17.41 20.73 37.26 36.86 44.87 34.52 44.48 51.85 0.2681 0.2851 0.2746 0.3046 0.3913 0. 0.3869 0.4460 Table 1. Performance comparison of different methods on the dataset. Red cells indicate the best performance, and light red cells indicate the second-best performance for each metric. s1 refers to single-step diffusion, while s50 denotes diffusion with 50 steps. Methods PSNR SSIM LPIPS MANIQA MUSIQ Ours-α0 30.06 Ours-α0.5 30.39 Ours-α0.7 30.31 Ours-α0.9 30.10 Ours-α1 29.89 Ours-α1.05 29.75 0.8667 0.2715 0.8743 0.2039 0.8731 0.1705 0.8685 0.1562 0.8633 0.1504 0.8598 0. 0.2276 0.2557 0.2782 0.2956 0.3078 0.3138 31.24 38.96 44.48 48.14 50.63 51.85 Table 2. Performance comparison of different diffusion injection factors. Methods PSNR SSIM LPIPS MANIQA MUSIQ 17.12 Base w/o FWHM 26.62 w/o Neg prompt 28.21 Ours-α1 29. 0.7685 0.3455 0.8414 0.1869 0.8571 0.1953 0.8633 0.1504 0.2332 0.2966 0.2587 0.3078 38.27 50.55 44.15 50.63 Table 3. Quantitative results of the ablation study for different modules in our method. To evaluate the effectiveness of various components in our method, we performed an ablation study by selectively removing key modules and observing the impact on performance. Table 3 and Fig. 9 presents the results. Without any of our proposed modules, simple LoRA fine-tuning base model fails to achieve effective image restoration. Moreover, removing any individual module leads to decline in both quantitative metrics and visual quality, further validating the effectiveness of our method. Furthermore, Fig. 8 qualitatively demonstrates the effectiveness of degradation learning, indicating that our method accurately simulates the imaging effects of MetaCamera. 6. Conclusion We proposed diffusion-based framework for high-quality metalens photography. Our approach introduced an SVDA module for optical and sensor degradation awareness. multipath diffusion strategy and an instantly tunable decoder further enhanced reconstruction quality and adaptability. Experiments showed that our method outperformed existing approaches, setting new standard for high-quality metalens photography and advancing computational imaging. Figure 7. dynamically adjust the diffusion intensity. Instantly tunable decoding demonstration. Users can Figure 8. Degradation learning qualitative results. Our method effectively simulates the imaging effects of MetaCamera. Figure 9. Qualitative results of the ablation study for different modules in our method."
        },
        {
            "title": "References",
            "content": "[1] Francesco Aieta, Mikhail Kats, Patrice Genevet, and Federico Capasso. Multiwavelength achromatic metasurfaces by dispersive phase compensation. Science, 347(6228):1342 1345, 2015. 2 [2] Nick Antipa, Grace Kuo, Reinhard Heckel, Ben Mildenhall, Emrah Bostan, Ren Ng, and Laura Waller. Diffusercam: lensless single-exposure 3d imaging. Optica, 5(1):19, 2017. 2, 3 [3] Praneeth Chakravarthula, Jipeng Sun, Xiao Li, Chenyang Lei, Gene Chou, Mario Bijelic, Johannes Froesch, Arka Majumdar, and Felix Heide. Thin on-sensor nanophotonic array cameras. ACM Transactions on Graphics (TOG), 42(6):1 18, 2023. 2 [4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1229912310, 2021. 3 [5] Ji Chen, Xin Ye, Shenglun Gao, Yuxin Chen, Yunwei Zhao, Chunyu Huang, Kai Qiu, Shining Zhu, and Tao Li. Planar wide-angle-imaging camera enabled by metalens array. Optica, 9(4):431437, 2022. 2 [6] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image superresolution transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2236722377, 2023. 3 [7] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual aggregation transformer for image super-resolution. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12312 12321, 2023. [8] Sunghyun Cho and Seungyong Lee. Fast motion deblurring. In ACM SIGGRAPH Asia 2009 papers, pages 18. 2009. 1 [9] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture IEEE transactions on pattern analysis and masimilarity. chine intelligence, 44(5):25672581, 2020. 6 [10] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning deep convolutional network for image In Computer VisionECCV 2014: 13th super-resolution. European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13, pages 184199. Springer, 2014. 3 [11] Thomas Eboli, Jean-Michel Morel, and Gabriele Facciolo. In EuFast two-step blind optical aberration correction. ropean Conference on Computer Vision, pages 693708. Springer, 2022. 6, 8 [12] Jacob Engelberg and Uriel Levy. The advantages of metalenses over diffractive lenses. Nature communications, 11(1): 1991, 2020. 2 [13] Chi-Mao Fan, Tsung-Jung Liu, and Kuan-Hsien Liu. Sunet: Swin transformer unet for image denoising. In 2022 IEEE International Symposium on Circuits and Systems (ISCAS), pages 23332337. IEEE, 2022. 3 [14] Johannes Froch, Praneeth Chakravarthula, Jipeng Sun, Ethan Tseng, Shane Colburn, Alan Zhan, Forrest Miller, Anna Wirth-Singh, Quentin AA Tanguy, Zheyi Han, et al. Beating bandwidth limits for large aperture broadband nanooptics. arXiv preprint arXiv:2402.06824, 2024. [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [16] Tian Gu, Hyun Jung Kim, Clara Rivero-Baleine, and Juejun Hu. Reconfigurable metasurfaces towards commercial success. Nature Photonics, 17(1):4858, 2023. 1 [17] Felix Heide, Mushfiqur Rouf, Matthias Hullin, Bjorn Labitzke, Wolfgang Heidrich, and Andreas Kolb. High-quality computational imaging through simple lenses. ACM Transactions on Graphics (ToG), 32(5):114, 2013. 2 [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 3 [20] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. [21] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 4 [22] Salman Siddique Khan, Varun Sundar, Vivek Boominathan, Ashok Veeraraghavan, and Kaushik Mitra. Flatnet: Towards photorealistic scene reconstruction from lensless measurements. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):19341948, 2020. 2 [23] Sun-Je Kim, Changhyun Kim, Youngjin Kim, Jinsoo Jeong, Seokho Choi, Woojun Han, Jaisoon Kim, and Byoungho Lee. Dielectric metalens: properties and three-dimensional imaging applications. Sensors, 21(13):4584, 2021. 1 [24] Lingshun Kong, Jiangxin Dong, Jianjun Ge, Mingqiang Li, and Jinshan Pan. Efficient frequency domain-based transformers for high-quality image deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 58865895, 2023. 3 [25] Dilip Krishnan and Rob Fergus. Fast image deconvolution using hyper-laplacian priors. Advances in neural information processing systems, 22, 2009. 1 [26] Byeonghyeon Lee, Youbin Kim, Yongjae Jo, Hyunsu Kim, Hyemi Park, Yangkyu Kim, Debabrata Mandal, Praneeth Chakravarthula, Inki Kim, and Eunbyung Park. Metaformer: High-fidelity metalens imaging via aberration correcting transformers. arXiv preprint arXiv:2412.04591, 2024. 3 [27] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 479:4759, 2022. 2, 3 [28] Xiu Li, Jinli Suo, Weihang Zhang, Xin Yuan, and Qionghai Dai. Universal and flexible optical aberration correction using deep-prior based deconvolution. pages 26132621, 2021. [29] Yan Li, Shuyi Chen, Haowen Liang, Xiuying Ren, Lingcong Luo, Yuye Ling, Shuxin Liu, Yikai Su, and Shin-Tson Wu. Ultracompact multifunctional metalens visor for augmented reality displays. PhotoniX, 3(1):29, 2022. 1 [30] Yawei Li, Yuchen Fan, Xiaoyu Xiang, Denis Demandolx, Rakesh Ranjan, Radu Timofte, and Luc Van Gool. Efficient and explicit modelling of image hierarchies for image In Proceedings of the IEEE/CVF Conference restoration. on Computer Vision and Pattern Recognition, pages 18278 18289, 2023. 3 [31] Zhaoyi Li, Peng Lin, Yao-Wei Huang, Joon-Suh Park, Wei Ting Chen, Zhujun Shi, Cheng-Wei Qiu, Ji-Xin Cheng, and Federico Capasso. Meta-optics achieves rgb-achromatic Science Advances, 7(5): focusing for virtual reality. eabe4458, 2021. 1 [32] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 18331844, 2021. 3, 6, 8 [33] Dianmin Lin, Pengyu Fan, Erez Hasman, and Mark Brongersma. Dielectric gradient metasurface optical elements. science, 345(6194):298302, 2014. 2 [34] Hanzhou Liu, Binghan Li, Chengkai Liu, and Mi Lu. Deblurdinat: lightweight and effective transformer for image deblurring. arXiv e-prints, pages arXiv2403, 2024. 3 [35] Zhisheng Lu, Juncheng Li, Hong Liu, Chaoyan Huang, Linlin Zhang, and Tieyong Zeng. Transformer for single image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 457466, 2022. [36] Joseph Mait, Ravindra Athale, Joseph van der Gracht, and Gary Euliss. Potential applications of metamaterials to computational imaging. In Frontiers in Optics, pages FTu8B1. Optica Publishing Group, 2020. 2 [37] Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. Making completely blind image quality analyzer. IEEE Signal Processing Letters, 20(3):209212, 2013. 6 [38] Kristina Monakhova, Kyrollos Yanny, Neerja Aggarwal, and Laura Waller. Spectral diffusercam: lensless snapshot hyperspectral imaging with spectral filter array. Optica, 7(10): 12981307, 2020. 2 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 3 [40] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superIEEE transactions on resolution via iterative refinement. pattern analysis and machine intelligence, 45(4):47134726, 2022. 2, 3 [41] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. [42] Christian Schuler, Harold Christopher Burger, Stefan Harmeling, and Bernhard Scholkopf. machine learning approach for non-blind image deconvolution. pages 1067 1074, 2013. 2 [43] Endri Stoja, Simon Konstandin, Dennis Philipp, Robin Wilke, Diego Betancourt, Thomas Bertuch, Jurgen Jenne, Reiner Umathum, and Matthias Gunther. Improving magnetic resonance imaging with smart and thin metasurfaces. Scientific reports, 11(1):16179, 2021. 1 [44] Jun Tanida, Tomoya Kumagai, Kenji Yamada, Shigehiro Miyatake, Kouichi Ishida, Takashi Morimoto, Noriyuki Kondou, Daisuke Miyazaki, and Yoshiki Ichioka. Thin observation module by bound optics (tombo): concept and experimental verification. Applied optics, 40(11):18061813, 2001. 2 [45] Radu Timofte and Agustsson. Ntire 2017 challenge on sinIn 2017 gle image super-resolution: Methods and results. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 11101121, 2017. 6 [46] Radu Timofte, Eirikur Agustsson, Luc Van Gool, MingHsuan Yang, and Lei Zhang. Ntire 2017 challenge on single In Proceedimage super-resolution: Methods and results. ings of the IEEE conference on computer vision and pattern recognition workshops, pages 114125, 2017. 6 [47] Ethan Tseng, Shane Colburn, James Whitehead, Luocheng Huang, Seung-Hwan Baek, Arka Majumdar, and Felix Heide. Neural nano-optics for high-quality thin lens imaging. Nature communications, 12(1):6493, 2021. 2, 6, 8 [48] Zhijun Tu, Kunpeng Du, Hanting Chen, Hailing Wang, Wei Li, Jie Hu, and Yunhe Wang. Ipt-v2: Efficient image processing transformer using hierarchical attentions. arXiv preprint arXiv:2404.00633, 2024. 3 [49] Kartik Venkataraman, Dan Lelescu, Jacques Duparre, Andrew McMahon, Gabriel Molina, Priyam Chatterjee, Robert Mullis, and Shree Nayar. Picam: An ultra-thin high performance monolithic camera array. ACM Transactions on Graphics (TOG), 32(6):113, 2013. [50] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023. 6 [51] Shuming Wang, Pin Chieh Wu, Vin-Cent Su, Yi-Chieh Lai, Mu-Ku Chen, Hsin Yu Kuo, Bo Han Chen, Yu Han Chen, Tzu-Ting Huang, Jung-Hsi Wang, et al. broadband achromatic metalens in the visible. Nature nanotechnology, 13(3): 227232, 2018. 2 [52] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [53] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1768317693, 2022. 3 [54] Alexander White, Parham Khial, Fariborz Salehi, Babak Hassibi, and Ali Hajimiri. silicon photonics computational lensless active-flat-optics imaging system. Scientific Reports, 10(1):1689, 2020. 2 [55] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series. The MIT press, 1964. 1, 3, [56] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. Seesr: Towards semanticsaware real-world image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2545625467, 2024. 2, 3, 8 [57] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image super-resolution. Advances in Neural Information Processing Systems, 37:9252992553, 2025. 3, 8 [58] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse In Proceedrepresentation for natural image deblurring. ings of the IEEE conference on computer vision and pattern recognition, pages 11071114, 2013. 1 [59] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11911200, 2022. 6 [60] Gwanho Yoon, Kwan Kim, Se-Um Kim, Seunghoon Han, Heon Lee, and Junsuk Rho. Printable nanocomposite metalens for high-contrast near-infrared imaging. ACS nano, 15 (1):698706, 2021. 1 [61] Nanfang Yu and Federico Capasso. Flat optics with designer metasurfaces. Nature materials, 13(2):139150, 2014. 2 [62] Aiping Zhang, Zongsheng Yue, Renjing Pei, Wenqi Ren, and Xiaochun Cao. Degradation-guided one-step image super-resolution with diffusion priors. arXiv preprint arXiv:2409.17058, 2024. [63] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018."
        }
    ],
    "affiliations": []
}