{
    "paper_title": "Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities",
    "authors": [
        "Jinyang Wu",
        "Chonghua Liao",
        "Mingkuan Feng",
        "Shuai Zhang",
        "Zhengqi Wen",
        "Pengpeng Shao",
        "Huazhe Xu",
        "Jianhua Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance (\"thought patterns\"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 9 6 5 1 . 5 0 5 2 : r Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities Jinyang Wu1 Chonghua Liao2 Mingkuan Feng1 Shuai Zhang1 Zhengqi Wen3 Pengpeng Shao3 Huazhe Xu Jianhua Tao13 1 Department of Automation, Tsinghua University 2 Institution for Interdisciplinary Information Sciences, Tsinghua University 3 Beijing National Research Center for Information Science and Technology 4 Shanghai Qi Zhi Institute 5 Shanghai AI Lab {wu-jy23,lch22}@mails.tsinghua.edu.cn, zhang_shuai@mail.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the models output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), novel framework that augments RL by incorporating external high-level guidance (thought patterns). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPOs potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has demonstrated remarkable success in enhancing the reasoning capabilities of large language models (LLMs), as exemplified by OpenAI-o1 [1], DeepSeek-R1 [2], and Kimi-1.5 [3]. In contrast to traditional approaches that rely on human-curated annotations [4, 5], contemporary RL training paradigms [2, 6] directly optimize base language models using simple, automatically computable reward function. This approach enables models to develop sophisticated Chain-of-Thought (CoT) [7] capabilities, and autonomously incentivize advanced reasoning behaviors, including problem decomposition, self-reflection, and iterative refinement [8, 9]. Recent RL research primarily focuses on enhancing training stability, efficiency and performance through two key aspects: (1) addressing inherent limitations of RL algorithm [1012], such as length bias and KL divergence constraints; and (2) improving data organization and reducing data dependency [1315], such as enabling no-supervision training. Despite these advancements, existing GRPO-based methods typically bias the models self-generated output distribution toward rewardmaximizing trajectories without incorporating external knowledge. This inherent limitation constrains exploration capacity and results in narrower reasoning capabilities compared to base models [9, 8]. Equal contributions. Figure 1: Overall performance across five competition-level benchmarks (MATH-500, AIME 2024, AMC, OlmpiadBench, and Minerva Math). TAPO significantly outperforms existing RL methods, especially on the challenging AIME and AMC benchmark (99% and 41% over GRPO). While very recent and concurrent work LUFFY [16] introduces off-policy guidance to enhance on-policy learning, it necessitates supervision signals from computationally expensive strong policy model (DeepSeek-R1 [2] in the paper). Moreover, the substantial capability gap between the external strong policy and the policy model being trained may lead to training instability issues. To address these limitations, we propose TAPO, Thought-Augmented Policy Optimization framework for LLM reasoning. Building upon conventional RL methods like GRPO [6], TAPO introduces high-level thought patterns that effectively bridge external guidance and model internal reasoning capabilities during training. Specifically, we design thought library, general repository storing high-level thought templates abstracted from 500 prior samples. Each template represents an abstract problem-solving strategy for category of problems and serves as reasoning guidance. For each incoming question during GRPO sampling, we adaptively identify and apply relevant thought templates from this library to enhance the reasoning process. This dynamic integration of external guidance with internal model abilities enables the system to internalize more generalizable and explainable reasoning behaviors, stabilize model learning, and produce more powerful reasoning models. Extensive experiments demonstrate that TAPO significantly outperforms GRPO across diverse datasets, achieving an average improvement of +12.0 points, including gains of 99% on AIME, 41% on AMC, and 17% on Minerva Math. As shown in Figure 1, our method also surpasses other powerful RL approaches. Moreover, TAPO proves effective across various model scales and architectures while exhibiting strong generalization to out-of-distribution reasoning tasks. Notably, our method can achieve stable learning on Llama3.2-3B-Base, which has been previously documented to struggle with standard GRPO training [8, 11]. Further analysis confirms that introducing external guidance enhances both model output explainability and readability. Our core contributions are: Novel RL framework: We propose Thought-Augmented Policy Optimization (TAPO), which enhances model reasoning by integrating external high-level thought guidance. Remarkable Performance: TAPO significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math, as well as previous powerful RL methods. Superior Generalization and Enhanced Output Quality: TAPO extends effectively to out-ofdistribution tasks, various model types while improving output explainability and readability."
        },
        {
            "title": "2 Thought-Augmented Policy Optimization",
            "content": "In this paper, we aim to investigate RL for LLMs with external guidance, as shown in Figure 2. We first introduce the popular RL method GRPO [6] (Section 2.1), then present our extended GRPO framework incorporating high-level thought guidance (Section 2.2), and finally describe how to construct thought library that provides external guidance for RL training (Section 2.3). 2 Figure 2: Flowchart of TAPO: Enhancing policy model capabilities through integration of external guidance (high-level thought patterns). This thought augmentation establishes an optimal balance between model internal exploration and external strategy exploitation. 2.1 Group Relative Policy Optimization (GRPO) LLMs as Markov Decision Processes The generation process of LLMs can be formulated as token-level Markov Decision Process (MDP) M(S, A, r, pQ) [11, 17], where represents states (observation sequences) and represents the action space (vocabulary). At each step t, the state st consists of the concatenation of the input question and all tokens generated so far o<t. This state serves as input to the policy model πθ(st). Specifically, the policy processes st = (q, o<t) = (q1, q2, . . . , ql, o1, o2, . . . , ot1), where qi denotes the i-th token of question and oj,<t represents the token generated by πθ at step j. The policy then samples the next token from the vocabulary A. In the RL framework, the entropy-regularized objective [18] to be optimized is: (cid:20) (πθ) = qpQ oπθ(q) (cid:21) [R(q, o)] β DKL[πθ(q))πref(q)] , (1) where R(q, o) = (cid:80)o t=1 r(st, ot) denotes the return [17] of the trajectory (q; o), r() represents the reward model, and πref is used to denote reference policy. The KL regularization term is usually adopted to prevent the policy model πθ from deviating too far from the reference model πref. GRPO Traditional RL approaches such as Proximal Policy Optimization (PPO) [19] employ policy gradient methods to optimize the objective in Equation 1. GRPO [2] offers an elegant simplification of PPO by eliminating the need for an additional reward model. It assigns scalar reward to each trajectory and normalizes these rewards across the group. Specifically, let πθold denote the policy model before updating. Given an input question and outputs {o1, . . . , oG} generated by πθold, the normalized reward Ai,t is shared across all tokens in oi: Ai,t = r(oi) mean({r(oi) oi πθold(st), = 1, 2, . . . , G}) std({r(oi) oi πθold (st), = 1, 2, . . . , G}) . (2) Then, the GRPO objective function is shown below: JGRPO(πθ) = 1 (cid:88) i=1 1 oi with probability ratio ρi,t = t=1 πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) oi (cid:88) {min [ρi,tAi,t, ˆρi,tAi,t] β DKL [πθ(q))πref(q)]} and clipped ratio ˆρi,t = clip(ρi,t; 1 ϵ, 1 + ϵ) . (3) In practical implementations of GRPO, r() typically represents rule-based verifier. 2.2 Extending GRPO with High-Level External Guidance In this subsection, we formally extend GRPO by incorporating high-level external guidance to enhance the models internal reasoning capabilities. For multi-step reasoning tasks, it is typically 3 easier for weak models to generate one correct step than to complete the entire reasoning steps in single inference. We leverage this characteristic through our guidance mechanism. We define guidance to be function that transforms an input question into thought-augmented form qaug. When prompted with guidance, the model generates partial solution which is then combined with the original question to create question with hints qaug. Specifically, for specific guidance gj, we have qaug = gj(q). Then, for each augmented question qaug , we sample micro group of Gj outputs oj,1, oj,2, . . . , oj,Gj from the old policy model, representing diverse reasoning paths facilitated by the guidance. Section 2.3 provides details of this guidance mechanism. Given multiple diverse guidances g1, ..., gj, the GRPO objective can be reformulated as: JGRPO(πθ) = 1 (cid:80)g i=1 Gi probability ratio ρi,j,t = oi (cid:88) (cid:88) Gi(cid:88) 1 oi i=1 j=1 πθ(oi,j,tqaug πθold(oi,j,tqaug t=1 , oi,j,<t) , oi,j,<t) {min [ρi,j,tAi,t, ˆρi,j,tAi,t] β DKL [πθπref]} , , clipped ratio ˆρi,j,t = clip(ρi,j,t; 1 ϵ, 1 + ϵ) . (4) For simplicity, we denote the objective for each thought guidance as: Ji(πθ) = 1 Gi Gi(cid:88) j=1 1 oi oi (cid:88) t=1 {min [ρi,j,tAi,t, ˆρi,j,tAi,t] β DKL [πθ(q))πref(q)]} , (5) then, the extended GRPO objective (Equation 4) can be written as: (cid:88) i=1 GiJi(πθ) . JGRPO(πθ) = 1 (cid:80)g i=1 Gi This formulation can be viewed as weighted sum of each guidance-specific objective, with Gi serving as the weight for each term. Consequently, we are optimizing the model under multiple diverse high-level thought guidances simultaneously. As demonstrated in DAPO [10], prompts yielding binary accuracy values (0 or 1) produce no gradients, impeding learning and reducing sample efficiency. Let pj ( 1) denote the probability of obtaining zero accuracy when sampling from πθold(qaug ). The probability of obtaining at least one positive sample in the training group becomes 1 Πg pi ( 1 pj, j). Thus, we can infer that training with such grouped samples, guided by more diverse instructions, will lead to more stable model learning. (6) In our implementation, we generate equal outputs for each guidance (G1 = G2 = = Gg), assigning uniform learning weights. Future work could explore different or dynamic weight allocation strategies based on the learning process. Notably, when = 1 and questions remain unaugmented (q = qaug), Equation 4 degenerates to the vanilla GRPO objective (Equation 3). Therefore, GRPO can be viewed as special case of our more general framework. 2.3 Thought Library and Augmented Reasoning In this subsection, we describe how to construct our thought library, which provides external guidance for RL training in Section 2.2. As illustrated in previous work [20, 21], humans typically solve complex reasoning tasks by applying universal guidelines (thought patterns) induced from similar problems rather than starting from scratch. These high-level thought patterns help address unfamiliar tasks by leveraging previously successful reasoning strategies. Inspired by prior work [22, 23], we introduce thought library, lightweight hub of high-level thought templates abstracted from just 500 seed datathat adaptively provides relevant thought patterns during GRPO sampling. Thought Library Starting with small set of seed samples = {s1, . . . , ss}, we employ Monte Carlo Tree Search (MCTS) [2426] to generate solution trees. For each question si S, predefined action set = {a1, ..., aA} and model π, MCTS build search tree Ti where: the root node represents question si, each edge denotes an action A, and each child node contains partial solutions generated by π under the corresponding action. path from the root to leaf node nj,d forms solution trajectory tj = (si, aj,1, nj,1, . . . , aj,d, nj,d). Each intermediate node nj,l is generated based on the cumulative context of its parent nodes and the current action, i.e. nj,l = 4 π([si, aj,1, nj,1, . . . , aj,l]). Through this process, we obtain diverse set of solution traces = {t1, t2, . . . , tt}. The MCTS algorithm will assign final reward R(tjsi) to each trace tj T. Further MCTS details are provided in Appendix A.1. Through the above process, for each seed question si, we obtain multiple solution traces. To identify the optimal trajectory for si, we utilize simple path selection metric proposed in HiAR-ICL [23]: Score(si, tj) = R(tjsi) (1 b) C(tj) , (7) where C(tj) represents trajectory complexity (action count), and (set to 0.95) balances solution quality against complexity. This scoring function selects trajectories that maximize accuracy while maintaining procedural conciseness among multiple potential solutions. For each question si S, we select the optimal solution trace ti,best that maximizes this score. Since each node in ti,best corresponds to an instantiated action ai,l A, we retain the more general action-trace as high-level thought pattern Tj = (a1, . . . , ad), and aggregate these patterns to construct our thought library = { ˆT1, . . . , ˆTs}. This aggregation is guided by the Problem Condition Complexity (PCC) [27, 28], which represents the number of known prior conditions in and can be calculated by the model π. Ultimately, each thought template in our library contains both high-level thought pattern (e.g., a1 a2 a4) and the average PCC of questions sharing this pattern: ˆTj = (PCCTj , Tj). These templates represent generalized problem-solving strategies for similar problems and serve as the external guidance described in Section 2.2. Detailed implementations are provided in Appendix A.2. Figure 3: Schematic diagram of actionchain-structured solution trajectory. Reasoning with Guidance Drawing on meta-reasoning principles [29, 30], we adaptively identify the most relevant high-level thought patterns for each new problem. During GRPO sampling for new incoming question qt, we compute its PCC metric and calculate the absolute distance dj = PCCqt PCCTj for each ˆTj L. We then sort them to obtain the most similar templates { ˆTi1, . . . , ˆTik } that best align with the questions complexity. The thought patterns on these templates, which are sequences of actions, guide the step-by-step reasoning process for question qt. Notably, we use MCTS to build the reasoning tree, abstract it to high-level thought library, and then match new questions with it. In fact, such thought library could also be obtained in other ways. For example, human experts could write general solution approach for each category of problems and then match new questions with existing problem types. We leave this for future work."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Training Datasets To keep the training recipe simple, we select training data exclusively from the training set of MATH [31] datasets. Following prior studies [32, 11], we only use MATH level 3-5 problems for training, yielding 5.5K examples. We randomly sample 500 instances to generate high-level thought patterns, with the remaining 5K examples for training in this work. Evaluation We mainly focus on several widely used math reasoning benchmarks, including MATH500 [31], AIME 2024 [33], AMC [33], Minerva [34], OlympiadBench [35], GSM8K [36], College Math [37], and Gaokao23 [38]. Since our RL training focus on math reasoning, we further assess the generalization capability on three out-of-distribution benchmarks: GPQA-Diamond [39] (science sraduate knowledge), ARC-C [40] (open-domain reasoning), and MMLU-Pro [41] (questions from academic exams and textbooks). Following common practice and previous work [11, 13], we use greedy decoding during evaluation. We also limit the sampling budget to 3000 tokens. Baseline Methods We benchmark TAPO with the following baselines on Qwen2.5-Math-7B: (1) GRPO [6], simplified PPO variant using identical 5k training samples as TAPO; (2) SimpleRLZero [32], which applies GRPO to approximately 24k math samples from GSM8K [36] and MATH [31]; (3) OpenReasoner-Zero [42], employing PPO with 129k samples from diverse sources 5 Table 1: Main results (%) on five competition-level reasoning benchmarks based on Qwen2.5-Math7B-Base. The best results on each benchmark are highlighted in bold. TAPO outperforms RL baselines and shows significant gains over GRPO, with relative improvements provided. Model MATH500 AIME24 AMC Minerva Olympiad Avg. Qwen2.5-Math [45] Qwen2.5-Math-Instruct [45] SimpleRL-Zero [32] OpenReasoner-Zero [47] PRIME-Zero [43] Oat-Zero [11] LUFFY [16] GRPO [6] TAPO (Ours) () 50.8 81.0 74.6 81.0 79.0 79.6 83. 76.2 83.4 +9.4% 13.3 13.3 26.7 16.7 20.0 30.0 26.7 42.5 55.0 60.0 57.5 60.0 60.0 70.0 12.1 32. 27.6 32.7 36.4 34.2 32.7 17.2 38.8 35.8 43.2 40.6 39.9 46.2 27.2 44.1 44.9 46.2 47.2 48.7 51.7 16.7 33.3 +99.4% 55.0 77.5 +40.9% 32.7 38.2 +16.8% 38.1 46.2 +21.2% 43.8 55.8 +27.4% including AIME; (4) PRIME-Zero [43], utilizing implicit process rewards with policy rollouts and outcome labels on 150k NuminaMath [33] queries; (5) Oat-Zero [11], which introduces Dr.GRPO to mitigate length bias, trained on 8k MATH questions; and (6) LUFFY [16], featuring mixed-policy GRPO that incorporates DeepSeek-R1s outputs, trained on 45k samples from OpenR1-Math-220k [44]. Implementation Details Follow previous work [43, 32, 11], we primarily use Qwen2.5-Math7B [45] as the default model. Additionally, we apply TAPO to Qwen2.5-Math-1.5B [45], Qwen2.5Math-1.5B-Instruct [45], Qwen2.5-7b-instruct [46], Llama-3.2-3B-Instruct [5], and Llama-3.1-8BInstruct [5] to showcase its adaptability across different model scales and types. For RL training, we follow the Open-R1 [44] pipeline. Following previous work [11, 16], we remove the KL loss term by setting β = 0 and employ the Dr.GRPO loss. Our training configuration includes batch size of 128, generating 16 samples per prompt. In our implementation, we set the number of guidance to 2 (i.e., = 2) by default. We generate an equal number of rollouts for each guidance, which means G1 = G2 = 16/2 = 8. The reward function is binary accuracy metric verified by Math-Verify. We train for 500 steps for all experiments. All training experiments are conducted using 8 A100 GPUs. More implementation details are provided in Appendix B."
        },
        {
            "title": "4 Results and Discussion",
            "content": "The section presents the results of TAPO from four aspects: 4.1 Main Results, 4.2 Training Dynamics, 4.3 Ablation Study and Discussion, and 4.4 Case Study. 4.1 Main Results Reasoning Benchmark Performance Following previous works [11, 16], Table 1 presents the main results across five competition-level reasoning benchmarks. We compare TAPO with multiple representative RL methods, as described in Baseline Methods. To ensure fair comparison, all baselines are based on Qwen2.5-Math-7B. Our evaluation results reveals three key insights: TAPO achieves an average score of 55.8, significantly outperforming existing powerful RL methods by margin of +4.1 points over the best baseline, clearly demonstrating the benefit of integrating high-level external guidance with model internal reasoning capabilities. On challenging datasets like AMC (+7.5 points over best baseline), TAPO significantly outperforms other methods. By adaptively integrating action-chain structured thought patterns during GRPO training, our method enables more nuanced problem decomposition, generates higher-quality training samples, and thus facilitates more effective model learning. Compared to its GRPO counterpart, TAPO consistently outperforms across all benchmarks, achieving notable improvement of +12.0 points on average. This consistent performance gain provides more robust and effective alternative for RL training. 6 Table 2: Accuracy (%) results of different LLMs across eight benchmarks. The best results in each box are highlighted in bold. We provide the relative improvement of our method compared to GRPO. Method AIME24 AMC MATH500 GSM8K Minerva Olympiad CollegeMath Gaokao23 Avg. CoT GRPO Ours () CoT GRPO Ours () CoT GRPO Ours () CoT GRPO Ours () CoT GRPO Ours () 10.0 13.3 16.7 +25.6% 6.7 13.3 16.7 +25.6% 13.3 13.3 16.7 +25.6% 42.5 40.0 55.0 +37.5% 47.5 52.5 55.0 +4.8% 47.5 57.5 67.5 +17.4% 6.7 3.3 6.7 20.0 25.0 27.5 +103.0% +10.0% 59.0 66.4 69.0 +3.9% 68.2 76.8 76.0 -1.0% 73.2 76.6 78.0 +1.9% 38.3 47.8 48.8 +2.1% 3.3 3.3 6.7 20.0 22.5 30. +103.0% +33.3% 36.6 45.0 52.2 +16.0% Qwen2.5-1.5B-Math [45] 74.6 74.7 84.2 +12.7% 24.3 25.0 31.6 +26.4% 27.6 30.1 33.6 +11.7% Qwen2.5-1.5B-Math-Instruct [45] 76.8 85.9 86.5 +1.0% 28.3 28.3 29.4 +3.9% 36.9 36.7 39.7 +8.2% Qwen2.5-7B-Instruct [46] 90.0 90.1 91.5 +1.6% 30.5 32.4 36.8 +13.6% 38.8 36.1 40.6 +12.5% Llama-3.2-3B-Instruct [5] 69.3 75.2 78.8 +4.8% 11.8 17.6 18.4 +4.6% 12.6 14.5 16.0 +10.4% Llama-3.1-8B-Instruct [5] 77.2 82.9 85.2 +2.8% 16.2 21.0 26.8 +27.7% 15.9 16.1 17.3 +7.5% 39.5 40.5 47.3 +16.8% 47.1 45.9 48.3 +5.3% 46.9 44.5 50.6 +13.7% 23.8 34.1 32.5 -4.7% 13.3 31.7 34.1 +7.6% 49.6 52.7 54.8 +4.0% 63.1 65.2 65.7 +1.0% 64.2 62.9 65.2 +3.7% 33.5 40.8 43.1 +5.7% 29.9 40.8 42.6 +4.5% 40.9 42.8 49.0 +14.5% 46.8 50.6 52.2 +3.2% 50.5 51.6 55.8 +8.1% 27.6 32.2 34.0 +5.6% 26.5 32.9 36.7 +11.6% Out-of-Distribution Generalization Recent studies have highlighted the critical impact of distributional bias on LLMs reliability [48, 49]. Despite impressive in-distribution (ID) performance, these models substantially underperform when confronted with out-of-distribution (OOD) data [50, 51]. To assess TAPOs OOD generalization capabilities, we evaluate on three challenging benchmarks: ARC-C, GPQA-Diamond, and MMLU-Pro. Given that all compared methods were trained on mathematical data, this setup provides robust OOD evaluation. As shown in Figure 4, TAPO outperforms GRPO by 13.7% on average across OOD tasks. These results highlight TAPOs effectiveness in using high-level external guidance to enhance OOD generalization. Extension to More Models To demonstrate TAPOs effectiveness across different scales and model types, we extend TAPO to several weaker models: Qwen2.5-1.5B-Math, Qwen2.5-1.5BMath-Instruct, Qwen2.5-7B-Instruct, Llama-3.23B-Instruct and Llama-3.1-8B-Instruct. As shown in Table 2, TAPO achieves significant improvements across all models. Taking Qwen2.51.5B-Math as an example, TAPO achieves an average improvement of 14.5%. similar trend is also observed on Qwen2.5-1.5BMath-Instruct, Qwen2.5-7B-Instruct, Llama-3.23B-Instruct, and Llama-3.1-8B-Instruct, where TAPO achieves improvements of 3.2%, 8.1%, 5.6%, and 11.6%, respectively. 4.2 Training Dynamics Figure 4: Results on three out-of-distribution benchmark datasets (Qwen2.5-Math-7B-Base). In this section, we aim to explore the behavioral differences between TAPO and GRPO through the training reward curves. We conduct experiments on Qwen-2.5-7B-Math-Base and Llama3.2-3B-Base. More Stable Model Learning As illustrated in Figure 5, TAPO consistently achieves higher overall training rewards than GRPO for both models. While this advantage appears modest for Qwen2.57 (a) Qwen2.5-Math-7B-Base (b) Llama-3.2-3B-Base Figure 5: Training Reward Curve on Qwen2.5-Math-7B-Base and Llama-3.2-3B-Base. (a) Training Reward Curve (b) Evaluation Performance Figure 6: Ablation results with different numbers of thought patterns (external guidance). Math-7B-Base, it becomes substantially more pronounced when using the weaker Llama3.2-3B-Base. Notably, GRPO tends to collapse during later training stages. This observation aligns with previous findings [8, 11], which demonstrate that directly applying GRPO to weak models like Llama3.2-3BBase frequently leads to training collapse across both mathematical tasks and reasoning games. In contrast, TAPO maintains stable training dynamics even with such weak models. The enhanced stability of TAPO stems from its strategic integration of thought patterns into the sampling process. By breaking down complex problems into manageable subtasks, TAPO provides explicit external guidance. This equips weaker models with advanced task decomposition capabilities usually exclusive to stronger models. As result, weaker models can learn from difficult examples that typically fail to produce positive outcomes under standard GRPO training. As shown in Section 2, this mechanism directly contributes to enhanced training stability and robust model performance. 4.3 Ablation Study and Discussion We investigate the effect of external guidance quantity (g = 1, 2, 4, 8) on training dynamics with fixed total rollouts. Figure 6 shows training rewards and evaluation results across varying thought pattern quantities. For clearer visualization, we uniformly adjust performance on AMC and MATH by subtracting fixed value without affecting conclusions. We observe that more diverse guidance (g = 1, 2, 4) typically yields higher training rewards. However, when guidance becomes excessive (g = 8), performance declines slightly while still outperforming minimal guidance (g = 1). This may be because: (1) Smaller makes it harder to sample positive examples; (2) Larger increases the likelihood of sampling positive examples, but excessive quantities can overwhelm the 8 Figure 7: Comparison of GRPO and TAPO for simple geometry problem from the MATH dataset. models limited learning capacity with too many patterns. In other words, when pattern quantity becomes excessive, the model faces too many learning tasks simultaneously, exceeding its processing capabilities. Therefore, designing an effective thought-augmented framework requires balancing pattern quantity and model capacity to ensure stable learning while maintaining generalization ability. Evaluation performance in Figure 6(b) follows similar trend, highlighting the importance of balancing guidance quantity with model capacity. 4.4 Case Study We compare the reasoning processes of TAPO and GRPO on geometry problem in Figure 7. GRPO produces less readable outputs with reasoning interspersed with code and inconsistent language. In contrast, TAPO first identifies its solution strategy (e.g., divide-and-conquer), then systematically addresses each subproblem with clear solutions. This demonstrates how TAPO training enhances both the readability and interpretability of the models reasoning process."
        },
        {
            "title": "5 Related Work",
            "content": "RL for LLMs Recent advances in LLM reasoning, such as OpenAI-o1 [1], DeepSeek-R1 [2], and Kimi-k1.5 [3], have shifted focus from Chain-of-Thought (CoT) [7] and supervised fine-tuning (SFT) [52, 53] to reinforcement learning (RL). Contemporary research have primarily focused on: (1) addressing inherent limitations of GRPO [1012], such as length bias and KL divergence constraints; and (2) improving data organization and reducing data dependency [1315]. However, these methods typically bias the models output distribution toward reward-maximizing paths without introducing external knowledge, narrowing reasoning capabilities compared to base models [9]. While the recent concurrent work LUFFY [16] introduces off-policy guidance to enhance on-policy learning, it still relies on supervision signals from strong policy (DeepSeek-R1). Moreover, the substantial capability gap between the external strong policy and the policy model being trained potentially increases training instability. In contrast, TAPO incorporates external high-level thought guidance to augment the models intrinsic capabilities without strong policy. By integrating diverse thought patterns, TAPO enables more stable learning and enhanced reasoning performance. Reasoning with Guidance common approach to enhancing model response quality involves augmenting input questions with external prompts [5458]. This methodology has been widely applied to reasoning tasks with varying implementation strategies. Some research adaptively searches for suitable exemplars to perform few-shot COT prompting [59], while others focus on decomposing complex reasoning tasks into simpler, sequential subtasks [60, 61]. The partial solutions derived 9 through few-shot prompting or task decomposition are subsequently concatenated with the original problem as guiding hints, thereby reducing problem complexity. However, these approaches typically necessitate meticulous prompt design and exhibit strong dependencies on example quality. Although recent works have advanced from specific examples toward more abstract high-level thought patterns [22, 23, 62], they primarily enhance the reasoning capabilities of fixed models through guidance. Moreover, few studies have investigated how to effectively integrate high-level guidance with RL training paradigms. Our work bridges this gap by introducing external abstract problem-solving guidance into RL training, achieving superior performance while maintaining flexibility."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce TAPO (Thought-Augmented Policy Optimization), novel RL framework that addresses fundamental limitations in current approaches for training reasoning models. By incorporating external high-level thought patterns into policy optimization, TAPO effectively bridges model-internal exploration with structured external guidance. Unlike conventional methods that merely bias toward reward-maximizing trajectories, our approach adaptively integrates abstract reasoning strategies during training, enhancing model capabilities across diverse problems. Extensive experiments demonstrate TAPOs significant improvements over GRPO, with gains of 99% on AIME, 40% on AMC, and 17% on Minerva Math. Our method maintains effectiveness across various model scales and architectures, including weak models that typically struggle with standard GRPO training. Moreover, TAPO produces models with enhanced output explainability and readability. These results establish TAPO as promising direction for developing more powerful, generalizable, and interpretable reasoning systems, opening avenues for future research on integrating high-level thought patterns into model training across broader reasoning domains."
        },
        {
            "title": "References",
            "content": "[1] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al., Openai o1 system card, arXiv preprint arXiv:2412.16720, 2024. [2] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [3] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al., Kimi k1. 5: Scaling reinforcement learning with llms, arXiv preprint arXiv:2501.12599, 2025. [4] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [5] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [6] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [7] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al., Chain-ofthought prompting elicits reasoning in large language models, Advances in neural information processing systems, vol. 35, pp. 2482424837, 2022. [8] K. Gandhi, A. Chakravarthy, A. Singh, N. Lile, and N. D. Goodman, Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, arXiv preprint arXiv:2503.01307, 2025. [9] Y. Yue, Z. Chen, R. Lu, A. Zhao, Z. Wang, S. Song, and G. Huang, Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, arXiv preprint arXiv:2504.13837, 2025. [10] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, et al., Dapo: An open-source llm reinforcement learning system at scale, arXiv preprint arXiv:2503.14476, 2025. [11] Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin, Understanding r1-zero-like training: critical perspective, arXiv preprint arXiv:2503.20783, 2025. [12] Z. Lin, M. Lin, Y. Xie, and R. Ji, Cppo: Accelerating the training of group relative policy optimization-based reasoning models, arXiv preprint arXiv:2503.22342, 2025. [13] Y. Zuo, K. Zhang, S. Qu, L. Sheng, X. Zhu, B. Qi, Y. Sun, G. Cui, N. Ding, and B. Zhou, Ttrl: Test-time reinforcement learning, arXiv preprint arXiv:2504.16084, 2025. [14] A. Zhao, Y. Wu, Y. Yue, T. Wu, Q. Xu, Y. Yue, M. Lin, S. Wang, Q. Wu, Z. Zheng, and G. Huang, Absolute zero: Reinforced self-play reasoning with zero data, 2025. [15] Y. Wang, Q. Yang, Z. Zeng, L. Ren, L. Liu, B. Peng, H. Cheng, X. He, K. Wang, J. Gao, et al., Reinforcement learning for reasoning in large language models with one training example, arXiv preprint arXiv:2504.20571, 2025. [16] J. Yan, Y. Li, Z. Hu, Z. Wang, G. Cui, X. Qu, Y. Cheng, and Y. Zhang, Learning to reason under off-policy guidance, arXiv preprint arXiv:2504.14945, 2025. [17] R. S. Sutton, A. G. Barto, et al., Reinforcement learning: An introduction, vol. 1. MIT press Cambridge, 1998. [18] J. Schulman, X. Chen, and P. Abbeel, Equivalence between policy gradients and soft q-learning, arXiv preprint arXiv:1704.06440, 2017. 11 [19] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [20] D. Kahneman, Thinking, Fast and Slow. New York, NY: Farrar, Straus and Giroux, 2011. [21] J. D. K. Ongchoco, J. Knobe, and J. Jara-Ettinger, Peoples thinking plans adapt to the problem theyre trying to solve, Cognition, vol. 243, p. 105669, 2024. [22] L. Yang, Z. Yu, T. Zhang, S. Cao, M. Xu, W. Zhang, J. E. Gonzalez, and B. Cui, Buffer of thoughts: Thought-augmented reasoning with large language models, Advances in Neural Information Processing Systems, vol. 37, pp. 113519113544, 2024. [23] J. Wu, M. Feng, S. Zhang, F. Che, Z. Wen, and J. Tao, Beyond examples: High-level automated reasoning paradigm in in-context learning via mcts, arXiv preprint arXiv:2411.18478, 2024. [24] L. Kocsis and C. Szepesvári, Bandit based monte-carlo planning, in European conference on machine learning, pp. 282293, Springer, 2006. [25] Z. Qi, M. Ma, J. Xu, L. L. Zhang, F. Yang, and M. Yang, Mutual reasoning makes smaller llms stronger problem-solvers, arXiv preprint arXiv:2408.06195, 2024. [26] X. Guan, L. L. Zhang, Y. Liu, N. Shang, Y. Sun, Y. Zhu, F. Yang, and M. Yang, rstarmath: Small llms can master math reasoning with self-evolved deep thinking, arXiv preprint arXiv:2501.04519, 2025. [27] F.-L. Lee and R. Heyworth, Problem complexity: measure of problem difficulty in algebra by using computer, EDUCATION JOURNAL-HONG KONG-CHINESE UNIVERSITY OF HONG KONG-, vol. 28, no. 1, pp. 85108, 2000. [28] S. E. Embretson and R. C. Daniel, Understanding and quantifying cognitive complexity level in mathematical problem solving items, Psychology Science, vol. 50, no. 3, p. 328, 2008. [29] S. Russell and E. Wefald, Principles of metareasoning, Artificial intelligence, vol. 49, no. 1-3, pp. 361395, 1991. [30] C. N. De Sabbata, T. R. Sumers, and T. L. Griffiths, Rational metareasoning for large language models, arXiv preprint arXiv:2410.05563, 2024. [31] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, Measuring mathematical problem solving with the MATH dataset, in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [32] W. Zeng, Y. Huang, Q. Liu, W. Liu, K. He, Z. Ma, and J. He, Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, arXiv preprint arXiv:2503.18892, 2025. [33] J. Li, E. Beeching, L. Tunstall, B. Lipkin, R. Soletskyi, S. Huang, K. Rasul, L. Yu, A. Q. Jiang, Z. Shen, et al., Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. https://huggingface.co/datasets/Numinamath, 2024. Hugging Face repository, 13:9. [34] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al., Solving quantitative reasoning problems with language models, Advances in Neural Information Processing Systems, vol. 35, pp. 38433857, 2022. [35] C. He, R. Luo, Y. Bai, S. Hu, Z. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, et al., Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, 2024. [36] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. 12 [37] Z. Tang, X. Zhang, B. Wang, and F. Wei, Mathscale: Scaling instruction tuning for mathematical reasoning, arXiv preprint arXiv:2403.02884, 2024. [38] X. Zhang, C. Li, Y. Zong, Z. Ying, L. He, and X. Qiu, Evaluating the performance of large language models on gaokao benchmark, arXiv preprint arXiv:2305.12474, 2023. [39] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, GPQA: graduate-level google-proof q&a benchmark, in First Conference on Language Modeling, 2024. [40] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, Think you have solved question answering? try arc, the ai2 reasoning challenge, arXiv:1803.05457v1, 2018. [41] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, et al., Mmlu-pro: more robust and challenging multi-task language understanding benchmark, arXiv preprint arXiv:2406.01574, 2024. [42] J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H.-Y. Shum, Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, arXiv preprint arXiv:2503.24290, 2025. [43] G. Cui, L. Yuan, Z. Wang, H. Wang, W. Li, B. He, Y. Fan, T. Yu, Q. Xu, W. Chen, et al., Process reinforcement through implicit rewards, arXiv preprint arXiv:2502.01456, 2025. [44] H. Face, Open r1: fully open reproduction of deepseek-r1, January 2025. [45] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, K. Lu, M. Xue, R. Lin, T. Liu, X. Ren, and Z. Zhang, Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. [46] Qwen Team, Qwen2.5: party of foundation models, September 2024. [47] J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H.-Y. Shum, Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. [48] L. Yuan, Y. Chen, G. Cui, H. Gao, F. Zou, X. Cheng, H. Ji, Z. Liu, and M. Sun, Revisiting out-of-distribution robustness in NLP: Benchmarks, analysis, and LLMs evaluations, in Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [49] J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, W. Ye, H. Huang, X. Geng, B. Jiao, Y. Zhang, and X. Xie, On the robustness of chatgpt: An adversarial and out-ofdistribution perspective, IEEE Data Eng. Bull., vol. 47, no. 1, pp. 4862, 2024. [50] L. Berglund, M. Tong, M. Kaufmann, M. Balesni, A. C. Stickland, T. Korbak, and O. Evans, The reversal curse: LLMs trained on is fail to learn is a, in The Twelfth International Conference on Learning Representations, 2024. [51] H. Yang, Y. Zhang, J. Xu, H. Lu, P.-A. Heng, and W. Lam, Unveiling the generalization power of fine-tuned large language models, in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), (Mexico City, Mexico), pp. 884899, Association for Computational Linguistics, June 2024. [52] C. Li, W. Wang, J. Hu, Y. Wei, N. Zheng, H. Hu, Z. Zhang, and H. Peng, Common 7b language models already possess strong math capabilities, arXiv preprint arXiv:2403.04706, 2024. [53] E. Yeo, Y. Tong, X. Niu, G. Neubig, and X. Yue, Demystifying long chain-of-thought reasoning in LLMs, in ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models, 2025. [54] A. Asai, S. Min, Z. Zhong, and D. Chen, Retrieval-based language models and applications, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pp. 4146, 2023. 13 [55] G. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozière, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz, et al., Augmented language models: survey, arXiv preprint arXiv:2302.07842, 2023. [56] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih, Replug: Retrieval-augmented black-box language models, arXiv preprint arXiv:2301.12652, 2023. [57] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, H. Wang, and H. Wang, Retrievalaugmented generation for large language models: survey, arXiv preprint arXiv:2312.10997, vol. 2, p. 1, 2023. [58] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y. Geng, F. Fu, L. Yang, W. Zhang, J. Jiang, and B. Cui, Retrieval-augmented generation for ai-generated content: survey, arXiv preprint arXiv:2402.19473, 2024. [59] Z. Zhang, A. Zhang, M. Li, and A. Smola, Automatic chain of thought prompting in large language models, arXiv preprint arXiv:2210.03493, 2022. [60] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al., Least-to-most prompting enables complex reasoning in large language models, arXiv preprint arXiv:2205.10625, 2022. [61] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal, Decomposed prompting: modular approach for solving complex tasks, arXiv preprint arXiv:2210.02406, 2022. [62] L. Yang, Z. Yu, B. Cui, and M. Wang, Reasonflux: Hierarchical llm reasoning via scaling thought templates, arXiv preprint arXiv:2502.06772, 2025. [63] G. Chaslot, S. Bakkes, I. Szita, and P. Spronck, Monte-carlo tree search: new framework for game ai, in Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, vol. 4, pp. 216217, 2008. [64] W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao, Mastering atari games with limited data, in Advances in Neural Information Processing Systems (M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 2547625488, Curran Associates, Inc., 2021. [65] A. Zhou, K. Yan, M. Shlapentokh-Rothman, H. Wang, and Y.-X. Wang, Language agent tree search unifies reasoning, acting, and planning in language models, in Forty-first International Conference on Machine Learning, 2024. [66] H. Dong, Z. Ding, and S. Zhang, Deep Reinforcement Learning: Fundamentals, Research and Applications, vol. 1 of eBook Packages: Mathematics and Statistics. Springer Singapore, 1 ed., 2020. [67] P. I. Jaffe, R. A. Poldrack, R. J. Schafer, and et al., Modelling human behaviour in cognitive tasks with latent dynamical systems, Nature Human Behaviour, vol. 7, pp. 9861000, 2023. [68] S. Da Silva, System 1 vs. system 2 thinking, Psych, vol. 5, no. 4, pp. 10571076, 2023. [69] Y. Qin, X. Li, H. Zou, Y. Liu, S. Xia, Z. Huang, Y. Ye, W. Yuan, H. Liu, Y. Li, et al., O1 replication journey: strategic progress reportpart 1, arXiv preprint arXiv:2410.18982, 2024. [70] L. Kocsis and C. Szepesvári, Bandit based monte-carlo planning, in Machine Learning: ECML 2006 (J. Fürnkranz, T. Scheffer, and M. Spiliopoulou, eds.), (Berlin, Heidelberg), pp. 282293, Springer Berlin Heidelberg, 2006. [71] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou, Self-consistency improves chain of thought reasoning in language models, in The Eleventh International Conference on Learning Representations, 2023."
        },
        {
            "title": "Technical Appendix of TAPO",
            "content": "The supplementary material provides in-depth insights into our TAPO method, covering additional algorithm details (A), experimental details (B), and case study (C). The appendix is organized as follows:"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Thought-Augmented Policy Optimization"
        },
        {
            "title": "2.1 Group Relative Policy Optimization (GRPO)",
            "content": ". . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.3 Thought Library and Augmented Reasoning . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 Experimental Setup 4 Results and Discussion 4.1 Main Results . . . . 4.2 Training Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Ablation Study and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Related Work 6 Conclusion More Details about TAPO A.1 Monte Carlo Tree Search (MCTS) . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Thought Library Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Adaptive Retrieval and Instantiation of Thought Patterns . . . . . . . . . . . . . . Experimental Details Case Study Future Directions 1 3 3 4 5 6 7 8 9 9 10 16 17 18 18"
        },
        {
            "title": "A More Details about TAPO",
            "content": "In this section, we provide comprehensive elaboration of the TAPO algorithms technical details. We further describe the specific implementation of the Monte Carlo Tree Search algorithm, the construction process of our thought library, and the adaptive retrieval and instantiation mechanisms for thought patterns during reasoning. A.1 Monte Carlo Tree Search (MCTS) As heuristic search algorithm, MCTS has demonstrated remarkable success in complex reasoning and decision-making environments [6365]. The algorithm conceptualizes search spaces as tree structures and has achieved significant breakthroughs across various domains, most notably in gameplaying AI such as AlphaGo and AlphaZero [66]. As described in Section 2.3 in the main text, we employ MCTS to generate solution trees based on small set of 500 seed samples. To implement MCTS effectively, we first define predefined action set. Understanding human complex reasoning is crucial for modeling cognitive processes [67]. Existing studies distinguish between two cognitive systems: System 1 and System 2 [20, 68]. While System 1 represents fast, intuitive, yet error-prone thinking, System 2 involves slow, deliberative thinking with superior performance. With the emergence of advanced models like OpenAIs o1, developing efficient System 2 approaches to emulate human cognitive processes has gained significant research attention [69, 2]. Inspired by this and following previous work [23, 25], we introduce five human-like reasoning actions to bridge the gap between model reasoning and human cognition: Divide and Conquer (DC, a1): Approaching complex problems by breaking them into manageable sub-problems for easier resolution. Self-Reflection (SR, a2): Assessing and refining prior solutions during the reasoning process to ensure correctness. System Analysis (SA, a3): Analyzing the overall structure of the problem and identifying the constraints and conditions before addressing it, thereby clarifying task requirements effectively. One-Step Thought (OST, a4): Aiming to address single aspect of the problem through focused and concise reasoning step. Chain-of-Thought (CoT, a5): Adopting sequential reasoning process that builds series of connected logical steps. Based on the above predefined action set = {a1, ..., aA} and model π, for each question si S, MCTS builds search tree Ti where: the root node represents question si, each edge denotes an action A, and each child node contains partial solutions generated by π under the corresponding action. path from the root to leaf node nj,d forms solution trajectory tj = (si, aj,1, nj,1, . . . , aj,d, nj,d). Each intermediate node nj,l is generated based on the cumulative context of its parent nodes and the current action, i.e. nj,l = π([si, aj,1, nj,1, . . . , aj,l]). Specifically, the MCTS algorithm involves an iterative search process with four key steps: selection, expansion, simulation, and backpropagation: (1) Selection. This operation identifies optimal nodes for expansion. Starting from the root node, child node is chosen at each tree level until reaching leaf node, defined as achieving maximum tree depth or arriving at an answer here. To balance the exploration and exploitation, we employ the well-known Upper Confidence Bounds applied to Trees (UCT) [70] for node selection: CT (s) = Q(s) + (cid:115) ln (p) (s) (8) where Q(s) is the reward value for node s, (s) is the visit count, is the parent node, and is the exploration weight. The node with the highest UCT value is selected for subsequent phases, balancing exploration and exploitation. (2) Expansion. The selected node is expanded by sampling actions from π and generating corresponding reasoning outcomes. These child nodes are then added to the tree. (3) Simulation. Starting from the selected node, we iteratively sample and expand nodes until reaching terminal state (maximum depth or final answer node). To enhance efficiency, we implement an 16 Figure 8: An illustration of four phases in an iteration of MCTS for complex reasoning tasks. early termination strategy based on self-consistency [71]. This strategy exploits the observation that repeatedly sampled actions at the same state likely indicate successful task completion. If the models consistency score exceeds threshold c, i.e., SC(s) > c, the simulation terminates early (4) Backpropagation. Upon simulation completion, node information is updated along the simulation path s0, ...sd. Visit counts are incremented (N (s) (s) + 1), and node value Q(s) is propagated backward to its parent node using the following equation: Q(p) (1 α)Q(p) + αQ(s) (9) where α is discount factor for future rewards. For terminal nodes, following prior work [25], we adopt the likelihood (confidence) of self-consistency majority voting as the reward value, enabling supervision-free generalization. Through the above four process, we obtain diverse set of solution traces = {t1, t2, . . . , tt} for question si S. The MCTS algorithm will assign final reward R(tjsi) to each trace tj T. Figure 8 illustrates the four phases in an iteration, expanding the tree and then updating reward values. A.2 Thought Library Construction For each question si S, we obtain its solution tree through MCTS, which provides multiple concrete solution paths for si. As described in Section 2.3 in the main text, we then need to identify the best reasoning path for si and abstract it into generalizable thought patterns. To identify the optimal trajectory for each question si, we employ balanced scoring metric proposed in HiAR-ICL [23]: Score(si, tj) = R(tjsi) (1 b) C(tj) , (10) where C(tj) represents trajectory complexity (action count), and (set to 0.95) balances solution quality against complexity. This scoring function selects trajectories that maximize accuracy while maintaining procedural conciseness among multiple potential solutions. For each question si S, we select the optimal solution trace ti,best that maximizes this score. Since each node in ti,best corresponds to an instantiated action ai,l A, we extract the general action sequence as high-level thought pattern Tj = (a1, . . . , ad). For instance, successful solution might follow the pattern a1 a2 a4 (System Analysis One-Step Thought Divide and Conquer). To organize these extracted patterns effectively, we introduce Problem Condition Complexity (PCC) [27, 28] as categorization metric. PCC quantifies the number of known prior conditions in question si and can be calculated by the model π. Similar problems tend to share similar PCC values, making this metric effective for pattern aggregation. Through this process, each question si is associated with its optimal thought pattern, with some questions naturally sharing identical patterns (e.g. a1 a2 a4). Our final thought library = { ˆT1, . . . , ˆTs} consists of entries where each thought template ˆTj contains both high-level 17 thought pattern and the average PCC of questions sharing this pattern: ˆTj = (PCCTj , Tj). These templates represent generalized problem-solving strategies and serve as external guidance for similar problems encountered during GRPO training. A.3 Adaptive Retrieval and Instantiation of Thought Patterns When encountering new problem during GRPO training, we employ an adaptive retrieval mechanism to identify and apply the most relevant reasoning strategies from our thought library. This approach is grounded in meta-reasoning principles [29, 30], which emphasize the importance of selecting appropriate problem-solving strategies based on problem characteristics. Adaptive Retrieval For each new incoming question qt encountered during GRPO sampling, we first compute its PCC metric. This complexity measure serves as fingerprint that characterizes the questions structure and difficulty. We then compare this value against the PCC values of all templates in our thought library by calculating the absolute distance for each template ˆTj L: dj = PCCqt PCCTj (11) This distance metric quantifies how similar the current questions complexity is to those problems from which each thought pattern was derived. Pattern Selection and Application After computing these distances, we rank the templates and select the most similar ones { ˆTi1, . . . , ˆTik } that minimize this distance measure. These selected templates contain high-level thought patterns that have proven effective for problems with similar complexity profiles. The retrieved thought patterns, which are sequences of abstract reasoning actions (a1, . . . , ad), guide the step-by-step reasoning process for question qt. During GRPO sampling, these patterns serve as external guidance that effectively balances between exploiting known successful strategies and allowing for model-internal exploration. This adaptive retrieval mechanism ensures that the model leverages appropriate reasoning strategies based on problem characteristics, rather than attempting to apply one-size-fits-all approach. By dynamically matching problems with relevant thought patterns, our framework enables more targeted and effective sampling across diverse problem types."
        },
        {
            "title": "B Experimental Details",
            "content": "In addition to the implementation details presented in the main text, we provide supplementary experimental details here. During training, we generate with rollout parameters of temperature=0.8 and top-p=0.95, and maximum generation of 1500 tokens. The reward function is binary accuracy metric verified by Math-Verify, defined as r(o) = 1{o contains the correct final answer}. Moreover, we employ cosine learning rate decay with warm-up. The maximum learning rate is set at 3 106, and the warm-up ratio is set at 0.1. We use the same system prompt for all experiments, as shown in Figure 9. Figure 9: The system prompt used for all experiments."
        },
        {
            "title": "C Case Study",
            "content": "To further analyze the improvements of TAPO over conventional GRPO, we compare their reasoning processes on representative mathematical problems from the MATH dataset in Figure 10 and Figure 11. 18 In Figure 10, we observe how both methods approach an arithmetic mean problem. GRPO produces solution with scattered notation and repetitive statements, particularly evident in its final steps where it repeatedly states The problem involves ... final answer is 46. In contrast, TAPO demonstrates more structured approach by explicitly introducing step by step thinking process. The thoughtaugmented process methodically builds upon each reasoning step, clearly identifying the relationship between the original sum (984), the highest score (98), and the remaining scores to derive the lowest score (46). This structured approach results in more readable and logically coherent solution. Figure 11 presents more challenging problem involving complex numbers and Vietas formulas. Here, the limitations of GRPO become more pronounced. While GRPO initially applies the correct formula, its reasoning process deteriorates into incoherent text fragments and coding artifacts (e.g., Tre localVEC? and various non-mathematical expressions). This demonstrates how GRPO struggles with maintaining coherent reasoning for complex problems. In contrast, TAPO maintains its structured approach throughout, clearly stating the problem context, applying Vietas formulas with proper explanation, and presenting clean, coherent solution without extraneous text or errors."
        },
        {
            "title": "D Future Directions",
            "content": "While TAPO demonstrates significant improvements in reasoning performance, our current work primarily focuses on mathematical reasoning and knowledge-intensive reasoning tasks. In future research, we plan to explore the application of thought-augmented reinforcement learning to more diverse tasks and domains, particularly retrieval-augmented generation and multimodal reasoning scenarios that integrate visual and textual information. In summary, TAPO establishes strong foundation for effectively integrating external knowledge with reinforcement learning for enhanced reasoning capabilities across various AI applications. Figure 10: Comparison of GRPO and TAPO for simple algorithm problem from the MATH dataset. 19 Figure 11: Comparison of GRPO and TAPO for difficult algorithm problem from the MATH dataset."
        }
    ],
    "affiliations": [
        "Beijing National Research Center for Information Science and Technology",
        "Department of Automation, Tsinghua University",
        "Institution for Interdisciplinary Information Sciences, Tsinghua University",
        "Shanghai AI Lab",
        "Shanghai Qi Zhi Institute"
    ]
}