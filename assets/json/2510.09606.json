{
    "paper_title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
    "authors": [
        "Peiwen Sun",
        "Shiqiang Lang",
        "Dongming Wu",
        "Yi Ding",
        "Kaituo Feng",
        "Huadai Liu",
        "Zhen Ye",
        "Rui Liu",
        "Yun-Hui Liu",
        "Jianan Wang",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the current surge in spatial reasoning explorations, researchers have made significant progress in understanding indoor scenes, but still struggle with diverse applications such as robotics and autonomous driving. This paper aims to advance all-scale spatial reasoning across diverse scenarios by tackling two key challenges: 1) the heavy reliance on indoor 3D scans and labor-intensive manual annotations for dataset curation; 2) the absence of effective all-scale scene modeling, which often leads to overfitting to individual scenes. In this paper, we introduce a holistic solution that integrates a structured spatial reasoning knowledge system, scale-aware modeling, and a progressive training paradigm, as the first attempt to broaden the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using a task-specific, specialist-driven automated pipeline, we curate over 38K video scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising approximately 1M spatial QA pairs spanning 19 diverse task types. While specialist models can inject useful domain knowledge, they are not reliable for evaluation. We then build an all-scale benchmark with precise annotations by manually recording, retrieving, and assembling video-based data. However, naive training with SpaceVista-1M often yields suboptimal results due to the potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a spatial reasoning model that accepts dense inputs beyond semantics and uses scale as an anchor for scale-aware experts and progressive rewards. Finally, extensive evaluations across 5 benchmarks, including our SpaceVista-Bench, demonstrate competitive performance, showcasing strong generalization across all scales and scenarios. Our dataset, model, and benchmark will be released on https://peiwensun2000.github.io/mm2km ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 6 0 6 9 0 . 0 1 5 2 : r Work in progress SPACEVISTA: ALL-SCALE VISUAL SPATIAL REASONING FROM mm TO km Peiwen Sun, Shiqiang Lang, Dongming Wu, Yi Ding, Kaituo Feng, Huadai Liu, Zhen Ye, Rui Liu, Yun-Hui Liu, Jianan Wang, Xiangyu YueQ Multimedia Lab, Chinese University of Hong Kong, Astribot, Beijing University of Posts and Telecommunications, Hong Kong University of Science and Technology (cid:140)Website: SpaceVista Homepage Figure 1: Prior works of spatial reasoning have largely focused on indoor (1-30 m) scenes, while our SpaceVista model and dataset span scales from mm (1e-3 m) to km (1e+3 m). This six-orderof-magnitude range introduces not only scale variation but also rich semantics and diverse tasks. SpaceVista enables all-scale spatial reasoning by integrating cues from micro-objects to macro-scenes."
        },
        {
            "title": "ABSTRACT",
            "content": "With the current surge in spatial reasoning explorations, researchers have made significant progress in understanding indoor scenes, but still struggle with diverse applications such as robotics and autonomous driving. This paper aims to advance all-scale spatial reasoning across diverse scenarios by tackling two key challenges: 1) the heavy reliance on indoor 3D scans and labor-intensive manual annotations for dataset curation; 2) the absence of effective all-scale scene modeling, which often leads to overfitting to individual scenes. In this paper, we introduce holistic solution that integrates structured spatial reasoning knowledge system, scaleaware modeling, and progressive training paradigm, as the first attempt to broaden the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using task-specific, specialist-driven automated pipeline, we curate over 38K video scenes across 5 spatial scales to create SpaceVista-1M, dataset comprising approximately 1M spatial QA pairs spanning 19 diverse task types. While : Equal contribution : Project leader Q: Corresponding authors 1 Work in progress specialist models can inject useful domain knowledge, they are not reliable for evaluation. We then build an all-scale benchmark with precise annotations by manually recording, retrieving, and assembling video-based data. However, naive training with SpaceVista-1M often yields suboptimal results due to the potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, spatial reasoning model that accepts dense inputs beyond semantics and uses scale as an anchor for scale-aware experts and progressive rewards. Finally, extensive evaluations across 5 benchmarks, including our SpaceVista-Bench, demonstrate competitive performance, showcasing strong generalization across all scales and scenarios. Our dataset, model, and benchmark will be released on our project page(cid:140)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Spatial reasoning, the ability to sense, interpret, and interact with environments across scales from tiny objects understanding to remote drone sensing, is crucial for next-generation intelligent systems. It significantly enhances 3D and even 4D scene understanding, enabling agents to interpret complex environments from easily obtainable videos. All-scale reasoning capability supports diverse applications: mm for advanced manufacturing (Song et al., 2024), cm and for embodied intelligence (Pan et al., 2025), 10m for autonomous driving (Liu et al., 2022), and 100m for drone-based sensing (Xiao et al., 2023). Recent research (Yang et al., 2025a), especially on how Multimodal Large Language Models (MLLMs) perceive and recall space, is narrowing the gap in visual spatial reasoning. The current works on spatial reasoning primarily focus on improvements from two perspectives: data and model. From the data perspective, pioneer works (Ouyang et al., 2025; Zhang et al., 2025e; Deng et al., 2025b) utilize more scanning-based data, or image-based data employing fully automated pipelines to acquire additional information for Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During modeling indoor spatial scenes, Wu et al. (2025a); Zheng et al. (2025) leverage latent features from VGGT (Wang et al., 2025a) by incorporating geometric information to enhance spatial understanding. Concurrently, series of outstanding works (Ouyang et al., 2025; Zhang et al., 2025e) have improved the performance of existing models by refining the training and thinking approaches. Moreover, Wu et al. (2025b) employs multi-turn dialogues to enhance selfcorrection capabilities. Figure 2: (a) and (b) show model performance and dataset distribution across scales. Current models and datasets necessitate all-scale spatial reasoning. Despite these works advancements, their spatial perception capabilities are primarily limited to indoor settings, specific objects, and constrained scales, as shown in the the bar chart Fig.1. Moreover, current methodologies lack dedicated training frameworks for holistic all-scale scene understanding. To bridge this gap, we introduce the first comprehensive solution to address data, model, and evaluation dimensions for all-scale scenarios. Previous datasets (Yang et al., 2025a;b; Ouyang et al., 2025; Zhang et al., 2025e) for spatial reasoning have primarily been constructed based on indoor scanning video data (Dai et al., 2017; Yeshwanth et al., 2023) as shown in Fig. 2(b). These indoor datasets often feature relatively simple scenes and depend on manual 3D annotations. Scaling up to build large-scale, wild datasets encompassing video scenes ranging from mm to km presents two major challenges: 1) the high cost of large-scale annotation from complex and wild scenes; 2) the difficulty in obtaining precise evaluations that 2 Work in progress align with the physical world. To address these challenges, we use an automated pipeline leveraging popular specialized models to generate structured training data across 5 different scales. Since different scales have distinct characteristics and applications, we define several scale-specific tasks for better application, i.e., manipulation planning and area estimation. Overall, we provide over 1 million QA pairs across 19 diverse tasks from around 38K wild video scenes. To adapt to different stages of training, we provide both answers with rationale for SFT and regression/multiple-choice answers for RL. To facilitate accurate evaluation, we collect highly accurate SpaceVista-Bench through manually recording or retrieving authoritative sources, supplemented with human annotations. Most popular reasoning models are optimized for indoor settings, which leads to clear limitations: their responses often deviate significantly, in tabletop and other diverse real-world scenes illustrated in Fig. 2(a). We address this by first injecting SpaceVista-1M knowledge to fine-tune existing models with the self-supervised visual encoder to make compensation for the classic semantic visual tokenizer, enabling extra geometry-based and depth-based spatial understanding. However, naive fine-tuning rarely yields optimal results, largely due to cross-scale conflicts between scenes and objects based on our observation. To address this, we introduce LoRA-like scale experts that cooperates with scale router during fine-tuning. Moreover, to strengthen the models ability to learn scale-centric spatial reasoning processes, we design training strategy that uses scale as an anchor for progressive rewards. During evaluation, SpaceVista-7B shows superior understanding of spatial layout, size, and comparison, delivering clear improvement on popular benchmarks and SpaceVista-Bench. Our key contributions with this comprehensive solution are: Developing an automated pipeline to create diverse, real-world, all-scale reasoning dataset, SpaceVista-1M, with 1M QA pairs across 5 scales and 19 tasks (including specific-scale tasks), and supporting both cold start with rationale and high-quality reinforced learning. Introducing SpaceVista-7B, spatial reasoning model that integrates rich spatial information and employs scale experts with customized training strategy to alleviate potential crossscale conflicts during all-scale finetuning. Hand-crafting SpaceVista-Bench, an accurate video benchmark spanning all scales, by measuring and recording real-world objects, retrieving authoritative sources, and performing human annotation."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Visual Reasoning. Currently, vision-based general reasoning has seen diverse developments (Tan et al., 2025; Wang et al., 2025b; Qiao et al., 2025). General MLLMs (Wang et al., 2025c; Bai et al., 2025) first provided the basic understanding ability towards video to the community. Pioneering works (Feng et al., 2025; Liao et al., 2025) started to provide reasonable rewards during model training using Group Relative Policy Optimization (GRPO) for the reasonable Chain of Thought (CoT). Then, visual reasoning (Li et al., 2025c; Chen et al., 2025; Liu et al., 2025c) was considered from broader perspectives, ranging from data to training structure. In general video reasoning, spatial claims are generally divided into two categories: 2D plane-based spatial reasoning (Han et al., 2025; Zhou et al., 2025), and 3D space-based spatial reasoning (Wu et al., 2025a; Zheng et al., 2025). This paper primarily focuses on the latter. Although these general models have achieved certain degree of spatial ability, spatial MLLM is still in its early stages. Spatial Reasoning. Mainstream spatial reasoning models can be categorized based on input modalities into image (Ma et al., 2025; Liu et al., 2025b), multi-image (Xu et al., 2025), multi-view (Li et al., 2025b), video (Wu et al., 2025a; Zheng et al., 2025; Ouyang et al., 2025; Zhang et al., 2025b; Ghazanfari et al., 2025), and simulation (Li et al., 2025a; Tang et al., 2025; Zhang et al., 2025c; Wang et al., 2025d; Zhang et al., 2025f). Among these categories, video stands out as the challenging task due to the difficulty of data acquisition and modeling. As the first work in spatial reasoning, VSI-Bench (Yang et al., 2025a) introduced video-based benchmark that removes linguistic shortcuts and evaluated MLLMs on spatial tasks such as counting, direction, and planning, highlighting substantial performance gaps compared to humans. InternSpatial (Deng et al., 2025b), SPAR (Zhang et al., 2025e), and SpaceR (Ouyang et al., 2025) enriched spatial supervision through extensive QA pairs spanning indoor and other limited settings. 3 Work in progress Figure 3: Fig.(a) shows our automated data construction pipeline. The pie charts (b-c) depict the composition of scenes and sources. The bar charts (de) show object sizes ranging mm-100m, while object-to-camera distances typically span 10-600m. Accordingly, we claim SpaceVista-1M basically covers the mm-km scale. The word clouds (f-g) provide glimpse of the scene diversity. Qi et al. (2025) used the bird-view map to aid overall understanding. Then, Spatial-MLLM (Wu et al., 2025a) and VG-LLM (Zheng et al., 2025) adopted geometry-aware dual encoders to capture geometry cues and inferred occluded structures from monocular inputs. Additionally, spatial reasoning on long (Zhang et al., 2025b), omni (Dongfang et al., 2025), ego-centric (Wu et al., 2025c) and aerial video (Zhang et al., 2025b) were also explored separately. However, the systematic data and model with all-scale video scenes remain unexplored. All-Scale Exploration. The challenge of multi-scale in early years lay in information loss within low-resolution image patches (Zhao, 2025; Nikouei et al., 2025), which has almost no effect on spatial reasoning. In this paper, all-scale primarily concerns the real scales of the physical world, including distances, semantics, and object states across different scales. Deng et al. (2025a) pushed the limits of 3D perception and reconstruction from meters to kilometers; Wen et al. (2025) extended metric depth estimation from close range to infinity; and Liu et al. (2025a) curated uncommon objects, ranging from screws to airplanes, with object-centric annotations. Together, these developments underscore the need for AI to move beyond simple single-scale memorization toward robust, multiscale, and reasonable visual understanding."
        },
        {
            "title": "3 DATASET",
            "content": "Due to high labeling cost, Tab.1 and Fig.2 show the clear drawback of the previous datasets. The limited data and performance constraints in existing models necessitate the creation of dataset with all-scale spatial context. We propose SpaceVista-1M, diverse, real-world, all-scale reasoning dataset, as the first to the best of our knowledge. SpaceVista-1M primarily comprises diverse spatial reasoning questionanswer pairs, with rich semantic (category, rationale), 2D (mask, box, point), and 3D (depth, camera parameters, point cloud) annotations, obtained either natively or through processing. The construction pipeline in Fig. 3 follows the step-by-step procedure of preparing, transforming, and generating to obtain an all-scale dataset by integrating specialized models. Data Preparation. We begin by selecting widely used video datasets that provide 3D scene modeling (Ling et al., 2024; Xia et al., 2024; Park et al., 2020; Liu et al., 2025a; Dai et al., 2017; Yeshwanth et al., 2023) along with camera intrinsic and extrinsic parameters. Most of these sources are videos of static scenes without moving objects. Leveraging the known camera parameters, we estimate depth maps and normal maps using specialized metric depth models (Hu et al., 2024; Piccinelli et al., 2025) and video depth models (Chen et al., 2025). For semantic understanding, we extract per-frame semantics and bounding boxes using proprietary grounding specialists (Ren et al., 2024; Liu et al., 2023b). To establish cross-frame object consistency, by further integrating SAM 2 (Ravi et al., 2024) with the previously mentioned grounding experts, we enable robust object ID association and mask 4 Work in progress generation. This pipeline ensures both semantic and spatial consistency across frames. Detailed preparation can be found in Appendix. B.3.1 Task Construction. With the help of official camera parameters and the preparations mentioned above, we can obtain the positions and dimensions of target objects. As common practice (Deng et al., 2025b), we adopt canonical view space of the reference frame, defined as 3D Cartesian coordinate system centered at the cameras optical center. We then design 19 tasks and their corresponding workflows, even including scale-specific tasks such as tabletop object manipulation and drone-view area estimation. Taking object counting as an example, which follows: detect objects, propagate masks across frames, track identities over time, filter out scenes with camera parameters and ambiguous objects, and derive temporally consistent counts. For each task, we obtain the data by similar carefully designed computational workflows. detailed description of each task and its workflow can be found in Appendix B.3. QA Construction. The pipeline for constructing the QA data is shown in Fig. 3. At the construction level of QA, we employ two strategies: GPTbased and template-based. For relatively fixed questions such as counting and object size, we adopt templatebased approach to obtain reasonable QA pairs. To ensure the diversity of the questions, we manually curate over 3,000 templates. However, for more flexible questions like planning, we use GPT-based (OpenAI, 2025a) method to generate reasonable answers in naturally language. Additionally, through appropriate randomizing and prompting, we obtain multiple options to serve as rewards for RL. QA previews can be found in Appendix F.3 Table 1: Comparison of popular spatial reasoning datasets. Only spatial reasoning QA is included. Lower QA/Scene Ratio usually means more diverse language and visual scenes. free,reg, and mc mean free-form, regression, and multiple-choice, respectively. SpaceVista-1M does not differentiate QA pairs by the type; i.e., the semantically similar questions with reg/mc/free answers are counted only once. Usage Dataset Train SpaceR SPAR-7M Spatial-MLLM InternSpatial SpaceVista-1M (Ours) Type reg/mc reg/mc/free reg/mc/free free free/reg/mc Benchmark TempCompass VideoMME All-Angles VSI-Bench MMSI-Bench SPAR-Bench STI-Bench SpaceVista-Bench (Ours) mc mc mc reg/mc mc reg/mc mc reg/mc QA Pairs Video Scenes QA/Scene Ratio 191K 7M 120K 2.5M 1M 7.5K 2.7K 2.1K 5.0K 1.0K 7.2K 2.0K 3K 1.2K 4.5K 1.5K 5.5K 38K 0.4K 0.9K 90 0.3K - - 0.3K 0.5K 159 1,556 83 455 25 18 3 23 17 - - 7 6 CoT Annotation. To facilitate an efficient cold start, we follow Feng et al. (2025) to leverage cognition-inspired few-shot prompting strategy with Qwen2.5-VL-72B-Instruct (Bai et al., 2025) to generate CoT rationales. After employing the filtering policy for low-quality or inconsistent rationale outputs, we obtain the CoT for SpaceVista-1M, with high-quality rationale for fundamental knowledge injection for SFT. Input Extension. Usually, people refer to objects in videos using more than just language. To support this, we extend video-based QA with extra annotations from the videos key frames. Besides plain visual input, we allow three extra inputs: point, bounding box, and mask, which may support future interactive usage. Each input type is designed to fit its own template and CoT rationales. Quality Control & Evaluation. To ensure data quality, we conduct manual verification on small portion training set for quality control. However, for measurement-related evaluation, human judgment is also susceptible to experiential bias. We choose more reliable pathway based on measuring and recording real-world data, retrieving authoritative sources, and performing human annotation for both distance and non-distance problems, shown in the green block Fig.4(a). For tiny and tabletop scenes, we capture and annotate videos of over 50 objects of different sizes. For some indoor and outdoor scenes, we search for the landmarks and retrieve statistics from authoritative sources like Wikipedia. As for other tasks like camera moving, the experts is hired for checking and annotating. By aligning the answer with the physical world, SpaceVista-Bench comprises more than 3,000 QA pairs with 99% accuracy across 500 unique video scenes. Please refer to the details and analysis in Appendix B.2.7. 5 Work in progress Figure 4: The left part (a-d) shows that the undifferentiated mixture of cross-scale knowledge hinders, rather than facilitates, the models reasoning process. The horizontal axis represents the scale discrepancy, defined as answer (=1 for the ideal situation), and the vertical axis denotes the proportion of answers. Fig.(e) is our SpaceVista model, where <think> is omitted for clarity. gt In summary, we propose SpaceVista-1M, an open-source, real-world, all-scale dataset with spatial video QA. SpaceVista-1M contains 1 million QA pairs spanning 19 tasks, 5 scale types, and over 50 subscene categories. Additionally, we encourage readers to consult the appendix, which presents meticulous source investigations (Sec. B.2), systematic processing procedures (Sec. B.3), in-depth distribution analyses (Sec. B.4), and also licensing (Sec. B.4.8)."
        },
        {
            "title": "4 METHOD\nOverview. Our objective is to enhance spatial reasoning by elaborately designing and conditioning\nthe model on explicit and detailed all-scale information. We first utilize a dense, expressive self-\nsupervised encoder beyond semantics to strengthen the model’s overall spatial perception.",
            "content": "However, mixing different types of knowledge without distinction hinders, rather than facilitates the models reasoning in Fig. 4(a-d), problem known as knowledge conflict. In all-scale reasoning, this conflict appears when similar visual patterns are interpreted differently at different scales. To mitigate such conflict, we propose LoRA-like scale expert architecture to maintain the independence of scale-level knowledge, while maintaining parameter efficiency, as shown in Fig 4(e). Finally, drawing on human reasoning about scale, we introduce reward-based progressive reasoning paths that employ essential anchors to constrain the reasoning process to reliable CoT path. Preliminaries. The number of frames is first denoted as with the temporal patch size τ . The visual representations from Qwen-2.5-VL visual encoder are denoted as FV RtdV HW , where = τ is temporal dimension of the feature, dV is the feature dimension per patch, and and are the numbers of patches along the height and width of each frame, respectively. Then, each dV of FV is directly converted to an image token as input. Beyond Semantics. Most open-sourced MLLM tokenizers including Qwen-2.5-VL visual encoder are pretrained on semantically rich textimage pairs via contrastive training, and thus often lack wellformed understanding of information beyond semantics. Meanwhile, El Banani et al. (2024); Tong et al. (2024) draw valuable conclusion that self-supervised vision models, such as DINO series, learn rich depth, normal, and pattern representations. Therefore, leveraging popular DINOv3 (Siméoni et al., 2025)s strong dense features seems to be natural approach beyond simple semantics. The last layer of DINOv3 produces patch-level dense features FD RT dDHDWD . We pad and regularize the original image to align with the patch size p, enforcing HD = and WD = . We then apply simple MLP, RdD RdV , to map channel dimensions. For the temporal dimension, we use the same temporal pooling with the previously mentioned temporal patch size τ to aggregate across , yielding RtdV HW . The fusion of the video feature FV and dense feature features is shown as: = CA(FV , D, D) + FV , (1) 6 Work in progress where CA(q, k, v) denotes multi-layer cross-attention over the query, key, and value inputs. Then, we convert , and the remaining calculations proceed as before. into fused image token Scale Experts Design. During all-scale mixed training in Fig.4(a-d), potential cross-scale knowledge conflicts lead to suboptimal results. This underscores the importance of preserving knowledge independence between scales during training. Inspired by Wu et al. (2024a); Buehler & Buehler (2024); Chen et al. (2024a), we further introduce LoRA-like module that adds scale experts by finetuning only 0.5% of the overall parameters for each expert. The original LoRA is using Rdr and Rrd with the rank min(d, k) to approximate orginal weights W0. To construct scale LoRA experts, We attach scale experts {(Ai, Bi)}M i=1 to mitigate potential scale-level knowledge interference. Each expert has base weight αi and is dynamically scaled by learned factor λi: = W0x + (cid:88) i= BiAix, where α α = αi λi, (2) where x, are the input and output of the projection layer, and α is the scaled factor. The learned factor λi is obtained through scale router-primarily an MLP and softmax. We apply scale experts to each layer of the foundation LLM. Therefore, different layers, according to their respective conditions, obtain appropriate λi to allocate the experts within the layer. Given that scenarios of scales can overlap (for example, an indoor scene may include some tabletop context), in the ideal case, the routers can select the suitable experts at different layers. Process Reward Design. After basic SFT training, RL is used to align the model with human perception. Inspired by how humans approach spatial observation tasks, we model the reasoning process explicitly. Humans typically proceed by: 1) identifying the task-specified semantics (if they help), 2) perceiving the global scale by inspecting surrounding objects (if it helps), and 3) inferring the answer from spatial relations. Following this paradigm, we construct 3 different anchors for RL that enforce the reasoning path to traverse the resulting anchor states. While certain reasoning anchors are not helpful to some tasks, we provide the minimal, sufficient ground-truth anchors for each question to guide the model in selecting the appropriate ones. We design the following three reward components based on these anchor formats: <semantics>, <scale>, and <answer>. Semantic reward Rsemantic is used to identify the referenced objects; Scale reward Rscale is used to estimate the scale of the overall scene; Correctness reward Ranswer is used to ensure the answer is well derived. The updated correctness reward Ranswer can be formed into Ranswer = 3 (cid:88) (cid:89) k=1 n= Rjn , with (j1, j2, j3) = (answer, scale, semantic), where Rscale = max(0, 1 log Cans log Cgt 2 ), Rsemantic = SansSgt SansSgt . (3) (4) Cans, Cgt is the estimated scene scale in the same measurement; Sans, Sgt is the calculated semantic embedding. Cgt and Sgt can be easily obtained from Sec.3. It is crucial to note that the order of (j1, ..., jn) matters; rewards at the beginning are stricter and more important. Also, because tasks differ, for example in the camera rotation task, Rsemantic and Rscale are not needed. Thus, Ranswer under such circumstances collapses to standard Ranswer. The calculation of format reward Rformat and answer reward Ranswer remains the same as common practice (Feng et al., 2025; Guo et al., 2025a) to encourage the generation of valid and executable answers. Therefore, our reward design forms the accurate reward signals to ensure all-scale spatial compliance and encourage human-like thinking. It is worth noting that the evaluation does not involve these anchors besides the actual answer. RL Training Objective. For each question i, we define the reward Ri to include both the updated correctness reward Ranswer and Rformat following Guo et al. (2025a), and use this overall reward Ri to compute groupwise normalized advantages Ai = Rimean({Rj }) . {Rj} is the response group related to Ri. The final policy πθ is updated by maximizing std({Rj }) J(θ) = Eq,{oi} (cid:34) 1 (cid:88) (cid:18) i=1 min (cid:18) πθ(oi q) πθold (oi q) Ai, clip (cid:18) πθ(oi q) πθold (oi q) (cid:19) (cid:19)(cid:19) , 1 ϵ, 1 + ϵ Ai β DKL(πθ πref ) , (5) (cid:35) where πθold and πθ are the old and new policy model respectively. DKL represents KL divergence. 7 Work in progress Table 2: Performance comparison across five spatial reasoning benchmarks. Among them, SpaceVistaBench is our proposed all-scale benchmark. Open-sourced general models are evaluated with comparable size. The highest performance of the open-sourced model is marked bold. Multi-Image Video MMSI-Bench SPAR-Bench VSI-Bench STI-Bench SpaceVista-Bench Model Human 97.2 67.3 79.2 - Closed-sourced Commercial Model & 70B-class model GPT-5(OpenAI, 2025) Gemini-2.5-pro(DeepMind, 2025) InternVL3.5-38B (Wang et al., 2025c) Qwen2.5-VL-72B (Bai et al., 2025) 40.7 36.9 36.9 30.7 37.4 36.3 31.0 32.4 Open-sourced General Model LLAVA-Onevision-7B (Li et al., 2024a) LLaVA-NeXT-Video-7B (Liu et al., 2024a) InternVL3.5-8B (Wang et al., 2025c) Qwen2.5-VL-7B (Bai et al., 2025) 24.5 26.8 30.9 31.7 30.6 31.3 36.0 33.1 Open-sourced Specialized Model SpaceR-7B (Ouyang et al., 2025) SpatialMLLM-4B (Wu et al., 2025a) VILASR-7B (Wu et al., 2025b) VG LLM-4B (Zheng et al., 2025) Qwen2.5-VL-7B w/. SpaceVista-1M SpaceVista-7B (Ours) SpaceVista-7B (Ours) w/. RL 26.1 27.0 30.2 - 27.3 29.1 32.3 37.6 31.5 37.6 - 36.9 38.1 41.6 44.2 45.0 66.3 30.7 32.4 35.6 38.2 32.7 46.9 48.4 45.4 46. 42.0 46.3 48.6 39.3 41.4 39.2 40.7 29.0 29.9 33.2 32.1 37.0 30.5 31.5 29.3 35.0 35.9 38.2 81. 33.7 33.8 30.7 31.1 13.6 23.7 24.5 28.9 21.2 24.2 23.6 28.8 29.5 34.5 36.7 Table 3: Module ablation study using Qwen-2.5VL-3B on SpaceVista. Table 4: Modality ablation study of the extra input types beyond semantic information. Module VSI-Bench SpaceVista-Bench Input VSI-Bench SpaceVista-Bench Vanilla w/. Scale w/. Scale &Semantic w/. Expert Finetuning 44.4 46.3 (+1.9) 46.8 (+2.4) 45.8 (+1.4) 31.0 34.8 (+3.8) 35.4 (+4.4) 34.8 (+3.8) Vanilla w/. VGGT w/. DINOv3 w/. VGGT + DINOv3 44.4 44.3 (-0.1) 46.4 (+2.0) 45.3 (+0.9) 31.0 31.4 (+0.4) 32.1 (+1.1) 31.7 (+0.7) Training Strategy. We start with cold-start phase on SpaceVista-1M, optimizing the input projection, feature-fusion modules, and scale experts. Next, we introduce the scale router to further train each scale-specific expert on the appropriate inputs, encouraging specialization. Finally, building on the SFT model, we apply RL training to obtain the final SpaceVista-7B reasoning model."
        },
        {
            "title": "5 EXPERIMENT\nDatasets. We use SpaceVista-1M in Sec. 3 for SFT and RL; its sources are detailed in Appendix B.2.\nModel Configurations. Our model is built\non Qwen2.5-VL-7B for main experiments and\nQwen2.5-VL-3B for ablation. Our model is\ntrained on up to 16 NVIDIA A800 (80GB)\nGPUs. We process a maximum of 32 frames\nduring training, each with a resolution of 128 ×\n28×28 pixels. During inference, we increase the\nresolution (256 × 28 × 28 pixels) to enhance per-\nformance. During the expert training phase, we\nemploy 4 experts, each tailored to a distinct sce-\nnario. We set the group size of GRPO to 8. We\nfirst perform SFT on CoT data of SpaceVista-\n1M for two epochs to obtain the SFT model.\nThis is followed by RL training for 2.5k steps\non multi-choice and regression data to produce\nthe final SpaceVista-7B. Additional details are\nprovided in Appendix C.1.",
            "content": "Benchmarks. We evaluate our model on 5 benchmarks, VSI-Bench (Yang et al., 2025a), STI-Bench (Li et al., 2025e), SpaceVista-Bench (Ours), MMSI-Bench (Yang et al., 2025b) and SPAR-Bench (Zhang et al., 2025d). Among the benchmarks, the former three are video-based, while the latter two are multi-image benchmarks. We argue that video and multi-image tasks share Figure 5: Visualization of scale-expert activations on salient tokens with an appropriate threshold. This shows the router selects experts based on the input. 8 Work in progress Table 5: The SpaceVista-Bench leaderborad. We utilize green (1st) , blue (2nd) , and yellow (3rd) backgrounds to distinguish the top three results within each scene. We employ bold and underlined text to denote the bests and second-best results across all open-source models. All the baselines are instruction-tuned and are evaluated on the same resolution and fps. Models Tiny Tabletop Tabletop Indoor Outdoor Overall SpaceVista-Bench Closed-sourced Commercial Model GPT-5(OpenAI, 2025) GPT-4o(Hurst et al., 2024) Gemini-2.5-pro(DeepMind, 2025) Gemini-2.5-flash(DeepMind, 2025) Claude-Sonnet-4(Anthropic, 2025b) Claude-Opus-4.1(Anthropic, 2025c) 32.3 21.7 33.0 20.7 27.3 21.7 20.3 13. 38.7 30.0 19.3 29.5 Open-Source General Model Internvl3.5-38B (Wang et al., 2025c) Internvl3.5-14B (Wang et al., 2025c) Internvl3-78B (Zhu et al., 2025) Internvl3-38B (Zhu et al., 2025) GLM-4.5V (Team et al., 2025) GLM-4.1V-Thinking (GLM et al., 2024) Qwen2.5VL-72B (Bai et al., 2025) Qwen2.5VL-32B (Bai et al., 2025) LLAVA-Onevision-72B (Li et al., 2024a) LLAVA-Onevision-7B (Li et al., 2024a) 29.3 27.7 38.3 18.7 23.0 30.7 27.7 25.3 25.0 17.5 25.2 22.3 23.3 14. 17.8 19.3 20.3 19.3 12.0 8.0 Open-Source Specialized Model SpaceR (Ouyang et al., 2025) Spatial-MLLM (Wu et al., 2025a) SpaceVista-7B (Ours) 12.9 17.3 33.4 17.3 20.3 37.1 39.0 34.3 34.5 19.9 38.1 24.3 41.2 31.3 42.2 34. 27.3 29.0 29.6 38.1 15.3 13.3 34.9 36.1 42.2 43.0 38.3 29.0 26. 34.1 30.0 27.0 24.3 30.3 38.0 25.2 13.3 28.0 30.7 11.7 11.6 19.8 23.1 34. 33.7 26.9 33.8 24.4 29.7 26.4 30.7 26.4 33.5 26.5 23.3 23.1 26.4 28. 16.0 12.6 21.2 24.2 36.7 rather strong similarities and collectively serve as important benchmarks for cross-frame spatial understanding. For all evaluations, we follow the configuration used in the official Qwen2.5-VL demo, with topp = 0.001 and temperature = 0.01. Comparison on Spatial Reasoning Datasets. Our method attains competitive performance across all spatial reasoning benchmarks in Tab. 2. On VSI-Bench, we achieve comparable results approaching the state of the art. More importantly, our approach delivers substantially superior performance in our all-scale benchmark SpaceVista-Bench, markedly exceeding 3% compared with proprietary and open-source models. Thus, SpaceVista-1M represents robust baseline for both indoor and all-scale scenes, where the full comparison table of each benchmark is shown in Appendix. D.4 for reference. Comparison on Subsets of SpaceVista-Bench. Our SpaceVista-7B, although exhibiting minor improvements on indoor scenes, attains comparatively high comprehensive scores across other scenarios and in overall evaluations. The results in Tab.5 indicate clear boost of around 6% compared with any size of the open-source models in comprehensive all-scale spatial reasoning. And SpaceVista-Bench also serves as an accurate benchmark for all-scale reasoning. Table 6: Ablation of the number of experts based on the same training settings. Num of Expert(s) (M ) Training Data (Each Expert) VSIBench SpaceVista -Bench (Ours) None 1 2 4 All All 1/2 1/4 44.4 44.2 (-0.2) 45.6 (+1.2) 45.7 (+1.3) 31.0 31.0 (0) 32.7 (+1.7) 32.9 (+1.9) Ablation on Each Component. 1) Scale Expert: We examine how potential information conflicts during cross-scale training are mitigated. As shown in Tab.3, the experts yield substantial gains. As the number of experts increases, the performance also improves accordingly in Tab. 6. Furthermore, visualizing the activation distributions of different LoRA experts across scenes  (Fig.5)  indicates that scale-specific knowledge is somehow disentangled. 2) Reward: In Tab. 3, the progressive reward achieves higher performance than the unconstrained reasoning path. These optional anchors indeed serve as valuable halfway point in the all-scale reasoning process. This highlights the importance of specifying thinking anchors when designing all-scale reasoning. Ablation on Each Modality. As shown in Tab. 4, incorporating DINO v3 yields greater gains than VGGT with its obvious advantage of self-supervised dense cues. In contrast, VGGTs raw geometry 9 Work in progress features are harder for simple fusion model to use without the strong decoder. Also, VGGT can be easily influenced by the blur or occlusion in the video. We further provide performance of the rendered 2.5D in Appendix. D.6 as interesting explorations. More Experiments. To facilitate deeper understanding, we provide more previews, statistics, experiments, user studies, and discussion in the appendix, especially Appendix D,E for more insights."
        },
        {
            "title": "6 DISCUSSION AND CONCLUSION\nDiscussion. It is believed that SpaceVista can facilitate widespread application in various areas on all\nscales, such as 1) spatial captioning, 2) spatial guided visual generation, 3) interactive world models.\nAlthough our all-scale model shows strong performance in various spatial reasoning tasks, there is\nstill potential for improvement, for example, µm level for precision manufacturing, mm-level for\nmedical surgery, km-level coverage for remote sensing, and 10km-scale for cartography.",
            "content": "Conclusion. In this work, we introduce novel task for all-scale reasoning from visual spatial context, which requires the machine to understand multimodal information and respond with the correct answer and rationale. To advance this field, we develop the first open-source, all-scale, spatial reasoning dataset, SpaceVista-1M, for cold start and reinforcement learning. Additionally, we handcraft SpaceVista-Bench, an accurate, multi-scale, video-based benchmark that strictly adheres to physical world measurements and perceptions. Our proposed SpaceVista-7B model further establishes robust baseline with enhanced cross-scale perception. During experiments, we compare our SpaceVista-7B model with several existing models and demonstrate our proposed models promising performance in all-scale reasoning. Additionally, our task and dataset have great potential in applications such as industrial manufacturing, embedded systems, and autonomous driving to understand complicated spatial environments in the wild. 10 Work in progress"
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/ claude-3-7-sonnet, 2025a. Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, 2025b. Accessed: 2025-07-24. Anthropic. Claude opus 4.1. https://www.anthropic.com/news/claude-opus-4-1, 2025c. Version VIII, released in 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. Eric Buehler and Markus Buehler. X-lora: Mixture of low-rank adapter experts, flexible framework for large language models with applications in protein mechanics and molecular design. APL Machine Learning, 2(2), 2024. Shaoxiang Chen, Zequn Jie, and Lin Ma. Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms. arXiv preprint arXiv:2401.16160, 2024a. Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2283122840, 2025. Yihang Chen, Qianyi Wu, Mengyao Li, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Fast feedforward 3d gaussian splatting compression. arXiv preprint arXiv:2410.08017, 2024b. Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024c. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024d. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024e. Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024. Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. Google Deepmind. era. tic google-gemini-ai-update-december-2024/#ceo-message, 2024. Gemini agenhttps://blog.google/technology/google-deepmind/ new ai model 2.0: our the for Google DeepMind. Gemini https://blog.google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025/, 2025. Our most 2.5: intelligent ai model. Work in progress Kai Deng, Zexin Ti, Jiawei Xu, Jian Yang, and Jin Xie. Vggt-long: Chunk it, loop it, align itpushing vggts limits on kilometer-scale long rgb sequences. arXiv preprint arXiv:2507.16443, 2025a. Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou, et al. Internspatial: comprehensive dataset for spatial reasoning in vision-language models. arXiv preprint arXiv:2506.18385, 2025b. Zihao Dongfang, Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Danda Pani Paudel, Luc Van Gool, Kailun Yang, and Xuming Hu. Are multimodal large language models ready for omnidirectional spatial reasoning? arXiv preprint arXiv:2505.11907, 2025. Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2179521806, 2024. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Sara Ghazanfari, Francesco Croce, Nicolas Flammarion, Prashanth Krishnamurthy, Farshad Khorrami, and Siddharth Garg. Chain-of-frames: Advancing video understanding in multimodal llms via frame-aware reasoning. arXiv preprint arXiv:2506.00318, 2025. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 24852494, 2020. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. Agrim Gupta, Piotr Dollár, and Ross B. Girshick. Lvis: dataset for large vocabulary instance segmentation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 53515359, 2019. Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2618126191, 2025. 12 Work in progress Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Steven LaValle. Rapidly-exploring random trees: new tool for path planning. Research Report 9811, 1998. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025a. Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, et al. Viewspatial-bench: Evaluating multiperspective spatial localization in vision-language models. arXiv preprint arXiv:2505.21500, 2025b. Hongyu Li, Songhao Han, Yue Liao, Junfeng Luo, Jialin Gao, Shuicheng Yan, and Si Liu. Reinforcement learning tuning for videollms: Reward design and data efficiency. arXiv preprint arXiv:2506.01908, 2025c. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024b. Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024c. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025d. Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are mllms ready for precise spatial-temporal world understanding? arXiv preprint arXiv:2503.23765, 2025e. Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, and Zhijie Deng. Improved visual-spatial reasoning via r1-zero-like training. arXiv preprint arXiv:2504.00883, 2025. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2216022169, 2024. Fei Liu, Zihao Lu, and Xianke Lin. Vision-based environmental perception for autonomous driving. Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering, 239:39 69, 2022. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. Work in progress Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023b. Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Y. Zhang, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, and David Novotny. Uncommon objects in 3d. In arXiv, 2025a. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv: 2403.00476, 2024b. Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, et al. Spatialcot: Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning. arXiv preprint arXiv:2501.10074, 2025b. Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. Nvila: Efficient frontier visual language models, 2024c. URL https://arxiv.org/abs/2412.04468. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025c. Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jianwen Xie, and Alan Yuille. Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning. arXiv preprint arXiv:2504.20024, 2025. Mahya Nikouei, Bita Baroutian, Shahabedin Nabavi, Fateme Taraghi, Atefe Aghaei, Ayoob Sajedi, and Mohsen Ebrahimi Moghaddam. Small object detection: comprehensive survey on challenges, techniques and real-world applications. ArXiv, abs/2503.20516, 2025. OpenAI. Introducing gpt-4.5. https://openai.com/index/introducing-gpt-4-5/, 2025a. OpenAI. Introducing o3 and o4 mini. introducing-o3-and-o4-mini/, 2025b. https://openai.com/index/ OpenAI. GPT-5 System Card. Technical report, OpenAI, August 2025. Accessed: 2025-08-10. Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, and Hao Dong. Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1735917369, 2025. Kiru Park, Timothy Patten, and Markus Vincze. Neural object learning for 6d pose estimation using few cluttered images. In The European Conference on Computer Vision (ECCV), 2020. Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. UniDepthV2: Universal monocular metric depth estimation made simpler, 2025. URL https://arxiv.org/abs/2502.20110. Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. Gpt4scene: Understand 3d scenes from videos with vision-language models. arXiv preprint arXiv:2501.01428, 2025. 14 Work in progress Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang, Enhui Wan, Sitong Zhou, Guanting Dong, Yuchen Zeng, Yida Xu, et al. We-math 2.0: versatile mathbook system for incentivizing visual mathematical reasoning. arXiv preprint arXiv:2508.10433, 2025. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. Johannes Lutz Schönberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European conference on computer vision, pp. 746760. Springer, 2012. Oriane Siméoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stanley T. Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. ArXiv, abs/2411.16537, 2024. Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, and Kai Chen. Lego-puzzles: How good are mllms at multi-step spatial reasoning? arXiv preprint arXiv:2503.19990, 2025. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024. 15 Work in progress Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2168621697, 2024a. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434, 2025b. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025c. Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. Spatial457: diagnostic benchmark for 6d spatial reasoning of large multimodal models, 2025d. URL https://arxiv.org/abs/2502.08636. Tao Wen, Jiepeng Wang, Yabo Chen, Shugong Xu, Chi Zhang, and Xuelong Li. Metric-solver: Sliding anchored metric depth estimation from single image. arXiv preprint arXiv:2504.12103, 2025. Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025a. Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing, 2025b. URL https://arxiv.org/abs/2506.09965. Peiran Wu, Yunze Liu, Miao Liu, and Junxiao Shen. St-think: How multimodal large language models reason about 4d worlds from ego-centric videos. arXiv preprint arXiv:2503.12542, 2025c. Xun Wu, Shaohan Huang, and Furu Wei. Mixture of lora experts. arXiv preprint arXiv:2404.13628, 2024a. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024b. URL https://arxiv. org/abs/2412.10302. Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Jiaping Xiao, Rangya Zhang, Yuhang Zhang, and Mir Feroskhan. Vision-based learning for drones: survey. IEEE transactions on neural networks and learning systems, PP, 2023. Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint arXiv:2505.17015, 2025. Tong Xu. Recent advances in rapidly-exploring random tree: review. Heliyon, 10(11):e32451, ISSN 2405-8440. doi: https://doi.org/10.1016/j.heliyon.2024.e32451. URL https: 2024. //www.sciencedirect.com/science/article/pii/S2405844024084822. Work in progress Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1063210643, 2025a. Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, and Jiangmiao Pang. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025b. Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025c. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Ruoyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, and Yi Ma. Seeing from another perspective: Evaluating multi-view understanding in mllms. arXiv preprint arXiv:2504.15280, 2025. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, and Deli Zhao. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025a. URL https://arxiv.org/abs/2501.13106. Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal tool-augmented reinforcement learning for long video reasoning. arXiv preprint arXiv:2508.04416, 2025b. Haoyu Zhang, Meng Liu, Zaijing Li, Haokun Wen, Weili Guan, Yaowei Wang, and Liqiang Nie. Spatial understanding from videos: Structured prompts meet simulation data. arXiv preprint arXiv:2506.03642, 2025c. Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, et al. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv preprint arXiv:2503.22976, 2025d. Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yujie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, and Li Zhang. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv preprint arXiv:2503.22976, 2025e. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024a. URL https://arxiv.org/abs/2406.16852. Wanyue Zhang, Yibin Huang, Yangbin Xu, JingJing Huang, Helu Zhi, Shuo Ren, Wang Xu, and Jiajun Zhang. Why do mllms struggle with spatial understanding? systematic analysis from data to architecture. arXiv preprint arXiv:2509.02359, 2025f. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024c. URL https://arxiv.org/abs/2410.02713. Zixuan Zhao. Advances and challenges in small object detection: comparative analysis of state-ofthe-art models and future directions. Theoretical and Natural Science, 79:145153, 01 2025. Work in progress Duo Zheng, Shijia Huang, Yanyang Li, and Liwei Wang. Learning from videos for 3d world: Enhancing mllms with 3d vision geometry priors. arXiv preprint arXiv:2505.24625, 2025. Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Xin Eric Wang, and Achuta Kadambi. Vlm4d: Towards spatiotemporal awareness in vision language models. arXiv preprint arXiv:2508.02095, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. Work in progress"
        },
        {
            "title": "APPENDICES CONTENTS",
            "content": "A Important Information A.1 Task Distribution . A.2 Performance Radar Data Construction B.1 Data Comparison . B.2 Data Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.1 Tiny Tabletop Scene . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.2 Tabletop Scene . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2. Indoor Scene . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.4 Wild Indoor Scene . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.5 Outdoor Scene . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.6 Drone Scene . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.7 Our Own Collected Data . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Task Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.2 Type: Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.3 Type: Counting . B.3.4 Type: Planning . B.3.5 Type: Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.6 Data Post-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.7 Benchmark Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Data Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4.1 Target Category Distribution . . . . . . . . . . . . . . . . . . . . . . . . . B.4.2 Scale Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4.3 Subscene Type Distribution . . . . . . . . . . . . . . . . . . . . . . . . . B.4.4 Object Size Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4.5 Camera To Object Distribution . . . . . . . . . . . . . . . . . . . . . . . . B.4.6 QA Statistics Across Scenes . . . . . . . . . . . . . . . . . . . . . . . . . B.4.7 Data Quality Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4.8 License . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Supplementary citation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Model Detail C.1 Parameter Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Patch Level Encoder Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 LoRA Like Expert Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 21 21 21 22 22 23 23 23 24 24 24 25 25 27 28 28 29 29 29 31 31 31 31 32 33 33 34 34 34 Work in progress Observation Results D.1 GRPO Reward Observation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Expert Observation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Reasoning Vs Memorizing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Detailed Analysis on Each Benchmark . . . . . . . . . . . . . . . . . . . . . . . . D.5 The Hardest Scene D.6 Why 2.5D>3D . . . D.7 Scaling-Up Analysis D.8 Leaderboard Detail . . . . . . . FAQ E.1 Error Accumulation . . E.2 All Scale Possibilities . E.3 Discussion of Dataset . Preview F.1 Scene Preview . . . F.2 Template Preview . F.3 QA Preview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 35 35 36 38 40 40 40 40 42 42 42 43 43 20 Work in progress"
        },
        {
            "title": "A IMPORTANT INFORMATION",
            "content": "A.1 TASK DISTRIBUTION Our SpaceVista-1M consists of wide range of tasks, including both general tasks and scale-specific tasks. Fig. A6 illustrates the data composition for each scene task, where bubble sizes indicate the relative data volume. Figure A6: Statistical chart of QA types. The spatial reasoning tasks for various scenes include abbreviations, for example, Est. for Estimation, Dist. for Distance, Loc. for Location, and Com. for Comparison. A.2 PERFORMANCE RADAR The comparison across models is carried out on multiple spatial reasoning benchmarks. We evaluate eight multimodal large models on five distinct benchmarks, with the results visualized in the radar chart in Fig. A7. SpaceVista-7B achieves significant improvement across the benchmarks, highlighting its superiority in spatial reasoning tasks. While models, including LLAVA-Onevision-7B (Li et al., 2024a), demonstrate competitive performance, SpaceVista-7B consistently exhibits superior robustness and adaptability across range of tasks, thereby solidifying its position as robust model in spatial reasoning."
        },
        {
            "title": "B DATA CONSTRUCTION",
            "content": "Our SpaceVista-1M dataset spans 19 spatial reasoning task types, including scale-specific tasks, comprising 1 million QA pairs and 38 thousand videos collected across diverse scenes. This scale and variety enable large-scale training of perceptual understanding and spatial reasoning, and support comparative analysis across tasks and environments. 21 Work in progress Figure A7: Comparison across popular spatial reasoning benchmarks. Our SpaceVista-7B model achieves certain performance boosts across all benchmarks. This chapter details the data sources for each scene category (Sec. B.2), the end-to-end task construction pipeline (Sec. B.3.1), and key dataset statistics (Sec. B.4). B.1 DATA COMPARISON Table B7: The datasets we used to build SpaceVista-1M and SpaceVista-Bench. means the datasets are only used for evaluation in SpaceVista-Bench. means data collected by us and used for accurate evaluation. The definition of scenes is the number of unique spaces, and one scene can be transformed into multiple questions. Dataset Type uCO3D(Liu et al., 2025a) WildRGB-D(Xia et al., 2024) SMOT(Park et al., 2020) SpaceR(Ouyang et al., 2025) Spar-Bench(Zhang et al., 2025e) Scannet Series(Dai et al., 2017; Yeshwanth et al., 2023) VSI-Bench(Yang et al., 2025a) MMSI-Bench(Yang et al., 2025b) DL3DV(Ling et al., 2024) STI-bench(Li et al., 2025e) Our own collected data Tiny, Tabletop Tabletop Tabletop Indoor Indoor Indoor Indoor Indoor Drone, Indoor, Outdoor Indoor, Outdoor, Tabletop Tiny, Tabletop, Outdoor Scenes 10,000 11,300 13 1,500 4,500 460 288 231 10,510 372 500 Our current dataset encompasses broad diversity of scene categories, as summarized in Tab. B7. The data sources span wide range of scenarios, including tiny, tabletop, indoor, outdoor, and drone-view. To ensure evaluation quality and robustness, we apply multiple rounds of processing and rigorous filtering to all collected data. We remove redundant or inconsistent samples across datasets. Because scenes may overlap across sources, which can compromise the independence of the training and test splits, we removed from the training set any scene that appears in all the benchmarks. This strict separation prevents leakage and enables fair assessment of generalization. Consequently, the SpaceVista-1M provides broad scene diversity, with clean, reliable benchmark SpaceVista-Bench. B.2 DATA SOURCE Sec. B.2 presents data sources that form our dataset, and systematically describes the provenance and acquisition of seven scene sources. These sources combine multiple public datasets and our own collected data, as detailed in Sec. B.2.1B.2.7. These scenes span object-centric through scene-level contexts and exhibit substantial variation in scale, shape, pattern, and illumination. When building the dataset, our foundational data construction process must adhere to the following key criteria: 22 Work in progress Video Data with 3D Modeling: The data must consist of video sequences accompanied by either official or third-party 3D modeling. This enables effective use of camera parameters for robust data processing. Multi-Frame & Multi-Scale: The dataset should support meaningful spatial reasoning across multiple frames and scales. Its complexity must be sufficient to prevent trivial single-frame assessments from representing the full sequence. Comprehensive Annotations & Metadata: Each sample must include the following: (a) camera intrinsics and extrinsics, (b) detection and segmentation labels, and (c) dense depth maps. These elements support broad range of downstream tasks. B.2.1 TINY TABLETOP SCENE We curate small-scale, small-object videos from uCO3D (Liu et al., 2025a), selecting sequences where the object size falls below predefined threshold to instantiate the tiny tabletop scenario. uCO3D comprises approximately 170,000 high-resolution, object-centric 360-degree videos captured via crowdsourcing, covering more than 1,000 LVIS (Gupta et al., 2019) categories grouped into 50 categories. For each video, uCO3D applies VGGSfM (Wang et al., 2024a) for motion analysis and 3D Gaussian Splatting to generate accurate camera poses, depth maps, sparse and dense point clouds, and semantic captions. The resulting subset contains everyday small objects, such as stationery, food, and decorative items, placed on flat surfaces such as tables, counters, and shelves. These scenes provide complete viewpoint coverage, precise geometry, and rich semantic labels, which make them well-suited for fine-grained 3D object modeling and spatial video reasoning. Here, we only select small part of uCO3D for around 10,000 videos for tiny objects after filtering. B.2.2 TABLETOP SCENE For tabletop scene modeling, we select two datasets: WildRGB-D (Xia et al., 2024) and SMOT (Park et al., 2020). WildRGB-D consists of approximately 8,500 objects across 46 categories, recorded in around 20,000 RGB-D videos, with iPhones rotating 360 degrees around objects to replicate real-world interactions. It includes single-object, multi-object, and hand-occlusion videos, all automatically annotated via SLAM-generated camera poses and reconstructed point clouds, making it suitable for spatial reasoning tasks. To select samples for spatial reasoning, we specifically choose around 10,000 videos with multiple objects in scene. SMOT (Park et al., 2020) is challenging small dataset collected by mobile robot, comprising 13 video sequences. The tabletop, commonly referred to as the table scene, encompasses not only the planar surface of table but also extends to various other surfaces, including sand, beds, wardrobes, floors, and similar environments. In combination, these datasets offer richly varied planar scenes, providing robust foundation for challenging spatial video reasoning benchmarks. B.2.3 INDOOR SCENE Indoor scenes are among the earliest domains studied in spatial video reasoning. Key datasets, including ScanNet (Dai et al., 2017) and ScanNet++ (Yeshwanth et al., 2023), collect RGB-D scans using handheld cameras, yielding aligned RGB images, depth maps, and 3D reconstructions. ScanNet contains more than 1,500 scenes and 2.5 million frames spanning common indoor spaces, such as offices and bedrooms, with annotations for over twenty object categories. ScanNet++ extends this setting with higher geometric fidelity and more complex layouts. The combination of focused object classes, structured environments, and rich annotations makes these datasets central benchmarks for spatial reasoning. B.2.4 WILD INDOOR SCENE Beyond scan-based indoor modeling, DL3DV (Ling et al., 2024) adopts video-based pipeline that replaces active scanning with video capture and camera parameter estimation. Building on this framework, and further compressed using 3D Gaussian Splatting (Chen et al., 2024b), DL3DV enables high-precision 3D reconstruction of wild indoor scenes. The dataset covers broad range of object categories, including challenging reflective and transparent instances. Compared with 23 Work in progress conventional scan-based datasets, these scenes exhibit greater geometric and appearance variability, providing more realistic and demanding benchmark for spatial video reasoning. B.2.5 OUTDOOR SCENE In addition to tabletop and indoor scene modeling, DL3DV (Ling et al., 2024) collects extensive inthe-wild outdoor videos encompassing landmarks, street corners, private courtyards, and urban parks. Camera parameters are calibrated using COLMAP (Schönberger et al., 2016; Schönberger & Frahm, 2016). The DL3DV-10K dataset includes 10,510 videos in 4K resolution, totaling about 51.2 million frames, covering 65 types of locations. Each video is annotated for whether it is indoors or outdoors as well as for levels of reflection, transparency, and lighting conditions. Compared to conventional scan-based indoor datasets, these outdoor scenes exhibit richer geometric complexity, greater diversity of materials, and wider environmental variation, offering more challenging benchmarks for spatial video reasoning. B.2.6 DRONE SCENE DL3DV (Ling et al., 2024) extends outdoor scene modeling by incorporating drone-captured videos that provide aerial perspectives to complement ground level views. Videos are recorded using unmanned aerial vehicles (UAVs), and camera parameters are calibrated through COLMAP (Schönberger et al., 2016; Schönberger & Frahm, 2016), following the same reconstruction pipeline applied to handheld footage. The DL3DV Drone subset consists of more than 100 videos covering variety of scenes, including open plazas, tree-lined pathways, rooftop platforms, and landmark facades. DL3DV enhances spatial video reasoning by introducing unique geometric structures and varied viewpoints. Although the data scale is not as large as tabletop or indoor, the drone-view scenes establish more rigorous benchmark for aerial mapping and spatial video reasoning by expanding scene diversity and viewpoint range. B.2.7 OUR OWN COLLECTED DATA The data collection methods described above rely on advanced specialized models and fully automated pipelines. While we incorporate limited manual filtering, whether the resulting data can be used as an accurate evaluation of real-world perception is still question. This limitation motivates our collection of higher-fidelity data to better align with physical world perception. Our dataset consists of two types: 1) measured, recorded, and manually annotated data, and 2) existing video data enhanced by retrieving and verifying publicly available information. The former is suitable for tiny objects, tabletop objects, whereas the latter is designed for indoor and outdoor scenarios. Figure B8: Our self-collected data features various categories of objects, with tabletops and tiny tabletops ranging from 0.4m to 3mm, even including transparent and reflective objects. Figure B9: photo of the real scene for the collection of tiny tabletop. Data from self-recording and measurement. Precise spatial annotations (e.g., location and dimensions) are scarce in existing datasets such as uCO3D and WildRGB-D. To address this, we captured length and positional data for nearly 50 object categories across diverse scenarios. Using GoPro 11, iPhone 15, and Vivo X70, we systematically varied object arrangements, distances, lighting conditions, and backgrounds into over 200 videos and 1,000 QA pairs. As illustrated in Fig. B8 and Fig. B9, they show the objects used for self-collected data and real scene of tiny tabletop data Work in progress collection. Although we collected the raw high-resolution videos up to 2.7K/60fps, it is still necessary to resize and resample it for better comparison. The resulting measurements are consolidated into unified perceptual space that closely approximates physical world geometry. Data Retrieved from authoritative sources. Adopting similar rationale, it is apparent that spatial information derived solely from wild videos lacks the precision required for robust evaluation. Consequently, alternative methodologies must be explored. To address this, we propose systematic approach that first identifies landmark objects within existing datasets and then manually retrieves images of these objects from authoritative sources, such as Wikipedia1, architectural drawings, and official design documents, to obtain accurate spatial information, as shown in Fig. B10. This method ensures that the evaluation data is not only more precise but also more consistent with human perceptual judgments and preferences. Figure B10: Examples of identifying outdoor landmark objects from existing datasets and retrieving their scale-related ground truth data. B.3 TASK CONSTRUCTION Upon acquiring the appropriate dataset, we initially perform necessary data preparation and processing in Sec. B.3.1. Subsequently, we carefully design workflow for each task (Sec. B.3.3-B.3.5), and we present detailed task explanations in Tab. B8. The final output consists of high-quality QA pairs, facilitating the cold-start and reinforcement learning processes of MLLMs. B.3.1 DATA PREPARATION Previous popular approaches, such as InternSpatial (Deng et al., 2025b), required estimating camera intrinsic and extrinsic parameters, which introduced cumulative errors that propagated through subsequent tasks. However, since we exclusively utilize datasets with known camera parameters (as detailed in Sec B.2), our framework operates under conditions close to ground truth. We first employ Metric3Dv2 (Hu et al., 2024) and UniDepthV2 (Piccinelli et al., 2025) to obtain accurate metric depth maps and normal maps. The metric depth maps provide precise distance measurements between the camera and scene objects, while the normal maps facilitate robust plane estimation. There are two challenges during construction. 1) Video consistency: According to observation, the metric depth model may not have that level of consistency across frames. So, we use Video-Depth-Anything (Chen et al., 2025) to ensure consistency by minimizing the energy function, = argmin 1https://www.wikipedia.org/ (cid:110) 2 + λ t(D) t(N )2 (cid:111) , (6) 25 Work in progress Table B8: Detailed explanation of 19 tasks included in SpaceVista-1M. Task Description Position Comparison Size Comparison Existence Estimation Object Counting Rotation Estimation Absolute Distance Object Size Route Planning Appearance Order Depth Estimation View Change Inference Object Matching Spatial Relation General Indoor Scenes Compare the positions of two objects within or across frames, assessing their spatial relationships in terms of left/right, above/below, and near/far. Compare the positions of two objects within or across frames, involving three pairs of size relationships: wider/thinner, taller/shorter, larger/smaller. Determine whether there are objects across frames whose positional/size relationships with the specified object meet the constraint conditions. Estimate how many objects meet the constraint conditions across frames. Estimate the rotation angle of an object across multiple frames. Estimate the closest distance between two objects within or across the frames. Estimate the longest dimension of an object within or across the frames. Choose what action should be performed between sequence of actions within or across the frames in order to route from start point to target. Given video, determine the -th appearance order of several objects. Estimate the relative or absolute distance of objects from the camera viewpoint in single image or across multiple images. Infer how the camera viewpoint has changed (position and orientation) across the video frames. Determine whether two objects in the beginning and end frames of video are the same physical object instance or different instances of the same object type. Analyze and describe the spatial relationships (e.g., support, hanging, adhesion, stacking, encircling, plug-in) between multiple objects or cameras across the frames. Every Type in General Room Size All task types from Indoor Scenes can be applied to drone-view perspectives. Estimate the volume of the room(s) across the frames. Indoor Scenes Every Type in General Navigation Every Type in General Route Plan Area Estimation Every Type in General Object Location Destination Location Obstacles Location Outdoor Scenes All task types from Indoor Scenes apply to Outdoor Scenes except for Room Size estimation. Determine the optimal path or movement strategy to navigate from one location to another across different views (similar to the Route Planning mentioned in Indoor Scenes). Drone-View Scenes All task types from Indoor Scenes can be applied to drone-view perspectives. Given series of aerial images, choose what action should be performed between sequence of actions in order to route from start point to target (similar to the Route Planning mentioned in Indoor Scenes). Estimate the size or area of regions or objects from an aerial perspective. Tabletop Scenes All task types from Indoor Scenes can be applied to drone-view perspectives. Determine the precise position of objects on table surface, typically corresponding to other objects. Identify target positions related to single objects (i.e. left, right, front ...) as part of manipulation planning. Identify and locate objects with the AABB box that may interfere with manipulation as part of manipulation planning. Manipulation Planning Determine the sequence of actions needed to rearrange objects or achieve specific configuration on the table. 26 Work in progress where ,N represent metric depth model maps and Video-Depth-Anything map . 2) Extreme Scale: Although the metric depth model is trained on the datasets as DDAD (Guizilini et al., 2020) and NYUv2 (Silberman et al., 2012), it may have certain level of adaptation to the extreme situations. For extreme situations, including drone-view and tiny objects, it is still necessary to provide prerequisite to adjust the depth normalization accordingly. For fine-grained semantic understanding at the pixel level, we leverage the advanced proprietary model DINO-X (Ren et al., 2024) to extract semantic information and bounding boxes for complex scenes, while relying on Grounding DINO (Liu et al., 2023b) for simpler samples. To address cross-frame consistency challenges in video data, we integrate the aforementioned grounding models with SAM2s (Ravi et al., 2024) advanced tracking capabilities, generating temporally consistent masks and unique object IDs across frames based on Grounded-SAM22. By this stage, we obtain comprehensive understanding of each frame, including bounding boxes, masks, categories, and object IDs, laying solid foundation for downstream task formulation. B.3.2 TYPE: DISTANCE The distance-related tasks, including object size, room size, object distance, and relative distance, rely on depth maps and computer vision techniques to measure object and spatial dimensions from monocular images. The method converts 2D depth keypoints into 3D point clouds using camera calibration parameters and applies Principal Component Analysis (PCA) to extract dimensional information, focusing on objects larger than 2020 pixels. For object size estimation, the system segments visible objects using instance masks and projects the masked depth values into 3D space. PCA determines the principal axes of the point cloud, with height measured along the vertical axis and width derived from the convex hull of points projected onto the dominant plane. Relative distances are calculated by comparing 3D centroids in world coordinates, and room dimensions are estimated by analyzing the spatial distribution of depth points and identifying major planar surfaces corresponding to walls. The method uses camera intrinsics and extrinsics to express all measurements in consistent world coordinate system, addressing the scale ambiguity of monocular systems. Multiple frames are processed to improve robustness, with temporal averaging reducing noise in the estimates. The technique assumes piecewise rigid scenes, operates on standard RGB images, and produces metricscale measurements. Accuracy depends on the quality of depth estimation and segmentation. Overall, it demonstrates how 2D computer vision pipelines can be extended to 3D measurement tasks through precise geometric reasoning. B.3.3 TYPE: COUNTING Figure B11: Automatic Processing Pipeline for Counting Task Scenes. Through data filtering, object tracking, and counting, the final counting video is obtained after data confirmation. Object counting across real-world scenes faces diverse visual conditions and high cost of manual labeling, which motivates an automatic pipeline that adapts to scene type. The automatic pipeline addresses object counting through two methodologies tailored to specific scenarios, and Fig. 2https://github.com/IDEA-Research/Grounded-SAM-2 27 Work in progress illustrates the workflow that maintains high accuracy while reducing manual effort across indoor, outdoor, and tabletop scenes. For outdoor video sequences, the open-vocabulary detection model (Ren et al., 2024; Cheng et al., 2024) uses text prompts with confidence threshold of 0.3 for zero-shot detection, projects 2D observations into 3D world coordinates to enforce spatial consistency, and tracks objects via motion prediction with confirmation after at least ten consistent detections. Given the difficulty of reliably detecting very small objects in outdoor scenes and to mitigate ID switching and trajectory fragmentation under severe occlusions, scenes are prefiltered to those containing 2 to 10 objects with minimum bounding-box size of 32 pixels. For tabletop scenarios, grounding model (Ren et al., 2024; Liu et al., 2023b) and SAM2 (Ravi et al., 2024) are employed, where open-vocabulary detection uses text and bounding box thresholds of 0.4, and mask propagation applies IoU and center distance thresholds of 0.4 and 32 pixels, respectively, to distinguish instances. Both methodologies output object categories and their corresponding counts for each video. B.3.4 TYPE: PLANNING Figure B12: Visualization of robotic manipulation planning. Fig.(a) visualizes the option for moving the red box to the left of the upper box. Fig.(b) represents the key frame to carry out the manipulation. In robotic manipulation tasks, effective route planning is essential to ensuring smooth and accurate object movement. The route planning pipeline proceeds as follows. First, depth information and object detection are utilized to identify the category, position, shape, and size of all objects within the image. Subsequently, an arbitrary object is selected as the manipulation source and another as the target position, with the objective being to relocate the source object to designated position (e.g., front, back, left, right, or above) relative to the target object. Based on this configuration, an LLM generates corresponding manipulation instructions, such as What is the correct route of placing the apple on the box. Next, the actual spatial positions of the objects are computed using both intrinsic and extrinsic camera parameters. The Rapidly-exploring Random Tree (RRT) (LaValle, 1998; Xu, 2024) algorithm is then employed to plan collision-free path, where the bounding boxes of objects serve as obstacle constraints during path computation. Finally, two types of data are generated from the planned path: 1) multiple paths are projected onto the camera plane, with the correct trajectory serving as the ground truth answer, and 2) the coordinate variations along the path are translated into natural language instructions via the LLM. For instance, when the x-coordinate of the object decreases while the y-coordinate remains constant in the camera space, the LLM produces the instruction move the object to the left. Fig. B12 demonstrates the visualization of robotic manipulation under the option, showing the planned movement of the red box to the left of the upper box. This figure highlights the spatial relationship and intended positioning within the manipulation task. B.3.5 TYPE: RELATION In spatial relation analysis, we combine semantic information with 3D positional data through an automatic reasoning process to ensure consistency in both semantic and spatial aspects. Our analysis operates primarily at the semantic level. We first identify and extract common candidate 28 Work in progress relations, such as support, attach, insert, and surround. Based on the consistent 3D keypoint semantics established earlier, we generate potential relation pairs that may exhibit these spatial relationships. These candidate pairs are then evaluated for spatial plausibility by integrating 3D positional data with the few-shot prompt through Chain-of-Thought (CoT) reasoning using the foundation model. Finally, the validated pairs are processed by GPT for transformation and answer generation, ensuring semantically and spatially consistent outputs. B.3.6 DATA POST-PROCESSING To address the cold-start challenge in SFT, we prioritize the acquisition of explicit thinking process rationalesstep-by-step explanations that clarify how answers are derived. For example, in object counting, the model is prompted to articulate intermediate reasoning (e.g., there are 2 cups on the table and 3 on the chair, totaling 5), enriching task understanding and facilitating more robust generalization. Following common practice (Feng et al., 2025), we acquire high-quality rationales by distilling from advanced open-source and proprietary large models. Specifically, we use Qwen2.5-VL-72B and Gemini-2.5-Pro for complex tasks, and Qwen2.5-VL-32B for simpler ones, balancing reasoning depth with efficiency. We then compare these generated rationales and their corresponding answers with previously collected cases. When GPT answers are different from the answers from previous workflows, we apply confidence-based filtering strategy to curate the training set, retaining only instances with consistent, well-supported reasoning. This pipeline generates cleaner, rationaleaugmented dataset, mitigating SFT cold-start effects and enhancing downstream performance. B.3.7 BENCHMARK CONSTRUCTION Our benchmark comprises two components: 1) Measurement-Related. For the scale-related portion requiring precise scale annotations, we collect approximately 500 videos across diverse scenes using the two methods described in Appendix B.2.7 and human annotation for other spatial tasks, covering tiny, tabletop, and outdoor settings. For the indoor evaluation set, we instead selected suitable data from ScanNet-based datasets (e.g., VSI-bench and SPAR-bench) and constructed series of scale-focused questions on top of these bases. 2) Non-Measurement. For the non-measurement questions, we manually annotate the data collected in the previous step to produce additional spatial reasoning QA pairs. In total, we curate 3,000 fully human-annotated QA pairs for model evaluation. B.4 DATA STATISTICS From visual perspective, our dataset comprises wild scenes spanning scales from millimeters to kilometers. Although the raw dataset contains over 100 million frames, we calculate unsupervised annotations as intermediate information at both the pixel and semantic levels for curated subset of 10 million frames. These frames vary in resolution from 480p to 2.7K, with frame rates ranging from 24 to 30 fps. During data processing, we preserve the original resolution whenever possible and apply uniform sampling during training as needed. In terms of the QA component, we employ combination of templated generation and GPT-based methods to produce 1 million QA pairs with theoretical duplication rate of only 0.0005%. These pairs are structured into diverse answer formats, including free-form, multiple-choice, and regressionbased responses, catering to different analytical needs. Rigorous quality control measures are implemented, with detailed analyses provided in Sec. B.4.7. In this section, we first conduct diversity analysis of the visual scenes, examining their composition, categories, and object size distributions (Sec. B.4.1-Sec. B.4.5). We then present statistical overview of the QA pairs, along with an evaluation of quality control mechanisms (Sec. F.3-Sec. B.4.7). And also, at the beginning of the appendix, Fig. A6 illustrates the data composition for each scene task, where bubble sizes indicate the relative data volume. B.4.1 TARGET CATEGORY DISTRIBUTION The introduction of diverse scenarios, such as tabletop, indoor, and outdoor, aims to establish more inclusive object composition system. Due to the limited drone data, we incorporate drone-view data Work in progress into the outdoor analysis. By approximating complex object distribution patterns to the real world, this approach enhances the scene adaptation capabilities of visual reasoning models. To quantitatively assess the impact of scene diversity on model generalization, we use the word cloud to compare object distribution characteristics across different scenarios, as shown in Figs. B13B18. The results reveal that indoor scenes are predominantly composed of rigid objects such as furniture and electronics, exhibiting highly structured spatial layout. In contrast, outdoor scenes feature more scale-varying objects like vehicles and natural landscapes, demonstrating spatial openness. Meanwhile, tabletop scenes focus on manipulable items such as tools and daily necessities, reflecting precise spatial arrangements. These cross-scene differences provide complementary training samples, effectively mitigating the risk of overfitting to specific scenarios. Thus, the necessity of multi-scenario strategy to enhance cross-domain generalization is validated. Overall, each subset scenario differs significantly from the previous indoor-dominated setting, highlighting the diversity of our scenes. Figure B13: The word cloud of the previous indoor spatial reasoning datasets. Figure B14: The word cloud of our indoor subset. Figure B15: The word cloud of our outdoor subset. Figure B16: The word cloud of our tabletop subset. Figure B17: The word cloud of our tiny tabletop subset. Figure B18: The word cloud of the self-collected subset. Note: We use standard ISO 7046 to denote the models of the screw, which looks like m4*10. 30 Work in progress B.4.2 SCALE DISTRIBUTION To evaluate the dynamic range of depth across different scenes, we statistically analyze the distributions of the maximum and minimum depth values in each scenario, with the results visualized in Fig. B19. This analysis reveals the variation in extreme depth ranges. Notably, the farthest depth point progressively decreases from the drone scene to the tiny tabletop scene, indicating consistent reduction in the overall scene. While we can see some extreme values for tiny object scenes, it might be the small object around the window, and extreme depth represents the outside of the window view. It is not unavoidable for data construction and will not affect the overall quality. Figure B19: The distribution of the maximum depth value of our dataset. The maximum distance denotes the farthest point observed. Figure B20: The distribution of the specific sceneries. Note: this chart is just for basic knowledge. Due to the latter filtering policy, there might be some vague or inaccurate analysis. B.4.3 SUBSCENE TYPE DISTRIBUTION While our dataset is largely derived from multiple existing sources, we perform thorough analysis of its scene type diversity. As shown in Fig. B20, the dataset covers broad range of real-world scenarios, enhancing its complexity and generalizability. To quantify this diversity, we utilize LLM for scene understanding, leveraging object-level annotations from the video data. However, certain subsets, such as partial tabletop scenes and most of the tiny tabletop data, are excluded from the analysis due to limited visual cues. As result, these statistics primarily illustrate the datasets variety rather than providing an exact distribution for downstream tasks. B.4.4 OBJECT SIZE DISTRIBUTION To enhance spatial understanding at design scales, we analyze the distribution of object sizes in the dataset. The results, shown in Fig. B21, reveal relatively uniform distribution for objects smaller than 50m, while those exceeding 100m exhibit certain tail distribution. This trend likely reflects real-world bias in object sizing, with high-rise buildings, common in urban environments, dominating the larger size categories. Consequently, the observed minor long-tail distribution aligns with real-world phenomena and is considered an acceptable characteristic of the dataset. B.4.5 CAMERA TO OBJECT DISTRIBUTION To examine biases regarding camera positioning relative to the subject, we analyze the distance (depth) between the camera and the primary object, with the statistical results shown in Fig. B22. The distribution of object-camera distances follows spindle-shaped pattern, with few instances where the object is positioned closer than 10 cm or farther than 500 from the camera. This trend is largely influenced by the focusing limitations of most hardware, like lenses, which exhibit reduced sensitivity to objects at extreme distances. Notably, this distribution mirrors that of conventional optical devices in real-world settings and should not be interpreted as dataset bias. 31 Work in progress Figure B21: The distribution of the size of the existing objects. Figure B22: The distribution of the distance between the target object and the camera. B.4.6 QA STATISTICS ACROSS SCENES We also provide the overall statistics of SpaceVista-1M dataset in Tab. B9. The SpaceVista-1M dataset consists of approximately 1 million QA pairs, covering wide range of tasks and scene types across all scales, from tiny tabletop objects to large-scale outdoor and drone-view scenarios, with scales ranging from 1 millimeter to 0.7 kilometers. Its diversity offers extensive challenges for model training and evaluation, enhancing the models adaptability and reasoning capabilities across different environments. Table B9: Statistics of QA Pairs for different tasks in SpaceVista-1M. Task Category Scale Distribution Total 1mm-0.7km 2mm - 5cm Tiny Tabletop Tabletop 5cm-2m Indoor 0.5m-20m 0.3m-50m Wild Indoor Outdoor Drone-View 0.5m-500m 10m-0.7km All Scenes 1,014K Position Comparison Size Comparison Existence Estimation Rotation Estimation Relative Distance Absolute Distance Object Counting Object Size Route Plan Appearance Order Depth Estimation View Change Inference Object Matching Spatial Relation Room Size Navigation Area Estimation 70.5K 88K 82K 85.5K 81K 99K 21.3K 157K 2.5K 27.3K 102K 51.7K 102K 19K 15.3K 0.8K 0.3K Obstacles Location Manipulation Planning 3K 6K 79K 15K 18K 15K 19K 6K 3K - 3K 242K 162.5K 213.3K 284.3K 33.1K General Scenes Tasks 22K 11K 26K 11K 33K - 15K 10K 8K 12K 10K 8K 25K 20K 24K 25K 1K 30K 4K 32K 27K 24K 6K Indoor Scenes Tasks 18K 30K 20K 22K 15K 13K 3.5K 38K 1K 3K 15K 4K 26K 4K 20K 40K 20K 25K 30K 34K 5.5K 34K 1K 4.5K 23K 6.5K 32K 8K 14.5K 0.8K Outdoor Scenes Tasks Drone-View Scenes Tasks Tabletop Scenes Tasks 3K 3K 0.8K 0.5K 10K 2K 0.5K 1K 1K 0.3K 7K 0.5K 0.8K 3K 0.2K 5K 1K 0.3K B.4.7 DATA QUALITY CONTROL During construction of our dataset, we distinguish between two notions of answer correctness: 1) strict correctness, which requires that an answer conform to objective physical reality, and 2) perceptual correctness, which requires that an answer align with typical human judgments. Since strict correctness is difficult to 1 for training data derived from in-the-wild videos (due to issues like missing calibration, occlusions, and limited metadata), we adopt the perceptual criterion. Specifically, during validation, we present annotators with both the question and candidate answer and ask them to judge its acceptability. Consequently, the reported accuracy should be interpreted as agreement with human perception rather than strict fidelity to physical-world quantities or metric scale. For these statistics and the user study, we use MTurk3 for these statistics and the user study. Dataset tasks and corresponding human checking accuracies are shown in Fig. B10. It is important that perceptual 3https://www.mturk.com/ Work in progress correctness is only used in training data quality control, while model evaluation still follows strict correctness. Table B10: Human checking accuracy over each task category. means we observe unusual variation for different annotators. Task Categories Task Accuracy Task Accuracy Task Accuracy Task Accuracy Position Comp. 95% Room Size 84% View Change 96% Manip. Plan 73% Size Comp. 84% Object Count 87% Object Match 93% Absolute Dist. 84% Existence Est. 94% Object Size 81% Spatial Rel. 95% Depth Est. 95% Rotation Est. 95% Route Plan 53% Navigation 63% Obstacles 67% Relative Dist. 82% Appear. Order 80% Area Est. 78% B.4.8 LICENSE We conduct systematic review of the open-source licenses for the datasets we use, with the results summarized in Tab. B11. The analysis indicates that CC BY 4.0 and Apache License 2.0 are the most widely adopted. After comprehensive consideration, our SpaceVista-1M dataset adopts the Creative Commons Attribution (CC BY) 4.0 or Apache License 2.0 for different sources of data, which is already used by most of the source data. Table B11: The licenses for the dataset and benchmark included in this paper. Dataset Type License Benchmarks VSI-Bench(Yang et al., 2025a) STI-bench(Li et al., 2025e) MMSI-Bench(Yang et al., 2025b) STI-Bench(Li et al., 2025e) Spar-Bench(Zhang et al., 2025e) SpaceVista-Bench (Ours) Indoor Indoor Indoor Outdoor, Tabletop Indoor Tiny, Tabletop, Indoor, Outdoor Apache License License 2.0 & CC BY 4. Apache License 2.0 Apache License 2.0 CC BY 4.0 Apache License 2.0 Apache License 2.0 Training Datasets uCO3D(Liu et al., 2025a) SMOT(Park et al., 2020) WildRGBD(Xia et al., 2024) SpaceR(Ouyang et al., 2025) Scannet Series(Yeshwanth et al., 2023) DL3DV(Ling et al., 2024) SpaceVista-1M (Ours) Tiny, Tabletop Tabletop Tabletop Indoor Indoor Indoor, Outdoor, Drone Tiny, Tabletop, Outdoor CC BY 4.0 Unknown None CC BY-NC 4.0 ScanNet Terms of Use DL3DV-10K Terms of Use Apache License License 2.0 & CC BY 4.0 B.5 SUPPLEMENTARY CITATION Due to the page limit, we have omitted some citations in Tab. 1. Here, we provide supplementary table of citations. Table B12: Supplementary citation of Tab. 1 Dataset Citation Ouyang et al. (2025) All-Angles Zhang et al. (2025e) MVBench VSI-Bench Dataset SpaceR SPAR-7M Spatial-MLLM Wu et al. (2025a) InternSpatial VideoMME TempCompass Deng et al. (2025b) MMSI-Bench SPAR-Bench Fu et al. (2024) STI-Bench Liu et al. (2024b) Citation Yeh et al. (2025) Li et al. (2024b) Yang et al. (2025a) Yang et al. (2025c) Zhang et al. (2025e) Li et al. (2025e) 33 Work in progress"
        },
        {
            "title": "C MODEL DETAIL",
            "content": "C.1 PARAMETER SETTING SFT. The model architecture is based on Qwen2.5-VL-7B-Instruct, 7-billion parameter visionlanguage model capable of processing both images (resized to 100,352 pixels) and videos (16,384 pixels at 16/32 frames). In the ablation study, we use the 3B model for efficiency. For fine-tuning, we employ selective freezing strategy: while the vision tower and multi-modal projector remain frozen to preserve pretrained visual representations, the language model is fully trainable. Training utilizes full parameter fine-tuning with DeepSpeed4 ZeRO-2 configuration for memory optimization. The model is trained on our proposed dataset for spatial understanding in indoor environments, with samples truncated at 32,768 tokens. We implement cosine learning rate schedule (initial LR=5e-7) with 10% warmup over 2 epochs. We maintain computational efficiency through mixed-precision bfloat16 training. RL. We conduct our experiments using the Qwen2.5-VL (Bai et al., 2025) on custom spatial dataset. The training utilizes 7 GPUs with DeepSpeed acceleration and mixed-precision bf16 training with flash attention. Key hyperparameters include batch size of 1 per device, gradient accumulation steps of 1, an initial learning rate of 1e-6 with cosine scheduling, and weight decay of 0.01. The model processes input sequences up to 16,384 tokens long while generating outputs up to 1,024 tokens. Training runs for 2 epochs with evaluation performed every 200 steps. For inference, we use vLLM on separate GPU with temperature 1.0 and generate 8 samples per input. Other Setting. We set the number of experts to 4 in most cases. We also add LoRA with the same default behavior as PEFT. Additionally, we apply expert scaling factors on layer-wise basis rather than globally. Ablation Setting. Unless otherwise noted, we conduct all ablation experiments using the Qwen2.5VL-3B model because of resource constraints; all other settings are identical to those described above. C.2 PATCH LEVEL ENCODER ABLATION We evaluate several visual encoders with dense feature or geometry-aware representations, including VGGT-1B (Deng et al., 2025a)(the only publicly available model) and the generalDINOv3 ViTBase, and perform ablations on the patch encoder. Tab. C13 reports the performance gains and computational costs associated with each model. Across encoders, DINOv3 achieves more favorable efficiencyaccuracy trade-offs with smaller parameter budget. We attribute this to its self-supervised pretraining, which is not constrained by labeled data and thus confers stronger generalization. In contrast, VGGT exhibits strong reconstruction capabilities but depends on annotations that lack rich semantic content and further relies on large decoder to recover geometry. Consequently, compared to VGGT, DINOv3 features are more readily consumed by the fusion module, facilitating more effective mapping. Table C13: Ablation of the patch-level encoder across different sizes of models on the indoor set VSI-Bench based on the same SFT training settings. Model&Parameter Video-Only +VGGT +DINO v3 +VGGT +DINO SpaceVista-3B (Ours) SpaceVista-3B (Ours) w/o. fusion module SpaceVista-7B (Ours) Extra Parameter 41.9 - 45.0 0 43.3 42.0 45.7 43.5 44.8 46. 43 .3 44.7 46.0 909M 303M 1,320M C.3 LORA LIKE EXPERT ABLATION On top of the same 3B pretrained base model, we compare three training strategies: 1) Fullparameter Fine-tuning, 2) Vanilla LoRA, and 3) LoRA-like Expert, with the results shown 4https://github.com/deepspeedai/DeepSpeed 34 Work in progress Table C14: Ablation of the LoRA-like expert in the SFT training stage. Model Benchmark w/. Full-parameter Fine-tuning w/. Vanilla LoRA Fine-tuning w/. LoRA-like Expert (model-wise) w/. LoRA-like Expert (layer-wise) SpaceVista-3B VSI-Bench SpaceVista-Bench Trainable Parameters 43.5 29.5 3B 42.9 29.4 20M 43.9 32. 45.3 33.0 80M+30M 80M+34M in Fig. C14. We observe that vanilla SFT-based fine-tuning still suffers from latent cross-scale information conflicts. The difference between model-wise and layer-wise is that, for each input, the router is calculated and implemented to the whole model or to separate layers, respectively. In contrast, the model-wise LoRA-like Expert yields clear gains over both full-parameter fine-tuning and vanilla LoRA. Furthermore, scaling to higher-capacity, layer-wise LoRA-like Expert delivers additional improvements."
        },
        {
            "title": "D OBSERVATION RESULTS",
            "content": "D.1 GRPO REWARD OBSERVATION During reinforcement learning training, we observe relatively stable increase in reward without evidence of reward hacking, as shown in Fig. D23. In most settings, the reward reliably converges within few thousand environment steps, after which further training yields minimal additional improvements. This suggests that the learning dynamics are well-behaved under our setup and that extending training beyond the convergence point offers limited marginal benefit. Figure D23: Visualization of GRPO updated and normalized correctness reward chart. This figure visualizes how the reward grows during the RL training stage. D.2 EXPERT OBSERVATION We select 10 samples from tiny and indoor scenes and visualize the expert scale distribution in Fig. D24. As shown, inputs from each scene type tend to activate the expert specialized for that scene. This demonstrates the models ability to distinguish scene-specific characteristics and allocate resources accordingly. By activating the most relevant expert, the model ensures efficient processing and enhanced performance in scene-specific tasks, highlighting its ability to focus on distinct features and patterns within each scene. D.3 REASONING VS MEMORIZING In our experiments, we observe that models often exhibit strong bias toward memorizing fixed sizes for certain objectsfor instance, chairs are typically assumed to be 50-70 cm tall. Consequently, the network tends to rely on memorized size priors rather than reasoning about object scale. However, this phenomenon presents dual nature. On one hand, human perception of size and scale also depends on reference objects and familiar benchmarks, which are essential for intuitive understanding. 35 Work in progress Figure D24: Visualization of the normalized scale of each expert with different selected samples. It reflects the models capacity to allocate resources according to the inherent properties of each scene. On the other hand, since real-world spatial relationships can vary significantly, such biases may lead to erroneous judgments in atypical cases. As the discussion, to systematically evaluate the impact of this bias and its potential implications for advancing the field, we design three specialized subsets at the same scale: Seen Set: Common object categories from the training distribution (i.e., bicycle, table, chair). Seen Set with Various Scales: Same objects of the same category (i.e., different sizes and shapes of screw). Unseen Set: Rare or culturally specific objects requiring contextual size reasoning (i.e., ethnic items with regional characteristics, such as traditional food). The Seen Set provides baseline performance metrics for familiar objects but may overlook biases due to training conformity. The Seen Set with scale variety directly probes size generalization for known categories, but it is limited to variations within seen objects. The Unseen Set evaluates robustness to novel, culturally diverse scenarios but risks introducing confounders beyond scale bias. Collectively, these subsets balance ecological validity with experimental control, offering comprehensive framework to diagnose size-related biases. This structured approach enables us to analyze how size biases manifest under different conditions, combining ecological validity with controlled experimentation. As shown in Fig. D15, all-scale training benefits the overall reasoning model; however, the general models still tend to memorize the regular size of the target object. Table D15: Reasoning VS memorizing analysis of different subsets. Model Seen Set (Normal) Seen Set (Various Scales) Unseen Set Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct SpaceVista-7B (Ours) 35.7 37.0 37.3 34.7 38.9 41.0 23.1 28.0 32.8 D.4 DETAILED ANALYSIS ON EACH BENCHMARK We conduct comprehensive evaluation of SpaceVista-7B across multiple benchmarks, including STI-Bench(Li et al., 2025e), SPAR-Bench(Zhang et al., 2025e), MMSI-Bench(Yang et al., 2025b) and VSI-Bench(Yang et al., 2025a). In this section, we analyze SpaceVista-7Bs performance on each benchmark and compare it to other state-of-the-art models. The results from these benchmarks provide thorough assessment of SpaceVista-7Bs spatial reasoning capabilities, highlighting its versatility and adaptability across diverse tasks. 36 Work in progress Table D16: Performance comparison of our SpaceVista-7B and other baselines on STI-Bench.We use bold and underlined text for the top two within open-source categories, while ranks are computed across all model categories. In Static Understanding, Dim. Meas. refers to Dimensional Measurement. In Dynamic Understanding, Disp. & P.L., Speed & Acc., Ego Orient., Traj. Desc., and Pose Est. represent Displacement and Path Length, Speed and Acceleration, Ego-Centric Orientation, Trajectory Description, and Pose Estimation, respectively. This table includes only the popular model for which the detailed scores are available. For average-score comparisons, see Table 2. Model/Method Rank Avg. Static Understanding Dynamic Understanding Dim. Meas. Spatial Relation 3D Video Grounding Disp. & P.L. Speed & Acc. Ego Orient. Traj. Desc. Pose Est. GPT-4o (Hurst et al., 2024) Gemini-2.0-Flash (Deepmind, 2024) Claude-3.7-Sonnet (Anthropic, 2025a) Gemini-2.5-Pro (DeepMind, 2025) VideoLLaMA3-7B (Zhang et al., 2025a) MiniCPM-V-2.6 (Yao et al., 2024) VideoChat-R1 (Li et al., 2025d) InternVL2.5-78B (Chen et al., 2024d) VideoChat-Flash (Li et al., 2024c) SpaceVista-7B (Ours) Closed-source Models 27.1 31.9 29.8 38.7 51.8 50.0 45.5 53.8 Open-source Models 29.4 27.7 23.2 29.9 33.6 33. 48.6 44.5 47.3 52.8 51.4 47.2 34.8 38.7 40.5 41.4 35.2 26.9 32.8 38.5 36.3 38.2 29.0 31.8 35.7 36. 36.1 29.0 31.5 31.6 33.1 37.6 8 3 2 1 7 10 9 4 6 5 23.2 27.7 28.9 33. 21.5 19.0 22.4 24.9 27.1 23.6 35.4 32.1 38.8 33.1 36.7 25.7 31.1 37.2 32.3 37.3 33.7 10.8 40.0 52. 23.2 7.0 26.0 49.2 22.2 39.6 32.0 38.5 47.4 47.4 54.6 30.8 47.9 43.6 54.2 43.1 53.6 61.3 62.6 50. 48.1 35.6 48.3 53.6 51.4 51.2 Table D17: Performance comparison of our SpaceVista-7B and other baselines on SPAR-Bench.We use bold and underlined text for the top two within open-source categories, while ranks are computed across all model categories. OO, OC, and MV refer to object-object, object-camera, and multi-view, respectively. This table includes only the popular model for which the detailed scores are available. For average-score comparisons, see Table 2. Model/Method R g. w M - - e - e - - e - e - - D - D - - D d c s o M C - D h Vi H - i - - i - - j O - j V - - j O - I M - - I O - I M - - I Baseline Chance Level (Random) Chance Level (Frequency) - 5 - - 25.09 23.82 22.02 31.25 25.27 22.16 25.81 24.42 24.17 26.89 22.65 24.50 32.74 31.19 43.09 43.51 17.38 13.05 41.90 30.99 27.40 32.17 38.25 29.01 26.75 59.00 32.29 52.94 50.60 28.25 26.92 26.59 26.34 26.74 26.49 25.77 - - - - - - - - - - SPAR-Bench(full) 12 28.06 21.74 18.06 24.81 23.20 20.97 19.47 19.95 26.83 20.61 22.83 39.69 23.00 5.81 35.42 51.18 55.95 46.00 31.59 23.82 36.02 34.30 17.55 22.41 InternVL2-2B (Chen et al., 2024e) 32.01 28.94 23.94 27.22 20.00 18.12 42.57 40.16 31.29 28.18 29.16 49.87 21.00 16.62 35.70 56.76 55.36 40.25 36.81 25.21 28.76 32.27 21.19 24.65 6 InternVL2-4B (Chen et al., 2024e) 33.02 26.83 25.75 30.88 20.76 20.78 39.03 36.19 19.15 22.19 36.49 63.36 28.00 18.11 37.37 64.71 54.46 42.75 37.36 26.32 34.14 31.10 20.86 24.65 4 InternVL2-8B (Chen et al., 2024e) 10 30.14 25.79 39.67 39.72 12.12 15.03 30.94 29.59 20.22 19.02 22.93 37.91 24.25 6.64 36.41 51.47 56.85 50.25 33.79 24.10 27.15 35.17 26.49 22.41 InternVL2.5-2B (Chen et al., 2024d) 30.55 25.66 29.06 32.97 21.77 16.83 20.84 26.85 28.13 28.79 29.75 47.07 33.25 8.92 35.16 54.12 58.93 35.50 29.67 34.63 24.73 31.39 19.21 28.29 9 InternVL2.5-4B (Chen et al., 2024d) 36.28 29.46 25.78 29.31 23.79 18.76 46.82 42.68 22.62 25.89 31.88 61.32 28.00 6.32 43.80 59.71 56.85 51.75 44.23 41.55 36.56 41.57 22.52 39.50 InternVL2.5-8B (Chen et al., 2024d) 2 LLaVA-Onevision-0.5B (Li et al., 2024a) 11 29.48 30.14 49.22 42.72 18.04 14.92 31.48 25.67 28.98 30.10 15.89 24.43 21.75 1.50 33.42 50.88 50.00 32.00 27.75 26.04 30.91 34.01 24.50 24.65 31.20 21.79 30.33 26.94 18.58 13.87 10.43 13.64 31.24 29.29 26.13 38.68 30.25 9.47 40.14 56.47 55.06 37.25 48.63 38.23 30.38 33.72 26.49 35.01 LLaVA-Onevision-7B (Li et al., 2024a) 7.07 17.82 11.14 27.55 26.21 25.25 31.20 28.22 54.12 49.11 21.75 25.27 12.47 23.92 27.62 24.83 14.85 Qwen2-VL-2B (Wang et al., 2024b) 30.74 27.52 35.97 35.22 20.83 12.88 28.68 29.95 28.21 28.45 20.44 35.37 20.25 5.69 37.03 59.71 52.38 30.25 38.46 41.00 22.04 28.49 22.52 38.38 Qwen2-VL-7B (Wang et al., 2024b) 33.07 28.75 31.33 33.66 21.99 14.97 42.88 37.73 23.83 23.64 22.97 33.33 28.75 6.83 40.27 58.24 51.49 44.75 50.00 32.13 33.87 32.85 27.15 31.93 Qwen2.5-VL-7b (Bai et al., 2025) 5.26 18.73 9.12 26.50 24.43 26.75 28.31 34.09 51.18 52.38 34.25 24.18 26.87 34.68 29.94 22.52 30.81 LLaVA-v1.5-7b (Liu et al., 2023a) 6.25 32.14 6.37 39.52 10.47 21.52 5.88 LLaVA-v1.6-7b (Liu et al., 2023a) 7 13 24.60 19.43 38.03 40.63 18.84 14.09 7.81 8 3 14 23.65 10.85 5.17 12.53 17.37 11.34 7.25 15 13.21 8.53 12.14 0.00 20.35 0.27 10.76 0.41 24.27 0. 0.00 20.18 51.76 7.74 4.79 7.75 6.62 SpaceVista-7B (Ours) 41.68 42.51 57.78 51.94 24.44 20.22 57.02 51.12 42.62 34.98 36.02 31.04 41.00 0.00 46.82 66.76 63.10 56.5 50.00 41.55 37.10 37.21 27.15 42.02 On STI-Bench, SpaceVista-7B ranks fifth, demonstrating strong performance in tasks such as 3D video grounding, speed, and acceleration, with 37.6% in 3D video grounding and 37.3% in speed tasks, surpassing other models. Among the evaluated models, Gemini-2.5-Pro(DeepMind, 2025) achieves the highest performance with 41.4%, followed closely by Claude-3.7-Sonnet(Anthropic, 2025a). SpaceVista-7B shows competitive performance in these tasks, achieving results close to the proprietary model, reflecting its adaptability. The evaluation results are presented in Tab. D16. The SPAR-Bench evaluation further highlights SpaceVista-7Bs strengths, particularly in tasks requiring complex spatial reasoning, where it reaches 41.0% in camera motion and 57.0% in objectcamera diatance estimation tasks, as summarized in Table D17. In the MMSI-Bench evaluation, SpaceVista-7B demonstrates exceptional performance in multi-image camera-object spatial reasoning, achieving 45.3%, showcasing strong capabilities in multi-step reasoning and positional relationship understanding, significantly outperforming advanced models such as Gemini-2.5-Pro and Claude-3.7Sonnet, as presented in Table D18. Finally, in the VSI-Bench evaluation, SpaceVista-7B outperforms all other models, excelling in object counting, appearance sequencing, and absolute distance tasks, achieving 62.9% in object counting and 36.0% in absolute distance, surpassing several open-source 37 Work in progress models, including LLaVA-Video-72B(Zhang et al., 2024c) and LLaVA-OneVision-72B(Li et al., 2024a). The results of this evaluation are shown in Tab. D19. Table D18: Performance Comparison of our SpaceVista-7B and other baselines on MMSI-Bench.We use bold and underlined text for the top two within open-source categories, while ranks are computed across all model categories. Cam., Obj., Reg., Meas., and Appr. denote Camera, Object, Region, Measurement, and Appearance, respectively. This table includes only the popular model for which the detailed scores are available. For average-score comparisons, see Table 2. Model/Method Rank Avg. Positional Relationship Attribute Motion MSR Cam.-Cam. Obj.-Obj. Reg.-Reg. Cam.-Obj. Obj.-Reg. Cam.-Reg. Meas. Appr. Cam. Obj. Blind GPT-4o Random Guessing Human Level o3 (OpenAI, 2025b) GPT-4.5 (OpenAI, 2025a) GPT-4o (Hurst et al., 2024) Gemini-2.5-Pro (DeepMind, 2025) Claude-3.7-Sonnet (Anthropic, 2025a) Seed1.5-VL (Guo et al., 2025b) InternVL3-78B (Zhu et al., 2025) InternVL2.5-78B (Chen et al., 2024d) Qwen2.5-VL-72B (Bai et al., 2025) LLaVA-OneVision-72B (Li et al., 2024a) InternVL3-38B (Zhu et al., 2025) InternVL2.5-38B (Chen et al., 2024d) Qwen2.5-VL-32B (Bai et al., 2025) InternVL2.5-26B (Chen et al., 2024d) NVILA-15B (Liu et al., 2024c) InternVL3-14B (Zhu et al., 2025) Llama-3.2-11B-Vision (Grattafiori et al., 2024) InternVL3-9B (Zhu et al., 2025) InternVL3-8B (Zhu et al., 2025) InternVL2.5-8B (Chen et al., 2024d) NVILA-8B (Liu et al., 2024c) Qwen2.5-VL-7B (Bai et al., 2025) LLaVA-OneVision-7B (Li et al., 2024a) InternVL2.5-4B (Chen et al., 2024d) Qwen2.5-VL-3B (Bai et al., 2025) InternVL3-2B (Zhu et al., 2025) InternVL2.5-2B (Chen et al., 2024d) InternVL3-1B (Zhu et al., 2025) InternVL2.5-1B (Chen et al., 2024d) DeepSeek-VL2 (Wu et al., 2024b) DeepSeek-VL2-Small (Wu et al., 2024b) DeepSeek-VL2-Tiny (Wu et al., 2024b) SpaceVista-7B (Ours) 32 29 2 3 7 4 10 8 12 12 5 13 23 16 17 15 6 20 27 21 26 10 14 25 30 23 22 28 9 19 24 18 11 31 5 22.7 25.0 97.2 41.0 40.3 30.3 36.9 28.7 29.7 28.5 28.5 30.7 28.4 26.3 27.9 27.7 28.0 30.5 26.8 25.4 26.7 25.7 28.7 28.1 25.9 24.5 26.3 26.5 25.3 29.0 27.0 26.1 27.1 28.6 24. 30.7 Baseline 17.0 25.0 98.9 29.6 25.0 97.5 Closed-source Models 39.4 29.8 24.5 31.9 26.6 30. 37.0 39.5 23.5 39.5 22.2 25.9 Open-source Models 23.4 22.3 34.0 31.9 20.2 22.3 26.6 19.1 39.4 24.5 30.8 25.5 31.9 27.7 29.8 24.5 33.0 23.4 27.7 25.5 27.7 35.1 26.6 31.9 28.7 27.7 23.2 32.1 39.5 34.6 33.3 33.3 35.8 29.6 29.6 28.4 24.7 32.0 32.1 37.0 29.6 24.7 24.7 29.6 21.0 30.9 29.6 24.7 22.2 24.7 22.2 18.5 21.0 30. 20.2 25.0 95.7 45.2 34.4 34.4 39.7 32.3 32.2 34.4 23.7 25.8 43.0 21.5 18.3 24.7 24.7 30.1 19.4 25.8 18.3 25.8 32.3 17.2 24.7 20.4 31.2 26.9 26.9 28.0 24.7 23.7 23.7 24.7 29.0 26.9 13.9 25.0 94.2 44.2 51.2 19.8 45.3 34.9 23. 12.8 29.1 23.3 30.2 23.3 22.1 22.1 33.7 36.0 23.3 25.6 29.1 25.6 32.6 30.2 25.6 29.1 31.4 29.1 31.4 37.2 30.2 25.6 36.0 33.7 23.3 45.3 29.4 25.0 98.8 47.1 47.1 37.6 35.2 37.6 38.8 37.6 31.8 34.1 37.6 35.3 38.8 32.9 31.8 38.8 37.6 21.2 31.8 35.3 24.7 22.4 29.4 25.9 34.1 28.2 28.2 29.4 29.4 31.8 30.6 38.8 17.6 27. 19.2 25.0 96.4 62.6 55.4 27.7 43.3 42.2 32.5 26.5 42.2 36.1 38.6 25.3 34.9 31.3 37.3 20.5 24.1 25.9 22.9 28.9 32.5 34.9 26.5 30.1 25.3 34.9 27.7 36.1 30.1 25.3 22.9 27.7 31.3 36.1 21.8 25.0 95.3 12.1 25.0 98. 20.2 29.0 20.2 25.0 25.0 25.0 98.6 98.7 97.0 54.7 39.1 32.8 51.5 25.0 39.0 37.5 35.9 45.3 28.1 39.1 37.5 31.2 35.9 29.7 31.2 20.3 29.7 23.4 26.6 34.4 25.0 29.7 23.4 31.2 26.6 43.8 32.8 31.2 28.1 28.1 14.1 28.8 33.3 31.8 21.2 22.7 21.2 19.7 19.7 27.3 19.7 21.2 25.8 24.2 30.3 31.8 22.7 19.7 24.2 24.2 27.3 25.8 18.2 25.8 24.2 16.7 22.7 15.2 28.8 30.3 15.2 33.3 24.2 31.1 32.9 34.9 41.9 40.8 36.4 35.1 36.8 30.8 36.4 30.2 34.3 21.6 32.9 22.7 36.4 25.0 26. 28.4 31.6 29.3 17.6 26.3 27.3 27.0 30.3 27.3 13.5 32.9 15.7 16.2 31.6 25.8 14.9 38.2 25.3 18.9 35.5 27.8 10.8 31.6 26.8 18.9 35.5 27.8 24.3 31.6 29.3 25.6 28.9 19.2 16.2 38.2 26.8 16.2 32.9 14.6 16.2 31.6 30.3 25.7 34.2 29.8 20.3 39.5 25.8 18.9 34.2 11.6 13.5 31.6 36.8 17.6 27.6 23.2 12.2 23.7 23.7 21.6 31.6 26.8 17.6 19.7 26.3 17.6 25.0 26.3 28.4 26.3 28.3 24.3 25.0 29.8 14.9 25.0 27.3 34.4 26.7 23.3 35.5 25.8 D.5 THE HARDEST SCENE When testing scenes at varying scales, several critical questions arise: Which scenarios pose greater challenges, and to what extent is data complexity the primary bottleneck? To systematically investigate these issues, we design controlled observational experiment. We identify tasks that exhibit consistent properties across different scales, including object size, object comparison, absolute and relative distance, and depth estimation. For fairness in comparison, we train models using videos from diverse scenes while maintaining similar quantities of QA pairs and video samples. Under these controlled conditions, we evaluate and compared performance across different scale-dependent scenarios. In Tab.D20, it seems indoor data is the easiest task. We hypothesize that human-scale estimation biasarising because both humans and GPT focus on objects expressible in basic units like meters in pretraining corporaleads to this preference. 38 Work in progress Table D19: Performance comparison of our SpaceVista-7B and other baselines on VSI-Bench.We use bold and underlined text for the top two within open-source categories, while ranks are computed across all model categories. This table includes only the popular model for which the detailed scores are available. For average-score comparisons, see Table 2. Model / Method Rank Avg. Obj Appear ance Order Object Abs Distance Object Counting Object Rel Distance Object Size Estimation Room Size Estimation Route Planning Object Rel Direction GPT-4o(Hurst et al., 2024) Gemini-1.5 Flash (API)(Team et al., 2024) Gemini-1.5 Pro (API)Team et al. (2024) InternVL2-2B(Chen et al., 2024e) InternVL2-8B(Chen et al., 2024e) InternVL2-40B(Chen et al., 2024e) LongVILA-8B(Chen et al., 2024c) VILA-1.5-8B(Lin et al., 2023) VILA-1.5-40B(Lin et al., 2023) LongVA-7B(Zhang et al., 2024a) LLaVA-Video-7B(Zhang et al., 2024c) LLaVA-Video-72B(Zhang et al., 2024c) LLaVA-NeXT-Video-7B(Zhang et al., 2024b) LLaVA-NeXT-Video-72B(Zhang et al., 2024b) LLaVA-OneVision-0.5B(Li et al., 2024a) LLaVA-OneVision-7B(Li et al., 2024a) LLaVA-OneVision-72B(Li et al., 2024a) Qwen2.5-VL-7B (Bai et al., 2025) SpaceVista-7B (Ours) 10 3 16 6 7 17 14 12 13 8 4 8 4 15 11 5 9 1 34.0 42.1 45.4 26.5 37.5 37.0 21.6 28.9 31.2 29.2 35.6 40.9 35.6 40.9 28.0 32.4 40.2 34.4 48.6 Proprietary Models(API) 28.5 37.8 34.6 5.3 30.8 30.9 Open-source Models 6.3 46.4 44.7 25.5 24.8 32.9 15.7 30.6 48.6 30.6 48.6 5.8 24.4 44.6 32.7 56.3 24.0 29.0 26.2 9.1 21.8 24.8 16.6 14.0 22.8 14.0 22.8 28.4 20.2 23.9 17. 36.0 46.2 49.8 56.2 25.7 31.3 41.3 29.1 17.4 22.4 38.0 48.5 48.9 48.5 48.9 46.1 47.7 43.5 34.0 62.9 37.0 37.7 51.3 32.1 38.0 47.6 29.6 32.1 40.5 33.1 43.5 42.4 43.5 42.4 28.3 42.5 42.5 35. 44.2 43.8 53.5 64.1 20.0 48.9 48.2 16.7 50.3 48.7 38.9 47.8 57.4 47.8 57.4 15.4 47.4 57.6 51.9 58.1 38.2 54.4 43.6 29.2 44.2 27.5 0.0 18.8 22.7 22.2 24.2 35.3 24.2 35.3 28.3 12.3 37.5 36. 42.0 31.5 31.5 36.0 30.4 28.9 27.8 32.5 31.0 31.5 25.4 34.0 35.0 34.0 35.0 34.5 29.4 32.5 29.4 38.9 41.3 41.0 46.3 44.1 33.4 32.7 30.7 34.8 25.7 43.3 42.4 36.7 42.4 36.7 36.9 35.2 39.9 37. 49.7 Table D20: Results analysis of different scenes. The model mentioned below is trained in balanced subset of SpaceVista-1M for better control of experiment conditions. Model SpaceVista-Bench (Ours) Indoor Outdoor Tabletop Tabletop Qwen2.5-VL-7B 30.34 w/. balance training 38. 18.31 24.90 23.79 30.17 19.37 20.86 D.6 WHY 2.5D>3D In addition to introducing VGGT(Wang et al., 2025a) and DINO v3(Siméoni et al., 2025) as extra signals, we conduct series of targeted ablation studies. This suggests that representation formats like VGGT, when used in their native encoder output, are wonderful for capturing geometry information, but suboptimal for capturing semantic information or overall scenes, especially for low resolution and uncommon scenarios. In Tab.D21, we use 3D to denote the pure geometric features from VGGT, Table D21: Comparison of the robustness of the model training of 3D and 2.5D. All the models are trained on 3D or 2.5D data along with the video. However, we vary the evaluation input of these models to see the robustness. denotes experiments we consider unnecessary. low means using low resolution visual for 3D reconstruction. This table includes only the popular model for which detailed score is available. For average-score comparisons, see Table 2. (n%) means the relative decrease compared to the original input. Settings Eval Input VSI-bench SpaceVista-Bench Training with w/. 3D Training with w/. 2.5D visual w/. 3D visual w/. 3D (low) visual w/o. 3D 44.3 38.1 (-14%) 34.0 (-23%) 31.4 visual w/. 2.5D visual w/. 2.5D (low) 43.9 (-4%) visual w/o. 2.5D 40.7 (-10%) 45.6 33.0 32.3 (-2%) 29.1(-12%) Work in progress and 2.5D to denote the additional 12 viewing angles of the overall scene rendered by the decoder and the renderer. We use the special prompt and the image token to provide As shown in Tab.D21, 2.5D is usually more robust in spatial reasoning. Rendering to 2.5D enables effective exploitation of pretrained image tokenizers, which in turn provides more reliable semantic information. Below is the special prompt for 2.5D finetuning. Please think about this question as if you were human pondering deeply. Consider detailed information from the video frames and coarse spatial information from the 3D point cloud image. Provide the models thought process and reasoning between the <think> </think> tags, and give your final answer between the <answer> </answer> tags. <video> The images below are obtained from the 3D point clouds based on the video frames above. The following point cloud images are randomly selected viewpoints; some may be completely unhelpful, while others may contain important information. Please discern carefully. <image> Provide your reasoning between the <think> </think> tags and your final answer between the <answer> </answer> tags. D.7 SCALING-UP ANALYSIS We investigate prospective scaling behavior across three model sizes3B, 7B, and 32Bto inform future model development. Our analysis is conducted using the same SpaceVista-1M dataset while holding all model settings nearly constant. However, there is minor difference between different scale models. We use LoRA rather than full scale to finetune 32B model. Since using more experts will inevitably increase the inference time, we use fewer experts as the scale increases. However, we still hold the strong belief that it does not affect the overall scaling exploration of our SpaceVista-1M data and model. Table D22: Scaling model with SpaceVista-1M. Qwen2.5-VL-*B indicates that the SFT model used for evaluation is trained on the corresponding base model. Foundation Model Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-32B VSI-Bench SpaceVista-Bench 49.0 36.3 43.5 29.5 46.3 34. As summarized in Tab. D22, the dataset affords certain degree of support for the 32B models capabilities. Nevertheless, beyond this observation, the main results are achieved by the 7B configuration, whereas ablation studies are primarily conducted with the 3B model. D.8 LEADERBOARD DETAIL To assess the spatial reasoning ability of both closed-source and open-source models, we evaluate the latest available versions. Tab. 5 presents their performance across the Tiny Tabletop, Tabletop, Indoor, and Outdoor scenarios, whereas Tab. D23 provides an overview of their release dates and sources. For closed-source models accessed via API and open-source models, the generation configurations are summarized in Tab. D24 and D25, respectively."
        },
        {
            "title": "E FAQ",
            "content": "E.1 ERROR ACCUMULATION Our data construction pipeline is primarily based on metric depth estimation and the corresponding transformation to canonical view space. It should be noted that this approach may introduce potential error accumulation, especially considering that current metric depth estimation models have not yet achieved high performance at full scale. To address concerns regarding error accumulation, we justify our methodology from the following perspectives: 1) data quality assurance: To ensure alignment with human perception, we implement multi-tiered validation process. Specifically, we conduct manual verification on subset of the training set, perform full human annotation on the entire test set, and additionally collect real-world 40 Work in progress Table D23: The release time and model source of LLMs used Model Release Time Source GPT-5(OpenAI, 2025) GPT-4o(Hurst et al., 2024) Claude-Opus-4.1(Anthropic, 2025c) Claude-Sonnet-4(Anthropic, 2025b) Gemini-2.5-Pro(DeepMind, 2025) Gemini-2.5-Flash(DeepMind, 2025) Internvl3.5-38B (Wang et al., 2025c) Internvl3.5-14B (Wang et al., 2025c) Internvl3-78B (Zhu et al., 2025) Internvl3-38B (Zhu et al., 2025) GLM-4.5V (Team et al., 2025) 2025-08 2024-05 2025-08 20252025-06 2025-06 2025-08 2025-08 2025-04 20252025-08 https://openai.com/gpt-5/ https://gpt4o.ai/ https://www.anthropic.com/news/claude-opus-4-1 https://www.anthropic.com/claude/sonnet https://deepmind.google/technologies/gemini/pro/ https://deepmind.google/models/gemini/flash/ https://huggingface.co/OpenGVLab/InternVL3_5-38B-Instruct https://huggingface.co/OpenGVLab/InternVL3_5-14B-Instruct https://huggingface.co/OpenGVLab/InternVL3-78B https://huggingface.co/OpenGVLab/InternVL3-38B https://www.glm45.com/glm45v GLM-4.1V-Thinking (GLM et al., 2024) 2025-07 https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking Qwen2.5VL-72B (Bai et al., 2025) Qwen2.5VL-32B (Bai et al., 2025) LLAVA-Onevision-72B (Li et al., 2024a) LLAVA-Onevision-7B (Li et al., 2024a) 2025-01 2025-01 2024-08 2024-08 https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct https://huggingface.co/llava-hf/llava-onevision-qwen2-72b-ov-hf https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov Table D24: Generating parameters for Closed-Source LLMs. Model GPT-5 Generation Setup \"model\" : \"gpt-5\", \"temperature\" : 0, \"max_tokens\" : 1024 GPT-4o \"model\" : \"gpt-4o\", \"temperature\" : 0, \"max_tokens\" : 1024 Claude-Opus-4.1 \"model\" : \"claude-opus-4.1\", \"temperature\" : 0, \"max_tokens\" : 1024 Claude-Sonnet-4 \"model\" : \"claude-sonnet-4\", \"temperature\" : 0, \"max_tokens\" : 1024 Gemini-2.5-Pro \"model\" : \"gemini-2.5-pro\", \"temperature\" : 0, \"max_tokens\" : 1024 Gemini-2.5-Flash \"model\" : \"gemini-2.5-flash\", \"temperature\" : 0, \"max_tokens\" : measured data to construct dedicated test subset. These measures effectively ensure that the automatically generated data remains suitable for learning human perceptual models. We argue that even if minor error accumulation exists, it does not compromise the overall quality and contribution of the dataset. 2) forward-looking methodological contribution: The proposed data construction framework and model architecture will have significant impact on the field of all-scale spatial reasoning. Importantly, as more accurate all-scale inference methods emerge in the future, we will continuously integrate higher-quality data to refine this work. This dynamic updating mechanism ensures the long-term relevance and value of our research. Table D25: Generating parameters for Open-Source LLMs. Model Generation Setup Internvl3.5-38B do_sample = False, temperature = 0, max_new_tokens = Internvl3.5-14B do_sample = False, temperature = 0, max_new_tokens = 512 Internvl3-38B do_sample = False, temperature = 0, max_new_tokens = 512 Internvl3-78B do_sample = False, temperature = 0, max_new_tokens = GLM-4.5V do_sample = False, temperature = 0, max_new_tokens = 1024 GLM-4.1V-Thinking do_sample = False, temperature = 0, max_new_tokens = 1024 Qwen2.5VL-32B do_sample = False, max_new_tokens = Qwen2.5VL-72B do_sample = False, max_new_tokens = 1024 LLAVA-Onevision-7B do_sample = False, temperature = 0, max_new_tokens = 1024 LLAVA-Onevision-72B do_sample = False, temperature = 0, max_new_tokens = 1024 41 Work in progress E.2 ALL SCALE POSSIBILITIES Currently, our data coverage remains limited in addressing the full spectrum of spatial scales, despite the equal importance of spatial understanding across these domains. At fine scales, domains such as minimally invasive surgery call for millimeter-level models, while precision manufacturingespecially semiconductor productionpushes into the nanometer range. These capabilities underpin progress in healthcare and technology. In contrast, large-scale applications, including satellite remote sensing and cartography, typically work with resolutions of 10 kilometers or greater. While spatial understanding is equally essential across these extremes, the imaging and 3D modeling techniques involved extend well beyond conventional real-world sensing methods. As result, our current work does not fully address these diverse scales. Nevertheless, we aim to expand our capabilities in the future by integrating modeling across broader range of dimensions, thereby bridging these gaps and enabling more unified spatial analysis. E.3 DISCUSSION OF DATASET We use the free-form subset of SPAR-7M(Zhang et al., 2025e), which consists of approximately 100K samples, about 1% of the original dataset. This part of the data is later processed and filtered with original Scannet (Dai et al., 2017), Scannet++ (Yeshwanth et al., 2023), and ARKitScenes (Baruch et al., 2021) to fit the requirements of our dataset. However, we do not consider our model to be trained on SPAR-7M, nor do we compare it against models trained on SPAR-7M in SparBench. We observe that SPAR-7Ms data design leads to over 200 QA pairs per scene on average, which can cause overfitting in indoor scenarios. Instead, we leverage SPAR-7Ms scan-based characteristics to construct our own CoT for cold-start purposes. It is important to note that neither SpaceR nor SPAR-7M includes CoT reasoning. We generate CoT following the method described in Sec. 3 and apply filtering and screening to ensure quality. These processed data sources, along with the wild video dataset, are integrated into SpaceVista-1M, while acknowledging the additional labeling and filtering steps involved in our pipeline. Overall, these decisions support our position that our data retains meaningful degree of independence from SPAR-7M and SpaceR."
        },
        {
            "title": "F PREVIEW",
            "content": "F.1 SCENE PREVIEW Indoor Scenes. Our indoor dataset consists of simple and clean room-scale environments such as living rooms, meeting rooms, and classrooms. An overview of the data is provided in Fig. F25, highlighting the simplicity and cleanliness of our indoor scenes compared to more complex wild indoor environments. Living rooms feature sofas, coffee tables, and shelves arranged along walls with open floor space. Meeting rooms include evenly spaced chairs around central table, while classrooms have rows of desks facing blackboard or screen. These scenes show limited object variety and limited scene complexity. Wild Indoor Scenes. Representative wild indoor scenes, captured via multi-view smartphone recordings in complex and unconstrained environments such as shopping malls, banquet halls, and art galleries, are illustrated in Fig. F26. These scenes exhibit diverse architectural layouts and high object density. Like in shopping malls, elements such as escalators, display shelves, and glass facades create multi-layered structures with frequent reflections and occlusions. Compared to previous indoor scenes, wild indoor scenes have irregular layouts, dense furniture, diverse objects, and uneven lighting, leading to more complex spatial arrangements. This contrast underscores the structured and clear nature of our data, which supports controlled spatial reasoning evaluation. Outdoor Scenes. Our outdoor scenes include various environments such as parks, tourist landmarks, and others, captured from both ground and aerial views, as shown in Fig. F27. Parks contain irregularly shaped walking paths winding through dense clusters of trees, shrubs, and open lawns, creating mix of natural textures and spatial variations. These areas often include water features, benches, and varied terrain elevations. Therefore, outdoor scene layouts usually involve plazas, staircases, and structured open spaces that introduce rich geometric complexity. 42 Work in progress Drone Scenes. Fig. F28 shows examples from drones perspective. Aerial, low-angle, and oblique views offer detailed spatial structures that are not easily visible from the ground. Playgrounds exhibit clear arrangements of play equipment and open spaces, while parking lots display orderly rows of vehicles and marked boundaries. Parks show clusters of trees, pathways, and water bodies, revealing layered combination of natural and built elements. These diverse viewpoints provide more complete understanding of scene layout and environmental features, supporting improved spatial reasoning. Tabletop Scenes. Examples of tabletop scenes are illustrated in Fig. F29. These scenes capture everyday objects such as keyboards, boxes, and fruits arranged on tabletops, characterized by natural occlusions, varying object placements, and diverse background textures. The dataset employs dynamic multi-view acquisition using mobile devices, enabling richer structural coverage compared to traditional static indoor datasets. This approach captures subtle interactions between objects and background elements, as well as changes in viewpoint and lighting conditions. Tiny Tabletop Scenes. The Fig. F30 shows the tiny tabletop scenes from our dataset. These data are 360-degree turntable videos to capture objects from every angle, solving occlusion issues and improving scene completeness. Our Collected Scenes.We use mobile devices to capture and collect data for some Tabletop and Tiny Tabletop scenes. Our collected data, shown in Fig. F31, features diverse objects and detailed multi-view coverage, enabling fine-grained spatial analysis. The data is similar to the previously mentioned tabletop and tiny tabletop. Tabletop scenes have relatively large objects and rich and diverse backgrounds, which are suitable for capturing diverse objects and natural environments in daily life; while Tiny Tabletop scenes focus on smaller objects, emphasizing detail integrity and multi-view coverage, which facilitates in-depth research on the subtle structure and morphology of these scenes. F.2 TEMPLATE PREVIEW As shown in Tab. F26, we present three exemplar applications: point input for Object Counting, bounding box input for Object Distance, and original input for Spatial Relation. Other scenes and tasks are similar to the example template. F.3 QA PREVIEW We provide comprehensive set of SpaceVista-1M QA pairs here for preview in Tab.F27-Tab.F45. Note that the RL-oriented multiple-choice and regression formats omit anchors like <semantic> and <scale>, since they can be easily injected during training from the meta information. Since if objects are referred to by bounding box, the only changes needed are to change the object name into the corresponding object point/bbox/mask. Each question takes only one video with one form of referring. For example, Where is the toothbrush relative to the keyboard from the view of the start frame? Where is the red mask referred object relative to the keyboard from the view of the start frame?. So, in this preview, we only provide the natural language questions for clarity. Overall, these previews highlight the diversity of our all-scale reasoning SpaceVista-1M dataset. 43 Work in progress Table F26: Multi-type template preview. Examples using the point input for Object Counting, the bounding-box input for Object Distance, and the original input for Spatial Relation. Point Input Template - Refer to the red point in the starting frame and count how many objects are of that type. - Count the number of objects whose class is referred to by the red point in the first frame throughout the video. - Using the red point in the first frame as reference, count how many objects of that class appear in the entire video. - Count every object like the one highlighted by the red point in the videos first frame. - Find all video objects that are of the same kind as the one identified by the red point. - Identify the class from the red point in frame one and tally all instances of that class in the video. - How many objects in the video resemble the one tagged with the red point in the first frame? - Search for all items that belong to the same class as the one shown by the red point in frame one. - Track all objects of the same category as the red-point one from the first frame and count them. - Count the total number of objects in the video that correspond to the class defined by the red point in the first frame. - Use the red point to find class and count how many such instances are there in the video. - Using the initial frames red point as guide, total up all objects of that class. - From the first frames red point, find that class and count its appearances across the video. - Match the object under the red point to others in the video and count them. - Take the red-pointed object as example and count all others like it in the video. Bounding Box Input Template - How far apart do the objects enclosed by the red bounding box and blue bounding box appear in these frames? - What space lies between the red bounding box and the blue bounding box in these frames? - What is the distance measurement between the red bounding box and blue bounding box in the video? - What is the distance between the red-bounded object and the blue-bounded object in the video? - Measure the distance separating the red bounding box and blue bounding box in the video frames. - What is the estimated distance between the red bounding box and the blue bounding box in the video? - What is the measured distance between the red bounding box and blue bounding box in the footage? - Calculate the ground distance from the red bounding box to the blue bounding box based on the frames. - Find the ground distance between the red bounding box and the blue bounding box in these images. - How wide is the space between the red bounding box and the blue bounding box in the video? - Based on the frames, what is the distance from the red bounding box to the blue bounding box? - Please estimate the ground distance between the red bounding box and the blue bounding box in these images. - What is the approximate distance between the red bounding box and blue bounding box in these images? - Provide an estimate for the distance between the red bounding box and blue bounding box seen in the footage. - How far is the red bounding box from the blue bounding box in the frames? Original Input Template - Describe how desk and chair are spatially positioned relative to each other. - What is the spatial relation type between desk and chair in the video? - What type of spatial relationship exists between desk and chair in these frames? - Estimate the spatial relation (such as support, stacking, adhesion, hanging, plug-in) between desk and chair in these frames. - What is the most likely spatial relationship (support, stacking, adhesion, hanging, plug-in) between cabinet and book? - Can you describe the spatial relationship type of awning and awning? - Identify how picture and ceiling are spatially related in the video sequence. - Between desk and chair, what spatial link exists? - What spatial relation links tag to hat in the given frames? - What spatial relation best fits cable and computer mouse in the video frames? - Identify how cable and socket are spatially related in the video sequence. - Describe the spatial relation (e.g., support, stacking, adhesion, hanging, plug-in) between fork and spoon. - Explain the spatial relation between toy camera and building blocks in the video. - How would you classify the spatial relation between sticky note and tumbler? - What type of spatial relationship exists between toy block and toy train in these frames?. 44 Work in progress Figure F25: Indoor data are rather simple and clean scenes inside room. The overall scene is not as complex as the wild indoor scene. Figure F26: Wild indoor data includes more light changes, reflections, and transparency. The objects included are more diverse. 45 Work in progress Figure F27: Outdoor data is jointly collected from ground views, incorporating street, park, building and so on. Figure F28: Drone data captures ground objects from above at oblique angles, providing more complete structural coverage than traditional ground-based capture methods. Work in progress Figure F29: In this tabletop scene, videos capture tabletop objects exhibiting rich background variation and natural occlusions, delivering clearer structural coverage of the objects than traditional static indoor datasets. Figure F30: Tiny tabletop objects captured with rich details for small objects, focusing on fine-scale scenes, unlike typical large or complex indoor or outdoor datasets. 47 Work in progress Figure F31: These samples are collected by us. As small-scale, Tabletop, and Tiny Tabletop datasets offer rich details with accurate annotation. Table F27: The spatial relation task QA preview. Spatial Relation Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input What is the most likely spatial relationship between the red point and the blue point? Options: A.Stacking B. Dhesion C. Support D. Adjacent E. Plug-in Bounding Box Input Point Input Mask Input [{\"x1\":91, \"y1\":181, \"x2\":439, \"y2\":458}, {\"x1\":241, \"y1\":471, \"x2\":479, \"y2\":627}] [[328, 287], [548, 360]] SFT GT <think>Based on the spatial relationship in the image, the red point appears to be positioned below or supporting the blue point. Given their relative arrangement and visual positioning, the red point is most likely the foundation or support for the blue point.</think> <answer>The red point likely serves as the foundation or support for the blue point.</answer> RL GT <think>*</think><answer>D</answer> 48 Work in progress Table F28: The camera moving task QA preview. Camera Moving Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input What directional path did the camera follow in the video, ignoring rotation? Options: A.Front B. Back C. Left D. Right E: Down Bounding Box Input Not applicable. Point Input Not applicable. Mask Input Not applicable. SFT GT <think>The images show sequence of shots from the same scene, with each frame zooming in slightly more. This indicates that the camera is moving forward towards the structure, capturing more detailed parts of the scene as it progresses. The camera keeps moving forward.</think> <answer>Front</answer> RL GT <think>*</think><answer>A</answer> Table F29: The position comparison task QA preview. Position Comparison Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input Where is the toothbrush relative to the keyboard from the view of the start frame? Options: A.Left B. Right C. Front D. Back Bounding Box Input Point Input Mask Input Not applicable. [237, 9] SFT GT <think>The video starts with the toothbrush on the left side in the green cup. As the view rotates, its position shifts from left to right and then back to left. Before disappearing, it remains on the left side. So the toothbrush is on the left from the view of the start frame.</think> <answer>Left</answer> RL GT <think>*</think><answer>A</answer> 49 Work in progress Table F30: The size comparison task QA preview. Size Comparison Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input Based on the height of the referred object shown in the video, which is taller: the red point object or the blue point object? Options: A.Streetlight B. Sign Bounding Box Input Point Input Mask Input [[185, 52], [101, 144]] [{\"x1\":17, \"y1\":110, \"x2\":94, \"y2\":248}, {\"x1\":126, \"y1\":74, \"x2\":163, \"y2\":220}] SFT GT <think>By analyzing the position of the light source in the image, its clear that the streetlight is situated above the sign. Additionally, due to its height and the relatively distant position, the streetlight has broader light coverage, further suggesting it is taller than the sign.</think> <answer>Streetlight.</answer> RL GT <think>*</think><answer>A</answer> Table F31: The existence estimation task QA preview. Existence Estimation Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input Determine which objects in the cabinet, printer, dishwasher, and trash bin are found in the footage. Options: A.Printer B. Cabinet C. Dishwasher D. Trash bin Bounding Box Input Not applicable. Point Input Not applicable. Mask Input Not applicable. SFT GT <think>Based on the content in the images, can see clearly visible trash bin placed on the ground. Its presence is obvious in the scene, as it is clearly shown, and other objects in the image provide reference for its position and shape. Therefore, it is reasonable to infer the existence of the trash bin.</think> <answer>Trash bin</answer> RL GT <think>*</think><answer>D</answer> 50 Work in progress Table F32: The rotation estimation task QA preview. Rotation Estimation Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags. Text Input Calculate the rotation angle of the object in the red bounding box between the first and last frame. Bounding Box Input Point Input Mask Input [{\"x1\":911, \"y1\":605, \"x2\":1201, \"y2\":853}] [[480, 989]] SFT GT <think>By observing the change in the objects position between the frames, estimate its rotation angle. Based on the relative rotation in the two images and using reference points, approximate the object has rotated by about 136 degrees. The exact angle might vary slightly, but its within this range.</think> <answer>136 degrees</answer> RL GT <think>*</think><answer>136</answer> Table F33: The relative distance task QA preview. Relative Distance Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags. Text Input How close does the red mask get to the blue mask in the frames (in centimeters)? Bounding Box Input Point Input Mask Input [{\"x1\":194, \"y1\":172, \"x2\":299, \"y2\":511}, {\"x1\":393, \"y1\":377, \"x2\":479, \"y2\":483}] [[351, 245], [428, 442]] SFT GT <think>Based on the content of frames, the relative position of the red and blue objects changes minimally. The bottle seems 20 centimeters tall. Therefore, the relative distance between them in the frames is estimated to be approximately 7.8 centimeters. The contrast between their shapes and background positions helps in making this estimation.</think> <answer>8 centimeters</answer> RL GT <think>*</think><answer>7.8</answer> Work in progress Table F34: The absolute distance task QA preview. Absolute Distance Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags. Text Input What is the estimated distance between the red mask and the blue mask in the video (in meters)? Bounding Box Input Point Input Mask Input [{\"x1\":250, \"y1\":228, \"x2\":307, \"y2\":314}, {\"x1\":402, \"y1\":229, \"x2\":479, \"y2\":300}] [[270, 278], [264, 442]] SFT GT <think>The red and blue objects are two regular boxes of around 0.1m in size. Based on the position of the red and blue objects in the image, the estimated absolute distance between them is approximately 0.31 meters. This estimation is made based on the size and arrangement of the objects in the frame.</think> <answer>0.31 meters.</answer> RL GT <think>*</think><answer>0.31</answer> Table F35: The room size task QA preview. Room Size Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags. Text Input Give an estimate of the room size based on the video (in square meters). Bounding Box Input Not applicable. Point Input Not applicable. Mask Input Not applicable. SFT GT <think>By observing the spatial layout in the image, estimate the rooms size. Based on the relative proportions and perspective of the table and sofa within the room, and considering typical room size proportions with normal table and sofa size, the area of the room is approximately 24.33 square meters. This estimation is derived from the visible space in the image.</think> <answer>24.33 square meters</answer> RL GT <think>*</think><answer>24.33</answer> 52 Work in progress Table F36: The object counting task QA preview. Object Counting Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags. Text Input How many bowls can be seen throughout the video? Bounding Box Input Point Input Mask Input [{\"x1\":81, \"y1\":357, \"x2\":408, \"y2\":539}] [[439, 243]] SFT GT <think>Based on the objects in the image, two bowls are clearly visible in different positions. Their number is easily identifiable in the frame. Therefore, it can be estimated that there are 2 bowls visible in the video.</think> <answer>2 bowls</answer> RL GT <think>*</think><answer>2</answer> Table F37: The object size task QA preview. Object Size Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags. Text Input How wide is the toy in the first frame (in centimeters)? Bounding Box Input Point Input Mask Input [{\"x1\":112, \"y1\":547, \"x2\":717, \"y2\":1089}] [[820, 437]] SFT GT <think>By observing the size of the toy in the image and its relative proportion to surrounding objects, the width of the toy can be estimated. Based on the proportional reference in the frame, the toys width is approximately 7.4 centimeters.</think> <answer>7.4 centimeters</answer> RL GT <think>*</think><answer>7.4</answer> Work in progress Table F38: The route plan task QA preview for evaluation. Route Plan/Navigation Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input What actions should you take to navigate from the toilet, facing the bathtub, to the door, given the following steps: 1. [please fill in] 2. Go forward until the door. You have reached the final destination? A.Turn left B.Turn right C.Turn back Bounding Box Input Not applicable. Point Input Not applicable. Mask Input Not applicable. SFT GT <think>Based on the layout in the image, assuming you start at the toilet, facing the bathtub, the first step is to turn right, then continue forward until you reach the door. This path is aligned with the relative positioning of the objects in the room.</think> <answer>Turn right</answer> RL GT <think>*</think><answer>B</answer> Table F39: The appearance order task QA preview. Appearance Order Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input Please provide the sequence in which these objects appear in the video: table, chair, blackboard. Options: A.[\"table\", \"blackboard\", \"chair\"] B. [\"chair\", \"table\", \"blackboard\"] C. [\"blackboard\", \"chair\", \"table\"] D. [\"table\", \"chair\", \"blackboard\"] Bounding Box Input Not applicable. Point Input Not applicable. Mask Input Not applicable. SFT GT <think>Based on the objects sequence in the image, the table appears first, followed by the chair, and finally the blackboard. This order is determined by the relative positions of the objects in the frame.</think> <answer>Table, chair, blackboard.</answer> RL GT <think>*</think><answer>D</answer> Work in progress Table F40: The depth estimation task QA preview. Depth Estimation Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags. Text Input Please provide the depth value of the object labeled by the red mask in the initial frame (in meters). Bounding Box Input Point Input Mask Input [{\"x1\":466, \"y1\":528, \"x2\":620, \"y2\":1043}] [[733, 535]] SFT GT <think>Based on the position of the object labeled by the red mask in the image and considering the perspective and size of the object, the estimated depth of the object is 0.72 meters with the reference spoons length of 0.17 meters.</think> <answer>0.72</answer> RL GT <think>*</think><answer>0.72</answer> Table F41: The view change inference task QA preview. Area Estimation Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags. Text Input What is the area of the roof shown in the video (in square meters)? Bounding Box Input Point Input Mask Input [{\"x1\":161, \"y1\":48, \"x2\":312, \"y2\":85}] Not applicable. SFT GT <think>Based on the perspective in the image and the relative size of the objects, the area of the roof can be estimated. By considering the proportions of the objects and the background perspective, the area of the roof is approximately 146.0 square meters.</think> <answer>The area of the roof in the video is 146.0 square meters.</answer> RL GT <think>*</think><answer>146.0</answer> 55 Work in progress Table F42: The object matching task QA preview. Object Matching Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input Which bounding box (green, blue, or yellow) in the last frame is the same object as the red bounding box in the first frame? Options: A.Yellow B. Green Point Input Mask Input Not applicable. Not applicable. Bounding Box Input [{\"x1\":87, \"y1\":382, \"x2\":398, \"y2\":493}] [{\"x1\":122, \"y1\":515, \"x2\":237, \"y2\":617}, {\"x1\":62, \"y1\":376, \"x2\":381, \"y2\":534}] SFT GT <think>By analyzing the object positions and color contrasts in the image, the green bounding box matches the object of the red bounding box in the first frame. This conclusion is based on the relative positions and sizes of the objects in both frames.</think> <answer>The green box.</answer> RL GT <think>*</think><answer>B</answer> Table F43: The obstacles location task QA preview. Obstacles Location Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input Which object will cause obstacles when moving the pliers to the left of the box? (Red point denotes the pliers, Blue point denotes the target position.) Options: A.Pineapple B. Charging plug C. None Bounding Box Input Not applicable. Point Input Not applicable. Mask Input Not applicable. SFT GT <think>The red-handled pliers are on the right, and the target is on the left side of the box at the lower right. If the pliers are moved to the boxs left side, the path crosses the pineapple in the center of the table, making it the main obstacle.</think> <answer>Pineapple.</answer> RL GT <think>*</think><answer>A</answer> Work in progress Table F44: The manipulation planning task QA preview. Manipulation Planning Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input How to move the box to the front of the apple from the viewpoint of the first frame? (Red point denotes the box, blue point denotes the target position.) Options: A.Moving backward 43.6cm B. Moving left 10.2cm C. Moving up 45.7cm D. Moving backward 28.1cm Bounding Box Input Not applicable. Point Input Not applicable. Mask Input Not applicable. SFT GT <think>The red point denotes the current position of the box, and the blue point denotes the target. The task is to move the box in front of the apple by shifting it along the red-to-blue direction about 28.1cm.</think> <answer>Move the book backwards 28.1cm to put the box in front of the apple.</answer> RL GT <think>*</think><answer>D</answer> Table F45: The area estimation task QA preview. View Change Inference Task Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection. Video: <video> Question: <text> During RL: Please provide the thinking process within the <think> </think> tags. Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. Text Input What is the view change between the input frames? Options: A.Back B. Down C. Right D. Left E. Front Bounding Box Input Not applicable. Point Input Not applicable. Mask Input Not applicable. SFT GT <think>By analyzing the angle change between the frames, its clear that the view shifts downward. This conclusion is drawn from comparing the position and angle of objects in the beginning frames.</think> <answer>Downward</answer> RL GT <think>*</think><answer>B</answer>"
        }
    ],
    "affiliations": [
        "Astribot, Beijing University of Posts and Telecommunications",
        "Hong Kong University of Science and Technology",
        "Multimedia Lab, Chinese University of Hong Kong"
    ]
}