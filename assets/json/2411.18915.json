{
    "paper_title": "MATATA: a weak-supervised MAthematical Tool-Assisted reasoning for Tabular Applications",
    "authors": [
        "Vishnou Vinayagame",
        "Gregory Senay",
        "Luis Martí"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mathematical reasoning capabilities are increasing with tool-augmented language agents, but methods often rely either on closed-source or large models, external data, or extensive prompt engineering. This work introduces MATATA, a novel cost-effective method to train LLM agents for tabular data problems through reasoning, planning, and tool use. With a progressive self-improvement paradigm and an iterative weak supervision, it empowers 3.8B/8B Small Language Models (SLMs), particularly suited for local hosting and sensitive business contexts where data privacy is crucial. By employing a flexible and reusable tools across different datasets, it achieves robust performance with effective scalability across shared tasks. Experiments show that MATATA reaches state-of-the-art performances on FinQA and TAT-QA among reasoning frameworks based on open-source models. Moreover, MATATA models compete with GPT-4 based frameworks on TabMWP, while being SLMs."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 2 ] . [ 1 5 1 9 8 1 . 1 1 4 2 : r MATATA: weak-supervised MAthematical Tool-Assisted reasoning for Tabular Applications Vishnou Vinayagame, Gregory Senay, and Luis Martí Docugami Inc, Kirkland, WA 98033, USA http://www.docugami.com {vishnou,greg,lmarti}@docugami.com Abstract. Mathematical reasoning capabilities are increasing with toolaugmented language agents, but methods often rely either on closedsource or large models, external data, or extensive prompt engineering. This work introduces MATATA, novel cost-eﬀective method to train LLM agents for tabular data problems through reasoning, planning, and tool use. With progressive self-improvement paradigm and an iterative weak supervision, it empowers 3.8B/8B Small Language Models (SLMs), particularly suited for local hosting and sensitive business contexts where data privacy is crucial. By employing ﬂexible and reusable tools across diﬀerent datasets, it achieves robust performance with eﬀective scalability across shared tasks. Experiments show that MATATA reaches state-of-the-art performances on FinQA and TAT-QA among reasoning frameworks based on open-source models. Moreover, MATATA models compete with GPT-4 based frameworks on TabMWP, while being SLMs. Keywords: Mathematical Reasoning Small Language Model WeakSupervision."
        },
        {
            "title": "1 Introduction",
            "content": "Recent developments have popularized Large Language Models (LLMs) [1,3,24] as potent tools for tackling complex mathematical reasoning problems [18]. Different strategies encompass an extensive range of techniques, including specialized models like [21, 31], Chain-of-Thought or Program-of-Thought [4, 25], or, tool-based compositional frameworks like [8, 14, 16, 27]. Progress in the study of mathematical reasoning with structured and unstructured data has been tangible thanks to various datasets and benchmarks, such as [5, 17, 30]. Tool-augmented LLMs are particularly prevalent in those benchmarks as they combine program assistance, data manipulation, natural language reasoning, and many other tools. These frameworks leverage the increasing planning capabilities of LLMs [2, 26] to orchestrate the tools, enabling multi-step problem-solving. Some studies focus on prompt engineering methods to get the best from closed-source models like GPT-4 [4, 16, 22, 25]. Others involve ﬁne-tuning multiple specialized Small Language Models (SLMs) in teacher-student fashion with data annotated automatically by larger models [8, 14], or extensively by 2 V. Vinayagame et al. Fig. 1. Example from FinQA with questions over tabular and textual data. human experts [5, 30, 31]. Other methods focus on self-improvement, with either training veriﬁers [10], generators for data augmentation [28], or multi-sampling techniques [29]. The diversity in the types of approaches underscores the potential of language models in enhancing reasoning capabilities. However, each approach faces dependencies or limitations, summarized in Table 1. Prompt engineering is labor-intensive, usually requires hand-crafting eﬀorts, and may not fully capture data complexity and diversity. Closed-source models raise privacy concerns, especially with business documents, and present scalability issues due to their cost and latency, whether for querying models directly or generating high-quality reasoning data. Moreover, using teacher models to generate reasoning steps is often data-ineﬃcient [10], discarding incorrect solutions, and human expert annotation [31] is costly alternative. To overcome this issue, various self-improvement methods have been developed. However, most of them require multiple samplings and/or additional models [10,13], leading to an increase in computational and time costs. Inspired by those challenges, this work introduces novel approach for mathematical reasoning over tabular data. The proposed method aims to reduce both prompt engineering eﬀorts, reliance on larger or external models, privacy concerns, and computational ineﬃciencies while maintaining comparable performances to existing methods. The main contributions of this work are: 1) tool-augmented framework that composes shared tools from diverse datasets; 2) weak-supervised ﬁne-tuning for the tools from self-generated correct reasoning trajectories; 3) weak-supervised preference optimization to align the tools using all updated reasoning trajectories. MATATA: MAthematical Tool-Assisted reasoning for Tabular Applications 3 Table 1. Overview of LLM reasoning frameworks over tabular data. PE: Prompt Engineering; FT: Fine-Tuning. Method Tool-use GPT-4 reliance Human-experts Datasets used PoT [4] EEDP [22] Chameleon [16] ToRA [8] Husky [14] TAT-LLM [31] FinQANet [5] TagOP [30] MATATA PE PE PE FT FT FT FT FT FT é é Ë Ë Ë é é é Ë"
        },
        {
            "title": "2 Related Work",
            "content": "Ë Ë Ë Ë Ë é é é é é é é é é Ë Ë Ë é FinQA, TAT-QA, TabMWP FinQA, TAT-QA, TabMWP TabMWP TabMWP FinQA, TabMWP FinQA, TAT-QA FinQA TAT-QA FinQA, TAT-QA, TabMWP"
        },
        {
            "title": "2.1 Datasets for Mathematical Reasoning",
            "content": "Evaluating LLMs mathematical reasoning capabilities is usually focused on datasets like MATH [9] and GSM8K [6], which explore mathematical problemsolving. While these benchmarks span broad domain of mathematical subjects, the data format is limited to textual word problems. In contrast, other benchmarks like FinQA [5] and TAT-QA [30] represent more complex and realworld reasoning scenarios. These datasets include tabular and textual data, requiring reasoning over ﬁnancial reports with intricated contexts. Moreover, TabMWP [17] expands to mathematical word problems over diﬀerent subjects. Those three datasets need multi-step reasoning, information extraction, data manipulation, tabular and contextual understanding and numerical calculations."
        },
        {
            "title": "2.2 Tool-Augmented Frameworks",
            "content": "Multi-step reasoning has emerged as an approach for mathematical reasoning, particularly over tabular data. Diverse prompting techniques [4, 15, 22, 25] have been explored to encourage stepwise and critical thinking, traditionally relying on closed-source LLMs. Other frameworks adopt LLMs as tools with dedicated planning: both TORA [8] and Husky [14] ﬁne-tune open-source LLMs with training data generated by GPT-4, while Chameleon [16] relies on prompt engineering, directly backed up by ChatGPT/GPT-4. TAT-LLM [31] relies on ﬁxed stepwise pipeline for reasoning decomposition, with ﬁne-tuned model trained on data heavily annotated by human experts. Those ﬁne-tuning approaches rely on closed-source models or human-annotations to generate high-quality synthetic data to train on. 4 V. Vinayagame et al."
        },
        {
            "title": "2.3 Alignment Methods",
            "content": "To align LLMs on downstream tasks, methods have progressed from Reinforcement Learning from Human Feedback (RLHF) [19] to Direct Preference Optimization (DPO) [20], and more recently Kahneman-Tversky Optimization (KTO) [7]. DPO oﬀers cost-eﬀective alternative to RLHF by eliminating the need for separately trained reward model. Instead DPO relies on preference dataset with positive and negative completions for given inputs. In contrast to traditional methods that optimize the log-likelihood of preferences, KTO focuses on maximizing generation utility. KTO is also less data-intensive than DPO as it leverages binary preference data, i.e., completions paired with boolean labels indicating the correctness of the completion. Therefore, it is particularly wellsuited for downstream tasks following Supervised Fine-Tuning (SFT)."
        },
        {
            "title": "3.1 Motivations",
            "content": "MATATA is novel cost-eﬀective method for training LLM agents designed to tackle tabular data problems through reasoning, planning, and tool use. This work explores the following questions: Can high performance be achieved while reducing reliance on external models, data, and extensive prompt engineering? Can tool-augmented SLM framework for mathematical reasoning self-improve through weak supervision? The underlying assumption is that, even with lacking baseline generating training data, weak supervision provides suﬃcient control over each tool to enhance its performance. To answer this, our framework takes inspiration from [8, 14, 16] and employs planner with specialized tools to generate multistep reasoning trajectories. By decomposing complex problems into subtasks, each of them is then solved locally with ﬁne-tuned SLMs. This approach is particularly relevant when handling tabular contexts from business documents, where privacy concerns preclude using closed-source LLMs. Unlike other existing ﬁnetuned tool-augmented LLM approaches like [8, 14], MATATA uses ﬁne-tuned specialized models without relying on GPT-4 to generate training multi-step reasoning trajectories. Prompt engineering eﬀorts are also limited to minimize human involvement and demonstrate the systems ability to improve and evolve with minimal initial guidance, contrary to [4, 16, 22]."
        },
        {
            "title": "3.2 Proposal",
            "content": "MATATA Compositional Reasoning The compositional reasoning process is described in Algorithm 1. The planning draws inspiration from Chameleon [16]. Given context with tabular and textual data, question q, and pre-deﬁned set of tools with tool-speciﬁc prompts pT , the planner decomposes the task into sequence of sub-tasks. Each sub-task is then executed sequentially with MATATA: MAthematical Tool-Assisted reasoning for Tabular Applications 5 the appropriate specialized tool (program generation, row or column selection, text extraction, etc.). Each tool interacts with and the previous outputs to generate intermediate results and update the trajectory. Finally, parsing tool extracts the answer from the last intermediate result of the trajectory. Algorithm 1 Compositional reasoning Require: context c, question q, models (Mt)tT , tool-speciﬁc instructions (pt)tT , set of tools Ensure: Trajectory τ contains the solution 1: τ 2: Mplan(τ pplan ) 3: for each tool Ti in plan do oi Mt(τ pTi ) 4: ri (Ti, oi) 5: τ τ U(c, ri) 6: 7: end for 8: return τ Initialize trajectory Generate plan Tool output Tool execution Trajectory update MATATA Weak-Supervision Stages In the initial stage, tools are few-shot prompted [3] with tool-speciﬁc demonstrations. Those prompts contain examples of how each tool should behave but have not been over-engineered, thus reducing human involvement. This baseline system generates reasoning trajectories used as training data. Consequently, the model improves by learning from its own generated reasoning, as shown in [29]. Once the tools are ﬁne-tuned, the toolspeciﬁc prompts are replaced by just the tools name, mechanically accelerating the inference process, as models have fewer input tokens to process. weak-supervised ﬁne-tuning is then applied to the trajectories, to compensate for the limited guidance put into prompt engineering. Only the correctness of the ﬁnal answer is used to reﬁne the tools without knowing how they should behave, hence the weak signal. Each trajectory is broken down into input/output pairs for the tools involved. Our work redeﬁnes the teacher-student paradigm as the same model generates the trajectories used for weak supervision ﬁne-tuning. The ﬁrst training stage is SFT [19], where each tool is ﬁne-tuned on prompt/completion pairs from valid trajectories generated by the baseline system. After the SFT stage, tools are aligned with KTO [7]. Using KTO is motivated by weak supervision, as it only requires binary signal (correct/incorrect). KTO does not require multiple samplings for given problem, unlike DPO [20], or additional models like veriﬁers or generators [10], or bootstrapping methods [29]. At tool level, for each prompt/completion pair, the preference label is the weak signal from the ﬁnal answer, without requiring step-by-step annotations. Using all invalid trajectories during the alignment phase could help increase domain knowledge in areas un6 V. Vinayagame et al. covered with SFT, countering the tendency of other methods to improve mostly in the frameworks areas of proﬁciency. This work employs self-improvement through multi-stage ﬁne-tuning with the same model, inspired by [12]. For each tool , prompt/completion pairs (ˆxt, ˆyt) from self-generated trajectories serve to ﬁne-tune LoRA adapters [11]. For SFT, the dataset Dt SFT consists of pairs extracted from valid trajectories. The negative log-likelihood objective is deﬁned as: LSFT(M t) = (ˆxt,ˆyt)Dt SFT k=1 log t(ˆyt ˆyt <k, ˆxt) (1) After the ﬁrst training stage, the updated tools generate trajectories for the KTO alignment stage. For each tool , the dataset Dt KTO includes pairs for correct and incorrect solutions. The KTO objective is deﬁned as in [7]: LKTO(πt θ, πt ref) = (ˆxt,ˆyt)Dt KTO [λˆyt v(ˆxt, ˆyt)] (2) with the value function, λ the loss-aversion coeﬃcient, πt ref the model before θ the model being aligned. Once ﬁne-tuned and aligned, the alignment, and πt framework can be evaluated."
        },
        {
            "title": "4.1 Training Details",
            "content": "Experiments are conducted on FinQA [5], TAT-QA [30], and TabMWP [17]. The selected tools for this experiment are inspired by Chameleon [16]. The following tools are shared across datasets: Program Generator, Solution Generator (Natural Language reasoner), Context Extractor, Row/Column Extractor, and Knowledge Retrieval. The task-speciﬁc tools are: unique Planner for each dataset, and Span Extractor for TAT-QA only. This modular design enables ﬂexible handling of diverse datasets, allowing to add and share new tools when incorporating additional datasets. Train sets are used to generate reasoning trajectories and train the tools. The best model is selected with respect to scores on validation sets. Results on the test sets are reported in Table 2 using the datasets metrics: Accuracy or Exact-Match. All datasets and splits are used entirely, as in their respective papers. Two light-weight language models are used as baseline and for ﬁne-tuning: microsoft/Phi-3-mini-4k-instruct 3.8B [1] and mistralai/Ministral-8BInstruct-2410 [23]. All trainings are conducted on single NVIDIA A100 GPU, for combined total of about 128 GPU hours. Throughout the training process, one adapter is trained for each tool in the framework Mt SF SF +KT . Both SFT and KTO adapter ﬁne-tuning use the following hyperparameters: learning rate of 1e-5 and batch size of 32 over 10 epochs, with LoRA of 64, α of 32 and dropout of 0.05. KTO uses the default β parameters at 0.1, and task-speciﬁc weights are applied to address class imbalance between valid and invalid trajectories. MATATA: MAthematical Tool-Assisted reasoning for Tabular Applications 7 Table 2. Accuracy/Exact-Match scores across evaluation datasets. PE: Prompt Engineering; FT: Fine-Tuning. Best overall scores underlined, best section scores bolded. Framework Model Method FinQA TAT-QA TabMWP Acc. EM Acc. Closed-source models Chameleon [16] EEDP [22] ChatGPT GPTChatGPT GPT-4 PoT [4] PoT-SC-Codex Open-source models FinQANet [5] RoBERTa TagOP [30] RoBERTa TORA [8] Husky [14] Code-LLama-34B Llama2-70B Llama2-7B Llama3-8B Llama2-7B TAT-LLM [31] Llama2-13B MATATA (Ours) Llama2-70B Phi3-mini 3.8B Ministral-8B PE PE PE PE PE FT FT FT FT FT FT FT FT FT FT FT - - - - 61.88 79.73 76.05 88.67 93.28 98. - - 68.1 70.2 81.8 61. - - - - 20.8 20. 65.13 71.93 50.1 - - 42. 42.3 76.4 77.51 76.81 81.42 70. 74.44 77.59 77.81 - - 70. 74.0 77.6 76.6 - - - 96.66 98."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "The experimental results in Table 2 show the performances of MATATA-Phi3mini 3.8B and MATATA-Ministral 8B, respectively referred to as MATATA3.8B and MATATA-8B. MATATA outperforms Husky, which relies on training data annotated by GPT-4, and PoT-Codex, GPT-3 variant, on all 3 datasets. Both RoBERTa-based baseline models for FinQA and TAT-QA are also surpassed. On TAT-QA, MATATA-8B ranges between TAT-LLM-13B and TAT-LLM-70B, while TAT-LLM uses human-expert annotations for training. The gap between MATATA-8B and TAT-LLM-70B is of only 3.61%, despite the model size decrease. On FinQA, MATATA-8B is above all methods, and MATATA-3.8B ranges between TAT-LLM-7B and -13B, and between ChatGPT and GPT-4 with EEDP-prompting, while being much smaller than those models. 8 V. Vinayagame et al. Table 3. Evolution of MATATA-3.8B and MATATA-8B Accuracy and EM over training stages; with one vs. all training set(s). Method Training dataset(s) FinQA TAT-QA TabMWP Phi3-mini 3.8B baseline + weak-sup SFT + weak-sup KTO Ministral 8B baseline + weak-sup SFT + weak-sup KTO - One All One All - One All One All 47.06 43. 62.00 67.74 68.19 71.56 69.75 71. 83.87 92.89 96.36 94.05 70.10 74. 96.66 57.11 51.29 70.97 75.68 75. 76.19 74.63 76.36 80.89 89.85 96. 93.38 77.59 77.81 98.12 On TabMWP, MATATA-8B exceeds all methods not using GPT-4, ranking 2nd on the TabMWP leaderboard, with only 0.66% diﬀerence behind Chameleon GPT-4. For its range of parameters and size, MATATA competes with models ﬁne-tuned on higher-quality reasoning data, larger, or extensively prompted. This also conﬁrms the eﬀectiveness of our method, intuiting that ﬁne-tuning larger model with weak-supervision could further improve those scores. To investigate knowledge transfer across datasets with shared tools, two training strategies are explored: (1) training the tools on separate datasets, and (2) training shared tools on the combination of datasets. Results reported in Table 3 show that training tools with weak-trajectories from all datasets outperforms single dataset training, in both SFT and KTO stages. This improvement can be attributed to better coverage over trajectories of each tool, with the increase and diversiﬁcation of training data. It also conﬁrms the reusability and scalability of MATATA: the same framework could be enhanced with other datasets and more shared tools, bringing potential improvements to existing results. Also, the performance increase from baseline models to ﬁne-tuned ones is between 12.79% and 31.39% for the 3.8B model, and between 17.23% and 31.39% for the 8B model. Although the baseline model receives minimal initial guidance through low-eﬀort prompt engineering, applying weak-supervision with SFT and KTO enables tools to naturally adopt their expected behaviors, improving the baseline performances signiﬁcantly. MATATA: MAthematical Tool-Assisted reasoning for Tabular Applications"
        },
        {
            "title": "5 Conclusion",
            "content": "This work explore novel method for language agents to tackle tabular data problems using only SLMs, without relying neither on larger and/or close-source models, or external data. This is essential for sensitive business documents, such as those in FinQA or TAT-QA, as submitting those to closed-source LLMs can raise security concerns. With minimal eﬀorts spent on hand-crafting the prompts, human labor is also reduced, adapting the framework to its users rather than the opposite. Competitive results validate that tool-augmented SLMs with weaksupervision can meet performances of existing reasoning methods. This work serves as step for future developments in creating small, reliable, and secure locally-operated SLM frameworks capable of complex reasoning tasks. Acknowledgments. This work was partially supported by Mathematics of Information Technology and Complex Systems (MITACS) Accelerate, grant IT41737."
        },
        {
            "title": "References",
            "content": "1. Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A.A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Cai, Q., Chaudhary, V., Chen, D., Chen, D., Chen, W., Chen, Y.C., Chen, Y.L., Cheng, H., Chopra, P., Dai, X., Dixon, M., Eldan, R., Fragoso, V., Gao, J., Gao, M., Gao, M., Garg, A., Giorno, A.D., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R.J., Hu, W., Huynh, J., Iter, D., Jacobs, S.A., Javaheripi, M., Jin, X., Karampatziakis, N., Kauﬀmann, P., Khademi, M., Kim, D., Kim, Y.J., Kurilenko, L., Lee, J.R., Lee, Y.T., Li, Y., Li, Y., Liang, C., Liden, L., Lin, X., Lin, Z., Liu, C., Liu, L., Liu, M., Liu, W., Liu, X., Luo, C., Madan, P., Mahmoudzadeh, A., Majercak, D., Mazzola, M., Mendes, C.C.T., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., Perez-Becker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Ren, L., de Rosa, G., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Shen, Y., Shukla, S., Song, X., Tanaka, M., Tupini, A., Vaddamanu, P., Wang, C., Wang, G., Wang, L., Wang, S., Wang, X., Wang, Y., Ward, R., Wen, W., Witte, P., Wu, H., Wu, X., Wyatt, M., Xiao, B., Xu, C., Xu, J., Xu, W., Xue, J., Yadav, S., Yang, F., Yang, J., Yang, Y., Yang, Z., Yu, D., Yuan, L., Zhang, C., Zhang, C., Zhang, J., Zhang, L.L., Zhang, Y., Zhang, Y., Zhang, Y., Zhou, X.: Phi-3 technical report: highly capable language model locally on your phone (2024), https://arxiv.org/abs/2404.14219, mIT License 2. Aksitov, R., Miryooseﬁ, S., Li, Z., Li, D., Babayan, S., Kopparapu, K., Fisher, Z., Guo, R., Prakash, S., Srinivasan, P., Zaheer, M., Yu, F., Kumar, S.: Rest meets react: Self-improvement for multi-step reasoning llm agent (2023), https://arxiv.org/abs/2312.10003 3. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners (2020), https://arxiv.org/abs/2005.14165 10 V. Vinayagame et al. 4. Chen, W., Ma, X., Wang, X., Cohen, W.W.: Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (2023), https://arxiv.org/abs/2211. 5. Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.H., Routledge, B., Wang, W.Y.: Finqa: dataset of numerical reasoning over ﬁnancial data. Proceedings of EMNLP 2021 (2021), mIT License 6. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., Schulman, J.: Training veriﬁers to solve math word problems (2021), https://arxiv.org/abs/2110.14168 7. Ethayarajh, K., Xu, W., Muennighoﬀ, N., Kto: Model https://arxiv.org/abs/2402.01306 alignment as prospect Jurafsky, D., Kiela, D.: (2024), optimization theoretic 8. Gou, Z., Shao, Z., Gong, Y., Shen, Y., Yang, Y., Huang, M., Duan, N., Chen, W.: Tora: tool-integrated reasoning agent for mathematical problem solving (2024), https://arxiv.org/abs/2309.17452 9. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., Steinhardt, J.: Measuring mathematical problem solving with the math dataset (2021), https://arxiv.org/abs/2103.03874 10. Hosseini, A., Yuan, X., Malkin, N., Courville, A., Sordoni, A., Agar- (2024), self-taught reasoners veriﬁers for wal, R.: V-star: Training https://arxiv.org/abs/2402.06457 11. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, large language models (2021), L., Chen, W.: Lora: Low-rank adaptation of https://arxiv.org/abs/2106.09685 12. Huang, J., Gu, S., Hou, L., Wu, Y., Wang, X., Yu, H., Han, J.: Large language models can self-improve. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 10511068. Association for Computational Linguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.emnlp-main.67, https://aclanthology.org/2023.emnlp-main.67 13. Hwang, H., Kim, D., Kim, S., Ye, S., Seo, M.: Self-explore: Enhancing mathematical reasoning in language models with ﬁne-grained rewards. arXiv preprint https://doi.org/10.48550/arXiv.2404.10346, arXiv:2404.10346 https://arxiv.org/abs/2404.10346 (2024). 14. Kim, J., Paranjape, B., Khot, T., Hajishirzi, H.: Husky: uniﬁed, open-source language agent for multi-step reasoning (2024), https://arxiv.org/abs/2406.06469 15. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., Cobbe, K.: Lets verify step by step (2023), https://arxiv.org/abs/2305.20050 16. Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.W., Wu, Y.N., Zhu, S.C., Gao, J.: Chameleon: Plug-and-play compositional reasoning with large language models. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=HtqnVSCj3q 17. Lu, P., Qiu, L., Chang, K.W., Wu, Y.N., Zhu, S.C., Rajpurohit, T., Clark, P., Kalyan, A.: Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning (2023), https://arxiv.org/abs/2209.14610, mIT License 18. Lu, P., Qiu, L., Yu, W., Welleck, S., Chang, K.W.: survey of deep In: Rogers, A., Boyd-Graber, J., the 1: Long Papers). learning for mathematical Okazaki, N. Association for Computational Linguistics 61st Annual Meeting (eds.) Proceedings reasoning. (Volume the of of MATATA: MAthematical Tool-Assisted reasoning for Tabular Applications 1460514631. Association pp. Canada https://aclanthology.org/2023.acl-long.817 for Computational Linguistics, Toronto, https://doi.org/10.18653/v1/2023.acl-long.817, 2023). (Jul 19. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R.: Training language models to follow instructions with human feedback. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. NIPS 22, Curran Associates Inc., Red Hook, NY, USA (2024) 20. Rafailov, R., Sharma, A., Mitchell, E., Manning, C.D., Ermon, S., Finn, C.: Direct preference optimization: Your language model is secretly reward model. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://arxiv.org/abs/2305.18290 21. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y.K., Wu, Y., Guo, D.: Deepseekmath: Pushing the limits of mathematical reasoning in open language models (2024), https://arxiv.org/abs/2402.03300 22. Srivastava, P., Malik, M., Gupta, V., Ganu, T., Roth, D.: Evaluating llms reasoning in ﬁnancial document question answering (2024), mathematical https://arxiv.org/abs/2402.11194 23. Team, M.A.: Un ministral, des ministraux (2024), https://mistral.ai/fr/news/ministraux/, mistral Research License 24. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.: Llama 2: Open foundation and ﬁne-tuned chat models (2023), https://arxiv.org/abs/2307. 25. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models (2023), https://arxiv.org/abs/2201.11903 26. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, Y.: React: Synergizing reasoning and acting in language models https://arxiv.org/abs/2210.03629 I., Narasimhan, K., Cao, (2023), 27. Yin, S., You, W., Ji, Z., Zhong, G., Bai, J.: Mumath-code: Combining tool-use large language models with multi-perspective data augmentation for mathematical reasoning (2024), https://arxiv.org/abs/2405.07551 28. Yuan, Z., Yuan, H., Li, C., Dong, G., Lu, K., Tan, C., Zhou, C., Zhou, J.: Scaling relationship on learning mathematical reasoning with large language models (2023), https://arxiv.org/abs/2308. 29. Zelikman, E., Wu, Y., Mu, J., Goodman, N.D.: ping reasoning with reasoning. arXiv preprint arXiv:2203.14465 https://arxiv.org/abs/2203.14465 Star: Bootstrap- (2022), 30. Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., Chua, T.S.: Tatqa: question answering benchmark on hybrid of tabular and textual content V. Vinayagame et al. in ﬁnance (2021), https://arxiv.org/abs/2105.07624, creative Commons (CC BY) Attribution 4.0 International 31. Zhu, F., Liu, Z., Feng, F., Wang, C., Li, M., Chua, T.S.: Tat-llm: specialized language model for discrete reasoning over tabular and textual data (2024), https://arxiv.org/abs/2401."
        }
    ],
    "affiliations": [
        "Docugami Inc, Kirkland, WA 98033, USA"
    ]
}