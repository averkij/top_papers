{
    "paper_title": "BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks",
    "authors": [
        "Juan Rodriguez",
        "Xiangru Jian",
        "Siba Smarak Panigrahi",
        "Tianyu Zhang",
        "Aarash Feizi",
        "Abhay Puri",
        "Akshay Kalkunte",
        "François Savard",
        "Ahmed Masry",
        "Shravan Nayak",
        "Rabiul Awal",
        "Mahsa Massoud",
        "Amirhossein Abaskohi",
        "Zichao Li",
        "Suyuchen Wang",
        "Pierre-André Noël",
        "Mats Leon Richter",
        "Saverio Vadacchino",
        "Shubbam Agarwal",
        "Sanket Biswas",
        "Sara Shanian",
        "Ying Zhang",
        "Noah Bolger",
        "Kurt MacDonald",
        "Simon Fauvel",
        "Sathwik Tejaswi",
        "Srinivas Sunkara",
        "Joao Monteiro",
        "Krishnamurthy DJ Dvijotham",
        "Torsten Scholak",
        "Nicolas Chapados",
        "Sepideh Kharagani",
        "Sean Hughes",
        "M. Özsu",
        "Siva Reddy",
        "Marco Pedersoli",
        "Yoshua Bengio",
        "Christopher Pal",
        "Issam Laradji",
        "Spandanna Gella",
        "Perouz Taslakian",
        "David Vazquez",
        "Sai Rajeswar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io ."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 6 2 6 4 0 . 2 1 4 2 : r BIGDOCS: AN OPEN AND PERMISSIVELY-LICENSED DATASET FOR TRAINING MULTIMODAL MODELS ON DOCUMENT AND CODE TASKS Juan Rodriguez1,2,3,a Xiangru Jian1,4,a Siba Smarak Panigrahi1,2,a Tianyu Zhang1,2,5,a Aarash Feizi1,2,6,a Abhay Puri1,a Akshay Kalkunte1,a François Savard1,a Ahmed Masry1,7,b Shravan Nayak1,2,5,b Rabiul Awal1,2,5,b Mahsa Massoud1,2,6,b Amirhossein Abaskohi1,8,b Zichao Li1,2,6,b Suyuchen Wang2,5,b Pierre-André Noël1,b Mats Leon Richter1,b Saverio Vadacchino1 Shubham Agarwal1,2 Sanket Biswas 9 Sara Shanian1 Ying Zhang1 Noah Bolger1 Kurt MacDonald1 Simon Fauvel1 Sathwik Tejaswi1 Srinivas Sunkara1 Joao Monteiro1 Krishnamurthy DJ Dvijotham1 Torsten Scholak1 Nicolas Chapados1 Sepideh Kharagani1 Sean Hughes1 M. Özsu4 Siva Reddy1,2,6,10 Marco Pedersoli1,3 Yoshua Bengio2,5,10 Christopher Pal1,2,10,11 Issam Laradji1,8 Spandana Gella1 Perouz Taslakian1 David Vazquez1 Sai Rajeswar1,2 1ServiceNow 2Mila 3École de Technologie Supérieure 4University of Waterloo 5Université de Montréal 6McGill University 7York University 8University of British Columbia 9Universitat Autònoma de Barcelona 10CIFAR AI Chair 11Polytechnique Montréal"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require longstructured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, benchmark suite with 10 novel tasks where we create datasets that reflect realworld use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocsBench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Visually-Rich Document (VRD) data containing text and structured elements (such as charts, infographics, diagrams, sketches, tables, etc.) are essential cues for users to efficiently understand complex information holistically. To facilitate this understanding, foundation models for VRDs must process these structured documents and subsequently extract key insights, identify patterns, and Preprint. Under Review. OCR-related CORD Park et al. (2019) (3.2 K) COCO-Text Veit et al. (2016) (47.0 K) PubTables-1M Smock et al. (2022) (1371.3 K) OCR-VQA Mishra et al. (2019) (495.9 K) ArxivOCR* (446.0 K) Structure TextOCR Sidorov et al. (2020) (43.5 K) CDIP-1M (2985.0 K) DocBank Li et al. (2020b) (400.0 K) Datikz Belouadi et al. (2024) (189.1 K) TableBank Li et al. (2020a) (463.3 K) GUI2UserIntent Cheng et al. (2024) (80.1k)"
        },
        {
            "title": "Captioning",
            "content": "Open4Business Singh et al. (2020) (27.9 K) TabFact Chen et al. (2020) (39.5 K) Chart2Markdown* (6.5 K) GUI2Summary Cheng et al. (2024) (80.1k) Q&A SVGCap Rodriguez et al. (2023a) (100.0 K) ArxivTableCap Arkea (2024) (156.2 K) Chart2Caption* (8.0 K) WikiTQ Pasupat & Liang (2015) (27.8 K) FigureQA Kahou et al. (2017) (80.0 K) UniChart Masry et al. (2023) (605.0 K) GUI-VQA Cheng et al. (2024) (80.1k)"
        },
        {
            "title": "Code",
            "content": "Sreenshot2HTML* (11.3 K) Table2LaTex Arkea (2024) (78.6 K) Image2Flow* (20.0 K) Image2SVG Rodriguez et al. (2023a) (219.5 K) Figure 1: BigDocs: Large-Scale Structured Continual Pretraining and Finetuning Dataset. The inner circle represents the distribution of BigDocs, detailing the categories. The outer circle displays the specific datasets compiled to form 7 million image-text pairs. Datasets with * denotes our contribution. generate concise summaries and responses from user requests with reasoning (Landeghem et al., 2023; Zhu et al., 2022). Recent advances in multimodal AI (Yang et al., 2023; Team et al., 2024b) have demonstrated impressive capabilities, including generating functional web pages (Zheng et al., 2024), automating document understanding workflows (Wang et al., 2023b), and extracting detailed information from documents to produce comprehensive reports (Borchmann, 2024). However, the datasets used to train these models remain closed-source, with undisclosed details and restrictive licensing, which hampers their broader adoption in research and the advancement of open-source model development. In contrast, current open-source models and datasets (Chen et al., 2023; Liu et al., 2023b; 2024) primarily focus on basic document understanding tasks, such as optical character recognition (OCR), e.g., DocStruct4M (Hu et al., 2024), or basic question-answering and mathematical problems, e.g., Cambrian-7M (Tong et al., 2024). These efforts do not sufficiently address the complexity of processing intricate visual documents or generating long-structured outputs, such as JSON and HTML, which are valuable in real-world applications. In this work, we present BigDocsc, large-scale and open dataset, benchmark suite, and models specifically designed for user-facing document-related tasks. BigDocs aims to bridge existing gaps by enabling open-source models to meet the rising demand for sophisticated document understanding technologies. Comprising 7.5 million image-text pairs, BigDocs is carefully curated to support three core areas: (1) Document Information Extraction, which includes enhanced OCR for diverse document types, named entity recognition, layout analysis, and table detection; (2) Document Understanding, covering semantic comprehension tasks such as document classification, question answering, and analysis of diagrams; and (3) Document Creation and Manipulation, which involves converting visual data into structured formats like HTML, LaTeX, and JSON. Our survey of 133 existing datasets revealed that 80% of them (i.e., around 100 datasets) have either non-permissive licenses (Jaume et al., 2019; Štˇepán Šimsa et al., 2023) or no clear licensing information (Chaudhry et al., 2019; kleister Charity, 2021), creating barriers to reuse and transparency. In response, BigDocs prioritize datasets with permissive licenses (e.g., CC-BY-4.0, MIT) and documentrelated information, ultimately retaining 16 fully accessible datasets. To further support accessibility, we developed the BigDocs Toolkit, which offers modular tools for data preprocessing, filtering, and consolidation. Additionally, we introduced unified metadata framework to enhance dataset traceability (e.g., properties, sources, licenses), including detailed documentation of transformations applied by us to the original data. We also conduct data contamination analysis on downstream tasks data, showing that BigDocs-7.5M has lower contamination rates than previous datasets. To further advance document intelligence, BigDocs-Bench offers 10 novel downstream tasks, each with four splits: train, validation, test, and hidden test (with 329k training samples, 11k validation aFirst Author Equal contribution. bSecond Author Equal contribution. chttps://bigdocs.github.io. 2 Preprint. Under Review. samples, 10k testing samples). These tasks focus on structured output generation, including code formats such as HTML, LaTeX, SVG, and Markdown. Our experimental results demonstrate that models trained on the BigDocs suite outperform those trained on existing datasets like DocStruct4M (Hu et al., 2024) on standard document benchmarks. Additionally, automatic and human evaluations on the novel tasks introduced in BigDocs-Bench highlight the advanced capabilities of these models in generating long-format, structured outputs. User evaluations reveal preference for our models outputs 88% of the time over Phi3.5 Instruct and 63% over GPT-4. Built with commitment to accountability, responsibility, and transparency (ART) (Bommasani et al., 2023; Vogus & Llansóe, 2021), BigDocs will be open-sourced (upon acceptance), including datasets, models, and documentation to foster responsible AI development. In summary, BigDocs contributions include: 1. BigDocs-7.5M, large-scale, license-permissive dataset designed for continual pretraining (further training from pretrained foundation model checkpoint) and downstream finetuning (e.g., to follow instructions or task formats) of multimodal models on document-related tasks. It includes traceable metadata and curated licensing drawing from document-rich multiple data sources, ensuring full public accessibility. 2. BigDocs-Bench, set of 10 new benchmarks, including test datasets as well as corresponding innovative evaluation metrics for multimodal models to generate long-structured code outputs from images, including formats such as HTML, LaTeX, Markdown, and SVG. 3. BigDocs Toolkit, unified tools supporting open-source efforts. These tools allow efficient data curation, filtering, formatting, and preparation for training models generating structured outputs. 4. BigDocs Models: We conduct extensive experiments using four state-of-the-art public models, demonstrating the advantages of training with BigDocs over alternative datasets and enabling the models to learn novel tasks through our dataset suite."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multimodal Datasets. General-purpose vision-language datasets like COCO Caption (Chen et al., 2015) and SBUCaption (Ordonez et al., 2011) primarily feature photographic content, lacking visually-rich document (VRD) data (Sharma et al., 2018; Changpinyo et al., 2021). In contrast, our focus is on text-heavy datasets including PDFs, tables, and invoices (Veit et al., 2016; Mishra et al., 2019; Singh et al., 2021; Li et al., 2020b;a; Soboro, 2022), which are crucial for tasks like information extraction and parsing (Masry et al., 2022; Rodriguez et al., 2023c;b). While datasets like DocStruct4M (Hu et al., 2024) and Cambrian7M (Tong et al., 2024) partially address these needs, they often lack permissive licenses or are not open-source. Kosmos-2.5 (Lv et al., 2023) focuses on building large document dataset; however, the authors did not make it public. BigDocs fills these gaps by providing 7.5M permissively licensed image-text pairs from 16 academic datasets and other open platforms, supporting diverse document understanding tasks. Responsible Data and Licensing. Enterprise models like GPT-4 (OpenAI, 2023) and Claude (Anthropic, 2024) are often closed-source, lacking transparency in training data (Bommasani et al., 2024). Foundational works (Gebru et al., 2021; Bender & Friedman, 2018) emphasize the importance of open access and transparency in dataset documentation. Previous works promoting open-access such as StarCoder (Li et al., 2023), The Stack (Kocetkov et al., 2022), FineWeb (Penedo et al., 2024), and LLaMA (Dubey et al., 2024), have addressed this by releasing data or models for language models pretraining. Our work builds on these efforts by creating curated, well-documented resource for open access (Laurençon et al., 2023), addressing licensing complexities ranging from permissive licenses (e.g., Apache 2.0, MIT (Hu et al., 2024; Gadre et al., 2023)) to restrictive ones like CC BY-SA (Foundation, 2024; Zhang et al., 2024) and CC BY-NC-SA (Wang et al., 2024b). Compound datasets, like Cambrian-7M (Tong et al., 2024), face mixed licensing issues, while some datasets, such as DocVQA (Mathew et al., 2021b), have different licenses for images and annotations. In developing BigDocs, we prioritized permissive licensing in data curation and contributed to new datasets that adhere to open-access principles, ensuring transparency at all stages of development. Multimodal Document Understanding Models. Recent advancements have introduced several general-purpose multimodal models (Liu et al., 2023b;a; 2024; Bai et al., 2023b; Laurençon et al., 2024; Tong et al., 2024). For example, LLaVA (Liu et al., 2023b) integrates vision encoder 3 Preprint. Under Review. Figure 2: BigDocs-7.5M Dataset Curation. The figure illustrates the extraction, filtering, and curation process of BigDocs-7.5M, which emphasizes maintaining permissive licensing. To build BigDocs-7.5M, we first gather publicly-available vision-language datasets, particularly those centered on document analysis, and apply rigorous filtering process. We then augment these datasets with our own crawled data. Finally, we standardize all samples and tasks into unified format to produce BigDocs-7.5M. with language model, with later versions enhancing reasoning and OCR capabilities (Liu et al., 2023a; 2024). Qwen2-VL (Bai et al., 2023b) processes images at native resolutions, while Phi 3.5 Vision (Abdin et al., 2024) offers lightweight model for reasoning tasks. Specialized models for visually-rich documents, like DocOwl1.5 (Hu et al., 2024) and DocLLM (Wang et al., 2023a), have also gained traction, particularly for commercial applications. However, the datasets for training these models are often not publicly available or have restrictive licenses (Hu et al., 2024). Our work, BigDocs, addresses this by consolidating existing permissive datasets to support the development of reproducible, commercially viable document understanding models."
        },
        {
            "title": "3 BIGDOCS-7.5M",
            "content": "BigDocs-7.5M is large-scale, license-permissive, and carefully curated dataset for visual document understanding designed to train foundational models across various document types and tasks. It consolidates public datasets and newly crawled data with permissive licenses by preprocessing, cleaning, and filtering them into unified collection of 7.5 million image-text pairs. All curated datasets and related artifacts will be openly released to foster community collaboration (Bender & Friedman, 2018). The curation process is illustrated in Figure 2 and detailed below."
        },
        {
            "title": "3.1 DATASET CURATION PROCESS",
            "content": "Existing Dataset Acquisition. The authors, along with domain experts and researchers, guided the collection strategy, assessing dataset relevance, quality, and diversity. We gathered 133 public vision-language datasets by searching academic repositories, open data platforms, and research papers. The collection focused on tasks like image captioning (Chen et al., 2015; Sidorov et al., 2020), OCR (Park et al., 2019; Smock et al., 2022), visual question answering (Mishra et al., 2019; Mathew et al., 2021b), scene-text recognition (Veit et al., 2016; Singh et al., 2021), and document layout analysis (Li et al., 2020a;b), resulting in diverse multimodal dataset repository. Datasheets for Datasets. During data acquisition, we compiled detailed datasheets for each dataset, capturing metadata such as ownership, status, size, references, source type, annotations, licensing, and specific observations. We filtered datasets based on licensing compatibility and relevance to our document-related tasks, then extended the datasheets of selected datasets for better categorization. This extension organized datasets by attributes such as medium type (e.g., digital, scanned), document type (e.g., articles, infographics), sourcing method, text type (e.g., computer-generated, handwritten), structure, language, timeframe, and licensing. For licensing, we documented both the image licenses and annotation licenses separately, as these often differed and impacted the overall permissiveness and usability of each dataset. This structured approach aligned datasets with specific use cases (e.g., OCR, structured parsing) and grouped them for pretraining, finetuning, and evaluation, ensuring effective integration into our visual document understanding pipeline. 4 Preprint. Under Review. (a) Threshold 0.96 (b) Threshold 0. (c) Threshold 0.98 Figure 3: Assessing data contamination (smaller is better). The radial axis (log scale) indicates the proportion of images from the evaluation dataset that exhibit similarity to training sample beyond given threshold according to CLIP. Human evaluations indicate that most instances captured at threshold of 0.98 are problematic, and most problematic samples are identified at threshold of 0.96. Except for MMMU and DudeMini, BigDocs-7.5M (blue/darkred) is less contaminated compared to DocStruct4M (red). License Filtering. key criterion for dataset selection was ensuring permissive licenses (e.g., CC-BY, MIT, Apache 2, CC0) for both images and annotations, suitable for open access and commercial use (more details on various of licenses in Appendix A.7). Datasets with non-permissive licenses, like DVQA (Kafle et al., 2018) (CC-BY-NC 4.0) or DocILE (Štˇepán Šimsa et al., 2023) (non-commercial use), or with no license information, like DeepForm (Svetlichnaya, 2020), were excluded. Ultimately, we prioritized permissive licenses for both text and images, resulting in 20% being kept, while 7.5% moderately restrictive and 72.5% non-permissive were discarded. Some included datasets still have images under less clear terms, such as Fair Use (e.g., OCR-VQA), documented in the metadata."
        },
        {
            "title": "3.2 BIGDOCS TOOLKIT: DATA PREPROCESSING, FILTERING, AND CONSOLIDATION",
            "content": "The BigDocs Toolkit provides modular tools for preprocessing, filtering, contamination management, metadata management, and dataset loading. These components work in unison to streamline the integration of large-scale document datasets, ensuring quality and ease for efficient model training. # Datamaker Module. The BigDocs Toolkit offers modular framework for dataset curation, focusing on standardization, quality control, and metadata management. Its core DataMaker class acts as template for handlers that extract annotations and convert raw data into standardized format. universal function processes tasks like OCR, VQA, and code generation, ensuring consistency. Bounding boxes are standardized, and corrupted samples are filtered. The Toolkit also generates metadata to enhance transparency, covering licensing and processing details (see Appendix A.9). (cid:18) Unified Metadata Framework. We propose unified metadata framework for BigDocs to ensure transparency and traceability. This framework thoroughly examines each raw data source, extracts fine-grained license information, and documents transformations applied to the data (e.g., different sources may have distinct licenses). Each data sample includes metadata attribute detailing its properties, licenses, sources, and transformations (see Appendix A.9 for an example in Figure 13 and structure details). To our knowledge, this is the first systematic approach to track metadata for visually rich documents, advancing transparency in multimodal dataset curation. Assessing Contamination. The presence of downstream evaluation data in training datasets can significantly affect the accurate measurement of models effectiveness (Magar & Schwartz, 2022). BigDocs-7.5M contains samples from the training split of TabFact, WTQ, and TextVQA. However, any evaluation dataset may priori overlap with training datasets through less direct dependencies. We favor transparency strategy: search for overlaps and report what was found. Figure 3 gives an overview of the main observations on this front; see Appendix A.3 for additional details. 5 Preprint. Under Review. Figure 4: 8 of the new tasks introduced in BigDocs-Bench. These tasks share focus on understanding the underlying structure of visually rich documents, with many also requiring generating lengthy outputs, such as SVG and HTML code. More tasks are shown in Figure 9 and 10."
        },
        {
            "title": "3.2.1 THE RESULTING BIGDOCS-7.5M",
            "content": "BigDocs-7.5M consists of 7.5M image-text pairs for training (4M unique images), 500k for validation (234k unique images), and 470k for testing (261k unique images). These data points aggregate four document-related tasks OCR, structured parsing, captioning, and question-answering enabling models to handle diverse document use cases (see Figure 1 and Appendix and A.1 for more details). Low-quality data is filtered out, and metadata details our best-effort licensing compliance, making the dataset suitable for training foundational models, even in commercial applications."
        },
        {
            "title": "4 BUILDING BIGDOCS-BENCH",
            "content": "In the previous section, we introduced BigDocs-7.5M, unified and permissive dataset for training models on document understanding tasks. Building on this, we present BigDocs-Bench, benchmark suite for evaluating downstream tasks that transform visual inputs into structured outputs, such as GUI2UserIntent (fine-grained reasoning), Image2Flow (structured output), Chart2Caption (understanding), and Screenshot2HTML (creative generation). BigDocs-Bench includes ten specialized tasks, with 329k training samples, 11k validation samples, 10k testing samples, and an additional hidden test set. Refer to Table 1 for task details and Figure 4 for examples."
        },
        {
            "title": "4.1 BIGDOCS-BENCH TASKS SUITE",
            "content": "Table 1: Statistics of the ten downstream tasks in BigDocs-Bench. GPT2 tokenizer is used to produce token numbers of both queries and annotations (if any), where the format is (avg std). Downstream Task Screenshot2HTML Table2LaTeX Image2SVG Image2Flow(GraphViz) Image2Flow (JSON) (cid:247) Chart2Markdown Chart2Caption ł GUI2UserIntent GUI2Summary (cid:220) GUI-VQA # Training # Validation # Public Test # Hidden Test # Text Tokens 9,338 77,669 198,000 8,000 8,000 4,516 5,412 79,000 79,000 78,991 1,000 1,000 2,000 1,000 1,000 1,000 1,300 1,000 1,000 1,000 500 500 748 500 500 500 650 500 500 500 500 500 748 500 500 500 650 500 500 500 32,700 53,105 438 540 2,871 1,728 418 124 1,771 601 1,559 4,442 94 49 28 4 132 25 35 Screenshot2HTML: We introduce benchmark for Screenshot2HTML conversion, with 10,838 real-world website screenshots paired with HTML code (Appendix A.4.2). Curated from diverse, text-heavy websites in the FineWeb corpus (Penedo et al., 2024), it contrasts with synthetic GPT-generated sites from prior work (Laurençon et al., 2024b). Using Playwright, we retrieved, rendered, and filtered sites for accessibility, content, and licensing. External assets (e.g., CSS, fonts) were inlined, JavaScript removed, and images replaced to focus on structure. Screenshot2HTML evaluates the accuracy of HTML generated from webpage screenshots, emphasizing layout fidelity and semantic correctness. Table2LaTeX: We propose benchmark for Table2LaTeX conversion, consisting of 79,669 table images paired with original LaTeX code and captions (details in Appendix A.4.3). The dataset was 6 Preprint. Under Review. curated by crawling arXiv papers with permissible licenses and extracting tables from PDFs and TeX source files.d Instead of relying on imperfect PDF detection, tables were rendered from the LaTeX .tex code to ensure accurate visuals. For instance, an image of table is paired with its corresponding LaTeX snippet code and caption. This benchmark supports precise table extraction and LaTeX generation evaluation from academic documents. Image2SVG: We present benchmark for Image2SVG conversion, curated from the existing SVG-Stack collection by StarVector (Rodriguez et al., 2023a) (details in Appendix A.4.4). The dataset includes 200k raster images paired with SVG code (e.g., flowchart image paired with SVG code replicating it) or descriptive text. We filtered for complex designs, using image entropy to exclude simple graphics, and ranked image-text pairs by CLIP Score (Hessel et al., 2022; Radford et al., 2019). This benchmark evaluates models on precise vector image reconstruction and scalability. Image2Flow: We introduce two benchmarks, Image2Flow(GraphViz) and Image2Flow(JSON), mapping flowchart images to JSON or GraphViz code (Appendix A.4.1). The dataset includes 10,000 flowchart samples with GraphViz files generated using LLaMA 3.1 (Dubey et al., 2024) and JSON files detailing nodes and connections. Random colors and styles were applied for diversity. Unlike FlowchartQA (Tannert et al., 2023), which focuses on QA over preprocessed flowcharts, this benchmark tests models ability to extract structure from raw images. (cid:247) Chart2Markdown: This dataset assesses the models capabilities in extracting data values from chart images. Formally, the model is given chart image and asked to produce data table of the underlying data table in markdown format. To create this dataset, we crawled recent chart images from the Statista websitee, focusing on charts from 2023 and 2024 that were not used in prior datasets like UniChart (Masry et al., 2023) and ChartQA (Masry et al., 2022). This ensures that the dataset reflects the most up-to-date facts and trends and overlaps less with existing datasets and benchmarks. We collected 6,516 chart images with their data tables and human-written summaries. Chart2Caption: We introduce benchmark for Chart2Caption conversion, aiming to generate textual summaries from chart images. The dataset includes 6,516 samples, with charts sourced from Kaggle by running public analytics notebooks and extracting charts and code. Summaries were generated using multimodal InterVL2-26B model (Chen et al., 2023; 2024) based on custom prompt (see Appendix A.4.5) and augmented with human-written summaries from the Chart2Markdown task. This benchmark evaluates models abilities to interpret and summarize visual data representations. ł GUI2UserIntent: This benchmark interprets user intent from GUI interactions, identifying elements linked to clicked bounding boxes (details in Appendix A.4.7). The dataset, repurposed from SeeClick (Cheng et al., 2024), includes 80,000 website screenshots with bounding box coordinates and corresponding user intents sourced from Common Crawl to capture user interactions effectively. GUI2Summary: The GUI2Summary task generates descriptions of website screenshots, focusing on web layouts. We synthesized 80,000 summaries (under 100 words each) using InternVL28B (Chen et al., 2023) in zero-shot setting (details in Appendix A.4.8). Each summary provides an overview of the main content, referencing key visual elements, layout, and color schemes. (cid:220) GUI-VQA: The GUI-VQA task answers questions about website screenshots, focusing on content and elements. We generated 80k QA pairs using sentences from GUI-to-Summary and prompting LLaMA 3.1-8b (Dubey et al., 2024) in zero-shot setting (details in Appendix A.4.9)."
        },
        {
            "title": "4.2 FILTERING AND QUALITY",
            "content": "Z Filtering and Verification with BigDocs Toolkit. To ensure high-quality, open-access dataset, we employed an NSFW detector (e.g., Llama-Guard-3) and filtering tools to eliminate harmful content, corrupted images, misaligned annotations, and personally identifiable information (PII). The BigDocs Toolkit, adhering to ART principles, streamlined web crawling and filtering processes. This multi-layered approach, iteratively refined using typical errors identified during human verification, ensured clean and reliable dataset. The test set underwent manual human verification, with at least two annotators per sample to ensure quality and accuracy. Sampling-based checks on the training dhttps://arxiv.org/ ehttps://www.statista.com/ 7 Preprint. Under Review. split further validated the robustness of the filtering process, achieving 99% pass rate (details in A.13)."
        },
        {
            "title": "5 TRAINING MULTIMODAL MODELS ON BIGDOCS",
            "content": "We trained several state-of-the-art multimodal models of varying sizes and architectures on BigDocs to assess its effectiveness for document-based continual pretraining and downstream finetuning. For comparison, we also trained the models on DocStruct4M, the closest alternative in terms of scale and document-oriented tasks. Additionally, we experiment on the training set from BigDocs-Bench that requires generating long structure outputs, e.g., valid code outputs. We follow two stages of training: continual pretraining (CPT) and downstream finetuning (FT). The CPT stage involves training on large domain-specific datasets, such as BigDocs and DocStruct4M, learning general tasks like OCR, layout understanding, and captioning. For FT smaller datasets, such as DocDownStream and BigDocs-Bench, to focus on specific tasks like question answering or generating HTML from images. previous stage of pretraining (PT) is typically performed for general multimodal alignment. In our framework, we do not perform this stage and rely on publicly available checkpoints. While pretraining (PT) is typically performed for general multimodal alignment, we rely on public checkpoints instead. In CPT, we train the image encoder and connector to align image features with the LLM. For FT, both the connector and LLM remain unfrozen. See Figure 11 in Appendix A.5 for more details."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Baseline Models. We selected DocOwl1.5-8B (Hu et al., 2024), Qwen2VL-2B (Bai et al., 2023a), Phi-3.5-Vision-4B (Abdin et al., 2024), and LLaVa-NeXT-7B (Li et al., 2024) for our training experiments. These models were chosen due to their focus on document-related tasks (DocOwl1.5), their openness regarding checkpoints (Qwen2VL, DocOwl1.5), and their state-of-the-art performance and task generalization capabilities (LLaVa-NeXT, Phi-3.5). Training Details. We conduct all experiments using 8 nodes of 8 H100 GPUs, using Fully Sharded Data Parallel (FSDP) for distributed training. All experiments use batch size of 256 and learning rate of 2e-5, with AdamW as the optimizer. More training details are provided in Appendix A.5. Evaluation Benchmarks & Metrics. In addition to the newly introduced BigDocs-Bench, we assess performance on well-known document-oriented benchmarks, termed as General Document Benchmarks. We select these benchmarks for their relevance and diverse range of document tasks. DocVQA (Mathew et al., 2021b), InfoVQA (Mathew et al., 2021a), DeepForm (Svetlichnaya, 2020), KLC (Stanisławek et al., 2021) (Kleister Benchmark for Key information extraction), WTQ (Pasupat & Liang, 2015) (Wikipedia Tables), TabFact (Chen et al., 2020), ChartQA (Masry et al., 2022), TextVQA (Singh et al., 2019), MMMU (Yue et al., 2024), DUDE (Landeghem et al., 2023), SlideVQA (Tanaka et al., 2023), and TableVQA (Kim et al., 2024). We utilize (and extended) VLM Eval Kit (Duan et al., 2024). In BigDocs-Bench, we employ the following evaluation methods based on each tasks characteristics. For Screenshot2HTML, inspired by related works in this domain (Reis et al., 2004), we compute Tree Edit Distance (TE Dist.) between the Document Object Model (DOM) of ground truth and generation. For Table2LaTeX, we report TeXBLEU (Jung et al., 2024). For Image2SVG, we compare the cosine similarity between the DINOv2 (Oquab et al., 2023) representations of the ground-truth and generated SVG images (DINOScore). For the Image2Flow tasks, we propose the Length-Shape Triplet F1 (LST F1) score between ground-truth and generated flowcharts edge sets. More specifically, each edge is represented as (s, e, d) triplet, where is the edge label, and and are the source and destination nodes labels concatenated with their shapes, respectively. For Chart2Markdown, we adopt the RMSF1 metric for markdown tables (Liu et al., 2022; Masry et al., 2023; 2024). For summarization and VQA tasks, we report Rouge-L F1 score (Lin, 2004). For more details about the evaluation process, please refer to Appendix A.6. Setup. For our CPT on DocOwl1.5-8B and Qwen2VL-2B, we used base weights, while on Phi3.5-Vision-4B and Llava-NeXT-7B, we initialized from their instruction-tuned versions, since their base weights are not publicly available. We first trained each model on CPT corpus, either Preprint. Under Review. DocStruct4M (Hu et al., 2024) or BigDocs-7.5M, for one epoch. Following this, we performed further alignment (finetuning) using DocDownStream to enhance the models ability to follow instructions. For each selected model, we also evaluated the author-provided base model and the instruction-tuned version (separate from the base checkpoint, if available) as baselines and reported the performance on general document benchmarks. We also provide comprehensive evaluation on the proposed BigDocs-Bench. We conducted an off-the-shelf performance analysis on BigDocs-7.5M using models such as GPT4 (Achiam et al., 2023), Claude (Anthropic, 2024), Gemini Pro (Team et al., 2024a) and Qwen2VL-72B (Wang et al., 2024a) and Idefics2 (Laurençon et al., 2024a). In addition, we also evaluated the previously selected models on their instruction versions, including DocOwl1.5-8B, Qwen2VL-2B, Phi-3.5-Vision-4B, and Llava-NeXT-7B. To incorporate the new capabilities introduced in BigDocs-Bench, we further finetuned these models after BigDocs CPT using the training set from BigDocs-Bench (see Table 3). For each model, we only evaluate the instruction-tuned version (where available) as baselines, reporting their respective performance. Table 2: General Document Benchmarks. Models trained on {BigDocs-7.5M+DocDownstream} perform competitively across multimodal document benchmarks. We compare them to base checkpoints, instruction-tuned models, and those trained on {DocStruct4M+DocDownstream}. BigDocs models show consistent performance. Model DocOwl1.5-8B (instruct) DocOwl1.5-8B (base) DocOwl1.5-8B (base) + DocStruct4M DocOwl1.5-8B (base) + BigDocs (Ours) Qwen2-VL-2B (instruct) Qwen2-VL-2B (base) Qwen2-VL-2B (base) + DocStruct4M Qwen2-VL-2B (base) + BigDocs (Ours) Phi3.5-Vision-4B (instruct) Phi3.5-Vision-4B + DocStruct4M Phi3.5-Vision-4B + BigDocs (Ours) LLaVA-NeXT-7B (instruct) LLaVA-NeXT-7B + DocStruct4M LLaVA-NeXT-7B + BigDocs (Ours)"
        },
        {
            "title": "TEST",
            "content": "80.73 2.07 75.99 78.70 89.16 7.26 59.53 57.23 86.00 86.76 87.05 63.51 60.95 57.13 49.94 1.84 46.88 47.62 64.11 0.78 32.00 31.88 56.20 68.90 70.05 30.90 26.14 24.47 68.84 0.00 62.77 64.39 32.38 0.00 53.98 49.31 10.47 70.12 70.97 1.30 39.78 46."
        },
        {
            "title": "K L C\nTEST",
            "content": "37.99 0.00 35.21 36.93 25.18 0.00 36.38 34.39 7.49 37.83 37.45 5.35 28.34 31."
        },
        {
            "title": "W T Q\nTEST",
            "content": "38.87 0.00 32.86 35.69 38.20 0.00 28.48 31.61 17.18 51.30 51.21 20.06 25.90 27."
        },
        {
            "title": "M U\nVAL",
            "content": "M 79.67 0.00 71.56 72.65 57.21 0.00 64.24 64.75 30.43 82.12 81.24 52.83 67.72 72.58 68.56 0.00 68.36 65.80 73.40 0.00 54.44 68.60 82.16 79.76 81.56 52.12 61.20 54.72 68.91 0.00 65.08 67.30 79.90 1.14 55.89 61.01 73.12 68.60 68.72 65.10 52.25 49.06 33.67 24.44 33.67 32.33 42.00 34.89 34.89 35.67 46.00 44.11 45.00 38.89 25.78 17. SlideV A-M"
        },
        {
            "title": "Score",
            "content": "Avg. 34.64 19.07 29.00 32.55 45.23 28.43 28.78 27.19 37.20 35.52 36.15 17.94 21.70 22.88 31.62 3.30 27.03 29.60 46.50 14.55 22.68 17.46 30.93 31.90 32.47 7.46 15.33 16.07 52.60 13.63 46.27 49.03 43.07 0.00 46.53 47.53 70.70 69.17 67.77 32.87 27.03 33.13 53.84 5.36 49.56 51.05 53.03 7.25 43.15 43.89 45.66 60.51 60.80 32.36 37.68 37."
        },
        {
            "title": "5.2 QUANTITATIVE RESULTS",
            "content": "Results on Existing Document Downstream Tasks. Table 2 presents the models performance across general document benchmarks. Base models perform poorly, mainly due to their inability to follow user instructions, like answering questions. Phi3.5 Vision, originally optimized for reasoning tasks, achieves an average score of 60.80% when finetuned on BigDocs, enhancing its ability to handle complex document-based tasks. For Qwen2-VL, an interesting pattern emerges: while the instruction-tuned version excels on tasks reported in its technical report (e.g., DocVQA, InfoVQA, ChartQA, MMMU), the BigDocs-trained model surpasses it on new tasks like TabFact, DeepForm, KLC, and TableVQA, suggesting that Qwen2-VLs instruction-tuning may rely on complex prompt engineering and task-specific optimizations. We find that performing additional continual pretraining and finetuning on instruction-tuned models does not significantly degrade performance. Moreover, we observe substantial improvements on previously underperforming tasks. As shown in Table 2, Phi3.5 and LLaVA-Next show marked gains on DeepForm, KLC, WTQ, and SlideVQA. However, LLaVAs performance declined on MMMU, indicating the need for more multiple-choice question data. This reinforces our argument that transparency in training datasets is essential for proper evaluation. Finally, across all models, BigDocs training yields higher average scores compared to training on DocStruct4M, even with lower contamination rates on most benchmarks, as we highlighted in Section 3. These findings indicate that BigDocs supports better generalization and robustness without complex task-specific optimizations while also being license-permissive. Results on BigDocs-Bench Tasks. Table 3 shows results for our proposed downstream tasks, evaluating models ability to generate lengthy structured and valid code outputs, and reasoning from GUIs. Overall, BigDocs models consistently outperform both open and closed models on most tasks, particularly in Flow and GUI tasks such as GUI2UserIntent, GUI2Summary, GUI-VQA, and Image2Flow, revealing areas where existing models fall short. The performance gap is narrower on tasks like Screenshot2HTML, Image2SVG and Chart2Caption, suggesting these tasks have been 9 Preprint. Under Review. Table 3: Comparison of model performance in BigDocs-Bench. BigDocs models trained on {BigDocs7.5M+DocDownstream+BigDocs-Bench train-split}, which combine CPT and FT, outperform all baselines in tasks requiring long-format code generation, particularly in flow generation, GUI reasoning, and image-toLaTeX generation, surpassing even state-of-the-art closed models. The symbol denotes that the model required 1-shot prompts as opposed to the default 0-shot prompts . Chart2 R SF1 Chart2Cap. RO GE-L F1 Image2Flow (GraphViz)"
        },
        {
            "title": "LST",
            "content": "F1 Image2Flow (JSO N)"
        },
        {
            "title": "LST",
            "content": "F1 F1 UI2Sum. RO GE-L F1 UI2Intent RO GE-L Image2SV G"
        },
        {
            "title": "Score",
            "content": "Screenshot2H Table2Latex"
        },
        {
            "title": "D O M",
            "content": "Dist. TE F1 UI-V RO GE-L"
        },
        {
            "title": "Score",
            "content": "Avg."
        },
        {
            "title": "Model",
            "content": "Open Models DocOwl-1.5-8B Qwen2-VL-2B Phi3.5-V-4B LLaVA-NeXT-7B Idefics2-8B Llama-3.2.90B Qwen2-VL-72B Closed Models GPT-4o 20240806 Claude-3.5 Sonnet GeminiPro-1.5 BigDocs Models (ours) DocOwl-1.5-8B + BigDocs Qwen2-VL-2B + BigDocs LLaVA-NeXT-7B+ BigDocs Phi3.5-v-4B + BigDocs 0.08 41.17 60.64 22.00 25.34 45.21 70.47 66.70 54.81 76.63 74.43 72.25 72.78 84.01 18.69 22.88 21.88 20.67 20.95 20.60 19.42 25.23 23.59 25. 33.38 33.74 32.88 36.78 0.00 0.00 1.61 1.58 1.17 0.73 1.07 22.66 13.92 11.51 42.16 41.61 59.66 63.07 0.00 0.00 0.65 0.46 0.00 0.52 0.23 27.28 37.46 33.59 48.54 52.11 71.49 71.86 11.22 23.98 27.80 21.99 8.75 22.16 18.80 27.12 26.45 25.54 45.55 42.59 46.14 47.32 13.88 17.70 10.81 12.38 5.06 12.04 33.94 17.57 13.12 16. 89.15 71.65 79.55 86.91 3.58 23.18 34.57 20.53 37.73 45.97 54.43 60.34 25.46 15.21 33.66 33.51 60.63 34.65 3.50 6.46 4.25 5.00 3.56 7.32 10.03 10.33 9.70 7. 3.64 9.20 10.40 12.05 75.07 74.83 74.14 73.81 74.50 74.79 74.51 74.65 74.44 75.22 81.28 78.54 80.79 81.94 27.22 26.40 34.96 27.54 27.76 27.28 30.67 36.58 26.58 35. 43.46 33.97 40.67 44.81 15.32 23.66 27.13 20.60 20.48 25.66 31.36 36.84 30.55 32.32 49.52 46.92 55.50 56.34 Figure 5: Human evaluation results comparing Phi3.5 BigDocs-Bench against Phi3.5 Instruct and GPT-4o on two tasks: Table2LaTex (Left) and Screenshot2HTML (Right). explored in the literature but not extensively enough. Phi3.5-V4B + BigDocs stands out as the top performer in 8 tasks out of 10, with an average score of 50.46. However, its performance on Image2SVG (25.98 points behind LLaVA-NeXT-7B + BigDocs) indicates less exposure to SVG data in its instruction tuning. We find that the model can generate valid code outputs in different formats, including SVG, HTML, JSON, or Latex, when conditioned on images."
        },
        {
            "title": "5.3 HUMAN EVALUATIONS AND QUALITATIVE RESULTS",
            "content": "We conducted human evaluation comparing the performance of Phi-BigDocs, Phi-Instruct, and GPT-4o on Screenshot2HTML and Table2LaTeX tasks. Twenty-eight evaluators participated, providing 1,900 annotations. For Table2LaTeX, evaluators assessed if the LaTeX table matched the input table. For Screenshot2HTML, they evaluated the visual similarity between the rendered HTML and the screenshot. Evaluators chose between Win (one output was superior), Neither (both were poor), or Both (outputs were equally good). See Appendix A.8 for more details on the evaluation platform. From human evaluation results in Figure 5, for the Table2LaTeX task, Phi3.5 BigDocs wins 88% of the time against Phi3.5 Instruct and achieves 63% win rate, with 31% draw rate, against GPT-4o. These results highlight our models ability to accurately preserve the tables structure, including lines, borders, and margins, whereas GPT-4o often struggles to maintain consistent formatting despite capturing content accurately. In the Screenshot2HTML task, Phi3.5 BigDocs achieves 65% win rate against Phi3.5 Instruct and performs competitively against GPT-4o, with 36% win rate and 7% draw rate, demonstrating its strong capability to reproduce visual elements faithfully. Qualitative Results. We provide qualitative results in Appendix A.10. Figure 8 presents outputs from experiments with the Phi-3.5-Vision-4B model on BigDocs-Bench for tasks like 10 Preprint. Under Review. Chart2Markdown, Table2LaTeX, and Image2SVG. The model delivers visually consistent outputs and generates valid code across formats, adhering to task instructions. Table 6 compares sample outputs between Phi-3.5-Vision models and GPT-4o, highlighting the strong performance of our BigDocs trained version in captioning and VQA tasks."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce BigDocs-7.5M, large-scale, license-permissive dataset for training multimodal models on document and code-related tasks. Along with comprehensive suite of tools and data analysis, we present BigDocs-Bench, featuring 10 downstream tasks that assess models ability to generate long-format code outputs from images. These tasks serve as practical benchmarks for real-world applications. Our experiments show that models trained on BigDocs outperform those trained on existing datasets. Furthermore, training on the BigDocs-Bench train split endows the resulting models with new capabilities and significantly enhances their ability to generate long, structured outputs. All BigDocs artifacts will be freely available under permissive licenses. Limitations Our work presents pioneering license-permissive dataset for multimodal document understanding, achieving strong performance across tasks. However, there are limitations: (1) Suboptimal performance on some public benchmarks, indicating need to refine the data mixture and explore additional sources. (2) Limited context length, as models are trained with maximum of 8192 tokens, restricting performance on tasks with long, structured outputs like HTML and SVG. (3) Uncertainty in the commercial viability of base models, as their pretraining data lacks transparency."
        },
        {
            "title": "ETHICS AND REPRODUCIBILITY STATEMENT",
            "content": "Ethics Statement Our work is centered around responsible and transparent curation of datasets for multimodal document understanding models. While we have made extensive efforts to filter harmful content from our dataset, we cannot fully guarantee that the models will not generate offensive language. Additionally, we have taken significant steps to remove personally identifiable information (PII) from the compiled datasets to protect user privacy. However, we cannot ensure that the generated code will be free of malicious snippets, and developers are encouraged to implement protection protocols to safeguard against potential risks. Finally, all human evaluation studies were conducted by collaborator researchers, and no PII was collected during this process. Reproducibility Statement We are committed to ensuring the reproducibility of our work by providing all necessary details and resources. All artifacts, including code, datasets, model weights, data sheets, and metadata, will be publicly released. Furthermore, we have fully documented all hyperparameters, experimental setups, and evaluation metrics to allow for accurate replication of our results. For human evaluation, we provide clear instructions and describe the environment used for comparison to ensure transparency and consistency."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We sincerely thank Ghazwa Darwiche, Christian Hudon, Tom Murray, and Fanny Rancourt for their valuable administrative and technical support. We also appreciate Aishwarya Agarwal for her insightful technical discussions. Furthermore, we acknowledge MITACS for supporting the recruitment of visiting student researchers who played significant role in this project. Yoshua Bengio is partially supported by Microsoft and Samsung."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 11 Preprint. Under Review. Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. Credit Mutuel Arkea. Aftdb dataset, 2024. URL https://huggingface.co/datasets/ cmarkea/aftdb. Hugging Face Dataset. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv: 2309.16609, 2023a. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023b. Jonas Belouadi, Anne Lauscher, and Steffen Eger. Automatikz: Text-guided synthesis of scientific vector graphics with tikz. In IEEE Conference Computer Vision Pattern Recognition, 2023. Jonas Belouadi, Simone Paolo Ponzetto, and Steffen Eger. Detikzify: Synthesizing graphics programs for scientific figures and sketches with tikz, 2024. Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587604, 2018. Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang. The foundation model transparency index. arXiv preprint arXiv:2310.12941, 2023. Rishi Bommasani, Kevin Klyman, Sayash Kapoor, Shayne Longpre, Betty Xiong, Nestor Maslej, and Percy Liang. The foundation model transparency index v1.1. arXiv preprint arXiv:2310.12941, 2024. Łukasz Borchmann. Notes on applicability of gpt-4 to document understanding. arXiv preprint arXiv:2405.18433, 2024. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing webscale image-text pre-training to recognize long-tail visual concepts. In IEEE Conference Computer Vision Pattern Recognition, 2021. Ritwick Chaudhry, Sumit Shekhar, Utkarsh Gupta, Pranav Maneriker, Prann Bansal, and Ajay Joshi. Leaf-qa: Locate, encode & attend for figure question answering. In Winter Conference on Applications of Computer Vision, 2019. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. Tabfact: large-scale dataset for table-based fact verification. In International Conference Learning Representations, 2020. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 12 Preprint. Under Review. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Annual Meeting of the Association for Computational Linguistics, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference Learning Representations, 2024. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Wikimedia Foundation. Wikimedia downloads, 2024. URL https://dumps.wikimedia.org. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alexandros G. Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. In Adv. Neural Information Processing Systems, 2023. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12): 8692, 2021. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: referencefree evaluation metric for image captioning, 2022. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: dataset for form understanding in noisy scanned documents. In International Conference on Document Analysis and RecognitionWorkshop, 2019. Kyudan Jung, Nam-Joon Kim, Hyongon Ryu, Sieun Hyeon, Seung jun Lee, and Hyeok jae Lee. Texbleu: Automatic metric for evaluate latex format. arXiv preprint arXiv: 2409.06639, 2024. Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In IEEE Conference Computer Vision Pattern Recognition, 2018. Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. In International Conference Learning Representations, 2017. Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024. 13 Preprint. Under Review. kleister Charity. kleister charity dataset, 2021. URL https://github.com/applicaai/ kleister-charity. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. Transactions Machine Learn Research, 2022. Jordy Van Landeghem, Rubén Tito, Łukasz Borchmann, Michał Pietruszka, Paweł Józiak, Rafał Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Ackaert, Ernest Valveny, Matthew Blaschko, Sien Moens, and Tomasz Stanisławek. Document understanding dataset and evaluation (dude). In International Conference Computer Vision, 2023. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024a. Hugo Laurençon, Léo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv preprint arXiv:2403.09029, 2024b. Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv: 2405.02246, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv: 2408.03326, 2024. Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. Tablebank: benchmark dataset for table detection and recognition. arXiv preprint arXiv:1903.01949, 2020a. Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: benchmark dataset for document layout analysis. In International Conference on Computational Linguistics, 2020b. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! Transactions Machine Learn Research, 2023. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out. Association for Computational Linguistics, 2004. Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. Deplot: One-shot visual language reasoning by plot-to-table translation. arXiv preprint arXiv:2212.10505, 2022. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Adv. Neural Information Processing Systems, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al. Kosmos-2.5: multimodal literate model. arXiv preprint arXiv:2309.11419, 2023. Preprint. Under Review. Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Dublin, Ireland, May 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.acl-short.18. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. UniChart: universal vision-language pretrained model for chart comprehension and reasoning. In Empirical Methods in Natural Language Processing, 2023. Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. ChartInstruct: Instruction tuning for chart comprehension and reasoning. In Annual Meeting of the Association for Computational Linguistics, 2024. Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. Jawahar. Infographicvqa. In Winter Conference on Applications of Computer Vision, 2021a. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images. In Winter Conference on Applications of Computer Vision, 2021b. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, 2019. OpenAI. GPT-4V(ision) System Card. https://cdn.openai.com/papers/GPTV_ System_Card.pdf, September 2023. Accessed: 2023-11-05. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. In Adv. Neural Information Processing Systems, 2011. Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk In Adv. Neural Information Lee. Cord: consolidated receipt dataset for post-ocr parsing. Processing Systems Workshop, 2019. Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Annual Meeting of the Association for Computational Linguistics, 2015. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference Machine Learning, 2021. Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. D. C. Reis, P. B. Golgher, A. S. Silva, and A. F. Laender. Automatic web news extraction using tree edit distance. In International Conference on World Wide Web, 2004. 15 Preprint. Under Review. Juan Rodriguez, Shubham Agarwal, Issam Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: Generating scalable vector graphics code from images. arXiv preprint arXiv:2312.11556, 2023a. Juan Rodriguez, David Vazquez, Issam Laradji, Marco Pedersoli, and Pau Rodriguez. Figgen: Text to scientific figure generation. In International Conference Learning Representations, 2023b. Juan Rodriguez, David Vazquez, Issam Laradji, Marco Pedersoli, and Pau Rodriguez. Ocr-vqgan: Taming text-within-image generation. In Winter Conference on Applications of Computer Vision, 2023c. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics, 2018. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioningwith reading comprehension. In European Conference Computer Vision, 2020. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In IEEE Conference Computer Vision Pattern Recognition, 2019. Amanpreet Singh, Niranjan Balasubramanian, and B. Open4business(o4b): An open access dataset for summarizing business documents. arXiv preprint arXiv:2011.07636, 2020. Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In IEEE Conference Computer Vision Pattern Recognition, 2021. Brandon Smock, Rohith Pesala, and Robin Abraham. Pubtables-1m: Towards comprehensive table extraction from unstructured documents. In IEEE Conference Computer Vision Pattern Recognition, 2022. Ian Soboro. Complex document information processing (cdip) dataset. https://doi.org/10. 18434/mds2-2531, 2022. Accessed: 2024-06-20. Tomasz Stanisławek, Filip Gralinski, Anna Wróblewska, Dawid Lipinski, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and Przemysław Biecek. Kleister: key information extraction datasets In International Conference on Document involving long documents with complex layouts. Analysis and Recognition, 2021. Svetlichnaya. Deepform: Understand structured documents at scale, 2020. Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Association for the Advancement of Artificial Intelligence, 2023. Simon Tannert, Marcelo G. Feighelstein, Jasmina Bogojeska, Joseph Shtok, Assaf Arbelle, Peter W. J. Staar, Anika Schumann, Jonas Kuhn, and Leonid Karlinsky. FlowchartQA: The first large-scale benchmark for reasoning over flowcharts. In Piush Aggarwal, \"Ozge Alaccam, Carina Silberer, Sina Zarriess, and Torsten Zesch (eds.), Proceedings of the 1st Workshop on Linguistic Insights from and for Multimodal Language Processing, pp. 3446, Ingolstadt, Germany, September 2023. Association for Computational Lingustics. URL https://aclanthology. org/2023.limo-1.5. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, and Others. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2024a. Preprint. Under Review. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yifan Ding, Xinyang Geng, Fred Alcober, Roy Frostig, Mark Omernick, Lexi Walker, Cosmin Paduraru, Christina Sorokin, Andrea Tacchetti, Colin Gaffney, Samira Daruki, Olcan Sercinoglu, Zach Gleicher, and Others. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024b. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. Caitlin Vogus and Emma Llansóe. Making transparency meaningful: framework for policymakers. Center for Democracy and Technology, 2021. Štˇepán Šimsa, Milan Šulc, Michal Uˇriˇcáˇr, Yash Patel, Ahmed Hamdi, Matˇej Kocián, Matyáš Skalický, Jiˇrí Matas, Antoine Doucet, Mickaël Coustaty, and Dimosthenis Karatzas. Docile benchmark for document information localization and extraction. In International Conference on Document Analysis and Recognition, 2023. Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In ACM Symposium on User Interface Software and Technology, 2021. Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. Docllm: layout-aware generative language model for multimodal document understanding. arXiv preprint arXiv: 2401.00908, 2023a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark, 2024b. Zilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, and Sandeep Tata. Vrdu: benchmark for visually-rich document understanding. In International Conference on Knowledge Discovery & Data Mining, 2023b. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In IEEE Conference Computer Vision Pattern Recognition, 2024. Tianyu Zhang, Suyuchen Wang, Lu Li, Ge Zhang, Perouz Taslakian, Sai Rajeswar, Jie Fu, Bang Liu, and Yoshua Bengio. Vcr: Visual caption restoration. arXiv preprint arXiv: 2406.06462, 2024. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Preprint. Under Review. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In ACM International Conference on Multimedia, 2022."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 BIGDOCS-7.5M TASKS 1. OCR-Related Tasks: These tasks involve converting images of text (e.g., scanned documents) into machine-readable formats, including transformations such as bounding-box-totext and text-to-bounding-box (text localization as described in Hu et al. (2024)). Models learn to recognize and map textual information within images. 2. Structured Parsing and Extraction: This task focuses on extracting and transforming structured data from documents, like parsing tables, forms, and charts into formats such as JSON or Markdown. It includes handling documents with complex visual layouts and sometimes incorporates bounding box information for individual elements. 3. Captioning and Summarization: This task requires models to generate captions or summaries for visual or textual content, such as figures, charts, or document sections. The models provide concise descriptions or overviews, enhancing document comprehension. 4. Question-Answering (QA): QA tasks involve responding to questions posed over structured or visual data (e.g., tables, figures). This includes multi-turn QA, where models address series of related questions to improve their reasoning and comprehension. A.2 DATASETS INCLUDED IN BIGDOCS-7.5M The following datasets are utilized in our work, supporting the four core tasks mentioned in Section 3.2.1. We reference the task supported by each dataset via the index in Section A.1. Note that datasets with * are those fully or partially curated by us (details explained in the description of each of them). 1. TabFact (Chen et al., 2020): [Task included: (2), (4)] TabFact is used for question-answering tasks, where models check whether given statement is supported or refuted by table. It also involves structured parsing, helping models extract and process structured table data into formats like Markdown. 2. Open4Business (O4B) (Singh et al., 2020): [Task included: (2), (3)] O4B is dataset focused on business-related documents and is processed for structured parsing and extraction, as well as captioning and summarization, allowing models to retrieve key insights from documents and generate summaries or descriptions. 3. WikiTQ (Pasupat & Liang, 2015): [Task included: (2), (4)] WikiTableQuestions is used for question-answering tasks over tables, where models answer questions based on table data, and structured parsing for converting table data into Markdown. 4. CORD (Park et al., 2019): [Task included: (1), (2)] CORD is dataset for parsing receipts, used for both OCR-related tasks and structured parsing and extraction, helping models interpret structured financial data from document layouts. This latter task requires extracting entities and providing their text, category, and location as JSON output. 5. UniChart (Masry et al., 2023): [Task included: (2), (4)] UniChart is used for structured parsing and extraction to extract structured information from chart-like tables, converting complex visual layouts into formats like JSON or Markdown. And also for questionanswering tasks, where models answer questions related to the chart content. 6. TextOCR (Sidorov et al., 2020): [Task included: (1)] TextOCR is processed for OCR-related tasks, enabling models to perform bounding-box-to-text transformations on scene text from images. 18 Preprint. Under Review. 7. COCO-Text (Veit et al., 2016): [Task included: (1)] COCO-Text is used for OCR-related tasks, helping models extract and recognize text from real-world images with natural scene text. 8. CDIP-1M*: [Task included: (1), (2)] To create this dataset, we sourced from the original IIT-CDIP (Soboro, 2022) dataset and curated it into CDIP-1M. CDIP-1M is processed for OCR-related tasks and structured parsing, focusing on extracting text and structure from large-scale scanned document collections. We used an in-house OCR engine to get the text (i.e. annotations) from its 11M documents from the source. Like the text localization task in DocStruct4M Hu et al. (2024), we generate word-, lineand block-level bounding-box-totext and text-to-bounding-box QA pairs. We subsample zones randomly, and based on OCR confidence, large fraction of the images are pretty noisy. In addition, for the block level, we generate structured parsing QA pairs where text lines and their location need to be given as JSON by the model. 9. PubTables-1M (Smock et al., 2022): [Task included: (1), (2)] PubTables-1M is large dataset of tables from scientific papers. It is used for OCR-related tasks to extract information in tables. It is also processed for structured parsing and extraction, allowing models to handle scientific tables and convert them into markdown. 10. FigureQA (Kahou et al., 2017): [Task included: (4)] FigureQA is focused on questionanswering tasks, where models answer questions based on charts and figures, improving reasoning over visual and tabular data. 11. DocBank (Li et al., 2020b): [Task included: (2), (4)] DocBank is utilized for structured parsing and extraction and question-answering tasks, enabling models to interact with scholarly documents and extract structured data from layouts. 12. TableBank (Li et al., 2020a): [Task included: (2)] TableBank is large dataset used for structured parsing and extraction, helping models parse table structures from both Word and LaTeX documents into structured formats. 13. OCRVQA (Mishra et al., 2019): [Task included: (1),(4)] OCRVQA focuses on OCR-related tasks and question-answering tasks over OCR-extracted text, where models answer questions based on text and visual data from real-world scenes. 14. Datikz (Belouadi et al., 2023): [Task included: (2), (3)] Datikz is processed for captioning and summarization as well as structured parsing, enabling models to describe and interpret complex diagrams and generate structured data from them. 15. ArxivOCR*: [Task included: (1)] The ArxivOCR dataset contains OCR-scanned academic papers and is used for OCR-related tasks, where models perform bounding-box-to-text transformations, improving accessibility and structure for scholarly articles. This dataset is curated by us. We filter out the papers from Arxiv that have permissive licenses, i.e. CC-BY 4.0 and CC0 in this case. Then we use in-house OCR engines to produce OCR results on the pages from the papers collected. 16. ArxivTableCap*: [Task included: (3)] The ArxivTable dataset focuses on generating captions for the tables and figures extracted from Arxiv papers, helping models describe the content and context of tables in academic papers. Among the 156.2k samples, 70k of them are from AFTdb Arkea (2024). We perform the filtering to make sure all the selected ones are in papers with permissive licenses, i.e. CC-BY 4.0 and CC0 in this case. 17. SVGCap Dataset (Rodriguez et al., 2023a): [Task included: (3)] The SVG Dataset supports captioning and summarization tasks, where models generate descriptive captions for SVG content, summarizing the structure and elements of vector graphics. A.3 ASSESSING CONTAMINATION BigDocs-7.5Ms direct dependency on TabFact, WTQ, and TextVQA is restricted to their training splits, and DocStruct4M reports similar dependency on DocVQA, InfoVQA, DeepForm, KLC, ChartQA, TabFact, WTQ, and TextVQA. However, overlaps between either of these training sets and evaluation datasets may emerge through indirect means (e.g., the same source material was involved). Our primary automatic approach to assess contamination consists of embedding all images from the reference dataset (i.e., BigDocs-7.5M or DocStruct4M) using pretrained CLIP (Radford et al., 2021), Preprint. Under Review. (a) InfoVQA, SlideVQA-M, and TabFact overlap very little with BigDocs-7.5M (train). (b) Zoom on the above (same legend). Figure 6: Cumulative distribution by cosine similarity. Each curve shows the proportion of samples in an evaluation dataset for which there exists at least one sample in the reference dataset with cosine similarity higher than the specified threshold. Lower values are better; higher cosine similarity thresholds are more pertinent. Based on human evaluations, samples with cosine similarity < 0.96 are unlikely to be problematic, whereas those > 0.98 are most likely problematic. Except for MMMU, and DudeMini, BigDocs-7.5M appears less contaminated than DocStruct4M. Table 4: Detailed Results on Contamination Experiments, related to Figure 6. Lower values are better; higher cosine similarity thresholds are more pertinent. Except for MMMU and DudeMini, BigDocs-7.5M appears to be less contaminated by these metrics than DocStruct4M. Figure 3s data comes from this table."
        },
        {
            "title": "Similarity\nThreshold\nCosine",
            "content": "0.99 0.98 0.97 0.96 0."
        },
        {
            "title": "Reference Dataset",
            "content": "DocStruct4M BigDocs-7.5M DocStruct4M BigDocs-7.5M DocStruct4M BigDocs-7.5M DocStruct4M BigDocs-7.5M DocStruct4M BigDocs-7.5M"
        },
        {
            "title": "DocV Q A",
            "content": "0.016 0.0036 0.065 0.012 0.14 0.049 0.26 0.12 0.39 0.24"
        },
        {
            "title": "InfoV Q A",
            "content": "0.0 0.0 0.0032 0.0 0.0032 0.0 0.0057 0.0 0.0057 0.0"
        },
        {
            "title": "K L C",
            "content": "0.037 0.0 0.20 0.0 0.41 0.0 0.57 0.033 0.72 0.21 1500 300 0.0033 0.0 0.061 0.0033 0.16 0.043 0.24 0.12 0.30 0.22"
        },
        {
            "title": "W T Q",
            "content": "0.0 0.0 0.0 0.0 0.0064 0.0 0.031 0.0 0.062 0.0"
        },
        {
            "title": "TabFact",
            "content": "0.00039 0.0 0.036 0.0 0.089 0.0 0.14 0.0 0.20 0.0"
        },
        {
            "title": "TextV Q A",
            "content": "0.034 0.025 0.36 0.10 0.55 0.17 0.63 0.28 0.68 0.45 2500 1509 0.0 0.0 0.0020 0.0 0.0044 0.0 0.0096 0.0 0.012 0.0"
        },
        {
            "title": "M U",
            "content": "M 0.0026 0.0 0.0044 0.0035 0.0053 0.0061 0.0070 0.021 0.015 0.045"
        },
        {
            "title": "DudeMini",
            "content": "SlideV A-M"
        },
        {
            "title": "TableV Q A",
            "content": "0.0089 0.0089 0.013 0.011 0.048 0.053 0.15 0.14 0.28 0.22 5275 609 0.0 0.0 0.0 0.0 0.0014 0.0 0.0014 0.0 0.0014 0.0012 19600 3596 0.0027 0.0 0.0067 0.0053 0.0067 0.0053 0.017 0.0087 0.04 0.023 1500 751 20 Preprint. Under Review. (a) With respect to BigDocs-7.5M; top 200 samples human-labeled. (b) With respect to DocStruct4M; top 100 samples human-labeled Figure 7: Human evaluation of DocVQAs overlap. Among the 1284 unique images in DocVQAs samples, we use the same cosine similarity method as in figure 6 to identify the samples that are the most similar to samples in the corresponding training set. Although we prioritize using only the closest match, we also retrieve the second and third closest matches after deduplication (i.e., if the next closest match is identical to the previous match, skip it). human is then tasked to label the top matches as either different, same template or same document (see text for definitions). Counts are provided for the most relevant 0.01-wide cosine similarity intervals. Most samples below 0.96 are different, and most samples above 0.98 are not. From these numbers, we expect less than 10% of the samples with cosine similarity below 0.97 to be same template, and less than 1% of the samples with cosine similarity below the same threshold to be same document. All identified same document are found at the first rank. There is only one instance where the first rank is different but same template is identified at higher rank. namely clip-ViT-B-32 from sentence-transformers (Reimers, 2019). We similarly embed each image from an evaluation dataset and retrieve the reference image with the highest cosine similarity: the closer this measure is to 1.0, the more likely it is that the evaluation image is part of the reference dataset. Figures 3 and 6, as well as table 4 all report these values. Except when the cosine similarity is exactly 1.0 indicating an exact match interpretations of these scores must be grounded in human evaluations. However, assessing whether two images are the same can be non-trivial task, even for human eyes. Consider the following edge cases: receipts for recurring orders emitted on different days; the same form filed by different people; different versions of the same form at the same company; and different full-page table from the same appendix of the same report. Technically, all these cases involve pairs of different documents. However, one could argue that training on one such samples confers an unfair advantage to model evaluated on the other sample. For this reason, we annotate such instances as same template whenever we encounter them. 21 Preprint. Under Review. Conversely, the actual same document can appear as quite diversified images. Indeed, stamps, watermarks, censorship, and/or annotations may be apposed posteriori to document, in addition to scaling, crops, rotations, and fax-induced artifacts. In all these cases, the original intent to communicate the same information matters: different copies of the same memorandum, scanned separately, are here treated as the same document. With these definitions in hand, human is tasked with labeling the samples that are most likely to be problematic in the DocVQA evaluation dataset. Figure 7 reports the results, calibrating how we interpret the cosine similarities for other evaluation datasets. Note that this labeling procedure does not alter the composition of BigDocs-7.5M: we leave all samples, problematic or not, in the dataset. Instead, we release the annotations, which may help the community develop an intuition of the overlaps that may not have been identified yet, or even enable better detection strategies in the future. A.4 DESCRIPTION OF DOWNSTREAM TASKS PROPOSED IN BIGDOCS-BENCH The following is formal description of the downstream tasks we aim to solve using the proposed BigDocs-Bench dataset. A.4.1 IMAGE2FLOW The task at hand is an image-to-flow conversion, where the input is an image of flowchart, and the output is the corresponding information in JSON format. This JSON includes the nodes and edges that represent the flowcharts structure. Formally, given an image representing flowchart, the goal is to extract graph = (V, E) where is the set of nodes, and is the set of directed edges between these nodes. The output JSON(G) contains two main components: (1) list of nodes with their corresponding attributes such as node ID, label, shape, and description, and (2) list of edges with their source node ID, destination node ID, and label (if any). The dataset for this task consists of 10,000 samples. Each sample includes an image of flowchart, its corresponding Graphviz file, and JSON representation. The dataset was generated using the following pipeline: First, random number between 5 and 15 was generated to determine the number of nodes in each flowchart. Based on the total number of nodes, distribution was assigned to different types of nodes, such as conditions (diamond shape), statements (rectangular nodes), and others (parallelogram, circle). random flow direction was then selected from the options: BT (bottom to top), TB (top to bottom), LR (left to right), and RL (right to left). Using the LLaMA 3.1 model, we used the prompt in Table A.4.1 to generate Graphviz file. ı Prompt used with Llama 3.1 to generate Graphviz data {total_nodes}. Create directed flowchart graph in DOT format with the following specifications: - The direction of the graph should be {direction}. - Total number of nodes: - Nodes distribution: {nodes}. - The edges should connect the nodes in way that makes sense: * Each node should have at least one outgoing edge. * For diamond nodes, there should be at least two outgoing edges. The graph can include any colors or additional styling. Generate valid and coherent graph. represent an enterprise-level workflow like the steps of an incident resolution workflow. labels are so important. format no need for any python code or any description about the graph. The enterprise-level workflow Just give me the graph in Graphviz"
        },
        {
            "title": "The graph should",
            "content": "22 Preprint. Under Review. Figure 8: Qualitative Results. Generations of our Phi3.5-Vision model on the presented document downstream tasks, as part of the test set of BigDocs-Bench. We show samples of Chart2Markdown, Table2LaTex, Image2SVG, and Image2Flow (JSON). single model trained on BigDocs datasets can generate image-conditioned code in different coding languages while providing valid outputs. Random colors and styles were applied to remove model biases toward specific stylings. The generated Graphviz files (.gv) were then converted to PNG images and JSON files. The JSON files contain two main components: Each node has an ID, label, shape, and description of the shape (e.g., diamonds represent conditions). Each edge contains the source and destination node IDs and label if present (the label represents the text on the edge). A.4.2 SCREENSHOT2HTML The Screenshot-to-HTML conversion task involves transforming an image of websites layout, such as screenshot, into the corresponding HTML code that accurately reconstructs the structure and content of the original website. This process enables the generation of functional website solely from its visual representation, facilitating applications in web design automation, accessibility enhancements, and rapid prototyping. Formally, given an input image depicting the visual layout of website, the objective is to generate the HTML structure that includes essential web elements such as headers, paragraphs, images, links, forms, and navigation bars. The resulting HTML code HTML(H) should not only replicate the visual appearance of the original website but also ensure semantic correctness and structural integrity, enabling the recreated website to be interactive and accessible. Data Collection and Filtering Process. The Image-to-HTML dataset was curated through an automated pipeline designed to ensure the quality and diversity of website layouts. Utilizing the Playwright library, the system asynchronously retrieves and renders websites from comprehensive list of URLs (Penedo et al., 2024). Each website undergoes series of checks to verify its accessibility, compliance with robots.txt directives, and predominance of English content. Additionally, content appropriateness is assessed by filtering out websites containing NSFW language. External CSS and JavaScript resources are inlined to maintain consistency and reduce dependencies, and unnecessary or oversized scripts are removed. Images are replaced with placeholders, and background images are eliminated to focus on structural elements. The viewport is adjusted to capture only the visible portion of each page, enhancing the clarity of the layout representations. Websites are further evaluated based on performance and structural metrics, including load time, page size, number of JavaScript and CSS files, DOM depth, and total number of HTML elements. Technologies and frameworks used by each website are identified to exclude those utilizing disallowed technologies, e.g., allowing website to render without JavaScript. We also removed comments and prettified the HTML for standardization. Only websites that meet all predefined criteria are included in the final dataset. 23 Preprint. Under Review. Dataset Statistics. The resulting dataset comprises 11,000 website samples, each with highresolution screenshot and its corresponding HTML representation. On average, each layout contains 20.3 HTML elements, reflecting diverse range of website designs. This diversity provides robust foundation for training and evaluating image-to-HTML conversion models, ensuring the dataset is both comprehensive and representative of various web structures. A.4.3 TABLE2LATEX The task involves identifying and associating tables in academic PDFs with their corresponding LaTeX code and captions. The aim is to precisely match each table image with the LaTeX source used to generate it and the relevant caption, ensuring accurate alignment between the visual content and its textual description. More specifically, given table image I, the LaTeX code used to render the table, and the caption Tcaption, the goal is to create dataset Dataset(I, C, Tcaption) that establishes reliable association between the tables, their LaTeX source, and their descriptive captions. We crawled publicly available, license-compliant arXiv papers to create this dataset, collecting both their PDFs and associated TeX source files. We began by parsing the TeX files to extract the LaTeX code for tables and their captions. Next, we used the PDF parsing library PyMuPDF to locate tables within the PDFs. However, this table detection process proved to be imperfect, as the algorithm frequently misidentified content with parallel lines as tables, leading to false positives. To address this challenge, we adopted an alternative approach. Instead of cropping tables directly from the PDFswhere false positives were commonwe chose to render the parsed LaTeX table code to generate accurate table images. This method ensured that the images faithfully represented the original table formatting, reducing detection errors and improving the reliability of the dataset. As result, we created high-quality dataset comprising over 95,000 table images, each paired with its corresponding LaTeX code and caption, providing valuable resource for further research into table structures in academic papers. A.4.4 IMAGE2SVG & TEXT2SVG Scalable Vector Graphics (SVG) provide precise alternative to pixel-based images, capable of representing diagrams, icons, plots, and graphic designs with superior detail and scalability. Unlike raster images, SVGs can be scaled to any resolution without losing quality. In this work, we introduce the task of image-to-SVG generation, where the goal is to process an input image and generate SVG code that closely resembles the image upon rendering. This task requires advanced parsing capabilities for textual and numerical data and an understanding of various shapes, such as squares and arrows, commonly found in diagrams. Given an input image I, the model outputs SVG code that visually replicates the image. Additionally, the task can extend to scenarios where textual description serves as input, yielding SVG code that aligns with the described content. We curate dataset of images, SVG codes, and texts sourced from the SVG-Stack dataset introduced by StarVector (Rodriguez et al., 2023a). The curation process involves filtering to ensure high-quality samples. First, we filter based on image entropy to select images with complex designs and intricate details, excluding simpler icons or shapes. Second, we compute the CLIP Score (Hessel et al., 2022; Radford et al., 2019) for image-text pairs and retain the top 100k examples to build our curated SVG dataset. The SVG-Stack dataset adheres to permissive licensing standards, originating from TheStack (Kocetkov et al., 2022), carefully designed for open and permissive use. A.4.5 CHART2MARKDOWN This dataset is novel contribution of our work, designed to assess the models capabilities in extracting data values from chart images. In this task, the model is given chart image and asked to produce data table of the underlying data table in markdown format. To create this dataset, we crawled recent chart images from the Statista website f, focusing on charts from 2023 and 2024 that were not used in prior datasets like UniChart Masry et al. (2023) and ChartQA Masry et al. (2022). This ensures that the dataset reflects the most up-to-date facts and fhttps://www.statista.com/ 24 Preprint. Under Review. trends and overlaps less with existing datasets and benchmarks. We collected 6,516 chart images, their corresponding data tables, and human-written summaries. A.4.6 CHART2CAPTION The task at hand is chart-to-caption conversion. The input consists of an image of chart along with the code used to generate it and the datasets name and description. The output is textual caption of the important insights and information conveyed by the chart. Formally, given chart image I, the code used to generate the chart, and the dataset information D, the goal is to produce caption Caption(I, C, D) that highlights key insights and information represented in the chart. The dataset for this task consists of 1,496 pairs of chart images and the corresponding code that generated these charts. The data was collected by selecting various Kaggle public datasets and their associated data analytics notebooks. We executed these notebooks locally and parsed their outputs to generate the chart-image and code pairs. To generate the captions, we used the prompt below with the provided chart image, code, dataset name, and description. The caption was generated by using InterVL2-26B (Chen et al., 2023; 2024). This process allows us to leverage the models capabilities to generate meaningful captions based on the provided context of the chart, code, and dataset description. ı Prompt used with InternVL2-26B to generate chart summaries"
        },
        {
            "title": "This is a notebook with name",
            "content": "You are powerful data analyst. \"{dataset_name}\". In the description of this dataset, its told that: \"{dataset_description}\". You are seeing plot from this notebook. Here is the code that was used to generate this plot: {code} Now as data analyst, summarize the important insights and information about the chart. A.4.7 GUI2USERINTENT The GUI2UserIntent task tests the abilities of grounding on GUI. Concretely, given the bounding box coordinate clicked by the user, the goal is to identify the text element the user intends to interact with. While this is similar to the GUI grounding task introduced in Cheng et al. (2024), which predicts the bounding box based on user query, it differs in its focus on interpreting user clicks. The datasets are curated by repurposing the pretraining dataset for SeeClick (Cheng et al., 2024). The original dataset includes clickable text elements and their bounding box coordinates in website screenshots. We directly extracted them to curate the GUI-to-UserIntent dataset. A.4.8 GUI2SUMMARY The GUI2Summary task is similar to the Chart-to-Summary task in NovelCharts; however, instead of charts, the input images are website screenshots. Unlike existing UI summary datasets like Screen2Words (Wang et al., 2021) that focus on short, phrase-level summaries, our GUI-to-Summary dataset provides paragraph-level summaries. These summaries are comprehensive, providing brief overview of the main content, referencing key visual and textual elements in the screenshots, and additional aspects like layout and color schemes. To synthesize data for the GUI-to-Summary task, we used InternVL2-8B (Chen et al., 2023), prompting it with website screenshots to create concise descriptions of the main content and layout. Appendix A.4.8 shows the specific prompt used for data generation. ı Prompt used with InternVL2-8B to generate website screenshot summaries Summarize this website in less than 100 words. content, layout, color, and other style elements."
        },
        {
            "title": "Cover the main",
            "content": "25 Preprint. Under Review. Figure 9: GUI-to-Summary and GUI-VQA introduced in BigDocs-Bench. Figure 10: Chart-to-Markdown task introduced in BigDocs-Bench. A.4.9 GUI-VQA In the GUI-VQA task, the model answers questions about website screenshots, covering the overall content and specific elements like buttons, text boxes, and menu items. This task requires recognizing key components within the GUI and understanding their functionalities and interdependencies. We synthesized data for GUI-VQA using the GUI-to-Summary dataset. Specifically, we sampled sentence from each summary and prompted LLaMA 3.1-8b (Dubey et al., 2024) to convert it into QA pair. The prompt is shown in Appendix A.4.9. ı Prompt used with InternVL2-8B to convert summary to QA pair. You will be provided with sentence. it into question-answer pair. on factual information and avoid subjective inquiries. not generate Yes/No questions. format: Q: {question} A: {answer}"
        },
        {
            "title": "Your task is to convert",
            "content": "26 Preprint. Under Review. Figure 11: Training Stages for BigDocs. Three-Stage Training Pipeline for multimodal Document Understanding with BigDocs. Our approach consists of: (1) General Vision-Language Pretraining, (2) Document-Specific Continual Pretraining, and (3) Supervised Finetuning for Document Tasks. We evaluate the impact of BigDocs by using checkpoints of models after Stage 1, as provided by their original authors, and comparing them with models that undergo all three stages. Performance is assessed based on standard document tasks and novel tasks, including HTML/SVG code generation, flowchart generation/parsing, and LaTeX interpretation. A.5 DETAILS ON TRAINING VISION-LANGUAGE MODELS ON BIGDOCS Figure 11 illustrates our training pipeline for evaluating the quality of BigDocs and its impact compared to other datasets. The process has three stages: (1) Pretraining, which focuses on general modality alignment; (2) Continual pretraining, where models are further aligned to specific domain, such as documents, using general tasks like OCR and captioning; and (3) Finetuning, which uses smaller datasets to prepare models for specific downstream tasks, like document processing. We did not perform stage 1 in this paper, instead relying on checkpoints after pretraining. Additionally, we conducted some experiments using instruction-tuned checkpoints. Throughout all three stages, we employ generative loss objective, specifically categorical cross-entropy loss, to predict the next token given the context. The goal of BigDocs-7.5M is to enhance the models understanding and alignment with document-specific structures. At the same time, BigDocs-Bench focuses on training models to handle tasks that require processing images and converting them into structured code representations. These outputs often follow strict validity constraints to ensure they are syntactically correct. Additionally, the BigDocs-Bench test sets introduce novel benchmarks comprising five company-related downstream tasks, further validating the models ability to generalize to real-world document-based applications. Training Details. We conduct all our experiments on cluster of 64 H100 80GB GPUs leveraging the Transformers library (Wolf et al., 2019) for model development. Using Accelerate (Gugger et al., 2022) and Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023), we distribute training over all GPUs in our cluster. We wrap the transformer blocks in both the encoder and the decoder into separate FSDP units with activation checkpointing to achieve data parallel of 64 without running out of memory. FlashAttention-2 (Dao, 2024) gives significant speedup on our training runs. For reproducibility, we provide the hyperparameters and training details of our experiments. All experiments maintain constant total batch size of 256 and learning rate of 2e-5. We utilize the AdamW optimizer with the following parameters: cosine learning rate scheduler, 60 warm-up steps, beta1 coefficient of 0.95, beta2 coefficient of 0.999, weight decay of 1e-6, and an epsilon value of 1e-8. For the CPT experiments, models are trained for 1 epoch, while the finetuning experiments on the DocDownstream dataset and BigDocs-Bench are conducted over 3 epochs. A.6 DETAILS ON BIGDOCS-BENCHS EVALUATION In this section, we dive into more details of the evaluation process for BigDocs-Bench, including the preprocessing procedure before the evaluation and the implementation details of the metrics. A.6.1 PREPROCESSING Before conducting the evaluation, we perform the following preprocessing step for both the ground truth and generation texts: Preprint. Under Review. 1. Image2Flow (GraphViz): If the text contains markdown-style fenced code blocks, we use regular expression to extract the contents within the first code block; otherwise, we treat the entire text as GraphViz code and remove anything before digraph or graph if either exists. The flowchart triplets are extracted with the pydot library. 2. Image2Flow (JSON): If the text contains markdown-style fenced code blocks, we use regular expression to extract the contents within the first code block; otherwise, we remove all contents outside the first { and last } in the text. 3. Screenshot2HTML: If the text contains markdown-style fenced code blocks, we use regular expression to extract the contents within the first code block; otherwise, we treat the entire text as the output HTML code. We first normalize the HTML code with htmlmin.minify(), which removes the comments in HTML and condenses the attributes to their most miniature possible representations. Then, we create the DOM tree with BeautifulSoup4. 4. Table2LaTeX: If the text contains markdown-style fenced code blocks, we use regular expression to extract the contents within the first code block; otherwise, we treat the entire text as the output LaTeX code. We use series of regular expressions to normalize the contents. This procedure removes the comments, excessive whitespaces, and commands such as label, cite, citep, citet, ref, eqref, and pageref. 5. Image2SVG & Text2SVG: If the text contains markdown-style fenced code blocks, we use regular expression to extract the contents within the first code block; otherwise, we treat the entire text as the output SVG code. We parse the SVG code and generate PNG image with the cairosvg library. 6. Chart2Markdown: If the text contains markdown-style fenced code blocks, we use regular expression to extract the contents within the first code block; otherwise, we treat the entire text as the output Markdown code. We use regular expressions to remove the codes comments, links, and excessive whitespaces. 7. Chart2Caption, GUI2UserIntent, GUI2Summary, & GUI-VQA: We directly evaluate the generated texts against the ground truth. A.6.2 METRICS Here, we provide brief explanations of BigDocs-Benchs evaluation metrics. 1. Flowchart Triplet F1: Designed for Image2Flow tasks, this metric evaluates the accuracy of node relationships in flowchart codes, focusing on the correctness of edge triplets (s, e, d) extracted from GraphViz or JSON representations. Here, denotes the edge label (set to None if unlabeled), while and represent the source and destination nodes, formatted as label#shape. The Triplet F1 score is calculated by comparing the generated triplet list against the ground truth, disregarding the ordering of nodes and edges to emphasize relational accuracy. 2. HTML DOM Tree Edit Distance: Applicable to Screenshot2HTML tasks, this metric measures the similarity between generated and ground-truth DOM trees using the Tree Edit Distance. Leveraging BeautifulSoup4 with lxml parsing and the zss library, the distance is normalized by the larger node count of the compared trees. Invalid GraphViz or JSON generations receive Triplet F1 score of 0. 3. TeXBLEU (Jung et al., 2024)g: Utilized for the Table2LaTeX task, TeXBLEU employs LaTeX-trained tokenizer and finetuned embedding model with positional encoding. Unlike traditional BLEU, TeXBLEU assesses similarity based on n-gram token precision, demonstrating higher correlation with human evaluations of LaTeX math expressions compared to BLEU, SacreBLEU, and Rouge (Jung et al., 2024). 4. RMSF1 (Liu et al., 2022)h: Used for the Chart2Markdown task, RMS F1 treats tables as mappings from headers to values, measuring textual and numeric similarity through gAvailable at https://github.com/KyuDan1/TeXBLEU. hAvailable at master/deplot. https://github.com/google-research/google-research/tree/ Preprint. Under Review. normalized Levenshtein distance and relative distance. This approach ensures robustness against row and column permutations and transpositions, making it well-suited for evaluating flexible Markdown table structures. 5. DINOv2Score: Employed in the Image2SVG task, this metric calculates the cosine similarity between representations of the ground-truth and generated images using DINOv2 (Oquab et al., 2023), which better captures image similarity than comparable models i. Invalid SVG generations are assigned DINOv2Score of 0. 6. Rouge-L F1 (Lin, 2004): Applied to summarization and VQA tasks, Rouge-L F1 measures the longest common subsequence between the generated and reference texts. We compute this score using the implementation provided by torchmetrics. A.7 MAKING BIGDOCS LICENSE-PERMISSIBLE We dedicated significant effort to acquiring license-permissible dataset suitable for training models for commercial purposes in document images. We aim to create large-scale dataset that supports various tasks relevant to companies while adhering to accountability, responsibility, and transparency principles. To achieve this, we thoroughly investigated all public datasets concerning their licenses, evaluating both the sources of the images and their annotations. Our complete analysis, summarized in Table 5, enabled us to identify and retain only the sources that meet permissive licensing criteria. Dataset licensing frameworks are crucial in determining how data can be used, shared, and modified, generally falling into two categories: permissive and restrictive licenses. Permissive licenses offer the most freedom, allowing for broad usage and modification of the data. For instance, the CC0 license places the data in the public domain, enabling unrestricted use, modification, and distribution. The MIT License permits both commercial and non-commercial applications, provided the license terms are retained in any distribution. In contrast, the Apache 2.0 License extends these freedoms with an additional grant of patent rights. In contrast, restrictive licenses imposes certain limitations on data usage. The CC BY license requires users to provide attribution to the original creator. In contrast, the CC BY-SA license demands that any derivative works also carry the same licensing terms. More restrictive options, like the CC BY-NC and CC BY-ND licenses, prohibit commercial use and modifications, respectively. In cases where the licensing terms are unclear, it is prudent to exercise caution in using the data, as misinterpretation can lead to legal risks. Additionally, the concept of Fair Use allows for limited use of copyrighted material without explicit permission, particularly for purposes such as research, criticism, or commentary. However, Fair Use does not equate to unrestricted permission, and its applicability can vary, necessitating careful consideration when applied to dataset usage in research contexts. A.8 HUMAN EVALUATION To ensure fairness and minimize bias, we invited evaluators from diverse backgrounds, including PhD researchers, ML practitioners, multimodal AI experts, and individuals from both technical and non-technical domains. Key authors were excluded from participating to avoid conflicts of interest. Some non-key authors, such as advisors and authors who performed smaller tasks on the data creation effort, participated. These authors did not know the performance of models on the proposed tasks. For the evaluation, we used our own web interface, to make sure that all participants were anonymous and model outputs were randomized to prevent pattern recognition. We are open to releasing the detailed human evaluation results, as supplementary document. We developed web application using Flask, Javascript, and HTML/CSS where users are presented with pairs of outputs from two models and are asked to judge which model has better output for given input image. See Figure 12 for snapshot of the platform. 29 Preprint. Under Review. Table 5: Datasheet of candidate datasets. For transparency purposes, we provide detailed description of over 100 datasets considered in curating our BigDocs dataset. This table presents systematic analysis of public datasets, including information on medium types, source documents, text structures, languages, years, annotation types and methods, and licensing for both data and annotations. We also share sample counts across different modalities and splits. Some fields may be blank due to unavailable information. This comprehensive overview enables assessment of each datasets characteristics and potential contributions to BigDocs."
        },
        {
            "title": "Total Size",
            "content": "Documents 304997 170639 Annotations 304997 300000 Documents Image - Figure Image - Figure"
        },
        {
            "title": "Units",
            "content": "Annotations Full page annotation Element - Q&A pairs Dataset Name Accronym unichart-table unichart-qa"
        },
        {
            "title": "RecipeQA",
            "content": "ST-VQA"
        },
        {
            "title": "Text Type\nComputer Generated\nComputer Generated",
            "content": "Text Structure Structures - Charts Structures - Charts"
        },
        {
            "title": "Text Languages\nEnglish\nEnglish",
            "content": "Digital, Word, Latex Digital, Word, Latex Scanned, Digital Digital Article - Scientific paper Repository - ArXiV"
        },
        {
            "title": "English",
            "content": "Article - Scientific paper Crawling, Repository - ArXiV"
        },
        {
            "title": "Text with Structures",
            "content": "Legal, Bussines"
        },
        {
            "title": "Infographics",
            "content": "Repository - Goverment Crawling, Repository Computer Generated Handwritten, Typewritten, Computer Generated Structures, Text with Structures Infographics English, Chinese, Japanese, Arabic, Other English"
        },
        {
            "title": "Photo",
            "content": "Book - Textbook, Infographics Natural Scene"
        },
        {
            "title": "English",
            "content": "Book - Manuals"
        },
        {
            "title": "English",
            "content": "Digital - Website Photo"
        },
        {
            "title": "Synthetic",
            "content": "OCR-VQA"
        },
        {
            "title": "DVQA",
            "content": "Bunny-v1.0-data EXAMS-V"
        },
        {
            "title": "Computer Generated",
            "content": "Structures - Figures"
        },
        {
            "title": "Computer Generated",
            "content": "Infographics - Covers"
        },
        {
            "title": "Computer Generated",
            "content": "Structures - Charts"
        },
        {
            "title": "English",
            "content": "Repository - Goverment"
        },
        {
            "title": "Natural Image\nText with Structures",
            "content": "Annotations Type Layout Q&A (Question and Answer) OCR, Layout OCR, Q&A (Question and Answer) Q&A (Question and Answer) Q&A (Question and Answer) OCR, Layout Q&A (Question and Answer) Q&A (Question and Answer) Q&A (Question and Answer) Q&A (Question and Answer) Q&A (Question and Answer) Q&A - Dialogs Q&A (Question and Answer)"
        },
        {
            "title": "Weak Supervision",
            "content": "Apache 2 Apache"
        },
        {
            "title": "Good to use",
            "content": "500000 OCR, Table Detection/Extraction"
        },
        {
            "title": "Unknown\nlicense",
            "content": "Apache"
        },
        {
            "title": "Borderline",
            "content": "424045 Annotators - Crowdsourced Annotators - In house Unknown"
        },
        {
            "title": "Weak Supervision",
            "content": "license CC BY-NC 3.0 CC BY 2."
        },
        {
            "title": "Various\ncenses",
            "content": "LiAnnotators - Crowdsourced Synthetic"
        },
        {
            "title": "Borderline",
            "content": ""
        },
        {
            "title": "Not sure",
            "content": "5485 1076 11639 19779 22020 BYCC SA Various Licenses Unknown license MIT"
        },
        {
            "title": "Good to use",
            "content": ""
        },
        {
            "title": "Fair Use",
            "content": "Apache"
        },
        {
            "title": "Borderline",
            "content": ""
        },
        {
            "title": "Weak Supervision",
            "content": "CC BY-NC 4.0 BYCC NC 4."
        },
        {
            "title": "Not good to\nuse",
            "content": "300k 3487194 500000 562697"
        },
        {
            "title": "Full page annotation",
            "content": "Multi-page Element - Table structure Multi-page Element - Q&A pairs"
        },
        {
            "title": "Image",
            "content": "Element - Q&A pairs Multi-page Element - Q&A pairs"
        },
        {
            "title": "Word",
            "content": "Multi-page Element - Q&A pairs"
        },
        {
            "title": "Image",
            "content": "Element - Q&A pairs Element - Q&A pairs Element - Q&A pairs Element - Q&A pairs"
        },
        {
            "title": "Weak Supervision",
            "content": "CC 4.0 BY-SA Apache"
        },
        {
            "title": "Good to use",
            "content": "2000000 20932 20932 Multi-page Element - Answers CORD FUNSD PWC COCOText TextOCR PubTables-1M-TSRTFA PubTables-1M-TD SROIE VRDU-RegistrationForm VRDU ad-buy Websight1 Websight TextVQA Paper2Fig100k CommonCatalog RVL-CDIP English, Chinese, French, German, Italian, Arabic, Polish, Hungarian, Bulgarian, Croatian, Serbian English"
        },
        {
            "title": "DocILE",
            "content": "LEAF-QA"
        },
        {
            "title": "VisualMRC",
            "content": "Open4Business TabFact FinTabNet.c SVIT Digital, Scanned Digital"
        },
        {
            "title": "Digital",
            "content": "Article - Wikipedia Digital - Website Digital Digital Digital Photo"
        },
        {
            "title": "Article",
            "content": "Report - financial Article - Wikipedia Report - Financial Natural Scene Kleister Charity (KLC) Digital Report Kleister-NDA Digital - PDF Repository - Goverment Repository - Goverment Repository Wikipedia Crawling - Repository - Goverment Repository Wikipedia Repository - Goverment Dataset - Repository - Goverment"
        },
        {
            "title": "Computer Generated",
            "content": "Structures - Charts"
        },
        {
            "title": "Computer Generated",
            "content": "Structures - Tables"
        },
        {
            "title": "English",
            "content": "Computer Generated Text with Structures English Computer Generated Structures - Tables English Computer Generated Structures - Tables English Computer Generated Natural Image English Computer Generated Text with Structures English Scanned Bussines - Receipts Crowdsourced Computer Generated Structures - Forms English OCR, Layout, KIE DeepForm Digital - PDF Administrative Repository - Goverment - FCC-PIF Scanned Bussines Dataset Structures - Forms English OCR, Layout, KIE Typewritten, Computer Generated Computer Generated Text with Structures English"
        },
        {
            "title": "Not good to\nuse\nNot sure",
            "content": "240k"
        },
        {
            "title": "Good to use",
            "content": ""
        },
        {
            "title": "Not good to\nuse\nGood to use",
            "content": "10000 17458 Good to use 16575 Good to use"
        },
        {
            "title": "Unknown\nlicense",
            "content": "BY-SA Unknown license CC BY 3.0, CC BY 4.0 CC 4.0 CDLAPermissive Various censes Unknown license Unknown license CC BY 4.0 LiUnknown license CC SA 4.0 Custom BYCC BY CC 4.0 CDLAPermissive CC BY 4.0 Unknown license Unknown license CC 4.0 Custom BY KIE (Key Information Extraction) Q&A (Question and Answer) Q&A (Question and Answer) Q&A - Refered to ROI Summary Annotators - Crowdsourced Weak Supervision Annotators - Crowdsourced Annotators - Crowdsourced Weak Supervision Q&A - True/False sentences OCR, TD, TSE Annotators - Crowdsourced Weak Supervision Q&A (Question and Answer) NER (Name-Entity Recognition) LLM generated - No permissive like GPT Annotators - Crowdsourced Annotators - Crowdsourced Annotators - Volunteers KIE (Key Information Extraction), OCR Caption - Bounding box 106608 106608 Multi-page"
        },
        {
            "title": "Full page annotation",
            "content": "Image - Figure Element - Q&A pairs Image - Table Element - Q&A pairs"
        },
        {
            "title": "Image",
            "content": "Element - Q&A pairs Multi-page Element - Summary Image - Table Element - Sentence Image - Table Element - Table structure Element - Q&A pairs Various 108077 4240311 Image Not sure 2778 2778 Multi-page Full page annotation Not sure Good to use 1000 Not good to use Not sure 199 20000 1000 Image Image Full page annotation Full page annotation 20000 Multi-page Full page annotation Good to use 948000 Good to use Not sure 948000 1000 Not sure Not sure 1915 641 Good to use 823000 Good to use 2000000 - - - - - - - Word Word Full page annotation Image Image Multi-page Multi-page Image Multi-page Multi-page Image - Screenshot Code Image - Screenshot Code Good to use 28408 45336 Borderline 102453 Good to use 26232417 34.3M Good to use 399999 Image Image Image Image Element - Q&A pairs Element - Caption Element - Caption Full page annotation MIT CC 4.0 CC 4.0 MIT MIT MIT BY BY Unknown license Unknown license CC 4.0 CC 4.0 CC 4.0 CC 4.0 Various Licenses BY BY Photo Photo Natural Scene Natural Scene Dataset Dataset Computer Generated Natural Image Computer Generated Natural Image English English Annotators - Crowdsourced Annotators - In house CC BY 2.0 Unknown license Not sure Borderline BY 2000 63686 BY Good to use 173589 903069 Digital - PDF Article - Scientific paper Repository - Pubmed Computer Generated Structures - Tables English OCR, TD, TSE Automated Digital - PDF Scanned Article - Scientific paper Bussines - Receipts Repository - Pubmed Computer Generated Computer Generated Dataset Structures - Tables Structures - Forms English English OCR, TD, TSE OCR, Layout Automated Annotators Unknown license Computer Generated Structures - Forms English OCR, Layout Annotators - In house Unknown license Computer Generated Structures - Forms English OCR, Layout Annotators - In house Unknown Scanned Administrative Digital - PDF Administrative Digital - Website Digital - Website Photo Synthetic Synthetic Natural Scene Repository - Goverment Repository - Goverment Synthetic Synthetic Dataset Computer Generated Text with Images English Computer Generated Text with Images English Computer Generated Natural Image English Article - Scientific paper Repository - ArXiV Computer Generated Structures - Figures English license CC BY 4. LLM generated - Permisive like Llama LLM generated - Permisive like Llama Annotators - In house CC BY 2.0 CC BY 4.0 Automated LLM generated - Permisive like Llama Automated Various censes Various censes LiLiOCR, Q&A (Question and Answer) Caption Caption Text with Structures English OCR, Class Digital Photo Scanned Natural Scene Repository No Text Article, Report, Legal documents... Article, Report, Legal documents... Article, Report, Legal documents... Various Natural Scene Repository - Goverment Repository - Goverment Repository - Goverment Crawling Dataset Handwritten, Typewritten, Computer Generated Handwritten, Typewritten, Computer Generated Handwritten, Typewritten, Computer Generated Various No Text IIT CDIP Colection Scanned OCR-IDL Scanned PDFA ShareGPT4V Digital - PDF Photo ShareGPT4V-PT Photo Natural Scene LLaVA-Instruct-150K Photo Natural Scene LLaVA-CC3M-Pretrain595K LAION-400M LAION-5B LAION-COCO LAION-Aesthetic LVIS Instruct 4V Visual Genome OK-VQA A-OKVQA VisDial Hateful Memes Table-LLaVA Photo Photo Photo Photo Photo Photo Photo Photo Photo Photo Digital Digital Natural Scene Natural Scene Natural Scene Natural Scene Natural Scene Natural Scene Natural Scene Natural Scene Natural Scene Natural Scene Dataset Dataset Dataset Crawling Crawling Dataset Dataset Dataset Dataset Dataset Dataset Dataset No Text No Text No Text No Text No Text No Text No Text No Text No Text No Text No Text No Text Text with Structures English OCR, Class Automated Good to use 11000000 Image Full page annotation Text with Structures English OCR, Class Automated WTFPL Good to use 26621635 26621635 Page Full page annotation Text with Structures English OCR Caption Caption Q&A - Dialogs Q&A - Dialogs English Caption Automated LLM generated - No permissive like GPT LLM generated - No permissive like GPT LLM generated - No permissive like GPT LLM generated - No permissive like GPT Automated Multilingual Caption Automated English English English English English English English Caption Caption Q&A - Dialogs Q&A - Refered to ROI Q&A (Question and Answer) Q&A (Question and Answer) Q&A - Dialogs LLM generated - Permisive like Llama LLM generated - Permisive like Llama LLM generated - No permissive like GPT Annotators - Crowdsourced Annotators - Crowdsourced Annotators - Crowdsourced Annotators - Crowdsourced Annotators - Crowdsourced LLM generated - No permissive like GPT Not good to use Not good to use Not good to use Borderline LiLiBY BY BYBYCommonCrawl CommonCrawlNot sure CC Various NC 4.0 censes CC Various NC 4.0 censes CC 4.0 CC 4.0 CommonCrawl CC 4.0 CommonCrawl CC 4.0 CommonCrawl CC 4.0 CommonCrawl CC 4.0 MIT Various Various Various Various BY BY BY BY Borderline 100000 100000 1200000 1200000 158000 158000 595000 Image Image Image Image Element - Caption Element - Caption Element - Q&A pairs Element - Q&A pairs 413000000 Image Element - Caption 5850000000 5850000000 Image Element - Caption 600000000 600000000 Image Element - Caption 1200000000 Image Element - Caption 100000 200000 Various censes Various censes Various censes Various censes Custom LiLiLi- LiBY CC 4.0 Unknown license Unknown license Unknown license Custom Good to use 108077 1773258 Not sure Not sure 14031 14055 24900 Not sure 133060 1330600 Not sure 10000 Image Image Image Image Image Image Element - Q&A pairs Element - Q&A pairs Element - Q&A pairs Element - Q&A pairs Element - Q&A pairs Element - Class Various censes LiUnknown license Not good to use Image - Table Element - Q&A pairs 10000 97000 Infographics Crowdsourced Computer Generated Text with Images English Class Article Dataset Computer Generated Structures - Tables English Q&A (Question and Answer) 36786 1.8M 22033 30562 17458 117854 A.9 DETAILS OF UNIFIED METADATA FRAMEWORK We propose unified metadata framework for the BigDocs dataset to ensure transparency and traceability. This framework is organized into three primary keys: license, origin, and an optional features section. By adopting this structure, we provide standardized and flexible system that enhances the datasets usability, ensuring clarity for research and commercial applications. Along with the dataset, we include two separate files named tracking_instructions.json and tracking_transformations.json, which are dictionaries that store information standards across large subsets of samples, reducing redundancy and promoting consistency in metadata. License Metadata The license key is mandatory and provides detailed information regarding the licensing terms of both the images and annotations. It is structured as dictionary with three sub-keys: ihttps://medium.com/aimonks/clip-vs-dinov2-in-image-similarity-6fa5aa7ed8c6 Preprint. Under Review. Figure 12: Human Evaluation Platform. user is presented with two model outputs and asked to select the one that is more accurate in relation to the original model input. Figure 13: Metadata framework from our BigDocs Toolkit for sample in the BigDocs 7.5M dataset. It includes details about image properties, licenses, data sources, and transformation steps, such as using the table title as an image caption. image_license (mandatory): Specifies the license under which the image is provided. annotation_license (mandatory): Indicates the license for the annotations or labels associated with the image (e.g., text, bounding boxes, tables). license_note (optional): Includes additional licensing information or clarifications, such as specific usage conditions or exceptions. This allows for clear communication of any nuances in licensing terms. Origin Metadata The origin key is mandatory and serves as traceability mechanism, allowing users to track each data sample back to its source. This key is also structured as dictionary and includes the following sub-keys: data_id (mandatory): unique identifier for each data sample (e.g., file name, index, or ID in the source dataset). tracking_instruction_id (mandatory): reference to an entry in the tracking_instructions.json file, specifying how to use the metadata fields to trace the data sample back to its source. Preprint. Under Review. tracking_transformation_id (optional): reference to an entry in the tracking_transformations.json file, providing high-level description of any transformations or modifications applied to the original sample. dataset_url (optional): The URL of the original dataset from which the sample was obtained, if applicable. image_path (optional): The local path to the image within the raw dataset files, if available. This key is helpful when the dataset is stored in raw form. image_url (optional): The URL from which the image was initially downloaded. Misc. (optional): Other information related to the origin of the data entry. Features Metadata The features key provides detailed characteristics of the data sample. This can be useful for downstream tasks that require specific information about the sample, like the image size and type of annotations. image_height: The height of the image, in pixels. image_width: The width of the image, in pixels. Common Metadata and tracking_transformations.json files are designed to store information common to many samples, minimizing redundancy and ensuring consistency. Each file is structured as dictionary: tracking_instructions.json Files."
        },
        {
            "title": "The",
            "content": "tracking_instructions.json: Contains instructions on how to use the metadata fields (such as data_id, dataset_url, etc.) to trace the data sample back to its source. Each entry is uniquely identified and can be referenced in the individual sample metadata by tracking_instruction_id. tracking_transformations.json: Contains descriptions of any transformations, processing steps, or additions to the original samples. This includes synthetic generation processes, data augmentation steps, or other modifications. Each transformation is uniquely identified and can be referenced in the individual sample metadata by tracking_transformation_id. A.10 QUALITATIVE RESULTS We provide detailed qualitative analysis to highlight the performance of different models on tasks within BigDocs-Bench. Table 6 showcases the outputs of our best-performing model, Phi-3.5-Vision trained on BigDocs, alongside publicly available instruction-tuned model and GPT4o. The analysis covers tasks such as GUI-VQA, Chart2Caption, Image2Latex, and Screenshot2HTML. Notably, the BigDocs-trained model demonstrates more concise and accurate free-form outputs for VQA tasks, generates perfect LaTeX tables from table images, and produces precise HTML code from web screenshots. Additionally, we present series of qualitative examples across BigDocs-Bench tasks, detailed in Tables 9-28. These examples provide direct comparisons of outputs from all models discussed in the paper. The examples are accompanied by thorough error analysis that highlights each models strengths, limitations, and both success and failure cases. A.11 GLOSSARY & TASK DEFINITIONS We facilitate list of concepts introduced and utilized throughout our research. Below is list of these concepts and tasks, along with their definitions, including descriptions, inputs, and outputs. 1. Text with Structures Description: Analyzing documents combining text with elements like headers, lists, and paragraphs to understand the overall structure and organization. Input: Documents containing structured text elements such as headers, lists, and paragraphs. Preprint. Under Review. Table 6: Task Structure and Samples with Model Outputs for Phi Models and GPT-4o r I s - 5 . 3 - P"
        },
        {
            "title": "Query",
            "content": "s g - r I s - 5 . 3 - o 4 - GUI-VQA [What opens when the user clicks [550, 0, 600, 80]?] [The image does not provide information on what opens when the user clicks the specified coordinates.] [View the news] [By clicking on the coordinates [620, 50, 650, 70], the user will navigate to the \"news\" section of the website.] Chart2Caption [I need caption for the chart in the image, please] [The image displays pie chart with various segments, each representing different range of values and their corresponding percentages. The segments are color-coded and labeled with both the value range and the percentage they represent. The chart is sourced from Statista 2024] [In 2022, the most common salary range for employees in the United Kingdom was between 100,000 and 149,999 British pounds, with 19.3 percent of employees falling into this bracket. Employees earning between 200,000 and 249,999 British pounds made up the second most common salary range, at 15.8 percent.] [Distribution of Income Levels: Pie Chart Showing the Percentage of Households by Income Bracket in 2024] Image2Latex [Create LaTeX code to produce this table.] Screenshot2HTML [Could you create Hypertext Markup Language code from the screenshot?] Output: structured representation of the documents content. 2. Structures - Figures Description: Identifying and interpreting figures, diagrams, and other visual elements in documents to extract the information they represent. Input: Documents with embedded figures, diagrams, and visual elements. Output: detailed understanding or extraction of the visual content. 3. Infographics - Covers Description: Examining infographics and cover pages that use mix of text and graphics to communicate information visually. Input: Infographics and cover pages containing text and visual elements. Output: comprehensive analysis of the combined visual and textual information. 4. Structures - Tables Description: Finding and understanding tables in documents, focusing on accurately capturing the data and relationships they present. Input: Documents containing tabular data. Output: structured dataset or representation of the tabular information. 5. Structures - Forms Description: Recognizing and processing forms within documents, extracting data from fields and labels typically found in structured forms. 33 Preprint. Under Review. Input: Documents with structured forms, including fields and labels. Output: An organized collection of the extracted form data. 6. Natural Image Description: Analyzing photographs or natural images in documents, interpreting their content in the context of the surrounding text. Input: Documents containing photographs or natural images. Output: An interpreted or categorized representation of the image content. A.12 ABLATION EXPERIMENTS A.12.1 ABLATIONS ON DATA CURATION We ablated the main data curation procedures on BigDocs, namely the use of 1) VQA format during CPT, i.e. using (question, image, answer) tuples with assistant and user conversation formats, 2) the use of heavy OCR tasks during CPT (converting most of our document images into OCR tasks) and our 3) text-image alignment tasks, mainly captioning of many datasets. Experiment Setup. Using Table 7: Ablation Experiment on the effect of VQA formatting, presence of OCR data, and presence of captioning data in BigDocs-7.5M. Results show the importance of including these curation steps and tasks. See discussion in Section A.12."
        },
        {
            "title": "M U\nVAL",
            "content": "M M"
        },
        {
            "title": "TEST",
            "content": "SlideV A-M"
        },
        {
            "title": "Score",
            "content": "Avg. 87.05 82.65 (-4.4) 85.79 (-1.26) 85.36 (-1.69) 70. 70.97 37.45 51.21 81.24 81.56 68. 45.00 36.15 32.47 67.77 60.80 54.92 (-15.13) 40.64 (-30.33) 60.03 (-10.02) 54.19 (-16.78) 57.98 (-12.07) 53.36 (-17.61) 28.8 (-8.65) 34.9 (-2.55) 35.09 (-2.36) 36.93 (-14.28) 65.3 (-15.94) 78.4 (-3.16) 65.24 (-3.48) 52.68 (1.47) 50.91 (-0.3) 73.76 (-7.48) 72.5 (-8.74) 81.56 (0) 82.4 (0.84) 72.07 (3.35) 71.92 (3.2) 42.33 (-2.67) 42.56 (-2.44) 40.89 (-4.11) 34.71 (-1.44) 8.37 (-4.1) 2 61.13 (-6.64) 36.72 (0.57) 36.76 (0.61) 32.07 (-0.4) 31.97 (-0.5) 70.83 (3.06) 69.37 (1.6) 51.62 (-9.19) 58.1 (-2.71) 57.38 (-3.43)"
        },
        {
            "title": "No VQA",
            "content": "30% OCR 30% caption the Phi3.5-Vision-Instruct model, we created three modified versions of BigDocs-7.5M: one without the VQA format, one with OCR tasks removed, and one with reduced captioning tasks (text-image alignment). Each dataset variant was used to train the model, followed by fine-tuning the model on our instruction-tuning datasets. We evaluated these models on the benchmarks listed in Table 2 (General Document Benchmarks). The results are presented below. In parentheses, we show the delta with respect to the original result. Main Findings. 1. VQA Format: VQA formatting proved crucial, as its removal led to 9.19% average performance decrease. This was especially apparent in InfoVQA (-15.13), DeepForm (- 30.33), WTQ (-14.28), and TabFact (-15.94). common pattern observed here is lack of adherence to the requested output format as demonstrated in the example in Table X. We hypothesize that the VQA format used in the continual pre-training stage (CPT) enables better instruction tuning and supports multitask generalization. 2. OCR Impact: Removing OCR tasks led to an average performance drop of 2.71%, with varying impacts across benchmarks. Notably, we observed significant declines on InfoVQA (-10.02), DeepForm (-16.78), and TabFact (-7.48), while results on DUDE, SlideVQA, and ChartQA remained largely unaffected. The reduction in OCR data increased hallucination in tasks requiring precise information extraction. For instance, in DeepForm, the true negative rate dropped from 62% to 0%, as models began hallucinating unrelated values from other parts of the document. Examples illustrating this behavior are provided in Tables and Y. Interestingly, in some cases such as TableVQA (+3.06) and TextVQA (+3.35), performance improved, likely due to the nature of the question-answering tasks, which often favor free-form text generation over exact OCR matches. Table 21 shows an example of this phenomenon. 34 Preprint. Under Review. 3. Text-Image Alignment: Reducing text-image alignment tasks, particularly captioning, resulted in 3.4% performance drop, with notable declines on InfoVQA (-12.07), DeepForm (-17.61), and TabFact (-8.74). This drop primarily stemmed from errors in parsing values for arithmetic operations, especially when data formats varied, such as converting text or unique visual elements into percentages illustrated by examples in Table X, Table and Table Z. Tasks without significant drops, like WTQ and ChartQA, involve simpler text-visual correspondences that dont require complex conversions. See Table 22 for qualitative example A.12.2 ABLATION ON CONTEXT LENGTH The results presented thus far have been limited to context length of 8192 tokens. Given that the Screenshot2HTML task averages around 32k tokens per example (as shown in Table 1), it encounters significant challenges under this 8k-token constraint. This task ranks among those with the lowest scores (see Table 3), which contributes to decline in the overall aggregated average score. Despite this limitation, we found that the generated HTML code remains compilable, and the resulting DOM can still be evaluated using the DOM Tree Edit Distance metric, even though the HTML output is truncated due to the context limit. However, this inability to handle complete HTML outputs highlights the necessity for models capable of processing extended context lengths. To better understand this issue, we analyzed the model outputs (HTML code) in terms of histogram of token lengths, comparing them with the ground truth lengths in the test set (see Figure18). It is important to note that the test set is designed so that the images can be represented in fewer than 8192 tokens (as shown in the left figure). This figure highlights that token length constraints can adversely affect the task, as most samples approach the maximum token length of 6k (after accounting for image tokens), as seen in the right figure. Although the test set is designed to be generated within reasonable token length, the models training on larger contexts leads it to attempt to generate longer HTML outputs. We fixed the context length at 8192 tokens for several key reasons. First, not all models evaluated in our benchmark natively support larger context lengths, and extending this limit could disadvantage some models. Second, increasing context lengths significantly boosts GPU memory usage, complicating both training and inference at scale. Finally, to ensure fair comparisons across all models, we standardized the context length at 8k tokens. We conducted an ablation study on context length by training the Phi3.5-Vision model with context length of 32k tokens. Subsequently, we re-evaluated the Screenshot2HTML task using context lengths ranging from 512 to 16k tokens. However, generating 32k tokens resulted in out-of-memory errors. The results are presented in Table 29, where we applied greedy decoding (second column) and beam search with width of 2 and length penalty of -1.0 (third column). The results are revealing. We do not observe any performance boost when training and evaluating with longer context lengths. The greedy decoding algorithm performs worse with longer contexts, not exceeding score of 10.99. In contrast, beam search with length penalty approach yields better results, reaching 15.80, but still does not benefit from larger context lengths. Additionally, we conducted qualitative error analysis (see Figures 19 - 22) and found that the model often struggles with stopping generation, frequently hallucinating repetitive patterns such as nested <div> elements or excessively long URLs and references. However, well-formatted HTML code is generated in approximately 40% of cases (Figure 18, top figure). Applying length penalty helps mitigate this hallucination, leading to improved performance. This indicates further potential for enhancing results with alternative generation techniques. This indicates that the performance issue may not stem from context length but rather from the inherent difficulty of the Screenshot2HTML task. This aligns with the observation that the test sets average context length is approximately 3k tokens (Figure18, left) and does not require more than 8k tokens. We suspect that the primary limitation lies in the insufficient amount of training data (currently around 10k samples) which may hinder the models ability to learn HTML generation effectively. 35 Preprint. Under Review. A.13 DETAILS IN DATA VERIFICATION ON BIGDOCS-BENCH This section provides detailed description of the verification methodology, evaluation criteria, and measures taken to ensure the quality and reliability of BigDocs-Bench. We outline the qualifications of the human annotators, the multi-step verification process combining automated filtering and human review, and the strategies used to address the challenges posed by the datasets large size and synthetic nature. The section also discusses our sampling-based quality assurance for the training split, inter-rater reliability measures, and the conservative approach we adopted to prioritize recall and dataset safety. These steps collectively demonstrate our commitment to maintaining high standards in creating robust and ethical benchmark. A.13.1 VERIFICATION METHODOLOGY We generally follow 4-step strategy to perform human verification as follows, 1. Automatic Filtering Tools. We implemented multiple automatic filtering tools as the first step in our quality control process. These tools were designed to detect issues such as bad annotations, NSFW content (Llama-Guard-3 from https://huggingface.co/meta-llama/Llama-Guard-3-11B-Vision), and Personally Identifiable Information (PII). This automated process reduced the volume of problematic samples before any human verification was performed. 2. Human Verification for Test Split. To ensure high-quality test split, we only do human verification focused on the test set as our automatic filtering is already quite robust. Annotators identified and removed problematic samples following the guideline, for example, unnecessary comments in LaTeX and HTML code (e.g., in Table2LaTeX and Screenshot2HTML tasks), which could cause rendering issues. Similarly, flowcharts with excessively isolated nodes in tasks like Image2Flow were flagged and removed. For Image2SVG and three GUI-related datasets, we spotted and removed those images with PII or NSFW content. All these typical errors were collected and documented during this process to inform our automatic filtering tools. 3. Refining Automatic Filtering Tools for Training Split. The typical errors identified during human verification of the test set were used to iteratively improve our automatic filtering tools. This enhancement enabled the tools to better detect and handle similar issues in the training split, which was impractical to verify comprehensively through human labor. 4. Sampling-Based Human Verification for Training Split. To ensure the training split met quality standards, we randomly sampled 100 samples from all tasks, proportional to the size of their respective training splits. These samples were verified by annotators based on the criteria mentioned below, and the overall pass rate was 99% (i.e. 99 good samples), reflecting high level of quality for the training data after applying the refined filtering tools. Specifically, every task has pass rate of 100% except GUI-VQA, which has pass rate of 92% (i.e. 1 bad sample out of 13, which is not relevant to the task since the question is not quite related to the given image). A.13.2 VERIFICATION CRITERIA FOR VERIFIERS The verification process for BigDocs-Bench prioritized several critical aspects to ensure the datasets quality, reliability, and suitability for real-world applications. 1. First, data integrity was primary focus, ensuring accurate alignment between inputs and outputs. For example, in tasks like Image2SVG, visual elements in images were verified to match their corresponding SVG annotations, ensuring precise vector representation. Similarly, for Screenshot2HTML, rendered HTML outputs were checked against the screenshots to confirm structural and semantic fidelity. 2. Second, the relevance of synthetic samples was thoroughly assessed to confirm that they were realistic and reflective of the intended task objectives. For instance, in the Image2Flow task, flowchart samples were evaluated to ensure that the generated JSON or GraphViz code meaningfully represented the logical flow of the diagrams without excessive isolated nodes or disconnected components, which could compromise their utility. 3. Finally, to ensure content safety, NSFW content and PII, such as explicit material, or personal data, were removed. 36 Preprint. Under Review. A.13.3 DETAILS ON HUMAN VERIFIERS We engaged team of 6 human verifiers, consisting of graduate-level researchers and professionals with expertise in multimodal datasets and document analysis. All human verifiers were thoroughly trained on the verification criteria outlined below before commencing the review process. Each data sample was reviewed by at least two verifiers to ensure thoroughness. A.13.4 INTER-RATER RELIABILITY In the test split of our benchmark, we prioritized recall to ensure no potentially problematic samples were missed. For tasks like filtering NSFW content, detecting annotation errors, or identifying corrupted data, any sample flagged as problematic by single verifier was removed. This conservative recall-focused approach maximized dataset quality and safety while addressing misalignment between raters. A.14 RESULTS ON LARGER MODELS We have evaluated larger models on general document benchmarks, and the results are presented in Table 8. Models with more than 8 billion parameters perform exceptionally well on the general document benchmarks leaderboard. The top performer is GPT-4o, achieving an average score of 64.62, followed by Qwen2-VL-72B with 58.40 and GeminiPro-1.5 with 57.05. Table 8: General Document Benchmarks. Models trained on {BigDocs-7.5M+DocDownstream} perform competitively across multimodal document benchmarks. We compare them to base checkpoints, instruction-tuned models, and those trained on {DocStruct4M+DocDownstream}. We also report scores on state-of-the-art models >7B for reference. BigDocs models show consistent performance across tasks. * Refers the use of Chain-of-Thought (CoT) Model DocOwl1.5-8B (instruct) DocOwl1.5-8B (base) DocOwl1.5-8B (base) + DocStruct4M DocOwl1.5-8B (base) + BigDocs (Ours) Qwen2-VL-2B (instruct) Qwen2-VL-2B (base) Qwen2-VL-2B (base) + DocStruct4M Qwen2-VL-2B (base) + BigDocs (Ours) Phi3.5-Vision-4B (instruct) Phi3.5-Vision-4B + DocStruct4M Phi3.5-Vision-4B + BigDocs (Ours) LLaVA-NeXT-7B (instruct) LLaVA-NeXT-7B + DocStruct4M LLaVA-NeXT-7B + BigDocs (Ours) Llama-3.2-90B GPT-4o 20240806 Claude-3.5 Sonnet GeminiPro-1.5 Qwen2-VL-72B"
        },
        {
            "title": "TEST",
            "content": "80.73 2.07 75.99 78.70 89.16 7.26 59.53 57.23 86.00 86.76 87.05 63.51 60.95 57.13 74.15* 92.80 88.48 91.23 96.50 49.94 1.84 46.88 47.62 64.11 0.78 32.00 31.88 56.20 68.90 70.05 30.90 26.14 24.47 48.71 66.37 59.05 73.94 84.50 68.84 0.00 62.77 64.39 32.38 0.00 53.98 49.31 10.47 70.12 70.97 1.30 39.78 46.38 4.18 38.39 31.41 32.16 30."
        },
        {
            "title": "K L C\nTEST",
            "content": "37.99 0.00 35.21 36.93 25.18 0.00 36.38 34.39 7.49 37.83 37.45 5.35 28.34 31.09 1.81 29.92 24.82 24.07 24."
        },
        {
            "title": "W T Q\nTEST",
            "content": "38.87 0.00 32.86 35.69 38.20 0.00 28.48 31.61 17.18 51.30 51.21 20.06 25.90 27.06 24.20 46.63 47.13 50.29 55."
        },
        {
            "title": "M U\nVAL",
            "content": "M SlideV A-M"
        },
        {
            "title": "Score",
            "content": "Avg. 79.67 0.00 71.56 72.65 57.21 0.00 64.24 64.75 30.43 82.12 81.24 52.83 67.72 72.58 63.01 81.10 53.48 71.22 0.00 68.56 0.00 68.36 65.80 73.40 0.00 54.44 68.60 82.16 79.76 81.56 52.12 61.20 54.72 11.36* 85.70 51.84 34.68 88.30 68.91 0.00 65.08 67.30 79.90 1.14 55.89 61.01 73.12 68.60 68.72 65.10 52.25 49. 71.69 70.46 71.42 68.16 85.50 33.67 24.44 33.67 32.33 42.00 34.89 34.89 35.67 46.00 44.11 45.00 38.89 25.78 17.78 57.78 69.10 64.78 58.22 64.50 34.64 19.07 29.00 32.55 45.23 28.43 28.78 27.19 37.20 35.52 36.15 17.94 21.70 22.88 41.24 54.55 35.11 48.15 35.87 31.62 3.30 27.03 29.60 46.50 14.55 22.68 17.46 30.93 31.90 32.47 7.46 15.33 16. 26.09 67.58 0.00 52.05 2.15 52.60 13.63 46.27 49.03 43.07 0.00 46.53 47.53 70.70 69.17 67.77 32.87 27.03 33.13 41.57 72.87 81.27 80.43 74.23 53.84 5.36 49.56 51.05 53.03 7.25 43.15 43.89 45.66 60.51 60.80 32.36 37.68 37.70 38.82 64.62 50.73 57.05 58.40 A.15 CORRELATION ANALYSIS OF BIGDOCS-BENCH WITH OTHER VLM BENCHMARKS To assess the novelty of BigDocs-Bench in the VLM landscape, we conduct correlation analysis that highlights how BigDocs-Bench distinguishes itself from existing benchmarks. Specifically, we leverage model results across VLM benchmarks to demonstrate that BigDocs-Bench offers unique dimensions of model evaluation and analysis. First, we analyze the correlation between Document Benchmarks by comparing the results from Table 2 and Table 1. Following this, we expand the analysis to include results from several widely used VLM benchmarks, providing comprehensive view of the relationship between BigDocs-Bench and prior benchmarks in the field. Analysis Setup. We selected all benchmarks listed in Tables 2 and 1, encompassing both General Document Benchmarks and our proposed BigDocs-Bench, respectively. We constructed matrix encompassing all models presented in the paper, namely DocOwl-1.5-8B, Qwen2-VL-2B, LLaVANeXT-7B, Phi3.5-v-4B, both the off-the-shelf public versions and the ones trained on BigDocs. We also extended Table 2 with scores for GPT4o, Claude 3.5 Sonet, Qwen2-VL-72B, GeminiPro-1.5, 37 Preprint. Under Review. and Llama-3.2-90B, to obtain full matrix. Note that we have now extended our baselines with Llama-3.2-90B. Finally, we aggregated results for all metrics in the two groups of benchmarks. We have edited our manuscript to include Table 8, which is an extension of Table 2 with scores from all models. In our broader study with general VLM benchmarks, including but not limited to documents, we expanded the analysis to encompass 28 additional VLM benchmarks (see Figure 16 for the full list). Missing scores were imputed using the mean to complete the matrix for models and benchmarks. Methods. 1. Cosine similarity matrix and dendrograms: We normalize metric scores by centering (subtracting the mean) and scaling (dividing by the Euclidean norm). We then apply hierarchical clustering using cosine distance to group benchmarks and visualize their relationships. Cosine similarity matrices and dendrograms illustrate these clusters. 2. Principal Components Analysis (PCA): We represent each benchmark as feature array composed of scores from all models and project these arrays onto the two principal dimensions. A.15.1 CORRELATION WITH DOCUMENT BENCHMARKS Our results shown in Figures 14 and 15 show that BigDocs-Bench tasks and benchmarks are different from every other document dataset. Figure 14 presents the correlation matrix and dendrograms. Circles represent BigDocs-Bench tasks (our benchmarks), while triangles denote other document benchmarks included in the paper. These results demonstrate that BigDocs-Bench tasks are notably distinct from other benchmarks, as indicated by their low correlation with them. clear grouping emerges for tasks related to VQA, as well as BigDocs-Bench tasks involving code generation in LaTeX, JSON, GraphViz, HTML, and SVG. Notably, HTML and SVG tasks form cluster, likely due to their characteristically long output sequences. Additionally, DeepForm and KLC stand out as separate cluster, distinct from all others. Figure 15 illustrates the PCA results. The clusters distinctly separate BigDocs-Bench tasks from other benchmarks, reaffirming their unique characteristics. Notably, the average aggregated score from BigDocs-Bench (left side) is the furthest from the aggregated score of other benchmarks (right side), highlighting the distinctiveness of our benchmarks. However, certain tasks, such as Image2SVG and Screenshot2HTML, show stronger correlations with KLC, DeepForm, and TabFact. This is likely due to the shared level of difficulty across these benchmarks. A.15.2 CORRELATION WITH GENERAL VLM BENCHMARKS Figures 16 and 17 show that BigDocs-Bench tasks and benchmarks are unique in the broad VLM space. Figure 16 presents the correlation matrix and dendrograms. Circles represent BigDocs-Bench tasks (our benchmarks), upward triangles denote other document benchmarks included in the paper, and downward triangles denote general VLM benchmarks. BigDocs-Bench tasks demonstrate distinctive evaluation characteristics compared to existing benchmarks in the VLM space. This is evidenced by the clear clustering pattern, where BigDocs-Bench tasks form their own correlated group while showing minimal correlation with other benchmark types. Results also reveal three main clusters of tasks: general vision-language tasks (like COCO VAL and VCR), document understanding tasks, and GUI/table-related tasks (such as our tasks GUI2Sum and Table2Latex). This clustering pattern suggests that BigDocs-Bench is addressing unique aspects of document understanding that isnt captured by existing benchmarks. Figure 17 illustrates the PCA results, revealing that most BigDocs-Bench tasks are clustered in the bottom-right corner, distinctly separated from other benchmarks. Notably, tasks within BigDocsBench form clear clusters based on their type, such as VQA, Flows, Charts, or GUIs. General VLM benchmarks create dense cluster, while document benchmarks are more sparse. Additionally, the aggregated BigDocs-Bench average score is positioned far from other benchmarks, further highlighting its uniqueness. 38 Preprint. Under Review. Figure 14: Correlation matrix and dendrogram for BigDocs-bench and General Document Benchmarks. The matrix (left) shows benchmark relationships, with BigDocs-Bench tasks (circles) clustering separately from other General Document Benchmarks (triangles). The dendrogram (right) highlights distinct groupings, including BigDocs-Bench tasks for code and GUI generation clustering together and VQA tasks forming separate cluster. Tasks like DeepForm and KLC align closely, reflecting their shared complexity. Figure 15: PCA of BigDocs-Bench and General Document Benchmarks. BigDocs-Bench (circles) form distinct clusters from other document benchmarks (triangles), emphasizing their unique characteristics. Tasks related to code generation, such as Image2Flow and Table2Latex, cluster together, while VQA tasks, including those from other benchmarks, are further apart. 39 Preprint. Under Review. Figure 16: Correlation matrix and dendrogram for BigDocs-Bench and General VLM Benchmarks. The matrix (left) illustrates benchmark relationships, with BigDocs-Bench tasks (circles) clustering separately from other benchmarks. The dendrogram (right) reveals three main clusters: general VLM tasks (e.g., COCO, VCR), document understanding tasks, and GUI/table-related tasks (such as GUI2Sum and Table2Latex). Figure 17: PCA of BigDocs-Bench and General VLM Benchmarks. BigDocs-Bench (circles) form distinct clusters, clearly separated from other benchmarks, highlighting its unique characteristics. dense cluster is observed for general VLM benchmarks (downward triangles), while document benchmarks (upward triangles) are more dispersed. 40 Preprint. Under Review. Table 9: Test Example for the task of Chart2Caption. All models other than Phi3.5-V-4B make mistakes in the caption. LLaVA-NeXT-7B does not offer relevant information about the chart, but about its format and LLaVA-NeXT-7B + BigDocs hallucinates. BigDocs models make factual mistakes, while Claude and GPT4o have trouble with the accent marks."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B Phi3.5-V-4B + BigDocs LLaVA-NeXT-7B Can you give me caption for the chart in the image? The novel coronavirus (COVID-19) caused certain number of deaths within the French population. With 29,101 victims, the Paris Region, Île-de-France, recorded the highest number of deaths in France as of June 30, 2023. On the other hand, the regions of metropolitan France least affected were those of Brittany and Corsica. For further information about the coronavirus (COVID-19) pandemic, please visit our dedicated Facts and Figures page. The chart is horizontal bar graph displaying the number of deaths in various regions of France. The x-axis represents the number of deaths, ranging from 0 to 35,000, and the y-axis lists the regions. Île-de-France has the highest number of deaths at 29,101, followed by Auvergne-Rhône-Alpes with 17,750, and Grand Est with 14,958. The graph continues to list other regions with descending numbers of deaths, ending with Corsica at 538. The source of the data is indicated as Statista 2024. In 2022, the department of Île-de-France recorded the highest number of deaths in France, with 29,101 deaths. The department of Hauts-deFrance recorded the second highest number of deaths, with 13,907 deaths. The chart in the image is bar graph titled \"Collapse statistic\" and it shows the number of deaths for various locations in France. The locations are listed on the left side of the chart, and the number of deaths is represented by the height of each bar. The bars are color-coded, and the chart includes legend to explain the color coding. The chart also has button labeled \"Collapse statistic\" at the bottom, suggesting that the user can interact with the chart to collapse or expand the data. LLaVA-NeXT-7B + BigDocs Since the start of the French Revolution in 1793, over 38,000 people have been executed in France. The most common cause of death was by guillotine, procedure in which sharp knife is swung to the victims neck to sever the head from the body. The most recent execution was carried out on October 5, 2023, following the death of retired police officer. Qwen2-VL-2B Number of deaths in France by region in Qwen2-VL-2B + BigDocs The French department with the highest number of deaths due to the coronavirus (COVID-19) was Île-de-France, with 29,101 deaths as of January 13, 2023. The French authorities are constantly trying to increase the testing capacity to conduct PCR tests on people who have symptoms in order to try and discover the exact death toll. DocOwl-1.5-8B The department with the highest number of deaths in France in 2019 was Île-de-France. DocOwl-1.5-8B + BigDocs In 2022, there were 29,101 deaths in France. Of these deaths, 7,455 were in the Bourgogne region, making it the department with the highest number of deaths in France that year."
        },
        {
            "title": "Claude",
            "content": "GPT4o suitable caption for this chart could be: Number of COVID-19 Deaths by Region in France This horizontal bar chart shows the number of deaths, presumably from COVID-19, across different regions of France. The regions are listed on the left, with 00cele-de-France having the highest number of deaths at 29,101, while Corsica has the lowest at 538. The data appears to be presenting cumulative death tolls, likely over the course of the pandemic, for each of Frances administrative regions. Number of Deaths by Region in France: 00cele-de-France Leads with 29,101 Deaths, Followed by Auvergne-Roine-Alpes and Grand Est. 41 Preprint. Under Review. Table 10: Chart2Caption: Comparison of model responses showing that models using the BigDocs dataset produce more concise and relevant captions than baseline models while still making some factual errors."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B Phi3.5-V-4B + BigDocs LLaVA-NeXT-7B need caption for the chart in the image, please. This graph shows the male civilian labor force participation rate in the United States from 1990 to 2023. In 2023, 68.1 percent of the male civilian population participated in the job market. The chart is line graph that represents the labor force participation rate over span of years from 1990 to 2023. The vertical axis is labeled as Labor force participation rate and is measured in percentages, ranging from 66% to 78%. The horizontal axis represents years, marked at two-year intervals from 1990 to 2020, with additional single-year markers for 2021, 2022, and 2023. The line graph shows general downward trend in the labor force participation rate over the observed period. The rate starts at approximately 76% in 1990, experiences slight fluctuations, and then begins more consistent decline from around 2000 onwards, reaching its lowest point at about 67% in 2020. After 2020, there is slight increase, followed by plateau around 68% in the most recent years displayed. In 2023, 68.1 percent of the female labor force participated in the job market. This is slight increase from the previous year, when 67.7 percent of the female labor force participated in the job market. The chart in the image shows line graph with downward trend, indicating decrease in certain metric over time. The metric is labeled as \"Labor Force Participation Rate,\" which is measure of the number of people who are either working or actively seeking employment as percentage of the total population. The graph is set against white background with blue line representing the data points, and there are additional information icons at the top right corner, suggesting that the user can click on them for more details or options. The chart is titled \"Additional Information\" and includes \"Show source\" link, indicating that the data is sourced from Statista 2024. LLaVA-NeXT-7B + BigDocs In 2022, the labor force participation rate in the United States stood at 67.3 percent. The term labor force participation rate is used by the U.S. Bureau of Labor Statistics (BLS) to describe the percentage of the civilian non-institutional non-military population that is employed or seeking employment. Qwen2-VL-2B The chart depicts the labor force participation rate in the United States from 1990 to 2023. The labor force participation rate is the percentage of the population that is either employed or actively seeking employment. The data shows significant decline in the labor force participation rate over the years, with notable drop in the late 2000s and early 2010s. Qwen2-VL-2B + BigDocs In 2023, about 68.5 percent of the labor force participated in job-related activities in the United States. This is decrease from the previous year, when about 69.1 percent of the labor force participated in job-related activities. The labor force participation rate peaked in 2021, at 70.4 percent. DocOwl-1.5-8B The blue line chart represents the labor force participation rate of the labor force in the United States. DocOwl-1.5-8B + BigDocs In 2023, the labor force participation rate in the United States was 68.2 percent. This is decrease from the previous year, when the labor force participation rate stood at 68.4 percent."
        },
        {
            "title": "Claude",
            "content": "A suitable caption for this chart could be:nn\"Labor Force Participation Rate in the United States, 1990-2023\"nnThis caption accurately describes what the graph is showing: trend line of the labor force participation rate in the US over period of approximately 33 years, from 1990 to 2023. The graph demonstrates general decline in the labor force participation rate over this time period, starting at around 76% in 1990 and ending at about 68% in 2023. GPT4o Trends in Labor Force Participation Rate (1990-2022): Decline with Recent Stabilization. 42 Preprint. Under Review. Table 11: Evaluation Example for GUI-VQA: Models utilizing the BigDocs dataset consistently produce the most accurate and concise outputs, perfectly matching the ground truth without any extraneous or unnecessary information."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B [What information appears when clicking [330, 530, 390, 590]?] Access the Laundrette. The image does not provide information on what appears when clicking [330, 530, 390, 590]. Phi3.5-V-4B + BigDocs Access the Laundrette. LLaVA-NeXT-7B The image youve provided appears to be screenshot of webpage or document with list of services offered by company. The services are listed under different categories, and when you click on the numbers (330, 530, 390, 590), it seems to be way to navigate to different sections of the website or document. LLaVA-NeXT-7B + BigDocs Access the Laundry. Qwen2-VL-2B When clicking on the \"Motor Repair\" button, the information that appears is related to the maintenance and repair of motor systems. Qwen2-VL-2B + BigDocs Access the Laundrette. DocOwl-1.5-8B The information that appears when clicking [330, 530, 390, 590] is the same as the information that appears when clicking [330, 530, 390, 610]. DocOwl-1.5-8B + BigDocs Access the Laundrette."
        },
        {
            "title": "Claude",
            "content": "Gpt4o The image doesnt show interactive elements or clickable areas. However, at the approximate coordinates you mentioned [330, 530, 390, 590], there is green button labeled \"Exhaust\" in the left sidebar menu of the webpage. This button is part of list of services offered by FilterXchange, which includes other options like Filters, Canopies, Fans, Motor Repair, and several others. Clicking on the coordinates [330, 530, 390, 590] would likely correspond to the Fans button. The information related to Fans would appear, detailing services such as fan maintenance and repairs.\" Table 12: Qualitative test example for the task of GUI-VQA. GPT-4o (20240806), LLaVA-NeXT-7B , Phi3.5-V-4B , and Qwen2-VL-2B tend to hallucinate the response, mostly ignoring the input image. LLaVA-NeXT-7B and Claude are more conversational, however, the latter predicts correctly that the answer is Home Security"
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B Clicking [680, 50, 750, 110] gives the user access to what? Display the Home Security. The READ MORE button Phi3.5-V-4B + BigDocs Display the Home Security. LLaVA-NeXT-7B The image youve provided appears to be screenshot of website for company that provides CCTV, Alarms, and Security Services. The text \"Clicking [680, 50, 750, 110] gives the user access to what?\" is not visible in the image. If youre asking about what clicking on the \"Read More\" button would do, it would likely take the user to page with more detailed information about the services offered by the company. If youre asking about the content of the image, the visible part of the image shows security camera mounted on wall, which is common piece of equipment used for surveillance in security systems. LLaVA-NeXT-7B + BigDocs Display the Home Security. Qwen2-VL-2B Clicking [680, 50, 750, 110] gives the user access to Home Automation. Qwen2-VL-2B + BigDocs Display the Home Security. DocOwl-1.5-8B The user gets access to home automation. DocOwl-1.5-8B + BigDocs Display the Home Security. Claude-3.5 Sonnet Clicking in the area [680, 50, 750, 110] would likely give the user access to the Home Security section of the website. This can be inferred from the navigation menu at the top of the page, where Home Security is listed as one of the main service categories offered by CRH Communications, along with Smart Wiring, Home Automation, and Home Theater/AV.\" GPT-4o (20240806) \"Clicking in that area would likely give the user access to the Contact page. 43 Preprint. Under Review. Table 13: Qualitative test example for the task of GUI2Intent. All the models trained with BigDocs along with Qwen2-VL-2B produce the correct intent, while the latter is more conversational. Phi3.5-V-4B , LLaVA-NeXT-7B and DocOwl-1.5-8B infer incorrect sections of the image."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B If the user clicks [390, 20, 430, 60], what will they view? Display the HOME. They will view the Impressum section of the website. Phi3.5-V-4B + BigDocs Display the HOME. LLaVA-NeXT-7B The image youve provided appears to be screenshot of webpage with navigation menu on the left side. If the user clicks on the \"Kontakt\" menu item, which is the third option from the top, they will likely be taken to page that provides contact information or contact form for the website. This is common feature on websites where users can get in touch with the organization or individuals associated with the site. LLaVA-NeXT-7B + BigDocs Display the HOME. Qwen2-VL-2B If the user clicks on the \"HOME\" link, they will be directed to the main page of the website, which typically contains the homepage or navigation menu with links to different sections of the website. Qwen2-VL-2B + BigDocs Display the HOME. DocOwl-1.5-8B If the user clicks [390, 20, 430, 60], they will view Tiertherapie blog. DocOwl-1.5-8B + BigDocs Display the HOME. Table 14: Qualitative test example for the task of GUI2Intent. All the models trained with BigDocs along with Qwen2-VL-2B produce the correct intent, while the latter is more conversational. Phi3.5-V-4B , LLaVA-NeXT-7B and DocOwl-1.5-8B provide confused responses."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B What will the user encounter by clicking [400, 160, 450, 190]? View the About Us. The user will encounter section of the website that is not fully visible in the image. Phi3.5-V-4B + BigDocs View the About Us. LLaVA-NeXT-7B The image youve provided appears to be screenshot of website, specifically dental clinics website. By clicking on the coordinates youve mentioned (400, 160, 450, 190), the user would likely be directed to specific section or page of the website. However, without more context or clearer view of the websites layout, its difficult to determine exactly what the user would encounter. It could be link to specific service offered by the clinic, page about particular dental procedure, contact form, or any other type of content that is relevant to the clinics services. LLaVA-NeXT-7B + BigDocs View the About Us. Qwen2-VL-2B The user will be directed to the \"About Us\" section of the Menai Dental Centre website. Qwen2-VL-2B + BigDocs View the About Us. DocOwl-1.5-8B The user will encounter phone number by clicking on the respective number. DocOwl-1.5-8B + BigDocs View the About Us. 44 Preprint. Under Review. Table 15: Qualitative test example for the task GUI2Sum. BigDocs models accurately capture the spatial orientation and stylistic aspects of of the webpage while making errors parsing the restaurant name. LLaVANeXT-7B expresses spatial disorientation, while Qwen2-VL-2B and DocOwl-1.5-8B provide too simplistic and incorrect responses respectively."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B Phi3.5-V-4B + BigDocs LLaVA-NeXT-7B LLaVA-NeXT-7B + BigDocs Can you give me caption for the webpage in the image? This website features modern and colorful food menu for Gunbaje, restaurant offering Korean tapas and small plates. The layout is clean and organized, with prominent \"FOOD MENU\" title at the top. The menu is divided into sections for TAPAS, DINNER, and FEED ME, each with distinct color background and clear pricing. The drinks menu, labeled \"DRINK MENU,\" is displayed below with vibrant design and includes beer, soju, and cocktails. The website also includes social media icons and login button, enhancing user engagement. The image displays webpage with food and drink menu for restaurant named Gunbae. The top of the page has navigation options such as Home, Gunbae Menu, Order Online, Location & Hours, Book now, and social media icons for Instagram and Facebook. There is also Log In option. The main content of the page is divided into two sections: FOOD MENU and DRINK MENU. The food menu is colorful and lists various dishes with prices, including TAPAS, DINNER, FEED ME, and SMALL PLATES. It also mentions WESTERN, GUNBAE FREE, and VEGAN options. The drink menu is partially visible, showing BEER and SOJU categories with specific beer brands and prices. The bottom of the image is cut off, and the full drink menu is not visible. This website features food menu for restaurant named \"\"Gunbice.\"\" The main content includes colorful and organized menu with various dishes categorized under \"\"TAPAS,\"\" \"\"FOOD,\"\" and \"\"DRINK.\"\" The layout is clean and easy to navigate, with beige background and mix of vibrant and pastel colors for the menu items. The website also includes social media icons and login section at the top. The overall style is modern and visually appealing, designed to attract customers with variety of food and drink options. The image shows webpage with food menu on the left and drink menu on the right. The food menu lists various dishes with their prices, and the drink menu displays selection of beverages with their respective prices. The design is clean and organized, with color scheme that includes shades of blue, green, and orange. The layout is user-friendly, with clear headings and easy-to-read text. The overall impression is that of modern, inviting restaurant or cafe website. This website features vibrant and modern menu for restaurant named \"Rockin Randy.\" The layout is clean and organized, with beige background that contrasts with the colorful menu items. The main content includes detailed food menu with sections for beer, cider, and drinks, each listing various options with prices and descriptions. Icons and graphics, such as beer tap and drink, are used to visually represent the menu items. The website also includes navigation options like \"Home,\" \"Gastrobar Menu,\" \"Order Online,\" \"Location & Hours,\" and \"Book now,\" along with social media icons and login button. The overall style is contemporary and user-friendly, making it easy for customers to browse and order their drinks. Qwen2-VL-2B \"Certainly! Heres caption for the webpage: \"\"Experience the flavors of Gunbae with our delicious food and refreshing drinks!\"\"\" Qwen2-VL-2B + BigDocs This website is food menu page for Gunbae, featuring modern and clean layout with dark background and white text. The main content includes \"FOOD MENU\" section with various categories like Tapas, Small Plates, Buffets, and Desserts. The layout is organized with navigation bar at the top, large food menu area, and \"DRINK MENU\" section below. The color scheme is primarily dark with accents of green and white, creating sophisticated and inviting atmosphere. The website also includes social media icons and login button for users. DocOwl-1.5-8B \"Food Menu\" is the caption for the webpage in the image. DocOwl-1.5-8B + BigDocs This website features food menu for restaurant named \"Gunbac.\" The layout is clean and organized, with beige background and mix of black, white, and orange text. The main content includes detailed menu with various categories such as appetizers, mains, and drinks, each with pricing and descriptions. The menu items are visually appealing with images and icons. The website also has navigation options for Home, Gunbac Menu, Order Online, Location & Hours, and Book now. Social media icons for Facebook and Instagram are present at the top right. 45 Preprint. Under Review. Table 16: Qualitative test example for GUI2Sum. Phi3.5-V-4B and the BigDocs models are able to extract correct and relevant information from this simple and clean webpage, while the other baseline models make mistakes or are too simplistic in their caption."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B Phi3.5-V-4B -BigDocs Can you give me caption for the webpage in the image? The website displays \"Sorry, you have been blocked\" error message, indicating that the user is unable to access rentcafecloudflarecn.com. The layout is minimalistic, with white background and central red \"X\" symbolizing the block. The text is divided into two sections: \"Why have been blocked?\" and \"What can do to resolve this?\" The website explains that the block is due to security service protecting against online attacks, and suggests contacting the site owner with the provided Cloudflare Ray ID for assistance. The overall style is clean and straightforward, with focus on delivering the necessary information clearly. The image shows webpage with message indicating that the user has been blocked from accessing the website rentcafecloudflarecn.com. There is large red symbol in the center, which typically represents block or error. Below the X, there are two sections of text providing information on why the user might have been blocked and what actions can be taken to resolve the issue. The text suggests that the block could be due to security service protecting the website from online attacks and lists potential actions that could have triggered the block. There is also mention of Cloudflare Ray ID and an IP address, along with note that the performance and security are by Cloudflare. The website displays blocked access message with prominent red \"\"X\"\" symbol. The main content explains that the user has been blocked due to triggering security solution, possibly from submitting certain words, phrases, SQL commands, or malformed data. The layout is minimalistic, with white background and simple, clean design. The text is black, except for the red \"\"X\"\" and the Cloudflare logo, which adds touch of color. The message also provides Cloudflare Ray ID for the user to include in support email to the site owner for resolution. LLaVA-NeXT-7B \"The caption for the webpage in the image could be: \"\"Access Denied: Your Account is Blocked - Unable to Access Rentafire.com\"\" \" LLaVA-NeXT-7B + BigDocs The website displays \"Sorry, you have been blocked\" error message, indicating that the user is unable to access rentcafecloudflarecn.com. The layout is minimalistic, with white background and central red \"X\" symbolizing the block. The text is concise, explaining the block is due to security service protecting the website from online attacks, and suggests contacting the site owner with the provided Cloudflare Ray ID for resolution. The overall style is clean and straightforward, focusing on delivering the necessary information clearly. Qwen2-VL-2B \"Sorry, you have been blocked: Unable to access rentcafecloudflarecn.com\" Qwen2-VL-2B + BigDocs The website displays blocked access message with prominent red \"X\" symbol. The main content explains that the user has been blocked due to triggering security solution, possibly through actions like submitting certain words or phrases, SQL commands, or malformed data. The layout is minimalistic, with white background and simple, clean design. The text is straightforward and informative, providing reasons for the block and suggesting contacting the site owner for resolution. The Cloudflare Ray ID is also included for further assistance. DocOwl-1.5-8B The webpage in the image displays red cross and the message Sorry, you have been blocked. DocOwl-1.5-8B + BigDocs The website displays blocked access message with prominent red \"X\" symbol. The main content explains that the user has been blocked due to triggering security solution, possibly from submitting inappropriate content or commands. The layout is minimalistic, with white background and simple, clean design. The text is black, except for the red \"X\" and the Cloudflare logo. The message provides reason for the block and suggests contacting the site owner for resolution, including the Cloudflare Ray ID for reference. 46 Preprint. Under Review. Table 17: Test Example for the task of Table2Latex. BigDocs models preserve the structure of the table, lines from rows and columns, and font styles like bolds or italics."
        },
        {
            "title": "Content",
            "content": "[Please generate the necessary LaTeX script to draw this table.]"
        },
        {
            "title": "User Query",
            "content": "Phi3.5-V-4B Phi3.5-V-4B + BigDocs Gpt4o Table 18: Test Example for the task of Table2Latex. Phi3.5-V-4B was not able to format the table with all the rows, and used incorrect lines, and font style (boldness of first column and row)."
        },
        {
            "title": "Content",
            "content": "[Create LaTeX script to illustrate this table.]"
        },
        {
            "title": "User Query",
            "content": "Phi3.5-V-4B Phi3.5-V-4B + BigDocs 47 Preprint. Under Review. Table 19: Test Example for the task of Table2Latex: Boldness and details of lines are perfectly aligned with input in the Phi3.5-V-4B + BigDocs example."
        },
        {
            "title": "User Query",
            "content": "[Please generate the necessary LaTeX script to draw this table.] Phi3.5-V-4B Phi3.5-V-4B + BigDocs GPT4o Table 20: Test Example for the task of Table2Latex BigDocs failure. Phi3.5-V-4B+BigDocs was not able to format the question marks in the first row."
        },
        {
            "title": "User Query",
            "content": "[Please create LaTeX code to produce this table.] Phi3.5-V-4B + BigDocs 48 Preprint. Under Review. Table 21: Qualitative test example 1 for the DeepForm task. When asked question that cannot be answered from the image, Phi3.5-V-4B + BigDocs(OCR reduction) hallucinates response while Phi3.5-V-4B + BigDocs correctly answers in the negative, demonstrating the role of OCR data in enhancing precise information extraction."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B + BigDocs Phi3.5-V-4B + BigDocs(OCR reduction) What is the value for the contract_num?"
        },
        {
            "title": "None",
            "content": "12-15 49 Preprint. Under Review. Table 22: Qualitative test example 2 for the DeepForm task. When asked question that cannot be answered from the image, Phi3.5-V-4B + BigDocs(OCR reduction) hallucinates an unrelated response from different section of the image while Phi3.5-V-4B + BigDocs correctly answers in the negative."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B + BigDocs What is the value for the flight_from?"
        },
        {
            "title": "None",
            "content": "Phi3.5-V-4B + BigDocs(OCR reduction)"
        },
        {
            "title": "National Media Group",
            "content": "Table 23: Qualitative test example 1 for the InfoVQA task. Phi3.5-V-4B + BigDocs is able to use the combination of the percentage and the pictorial representation of the relevant data and produce the correct count while Phi3.5-V-4B + BigDocs(caption reduction) gets confused with different region of the image, demonstrating the robustness to text and image representations that captioning data brings."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B + BigDocs Phi3.5-V-4B + BigDocs(caption reduction) How many women out of every 4 women are domestic violence survivors? 3 3 50 Preprint. Under Review. Table 24: Qualitative test example 2 for the InfoVQA task. When tasked with extracting percentage based on textual and pictographic depiction of data, Phi3.5-V-4B + BigDocs(caption reduction) incorrectly picks percentage value in the vicinity, while Phi3.5-V-4B + BigDocs correctly infers the answer."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B + BigDocs Phi3.5-V-4B + BigDocs(caption reduction) What percentage of Americans are online? 90% 90% 70% 51 Preprint. Under Review. Table 25: Qualitative test example 3 for the InfoVQA task. Phi3.5-V-4B + BigDocs is able to exploit structure in the image and compute the correct answer, while Phi3.5-V-4B + BigDocs(caption reduction) hallucinates an unrelated answer."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B + BigDocs Phi3.5-V-4B + BigDocs(caption reduction) How long (in years) was the American Civil War? 4 4 52 Preprint. Under Review. Table 26: Qualitative test example for the InfoVQA task. Phi3.5-V-4B + BigDocs(no VQA format) does not adhere to the requested format and also provides wrong answer in contrast to Phi3.5-V-4B + BigDocs which gets both the format and the answer correct. These errors highlight the importance of VQA formatting of the BigDocs dataset."
        },
        {
            "title": "Ground Truth",
            "content": "Phi3.5-V-4B + BigDocs What percentage of women have experienced completed rape or attempted rape? 20% 20% Phi3.5-V-4B + BigDocs(no VQA format) 1 in 2 women have experienced sexual violence other than rape in their lifetime. 53 Preprint. Under Review. Table 27: Evaluation example from the Image2HTML task. The HTML code is rendered for easier comparison and analysis. The original Phi3.5-Vision performs poorly, while the BigDocs fine-tuned version generates highly accurate results. Although GPT-4 produces correct text, its structure differs significantly from the original image. These results demonstrate that fine-tuning on BigDocs leads to massive improvement in performance on this task."
        },
        {
            "title": "Original Image",
            "content": "Phi3.5-V-4B Output (Rendered) Phi3.5-V-4B + BigDocs Output (Rendered) GPT4o Output (Rendered) Table 28: Test Example for the task of GUI-VQA, showing BigDocs model failure. Phi3.5-V-4B+BigDocs was not able to give the correct output."
        },
        {
            "title": "User Query",
            "content": "[If the user clicks [400, 770, 410, 780], what will they view?]"
        },
        {
            "title": "Ground Truth",
            "content": "Display the SAD. Phi3.5-V-4B + BigDocs Display the DRZAVA. 54 Preprint. Under Review. Figure 18: Comparison of token length histograms on the test set of Screenshot2HTML task, and the generated outputs of our model (Phi3.5-Vision+BigDocs) Figure 19: Failure case on the task of Screenshot2HTML. Output is generated by the Phi3.5-Vision + BigDocs model. Context Length HTML Score HTML Score Length penalty = -1 13.96 15.80 14.71 13.88 13.60 13.43 512 1,024 2,048 4,096 8,192 16,384 Greedy 10.99 11.49 9.81 8.83 8.38 8.24 Table 29: HTML Scores for Phi3.5-Vision+BigDocs with varying context lengths. HTML Score defined as DOM Tree Edit Distance. Preprint. Under Review. Figure 20: Failure case on the task of Screenshot2HTML. Output is generated by the Phi3.5-Vision + BigDocs model. Figure 21: Failure case on the task of Screenshot2HTML. Output is generated by the Phi3.5-Vision + BigDocs model. 56 Preprint. Under Review. Figure 22: Success case on the task of Screenshot2HTML. Output is generated by the Phi3.5-Vision + BigDocs model."
        }
    ],
    "affiliations": [
        "CIFAR AI Chair",
        "McGill University",
        "Mila",
        "Polytechnique Montréal",
        "ServiceNow",
        "Universitat Autònoma de Barcelona",
        "University of British Columbia",
        "University of Waterloo",
        "Université de Montréal",
        "York University",
        "École de Technologie Supérieure"
    ]
}