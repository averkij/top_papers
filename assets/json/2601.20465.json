{
    "paper_title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
    "authors": [
        "Yang Li",
        "Jiaxiang Liu",
        "Yusong Wang",
        "Yujie Wu",
        "Mingkun Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning."
        },
        {
            "title": "Start",
            "content": "BMAM: Brain-inspired Multi-Agent Memory Framework Jiaxiang Liu1 Yusong Wang2 Yujie Wu3 Mingkun Xu1* Yang Li1 1 Guangdong Institute of Intelligence Science and Technology, Zhuhai, China 2 Institute of Science Tokyo 3 The Hong Kong Polytechnic University 6 2 0 2 8 2 ] . [ 1 5 6 4 0 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, failure mode we term soul erosion. We present BMAM (Braininspired Multi-Agent Memory), generalpurpose memory architecture that models agent memory as set of functionally specialized subsystems rather than single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45% accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays critical role in temporal reasoning."
        },
        {
            "title": "Introduction",
            "content": "Language-model-based agents increasingly operate in settings that require maintaining and reasoning over information accumulated across extended interactions, spanning diverse tasks, domains, and time scales. Such agents must retain past experiences, organize them into usable memory structures, and retrieve relevant information under varying goals and contexts. However, large language models are constrained by finite context windows and lack an explicit mechanism for managing longterm memory beyond the current input (Packer *Corresponding author. 1 et al., 2023; Maharana et al., 2024). Retrievalaugmented generation (RAG) partially alleviates this limitation by fetching external documents on demand, but it treats memory as an external text repository rather than an internal, evolving system. As result, RAG-style approaches provide limited support for persistent memory accumulation, temporal organization, and cross-episode reasoning, motivating the need for general-purpose memory framework that can support long-horizon agent behavior across tasks rather than task-specific retrieval pipelines (Zhang et al., 2025b; Hu et al., 2025). Evidence from cognitive science suggests that memory is not single monolithic store, but is supported by multiple functionally specialized subsystems operating over complementary time scales (e.g., fast episodic encoding alongside slower semantic consolidation and executive control) (OReilly et al., 2014). Inspired by this view, we propose BMAM (Brain-inspired Multi-Agent Memory Framework), brain-inspired multi-agent memory architecture that decomposes agent memory into interacting subsystems responsible for episodic storage, semantic consolidation, salienceaware selection, and intent-conditioned control (Li et al., 2025b). BMAM constructs internal memory representations rather than relying solely on external retrieval, and employs timelineindexed episodic memory organization to support temporally grounded access to past experiences. The framework further integrates hybrid retrieval mechanism that combines lexical, dense, knowledge-graph, and temporal signals via reciprocal rank fusion, together with asynchronous memory consolidation processes inspired by complementary learning principles. To coordinate memory access across different temporal scales, BMAM adopts hierarchical memory control mechanism from recent work, enabling both fast context-level access and slower consolidated memory retrieval. In preliminary analyses of long-horizon agent behavior, we observe recurring failure pattern in which fragmented or misaligned memory leads to degradation in temporal coherence and identityrelated behavior across interactions, which we refer to as soul erosion, providing diagnostic lens for failures of long-term memory management in general-purpose agent settings. Our main contributions are: We identify and characterize soul erosion, recurring failure pattern in long-horizon agent behavior where fragmented or misaligned memory leads to degradation in temporal coherence and identity-related behavior. We propose BMAM, brain-inspired framework that addresses this challenge by decomposing memory into specialized subsystems (episodic, semantic, salience). Crucially, we introduce timeline-indexed organization and hybrid retrieval strategy that fuses lexical, semantic, and temporal signals for robust grounding. We validate BMAM on the LoCoMo benchmark, achieving 78.45% accuracy and outperforming baselines in long-horizon settings. Further ablation studies empirically confirm the critical role of the hippocampus-inspired subsystem in enabling temporal reasoning. Soul Erosion: Why Memory Matters We use the term soul erosion to describe recurring failure pattern in long-horizon agent interactions, where fragmented or misaligned memory leads to degradation in behavioral continuity and identity-related behavior. Analogous to how human identity relies on the continuity of autobiographical memory (Wilson and Ross, 2003; Bluck and Liao, 2013), an AI agents soul (its consistent preferences, behavioral tendencies, and interaction patterns) may gradually degrade when long-term memory is poorly organized or inconsistently accessed. Formal Definition We formalize soul erosion as composite degradation metric over three orthogonal dimensions. Let Mt denote the agents memory state at interaction step t. We define the soulfulness score as: S(Mt) = αT (Mt)+βC(Mt)+γ I(Mt) (1) where () measures temporal coherence (ability to correctly order and recall when events occurred), C() measures semantic consistency (absence of factual contradictions), and I() measures identity preservation (retention of user-specific preferences and traits). The weights α, β, γ 0 with α + β + γ = 1 reflect task-specific importance."
        },
        {
            "title": "Soul erosion is then defined as the degradation",
            "content": "of soulfulness over time: E(t0, t) = S(Mt0) S(Mt) (2) where t0 is reference point (e.g., initial interaction or last memory consolidation). positive indicates soul erosion has occurred. In our experiments, we operationalize these components using benchmark proxies: via LoCoMo temporal accuracy, via cross-session consistency metrics, and via PrefEval and PersonaMem scores. Soul erosion encompasses three distinct failure modes (Figure 1), each arising from different memory failures and requiring specialized countermeasures: (1) Temporal Erosion The agent loses track of when events occurred, leading to anachronistic or temporally inconsistent responses. Cognitive research shows that temporal context is fundamental to episodic memory organization (Howard and Kahana, 2002; Eichenbaum, 2014), and benchmarks like LoCoMo and LongMemEval (Maharana et al., 2024; Wu et al., 2024) reveal that LLM agents frequently fail on temporal queries. As shown in Figure 1 (left), without explicit temporal organization, the agent may confuse event order, overlook durations, or fail to answer time-dependent queries. BMAM addresses temporal erosion through StoryArc timeline indexing, which maintains explicit temporal structure over stored experiences. (2) Semantic Erosion Facts and relationships degrade or become internally inconsistent across interactions. This mirrors the forgetting and interference phenomena studied in human memory (Wixted, 2004; Anderson, 2003), where memories compete and degrade without proper consolidation. As depicted in Figure 1 (center), the agent may provide contradictory answers about the same entity over time. HippoRAG (Jimenez Gutierrez et al., 2024) and memory surveys (Zhang et al., 2025b) highlight this challenge. BMAM counters semantic erosion through hippocampusto-temporal-lobe consolidation, which promotes 2 frequently accessed and high-confidence episodic memories into stable semantic representations. (3) Identity Erosion User preferences, personality traits, and persistent behavioral patterns may be overwritten or lost as new context accumulates. Research on autobiographical memory emphasizes that identity coherence depends on preserving self-relevant experiences (Conway, 2005; McAdams, 2001). Benchmarks like PersonaMem and PrefEval (Jiang et al., 2025; Zhao et al., 2025) demonstrate that current systems struggle to maintain user-specific information. As shown in Figure 1 (right), this failure mode undermines personalization: the agent forgets who the user is. BMAM mitigates identity erosion through amygdala-inspired salience tagging, which prioritizes identity-relevant information and protects it from being overwhelmed by transient context. Multi-Agent Coordination as Erosion Defense central design insight of BMAM is that these three forms of erosion arise from distinct memory failures and cannot be fully addressed by single mechanism. Cognitive neuroscience research demonstrates that human memory relies on multiple specialized systems (the hippocampus for episodic encoding, the neocortex for semantic consolidation, and the amygdala for emotional salience) that interact to maintain coherent longterm memory (OReilly et al., 2014). Inspired by this functional specialization, BMAM distributes memory functions across multiple interacting components, each targeting specific erosion type (Figure 1). Our ablation studies  (Table 6)  empirically validate this design: removing the hippocampusinspired episodic memory causes the largest performance drop, confirming its critical role, while other components contribute complementary defenses against different erosion types."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Memory Architectures for LLM Agents Retrieval-augmented generation (RAG) improves factual grounding but treats memory as implicit and transient; retrieved passages are not reorganized into stable internal structures (Zhang et al., 2025a; Wang et al., 2024; Xu et al., 2024). Agent-centric frameworks address this limitation through explicit memory management. MemGPT pioneered virtual context management by treating LLMs as operating systems with hierarchical memory tiers (Packer et al., 2023). MemoryBank extends this with forgetting mechanisms inspired by Ebbinghaus curves (Zhong et al., 2024). A-MEM introduces agentic memory that autonomously manages storage and retrieval (Xu et al., 2025b). Production systems like Mem0, Memobase, and MemOS provide scalable memory APIs with multi-component stores (Chhikara et al., 2025; memodb-io, 2025; Li et al., 2025a). Hierarchical approaches organize memory by semantic abstraction levels (Sun and Zeng, 2025; Wang et al., 2025; Xu et al., 2025a). Memoryaugmented transformers have explored various mechanisms for extending context, including segment-level recurrence (Dai et al., 2019), kNN-augmented attention (Wu et al., 2022), and brain-inspired episodic memory (Das et al., 2024). These approaches primarily target token-level or sequence-level prediction, whereas BMAM targets long-horizon agent memory management: what to store, how to organize it temporally, and how to retrieve it under changing goals. Brain-Inspired and Cognitive Approaches Cognitive neuroscience motivates separating fast episodic encoding from slower semantic consolidation and salience-based prioritization (OReilly et al., 2014). This principle has inspired systems like HippoRAG for hippocampus-style indexing (Jimenez Gutierrez et al., 2024), Nemori for event segmentation (Nan et al., 2025), and reflective memory systems that learn from experience through prospective and retrospective reflection (Tan et al., 2025; Shinn et al., 2023). Recent architectures emphasize tight coupling between perception and memory, forming closed loops that support adaptive long-term memory (Wang et al., 2023; Park et al., 2023). Compared to these approaches, BMAM differs in three key aspects: (1) multi-region coordination: while HippoRAG focuses on hippocampal pattern separation, BMAM models interactions among multiple brain-region analogs (hippocampus, temporal lobe, amygdala, prefrontal cortex); (2) explicit temporal indexing: unlike Nemoris event boundaries, BMAM maintains continuous timeline structures that support arbitrary temporal queries; (3) salience-aware consolidation: BMAM integrates amygdala-inspired importance signals into the consolidation process, prioritizing identity-relevant information over transient context. 3 Figure 1: Soul erosion types and BMAM countermeasures. Each erosion mechanism requires specialized defense: temporal erosion is addressed by StoryArc timeline indexing, semantic erosion by hippocampus-to-temporal-lobe consolidation, and identity erosion by amygdala salience tagging. Benchmarks Long-term memory benchmarks evaluate temporal reasoning (LoCoMo (Maharana et al., 2024), LongMemEval (Wu et al., 2024)), preference consistency (PrefEval (Zhao et al., 2025)), and persona recall (PersonaMem (Jiang et al., 2025)), providing complementary perspectives on the challenges BMAM addresses."
        },
        {
            "title": "3 BMAM Framework",
            "content": "BMAM adopts coordinator-centered multi-agent architecture that decomposes long-term memory into functionally specialized components while maintaining unified memory substrate. central coordinator routes information among interacting subsystems responsible for memory storage, retrieval, consolidation, and control, enabling modular specialization without fragmenting memory state. Memory Loop and Coordination BMAM implements an explicit memory loop inspired by hippocampusneocortex dynamics. Incoming experiences are encoded into episodic memory using fast, discriminative representations and tagged with salience signals, while relevant content is maintained in constrained working-memory buffer to support immediate reasoning. Over time, selected episodic information is consolidated into semantic memory and shared knowledge graph. Retrieval closes the loop by jointly accessing episodic and semantic evidence under temporal constraints, with feedback signals adjusting consolidation priorities and routing decisions. Functionally Specialized Memory Components BMAM decomposes memory into complementary subsystems with explicit roles and capacities. Episodic memory stores temporally grounded interaction traces and supports discriminative addressing. Semantic memory consolidates stable facts and relations into shared knowledge graph. salience-aware component computes importance signals from interaction cues (e.g., novelty, conflict, or user feedback) that modulate consolidation scheduling and retrieval weighting. The Prefrontal component implements executive control functions inspired by the prefrontal cortexs role in working memory maintenance and cognitive control (Miller and Cohen, 2001). Specifically, it performs three functions: (1) query routing, classifying incoming queries along dimensions (temporal, identity, preference, factual) to determine which memory subsystems to consult; (2) working-memory buffering, maintaining capacitylimited buffer (10 items) of recent context for immediate reasoning without full memory retrieval; and (3) attention allocation, dynamically weighting evidence sources based on query requirements. Control-oriented components, including the Prefrontal buffer and Basal Ganglia procedural patterns, together provide complementary protections against different forms of memory degradation. Unified Memory Substrate and Temporal Indexing BMAM employs unified memory substrate that combines keyvalue episodic storage, vectorbased similarity indexing, and shared knowledge graph. Episodic memories are organized into Figure 2: BMAM architecture overview. central coordinator orchestrates multiple functionally specialized memory subsystems sharing unified memory substrate with episodic timelines, knowledge graph, and vectorbased storage. timeline-indexed structure that records minimal narrative units indexed by entities, events, and timestamps. This temporal organization enables queries involving order, duration, and temporal relations (e.g., before/after, first/last), while consolidation processes selectively lift episodic information into semantic form to ensure consistency across representations. as an episodic memory trace, capturing the contextual content together with inferred temporal and semantic attributes. Salience signals are computed from interaction cues (e.g., novelty, conflict, or user feedback) and attached to the episode. To support efficient short-term reasoning, compact summary of recent episodes is maintained in constrained working-memory buffer. Hierarchical Coordination and Retrieval To support long-horizon interactions, BMAM adopts hierarchical memory coordination mechanisms that regulate memory access and updates across multiple time scales. Fast paths support immediate context-level access, while slower paths govern semantic consolidation and procedural stabilization. Retrieval integrates fast-path detection, iterative interaction between episodic and control components, and uncertainty-driven multi-round retrieval. Evidence from episodic memory, semantic memory, and the knowledge graph is combined with temporal constraints, and feedback signals dynamically reweight lexical, dense, entity-based, and temporal cues. Memory Lifecycle BMAM models memory as dynamic lifecycle governing encoding, consolidation, retrieval, and revision, summarized in Appendix Figure 5. Input Analysis and Episodic Encoding We first analyze each incoming interaction to extract entities, temporal expressions, and intent cues relevant to memory formation. The interaction is encoded Consolidation and Temporal Organization Next, BMAM employs complementary learning process in which frequently accessed and highconfidence episodic memories are selectively consolidated into semantic memory. Consolidated information populates shared knowledge graph that maintains stable facts and relations across interactions. In parallel, episodic memories are organized into timeline-indexed structure that records entitycentric events with associated temporal information. This temporal organization enables reasoning over event order, relative timing, and durations, supporting queries such as when, before/after, and how long without requiring full episodic recall. Hybrid Retrieval and Temporally Grounded Answering To answer query, BMAM retrieves relevant evidence from multiple sources, including episodic memory, semantic memory, and the timeline-indexed event structure. Each source produces ranked list of candidates, and lexical, dense, relational, and temporal signals are 5 fused using (weighted) reciprocal rank fusion: score(d q) = (cid:88) sS ws + ranks(d q) , (3) ws in Eq. 3) change what is stored, how it is indexed, and how evidence is combined, enabling BMAM to evolve its memory behavior as experience accumulates. where ranks(d q) is the rank of candidate under source s, = 60 is the smoothing constant following standard RRF practice, and ws reflects the current preference over evidence sources. For time-dependent questions, temporal evidence is extracted from the timeline organization to compute relative orderings and durations, which are then used to generate temporally grounded answers. This retrieval process is adaptive: uncertainty and salience signals may trigger additional retrieval rounds or reweight evidence sources. Background Optimization and Memory Revision In parallel with online interaction, memory organization in BMAM is continuously refined through background processes. Episodic memories may be reconsolidated when re-accessed, increasing their stability or updating their content as new evidence emerges. Low-value or outdated memories are gradually pruned, while salience-relevant episodes receive prioritized consolidation. These processes allow BMAM to revise memory over time, preventing uncontrolled growth and reducing the accumulation of inconsistent or obsolete information. Continual Learning and Plasticity Over longer interaction horizons, BMAM treats memory as plastic substrate rather than static store. Continual learning emerges from ongoing consolidation and reconsolidation, whereby retrieved evidence can update semantic memory instead of being frozen after first storage. Conceptually, if pt(f ) denotes the confidence of semantic fact at time t, and ˆpt(f ) is an evidence-based estimate from new retrieval/verification, then memory revision can be expressed as an exponential moving average: pt+1(f ) = (1 λ)pt(f ) + λˆpt(f ), (4) where λ (0, 1) is the update rate. This enables knowledge updates while damping noisy evidence. When confidence is low or information is incomplete, the system may actively seek clarification through follow-up interaction, strengthening memory traces and reducing uncertainty. Over time, adaptive routing, salience-weighted storage, and confidence-calibrated retrieval (e.g., by adjusting"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We evaluate BMAM on four benchmarks designed to test long-horizon memory and personalization capabilities  (Table 1)  . Our evaluation focuses primarily on LoCoMo and LongMemEval, which together capture complementary challenges in conversational memory, temporal reasoning, and memory consistency over extended interactions. Primary Benchmarks We focus primarily on LoCoMo (Maharana et al., 2024) and LongMemEval (Wu et al., 2024), which together capture complementary challenges in long-horizon memory. LoCoMo evaluates recall of facts, relationships, and events across extended multi-session dialogues, emphasizing single-hop factual recall, multi-hop reasoning, and temporally grounded questions. LongMemEval complements this with cross-session recall, preference tracking, knowledge updates, and explicit temporal reasoning, stressing memory consistency under evolving information. Additional Benchmarks We further evaluate BMAM on PersonaMem and PrefEval, which focus on persona consistency and preference alignment, respectively. These benchmarks test whether memory systems can preserve user-specific information and behavioral preferences across interactions, complementing the conversational and temporal challenges posed by LoCoMo and LongMemEval. Dataset Scale Task Focus Metric LoCoMo 10 groups, 1986 QA LongMemEval 500 questions long-horizon dialogue long-term memory Accuracy Accuracy PersonaMem 20 users, 589 QA persona recall Accuracy PrefEval 1000 questions (MCQ) preference alignment Pers. rate Table 1: Datasets and evaluation metrics used in BMAM experiments. Baselines We compare BMAM against seven memory-augmented LLM systems: MemOS (Li 6 et al., 2025a), memory operating system with unified memory scheduling; Mem0 (Chhikara et al., 2025), scalable memory-centric architecture with optional graph-based memory; MIRIX (Wang and Chen, 2025), multi-agent system with six specialized memory types; Zep (Rasmussen et al., 2025), temporally-aware knowledge graph engine; Memobase (memodb-io, 2025), Supermemory (supermemoryai, 2025), and MemU (NevaMind-AI, 2025). Baseline results are from Li et al. (2025a); we re-run MemOS with GPT-4o-mini for fair comparison. Evaluation Protocol For all benchmarks, persistent memory is reset between independent evaluation units (e.g., LoCoMo conversation groups or individual users) while being preserved within each unit to reflect realistic interaction histories. Conversation logs are ingested through BMAMs memory lifecycle prior to evaluation, and queries are issued in evaluation mode without additional learning. We follow the evaluation protocol, metrics, and judge prompts from MemOS1; baselines are evaluated using their official scripts. Crucially, to ensure fair comparison, we re-evaluated the strongest baseline (MemOS) using the identical LLM backend (GPT-4o-mini) as BMAM, eliminating discrepancies arising from model version updates. During evaluation, all background processes (consolidation, reconsolidation, pruning) are disabled; memory state is frozen after ingestion to prevent test-time learning."
        },
        {
            "title": "4.2 Results",
            "content": "Table 2 summarizes our main results. Dataset Metric Score Correct/Total LoCoMo Accuracy LongMemEval Accuracy PersonaMem Accuracy Pers. rate PrefEval 78.45% 1558/1986 67.60% 48.9% 72.90% 338/500 288/589 729/1000 Table 2: BMAM results across four long-term memory benchmarks. Across benchmarks, BMAM is strongest on LoCoMo and PrefEval, while PersonaMem remains challenging: its multiple-choice format requires exact surface-form matching, whereas BMAMs retrieval is optimized for open-ended generation, and its emphasis on shallow persona attributes differs from BMAMs focus on temporally grounded 1Official repository: https://github.com/MemTensor/ MemOS; paper: https://arxiv.org/abs/2507. Figure 3: LoCoMo benchmark comparison. BMAM achieves 78.45% using the official MemOS evaluation scripts; Note that MemOS was re-run using GPT-4omini for strict comparability; other baselines utilize reported results. identity. We provide more detailed discussion in Appendix A.5. Temporal reasoning remains key open challenge, and improving normalized temporal outputs and cross-session integration is an important direction for future work. LoCoMo Performance On LoCoMo, BMAM achieves an overall accuracy of 78.45% under our MemOS-aligned evaluation protocol (Figure 3). We compare against reported baselines from MemOS (not re-run); because model backends, prompts, and infrastructure may differ, these comparisons are indicative rather than strictly comparable. Performance varies across question types: single-hop (82.0%), multi-hop (70.4%), temporal (62.3%), and open-domain (79.6%). Strong singlehop and open-domain performance indicates effective episodic retrieval and evidence fusion, while gains in multi-hop questions reflect the benefit of semantic consolidation. Temporal questions remain the most challenging category, highlighting the difficulty of precise temporal reasoning over long interaction histories. LongMemEval Performance BMAM achieves an overall accuracy of 67.60% on LongMemEval, with substantial variation across categories  (Table 3)  . The model performs strongly on preferencerelated and within-session recall tasks, including single-session preference (100%) and singlesession user facts (87.1%). Performance on knowledge updates (70.5%) indicates that BMAM can incorporate corrected information through memory revision. Lower accuracy on temporal-reasoning (59.4%) and multi-session recall (52.6%) reflects the increased difficulty of cross-session temporal integration and explicit time computation. 7 Category Accuracy Correct/Total Single-session-preference Single-session-user Single-session-assistant Knowledge-update Temporal-reasoning Multi-session 100.0% 87.1% 76.8% 70.5% 59.4% 52.6% 30/30 61/70 43/56 55/78 79/133 70/133 Table 3: LongMemEval per-category performance. Ablation Analysis To examine component contributions, we conduct ablation experiments on LoCoMo subset (Figure 4). Removing the hippocampus-inspired episodic memory leads to 24.62% accuracy drop, confirming its central role. Cognitive Trade-offs and Component Specificity It is noteworthy that removing the Prefrontal (+5.03%) and Temporal Lobe (+4.02%) yields overall gains on this subset. We analyze this as an efficiency-robustness trade-off. The subset is dominated by single-hop factual queries (67%), where direct episodic retrieval suffices; for these \"System 1\" tasks, higher-order processing introduces routing overhead without added value. However, this overhead is the cost of complex reasoning. granular analysis confirms that these components are critical for their intended functions: specifically on temporal queries, removing the Temporal Lobe causes sharp 12.3% accuracy drop (masked in the aggregate score). This validates that while BMAMs higher-order regions introduce overhead on simple retrieval, they are indispensable for the long-horizon temporal grounding and reasoning that constitutes the core of soulfulness. We therefore emphasize that the primary contribution is the architectural pattern. The braininspired decomposition provides principled organization that achieves strong overall performance (78.45% on LoCoMo), balancing fast episodic access with necessary control mechanisms. All experiments were run three times; reported numbers represent the mean across runs. Error Analysis We manually examined 50 randomly sampled errors from LoCoMo to identify failure patterns. Three categories dominate: (1) Temporal confusion (38%): questions requiring precise date computation or relative ordering (e.g., How many days between and Y?) often fail due to incomplete timestamp extraction or ambiguous temporal expressions in the source dialogues. (2) Entity ambiguity (28%): when multiple entities Figure 4: Brain-region ablation on LoCoMo. Hippocampus removal causes 24.62% drop, validating episodic memory as the critical backbone. Varied effects for other components reflect tight coupling (see text). share similar attributes, retrieval may return the wrong entitys information, particularly for multihop questions requiring entity disambiguation. (3) Retrieval coverage (22%): relevant evidence is stored but not retrieved, typically when the query phrasing differs substantially from the stored memorys surface form. The remaining 12% involve annotation ambiguities or require external knowledge beyond the conversation. These patterns suggest that improving temporal normalization and entityaware retrieval are promising directions for future work."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented BMAM, brain-inspired multi-agent memory framework that addresses soul erosion, the gradual degradation of behavioral continuity in long-horizon AI agents. By decomposing memory into functionally specialized subsystems (episodic, semantic, salience-aware) coordinated through shared control, BMAM provides generalpurpose architecture for persistent memory management. We introduced soul erosion as diagnostic lens connecting empirical failure patterns to memory organization choices. Our experiments demonstrate that BMAM achieves 78.45% accuracy on LoCoMo, with ablation studies confirming the critical role of hippocampus-inspired episodic memory. The frameworks modular design enables systematic diagnosis of memory failures: error analysis reveals that temporal confusion (38%) and entity ambiguity (28%) remain the dominant failure modes, while cross-session integration (52.6% on multisession tasks) poses the greatest challenge for longhorizon memory. These findings motivate future work on temporal normalization, entity-aware re8 trieval, and improved cross-session consolidation. Future directions include multi-modal memory, embodied agents, and adaptive component activation. Beyond text, extending BMAM to multi-modal memory (images, audio) and embodied agent settings where temporal grounding is tied to physical actions presents additional challenges, as does developing adaptive mechanisms that dynamically activate or bypass components based on query complexity."
        },
        {
            "title": "6 Limitations",
            "content": "Our evaluation focuses on four established longterm memory benchmarks. While these benchmarks capture core challenges in long-horizon conversational memory, broader validation across additional domains remains future work. While older baseline results are reported from their original papers, we explicitly re-evaluated the primary baseline (MemOS) under our specific experimental setting (GPT-4o-mini) to validate architectural gains independent of the foundation model."
        },
        {
            "title": "7 Ethics Statement",
            "content": "Persistent memory systems raise important considerations related to user consent, data ownership, and long-term data retention. While BMAM does not introduce ethical risks beyond those associated with existing memory-augmented agents, responsible deployment requires transparent memory policies, mechanisms for user control over stored information, and support for data deletion upon request. These considerations are essential for maintaining user trust and ensuring compliance with applicable privacy regulations."
        },
        {
            "title": "References",
            "content": "Michael Anderson. 2003. Rethinking interference theory: Executive control and the mechanisms of forgetting. Journal of Memory and Language, 49(4):415 445. Susan Bluck and Hsiao-Wen Liao. 2013. was therefore am: Creating self-continuity through remembering our personal past. The International Journal of Reminiscence and Life Review, 1(1):712. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413. Martin Conway. 2005. Memory and the self. Journal of Memory and Language, 53(4):594628. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 29782988. Association for Computational Linguistics. Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarathkrishna Swaminathan, Sihui Dai, Aurélie Lozano, Georgios Kollias, Vijil Chenthamarakshan, Jiˇrí Navrátil, Soham Dan, and Pin-Yu Chen. 2024. Larimar: large language models with episodic memory control. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Howard Eichenbaum. 2014. Time cells in the hippocampus: new dimension for mapping memories. Nature Reviews Neuroscience, 15(11):732744. Marc Howard and Michael Kahana. 2002. distributed representation of temporal context. Journal of Mathematical Psychology, 46(3):269299. Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, and 1 others. 2025. Memory in the age of ai agents. arXiv preprint arXiv:2512.13564. Bowen Jiang, Zhuoqun Hao, Young Min Cho, Bryan Li, Yuan Yuan, Sihao Chen, Lyle Ungar, Camillo Jose Taylor, and Dan Roth. 2025. Know me, respond to me: Benchmarking LLMs for dynamic user profiling and personalized responses at scale. In Second Conference on Language Modeling. Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. Hipporag: Neurobiologically inspired long-term memory for large language models. Advances in Neural Information Processing Systems, 37:5953259569. Zhiyu Li, Shichao Song, Chenyang Xi, Hanyu Wang, Chen Tang, Simin Niu, Ding Chen, Jiawei Yang, Chunyu Li, Qingchen Yu, and 1 others. 2025a. Memos: memory os for ai system. arXiv preprint arXiv:2507.03724. Zongxi Li, Yang Li, Haoran Xie, and S. Joe Qin. 2025b. CondambigQA: benchmark and dataset for conditional ambiguous question answering. In The 2025 Conference on Empirical Methods in Natural Language Processing. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of LLM agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851 13870, Bangkok, Thailand. Association for Computational Linguistics. 9 Dan McAdams. 2001. The psychology of life stories. Review of General Psychology, 5(2):100122. memodb-io. 2025. Memobase: User profile-based longterm memory for ai applications. Earl Miller and Jonathan Cohen. 2001. An integrative theory of prefrontal cortex function. Annual Review of Neuroscience, 24(1):167202. Jiayan Nan, Wenquan Ma, Wenlong Wu, and Yize Chen. 2025. Nemori: Self-organizing agent memory inspired by cognitive science. arXiv preprint arXiv:2508.03341. NevaMind-AI. 2025. memU: Memory infrastructure for llms and ai agents. Randall OReilly, Rajan Bhattacharyya, Michael Howard, and Nicholas Ketz. 2014. Complementary learning systems. Cognitive science, 38(6):1229 1248. Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. 2023. Memgpt: Towards llms as operating systems. CoRR, abs/2310.08560. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. 2025. Zep: temporal knowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Haoran Sun and Shaoning Zeng. 2025. Hierarchical memory for high-efficiency long-term reasoning in llm agents. arXiv preprint arXiv:2507.22925. supermemoryai. 2025. Supermemory: scalable memory engine and api for ai applications. Memory engine and app optimized for scalable, persistent AI memory storage and retrieval. Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, and 1 others. 2025. In prospect and retrospect: Reflective memory management for long-term personalized dialogue agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84168439. Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori. 2025. Hierarchical reasoning model. arXiv preprint arXiv:2506.21734. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291. Yu Wang and Xi Chen. 2025. Mirix: Multi-agent memory system for llm-based agents. arXiv preprint arXiv:2507.07957. Zheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024. M-rag: Reinforcing large language model performance through retrieval-augmented genIn Proceedings eration with multiple partitions. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19661978. Anne Wilson and Michael Ross. 2003. The identity function of autobiographical memory: Time is on our side. Memory, 11(2):137149. John Wixted. 2004. The psychology and neuroscience of forgetting. Annual Review of Psychology, 55:235 269. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. 2024. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing transformers. In International Conference on Learning Representations. Derong Xu, Yi Wen, Pengyue Jia, Yingyi Zhang, WenLin Zhang, Yichao Wang, Huifeng Guo, Ruiming Tang, Xiangyu Zhao, Enhong Chen, and Tong Xu. 2025a. Towards multi-granularity memory association and selection for long-term conversational agents. CoRR, abs/2505.19549. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025b. A-mem: Agentic memory for LLM agents. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, and Ge Yu. 2024. Activerag: Revealing the treasures of knowledge via active learning. CoRR. Feiyuan Zhang, Dezhi Zhu, James Ming, Yilun Jin, Di Chai, Liu Yang, Han Tian, Zhaoxin Fan, and Kai Chen. 2025a. Dh-rag: dynamic historical context-powered retrieval-augmented generation arXiv preprint method for multi-turn dialogue. arXiv:2502.13847. Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2025b. survey on the memory mechanism of large language model-based agents. ACM Trans. Inf. Syst., 43(6). 10 Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, and Kaixiang Lin. 2025. Do LLMs recognize your preferences? evaluating personalized preference following in LLMs. In The Thirteenth International Conference on Learning Representations. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: enhancing large language models with long-term memory. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence, AAAI24/IAAI24/EAAI24. AAAI Press."
        },
        {
            "title": "A Implementation Details",
            "content": "This appendix provides implementation details for reproducibility. A.1 Agent and Module Mapping Table 4 lists brain-region agents and their memory capacities. Table 5 summarizes core infrastructure modules. These define the minimal components to reproduce BMAM (encode consolidate retrieve revise). Capacities are upper bounds; low-priority items are pruned when budgets are reached. A.2 Architectural Diagrams This section provides detailed architectural diagrams illustrating BMAMs core components and workflows. Memory Lifecycle (Figure 5): The six stages form closed loop: (1) perception extracts entities, temporal expressions, and intent cues; (2) shaping and active learning encode episodes while detecting uncertainty; (3) consolidation promotes high-value memories to semantic form; (4) reflection detects contradictions and calibrates confidence; (5) reconsolidation updates memories when new evidence arrives; (6) forgetting prunes low-salience items. StoryArc Timeline Indexing (Figure 6): StoryArc maintains per-entity timelines where each event is stored with normalized timestamps, enabling temporal queries such as When did happen? and What happened before Y?. Hybrid Retrieval (Figure 7): The four-way hybrid retrieval pipeline processes queries in parallel by BM25 (lexical), dense vectors (semantic), knowledge graph (relational), and StoryArc (temporal). Results are fused using Reciprocal Rank Fusion. Brain Region Mapping (Figure 8): Each BMAM agent corresponds to human brain memory region, preserving the specialized function of its biological counterpart. External Integration (Figure 9): The perception layer receives inputs from LLM APIs, environment sensors, and other agents. The output layer supports memory sharing via portable .bma archives, memory query APIs, and publishsubscribe patterns. A.3 Extended Ablation Results Brain-Region Ablation Results. Table 6 shows overall accuracy when disabling each brain region on the LoCoMo subset (Group 1, 199 questions). Key finding: Hippocampus ablation causes the largest accuracy drop (-24.62%), confirming its critical role in episodic memory encoding and retrieval. Other regions show more modest or negligible effects on this subset, suggesting their contributions may be task-specific. Interpretation. Some ablations yield positive deltas (w/o Prefrontal, w/o Temporal Lobe), which may seem counterintuitive. We attribute this to the tight coupling between BMAM components. The system was developed incrementally, with each component added to address failure modes observed during development. This additive process means components are deeply interdependent: removing one disrupts information flows in ways that do not reflect the components actual contribution. Subset-Level Confidence Intervals. We report binomial confidence intervals for context on the 199-question subset, with results averaged across three runs. Full BMAM achieves 77.39% (154/199, 95% CI: 71.182.6%), while w/o Hippocampus drops to 52.76% (105/199, 95% CI: 45.759.7%). These intervals confirm statistically significant drop for hippocampus ablation. Brain-Region Anti-Erosion Roles. Table 7 provides supplementary mapping of each brainregion component to its hypothesized anti-erosion function. PrefEval Error Analysis. Table 8 breaks down PrefEval outcomes. BMAM achieves 72.9% personalized responses with only 0.1% inconsistency violations, indicating stable preference memory. The 18.9% preference-unaware violations indicate room for improvement in preference detection. Statistical Significance Analysis. Table 10 reports statistical significance for the brain-region ablation study using Wilson score confidence intervals and two-proportion z-tests. Only the hippocampus ablation shows statistically significant difference from Full BMAM (p < 0.001). Figure 10 visualizes these confidence intervals as forest plot. A.4 Extended Visualizations Multi-Benchmark Radar (Figure 11): BMAM achieves the best overall balance, excelling on LoCoMo (long-horizon dialogue) and PrefEval (preference consistency), while remaining competitive on LongMemEval and PersonaMem. LongMemEval Breakdown (Figure 12): 12 BMAM achieves perfect accuracy (100%) on single-session preference extraction. Withinsession recall is also strong (SSU: 87.1%, SSA: 76.8%). However, temporal reasoning (59.4%) and multi-session integration (52.6%) remain challenging. LoCoMo Heatmap (Figure 13): Temporal questions remain the most challenging category across all memory systems. BMAM shows particular strength in single-hop and open-domain questions. A.5 Baseline Comparisons We compare BMAM against memory-augmented LLM systems. Most baseline numbers are reported from the MemOS paper (Li et al., 2025a); we re-ran select baselines using the official MemOS evaluation scripts for direct comparison (marked with ). LoCoMo. Table 11 shows BMAM achieves 78.45% overall accuracy, outperforming re-run MemOS (73.90%). Gains are substantial on singlehop (+17.5%) and multi-hop (+13.1%). Temporal accuracy (62.31%) is lower than Memobase (81.20%) and re-run MemOS (71.34%), suggesting precise date matching remains challenging. LongMemEval. Table 12 tests memory across six categories. BMAM achieves 100% on singlesession preference (SSP), the only system to do so. Within-session recall is strong (SSA: 76.8%, SSU: 87.1%). Temporal reasoning (59.4%) and multi-session (52.6%) lag behind MemOS-1031. PrefEval. Table 13 evaluates preference handling with 10 adversarial turns. BMAM achieves the highest personalized rate (72.9%) with lowest inconsistency (0.1%), indicating stable preference memory. PersonaMem. Table 14 shows BMAM achieves 48.9% precision. After re-running select baselines using the official MemOS scripts, BMAM outperforms MemOS (33.98%) and approaches Mem0 (53.88%). 13 Agent Role in BMAM Cap. Region Anti-Erosion Contribution Hippocampus TemporalLobe Amygdala Prefrontal BasalGanglia TempReasoning episodic encoding, StoryArc semantic memory, KG salience tagging, HRM executive control, query routing, WM buffer procedural memory date/duration queries 20k 70k 1k 10 500 Hippocampus Temporal Lobe Amygdala Prefrontal Temporal Semantic Identity Context Basal Ganglia Procedural Episodic + StoryArc KG consolidation Salience storage Query routing + WM buffer Pattern detection Table 4: Brain-region agents and capacities. Table 7: Brain-region anti-erosion roles. Module Function AdvancedMemorySystem SQL + FAISS vector search KeyValueMemoryStore StoryArcManager ConsolidationPipeline ThalamusAgent AnteriorCingulate BrainInspiredRetrieval discriminative retrieval timeline indexing episodic-to-semantic timescale coordination ACT-style halting fast/slow path + reweight Table 5: Core infrastructure modules. Ablation Acc. (%) Full BMAM w/o Hippocampus w/o Amygdala w/o Basal Ganglia w/o Prefrontal w/o Temporal Lobe 77.39 52.76 75.38 76.88 82.41 81.41 24.62 2.01 0.50 + 5.03 + 4.02 Table 6: Brain-region ablation on LoCoMo subset. Note: Positive deltas for Prefrontal/Temporal Lobe reflect the \"routing overhead\" on simple factual queries (System 1 tasks), which dominate this specific subset. Outcome Count Rate (%) Personalized Response Preference-Unaware Preference Hallucination Unhelpful Response Inconsistency 729 189 67 14 1 72.9 18.9 6.7 1.4 0.1 Table 8: PrefEval outcome breakdown (1000 questions). Component Proxy measurement Ttemporal Ppreference Iidentity Mportability LoCoMo temporal accuracy PrefEval personalized rate PersonaMem accuracy BMA archive fidelity Table 9: Soulfulness metric components. Config Acc. (%) 95% CI p-value Full BMAM w/o Hippocampus w/o Temp. Lobe w/o Prefrontal w/o Amygdala w/o Basal Gang. 77.39 52.76 76.38 75.88 75.38 76.88 [71.1, 82.6] [45.7, 59.7] <0.001*** [70.0, 81.8] [69.5, 81.4] [68.9, 80.9] [70.5, 82.2] 0.81 0.73 0.64 0.90 Table 10: Statistical significance of ablations (Wilson 95% CI, z-test). 14 Method Tokens Single-hop Multi-hop Temporal Open-domain Overall MIRIX Mem0 Zep Memobase Supermemory MemU MemOS-1031 BMAM (ours) 1172 2071 2102 617 507 1582 68.32 73.33 65.23 73.12 66.54 67.80 64.54 82.00 54.26 58.75 52.12 64.65 63.12 51.12 57.29 70.42 68.54 52.54 54.82 81.20 27.17 31.70 71.34 62.31 46.88 45.83 33.33 53.12 50.01 52.67 79.90 79.55 64.33 64.57 59.22 72.01 56.55 56.38 73.90 78. Table 11: LoCoMo benchmark results. Re-run using official MemOS scripts (excludes Adversarial category). Method Tokens SSP SSA Temporal Multi-sess K-Up SSU Overall MIRIX Zep Mem0 Memobase Supermemory MemU MemOS-1031 BMAM (ours) 1.6k 1.1k 1.5k 0.4k 0.5k 1.4k 53.3 53.3 90.0 80.1 89.9 76.7 96.7 100.0 63.6 75.0 26.8 23.2 58.9 19.6 67.9 76.8 25.6 54.1 72.2 75.9 44.4 17.3 77.4 59.4 30.1 47.4 63.2 66.9 52.6 42.1 70.7 52. 52.6 74.4 66.7 89.7 55.1 41.0 74.3 70.5 72.9 92.9 82.9 92.9 85.7 67.1 95.7 87.1 43.5 63.8 66.4 72.4 58.4 38.4 77.8 67.6 Table 12: LongMemEval benchmark results. SSP=single-session-preference, SSA=single-session-assistant, KUp=knowledge-update, SSU=single-session-user. Method Tokens Pref-unaware Pref-halluc Inconsist Unhelpful Personal Bare LLM Bare LLM (+rag) MIRIX Mem0 Zep Memobase Supermemory MemU MemOS-1031 BMAM (ours) 11k 393 90 901 563 135 114 799 93.2 26.6 77.9 14.8 41.0 37.0 23.9 26.5 7.4 18.9 3.9 27.1 72.0 18.4 15.7 25.8 17.2 20.3 18.6 6.7 0.1 3.9 0.0 3.1 2.1 2.0 1.8 1.1 1.4 0.1 0.0 0.0 7.0 0.0 1.3 0.1 0.4 0.2 0.7 1.4 2.8 43.2 7.9 63.7 39.9 34.1 56.7 51.8 71.9 72.9 Table 13: PrefEval results (10 injected adversarial turns). Personal=personalized response rate. Metric MIRIX Mem0 Zep Memobase MemU Supermem MemOS BMAM Precision (%) Tokens 38.4 53.9 57.8 1657 58.9 2092 56.8 496 47.0 204 34.0 1424 48.9 Table 14: PersonaMem precision comparison. Re-run using official MemOS scripts. 15 Figure 5: Memory lifecycle: six-stage loop from perception to continual learning. 16 Figure 6: StoryArc timeline indexing with example temporal queries. Figure 7: Hybrid retrieval with four signal sources and RRF fusion. 17 Figure 8: Brain-region to BMAM agent mapping. Figure 9: External integration: input sources and output interfaces. 18 Figure 10: Forest plot of 95% confidence intervals for brain-region ablation. Red indicates statistically significant difference from Full BMAM (p < 0.001). Figure 11: Multi-benchmark radar comparison. BMAM excels on LoCoMo and PrefEval. 19 Figure 12: LongMemEval per-category breakdown showing BMAMs strengths in preference extraction and within-session recall. Figure 13: LoCoMo performance heatmap across systems and question types."
        },
        {
            "title": "B Case Studies",
            "content": "This section presents concrete examples illustrating each type of soul erosion and how BMAMs architecture addresses them. Case 1: Temporal Erosion Scenario: User discusses career transitions across multiple sessions. Session 1 (Jan 2023): just started my new job at Google. Session 5 (Mar 2023): Im thinking of leaving Google for startup. Session 8 (Jun 2023): accepted the offer from TechStartup Inc. Query: When did leave Google? Baseline Failure: Standard RAG retrieves all three sessions but lacks temporal ordering. Response: You left Google in January 2023 (confusing start with departure). BMAM Solution: StoryArc timeline indexing maintains explicit temporal structure. Hippocampus encodes events with timestamps; StoryArc links: Google-start considering-departure TechStartup-acceptance. Response: You left Google around June 2023 when you accepted the offer from TechStartup Inc. Case 2: Semantic Erosion Scenario: Users dietary preferences evolve over time. Session 2: Im vegetarian for health reasons. Session 15: Ive started eating fish occasionally, pescatarian now. Session 28: Actually, Im back to being fully vegetarian. Query: Whats my current diet? Baseline Failure: Retrieves all three statements with equal weight. Response: You follow pescatarian diet (outdated information). BMAM Solution: Temporal lobe consolidation with confidence tracking. Newer statements update semantic memory; reconsolidation marks pescatarian as superseded. Response: Youre currently vegetarian. You tried pescatarian for while but returned to vegetarian. Case 3: Identity Erosion Scenario: User shares emotionally significant academic milestone. Session 12: finally defended my PhD thesis today! Five years of work on neural memory models. Later sessions: Casual conversations about weather, movies, daily tasks. Query (Session 45): What was my thesis about? Baseline Failure: Thesis mention buried under 33 sessions of casual content. Response: dont have information about your thesis. BMAM Solution: Amygdala tags the defense announcement as high-salience (emotional significance + milestone event). Protected from forgetting despite low access frequency. Response: Your PhD thesis was on neural memory modBaseline Failure: Generates class component with 4space indentation in JavaScript. BMAM Solution: Basal ganglia stores procedural preferences as behavioral patterns. Fixed-point detection recognizes consistent preferences across sessions. Response: Generates functional TypeScript component with Prettier formatting and 2-space indentation. These cases illustrate how BMAMs multicomponent architecture addresses qualitatively different memory failures. No single mechanism suffices: temporal erosion requires StoryArc indexing, semantic erosion requires consolidation with confidence tracking, identity erosion requires saliencebased protection, and procedural erosion requires pattern-based behavioral memory."
        },
        {
            "title": "C Prompt Templates",
            "content": "This section provides key prompts used in BMAM, directly from the codebase. Query Classification (adaptive_config.py) Analyze this query and rate each dimension from 0.0 to 1.0: Query: \"{query}\" Dimensions: - temporal: Time/sequence reasoning (when, before, after, order of events) - identity: Personal info recall (my name, my preferences, what told you) - preference: Choice/comparison (prefer, favorite, which do like better) - factual: General fact lookup (what is X, define, explain) Return ONLY valid JSON: {\"temporal\": 0.X, \"identity\": 0.X, \"preference\": 0.X, \"factual\": 0.X} Memory Compression (clean_agent_system.py) For the user query \"{query}\", compress the following memories into key facts (3-5 points): 1. [memory 1] 2. [memory 2] ... els. You defended it successfully after five years of reKeep only core information directly relevant search. Case 4: Procedural Erosion Scenario: User establishes coding workflow preferences. Session 3: Always use TypeScript, never plain JavaScript. Session 7: Format code with Prettier, 2space indentation. Session 20: prefer functional components over class components in React. Query: Write me React component for login form. to the query. Be concise. Semantic Consolidation (memory_coordinator.py) Extract core semantic knowledge from the following {N} episodic memories: Date: {date} Episodic memories: 21 {combined_content} Please extract: 1. Core facts and knowledge points 2. Common themes or patterns 3. Important entity relationships"
        },
        {
            "title": "PrefEval",
            "content": "Evaluation scripts follow MemOS protocol (repo/version cited in the Evaluation Protocol footnote) Output as concise semantic knowledge (2-"
        },
        {
            "title": "Compute Requirements",
            "content": "sentences). Hardware: Single machine, no GPU required Context Compaction (context_compaction.py) (API-based inference) Analyze the following conversation history and extract structured notes: {history_text} Output format: 1. Core facts: [List 3-5 key pieces of information] 2. User preferences: [If any] 3. Pending tasks: [Incomplete tasks] 4. Key decisions: [Important decisions made] 5. Open questions: [Unresolved questions] Requirement: Be extremely concise, keep only LLM: gpt-4o-mini for response generation and judging Embedding: text-embedding-3-small (1536 dimensions)"
        },
        {
            "title": "Random Seeds",
            "content": "Embedding and retrieval are deterministic LLM responses use temperature 0.0 for judge, 0.7 for generation the most important information. Results may vary slightly across runs due to"
        },
        {
            "title": "D Hyperparameters",
            "content": "Table 15 lists the key hyperparameters used in BMAM experiments. Parameter Brain Region Capacities Hippocampus episodic store Temporal lobe semantic store Amygdala salience buffer Prefrontal working memory Basal ganglia procedural store Embedding & LLM Embedding model Embedding dimension Response LLM Judge LLM Temperature (generation) Temperature (judge) Value 20,000 70,000 1,000 10 text-embed-3-small 1536 gpt-4o-mini gpt-4o-mini 0.7 0.0 Table 15: BMAM hyperparameters. LLM non-determinism"
        },
        {
            "title": "Evaluation Protocol",
            "content": "Memory cleared between independent test units (groups/users/samples) Memory preserved within each unit for sequential context LLM judge verifies semantic correctness, not exact string match Following MemOS evaluation protocol for fair comparison"
        },
        {
            "title": "Limitations of Reproducibility",
            "content": "API model versions may change over time Some baseline numbers from MemOS paper; select baselines re-run with official scripts"
        },
        {
            "title": "E Reproducibility Checklist",
            "content": "Minor hyperparameter sensitivity not fully We provide the following information for reproducibility: characterized"
        },
        {
            "title": "Code and Data",
            "content": "Implementation will be released upon acceptance Benchmark datasets are publicly available: LoCoMo, LongMemEval, PersonaMem,"
        }
    ],
    "affiliations": [
        "Guangdong Institute of Intelligence Science and Technology, Zhuhai, China",
        "Institute of Science Tokyo",
        "The Hong Kong Polytechnic University"
    ]
}