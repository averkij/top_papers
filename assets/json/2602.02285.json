{
    "paper_title": "Statistical Learning Theory in Lean 4: Empirical Processes from Scratch",
    "authors": [
        "Yuanhe Zhang",
        "Jason D. Lee",
        "Fanghui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 5 8 2 2 0 . 2 0 6 2 : r Statistical Learning Theory in Lean 4: Empirical Processes from Scratch Yuanhe Zhang Jason D. Lee Fanghui Liu February 3, 2026 Abstract We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including complete development of Gaussian Lipschitz concentration, the first formalization of Dudleys entropy integral theorem for sub-Gaussian processes, and an application to leastsquares (sparse) regression with sharp rate. The project was carried out using humanAI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing granular, line-by-line understanding of the theory. This work establishes reusable formal foundation and opens the door for future developments in machine learning theory. Github: https://github.com/YuanheZ/lean-stat-learning-theory"
        },
        {
            "title": "Introduction",
            "content": "Statistical learning theory (SLT), and more generally, machine learning theory, successfully guided the progress of machine learning over the past two decades, informing foundational concepts such as bias-variance trade-offs, regularization, and cross-validation (Hastie et al., 2009). Now it tries to capture the picture for complex architectures such as deep neural networks (LeCun et al., 2015) and large language models (Brown et al., 2020), e.g., double descent (Belkin et al., 2019; Mei & Montanari, 2022), benign overfitting (Bartlett et al., 2020; Tsigler & Bartlett, 2023), and single/multi-index model (Montanari & Urbani, 2025; Abbe et al., 2022; Bruna & Hsu, 2025). However, as models become increasingly complex, contemporary theoretical analyses have grown substantially longer and more intricate. Modern proofs often rely on wide range of advanced mathematical tools or inspired by statistical physics. This broad techniques place significant strain on human review (i.e., verification at scale): it becomes difficult to verify intermediate lemmas, track logical dependencies, and clearly identify which techniques are applicable at each stage of the argument. Besides, some core techniques in SLT, e.g., concentration inequalities, covering, are without structured, machine-readable library, leading to untapped reusability. Formalization in interactive theorem provers such as Lean 4 (Moura & Ullrich, 2021) addresses both challenges and is rapidly gaining traction in the math community. By encoding proofs in formal language, we obtain machine-checkable correctness guarantees while simultaneously creating structured, queryable library of results. We argue that formalizing SLT is not merely an exercise in rigor, but Department of Statistics, University of Warwick, United Kingdom; Email: yuanhe.zhang@warwick.ac.uk Department of Electrical Engineering and Computer Sciences, also Department of Statistics, University of California, Berkeley, USA; Email: jasondlee@berkeley.edu School of Mathematical Sciences, Institute of Natural Sciences and MOE-LSC, Shanghai Jiao Tong University, China. Part of work was done at Department of Computer Science, and Centre for Discrete Mathematics and its Applications (DIMAP), University of Warwick, United Kingdom; Email: fanghui.liu@{sjtu.edu.cn,warwick.ac.uk} (Corresponding author) 1 foundation for scalable, automated theoretical analysis of machine learning systems. Current Lean 4 implementation in machine learning includes reinforcement learning theory (Zhang, 2025), optimization (Li et al., 2024; 2025a;b). The most close to our work is conducted by Sonoda et al. (2025) on generalization bounds via Rademacher complexity, but limited to simple settings, see the discussion in Section 2.2. Unlike more self-contained mathematical areas such as number theory or algebrawhere formalization has flourished thanks to clean axiomatic foundationsSLT lies at the intersection of multiple disciplines, related to empirical process theory (Van Der Vaart & Wellner, 1996). To be specific, as shown in Fig. 1, the excess risk of learning algorithm is governed by the supremum of an empirical process indexed by the loss class. Controlling this supremum requires two interlocking components: concentration inequalities (Boucheron et al., 2013) that convert high-probability bounds into link to complexity measure, and capacity control that quantifies the effective size of localized function classes via complexity measure and metric entropy. Each tool demands careful treatment of measurability, integrability, and topological assumptions that textbooks routinely leave implicit. More importantly, these tools remain extremely undeveloped in Lean 4. In this work, we rise to this challenge by formalizing all infrastructures for SLT in Lean 4 from scratch. Starting from basic measure-theoretic probability and analysis, we systematically develop the full stack of tools required for modern generalization analysis. The dependency structure, illustrated in Fig. 1, reveals our formalization path, including several key parts of representative books (Wainwright, 2019; Boucheron et al., 2013). Our contributions are: Figure 1: Lean formulation for Localized Empirical Process Framework. It includes the blue part for the capacity control and the red part for concentration. The colored zone indicates the major results in the chapters of Wainwright (2019) (High Dimensional Statistics, HDS) and Boucheron et al. (2013) (Concentration Inequality, CI). 1. Implementation on Gaussian Lipschitz Concentration: We construct complete formal development of Gaussian Lipschitz concentration which requires building substantial infrastructure across Efron-Stein inequality, Gaussian Poincaré inequality, Density argument, and Gaussian logarithmic Sobolev inequality (LSI). The Gaussian LSI, in particular, is foundational tool in high-dimensional probability with far-reaching applications beyond learning theory. To our knowledge, this constitutes the first formalization of the complete Gaussian analysis tools in any theorem prover. 2. Implementation on Dudleys Entropy Integral: We provide the first formalization of Dudleys entropy integral theorem for sub-Gaussian processes in Lean 4. This is cornerstone result in empirical process theory that bounds the expected supremum of stochastic process by an integral involving metric entropy. Our formalization encompasses the full generality of sub-Gaussian processes. The development required formalizing the sophisticated chaining technique that decomposes stochastic process into telescoping sum over dyadic approximations, along with rigorous treatment of covering and packing numbers in metric spaces. 3. Application: Least-Squares Framework via Localized Empirical Process: We demonstrate the practical utility of our formalizations by developing unified framework for least-squares regression based on the localized empirical process. We further test the functionability of our formal unified framework on linear regression and ℓ1-constrained regression to obtain sharp rates up to minimax-level. 2 4. Human-AI Collaborative Formalization Paradiagm: Our formalization is completed through structured collaboration between human mathematicians and Claude Code (Anthropic, 2025a) with Opus-4.5 (Anthropic, 2025b). Humans analyze Mathlibs infrastructure, design proof strategies, and decompose complex theorems into manageable lemmas; the AI agent executes these plans and constructs formal proofs. The entire process totals 500 hours of supervised development, with all formalizations compiled without sorry or axiom. This demonstrates that large-scale formalization projects, previously requiring years of expert effort, can be substantially accelerated through careful human-AI collaboration. The scale of our contribution is substantial: the project comprises approximately 30,000 lines of Lean 4 code. We provide list of our major formalizations in Section A. Crucially, this effort goes far beyond implementation. Achieving complete formalization requires granular, line-by-line understanding of SLT: every definition, assumption, inequality, and logical dependency must be explicitly identified, verified, and composed into coherent proof structure. This makes the project particularly valuable for student training in theoretical machine learning. Engaging with the formalization demands mastery of the full technical stack of SLT rather than superficial familiarity. By providing rigorous, end-to-end formal infrastructure for SLT grounded in the empirical process, this work offers principled training ground and opens the door for students seeking to develop deep theoretical competence in modern machine learning."
        },
        {
            "title": "2 SLT from Natural Language to Lean 4",
            "content": "In this section, we take an overview of the structure of SLT in natural language and diagnose what are missing or should be built in Lean 4."
        },
        {
            "title": "2.1 Statistical learning theory for generalization",
            "content": "Empirical process theory provides unified uniform convergence framework for generalization guarantees of learning algorithms by exploiting the geometry of function classes. As shown in Fig. 1, formally, let be hypothesis class and ℓf (z) be loss function associated with F. For broad class of empirical risk minimization and regularized learning procedures, the excess risk of an estimator ˆf admits (cid:12)(ˆP P)(ℓf ℓf )(cid:12) (cid:12) R( ˆf ) R(f ) sup (cid:12) (cid:125) (cid:123)(cid:122) (cid:124) empirical process fluctuation +(confidence) , where and ˆP denote the population and empirical measures, respectively. This decomposition highlights that generalization is governed by the uniform deviation of an empirical process indexed by the excess loss class. The global structure of this framework includes two main parts via several probabilistic toolbox (see Fig. 1). Concentration: High-probability bounds on the empirical process fluctuation can be obtained via concentration inequalities, such as Gaussian Lipschitz concentration (Wainwright, 2019, Corollary 14.15). These yield bounds in the form of critical radius δ. Capacity Control: The key to obtain δ is localization: rather than controlling the empirical process over the entire class F, one restricts attention to localized class F(δ) = {f : d(f, ) δ} consisting of functions within radius δ of the optimum with respect to suitable metric d. The effective size of F(δ) is quantified by localized complexity measures, e.g., Gaussian complexity Gn(F(δ)) and Rademacher complexity Rn(F(δ)). Their connection to geometry is made explicit through metric entropy, log (F(δ), ϵ, d) defined via covering numbers: how many balls of radius ϵ are needed to cover F(δ). In particular, chaining arguments, most notably Dudleys entropy integral, relate localized Gaussian complexity to metric entropy: Gn(F(δ)) 1 (cid:112)log (ϵ, F(δ), d) dϵ . (cid:90) 2δ 3 This characterization reveals how complexity accumulates across scales and leads to solution set whose smallest solution defines the critical radius δ. The resulting bounds recover sharp minimax rates in parametric settings and extend naturally to nonparametric models."
        },
        {
            "title": "2.2 Formalization Gaps",
            "content": "Despite the theoretical maturity of the above framework, its formalization in Lean 4 remains in its infancy. Recent work by Sonoda et al. (2025) formalizes generalization bounds via Rademacher complexity, including basic tools such as McDiarmids inequality and Hoeffdings lemma. However, their analysis controls the empirical process over the entire function class, leading to loose rates and limited applications. The sharper localized empirical process framework goes beyond this in two folds: First, the required concentration machinery in this project is substantially more advanced. While McDiarmid-type inequalities suffice under boundedness assumptions, localized analysis relies on Gaussian Lipschitz concentration, whose proof draws on deep chain of results from functional analysis and probability theory, that requires formulation or significant changes in Lean 4. Second, the capacity control has quite limited formulation in latest Lean library. Formalizing localization requires developing covering numbers, chaining arguments underlying Dudleys integral, localized complexity measures, and the fixed-point analysis that determines the critical radius. Our work bridges precisely this gap via the comprehensive Lean 4 formalization. Importantly, this effort goes far beyond mechanical translation. Natural-language proofs routinely suppress measurability and topological assumptions, conflate almost-sure and pointwise statements, and compress multi-step arguments into informal phrases. Formalization forces each of these gaps to be made explicit and resolved. Moreover, Lean demands careful proof engineering: for example, formulazation of Dudleys entropy integral requires systematic coordination between different notions of integration (e.g., Bochner and interval integrals), which are distinct in Lean but mixed in language proofs."
        },
        {
            "title": "3 Formulation details and challenges",
            "content": "In this section, we present the details of our formalization (see Fig. 2), covering Gaussian Lipschitz concentration in Section 3.1 and Dudleys entropy integral bound in Section 3.2. For clarity, we first state each result in natural-language theorem form, followed by the corresponding Lean 4 formalization, where we explicitly discuss the key modeling and proof-engineering challenges."
        },
        {
            "title": "3.1 High-Dimensional Gaussian Analysis Toolbox",
            "content": "Gaussian functional inequalities are the backbone of HDS and SLT. However, the complete proofs form long chain of disparate methods. Each link relies on different piece of analysis, and each is nontrivial to formalize. We build reusable, deliberately end-to-end formal toolbox that supports the full analytic pipeline from scratch, see the red part of Fig. 2 with the following steps. Theorem 3.1 (i. Efron-Steins Inequality, Theorem 3.20 in Boucheron et al. (2013)). Let = (X1 , ... , Xn) be vector of independent random variables and = (X) be square-integrable function of X. Denote E(i) as the conditional expectation conditioned on (X1 , ... , Xi1 , Xi+1 , ... , Xn). Then, Var(Z) (cid:88) (cid:20)(cid:16) i=1 E(i)[Z] (cid:17)2(cid:21) . We start with formalizing E(i) as follows: noncomputable def condExpExceptCoord (i : Fin n) (f : (Fin Ω) R) : (Fin Ω) := fun => (cid:82) y, (Function.update y) (µs i) 4 Figure 2: The dependency graph of our formalizations. All the contents in the graph have not been implemented in Lean 4 before. Implementation challenge: Theorem 3.1 allows each Xi to have distinct distribution µi, which complicates measure-theoretic arguments for coordinate-wise updates. We address this by formalizing universal transfer lemma: resampling one coordinate with fresh independent sample preserves the joint distribution. Though requiring extra formalization of measure-theoretic machinery (measure rectangles), this lemma powers 20+ usages across tower properties, Fubini-style swapping, and slice integrability. Then, we can formalize Theorem 3.1 as: theorem efronStein (f : (Fin Ω) R) (hf : MemLp 2 µs) : variance µs Σ : Fin n, (cid:82) x, (f - condExpExceptCoord (µs := µs) x)^2 µs := by ii. Gaussian Poincaré Inequality: To formalize Gaussian Poincaré inequality, we need to use Efron-Steins infrastructures and require series of results as below. Implementation challenge: Formalization combines Taylor expansion bounds with weak convergence of Rademacher sums to Gaussian, which needs careful measure-theoretic tracking through bounded continuous function wrappers and coordinate-permutation symmetry. Notice that such machinery is frequent in Gaussian analysis. Corollary 3.2. Let be standard Gaussian random variable and (R). Then, Var[f (X)] (cid:2)f (X)2(cid:3) . Notice that Corollary 3.2 is not directly used to derive the Gaussian LSI, instead its intermediate proof is re-used. We formalize Corollary 3.2 as: theorem gaussianPoincare {f : R} (hf : CompactlySupportedSmooth f) : variance (fun => x) stdGaussian.toMeasure (cid:82) x, (deriv x)^2 stdGaussian.toMeasure := by iii. Density Arguments: The density arguments provide an efficient tool which let people prove inequalities for smooth and compactly supported function class (easier to apply convergence theorems) then extend to general class by such argument. This is the key to: 1) extend Gaussian LSI from to 1, and 2) extend Gaussian Lipschitz concentration from to the general Lipschitz class. Such arguments are often skipped in textbooks (Boucheron et al., 2013) due to the complexity. We start with defining the membership of Gaussian Sobolev space1 1,2(γn) as: 1Here should be continuously differentiable which ensures the derivative is well-defined so we can use fderiv from Lean 4. 5 def MemW12Gaussian (n : N) (f : R) (γ : Measure (E n)) : Prop := MemLp 2 γ MemLp (fun (cid:55) fderiv x) 2 γ and the squared Gaussian Sobolev norm as: noncomputable def GaussianSobolevNormSq (n : N) (f : R) (γ : Measure (E n)) : R0 := eLpNorm 2 γ ^ 2 + eLpNorm (fun (cid:55) fderiv x) 2 γ ^ 2 The main density theorem is provided by: Theorem 3.3. The space of smooth compactly supported functions the standard Gaussian measure. is dense in 1,2(γn), where γ is This can be used to extend Gaussian LSI to 1 class with the following Lean 4 formulation. theorem dense_smooth_compactSupport_W12Gaussian : : R, MemW12Gaussian (stdGaussianE n) Differentiable Continuous (fun => fderiv x) ε > 0, : R, ContDiff ( : N) HasCompactSupport GaussianSobolevNormSq (f - g) (stdGaussianE n) < ENNReal.ofReal ε := by Remark: stdGaussianPi is the product measure of independent standard Gaussians, with its pushforward stdGaussianE to Euclidean space via the equivalence. To extend the concentration theorem, we need specialized density lemma based on the Lipschitz mollification (Rn) with (cid:82) ρ(x) dx = 1 then define technique. In our formalization, we pick nonnegative mollifier ρ smooth approximation to Lipschitz function via ρϵ(x) = ϵnρ(x/ϵ) , fϵ := ρϵ . (1) (Rn) and preserves the Lipschitz constant of . The lemma is presented as: Notice that fϵ lives within Lemma 3.4. Let : Rn be Lipschitz function, there is constant Cρ := (cid:82) u2ρ(u) du < such that sup xRn fϵ(x) (x) LCρϵ . Hence, fϵ uniformly as ϵ 0. Our formalization of Lemma 3.4 is: theorem mollify_tendsto_of_lipschitz {f : R} {L : R0} (hf : LipschitzWith f) (x : n) : Filter.Tendsto (fun ε => mollify ε x) (nhdsWithin 0 (Set.Ioi 0)) (nhds (f x)) := by Implementation challenge: We formalize large amount of smooth approximation and convolutions in this part, which is an integration of functional analysis with measure-theoretic probability theory. Theorem 3.5 (iv. Gaussian LSI, Theorem 5.4 of (Boucheron et al., 2013)). Let = (X1 , ... , Xn) be vector of independent standard Gaussian random variables and : Rn be continuously differentiable function with E[f (X)] < . Then, Ent(f 2) 2E (cid:2)f (X) 2 (cid:3) . By defining the entropy Ent(f ) as Ent(f ) = [f (X) log (X)] E[f (X)] log E[f (X)] , with its formulation, def entropy (µ : Measure Ω) (f : Ω R) : := (cid:82) ω, ω * log (f ω) µ - ((cid:82) ω, ω µ) * log ((cid:82) ω, ω µ) 6 now we formalize Theorem 3.5 as: theorem gaussian_logSobolev_W12_pi {n : N} {g : (Fin R) R} (hg : MemW12GaussianPi (stdGaussianPi n)) (hg_diff : Differentiable g) (hg_grad_cont : i, Continuous (fun => partialDeriv x)) (hg_log_int : Integrable (fun => (g x)^2 * log ((g x)^2)) (stdGaussianPi n)) : entropy (stdGaussianPi n) (fun => (g x)^2) 2 * (cid:82) x, gradNormSq (stdGaussianPi n) := by where gradNormSq is the squared norm of gradients. In the next, we briefly present the high-level proof strategy for formalization. We first formalize onedimensional case and generalize to dimension-free via tensorization later. Now let ε1 , ... , εn be independent Rademacher random variables and fix 2 k=1 εk, building upon the infrastructures in Theorem 3.1, we use Taylors limit and CLT to obtain (R). Define the Rademacher sum Sn := n1/2 (cid:80)n lim (cid:88) (cid:20) (cid:18) Sn + k=1 1 εk (cid:19) (cid:18) Sn (cid:19)(cid:21)2 1 + εk = 4E (cid:2)f (X)2(cid:3) , (0, 1) . For 2 (R), by CLT, we can obtain Ent (cid:2)f 2(Sn)(cid:3) = Ent[f (X)2] . lim (2) (3) We then bridge Eq. (2) and Eq. (3) by formalizing the Bernoulli logarithmic Sobolev inequality (LSI) (Boucheron et al., 2013, Theorem 5.1) then taking limit to both sides, we can derive the 1D Gaussian LSI for 2 (R), i.e. Ent(f 2) 2E (cid:2)f (X)2(cid:3). (R) version to Remark: For the proof of Gaussian Lipschitz concentration, we can directly tensorize this 2 be dimension-free. We further use Theorem 3.3 to extend the above inequality from to 1(R) for general toolbox. Since Gaussian LSI is direct consequence of entropy subadditivity and one-dimensional case of LSI, we need the formulation of the subadditivity of entropy theorem (Boucheron et al., 2013, Theorem 4.22), the key technique of tensorization. Theorem 3.6 (v. Tensorization, Theorem 4.22 of (Boucheron et al., 2013)). Let = (X1 , ... , Xn) be vector of independent random variables and = (X) be nonnegative measurable function of such that Φ(Y ) = log is integrable. Define Ent(i)(Y ) as the conditional entropy given (X1 , ... , Xi1 , Xi+1 , ... , Xn). Then, Ent(Y ) Ent(i)(Y ) . (cid:34) (cid:88) (cid:35) i=1 The formalization follows from similar telescoping strategy in Theorem 3.1 and our formalization of duality formula Ent(Y ) = sup [Y (log log E(T ))] , where the supremum is over all integrable and nonnegative random variables. theorem entropy_subadditive (f : (Fin Ω) R) (hf_meas : Measurable f) (hf_nn : 0 m[µs] f) (hf_int : Integrable µs) (hf_log_int : Integrable (fun => * log (f x)) µs) : LogSobolev.entropy µs Σ : Fin n, (cid:82) x, condEntExceptCoord (µs := µs) µs := by Theorem 3.7 (vi. Gaussian Lipschitz Concentration, Theorem 5.6 of (Boucheron et al., 2013)). Let = (X1 , ... , Xn) be vector of independent standard Gaussian random variables and : Rn be L-Lipschitz function. Then, for any > 0, (f (X) [f (X)] t) 2 exp (cid:18) t2 2L2 (cid:19) . 7 We formalize Theorem 3.7 as: theorem gaussian_lipschitz_concentration {f : (EuclideanSpace (Fin n)) R} {L : R0} (hn : 0 < n) (hL : 0 < L) (hf : LipschitzWith f) (t : R) (ht : 0 < t) : let µ := stdGaussianE (µ {x - (cid:82) y, µ}).toReal 2 * exp (-t^2 / (2 * (L : R)^2)) := by We briefly describe the proof strategy. We formalize the Herbst argument for fϵ defined in Eq. (1). For any λ R, we apply the Theorem 3.5 to the function eλfϵ(X)/2 Ent(eλfϵ ) 2E (cid:13) (cid:13) (cid:13)eλfϵ(X)/2(cid:13) 2 (cid:13) (cid:13) 2 λ2L2 2 (cid:104) eλfϵ(X)(cid:105) . By differential inequality (we formalize as Gronwall-type ratio bound) and taking limit by Lemma 3.4, we can obtain log exp λ(f (X) Ef (X)) L2 , (cid:16) (cid:17) λ2 2 completing the proof of Theorem 3.7 via Chernoffs bound."
        },
        {
            "title": "3.2 Dudley’s Entropy Integral Bound",
            "content": "Dudleys bound is the canonical bridge to link covering number with complexity measures. formal proof of the general Dudleys bound thus supplies foundational, widely reusable theorem that supports broad range of theoretical results. Textbook statements (Boucheron et al., 2013; Vershynin, 2018; Wainwright, 2019) typically have incomplete hypotheses such as skipping integrability or hiding the constant. Our formalization uses the following statement. Theorem 3.8 (Dudleys Entropy Integral Bound). Let (A, d) be pseudo-metric space and totally bounded set with diameter at most > 0. Let {Xt}ts be normalized sub-Gaussian process with parameter σ > 0, which has integrable exponential-moment for increments and continuous sample paths on s, assume that the entropy integral is finite. Then, (cid:20) Xt sup ts (cid:21) 12 2σ (cid:90) (cid:112)log (ε, s, d) dε , 0 where (ε, s) denotes the ε-covering number of s. The formalization of Theorem 3.8 is given by: theorem dudley {µ : Measure Ω} [IsProbabilityMeasure µ] {X : Ω R} {σ : R} (hσ : 0 < σ) (hX : IsSubGaussianProcess µ σ) {s : Set A} (hs : TotallyBounded s) {D : R} (hD : 0 < D) (hdiam : Metric.diam D) (t0 : A) (ht0 : t0 s) (hcenter : ω, t0 ω = 0) (hX_meas : t, Measurable (X t)) (hX_int_exp : : A, : R, Integrable (fun ω => Real.exp (l * (X ω - ω))) µ) (hfinite : entropyIntegralENNReal = ) (hcont : ω, Continuous (fun (t : s) => t.1 ω)) : (cid:82) ω, s, ω µ (12 * Real.sqrt 2) * σ * entropyIntegral := by We build dudley totally from scratch. We start with ϵ-nets: def IsENet {A : Type*} [PseudoMetricSpace A] (t : Finset A) (eps : R) (s : Set A) : Prop := (cid:83) t, closedBall eps We then define the covering number (ϵ , , d) as the minimal cardinality of an ϵ-net: def coveringNumber {A : Type*} [PseudoMetricSpace A] (eps : R) (s : Set A) : WithTop Nat := sInf {n : WithTop Nat : Finset A, IsENet eps (t.card : WithTop Nat) = n} We then define the metric entropy as the logarithm of the covering number, with appropriate handling of edge cases: def metricEntropy (eps : R) (s : Set A) : := match coveringNumber eps with => 0 (n : N) => if 1 then 0 else Real.log Taking the square root of entropy, denoted sqrtEntropy from metricEntropy, we formulate the entropy integral via two-level design. The canonical definition uses extended non-negative reals: def entropyIntegralENNReal (s : Set A) (D : R) : R0 := (cid:82) eps in Set.Ioc 0 D, ENNReal.ofReal (sqrtEntropy eps s) real-valued wrapper entropyIntegral extracts the toReal component under finiteness hypothesis. Next, we formalize sub-Gaussian processes via moment generating function bounds, i.e. def IsSubGaussianProcess (µ : Measure Ω) (X : Ω R) (σ : R) : Prop := : A, : R, µ[fun ω => exp (l * (X ω - ω))] exp (l^2 * σ^2 * (dist t)^2 / 2) The chaining argument constructs hierarchy of ε-nets at dyadic scales εk = 2k, encapsulated in our DyadicNets structure. The key technique is telescoping decomposition: for any TK, we write Xu Xt0 as base term from the coarsest net plus increments (cid:80)K1 k=0 (Xπk+1(u) Xπk(u)) through successive projections πk. Applying finite maximum bounds for sub-Gaussian processes to each increment and summing yields bound in terms of the dyadic sum RK(s, D) := (cid:80)K1 k=0 εk The proof then proceeds through two limit arguments. First, we extend from finite nets to countable dense sequence via Fatous lemma. Since Fatou requires nonnegative integrands but the supremum may be negative, we introduce shift function that cancels in expectation. Second, we extend to the uncountable set by exploiting path continuity which concludes the target. We present detailed formalization proof in three stages at Section B. (cid:112)log (εk, s, d). Implementation challenge: Lean 4 has two integration formalisms: the nonnegative improper integral (cid:82) for R0 and the interval integral for real-valued functions. (cid:82) is technically convenient for measure-theoretic arguments such as Fubini above, but it lives in R0, so every real-valued bound requires ofReal/toReal conversions and extra side conditions. Real-valued inequalities are far smoother in R, such as taking limit and integration. Hence we define the entropy integral canonically in ENNReal, but state Dudleys bound with entropyIntegral for user-friendly downstream use without loss of generality."
        },
        {
            "title": "4 Application: Least Squares Framework",
            "content": "In this section, building on Wainwright (2019, Chapter 13), we present our formalization of the least-squares framework, including linear regression (Section 4.1) and ℓ1-constrained regression following Raskutti et al. (2011) (Section 4.2). Both rely on localized capacity control via covering numbers, leveraging the infrastructure developed in Section 3. To present clearly, we follow Wainwrights approach to focus on the prediction error, which has direct translation to excess risk via Wainwright (2019, Corollary 14.15). Problem Setup. Consider the nonparametric regression model yi = (xi) + σwi for = 1, . . . , n, where wi i=1(f (xi) yi)2. Our goal is to control the prediction error ˆf 2 i.i.d. (0, 1). Given hypothesis class F, the empirical risk minimizer is ˆf := argminf i=1( ˆf (xi) (xi))2. := 1 (cid:80)n (cid:80)n 1 We encapsulate this setup in RegressionModel structure and formalize the ERM property: structure RegressionModel (n : N) (X : Type*) where : Fin X; f_star : R; σ : R; hσ_pos : 0 < σ noiseDistribution : Measure (Fin R) := stdGaussianPi 9 def isLeastSquaresEstimator (y : Fin R) (F : Set (X R)) (x : Fin X) (f_hat : R) : Prop := f_hat F, Σ i, (y - f_hat (x i))^2 Σ i, (y - (x i))^2 Localization. We define the shifted class := {f : F} and assume it is star-shaped : 0 and αh for all and α [0, 1]. The localized Gaussian complexity at radius δ is Gn(F(δ)) := Ew sup gF gnδ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:88) i=1 wig(xi) . (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Main Results. The central bridge to link prediction error with Eq. (4) is the critical inequality Next, we use Theorem 3.7 to formalize the master error bound (Wainwright, 2019, Theorem 13.5). Gn (F(δ)) δ δ 2σ . (4) (5) Theorem 4.1 (Master error bound, Theorem 13.5 of (Wainwright, 2019)). Suppose is star-shaped, and let δn > 0 be any radius satisfying Eq. (5). Then, for any δn, we have (cid:1) exp(ntδn/2σ2) . (cid:0)f 2 16tδn To apply this bound, it remains to upper bound Eq. (4). Such an upper bound can be substituted into Eq. (5) to obtain an explicit upper bound on the critical radius δ. To this end, we use Theorem 3.8 to formalize the following capacity control: Theorem 4.2. For any star-shaped class , we have Gn (F(δ)) 24 2 (cid:90) 2δ 0 (cid:112)log (ϵ, F(δ), n) dϵ . Remark: Due to the page limit, we present the Lean details of Theorems 4.1 and 4.2 in Section C. This reduces the problem to bounding covering numbers, which we demonstrate in two applications to verify the functionality of our framework and shape the formalization standard for covering calculus."
        },
        {
            "title": "4.1 Linear Regression",
            "content": "We consider the linear regression case (n d) where the ground truth model is yi = θ, xi + σwi associated with the linear predictor class = {f () = θ, : θ Rd} formalized as linearPredictorClass. We apply the general framework to obtain the following rate theorem. Theorem 4.3. Let Rnd be the design matrix, define := rank(X). Then, for the linear predictor class F, (cid:18) ˆf 2 C1 (cid:19) σ2r 1 exp (C2r) , for some constants C1 , C2 > 0. Our formalization is: theorem linear_minimax_rate_rank (hn : 0 < n) (M : RegressionModel (EuclideanSpace (Fin d))) (hf_star : M.f_star linearPredictorClass d) (hr : 0 < designMatrixRank M.x) (f_hat : (Fin R) (EuclideanSpace (Fin d) R)) (hf_hat : w, isLeastSquaresEstimator (M.response w) (linearPredictorClass d) M.x (f_hat w)) : C1 C2 : R, C1 > 0 C2 > 0 (stdGaussianPi {w (empiricalNorm (fun => f_hat (M.x i) - M.f_star (M.x i)))^2 C1 * M.σ^2 * (designMatrixRank M.x) / n}).toReal 1 - exp (-C2 * (designMatrixRank M.x)) := by 10 where designMatrixRank is r. We briefly present the formalization strategy. We apply Theorem 4.2 to then our goal is to upper bound the covering number. Then, we formalize the following Euclidean reduction log (ϵ, F(δ), n) log (ϵ, Br 2(δ), 2) , 2(δ) is the ℓ2 ball of radius δ on Rr. We then formalize the covering number bound on ℓ2 ball where Br (Vershynin, 2018, Corollary 4.2.11). Theorem 4.4. The covering numbers of ℓ2 ball of radius on Rι satisfy for any ϵ > 0: (ϵ, Bι 2(R), 2) (cid:18) 1 + (cid:19)ι . 2R ϵ theorem coveringNumber_euclideanBall_le {R eps : R} (hR : 0 R) (heps : 0 < eps) : ((coveringNumber eps (euclideanBall : Set (EuclideanSpace ι))).untop (ne_top_of_lt (coveringNumber_lt_top_of_totallyBounded heps (euclideanBall_totallyBounded R))) : R) (1 + 2 * / eps) ^ Fintype.card ι := by Therefore, we can get δ = O((cid:112)r/n) to conclude Theorem 4.3."
        },
        {
            "title": "4.2 High-Dimensional ℓ1 Regression",
            "content": "We consider the ℓ1-constrained regression (equivalent to Lasso), which allows for > case. The function 1(R) is ℓ1-ball of radius on Rd. Following the setting in class is FR = {f () = θ, : θ Bd Raskutti et al. (2011), the key of deriving rate is the Euclidean covering of ℓ1-convex hull. We formalize this bound as: 1(R)} where Bd Lemma 4.5. Assume is normalized column-wise to have ℓ2 norm bounded by n. For any ϵ > 0. (cid:0)ϵ, absconv1(X/ n; R), 2 (cid:1) (2d + 1)R2/ϵ2 , where absconv1(X/ n; R) := {Xθ/ ϵ2 log d, which can admit the O(R(cid:112)log d/n)-rate. The empirical covering follows log (ϵ, FR(δ), n) R2 The implementation challenge of Lemma 4.5 arises from the need of Maureys argument, see more details in Section D. : θ1 R}."
        },
        {
            "title": "5 Conclusion",
            "content": "We present the first large-scale Lean 4 formalization of SLT-approximately 30,000 lines of verified code building all infrastructures from scratch through human-AI collaboration. The developed Lean 4 formulation framework includes the high-dimensional Gaussian analysis toolbox and Dudleys entropy integral toolbox, which deepens mathematical understanding and open the door to formulation of modern machine learning theory."
        },
        {
            "title": "Acknowledgment",
            "content": "Y. Z. was supported by Warwick Chancellors International Scholarship. JDL acknowledges support of Open Philanthropy, NSF IIS 2107304, NSF CCF 2212262, ONR Young Investigator Award, NSF CAREER Award 2144994, and NSF CCF 2019844. F. L. was supported by Warwick-SJTU fund. We thank Zulip2 for the project organization tool and Sulis3 for CPU computation resources. 2https://zulip.com/ 3https://warwick.ac.uk/research/rtp/sc/sulis/ 11 References Abbe, E., Adsera, E. B., and Misiakiewicz, T. The merged-staircase property: necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pp. 47824887. PMLR, 2022. Anthropic. Claude code, 2025a. URL https://github.com/anthropics/claude-code. Accessed: 2026-0127. Anthropic. System Card: Claude Opus 4.5, 2025b. URL http://www.anthropic.com/ claude-opus-4-5-system-card. Accessed: 2026-01-27. Bartlett, P. L., Long, P. M., Lugosi, G., and Tsigler, A. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):3006330070, 2020. Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling modern machine-learning practice and the classical biasvariance trade-off. Proceedings of the National Academy of Sciences, 116(32):1584915854, 2019. Boucheron, S., Lugosi, G., and Massart, P. Concentration Inequalities: Nonasymptotic Theory of Independence. Oxford University Press, 02 2013. URL https://doi.org/10.1093/acprof:oso/9780199535255. 001.0001. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, pp. 18771901, 2020. Bruna, J. and Hsu, D. Survey on algorithms for multi-index models. arXiv preprint arXiv:2504.05426, 2025. Daras, G., Dean, J., Jalal, A., and Dimakis, A. Intermediate layer optimization for inverse problems using deep generative models. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 24212432. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/daras21a.html. Hastie, T., Tibshirani, R., Friedman, J., et al. The elements of statistical learning, 2009. LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. nature, 521(7553):436444, 2015. Li, C., Wang, Z., He, W., Wu, Y., Xu, S., and Wen, Z. Formalization of complexity analysis of the first-order optimization algorithms. CoRR, 2024. Li, C., Wang, Z., Bai, Y., Duan, Y., Gao, Y., Hao, P., and Wen, Z. Formalization of algorithms for optimization with block structures. arXiv preprint arXiv:2503.18806, 2025a. Li, C., Xu, S., Sun, C., Zhou, L., and Wen, Z. Formalization of optimality conditions for smooth constrained optimization problems. arXiv preprint arXiv:2503.18821, 2025b. Mei, S. and Montanari, A. The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75(4):667766, 2022. Montanari, A. and Urbani, P. Dynamical decoupling of generalization and overfitting in large two-layer networks. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Moura, L. d. and Ullrich, S. The lean 4 theorem prover and programming language. In International Conference on Automated Deduction, pp. 625635. Springer, 2021. Pisier, G. Probabilistic methods in the geometry of banach spaces. In Probability and Analysis: Lectures given at the 1st 1985 Session of the Centro Internazionale Matematico Estivo (CIME) held at Varenna (Como), Italy May 31June 8, 1985, pp. 167241. Springer, 2006. Raskutti, G., Wainwright, M. J., and Yu, B. Minimax rates of estimation for high-dimensional linear regression over ℓq-balls. IEEE transactions on information theory, 57(10):69766994, 2011. Sonoda, S., Kasaura, K., Mizuno, Y., Tsukamoto, K., and Onda, N. Lean Formalization of Generalization Error Bound by Rademacher Complexity. arXiv preprint arXiv:2503.19605, 2025. 12 Tsigler, A. and Bartlett, P. L. Benign overfitting in ridge regression. Journal of Machine Learning Research, 24(123):176, 2023. van der vaart, A. and Wellner, J. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer Series in Statistics. Springer New York, 2013. URL https://books.google.co.uk/books?id= zdDkBwAAQBAJ. Van Der Vaart, A. W. and Wellner, J. A. Weak convergence. In Weak convergence and empirical processes: with applications to statistics, pp. 1628. Springer, 1996. Vershynin, R. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018. Wainwright, M. J. High-dimensional statistics: non-asymptotic viewpoint, volume 48. Cambridge university press, 2019. Zhang, S. Towards formalizing reinforcement learning theory. arXiv preprint arXiv:2511.03618, 2025."
        },
        {
            "title": "A List of Key Results",
            "content": "Our formalization library contains more than 1000 theorems/lemmas. In this section, we aim to present the major results of our formalizations and provide an exact reference to locate the statement. The list is shown in Table 1. Table 1: List of our key formalization results with exact reference from textbooks."
        },
        {
            "title": "Reference",
            "content": "efronStein gaussianPoincare han_inequality entropy_duality entropy_duality_T entropy_subadditive isENet_of_maximal coveringNumber_euclideanBall_le coveringNumber_l1Ball_le subGaussian_finite_max_bound dudley coveringNumber_lt_top_of_totallyBounded Vershynin (2018, Remark 4.2.3) Vershynin (2018, Lemma 4.2.6) Vershynin (2018, Corollary 4.2.13) Daras et al. (2021, Theorem 2) Wainwright (2019, Exercise 2.12) Boucheron et al. (2013, Corollary 13.2) Boucheron et al. (2013, Theorem 3.1) Boucheron et al. (2013, Theorem 3.20) Boucheron et al. (2013, Theorem 4.1) Boucheron et al. (2013, Theorem 4.13) Boucheron et al. (2013, Remark 4.4) Boucheron et al. (2013, Theorem 4.22) Boucheron et al. (2013, Theorem 5.1) Boucheron et al. (2013, Theorem 5.4) Boucheron et al. (2013, Theorem 5.5) Boucheron et al. (2013, Theorem 5.6) Wainwright (2019, Proposition 5.17) Wainwright (2019, (5.48) Gaussian Case) Wainwright (2019, Theorem 13.5) Wainwright (2019, Lemma 13.6) Wainwright (2019, Example 13.8) Wainwright (2019, Lemma 13.12) Raskutti et al. (2011, Lemma 4, = 1) bernoulli_logSobolev gaussian_logSobolev_W12_pi lipschitz_cgf_bound gaussian_lipschitz_concentration one_step_discretization local_gaussian_complexity_bound master_error_bound gaussian_complexity_monotone linear_minimax_rate_rank bad_event_probability_bound l1BallImage_coveringNumber_le Dudleys Formalization Proof Details The chaining argument requires constructing hierarchy of ε-nets at geometrically decreasing scales. We encapsulate this in the DyadicNets structure, which provides for each level finite set Tk that is an εk-net of s, where εk = 2k is the dyadic scale at level k. For the proof to succeed, we need good\" dyadic nets satisfying the cardinality bound Tk (εk+1, s, d). This relates the net size at level to the covering number at the finer scale εk+1 = εk/2, which is essential for bounding the expected maximum of sub-Gaussian increments at each level later. Another critical chaining is the dyadic approximation of the entropy integral. Define the dyadic sum RK(s, D) := K1 (cid:88) k=0 εk (cid:112)log (εk, s, d) . This sum approximates the entropy integral via Riemann-like discretization at geometrically spaced points. Furthermore, the chaining argument requires defining sequence of approximations π0(u), π1(u), . . . , πK(u) = through the net hierarchy for each point in the finest net TK. There are two natural approaches: Direct projection: Standard presentations (Boucheron et al., 2013; Vershynin, 2018) define πk(u) as the 14 nearest point in Tk to the original point u. The triangle inequality then gives d(πk(u), πk+1(u)) d(πk(u), u) + d(u, πk+1(u)) εk + εk+1 = 3 2 εk . Recursive projection: An alternative, used in van der vaart & Wellner (2013); Wainwright (2019), defines πk(u) as the nearest point in Tk to πk+1(u), i.e. the coarser approximation is chosen to approximate the finer one, not the original point. For TK: (cid:40) πk(u) = nearest point in Tk to πk+1(u) if = if < Our formalization uses the recursive projection, which yields tighter constant. Since πk+1(u) Tk+1 and Tk is an εk-net of s, we have d(πk(u), πk+1(u)) εk directly, without the factor of 3/2. This improvement from 3 2 εk to εk propagates through the proof. While modest at each level, it accumulates to noticeable reduction in the final constant. Then, the formal proof of dudley proceeds in three stages: B.1 Stage 1 The first stage establishes bound for the expected supremum over finite net. For the net TK at level K, our target is to prove For any TK, we write (cid:20) sup uTK (cid:21) (Xu Xt0) 2σ RK+1(s, D) . Xu Xt0 = (Xπ0(u) Xt0) + K1 (cid:88) k=0 (Xπk+1(u) Xπk(u)) , where πk denotes the recursive projection to level k. The first term is the base term from the coarsest net, and the sum captures the increments as we move through finer nets. For the base term, using the finite maximum bound for sub-Gaussian processes, we have (cid:20) sup uTK (Xπ0(u) Xt0) (cid:21) σϵ0 (cid:112)2 log T0 , since all points in the level-0 net are within distance of t0. The cardinality bound T0 (ε1, s) allows us to express this as the first term of RK+1(s, D) scaled by 2 2σ. For the increment terms, the key observations are that these increments have distance at most εk by the recursive projection bound, and that the number of distinct pairs (πk(u), πk+1(u)) is at most Tk+1 since πk+1(u) Tk+1 determines the pair. Applying the finite maximum bound produces bound of the form: (cid:20) sup uTK (Xπk+1(u) Xπk(u)) (cid:21) σεk (cid:112)2 log Tk+1 . By the cardinality boundness of good\" nets, we sum over and re-index to obtain (cid:34) sup uTK K1 (cid:88) (cid:35) (Xπk+1(u) Xπk(u)) k= K1 (cid:88) k=0 σεk (cid:112)2 log (εk+2, s, d) 4 2σ RK+1(s, D) . 15 B.2 Stage 2 This stage extends the bound from finite nets to the countable supremum over dense sequence (tn)nN in s, where Fatous lemma enters. For convenience, we work in normalized sub-Gaussian settings (Xt0 0) without loss of generality. The central challenge is that Fatous lemma requires nonnegative integrands, but the supremum YK := supuTK Xu may be negative. We resolve this by introducing shift function g(ω) := inf KN XπK (t0)(ω) where πK(t0) denotes the projection of t0 onto TK. With shift function in hand, we define ZK := YK 0. Fatous lemma gives (cid:104) lim inf (cid:105) ZK lim inf E[ZK]. By path continuity and sequence density, lim inf ZK = supnN Xtn g. Since E[ZK] = E[YK] E[g], the E[g] cancels: (cid:20) (cid:21)"
        },
        {
            "title": "Xtn",
            "content": "sup nN lim inf (cid:16) 6 2σ RK+1(s, D) (cid:17) . The final step shows lim inf RK+1(s, D) 2 (cid:90) (cid:112)log (ε, s, d) dε . The approximation RK(s, D) 2 (entropy integral) + 2 (tail) holds, where the tail is εK as . Taking lim inf eliminates the tail, yielding: (cid:112)log (εK, s) 0 (cid:20) Xtn sup nN (cid:21) 12 2 (cid:90) (cid:112)log (ε, s, d) . 0 B.3 Stage 3 This stage converts the supremum over the uncountable set to the countable supremum over the dense sequence. Since is totally bounded in (A , d), it is separable, so there exists countable dense sequence (tn)nN s. By the sample path continuity, for each ω the map (cid:55) Xt(ω) is continuous on s. Since the supremum of continuous function over set equals its supremum over any dense subset, we have sup ts Xt(ω) = sup nN Xtn(ω) . Combining this with the bound from the second stage gives (cid:20) Xt sup ts (cid:21) 12 2σ (cid:90) (cid:112)log (ε, s, d) dε 0 completing the proof. Lean 4 Formalization of Least Squares In this section, we present more implementation details of Theorem 4.1 and Theorem 4.2. We aim to show that the formalization exposes and resolves implicit assumptions missed in standard textbooks, enforcing granular understanding of the theory. 16 C.1 Master Error Bound First, the complete formalization of Theorem 4.1 is: theorem master_error_bound (hn : 0 < n) (M : RegressionModel X) (F : Set (X R)) (hF_star : M.f_star F) (δ_star : R) (hδ : 0 < δ_star) (hCI : satisfiesCriticalInequality M.σ δ_star (shiftedClass M.f_star) M.x) (hH_star : IsStarShaped (shiftedClass M.f_star)) (t : R) (ht : δ_star t) (f_hat : (Fin R) (X R)) (hf_hat : w, isLeastSquaresEstimator (M.response w) M.x (f_hat w)) (hne : (empiricalSphere (shiftedClass M.f_star) (Real.sqrt (t * δ_star)) M.x).Nonempty) (hint_u : Integrable (fun => localizedBall (shiftedClass M.f_star) (Real.sqrt (t * δ_star)) M.x, (n : R)1 * Σ i, * (M.x i)) (stdGaussianPi n)) (hint_δ : Integrable (fun => localizedBall (shiftedClass M.f_star) δ_star M.x, (n : R)1 * Σ i, * (M.x i)) (stdGaussianPi n)) (hbdd : : Fin R, BddAbove {y localizedBall (shiftedClass M.f_star) (Real.sqrt (t * δ _star)) M.x, = (n : R)1 * Σ i, * (M.x i)}) : (stdGaussianPi {w (empiricalNorm (fun => f_hat (M.x i) - M.f_star (M.x i)))^2 16 * * δ _star}).toReal 1 - exp (-n * * δ_star / (2 * M.σ^2)) := by Now we will introduce the technical hypotheses: hCI: The chosen radius δ > 0 should satisfy the critical inequality. This is used to control the probability of the bad event in bad_event_probability_bound, which is standard proof need as Wainwright (2019, Lemma 13.2). hH_star: The shifted class is star-shaped. The need follows similar reason to hCI. hne: The empirical sphere is non-empty. The bad event is defined in terms of supremum over the empirical sphere at certain radius. If this sphere is empty, the supremum would be vacuously or ill-defined. In practice, for most function classes (e.g., linear or constrained class), this holds automatically. But in formal proof, this must be stated explicitly. Integrability at scale u, i.e. the supremum of the empirical process over the localized ball at radius hint_u: = tδ is integrable with respect to the Gaussian measure. The bad_event_probability_bound uses this. The proof involves computing or bounding the expectation of supremum and then applying sub-Gaussian concentration. Both steps require that the supremum random variable is integrable. In the formal proof, Leans measure theory requires it as an explicit hypothesis. hint_δ: Integrability at scale δ, same as hint_u, but at the radius δ instead of u. The proof bounds the expectation of supremum by relating it to the local Gaussian complexity, which is itself an expectation at scale δ. hbdd: Boundedness above of the process, which is another technical measure-theoretic condition. The proof takes suprema () over localized balls. In Lean 4, over an unbounded set can give meaningless results (default to 0 or ). The BddAbove condition ensures the iSup is genuine supremum. C.2 Localized Gaussian Complexity via Dudley The complete formalization of Theorem 4.2 is: lemma local_gaussian_complexity_bound (n : N) (hn : 0 < n) (H : Set (X R)) (δ : R) (hδ : 0 < δ) (x : Fin X) (hH_star : IsStarShaped H) (hH_tb : TotallyBounded (empiricalMetricImage localizedBall δ x)) (hfinite : entropyIntegralENNReal (empiricalMetricImage localizedBall δ x) (2*δ) = ) 17 (hcont : w, Continuous (fun (v : (empiricalMetricImage localizedBall δ x)) => innerProductProcess v.1 w)) (hint_pos : Integrable (fun => localizedBall δ x, empiricalProcess w) (stdGaussianPi n)) (hint_neg : Integrable (fun => localizedBall δ x, -empiricalProcess w) (stdGaussianPi n)) : LocalGaussianComplexity δ (24 * Real.sqrt 2) / Real.sqrt * entropyIntegral (empiricalMetricImage localizedBall δ x) (2*δ) := by Now we will introduce the technical hypotheses: hfinite: The entropy integral is finite. If the entropy integral is infinite, the bound is vacuous. The finiteness in ENNReal ensures the real-valued entropyIntegral is well-defined and the bound is meaningful. This is formalization concern: the external dudley theorem needs to know the integral converges. hcont: Pathwise continuity of the process. This is also formalization need for the external dudley theorem. This should hold automatically in finite dimensions, but in formal proof in Lean, the continuity needs to be stated with respect to the subspace topology on the image set. Integrability of the positive supremum. The proof manipulates integrals (uses linearity of expechint_pos: tation, comparison of integrands), which requires the functions being integrated to be integrable. Lean 4s integral of non-integrable function is defined to be 0, which would make the bound vacuously useless. hint_neg: Same measure-theoretic regularity as above, but for the negative side of the process. The negative version is needed separately because the supremum of negative function is not simply negative infimum of function in terms of integrability. C.3 Observation The hypotheses hint_u, hint_δ, hint_pos, hint_neg, and hne are typically missed in standard textbook treatments but act as essential roles to make the statement hold rigorously. The formalization makes precise exactly what must be verified when extending the theory to new settings. Lean 4 Formalization of Covering of ℓ1-Convex Hull In this section, we present more implementation details of Lemma 4.5. Our formalized statement of Lemma 4.5 is: def l1BallImage (x : Fin EuclideanSpace (Fin d)) (R : R) : Set (EmpiricalSpace n) := {v θ : EuclideanSpace (Fin d), l1norm θ = fun => (1 / Real.sqrt n) * @inner _ _ θ (x i)} theorem l1BallImage_coveringNumber_le {R ε : R} (hR : 0 R) (hε : 0 < ε) (x : Fin EuclideanSpace (Fin d)) (hcol : columnNormBound x) (hn : 0 < n) : coveringNumber ε (l1BallImage R) (2 * + 1) ^ ^ 2 / ε ^ 2+ := by The major challenge is that the formalization strategy used in Theorem 4.4 can only obtain combinatorial complexity grows exponentially with d. So we need to formalize the probabilistic rather than combinatorial methods. In general, our formalization mirrors the Maureys Empirical Method (Pisier, 2006). Step 1: Probabilistic Representation Any point = Xθ/ an expected value: in absconv1(X/ n; R) can be written as where is random variable taking values in {0} {R X[:,j]/ n} with probabilities proportional to θj. Step 2: Variance Control The random variable has bounded second moment: = E[Z] E[Z2] θ1 18 This uses the column normalization X[:,j]2 Step 3: Averaging Reduces Variance For i.i.d. copies Z1, . . . , Zk, the average Zk = 1 satisfies: n. (cid:80)k ℓ=1 Zℓ E[ Zk v2] θ1 R2 Step 4: Existence via Pigeonhole Since the expected squared distance is at most R2/k, there exists specific sample achieving this bound. Setting = R2/ε2 ensures distance ε. Step 5: Finite Net Construction The set of all possible k-averages forms finite net Nk with: since each sample comes from space of size 2d + 1. Nk (2d + 1)k"
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "University of California, Berkeley",
        "University of Warwick"
    ]
}