{
    "paper_title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
    "authors": [
        "Alexander Zhang",
        "Marcus Dong",
        "Jiaheng Liu",
        "Wei Zhang",
        "Yejie Wang",
        "Jian Yang",
        "Ge Zhang",
        "Tianyu Liu",
        "Zhongyuan Peng",
        "Yingshui Tan",
        "Yuanxing Zhang",
        "Zhexu Wang",
        "Weixun Wang",
        "Yancheng He",
        "Ken Deng",
        "Wangchunshu Zhou",
        "Wenhao Huang",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench."
        },
        {
            "title": "Start",
            "content": "CodeCriticBench: Holistic Code Critique Benchmark for Large Language Models Alexander Zhang2,, Marcus Dong2,, Jiaheng Liu1,2,,, Wei Zhang4, Yejie Wang6, Jian Yang4, Ge Zhang2, Tianyu Liu2, Zhongyuan Peng5, Yingshui Tan3, Yuanxing Zhang7, Zhexu Wang6, Weixun Wang3, Yancheng He3, Ken Deng3, Wangchunshu Zhou2,8, Wenhao Huang2, Zhaoxiang Zhang5 1NJU, 2M-A-P, 3Alibaba, 4BUAA, 5CASIA, 6BUPT, 7Kuaishou, 8OPPO https://github.com/multimodal-art-projection/CodeCriticBench"
        },
        {
            "title": "Abstract",
            "content": "The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench. 5 2 0 2 3 2 ] . [ 1 4 1 6 6 1 . 2 0 5 2 : r Figure 1. Illustration of the Basic Critique Evaluation and Advanced Critique Evaluation. * Equal Contribution. Corresponding Author."
        },
        {
            "title": "Contents",
            "content": ""
        },
        {
            "title": "2.1 Code LLMs .",
            "content": ". . . . . . ."
        },
        {
            "title": "3.1 Overview .",
            "content": ". . . ."
        },
        {
            "title": "3.2 Data Collection .",
            "content": ". . . . . ."
        },
        {
            "title": "3.4 Evaluation Metrics .",
            "content": "."
        },
        {
            "title": "4.1 Baselines .",
            "content": ". ."
        },
        {
            "title": "4.2 Main Results .",
            "content": ". . . ."
        },
        {
            "title": "4.3 Further Analysis .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A More Experimental Results",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Anonymous Data Link . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Relations Between Basic Correctness and Advanced Evaluations Scores . . . . . . A.3 Case Visualization of CodeCriticBench . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Scaling Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Different Application Scenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Bug Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Critique Evaluation on Code Gen and Code QA Subset . . . . . . . . . . . . A.8 Critique Evaluation on Multiple Fine-Grained Dimensions . . . . . . . . . . . . ."
        },
        {
            "title": "C Model Lists",
            "content": "2 3 4 4 4 4 5 6 6 7 7 8 13 17 17 17 18 18 18 19 20 21 1. Introduction Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains [Team, 2024, Touvron et al., 2023, Rozi√®re et al., 2023], including natural language processing, code generation and complex reasoning tasks [Wang et al., 2025, Liu et al., 2024a,b, Bai et al., 2024]. As these models continue to evolve, their ability to critique and refine their outputs has emerged as crucial area of research. This critique capacity is fundamental to enhancing the reasoning abilities of LLMs, which enables them to provide detailed analysis and constructive feedbacks and improves the quality and reliability of their outputs [Madaan et al., 2023, Song et al., 2025b]. Recently, several critique benchmarks have been proposed [Lan et al., 2024b, Lin et al., 2024, Tan et al., 2024, Song et al., 2025a, Zheng et al., 2024a, Tang et al., 2025]. For example, CriticBench [Zheng et al., 2024b] is proposed to assess LLMs abilities to critique across variety of tasks including math, commonsense, symbolic, code and algorithmic, where the code subset is from the code generation datasets (i.e., Humaneval [Chen et al., 2021] and MBPP [Austin et al., 2021]). However, these benchmarks have predominantly focused on diverse reasoning tasks in general domains, leaving significant gap in the evaluation of code-related tasks. Moreover, these benchmarks often lack comprehensive evaluation across different dimensions of critique capacity. Besides, in software development, the ability of LLMs to generate, understand and critique code is important. As LLMs are increasingly employed in coding assistance tools and automated code review systems, there is pressing need for robust framework to evaluate their code critique capabilities. This evaluation should encompass not only the accuracy of code generation but also the models ability to provide insightful feedback, identify errors and suggest improvements in the code QA scenario. To address these limitations, as shown in Figure 1, we introduce CodeCriticBench, holistic code critique benchmark for LLMs. Specifically, first, our benchmark covers two mainstream code tasks: code generation and code QA. These tasks are presented with varying levels of difficulty, allowing for nuanced assessment across different coding challenges. Second, CodeCriticBench incorporates both basic and advanced critique evaluations for assessing different characteristics. For the basic setting, we prompt the judge model to provide the correct/error response and the corresponding reasoning process. For the advanced setting, we have developed fine-grained evaluation checklists for each problem, enabling more detailed and precise assessment of the models critique capabilities, which allows for more thorough evaluation of existing LLMs. In summary, our contributions are as follows: To evaluate the critique abilities of LLMs on the code domain, we introduce the first holistic code critique benchmark CodeCriticBench, which includes the critique on both code generation and code QA tasks. Based on our proposed CodeCriticBench, we design both the basic and advanced critique evaluations, which provide comprehensive analysis of the critique ability. We systematically evaluate the critique abilities of 38 LLMs (including general and codespecific models) on CodeCriticBench and provide detailed analysis regarding the critique capabilities of LLMs. 3 2. Related Works 2.1. Code LLMs Large Language Models (LLMs) have rapidly developed and are significantly impacting automated software development. These foundational models can produce fluent human language and understand semantics to perform complex tasks, bringing new possibilities to automated code generation, including Santacoder Allal et al. [2023], CodeGPT Lu et al. [2021], etc. More and more open-source and proprietary Code LLMs have emerged and demonstrated competitive performance, including Starcoder Li et al. [2023], CodeLlama Roziere et al. [2023], Wizardcoder Luo et al. [2023], DeepSeek-Coder Guo et al. [2024], Qwen-Coder Hui et al. [2024], Claude-3.5-Sonnet Anthropic [2024] and GPT-4o OpenAI [2024] etc. 2.2. Critic Models for LLMs Reinforcement learning from human feedback has proven effective Achiam et al. [2023], though it can be very labor-intensive. promising approach involves using LLMs to themselves to assist in the evaluation, which can further improve model outputs Saunders et al. [2022], McAleese et al. [2024]. Accurate and informative critique feedback allows LLMs to refine outputs, moving towards more advanced intelligence. However, despite their problem-solving strengths, LLMs currently demonstrate weak performance in critique tasks [Zheng et al., 2024b, Yang et al., 2024]. Improving LLM critique abilities relies on supervision from human annotations Saunders et al. [2022], McAleese et al. [2024] and stronger LLMs acting as human proxies Lan et al. [2024a], Zhang et al. [2024], Ke et al. [2024], Ankner et al. [2024], Zheng et al. [2024b], Yang et al. [2024], Sharma et al. [2024]. Recently, some critic benchmarks [Zheng et al., 2024b, Tang et al., 2025] have also been proposed. 3. CodeCriticBench 3.1. Overview 4,"
        },
        {
            "title": "Number",
            "content": "Table 1. Dataset statistics of CodeCriticBench."
        },
        {
            "title": "Statistics",
            "content": "#Problems Difficulty Level - Easy/Medium/Hard CodeCriticBench is holistic evaluation benchmark for Code Critic tasks, encompassing code evaluation in both Code Generation and Code QA tasks. Table 1 presents an overview of CodeCriticBench, which comprises 4,300 samples. Each sample includes question, an answer, set of fine-grained evaluation checklists across multiple dimensions and associated labelsnamely, correctness labels, perdimension evaluation scores, final score and difficulty level. Furthermore, we employ the LLaMA3 tokenizer Dubey et al. [2024] to determine the token counts for questions, answers and evaluation problems in Table 1. For finegrained evaluation checklists, token counts are summed across all evaluation problems. As reported in Table 1, the average token length is 451.06 for questions, 322.22 for answers and 287.49 for the aggregated evaluation checklists. - maximum length - minimum length - average length - maximum length - minimum length - average length - maximum length - minimum length - average length Fine-Grained Evaluation Checklists"
        },
        {
            "title": "Answer",
            "content": "32, 063 tokens 8 tokens 451.06 tokens 676 tokens 97 tokens 287.49 tokens 32175 tokens 9 tokens 322.22 tokens 1, 517/1, 084/1, 699 4 Table 2. Comparisons between CodeCriticBench and other benchmarks. Code Gen denotes Code Generation. - represents that the corresponding task is not included in the dataset."
        },
        {
            "title": "Benchmark",
            "content": "Data Size Code Gen. Code QA Basic Advanced"
        },
        {
            "title": "CodeCriticBench",
            "content": "3,825 3,608 350 2,093 4,300 464 1,340 42 - 3,200 - - - - 1, Most existing Code Critic Benchmarks primarily focus on evaluating basic properties, such as code correctness. In contrast, CodeCriticBench offers both fundamental assessment of code correctness and comprehensive evaluation of associated questionanswer pairs, alongside fine-grained scoring system that spans multiple dimensions. Notably, every data sample in CodeCriticBench is paired with uniquely tailored, fine-grained evaluation checklists, ensuring that each instance is assessed in context-specific and comprehensive manner. Table 2 presents comparison between CodeCriticBench and other Code Critic Benchmarks. 3.2. Data Collection Code Generation. In the code generation task, instructions are mainly algorithmic problems. We collect many such data (restricted from CodeForces, to test sets) MBPP Austin et al. [2021] and LiveCodeBench Jain et al. [2024]. Given the limited test sets in MBPP and LiveCodeBench, we employ DeepSeek-v3 to rewrite problem while preserving their semantics, thereby enhancing diversity and test case reusability. To assess the models ability to detect specific programming errors, we create Debug subset. We first compile list of bugs through iterative discussions with experts and LLMs, then prompt the LLM to insert specified error types into sandbox-verified correct code. The modified samples undergo two filtering rounds: sandbox execution to confirm errortriggering and manual review to ensure the errors matched their intended categories. Details on the error types and bug insertion prompts are provided in Appendix B. Figure 2. Illustration of data collection process. Code QA. Building on previous work Baars and Meester [2019], we first apply rule-based filtering method to clean the extracted content, removing site information, ads and HTML tags. To ensure our benchmark reflects real-world scenarios, we collect authentic code requirements and manually craft responses from StackOverflow. Unlike Yue et al. [2024], which directly extract questionanswer pairs, we use Qwen2.5-72B to generate new questions. As shown in Figure 2, Qwen2.5-Coder samples the same document multiple times to generate diverse responses, which are then filters through manual and LLM-assisted reviews, forming high-quality subset. 5 3.3. Dataset Construction Difficulty. To assess sample difficulty, we employ twelve state-of-the-art LLMs1 and classify each sample based on the proportion of models that produce correct predictions. Specifically, sample is labeled Easy if at least 80% of the models reason correctly, Medium if the success rate is between 60% and 80% and Hard if fewer than 60% succeed. Note that because random scoring yields an expected success rate of approximately 50%, success rate below 60% suggests that the models performance is close to random guessing. This process yields 1,517 Easy, 1,084 Medium and 1,699 Hard samples, ensuring balanced distribution of difficulty levels in CodeCriticBench. Category. To determine the specific application scenarios of the Code QA subset, we prompt DeepSeek-v3 to classify the data based on predefined category list adopted from Liu et al. [2024d]. Detailed prompts used for this classification are provided in Appendix B. Correctness. For the Code Gen subset, each sample is accompanied by test cases that facilitate automated correctness evaluation within sandbox. sample is marked as Correct only if all test cases pass; otherwise, it is labeled as Error. In contrast, for the Code QA subset, we engage 20 volunteers with coding experience to assess correctness. Each question is independently evaluated by three raters and the final correctness label is determined by majority vote. Fine-Grained Evaluation Checklists and Corresponding Scores. In contrast to previous Code Critic Benchmarks, we introduce more detailed and customized scoring framework. Initially, we define 10 evaluation dimensions for both code generation and code QA tasks through iterative discussions with human experts and LLMs. Next, we employ prompts for DeepSeekV3 to generate tailored evaluation questions for each data instance. To ensure the generated questions met the desired criteria, we perform random sampling and manual inspections, repeating the process until the pass rate exceeds 95%. Inspired by previous work Que et al. [2024], we manually annotate 20% of the dataset to establish baseline scores. Subsequently, three advanced models (Claude3.5-Sonnet, GPT-4o and Gemini2.0), evaluate the entire dataset, with each evaluation dimensions final score determined by majority vote. Finally, we calibrate the LLM-generated scores by applying linear regression, using them as independent variables (x) and the human ratings as the dependent variable (y), to produce the final scores for all samples. Detailed prompts and multiple evaluation names are provided in Appendix and A.8. 3.4. Evaluation Metrics We employ multiple evaluation metrics to rigorously assess our models performance. Accuracy (ACC). ACC measures binary classification performance by determining whether the models predictions match the ground truth labels for basic critique evaluation. Let ùëÅ denote the total number of instances, ÀÜùë¶ùëñ the predicted label for instance ùëñ and ùë¶ùëñ the true label. Accuracy is computed as follows: ACC = 1 ùëÅ ùëÅ ùëñ=1 ùêº( ÀÜùë¶ùëñ = ùë¶ùëñ) (1) where ùêº( ÀÜùë¶ùëñ = ùë¶ùëñ) is an indicator function that returns 1 if the prediction is correct and 0 otherwise. Mean Squared Error (MSE). MSE quantifies the discrepancy between the models predicted and 1Claude3.5-Sonnet, GPT 4o, DeepSeek-v2.5, DeepSeek-v3, Doubao-Coder-Preview, Llama3.3-70B-Instruct, Qwen2.5-72B-Instruct, DeepSeek-R1-Distill-Qwen-32B, DeepSeek-R1, GLM-4-Plus, Qwen2.5-Max and OpenAI o1-Preview. 6 true scores across multiple dimensions for advanced critique evaluation, which measures how closely the models predictions approximate the actual values. MSE is calculated as follows: MSE = 1 ùëÅ ùëÅ ùëñ=1 ( ÀÜùë¶ùëñ ùë¶ùëñ)2 (2) where ÀÜùë¶ùëñ represents the predicted score and ùë¶ùëñ the true score for instance ùëñ. Pass@1 Accuracy. In code error detection, each code snippet contains at least one error (ùëõùëñ 1). The models objective is to detect at least one error per snippet. Let ÀÜùê∏ùëñ denotes the set of errors predicted by the model for instance ùëñ and ùê∏ùëñ the set of actual errors. prediction is considered successful if the intersection between ÀÜùê∏ùëñ and ùê∏ùëñ is non-empty. Pass@1 accuracy is defined as: ACCPass@1 = 1 ùëÅ ùëÅ ùëñ=1 ùêº( ÀÜùê∏ùëñ ùê∏ùëñ ) (3) 4. Experiments 4.1. Baselines We base our model selection on benchmarks from the code domain, including those outlined in Liu et al. [2024c] and Liu et al. [2024d], ultimately selecting 38 models for evaluation. These models encompass both open-source and closed-source options, with sizes ranging from 0.5 billion to over 100 billion parameters. detailed list of the models tested, along with their respective links, is provided in Tables 7 and 8 in Appendix C. 4.2. Main Results"
        },
        {
            "title": "Model",
            "content": "Table 4. The accuracy of different models in identifying programming error types. Critique Evaluation. Table 3 shows the basic evaluation (ACC metric) and advanced evaluation (MSE metric) for all models in our study. The results are grouped by model size, with separated blocks for closed-source and o1-like models. Within each size block, models are sorted by ACC. As observed, performance generally improves with more parameters, with o1-like models achieving milestone results and all exceed 70%. In the advanced evaluation, DeepSeek-R1 performs best on the Code Gen subset with an MSE of 3.92, while Claude3.5-Sonnet leads in Code QA with an MSE of 1.02. This difference likely reflects the distinct nature of the two tasks: Code QA prioritizes concise, high-quality answers, while Code Gen emphasizes solving algorithmic problems that require stronger reasoning abilities, whereas Code QA may benefit more from dialogue optimization. Qwen2.5-Coder-0.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct Qwen2.5-72B-Instruct GLM-4-Plus Claude3.5-Sonnet Qwen2.5-Max GPT 4o Doubao-Coder-Preview 21.50 32.75 48.00 61.00 41.00 62.25 47.50 61.25 47.75 54.00 59.50 61.00 67. Close-Sourced Open-Sourced Bug Identification. We evaluate thirteen models to assess their accuracy in identifying code error types. As shown in Table 4, larger models generally perform better in distinguishing code errors, which aligns well with our expectations. 7 Table 3. Results of different models. gen and qa denote code generation and code qa tasks respectively. ACC and MSE metrics are used for basic and advanced critique evaluations."
        },
        {
            "title": "ACCAll ACCgen ACCqa MSEAll MSEgen MSEqa",
            "content": "0.5B+ Instruction Tuned Model Qwen2.5-Coder-0.5B-Instruct 51.79 51.28 53.27 24. 23.96 25.06 1B+ Instruction Tuned Model Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct OpenCoder-1.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct 50.98 51.44 53.16 54.47 55.21 51.56 52.31 53.06 53.87 55. 49.27 48.91 53.45 56.18 55.18 6B+ Instruction Tuned Model CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct CodeQwen1.5-7B-Chat OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Yi-Coder-9B-Chat 54.47 54.63 55.63 56.37 57.95 61.67 53.94 51.81 55.41 57.66 56.78 63.66 56.00 62.82 56.27 52.64 61.36 55. 13B+ Instruction Tuned Model CodeLlama-13B-Instruct StarCoder2-15B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeek-v2-Lite-Chat DeepSeekCoder-v2-Lite-Instruct Qwen2.5-Chat-14B-Instruct 52.17 53.43 59.00 59.20 59.81 59.98 52.16 53.59 56.38 58.78 59.34 58.59 52.18 52.97 66.64 60.42 61.18 64.00 20B+ Instruction Tuned Model CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct 54.79 61.67 63.98 54.06 59.00 62.38 56.91 69.45 68.64 70B+ Instruction Tuned Model DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Qwen2.5-72B-Instruct 60.35 64.42 65.91 68.35 58.46 62.42 65.16 68.44 65.85 70.23 68.09 68.09 Close-Sourced API Model GPT 4o-mini Doubao-Coder-Preview GLM-4-Plus DeepSeek-v3 Qwen2.5-Max GPT 4o Claude3.5-Sonnet 60.56 61.42 61.55 62.00 63.36 68.06 68. 58.31 60.06 60.94 61.44 62.74 67.56 66.06 o1-like Models QwQ-32B-Preview Gemini2.0-Flash-Thinking OpenAI o1-mini DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1 OpenAI o1-Preview 57.35 64.53 71.77 72.49 72.76 75.30 56.59 64.88 76.06 75.38 79.09 80.53 67.09 65.36 63.35 63.64 65.17 69.53 76. 58.82 63.55 59.27 64.09 54.36 59.89 29.26 22.67 27.60 16.56 12.43 19.12 8.07 18.22 19.33 5.64 14.21 13.94 21.42 5.08 5.57 5.67 4.38 13.45 4.60 4.32 3.97 4.19 4.78 3. 3.92 6.51 3.60 3.64 4.09 4.15 3.78 7.20 3.88 4.92 4.34 4.20 4.81 28.27 24.48 28.07 17.13 13.01 20.89 8.73 19.06 19.45 6.17 14.28 15.55 20.92 5.57 6.35 6.46 5.02 15.06 5.19 5. 4.78 5.14 5.65 4.61 4.82 7.07 4.25 4.49 5.04 5.04 4.73 8.07 4.80 6.08 5.25 3.92 5.68 32.13 17.39 26.23 14.93 10.76 13.96 6.15 15.78 18.99 4.10 14.02 9.26 22.88 3.65 3.29 3.35 2. 8.76 2.89 2.09 1.63 1.46 2.24 2.20 1.30 4.90 1.69 1.18 1.33 1.55 1.02 4.67 1.19 1.54 1.71 5.02 2.26 4.3. Further Analysis Scaling Law. To assess the effectiveness of the scaling law, we visualize the performance across nearly all models in our study. Figure 3 presents the results of the basic evaluation ACC, with 8 corresponding MSE results provided in Appendix A.4. The data clearly demonstrate that as the number of parameters increases, the ACC increases, which further validate the robustness of our CodeCriticBench dataset. Figure 3. Scaling law on basic critique evaluation (ACC) across models. * indicates an estimated parameter size. Different Application Scenes. Our Code QA data subset includes 11 scenarios, such as Fundamental Programming, Software Engineering and Mathematics. To evaluate model performance across these domains, we test five models of varying sizes. We calculate their ACC on basic evaluation and MSE on advanced evaluation. As shown in Figures 4 and 17, larger models generally show improved ACC and reduced MSE, supporting our expectations and reinforcing their validity. Figure 4. Comparison across different models on Code QA (Basic Critique Evaluation). Different Difficultys Performance. As previously discussed, the difficulty of data is quantified by the proportion of models that accurately predict its correctness. To further analyze performance across different difficulty levels, we evaluate twelve advanced LLMs. Figure 5 shows that all models perform well on Easy data, with accuracies above 90%. For Medium data, the top performers are Qwen2.5-72B-Instruct, GPT 4o, Claude3.5-Sonnet and DeepSeekR1-Distill-Qwen-32B, each achieving around 82%. This performance is consistent with their rankings on other established leaderboards. In contrast, DeepSeek-R1 and OpenAI o1-Preview underperform on Easy to Medium data, likely due to overthinking. On more challenging data, most models show significant drop in performance, with accuracies around 30%. However, DeepSeek-R1 and OpenAI o1-Preview maintain relatively strong performance, with accuracies 9 of 51.97% and 55.75%, respectively. We hypothesize their strong performance is partly due to o1-like architecture, which help them handle complex data. Figure 5. Model performance (ACC) on different difficulty levels (Basic Critique Evaluation) . Different Bug Types. To gain more intuitive understanding of how models with different parameter sizes detect code errors, we select five common error types and evaluate six models of varying sizes to assess their ability to correctly identify the corresponding error types. As shown in Figure 6, the accuracy of error type identification generally improves with the increase in model size. Among the five error types, Qwen2.5-72B, Claude3.5-Sonnet and GPT-4o emerge as the top-performing models, each securing first place in the identification of 2, 2 and 1 error categories, respectively. Figure 6. Comparison of the accuracy of different models in identifying five common programming error types. Relations Between Basic Correctness and Advanced Evaluations Scores. To explore the relationship between data correctness and the final multidimensional score, we examine the distribution of correct and error data across different scores. As shown in Figure 7, almost all Error instances have scores below 5, while Correct instances consistently score 5 or higher. Correct scores are mainly around 7, 8 and 9, while error scores are typically between 2 and 3. Additional statistical details for the Code Gen and Code QA subsets can be found in Appendix A.2. Overall, the consistency between data correctness and final scores supports the robustness of our dataset. Figure 7. Rating distribution of the CodeCriticBench. Error Studies. We evaluate seven distinct LLMs, both open-source and closed-source, of varying sizes, to assess their ability to identify three common error types: Reference Error, 10 Performance Issue and Security Vulnerability, out of total of twenty-three categories. The results for these error types are shown in Figure 8, with additional details in Appendix A.6. As shown in Figure 8, models with higher capabilities perform better at distinguishing errors. However, most models, except Doubao-Coder-Preview and Claude3.5-Sonnet, struggle with identifying Performance Issue. We suggest this is due to insufficient code-related optimization and models smaller sizes. Figure 8. Experimental accuracy results of different models across various error types. Why Advanced Critique Evaluation Is Important. To further validate the consistency between basic critique, advanced critique and human evaluations, we randomly sample 400 instances (named CodeCritic_400) and test 8 models of different sizes, including both open-source and closed-source models. We then rank the models based on human, basic critique and advanced critique evaluations. As shown in Figure 9, rankings of the advanced setting closely match human evaluation, while results of the basic setting differ significantly. This highlights the effectiveness of our advanced evaluation approach, which provides more accurate and consistent results with human evaluations. Figure 9. Comparison of ranking of model responses by three methods: basic critique, advanced critique and human evaluations. Case Visualization of CodeCriticBench. To get an intuitive understanding of data in CodeCriticBench, we present visualization for the Code QA correct example in Figure 10, with additional visualization results provided in Appendix A.3. As observed, each sample consists of 11 question, an answer, multi-dimensional evaluation checklists and associated labels, which include correctness, multi-dimensional scores and final scores. Furthermore, each sample includes supplementary attributes such as partition identifiers, subset names and difficulty levels. Figure 10. Example of correct case of code qa."
        },
        {
            "title": "Model",
            "content": "55.00 47."
        },
        {
            "title": "CoT ACC MSE",
            "content": "Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-1.5B-Instruct Table 5. The effect of CoT on model evaluation. Effect of CoT Evaluation. During the basic evaluation, we observe that using single prompt to assess answers correctness often led to incorrect prediction. However, using fine-grained evaluation checklists, where the model scores each question individually before aggregating the results, produced more accurate outcomes. This improvement is due to the model having access to more detailed context. Based on this, we randomly select 400 instances (CodeCritic_400) and test four models of varying sizes. The CoT prompt used in this experiment is in Appendix B. As shown in Table 5, the use of the CoT leads to improvements in both ACC and MSE. Compared to the evaluation without CoT, Qwen2.5-Coder3B-Instruct shows gains of 7.5% in ACC and 1.31 in MSE, while Qwen2.5-Coder-14B-Instruct demonstrates improves of 15.5% in ACC and 3.13 in MSE. These results clearly indicate that providing more useful information enhances model evaluation accuracy, further validating the necessity of incorporating fine-grained evaluation metrics. Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-7B-Instruct 66.00 50.50 54.75 47.25 56.50 49.75 14.57 11.89 1.99 5.12 8.33 9. 4.06 4.31 Effect of Critique. To evaluate the effect of model-generated critiques in improving answers, we use four Qwen2.5-Coder models (1.5B, 3B, 7B, 14B) as critics, with GPT-4o as the evaluator and CodeCritic_400 as data. First, the critic model generates critiques for (ùëÑ, ùê¥) pairs based on fine-grained evaluation checklists. The evaluator then scores original pairs. Next, the polishing model refines answers using critiques and the evaluator scores the updated answers. The critic model varies, while the evaluator and the polishing model remain the same. As shown in Figure 11, applying critiques improves scores and critique quality increases with model size, aligning with the models capabilities. 12 Figure 11. Scoring results of QA pairs before and after applying critiques to refine the answers. 5. Conclusion In this work, we introduce CodeCriticBench, comprehensive benchmark designed to evaluate the code critique capabilities of large language models (LLMs) through two core tasks: code generation and code-based question answering (QA), each incorporating multiple levels of difficulty. Our benchmark comprises both basic and advanced evaluation metrics that target distinct aspects of LLM performance. In the basic evaluations, we assess whether the model can accurately judge the correctness of given question-answer pair. For the advanced evaluations, we employ detailed, fine-grained checklists to facilitate thorough assessment of the models critique abilities. Extensive testing of current LLMs with CodeCriticBench demonstrates its efficacy in measuring and comparing code critique performance across different models, thereby providing valuable insights for the continued refinement of foundational models."
        },
        {
            "title": "Limitations",
            "content": "While CodeCriticBench comprehensively incorporates both basic and advanced evaluations for code-related tasksincluding code generation and code question answeringit is not without limitations. For example, our evaluation is confined to single-file scenarios. In future work, we plan to extend it to encompass repository-level critiques. Furthermore, as CodeCriticBench is presently focused solely on code, we intend to broaden its scope to include additional domains, thereby enabling the assessment of critique capabilities across wider range of tasks and application scenarios."
        },
        {
            "title": "References",
            "content": "J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, et al. Santacoder: dont reach for the stars! arXiv preprint arXiv:2301.03988, 2023. Z. Ankner, M. Paul, B. Cui, J. D. Chang, and P. Ammanabrolu. Critique-out-loud reward models. arXiv preprint arXiv:2408.11791, 2024. Anthropic. Introducing claude. https://www.anthropic.com/claude, 2024. J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/abs/2108.07732. S. Baars and S. Meester. Codearena: Inspecting and improving code quality metrics using minecraft. In 2019 IEEE/ACM International Conference on Technical Debt (TechDebt), pages 6870. IEEE, 2019. G. Bai, J. Liu, X. Bu, Y. He, J. Liu, Z. Zhou, Z. Lin, W. Su, T. Ge, B. Zheng, et al. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762, 2024. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code, 2021. URL https: //arxiv.org/abs/2107.03374. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li, et al. Deepseekcoder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. I. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. ArXiv, abs/2403.07974, 2024. URL https://api.semanticscholar.org/Corp usID:268379413. P. Ke, B. Wen, A. Feng, X. Liu, X. Lei, J. Cheng, S. Wang, A. Zeng, Y. Dong, H. Wang, et al. Critiquellm: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1303413054, 2024. 14 T. Lan, W. Zhang, C. Lyu, S. Li, C. Xu, H. Huang, D. Lin, X.-L. Mao, and K. Chen. Training language models to critique with multi-agent feedback. arXiv preprint arXiv:2410.15287, 2024a. T. Lan, W. Zhang, C. Xu, H. Huang, D. Lin, K. Chen, and X.-L. Mao. Criticeval: Evaluating largescale language model as critic. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. Z. Lin, Z. Gou, T. Liang, R. Luo, H. Liu, and Y. Yang. Criticbench: Benchmarking llms for critique-correct reasoning, 2024. J. Liu, Z. Ni, H. Que, T. Sun, N. Wang, J. Yang, JiakaiWang, H. Guo, Z. Peng, G. Zhang, J. Tian, X. Bu, K. Xu, W. Rong, J. Peng, and Z. Zhang. Roleagent: Building, interacting, and benchmarking high-quality role-playing agents from scripts. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. URL https: //openreview.net/forum?id=hORTHzt2cE. J. Liu, Z. ZhiqiBai, Y. Zhang, C. Zhang, Y. YuangZh, G. Zhang, J. JiakaiWang, H. Que, Y. Chen, W. Su, T. Ge, J. Fu, W. Chen, and B. Zheng. E2-LLM: Efficient and extreme length extension of large language models. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 42434253, Bangkok, Thailand, Aug. 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.252. URL https://aclanthology.org/2024.findings-acl.252/. S. Liu, L. Chai, J. Yang, J. Shi, H. Zhu, L. Wang, K. Jin, W. Zhang, H. Zhu, S. Guo, et al. Mdeval: Massively multilingual code debugging. arXiv preprint arXiv:2411.02310, 2024c. S. Liu, H. Zhu, J. Liu, S. Xin, A. Li, R. Long, L. Chen, J. Yang, J. Xia, Z. Peng, et al. Fullstack bench: Evaluating llms as full stack coder. arXiv preprint arXiv:2412.00535, 2024d. S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, et al. Codexglue: machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021. Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= S37hOerQLB. N. McAleese, R. M. Pokorny, J. F. C. Uribe, E. Nitishinskaya, M. Trebacz, and J. Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o, 2024. H. Que, F. Duan, L. He, Y. Mou, W. Zhou, J. Liu, W. Rong, Z. M. Wang, J. Yang, G. Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. 15 B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. B. Rozi√®re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. D√©fossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve. Code llama: Open foundation models for code. arXiv preprint arXiv: 2308.12950, 2023. W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. A. Sharma, S. Keh, E. Mitchell, C. Finn, K. Arora, and T. Kollar. critical evaluation of ai feedback for aligning large language models. arXiv preprint arXiv:2402.12366, 2024. M. Song, Z. yu Su, X. Qu, J. Zhou, and Y. Cheng. Prmbench: fine-grained and challenging benchmark for process-level reward models. 2025a. X. Song, Y. Wu, W. Wang, J. Liu, W. Su, and B. Zheng. Progco: Program helps self-correction of large language models. ArXiv, abs/2501.01264, 2025b. URL https://api.semanticscho lar.org/CorpusID:275212297. S. Tan, S. Zhuang, K. Montgomery, W. Y. Tang, A. Cuadron, C. Wang, R. A. Popa, and I. Stoica. Judgebench: benchmark for evaluating llm-based judges. ArXiv, abs/2410.12784, 2024. Z. Tang, Z. Li, Z. Xiao, T. Ding, R. Sun, B. Wang, D. Liu, F. Huang, T. Liu, B. Yu, et al. Realcritic: Towards effectiveness-driven evaluation of language model critiques. arXiv preprint arXiv:2501.14492, 2025. L. Team. The llama 3 herd of models. arXiv preprint arXiv: 2407.21783, 2024. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. P. Wang, Y. Wu, N. Wang, J. Liu, X. Song, Z. Peng, K. Deng, C. Zhang, JiakaiWang, J. Peng, G. Zhang, H. Guo, Z. Zhang, W. Su, and B. Zheng. MTU-bench: multi-granularity tool-use benchmark for large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=6guG2OlXsr. L. Yang, Z. Yu, T. Zhang, M. Xu, J. E. Gonzalez, B. Cui, and S. Yan. Supercorrect: Supervising and correcting language models with error-driven insights. arXiv preprint arXiv:2410.09008, 2024. X. Yue, T. Zheng, G. Zhang, and W. Chen. Mammoth2: Scaling instructions from the web. arXiv preprint arXiv:2405.03548, 2024. L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. C. Zheng, Z. Zhang, B. Zhang, R. Lin, K. Lu, B. Yu, D. Liu, J. Zhou, and J. Lin. Processbench: Identifying process errors in mathematical reasoning. ArXiv, abs/2412.06559, 2024a. URL https://api.semanticscholar.org/CorpusID:274598010. X. Zheng, J. Lou, B. Cao, X. Wen, Y. Ji, H. Lin, Y. Lu, X. Han, D. Zhang, and L. Sun. Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic. arXiv preprint arXiv:2408.16326, 2024b. 16 A. More Experimental Results A.1. Anonymous Data Link We have released our data on an anonymous website (https://anonymous.4open.scienc e/r/CodeCriticBench-D657/). A.2. Relations Between Basic Correctness and Advanced Evaluations Scores We present the fine-grained score distributions for both the Code Gen and Code QA subsets. As illustrated in Figures 12a and 12b, these distributions exhibit favorable characteristics, thereby affirming the validity of our dataset. (a) The Code Gen subset. (b) The Code QA subset. Figure 12. Rating distribution. A.3. Case Visualization of CodeCriticBench We present remaining case visualization. Specifically, Figures 1015 illustrate correct example from the Code Gen subset, an error example from the Code Gen subset and an error example from the Code QA subset, respectively. Figure 13. Example of correct case of code generation. 17 Figure 14. Example of error case of code generation. Figure 15. Example of error case of code qa. A.4. Scaling Law We present the scaling law results for the MSE in advanced critique evaluation. As depicted in Figure 16, an increase in model parameters is accompanied by steady decrease in MSE, which aligns well with our expectations and further validates the rationality of our dataset. A.5. Different Application Scenes We present comparative analysis of the advanced evaluation MSE across various models and application scenarios within the Code QA subset. As depicted in Figure 17, an increase in model parameters is generally accompanied by steady decrease in MSE. Notably, Claude3.5Sonnet consistently achieves lower MSE across nearly all scenarios, which may be attributed to its specialized optimizations for code and dialogue contexts. A.6. Bug Identification Table 6 presents the performance metrics of evaluated models across the 23 distinct error categories comprising our Debug evaluation subset. These categories span critical software integrity domains, including but not limited to Input Validation and Data Processing Error, 18 Figure 16. Scaling law on advanced critique evaluation (MSE) across models. * indicates an estimated parameter size. Figure 17. Comparison across different models on Code QA (Advanced Critique Evaluation). Security Vulnerabilities and Reference Error, providing comprehensive assessment of model predictions robustness in practical debugging scenarios. A.7. Critique Evaluation on Code Gen and Code QA Subset We present more fine-grained scores for all the models used in our experiments across different scenarios. Tables 9 and 10 display the ACC and MSE of basic evaluation for each subset under the Code Gen subset. Tables 11 and 12 show the ACC and MSE of basic evaluation for each application scenario within the Code QA subset. Note that we have adopted the following abbreviations in our tables to denote various application scenarios: Fundamental Programming (FP), Advanced Programming (AP), Software Engineering (SE), Data Analysis (DA), Mathematics (MA), Desktop and Web Development (DW), Machine Learning (ML), Scientific Computing (SC), Databases (DS), Multimedia (MM) and Operating Systems (OS). A.8. Critique Evaluation on Multiple Fine-Grained Dimensions Our Code Gen subset incorporates 10 fine-grained evaluation dimensions, including [Correctness Verification, Time Complexity Optimization, Space Complexity, Code Readability, 19 Table 6. The accuracy of different models in identifying programming error types. Error Category All Configuration Management Error Data Management Error Input Validation and Data Processing Error Monitoring and Logging Management Error Environment Variable Error Dependency Management Error Syntax Error Design Flaw Security Vulnerability Log Security Issue Reference Error Session Management Error Code Quality and Maintenance Error Logic Error Testing and Verification Error Network and Communication Error Exception Handling Error User Permission and Authentication Error File and I/O Error Type Error Internationalization and Localization Error Performance Issue Concurrency and Multithreading Error Qwen2.5-Coder 0.5B-Instruct Qwen2.5-Coder 1.5B-Instruct Qwen2.5-Coder 3B-Instruct Qwen2.5-Coder 7B-Instruct Qwen2.5-Chat 7B-Instruct Qwen2.5-Coder 14B-Instruct Qwen2.5-Chat 14B-Instruct Qwen2.5 72B-Instruct GPT 4o Claude3.5 Sonnet Doubao-Coder Preview GLM-4 Plus Qwen2.5 Max Open-Sourced Close-Sourced 21.50 50.57 40.00 7.41 1.85 0.00 0.00 19.64 2.63 12.73 0.00 3.75 0.00 2.04 1.35 3.51 0.00 2.38 0.00 0.00 0.00 1.47 0.00 0. 32.75 3.45 5.45 31.48 0.00 0.00 0.00 67.86 0.00 25.45 0.00 2.50 14.29 0.00 24.32 0.00 14.75 4.76 6.25 10.96 21.43 1.47 2.30 23.21 48.00 1.15 10.91 42.59 0.00 20.00 25.00 37.50 13.16 23.64 0.00 8.75 14.29 0.00 29.73 0.00 9.84 4.76 4.69 24.66 33.33 5.88 14.94 57.14 61.00 2.30 7.27 27.78 1.85 20.00 29.17 25.00 10.53 27.27 0.00 41.25 33.33 4.08 31.08 5.26 14.75 4.76 15.62 31.51 38.10 14.71 12.64 85.71 41.00 3.45 3.64 37.04 0.00 40.00 4.17 28.57 2.63 21.82 0.00 20.00 14.29 0.00 40.54 0.00 4.92 0.00 7.81 17.81 7.14 2.94 1.15 60.71 62.25 3.45 3.64 46.30 0.00 30.00 33.33 51.79 2.63 41.82 0.00 40.00 23.81 0.00 27.03 3.51 9.84 0.00 14.06 31.51 23.81 8.82 16.09 76.79 47.50 1.15 7.27 20.37 0.00 30.00 33.33 30.36 18.42 27.27 11.11 38.75 14.29 4.08 25.68 0.00 0.00 7.14 12.50 20.55 33.33 7.35 11.49 41. 61.25 0.00 10.91 50.00 0.00 0.00 16.67 39.29 7.89 38.18 0.00 40.00 14.29 8.16 31.08 3.51 3.28 9.52 15.62 20.55 45.24 4.41 19.54 80.36 61.00 1.15 0.00 37.04 0.00 40.00 25.00 21.43 0.00 45.45 0.00 52.50 4.76 8.16 36.49 5.26 6.56 2.38 17.19 32.88 38.10 7.35 13.79 69.64 54.00 0.00 3.64 33.33 5.56 10.00 16.67 14.29 18.42 47.27 0.00 41.25 9.52 12.24 22.97 5.26 3.28 4.76 15.62 1.37 26.19 7.35 29.89 75.00 67.00 3.45 1.82 22.22 5.56 10.00 25.00 41.07 7.89 47.27 11.11 48.75 14.29 14.29 36.49 15.79 8.20 4.76 21.88 26.03 26.19 10.29 27.59 66.07 47.75 0.00 12.73 9.26 1.85 10.00 20.83 23.21 10.53 52.73 11.11 38.75 9.52 2.04 18.92 1.75 4.92 2.38 14.06 24.66 30.95 7.35 12.64 55.36 59.50 3.45 7.27 37.04 1.85 20.00 25.00 26.79 5.26 40.00 0.00 45.00 19.05 6.12 31.08 3.51 6.56 4.76 15.62 32.88 30.95 7.35 16.09 64. Robustness Validation, Algorithm Optimization, Comprehensive Testing, Output Format, Code Style Consistency, Maintainability], while the Code QA subset is assessed across 10 distinct dimensions, including [Correctness, Completeness, Performance, Maintainability, Clarity, Depth, Practicality, Logic Coherence, Innovation, Reliability]. Given that our advanced evaluation includes multiple evaluation dimensions, we further measure the MSE results across these dimensions for both the Code Gen subset and Code QA subset of the dataset. Table 13 presents the results for all evaluation dimensions within the Code Gen subset. Tables 14-17 show the results for each specific evaluation dimension across different subsets. Table 18 displays the results for all evaluation dimensions within the Code QA subset, while Tables 19-29 provide the results for each evaluation dimension within various specific application scenarios. B. Prompt We present all the evaluation prompts utilized in our experiment. Specifically, Figure 18 displays the prompt for basic evaluation ACC, Figure 19 presents the prompt for advanced evaluation MSE and Figure 22 shows the prompt for assessing the models ability to identify the corresponding code error type. Figures 26 and 27 illustrate the prompts for basic and advanced evaluations using the CoT method, respectively. Additionally, Figure 28 provides the prompt used for polishing and modifying the answer based on feedback, which is employed to verify the effectiveness of the critique. To enhance the transparency and credibility of our data construction process, we have made public the specific prompts used to create the Debug dataset subset, as well as the prompts employed for generating customized, fine-grained evaluation questions for each questionanswer pair (ùëÑ, ùê¥) in the advanced evaluation phase. In particular, Figure 20 illustrates the types of programming errors we identified and categorized, encompassing 23 major categories and over 110 subcategories. Figure 21 presents the corresponding prompts used to insert these programming errors. The detailed classification within our Code QA scenario was also generated through prompts utilizing large language models (LLMs), as shown in Figure 23. Lastly, Figures 24 and 25 display the prompts for generating customized, fine-grained evaluation questions for each question-answer pair (ùëÑ, ùê¥) in the Code Generation and Code QA scenarios. C. Model Lists Our experiments utilizes diverse set of inference models, spanning both closed-source and open-source architectures and covering broad spectrum of sizes. Specifically, Table 7 details the open-source models employed in our study, including those from the Qwen series (Qwen2.5Coder/Chat, QwQ, CodeQwen), the LLaMA series (CodeLlama, Llama), the DeepSeek series (DeepSeek-Coder/Chat), as well as the OpenCoder, Yi-Coder series and StarCoder2. These models range in size from 0.5B to over 200B, ensuring extensive coverage across different scales. Table 8, on the other hand, presents the closed-source models used, which encompass some of the most powerful models available to date, such as Claude3.5-Sonnet, GPT 4o-mini, GPT 4o, Gemini2.0-Flash-Thinking, DeepSeek-v3, GLM-4-Plus, Qwen-Max and the current popular o1-like models, including OpenAI o1-Preview, OpenAI o1-mini and DeepSeek-R1. Table 7. Open-sourced models adopted in our experiments. Open-Sourced Model"
        },
        {
            "title": "Model Link",
            "content": "CodeQwen1.5-7B-Instruct Qwen2.5-Coder-0.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Chat-14B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview CodeLlama-7B-Instruct CodeLlama-13B-Instruct CodeLlama-34B-Instruct Llama3.3-70B-Instruct https://hf.co/Qwen/CodeQwen1.5-7B-Instruct https://hf.co/Qwen/Qwen2.5-Coder-0.5B-Instruct https://hf.co/Qwen/Qwen2.5-Coder-1.5B-Instruct https://hf.co/Qwen/Qwen2.5-Coder-3B-Instruct https://hf.co/Qwen/Qwen2.5-Coder-7B-Instruct https://hf.co/Qwen/Qwen2.5-Coder-14B-Instruct https://hf.co/Qwen/Qwen2.5-Coder-32B-Instruct https://hf.co/Qwen/Qwen2.5-7B-Instruct https://hf.co/Qwen/Qwen2.5-14B-Instruct https://hf.co/Qwen/Qwen2.5-32B-Instruct https://hf.co/Qwen/Qwen2.5-72B-Instruct https://hf.co/Qwen/QwQ-32B-Preview https://hf.co/meta-llama/CodeLlama-7b-Instruct-hf https://hf.co/meta-llama/CodeLlama-13b-Instruct-hf https://hf.co/meta-llama/CodeLlama-34b-Instruct-hf https://hf.co/meta-llama/Llama-3.3-70B-Instruct DeepSeek-Coder-1.3B-Instruct DeepSeek-Coder-V2-Lite-Instruct DeepSeek-Coder-V2-Instruct DeepSeek-v2-Lite-Chat DeepSeek-v2-Chat DeepSeek-v2.5 DeepSeek-R1-Distill-Qwen-32B https://hf.co/deepseek-ai/deepseek-coder-1.3b-instruct https://hf.co/deepseek-ai/DeepSeek-Coder-V2-Lite-instruct https://hf.co/deepseek-ai/DeepSeek-Coder-V2-Instruct https://hf.co/deepseek-ai/DeepSeek-V2-Lite-Chat https://hf.co/deepseek-ai/DeepSeek-V2-Chat https://hf.co/deepseek-ai/DeepSeek-V2.5 https://hf.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B OpenCoder-1.5B-Instruct OpenCoder-8B-Instruct Yi-Coder-1.5B-Chat Yi-Coder-9B-Chat StarCoder2-15B-Instruct-v0.1 https://hf.co/infly/OpenCoder-1.5B-Instruct https://hf.co/infly/OpenCoder-8B-Instruct https://hf.co/01-ai/Yi-Coder-1.5B-Chat https://hf.co/01-ai/Yi-Coder-9B-Chat https://hf.co/bigcode/starcoder2-15b-instruct-v0.1 Table 8. Close-sourced models (APIs) adopted in our experiments. Close-Sourced Model"
        },
        {
            "title": "API Entry",
            "content": "Claude-3.5-Sonnet OpenAI o1-Preview OpenAI o1-mini GPT 4o-mini GPT 4o Gemini2.0-Flash-Thinking Doubao-Coder-Preview DeepSeek-v3 DeepSeek-R1 GLM-4-Plus Qwen-Max https://www.anthropic.com/news/claude-3-5-sonnet https://platform.openai.com/docs/models#o1 https://platform.openai.com/docs/models#o1 https://platform.openai.com/docs/models#gpt-4o-mini https://platform.openai.com/docs/models#gpt-4o https://ai.google.dev/gemini-api/docs/thinking https://www.volcengine.com/product/doubao https://www.deepseek.com https://www.deepseek.com https://open.bigmodel.cn/dev/api/normal-model/glm-4 https://www.aliyun.com/product/bailian 21 Table 9. Results of different models on basic critique evaluations ACC in the Code Gen Subset Dataset."
        },
        {
            "title": "All MBPP CodeForce LiveCodeBench Debug",
            "content": "0.5B+ Instruction Tuned Model Qwen2.5-Coder-0.5B-Instruct 51.28 50.88 50.12 52. 51.25 1B+ Instruction Tuned Model Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct OpenCoder-1.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct 51.56 52.31 53.06 53.87 55.22 61.75 54.00 57.38 56.25 56.25 51.50 52.75 51.62 50.62 56. 6B+ Instruction Tuned Model Qwen2.5-Chat-7B-Instruct CodeLlama-7B-Instruct CodeQwen1.5-7B-Chat Qwen2.5-Coder-7B-Instruct OpenCoder-8B-Instruct Yi-Coder-9B-Chat 51.81 52.16 55.41 56.78 57.66 63.66 49.88 51.25 58.63 60.38 58.75 61.12 52.00 52.50 52.75 56.38 56.12 64.50 13B+ Instruction Tuned Model CodeLlama-13B-Instruct StarCoder2-15B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct DeepSeek-v2-Lite-Chat DeepSeekCoder-v2-Lite-Instruct 52.16 53.59 56.38 58.59 58.78 59.34 50.75 54.25 57.75 56.62 61.75 63.25 52.50 53.50 57.25 62.62 58.63 57.50 20B+ Instruction Tuned Model CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct 54.06 59.00 62.38 56.00 58.75 60.62 52.12 57.88 63.12 70B+ Instruction Tuned Model DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Qwen2.5-72B-Instruct 58.46 62.42 65.16 68. 55.00 60.62 61.00 66.88 60.25 63.33 67.88 70.12 Close-Sourced API Model GPT 4o-mini Doubao-Coder-Preview GLM-4-Plus DeepSeek-v3 Qwen2.5-Max Claude3.5-Sonnet GPT 4o 58.31 60.06 60.94 61.43 62.74 66.06 67.56 53.25 61.75 53.50 60.88 59.70 58.00 63. o1-like Model QwQ-32B-Preview Gemini2.0-Flash-Thinking DeepSeek-R1-Distill-Qwen-32B OpenAI o1-mini DeepSeek-R1 OpenAI o1-Preview 56.59 64.88 75.38 76.06 79.09 80.53 59.00 70.25 67.75 64.00 65.75 65.75 57.25 60.88 59.62 64.88 61.12 70.88 73.75 54.87 57.00 82.75 82.88 86.12 84. 22 42.75 51.38 53.87 50.50 52.38 54.00 50.25 53.25 59.13 52.38 66.62 50.31 50.50 59.62 58.25 62.14 60.25 47.15 66.38 65.50 64.62 66.38 55.62 67. 66.00 58.00 72.25 61.62 71.50 68.88 64.62 56.05 63.50 87.33 87.75 89.25 91.38 50.25 51.12 49.38 58.13 56.12 51.38 54.62 57.00 51.25 63.38 62.38 55.16 56.12 50.88 56.88 51.83 56.38 61.08 53.00 60. 53.94 59.29 76.12 68.88 56.75 59.62 58.38 58.32 58.63 66.50 68.00 56.46 68.75 63.52 69.62 75.06 80.25 Table 10. Results of different models on advanced critique evaluations MSE in the Code Gen Subset Dataset."
        },
        {
            "title": "All MBPP CodeForce LiveCodeBench Debug",
            "content": "0.5B+ Instruction Tuned Model Qwen2.5-Coder-0.5B-Instruct 23.96 20.75 24.77 27. 23.13 1B+ Instruction Tuned Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct 28.27 28.07 24.48 17.13 13.01 27.28 30.73 21.12 14.52 9.88 26.58 29.95 23.93 18.26 15. 6B+ Instruction Tuned Model CodeLlama-7B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct 20.89 19.45 19.06 14.28 8.73 6.17 19.80 13.56 16.47 11.95 6.95 4.72 23.16 24.09 20.99 16.73 8.76 6.63 13B+ Instruction Tuned Model StarCoder2-15B-Instruct CodeLlama-13B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct 20.92 15.55 6.46 6.35 5.57 5.02 17.12 14.64 5.82 5.75 4.41 4.28 22.38 18.24 7.89 7.40 6.00 5.09 20B+ Instruction Tuned Model CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct 15.06 5.19 5.09 17.23 4.45 4.03 16.98 6.23 5.10 70B+ Instruction Tuned Model Llama3.3-70B-Instruct DeepSeekCoder-v2-Instruct DeepSeek-v2.5 Qwen2.5-72B-Instruct 5.65 5.14 4.78 4. 4.45 4.59 4.11 4.42 5.25 5.23 4.63 4.46 Close-Sourced API Model Doubao-Coder-Preview Qwen2.5-Max GPT 4o GPT 4o-mini Claude3.5-Sonnet DeepSeek-v3 GLM-4-Plus 7.07 5.04 5.04 4.82 4.73 4.49 4.25 5.68 4.54 4.51 4.61 3.66 3.49 3. o1-like Model QwQ-32B-Preview OpenAI o1-mini OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B Gemini2.0-Flash-Thinking DeepSeek-R1 8.07 6.08 5.68 5.25 4.80 3.92 9.21 5.87 6.21 4.91 4.29 3.79 7.75 5.26 4.87 5.04 5.26 6.05 4.79 6.95 6.27 5.73 5.82 4.55 3. 23 31.29 25.25 28.95 18.58 14.47 20.48 19.12 21.19 14.54 8.66 5.65 24.50 14.80 5.53 5.59 4.61 5.00 14.24 3.28 5.14 7.17 5.50 5.27 4. 5.67 4.79 5.37 5.10 5.29 4.12 4.22 6.87 6.54 4.44 4.12 5.57 2.31 27.94 26.35 23.94 17.14 12.67 20.08 21.04 17.59 13.88 10.55 7.69 19.65 14.57 6.62 6.81 7.27 5.70 11.80 6.78 6. 5.74 5.22 5.08 4.88 9.16 5.58 5.40 4.52 4.70 4.31 4.04 9.26 5.67 6.33 6.13 4.81 5.84 Table 11. Results of different models on basic critique evaluations ACC in the Code QA Subset Dataset."
        },
        {
            "title": "Size",
            "content": "0.5B+ 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like"
        },
        {
            "title": "All",
            "content": "FP AP SE"
        },
        {
            "title": "DA MA DW ML",
            "content": "SC"
        },
        {
            "title": "DB MM OS",
            "content": "Qwen2.5-Coder-0.5B-Instruct 53.27 53.00 46.00 52.00 52. 53.00 49.00 69.00 59.00 50.00 55. 48.00 46.00 43.00 55.00 57.00 62.00 50.00 58.00 56.00 58.00 62.00 70.00 53.00 61.00 61.00 64.00 69.00 70.00 56.00 70.00 82.00 69.00 68.00 68.00 74. 65.00 66.00 61.00 63.00 64.00 75.00 78.00 53.00 56.00 63.00 69.00 68.00 66.00 51.00 61.00 58.00 58.00 62.00 54.00 54.00 56.00 61.00 62.00 61.00 49.00 54.00 61.00 64.00 54.00 62.00 60.00 59.00 64. 65.00 69.00 68.00 67.00 57.00 59.00 58.00 62.00 67.00 71.00 75.00 51.00 58.00 55.00 63.00 65.00 62.00 46.00 44.00 47.00 53.00 51.00 46.00 50.00 53.00 50.00 62.00 65.00 46.00 53.00 64.00 62.00 72.00 73. 48.00 74.00 73.00 71.00 73.00 76.00 70.00 67.00 66.00 73.00 69.00 74.00 70.00 87.00 59.00 56.00 59.00 51.00 61.00 65.00 47.00 47.00 42.00 50.00 47.00 48.00 49.00 48.00 54.00 56.00 52. 53.00 54.00 52.00 55.00 64.00 59.00 55.00 65.00 61.00 61.00 67.00 61.00 73.00 65.96 65.00 66.00 65.00 68.00 70.00 70.00 60.00 60.00 63.00 64.00 64.00 61.00 53.00 44.00 58.00 75.00 68. 75.00 72.00 76.00 64.00 65.00 67.00 65.00 54.55 77.00 71.00 70.00 70.00 73.00 71.00 78.00 59.00 78.00 78.00 70.71 58.00 59.00 68.69 62.00 68.00 72.00 80.00 37.00 57.00 54.00 59.00 63.00 66. 48.00 44.00 55.00 57.00 60.00 51.00 56.00 55.00 60.00 63.00 69.00 54.00 49.00 67.00 70.00 62.00 69.00 65.00 67.00 65.00 63.00 65.00 70.00 71.00 62.00 61.00 65.00 58.00 70.00 61.00 78. 56.00 59.00 54.00 55.00 57.00 56.00 51.00 44.00 49.00 49.00 43.00 52.00 52.00 49.00 51.00 52.00 57.00 47.00 48.00 52.00 60.00 58.00 54.00 48.00 62.00 66.00 67.00 66.00 57.00 66. 59.00 62.00 66.00 66.00 63.00 61.00 70.00 60.00 53.00 66.00 58.00 62.00 62.00 52.00 61.00 52.00 59.00 62.00 57.00 61.00 58.00 61.00 67.00 67.00 57.00 54.00 57.58 63.00 67.00 73.00 64.00 72.00 70. 78.00 66.00 77.00 79.00 72.00 67.00 67.00 69.00 73.00 78.00 86.00 53.61 61.00 60.00 60.00 74.00 71.00 47.00 51.00 53.00 50.00 54.00 50.00 53.00 60.00 49.00 51.00 54.00 47.00 53.00 53.00 55.00 65.00 61. 51.00 69.00 60.00 57.58 57.00 61.00 64.00 61.00 65.00 60.00 66.00 64.00 69.00 60.00 49.43 56.00 56.00 49.45 63.00 59.00 DeepSeek-Coder-1.3B-Instruct Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-1.5B-Instruct OpenCoder-8B-Instruct Yi-Coder-9B-Chat CodeLlama-7B-Instruct CodeQwen1.5-7B-Chat Qwen2.5-Coder-7B-Instruct Qwen2.5-Chat-7B-Instruct CodeLlama-13B-Instruct StarCoder2-15B-Instruct DeepSeek-v2-Lite-Chat DeepSeekCoder-v2-Lite-Instruct Qwen2.5-Chat-14B-Instruct Qwen2.5-Coder-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-v2.5 Llama3.3-70B-Instruct Qwen2.5-72B-Instruct DeepSeekCoder-v2-Instruct GLM-4-Plus DeepSeek-v3 Qwen2.5-Max Doubao-Coder-Preview GPT 4o-mini GPT 4o Claude3.5-Sonnet DeepSeek-R1 QwQ-32B-Preview OpenAI o1-mini OpenAI o1-Preview Gemini2.0-Flash-Thinking DeepSeek-R1-Distill-Qwen-32B 48.91 49.27 53.45 55.18 56. 52.64 55.91 56.00 56.27 61.36 62.82 52.18 52.97 60.42 61.18 64.00 66.64 57.27 68.64 69.45 65.85 68.09 68.09 70.23 63.35 63.64 65.17 65.36 67.09 69.53 76.73 54.36 58.82 59.27 59.89 63.55 64. 50.00 48.00 62.00 47.00 53.00 48.00 51.00 40.00 46.00 66.00 57.00 52.00 45.83 53.00 54.00 61.00 72.00 55.00 67.00 75.00 68.69 67.00 68.00 66.67 66.00 71.00 68.42 67.00 66.00 72.92 73. 62.86 68.00 65.00 67.71 65.00 69.00 47.00 55.00 57.00 52.00 56.00 48.00 59.00 65.00 65.00 69.00 72.00 51.00 56.00 67.00 55.00 62.00 70.00 55.00 79.00 70.00 65.00 73.00 65.00 71. 64.00 59.00 64.00 72.00 61.00 65.00 87.00 58.00 63.00 57.00 62.00 57.00 68.00 24 Table 12. Results of different models on advanced critique evaluations MSE in the Code QA Subset Dataset."
        },
        {
            "title": "Size",
            "content": "0.5B+ 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model"
        },
        {
            "title": "All",
            "content": "FP AP SE"
        },
        {
            "title": "DA MA DW ML",
            "content": "SC"
        },
        {
            "title": "DB MM OS",
            "content": "Qwen2.5-Coder-0.5B-Instruct 25.06 25.92 24.92 24.91 29. 25.20 25.52 18.71 28.84 24.07 23. 24.64 32.74 27.31 19.72 17.74 11.52 24.59 19.26 18.27 12.61 4.68 4.48 23.79 8.23 6.06 3.58 3.75 2.54 8.04 3.92 1.24 2.25 2.48 1.72 1. 4.61 1.82 1.50 1.88 1.10 1.00 1.07 4.91 3.96 1.67 1.55 1.30 1.37 37.67 29.19 16.55 16.09 11.79 18.98 17.81 13.07 9.32 4.89 4.58 28.60 9.01 2.65 2.70 2.47 1.66 9.40 2.86 2. 2.09 2.65 1.02 0.88 6.19 2.10 1.37 1.72 1.18 0.78 0.81 8.61 6.10 2.61 1.61 1.26 0.94 33.47 28.31 16.79 15.15 9.25 20.28 17.60 12.81 13.68 3.60 6.22 22.30 11.07 5.91 3.25 3.83 2. 7.23 2.91 1.74 1.93 2.35 2.44 1.40 4.79 1.49 0.81 1.17 1.17 0.77 1.06 6.86 4.88 4.35 2.47 1.19 1.66 34.50 24.83 18.57 13.98 9.39 20.20 15.15 11.24 11.31 3.19 5. 25.07 10.20 2.61 3.15 3.18 3.08 14.64 2.13 1.64 2.21 1.83 1.10 1.52 4.08 1.85 1.45 1.40 1.49 0.73 1.04 3.71 3.77 1.70 1.30 1.12 1.03 25.28 20.11 15.61 16.11 12. 15.64 14.69 16.02 14.85 5.73 4.18 19.56 11.33 3.78 3.65 2.47 4.64 8.31 4.27 2.77 1.86 1.45 1.66 2.19 5.89 1.73 1.36 1.01 1.74 2.51 1.19 4.11 2.76 1.62 1.16 1.73 1. 34.15 34.02 26.35 18.31 13.83 21.13 18.99 16.71 26.80 4.78 7.32 25.62 9.48 2.43 3.20 2.57 2.20 9.01 3.23 3.14 2.12 2.60 1.25 1.39 4.69 1.42 1.73 1.11 0.91 1.13 0. 6.01 5.84 2.45 1.46 1.19 1.10 33.39 22.59 14.74 16.66 12.48 16.81 15.34 11.19 13.23 3.72 5.59 21.86 6.69 3.92 3.28 3.45 1.87 6.21 1.66 1.45 2.20 1.78 1.89 1. 4.97 1.55 1.01 1.07 1.19 1.17 0.85 2.75 4.25 1.88 1.31 0.92 0.85 31.86 25.08 15.45 13.00 10.18 18.02 11.57 14.10 13.01 3.25 6.05 20.58 10.21 2.32 3.58 2.43 1.95 12.33 2.97 1. 2.32 2.17 0.91 0.84 4.50 1.16 2.30 1.24 0.84 1.37 0.82 3.36 5.33 1.50 1.33 1.01 1.03 30.59 26.87 14.85 11.76 8.78 19.99 12.90 13.25 14.48 4.79 8.82 18.22 7.11 3.70 2.81 3.06 3. 7.13 2.27 2.36 2.31 2.70 2.00 2.05 4.37 1.82 1.78 1.43 1.01 1.41 1.02 3.65 5.21 2.45 2.71 3.71 1.62 Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Chat-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Chat-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct Llama3.3-70B-Instruct Qwen2.5-72B-Instruct DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Doubao-Coder-Preview GLM-4-Plus GPT 4o Qwen2.5-Max GPT 4o-mini DeepSeek-v3 Claude3.5-Sonnet DeepSeek-R1 QwQ-32B-Preview OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B OpenAI o1-mini Gemini2.0-Flash-Thinking 32.13 26.23 17.39 14.93 10. 18.99 15.78 14.02 13.96 4.10 6.15 22.88 9.26 3.65 3.35 3.29 2.51 8.76 2.89 2.09 2.24 2.20 1.63 1.46 4.90 1.69 1.55 1.33 1.30 1.18 1.02 5.02 4.67 2.26 1.71 1.54 1. 31.45 26.21 15.41 9.96 6.94 17.38 14.17 13.05 15.77 3.75 8.76 21.39 9.40 2.35 4.16 5.85 1.97 7.20 3.09 2.50 2.80 2.51 2.26 1.70 5.56 2.34 3.04 1.48 2.26 1.56 1. 7.20 5.78 2.53 2.59 1.84 1.26 28.35 24.05 17.22 15.42 11.35 15.87 16.00 14.55 8.69 2.73 6.15 24.64 8.72 4.44 3.46 3.80 2.23 6.84 2.46 1.94 2.52 1.68 1.70 1. 4.24 1.33 0.81 1.07 1.37 0.54 0.76 4.50 3.65 2.08 1.40 1.62 0.76 25 Table 13. Results of different models on advanced critique evaluations MSE in the Code Gen Subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct OpenCoder-1.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct Yi-Coder-9B-Chat StarCoder2-15B-Instruct CodeLlama-13B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct DeepSeek-v2.5 Llama3.3-70B-Instruct DeepSeekCoder-v2-Instruct Qwen2.5-72B-Instruct Doubao-Coder-Preview DeepSeek-v3 GPT 4o-mini GPT 4o Qwen2.5-Max GLM-4-Plus Claude3.5-Sonnet DeepSeek-R1 QwQ-32B-Preview OpenAI o1-mini OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B Gemini2.0-Flash-Thinking Correctness Verification Code Readability Robustness Validation Comprehensive Testing Space Complexity Code Style Consistency Output Format Maintain- -ability Time Complexity Algorithm Optimization 35.17 40.34 37.59 28.53 18.78 16.67 27.22 27.07 19.55 17.46 12.95 11.53 29.73 15.26 9.10 8.26 7.34 6. 13.41 7.40 4.68 5.74 5.52 5.42 3.59 9.43 6.63 5.02 4.13 4.00 3.92 3.47 9.07 8.97 6.28 6.06 5.80 5.71 30.70 34.04 30.75 33.76 12.78 12. 24.39 22.94 17.24 7.28 3.90 10.29 26.86 11.76 5.06 5.39 4.96 2.77 9.45 3.67 2.58 2.21 2.61 3.05 1.65 5.82 3.12 1.47 2.03 1.55 1.81 1.95 6.01 6.26 3.76 3.52 2.54 2. 26.72 28.72 26.10 28.70 13.84 13.68 22.05 20.66 16.93 10.81 8.05 11.16 24.43 13.91 7.54 7.55 7.25 6.02 12.11 5.83 4.59 4.45 5.23 4.68 3. 9.23 4.31 4.04 4.11 3.55 4.08 3.54 10.09 9.44 5.67 6.23 6.35 6.47 24.09 25.81 25.41 25.36 14.83 14.20 23.03 21.26 17.62 8.32 5.09 10.93 22.81 13.81 6.04 4.99 5.28 3. 12.18 4.32 3.34 2.56 4.15 2.95 2.08 10.41 3.76 3.45 2.48 2.79 2.16 2.46 7.19 7.55 4.10 3.55 3.94 3.66 50.49 55.82 50.82 55.37 19.94 17. 36.40 33.80 26.16 10.14 6.97 8.29 42.03 14.14 6.76 6.84 6.34 4.17 11.52 4.75 3.52 2.84 3.03 3.38 2.07 7.29 5.28 3.58 2.94 2.34 2.73 2.25 7.21 9.97 5.56 4.18 3.34 3. 35.20 39.10 35.65 39.30 14.96 15.62 27.75 25.17 21.55 10.35 5.91 13.23 32.07 14.66 5.70 6.12 7.63 4.01 11.78 5.71 4.66 3.11 3.41 3.77 2. 9.18 4.51 2.76 2.52 2.20 2.15 3.44 9.47 7.81 3.95 3.13 3.50 3.10 61.39 66.84 61.07 66.55 23.68 20.70 44.76 41.28 33.11 15.32 11.01 10.57 52.33 17.48 8.79 8.84 11.01 5. 13.92 8.97 5.14 3.88 4.55 4.44 3.42 11.63 6.79 4.15 4.06 3.04 3.55 3.91 10.33 13.18 6.64 5.45 4.50 4.74 36.78 39.57 37.07 39.45 15.23 13. 29.13 27.42 22.13 9.14 5.28 11.63 32.50 13.99 5.34 5.23 5.91 3.63 10.97 5.13 3.19 3.03 3.24 3.45 1.70 9.15 4.39 2.46 2.24 1.74 1.90 2.89 5.31 8.37 4.05 2.80 3.00 3. 42.97 48.42 43.62 24.38 18.45 16.76 32.16 30.22 21.25 10.70 7.83 9.08 36.79 13.76 7.09 7.21 8.11 5.07 10.77 6.49 4.24 3.69 3.87 4.12 2. 8.44 5.34 3.79 3.03 2.59 3.24 2.66 7.86 9.13 4.85 4.12 3.59 3.94 42.84 43.84 40.34 43.70 16.10 15.78 30.65 28.81 22.62 11.56 8.44 9.87 34.92 13.24 6.96 7.26 8.47 5. 11.12 7.33 4.40 3.59 4.57 4.21 2.88 9.66 5.70 4.72 3.32 2.92 3.41 3.48 7.90 9.56 5.22 4.31 4.48 4.83 Table 14. Results of different models on advanced critique evaluations MSE in the Code Gens MBPP subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct OpenCoder-1.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct CodeQwen1.5-7B-Chat OpenCoder-8B-Instruct Qwen2.5-Chat-7B-Instruct CodeLlama-7B-Instruct Yi-Coder-9B-Chat Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct DeepSeekCoder-v2-Lite-Instruct Qwen2.5-Chat-14B-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Coder-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct Llama3.3-70B-Instruct DeepSeekCoder-v2-Instruct DeepSeek-v2.5 Qwen2.5-72B-Instruct Doubao-Coder-Preview DeepSeek-v3 GPT 4o-mini GLM-4-Plus Qwen2.5-Max GPT 4o Claude3.5-Sonnet DeepSeek-R1 QwQ-32B-Preview OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B OpenAI o1-mini Gemini2.0-Flash-Thinking Correctness Verification Code Readability Robustness Validation Comprehensive Testing Space Complexity Code Style Consistency Output Format Maintain- -ability Time Complexity Algorithm Optimization 26.43 32.02 27.81 22.53 17.75 13.90 20.23 19.54 16.76 14.65 9.65 9. 22.82 14.22 7.53 7.17 6.92 5.85 13.39 6.24 4.22 5.99 5.06 4.82 3.72 8.97 5.12 5.01 3.95 3.88 3.69 3.07 7.76 7.13 5.27 5.10 5.00 4.82 23. 29.14 24.03 29.68 10.64 9.70 18.18 18.33 6.12 14.61 10.15 3.38 22.11 10.87 4.56 2.51 5.27 3.99 10.93 2.90 1.66 2.38 3.70 2.19 1.90 4.78 1.86 1.37 1.70 1.43 1.99 1. 4.54 4.81 4.15 2.26 3.35 2.88 17.57 20.26 17.17 21.54 11.09 9.81 15.72 14.71 7.49 13.38 9.00 5.67 18.80 13.06 6.42 5.05 6.02 4.74 13.00 4.30 3. 4.23 3.86 3.90 2.93 5.90 2.89 2.76 3.24 2.84 3.23 3.00 7.13 7.72 5.18 4.45 4.60 6.11 15.84 18.26 18.85 17.15 14.23 12.17 19.31 18.19 5.36 15.92 11.81 3. 17.21 14.70 3.74 3.35 3.85 3.80 15.18 3.46 3.08 3.92 2.89 2.01 2.10 7.02 2.50 2.95 2.19 2.68 2.43 2.31 5.66 6.46 2.87 2.92 3.75 3.44 48.93 57.32 48.77 58.31 19.44 16.41 28.67 29.52 5.81 24.01 8.80 5.64 40.65 13.34 5.62 3.00 5.83 3.69 13.83 3.52 1.98 2.20 2.97 2.06 1. 5.20 3.04 3.59 1.89 1.67 1.98 1.23 6.77 7.81 2.49 2.47 4.29 2.17 35.47 42.15 35.64 43.84 14.05 13.00 22.33 24.13 8.54 19.61 11.74 5.33 32.56 12.93 5.20 4.07 5.57 6. 12.62 4.39 3.96 3.07 3.75 2.60 1.68 8.41 3.44 3.08 1.59 2.29 2.29 2.48 6.52 6.99 2.65 3.46 3.53 2.35 47.13 53.41 46.03 55.42 16.93 14. 28.19 28.31 9.14 24.78 11.17 6.40 39.51 13.38 6.52 5.39 5.77 5.68 14.58 6.47 4.25 4.61 4.31 3.37 3.53 10.21 4.71 5.04 2.93 3.30 3.58 3.28 9.42 10.09 4.84 4.11 4.62 3. 31.68 37.14 32.95 38.25 12.07 11.06 22.62 21.40 5.60 17.76 9.49 4.37 28.27 12.61 3.59 3.61 3.96 4.14 11.26 3.77 2.32 2.23 3.64 2.96 1. 7.88 3.48 2.40 1.55 1.47 1.87 2.25 4.74 6.43 2.41 2.66 3.20 2.64 41.82 50.27 41.76 24.05 18.76 15.31 26.22 26.28 7.37 17.31 10.08 6.30 36.23 12.67 5.99 4.47 6.09 4. 11.43 5.07 2.72 3.16 3.84 3.11 2.26 6.49 3.64 4.10 2.83 2.05 2.41 1.81 7.36 7.21 2.99 2.79 3.92 2.76 37.50 44.75 38.65 45.71 14.27 13. 25.67 23.99 7.29 19.82 9.44 6.51 33.50 11.42 5.55 5.12 5.76 5.66 12.70 5.69 3.76 3.98 4.13 3.06 3.05 7.87 4.56 5.09 3.62 2.85 2.81 2.80 7.92 8.10 3.32 3.09 3.87 4. Table 15. Results of different models on advanced critique evaluations MSE in the Code Gens CodeForce subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct OpenCoder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-1.5B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct Yi-Coder-9B-Chat StarCoder2-15B-Instruct CodeLlama-13B-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct DeepSeekCoder-v2-Lite-Instruct CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct DeepSeek-v2.5 Qwen2.5-72B-Instruct Doubao-Coder-Preview DeepSeek-v3 GPT 4o-mini GLM-4-Plus Qwen2.5-Max GPT 4o Claude3.5-Sonnet DeepSeek-R1 QwQ-32B-Preview OpenAI o1-Preview OpenAI o1-mini DeepSeek-R1-Distill-Qwen-32B Gemini2.0-Flash-Thinking Correctness Verification Code Readability Robustness Validation Comprehensive Testing Space Complexity Code Style Consistency Output Format Maintain- -ability Time Complexity Algorithm Optimization 34. 38.90 37.69 25.00 18.29 17.96 29.19 28.03 21.40 16.45 13.26 12.69 30.09 17.06 9.50 7.02 6.02 5.42 15.57 7.85 4.10 8.82 6.00 5.03 3.25 8.73 8.44 5.03 3.93 3.92 3.85 3. 9.10 9.02 6.71 6.57 5.75 5.33 27.62 30.59 29.16 30.12 14.20 11.96 24.30 22.64 18.21 6.83 3.95 12.50 24.96 13.98 5.83 4.61 2.68 2.63 10.31 4.31 2. 5.99 2.49 2.26 1.54 7.42 4.40 1.46 1.53 1.52 2.01 2.65 6.55 4.89 3.07 3.64 2.66 2.99 26.64 27.77 26.22 26.82 16.34 14.28 24.07 20.85 18.33 10.92 9.11 11. 24.25 16.09 7.65 7.77 5.85 5.11 13.31 6.78 4.80 7.46 5.47 5.04 3.44 10.25 5.63 4.42 4.59 4.19 4.66 4.23 11.55 9.52 6.86 5.64 7.89 6.56 23. 24.72 24.49 24.28 15.18 14.77 23.59 21.29 18.53 7.75 5.10 11.30 22.72 15.45 6.29 5.28 3.29 2.63 12.51 4.28 3.26 4.90 3.81 2.21 1.96 9.80 4.97 3.33 2.07 2.66 2.28 2. 7.28 6.40 3.53 3.62 4.03 3.63 51.75 57.34 53.43 56.40 19.11 19.65 40.27 36.82 29.30 10.81 7.41 7.90 42.45 16.08 7.04 6.46 4.46 3.26 11.74 5.82 3. 6.80 3.23 3.16 2.40 7.57 8.01 4.07 2.99 2.97 3.33 2.64 7.16 9.83 5.16 6.39 3.74 4.59 35.96 39.40 37.18 38.77 16.31 14.69 30.71 27.90 23.66 10.74 6.17 13. 32.61 15.80 5.72 7.44 3.94 3.43 11.45 6.79 5.15 5.73 3.66 2.99 2.29 9.16 6.13 2.73 2.13 2.24 2.69 3.75 6.79 6.85 3.28 4.18 3.53 3.36 58. 64.40 60.31 63.49 23.05 22.52 46.84 42.01 33.51 16.22 11.91 10.29 51.13 20.77 9.21 12.22 6.06 4.94 14.30 11.28 5.08 10.40 5.08 4.69 4.18 13.04 9.72 4.36 3.72 3.56 4.87 4. 10.55 11.55 6.01 7.52 5.14 5.54 31.43 33.70 32.40 33.38 15.23 14.13 28.38 25.63 22.19 9.59 6.10 14.22 28.87 16.28 6.30 5.66 3.28 3.36 11.69 5.66 3. 6.13 3.17 2.88 1.81 10.09 5.56 2.35 1.96 1.92 2.23 3.39 4.88 6.14 2.91 4.00 3.21 3.69 46.22 51.93 48.45 20.84 18.38 17.39 36.51 33.73 24.09 12.06 9.06 8. 38.93 15.39 8.23 8.45 5.33 3.96 10.86 7.83 4.56 7.91 4.45 3.52 2.93 9.68 8.11 4.04 3.30 2.95 3.36 3.28 8.23 9.08 4.78 5.31 3.97 4.64 50. 42.65 40.60 42.03 16.74 14.92 33.33 30.33 24.56 11.77 8.46 10.46 34.46 14.93 7.80 7.88 4.97 4.18 10.64 7.59 3.92 7.53 4.35 3.17 2.82 10.23 7.56 4.84 2.97 2.89 3.14 3. 7.08 8.16 4.56 5.31 4.39 4.72 Table 16. Results of different models on advanced critique evaluations MSE in the Code Gens LiveCodeBench subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct OpenCoder-1.5B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct CodeQwen1.5-7B-Chat OpenCoder-8B-Instruct Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Chat-14B-Instruct Qwen2.5-Coder-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Qwen2.5-72B-Instruct Doubao-Coder-Preview DeepSeek-v3 GPT 4o-mini GPT 4o Claude3.5-Sonnet GLM-4-Plus Qwen2.5-Max DeepSeek-R1 QwQ-32B-Preview OpenAI o1-mini Gemini2.0-Flash-Thinking OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B Correctness Verification Code Readability Robustness Validation Comprehensive Testing Space Complexity Code Style Consistency Output Format Maintain- -ability Time Complexity Algorithm Optimization 45.79 51.04 48.18 35.07 21.61 16.58 34.25 33.91 33.91 23.81 17.80 12.91 36.90 14.85 10.10 8.12 7.33 7.00 14.71 6.33 5.31 6.47 6.03 5.66 4. 7.80 7.19 5.38 5.07 4.13 4.06 3.80 9.51 8.61 7.10 7.04 6.02 6.01 49.24 52.77 48.61 51.96 16.57 11.20 32.96 35.04 35.04 21.98 8.66 4.15 40.33 9.48 4.10 3.79 3.24 5. 7.43 3.66 3.23 1.55 2.33 2.55 1.38 3.43 3.70 1.85 2.04 1.52 1.59 1.43 6.04 8.59 4.83 2.72 2.99 2.50 40.35 42.89 39.79 41.85 18.42 15. 28.22 30.15 30.15 21.45 15.02 10.25 34.07 13.67 9.10 9.36 8.22 9.09 12.44 6.16 5.90 4.84 5.78 6.82 3.73 11.79 5.53 6.15 5.27 3.65 4.95 3.75 11.44 11.06 7.55 7.21 6.69 7. 29.94 31.57 30.54 31.65 16.07 14.37 23.22 26.34 26.34 18.14 9.36 4.84 26.89 12.34 6.49 4.83 4.43 5.28 11.49 3.59 3.15 2.44 2.86 5.03 2. 13.24 3.70 4.18 2.66 2.51 2.10 2.36 6.89 7.98 4.92 4.10 3.90 4.27 27 47.71 50.98 47.19 51.09 17.96 11.94 32.62 34.22 34.22 23.34 8.86 5. 39.94 11.73 6.07 5.22 4.59 5.17 10.12 4.20 4.15 2.63 3.15 3.81 2.06 7.02 4.97 3.25 3.10 2.93 2.45 2.18 7.62 9.82 6.19 4.65 4.78 4.10 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 75.26 79.92 72.97 79.21 27.35 15.83 50.29 52.35 52.35 36.81 14.02 8.32 62.76 15.63 8.38 6.28 5.53 9.57 13.19 6.79 4. 2.37 3.14 4.02 2.86 8.52 6.18 3.73 3.65 3.41 2.79 1.81 11.51 13.92 6.80 4.31 4.51 3.87 48.04 50.79 47.45 50.33 17.89 11.92 34.24 35.80 35.80 25.05 8.76 4. 40.95 11.99 5.03 4.27 3.97 6.16 10.33 4.77 3.14 2.23 2.76 3.44 1.55 7.42 4.31 2.91 2.62 1.87 1.66 1.50 5.88 10.62 4.86 3.27 3.01 2.90 35. 38.91 36.29 16.39 15.98 11.02 26.06 28.32 28.32 18.43 7.28 4.90 31.07 12.02 5.28 4.65 4.70 6.51 10.20 4.77 4.11 3.02 2.92 4.18 2.83 6.33 4.44 3.05 2.95 2.87 2.79 2. 6.98 7.42 4.72 4.55 3.92 3.53 36.23 38.13 35.78 38.30 15.79 11.96 26.25 28.16 28.16 20.51 8.96 5.67 31.53 11.43 5.80 6.19 5.27 6.52 10.84 4.88 3. 3.25 3.95 5.60 2.66 7.44 4.69 3.76 3.36 3.35 2.66 2.10 7.78 8.26 5.31 5.29 4.03 4.41 Table 17. Results of different models on advanced critique evaluations MSE in the Code Gens Debug subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct OpenCoder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-1.5B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Qwen2.5-Chat-7B-Instruct CodeLlama-7B-Instruct Qwen2.5-Coder-7B-Instruct Yi-Coder-9B-Chat StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Chat-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Qwen2.5-72B-Instruct Doubao-Coder-Preview DeepSeek-v3 GPT 4o-mini Qwen2.5-Max GPT 4o GLM-4-Plus Claude3.5-Sonnet QwQ-32B-Preview DeepSeek-R1 OpenAI o1-mini DeepSeek-R1-Distill-Qwen-32B OpenAI o1-Preview Gemini2.0-Flash-Thinking Correctness Verification Code Readability Robustness Validation Comprehensive Testing Space Complexity Code Style Consistency Output Format Maintain- -ability Time Complexity Algorithm Optimization 35.30 40.35 37.69 32.33 18.10 17.96 26.91 26.41 18.93 18.76 16.41 12.07 29.81 14.93 9.65 9.41 9.30 7. 9.86 9.25 5.17 6.76 5.18 4.35 3.35 12.29 5.82 4.67 4.42 3.96 3.73 3.36 11.32 10.04 6.56 6.40 6.28 5.75 25.15 26.66 24.41 26.16 13.77 12. 22.03 19.92 7.75 15.25 4.19 12.20 22.39 12.65 5.51 5.71 6.38 2.74 8.83 3.95 3.01 2.73 3.34 3.02 1.70 7.50 2.84 1.25 1.81 2.10 2.36 2.39 7.04 7.08 3.42 2.75 3.68 2. 26.83 28.28 25.70 28.32 14.51 13.10 22.67 20.34 11.34 16.39 8.26 11.81 23.57 13.29 8.38 7.55 8.07 5.53 9.72 6.61 4.56 4.30 4.43 4.96 3. 10.34 3.91 3.55 3.70 3.78 3.96 3.53 10.14 11.36 5.46 6.69 6.59 6.21 27.25 28.52 27.59 28.15 15.02 14.16 23.80 21.12 10.88 17.79 6.70 11.15 24.23 12.66 6.83 7.66 6.48 4. 9.46 6.04 3.93 3.64 3.48 3.79 2.26 11.49 3.79 3.29 3.52 2.57 2.29 2.69 9.52 8.98 4.12 4.51 3.90 3.45 53.51 57.99 53.81 56.14 23.75 22. 40.94 36.59 14.68 27.90 9.34 9.61 45.01 15.44 9.80 8.23 9.56 4.48 10.71 5.34 4.21 3.43 4.09 2.75 2.22 9.15 4.88 3.44 2.50 3.25 3.50 2.09 12.21 7.24 5.23 2.93 4.09 3. 34.16 35.75 34.12 35.30 17.55 16.14 28.41 25.29 11.76 21.38 6.24 14.81 31.09 15.24 8.74 6.19 7.05 4.02 11.28 5.95 4.88 3.74 4.13 3.51 2. 9.96 3.96 2.38 2.08 2.57 2.73 4.10 9.58 15.11 4.14 3.50 3.47 3.60 63.47 68.02 63.02 66.79 28.24 27.05 49.26 42.87 20.94 36.20 16.69 13.10 54.27 19.39 15.76 10.69 12.52 5. 13.68 10.86 6.14 4.97 5.34 4.47 3.10 14.43 6.12 3.57 3.50 4.01 4.65 4.23 16.82 9.69 7.28 4.78 6.31 5.21 34.21 35.32 33.84 34.73 17.32 16. 28.76 25.55 12.27 22.34 6.15 14.35 30.45 14.76 7.55 6.08 6.44 3.66 10.59 6.13 4.12 4.28 4.29 3.99 1.84 11.25 3.82 2.09 2.08 2.10 2.45 4.13 10.20 5.65 3.87 3.15 2.75 3. 48.51 53.35 48.61 36.22 22.61 21.68 37.80 35.12 16.19 25.35 11.19 11.35 41.31 15.12 12.52 8.98 10.49 5.79 10.64 8.39 5.53 5.07 5.83 3.69 2. 11.37 5.38 4.02 3.22 3.39 4.05 2.68 12.81 8.95 5.45 4.07 4.78 3.78 47.20 50.52 46.54 49.57 21.13 19.17 36.40 32.81 17.81 25.40 13.10 11.24 40.27 15.11 13.57 8.61 9.53 6. 10.57 11.16 6.20 4.83 4.59 4.14 3.02 13.06 5.93 5.33 3.91 3.89 4.50 4.14 13.66 8.85 6.17 5.83 5.24 4.84 Table 18. Results of different models on advanced critique evaluations MSE in the Code QA Subset Dataset across all fine-grained evaluation dimensions."
        },
        {
            "title": "Model",
            "content": "0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Qwen2.5-72B-Instruct Doubao-Coder-Preview GPT 4o DeepSeek-v3 GLM-4-Plus GPT 4o-mini Qwen2.5-Max Claude3.5-Sonnet QwQ-32B-Preview DeepSeek-R1 OpenAI o1-Preview Gemini2.0-Flash-Thinking DeepSeek-R1-Distill-Qwen-32B OpenAI o1-mini"
        },
        {
            "title": "Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance",
            "content": "40.75 43.30 42.51 29.37 21.08 17.38 39.72 29.31 21.15 18.03 14.91 8.92 34.81 12.12 7.99 6.50 5.43 5.35 10.32 6.65 6.55 4.31 3.38 3.19 2. 6.72 3.13 2.86 2.84 2.12 1.95 1.67 9.73 7.30 4.53 3.38 3.20 3.16 53.88 57.61 56.39 36.60 25.80 20.88 51.43 36.57 24.56 21.22 21.12 13.94 45.40 12.87 12.61 12.32 5.25 4. 12.17 11.94 11.69 4.67 3.51 2.61 2.48 4.87 3.51 3.55 2.42 2.31 1.88 1.74 11.34 9.12 3.90 2.35 2.84 3.06 33.81 35.68 35.24 24.85 19.24 15. 33.14 24.71 17.54 16.38 15.01 10.18 30.17 11.01 9.34 8.71 4.05 4.04 9.64 8.30 8.97 3.72 2.76 3.59 2.84 8.49 2.45 2.29 2.95 1.75 1.53 1.11 8.99 4.86 2.95 2.28 2.39 2. 40.37 43.19 40.97 28.25 19.55 16.76 38.42 28.75 20.21 16.25 11.82 8.46 34.05 12.07 7.38 5.68 6.94 6.77 10.37 5.28 5.43 5.01 4.22 3.53 3. 4.86 3.63 3.48 3.31 1.97 2.38 1.85 9.70 7.74 4.72 3.79 3.54 3.50 52.26 55.84 54.77 34.98 23.34 19.03 49.73 34.76 23.69 18.31 17.61 9.94 43.36 12.00 8.49 5.96 4.92 4. 10.10 5.90 6.20 4.72 3.48 2.82 2.67 4.09 3.49 3.25 2.73 2.20 1.86 1.97 10.64 6.15 3.84 2.14 2.52 2.79 39.14 41.77 41.32 27.78 20.82 17. 37.79 28.67 21.59 18.63 18.06 12.35 34.86 13.14 11.48 10.62 6.69 6.38 11.83 10.15 10.39 4.23 3.84 4.30 3.38 8.13 3.30 2.94 3.38 2.95 2.07 2.05 10.25 7.14 4.27 3.30 2.83 3. 39.04 41.71 40.89 26.27 18.97 15.49 37.68 27.02 18.89 15.92 14.19 9.31 33.39 11.39 8.57 7.48 5.16 4.97 9.85 6.96 7.42 4.21 3.46 3.28 3. 6.73 3.29 2.85 3.37 2.34 1.73 1.55 9.21 6.60 3.92 2.80 2.69 3.00 28 41.45 44.28 43.45 27.93 19.46 16.35 39.84 28.54 20.23 15.76 13.63 7. 34.99 11.72 7.13 5.62 5.28 5.09 9.39 4.86 5.46 4.62 3.51 3.16 3.10 7.20 3.38 2.74 3.31 1.96 1.61 1.81 9.46 5.06 3.68 2.05 2.59 3.15 39. 41.76 39.77 27.48 19.90 14.87 34.09 27.39 20.96 14.78 10.02 7.90 34.01 12.33 6.14 4.69 7.09 6.16 10.40 4.85 4.98 4.88 4.21 3.51 3.92 5.39 3.63 3.25 3.25 2.03 2.16 1. 8.88 7.91 5.36 3.62 3.59 3.68 40.21 42.69 41.90 27.74 19.70 15.69 39.17 28.62 19.54 15.92 12.60 8.06 34.23 11.68 6.97 4.81 4.89 5.23 9.61 4.71 4. 4.73 3.29 2.96 3.13 6.16 2.95 2.63 3.18 1.84 1.72 1.53 9.27 4.87 2.94 2.00 2.56 2.82 Table 19. Results of different models on advanced critique evaluations MSE in the Code QAs Fundamental Programming (FP) subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct DeepSeek-v2-Lite-Chat DeepSeekCoder-v2-Lite-Instruct Qwen2.5-Chat-14B-Instruct Qwen2.5-Coder-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Qwen2.5-72B-Instruct Llama3.3-70B-Instruct Doubao-Coder-Preview GPT 4o GPT 4o-mini GLM-4-Plus Qwen2.5-Max DeepSeek-v3 Claude3.5-Sonnet QwQ-32B-Preview DeepSeek-R1 OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B OpenAI o1-mini Gemini2.0-Flash-Thinking Depth Logical Coherence Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance 44.06 46.06 43.14 28.22 12.76 9.94 41.60 29.28 17.42 17.22 16.99 6.81 33.16 10.25 7.10 5.76 5.67 5.07 12.86 6.64 4. 4.57 3.49 3.29 3.16 7.76 5.44 3.78 3.43 2.67 2.21 2.11 10.09 8.11 5.23 4.32 3.16 2.93 60.14 62.58 57.25 35.67 12.22 9.56 52.93 33.64 16.53 22.56 22.46 5. 42.19 9.47 4.83 4.80 6.67 5.42 14.59 6.85 5.15 4.29 3.90 3.11 2.70 6.57 6.51 3.62 2.96 2.75 1.85 2.36 11.84 13.56 4.23 4.13 2.57 3.05 35. 36.59 34.64 23.48 12.60 9.32 32.34 23.77 12.10 15.52 14.32 5.13 28.54 9.13 6.69 4.48 5.09 3.03 11.65 4.96 4.43 3.11 2.87 3.54 3.97 11.05 4.11 2.80 3.02 2.09 1.40 1. 10.89 6.12 4.06 2.80 2.37 2.10 43.10 44.81 41.50 25.73 9.95 7.93 38.69 23.46 12.77 16.56 16.68 6.43 30.14 9.30 6.67 5.59 3.78 4.76 10.70 5.27 4. 5.02 2.94 3.47 3.48 7.21 5.51 3.77 3.99 2.31 2.29 2.50 11.13 6.94 5.77 4.30 3.96 2.67 53.45 55.22 51.09 30.89 11.02 9.80 46.25 29.25 16.43 19.12 19.11 5. 37.59 9.38 6.78 5.64 4.27 4.08 9.28 5.13 4.05 4.66 3.59 2.99 3.15 4.49 6.09 2.93 3.35 2.39 1.94 2.55 10.70 6.49 5.57 2.93 3.40 1.97 43. 45.37 43.96 29.53 12.80 11.33 39.27 27.01 17.03 19.04 18.81 8.18 31.42 10.44 7.67 7.51 5.44 5.16 14.09 5.26 5.30 4.31 4.46 3.58 4.84 9.10 5.59 3.73 4.30 2.54 2.33 3. 12.51 10.29 4.27 3.19 4.24 2.99 47.37 49.76 43.37 30.92 12.07 9.93 42.98 29.91 15.46 16.46 14.87 7.03 35.19 8.48 8.88 8.42 5.10 4.55 9.32 5.49 4. 6.25 4.60 4.04 3.53 6.19 6.53 2.92 4.02 3.10 3.03 2.74 10.93 8.47 5.28 4.15 2.75 3.08 43.64 45.23 41.92 27.49 9.54 8.41 38.88 25.05 14.92 19.53 15.51 4. 30.62 8.95 7.24 5.06 4.52 4.10 8.75 4.77 4.30 4.47 2.87 3.21 3.08 7.76 4.87 2.20 3.18 1.78 1.83 2.04 10.97 5.69 4.13 2.22 2.88 1.60 44. 46.58 41.03 30.15 12.54 8.50 32.81 25.52 16.98 15.22 13.63 7.60 33.12 10.17 9.45 8.68 3.23 5.40 11.82 4.25 4.18 5.39 5.42 4.03 4.20 4.52 5.77 2.87 4.18 2.22 2.39 1. 8.94 8.67 5.95 4.66 2.84 3.12 44.82 45.80 43.76 26.68 12.35 10.61 41.62 25.64 14.74 17.92 17.14 7.79 33.72 9.20 8.88 5.09 3.94 5.08 9.91 4.81 4. 4.31 2.81 3.23 3.52 7.93 4.72 3.25 3.55 2.15 2.49 2.21 9.66 5.83 3.67 4.08 3.55 2.24 Table 20. Results of different models on advanced critique evaluations MSE in the Code QAs Advanced Programming (AP) subset Dataset across all fine-grained evaluation dimensions."
        },
        {
            "title": "Model",
            "content": "0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model OpenCoder-1.5B-Instruct Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Chat-14B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-v2.5 Llama3.3-70B-Instruct Qwen2.5-72B-Instruct DeepSeekCoder-v2-Instruct Doubao-Coder-Preview Qwen2.5-Max GPT 4o DeepSeek-v3 GPT 4o-mini GLM-4-Plus Claude3.5-Sonnet QwQ-32B-Preview DeepSeek-R1 OpenAI o1-Preview Gemini2.0-Flash-Thinking OpenAI o1-mini DeepSeek-R1-Distill-Qwen-32B"
        },
        {
            "title": "Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance",
            "content": "45.51 47.26 46.46 29.62 25.54 18.83 45.56 29.25 25.04 16.67 13.32 10.63 42.52 11.94 7.88 7.36 7.16 7.04 11.17 9.85 7.47 6.73 2.88 2.68 2. 7.57 3.06 2.83 2.73 2.69 2.26 1.26 8.14 8.01 4.49 3.83 3.15 2.86 55.76 57.97 57.16 37.65 28.91 22.93 54.22 34.74 29.08 19.73 19.65 14.78 52.45 11.09 10.95 11.84 3.54 4. 12.89 14.46 11.07 7.23 2.91 2.09 2.46 3.86 2.78 2.62 1.95 2.55 1.76 1.84 10.11 10.50 4.41 2.20 4.55 3.39 41.08 42.39 42.39 26.04 21.78 16. 40.70 25.77 22.07 17.17 17.16 10.88 37.76 10.46 8.16 9.98 3.23 3.62 13.11 9.48 9.76 6.02 2.35 1.77 1.76 8.80 1.90 2.76 1.73 1.95 2.72 1.17 9.23 5.07 2.37 1.33 2.41 2. 41.18 40.33 42.84 24.99 20.11 16.49 41.24 26.85 22.07 15.70 9.30 5.43 35.67 10.95 5.14 5.50 6.95 8.54 10.15 7.05 4.75 6.58 4.36 3.38 2. 4.95 3.64 3.83 2.57 2.06 4.07 1.36 10.53 7.46 5.76 4.88 4.19 2.88 51.21 52.06 52.85 31.06 22.52 17.87 48.87 31.05 25.56 16.80 15.33 11.71 44.28 10.63 5.62 6.34 3.20 4. 9.17 10.29 5.00 7.62 2.79 2.24 2.55 2.61 3.11 2.89 1.70 1.99 2.66 1.57 9.83 5.18 4.79 2.37 3.51 2.80 39.29 41.32 40.50 28.40 23.57 18. 38.99 29.08 24.04 15.58 20.43 12.73 36.54 15.27 11.55 12.22 7.26 9.18 14.97 11.64 10.67 5.49 6.43 3.23 2.51 6.79 2.86 2.68 1.03 2.98 2.40 1.60 8.18 7.20 4.62 2.51 3.55 3. 37.07 38.99 38.11 22.34 17.47 12.92 36.34 22.61 20.18 13.63 12.14 7.73 34.50 10.05 5.64 7.53 4.81 5.16 9.88 7.26 5.16 6.08 3.12 2.31 1. 5.69 2.39 2.20 1.25 1.68 4.09 1.17 8.62 6.44 3.27 2.48 3.12 2.81 29 38.59 39.88 39.88 23.25 16.92 12.38 37.71 22.99 20.29 13.92 10.33 6. 34.80 9.37 4.03 6.10 4.33 4.76 9.80 6.88 4.84 7.10 3.61 2.05 2.20 6.59 2.35 2.05 1.77 1.86 4.28 1.43 8.20 4.55 3.31 1.75 2.66 2.33 38. 40.10 40.11 25.14 21.39 14.15 33.51 25.57 21.01 10.02 10.32 6.20 34.92 11.36 5.17 4.35 5.75 5.66 10.17 5.89 3.60 6.93 4.15 3.94 2.38 4.50 1.19 2.52 2.08 1.86 2.43 1. 8.24 7.46 4.66 4.13 3.61 2.85 37.36 38.19 38.53 22.16 16.79 12.93 37.85 24.62 22.16 13.19 10.20 6.01 34.48 10.50 3.41 5.63 5.07 6.13 10.22 5.76 4. 7.76 2.64 2.25 1.57 7.01 1.88 1.77 1.12 1.72 5.89 1.02 7.82 5.97 2.66 1.55 2.90 2.25 Table 21. Results of different models on advanced critique evaluations MSE in the Code QAs Software Engineering (SE) subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Qwen2.5-Chat-7B-Instruct CodeLlama-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Qwen2.5-72B-Instruct Llama3.3-70B-Instruct Doubao-Coder-Preview GPT 4o DeepSeek-v3 Qwen2.5-Max GLM-4-Plus Claude3.5-Sonnet GPT 4o-mini QwQ-32B-Preview DeepSeek-R1 OpenAI o1-mini Gemini2.0-Flash-Thinking OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B Depth Logical Coherence Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance 42.40 42.78 42.00 32.07 24.80 20.79 41.53 33.92 27.23 16.14 13.87 10.97 36.58 11.51 11.19 7.93 5.92 4.54 10.93 9.57 5. 5.12 4.78 3.34 3.24 6.17 4.70 4.06 2.70 2.47 1.88 1.45 12.17 7.13 3.41 3.09 2.94 2.85 55.81 57.08 55.89 39.88 30.91 25.54 53.09 39.91 30.52 22.56 16.57 18. 50.06 11.28 16.52 17.26 6.44 4.44 11.85 18.18 17.93 5.42 6.84 2.70 2.17 5.30 5.81 4.69 2.34 2.12 1.46 1.90 12.55 8.56 3.95 2.21 4.30 3.10 36. 36.60 36.60 25.91 22.52 19.43 35.66 26.86 20.39 16.30 13.15 14.18 33.47 10.84 12.97 12.55 5.06 3.98 9.46 12.48 11.86 5.26 3.78 2.94 3.43 8.63 4.23 2.63 2.06 2.71 1.31 1. 8.77 4.30 3.15 2.60 1.87 2.36 39.60 40.32 39.51 27.22 22.00 18.70 38.07 28.96 22.94 14.58 13.54 12.00 36.23 11.36 10.43 8.62 5.04 5.45 9.39 9.93 7. 3.73 4.79 3.25 2.62 6.69 5.84 2.99 2.55 3.10 1.64 2.05 10.12 6.20 2.88 2.87 2.89 1.95 57.80 59.63 59.29 40.49 30.58 23.39 53.75 40.25 31.47 19.58 17.92 10. 50.42 10.49 14.82 7.38 5.84 3.35 10.22 10.81 4.68 6.08 6.41 2.53 2.54 3.10 6.33 4.10 2.15 2.68 2.16 1.80 13.28 6.30 3.10 1.67 2.80 2.54 41. 42.54 41.58 30.48 25.67 22.44 40.88 33.55 25.34 20.02 16.10 17.33 36.76 12.17 17.29 14.98 7.61 6.21 11.06 15.15 14.98 5.12 5.69 2.93 3.60 7.59 5.33 4.18 3.00 3.61 1.95 2. 10.97 6.90 5.06 3.54 3.54 3.09 42.49 43.69 41.00 31.29 23.78 19.52 41.22 32.24 25.88 11.97 14.19 9.42 37.31 11.07 11.44 7.32 7.93 6.67 9.05 6.05 3. 6.76 6.60 3.86 4.26 5.63 5.11 4.54 3.01 2.88 1.78 1.89 10.40 6.47 3.28 3.12 2.99 3.32 41.45 43.42 42.23 28.67 21.41 18.45 39.73 29.83 23.30 13.37 13.21 8. 37.49 10.39 9.57 6.52 5.79 5.81 8.50 7.11 2.66 4.34 5.53 3.36 3.19 6.54 5.28 2.83 2.53 3.50 1.71 1.89 9.24 5.44 3.52 2.37 3.70 2.21 39. 40.96 38.40 28.82 22.62 17.07 36.33 31.49 24.46 8.96 14.71 9.10 35.34 12.09 8.71 4.21 8.39 7.06 11.00 6.35 3.74 5.85 5.39 5.07 4.18 6.88 5.88 4.29 3.87 3.65 2.03 2. 10.65 10.61 4.98 3.09 6.63 3.48 39.56 40.63 39.62 27.88 22.27 18.10 38.95 29.97 23.85 12.74 17.13 9.68 35.01 10.14 9.74 6.42 6.57 7.03 9.02 6.26 2. 4.68 4.32 3.62 3.68 5.37 5.52 2.43 3.40 3.15 1.58 1.74 10.12 3.89 3.22 2.23 2.90 2.37 Table 22. Results of different models on advanced critique evaluations MSE in the Code QAs Data Analysis (DA) subset Dataset across all fine-grained evaluation dimensions."
        },
        {
            "title": "Model",
            "content": "0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model OpenCoder-1.5B-Instruct Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Qwen2.5-Chat-7B-Instruct CodeLlama-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct DeepSeek-v2-Lite-Chat DeepSeekCoder-v2-Lite-Instruct CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Qwen2.5-72B-Instruct DeepSeek-v2.5 Doubao-Coder-Preview DeepSeek-v3 GLM-4-Plus GPT 4o Qwen2.5-Max GPT 4o-mini Claude3.5-Sonnet DeepSeek-R1 QwQ-32B-Preview OpenAI o1-Preview Gemini2.0-Flash-Thinking DeepSeek-R1-Distill-Qwen-32B OpenAI o1-mini"
        },
        {
            "title": "Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance",
            "content": "53.91 56.32 55.52 29.48 22.73 17.51 50.71 39.19 21.13 17.55 16.75 15.04 49.43 12.52 12.33 8.84 4.13 3.59 12.36 9.48 8.01 3.44 2.88 2.56 2. 7.19 5.84 3.25 2.69 1.87 1.85 1.59 14.20 11.15 6.52 3.39 2.36 2.21 64.58 67.10 65.71 35.79 27.50 20.07 60.43 45.51 25.14 23.04 19.93 20.33 55.21 13.56 17.79 18.42 3.61 2. 17.83 16.75 13.88 1.76 2.46 2.31 1.88 3.25 6.96 2.49 3.22 1.56 1.69 1.14 15.74 11.61 3.78 1.93 1.39 2.17 39.54 41.17 40.44 24.35 19.45 16. 38.12 29.16 17.35 16.94 14.32 13.22 35.67 11.82 12.15 12.24 3.65 1.78 11.02 9.95 10.59 2.27 3.29 3.41 3.05 9.27 3.91 3.40 2.26 1.46 1.37 0.79 5.80 9.88 3.06 2.00 1.43 1. 46.18 45.45 46.89 29.00 20.73 15.71 43.60 36.30 20.72 11.47 16.47 12.79 42.67 13.81 8.62 5.66 6.62 4.85 11.98 6.15 5.97 3.03 3.29 4.49 3. 4.94 4.81 3.85 4.68 2.57 1.64 1.25 12.10 10.86 7.74 3.79 3.39 3.34 59.13 62.70 60.74 33.61 23.24 16.79 56.41 41.33 22.57 18.61 15.18 13.93 51.17 11.87 9.76 6.87 4.05 2. 13.32 7.20 5.59 2.10 2.01 3.22 2.63 4.17 6.29 2.96 3.71 2.09 1.66 1.68 9.54 11.21 3.93 1.23 1.48 1.56 44.81 46.76 45.97 26.33 23.12 18. 42.14 34.66 22.16 23.39 18.44 16.77 41.52 13.43 14.91 15.40 6.82 4.64 14.09 13.77 14.48 2.73 4.12 4.12 2.79 10.03 5.57 4.01 4.13 2.25 2.83 2.22 10.24 11.84 5.38 2.12 2.30 3. 48.70 50.86 50.03 27.24 20.50 16.93 46.03 36.29 20.45 15.64 14.58 13.86 42.86 10.92 12.33 11.98 4.70 2.30 12.49 9.79 10.10 2.50 2.82 3.65 3. 7.47 5.30 3.75 3.93 1.74 1.84 1.33 10.98 10.51 5.29 2.42 1.96 2.44 30 47.06 48.57 48.08 27.71 20.23 15.48 44.37 35.60 20.52 15.28 15.59 11. 42.32 12.80 9.05 7.19 4.64 2.93 11.71 6.56 5.47 2.77 2.53 3.81 3.19 6.83 4.26 2.90 4.14 1.25 1.28 1.65 6.73 10.29 3.02 1.14 2.08 2.15 44. 44.34 45.21 27.03 21.34 16.84 36.61 33.82 21.72 8.77 17.45 10.45 40.25 12.79 6.97 5.84 6.23 4.17 10.47 5.00 6.94 3.53 4.12 5.01 3.67 7.72 5.18 3.39 5.93 3.18 2.00 1. 9.85 9.72 7.31 2.77 3.07 3.61 44.24 45.50 45.27 27.78 20.80 16.01 42.88 34.79 21.52 13.94 16.42 12.34 39.31 12.18 9.09 4.45 5.45 2.86 11.98 5.03 4. 3.07 2.80 4.31 3.45 6.55 3.72 2.98 4.18 1.49 1.53 1.18 7.42 10.16 3.27 2.04 2.16 2.15 Table 23. Results of different models on advanced critique evaluations MSE in the Code QAs Mathematics (MA) subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat CodeLlama-7B-Instruct Yi-Coder-9B-Chat Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Chat-14B-Instruct DeepSeekCoder-v2-Lite-Instruct CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-v2.5 Llama3.3-70B-Instruct Qwen2.5-72B-Instruct DeepSeekCoder-v2-Instruct Doubao-Coder-Preview GLM-4-Plus GPT 4o-mini GPT 4o Claude3.5-Sonnet Qwen2.5-Max DeepSeek-v3 QwQ-32B-Preview DeepSeek-R1 OpenAI o1-Preview Gemini2.0-Flash-Thinking DeepSeek-R1-Distill-Qwen-32B OpenAI o1-mini Depth Logical Coherence Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance 52.32 55.99 53.12 34.94 20.08 17.96 46.06 31.43 22.18 16.77 13.75 10.16 41.77 15.09 9.87 5.14 5.38 3.27 6.96 5.03 6. 4.73 2.91 3.03 1.45 4.99 2.53 2.55 2.94 1.62 1.88 1.40 12.32 8.37 5.80 2.62 2.75 1.54 45.26 47.62 45.45 30.55 21.63 17.31 39.15 27.67 21.96 20.25 13.60 10. 40.90 19.77 8.36 7.03 6.88 6.42 10.76 7.18 8.26 5.05 2.93 3.17 2.93 7.16 2.74 3.67 2.35 2.34 1.64 1.29 11.60 9.70 6.12 3.07 3.32 2.09 38. 40.33 38.79 26.75 17.84 15.19 32.56 23.92 18.99 15.62 9.62 7.27 28.42 16.02 5.98 7.87 3.82 5.70 9.53 3.69 4.59 6.11 3.22 3.08 2.47 4.33 2.14 2.49 2.17 2.06 1.77 1. 10.56 9.36 6.11 3.21 3.80 2.58 42.16 45.31 43.04 27.89 18.89 17.55 37.18 26.98 18.04 16.89 11.18 8.42 32.49 16.93 7.11 6.31 4.43 3.46 7.74 3.82 6. 5.27 2.46 2.75 2.99 9.56 2.93 2.81 2.85 1.74 1.59 1.62 12.06 6.23 4.92 2.34 3.20 2.38 43.15 46.09 42.16 31.29 20.95 13.29 35.57 27.83 18.56 19.98 11.22 7. 35.15 17.31 6.45 6.56 2.85 6.10 10.72 4.01 4.94 5.70 2.55 3.14 2.82 4.52 2.25 2.33 2.56 1.69 1.70 1.34 10.81 10.92 6.85 4.56 4.89 3.30 46. 50.73 48.33 33.01 21.10 16.26 42.30 31.18 19.71 16.30 12.50 8.07 35.80 16.67 6.71 5.98 4.39 4.41 9.65 3.48 5.84 6.97 2.58 3.19 2.33 5.31 2.73 2.72 2.10 2.14 2.01 2. 12.53 5.50 4.36 2.90 3.48 2.18 39.44 41.41 39.39 28.37 18.35 15.08 34.28 24.65 18.87 16.42 12.47 8.25 29.92 15.03 6.93 6.11 5.01 4.17 8.29 5.02 4. 4.60 2.53 2.36 1.94 6.23 2.37 2.36 2.36 2.03 1.60 1.46 10.63 8.89 6.17 4.57 4.54 2.49 50.08 52.87 50.19 33.01 22.74 18.64 42.44 30.99 24.14 19.04 13.41 11. 40.04 19.68 8.24 6.48 6.92 4.33 9.59 7.07 8.19 5.49 1.98 2.75 2.24 5.04 1.96 2.90 2.88 1.56 1.26 1.41 12.57 9.68 5.82 2.64 2.77 2.04 31. 33.03 32.16 24.05 18.58 14.03 28.84 22.53 17.40 13.46 10.41 7.78 29.15 13.26 6.94 4.34 5.27 2.92 7.60 5.79 6.89 3.73 2.95 2.47 1.59 9.71 2.70 1.92 1.94 1.17 1.15 1. 10.36 5.92 4.15 2.96 2.44 1.44 38.55 41.97 39.58 27.39 19.60 15.36 35.28 24.86 18.78 17.48 11.82 8.79 30.35 15.26 6.94 5.53 6.44 4.79 8.88 5.34 6. 5.12 2.41 2.38 2.64 7.64 2.91 2.68 2.54 1.68 1.49 1.10 9.65 8.30 5.87 3.28 3.28 2.27 31 Table 24. Results of different models on advanced critique evaluations MSE in the Code QAs Desktop and Web Development (DW) subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Qwen2.5-72B-Instruct Doubao-Coder-Preview GLM-4-Plus GPT 4o-mini GPT 4o Qwen2.5-Max DeepSeek-v3 Claude3.5-Sonnet QwQ-32B-Preview DeepSeek-R1 OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B Gemini2.0-Flash-Thinking OpenAI o1-mini Depth Logical Coherence Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance 56.98 61.72 59.68 37.93 24.28 16.75 57.47 36.38 23.69 19.73 14.15 5.99 47.77 10.62 6.37 5.07 4.11 4. 11.30 6.56 6.01 6.74 3.77 2.94 1.67 3.73 2.39 2.31 1.91 1.12 1.22 1.85 9.10 4.07 3.80 1.59 1.83 1.12 39.75 43.45 43.35 26.98 21.08 15. 40.85 27.36 19.57 19.78 11.89 9.18 37.39 10.25 9.16 8.58 6.11 6.38 10.19 8.17 7.55 4.77 3.51 3.81 2.59 8.27 3.23 2.67 2.34 1.79 1.11 1.88 7.00 6.14 3.71 1.82 2.07 2. 38.04 41.81 39.63 25.75 18.83 15.61 38.37 27.23 18.89 16.80 8.68 5.74 31.17 9.43 6.67 5.35 5.84 5.99 10.49 5.11 4.21 4.58 4.58 4.05 2. 4.21 3.12 1.33 2.50 1.86 1.26 1.58 7.76 6.85 4.51 2.48 4.02 1.56 40.97 44.59 43.46 27.05 19.33 15.51 41.82 27.30 18.46 15.43 11.17 4.28 34.52 11.06 6.27 4.95 4.77 5. 10.25 5.13 4.07 4.75 3.66 3.37 2.80 6.59 3.25 1.93 3.51 1.40 1.15 1.92 7.00 5.14 3.85 2.64 2.01 2.38 38.68 41.62 39.89 25.01 18.62 14. 36.62 27.61 20.49 16.31 7.88 5.70 39.05 11.90 5.06 5.13 8.17 7.22 8.83 5.26 3.54 4.08 5.20 4.03 3.85 4.42 3.43 1.62 3.47 2.87 1.35 1.64 7.52 7.01 4.11 2.68 3.27 2. 45.10 48.50 47.42 31.89 20.70 15.58 45.83 31.05 20.36 17.71 9.94 5.17 38.86 11.78 6.26 4.21 4.56 4.48 10.52 5.25 3.85 5.49 4.88 3.31 2. 5.84 2.87 1.85 2.65 1.44 0.95 1.36 8.18 4.29 3.62 1.72 1.77 1.58 42.60 47.11 46.00 31.50 20.89 15.06 44.90 29.37 22.31 17.47 12.33 5.10 36.76 10.27 7.90 6.80 4.99 4. 10.63 6.01 5.24 4.60 4.21 3.94 2.39 5.42 3.19 2.07 2.02 1.84 1.62 1.49 8.98 5.51 5.18 2.94 2.76 1.96 56.97 62.39 60.24 37.38 25.86 18. 58.01 38.00 25.42 22.49 16.99 9.22 49.44 9.56 11.41 12.38 4.42 5.20 12.82 10.05 9.23 6.95 3.54 3.09 1.89 4.53 2.57 1.90 2.13 1.49 1.52 1.63 9.29 6.81 3.35 1.91 2.14 0. 36.01 38.85 38.01 27.30 18.81 14.58 37.53 27.00 19.54 20.45 11.37 7.56 32.99 9.42 9.44 8.43 4.42 4.12 9.34 8.53 6.52 4.22 3.37 3.91 1. 7.65 3.27 1.67 1.57 1.76 1.26 0.92 8.27 4.63 3.55 2.08 1.90 1.94 40.85 44.77 43.41 27.93 19.54 14.04 42.20 28.50 19.53 16.71 10.96 5.12 35.85 11.54 7.98 7.80 4.48 5. 10.31 6.77 6.94 4.76 3.84 4.10 2.50 5.02 3.27 2.31 2.49 1.61 1.47 1.50 7.81 6.19 4.08 2.56 2.74 1.73 32 Table 25. Results of different models on advanced critique evaluations MSE in the Code QAs Machine Learning (ML) subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Coder-32B-Instruct CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct DeepSeek-v2.5 Qwen2.5-72B-Instruct Doubao-Coder-Preview DeepSeek-v3 GPT 4o GLM-4-Plus GPT 4o-mini Claude3.5-Sonnet Qwen2.5-Max QwQ-32B-Preview DeepSeek-R1 Gemini2.0-Flash-Thinking OpenAI o1-mini OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B Depth Logical Coherence Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance 29.82 31.58 30.77 25.91 21.61 18.82 29.08 23.87 19.83 13.85 11.68 9. 26.24 11.28 8.31 7.52 6.07 4.56 8.13 7.42 6.49 3.84 2.88 2.52 2.48 7.98 3.32 2.93 2.85 2.24 2.04 1.71 5.34 4.37 4.13 3.24 3.01 2.69 43. 46.40 45.54 35.68 29.59 25.57 41.59 31.52 28.28 17.48 17.61 17.97 38.13 12.08 17.80 18.32 6.52 4.44 17.51 10.18 14.92 5.44 2.74 3.10 2.38 6.77 4.84 3.19 2.66 2.87 2.19 1. 6.50 6.78 2.77 4.46 3.41 3.08 26.61 28.07 27.71 24.42 22.04 18.97 26.87 21.82 21.89 15.26 15.33 14.63 24.56 11.31 12.81 13.33 4.48 3.90 13.70 9.39 11. 3.02 3.77 1.79 2.01 7.32 2.92 1.82 2.74 1.72 1.18 1.34 6.22 4.42 3.07 2.13 2.12 2.68 33.38 35.97 35.61 29.28 23.89 20.60 33.34 24.87 21.98 15.14 13.10 13. 30.88 11.85 10.91 10.14 6.57 5.22 10.68 8.66 8.98 4.84 4.31 2.91 2.28 7.69 4.69 2.71 3.09 2.84 1.99 1.51 5.90 4.46 3.08 3.23 2.40 2.53 49. 51.14 50.15 42.38 30.93 28.27 45.99 37.19 33.52 15.37 15.99 13.27 41.17 13.84 11.95 7.15 5.62 2.72 8.51 9.25 5.49 4.80 2.25 2.33 1.38 4.16 4.53 1.65 2.42 2.75 2.21 1. 5.69 5.04 3.60 2.60 2.72 2.44 25.95 28.35 27.65 23.51 19.78 17.56 26.01 21.80 24.67 16.52 14.62 14.01 24.89 12.16 13.30 13.03 5.73 4.38 13.20 10.09 12. 4.18 4.43 2.66 2.21 8.23 3.43 2.46 2.68 2.33 1.95 1.47 5.92 4.19 7.16 3.89 3.69 2.66 33.75 36.87 35.35 28.56 22.75 21.74 33.70 26.22 23.50 15.28 13.12 10. 32.70 13.27 9.89 7.25 7.26 5.45 9.10 8.43 5.92 5.27 2.89 3.79 2.68 5.80 4.63 2.62 3.35 2.46 2.21 1.99 6.45 6.15 4.16 3.38 2.51 3.81 39. 43.52 42.62 31.58 26.89 23.98 39.13 30.98 28.09 14.27 14.74 10.99 36.40 12.44 10.99 7.61 8.17 5.19 9.44 7.40 6.40 4.33 3.20 2.43 2.04 7.67 3.58 2.20 3.52 2.83 2.12 1. 5.83 4.40 2.03 3.63 3.08 2.26 29.71 31.58 30.90 25.36 18.88 16.83 28.16 23.26 22.05 9.26 8.48 9.14 27.10 11.80 6.24 6.95 6.05 4.86 6.89 7.20 6. 5.22 3.49 3.70 2.70 6.81 3.88 1.89 4.33 3.06 1.40 2.19 6.76 5.86 7.30 3.95 4.76 3.50 29.50 31.65 31.34 24.71 19.86 18.25 29.15 24.36 19.35 11.45 10.33 7. 28.28 12.40 8.20 6.85 3.45 3.22 6.23 6.68 5.25 3.49 2.28 1.65 1.68 5.75 3.54 1.74 2.39 1.40 1.43 1.09 4.97 3.80 1.23 1.99 1.41 2.01 Table 26. Results of different models on advanced critique evaluations MSE in the Code QAs Scientific Computing (SC) subset Dataset across all fine-grained evaluation dimensions."
        },
        {
            "title": "Model",
            "content": "0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like OpenCoder-1.5B-Instruct Yi-Coder-1.5B-Chat DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-1.5B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat CodeLlama-7B-Instruct Yi-Coder-9B-Chat Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct CodeLlama-34B-Instruct Qwen2.5-72B-Instruct DeepSeek-v2.5 DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Doubao-Coder-Preview GPT 4o GLM-4-Plus DeepSeek-v3 GPT 4o-mini Qwen2.5-Max Claude3.5-Sonnet QwQ-32B-Preview DeepSeek-R1 OpenAI o1-Preview OpenAI o1-mini Gemini2.0-Flash-Thinking DeepSeek-R1-Distill-Qwen-32B"
        },
        {
            "title": "Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance",
            "content": "40.92 46.36 46.15 39.93 30.53 29.26 42.00 37.02 32.31 30.43 19.34 13.03 36.51 14.79 9.81 9.04 4.26 4.21 11.02 10.54 9.97 4.03 3.64 3.40 3. 7.48 4.32 3.27 2.10 2.03 1.78 1.29 12.58 8.61 4.48 3.77 3.10 3.05 50.51 57.60 57.60 45.04 35.70 33.70 51.16 47.17 24.30 34.09 25.70 20.71 43.41 17.06 16.88 18.09 4.58 4. 17.85 16.68 11.21 3.36 4.00 3.41 2.58 4.71 4.01 2.55 3.31 2.38 1.90 1.76 15.10 9.65 4.83 3.84 2.27 2.48 30.96 33.92 33.31 29.06 21.45 22. 30.91 26.96 17.67 20.31 17.93 13.63 27.42 13.72 11.29 10.78 4.04 3.97 11.56 11.18 8.86 4.06 3.35 2.77 3.82 7.43 2.73 3.33 2.36 1.55 1.31 1.16 9.30 5.07 2.64 2.59 2.33 2. 37.76 41.68 41.88 35.46 25.32 22.84 35.17 34.24 17.15 25.52 15.23 10.85 30.24 14.63 7.66 6.64 7.86 6.44 7.31 6.89 11.56 4.57 4.30 4.34 3. 4.71 3.88 2.87 3.18 1.43 1.95 2.14 13.23 8.97 5.67 5.01 4.70 4.29 49.62 55.02 55.67 40.89 31.06 29.82 49.80 43.38 21.60 31.09 22.49 13.17 41.25 15.03 8.98 8.98 3.43 4. 7.77 8.01 8.95 3.55 4.66 4.35 2.22 5.12 3.83 2.67 3.44 1.85 1.56 1.73 12.87 7.11 3.39 3.62 2.10 2.14 36.30 41.98 41.38 35.29 27.60 25. 37.75 35.30 22.33 26.73 22.02 15.05 32.81 15.33 13.31 13.85 6.13 4.82 14.38 14.28 12.52 5.04 2.96 4.65 4.87 7.41 4.28 3.10 2.75 2.68 1.89 1.86 12.30 8.05 4.94 4.15 3.81 2. 37.29 42.06 42.06 32.73 23.44 24.02 37.28 32.58 17.86 23.78 16.35 12.71 30.69 13.05 10.39 9.51 5.08 4.11 9.94 9.47 9.30 3.81 3.19 3.77 3. 7.74 3.43 2.86 2.40 2.26 1.16 1.57 11.81 7.39 4.24 3.78 2.54 2.37 33 40.67 46.33 45.75 34.46 24.74 23.99 40.96 35.13 15.69 25.95 17.63 10. 34.38 13.99 6.92 7.30 5.51 4.72 6.22 6.19 9.30 4.88 4.94 3.99 3.12 7.87 4.33 2.99 2.88 1.65 1.04 1.93 12.45 6.25 3.81 3.92 2.11 2.43 38. 41.49 42.17 36.02 20.67 24.63 36.43 32.26 16.00 28.13 10.03 8.70 32.45 15.78 6.21 3.68 10.05 5.10 8.35 5.34 12.76 4.25 4.57 4.85 3.09 4.48 3.66 2.71 2.48 1.69 1.48 1. 9.53 9.63 6.27 4.57 3.31 4.77 37.56 42.02 41.66 32.66 21.75 22.91 37.89 31.55 14.46 24.17 14.96 9.89 30.17 13.00 6.62 6.06 4.86 4.49 7.33 6.32 9. 4.49 4.82 3.54 3.07 6.12 3.65 2.54 2.51 1.26 1.45 1.78 12.30 4.85 3.36 3.37 2.05 2.23 Table 27. Results of different models on advanced critique evaluations MSE in the Code QAs Databases (DB) subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Chat-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-v2.5 Llama3.3-70B-Instruct DeepSeekCoder-v2-Instruct Qwen2.5-72B-Instruct Doubao-Coder-Preview DeepSeek-v3 GLM-4-Plus GPT 4o-mini Claude3.5-Sonnet GPT 4o Qwen2.5-Max QwQ-32B-Preview DeepSeek-R1 OpenAI o1-Preview Gemini2.0-Flash-Thinking DeepSeek-R1-Distill-Qwen-32B OpenAI o1-mini Depth Logical Coherence Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance 38.65 40.85 40.15 27.13 21.59 16.91 36.05 28.70 19.27 15.20 12.67 7.53 33.90 9.28 8.46 7.61 6.59 5.12 10.13 4.43 3. 4.63 3.91 3.11 2.96 6.39 4.70 2.91 1.87 1.68 1.60 1.52 7.15 5.19 3.19 3.07 3.06 2.80 50.80 56.44 55.56 34.74 27.05 20.18 50.97 35.98 22.12 21.60 25.12 11. 47.22 11.06 11.87 7.85 5.02 9.19 10.91 8.23 7.81 3.40 3.23 3.00 2.02 5.30 6.49 2.13 1.93 1.68 1.68 1.49 11.46 5.90 3.79 2.36 2.81 1.90 33. 36.37 35.60 25.18 19.98 15.56 33.33 25.07 18.07 15.14 15.99 8.44 30.69 9.82 8.88 6.69 4.27 7.08 7.55 6.92 6.22 3.38 3.66 3.19 3.13 7.76 3.29 3.00 1.57 1.15 1.13 1. 9.29 3.18 2.39 2.25 2.53 1.66 38.38 42.39 41.68 24.42 20.00 16.61 37.15 28.15 17.60 15.87 16.31 7.27 35.93 9.86 8.61 5.84 4.53 6.09 8.66 4.79 6. 3.97 3.13 3.30 2.52 7.39 5.93 3.47 2.57 1.41 1.54 1.13 7.64 4.48 3.56 2.09 2.26 2.22 45.36 50.84 50.43 31.10 22.60 19.44 45.27 32.11 21.81 14.87 16.03 7. 42.27 10.72 9.40 7.18 4.93 5.18 11.04 3.31 3.07 3.54 3.21 3.80 2.19 4.70 6.40 2.79 2.37 1.82 1.53 1.84 8.83 4.28 3.22 1.88 3.30 1.77 38. 41.84 41.08 26.93 22.17 17.75 37.50 29.90 21.36 18.32 19.08 10.43 36.29 10.61 11.99 7.38 6.56 9.75 9.86 9.09 8.44 4.52 4.21 3.66 3.09 9.24 5.61 3.72 3.13 1.97 1.72 1. 10.34 5.45 3.72 2.61 2.83 3.27 38.33 42.52 38.78 27.42 20.72 17.58 37.29 28.70 19.95 15.60 10.68 7.06 34.54 10.71 7.07 7.11 6.49 5.03 10.50 3.94 2. 4.06 2.72 4.38 3.09 4.04 6.08 3.63 1.79 1.33 1.45 1.82 7.36 5.67 3.80 2.85 2.92 2.32 39.77 43.71 43.57 27.53 21.00 18.19 39.31 29.25 20.41 14.97 12.45 7. 35.68 8.97 7.54 6.53 5.00 5.09 9.51 3.56 3.02 5.38 3.42 3.54 2.39 5.50 5.10 3.05 1.75 1.74 1.38 1.51 8.23 3.68 3.08 1.65 2.44 2.57 40. 46.36 44.09 29.42 23.43 17.79 38.22 31.34 20.62 16.16 10.06 7.28 35.46 9.52 6.96 5.21 5.14 4.18 10.37 3.96 2.94 5.71 2.29 3.37 2.90 4.06 6.56 3.04 1.42 1.54 1.80 1. 7.57 6.74 4.01 2.35 3.15 2.88 40.81 44.75 43.83 28.47 22.81 18.34 39.70 30.42 19.70 15.85 12.53 6.31 37.44 10.21 7.02 5.95 4.74 4.37 8.38 3.96 2. 5.14 2.82 3.63 2.66 5.43 4.94 2.35 1.80 1.07 1.17 0.99 8.72 4.19 2.07 1.62 2.17 2.12 Table 28. Results of different models on advanced critique evaluations MSE in the Code QAs Multimedia (MM) subset Dataset across all fine-grained evaluation dimensions."
        },
        {
            "title": "Model",
            "content": "0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct DeepSeekCoder-v2-Lite-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Coder-14B-Instruct Qwen2.5-Chat-14B-Instruct CodeLlama-34B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Chat-32B-Instruct DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct DeepSeek-v2.5 Qwen2.5-72B-Instruct Doubao-Coder-Preview GPT 4o GLM-4-Plus DeepSeek-v3 Qwen2.5-Max GPT 4o-mini Claude3.5-Sonnet QwQ-32B-Preview DeepSeek-R1 OpenAI o1-Preview OpenAI o1-mini Gemini2.0-Flash-Thinking DeepSeek-R1-Distill-Qwen-32B"
        },
        {
            "title": "Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance",
            "content": "37.28 39.84 39.39 26.69 18.84 15.93 35.80 23.02 18.47 17.34 16.08 5.10 30.42 14.18 6.22 4.97 3.76 3.33 11.22 5.36 3.67 3.30 3.23 3.10 2. 6.26 3.41 1.91 1.89 1.40 1.28 1.25 12.22 6.40 4.24 3.08 2.57 2.47 55.85 58.96 58.91 35.51 22.73 17.98 51.98 33.77 24.07 22.91 24.86 12.00 43.21 14.43 6.85 2.55 9.68 8. 11.94 10.97 7.71 3.14 2.79 3.79 2.31 3.38 3.94 2.19 3.94 1.88 1.60 1.61 13.09 7.35 2.05 2.86 1.48 2.44 30.54 32.30 31.84 21.31 16.66 13. 28.56 20.79 15.97 16.37 13.77 7.31 25.15 10.80 4.28 2.79 6.69 5.57 9.66 7.05 4.93 2.69 4.78 2.72 2.93 8.05 2.86 2.38 2.22 1.17 1.46 0.74 9.34 4.28 3.40 2.14 1.59 2. 39.91 43.07 42.63 25.93 17.56 14.80 35.63 22.33 17.82 15.55 9.43 6.91 32.45 12.36 7.57 5.23 4.94 4.47 11.56 4.94 3.37 3.27 3.09 3.30 2. 3.94 3.31 2.29 3.99 1.53 1.82 1.55 9.13 6.36 2.92 3.57 3.33 3.33 53.12 55.74 56.57 32.08 22.08 15.66 50.21 30.84 22.44 20.30 18.80 7.18 41.75 13.10 7.19 2.92 5.08 3. 10.81 6.16 3.40 2.58 3.62 3.14 3.19 3.15 4.08 2.19 3.32 1.35 1.54 2.07 12.18 6.21 2.85 2.65 1.79 2.41 37.42 39.69 39.29 23.82 16.13 15. 34.92 21.63 17.69 18.20 16.82 9.83 32.33 11.49 8.18 4.99 8.43 7.45 10.09 8.03 6.11 3.08 4.10 2.92 3.32 7.95 3.37 2.74 2.82 1.59 3.11 1.63 12.14 6.57 3.21 3.10 2.75 2. 36.18 37.82 37.82 22.86 15.18 12.74 33.64 22.77 16.22 16.57 13.77 7.89 29.29 11.44 6.29 2.94 7.29 5.84 10.03 7.17 4.80 2.92 3.78 2.75 3. 5.64 2.91 2.37 2.47 1.48 1.81 1.09 10.01 5.48 2.44 2.84 2.75 2.84 34 42.45 44.62 44.98 27.99 19.59 13.81 40.08 24.90 18.01 17.43 13.49 6. 34.08 13.05 6.11 3.24 3.84 3.89 9.60 4.56 2.63 2.96 3.31 2.84 3.03 8.04 3.80 2.43 3.28 1.29 1.79 2.04 11.29 5.09 4.23 3.74 3.34 3.70 36. 37.86 35.65 24.07 18.06 13.71 28.35 21.01 18.77 14.77 9.37 6.77 29.31 11.98 8.21 5.76 4.27 3.67 11.07 3.95 3.87 2.89 3.00 2.79 3.82 4.81 3.24 2.14 3.37 1.41 1.58 1. 8.33 5.19 3.63 2.55 2.47 2.81 38.01 40.32 40.09 26.92 19.32 12.72 35.70 22.86 17.04 15.04 11.30 6.62 30.39 12.08 6.78 3.32 4.14 3.13 9.15 4.36 3. 3.17 3.39 2.61 3.44 6.80 2.65 2.15 2.69 1.62 1.81 1.58 9.62 3.57 2.39 3.28 2.01 2.41 Table 29. Results of different models on advanced critique evaluations MSE in the Code QAs Operating Systems (OS) subset Dataset across all fine-grained evaluation dimensions. Size Model 0.5B+ Qwen2.5-Coder-0.5B-Instruct 1B+ 6B+ 13B+ 20B+ 70B+ Close-Sourced o1-like Model Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct CodeQwen1.5-7B-Chat CodeLlama-7B-Instruct Qwen2.5-Chat-7B-Instruct Yi-Coder-9B-Chat Qwen2.5-Coder-7B-Instruct StarCoder2-15B-Instruct CodeLlama-13B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeek-v2-Lite-Chat Qwen2.5-Chat-14B-Instruct DeepSeekCoder-v2-Lite-Instruct CodeLlama-34B-Instruct Qwen2.5-Chat-32B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-v2.5 Qwen2.5-72B-Instruct DeepSeekCoder-v2-Instruct Llama3.3-70B-Instruct Doubao-Coder-Preview GLM-4-Plus GPT 4o DeepSeek-v3 Claude3.5-Sonnet GPT 4o-mini Qwen2.5-Max DeepSeek-R1 QwQ-32B-Preview OpenAI o1-mini OpenAI o1-Preview DeepSeek-R1-Distill-Qwen-32B Gemini2.0-Flash-Thinking Depth Logical Coherence Innovation Practicality Clarity Reliability Completeness Maintainability Correctness Performance 48.52 55.47 53.62 30.87 21.40 13.43 48.56 30.73 17.53 19.28 17.75 10.77 39.59 10.31 7.70 2.28 6.34 5.95 11.14 6.87 4. 5.94 3.11 3.30 3.24 4.38 3.28 3.49 1.74 2.36 2.41 1.61 4.96 10.58 5.47 3.20 3.11 2.40 38.73 43.11 42.63 24.01 18.32 14.15 38.94 27.94 18.50 18.26 19.07 11. 33.08 13.87 12.53 6.60 10.24 6.65 12.80 9.89 8.57 6.21 3.88 4.72 4.12 7.45 4.55 2.09 1.94 2.13 2.68 2.11 4.44 10.00 4.92 3.86 3.61 3.48 39. 44.83 43.07 25.23 18.09 13.40 39.70 27.82 16.49 16.49 17.22 9.75 33.06 11.85 8.93 6.15 6.93 7.32 11.42 7.91 4.89 6.49 4.55 5.33 3.76 4.72 4.12 3.79 2.94 2.49 1.88 2. 6.80 9.64 6.85 4.15 4.76 4.58 39.46 43.20 41.66 24.17 17.31 12.40 39.08 26.39 15.44 14.86 16.28 9.46 32.25 11.09 7.39 2.74 6.44 5.60 10.53 6.02 4. 5.72 3.63 3.74 3.44 6.47 4.28 2.66 1.95 1.67 1.73 1.62 2.51 8.27 4.76 3.37 3.02 2.24 38.16 42.22 40.35 21.66 16.62 9.88 33.09 22.00 14.46 12.37 16.31 8. 31.41 10.75 7.30 4.81 6.34 7.31 10.26 4.28 4.06 5.98 4.21 5.41 3.27 6.18 4.22 2.93 2.80 2.23 1.77 2.03 5.46 10.06 6.08 5.06 4.09 3.61 39. 43.62 42.08 24.68 18.07 11.92 39.31 27.94 17.34 13.29 13.83 8.36 32.10 10.06 7.61 2.90 5.86 4.68 9.76 5.79 4.14 5.01 2.80 3.84 2.57 5.32 3.72 2.16 2.42 1.72 1.38 1. 3.41 8.18 4.88 2.81 3.61 2.52 37.23 41.51 40.25 23.79 15.99 11.24 38.89 26.00 17.26 15.88 15.62 7.26 31.39 11.71 7.39 5.24 4.89 4.05 9.52 5.88 4. 5.62 3.48 3.33 3.28 5.52 3.32 2.25 2.16 1.67 1.66 1.59 5.19 8.83 5.29 4.61 3.77 3.67 52.04 59.50 57.55 33.21 22.99 14.93 52.56 33.41 21.52 21.87 17.23 11. 40.87 11.04 11.90 2.87 10.21 4.77 11.46 11.80 9.67 6.06 2.38 2.65 2.07 4.33 3.27 3.01 1.99 1.80 1.89 1.70 8.54 10.97 4.51 2.75 3.57 2.66 32. 36.44 35.67 21.92 16.69 11.58 33.80 22.72 17.58 15.90 11.80 9.17 28.16 9.69 8.66 3.56 7.40 2.88 9.12 8.22 7.79 4.70 2.94 3.01 3.49 7.97 3.19 1.75 1.88 1.16 1.92 1. 5.10 7.63 4.40 2.83 2.74 2.79 36.58 41.02 39.37 22.70 17.20 12.06 36.67 25.06 16.23 14.87 15.22 7.73 30.52 10.53 7.55 4.23 7.02 5.88 10.10 6.62 6. 5.26 4.35 4.50 2.74 6.03 4.05 2.95 1.86 1.19 1.98 1.47 6.00 8.23 4.46 3.43 2.76 3."
        },
        {
            "title": "Basic Critique Evaluation Judge Prompt",
            "content": "You are senior programming expert. We request professional and precise evaluation of the AI assistants performance based on the users question. Evaluation Process: 1. Carefully review the users question and the assistants answer. 2. Verify whether the answer (may include code) fully meet the users requirements. 3. Pay close attention to specific aspects of the answer, including: . . . - Appropriateness of the answers perspective. - Accuracy of technical details. - Completeness of the solution. Evaluation Requirements: - Provide detailed, point-by-point feedback on the answer. - Each critique should be specific and self-contained. - Clearly identify any issues, avoiding vague or ambiguous descriptions. - Offer constructive suggestions for improvement. Evaluation Criteria: - \"Error\": The code contains incorrect conclusions, explanations, or fails to meet the specified requirements. - \"Correct\": The code is entirely accurate and meets all outlined requirements. Output Format: Provide the evaluation in JSON format as follows: json { . . } \"reasons\": \"Detailed explanation evaluating the response, addressing specific points\", \"is_assistant_correct\": \"Based on the evaluation, indicate whether the assistants response is [Correct, Error]\" Input Data: Start of Question $Question End of Question Start of Answer $Answer End of Answer Figure 18. Basic critique evaluation judge prompt."
        },
        {
            "title": "Advanced Critique Evaluation Judge Prompt",
            "content": "You are professional code evaluation expert with the following qualifications: - Expertise in programming languages and algorithms. - Skilled in accurately assessing answer (may include code) quality. - Proficient in defining and applying precise, professional scoring criteria. Evaluation Process: 1. Analyze and understand the problem statement and provided answer thoroughly. 2. Evaluate systematically against defined criteria. 3. Assign professional scores for each evaluation problem. 4. Aggregate individual scores to determine final, comprehensive score. - Each evaluation metric is scored on an integer scale from 1-10. - The comprehensive score is also on 1-10 scale. Output Specifications: - Scoring Range: . . - Justification: All scores must be supported by clear, specific reasoning. - Structure: The final output must be well-organized, concise and professional. - Objectivity: Scoring must be neutral and unbiased. Scoring Guidelines: - 1-2 points: Critical flaws; fails to meet requirements. - 3-4 points: Significant deficiencies; largely unusable. - 5-6 points: Functional but requires substantial improvement. - 7-8 points: Well-implemented with minor issues. - 9-10 points: High-quality, near-perfect implementation. Output Format: md 1. <Reason>, Score: xx 2. <Reason>, Score: xx ... Comprehensive evaluation: <Reason>, Comprehensive Score: xx Important Notes: - The final comprehensive score should reflect weighted assessment of all sub-scores. Input Data: Start of Question $Question End of Question Start of Answer $Answer End of Answer Start of Fine-Grained Evaluation Checklists $Checklists End of Fine-Grained Evaluation Checklists Figure 19. Advanced critique evaluation judge prompt."
        },
        {
            "title": "Full Error Typelists",
            "content": "1. Syntax Error: [Spelling mistakes (e.g., incorrect variable names), Missing semicolons or mismatched brackets, Incorrect use of keywords, Indentation errors (for indentation-sensitive languages), Incorrect comment formatting] 2. Reference Error: [Undefined variables, Undefined functions, Null pointer references, Array out of bounds, Dangling pointers] 3. Type Error: [Type conversion errors, Undefined type errors, Type checking errors] 4. Logic Error: [Incorrect conditional statements, Incorrect loop conditions, Algorithm errors, Variable scope errors, Variable name conflicts, Boundary condition errors] 5. Performance Issue: [Memory leaks, Performance bottlenecks, Memory fragmentation, High time complexity, Resource contention] 6. Design Flaw: [Resource leaks, Lack of memory management, Absence of modular design] 7. Security Vulnerability: [Unsafe string operations, Failure to prevent SQL injection, Use of insecure random number generators, Failure to handle external service unavailability, Privilege escalation vulnerabilities] 8. Configuration Management Error: [Not using version control systems, Failure to back up code, Absence of continuous integration and continuous delivery for code, Configuration file errors, Improper branch management, Lack of tag management, No tracking of historical versions] 9. Data Management Error: [Database errors, Data format errors, Data consistency errors, Data integrity errors] 10. Concurrency and Multithreading Error: [Race conditions, Deadlocks, Thread safety issues] 11. Input Validation and Data Processing Error: [Insufficient input validation, Failure to handle boundary conditions, Failure to handle user input formats, Failure to filter user input, Failure to validate user input] 12. Exception Handling Error: [Improper exception handling, Failure to handle exceptions, Failure to use appropriate exception types, Failure to use appropriate error codes or exception messages] 13. Internationalization and Localization Error: [Failure to consider internationalization and localization, Failure to handle timezone issues, Failure to handle string operations in different locales, Currency format errors, Date format errors] 14. Monitoring and Logging Management Error: [Insufficient logging, Failure to use logging frameworks, Lack of performance monitoring for code, Failure to use appropriate log levels, Failure to use appropriate log rotation mechanisms] 15. Code Quality and Maintenance Error: [Comment errors, Failure to follow coding standards, Failure to conduct code reviews, Failure to refactor code, Failure to perform static code analysis] 16. User Permission and Authentication Error: [Failure to handle user permission control, Failure to handle user permission management, Failure to handle user authentication and authorization, Insufficient identity verification, Permission validation errors] 17. Testing and Verification Error: [Failure to use unit tests, Insufficient modular testing of code, Lack of automated testing for code, Insufficient integration testing, Insufficient regression testing] 18. Network and Communication Error: [Failure to handle network request exceptions, Failure to handle user request timeouts, Network protocol errors, Data transmission errors, Connection timeouts] 19. File and I/O Error: [File permission errors, File format errors, File path errors, File locking issues, Failure to handle file operation exceptions] 20. Dependency Management Error: [Dependency management issues] 21. Session Management Error: [Failure to handle user session management] 22. Log Security Issue: [Log security issues] 23. Environment Variable Error: [Environment variable errors] Figure 20. Full Error typelists."
        },
        {
            "title": "Insert Bug Prompt",
            "content": "You are an experienced developer tasked with purposefully injecting errors into codes based on specified error types. Task Objectives: - Analyze the original codes structure and functionality. - Modify the code according to the provided error type. - Ensure that the injected error adheres to the specified category. - Preserve the overall functional integrity of the code. - Provide clear instructions for the error injection process. Error Injection Requirements: 1. Code Modification Principles: - Clarity and precision: Modifications must be straightforward and targeted. - Category compliance: The injected error must belong to the specified error category. - Avoid unsanctioned errors: Only introduce errors of the specified type. - Preserve code integrity: Ensure that the other parts of the code remain unaffected. 2. Error Type Selection: - Select from the specified error categories: Choose the error type from the predefined set of categories. - Subcategory compliance: Ensure that the subcategory chosen aligns with the main error category. - Full coverage: All specified error types must be incorporated. - Reasonable error placement: The error injection should be placed logically within the code, reflecting real-world scenarios. 3. Modification Description Requirements: - Specify modification locations: Clearly define where each change takes place. - Detail the modification process: Provide thorough explanation of how the modification is carried out. - Explain the error type: Clarify which error type corresponds to each modification. - Repair difficulty evaluation: Assess the ease or difficulty of repairing the error. Output Format: json { Changed_code: <Complete modified code>, Selected error type category: [<Error category 1>, ...], Selected error subcategory: [<Error subcategory for category 1>, ...], Number of error types included: <Integer>, Specific modified location and processing: [{ . . . . Difficulty of repair: <Easy Medium Difficult> } Location: <Description of the code location>, Corresponding error type: <Error type for this modification>, Modification description: <Detailed explanation of the modification> }, ...], Error Modification Evaluation Criteria: 1. Difficulty Level Definitions: - Easy: The error is clear and the repair method is straightforward. - Medium: The error requires some analysis to identify and repair. - Difficult: The error demands deep understanding of the code structure and the repair is complex. 2. Modification Quality Requirements: - Natural error injection: Ensure the error injection appears seamless and not forced. - Readability: Maintain the codes readability, ensuring the injected errors dont obscure the logic. - Error type accuracy: Ensure each error type is accurately identified and implemented. Notes: - Provide complete solution with all fields populated. - Ensure the instructions are clear and specific, with an emphasis on easy understanding. - The repair difficulty should be assessed objectively and realistically. - Maintain accuracy and completeness regarding the error types. Input Data: Start of Code $Code End of Code Start of Error Typelists $Error_Typelist End of Error Typelists Figure 21. Insert bug prompt."
        },
        {
            "title": "Identifying Programming Error Types Prompt",
            "content": "You are an experienced programming expert responsible for professionally categorizing code error types. Your task is to analyze the code and determine which predefined error category it belongs to. Available Error Categories: 1. Configuration Management Errorn 2. Data Management Errorn 3. Input Validation and Data Processing Errorn 4. Monitoring and Logging Management Errorn 5. Environment Variable Errorn 6. Dependency Management Errorn 7. Syntax Errorn 8. Design Flawn 9. Security Vulnerabilityn 10. Log Security Issuen 11. Reference Errorn 12. Session Management Errorn 13. Code Quality and Maintenance Errorn 14. Logic Errorn 15. Testing and Verification Errorn 16. Network and Communication Errorn 17. Exception Handling Errorn 18. User Permission and Authentication Errorn 19. File and I/O Errorn 20. Type Errorn 21. Internationalization and Localization Errorn 22. Performance Issuen 23. Concurrency and Multithreading Error Category Definitions: - Configuration Management Error: Issues related to system configuration settings - Data Management Error: Problems with data handling, storage, or retrieval - Input Validation Error: Failures in validating or processing input data - Monitoring/Logging Error: Issues with system monitoring or logging functions - Environment Variable Error: Problems with environment variables - Dependency Management Error: Issues with external dependencies or libraries - Syntax Error: Basic programming language syntax violations - Design Flaw: Fundamental problems in code architecture or design - Security Vulnerability: Security-related weaknesses - Log Security Issue: Security problems in logging mechanisms - Reference Error: Invalid references to variables or objects - Session Management Error: Problems with user session handling - Code Quality Error: Issues affecting maintainability or readability - Logic Error: Flaws in business logic or algorithms - Testing/Verification Error: Problems with testing processes - Network/Communication Error: Issues in network interactions - Exception Handling Error: Problems with error handling - User Permission Error: Authentication or authorization issues - File/I/O Error: Problems with file operations - Type Error: Issues with data types or type conversions - Internationalization Error: Problems with language/locale support - Performance Issue: Efficiency or resource usage problems - Concurrency Error: Issues with parallel processing - Carefully examine the code structure and syntax - Identify all existing errors - Match errors against the predefined categories Categorization Requirements: 1. Primary Analysis . . . 2. Classification Criteria: . . . 3. Analysis Steps: . . . . - Locate the specific error in the code - Compare it with the predefined categories - Determine the most appropriate classification - Provide reasoning for the classification - Exact Match: The error perfectly fits one of the predefined categories - Best Fit: If multiple issues exist, select the most significant one - No Match: If the error doesnt match any predefined category Output Format: json { . . } \"Category\": \"<Selected error category from the predefined list>\", \"Confidence\": \"<High/Medium/Low>\" Classification Notes: - Only select one primary category even if multiple errors exist - If no predefined category matches, select the most appropriate error category - Include specific code references in the explanation - Indicate confidence level in the classification - Provide clear reasoning for the chosen category Additional Guidelines: - Focus on syntactic and structural analysis - Consider the context of the entire code - Be specific about error location - Explain any ambiguous cases - Maintain objectivity in classification Input Data: Start of Code $Code End of Code 40 Figure 22. Identifying programming error types prompt. Prompt for Classify Application Scenarios in Code QA Subset As professional expert in technical field classification, you are tasked with accurately categorizing programming problems and code snippets into the appropriate technical domains. Required Expertise: 1. deep understanding of programming concepts and the distinctive characteristics of various technical fields. 2. The ability to accurately identify the core technical features of code and problems. 3. Proficiency in discerning the boundaries and relationships between different technical domains. 4. Capacity to provide well-reasoned and persuasive justification for each classification. Technical Field Labels: 1. Basic Programming: Foundational concepts such as syntax, control flow and data structures. 2. Advanced Programming: In-depth topics like design patterns, concurrency and performance optimization. 3. Software Engineering: Practices including project management, code quality, testing and deployment. 4. Data Analysis: Data processing, statistical analysis and data visualization. 5. Mathematics: Algorithms, computation methods and mathematical modeling. 6. Desktop and Web Development: User interface design, front-end and back-end development and user interactions. 7. Machine Learning: Model training, deep learning and artificial intelligence. 8. Scientific Computing: Numerical computations, simulations and scientific modeling. 9. Database: Data storage, query optimization and database design. 10. Multimedia: Image, audio and video processing, as well as multimedia applications. 11. Operating Systems: System programming, memory management and process scheduling. 12. Other: Fields that do not fall into the categories above. Evaluation Process: 1. Carefully Review the given problem and code. 2. Identify the core technical features and key concepts. 3. Match these features with the relevant technical fields. 4. Select the most appropriate label from the predefined list. 5. Justify your selection with detailed reasoning. Output Format: md Label classification: <Chosen label from the predefined list> Label selection rationale: <Provide detailed explanation, addressing the following aspects:> . . . . - Analysis of the core technical features - Alignment with the selected technical field - Reasons for not selecting other relevant fields - Confidence level in the selection Guidelines: - Only one predefined label should be selected. - Avoid ambiguity in your decision-making process. - Provide clear, specific reasoning for your classification. - Focus on the core technical aspects of the problem rather than superficial characteristics. Make sure to select the label that best represents the core technical features of the problem. Input Data: Start of Question $Question End of Question Start of Answer $Answer End of Answer Figure 23. Prompt for classify application scenarios in Code QA subset. 41 Fine-Grained Checklists Generation Prompt for Code Gen Subset You are senior code reviewer with extensive technical expertise. Your role is to formulate insightful, high-quality review questions for provided code and problems, ensuring they are accurately categorized by technical dimensions. Criteria for evaluation questions: - Conduct thorough analysis of the design intent and implementation strategy of the solution. - Go beyond surface-level checks and explore deeper potential optimizations or hidden value in the solution. - Combine the context of the problem and the provided answer to generate innovative, thought-provoking questions. - The questions should inspire developers to critically reflect on the quality of their solution and consider avenues for improvement. - Ensure that each question addresses unique dimension, avoiding overlap or redundancy. For each of the following dimensions, generate one question. Each question should focus on the corresponding evaluation aspect. Evaluation Dimensions: 1. Correctness Verification: Assess whether the code is capable of solving the problem accurately and passing all test cases. 2. Time Complexity Optimization: Focus on optimizing the efficiency and performance of the algorithm. 3. Space Complexity: Evaluate how efficiently the code uses memory and manages resources. 4. Code Readability: Focus on improving the clarity, understandability and standardization of the code. 5. Robustness Validation: Evaluate the codes ability to handle edge cases and exceptions. 6. Algorithm Optimization: Assess whether the algorithms design and implementation are optimal. 7. Comprehensive Testing: Evaluate the scope and strategy behind the test coverage. 8. Output Format: Verify that the codes output strictly adheres to the required format. 9. Code Style Consistency: Ensure that the code follows consistent coding standards and best practices. 10. Maintainability: Assess whether the code structure is modular, easily optimized and expandable in the future. For each evaluation dimension, provide question following this format. Output Format: md 1. Question: <Specific judgment question> . Dimension: <Select the most relevant evaluation . dimension> . Classification reason: <Provide detailed explanation of why this dimension was selected> . Classification confidence: <Integer value between 1-10> ... Ensure that all 10 dimensions are covered with their respective questions, with each question addressing distinct aspect of the codes quality. Input Data: Start of Question $Question End of Question Start of Answer $Answer End of Answer Figure 24. Fine-Grained checklists generation prompt for Code Gen subset. Fine-Grained Checklists Generation Prompt for Code QA Subset You are senior code review expert with deep technical expertise. Your task is to formulate insightful and high-quality review questions for the provided questions and answers, ensuring the accurate classification of these questions across various technical dimensions. Criteria for Evaluating Questions: - Conduct an in-depth analysis of the design intent and implementation strategy presented in the answer. - Move beyond basic technical assessments, exploring the deeper value and potential for optimization within the solution. - Consider the context of both the question and the specific answer to propose unique and valuable technical questions that demonstrate innovative thinking. - Ensure that the questions inspire developers to critically evaluate the quality of the answer and its potential for improvement. - Questions addressing different technical dimensions must be independent, avoiding overlap or redundancy. For each of the following dimensions, generate one question. Each question should focus on the corresponding evaluation aspect. Evaluation Dimensions: 1. Correctness Ensure the solution is accurate and addresses the core needs of the problem. 2. Completeness: Ensure the answer comprehensively covers all aspects of the problem without leaving out any critical information. 3. Performance: Assess whether the solution optimizes time and space complexity effectively. 4. Maintainability: Evaluate the clarity and structure of the solution to ensure it is easy to optimize and extend in the future. 5. Clarity: Ensure that the language used is concise and clear, facilitating understanding and quick access to essential information. 6. Depth: Evaluate whether the answer provides an in-depth analysis of the problem, offering thorough understanding rather than superficial response. 7. Practicality: Ensure that the solution is feasible and can be implemented effectively in real-world scenarios. 8. Logic Coherence: Assess whether the reasoning behind the solution is clear, coherent and convincing. 9. Innovation: Determine if the solution offers unique perspective or introduces an innovative approach. 10. Reliability: Ensure that the opinions and suggestions are based on reliable, verifiable evidence. For each evaluation dimension, provide question following this format. Output Format: md 1. Question: <Specific judgment question> . Dimension: <Select the most relevant evaluation . dimension> . Classification reason: <Provide detailed explanation of why this dimension was selected> . Classification confidence: <Integer value between 1-10> ... Ensure that all 10 dimensions are covered with their respective questions, with each question addressing distinct aspect of the answers quality. Input Data: Start of Question $Question End of Question Start of Answer $Answer End of Answer Figure 25. Fine-Grained checklists generation prompt for Code QA subset. 43 Basic Critique Evaluation Prompt (with CoT) You are an AI Senior Programming Expert with advanced analytical reasoning capabilities. Your task is to conduct comprehensive, step-by-step evaluation of an AI assistants solution using detailed Chain of Thought (CoT) approach. Evaluation Framework: Stage 1: Comprehensive Problem Understanding - Carefully analyze the original problem statement - Identify explicit and implicit requirements - Assess technical complexity and contextual constraints - Determine key evaluation dimensions Stage 2: Systematic Solution Decomposition - Break down the proposed solution into discrete components - Evaluate each components technical correctness - Assess code quality, efficiency and alignment with best practices - Trace the logical flow and problem-solving approach Stage 3: Detailed Performance Assessment - Verify functional accuracy - Analyze performance optimization potential - Evaluate scalability and maintainability - Check comprehensive error handling strategies Evaluation Methodology: 1. Maintain complete objectivity 2. Provide specific, actionable feedback 3. Explain reasoning transparently at each stage 4. Highlight both solution strengths and improvement opportunities Output Requirements: - Provide clear, structured Chain of Thought explanation - Render definitive assessment: [Correct/Error] - Include detailed reasoning for each evaluation stage - Offer constructive, implementable improvement suggestions Evaluation Criteria: - Technical Precision - Code Quality - Problem-Solving Approach - Efficiency and Optimization - Adherence to Best Practices Output Format: json { . . } \"is_assistant_correct\": \"Correct/Error\", \"detailed_reasoning\": \"Explicit step-by-step logical deduction\" Input Data: Start of Question $Question End of Question Start of Answer $Answer End of Answer Figure 26. Basic critique evaluation prompt (with CoT). 44 Advanced Critique Evaluation Prompt (with CoT)"
        },
        {
            "title": "You are an Advanced Code Evaluation Specialist with Comprehensive Analytical\nCapabilities",
            "content": "Evaluation Methodology: Systematic Chain of Thought (CoT) Code Assessment Core Evaluation Framework: Stage 1: Holistic Problem Understanding - Conduct an in-depth analysis of the problem domain - Identify multi-dimensional evaluation criteria - Map out complex technical expectations - Establish comprehensive assessment perspective Stage 2: Detailed Code Decomposition - Systematically break down code into core components - Analyze each segment through multiple lenses: . 1. Technical Correctness . 2. Algorithmic Efficiency . 3. Code Readability . 4. Scalability . 5. Best Practice Adherence Stage 3: Precision Scoring Methodology Evaluation Dimensions (Each Scored 1-10): 1. Algorithmic Complexity and Efficiency 2. Code Structure and Readability 3. Error Handling and Robustness 4. Performance Optimization 5. Solution Creativity 6. Technical Precision 7. Memory Management 8. Scalability Potential 9. Maintainability 10. Overall Problem-Solving Approach Scoring Nuanced Guidelines: - 1-2 points: Critical, fundamental failures - 3-4 points: Significant technical deficiencies - 5-6 points: Functional but requires major improvements - 7-8 points: Solid implementation with minor refinement needs - 9-10 points: Exceptional, near-perfect solution Comprehensive Evaluation Process: - Conduct granular, multi-dimensional assessment - Provide explicit reasoning for each score - Generate weighted comprehensive evaluation - Offer constructive, actionable improvement suggestions Reasoning Chain Output Format: json { . . } \"comprehensive_score\": \"Weighted comprehensive score\", \"detailed_reasoning\": \"Explicit step-by-step logical deduction\" Input Data: Start of Question $Question End of Question Start of Answer $Answer End of Answer Start of Fine-Grained Evaluation Checklists $Checklists End of Fine-Grained Evaluation Checklists Figure 27. Advanced critique evaluation prompt (with CoT). 45 Prompt for Refining Origin Answer Based on Models Critiques You are Professional Self-Correction AI Specialist Objective: Systematically Refine and Improve Your Original Response Evaluation Process: 1. Carefully review the detailed feedback provided 2. Critically analyze your original response 3. Identify specific areas requiring improvement 4. Develop comprehensive, precise correction strategy Key Refinement Focus Areas: - Accuracy of information - Completeness of solution - Clarity of explanation - Technical depth - Problem-solving approach Correction Guidelines: - Address each piece of feedback explicitly - Provide clear rationale for modifications - Enhance the original responses quality - Maintain the core intent of the original answer Output Requirements: 1. Detailed explanation of identified limitations 2. Comprehensive corrected response 3. Specific improvements made 4. Reasoning behind each modification Output Fromat: json { . } \"corrected_response\": \"Fully updated and refined answer\", Critical Evaluation Principles: - Be objective and critical - Focus on substantive improvements - Demonstrate intellectual rigor - Enhance overall response quality Mandatory Considerations: - Fully address all feedback points - Provide clear, precise modifications - Maintain professional and technical accuracy Input Data: Start of Question $Question End of Question Start of Answer $Answer End of Answer Start of Feedback $Feedback End of Feedback Figure 28. Prompt for refining origin answer based on models critiques."
        }
    ],
    "affiliations": [
        "Alibaba",
        "BUAA",
        "BUPT",
        "CASIA",
        "Kuaishou",
        "M-A-P",
        "NJU",
        "OPPO"
    ]
}