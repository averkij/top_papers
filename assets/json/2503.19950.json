{
    "paper_title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation",
    "authors": [
        "Han Chen",
        "Zicong Jiang",
        "Zining Zhang",
        "Bingsheng He",
        "Pingyi Luo",
        "Mian Lu",
        "Yuqiang Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions. LogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques.LogQuant integrates effortlessly with popular inference frameworks like Python's transformers library. Implementation can be available in https://github.com/Concyclics/LogQuantKV."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 5 9 9 1 . 3 0 5 2 : r LOGQUANT: LOG-DISTRIBUTED 2-BIT QUANTIZATION OF KV CACHE WITH SUPERIOR ACCURACY PRESERVATION Han Chen & Zining Zhang & Bingsheng He School of Computing National University of Singapore 21 Lower Kent Ridge Road, Singapore 119077 {chenhan, zzn}@u.nus.edu, hebs@comp.nus.edu.sg Zicong Jiang School of Electronic and Information Engineering South China University of Technology 381 Wushan Road, Tianhe District, Guangzhou, 510641 P. R. China 202420111170@mail.scut.edu.cn Pingyi Luo & Mian Lu & Yuqiang Chen 4Paradigm #03-20 Galaxis (West Lobby),Singapore 138522 {luopingyi, lumian, chenyuqiang}@4paradigm.com"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce LogQuant, groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions. LogQuant takes different approach. By applying log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques. LogQuant integrates effortlessly with popular inference frameworks like Pythons transformers library. Implementation can be available in https://github.com/Concyclics/LogQuantKV."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid evolution of Large Language Models (LLMs) has enabled context window expansion from 4k to 128k tokens (Meta, 2024; OpenAI, 2024a), driving demand for efficient KV cache management in applications like multi-round chatbot conversations (OpenAI, 2024a; Anthropic, 2024; DeepSeek, 2024) and document-based question answering (Gao et al., 2023; Lewis et al., 2020), where comprehensive contextual understanding is required. Moreover, reasoning models such as OpenAI o1 (OpenAI, 2024b), increased the demand for even longer reasoning contexts, xacerbated the memory challenges faced in KV cache management. Recent studies Zhang et al. (2024); Li et al. (2024); Dong et al. (2024) reveal KV caches linear memory growth with context length and even exceeds model weights in long context and batch 1 Figure 1: The observed log-distribution pattern is evident not only in the magnitude of attention scores but also in the positions of attention spikes. These spikes become sparser as the model attends to tokens further from the most recent position, indicating that the model not only focuses on nearby tokens. This phenomenon, illustrated here with Llama3-8B-Instruct (Dubey et al., 2024) on the GSM8K dataset (Cobbe et al., 2021), is consistent across different tasks and models, as further detailed in Section 2. inference, posing serious deployment challenges. Existing KV Cache compression methods adopt either eviction, (H2O (Zhang et al., 2024), Keyformer (Adnan et al., 2024), snapKV (Li et al., 2024)), aim to reduce memory usage by selectively removing tokens deemed unimportant. or quantization (QAQ (Dong et al., 2024), KiVi (Liu et al., 2024c)), reduce the precision of less important tokens, retaining more data while minimizing memory costs. Both struggle with importance identification. window-based methods (KiVi, StreamingLLM (Xiao et al., 2023)) risk missing distant important tokens, while attention-based approaches (H2O, keyformer) suffer prediction errors from historical scores. Our approach addresses these shortcomings by leveraging key insight: the positions of the attention spikes (i.e. high attention scores) follow log distribution as shown in Figure 1, resulting in sparser importance for tokens as they move further from the current position. By utilizing this property, we can outperform existing methods across wide range of tasks. Additionally, the original absolute positions of KV cache entries can be disregarded without changing the final attention results during the decoding phase, which allows us to enhance the speed of our log-distributed quantization method. The key contributions of this paper are as follows: Observation of Log-Distributed Attention Spikes: We observe that in various models and downstream tasks, the positions of high attention spikes follow log distribution, becoming sparser as tokens move further from the current position. This insight underpins our approach to estimate token importance. Design of LogQuant: Leveraging this log-distribution observation, we introduce LogQuant, 2-bit quantization technique that significantly improves accuracy. LogQuant outperforms existing methods like KiVi and H2O by better preserving important tokens, achieving 40% to 200% improvement in accuracy on complex tasks such as Math and Code Completion with the same or higher compression ratio. Throughput Optimization: By ignoring the absolute positions of KV cache entries, our method further optimizes the speed of quantization/dequantization process without affecting the final attention results, resulting in 25% increase in throughput and 60% increase in batch size. The remainder of the paper is organized as follows: Section 2 details the core concepts behind our proposed LogQuant methods, Section 3 present an extensive set of experiments, Section 4 summarizes our findings and discusses potential directions for future work. Figure 2: The maximum attention score of each token position across four consecutive decoding steps, marking the high attention positions for illustrating the unpredictable nature of attention scores. This analysis was conducted using Llama3-8B-Instruct (Dubey et al., 2024) on the GSM8K (Cobbe et al., 2021) and OpenBookQA (Mihaylov et al., 2018) datasets."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "In Section 2.1, we analyze the distribution of attention scores and evaluate the impact of quantization loss, both with and without sink tokens. Section 2.2 explores the distribution of token importance and introduces our log-based selection strategy. In Section 2.3, we compare the effects of quantization and eviction under this selection scheme, demonstrating the superiority of quantization over eviction. To further enhance efficiency, Section 2.4 prove that attention computation is positionagnostic. Finally, we present the implementation details of our proposed LogQuant method in Section 2.5. 2.1 PRELIMINARY STUDY OF KV CACHE AND ATTENTION SCORES There are two well-established observations in recent works particularly relevant to KV cache compression. First, many tokens exhibit consistently low attention scores, indicating that their KV cache entries can be safely compressed with minimal impact on performance (Liu et al., 2024c). Second, predicting token importance based on previous decoding steps is unreliable, as attention scores can vary significantly across iterations, making it difficult to accurately identify which tokens should be preserved (Dong et al., 2024; Jiang et al., 2024). This is also demonstrated in Figure 2. Inspired by the observation of sink tokens (Xiao et al., 2023), which are the first few tokens that consistently receive high attention scores (Figure 3), we included these tokens in the set maintained at original precision to improve accuracy in 2-bit quantization. However, as shown in Table 1, this adjustment yielded minimal improvement. This suggests that while sink tokens play role in defining the conversational context, maintaining high precision for only these tokens is insufficient, indicating that tokens beyond the first few are also crucial for preserving model performance. Table 1: Impact of retaining the first two tokens (referred to as Sink) at original precision. The final answer accuracy results on GSM8K Cobbe et al. (2021) are presented. We present the improvement as Sink. Both methods maintain the recent 128 tokens at original precision. Model baseline(BF16) KiVi(4-bit) KiVi(2-bit) KiVi(2-bit)+Sink(BF16) Sink Llama3.1-8B-Instruct Qwen1.5-7B-Chat 71.41 57.24 67.24 52.27 18.04 39.80 18.49 39. +0.45 -0.38 2.2 THE LOG-DISTRIBUTED ATTENTION PATTERN As mentioned in Section 1, our analysis of attention heads reveals log-distributed high-attention pattern, which motivates the development of quantization scheme that follows this distribution. We introduce selection scheme where window of size 2W retains the most recent consecutive tokens 3 Figure 3: Attention distribution across different token positions, represented as boxplots based on 25% quantiles across all attention heads. The median and overall distribution of attention scores for sink tokens (Xiao et al., 2023) (tokens 0 and 1) are greater than the sum of the most recent 128 tokens. The attention scores are derived from experiments using Llama3-8B-Instruct (Dubey et al., 2024) and the GSM8K (Cobbe et al., 2021) dataset. Figure 4: The attention coverage without the first two sink tokens for different selection methods (Liu et al., 2024c; Xiao et al., 2023; Zhang et al., 2024) and different models (Dubey et al., 2024; Yang et al., 2024; Abdin et al., 2024), tested on subset of the GSM8K (Cobbe et al., 2021) dataset. Details of LogQuant will be introduced in Section 2.5. in full precision. Following this, another window of size W/2 selects tokens spaced one token apart, and then window of size W/4 follows the similar pattern and so on. Finally, window of 3W tokens is reserved in full precision. This creates log-distributed token selection scheme. We compare this log-distributed selection to other methods: KiVi, which selects only the most recent 3W tokens; StreamingLLM, which selects the most recent 3W tokens plus the first four sink tokens; and H2O, which uses previous attention scores to select the top 3W tokens. To evaluate these methods, we define token coverage as the average attention score captured by the selection scheme: Token Coverage = (cid:80)3W i=1 Attention Score of Selected Tokens 3W . (1) Figure 4 presents the results, where we exclude the first two tokens for calibration, as they typically have high attention scores but contribute minimally to overall model performance (see Section 2.1). The results demonstrate that our log-distributed selection scheme covers high-attention tokens more effectively. This suggests that filtering tokens for quantization based on this log distribution leads to better token importance preservation. 4 Figure 5: Eviction and Quantization Loss on Attention Distribution 2.3 COMPARISON OF QUANTIZATION AND EVICTION STRATEGIES When implementing log-distributed token selection for KV Cache compression, two primary approaches emerge: quantization and eviction. These methods differ fundamentally in their operation. Quantization reduces the numerical precision of individual tokens, whereas eviction removes tokens entirely, thereby shortening the sequence length. This distinction becomes critical due to the nature of the attention mechanism. The softmax function normalizes attention scores such that their sum equals 1. Consequently, removing tokens through eviction creates larger deviations from the original attention distribution compared to precision reduction via quantization. Specifically, eviction eliminates certain tokens from the attention computation entirely, while quantization retains all tokens with reduced numerical accuracy. As demonstrated in Figure 5, this behavioral difference is visually apparent. Quantitative results on the GSM8K dataset using Llama3.1-8B (see Table 2) show that eviction-based methods produce twice and higher attention errors than quantization. Based on these findings, we select quantization as the compression strategy. Table 2: Comparison of L1 error with original attention for eviction and quantization. LogQuant (2-bit) KiVi (2-bit) LogQuant (Eviction) KiVi (Eviction) 432. 556.10 1076.70 1612.56 2.4 POSITION-AGNOSTIC ATTENTION CALCULATION LLM inference involves two phases: prefill and decoding (Section A). As described in Yuan et al. (2024), the decoding phase is computationally expensive and memory-bound due to the use of the KV Cache. In the prefill phase, the model processes the input prompt in single pass. However, during decoding, new tokens are generated one at time, and each generation step requires access to the entire KV Cache. This leads to inefficiencies in both memory usage and execution time. To mitigate these inefficiencies, we plan to accelerate the attention procedure. The attention operation can be expressed mathematically as follows: = Softmax(Q ) = V, (2) where is the attention distribution, 1 vector resulting from the softmax operation applied to the product of and the transpose of and is the output, 1d vector calculated by multiplying the attention distribution with the Value matrix . 5 Figure 6: LogQuants KV cache compression workflow. The number of reserved original-precision tokens increases from 2W to 3W . We then apply log-sparse strategy to filter the first 2W tokens, quantize half of these tokens, and compress the reserved token length back to 2W . Since the attention distribution aggregates values over all tokens, the specific ordering of tokens in the Key and Value matrices does not affect the final output. This property allows us to permute or reorder the Key and Value caches without any loss of accuracy. By leveraging this insight, we can optimize the KV Cache by concatenating high-precision tokens with quantized tokens while disregarding their original positions. This approach enhances memory locality and processing efficiency while maintaining the correctness of the attention computation. This leads to the relation: where is permutation of the indices {1, . . . , }. This enables us to optimize the KV Cache effectively. = AP VP , (3) 2.5 LOGQUANT: ALGORITHM AND IMPLEMENTATION Algorithm. After comparing different logarithmic bases logN , we found that base-2 logarithmic implementation is sufficiently effective for our purposes. To maintain logarithmic sparsity within specified length, we adopt this base-2 logarithmic approach. We fix window length configuration , allowing us to retain up to 3W tokens at original precision. Each time the length limit is reached, we reduce the density of tokens in the first two windows (each of length ) by retaining tokens at regular intervals, effectively halving the density. This process reduces the number of retained tokens in the first two windows from 2W to 2W 2 = . Subsequently, we add new tokens, resulting in full-precision window size of 2W 2 + = 2W . At this point, the densities become = p, where is the initial density and Wi denotes the i-th window. 2 and densityW2 densityW1 By continuously adding new tokens, LogQuant naturally forms log2 sparsity selection within the constrained length. The detailed selection process is described in Algorithm 1. Using this approach, the length of retained full-precision tokens fluctuates between 2W and 3W , providing more stable compression ratio compared to KiVi, where the length fluctuates between 0 and R, with being the length of retained full-precision tokens in KiVi. We illustrate the workflow in Figure 6, which visually represents the KV cache management process, enhancing the understanding of our algorithms implementation. = Implementation. Popular inference frameworks, such as Hugging Faces transformers library, have encapsulated KV Cache management into dedicated classes, which simplifies the integration of new methods. To leverage this modular design, we implemented LogQuant as derived class of the Cache class in the transformers library. This approach ensures seamless compatibility with 6 Algorithm 1 Log-based Filtering Token Selection Strategy if length(A) < 3W then concat(A, a*) 1: Input: (list of original precision tokens), a* (new token), (window length) 2: Output: (updated list of tokens) 3: procedure APPENDTOKEN(A, a, ) 4: 5: 6: 7: 8: 9: 10: 11: end procedure concat(A[0:2W:2], A[2W:3W]) concat(A, a*) end if return else various quantization backends, including Quanto (Face, 2024) and HQQ (Badri & Shaji, 2023). For our implementation, we utilized Quanto as the quantization backend, adopting the Key-per-channel strategy. Furthermore, we integrated LogQuant into Hugging Faces inference pipeline, enhancing its usability for efficient and precise inference workflows."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 SETTINGS Models. We evaluate KiVi and LogQuant by 3 popular model families: Llama3/Llama3.1 (Dubey et al., 2024), Qwen1.5/Qwen2 (Bai et al., 2023; Yang et al., 2024), and Microsoft Phi3 (Abdin et al., 2024). Qwen1.5 and Phi3 are based on Multi-Head Attention, whereas Llama3/3.1 and Qwen2 utilize Group-Query Attention. The quantization group size is set to the Hugging Face default value of 64, and the quantized precision is set to INT2. For KiVi, the maximum length of reserved original-precision tokens is set to [128, 192, 256]. For LogQuant, the window length is limited to 3 as it will reserve maximum of 3W original precision tokens to ensure that the total number of reserved original-precision tokens does not exceed that of KiVi. Datasets. We selected GSM8K(Grade School Math, (Cobbe et al., 2021)) and LongBench (Bai et al., 2024) due to their widespread use in evaluating KV cache quantization, ensuring our results are comparable to those in the literature. For GSM8K, we test with 5-shot from the training set for better accuracy and keep the length of the input token between 600 and 1700, the evaluation is based on the exact value of the final answer. For LongBench, we test all 21 datasets among 6 types of tasks and use the LongBenchs original pipeline for evaluation. The test dataset details are present in Table B5. 3.2 ACCURACY AND EFFICIENCY ANALYSIS 3.2.1 ACCURACY COMPARISON ON DIFFERENT PRECISION To illustrate the impact of quantized data precision, we evaluate the accuracy loss using Llama3.18B-Instruct under both 2-bit and 4-bit quantization for KiVi and LogQuant methods on LongBench. As shown in Table 3, both methods achieve performance comparable to the baseline across all tasks with 4-bit quantization. However, 2-bit quantization results in noticeable drop in accuracy, highlighting the trade-off between memory efficiency and performance. Notably, LogQuant demonstrates better accuracy compared to KiVi under the same conditions. 3.2.2 ACCURACY COMPARISON AMONG DIFFERENT CONFIGURATIONS As discussed in Section 3.2.1, 4-bit quantization incurs only slight accuracy loss across tasks. Therefore, we focus on 2-bit quantization in the following discussion to highlight LogQuants performance. To further investigate the accuracy loss resulting from quantization, we compared the following methods: 1) 16-bit baseline, 2) KiVi and 3) LogQuant across different configurations, we define the compression ratio as: 7 Table 3: Accuracy of Different Precision on Llama3.1-8B. Refer to the Table C6 for the scores of each specific task. The shows the difference to baseline. Category KiVi (2-bit) KiVi (4-bit) LogQuant (2-bit) LogQuant (4-bit) baseline Single-Document QA Multi-Document QA Summarization Few-shot Learning Synthetic Tasks Code Completion 38.89 ( -8.11) 34.02 ( -4.98) 16.10 ( -1.90) 52.51 ( -8.49) 45.02 ( -21.98) 43.06 ( -15.94) 47.75 ( +0.75) 39.74 ( +0.74) 17.94 ( -0.06) 61.34 ( +0.34) 67.74 ( +0.74) 59.53 ( +0.53) 41.91 ( -5.09) 36.08 ( -2.92) 16.62 ( -1.38) 56.43 ( -4.57) 52.51 ( -14.49) 52.10 ( -6.90) 47.73 ( +0.73) 39.93 ( +0.93) 17.92 ( -0.08) 61.21 ( +0.21) 67.68 ( +0.68) 59.57 ( +0.57) 47.71 39.96 18.08 61.22 67.78 59.78 Figure 7: Accuracy(EM) with different compression ratio in GSM8K tasks for different models. Original tensor size Tensor size in compressed format where, for sequence length and reserved original precision token length in BF16 model with 2-bit quantization, the compression ratio can be expressed as: (4) 16L 2(L R) + 16R . (5) We tested the three compression ratios using GSM8K across three model families, and the results summarized in Figure 7. Our findings demonstrate that the LogQuant method consistently outperforms KiVi across all three models at various compression ratios. The results also indicate that smaller models and small KV states models, such as Phi3-mini (3.8B) and Qwen2-7B (retaining only 1 4 .), experience more significant accuracy loss with 2-bit quantized KV caches. However, our method provides notable improvement in accuracy for these smaller models. 8 of KV heads than Query, while other GQA models typically retain at least 1 3.2.3 ACCURACY COMPARISON AMONG DIFFERENT TASKS To further investigate the accuracy loss across various tasks, we evaluate the seven task groups listed in Table B5 and report the average score for each method in Table 4. In the following, the task groups are abbreviated as follows: Math remains unchanged; Code refers to Code Completion; Few-shot stands for Few-shot Learning; Multi-QA represents Multi-Document QA; Single-QA denotes Single-Document QA; Summ. is short for Summarization; and Synth. stands for Synthetic Tasks. We set the reserved length to 128, meaning that LogQuant uses only 3 3 = 126 original precision tokens, which is slightly fewer than the 128 tokens reserved by KiVi. As shown in Table 4, for simpler tasks such as Summarization, quantization has little to no impact on performance compared to the 16-bit baseline. However, for more complex tasks such as Code Completion, Synthetic Tasks, 8 Figure 8: memory usage and throughput comparison between 2bit LogQuant and 16bit baseline under huggingface generation pipeline with llama3.1-8B and H100. and Math, quantization significantly affects accuracy, with LogQuant demonstrating better retention of accuracy than KiVi. Table 4: Task Group Average Score for Different Models with 2-bit KV Cache Quantization. (The best result of 2-bit quantization is in bold. Refer to Table D7 for the scores of each specific task in LongBench.) Model llama-3.1-8B-Instruct Qwen1.5-7B-Chat-AWQ Qwen1.5-14B-Chat-AWQ Qwen2-7B-Instruct Phi-3-mini-128k-instruct Method 16-bit Baseline KiVi LogQuant (ours) 16-bit Baseline KiVi LogQuant (ours) 16-bit Baseline KiVi LogQuant (ours) 16-bit Baseline KiVi LogQuant (ours) 16-bit Baseline KiVi LogQuant (ours) 3.2.4 EFFICIENCY COMPARISON Math Code Few-shot Multi-QA Single-QA Summ. 18.07 71.42 16.10 18.04 16.62 40.41 17.11 56.18 17.16 39.27 17.38 49.28 17.21 70.28 17.17 59.82 17.24 63.31 16.33 52.99 9.31 3.71 13.13 34.34 17.56 80.29 9.10 12.59 9.89 51.86 39.95 34.01 36.08 33.05 31.08 32.04 39.72 37.91 38.01 33.35 12.35 28.28 33.55 18.19 21.70 61.21 52.50 56.42 53.88 51.32 52.54 59.02 57.50 58.25 61.90 35.26 51.23 52.58 36.17 39.36 47.71 38.89 41.90 39.26 35.80 37.22 42.48 40.39 41.37 44.66 20.52 34.84 42.47 19.58 23.63 59.78 43.06 52.09 52.46 34.79 40.68 57.47 37.48 49.37 58.23 35.91 48.71 55.97 33.97 40. Synth. 67.78 45.02 52.51 26.50 10.00 13.50 61.33 46.85 52.17 43.00 11.42 22.83 48.00 4.83 5.39 To evaluate memory and throughput efficiency by NVIDIA H100 48G MIG with the HuggingFace pipeline, we conducted benchmark similar to that in (Turganbay, 2024), setting an average prompt length of 512 and maximum output length of 2000. We incrementally increased the batch size while recording peak memory usage and throughput for both LogQuant (2-bit with 126 reserved tokens) and the BF16 baseline on the Llama-3.1-8B model, until memory usage reached the 48GB limit. The hardware utilized was single NVIDIA H100 GPU. As shown in Figure 8, LogQuant achieves approximately 25% higher throughput by supporting larger batch size. Additionally, it allows for 60% increase in batch size within the same memory constraints under the HuggingFace pipeline. We also observed that, within the HuggingFace pipeline, inference with quantized cache does not immediately release original KV states, which limits memory compression and efficiency. Furthermore, the dequantization operation impacts throughput. These issues suggest that memory efficiency and speed could be further improved by employing operator fusion, enabling computation on the quantized cache directly with fused attention operation. We will explore this optimization in future work."
        },
        {
            "title": "4 CONCLUSION AND FUTURE WORK",
            "content": "In this paper, we introduced LogQuant, novel quantization technique designed to optimize KV Cache management in large language models (LLMs). Our approach leverages base-2 logarithmic strategy to maintain sparsity while accommodating an increased number of full-precision tokens. Through comprehensive evaluations, we demonstrated that LogQuant consistently outperforms existing methods, such as KiVi, across various model families and compression ratios, particularly benefiting smaller models that typically suffer from accuracy loss due to quantization. We further explored the efficiency of our implementation within the HuggingFace pipeline, achieving notable improvements in throughput and memory utilization. Additionally, our investigation into accuracy loss across different tasks highlighted LogQuants superior retention of performance, especially in complex tasks. These findings underscore the potential of LogQuant to enhance LLM inference in resource-constrained environments. Future work will focus on refining our quantization approach and investigating further optimizations, such as operator fusion, to maximize efficiency and performance in LLM applications."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 6:114127, 2024. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48954901, 2023. Anthropic. Claude. https://claude.ai/new, 2024. (Accessed on 09/26/2024). Hicham Badri and Appu Shaji. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), August 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek. Deepseek. https://chat.deepseek.com/, 2024. (Accessed on 09/26/2024). Shichen Dong, Wen Cheng, Jiayu Qin, and Wei Wang. Qaq: Quality adaptive quantization for llm kv cache. arXiv preprint arXiv:2403.04643, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Hugging Face. Optimum quanto, 2024. URL https://github.com/huggingface/ optimum-quanto. Accessed: 2024-09-06. 10 Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. Huiqiang Jiang, YUCHENG LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference: Accelerating pre-filling for long-context llms via dynamic sparse attention. In Workshop on Efficient Systems for Foundation Models II@ ICML2024, 2024. Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527, 2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activationaware weight quantization for llm compression and acceleration. arxiv. arXiv preprint arXiv:2306.00978, 2023. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixtureof-experts language model. CoRR, 2024a. Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. Minicache: Kv cache compression in depth dimension for large language models. arXiv preprint arXiv:2405.14366, 2024b. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. In Forty-first International Conference on Machine Learning, 2024c. Meta. Introducing llama 3.1: Our most capable models to date. https://ai.meta.com/ blog/meta-llama-3-1/, 2024. (Accessed on 09/26/2024). Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. OpenAI. Models - openai api. https://platform.openai.com/docs/models/ gpt-4-and-gpt-4-turbo, 2024a. (Accessed on 09/26/2024). OpenAI. Openai o1 hub openai. https://openai.com/o1/, 2024b. (Accessed on 09/26/2024). Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Raushan Turganbay. Unlocking longer generation with key-value cache quantization, 2024. URL https://huggingface.co/blog/kv-cache-quantization. Accessed: 2024-0924. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. arXiv preprint arXiv:2402.04617, 2024. 11 Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. CoRR, 2024. Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 6:114127, 2024. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48954901, 2023. Anthropic. Claude. https://claude.ai/new, 2024. (Accessed on 09/26/2024). Hicham Badri and Appu Shaji. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), August 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek. Deepseek. https://chat.deepseek.com/, 2024. (Accessed on 09/26/2024). Shichen Dong, Wen Cheng, Jiayu Qin, and Wei Wang. Qaq: Quality adaptive quantization for llm kv cache. arXiv preprint arXiv:2403.04643, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Hugging Face. Optimum quanto, 2024. URL https://github.com/huggingface/ optimum-quanto. Accessed: 2024-09-06. 12 Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. Huiqiang Jiang, YUCHENG LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference: Accelerating pre-filling for long-context llms via dynamic sparse attention. In Workshop on Efficient Systems for Foundation Models II@ ICML2024, 2024. Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527, 2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activationaware weight quantization for llm compression and acceleration. arxiv. arXiv preprint arXiv:2306.00978, 2023. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixtureof-experts language model. CoRR, 2024a. Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. Minicache: Kv cache compression in depth dimension for large language models. arXiv preprint arXiv:2405.14366, 2024b. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. In Forty-first International Conference on Machine Learning, 2024c. Meta. Introducing llama 3.1: Our most capable models to date. https://ai.meta.com/ blog/meta-llama-3-1/, 2024. (Accessed on 09/26/2024). Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. OpenAI. Models - openai api. https://platform.openai.com/docs/models/ gpt-4-and-gpt-4-turbo, 2024a. (Accessed on 09/26/2024). OpenAI. Openai o1 hub openai. https://openai.com/o1/, 2024b. (Accessed on 09/26/2024). Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Raushan Turganbay. Unlocking longer generation with key-value cache quantization, 2024. URL https://huggingface.co/blog/kv-cache-quantization. Accessed: 2024-0924. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. arXiv preprint arXiv:2402.04617, 2024. 13 Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. CoRR, 2024. Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024. 14 BACKGROUND & RELATED WORK: KV CACHE COMPRESSION The attention mechanism relies on three key components: the Query (Q), Key (K), and Value (V) vectors. For each token, LLM computes d-dimensional vector and compares it against all stored vectors, where is the length of the sequence processed. The result of this comparison is used to weigh the corresponding vectors, producing the final output. Mathematically, the attention operation is defined as: Attention(Q, K, ) = Softmax (cid:18) QK (cid:19) (6) LLM inference is generally divided into two phases: prefill phase for processing input tokens and decoding phase for generating new tokens. In decoding, each token generation reloads the entire KV Cache from previous tokens, causing time and memory inefficiencies. KV cache compression methods fall into two categories: training-free methods (using eviction and quantization without model retraining) and training-required methods (designing more efficient attention structures). Our approach focuses on enhancing training-free methods for broader applicability. Eviction selectively discards less important tokens, while quantization lowers the precision of key and value states to save memory. However, both methods risk significant information loss at high compression ratesespecially 2-bit quantization, which can greatly reduce accuracy. A.1 KV CACHE EVICTION Eviction methods aim to reduce KV cache memory usage in Large Language Models (LLMs) by discarding less important tokens. The early work H2O (Zhang et al., 2024) selects heavy hitter tokens based on cumulative attention scores, though this risks evicting tokens that may become important later. Keyformer (Adnan et al., 2024) improves on H2O by combining Key Attention with window attention mechanism, retaining both historically significant and recent tokens for better accuracy. MiniCache (Liu et al., 2024b) reduces memory by reusing Key and Value states across layers. This method assumes that some key and value representations are redundant across model layers and can be shared. InfLLM (Xiao et al., 2024) addresses very long contexts by dividing them into blocks and retaining representative tokens for block eviction decisions. A.2 KV CACHE QUANTIZATION Quantization reduces storage and boosts computational speed by using fewer bits to represent values. Earlier works, like AWQ (Lin et al., 2023) and Qserve (Lin et al., 2024), applied 4-bit quantization to the KV cache with minimal accuracy loss. Recent methods aim to compress the KV cache further while preserving accuracy. QAQ (Dong et al., 2024) dynamically adjusts the precision of the inGPU quantized cache by offloading all original-precision KV data to CPU memory. GEAR (Kang et al., 2024) improves accuracy by storing the quantization error of the KV cache as sparse matrix with low-rank decomposition. KiVi (Liu et al., 2024c) introduces 2-bit quantization by retaining recent window of full-precision tokens, balancing memory efficiency and accuracy. A.3 TRAINING-REQUIRED APPROACHES An early memory-reducing attention design is Multi-Query Attention (MQA, (Shazeer, 2019)), where all query heads share single pair of key and value heads. While this reduces memory, it significantly impacts accuracy. Grouped-Query Attention (GQA, (Ainslie et al., 2023)) addresses this by grouping query heads, with each group sharing the same key and value heads, preserving the generalization ability of multi-head attention while reducing KV cache size. Deepseek V2 (Liu et al., 2024a) introduces Multi-Head Latent Attention (MLA), which compresses key and value states using LoRA-based projections. To prevent disruption of position embeddings from LoRA compression, specific channels are reserved for position information only, excluding them from LoRA compression."
        },
        {
            "title": "B OVERVIEW OF TEST DATASETS",
            "content": "Table B5: Overview of all test datasets. Avg len (average length) is computed using the number of words for the English (code) datasets and the number of characters for the Chinese datasets. Accuracy (CLS) refers to classification accuracy, while Accuracy (EM) refers to exact match accuracy Task Group Dataset Avg len Metric Language #data Math Single-Document QA Multi-Document QA Summarization Few-shot Learning Synthetic Task Code Completion GSM8K NarrativeQA Qasper MultiFieldQA-en MultiFieldQA-zh HotpotQA 2WikiMultihopQA MuSiQue DuReader GovReport QMSum MultiNews VCSUM TREC TriviaQA SAMSum LSHT PassageCount PassageRetrieval-en PassageRetrieval-zh LCC RepoBench-P F1 F1 F1 F1 F1 F1 F1 240 Accuracy (EM) English English 18,409 English 3,619 English 4,559 Chinese 6,701 English 9,151 English 4,887 English 11,214 Chinese 15,768 Rouge-L English 8,734 Rouge-L English 10,614 Rouge-L English 2,113 Rouge-L 15,380 Rouge-L Chinese 5,177 Accuracy (CLS) English English F1 8,209 English 6,258 Rouge-L 22,337 Accuracy (CLS) Chinese English 11,141 Accuracy (EM) English 9,289 Accuracy (EM) Chinese 6,745 Accuracy (EM) Python/C#/Java 1,235 Edit Sim Python/Java 4,206 Edit Sim 1319 200 200 150 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200"
        },
        {
            "title": "C META DATA OF PRECISION COMPARISON",
            "content": "Table C6: Comparison on Llama3.1-8B-Instruct of different quantization precisions Dataset 2wikimqa dureader gov report hotpotqa lcc lsht multi news multifieldqa en multifieldqa zh musique narrativeqa passage count passage retrieval en passage retrieval zh qasper qmsum repobench-p samsum trec triviaqa vcsum KiVi (2-bit) KiVi (4-bit) LogQuant (2-bit) LogQuant (4-bit) Baseline 45.06 28.48 20.41 55.90 62.99 45.00 15.89 54.91 62.72 30.39 28.19 6.31 99.50 97.54 45.03 19.15 56.57 35.72 72.50 91.64 16.85 44.79 27.75 19.86 55.78 63.44 45.00 15.65 55.10 62.77 30.65 27.91 6.31 99.50 97.42 45.20 19.07 55.61 36.12 72.50 91.73 17.17 39.52 22.20 18.60 48.83 47.09 31.42 15.07 42.51 50.12 25.52 26.44 5.67 83.17 46.23 36.50 17.41 39.03 23.88 65.00 89.72 13.33 40.69 22.59 18.78 52.43 57.52 33.75 15.11 45.98 55.51 28.62 27.93 5.63 92.25 59.65 38.21 18.19 46.67 33.33 67.00 91.63 14. 45.18 27.99 20.09 55.85 62.85 45.00 15.64 54.63 63.27 30.70 28.28 6.15 99.50 97.38 44.74 18.92 56.28 35.45 72.50 91.89 17."
        },
        {
            "title": "D META DATA OF LONGBENCH RESULTS",
            "content": "Table D7: LongBench score of each dataset precision 16-bit 2-bit Task Group Baseline KiVi llama-3-8B-Instruct LogQuant (ours) 2WikiMultihopQA DuReader GovReport HotpotQA LCC LSHT MultiFieldQA-en MultiFieldQA-zh MultiNews MuSiQue NarrativeQA PassageCount PassageRetrieval-en PassageRetrieval-zh Qasper QMSum RepoBench-P SAMSum TREC TriviaQA VCSUM 37.24 16.73 17.8 46.1 56.85 25.25 44.44 56.3 16.59 21.44 22.07 6.5 66.0 91.0 43.69 17.49 51.32 33.22 74.0 90.48 0.16 31.72 12.45 12.8 43.87 31.73 21.5 38.68 43.96 15.76 19.56 19.82 5.5 53.0 33.45 33.9 17.01 31.99 22.44 72.5 87.65 0.17 llama-3.1-8B-Instruct 2WikiMultihopQA DuReader GovReport HotpotQA LCC LSHT MultiFieldQA-en MultiFieldQA-zh MultiNews MuSiQue NarrativeQA PassageCount PassageRetrieval-en PassageRetrieval-zh Qasper QMSum RepoBench-P SAMSum TREC TriviaQA VCSUM 45.06 28.48 20.41 55.9 62.99 45.0 54.91 62.72 15.89 30.39 28.19 6.31 99.5 97.54 45.03 19.15 56.57 35.72 72.5 91.64 16.85 39.52 22.2 18.6 48.83 47.09 31.42 42.51 50.12 15.07 25.52 26.44 5.67 83.17 46.23 36.5 17.41 39.03 23.88 65.0 89.72 13.33 Phi-3-mini-128k-instruct 19.12 10.38 8.83 31.33 39.85 2WikiMultihopQA DuReader GovReport HotpotQA LCC 35.78 22.75 18.7 50.44 57.44 Continued on next page 17 35.08 15.5 15.63 44.96 41.75 21.75 41.04 48.44 16.06 20.59 21.56 4.0 58.5 72.0 39.46 17.37 40.1 32.66 73.0 89.36 0.25 40.69 22.59 18.78 52.43 57.52 33.75 45.98 55.51 15.11 28.62 27.93 5.63 92.25 59.65 38.21 18.19 46.67 33.33 67.0 91.63 14.41 24.61 9.26 9.47 37.48 47.53 Table D7 continued from previous page Task Group Baseline KiVi LSHT MultiFieldQA-en MultiFieldQA-zh MultiNews MuSiQue NarrativeQA PassageCount PassageRetrieval-en PassageRetrieval-zh Qasper QMSum RepoBench-P SAMSum TREC TriviaQA VCSUM 2WikiMultihopQA DuReader GovReport HotpotQA LCC LSHT MultiFieldQA-en MultiFieldQA-zh MultiNews MuSiQue NarrativeQA PassageCount PassageRetrieval-en PassageRetrieval-zh Qasper QMSum RepoBench-P SAMSum TREC TriviaQA VCSUM 27.25 54.9 52.09 15.52 25.23 23.28 3.0 82.5 58.5 39.6 17.97 54.49 30.62 66.0 86.43 18.04 14.25 29.04 8.16 12.72 11.92 15.34 2.25 11.0 1.25 25.78 5.88 28.09 9.23 59.5 61.72 8.97 Qwen1.5-14B-Chat-AWQ 44.35 23.34 16.23 53.69 36.94 32.5 44.75 58.54 15.01 30.25 21.73 2.55 71.0 67.0 36.56 18.03 38.03 32.69 76.5 88.32 19.42 44.81 26.02 16.31 55.67 56.69 37.0 48.36 60.35 14.95 32.38 22.26 1.0 94.5 88.5 38.93 18.16 58.25 32.95 77.5 88.63 19. Qwen1.5-7B-Chat 2WikiMultihopQA DuReader GovReport HotpotQA LCC LSHT MultiFieldQA-en MultiFieldQA-zh MultiNews MuSiQue NarrativeQA PassageCount PassageRetrieval-en PassageRetrieval-zh Qasper QMSum 32.8 25.96 16.66 48.11 58.17 28.0 47.14 53.4 15.02 26.74 20.06 1.0 40.5 59.0 39.84 18.25 31.83 22.64 15.57 47.37 45.87 24.0 42.26 50.18 15.0 25.88 19.02 0.5 20.0 18.25 37.19 17.59 Continued on next page LogQuant (ours) 13.75 34.91 12.32 13.33 15.46 17.37 4.5 9.68 2.0 29.91 7.04 34.16 13.03 62.5 68.15 9.74 44.39 23.28 16.25 53.9 50.95 34.5 45.68 59.43 14.94 30.45 22.83 2.0 80.0 74.5 37.54 18.13 47.79 33.34 77.5 87.66 19.65 32.14 24.06 15.84 48.91 53.77 24.5 43.72 51.68 14.92 27.09 20.06 0.0 24.0 29.0 37.28 18.18 Table D7 continued from previous page LogQuant (ours) 30.76 33.31 67.5 87.37 19.34 33.46 24.36 16.65 46.0 52.33 27.0 45.85 46.73 15.16 24.36 20.14 0.0 18.5 22.0 36.16 17.77 29.03 32.06 63.5 87.61 19. 40.12 15.01 16.07 39.92 51.46 26.25 36.42 47.57 13.6 18.07 18.43 5.5 33.5 29.5 36.94 12.25 45.95 28.03 68.0 82.63 10.58 Baseline KiVi Task Group RepoBench-P SAMSum TREC TriviaQA VCSUM 45.46 33.01 70.5 86.76 17.98 26.33 29.7 69.5 86.51 19.15 Qwen1.5-7B-Chat-AWQ 30.82 23.1 16.31 47.17 44.56 25.5 42.87 45.51 15.04 23.23 19.58 0.0 16.0 14.0 35.27 17.34 25.02 28.3 65.0 86.48 19. 32.43 25.84 16.98 47.77 57.98 29.0 46.72 50.97 14.97 26.18 20.93 0.5 30.5 48.5 38.45 17.85 46.95 31.98 67.0 87.56 18.66 2WikiMultihopQA DuReader GovReport HotpotQA LCC LSHT MultiFieldQA-en MultiFieldQA-zh MultiNews MuSiQue NarrativeQA PassageCount PassageRetrieval-en PassageRetrieval-zh Qasper QMSum RepoBench-P SAMSum TREC TriviaQA VCSUM Qwen2-7B-Instruct 2WikiMultihopQA DuReader GovReport HotpotQA LCC LSHT MultiFieldQA-en MultiFieldQA-zh MultiNews MuSiQue NarrativeQA PassageCount PassageRetrieval-en PassageRetrieval-zh Qasper QMSum RepoBench-P SAMSum TREC TriviaQA VCSUM 44.15 19.22 18.09 44.3 57.72 44.0 46.89 61.48 15.58 25.71 24.43 5.0 69.0 55.0 45.82 17.92 58.74 35.94 78.0 89.66 13.74 11.33 13.08 10.82 17.39 36.63 23.0 21.97 33.67 8.53 7.58 5.29 5.5 19.25 9.5 21.16 9.08 35.18 18.23 58.25 41.56 8."
        }
    ],
    "affiliations": [
        "4Paradigm",
        "School of Computing National University of Singapore",
        "School of Electronic and Information Engineering South China University of Technology"
    ]
}