{
    "paper_title": "POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models",
    "authors": [
        "Wenshuo Chen",
        "Haosen Li",
        "Shaofeng Liang",
        "Lei Wang",
        "Haozhe Jia",
        "Kaishen Yuan",
        "Jieming Wu",
        "Bowen Tian",
        "Yutao Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 9 6 3 0 0 . 2 1 5 2 : r POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models Wenshuo Chen1* Haosen Li1 Shaofeng Liang1 Lei Wang2,3 Haozhe Jia1 Kaishen Yuan1 Jieming Wu1 Bowen Tian1 Yutao Yue1 1 The Hong Kong University of Science and Technology (Guangzhou) 2 Griffith University 3 Data61/CSIRO Project: polaris-code-official.github.io Figure 1. POLARIS serves as superior inversion foundation, significantly enhancing the performance of various downstream diffusion tasks. (Top two rows) Image restoration: In colorization, inpainting, deblurring, and super-resolution tasks, all results show significant fidelity improvements when POLARIS is used as the inversion foundation. (Right panel) Image editing: In the cat-to-dog editing task, POLARIS enables the model to successfully execute this complex object replacement. (Bottom row) Reconstruction: POLARIS achieves near-perfect inversion and reconstruction, providing robust and high-fidelity foundation for downstream tasks."
        },
        {
            "title": "Abstract",
            "content": "The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal critical, overlooked factor in reconstruction degradation: the ap- *Equal contribution. Corresponding author (yutaoyue@hkust-gz.edu.cn). proximate noise error. This error stems from approximating the noise at step with the prediction at step t1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as step-wise variable and derives mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks. 1. Introduction Recent advances in diffusion models [110] have enabled powerful real-world image editing capabilities [1121]. These methods rely on denoising inversion, which has become the core mechanism behind modern image manipulation and restoration. The dominant methodology for this task hinges on critical preliminary step: inverting source image into the models latent space [3, 22]. This process is commonly realized through deterministic schedulers, such as DDIM [2], which generate reconstructable latent trajectory that underpins subsequent guidance-based editing. The success of countless editing applications today is built upon the presumed reliability of this inversion process. However, the very foundation of DDIM inversion rests on critical approximation. To reverse the diffusion process, one must estimate the noise from previous step. This slight yet crucial discrepancy between the noise predictions at consecutive timesteps introduces persistent error that subtly corrupts the latent trajectory. The issue is dramatically amplified by Classifier-Free Guidance (CFG) [23], an indispensable tool for high-fidelity generation. Practitioners face difficult dilemma: low CFG scale struggles to preserve the images core semantics, while high scale magnifies the inversion error [24, 25], leading to conspicuous artifacts and visible departure from the source image [26]. In response to this challenge, family of methods [24, 25, 2732] has emerged. These approaches tacitly accept this foundational flaw in the inversion process and instead focus on designing intricate algorithms to mitigate its downstream effects during the denoising and editing phase. While effective, they often build intricate corrective layers on top of flawed base, frequently at the cost of implementation complexity, computational efficiency, and mathematical elegance. Furthermore, their corrective nature often leads to suboptimal preservation of the image background during editing, as the model struggles to disentangle editing effects from inversion error compensation. In this paper, we argue that more fundamental approach is necessary. Rather than compensating for the consequences of inversion error [24, 25, 29, 30], we are the first to confront its root cause: the inherent discrepancy in noise predictions between consecutive steps. We reframe the challenge, proposing to treat the inversion process as direct optimization problem. We formally define the inversion error as the Euclidean norm of the noise prediction difference and seek to minimize it. analytical solution for the optimal CFG scale proves to be fragile and impractical. Our key insight, however, is that by strategically pruning negligible historical dependency terms from the objective, we can derive rigorous and robust optimization. The solution yields an adaptive, optimal CFG scale for each inversion step, method we call Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS). This approach is remarkably simple, effective, and intuitive, achieving near-perfect inversion with dynamic CFG scale adjustment implemented in just one line of code. We demonstrate the power and versatility of POLARIS through extensive experiments on massive benchmarks such as Pick-a-Pic [33] and COCO 2017 [34], covering diverse editing and restoration tasks. Integrated into multiple models, POLARIS serves as seamless, plug-and-play module that substantially improves reconstruction fidelity and editing quality with negligible overhead. Our main contributions are: i. We identify fundamental yet overlooked source of error in denoising inversion: the noise-prediction discrepancy between consecutive steps. This is the root cause of trajectory drift amplified by fixed CFG. ii. We propose POLARIS, which reformulates inversion from error compensation to error-origin optimization by treating the guidance scale as dynamic per step variable and deriving closed form update rule that minimizes inversion error at its source. iii. Extensive quantitative and qualitative experiments confirm that POLARIS integrates seamlessly into existing diffusion pipelines, consistently achieving nearly stateof-the-art inversion performance with virtually negligible additional computational cost. 2. Related Work Diffusion model inversion. Diffusion models are the standard backbone for high-fidelity image synthesis, and inversion is the bridge to editable generative trajectories. Deterministic DDIM inversion [2] aims to reverse the sampling ODE [4, 31], but coupling with CFG breaks its nearbijection. Specifically, the fixed guidance scale (ω > 1) amplifies prediction errors inherent in the guidance vector (the difference between conditional and unconditional estimates), causing compounding trajectory drift. To compensate, optimization-based schemes like Null-Text Inversion [24] and Negative Prompt Inversion [30] adjust the conditioning, while others optimize the latent code [28]. Despite progress, these methods address the consequences of inversion error (via per-sample optimization) rather than its origin in the guidance-coupled reverse map. Editing and restoration via inversion. faithful inverted latent unlocks broad family of downstream tasks. For editing, Prompt to Prompt [11] manipulates crossattention to localize semantic changes while preserving layout; instruction-tuned approaches like InstructPix2Pix [21] enable intuitive text edits; Plug-and-Play [14] and MasaCtrl [12] inject features or control attention to enforce structural consistency. inversion acts as physics-aware prior: DDRM [35] propagates measurement-space corrections, DDNM [36] decomposes updates into range/null spaces to strictly enforce known constraints, and Diffusion Posterior Sampling (DPS) [37] applies consistency-gradient guidance. For restoration, Nevertheless, these pipelines typically rely on fixed or heuristically tuned guidance during inversion, leaving error accumulation unaddressed at its source. Position of our work. POLARIS reframes inversion from an error-compensation problem to an error-origin problem. Instead of optimizing embeddings or latents to counteract drift, we treat the guidance scale as step-wise variable and derive mathematically grounded rule that minimizes inversion error at each step. Concretely, we obtain closed-form, per-timestep scale from an orthogonality condition between conditional/unconditional noise-change vectors, yielding robust, plug-and-play dynamic schedule that can be dropped into standard DDIM inversion. POLARIS is thus complementary to prior pipelines: it preserves their modularity while replacing brittle, fixed hyperparameter with an adaptive, theoretically justified control law. 3. Preliminary Denoising diffusion implicit models. DDIM provide determinate sampling process. The denoising step maps sample xt to cleaner sample xt1 using U-Net prediction ε(xt), governed by schedule αt: xt1 = D(xt) = (cid:112)1 αt1 ε(xt) (cid:18) xt 1 αt ε(xt) + αt1 αt (cid:19) (1) The corresponding inversion process maps data sample back to noise. We denote the states along this inversion path with tilde, e.g., xt1 and xt. The process requires an approximation: since the ideal noise prediction ε(xt) cannot be computed directly [2] (as it depends on the unknown xt), it is approximated by the prediction from the current state, ε(xt1). This leads to the practical inversion formula: xt = I(xt1) = αt αt1 xt1 (cid:32) + 1 αt (cid:33) (cid:112)αt(1 αt1) αt1 ε(xt1) (2) This approximation makes the inversion computationally tractable but also renders the process nearly, rather than perfectly, invertible. Algorithm 1 DDIM Inversion-Sampling with POLARIS x0 Encode(I0) for = 0, . . . , 1 do Part 1: Inversion Input: image I0, source prompt cs, time step Output: inverted latent xT , scale list {ωt}T 1 t=1 1 2 3 4 5 6 if = 0 then ωt ω else 7 ωt εφ2 εφ εcs εφ εcs 2 + ϵ Compute εφ, εcs from step and 1. Record ωt in the scale list. end if ε(xt) (1 ωt)εφ(xt) + ωtεcs(xt) xt+1 I(xt) 8 9 10 11 end for 12 Part 2: Sampling Input: latent xT , target prompt ct, scale list {ωt}T 1 t=1 Output: reconstructed image 0 for = T, . . . , 1 do 1 + 1 2 ε(xt) (1 ωi)εφ(xt) + ωiεct(xt) 2 xt1 D(xt) 3 4 5 end for 0 Decode(x0) Classifier-free guidance. To control the influence of the conditional prompt on the generation process, we employ CFG [23]. This technique utilizes model ε trained to predict noise for both conditional and unconditional inputs. This is typically achieved by randomly replacing the condition with null token during training. At each denoising step t, we compute two noise predictions: the unconditional prediction εφ(xt) and the conditional prediction εc(xt). The final guided noise prediction for the denoising process, ϵ(xt), is linear combination of these two, controlled by guidance scale ω: ε(xt) = (1 ω)εφ(xt) + ωεc(xt). (3) Similarly, for an inversion path state xt1, the guided noise ϵ(xt1) is computed as: ε(xt1) = (1 ω)εφ(xt1) + ωεc(xt1). (4) The guidance scale ω is scalar hyperparameter. With ω = 0 the process is unconditional; with ω > 1 prompt guidance Figure 2. (A) Existing CFG-based DDIM methods introduce and accumulate errors at each step of the inversion process, eventually leading to the distribution shift of downstream tasks. (B) Our method actively seeks mathematically invertible path to obtain latent variable that is closer to the ideal one. During generation, the recorded ωt sequence is replayed, enabling high-fidelity reconstruction. (C) Shows our optimization goal from geometric point of view to minimize the inversion error. is amplified. In standard DDIM sampling/inversion, ω is typically constant across timesteps. 4. Methodology Our method aims to minimize the DDIM inversion error by dynamically optimizing the guidance scale ω at each timestep. We first derive theoretically exact but practically unstable solution. We then analyze its failure modes and propose robust approximation grounded in both empirical evidence and theoretical analysis. All formula derivations and proofs can be found in Appendix 7. 4.1. The Exact Solution and Its Instability The core of DDIM inversion error stems from the approximation ε(xt) ε(xt1). Our goal is to minimize this discrepancy, which we define as the inversion error τinv(t): τinv(t) = ε(xt) ε(xt1), where the guided noise at steps and 1 are given by: (cid:40) ϵ(xt) = (1 ωt)εφ(xt) + ωtεc(xt) ϵ(xt1) = (1 ωt1)εφ(xt1) + ωt1εc(xt1) (5) (6) By defining ω = ωt ωt1, we can expand the error term: τinv(t) = (1 ωt)(εφ(xt) εφ(xt1)) + ωt(εc(xt) εc(xt1)) + ω(εc(xt1) εφ(xt1)) = + bω, (7) where we define = (1 ωt)εφ + ωtεc as the error from prediction changes within the current step, and = εc(xt1)εφ(xt1) as the guidance direction from the previous state. The deltas are defined as εφ = εφ(xt) εφ(xt1) and εc = εc(xt) εc(xt1). Minimizing + bω2 with respect to ω yields the closed-form solution: ω = b2 . (8) This provides the optimal guidance scale for the next step as ωt = ωt1 + ω. Given an initial guidance scale ωT at the final timestep , the optimization process becomes sequential problem. At each inversion step (from 0 up to 1), the value of ωt is known from the calculation of the previous step. Consequently, the only variable in the optimization problem at step is ωt. This allows us to compute the optimal change using Eq. (8), which in turn determines the guidance scale for the subsequent step, ωt1. This establishes recursive formula for the guidance scale: ωt = ωt1 + ωt = ωt1 at bt bt2 (9) where the terms at and bt are computed at step t. By unrolling this recursive relationship, we can express the guidance scale ωt at any arbitrary step < as function of the initial value ω0 and the sum of all optimal changes from step 1 up to : ωt = ω0 + (cid:88) k=1 ωk = ω0 (cid:88) k=1 ak bk bk2 (10) Figure 3. Empirical validation of the negligibility of the historydependent term. It compares the average magnitude of the currentstep error term (red line) against the history-dependent term ω (yellow line). The squared ratio of their magnitudes (blue line) is shown to be significantly greater than the reference line across the vast majority of timesteps. It validates our core hypothesis that the history term bω is numerically negligible, justifying our approximation in Eq. (11). Eq. (10) explicitly shows that the optimal guidance scale at any point is the initial scale adjusted by the cumulative sum of all subsequent error-minimizing changes. We utilize Eq. (10) to compute the optimal guidance scale for each inversion timestep of the input image. These scales are then recorded and used for denoising at the corresponding timesteps during reconstruction. As shown in Figure 4, the experimental results indicate that our proposed exact dynamic scale method offers no significant improvement in reconstruction quality over the traditional fixed-scale approach; both methods produce images with significant deviations from the original. Moreover, the dynamic scales evolution curve exhibits extreme fluctuations. This phenomenon verifies the practical fragility of the exact solution, which often leads to divergent or nonsensical results. Its instability stems from strong dependence on the historical state vector b. This dependency can cause errors to accumulate and be catastrophically amplified across time steps. The following theorem, proved in 7.1, formalizes the instability: Theorem 1. The computation of ω is ill-posed, as small prediction noise δ can cause its error to diverge when the historical guidance direction approaches zero. 4.2. Robust Approximate Solution The failure of the exact solution stems from the historydependent term bω in Eq. (7). To develop robust alternative, we hypothesize that this term is numerically negligible compared to the current-step error term a. To validate this hypothesis, we conducted batch analysis experiment. We randomly selected 1000 samples from Figure 4. Instability of the Exact Solution and resulting reconstruction collapse. The guidance scale ω computed by this solution (green line) exhibits extreme fluctuations and instability. It leads to total collapse in the reconstruction (right), which is filled with artifacts and distortion compared to the original image (left). This phenomenon confirms the solutions practical fragility and strongly motivates our search for more robust approximation. the Pick-a-Pic dataset. Since our optimization objective is the mean squared error (MSE), the contribution of any error component to the total loss is proportional to its squared magnitude. Therefore, to assess the relative impact of the two error sources, at each step of the DDIM inversion, we computed the ratio of squared magnitudes: (a/bω)2. The results (see Figure 3) provide empirical support, showing that the square ratio is significantly greater than 1 across the vast majority of steps, averaging over 20. This confirms that is the dominant component of the inversion error; its average magnitude was 3.2662, compared to just 0.7215 for the history-dependent term ω b. Hence, the contribution of can be considered negligible. Based on this empirical evidence, we propose an approximation by simplifying the optimization to neglect the history-dependent term. After that, we define the error: τapprox(t) = = (1 ωt)εφ + ωtεc. (11) Our new goal is to find an optimal ωt at each step that independently minimizes τapprox(t)2. We derive the solution from geometric perspective (see Fig.2(c)). The error vector τapprox(t) is an affine combination of εφ and εc, hence its endpoint lies on the line connecting the endpoints of these two vectors. The minimumnorm point on this line is obtained by projecting the origin onto it, i.e., by enforcing that τapprox(t) is orthogonal to the lines direction. Let = εc εφ denote this direction. The orthogonality condition reads: τapprox(t) (εc εφ) = 0. (12) Table 1. Performance comparison of POLARIS and Fixed scale across various inference steps on Pick-a-Pic and COCO2017. Dataset Metric Method 10 20 30 50 60 70 80 90 COCO2017 Pick-a-Pic MSE () PSNR () Fixed scale POLARIS Fixed scale POLARIS 1827 506 15.94 22.04 2285 498 14.94 22.24 2417 490 14.68 22. 2614 493 14.32 22.36 2692 498 14.19 22.34 2429 467 14.67 22. 2617 477 14.33 22.54 2492 464 14.55 22.66 2764 478 14.08 22. 2866 476 13.92 22.55 SSIM () Fixed scale 0.5447 0.5079 0.4959 0.4851 0.4806 0.4869 0.4793 0.4824 0.4723 0.4691 0.6922 0.7043 0.7085 0.7087 0.7088 0.7135 0.7122 0.7143 0.7126 0.7133 POLARIS LPIPS () Fixed scale 0.4583 0.5062 0.5181 0.5330 0.5380 0.5206 0.5337 0.5255 0.5440 0.5498 0.2231 0.2039 0.1957 0.1950 0.1955 0.1870 0.1897 0.1862 0.1892 0.1886 POLARIS MSE () PSNR () Fixed scale POLARIS Fixed scale POLARIS 1688 632 15.86 20. 2514 539 14.13 20.81 2520 510 14.12 21.05 2530 527 14.10 20. 2583 533 14.01 20.87 2464 518 14.21 20.99 2537 521 14.09 20. 2535 519 14.09 20.98 2646 523 13.91 20.94 2691 514 13.83 21. SSIM () Fixed scale 0.4204 0.3442 0.3485 0.3500 0.3485 0.3558 0.3519 0.3506 0.3470 0.3460 0.5605 0.5992 0.6128 0.6140 0.6154 0.6204 0.6210 0.6219 0.6215 0.6239 POLARIS LPIPS () Fixed scale 0.3670 0.5041 0.4958 0.5020 0.5056 0.5037 0.5065 0.5192 0.5095 0.5091 0.1627 0.1213 0.1115 0.1138 0.1138 0.1101 0.1109 0.1096 0.1123 0.1102 POLARIS Solving this equation for ωt yields proposed robust solution: 5.1. Diffusion Inversion and Reconstruction ω(t) = εφ2 εφ εc εφ εc2 . (13) This solution is computed at each step using only information available at that step, thus avoiding the cascading error problem of the exact solution. The following theorem which is proved in 7.2 establishes the theoretical robustness of this approximate solution: Theorem 2. The computation of ω(t) is well-posed. Under the mild condition εφ εc > 0, its error is bounded and of the same order as the input prediction noise. It is worth noting that the additional computational cost of POLARIS comes entirely from calculating ω(t) based on known variables. Compared with models containing large number of parameters, this computational overhead is negligible. See Appendix 9.1 for details. 5. Experiment In this section, we conduct comprehensive set of experiments to thoroughly evaluate POLARIS. In Sec. 5.1, we present the most intuitive reconstruction experiments. Sec. 5.2 discusses the performance of POLARIS on the most representative inversion-based tasks. Sec. 5.3 demonstrates the broader applicability of POLARIS across various settings. Finally, Sec. 5.4 analyzes whether the scale still admits locally optimal solutions and examines the impact of different initialization strategies. To validate the effectiveness of POLARIS (Algorithm 1), we conducted large-scale reconstruction evaluation. It serves as the most direct test of inversion quality, since it measures how well latent code reproduces the original image. We compare POLARIS with fixed-scale DDIM inversion baseline implemented in Stable Diffusion v1.5 [3], evaluating both on COCO2017 [34] and Pick-a-Pic [33]. Reconstruction fidelity was measured from 10 to 100 inversion steps. high guidance scale (set to 7.5) [24] was used during reconstruction to ensure strong conditioning on the source prompt, and we assess its performance using 8-bit MSE, LPIPS [38], PSNR [39], and SSIM [40]. As shown in Table 1, POLARIS achieves significant improvements over the baseline on both the COCO2017 and Pick-a-Pic datasets. It indicates that our approach yields superior latent representation and markedly reduces the error from inversion estimation compared to the baseline. The bottom row of Figure 1 visualizes the 100-step inversion and denoising process. Compared with the baseline, which frequently produces erroneous sampling results, our method yields better latent noise with richer semantics. Because POLARIS incurs smaller joint error during denoising and inversion, our model can maintain consistency between the background and the main subject. Moreover, as the number of steps increases, the baselines cumulative error continues to grow. In contrast, our method exhibits decreasing error trend, which robustly highlights its ability Figure 5. Qualitative comparison of POLARIS on complex image editing tasks. Compared to the baseline approach, our POLARIS demonstrates superior editing fidelity and the ability to follow complex instructions. Table 2. Quantitative comparison of editing on reconstruction, aesthetic, and preference metrics. Bold* indicates the method is enhanced with POLARIS. Underlined denotes mask-based method. Method MSE () PSNR () SSIM () LPIPS () Aesthetic score () Preference Rate (%) SAGE 266.578 23.8728 0.8409 0.1172 SAGE* 216.464 24.7769 0.8725 0.1031 737.090 15.3488 0.6057 0.3810 P2P P2P 79.239 29.1414 0.9056 0.0699 65.566 29.9640 0.9099 0.0415 P2P* 5.7392 5.7484 5.5636 5.6842 5.7315 37 63 45 55 to mitigate error accumulation. 5.2. Image Editing We evaluate our image editing method on the EditBench dataset [41], focusing on the object-changing sub-task that poses diverse object manipulation challenges. For all experiments, we use the Stable Diffusion v1.5 model with DDIM sampler over 50 inference steps. To provide comprehensive evaluation, we compare POLARIS from two complementary perspectives: (i) pipeline-level baseline, Prompt-to-Prompt [11] (mask), as described in Section 10.1, to highlight the advancement of our complete inversion-sampling pipeline presented in Algorithm 1; and (ii) latent-level baseline, SAGE [15], which uses only the inverted latent without modifying the editing pipeline, to demonstrate the superior inversion quality achieved by POLARIS. For fair comparison, the inversion guidance scale of all baseline methods is fixed at 1.0. Quantitatively, we employ the provided masks to measure background preservation, and further assess overall visual quality and human preference using the Aesthetic Score. As shown in Table 2, the quantitative results demonstrate that POLARIS significantly enhances background preservation across both baseline methods. When integrated with SAGE, our method improves the Back-PSNR by near 1.0 dB and reduces the Back-MSE by nearly 20%. similar trend is observed with P2P (Mask), where POLARIS reduces the Back-LPIPS by approximately 40% and boosts the Back-PSNR to nearly 30 dB. This substantial improvement stems from POLARISs core design: by minimizing the inversion approximation error inherent in baselines, which leads to much more consistent reconstruction of the background (should remain unchanged). While our method demonstrates excellent background fidelity, the Aesthetic Score [42], proxy for semantic correctness, does not improve to the same degree. This is intentional, as our approach is engineered to minimize approximation error rather than to directly optimize for semantic accuracy. Therefore, POLARIS is orthogonal to methods focused on semantic alignment, making it complementary module that can be combined with them to boost overall editing performance. 5.3. Extension to Other Inversion Tasks Our experimental setup evaluates four inversion-based downstream tasks: Gaussian deblurring (σ = 40.0), 8 super-resolution, central-mask inpainting, and colorization on Stable Diffusion v1.5 with DDIM scheduler. All experiments use 50 inference steps at 512512 resolution. To ensure fair comparison, we preserve the original task-specific pipelines; the only modification concerns the guidancescale scheduling. In POLARIS, both the inversion and denoising stages employ the dynamic, per-step guidance scale ωt computed by our method, whereas all baseline methods use fixed scale of 1.0 in both stages. As shown in Table 3, POLARIS consistently improves reconstruction fidelity across all inversion-based restoration tasks and baselines. Compared to fixed-scale guidance, POLARIS generally reduces MSE and increases both PSNR and SSIM across most evaluated tasks, demonstrating its effectiveness in mitigating accumulated inversion errors. The consistent gains across DPS, DDRM, and DDNM further indicate that POLARIS generalizes well across architectures and degradation types, validating its plug-and-play applicability to diverse diffusion-based restoration pipelines. Table 3. Quantitative results comparison across deblurring, super-resolution, colorization, and inpainting. Our method, POLARIS, consistently improves performance over fixed-scale guidance across various tasks and baseline models. The indicates that the Colorization task is not supported by the original DDNM architecture. Baseline Method Deblurring Super-resolution Colorization Inpainting MSE PSNR SSIM MSE PSNR SSIM MSE PSNR SSIM MSE PSNR SSIM DPS [37] Fixed scale 327.32 23.98 POLARIS 300.25 24.38 0.798 365.76 23.51 0.805 286.64 24.58 0.800 761.29 20.09 0.819 755.96 20.14 0.719 690.69 20.45 0.716 632.57 20. DDRM [35] Fixed scale 284.43 24.57 POLARIS 247.31 25.18 0.819 329.97 24.00 0.828 246.24 25.33 0.807 703.60 20.61 0.830 639.17 21.00 0.732 566.30 21.39 0.735 489.51 22.06 DDNM [36] Fixed scale 330.44 23.95 POLARIS 320.26 24.12 0.792 344.62 23.64 0.794 290.28 24.41 0.802 0.815 735.80 20.21 700.72 20.49 0.719 0. 0.738 0.746 0.712 0.714 Figure 6. Sensitivity Analysis of the Initial Guidance Scale ω0, demonstrating the robustness of POLARIS. 5.4. Ablation Study Scale initialization. To investigate the sensitivity of POLARIS to the initial guidance scale, ω0, we conduct an ablation study on 100 samples randomly selected from the COCO 2017 validation set. For each image, we perform 50-step DDIM reconstruction with using POLARIS aligned in Algorithm 1. The core of the experiment involves manually setting the initial guidance scale ω0 for the first step of the inversion process to range of predefined values: {0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 7.5, 9.0, 10.0}. For each fixed initial value, all subsequent guidance scales (ω1 through ωT 1) are computed dynamically using the Eq. (13). We then evaluate the final reconstruction fidelity using SSIM and LPIPS to assess the impact of different initializations. Optimality verification. We observe that the dynamic guidance scales computed by POLARIS typically oscillate within narrow band around 1.0 between 0 and 2. It raises critical question: Does POLARISs performance stem from its precise, error-minimizing updates at each step, or merely from the generic oscillatory behavior it exhibits? To investigate this, we design control experiment Figure 7. Comparison of the inversion effects of POLARIS and uniform random scale ω. It confirms that the precise, errorminimizing trajectory of POLARIS (red line) is crucial, not just its oscillatory behavior (blue line). where, instead of optimization, we sample the guidance scale ωt uniformly at random from the interval (0, 2) at each inversion step t. This baseline mimics the oscillatory pattern but lacks the intelligent structure of POLARIS. The entire process is conducted over 50 steps. We compare the reconstruction fidelity of POLARIS against this random sampling approach (Figure 7). This experiment investigates whether our step-wise optimization truly yields superior trajectory, or if any similar unstructured oscillation would suffice. significant performance gap would confirm that the specific, error-minimizing trajectory found by POLARIS is crucial for high-fidelity reconstruction. 6. Conclusion We introduced POLARIS, novel method that addresses the fundamental problem of approximate noise error in DDIM inversion. By reformulating the process as per-step optimization and treating the guidance scale as dynamic variable, POLARIS minimizes error at its source, thereby preventing the error propagation common in standard methods. Our experiments show this seamless, plug-and-play approach achieves substantial improvements in reconstruction fidelity and background preservation across editing tasks, with virtually no computational overhead. While POLARIS excels at object and attribute manipulation, future work will focus on extending its application to complex, non-rigid edits, known challenge for existing models. Promising future directions also include adapting the POLARIS framework for video and 3D inversion and exploring methods to stabilize its theoretically exact solution to unlock even greater fidelity, further broadening its impact."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 2 [2] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. 2, 3 [3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 2, 6, 7 [4] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021. [5] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. [6] Haozhe Jia, Wenshuo Chen, Zhihui Huang, Lei Wang, Hongru Xiao, Nanqian Jia, Keming Wu, Songning Lai, Bowen Tian, and Yutao Yue. Physics-informed representation alignment for sparse radio-map reconstruction, 2025. [7] Wenshuo Chen, Haozhe Jia, Songning Lai, Keming Wu, Hongru Xiao, Lijie Hu, and Yutao Yue. Free-t2m: Frequency enhanced text-to-motion diffusion model with consistency loss, 2025. [8] Wenshuo chen, Hongru Xiao, Erhang Zhang, Lijie Hu, Lei Wang, Mengyuan Liu, and Chen Chen. Sato: Stable textIn Proceedings of the 32nd ACM to-motion framework. International Conference on Multimedia, MM 24, page 69896997. ACM, October 2024. [9] Wenshuo Chen, Kuimou Yu, Jia Haozhe, Kaishen Yuan, Zexu Huang, Bowen Tian, Songning Lai, Hongru Xiao, Erhang Zhang, Lei Wang, and Yutao Yue. Ant: Adaptive neural temporal-aware text-to-motion model. In Proceedings of the 33rd ACM International Conference on Multimedia, MM 25, page 98529861. ACM, October 2025. [10] Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneˇs, Wenshuo Chen, Albert Ali Salah, and Itir Onal Ertugrul. Dctdiff: Intriguing properties of image generative modeling in the dct space, 2025. [11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2, 3, 7 [12] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2256022570, October 2023. 3 [13] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation, 2023. [14] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation, 2022. [15] Guillermo Gomez-Trenado, Pablo Mesejo, Oscar Cordon, and Stephane Lathuili`ere. Dont forget your inverse ddim for image editing. IEEE Computational Intelligence Magazine, 20(3):1018, August 2025. 7 [16] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Conference on Computer Vision and Pattern Recognition 2023, 2023. [17] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 8 [18] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance, 2022. [19] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. arXiv preprint arXiv:2211.13227, 2022. [20] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Transactions on Graphics, 42(4):111, July 2023. [21] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 2, 3 [22] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 2 [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. 2, [24] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real arXiv preprint images using guided diffusion models. arXiv:2211.09794, 2022. 2, 6, 8 [25] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han. Prompt tuning inversion for text-driven image editing using diffusion models, 2023. 2 [26] Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifold-constrained classifier free guidance for diffusion models, 2024. 2 [27] Hansam Cho, Jonghyun Lee, Seoung Bum Kim, Tae-Hyun Oh, and Yonghyun Jeong. Noise map guidance: Inversion with spatial context for real image editing, 2024. 2 [28] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations, 2022. 2 [29] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code, 2023. 2, 8 [30] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models, 2024. 2, 8 [31] Fangyikang Wang, Hubery Yin, Yue-Jiang Dong, Huminhao Zhu, Chao Zhang, Hanbin Zhao, Hui Qian, and Chen Li. BELM: Bidirectional explicit linear multi-step sampler for exact inversion in diffusion models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [32] Dvir Samuel, Barak Meiri, Haggai Maron, Yoad Tewel, Nir Darshan, Shai Avidan, Gal Chechik, and Rami Ben-Ari. Lightning-fast image inversion and editing for text-to-image diffusion models, 2025. 2 [33] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. 2, 6, 8 [34] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015. 2, 6, 8 [35] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In Advances in Neural Information Processing Systems, 2022. 3, 8, 7 [36] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. The Eleventh International Conference on Learning Representations, 2023. 3, 8, 7 [37] Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems, 2024. 3, 8, 7 [38] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. 6, [39] Fernando A. Fardo, Victor H. Conforto, Francisco C. de Oliveira, and Paulo S. Rodrigues. formal evaluation of psnr as quality measurement parameter for image segmentation algorithms, 2016. 6, 9 [40] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. from error visibility to strucIEEE Transactions on Image Processing, Image quality assessment: tural similarity. 13(4):600612, 2004. 6, 9 [41] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi PontTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, and William Chan. Imagen editor and editbench: Advancing and evaluating textguided image inpainting, 2023. 7 [42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. 7, 9 POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Theoretical Analysis 7.1. Theorem 1: Inherent Flaws of the Exact Solution Proof. Let the noise prediction at any step be the sum of the true signal and perturbation term: The main computational terms in the exact solution are thus perturbed as follows: εc(xt) = ε (xt) + δc(xt). Let = (1 ωt)εφ + ωtεc, = εc(xt1) εφ(xt1), = + δa, = + δb. ωobs = (a + δa)(b + δb) + δb2 , ωtrue = b b2 , and the error be E(ω) = ωobs ωtrue. Then: (I) Full rational form of the error. Combining the terms over common denominator yields E(ω) = (a b)δb2 + 2(a b)(b δb) (a δb)b2 (δa b)b2 (δa δb)b2 b2 + δb2 . (14) (II) Limit analysis and divergence. Consider the parameterized path = tu (where = 1, 0+), and let E(t) = (t)/D(t). Expanding the numerator and denominator from (14) gives (t) = t(a u)δb2 + 2t2(a u)(uδb) t2(a δb) t3(δa u) t2(δa δb), D(t) = tu2 tu + δb2 = t4 + 2t3(uδb) + t2δb2. As 0+, this is an indeterminate form of 0/0. Applying LHˆopitals rule: (t) = (a u)δb2 + 4t(a u)(uδb) 2t(a δb) 3t2(δa u) 2t(δa δb), i. If (a u) = 0, then D(t) = 4t3 + 6t2(uδb) + 2tδb2. lim t0+ (t) = (a u)δb2 = 0, D(t) = 0, lim t0+ and therefore limt0+ E(t) = , meaning the error diverges. ii. If (a u) = 0, then limt0+ (t) = limt0+ D(t) = 0. Applying LHˆopitals rule second time: (t) = 4(a u)(uδb) 2(a δb) 6t(δa u) 2(δa δb), D(t) = 12t2 + 12t(uδb) + 2δb2. Taking the limit under the condition that (a u) = 0 yields lim t0+ E(t) = 2(a δb) 2(δa δb) 2δb2 = (a δb) + (δa δb) δb2 . If we further assume δb 0, then from (14), we find that E(t) (δa u)/t, which diverges. Hence, the exact solution is ill-conditioned as 0; the error magnitude grows proportionally to b1. 7.2. Theorem 2: Inherent Advantages of the Approximate Solution Proof. Let the true signals be ε , and the noisy signals be φ, ε εφ = ε φ + δφ, εc = ε + δc,"
        },
        {
            "title": "The true and observed values of the approximate solution are defined as",
            "content": "ωtrue = ε φ2 ε φ ε ε φ ε 2 , ωobs = εφ2 εφ εc εφ εc ."
        },
        {
            "title": "Let",
            "content": "N (εφ, εc) = εφ2 εφ εc, D(εφ, εc) = εφ εc2. Denote by and the values for the true signals, respectively. Then Nobs = + δN + O(δ2), Dobs = + δD + O(δ2), where δN , δD are the first-order linear perturbation terms with respect to the noise. The error can be written as E(ω) = NobsD Dobs DobsD = δN δD + O(δ2) DobsD . We assume that the true signals are well-separated in the latent space, i.e., there exists positive constant η > 0 such that ε φ ε η. The well-conditioning assumption directly provides lower bound for D: = ε φ ε 2 η2. For Dobs, the reverse triangle inequality gives: Dobs = (ε φ ε ) + (δφ δc)2 (ε φ ε δφ δc)2 (η δφ δc)2. Assuming the noise is sufficiently small (e.g., δφ δc η/2), we have Dobs (cid:17)2 (cid:16) η 2 = η2 4 . Thus, the lower bound for the entire denominator is: DobsD (cid:19) (cid:18) η2 4 η2 = η4 . Since δN , δD = O(δ), the first-order term in the numerator is bounded: δN δD C1δ, where C1 is constant that depends only on the norms of the true signals. Combining the upper bound of the numerator and the lower bound of the denominator, we obtain the final error bound: E(ω) C1δ + O(δ2) η4/4 = 4C1 η4 δ + O(δ2) = O(δ). Therefore, the approximate solution is first-order Lipschitz continuous map from the noisy input to the output error, meaning the computational error is on the same order as the input noise. 8. Generalized POLARIS under Score and Flow based Parameterizations Consider the probability flow ODE with field ψθ(x, t). For noise, score, or velocity, the inverse step is xi = xi+1 ψθ(xi+1, ti+1). POLARIS optimizes ωt to minimize the error τ (t)2 2. 8.1. Unified POLARIS Update Rule Let ψ(t) and ψc(t) denote the changes in unconditional and conditional predictions along the inversion trajectory. The error is approximated as τ (t) = (1 ωt)ψ(t) + ωtψc(t). Minimizing τ (t)2 2 yields unified closed-form update: ω = ψ 2 ψ ψ ψc2 2 ψc . (15) We now instantiate this generic rule for two specific parameterizations: 1. Score-based parameterization. The network predicts the score sθ(x, t) ϵθ(x, t)/σt. Setting ψ = s, Eq. (15) minimizes the error in the score space. Since σt 0 as 0, the metric penalizes errors in the final generation steps. 2. Flow-based parameterization. The network predicts velocity vθ(x, t) = αtϵθ(x, t) + βtxt. By setting ψ = v, Eq. (15) minimizes the error in the velocity space. This space offers straighter, more numerically stable trajectory for optimization. 8.2. Invariance of Fixed-Scale Baseline While POLARIS yields different trajectories depending on the chosen space (due to the metric change in the norm 2 prove that the standard fixed-scale CFG baseline remains invariant. 2), we Theorem 3. For fixed scale ω, the generative step is identical across Noise, Score, and Flow parameterizations. Proof. Both Score and Flow relate to the noise ϵ via time-dependent affine transformation Tt(): For Score, at 1/σt, bt = 0. For Flow, at = αt, bt = 1 αtxt. Applying CFG in the transformed space ψ: ψt = atϵt + bt. ψguided = ψ + ω(ψc ψ) (atϵguided + bt) = (atϵ + bt) + ω [(atϵc + bt) (atϵ + bt)] . The translation term bt cancels out in the subtraction. Dividing by the scalar at, we recover the noise-space update: Thus, the baseline trajectory is mathematically independent of the parameterization space. ϵguided = ϵ + ω(ϵc ϵ). (16) (17) (18) 8.3. Experimental Validation We evaluate POLARIS on COCO 2017 and Pick-a-Pic (100 samples each) by transforming SD v1.5 outputs into score and velocity spaces. Using 50 DDIM steps and replayed ωt, Table 4 confirms effectiveness across parameterizations, with the Score-based variant achieving the highest fidelity. Table 4. Quantitative comparison of POLARIS variants under Score and Flow parameterizations. Dataset Paradigm MSE () PSNR () SSIM () LPIPS () COCO 2017 Pick-a-Pic Score-based Flow-based Score-based Flow-based 333.64 389.72 235.03 311. 23.74 23.24 25.59 25.14 0.7353 0.7234 0.8262 0.8193 0.1402 0.1583 0.0758 0. 9. Additional Experimental Results 9.1. Computational Efficiency We conduct rigorous efficiency benchmark in Tab. 5 using 50 inference steps on single NVIDIA RTX 4090. Optimization-based methods like Null-Text Inversion achieve high fidelity but suffer from prohibitive latency, rendering them unsuitable for interactive applications. Similarly, EDICT doubles the computational load. In contrast, POLARIS operates as lightweight module. It boosts the DDIM baseline PSNR by +9.39 dB while incurring only 3% overhead. Notably, it outperforms other optimization-free approaches like Negative-Prompt and Direct Inversion in speed, offering the best fidelity-latency trade-off. Table 5. Efficiency Comparison. Method PSNR Time (s) DDIM inversion Null-Text inversion Negative-prompt inversion EDICT Direct inversion POLARIS(Ours) 14.10 26.11 23.38 26.53 26.32 23.49 7.71 135.48 8.32 16.21 12. 7.96 Scalability analysis. To further validate the negligible overhead of POLARIS, we report the runtime across varying inference steps (from 10 to 100) in Tab. 6. As shown, the additional time introduced by POLARIS averages only 3.3% compared to the fixed-scale baseline, confirming its scalability and efficiency. Table 6. Runtime comparison (s) between Fixed Scale and POLARIS across different inference steps. Method 10 30 40 50 60 70 90 100 Fixed scale POLARIS 1.6876 1.7113 3.1628 3.2435 4.6906 4. 6.2423 6.3940 7.7075 7.9618 9.2745 9.5557 10.5553 11.1370 12.3849 12.6824 13.7852 14. 15.0787 15.9283 9.2. Image Reconstruction As control experiment, we further conducted reconstruction tests under standard DDIM, where both inversion and sampling guidance scales were fixed to ω 1. Using the same Stable Diffusion v1.5 backbone, we evaluated reconstruction performance from 10 to 100 inference steps at intervals of 10. The results show that this baseline consistently underperforms POLARIS across all metrics and step counts. Table 7. Reconstruction Metrics Comparison on COCO and Pick-a-Pic Datasets with reconstruction scale = 1 as contrast. Metric Dataset 10 20 30 40 50 70 80 90 100 MSE () COCO2017 621.45 612.81 605.93 594.17 585.22 573.99 568.14 560.29 555.71 552.01 672.81 661.90 650.15 638.49 627.31 615.82 610.93 604.18 598.22 594.19 Pick-a-Pic PSNR () COCO2017 Pick-a-Pic 19.85 18.99 19.98 19.11 20.07 19.23 20.21 19. 20.35 19.47 20.48 19.59 20.59 19.66 20.68 19.74 20.75 19.82 20.81 19. SSIM () COCO2017 0.5011 0.5188 0.5290 0.5381 0.5453 0.5512 0.5549 0.5580 0.5601 0.5615 0.4123 0.4297 0.4411 0.4503 0.4588 0.4654 0.4688 0.4719 0.4740 0.4752 Pick-a-Pic LPIPS () COCO2017 0.2144 0.1981 0.1905 0.1842 0.1798 0.1743 0.1712 0.1689 0.1670 0.1659 0.2691 0.2553 0.2478 0.2411 0.2350 0.2298 0.2265 0.2231 0.2209 0.2190 Pick-a-Pic 9.3. Ablations on Sampling Schedulers Table 8 compares POLARIS with four alternative guidance schedules on COCO2017 using Stable Diffusion v1.5 under identical settings: the forward schedule (ωt), reverse schedule (ωT t+1), cosine decay (COSINE ) from 1 to 0, and fixedscale baseline (ω 1). Across 10 - 100 inference steps, POLARIS consistently attains the lowest MSE and LPIPS and the highest PSNR and SSIM, indicating superior reconstruction fidelity and structural preservation, while the fixed-scale baseline performs substantially worse, underscoring the importance of dynamic guidance scaling in both inversion and sampling. Table 8. Performance comparison of POLARISs and other sampling strategies across various inference steps on COCO2017. Dataset Metric Schedulers 10 30 40 50 60 70 90 COCO2017 MSE () PSNR () SSIM () LPIPS () ωT t+1 ωt ω 1 COSINE ωT t+1 ωt ω 1 COSINE ωT t+1 ωt ω 1 COSINE ωT t+1 ωt ω 1 COSINE 100 476 617 1138 805 22.55 21.22 18.57 20.06 506 688 1254 891 22.04 20.75 18.15 19. 498 651 1198 845 22.24 20.99 18.34 19.86 490 635 1153 822 22.36 21.11 18.51 19.98 493 639 1160 829 22.36 21.08 18.48 19. 498 642 1171 833 22.34 21.05 18.43 19.92 467 610 1120 798 22.62 21.28 18.64 20.11 477 615 1135 804 22.54 21.23 18.58 20. 464 608 1115 790 22.66 21.30 18.66 20.16 478 619 1142 810 22.53 21.21 18.55 20.04 0.6922 0.7043 0.7085 0.7087 0.7088 0.7135 0.7122 0.7143 0.7126 0.7133 0.6615 0.6730 0.6781 0.6779 0.6780 0.6831 0.6820 0.6845 0.6822 0.6830 0.5890 0.5995 0.6044 0.6039 0.6041 0.6088 0.6075 0.6101 0.6077 0.6085 0.6251 0.6368 0.6415 0.6410 0.6412 0.6466 0.6452 0.6480 0.6455 0.6463 0.2231 0.2039 0.1957 0.1950 0.1955 0.1870 0.1897 0.1862 0.1892 0.1886 0.2685 0.2451 0.2358 0.2349 0.2352 0.2255 0.2281 0.2240 0.2275 0.2268 0.4102 0.3855 0.3710 0.3701 0.3715 0.3601 0.3640 0.3588 0.3633 0.3621 0.3350 0.3106 0.2988 0.2975 0.2981 0.2870 0.2905 0.2855 0.2899 0. 9.4. Scaling POLARIS to Larger Diffusion Models We further extend POLARIS to Stable Diffusion XL, demonstrating that our method is not restricted to SD 1.5 but also scales effectively to larger, higher-capacity diffusion models. As shown in Table 9, POLARIS achieves even better results on this larger-parameter backbone, further confirming its scalability to large scale models. Table 9. Performance comparison of POLARIS and Fixed scale across various inference steps on Pick-a-Pic and COCO2017 in SDXL. Dataset Metric Method 10 20 30 40 50 70 80 90 100 COCO2017 Pick-a-Pic MSE () PSNR () Fixed scale POLARIS Fixed scale POLARIS 1558 479 16.52 21. 1932 435 15.58 22.54 2092 401 15.27 23.03 2348 385 14.76 23. 2436 366 14.60 23.61 2013 292 15.43 24.64 2316 310 14.82 24. 2071 258 15.32 25.18 2516 274 14.45 25.03 2668 283 14.19 24. SSIM () Fixed scale 0.5950 0.5856 0.5744 0.5575 0.5500 0.5711 0.5527 0.5661 0.5405 0.5322 0.7113 0.7478 0.7574 0.7651 0.7698 0.7854 0.7828 0.7967 0.7950 0.7933 POLARIS LPIPS () Fixed scale 0.4636 0.4914 0.5050 0.5205 0.5279 0.5004 0.5207 0.5053 0.5354 0.5467 0.2832 0.2568 0.2489 0.2366 0.2293 0.2021 0.2054 0.1815 0.1840 0.1872 POLARIS MSE () PSNR () Fixed scale POLARIS Fixed scale POLARIS 1149 309 18.07 24.01 1499 342 16.84 24. 1604 326 16.60 24.59 1842 337 16.01 24.60 1928 326 15.81 24. 1643 269 16.53 25.88 1868 268 15.96 25.88 1742 255 16.26 26. 2068 252 15.49 25.98 2214 240 15.21 26.30 SSIM () Fixed scale 0.6884 0.6735 0.6695 0.6530 0.6455 0.6628 0.6462 0.6538 0.6305 0.6222 0.8014 0.8181 0.8297 0.8323 0.8359 0.8478 0.8523 0.8552 0.8579 0.8625 POLARIS LPIPS () Fixed scale 0.3306 0.3745 0.3778 0.4002 0.4080 0.3846 0.4032 0.3928 0.4247 0.4360 0.1673 0.1745 0.1641 0.1636 0.1557 0.1353 0.1298 0.1241 0.1226 0.1150 POLARIS 10. Experimental Details 10.1. Baselines DDRM. [35] Denoising Diffusion Restoration Models formulates solution to general linear inverse problems without requiring task-specific training. The key insight is to treat the pre-trained diffusion model as powerful generative prior. At each reverse diffusion step, it first predicts clean image and then applies correction term derived from the measurement error. This iterative projection ensures that the final output is consistent with the given measurements, such as blurred or downsampled image. We use correction strength of η = 0.1. DDNM. [36] Denoising Diffusion Null-space Models offers highly efficient, analytical approach by leveraging the properties of linear transformations. the range-space, where the measurement provides deterministic information, and the null-space, where information is lost. DDNM uses the measurement to perfectly reconstruct the range-space component and employs the diffusion model exclusively to generate plausible details for the null-space component. This separation prevents the model from hallucinating artifacts in data-constrained regions. It decomposes the problem space into two orthogonal subspaces: DPS. [37] Diffusion Posterior Sampling introduces flexible, gradient-based guidance mechanism applicable to both linear and non-linear inverse problems. It interprets the reverse diffusion process as form of posterior sampling and guides the sampling trajectory using the gradient of likelihood function. In practice, this is implemented by computing the gradient of consistency loss function that measures how well the current prediction satisfies the problems constraints. This gradient is then used to perturb the denoised sample at each step, steering it towards high-likelihood solution. The guidance strength is set to λ = 0.2. P2P. [11] Prompt-to-Prompt is seminal zero-shot method for text-guided image editing that requires no model training or fine-tuning. Its key insight is that the cross-attention maps generated during the denoising process implicitly control the spatial layout and structure of the synthesized image. The method enables editing by injecting the attention maps from source images generation process into the generation process of target prompt. This ensures that the resulting image retains the structure of the original while incorporating the semantic changes from the new prompt. P2P Mask. We introduce P2P-Mask, dual-pipeline approach for high-fidelity, zero-shot real image editing. The process initiates with image inversion to encode the source image into its initial noisy latent representation, for which we compare two strategies: standard fixed scale DDIM inversion and our POLARIS method, which calculates an optimal guidance weight at each step. In the subsequent spatially-aware generation phase, we reconstruct the image using the target prompt. The core of this phase leverages cross-attention maps to dynamically generate soft spatial mask at each denoising step by analyzing the relative attention between edit tokens and preserve tokens. This mask is then used to spatially modulate the guidance strength, applying strong creative guidance to the edit regions while imposing matching reconstruction scale, correspondingly fixed or dynamic based on the inversion method, to the background. This design ensures that edits are precisely confined to the target area while maximizing the reconstruction fidelity of non-edited regions. SAGE. [15] Self-Attention Guidance for image Editing is novel method for text-guided image editing that introduces guidance mechanism based on self-attention maps. The core insight is that the self-attention maps generated during the inverse process contain rich structural and contextual information about the source image. SAGE leverages this by first performing DDIM inversion with the source prompt (cs) to record these intermediate self-attention maps. Subsequently, during the forward sampling process with the target prompt (ct), it enforces consistency by minimizing loss between the newly generated self-attention maps and the recorded ones. This allows for high-fidelity preservation of unedited regions without requiring explicit or costly reconstruction steps, striking an effective balance between editing accuracy and structural integrity. Stable Diffusion v1.5. [3] All of our experiments are built upon Stable Diffusion v1.5, powerful, publicly available latent diffusion model. It operates in compressed latent space for computational efficiency and uses frozen CLIP ViT-L/14 text encoder to interpret text prompts. The models UNet architecture is conditioned on these text embeddings via cross-attention, enabling it to generate high-fidelity images from vast range of textual descriptions. We use the official v1-5 checkpoint as the foundational backbone for all compared methods. Stable Diffusion XL. SDXL represents significant evolution in latent diffusion models, featuring substantially larger UNet architecture with approximately 2.6 billion parameters three times the size of SD v1.5. Unlike its predecessor, SDXL employs dual-text encoder system, combining CLIP ViT-L and OpenCLIP ViT-G/14, to capture both high level semantics and fine-grained details. It is trained natively at 1024 1024 resolution and utilizes micro-conditioning to handle varying aspect ratios. We incorporate SDXL into our experiments to validate the scalability of POLARIS and its effectiveness on sota, high-capacity foundation models. Null-Text Inversion. [24] It is an optimization-based technique designed to enable accurate reconstruction for real image editing. Recognizing that CFG amplifies inversion errors, it modifies the unconditional textual embedding (the null text) rather than the latent code. At each timestep of the inversion process, it performs an iterative optimization to find specific null embedding that steers the denoising trajectory back to the original latent state. This allows for high-fidelity reconstruction but incurs significant computational cost due to the per-step optimization loop. Negative-Prompt Inversion. [30] It offers more efficient alternative to heavy optimization methods. Instead of optimizing the null embedding at every single timestep independently, it seeks to optimize the negative prompt embedding to absorb the inversion error. By shifting the discrepancy caused by CFG into the negative prompt space, it allows the forward process to reconstruct the image using the standard sampling mechanism. This approach aims to maintain the editability of the positive prompt while correcting the trajectory drift, often achieving better balance between reconstruction quality and inference speed compared to full per-step optimization. Direct Inversion. [29] It proposes straightforward, one-shot solution to the reconstruction problem without iterative optimization. It acknowledges that the standard DDIM inversion path and the generation path diverge due to CFG. To bridge this gap, it explicitly records the difference (the error maps) between the inversion noise predictions and the reconstruction requirements at each step. During the editing phase, these recorded error maps are injected back into the diffusion process, forcing the trajectory to align with the source images spatial layout while allowing the target prompt to alter the semantics. EDICT. [17] Exact Diffusion Inversion via Coupled Transformations fundamentally resolves the non-invertibility of standard diffusion steps by redesigning the sampling process. It employs dual-variable formulation, maintaining two coupled latent states that are updated in an alternating fashion using affine coupling layers. This architecture renders the diffusion process mathematically bijective, enabling theoretically perfect reconstruction of the source image without any optimization or approximation, albeit at the cost of doubling the memory and computational requirements for maintaining the dual states. 10.2. Datasets Our method is evaluated on two standard and challenging benchmarks to assess its performance across diverse scenarios. MS-COCO 2017. [34] The Microsoft Common Objects in Context 2017 validation set is large-scale dataset widely used for benchmarking image generation and understanding tasks. It features diverse and complex everyday scenes, with each image annotated with five distinct, human-generated captions. This rich descriptive variance makes it an ideal benchmark for evaluating the robustness and generalization capabilities of text-guided image editing models, testing their ability to parse complex prompts and preserve background details. Pick-a-Pic. [33] Pick-a-Pic is large-scale dataset specifically designed to benchmark text-to-image models based on human preferences. It contains vast collection of text prompts, each paired with multiple generated image candidates, from which human annotators have selected the one that best aligns with the prompts intent and aesthetic quality. We utilize this dataset to evaluate our methods ability to produce high-quality and semantically accurate edits that align with user expectations, providing more nuanced assessment of performance beyond automated metrics. 10.3. Metrics To quantitatively evaluate the performance of our method and the baselines, we employ comprehensive set of metrics that assess different aspects of image quality, including pixel-level fidelity, perceptual similarity, and aesthetic appeal. Given ground truth image and generated image K, both of size n, the metrics are defined as follows: MSE. Mean Squared Error is fundamental metric for pixel-level fidelity. It computes the average squared difference between corresponding pixel values. All calculations are performed in the 8-bit integer domain [0, 255]. lower MSE value signifies more accurate pixel-wise reconstruction. MSE = 1 mn m1 (cid:88) n1 (cid:88) [I(i, j) K(i, j)] i=0 j=0 (19) PSNR. [39] Peak Signal-to-Noise Ratio is widely used metric for reconstruction quality, defined via the MSE. It measures the ratio between the maximum possible power of signal and the power of the corrupting noise. Expressed on logarithmic decibel (dB) scale, higher PSNR value indicates higher-quality reconstruction. PSNR = 10 log10 (cid:19) (cid:18) MAX2 MSE (20) where MAXI is the maximum possible pixel value, which is 255 for 8-bit images. SSIM. [40] Structural Similarity Index Measure evaluates perceptual similarity by assessing the degradation of structural information. Unlike pixel-based metrics, SSIM compares local patterns based on luminance, contrast, and structure. For two image windows and y, it is defined as: SSIM(x, y) = (2µxµy + C1)(2σxy + C2) + µ + C1)(σ2 + σ2 + C2) (µ2 (21) where µ is the mean, σ2 is the variance, σxy is the covariance, and C1, C2 are stabilization constants. The final SSIM is the mean over all windows. value closer to 1 indicates perfect structural similarity. LPIPS. [38] Learned Perceptual Image Patch Similarity is an excellent perceptual metric that better correlates with human judgment. It computes the distance between deep features of two image patches, the test image and the original image x0, extracted from pre-trained deep neural network. The distance is calculated as: LPIPS(x, x0) = (cid:88) wl HlWl (cid:88) h,w (cid:13) (cid:13)ˆyl hw ˆyl 0,hw (cid:13) 2 (cid:13) 2 (22) where ˆyl, ˆyl lower LPIPS score indicates that two images are more perceptually similar. 0 are the unit-normalized feature activations from layer of the network, scaled by layer-specific weights wl. AES. [42] The Aesthetic Score is used to evaluate the overall visual appeal of the generated images. This score is derived from pre-trained model, often trained on large-scale datasets with human aesthetic ratings. Unlike fidelity metrics, AES quantifies the subjective quality and artistic merit of the output. higher score suggests more aesthetically pleasing and visually coherent image. Preference rate. To calculate the preference rate, we presented participants with the source instruction and randomized pair of outputs (ours vs. baseline). Users were asked to select the result with better visual fidelity and instruction alignment. [Question]: Please compare the two edited images below based on the instruction. <image pairs.png> (1) Image has better visual fidelity and instruction alignment. (2) Image has better visual fidelity and instruction alignment. (3) Both images are of similar quality / Hard to tell. 11. Additional Visualizations Figs. 89 present additional qualitative results on text-guided image editing, showing that our method preserves background details while performing complex semantic modifications. Figs. 1013 further demonstrate its effectiveness across deblurring, super-resolution, inpainting, and colorization tasks. Note: The Degraded columns contain decoded initial latents, rather than pixel-space degradations, to reveal the true starting point of the reverse diffusion and the challenges inherent to the latent space. Figure 8. Editing Visualization. Figure 9. Editing Visualization. Figure 10. Gaussian Deblurring visualizations Figure 11. 8 Super-Resolution visualizations Figure 12. Central-mask Inpainting visualizations Figure 13. Image Colorization visualizations"
        }
    ],
    "affiliations": [
        "Data61/CSIRO",
        "Griffith University",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}