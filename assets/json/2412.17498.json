{
    "paper_title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
    "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Yunlong Liang",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT-o1, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to judge whether the translation in the current round is better than the previous one or not. In this manner, we collect tens of thousands of long-thought MT data, which is used to train our DRT-o1. The experimental results on literature translation demonstrate the effectiveness of the DRT-o1. Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by DRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its effectiveness. The project is available at https://github.com/krystalan/DRT-o1"
        },
        {
            "title": "Start",
            "content": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought Jiaan Wang, Fandong Meng*, Yunlong Liang, Jie Zhou Pattern Recognition Center, WeChat AI, Tencent Inc {torchwang,fandongmeng,yunlonliang,withtomzhou}@tencent.com DRT-o1 DRT-o1-7B DRT-o1-14B 4 2 0 2 3 ] . [ 1 8 9 4 7 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT-o1, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop multi-agent framework to translate these sentences via long thought. In the multi-agent framework, translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to judge whether the translation in the current round is better than the previous one or not. In this manner, we collect tens of thousands of long-thought MT data, which is used to train our DRT-o1. The experimental results on literature translation demonstrate the effectiveness of the DRT-o1. Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by DRT-o1 achieves 7.338.26 BLEU and 1.663.36 CometScore. Besides, DRT-o1-7B can outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its effectiveness."
        },
        {
            "title": "Introduction",
            "content": "Recently, the emergence of the O1-like models shows great performance in reasoning tasks, e.g., * Corresponding author. math and coding tasks (OpenAI, 2024b; Qin et al., 2024; Huang et al., 2024; Zhang et al., 2024; Zhao et al., 2024). With the help of long thought, LLMs tend to explore, reflect and self-improve the reasoning processes to achieve more accurate answers. In this paper, we introduce DRT-o1 that aims to bring the success of long thought to neural machine translation (MT). There are two key points in achieving this goal: i) suitable translation scenario to employ long thought in MT: Not all scenarios require long thought during translation. For example, in simple expressions, literal translation can meet most needs, and translation via long thought may be unnecessary. ii) method to synthesize MT data with long thought: Long thought SFT (supervised finetuning) data plays vital role in simulating LLMs long thought ability (Huang et al., 2024). Previous work typically pays much attention to how to synthesize long-thought SFT data in math and coding tasks (Qin et al., 2024; Huang et al., 2024; Zhao et al., 2024). For i), inspired by Van den Broeck (1981), possible scenario is translating sentences with similes or metaphors, where literal translation often fails to convey the intended semantics. Given that, we decide to mine such sentences from literature books. The mining process uses an advanced large language model (LLM) to first judge Q1: whether each literature sentence has any similes or metaphors. If has, the LLM will be asked to literally translate the sentence to target language, and give final judgment on Q2: whether literal translation is effective for native speakers of the target language to comprehend. If the answers of Q1 and Q2 are yes and no, respectively, the corresponding literature sentences will be reserved, and regarded as suitable to translate via long thought. For ii), after collecting the literal sentences with similes or metaphors, the next question is how to synthesize long thought MT samples. Previous work typically utilizes Monte Carlo Tree Search (MCTS) (Qin et al., 2024; Zhao et al., 2024; Zhang et al., 2024) or data distillation (Huang et al., 2024) (from existing O1-like models) to collect long thought SFT samples. Nevertheless, MCTS is typically used in math and coding tasks where multiple reasoning behaviors should be considered, and the method emphasizes complex reasoning that might not be efficient for machine translation. Besides, utilizing existing O1-like models for data distillation might (1) constrain the potential quality of the long-thought data; and (2) have data gap in MT since current O1-like models are typically optimized toward math and coding tasks. Therefore, we propose multi-agent framework to synthesize MT data with long thought. In detail, there are three agents in the framework, i.e., translator, an advisor and an evaluator. The synthesis process is iterative, consisting of the following three steps during each iteration: (1) the translator generates new translation conditioned on the previous steps translation and the corresponding refinement suggestions from the advisor; (2) the advisor evaluates the current translation and offers detailed feedback; (3) the evaluator assesses the current translation and gives an evaluation score using predefined scoring criteria. Once the translation score provided by the evaluator reaches pre-defined threshold or the number of iterations reaches maximum value, the iteration will stop. After that, the translation and suggestions in every step could form the long-thought MT samples. To improve the readability and fluency of the longthought data, we employ GPT-4o (OpenAI, 2024a) to reformulate the long-thought content. Based on the collected long-thought MT samples, we train (SFT) our DRT-o1-7B and DRT-o114B using the backbones of Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct (Qwen Team, 2024), respectively. Experimental results on literature translation demonstrate the effectiveness of the DRT-o1. For example, DRT-o1-7B outperforms Qwen2.57B-Instruct by 8.26 BLEU, 1.31 CometKiwi and 3.36 CometScore. It can also outperform QwQ32B-Preview by 7.82 BLEU and 1.46 CometScore. Our main contributions are concluded as follows: We propose DRT-o1 aiming at building LLMs with long-thought machine translation ability. To achieve this, we mine literature sentences with similes or metaphors, and collect MT samples with long-thought processes. To synthesize the long-thought MT samples, we propose multi-agent framework that involves translator, an advisor and an evaluator. These three agents collaborate in an iterative manner to produce long thoughts during MT. Lastly, GPT4o is used to further improve the quality of the synthetic long-thought MT samples. Experimental results on literature translation verify the effectiveness of our DRT-o1. With the help of long thought, LLMs can learn to think during the machine translation."
        },
        {
            "title": "2 DRT-o1 Data",
            "content": "We focus on English-to-Chinese translation, and we introduce how to collect DRT-o1 training data via three steps in this section: (1) collecting English sentences that tend to require long thoughts during translation ( 2.1); (2) synthesizing the long-thought translation process for the collected sentences by designed multi-agent framework ( 2.2); (3) improving the readability and fluency of the long-thought content to form the final longthought MT samples ( 2.3). In the end, we provide data statistics of the collected data to give deeper understanding ( 2.4). 2.1 Literature Book Mining Following Kryscinski et al. (2022), we leverage the literature books from the Project Gutenberg publicdomain book repository1, where the books are typically more than fifty years old and their copyrights have expired. About 400 English books are used to mine sentences with similes or metaphors. First, we extract all sentences from these books, and filter out too short or too long sentences, i.e., less than 10 words or more than 100 words, resulting in 577.6K literature sentences. Second, for each sentence, we use Qwen2.5-72B-Instruct (Qwen Team, 2024) to judge whether the sentence involves similes or metaphors, and discard the sentences that do not contain any ones. Third, for the remaining sentences, we let Qwen2.5-72B-Instruct literally translate them to Chinese, and then judge whether the translation satisfies native Chinese people. If the answer is negative, the corresponding sentence will be reserved, and regarded as suitable to translate via long thought. In this manner, we finally collect 63K (out of 577.6K) literature sentences involving similes or metaphors whose literal 1https://www.gutenberg.org/ Figure 1: The illustration of the multi-agent framework to synthesize long-thought MT samples. (a) translator iteratively produces translations under the suggestions provided by an advisor; (b) An advisor reviews the translation results and gives suggestions; (c) An evaluator assesses the translation results and gives an overall score to indicate the translation quality. translations also have flaws, called pre-collected sentences. 2.2 Multi-Agent Framework 1 , wsrc 1 , wtgt 2 , ..., wsrc 2 , ..., wtgt }, where wsrc For each pre-collected sentence (denoted as s), we design multi-agent framework to translate it from English to Chinese via long thought. As shown in Figure 1, our multi-agent framework includes three agents: translator, an advisor, and an evaluator. The synthetic process is illustrated as follows: (1) Word-level Translation. The translator first identifies the keywords that lie in the sentence, and then provides their translations under the consideration of the context. The keywords are denoted as src = {wsrc indicates the i-th keyword in s, and is the number of keywords. The translation of keywords is denoted as tgt = {wtgt }. This step enables the model to identify potential challenges in translating the entire sentence by breaking it down into sub-problems (i.e., word-level translation). (2) Preliminary Translation. The translator then provides preliminary sentence translation (t0) conditioned on both the source sentence (s) and its keyword bilingual pairs (W src, tgt). (3) Translation Refine Loop. In the refine loop, three agents work together to refine the translation iteratively. In each iteration step (start from = 1), the advisor first evaluates the translation in the previous step, i.e., tk1, and provides detailed feedback k1 for polishing it. Then, the evaluator gives an overall score of tk1 conditioned on both pre-defined scoring criteria and k1, the score is denoted as sk1. In the last of the iteration step, the translator takes its previous translation tk1, the corresponding feedback k1 and overall score sk1 into account to provide new translation tk. The translation refine loop will stop when the overall score reaches pre-defined threshold or the number of iteration steps meets the maximum. 2.3 Long Thought Reformulation After the multi-agent collaboration, we obtain long thought process: P(s) : src, tgt t0, 0, s0 t1, 1, s1 ... tm, m, sm (1) where P(s) denotes the multi-agent thought process for s, and is the number of iteration steps. To emphasize the valid thought process, translations without score change will be removed. That is, if si is equal to si1 (i = 1, 2, ..., m), we will discard ti, i, si in P(s), resulting in: (s) : src, tgt t0, 0, s0 tr1, r1, sr1 ... trn, rn, srn (2) where 1 r1 < r2 < ... < rn m, and is the number of remaining steps. If < 3, we will discard the whole sample, i.e., P(s) / (s). For the remaining samples, we follow Qin et al. (2024), and leverage GPT-4o (OpenAI, 2024a) to modify and polish (s) into self-reflection description. Finally, we obtain 22,264 machine translation samples with long thought. Figure 2 gives an example sample to illustrate the synthetic results. Figure 2: An example of long thought synthesized by the designed multi-agent framework and GPT-4o reformulation. # Sample Query Thought Output o1-journey Marco-O1 CoT data DRT-o1 data (training) DRT-o1 data (validation) DRT-o1 data (testing) 327 10, 19,264 1,000 2,000 41.53 52.73 37.25 37.43 37.19 486.05 673.98 527.64 531.36 525.44 3.41 52. 44.67 44.98 44.70 Table 1: The number of samples and average tokenlevel length of query, thought and output. Query and Output in DRT-o1 data mean the source sentences and the translated outputs, respectively. 2.4 Data Statistics We split the collected 22,264 samples into training, validation and testing sets with 19,264, 1,000 and 2,000 samples, respectively. Table 1 shows the data statistics of DRT-o1 data and previous O1like data. For Marco-O1 CoT data (Zhao et al., 2024), since it is not fully released, we use its demo data to calculate the data statistics. As we can see, the average number of tokens in our synthesized thought reaches 500+ tokens, which is similar to previous math-oriented O1-like CoT data."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setups Metrics. Following previous work, we adopt BLEU (Papineni et al., 2002), reference-free CometKiwi and reference-based CometScore (Rei et al., 2022) to evaluate the model translations. BLEU evaluates n-grams overlap between the generated translations and corresponding references, while CometScore evaluates the semantic similarity of translations against references. CometKiwi uses language model to judge whether translation conveys the semantics of the source sentence. To calculate CometKiwi and CometScore, we leverage the official codes2 and the official models3. To calculate the BLEU score, we use the sacrebleu toolkit4 to calulate the corpus-level BLEU. Backbones. We adopt Qwen2.5-7B-Instruct5 and Qwen2.5-14B-Instruct6 (Yang et al., 2024) as the backbones of DRT-o1. Implementation Details. Llama-Factory (Zheng et al., 2024) is used to instruct-tune LLMs. Following Wang et al. (2024), all LLMs are tuned on 8NVIDIA A100 GPUs (40G) with 1e-5 learning rate and 32 (84) batch size. We use the DeepSpeed optimization (Rasley et al., 2020), and set ZeRO-3 optimization. Following Qin et al. (2024), we set the number of training epochs to 3, and the training process costs 70 GPU hours and 124 GPU 2https://github.com/Unbabel/COMET 3https://huggingface.co/Unbabel/ wmt22-cometkiwi-da and https://huggingface. co/Unbabel/wmt22-comet-da 4https://github.com/mjpost/sacrebleu 5https://huggingface.co/Qwen/Qwen2. 5-7B-Instruct 6https://huggingface.co/Qwen/Qwen2. 5-14B-Instruct Figure 3: An generation case of DRT-o1-14B. Model BLEU CometKiwi CometScore enough. Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct QwQ-32B-preview Marco-o1-7B DRT-o1-7B DRT-o1-14B 27.02 30.23 27.46 29.48 35.28 37. 70.36 72.01 71.48 71.62 71.67 72.16 76.78 78.84 78.68 77.41 80.14 80.50 Table 2: Experimental results on literature translation. hours for 7B and 14B models, respectively. 3.2 Main Results Table 2 shows the results on literature translation. We compare our DRT-o1-7B and DRT-o114B with previous Qwen2.5-7B-Instruct, Qwen2.514B-Instruct, QwQ-32B-preview and Marco-o17B. After instruction tuning on our collected data, DRT-o1-7B outperforms Qwen2.5-7B-Instruct by 8.26 BLEU, 1.31 CometKiwi and 3.36 CometScore. DRT-o1-14B outperforms Qwen2.5-14B-Instruct by 7.33 BLEU, 0.15 CometKiwi and 1.66 CometScore. Besides, DRT-o1-14B achieves the best results in terms of all metrics, showing the effectiveness of long thought in machine translation. Figure 3 shows an example of DRT-o1-14B. As we can see, the model learns the thought process in our data collection. DRT-o1-14B first performs the word-level translation, and then attempts the preliminary translation. Next, it iteratively refines its translation until it thinks the translation is good"
        },
        {
            "title": "4 Related Work",
            "content": "Recently, O1-like models have shown great performance in reasoning tasks, especially math and coding tasks. After the emergency of OpenAI O1 model (OpenAI, 2024b), many efforts are given in reproducing OpenAI O1. For example, Qin et al. (2024) propose journey learning, training paradigm, to encourage LLMs to learn not just shortcuts, but the complete exploration process. With only 327 training samples, the journey learning enhances LLMs long-thought ability. Huang et al. (2024) explore the data distillation from existing O1-like models, and show the effectiveness of data distillation. Zhang et al. (2024) leverage Monte Carlo Tree Search (MCTS) to synthesize reasoning-enhanced code data, and train O1-Coder. Marco-o1 (Zhao et al., 2024) is proposed to deal with open-ended text generation. Marco-o1 shows the effectiveness of MCTS-enhanced inference method, and achieves great performance on math problems. Besides, Marco-o1 studies the machine translation cases with long thought, and shows the potentiality of O1-like models in dealing with MT."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce DRT-o1, an attempt to bring the success of long-thought reasoning to neural machine translation (MT). Specifically, we syn100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Raymond Van den Broeck. 1981. The limits of translatability exemplified by metaphor translation. Poetics today, 2(4):7387. Jiaan Wang, Fandong Meng, Yingxue Zhang, and Jie Zhou. 2024. Retrieval-augmented machine translation with unstructured knowledge. arXiv preprint arXiv:2412.04342. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. 2024. arXiv o1-coder: an o1 replication for coding. preprint arXiv:2412.00154. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. 2024. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, Bangkok, Thailand. Association for Computational Linguistics. thesize the machine translation long-thought samples by designed multi-agent framework and GPT4o reformulation. To collect the source sentences that are suitable for translation via long thought, we mine sentences with similes or metaphors from existing literature books. To synthesize the long thought machine translation process for these sentences, translator, an advisor and an evaluator collaborate to translate the source sentence iteratively. GPT-4o is further employed to enhance the readability and fluency of the thought process collected via the multi-agent framework. Based on the synthesized data, we train DRT-o1-7B and DRTo1-14B models (using Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct as backbones), and show their effectiveness in literature translation."
        },
        {
            "title": "References",
            "content": "Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489. Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022. BOOKSUM: collection of datasets for long-form narrative summarization. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 65366558, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. OpenAI. 2024a. Gpt-4o system card. arXiv preprint arXiv:2410.21276. OpenAI. 2024b. Learning to reason with large language https://openai.com/index/ models. learning-to-reason-with-llms/. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. Qwen Team. 2024. Qwen2.5: party of foundation models. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over"
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent Inc"
    ]
}