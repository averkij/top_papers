{
    "paper_title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
    "authors": [
        "Tianjin Huang",
        "Ziquan Zhu",
        "Gaojie Jin",
        "Lu Liu",
        "Zhangyang Wang",
        "Shiwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated exceptional performance across diverse tasks, yet their training remains highly resource-intensive and susceptible to critical challenges such as training instability. A predominant source of this instability stems from gradient and loss spikes, which disrupt the learning process, often leading to costly interventions like checkpoint recovery and experiment restarts, further amplifying inefficiencies. This paper presents a comprehensive investigation into gradient spikes observed during LLM training, revealing their prevalence across multiple architectures and datasets. Our analysis shows that these spikes can be up to $1000\\times$ larger than typical gradients, substantially deteriorating model performance. To address this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a novel optimizer designed to counteract gradient spikes through momentum reset and spike-aware gradient clipping. Extensive experiments, including both pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam and its variants across various tasks, including (1) LLM pre-training from 60M to 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time Series Forecasting. Additionally, SPAM facilitates memory-efficient training by enabling sparse momentum, where only a subset of momentum terms are maintained and updated. When operating under memory constraints, SPAM outperforms state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our work underscores the importance of mitigating gradient spikes in LLM training and introduces an effective optimization strategy that enhances both training stability and resource efficiency at scale. Code is available at https://github.com/TianjinYellow/SPAM-Optimizer.git"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 2 4 8 6 0 . 1 0 5 2 : r SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Tianjin Huang1,2, Ziquan Zhu5, Gaojie Jin1, Lu Liu1, Zhangyang Wang3, and Shiwei Liu4,2 1University of Exeter, 2Eindhoven University of Technology, 3University of Texas at Austin 4University of Oxford, 5University of Leicester"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated exceptional performance across diverse tasks, yet their training remains highly resourceintensive and susceptible to critical challenges such as training instability. predominant source of this instability stems from gradient and loss spikes, which disrupt the learning process, often leading to costly interventions like checkpoint recovery and experiment restarts, further amplifying inefficiencies. This paper presents comprehensive investigation into gradient spikes observed during LLM training, revealing their prevalence across multiple architectures and datasets. Our analysis shows that these spikes can be up to 1000 larger than typical gradients, substantially deteriorating model performance. To address this issue, we propose Spike-Aware Adam with Momentum Reset (SPAM), novel optimizer designed to counteract gradient spikes through momentum reset and spike-aware gradient clipping. Extensive experiments, including both pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam and its variants across various tasks, including (1) LLM pre-training from 60M to 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time Series Forecasting. Additionally, SPAM facilitates memory-efficient training by enabling sparse momentum, where only subset of momentum terms are maintained and updated. When operating under memory constraints, SPAM outperforms state-of-the-art memory-efficient optimizers such as GaLore and AdamMini. Our work underscores the importance of mitigating gradient spikes in LLM training and introduces an effective optimization strategy that enhances both training stability and resource efficiency at scale. Code is available at https://github.com/TianjinYellow/SPAM-Optimizer.git."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have become fundamental in advancing state-of-the-art AI systems. Scaling LLMs, such as GPT-3 (Brown, 2020) and LLaMA (Touvron et al., 2023), has showcased unprecedented capabilities. However, training these large-scale models is fraught with challenges, particularly training instability. major factor contributing to this instability is the occurrence of gradient and loss spikes during training, which disrupt the learning process at unpredictable intervals (Chowdhery et al., 2023; Zhang et al., 2022; Le Scao et al., 2023). While architectural innovations have been proposed to mitigate these issues (Nguyen & Salazar, 2019; Shoeybi et al., 2019; Zeng et al., 2022; Ding et al., 2021; Wang et al., 2024; Dettmers et al., 2021; Scao et al., 2022; Takase et al., 2023), none can completely prevent the occurrence of spikes. In practice, the most widely adopted solution is to manually intervene by restarting training from previous checkpoint and skipping data affected by the spike (Chowdhery et al., 2023). This method is resource-intensive, requiring frequent checkpoint saves, manual monitoring, and repeated experiment runs - all inefficient and undesirable. 1 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training"
        },
        {
            "title": "M\nL\nL",
            "content": "M - L Figure 1: Performance of LLM pre-training (top), quantization-aware training (W4A4, W8A8) (middle), and reinforcement learning (bottom). Experiments of LLM pre-training and low-bit training are based on LLaMA models on C4 Dataset. Reinforcement learning experiments are based on Mujoco environments and PPO. Moreover, the sheer scale of LLMs necessitates vast computational resources. For example, training LLaMA required over 2048 A100-80GB GPUs (Touvron et al., 2023), posing significant environmental and financial costs (Rillig et al., 2023; Patterson et al., 2021). These challenges highlight the need for more efficient training paradigms that reduce resource consumption without sacrificing performance. In this paper, we approach the issue from an optimization perspective rather than an architectural one. We first conduct an in-depth investigation of loss and gradient spikes during the training of various LLM architectures, spanning models from 60M to 1B parameters. Our study reveals several key observations: Small yet frequent loss bumps: Although catastrophic loss spikes are rare, we observe frequent small loss bumps that can easily be overlooked without close scrutiny. Gradient spikes accompanying loss bumps: These loss bumps, depiste small by their own, are consistently accompanied by significant gradient spikes, whose magnitudes can reach up to 1000 greater than typical gradients. These spikes persist across layers, architectures, and datasets, even with established techniques applied. Harmfulness of gradient spikes: By nullifying the spiked gradients, we observe notable improvements in training performance, confirming that these spikes have detrimental effect. Momentum-based optimizers, like Adam (Kingma, 2014; Loshchilov, 2017), suffer 2 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training particularly from the accumulation of these spikes in their momentum terms, as we demonstrate both empirically and theoritically. Inspired by these findings, we introduce Spike-Aware Adam with Momentum Reset (SPAM), an optimizer designed to counteract the negative effects of gradient spikes. SPAM introduces two key innovations: (1) periodic reset of the first and second moments to eliminate the harmful accumulation of spiked gradients, and (2) identification and adaptive re-scaling of spiked gradients to manageable levels, preserving their directional information while mitigating their magnitude. We validate SPAM through extensive experiments, demonstrating its superior performance across various LLM sizes in both pre-training and fine-tuning tasks. Furthermore, momentum reset enables the development of sparse momentum, where only selected subset of momentum terms is computed and stored during training, drastically reducing memory costs. Our results show that SPAM surpasses leading memory-efficient optimizers such as GaLore (Zhao et al., 2024) and Adam-Mini (Zhang et al., 2024a) with good margins, even under memory constraints. Summary of Contributions: Comprehensive analysis of gradient spikes across multiple LLM architectures, revealing their significant impact on training stability and performance. Introduction of SPAM, novel optimizer with momentum reset and spike-aware clipping that outperforms existing methods like Adam and Adafactor on various tasks, including LLM training, Quantization-Aware LLM training, Reinforcement Learning, and Time Series Forecasting. memory-efficient version of SPAM that leverages sparse momentum to reduce memory usage while maintaining superior performance compared to state-of-the-art memoryefficient optimizers."
        },
        {
            "title": "2 Gradient Spikes",
            "content": "In this section, we formally define gradient spikes and then present the intriguing findings from our investigation into the training loss and gradient dynamics during LLM training. Gradient spikes refer to phenomenon that occurs during training where the magnitude of certain gradients significantly exceeds their historical values. To more precisely identify and analyze instances of gradient spikes, we introduce the Gradient Spike Score as measurement of the deviation of gradients magnitude from its typical behavior over time. By quantifying this relative change, we can monitor the dynamics of gradients during training. Definition 2.1 (Gradient Spike Score). Let {g0, g1, . . . , gT 1, gT } be the sequence of gradient obtained during the training process from time step 0 to . The Spike Score of the gradient at the ith step, denoted as GSS(gi), is defined as the ratio of the magnitude of the gradient at that step to the average magnitude of the gradients across all steps: GSS(gi) = gi PT 1 +1 gradient gi is considered spiked gradient if its GSS(gi) exceeds predetermined threshold θ, i.e., GSS(gi) > θ indicating significant increase from typical fluctuations, often amounting to increases of two or three orders of magnitude. j=0gj 2.1 Presence of Gradient Spikes During LLM Training Building upon the above concepts, we further explore the presence of gradient spikes during LLM training. Specifically, we monitor the gradients of the entire model over the initial 1, 000 training steps and identify gradient spikes using the condition GSS(gi) > 50. Our investigation encompasses two widely adopted LLM architectures, LLaMA (Touvron et al., 2023)1 and Pythia (Biderman et al., 2023), with model sizes varying from 60M to 1B parameters. Experiments were conducted on two datasets: the well-known C4 dataset (Raffel 1We adopt the LLaMa models used in Lialin et al. (2023b); Zhao et al. (2024). 3 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Figure 2: Training loss lumps and their corresponding gradient spikes. Gradient trajectories are collected with LLaMa-60M, 350M, 1B models on C4 datasets. Gradient spikes are detected using GSS(gi) > 50. et al., 2020) and cleaner high-quality dataset, SlimPajama (Soboleva et al., 2023). Please refer to Appendix for more details. Our key observations can be summarized as follows: ① Loss bumps accompanying gradient spikes occur irregularly during LLM training. Although we do not observe severe loss spikes that lead to catastrophic divergence (Takase et al., 2023; Chowdhery et al., 2023), we do observe subtle loss bumps that happen quite frequently. For instance, Figure 2-top illustrates the training loss of LLaMA-60M, 350M, and 1B models, where several loss bumps can be seen during training, marked with red circles. We further investigate the models gradients at these moments and observe that gradient spikes coincide with the loss bumps, as demonstrated in Figure 2-bottom. While gradients remain small for most of the training, they suddenly become extremely large when loss spikes occur. ② Gradient spikes are widely presented in different layers, across different architectures, model sizes, and datasets. Overall, we observed many gradient spikes across all layer types, as detailed in Figure 3-(4) and Appendix & B, with LayerNorm layers, in particular, experiencing an exceptionally high frequency of spikes. Figure 2 demonstrates that models of varying sizes, from 60M to 1B, all exhibit gradient spikes. To verify whether architecture is the root cause of these spikes, we conducted experiments with Pythia-70M, which also suffers from numerous gradient anomalies, as shown in Figure 3. Additionally, we found that gradient spikes occur even when using cleaner, high-quality datasets such as SlimPajama, although the frequency of spikes is reduced with this cleaner dataset. ③ Advanced spike mitigation approaches cannot completely eliminate gradient spikes. We also evaluate whether previously proposed techniques for addressing spikes can eliminate gradient spikes. Specifically, we assess multiple approaches, including Scaled Initialization (Nguyen & Salazar, 2019; Shoeybi et al., 2019), Embed LN (Dettmers et al., 2021), Scaled Embed (Takase et al., 2023), and Embed Detach (Zeng et al., 2022). The results in Figure 4 show that while some approaches perform better than others, they cannot completely eliminate gradient spikes. More specifically, we find that Scaled Embed and Embed LN significantly reduce the number of gradient spikes, while the other methods offer little to no improvement, consistent with the findings reported in Takase et al. (2023). Our observation of loss bumps likely relates to the edge of stability (EoS) phenomenon (Cohen et al., 2021), where the sharpness of the network hovers near the stability threshold for the remainder of training while the loss continues to decrease, albeit non-monotonically. However, the EoS phenomenon has not been extensively studied at the scale of LLMs. Moreover, our study reveals that these loss bumps have harmful effects on LLM training, which were not observed in previous studies. SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Figure 3: Spike gradients present across different architectures and datasets. (1) (3): Plots of 100 randomly selected spike gradients (using GSS(gi) > 50) of LLaMa60M and Pythia-70M on C4 and SlimPajama datasets. (4): Number of spiked gradients every 5 layers during the first 1K steps in LLaMa-60M on C4. Figure 4: Advanced spike mitigation approaches can not completely eliminate gradient spikes. Gradient trajectories are collected with LLaMa-60M on C4. The spike gradient is detected via GSS(gi) > 50. 2.2 Effects of Gradient Spikes on LLM Training Figure 5: Left: Perplexity of the final model after zeroing out spiked gradients using various θ, GSS(gi) > θ. Experiments are conducted using LLaMa-60M on C4. Middle and Right: Impact of spiked Gradients on the first and second Moments. Simulated gradients (gi (µ, σ2) are used to visualize the prolonged effects of gradient spikes on the first and second moments, with large spike noise introduced at the 30th step. After identifying the presence of gradient spikes during training, crucial question arises: are these gradient spikes detrimental or, perhaps counterintuitively, beneficial to the training of LLMs? To address this, we conducted series of experiments as follows. Our findings confirm that gradient spikes are indeed harmful to LLM training, exerting prolonged negative effects on both the first and second moments, as discussed below. Gradient spikes negatively impact LLM training. One direct way to assess the impact of gradient spikes is by nullifying the spiked gradients during training and observing the final training performance. We first detect spiked gradients using various thresholds θ and then set those gradients to zero. Figure 5-Left reports the results of LLaMA-60M on C4. Surprisingly, zeroing out these spiked gradients leads to improved model performance, evidenced by reduction in perplexity. This observation clearly indicates that gradient spikes hinder effective training, and their removal is beneficial to overall model performance. 5 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Gradient spikes have prolonged detrimental effects on the first and second moments. Due to the exponential averaging of the momentum mechanism, the influence of gradient spike decays slowly over time. To demonstrate this, we conduct simulation experiment using Adam. In this experiment, we model the gradients as random variables drawn from Gaussian distribution with mean µ = 0.1 and variance σ2 = 0.1, i.e., gi (µ, σ2). We sample gradients and track their corresponding moments over 200 steps, introducing gradient spike at step 30 with large magnitude of 10. As shown in Figure 5-Middle and Right, the spikes amplification persists, influencing both moments across subsequent steps. For example, it takes approximately 50 steps for the first moment to recover from the spike, while the second moment takes significantly longer, with the effect persisting beyond 200 steps. Two key factors plausibly contribute to this difference: (1) the second moment typically employs larger exponential decay rate than the first (0.999 vs. 0.9); and (2) the second moment depends on the squared gradients, making it more sensitive to large spikes. 2.3 Prelinminary Analysis with Theory Implications We hereby provide very preliminary analysis to help probe why gradient spikes have significant impact on the regret bound of Adam-like algorithms. We strictly follow the setting and notations used in Alacaoglu et al. (2020). Specifically, referring to Theorem 1 in the paper, the regret bound consists of two main terms: R(T ) D2 2α(1 β1) i=1 ˆv1/2 T,i + α 1 + log p(1 β2)(1 γ) v T i=1 t=1 g2 t,i, where γ = β2 1 β2 the gradients gt. In their Lemma 3, it is shown that the norm mt2 accumulated gradients: . Gradient spikes directly affect these terms by increasing the magnitudes of depends on the ˆv1/2 mt2 ˆv1/2 (1 β1)2 p(1 β2)(1 γ) t i=1 j=1 βtj 1 gj,i. When gradient spikes occur, the values of gj,i become significantly larger for some and i, which in turn increases the bound on mt2 . This enlargement propagates through the analysis, particularly affecting the accumulation term PT 4, which is bounded by: in their Lemma t=1 αtmt2 ˆv1/2 ˆv1/2 X t=1 αtmt2 ˆv1/2 (1 β1)α 1 + log p(1 β2)(1 γ) v T i=1 t=1 g2 t,i. Here, gradient spikes increase PT t=1 g2 t,i spikes occur, leading to larger bound. significantly, especially in the coordinates where the Finally, in the main regret bound (Equation (9) in the paper), these enlarged terms result in looser (larger) overall regret bound due to the presence of gradient spikes. The increased ˆv1/2 directly contribute to the regret bound becoming less tight. T,i This theoretical implication highlights that while adaptive algorithms like AMSGrad adjust learning rates based on gradient history, they may perform worse in terms of regret when large gradient spikes are present due to the increased cumulative squared gradients and decreased effective learning rate. and PT t=1 g2 t,i It is important to note that our goal is not to claim theoretical innovations, but rather to quantitatively assess how gradient spikes degrade Adam-like optimization, and that is only explored in very limited context. We would like to clarify the limitations of this analysis: (1) The analysis assumes convexity, which may not apply in non-convex settings (but is often mitigated by assuming Polyak-Lojasiewicz condition or so). (2) The assumption gt G, where denotes the maximum allowable gradient bound, may be in conflict 6 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training with the presence of gradient spikes if is not sufficiently large to capture them. (3) There is significant dependence on G, and if is set too high to accommodate spikes, the constants in the regret bound grow disproportionately, potentially making the bound meaningless. Nonetheless, we find that our analysis aligns well with our experimental results, and we leave more rigorous theoretical exploration for future work."
        },
        {
            "title": "3 Spike-Aware Adam with with Momentum Reset (SPAM)\nIn this section, we introduce Spike-Aware Adam with Momentum Reset (SPAM). Unlike\nprevious solutions that introduce architectural innovations to mitigate the decremental ef-\nfects of gradient spikes (Nguyen & Salazar, 2019; Zeng et al., 2022; Dettmers et al., 2021;\nTakase et al., 2023), we attempt to address this issue from an optimization perspective.\nConcretely, we integrate Momentum Reset and Spike-Aware Clipping into Adam to deal\nwith gradient spikes. In addition, we introduce a memory-efficient version of SPAM, which\nincorporates Sparse Momentum, significantly reducing the memory footprint during LLM\ntraining. Pseudocode of SPAM is in Algorithm 1.\nMomentum Reset. To mitigate the detrimental effects of gradient spikes on training\nstability, we introduce Momentum Reset. Momentum Reset involves periodically resetting\nthe accumulated first and second moments used by adaptive optimizers such as Adam.\nThese optimizers rely on exponential moving averages of past gradients to inform parameter\nupdates. However, when a gradient spike occurs, it can significantly inflate these moments,\ncausing the impact of the spike to persist over many subsequent iterations. By resetting the\nmomentum terms at regular intervals of ∆T training iterations, we can prevent the lingering\ninfluence of anomalously large gradients on the optimizer’s state. This practice ensures that\nparameter updates are based on recent, more normal gradients rather than being skewed by\ngradient spikes. To mitigate potential instability caused by momentum reset, we perform\nN steps (N = 150 by default) of cosine warmup following each reset operation.\nSpike-Aware Clipping. To further mitigate gradient spikes during intervals, we introduce\nSpike-Aware Clipping. While our initial experiments indicate that setting spiked gradients\nto zero can enhance performance, this approach completely removes the learning signal for\nthose parameters, including valuable directional information critical to the optimization\nprocess. To address this, SPAM identifies gradients that exceed a predefined threshold θ and\nscales them to a manageable value, preserving their directional information while controlling\ntheir magnitude.",
            "content": "Detecting gradient spikes using GSS defined in Definition 2.1 would require knowing and storing all gradients in advancea method that is impractical for LLM training due to memory constraints. We adopt more memory-efficient, on-the-fly approach by leveraging the components already calculated by Adam. Formally, we detect gradient spikes by identifying gradients gi that meet the following condition: = where Vi is the second moment of Adam and θ is the threshold used for the approximate GSS = g2 . Note that we Vi only use GSS defined in Definition 2.1 for the gradient spike analysis in Section 2. For real training, we employ the above approximation version. Since Vi is essentially the moving average of g2 , this method efficiently identifies spikes without incurring additional overhead or the need to store the entire gradient history. Once detected, these spikes are clipped by scaling them to manageable value. Specifically, for each spike gradient, we apply the θVi. This technique is particularly useful when combined with operation: gi = sign(gi) Momentum Reset. By incorporating these strategies, SPAM effectively mitigates the negative impact of gradient spikes, improving training stability and performance. gi g2 Vi > θ Note that unlike the Update Clipping used in Adafactor (Shazeer & Stern, 2018), which is applied to the whole weight update matrix when its Root Mean Square is larger than 1, our spike-aware clipping is directly applied to the spiked gradients gi whose magnitudes are significantly larger than its Sparse Momentum. Momentum reset paves the way for the development of sparse momentum, technique designed to reduce memory usage and computation during the training of LLMs. In traditional momentum-based optimizers, such as Adam, momentum is updated and stored for all parameters, which can be memory-intensive for large-scale models. Sparse vi, e.g., > 50. 7 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training momentum offers more memory-efficient alternative by updating and maintaining only dynamically selected subset of moments at each iteration. The percentange of selected subset is denoted by %d. Key questions surrounding sparse momentum include how to effectively select parameter subsets, how to determine the sampling frequency, and whether to retain momentum for weights that are sampled consecutively . Our empirical analysis shows that random sampling is the most effective strategy for selecting subsets of parameters. For the other questions, we find that they align well with the momentum reset strategy. Specifically, setting the sampling frequency to match the momentum reset frequency, and resetting the momentum of all weights, even when they are sampled consecutively, yield the most robust results. Table 1: Comparison with various optimizers on pretraining various sizes of LLaMA models on C4. Perplexity is reported. 60M 130M 350M 1B Model Size 16.07 34.10 Adam-mini 16.13 34.09 Adam 15.77 Adam+Gradient-Clip-Value 33.65 15.22 Adam+Gradient-Clip-Norm 33.33 15.19 32.57 Adafactor 14.66 30.46 SPAM 11.6B 1.1B Training Tokens 24.85 24.91 24.72 24.88 23.98 23.36 2.2B 19.05 18.77 18.52 18.51 17.74 17.42 6.4B Table 2: Perplexity of Applying Advanced Techniques on LLaMA-60M. Perplexity is reported. Optimizer Perplexity Adam Adam+Embed LN Adam+Embed Detach Adam+Scaled Embed Adam+Scaled Initalization SPAM 34.09 33.61 34.48 33.87 34.29 30."
        },
        {
            "title": "4 Experiments",
            "content": "To demonstrate the efficacy of our proposed method, we conduct experiments on both pre-training and supervised fine-tuning using various sizes of the LLaMA model on the C4 dataset. Additionally, we evaluate its performance on Quantization-Aware Training, Reinforcement Learning, and Time Series Forecasting tasks. Baselines. We adopt several widely-used optimizers as our baselines. Since SPAM is built upon Adam, Adam serves as our most direct baseline. We also incorporate two common gradient clipping approaches with Adam: (1) Value Clip, which clips all gradients when their absolute value exceeds threshold; and (2) Norm Clip, which scales the entire gradient if the L2 norm of the gradient vector exceeds certain threshold. Additionally, we compare against another widely-used optimizer, Adafactor (Shazeer & Stern, 2018). In terms of spike mitigation techniques, we evaluate SPAM against previous approaches, including Scaled Initialization (Nguyen & Salazar, 2019; Shoeybi et al., 2019), Embed LN (Dettmers et al., 2021), Scaled Embed (Takase et al., 2023), and Embed Detach (Zeng et al., 2022). For memory-efficient optimization methods, we include Adam-Mini (Zhang et al., 2024a), GaLore (Zhao et al., 2024), LoRA (Hu et al., 2021), and ReLoRA (Lialin et al., 2023a). Architecture and hyperparameters. Following (Lialin et al., 2023a; Zhao et al., 2024), we conduct our experiments using the LLaMA-based architecture with various sizes from 60M to 1B parameters, incorporating RMSNorm (Shazeer, 2020) and SwiGLU activations (Zhang & Sennrich, 2019). For each model size, we use the same set of hyperparameters across methods, varying only the learning rate, where we sweep over set of learning rates from 1e4 to 1e3, incrementing by 2e4 for each optimizer. All experiments are conducted using the BF16 format. We set clip threshold as 1 and 1e 3 for Norm Clip and Value Clip, respectively, following the setting in Takase et al. (2023). We set hyper-parameters for Adafactor following the original paper (Shazeer & Stern, 2018) where ϵ1 = 1030, ϵ2 = 103 and = 1.0. For SPAM, we set reset intervals = 500, lr warmup step = 150 and GSS threshold θ = 5000. Detailed descriptions of our task setups and hyperparameters are provided in the Appendix D. 4.1 Performance of LLM Pre-training Standard Pre-training. We report the training curves of various LLaMA models on the C4 dataset as well as the final perplexity in Figure 1 and Table 1, respectively. Overall, we observe that SPAM consistently achieves superior performance. As memory-efficient approach, Adam-mini performs on par with Adam, consistent with the results reported SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training in Zhang et al. (2024a). Commonly used gradient clipping techniques such as Value Clip and Norm Clip improve performance over Adam, with the latter achieving slightly better results. Adafactor further outperforms the aforementioned approaches, demonstrating its effectiveness. SPAM consistently outperforms all baselines across various LLaMA model sizes, highlighting the benefits of integrating momentum reset and spike-aware clipping techniques. All spike mitigation approaches fall short of SPAM as shown in Table 2. Additionally, Appendix shows that SPAM can perform on par with or better than Adam in vision tasks. Table 3: Comparison with memory-efficient algorithms on pre-training various sizes of LLaMA models on C4 dataset. Validation perplexity is reported, along with memory estimate of the total of parameters, optimizer states based on BF16 format.The results of GaLore, Full-Rank, LoRA and ReLoRA are obtained from Zhao et al. (2024). Adam ReLoRA LoRA GaLore SPAM (Sparse) Training Tokens 60M 34.06 (0.36G) 37.04 (0.36G) 34.99 (0.26G) 34.88 (0.24G) 32.39 (0.24G) 1.1B 130M 25.08 (0.76G) 29.37 (0.80G) 33.92 (0.54G) 25.36 (0.52G) 23.98 (0.52G) 2.2B 350M 18.80 (2.06G) 29.08 (1.76G) 25.58 (1.08G) 18.95 (1.22G) 18.28 (1.22G) 6.4B 1B 15.56 (7.80G) 18.33 (6.17G) 19.21 (6.17G) 15.64 (4.38G) 15.60 (4.38G) 11.6B Memory-efficient Pre-training. We evaluate SPAM by specifying d% such that its memory usage, including both parameters and optimizer states, matches that of Galore. For Galore, LoRA, and ReLoRA baselines, we set the ranks = 128, 256, 256, 512 for the 60M, 130M, 350M, and 1B models, respectively, following the setup in GaLore (Zhao et al., 2024). The results in Table 3 show that SPAM consistently outperforms all the baselines by good margin, demonstrating its effectiveness as memory-efficient optimizer. 4.2 Performance on LLM Fine-tuning In this section, we evaluate the effectiveness of SPAM for supervised fine-tuning. Following Li et al. (2024), we fine-tune LLaMA2-7B on Commonsense170K (Hu et al., 2023) and test on 8 downstream tasks. We do not apply layer-wise weight updates for GaLore and SPAM. The rank is set to 8 for all low-rank baselines. Correspondingly, the density of SPAM is set to 0.25% to maintain comparable memory cost. The results are reported in Table 5. We observe that SPAM substantially outperforms other memory-efficient methods, exceeding full fine-tuning by notable margin. 4.3 Performence on Quantization-Aware Training We evaluate the effectiveness of SPAM for Quantization-Aware Training (QAT) across various quantization settings, including INT4 and INT8 for both weights and activations (W4A4, W8A8). Figure 1 (middle row) presents the training curves of LLaMA models with 60M, 130M, and 350M parameters on the C4 dataset. The results demonstrate that SPAM consistently outperforms Adam by significant margin. This result highlights the promise of SPAM on low-bit training of LLMs. 4.4 Performence on Reinforcement Learning The effectiveness of SPAM is further assessed in classical reinforcement learning tasks, including the Ant-V4, HalfCheetah-V4, and Hopper-V2 environments from the MuJoCo suite. Figure 1 (bottom row) displays the test rewards averaged over five repeated experiments using the PPO algorithm (Schulman et al., 2017). The results indicate that SPAM consistently surpasses Adam across these environments, further validating its advantages in reinforcement learning settings. 4.5 Performence on Time Series Forescasting We conducted additional experiments on time-series prediction tasks. In these experiments, we intentionally introduced anomalous data with probability A=10% to simulate gradient 9 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training anomalies. Experiments are conducted with 10 repeated runs on Weather time series data2 using PatchTST (Nie et al., 2023) model. The results are presented in Figure 6 The findings demonstrate that as the severity of anomalous data increases, SPAMs performance advantage over Adam becomes more pronounced, highlighting its effectiveness in mitigating the adverse impact of gradient spikes. Figure 6: Test loss during training process on weather time-series data. Anomalous data is generated by adding Gaussian noise to 10% of randomly selected input values. Specifically, the anomalies data are conducted with = + Gaussin(0, Max(X)) where is the inputs. 4.6 Performence on Vision Models We further evaluate SPAM on vision task. Specifically, we conducted experiments on ImageNet-1K using ConvNeXt-Tiny (Liu et al., 2022b) and ViT-Tiny (Touvron et al., 2021). We adopt the default training recipe from the official code of ConvNeXT3 and train all models for 120 epochs. We set = 25K, = 20 and θ = 5000 for SPAM. The results in Table 12 demonstrate that SPAM can achieve on par or better performance than vanilla AdamW. Table 4: SPAM performs on par or better than AdamW on vision tasks. Optimizer Model Metric 25% steps 50% steps 75% steps 100% steps AdamW ConNeXt-T Test Acc () ConNeXt-T Test Acc () SPAM Test Acc () ViT-Tiny AdamW Test Acc () ViT-Tiny SPAM 68.15 68.36 48.09 47.34 74.00 73.63 56.93 56. 78.83 78.85 65.06 65.57 80.89 81.04 69.71 69.98 Table 5: Fine-tuning performance of LLaMa2-7B on various downstream tasks. The Mem. denotes the running GPU memory. The mean and standard deviation of 10 repeated experiments are reported. Method Adam (Full FT) LoRA GaLore SPAM (d = 0.25%) SPAM (d = 100%) Mem. BoolQ 79.70.1 75.80.4 82.80.7 85.00.2 87.10.2 61G 26G 36G 36G 61G PIQA 79.10.1 79.00.1 78.40.2 78.90.2 79.50.1 SIQA 51.30.05 56.30.1 55.80.4 55.70.2 58.30.1 HellaSwag WinoGrande ARC-e 79.20.1 58.50.02 77.60.1 59.90.04 75.90.4 56.30.5 76.50.2 57.80.1 79.20.2 58.10.04 74.80.2 79.60.2 79.00.1 78.90.2 83.30.2 ARC-c 48.20.01 46.90.1 46.20.5 47.30.2 48.60.1 OBQA 36.20.2 34.40.3 34.20.1 35.10.3 40.10. Avg. 63.40.1 63.70.2 63.60.4 64.40.2 66.70."
        },
        {
            "title": "5 Ablation Study\nSelection strategy for sparse momentum. Many strategies have been proposed to\nselect subsets of parameters for sparse training, such as random selection (Liu et al., 2022a),\nmax weight magnitude (Mocanu et al., 2018), and max gradient magnitude (Evci et al.,\n2020). Among these strategies, the most effective approach for sparse momentum training\nremains unclear. To investigate this, we conduct experiments with LLaMA-60M on the\nC4 dataset. The results are reported in Figure 7-(1). Interestingly, we find that randomly",
            "content": "2https://www.bgc-jena.mpg.de/wetter/ 3https://github.com/facebookresearch/ConvNeXt 10 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Figure 7: Ablations for sparse subset selection strategy, momentum reset inteval, GSS threshold and warmup steps. None denote that the spike-aware clipping is not applied. Figure (1) is based on LLaMA-60M, Figure (2)-(4) are based on LLaMA-130M. selecting subsets of parameters performs significantly better than the other two strategies for our sparse momentum. One plausible explanation for this discrepancy is that random selection allows for rapid exploration across all model parameters, whereas gradientor weight-based strategies might be confined to the same subset of parameters during training. Momentum reset interval . To investigate the impact of interval , we conduct experiments based on LLaMA-130M and C4 with varying fromm 50 to 2500. The warmup steps is set to 150 and the thresthold θ is set to 5000. The results are reported in Figure 7-(2). We observe performance improvement as the interval decreases from 2500 to 500. However, when is further shortened, performance begins to degrade. This suggests that while momentum resets can enhance performance, excessively frequent resets may be detrimental to overall results. GSS threshold θ. Threshold θ decides which gradient are detected as spikes. To illustrate the impact of θ on SPAM, we present the results of LLaMA-130M in Figure 7-(3) with varying θ from 20000 to 10. The warmup steps is set to 150 and the interval is set to 500. We observe that performance improves as θ is reduced from extremely large values to smaller values, such as 1000, indicating that spike gradient clipping and momentum reset techniques have mutually reinforcing effect. However, excessively small θ may interfere with the true gradient, ultimately leading to degradation in performance. Warmup steps . We assess the impact of the warmup procedure following each momentum reset by presenting the performance of LLaMA-130M with different warmup steps, ranging from 0 to 200, in Figure 7-(4). The results indicate significant performance drop when no warmup is applied (N = 0), compared to when warmup is used. In addition, performance reach to optimal when the warmup duration is set to approximately 150 steps."
        },
        {
            "title": "6 Related Work",
            "content": "Instability of Training Large Language Models. LLMs are well-known for their training instability (Molybog et al., 2023), often experiencing irregular loss spikes that can lead to catastrophic divergence (Chowdhery et al., 2023). To address this issue, researchers have developed various stabilization techniques. While we outline several key approaches, we acknowledge that this overview may not cover all significant contributions in the field. One prominent approach involves architectural modifications. Xiong et al. (2020) demonstrated that using Post-LN in Transformers leads to larger gradients near the output layer, resulting in training instability, especially with large learning rates. In contrast, Pre-LN helps maintain well-behaved gradients during initialization, promoting more stable training. Embed LN, introduced by Dettmers et al. (2021), adds an additional LayerNorm after the embedding layer to improve stability, though it may cause performance degradation, as noted by Scao et al. (2022). Embed Detach, proposed by Ding et al. (2021) and further extended by Zeng et al. (2022) for LLMs, addresses loss spikes by shrinking embedding gradients. DeepNorm, developed by Wang et al. (2024), enhances stability in deep Transformers by scaling up the residual connection before applying LayerNorm. Additionally, αReparam (Zhai et al., 2023) re-parameterizes all linear layers using spectral normalization to prevent attention entropy collapse. 11 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Another set of approaches focuses on improving initialization to mitigate training instability. Scaled Embed, proposed by Takase et al. (2023), scales up embeddings to stabilize LayerNorm gradients. Scaled Initialization (Nguyen & Salazar, 2019) introduces parameter initialization strategy using smaller normal distribution (0, p2/5d/ 2N ) to stabilize training dynamics. Additionally, Fixup (Zhang et al., 2019; Huang et al., 2020) claims that proper initialization can entirely eliminate the need for LayerNorm. Momentum Reset. Momentum reset is not new approach. It has been used in Gu et al. (2013); Nesterov (2013) to solve the rippling behavior of Nesterovs Accelerated Gradient (NAG) (Nesterov, 1983) in the high-momentum regime, particularly in the context of convex optimization problems. Odonoghue & Candes (2015) further proposed adaptive reset where the momentum will be reset when an increase in the function value is observed. Unlike these earlier work, we leverage momentum reset to mitigate the detrimental effects of gradient spikes that arise during the training of billion-parameter language models, which present large-scale, non-convex optimization challenge. Memory-Efficient Optimizers. There have been several efforts to reduce Adams memory footprint. SM3 (Anil et al., 2019), lightweight variant of AdaGrad (Duchi et al., 2011), selects the learning rate for the i-th parameter by taking the minimum value from set of candidates, each associated with the maximum squared gradient under predetermined cover. Adafactor (Shazeer & Stern, 2018) and its variant CAME (Luo et al., 2023) utilize non-negative low-rank factorization over Adams second-moment estimate, v. Adam-mini (Zhang et al., 2024a) partitions the parameters into blocks and assigns single learning rate to each block to reduce memory. Similar approaches were proposed in (Zheng & Kwok, 2019; Ginsburg et al., 2019). Low-precision optimizers are studied in (Dettmers et al., 2021). Recently, GaLore (Zhao et al., 2024; Zhang et al., 2024b) enables the full-parameter training of LLMs through low-rank gradient updates."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we presented comprehensive study of gradient and loss spikes in LLM training, demonstrating their detrimental impact on training stability and performance across variety of architectures and datasets. To address this issue, we propose Spike-Aware Adam with Momentum Reset (SPAM), novel optimizer designed to counteract gradient spikes through momentum reset and spike-aware gradient clipping. The effectiveness of SPAM is backed up with extensive experiments across various LLM model sizes, where SPAM consistently outperformed Adam and other state-of-the-art optimizers by good margin. When operating under memory constraints, SPAM motivates the feasibility of sparse momentum training, outperforms state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini."
        },
        {
            "title": "References",
            "content": "Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and Volkan Cevher. new regret analysis for adam-type algorithms. In International conference on machine learning, pp. 202210. PMLR, 2020. Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization. Advances in Neural Information Processing Systems, 32, 2019. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397 2430. PMLR, 2023. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In 2018 Information Theory and Applications Workshop (ITA), pp. 110. IEEE, 2018. 12 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Jeremy Cohen, Simran Kaur, Yuanzhi Li, Zico Kolter, and Ameet Talwalkar. Gradient In International descent on neural networks typically occurs at the edge of stability. Conference on Learning Representations, 2021. Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via blockwise quantization. arXiv preprint arXiv:2110.02861, 2021. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International conference on machine learning, pp. 29432952. PMLR, 2020. Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, Huyen Nguyen, Yang Zhang, and Jonathan Cohen. Stochastic gradient methods with layer-wise adaptive moments for training of deep networks. arXiv preprint arXiv:1905.11286, 2019. Ming Gu, Lek-Heng Lim, and Cinna Julie Wu. Parnes: rapidly convergent algorithm for accurate recovery of sparse and approximately sparse signals. Numerical Algorithms, 64 (2):321347, 2013. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameterefficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs. Improving transformer optimization through better initialization. In International Conference on Machine Learning, pp. 44754483. PMLR, 2020. Diederik Kingma. Adam: method for stochastic optimization. arXiv:1412.6980, 2014. arXiv preprint Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2023. Pengxiang Li, Lu Yin, Xiaowei Gao, and Shiwei Liu. Owlore: Outlier-weighed layerwise sampled low-rank projection for memory-efficient llm fine-tuning. arXiv preprint arXiv:2405.18380, 2024. Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky. Relora: High-rank training through low-rank updates. In The Twelfth International Conference on Learning Representations, 2023a. Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky. Stack more layers differently: High-rank training through low-rank updates. arXiv preprint arXiv:2307.05695, 2023b. SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. arXiv preprint arXiv:2202.02643, 2022a. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1197611986, 2022b. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, and Yang You. arXiv preprint Came: Confidence-guided adaptive memory efficient optimization. arXiv:2307.02047, 2023. Stephan Mandt, Matthew Hoffman, and David Blei. variational analysis of stochastic gradient algorithms. In International conference on machine learning, pp. 354363. PMLR, 2016. Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):2383, 2018. Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, et al. theory on adam instability in large-scale machine learning. arXiv preprint arXiv:2304.09871, 2023. Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical programming, 140(1):125161, 2013. Yurii Nesterov. method for solving the convex programming problem with convergence rate (1/k2). In Dokl akad nauk Sssr, volume 269, pp. 543, 1983. Toan Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. arXiv preprint arXiv:1910.05895, 2019. Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time arXiv preprint series is worth 64 words: Long-term forecasting with transformers. arXiv:2211.14730, 2023. Brendan Odonoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes. Foundations of computational mathematics, 15:715732, 2015. David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Matthias Rillig, Marlene Agerstrand, Mohan Bi, Kenneth Gould, and Uli Sauerland. Risks and benefits of large language models for the environment. Environmental Science & Technology, 57(9):34643466, 2023. Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. What language model to train if you have one million gpu hours? arXiv preprint arXiv:2210.15424, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 14 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 45964604. PMLR, 2018. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. tail-index analysis of stochastic gradient noise in deep neural networks. In International Conference on Machine Learning, pp. 58275837. PMLR, 2019. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, and Nolan Dey. Hestness, deduplicated slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. of RedPajama. SlimPajama: Jacob Steeves, cleaned Joel and https://www.cerebras.net/blog/ 627B token version Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pp. 1034710357. PMLR, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 1052410533. PMLR, 2020. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua Susskind. Stabilizing transformer training by preventing attention entropy collapse. In International Conference on Machine Learning, pp. 4077040803. PMLR, 2023. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Hongyi Zhang, Yann Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun. Adam-mini: Use fewer learning rates to gain more. arXiv preprint arXiv:2406.16793, 2024a. 15 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Zhenyu Zhang, Ajay Jaiswal, Lu Yin, Shiwei Liu, Jiawei Zhao, Yuandong Tian, and Zhangyang Wang. Q-galore: Quantized galore with int4 projection and layer-adaptive low-rank gradients. arXiv preprint arXiv:2407.08296, 2024b. Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024. Shuai Zheng and James Kwok. Blockwise adaptivity: Faster training and better generalization in deep learning. arXiv preprint arXiv:1905.09899, 2019. 16 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training"
        },
        {
            "title": "A Statistics Analysis of Gradient Spikes across Various Types",
            "content": "of Layers It is important to examine whether gradient spikes exhibit preference for certain layers. To do so, we report the number of gradient spikes across various types of layers and the ratio of gradient spikes to the number of parameters in five types of layers: Embedding Layer, Attention Layer, FFN Layer, LayerNorm Layer, and LM Head Layer. The experiments were conducted with LLaMA-60M on the C4 dataset, with gradient spikes detected over 1000 training steps. The detailed statistics are provided in Table 6. We observe the following: ❶ The Embedding Layer exhibits the highest number of gradient spikes, also it has the largest parameter count. ❷ The LayerNorm Layer, however, experiences an exceptionally high frequency of spikes, even with the smallest number of parameters. Table 6: Number and Ratio of Gradient Spikes in each layer style of LLaMA. #Spikes are collected from 1000 training steps. Experiments are conducted with LLaMA-60M on C4. Module Name #Total Spikes #Total Params Embed Attention FFN LayerNorm LM Head 16384000 8388608 16908288 8704 16384000 11954001 86302 105415 949302 #Total Spikes #Total Params 0.729 0.010 0.006 109.06 0."
        },
        {
            "title": "B Locations of Loss Bumps and Gradient Spikes",
            "content": "To further investigate the correlation between loss bumps and gradient spikes, we present the locations of gradient spikes associated with the loss bumps in Table 7. The results reveal two key findings: ❶ Gradient spikes are presented in different layers associated with the loss bump; ❷ Gradient spikes typically occur before loss bumps, indicating that these gradient spikes may trigger loss bumps. Table 7: Location of Spike Gradient at Each Layer for Different Tasks. The spike gradient is detected via GSS(gi) > 50. The experiments are based on LLaMA-60M and Pythia-70M. Model LLaMA-60M (C4) Training Step When Loss Bump Occurs 198 LLaMA-60M (SlimPajama) Pythia-70M (C4) 207 328 394 358 578 Training Step When Spike Gradient Occurs in Each Layer 0th 5th 10th 15th 20th 25th 30th 35th 40th 45th 50th 55th 197 202 198 199 197 205 278 196 197 197 205 197 198 197 198 197 196 197 198 202 196 197 198 197 198 201 205 206 207 206 210 206 206 207 209 206 392 393 394 206 207 209 328 206 207 573 577 206 571 577 206 357 571 577 578 205 206 207 209 357 358 574 576 577 578 17 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training"
        },
        {
            "title": "C Pseudocode",
            "content": "Algorithm 1: SPAM Input: layer weight matrix Rmn, learning rate α, decay rates β1 = 0.9, β2 = 0.999, initial parameters w0, randomly initialize mask with density for each layer, the first moment m, the second moment v, threshold θ for GSS, momentum rerest interval , warmup scale total steps , small constant ϵ = 1 106. is total training steps. Output: optimized parameters wT . while < do Get gt Rmn ϕt(wt) warmup scale = 1 CosineAnnealing(M od(t, ), ) if Mod (t, ) = 0 then Generate Gradients random.rand(θ.shape) < zeros like(θ[M]) zeros like(θ[M]) Spike = gt[M] 2 > θ if sum(Spike M) > 0 then Random initialize the binary mask reset the first moment to zero reset the second moment to zero Detect spiked gradients gt[M][Spike M] = sign(gn[M][Spike M]) pθ v[Spike M] Spike Gradients CLIP mt = β1mt1 + (1 β1)gt vt = β2vt1 + (1 β2)g2 ˆmt = mt 1βt 1 ˆvt = vt 1βt 2 wt = wt1 α warmup scale t=t+ Return: optimized parameters wT ˆmt ˆvt+ϵ"
        },
        {
            "title": "D Architecture and Hyperparameters",
            "content": "We introduce details of the LLaMA architecture and hyperparameters used for pre-training, following Lialin et al. (2023a); Zhao et al. (2024). Table 8 shows the most hyperparameters of LLaMA models across model sizes. We use max sequence length of 256 for all models, with batch size of 512, with batch size of 131K tokens. For all experiments, we adopt learning rate warmup of 1000 training steps, and use cosine annealing for the learning rate schedule, decaying to 10% of the initial learning rate. Table 8: Configurations of LLaMA models used in this paper. Data amount are specified in #tokens. Params Hidden Intermediate Heads Layers Steps Data amount 60M 130M 350M 1 512 768 1024 2048 1376 2048 2736 5461 8 12 16 24 8 12 24 32 10K 20K 60K 89K 1.3B 2.6B 7.8B 11.6B For all methods across each model size (from 60M to 1B), we tune the learning rates from 1e4 to 1e3 with an increasing step of 2104 for pre-training tasks, and the best learning rate is selected based on the validation perplexity. We find that the hyperparameters, Interval and warmup step , are insensitive to model size and remain stable with the same learning rate across different model sizes. The detailed hyperparameter of SPAM on pre-training and fine-tuning are reported in Table 10 and Table 11. 18 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Table 9: Hyperparameters of SPAM for pre-training experiments in this paper. Hyper-Parameters LLaMA-60M LLaMA-130M LLaMA-350M LLaMA-1B Learning rate Interval Threshold θ Warmup steps Learning rate Interval Threshold θ Warmup steps Learning rate Interval Threshold θ Warmup steps 1e 3 500 5000 150 1e 3 500 5000 4e 3 500 5000 150 Standard Pretraining 8e 4 500 5000 150 4e 4 500 5000 150 Quantization-Aware Training 1e 3 500 5000 5e 4 500 5000 150 Memory-Efficient Pretraining 4e 3 500 5000 150 2e 3 500 5000 150 2e 4 500 5000 150 - 5e 4 1000 5000 300 Table 10: Hyperparameters of SPAM for RL, TSF, Vision experiments in this paper. Hyper-Parameters RL TSF Vision Learning rate Interval Threshold θ Warmup steps Total Epochs 3e 4 150 5000 50 100 1e 3 500 5000 150 100 4e 3 25000 5000 20 120 Table 11: Hyperparameters of SPAM for fine-tuning experiments in this paper. Hyper-Parameters LLaMA2-7B Standard Fine-tuning Learning rate Interval Threshold θ Warmup steps Learning rate Interval Threshold θ Warmup steps 5e 5 1000 5000 300 Memory-Efficient Fine-tuning 1e 4 250"
        },
        {
            "title": "E Vision Tasks",
            "content": "We further evaluate SPAM on vision task. Specifically, we conducted experiments on ImageNet-1K using ConvNeXt-Tiny (Liu et al., 2022b) and ViT-Tiny (Touvron et al., 2021). We adopt the default training recipe from the official code of ConvNeXT4 and train all models for 120 epochs. We set = 25K, = 20 and θ = 5000 for SPAM. The results in Table 12 demonstrate that SPAM can achieve on par or better performance than vanilla AdamW. 4https://github.com/facebookresearch/ConvNeXt 19 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Table 12: SPAM performs on par or better than AdamW on vision tasks. Optimizer Model Metric 25% steps 50% steps 75% steps 100% steps AdamW ConNeXt-T Test Acc () ConNeXt-T Test Acc () SPAM Test Acc () ViT-Tiny AdamW Test Acc () ViT-Tiny SPAM 68.15 68.36 48.09 47.34 74.00 73.63 56.93 56.47 78.83 78.85 65.06 65.57 80.89 81.04 69.71 69."
        },
        {
            "title": "F More Ablation Study of Subset Selectioin Strategies",
            "content": "Key questions surrounding sparse momentum include how to effectively select parameter subset and whether to retain momentum for weights that are sampled multiple times. To answer this questions, we conduct comparative studies based on LLaMA-60M and C4 and the results are shown in Figure 8. Figure 8-Left shows the performence of three subset selection strategies where we will reset all moments after each momentum reset and keep gradients for all unselected parameters. Figure 8-Middle shows the performence of three subset selection strategies where we will keep the overlapped moments after each momentum reset and keep gradients for all unselected parameters. Figure 8-Right shows the performence of three subset selection strategies where we will reset all the moments after each momentum reset and drop gradients for all unselected parameters in each updating step. We observe the following: ❶ Among the three subset selection strategiesMax weight magnitude-based, Max gradient magnitude-based, and Random selectionthe Random selection consistently outperforms the other two approaches. ❷ Comparing Figure 8-Left and Figure 8-Right, we see that resetting all moments after each momentum reset yields better performance than preserving overlapping moments. Figure 8: Ablations for subset selection strategies. The experiments are conducted with LLaMA-60M on C4."
        },
        {
            "title": "Real Training",
            "content": "We also measure the values of gradient, first moment, and second moment during the training of LLaMA-60M on the C4 dataset. The results are now presented in Figure 9. From the figure, we observe that during actual training, gradient spikes also have significant and prolonged detrimental impact on moments, especially on the second moment, providing further evidence to support our claims. 20 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Figure 9: Gradient spikes have prolonged detrimental effects on the first and second moments. Experiments are conducted on C4 dataset with LLaMA-60M. Sensitivity Analysis of Hyperparameter θ on LLM"
        },
        {
            "title": "Architectures",
            "content": "We conducted experiments to evaluate the sensitivity of the gradient spike clipping threshold, θ, across three widely used LLM architectures: LLaMA, Pythia, and OPT. These experiments were performed on pre-training tasks using the C4 dataset. The final perplexity is reported in Table 13. The results indicate that the gradient spike clipping threshold is not highly sensitive to the choice of LLM architecture. SPAM consistently outperforms Adam across wide range of θ. Furthermore, the optimal range for θ lies between 1000 and 5000. Table 13: Sensitivity Analysis of Hyperparameter θ on LLM architectures. Perplexity is reported. Architectures LLaMA-60M Pythia-70M OPT-125M θ = 500 30.77 34.4 28. θ = 1000 30.59 34.1 28.4 θ = 2500 30.57 34.1 28.5 θ = 5000 30.46 34.2 28.6 θ = 10000 Adam 34.09 38.34 32.20 30.82 35.1 29.0 GSS VS. Distribution Based Clipping We conducted an experiment using an outlier detection mechanism based on the assumption that stochastic gradient distributions follow Gaussian distribution, as suggested in (Simsekli et al., 2019; Chaudhari & Soatto, 2018; Mandt et al., 2016): (cid:16) n=1 PN Gbatch (G, δ2I), where Gbatch is the stochastic gradient, represents the gradient over the entire dataset, and δ2 is the variance. Since calculating on-the-fly during training is computationally infeasible, we approximate it using the moving average of Gbatch. The variance δ2 is estimated batch G(n)(cid:17)2 G(n) , where is the total training steps. Gradients online as: δ2 = 1 are then evaluated element-wise, and any element G(n) satisfying: G(n) batch G(n) > 3δ is identified as an outlier. Such outlier elements are clipped to satisfy: G(n) batch G(n) = 3δ. We conducted experiments using LLaMA-60M and LLaMA-130M to evaluate the performance of this Gaussian-based Clipping and compare it with our proposed GSS-based clipping. The results are reported in Table 14. As the table indicates, Gaussian-based clipping falls short of our GSS-based clipping. One possible explanation is that stochastic gradient distributions are very complex and Gaussian distribution can not reflect the true distribution. batch 21 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Table 14: Comparison between SPAM with spike-aware clipping and Gaussian-based clipping. Methods LLaMA-60M LLaMA-130M SPAM w/GSS based clipping SPAM w/ Gaussian based Clipping 30.46 30.83 23.36 25.93 GSS based Clipping VS. Nullifying We conducted experiments on LLaMA-60M and LLaMA-130M to compare the performance of Spike-Aware Clipping and Nullifying Gradient Spikes. As shown in Table 15 and Table 16, SPAM with Spike-Aware Clipping outperforms SPAM with Nullifying on both pre-training and fine-tuning tasks, demonstrating the effectiveness of Spike-Aware Clipping. Table 15: Comparison between SPAM w/ spike-aware clipping and SPAM w/ nullifying gradient spikes. Methods SPAM w/ Spike Aware Clipping SPAM w/ Nullifying LLaMA-60M LLaMA-130M 30.46 30.86 23.36 23.62 Table 16: Comparison between SPAM w/ spike-aware clipping and SPAM w/ nullifying gradient spikes on fine-tuning task. The experiments are based on pre-trained OPT-1.3B model. Methods SPAM w/ Spike Aware Clipping (d=100%) SPAM w/ Spike Aware Clipping (d=0.25%) SPAM w/ Nullifying(d=100%) SPAM w/ Nullifying (d=0.25%) WinoGrande COPA 59.4 58.3 58.0 57.4 79.0 75.0 78.0 75."
        },
        {
            "title": "K Computational Analysis",
            "content": "We measured the running time per iteration for both LLaMA-60M and LLaMA-130M. The results, presented in Table 17, indicate that SPAM incurs slightly higher computational overhead compared to Adam, Adam-mini, and Adafactor. This overhead is primarily due to the gradient spike detection operation and the gradient selection based on sparse masks. However, we believe that such small overhead is negligible compared to the overall pretraining time which can be dozens or hundreds of hours."
        },
        {
            "title": "L Ablations",
            "content": "SPAM introduces two key components: momentum reset and spike gradient clipping, in contrast to Adam. To evaluate the efficacy of each component, we perform ablation studies using SPAM without spike gradient clipping (SPAM w/o SGC) and SPAM without momentum reset (SPAM w/o MR) on the LLaMA-130M model. The results, presented in Figure 18, demonstrate that removing either component leads to decline in performance, underscoring their effectiveness. 22 SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training Table 17: Running Time per Iteration (second). The runtime is measured by the average of 100 iterations under one H100 GPU. Method Time per Iteration (LLaMA-60M) Time per Iteration (LLaMA-130M) Adam Adam-mini Adafactor GaLore (rank=128) SPAM(d=100%) SPAM(d=25%) 0.3666 (s) 0.3614 (s) 0.3778 (s) 0.3871 (s) 0.3814 (s) 0.3799 (s) 0.6397 (s) 0.6472 (s) 0.6565 (s) 0.6702 (s) 0.6683 (s) 0.6658 (s) Table 18: Ablations for SPAM. Experiments are based on LLaMA-130M. Methods Adam SPAM w/o SGC SPAM w/o MR SPAM Perplexity 24.91 23.95 24.32 23."
        }
    ],
    "affiliations": [
        "Eindhoven University of Technology",
        "University of Exeter",
        "University of Leicester",
        "University of Oxford",
        "University of Texas at Austin"
    ]
}