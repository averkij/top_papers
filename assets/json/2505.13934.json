{
    "paper_title": "RLVR-World: Training World Models with Reinforcement Learning",
    "authors": [
        "Jialong Wu",
        "Shaofeng Yin",
        "Ningya Feng",
        "Mingsheng Long"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 4 3 9 3 1 . 5 0 5 2 : r RLVR-World: Training World Models with Reinforcement Learning Jialong Wu1, Shaofeng Yin1,2, Ningya Feng1, Mingsheng Long1(cid:0) 1School of Software, BNRist, Tsinghua University, 2Zhili College, Tsinghua University wujialong0229@gmail.com, mingsheng@tsinghua.edu.cn https://thuml.github.io/RLVR-World"
        },
        {
            "title": "Abstract",
            "content": "World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both languageand video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers promising posttraining paradigm for enhancing the utility of generative models more broadly."
        },
        {
            "title": "Introduction",
            "content": "World models [16, 30], which predict state transitions under action interventions, offer opportunities for autonomously evaluating and optimizing the behavior policies of intelligent agents [19, 20]. By scaling world models across various modalitiessuch as text [57, 8, 14], video [6, 1], and sensory data [46, 65]significant progress has been made in range of applications, including games [7, 27], robotics [64], and autonomous driving [22, 44]. Effectively scaling world models involves training highly expressive models, e.g., Transformers, on massive data. This critically depends on differentiable, rich, and stable training signals. In practice, world models are typically trained using surrogate objectives such as maximum likelihood estimation (MLE). For instance, language models are trained via next-token prediction, which supports reasoning through chain-of-thought generation prior to producing final answers [59], while diffusion models optimize variational lower bound of the log-likelihood [21]. Moreover, non-endto-end architectures that incorporate separately trained components like visual tokenizers [10, 43] enhance training efficiency and stability, especially for large-scale models. This paradigm has built powerful probabilistic world models, as well as many other foundation models [4]. However, the ultimate objectives of world models are beyond capturing complex distributions of data, but to meet the usage requirements of transition prediction metrics, such as high accuracy or perceptual quality. Surrogate and non-end-to-end optimizations are agnostic of and often diverge from this. In fact, training video models with standard mean squared error is known to produce blurry predictions [37], and in language models, likelihood-based objectives have been linked to issues like repetition and hallucination [31, 26, 56]. promising emerging approach is tuning pre-trained models to directly optimize toward the target task via reinforcement learning with verifiable rewards (RLVR) [29, 15], which replaces the learned reward model in reinforcement learning from human Preprint. Figure 1: Training world models with reinforcement learning. (Left) World models are typically pre-trained using surrogate objectives like MLE, which misalign with task-specific prediction metrics. (Right) We propose post-training world models with RLVR to directly optimize these metrics. feedback (RLHF) [40] with faithful, rule-based reward function. Using this approach, the language model community is now producing impressive advances in tackling complex math reasoning and code generation problems. In this work, we explore the RLVR paradigm for training world models, referred to as RLVR-World. We first propose to unify world modeling across diverse modalities into general autoregressive generation framework. Concretely, current states and actions are encoded as sequence of question tokens using modality-specific tokenization schemes, while next states are encoded as response tokens, mirroring the language model formulation. Within this unified framework, we then comprehensively investigate RLVR for world models on two representative modalities: Language world models: Beyond the success of large language models (LLMs) in math and code domains, we introduce the world modeling task as new testbed for RLVR in LLMs. This task, predicting the transition of verbal world states, naturally lends itself to using prediction accuracy as verifiable reward. Our experiments show that RLVR can effectively fine-tune LLMs as language world models, yielding significant improvements, including +30.7% accuracy on text-based game state prediction [57] and +15.1% F1 score on web page state prediction [8]. Video world models: We pioneer the RLVR fine-tuning of autoregressive video world models [61] by directly measuring and optimizing perceptual metrics of decoded predicted frames against ground-truth observations. Notably, our method achieves substantial gains, e.g., +9.2% relative improvement on LPIPS [68], on robot manipulation trajectory prediction [5], with merely few hundred RLVR gradient steps, in contrast to the hundreds of thousands required by MLE training to achieve. We further prove the effectiveness of RLVR in bridging the gap between pre-trained models and the target world modeling task by showing that it mitigates the repetition issue, phenomenon commonly seen in LLMs [31] and also observed in our pre-trained video world model. Finally, we demonstrate the utility of reinforced world models in downstream applications, including policy evaluation [32] and model-predictive control [8]. We hope our method, experiments, and analysis will inspire future research to apply RLVR as general post-training paradigm to significantly boost the usefulness of world models, and more broadly, generative models."
        },
        {
            "title": "2 Related Work",
            "content": "World models. Learning accurate world models to predict environment state transitions in response to actions is fundamental to model-based planning and reinforcement learning [52]. Due to the inherent complexity and uncertainty of real-world environments, world models are more commonly implemented as generative models of next states conditioned on current states and actions [9, 16], rather than as deterministic models [45, 20]. For visual observations, sequential variational autoencoders have been widely adopted [25, 18, 17, 60], while recent advances have seen the emergence of diffusion-based world models [64, 2, 7], which offer superior visual fidelity. Another important line of work formulates world models autoregressively by discretizing visual inputs into sequences of tokens [39, 22, 61, 1]. This approach naturally extends to modeling additional modalities through unified token-based state representations. For example, previous work explores language-based state 2 representation [57], which has been applied for building world models of the Internet in web agents [8, 14]. Similarly, trajectories of proprioceptive sensors and actuators can also be quantized into token sequences [46, 65]. Motivated by the prospect of building multimodal world models [28, 34] and the potential to leverage the large language model (LLM) ecosystem, our work explores the RLVR paradigm for training world models under the unified autoregressive formulation. RL for generative models. Reinforcement learning has emerged as critical paradigm for posttraining generative models to better align with human preferences or task-specific objectives. In language models, InstructGPT [40] employs reinforcement learning from human feedback (RLHF) to enhance harmlessness, helpfulness, and honesty. However, RLHF is susceptible to reward model In contrast, DeepSeek-R1 [48, 15] adopts reinforcement learning with overoptimization [13]. verifiable rewards (RLVR), achieving significant advances in math, code, and logical reasoning domains. For visual generative models, text-to-image diffusion models have also been fine-tuned via reinforcement learning [3, 11] to optimize measurable metrics (e.g., compressibility) or human evaluations [62]. Our work identifies world modeling as an underexplored yet natural fit for RLVR, where prediction accuracy serves as task-aligned, verifiable reward, enabling direct optimization of generative models across various modalities as world models."
        },
        {
            "title": "3 Preliminaries",
            "content": "This section provides brief background on visual tokenization for unifying visual and verbal state representations, along with the reinforcement learning algorithm used in our work. Visual tokenization. Given an image RHW 3, the encoder of discrete visual tokenizer [55] maps to its latent representation Rhwd. This latent is then quantized by performing nearest neighbors lookup in codebook of embeddings = {ei}K i=1, yielding discrete representation [K]hw, which is passed through decoder to reconstruct the original image x. The token map can be flattened into 1D sequence of length and subsequently modeled by autoregressive models such as decoder-only Transformers [10]. For videos in RT HW 3, straightforward approach is to tokenize each frame independently using an image tokenizer [39, 7, 34]. However, this leads to excessively long token sequences. To mitigate this, Wu et al. [61] propose compressive tokenization method that exploits temporal redundancy in videos by tokenizing each frame into reduced number of tokens zt [K1]n, conditioned on shared context tokens zc [K2]N . Group relative policy optimization (GRPO) [48] is originally developed for post-training LLMs with reinforcement learning. Compared to PPO [47], GRPO eliminates the need for value function and estimates advantages in group-relative manner. Specifically, given question q, GRPO samples group of responses {oi}G i=1 from the behavior policy pθold, and computes the advantage of each response by normalizing its reward Ri within the group: ˆAi,t = Ri mean({Ri}G std({Ri}G i=1) i=1) . (1) (cid:33)(cid:35) (2) Similar to PPO, GRPO uses clipped objective with KL divergence penalty: JGRPO(θ) = qD,{oi}G (cid:34) 1 (cid:88) i=1 1 oi i=1pθold (q) oi (cid:88) (cid:32) min t=1 (cid:16) pi,t θ pi,t θold ˆAi,t, clip (cid:16) pi,t θ pi,t θold , 1 ε, 1 + ε (cid:17) (cid:17) ˆAi,t βDKL [pθpref] , where pi,t θ denotes pθ(oi,t q, oi,<t) for simpilicity. Refer to Shao et al. [48] for more details."
        },
        {
            "title": "4 RLVR-World: Training World Models with RLVR",
            "content": "As illustrated in Figure 2, this section introduces RLVR-World, unified framework for training world models across various modalities by reinforcement learning with verifiable rewards. 4.1 Problem Formulation Environments simulated by world models are typically formulated as Markov decision process (MDP) = (S, A, p, r, γ). Depending on the target task, the state space can flexibly include 3 Figure 2: Illustration of RLVR-World framework. World models across various modalities are unified under sequence modeling formulation, and task-specific prediction metrics serve as verifiable rewards. (Top) Language-based world models predict verbal state transitions in response to verbal actions. (Bottom) Video-based world models, equipped with visual tokenizer, predict future visual observations conditioned on action vectors. various modalities, such as verbal, visual, or proprioceptive signals. At each timestep, the agent observes state st S, takes an action at A, then transits to new state according to the distribution p(st+1 st, at), and receives an immediate reward rt r(st, at). More generally, the states can be partially observable and the transitions can be modeled as k-order Markov process p(st+1 stk+1:t, atk+1:t) 1. World models need to learn to approximate the state transition and the reward function accurately. Since rewards can be considered as an extended dimension of the state space [9], our focus is on modeling the transition distribution p(st+1 stk+1:t, atk+1:t). 4.2 World Models as Sequence Modeling While different architectures have been proposed for world models on top of different modalities, next-token prediction by decoder-only Transformers has emerged as general-purpose formulation applicable to tasks across various modalities. We unify world models into this general sequence modeling framework. To transform states and actions into tokens, different modalities have unique, commonly used tokenization schemes: Languages are processed by standard text tokenization techniques like BPE [12]. Images and videos are encoded by learned visual tokenizers. Low-dimensional continuous values, e.g., proprioceptive signals or joint torques, can be quantized to uniform bins over fixed range. Then, analogous to language models, we use manually designed templates to construct input token sequences q(s, a) as \"questions\" and output sequences o(s) as \"responses\". For simplicity and clarity, here we take the first-order Markov case p(s s, a) as an example, but the sequence modeling formulation can naturally extend to higher-order cases. We assume an existing world model pre-trained via maximum likelihood estimation (MLE): JMLE(θ) = log pθ(o(s) q(s, a)) = o(s) (cid:88) t=1 log pθ(ot(s) q(s, a), o<t(s)). (3) 1While partial observations are commonly denoted ot in classical RL literature, we keep using st to avoid confusion with the generated outputs {oi}G i=1 in our RLVR context. 4 4.3 Prediction Metrics as Verifiable Rewards i=1, from which the predicted next states ˆs We then post-train the world model using RLVR to directly optimize verifiable metrics for state transition prediction. Specifically, given an input q(s, a), the pre-trained model generates group of samples {oi}G are extracted using modality-specific decoding schemes, such as rule-based extractor for language and visual decoder for videos. The reward is computed by comparing each prediction in the group to the ground-truth next state s: Ri = sign(D) D(ˆs (4) where sign(D) = 1 if lower values of the metric indicate better predictions (e.g., mean squared error or perceptual loss for visual observations), and sign(D) = 1 otherwise. Using this task-oriented reward, we can fine-tune the world model according to the RL objective in Eq. (2). i, s), Remarks. (1) Instead of solving the original environment MDP in Section 4.1, we focus on learning its transition distribution by formulating the next-state prediction process as another MDP, optimized using RLVR. (2) RLVR-World is general framework, where input/output sequences and reward functions can be domain-specifically designed. We describe them in the experimental sections. (3) Our framework is compatible with various RL algorithms [47, 66], not limited to GRPO."
        },
        {
            "title": "5 Evaluating Language World Models with RLVR",
            "content": "In the following sections, we evaluate the effectiveness of our framework for training world models across different modalities, particularly language (Section 5) and video (Section 6), using RLVR. Inspired by the success in domains like math and code generation, we begin by evaluating our framework on world modeling as new verifiable task for LLMs, focusing on two domains: text games (Section 5.1) and web navigation (Section 5.2). Experimental details can be found in Appendix A. 5.1 Text Game State Prediction Dataset and task. We use ByteSized32-State-Prediction [57], dataset of text game state transitions for evaluating how well LLMs can serve as text-based world simulators. The dataset contains 76,369 transitions from 31 distinct text games, with 2954 high-quality transitions selected for testing. In this task, an LLM models the world simulator function : , where represents natural language contexts describing the task and action semantics, is the state space encoded as JSON objects, is the action space, is the task reward, and = {0, 1} indicates task completion. World model. We use DeepSeek-R1-Distill-Qwen-1.5B [15, 63] as our base model. Due to its limited capability, we first apply supervised fine-tuning (SFT) using responses generated by DeepSeek-R1 [15], and then fine-tune with RLVR using either binary accuracy reward = ((ˆs, ˆr, ˆw) = (s, r, w)), or task-specific one to reflect the problem structure (see Appendix A.1). Results. The results are shown in Table 1. Since predicting state transitions is inherently more difficult when action changes the state, all models perform significantly better on unchanged cases. Compared to SFT, our RLVR-World with minimalist binary reward can substantially improve performance, achieving +34.7% accuracy for unchanged and +8.9% for changed cases. Further incorporating human knowledge through tailored reward yields even larger gains (+44.8% unchanged, +9.6% changed). These results enable our 1.5B model to rival the overall performance of GPT-4, although the accuracy on changed cases still falls short due to the limited base models capacity. Table 1: Language world model: text game state prediction. The test set is divided into unchanged and changed subsets, depending on whether the ground-truth next state differs from the current state. Model Accuracy Unchanged Changed Overall Base (R1-Distill-Qwen-1.5B) SFT RLVR-World (Ours, binary) RLVR-World (Ours, task-specific) 11.98% 38.88% 73.57% 83.66% 0.08% 7.11% 24.21% 32.87% 33.14% 57.01% 33.80% 63.24% GPT-4 [57] 73.90% 51.60% 64.76% 5 Table 2: Language world model: web page state prediction and model predictive control for web agents. : relative performance gains from RLVR. Model Web Page State Prediction Web Agent Success Rate Precision Recall F1 Base (R1-Dist.-Qwen-1.5B) SFT RLVR-World (Ours) 15.59% 15.70% 11.83% 48.99% 56.05% 49.94% 72.77% 64.55% 65.11% n/a 12.06% 14.29% +48.5% +15.1% +30.3% +18.4% 5.2 Web Page State Prediction Dataset and task. We further evaluate our approach on more realistic web navigation scenarios, using web page state transition dataset collected by WMA [8] from the WebArena benchmark [69]. For training and testing, we select 7K-sample subset consisting of shorter-length samples to avoid out-of-memory issues during training. In this task, websites state is represented by its accessibility tree, which is simplified from its Document Object Model (DOM) tree, and an LLM is used to predict state transitions after user actions such as clicking. Item changes in the accessibility tree caused by actions are extracted using the Hungarian algorithm, enabling the model to predict these changes directly. Unlike WMA, which generates natural language descriptions of state changes, this design choice facilitates clear verification during the RLVR stage. World model. Following the previous setup, we adopt DeepSeek-R1-Distill-Qwen-1.5B as our base model. We first perform supervised fine-tuning (SFT) using chain-of-thought (CoT) data provided by WMA [8]. Subsequently, we apply RLVR, using the F1 score between predicted item changes ˆs and ground truth as the reward function: = F1(ˆs, s). The F1 score, defined as the harmonic mean of precision and recall, is computed by treating precision as the proportion of correctly predicted item changes among all generated ones, and recall as the proportion of correct predictions relative to the ground-truth item changes. An item change is considered correct only if it exactly matches the corresponding ground truth. Results. As shown in Table 2, the world model of the Internet can also be enhanced substantially by RLVR2, leading to the first key finding of our experiments: Finding on language world models: Beyond its success in math and coding, RLVR can also improve LLMs performance on world modeling tasks involving verbal state transitions. 5.3 Application: Model Predictive Control for Web Agents Lastly, we show that the reinforced language world models enable more powerful web agents. Setup. Following WMA [8], we build web agents for the WebArena benchmark [69], composed of three components: policy model, world model, and value model. The model predictive control pipeline proceeds as follows: the policy model first proposes multiple candidate actions; the world model then predicts the outcomes of these actions; finally, the value model scores each predicted outcome based on the task goal. The action with the highest score is selected for execution. For both the policy and value models, we use DeepSeek-V3 [33], while the world model is taken from our trained models in the previous section. Additional details are provided in Appendix A.3. Results. In Table 2, we compare web agents on top of SFTand RLVR-trained world models and observe significant improvements. We expect that further gains can be achieved by incorporating stronger policy and value models and extending maximum context length during world model training."
        },
        {
            "title": "6 Evaluating Video World Models with RLVR",
            "content": "We then take pioneering step in evaluating RLVR to train autoregressive video world models, offering analyses and insights into broader generative models beyond the scope of reasoning models. 2Due to our novel setup for predicting precise item changes instead of natural language descriptions, direct comparison with established methods like WMA is not feasible. We therefore compare only to our base models. 6 Table 3: Video world model: robot manipulation trajectory prediction. We report the mean and standard deviation for each metric calculated over three sampling runs. MSE, LPIPS, and SSIM scores are scaled by 100 for better readability. : relative performance gains from RLVR. Task Model Repetition Rate MSE PSNR SSIM LPIPS Single-step Prediction Base RLVR-World (Ours) Multi-step Prediction Base Base (w/ repetition rejection) RLVR-World (Ours) n/a n/a n/a 48.6% 0.0% 9.9% 0.3360.002 0.2870. 25.30.03 25.90.01 81.70.07 83.10.00 13.00.04 12.20.01 +14.3% +2.6% +1.6% +6.0% 0.6590.006 0.5930.002 0.4860.003 23.10.01 23.30.01 24.10.02 80.90.03 81.00.02 82.40.02 14.80.02 14.40.01 13.40.02 +79.6% +26.1% +4.5% +1.9% +9.2% Figure 3: Learning curves of video world models. Note the significant difference in the x-axis scale between the pre-training and post-training stages. 6.1 Setup Dataset and task. We use the RT-1 robotic manipulation dataset [5], which contains 87,212 tabletop teleoperation trajectories collected from Google Robot, with 1% left for testing. Each frame of visual observation has resolution of 256 320, and the action space consists of 13 dimensions, including arm and base movement. We evaluate world models on two task settings: (1) Single-step prediction, formulated as p(st+1 st3:t, at3:t): predicting the next observation given the past four-step observations and actions; (2) Multi-step prediction, formulated as p(st+1:t+7 st, at:t+6) = (cid:81)t+7 i=t+1 p(si st:i1, at:i1): predicting next seven-step observation conditioed on the current observation and future action sequence. Predictions are assessed against ground-truth observations using frame-level metrics: MSE, PSNR [24], SSIM [58], and LPIPS [68]. World model. Since no off-the-shelf video world models are available, we pre-train variants of iVideoGPT [61] as base models by ourselves. For each trajectory segment, observations and actions are tokenized and concatenated into unified token sequence. We train an image tokenizer to independently tokenize video frames for single-step prediction, but train compressive tokenizer from iVideoGPT to mitigate sequence length explosion for multi-step prediction. Each action dimension is discretized into 256 uniform bins, determined based on its value range across the entire dataset. During RLVR fine-tuning, we define the reward function as the sum of L1 and perceptual loss between decoded predicted and ground-truth frames: = (cid:80)t+T τ =t+1 [L1(ˆsτ , sτ ) + LPIPS(ˆsτ , sτ )], commonly used in visual tokenizer training [10]. See implementation details in Appendix A.4. 6.2 Main Results As shown in Table 3, RLVR-World significantly improves the base model across all visual metrics, demonstrating more accurate and perceptually better video predictions, showcased in Figure 6. We highlight that these gains are achieved with only hundreds of RLVR gradient steps, compared to hundreds of thousands required for MLE pre-training (see the training curves in Figure 3). Even after continuing pre-training for multi-step prediction with 150k additional steps1000 more than RLVR fine-tuningthe resulting LPIPS score remains at 14.5, still substantially lagging behind. Figure 4: Model analysis. (a) Test-time scaling: best performance among different numbers of generated samples. (b) RL training scaling: learning curves using different group sizes in GRPO. (c) Metric-oriented optimization: RLVR-World trained and tested on different visual metrics. 6.3 Model Analysis Mitigating repetition. Prior studies [31] have identified the likelihood objective as primary cause of repetition in LLM generation. We observe similar phenomenon in multi-step video prediction, as showcased in Figure 6. This likely stems from the fact that approximately 20% of tokens in each frame remain unchanged from the previous frame, encouraging the model to exploit this shortcut. By directly optimizing video-level prediction metrics rather than next-token likelihood, RLVR effectively mitigates this issue, reducing the repetition rate from 48.6% to 9.9%. To ensure our improvements are not merely due to reducing repetition, in Table 3, we include an additional baseline that repeatedly queries the base model for predictions until non-repetitive output is sampled. Metric-oriented optimization. To further demonstrate the effect of direct metric optimization, we post-train five variants of the base model using five different metrics as reward functions: MAE, MSE, PSNR, SSIM, and LPIPS. As shown in Figure 4, models fine-tuned with specific metric generally achieve the best performance when evaluated on that same metric. Test-time scaling. We evaluate the test-time scaling behavior of our base and RLVR-trained models by reporting the best metric achieved across samples [36] in Figure 4. RLVR-World improves oneshot performance, even outperforming the base models best-of-5 results. This is particularly valuable in practical scenarios where generating large numbers of samples is computationally expensive and ground-truth comparisons are unavailable. However, as increases to 100, the base model catches up and eventually surpasses RLVR-trained models, echoing findings from Yue et al. [67]. This suggests limitations of current RLVR methods and ample opportunities for future research. RL training scaling. While generating more samples at test time is expensive, Figure 4 shows that it is essential for training. Specifically, increasing the group size in GRPO improves both convergence speed and final performance by enhancing sample diversity and expanding the exploration space. Finding on video world models: RLVR bridges the gap between pre-training objectives and visual prediction metrics, leading to more accurate predictions, improved training efficiency, and reduced artifacts such as repetition. 6.4 Application: Real2Sim Policy Evaluation We finally show that our models can serve as real-world simulators for improved policy evaluation. Setup. Following SIMPLER [32], we evaluate four policy checkpoints from RT-1 [5] and RT-1-X [41] on six tasks involving opening and closing top, middle, and bottom drawers. Starting from real-world observation frame, policies can interact with video Figure 5: Real2Sim policy evaluation. Figure 6: Qualitative analysis: multi-step video prediction and policy evaluation on RT-1. world models to roll out neural simulated trajectories, allowing for policy evaluation without realworld deployment. Since our preliminary attempts on VLM-based automatic evaluation [53] fail to provide reliable judgments, we rely on human annotators to assess the success of simulated trajectories. Besides our base model, we compare against the simulators developed in SIMPLER. Experimental details can be found in Appendix A.5. Results. As shown in Figure 5, compared to handcrafted SIMPLER simulators, video world models yield smaller discrepancies between real and simulated success rates, suggesting world models as scalable approach to bridging the sim-to-real gap. Among the video world models, RLVR-World further improves upon the base model, achieving more accurate policy evaluation. Finding on model-based applications: RLVR-trained world models can improve downstream tasks, including policy evaluation and model predictive control."
        },
        {
            "title": "7 Discussion and Limitations",
            "content": "In this work, we pioneer RLVR for training world models across language and video modalities, with practical applications in web navigation and robotic manipulation. We believe RLVR has the potential to become general post-training paradigm for generative models. To this end, several challenges remain for future exploration: (1) Task-aligned rewards: While classical visual metrics align better with the world modeling task than MLE, they still fail to fully capture user-intended qualities. Incorporating constraints such as physical rules and temporal consistency will require more sophisticated reward designs. (2) Breaking performance barriers: Although RLVR yields significant gains, training typically converges within hundreds of steps. Unlocking continual improvements calls for deeper analysis of bottlenecks in models, data, and algorithms; (3) Out-of-distribution (OOD) generalization: Inspired by RLVRs success in enabling LLMs to generalize beyond training domains [49], it is important to study whether similar benefits extend to world models, particularly for counterfactual reasoning on OOD actions in sequential decision-making."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari. In NeurIPS, 2024. [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In ICLR, 2024. [4] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. RT-1: Robotics transformer for real-world control at scale. In RSS, 2023. [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [7] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In ICML, 2024. [8] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. In ICLR, 2025. [9] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in handful of trials using probabilistic dynamics models. In NeurIPS, 2018. [10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. [11] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. In NeurIPS, 2023. [12] Philip Gage. new algorithm for data compression. The Users Journal, 12(2):2338, 1994. [13] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In ICML, 2023. [14] Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al. Is your llm secretly world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559, 2024. [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In NeurIPS, 2018. [17] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In ICLR, 2020. [18] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In ICML, 2019. [19] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse control tasks through world models. Nature, pages 17, 2025. [20] Nicklas Hansen, Hao Su, and Xiaolong Wang. TD-MPC2: Scalable, robust world models for continuous control. In ICLR, 2024. [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 10 [22] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. [23] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. [24] Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics letters, 44(13):800801, 2008. [25] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for atari. In ICLR, 2020. [26] Adam Tauman Kalai and Santosh Vempala. Calibrated language models must hallucinate. In STOC, 2024. [27] Anssi Kanervisto, Dave Bignell, Linda Yilin Wen, Martin Grayson, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Tabish Rashid, Tim Pearce, Yuhan Cao, et al. World and human action models towards gameplay ideation. Nature, 638(8051):656663, 2025. [28] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. In ICML, 2024. [29] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [30] Yann LeCun. path towards autonomous machine intelligence. preprint posted on openreview, 2022. [31] Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, and Yixuan Su. Repetition in repetition out: Towards understanding neural text degeneration from the data perspective. In NeurIPS, 2023. [32] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. In CoRL, 2024. [33] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [34] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. In ICLR, 2025. [35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [36] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. [37] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016. [38] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. [39] Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models. In ICLR, 2023. [40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [41] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x models. In ICRA, 2024. 11 [42] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [44] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. [45] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. [46] Ingmar Schubert, Jingwei Zhang, Jake Bruce, Sarah Bechtle, Emilio Parisotto, Martin Riedmiller, Jost Tobias Springenberg, Arunkumar Byravan, Leonard Hasenclever, and Nicolas Heess. generalist dynamics model for control. arXiv preprint arXiv:2305.10912, 2023. [47] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [48] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [49] Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508, 2025. [50] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In EuroSys, 2025. [51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [52] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction. MIT press Cambridge, 1998. [53] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [54] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [55] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. [56] Chaojun Wang and Rico Sennrich. On exposure bias, hallucination and domain shift in neural machine translation. In ACL, 2020. [57] Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté, Peter Clark, and Peter Jansen. Can language models serve as text-based world simulators? In ACL, 2024. [58] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. [60] Jialong Wu, Haoyu Ma, Chaoyi Deng, and Mingsheng Long. Pre-training contextualized world models with in-the-wild videos for reinforcement learning. In NeurIPS, 2023. [61] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. In NeurIPS, 2024. [62] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In NeurIPS, 2023. [63] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [64] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In ICLR, 2024. [65] Shaofeng Yin, Jialong Wu, Siqiao Huang, Xingjian Su, Xu He, Jianye Hao, and Mingsheng Long. Trajectory world models for heterogeneous environments. arXiv preprint arXiv:2502.01366, 2025. [66] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [67] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [68] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [69] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. In ICLR, 2024."
        },
        {
            "title": "A Implementation Details and Extended Experiment Results",
            "content": "A.1 Text Game State Prediction Dataset detail. ByteSized32-State-Prediction [57] comprises 76,369 transitions collected from 31 distinct text-based games. curated subset of 2,954 high-quality transitions forms the official test set, which we use for both our SFT and RLVR experiments. The dataset includes task rules written either by human experts or large language models (LLMs). To minimize potential inaccuracies, we adopt the expert-written rules in our experiments. Each state is represented as JSON object, describing the environment objects and their associated properties (e.g., dish with \"clean\" or \"dirty\" status). Actions are described in natural language (e.g., clean the dish), and the world simulator aims to predict the resulting state, the task reward based on predefined rules (e.g., +1 for each clean dish), and task completion (e.g., when all dishes are clean). We categorize samples into \"unchanged cases\" when the action leaves the state unchanged, i.e., the ground truth next state is exactly the same as the current state, and \"changed cases\" otherwise. To reduce output complexity, as suggested by the original paper [57], we train our world models for state-difference prediction, generating updates solely for objects whose properties have changed. This design mitigates challenges in generating long, correctly formatted outputs with our 1.5B small base model. Prompt examples are provided in Appendix D. Supervised fine-tuning. To overcome the poor performance of our base model, DeepSeek-R1-Distill-Qwen1.5B3, especially its 0.08% accuracy on changed cases (see Table 1), we first apply supervised fine-tuning (SFT) with LoRA [23] to enable effective RL training. The ByteSized32-State-Prediction dataset lacks suitable SFT data, as it contains only final answers without intermediate chain-of-thought reasoning annotations. To remedy this, we prompt DeepSeek-R1 on subset of changed cases, apply reject sampling to collect the correct responses, and build training dataset of 4,237 samples within $40 budget. We fine-tune the model and report test accuracy across epochs in Figure 7. The checkpoint from epoch 10 is selected as the final SFT result. Figure 7: The learning curves of SFT for text game state prediction. RLVR. The reward is defined based on the difference between the models prediction and the ground truth. We explore two reward schemes: Binary reward assigns reward of 1 only if the prediction is completely correct; otherwise, the reward is 0. Task-specific reward considers three components: (1) accuracy over all properties accall, (2) accuracy over properties that are supposed to change accchanged, and (3) the binary reward. The final reward is computed as = α1 accall + α2 accchanged + α3 I(correct) (5) In our experiments, we use weights α1 = 0.1, α2 = 1, α3 = 0.2, which are chosen based on heuristic intuition without further tuning. To address data imbalancewhere over 85% of training samples are unchanged caseswe downsample unchanged cases while retaining all changed cases. For binary reward experiments, we set the changedto-unchanged ratio at 100 : 40, and for task-specific reward experiments, we use 100 : 5, since the task-specific reward more effectively teaches the model to output no change when appropriate. Empirically, increasing unchanged samples improves performance on unchanged cases but degrades changed case accuracy, aligning with our expectations. Though this ratio was tuned casually, our current setup sufficiently demonstrates RLVRs effectiveness in improving language-based world models. Hyperparameters for training are provided in Table 4. 3DeepSeek-R1-Distill-Qwen-1.5B follows the MIT License. 14 Table 4: Hyperparameters for language world model training."
        },
        {
            "title": "Hyperparameter",
            "content": "Batch size LoRA rank [23] LoRA α Epoch Learning rate Weight decay Max response length Batch size PPO (GRPO) mini batch size KL loss coefficient Group size Learning rate"
        },
        {
            "title": "Sampling",
            "content": "Top-p Temperature"
        },
        {
            "title": "Text Game Web Page",
            "content": "16 32 16 10 1 105 0.01 4096 128 64 1 103 5 1 106 1.0 1.0 8 32 16 4 1 105 0.01 7288 64 16 1 103 5 1 10 1.0 for prediction 0.95 for MPC 1.0 A.2 Web Page State Prediction Dataset detail. We use the dataset provided by the WMA repository4 [8]. As the dataset does not explicitly annotate precise changes to the accessibility tree in response to user actionsi.e., which items are added, removed, or updatedwe extract this information using the official script released by the authors5. An example of an item is: [1220] textbox ue60c focused: True required: False, which includes item ID, type, content, and additional attributes. We can then formulate the next-state prediction task as prompting the LLM to generate all changed items within the accessibility tree. To avoid out-of-memory (OOM) issues during supervised fine-tuning (SFT), we discard samples in which the total length of the prompt and the ground-truth response exceeds 5,000 tokens. After this filtering process, approximately 7,000 trajectories remain from the original 14,000. We allocate 99% of this subset for training and reserve the remaining 1% for testing. Supervised fine-tuning. For our new task formulation, we construct target responses for each sample in the WMA dataset by concatenating the chain-of-thought (CoT) annotations, which are generated by GPT-4o-mini and released by the authors, and our extracted next-state changes. We then perform supervised fine-tuning (SFT) with LoRA [23] on our base model, DeepSeek-R1-DistillQwen-1.5B. Training hyperparameters are summarized in Table 4. RLVR. We define the reward function using the F1 score, based on the predicted and groundtruth changed items. Let ˆs = {ˆc1, ˆc2, . . . , ˆcm} represent the set of predicted changed items, and = {c1, c2, . . . , cn} the set of ground-truth changed items. We define: True Positives (TP) = ˆs = {ˆc ˆs ˆc s} (6) (7) Precision (P) = TP ˆs , 1, 0, if ˆs > 0 if ˆs = 0 and = 0 if ˆs = 0 and > 4https://huggingface.co/datasets/LangAGI-Lab/world_model_for_wa_desc_with_tao_ dataset 5https://github.com/kyle8581/WMA-Agents/blob/main/dataset_construction/scripts/ annotation_for_tao.sh, under the MIT License 15 Recall (R) = TP , 1, 0, if > 0 if ˆs = 0 and = 0 if ˆs > 0 and = 0 F1 Score (Reward R) = (cid:26) 2PR P+R , 0, if + > 0 otherwise An item change is considered correct (i.e., true positive) only if it exactly matches ground-truth item change across all fields. For example, in the item change [1273] StaticText Vortex Running Shoes, the model must produce the exact same stringincluding all characters and formattingfor the prediction to be considered correct. In cases where no change occurs, the world model is trained to output special item None, which is treated as valid item and included in the F1 computation. RLVR hyperparameters are also listed in Table 4. We select the checkpoint that achieves the highest reward on the test set for final evaluation in the subsequent model predictive control experiments. A.3 Model Predictive Control for Web Agents Environments. To ensure consistency with prior work, all experiments are conducted using the official WebArena environment, deployed on an Amazon Web Services (AWS) EC2 instance preconfigured with Docker. WebArena covers five task domains: Shopping, Content Management Systems (CMS), Reddit, GitLab, and Mapping. Model predictive control. Following the method introduced in WMA [8], we use policy model to sample 20 candidate actions and select the three most frequently sampled for further evaluation. For each selected action, our trained world model performs next-state prediction. summarization model is then used to (1) identify the top 10 most salient state changes in the predicted next state, and (2) generate natural language summary describing those changes. This transition summary, along with the task objective, is provided to value model, which assigns score between 1 and 5. This scoring query is repeated 20 times, and the final score for each action is computed as the average of these 20 responses. The action with the highest average score is selected for execution. Except for the world model, all models, including policy, summarization, and value, are implemented by prompting DeepSeek-V3 with top-p sampling (p = 0.95). All prompts used in model predictive control are detailed in Appendix D. Domain-specific results. We report web agent performance on different domains in Table 5. Table 5: Domain-specific model predictive control performance for web agents. : relative performance gains from RLVR. Methods / Domains Shopping CMS Reddit Gitlab Map Overall SFT RLVR-World (Ours) 18.23% 11.54% 4.39% 6.63% 19.53% 12.07% 21.88% 10.99% 6.14% 10.71% 20.31% 14.29% +20.0% -4.8% +40.0% +61.5% +4.0% +18.4% A.4 Robot Manipulation Trajectory Prediction Hyperparameters of architectures and training process for robot manipulation trajectory prediction are listed in Table 6 and 7. Dataset. We use the RT-1 dataset released from Open X-Embodiment [41], which follows the Apache license and contains 87,212 trajectories with 256 320 visual observations and 13dimensional actions. Following Wu et al. [61], 99% of trajectories are used as the training set and 1% are left as the test set. During testing, fixed segment from each trajectory is used for prediction. Table 6: Hyperparameters for visual tokenizers in robot manipulation trajectory prediction. Unless specified, the compressive tokenizer shares the same hyperparameter values with the per-frame tokenizer."
        },
        {
            "title": "Value",
            "content": "Input resolution Layers per block Channels FSQ levels Token number Training steps Batch size Segment length Optimizer Learning rate L1 loss weight Perceptual loss weight Adversarial loss weight Discriminator layers Discriminator training start step Channels FSQ levels K1 Token number Context FSQ levels K2 Context token number Maximum cross-attention resolution Training steps Batch size Segment length Number of sampled frames 256 320 2 [128, 256, 256, 512, 768] 7 5 5 5 5 = 4375 16 20 = 320 5 105 16 8 AdamW [35] 5 104 1.0 1.0 0.1 6 10000 [128, 256, 256, 512] 7 5 5 5 5 = 4375 8 10 = 80 7 5 5 5 5 = 4375 32 40 = 1280 32 6 105 16 32 7 Per-frame Tokenizer Compressive Tokenizer [61] Visual tokenizer. For single-step and multi-step prediction settings, we train per-frame tokenizer and compressive tokenizer [61] respectively on RT-1 from scratch. The per-frame tokenizer is essentially VQGAN [10] with convolutional encoder-decoder architecture, which tokenizes each frame st in the trajectory independently into tokens zt [K]N where is the codebook size. The compressive tokenizer is implemented as conditional VQGAN with dual encoder-decoder pairs {(Ec, Dc), (Ep, Dp)}. The context encoder Ec first tokenizes context frame sc independently into context tokens zc [K2]N . Subsequently, each frame st is tokenized by Ep, conditioned on the context encoders feature maps through cross attention, into tokens zt [K1]n. Since the context features can capture rich shared information across the trajectory, the number of tokens per frame can be significantly reduced compared to independent tokenization (n ). We refer to Wu et al. [61] for further details on the compressive tokenizer. Architectural details of these tokenizers are provided in Table 6. Notably, we adopt finite scalar quantization (FSQ) [38] instead of vector quantization (VQ) [55] within our VQGANs, due to its superior codebook utilization. For per-frame tokenizer training, we sample batches of trajectory segments and independently reconstruct each frame to compute VQGAN losses. For compressive tokenizer training, the first frames of trajectory segments are used as context frames, and from the remaining frames, we randomly sample subset to reconstruct for training to reduce memory requirements. Sequence modeling formulation. We consider two task settings for video prediction and describe how token sequences are constructed for autoregressive Transformers: 17 Single-step prediction p(st+1 st3:t, at3:t): By denoting the tokenized representation of st and at as zt and bt respectively, the token sequence is constructed: = concat(zt3, bt3, zt2, bt2, , zt, bt, [bos], zt+1, [eos]), (8) where [bos] and [eos] are two special tokens6. Each dimension of actions is discretized into 256 uniform bins, with all dimensions sharing the same 256 codes. Visual and action codes are offset to avoid overlapping, resulting in total codebook size of 4633. The final sequence length is 4 (320 + 13) + 1 + 320 + 1 = 1654, where the underlined part of length 321 corresponds to output tokens used for computing the cross-entropy loss. Multi-step prediction p(st+1:t+7 st, at:t+6, sc) = (cid:81)t+7 Similarily, the token sequences is constructed as follows7: i=t+1 p(si st:i1, at:i1, sc): = concat(zc, zt, bt, zt+1, bt+1, zt+2, bt+2, . . . , zt+7, bt+7), (9) where zc represents the tokenized context frame. Codes for context tokens, per-frame tokens, and actions are offset to avoid index overlapping, resulting in total codebook size of 9006. The total sequence length is 1280 + 8 (80 + 13) = 2024. Only tokens of frames that need to be predicted contribute to the loss. During generation, predicted frame tokens and action tokens are appended to the sequence alternately. Autoregressive transformer. We adopt the standard architecture of LLaMA [54], instantiated as smaller models with 138M parameters, matching GPT-2 small [42]. Separate Transformers are pre-trained from scratch for single-step and multi-step prediction, respectively. During multi-step prediction training, context frames are sampled from earlier frames preceding the trajectory segment to prevent information leakage for prediction. Architecture and training are detailed in Table 7. RLVR. The pre-trained transformers are fine-tuned with GRPO. The reward function is defined as negative loss function of all predicted frames and ground truth. Specifically, for single-step prediction: R(ˆst+1, st+1) = L1(ˆst+1, st+1) LPIPS(ˆst+1, st+1); and for multi-step prediction: R(ˆst+1:t+7, st+1:t+7) = t+7 (cid:88) τ =t+1 [L1(ˆsτ , sτ ) + LPIPS(ˆsτ , sτ )] . (10) (11) We do not update visual tokenizers during fine-tuning. Model analysis. All experiments presented in Figure 4 are conducted in the single-step prediction setting using group size of 16, unless specified. Although we find that using group size of 32 yields slightly better performance, we report performance with group size of 16 in our main results  (Table 3)  to maintain consistency with the majority of our experiments. Training curves. We present the curves of training rewards during single-step prediction RLVR training in Figure 8. Qualitative analysis. Additional showcases of video prediction and policy evaluation are presented in Figure 9. A.5 Real2Sim Policy Evaluation Evaluated policies and baselines. We aim to evaluate popular open-source generalist policies for the Google Robot, including series of RT-1[5] checkpoints at different training stages: RT-1 trained to convergence (RT-1 (Converged)), RT-1 at 15% of training steps (RT-1 (15%)), and RT-1 at 6We note that these two tokens are not necessary in the sequence formulation but are included due to historical reasons. 7bt+7 is used here as placeholder for implementation convenience. Due to the causal architecture and the absence of loss terms on bt+7, it completely has no effect on the training process. 18 Table 7: Hyperparameters for transformers in robot manipulation trajectory prediction. Unless otherwise specified, transformers for single-step and multi-step prediction share the same hyperparameter values."
        },
        {
            "title": "Architecture",
            "content": "Layers Hidden size FFN intermediate size Number of attention heads RoPE θ [51] Vocabulary size Pre-training"
        },
        {
            "title": "Training steps",
            "content": "Optimizer Batch size Segment length Learning rate Sequence length RLVR Optimizer Batch size PPO (GRPO) mini batch size KL loss coefficient Group size Learning rate Weight decay 12 768 3072 12 10000.0 4633 for single-step prediction 9008 for multi-step prediction 9.9 105 for single-step prediction 4.5 105 for multi-step prediction AdamW [35] 32 5 for single-step prediction 8 for multi-step prediction 5 10 1654 for single-step prediction 2024 for multi-step prediction AdamW [35] 128 32 1 103 16 1 105 0.01 Sampling Top-k Temperature 100 1.0 Figure 8: Training curves of RLVR-World for single-step prediction: rewards, L1 losses, and perceptual losses. Figure 9: Additional qualitative analysis: multi-step video prediction and policy evaluation on RT-1. the beginning of training (RT-1 (Begin)), as well as RT-1-X [41]. SIMPLER [32] is collection of simulated environments for manipulation policy evaluation on common real robot setups, including different methods to align with real-world environments: \"SIMPLER-Visual Matching\" (short as SIMPLER-VM in our paper) and \"SIMPLER-Variant Aggregation\" (short as SIMPLER-VA). We compare our world models against SIMPLER as the representation for policy evaluation methods based on physical simulation, which requires substantial human effort to develop task-specific environments. Task selection. We experiment with three task types: \"pick coke can,\" \"move near,\" and \"open/close drawers,\" each further divided into subcategories (e.g., \"open top/middle/bottom drawer\"). In tasks involving the \"pick\" action, we observe high rate of false positive evaluations where models incorrectly predict successful trajectories despite poor action quality. This is likely due to the limitation in the expert-level training data, which lacks failed grasp attempts. As result, the model tends to predict successful grasps even when the robot arm remains some distance from the object. In contrast, the \"open/close drawers\" task offers more diverse training data, including several failure cases (e.g., drawer slipping from grasp), making it more suitable evaluation setting for both our base model and RLVR-World. Trajectory generation. For each task, we randomly sample 30 initial frames from the real-world RT-1 dataset to serve as initial observations for world model simulations. Trajectories are then generated iteratively: (1) the policy selects the next action based on the latest frame; (2) the world model predicts the subsequent frame given the action. Following SIMPLER [32], we limit each trajectory to maximum of 112 frames. Specifically, we use our trained world models for multi-step prediction, able to predict seven future frames conditioned on the current and initial context frame. To predict long trajectories, we apply sliding window approach, where the last predicted frame becomes the input first frame in the next generation round. To conclude, Our evaluation covers two world models (the base model and RLVR-World), four policies (RT-1 (Begin), RT-1 (15%), 20 RT-1 (Converged), and RT-1-X) [5, 41], six tasks (open/close top/middle/bottom drawer), and 30 trajectories per task, resulting in total of 2 4 6 30 = 1440 generated trajectories. Success judgment. We initially explored automatic evaluation of trajectory success using visionlanguage models (VLMs), including GPT-4o and Gemini 2.0 Flash API. However, both failed to provide reliable evaluations due to two main reasons: (1) results varied significantly with minor changes in the prompt, and (2) the models lack consistent criteriafor example, how much the drawer must be opened to be considered success varied across scenes and backgrounds. Therefore, we resort to human annotation for evaluation. To ensure consistency and rigor, single annotator is tasked with labeling success or failure, provided only with the final frame of each trajectory and the task description, without any information about the model or policy used. Quantitative results. The evaluated success rates of different methods are reported in Table 8. Table 8: Quantitative results for real2sim policy evaluation, corresponding to Figure 5."
        },
        {
            "title": "Task Model",
            "content": "RT-1 (Begin) RT-1-X RT-1 (15%) RT-1 (Converged) Open Drawer Close Drawer Real SIMPLER-VA SIMPLER-VM Base model RLVR-World Real SIMPLER-VA SIMPLER-VM Base model RLVR-World 0.0% 0.5% 0.0% 4.4% 3.3% 0.0% 13.2% 27.8% 20.0% 18.9% 51.9% 6.9% 29.6% 18.8% 33.3% 74.1% 51.9% 89.1% 56.7% 71.1% 70.4% 21.2% 46.3% 50.0% 62.2% 88.9% 32.3% 66.7% 76.7% 78.9% 81.5% 27.0% 60.1% 48.9% 62.2% 92.6% 37.6% 86.1% 81.1% 88.9%"
        },
        {
            "title": "B Computational Resources",
            "content": "Text game state prediction The SFT phase uses 4 80G A100 GPUs for 6.5 hours of training. The RLVR phase is conducted on 8 80G A100 GPUs over 22.5 hours. Both SFT and RLVR are implemented using the verl framework8 [50]. Web page state prediction SFT is performed on 8 40G A100 GPUs over 17 hours, while the RLVR training is conducted on 8 80G H100 GPUs for 25 hours. Both SFT and RLVR are implemented using the verl framework. Robotic manipulation trajectory prediction. All experiments in this domain are conducted on 40G A100 GPU cluster. For single-step prediction, we pre-train the tokenizer using approximately 360 GPU hours. Transformer pre-training takes 530 GPU hours, and RLVR post-training for 200 steps requires 3.5 hours with 4 40G A100 GPUs. For multi-step prediction, we pre-train the tokenizer with 480 GPU hours. Transformer pre-training takes 500 GPU hours, and RLVR post-training for 200 steps requires 10 hours with 4 40G A100. Both settings implement tokenizer and transformer pre-training with accelerate9 and RLVR post-training using verl."
        },
        {
            "title": "C Broader Impact",
            "content": "Academic research. In the era of foundation models, our work provides proof of concept that the task performance of generative models can be directly optimized using RLVR, demonstrated through the world modeling task. These results may inspire further research into this paradigm. For instance, task-aligned metrics rather than human preferences could be used to reinforce the visual 8https://github.com/volcengine/verl, following the Apache License. 9https://github.com/huggingface/accelerate, following the Apache License. 21 understanding capabilities of multimodal LLMs in tasks such as visual counting, object detection, and optical character recognition. Additionally, the effectiveness of RLVR as post-training strategy may further solidify the communitys preference for autoregressive generative models over alternatives like masked or diffusion models, as it allows leveraging the well-established LLM ecosystem with minimal modifications. Practical applications. Enhancing the accuracy of world models through RLVR has clear benefits for real-world autonomous agents, both in digital domains (e.g., web navigation) and physical settings (e.g., robot manipulation), as showcased in our experiments. By enabling agents to simulate the outcomes of actions before execution more accurately, RLVR-trained world models can improve agentic task performance and reduce the risk of harmful behaviors. However, as RLVR is relatively lightweight post-training stage, its effectiveness is ultimately bounded by the capabilities of the underlying base model. Continued efforts toward building more powerful foundation world models remain essential."
        },
        {
            "title": "D Prompt Examples",
            "content": "In this section, we present detailed prompts used in our LLM experiments to offer more intuitive understanding of language world models. These prompts are either directly taken from or adapted from prior work [57, 8]. Text game state prediction: We provide examples of the prompts in blue boxes, including the JSON-format game state, rules for action/object/score, as well as the complete prompt. Additionally, we provide an example answer for the state-difference prediction task. They are all kept identical to those in the original dataset. Web page state prediction and web agents : We present the prompts used in our study in orange boxes. Specifically, we include four types of prompts: Web State Prediction Prompt Example, Top-10 Salient Change Extraction Prompt, Next-State Change Summarization Prompt, and Web State Evaluation Prompt Example. The web state prediction prompt and next-change summarization prompt are modified versions of the original prompts proposed by WMA [8], while the top-10 salient change extraction prompt and value prediction (evaluation) prompt are used unchanged, adopted directly from WMA. 22 Text Game: State Difference Prediction Prompt Expample You are simulator of text game. Read the task description and the current environment observation description. Given the current game state in JSON, you need to decide the new game state after taking an action. Your response should be in the JSON format. It should have three keys: modified, removed, and score. The modified key stores list of all the object states that are added or changed after taking the action. Keep it an empty list if no object is added or modified. The removed key stores list of uuids of the objects that are removed. Keep it an empty list if no object is removed. The score key stores dictionary with three keys: score is the current game score, gameOver is boolean of whether the game is over, and gameWon is boolean of whether the agent won the game. If player earns score or wins/loses the game, you should reflect that change in the dictionary saved under the score key. Otherwise, you should set value of the score key to an empty dictionary.Note that while game states can be changed by actions, some game states may change over the time, which is described in the tick function of each object class. Note that while game states can be changed by actions, some game states may change over the time, which is described in the tick function of each object class. Here are two examples of both cases. Both examples are from the same example game. Example game task description: Your task is to wash the dirty dishes. Here are the descriptions of all game objects properties in the example game: {OBJECT_RULES} Here are descriptions of all game actions in the example game: {ACTION_RULES} Here is description of the score function of the example game: {SCORE_RULES} In the first example, the game state is changed by an action: Current observation: {GAME_OBSERVATION} Here is the game state: {GAME_STATE} The action to take is put dirty plate (ID: 5) in mug (ID: 6) The expected response is: {GAME_STATE_DIFFERENCE} In the second example from the same example game, the game state is changed over the time. Note that while in this example the game state is changed by time only, it is possible that game state is changed by both an action and time. Current observation: {Example_2 observation} Here is the game state: {GAME_STATE} The action to take is eat dishwasher (ID: 2) with dirty plate (ID: 5) The expected response is: {GAME_STATE_DIFFERENCE} Here is the game that you need to simulate: Task Description: Your task is to boil water. Here are the descriptions of all game objects properties: {OBJECT_RULES} Here are the descriptions of all game actions: {ACTION_RULES} Here is description of the score function of the game: {SCORE_RULES} Current observation: {GAME_OBSERVATION} Here is the game state: {GAME_STATE} The current game UUID base is 12 The action to take is: look 23 Text Game: Game State Example {game_state: [{name: agent (ID: 0), uuid: 0, type: Agent, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: in}, contains: [plate (ID: 5), mug (ID: 6), knife (ID: 7)]}, {name: plate (ID: 5), uuid: 5, type: Dish, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: on, dishType: plate, isDirty: True, foodMessName: orange}, contains: []}, {name: mug (ID: 6), uuid: 6, type: Dish, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: in, dishType: mug, isDirty: True, foodMessName: sandwhich}, contains: []}, {name: knife (ID: 7), uuid: 7, type: Dish, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: in, dishType: knife, isDirty: True, foodMessName: apple (ID: 11)}, contains: []}, {name: dishwasher (ID: 2), uuid: 2, type: DishWasher, properties: {isContainer: True, isMoveable: False, isOpenable: True, isOpen: True, containerPrefix: in, isDevice: True, isActivatable: True, isOn: False, cycleStage: 0, finishedCycle: False}, contains: [cup (ID: 4)]}, {name: cup (ID: 4), uuid: 4, type: Dish, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: in, dishType: cup, isDirty: True, foodMessName: peanut butter}, contains: []}, {name: bottle of dish soap (ID: 3), uuid: 3, type: DishSoapBottle, properties: {isContainer: False, isMoveable: True, isDevice: True, isActivatable: True, isOn: False}, contains: []}, {name: glass (ID: 8), uuid: 8, type: Dish, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: in, dishType: glass, isDirty: False}, contains: []}, {name: bowl (ID: 9), uuid: 9, type: Dish, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: in, dishType: bowl, isDirty: False}, contains: []}, {name: banana (ID: 10), uuid: 10, type: Food, properties: {isContainer: False, isMoveable: True, isFood: True}, contains: []}, {score: -1, gameOver: False, gameWon: False}]} Text Game: Game State Difference Example {modified: [{name: agent (ID: 0), uuid: 0, type: Agent, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: in}, contains: [mug (ID: 6), knife (ID: 7)]}, {name: mug (ID: 6), uuid: 6, type: Dish, properties: {isContainer: True, isMoveable: True, isOpenable: False, isOpen: True, containerPrefix: in, dishType: mug, isDirty: True, foodMessName: sandwhich}, contains: [plate (ID: 5)]}], removed: [], score: {}} Text Game: Action Rule Example put: Description: put an object into target container Rules: 1. The target must be container (Container) 2. The target container must be open 3. The object must be in the inventory 4. The object must be moveable (isMoveable) Text Game: Object Rule Example Object: Container Description: Abstract class for things that can be considered containers (e.g. drawer, box, table, shelf, etc.) Properties: - Container is container. - Container could be opened (e.g., e.g. drawer, door, box, etc.), or is it always \"open\" (e.g. table, shelf, etc.). - Container has property indicating if it is opened. - Container has property indicating the prefix to use when referring to the container (e.g. \"in the drawer\", \"on the table\", etc.). By default, the prefix is \"in\" Text Game: Score Rule Example The player wins the game by getting all dishes clean. The player gets one point for each dish that is cleaned. The player loses one point for each dish that is made dirty. 25 Web Page: Web State Prediction Prompt Example You are an intelligent agent that predicts the next state from given current action using your own logical reasoning. You will be given web-based tasks. Heres the information youll have: - The users objective: This is the task youre trying to complete. - The current web pages accessibility tree: simplified representation of the webpage, providing key information. - The current web pages URL: This is the page youre currently navigating. - The previous action: The action you just performed. It may help you track your progress. - The current action: The action that you will perform next to achieve the users objective in the current accessibility tree. The format of previous and current actions can fall into several categories: Page Operation Actions: click [id]: Click an element with specific id. type [id] [content]: Type the content into the field with the specified id. By default, the \"Enter\" key is pressed after typing unless [0] is appended. hover [id]: Hover over an element. press [key_comb]: Simulate pressing keyboard combination (e.g., Ctrl+v). scroll [down] or scroll [up]: Scroll the page. Tab Management Actions: new_tab: Open new browser tab. tab_focus [tab_index]: Switch focus to specific tab. close_tab: Close the currently active tab. URL Navigation Actions: goto [url]: Navigate to specific URL. go_back: Go to the previous page. go_forward: Go forward (if previous go_back was executed). Completion Action: stop [answer]: Use this when you believe the task is complete. Provide the final answer in brackets if applicable. To succeed, its essential to understand how the current action impacts the next state of the webpage. You must verify whether the current action successfully produces the intended effect. Reasoning Guidelines: 1. Generate your answer using logical REASONING. 2. Directly predict the next state based on the current action. 3. Format your prediction exactly as described below. Format Description: The output is divided into three sections: New items: Deleted items: Updated items: Each section should be separated by an empty line. If section has no content, write None. 26 Web Page: Web State Prediction Prompt Example (continued) Each item must follow this format: [ID] type content [additional attributes] Details: - [ID]: Numeric ID in square brackets. - type: Element type (e.g., button, textbox, StaticText, etc.). - content: Elements displayed content in single quotes. Can be empty: . - [additional attributes]: Optional key-value pairs such as required: False, focused: True, etc. Example 1 New and Updated Items: New items: [1273] StaticText Vortex Running Shoes [1272] button Search Deleted items: None Updated items: [1220] textbox ue60c focused: True required: False Now, lets start the task. Heres the task for you: URL: {URL} OBJECTIVE: {OBJECTIVE} PREVIOUS ACTION: {PREVIOUS ACTION} CURRENT OBSERVATION: {CURRENT OBSERVATION} CURRENT ACTION: {CURRENT ACTION} Please give the result directly in the format of the next state prediction. Web Agent: Top-10 Salient Change Extraction Prompt Summarize the key changes in the web page based on the following information. New items: {new_items} Updated items: {updated_items} Deleted items: {deleted_items} When summarizing, follow this output format: 1. [First key change] 2. [Second key change] 3. [Third key change] ... 10. [Tenth key change] Web Agent: Next-State Change Summarization Prompt You are an intelligent agent that predicts the next state based on the current action using your own logical reasoning. You will be given web-based task. You will have access to the following information: The users objective: The task youre trying to complete. The current observation: simplified representation of the current webpage. The current URL: The webpage youre currently on. Previous actions: The action(s) performed just prior to the current one. Current action: The action being evaluated for its effect on the webpage. Key changes in next state observation: summary of differences between the current and next state observations. Action types may include: Page Operation Actions: click [id]: Click an element with the given ID. type [id] [content]: Type content into field with the given ID (append [0] to suppress Enter). hover [id]: Hover over an element. press [key_comb]: Simulate key combination presses (e.g., Ctrl+v). scroll [down] or scroll [up]: Scroll the page. Tab Management Actions: new_tab, tab_focus [index], close_tab URL Navigation Actions: goto [url], go_back, go_forward Completion Action: stop [answer]: Use this when the task is complete. Provide the answer in the brackets if required. Your task: Predict the next state with proper reasoning. Follow these rules: 1. Begin your answer with: [Rationale] Lets think step by step... 2. Your reasoning must mention the key changes in the next state observation. 3. Then, describe the next state based on those key changes. 4. Output the prediction in this format: [Next State] The expected effect is that ... Input fields: URL: {url} OBJECTIVE: {objective} PREVIOUS ACTION: {prev_action} CURRENT OBSERVATION: {cur_observation} CURRENT ACTION: {cur_action} KEY CHANGES IN NEXT STATE OBSERVATION: {tao} 28 Web Agent: Web State Evaluation Prompt Example You are an expert in evaluating and guiding web navigation agent. Your task is to help the agent effectively complete given mission on website based on the users intent. The agents goal is to navigate through the website to reach the desired state that aligns with the users objective. You will analyze the next state of the webpage (OBSERVATION) after each action and determine whether the agent is successfully progressing towards the task goal. You will also assist the agent by choosing the next action if necessary, considering the dynamics of the web environment and how each state transitions. Key Points: 1. Understand the Intent: Identify the users goal (e.g., finding information, navigating to specific page, modifying content). Ensure the next state of the webpage aligns with achieving that goal based on the current state and users intent. 2. Evaluate the Next State: Assess how the next state contributes to reaching the intended goal. If the next state moves the agent closer to the users goal, evaluate it positively. If the next state does not progress towards the goal or leads to an error, suggest alternative actions. 3. State Guidance: If the agent is on the right track but hasnt completed the task yet, recommend further actions. Guide the agent toward state that reflects clear progress towards the goal. 4. Types of Tasks: Information Seeking: The next state must provide specific information (e.g., product price, reviews). Site Navigation: The next state must reflect navigation to the exact page or item. Content Modification: The next state must show that content has been successfully modified. General Task: The final state should reflect that the task is completed. Only issue stop action when the objective is met. 5. Common Pitfalls: Avoid corrupted input from repeated typing actions. Ensure the agent navigates to the specific item or content, not just to general page. Output Format with Likert Scale: Each next state will be evaluated on fine-grained scale from 1 to 5: 1: The next state is failure or leads away from the task. 2: The next state does not contribute meaningfully. 3: The next state is neutral; the agent is maintaining position. 4: The next state is helpful; progress is being made. 5: The next state is optimal; directly aligns with the task goal. Response Instructions: Write rationale providing detailed analysis of the next state and justify your chosen score. Output Format: [Rationale]: <your thought> [Score]: <15>"
        }
    ],
    "affiliations": [
        "School of Software, BNRist, Tsinghua University",
        "Zhili College, Tsinghua University"
    ]
}