{
    "paper_title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems",
    "authors": [
        "Peiran Li",
        "Xinkai Zou",
        "Zhuohang Wu",
        "Ruifeng Li",
        "Shuo Xing",
        "Hanwen Zheng",
        "Zhikai Hu",
        "Yuping Wang",
        "Haoxi Li",
        "Qin Yuan",
        "Yingmo Zhang",
        "Zhengzhong Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 4 6 5 7 0 . 6 0 5 2 : r SAFEFLOW: Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems Peiran Li1,4,8, Xinkai Zou2, Zhuohang Wu3, Ruifeng Li4, Shuo Xing1, Hanwen Zheng4 Zhikai Hu5, Yuping Wang6, Haoxi Li7, Qin Yuan4, Yingmo Zhang4 Zhengzhong Tu1 1Texas A&M University 2UC San Diego 3UC Irvine 4University of WisconsinMadison 5Carnegie Mellon University 6University of Michigan {lipeiran, tzz}@tamu.edu, x9zou@ucsd.edu 7Columbia University 8Meta"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, todays agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy. Warning: This paper contains examples that are offensive, biased, and unsettling."
        },
        {
            "title": "Introduction",
            "content": "Autonomous agent frameworks have recently been at the forefront of AI research, evolving from rulebased expert systems and symbolic planners towards todays language-based agents. Early intelligent agents operated within constrained environments, typically lacking generalizable reasoning and adaptive decision-making capabilities (Cheng et al., 2024a). The recent breakthroughs in large language models (LLMs) and vision-language models (VLMs) have sparked an explosion in agent Equal Contribution. Corresponding author. Preprint. capabilities. Modern agent frameworks leverage LLMs as the core reasoning engine, enabling unprecedented levels of autonomy, complex multi-step decision-making, and interactive tool use via language. For example, ReAct demonstrated that interleaving reasoning traces with textual actions allows language models to plan and act in tandem (Yao et al., 2023). Libraries like LangChain and open-source agents (e.g., AutoGPT and BabyAGI) further democratized and popularized this paradigm, chaining LLM prompts to create flexible real-world task-solving agents (Chase, 2022). Multi-modal extensions soon followed: systems such as HuggingGPT orchestrate an LLM with suite of expert models for vision, speech, and more modalities, while MM-ReAct and related approaches integrate visual perception into the ReAct loop (Shen et al., 2023; Yang et al., 2023). These advances mark new era of generally-capable agents operating across web, software, operating system, as well as physical environments. However, alongside their impressive capabilities, current LLM/VLM-based agents exhibit critical shortcomings in reliability and trustworthiness (Liu et al., 2024a; Xing et al., 2024). By default, existing frameworks do not track the provenance or integrity of information they consume and produce (Siddiqui et al., 2024). As result, low-quality or malicious inputs can easily corrupt an agents behavior. For instance, hidden instructions on webpage can hijack an LLM-based web agent (i.e., prompt injection), leading it to divulge confidential data or execute harmful commands (Euler). Security analyses have shown that even state-of-the-art agents (e.g., based on GPT-4) remain vulnerable to these exploits, which succeed with alarming reliability. Furthermore, VLM-based agents that integrate vision modalities exacerbate these concerns: the fusion of vision and language modalities introduces new attack surfaces (e.g., adversarial images) that current agents cannot robustly detect (Wu et al., 2025). single crafted image can cause multimodal agent to pursue an adversarys goals with high success rate, evidencing fundamentally inadequate safeguards (Xing et al., 2025c). In short, todays agent frameworks, despite impressive advances, lack system-level mechanisms for reliability, secure information flow, and resilience against adversarial inputs. To bridge this gap, we introduce SAFEFLOW, novel, principled protocol-level framework explicitly designed to build secure, reliable, and trustworthy LLM/VLM-based autonomous agents. Specifically, SAFEFLOWenforces fine-grained information flow control (IFC) through unified SafeFlowAgent-Level abstraction, which tracks the sensitivity and trustworthiness of every entity and information item across the agent system. By regulating how data is read, written, or propagatedbased on dynamic trust levelsSAFEFLOW prevents untrusted or malicious inputs from influencing high-integrity decisions. Unlike prior ad-hoc agent frameworks, SAFEFLOWsupports runtime-enforced confidentiality and integrity without relying on brittle, hardcoded rules. Moreover, to ensure scalable deployment, SAFEFLOW integrates transactional execution model with write-ahead logging, enabling step-by-step verification, recovery from partial failures, and replay of incomplete operations. Extending to multi-agent settings, SAFEFLOW introduces concurrency control and dependency-aware failure isolation to prevent race conditions and cascading faults. All security label adjustmentswhether for data or agentsare governed by trusted verifiers, ensuring minimal necessary exposure and full auditability. Collectively, these mechanisms transform agents from brittle prompt pipelines into principled systems capable of secure, trustworthy autonomous operation under adversarial and dynamic conditions. To evaluate our approach, we construct SAFEFLOWBENCH, comprehensive benchmark suite that stress-tests agents under adversarial, deceptive, and noisy conditions. Notably, it is, to our knowledge, the first-of-its-kind benchmark specifically designed to evaluate VLM-based agent safety in GUI-based environments, thereby addressing critical gap overlooked by existing agent benchmarks. Unlike prior benchmarks primarily measuring task completion accuracy, SAFEFLOWBENCH evaluates system-level properties, including ❶ assessing system-level security, ❷ integrity of information handling, and ❸ resilience to multimodal threats. Specifically, SAFEFLOWBENCH comprises two synergistic components: (1) the Multimodal Threat Stress Test (MTST), which introduces structured taxonomy of attacksincluding visual deception, interface manipulation, and system-level exploitsacross 332 scenarios drawn from real-world and synthetic digital environments; and (2) the Concurrent Agent Reliability Test (CART), which evaluates coordination, synchronization, and failure recovery across 25 high-contention multi-agent scenarios simulating realistic resource-sharing challenges. By rigorously capturing both single-agent operational safety and the complexities of multi-agent concurrency, parallel execution, and communication conflict,SAFEFLOWBENCH uniquely sets new standard for comprehensive autonomous agent evaluation. Empirical results demonstrate that agents built with SAFEFLOWcan maintain robust 2 performance and trust guarantees even in hostile, ambiguous, and high-traffic multi-agent environmentsmarking significant step toward trustworthy autonomy in the era of large-scale foundation model agents. Our contributions can be summarized as follows: We propose SAFEFLOW, the first safety framework designed for VLM-based agents, while remaining fully compatible with LLM-based agents. It integrates fine-grained information flow control (IFC), provenance-aware reasoning, and concurrency-safe coordination mechanisms, enabling agents to make decisions that are secure, interpretable, and resilienteven in adversarial or multi-agent environments. We introduce SAFEFLOWBENCH, comprehensive benchmark suite for evaluating agent trustworthiness under stress, which is, to our knowledge, the first specifically designed for assessing VLM-agent safety in GUI-based environments. Beyond traditional task completion metrics, SAFEFLOWBENCH assesses information integrity, system-level safety, adversarial robustness, and conflict handling in concurrent multi-agent scenarios. Empirical results show that agents equipped with SAFEFLOW significantly outperform existing baselines. Compared to state-of-the-art agent frameworks, SAFEFLOW-based agents achieve superior task completion rates while maintaining high standards of security, information integrity, and robustness, demonstrating practical trustworthiness without compromising capability."
        },
        {
            "title": "2 Related Work",
            "content": "Modern Agent and Agent Safety The emergence of LLMs has catalyzed surge in agent frameworks that use language models as central planners coordinating external tools. ReAct introduced prompting scheme interleaving reasoning and actions (Yao et al., 2023), inspiring systems like Toolformer, which enables LLMs to learn API invocation patterns via self-supervised training (Schick et al., 2023). Multi-modal extensions such as HuggingGPT further generalize this pattern by delegating tasks to expert models (e.g., for vision or speech) under the control of central LLM (Shen et al., 2023). Open-source libraries like LangChain and ModelScope-Agent provide abstractions for chaining prompts, managing memory, and integrating tools, enabling systems like AutoGPT and BabyAGI to iterate over perceive-plan-act loop (Chase, 2022; Li et al., 2023). Despite their success, current LLM/VLM-based agents lack formal mechanisms for reliability or safety. They propagate unverified information without provenance tracking or confidentiality safeguards, making them vulnerable to prompt injection, indirect leakage, and adversarial content (Euler; Siddiqui et al., 2024; Wu et al., 2024). Security failures have been demonstrated in GPT-4-based systems and AutoGPT-style agents, where crafted inputs or webpages caused unauthorized behavior, including code execution and data leakage (Euler; Wu et al., 2024). Without transactionality or fine-grained control, these agents are prone to cascading errors, hallucinated tool use, and systemic vulnerability, revealing the limitations of current prompt-based defenses (Siddiqui et al., 2024). Information Flow Control To address these challenges, recent work has explored dynamic information flow tracking to control how low-integrity inputs influence outputs (Siddiqui et al., 2024). Building on this intuition, our SAFEFLOWframework embeds fine-grained information flow control and transactional safeguards at the core of agent execution. It maintains explicit integrity levels for all content, supports rollback via write-ahead logging (WLA), and enforces policy-compliant reasoning and tool use. Inspired by system-level safety, database recovery, and reflective reasoning techniques (Cheng et al., 2024b), SAFEFLOWtreats agents as auditable, reversible systems. In parallel, we introduce SAFEFLOWBENCH, benchmark that complements task-focused suites like WebShop, MiniWoB, and VisualWebArena-Adv (Koh et al., 2024; Wu et al., 2025) by injecting adversarial inputs, concurrency conflicts, and misleading signals into realistic GUI-based scenarios. Together, SAFEFLOWand SAFEFLOWBENCHoffer principled approach to designing and evaluating secure, trustworthy agents. More details can be found in Appendix 3 Figure 1: Overview of SAFEFLOW."
        },
        {
            "title": "3 SAFEFLOW",
            "content": "The following section provides condensed overview of the SAFEFLOW framework. While we endeavor to highlight its core principles and mechanisms, this presentation is significant summary of our comprehensive methodology. For thorough understanding, including detailed formalisms, algorithmic specifics, and in-depth explanations, we strongly recommend referring to the complete exposition in Appendix C, which offers an unabridged discussion for optimal clarity and context. Agent-based systems powered by LLMs and VLMs are increasingly deployed in high-stakes applicationsfrom web automation to multi-modal decision-makingwhere they must interface with external environments, process unstructured user input, and autonomously execute plans. However, this flexibility introduces fundamental vulnerability: information is not static. It is dynamically generated, transformed, and acted upon in real time. This opens up an expansive attack surface: malicious content may infiltrate from external tools; user inputs may unintentionally disclose private credentials; and agent outputs may be distorted by adversarial or ambiguous signals. Standard content filters or static rule-based checks are ill-suited to this dynamic, multi-entity environment. SAFEFLOW is designed to address this challenge by introducing principled, auditable, and performant framework for fine-grained information flow control, built around three core pillars: (i) streamlined trust labeling mechanism; (ii) transactional logging and recovery system to guarantee traceable, verifiable execution; and (iii) rigorous but dynamic method for safely adjusting trust levels across agents and data. 3.1 SAFEFLOW Mechanism: Lightweight, Principled Flow Control Entity and Data Trust Levels. SAFEFLOW assigns every system entityUser (U ), Decider (D), and Environment (E)a scalar SAFEFLOW -Level (SFU , SFD, SFE), where smaller values indicate higher trust and stricter sensitivity. Each piece of information (I) also inherits level SFI based on its source entity. This forms lightweight but expressive abstraction of trust, enabling runtime flow control without the overhead of full implementation of Lattice-Based Access Control (LBAC). Flow Rules. SAFEFLOW enforces information flow via three key rules: Full Trust (Match): SFI = SFE allows to read and act on I. Skeptical Read (Higher Level): SFI > SFE permits reading but blocks action unless verified. No Access (Lower Level): SFI < SFE denies access to protect confidentiality. This ensures, for instance, that Decider cannot act on potentially manipulative content from low-trust source, or that sensitive user data is not exposed to untrusted environments. 4 Application in Multimodal Agents. In scenarios involving visual inpute.g., screenshots from external websitesunsafe prompts may be embedded visually (e.g., in pop-ups). Because VLMs fuse visual and language tokens, such prompts can easily evade traditional input sanitization. By enforcing SafeFlow-Level semantics at the level of both entities and content, SAFEFLOW provides an integrated solution for multimodal integrity. 3.2 Execution Reliability: Logging, Recovery, Isolation, and Concurrency Transactional Logging. To ensure verifiability and recoverability, SAFEFLOW implements transactional logging inspired by database and OS journaling. Each action or messagewhether from , D, or Eis logged with unique ID, timestamp, source, destination, and execution status (incomplete or complete). Each entry is tied to persistent user task and checked by the SAFEFLOW MONITOR, ensuring that execution stays on-task and cannot be hijacked by irrelevant stimuli or adversarial redirection, while also enabling traceable rollback and accountability in case of failure. Failure Containment via Dependency Graphs. SAFEFLOW models the interdependence of agent operations via DAG over task steps. Nodes represent individual agent actions; edges denote logical or data dependencies. Upon failure (e.g., timeout, invalid data), the system automatically traces and notifies downstream dependents, triggering localized rollback or replanning. This prevents cascading faults, akin to circuit breakers or compensating transactions in distributed systems. Concurrency Control. Multi-agent tasks often involve concurrent access to shared resources. SAFEFLOW ensures safe parallelism via global mutex system. Critical regions are protected through mutual exclusion, while task-aware scheduler prioritizes lock acquisition based on: (i) urgency (e.g., user input vs. background optimization), (ii) expected duration, and (iii) semantic coupling. This balances throughput and responsivenesse.g., enabling real-time speech transcription to proceed even as other agents reformat the underlying document. 3.3 SafeFlow-Level Adjustment: Secure Flow Enablement and Trust Governance Verifier-Gated Adjustments. Strict enforcement of SF -levels may block otherwise safe and necessary flows. To support continuity without compromising trust, SAFEFLOW introduces high-trust VERIFIER component responsible for adjusting SFI or SFE under rigorous, auditable conditions. Modifying Information Levels (SFI ) SAFEFLOWsupports two primary adjustments: Upgrading (Increasing Trust): If SFI > SFsink, the verifier checks content safety, task relevance, and causal linkage via logs. If all criteria pass, SFI is upgraded to SFsink, enabling execution. Downgrading (Minimal Exposure): If SFI < SFsink, the verifier sanitizes or abstracts the content and logs justification. Only the minimally required information is exposed. Modifying Entity Levels (SFE) Adjusting an entitys SFE is more consequential, as it redefines what data the entity can access globally. Downgrade on Violation: If an entity misuses information (e.g., hallucinating sensitive output), its SFE is elevated (decreased trust) to prevent future access at that level. Upgrade via Statistical Trust Estimation: Entities can earn trust through consistent, policycompliant behavior. SAFEFLOWmaintains dynamic Beta-distributed trust score PE based on recent operation history. Weighted by information sensitivity, PE must exceed threshold (e.g., 0.98) before the verifier lowers SFE (i.e., promotes trust). All adjustments are logged with full context, including reasoning traces and evidence trails, ensuring full auditability and accountability. In summary, through SafeFlowAgent-Levels, verifier-mediated trust transitions, and runtime enforcement across execution, logging, and coordination, SAFEFLOW establishes unified framework for secure, auditable, and adaptive information flow in LLM/VLM-based agent systems. It meets the dual demands of protecting sensitive information and enabling reliable autonomy at scale. 5 Figure 2: Overview of SAFEFLOWBENCH."
        },
        {
            "title": "4 SAFEFLOWBENCH",
            "content": "To rigorously evaluate the efficacy of our proposed SAFEFLOW frameworkand to address the broader need for comprehensive assessment of LLM/VLM-based agentswe introduce SAFEFLOWBENCH, unified benchmark suite named SAFEFLOWBENCH. Unlike existing benchmarks that focus solely on task performance or isolated safety metrics, SAFEFLOWBENCH provides multifaceted evaluation of agent trustworthiness and reliability under adversarial, deceptive, and concurrent conditions. It comprises two components: ❶ the Multimodal Threat Stress Test (MTST), which spans Webpage, Application, and OS environments and introduces comprehensive threat taxonomy covering visual deception, text forgery, interaction traps, and system-level exploits across 332 scenarios constructed via hybrid manual-automated generation; and the ❷ Concurrent Agent Reliability Test (CART), which simulates race conditions, mutex contention, and scheduling conflicts in 25 multi-agent scenarios (ranging from 2 to 5 agents), offering the first fine-grained concurrency benchmark for evaluating coordination and reliability in shared-resource settings. defining feature of SAFEFLOWBENCH is the deliberate inclusion of visually complex or misleading inputsboth in the environment and instructionsto stress-test the agents ability to interpret ambiguity, maintain information integrity, and recover from unsafe execution. Though developed to evaluate SAFEFLOW, its structured design and objective criteria make SAFEFLOWBENCH broadly applicable to future agent systems and foundational for trustworthy AI research. 4.1 Multimodal Threat Stress Test To rigorously evaluate agent robustness across realistic digital environments, MTST introduces comprehensive threat taxonomy spanning Webpage, Application, and Operating System (OS) settings, each presenting distinct vulnerabilities and interaction risks. This dataset encompasses wide range of possible attack vectors, and the taxonomy captures threats such as visual deception, content and text forgery, interaction traps, and system-level exploits (e.g., automatic execution or wallpaper-based manipulation), all designed to test agent resilience under adversarial and misleading conditions. This structured categorization enables scenario-specific stress-testing and systematic coverage of diverse attack surfaces. Compared to existing benchmarks, MTST provides significantly stronger adversarial challenge, with broader environmental coverage and higher threat completeness. Figure 2 provides visual summary of these categories and their key characteristics. MTST comprises 332 distinct scenarios, and each scenario within MTST is self-contained unit consisting of three key components: an environment image, user instruction prompt detailing the task the agent should perform, and set of evaluation principles that precisely define criteria for task 6 success and methods for detecting security breaches in that specific context. All the necessary data for each scenario, including the path to the environment image, the prompt text, and the evaluation principles, is stored in standardized JSON format. The scenarios within MTST were thus built using hybrid approach combining manual curation with automated generation techniques. Specifically, challenging environments were created or reproduced by manually collecting and adapting existing real-world or simulated scenarios, utilizing automated HTML scripting to generate dynamic or complex web pages, and by modifying webpage source code, often combined with image stitching to produce intricate or visually misleading interfaces. detailed breakdown of threat types and examples is provided in Appendix D. 4.2 Concurrent Agent Reliability Test To comprehensively evaluate an agent systems robustness in realistic deployment scenarios, we introduce novel concurrency stress-testing suite as core component of SAFEFLOWBENCH, which is Concurrent Agent Reliability Test (CART). Unlike prior benchmarks that assess agents in isolationoverlooking the challenges of shared-resource contentionour suite systematically probes agent coordination under high-contention, tightly coupled environments. Drawing from real-world applications such as collaborative editing, streaming pipelines, and decision-critical control loops, we design 25 richly contextualized multi-agent scenarios simulating race conditions, mutex contention, and scheduling conflicts. These tests span from simple two-agent cases (e.g., document editing, robotic task handover) to complex five-agent orchestration (e.g., hybrid task scheduling). Specifically, our suite includes 5 two-agent, 13 three-agent, 6 four-agent, and 1 five-agent scenarios, capturing wide spectrum of concurrency challenges (see Appendix for detailed descriptions). CART is the first benchmark to rigorously evaluate concurrent agent behavior with fine-grained control over synchronization semantics, establishing new standard for assessing the reliability and scalability of multi-agent AI systems in dynamic, resource-shared environments."
        },
        {
            "title": "5 Evaluating SAFEFLOWunder SAFEFLOWBENCH",
            "content": "In this section, we conduct comprehensive evaluation of our proposed benchmark, SAFEFLOWBENCH, using ten state-of-the-art VLMs, encompassing both open-source and proprietary models. Our results highlight the practical value of SAFEFLOWBENCH, and demonstrate the effectiveness of our proposed method, SAFEFLOW, in addressing the safety cahllenge posed by multimodal tasks. 5.1 Experimental Setting In our experiments, we initialize SafeFlow-Level values as follows: SFU = 3, SFD = 2, SFE = 3, and SFV = 0, where SFU , SFD, and SFE dynamically evolve over time based on system behavior, while SFV remains fixed to enforce high-integrity anchor. The Decider role (D) is instantiated with various state-of-the-art vision-language models (e.g., GPT-4o, Gemini 2.0/2.5), serving as the agent under test. The Verifier component (V ) is implemented using Gemini 2.5 Flash, selected for its strong reasoning capability and real-time API accessibility. All experiments were performed on computing cluster with 8 NVIDIA A6000 Ada GPUs. For trust estimation and dynamic adjustment of SFE, we adopt Beta-Bernoulli model with prior hyperparameters α0 = 1 and β0 = 1, representing uniform prior over behavioral compliance. We apply memory length parameter σ = 100 to account for the most recent 100 operations in the statistical trust score update, ensuring that trust evolution reflects recent and contextually relevant behavior. These settings align with the formulation described in Appendix C. 5.2 Robustness Under Multimodal Threats: Evaluation on Webpage, Application, and OS Environments To evaluate the robustness and safety guarantees provided by SAFEFLOW in complex, multimodal threat environments, we leverage the first component of the SAFEFLOWBENCHMTST, which comprises 332 scenarios simulating diverse attack surfaces across webpages, applications, and operating systems. Each scenario combines visual and textual elements to pose realistic and often adversarial challenges, including visual deception, ambiguous UI signals, and embedded unsafe 7 prompts that target vulnerabilities in perception, reasoning, and action alignment. This setting reflects practical deployment conditions, where agents must operate reliably despite the presence of hidden traps, dynamic content, and ambiguous cues. Model Without SAFEFLOW With SAFEFLOW Accgold Accunsafe Accunrelated Accgold Accunsafe Accunrelated o4-mini GPT-4.1 GPT-4o Gemini 2.5 Flash Gemini 2.0 Flash Gemini 2.0 Lite Qwen-VL-Plus GLM-4V-Plus LLaMA 3.2-Vision Mistral Small 3.1 31.0 30.7 20.5 22.0 15.4 10.5 9.6 4.8 16.6 6.3 56.3 59.6 66.9 67.8 74.4 69.3 67.2 78.9 66.6 68.4 12.7 9.6 12.7 10.2 10.2 20.2 23.2 16.3 19.9 25.3 97.6 99.4 97.9 99.1 98.8 95.8 94.0 94.6 93.7 92.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 2.4 0.6 2.1 0.9 1.2 4.2 6.0 5.4 6.3 7.8 Table 1: Evaluation results on MTST of SAFEFLOWBENCH comparing models with and without SAFEFLOW. Specifically, Accgold, Accunsafe, and Accunrelated denote the proportions of correct completions, unsafe actions, and unrelated responses, respectively. The integration of SAFEFLOW significantly improves agent safety and robustness under complex, dynamic, and highly adversarial conditions. The results in Table 1 provide compelling evidence of the transformative impact of the SAFEFLOWframework on the safety, reliability, and contextual alignment of multimodal agent systems. Without SAFEFLOW, all evaluated modelsincluding state-of-the-art systems like GPT-4.1, GPT-4o, and Gemini 2.5 Flashconsistently exhibited high rates of unsafe behavior (ranging from 56.3% to 78.9%) and non-goal-directed, unrelated actions (up to 25.3%), underscoring the inherent vulnerabilities of current LLM/VLM-based agents in open-ended, adversarial settings. These failure modes often stem from lack of fine-grained information flow control, trust-awareness, and runtime execution safeguardsgaps that SAFEFLOW directly addresses. Upon integrating SAFEFLOW, every model demonstrated dramatic improvement in task-aligned behavior, with near-perfect Accgold scores (94.0%99.4%) and complete elimination of unsafe actions across the board. Additionally, unrelated or hallucinated responses were reduced to singledigit percentages, reflecting the frameworks efficacy in maintaining semantic focus and minimizing distraction from misleading or ambiguous content. These gains validate the core design of SAFEFLOW: its lightweight yet expressive trust labeling, transactional logging for verifiable control, and Verifier-mediated trust transitions provide principled mechanism for runtime flow governance, even in multimodal, multi-agent environments. Overall, these findings highlight SAFEFLOWas robust and generalizable defense layer, crucial for deploying autonomous agents in real-world, high-stakes applications where trust, safety, and interpretability are non-negotiable. 5.3 SAFEFLOWEnables Robust Coordination in High-Concurrency Settings. As shown in Table 2, agent systems augmented with SAFEFLOW consistently outperform their vanilla counterparts across all concurrency levels. Without SAFEFLOW, even state-of-the-art visionlanguage models such as GPT-4.1 and Gemini 2.5 Flash fail to resolve complex race conditions in any of the 4-agent or 5-agent scenarios. In contrast, with SAFEFLOW, several models regain partial competency in 2-agent and 3-agent setups, highlighting the significant coordination bottlenecks present in unaugmented systems. This performance gap stems from SAFEFLOWs coordinated concurrency control mechanism, which combines mutual exclusion with task-aware scheduling. Unlike naive locking that leads to contentioninduced stalls or priority inversions, SAFEFLOW prioritizes latency-critical operations (e.g., real-time transcription) while safely deferring bulk or non-blocking tasks. This reduces contention overhead and minimizes operation conflicts in shared environments. Furthermore, SAFEFLOWs global mutex abstraction and its contextual task analysis enable finegrained synchronization in complex collaboration workflows such as collaborative writing, multimedia 8 Model Without SAFEFLOW With SAFEFLOW 2A (5) 3A (13) 4A (6) 5A (1) 2A (5) 3A (13) 4A (6) 5A (1) o4-mini GPT-4.1 GPT-4o Gemini 2.5 Flash Gemini 2.0 Flash Gemini 2.0 Lite Qwen-VL-Plus GLM-4V-Plus LLaMA 3.2-Vision Mistral Small 3.1 2 1 1 3 1 2 1 0 0 0 2 0 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 2 3 2 13 13 11 13 13 12 12 8 10 10 6 6 6 6 5 6 5 4 6 4 1 1 1 1 1 1 1 1 1 1 Table 2: CART Evaluation Results Success counts across 25 concurrency stress-test scenarios, grouped by the number of interacting agents per test case. Specifically, 2A, 3A, 4A, and 5A denote tests involving 2, 3, 4, and 5 agents, respectively. Each cell reports the number of scenarios successfully completed under the given configuration. SAFEFLOW significantly enhances coordination success, particularly in high-contention, multi-agent settings. processing, and hybrid task scheduling. This architectural choice allows agents to dynamically reason over temporal urgency and semantic coupling, leading to more stable and scalable multi-agent behavior. These results establish SAFEFLOW as critical enabler for deploying agent systems in real-world, concurrency-intensive applications."
        },
        {
            "title": "6 Generalization Beyond SAFEFLOWBENCH: Cross-Benchmark Evaluation",
            "content": "of SAFEFLOW To evaluate the generalization ability of SAFEFLOW beyond SAFEFLOWBENCH, we test it on AgentHarm (Andriushchenko et al., 2025), benchmark designed to assess the robustness of LLMbased agents under adversarial conditions. Unlike prior safety benchmarks focused on single-turn refusal, AgentHarm introduces 110 explicitly harmful agent tasks (440 with augmentations) across 11 high-risk categories, such as fraud and cybercrime. Crucially, it evaluates whether agents can be jailbroken using generic prompts and whether they retain the ability to complete coherent multi-step harmful tasks post-jailbreak. Notably, prior evaluations show that even leading safety-aligned models like Claude exhibit unsafe behavior under AgentHarm, often without requiring jailbreaks. In contrast, SAFEFLOWdemonstrates strong cross-benchmark robustness. Across all 440 adversarial task variants, SAFEFLOWsuccessfully refused every harmful request while maintaining agent coherence and task functionality in benign settings as shown in Table 3. No jailbreak attempttemplatebased or task-specificwas able to compromise the agent. This indicates that SAFEFLOWs informationflow enforcement and dynamic policy mechanisms generalize effectively across both task domains and adversarial strategies. These results highlight SAFEFLOWs ability to deliver end-to-end safety guarantees not only in controlled benchmarks but also under aggressive real-world threat models, positioning it as robust foundation for secure agent deployment. Model w/o SAFEFLOW w/ SAFEFLOW 48.40% 56.40% 11.10% 20.70% 14.40% 13.50% 72.00% 82.20% 20.70% 24.60% 19.30% 15.70% 3.10% 14.00% GPT-4o GPT-4.1 Claude 3 Haiku Claude 3 Sonnet Claude 3 Opus Claude 3.5 Sonnet Mistral Small 2 Mistral Large 2 Gemini 1.5 Flash Gemini 2.0 Flash (Lite) Gemini 2.0 Flash Gemini 1.5 Pro Llama-3 - 8b Llama-3 - 70b Table 3: Comparisons of Harm Scores on AgentHarm with and without SafeFlow Agent. SafeFlow consistently eliminates harmful behaviors across all models, demonstrating strong generalization and robust defense against adversarial agent attacks. 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%"
        },
        {
            "title": "7 Conclusion",
            "content": "This work presents SAFEFLOWAGENT, protocol-level framework that brings principled security and reliability to LLM/VLM-based agents through fine-grained information flow control and concurrency-safe execution. By unifying dynamic trust tracking, transactional recovery, and secure coordination, SAFEFLOW transforms agents into robust systems capable of operating under adversarial and concurrent multi-agent conditions. We also introduce SAFEFLOWBENCH, the first benchmark to systematically evaluate VLM-based agent safety in GUI-based environments. Extensive experiments on both SAFEFLOWBENCH and range of existing benchmarks demonstrate that SAFEFLOW consistently improves agent trustworthiness, task success rates, and concurrency robustness. Moreover, our framework is lightweight and modular, making it easy to integrate with existing LLM and VLM agents without major architectural changes. Furthermore, our architecture enables agents to robustly handle concurrency and resource contention, making it broadly applicable to real-world, large-scale deployments. Together, our contributions pave the way toward trustworthy, scalable, and practically deployable autonomy at scale."
        },
        {
            "title": "References",
            "content": "Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, and Xander Davies. Agentharm: benchmark for measuring harmfulness of llm agents, 2025. URL https://arxiv.org/abs/2410.09024. Harrison Chase. Langchain building applications with llms. https://github.com/hwchase17/ langchain, 2022. GitHub repository. Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. Exploring large language model based intelligent agents: Definitions, methods, and prospects, 2024a. URL https://arxiv.org/ abs/2401.03428v1. Submitted on 7 Jan 2024. Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. Exploring large language model based intelligent agents: Definitions, methods, and prospects, 2024b. URL https://arxiv.org/ abs/2401.03428. Dorothy E. Denning. lattice model of secure information flow. Communications of the ACM, (5):236243, 1976. doi: 10.1145/360051.360056. Lukas Euler. Hacking auto-gpt and escaping its docker container. URL https://positive. security/blog/auto-gpt-rce. Accessed: 2025-03-28. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks, 2024. URL https://arxiv.org/abs/2401. 13649. Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang, Fei Huang, and Jingren Zhou. Modelscope-agent: Building your customizable agent system with open-source large language models, 2023. URL https://arxiv.org/abs/2309.00986. Xiaogeng Liu, Peiran Li, Edward Suh, Yevgeniy Vorobeychik, Zhuoqing Mao, Somesh Jha, Patrick McDaniel, Huan Sun, Bo Li, and Chaowei Xiao. Autodan-turbo: lifelong agent for strategy self-exploration to jailbreak llms, 2024a. URL https://arxiv.org/abs/2410.05295. Xu Liu, Tong Zhou, Chong Wang, Yuping Wang, Yuanxin Wang, Qinjingwen Cao, Weizhi Du, Yonghuan Yang, Junjun He, Yu Qiao, et al. Toward the unification of generative and discriminative visual foundation model: survey. The Visual Computer, pages 142, 2024b. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023. URL https://arxiv.org/abs/2302.04761. Yongliang Shen, Kaitao Song, Xu Tan, et al. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. Shoaib Ahmed Siddiqui, Radhika Gaonkar, Boris Köpf, David Krueger, Andrew Paverd, Ahmed Salem, Shruti Tople, Lukas Wutschitz, Menglin Xia, and Santiago Zanella-Béguelin. Permissive information-flow analysis for large language models, 2024. URL https://arxiv.org/abs/ 2410.03055. Yuping Wang and Jier Chen. Eqdrive: Efficient equivariant motion forecasting with multi-modality In 2023 8th International Conference on Robotics and Automation for autonomous driving. Engineering (ICRAE), pages 224229. IEEE, 2023a. Yuping Wang and Jier Chen. Equivariant map and agent geometry for autonomous driving motion prediction. In 2023 International Conference on Electrical, Computer and Energy Technologies (ICECET), pages 16. IEEE, 2023b. 11 Yuping Wang, Xiangyu Huang, Xiaokang Sun, Mingxuan Yan, Shuo Xing, Zhengzhong Tu, and Jiachen Li. Uniocc: unified benchmark for occupancy forecasting and prediction in autonomous driving. arXiv preprint arXiv:2406.09246, 2025a. URL https://arxiv.org/abs/2503.24381. Zehao Wang, Yuping Wang, Zhuoyuan Wu, Hengbo Ma, Zhaowei Li, Hang Qiu, and Jiachen Li. Cmp: Cooperative motion prediction with multi-agent communication. IEEE Robotics and Automation Letters, 2025b. Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, and Aditi Raghunathan. Dissecting adversarial robustness of multimodal lm agents, 2025. URL https: //arxiv.org/abs/2406.12814. Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao. new era in llm security: Exploring security concerns in real-world llm-based systems, 2024. URL https: //arxiv.org/abs/2402.18649. Shuo Xing, Hongyuan Hua, et al. Autotrust: Benchmarking trustworthiness in large vision-language models for autonomous driving. arXiv preprint arXiv:2412.15206, 2024. Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, and Zhengzhong Tu. Openemma: Open-source multimodal model for end-to-end autonomous driving. In Proceedings of the Winter Conference on Applications of Computer Vision, pages 10011009, 2025a. Shuo Xing, Zezhou Sun, Shuangyu Xie, Kaiyuan Chen, Yanjia Huang, Yuping Wang, Jiachen Li, Dezhen Song, and Zhengzhong Tu. Can large vision language models read maps like human? arXiv preprint arXiv:2503.14607, 2025b. Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, and Zhengzhong Tu. Re-align: Aligning vision language models via retrieval-augmented direct preference optimization, 2025c. URL https://arxiv.org/abs/2502.13146. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action, 2023. URL https://arxiv.org/abs/2303.11381. Shunyu Yao, Jeffrey Zhao, Dian Yu, et al. React: Synergizing reasoning and acting in language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. URL https://openreview.net/forum?id=pl8jK1A2QIm. Junming Zhang, Weijia Chen, Yuping Wang, Ram Vasudevan, and Matthew Johnson-Roberson. Point set voting for partial point cloud analysis. IEEE Robotics and Automation Letters, 6(2):596603, 2021. Xinkai Zou, Yan Liu, Xiongbo Shi, and Chen Yang. Goal2story: multi-agent fleet based on privately enabled sllms for impacting mapping on requirements elicitation, 2025. URL https: //arxiv.org/abs/2503.13279."
        },
        {
            "title": "A Limitation",
            "content": "While SAFEFLOWintroduces additional runtime overhead due to fine-grained flow control and transactional safeguards, which may increase per-step execution time, its concurrency-safe design enables efficient multi-agent coordination. This parallelism helps mitigate latency in practical deployments, making the framework suitable for real-world, high-throughput agent systems."
        },
        {
            "title": "B Related Work",
            "content": "B.1 AI Agent The ability of LLMs to perform reasoning and decision-making through natural language has catalyzed wave of agent frameworks. ReAct pioneered the prompt-driven synergy of reasoning and acting, allowing language models to generate thought traces and explicit tool-using actions interchangeably (Yao et al., 2023). This approach inspired numerous systems that use LLMs as central brain orchestrating external tools or APIs. For example, Toolformer fine-tunes an LLM to insert API calls into its generation, enabling it to invoke calculators, search engines, and other modules as needed (Schick et al., 2023). Rather than relying on hard-coded tool-use, Toolformer lets the model learn when and how to use tools in self-supervised fashion. Other frameworks like HuggingGPT extend the agent concept to multi-model orchestration: an LLM acts as controller that plans tasks and delegates subtasks to specialist models (for vision, speech, etc.), then integrates their outputs (Shen et al., 2023). This demonstrates general template of using one AI (the LLM) to manage and invoke others, leveraging the abundant AI models available online. At the implementation level, open-source libraries such as LangChain and ModelScope-Agent provide high-level abstractions for building such agents (Chase, 2022; Li et al., 2023). They handle prompt templating, memory management, and tool plugin interfaces, which accelerated the proliferation of systems like AutoGPT and BabyAGI. These systems iterate an LLM through perceive-plan-act loop to break down objectives and execute tasks autonomously. Despite differences in architecture, common thread is that current agents heavily rely on the inherent reasoning capability of foundation models (often enhanced by chain-of-thought prompting or self-refinement) to drive decision-making. In the engineering field, Goal2Story (Zou et al., 2025) utilizes multi-agent systems to achieve goal-driven requirements elicitation. B.2 Agent Safety Despite their remarkable flexibility, current LLM/VLM agent frameworks offer little in the way of formal reliability guarantees or security controls. prominent issue is the unchecked propagation of information: agents do not verify the integrity or source of inputs, nor do they safeguard the confidentiality of sensitive data in prompts (Siddiqui et al., 2024). Consequently, malicious or low-integrity inputs can corrupt the agents reasoning and outputs. Siddiqui et al. (2024) underline that even single poisoned document retrieved by an LLM agent can change the models behavior in unexpected ways and compromise the entire system. Meanwhile, highly sensitive information given to the agent can be inadvertently leaked to untrusted tools or external outputs. These risks are exacerbated in complex agent systems where an LLMs output may be consumed by downstream software or stored for future use, creating supply chain of potential vulnerabilities (Wu et al., 2024). Recent security analyses of LLM-based systems confirm that attacks are no longer confined to prompting the model itself, but can exploit the interaction between the model and its tools. Wu et al. (2024) formulate the security of an LLM augmented with plugins, web access, and sandbox as multi-layer information flow problem: misalignment between layers (LLM, tool APIs, environment) introduces an expanded attack surface. Indeed, they demonstrated an end-to-end exploit on GPT-4 powered system that allowed extracting users private chat history without any direct prompt injection at the user level. The attack leveraged indirect injection via compromised website and gaps in how the LLMs output was post-processed, underscoring the systemic weaknesses of current agents. Another study found that an AutoGPT-based agent could be tricked into executing arbitrary code by simply browsing webpage containing hidden malicious instructions (Euler). In the absence of fine-grained control, the agent treated the invisible attacker text as trustworthy content and followed it, achieving remote code execution. These examples make clear that ad-hoc prompt-based defenses (e.g. policy prompts or user approval steps) are insufficient determined adversaries can bypass them, leading to the familiar cat-and-mouse cycle of attack and patch (Siddiqui et al., 2024). Furthermore, 13 agents often hallucinate tool uses or world states, and can enter failure loops due to the stochastic nature of LLM planning. Without transactionality or recovery mechanisms, single mistake may cascade into irrecoverable errors. As result, todays LLM agents are far from robust or secure enough for high-stakes deployments. B.3 Information Flow Control Addressing these gaps requires rethinking the architecture of LLM/VLM-based agents. Some initial efforts draw on classical security techniques. Notably, concurrent work by Siddiqui et al. (2024) proposes dynamic information flow tracking approach within LLM agents, which tags outputs with the labels of influential inputs. This mitigates over-conservatism by allowing low-integrity inputs that did not actually affect the answer to be filtered out. Such techniques point toward more principled control of information propagation. Our SAFEFLOWsystem builds on this intuition, embedding information flow control at the core of the agents reasoning and tool interface. Unlike prior frameworks, SAFEFLOWmaintains an explicit integrity level for every piece of content and prevents unsafe combinations for example, it can forbid an agent from using untrusted OCR text to compose high-confidence database query. It also logs all planned actions ahead (WLA) to enable rollbacks, concept inspired by database transaction logs and by safety requirements in robotics. Prior work in reliable AI has advocated for verifying each step of an LLMs reasoning or using reflective model to catch errors (Cheng et al., 2024b). SAFEFLOWgeneralizes this by treating the entire agent as transactional system whose state changes can be audited and reverted if they violate integrity or safety policies. In terms of secure memory management, our approach relates to ideas from the OS-security community (e.g., isolating caches and using tagged architectures), but here applied at the semantic level of an AI agents knowledge store. To spur progress in this direction, there is also pressing need for dedicated benchmarks that evaluate agent safety and trustworthiness. Most existing benchmarks (WebShop, MiniWoB, ALFWorld, etc.) focus on task performance in ideal conditions, not adversarial robustness (Wu et al., 2025). An exception is the recent VisualWebArena-Adv suite, which introduces adversarial tasks for web-based multimodal agents (Koh et al., 2024). The authors had to craft these specifically to measure how easily vision-language agents can be misled, highlighting the general scarcity of safety evaluations. Our proposed SAFEFLOWBENCHcontributes to filling this gap by systematically benchmarking agents under noisy, adversarial, and fault-prone scenarios. Inspired by OSWorlds comprehensive task set, SAFEFLOWBENCHadds malicious interventions (e.g. corrupted files, phishing websites, misleading tool outputs) into each scenario, testing whether an agents architecture can withstand or recover from them. By quantifying metrics like integrity violations prevented, recovery time, and success under attack, we hope SAFEFLOWBENCHwill complement performance benchmarks with rigorous measure of system-level safety. In summary, SAFEFLOWand SAFEFLOWBENCHalign with growing recognition that next-generation AI agents must be engineered with provable trustworthiness in mind our work strives to bring established principles from security and systems engineering into the design and evaluation of AI agents."
        },
        {
            "title": "C Detailed Explanation of SAFEFLOW",
            "content": "To clearly present the architecture and mechanisms underlying SAFEFLOW, we formalize the core entities within universal agent interaction framework. We define the core entities as follows: User (U): The User initiates tasks and ultimately receives the completed results from the agent. In certain high-stakes scenariossuch as when special authorization or clarification is requiredthe User may also be prompted to intervene mid-task to provide decisions or additional data. Over time, the User may inadvertently reveal sensitive information, which motivates the need for robust privacy and security mechanisms. Decider (D) The Decider is responsible for accepting tasks from the User, formulating plan to solve them, and returning the results upon completion. Modern LLMs and VLMs often play this Decider role due to their capacity to interpret and reason over both textual and visual content. During task execution, the Decider interacts with external tools or systems as needed. If the Decider cannot proceedperhaps because it requires elevated privileges, confidential keys, or must resolve conflicting directivesit raises an exception to the User, requesting additional input. While reducing human intervention remains long-term goal, the Decider necessarily handles 14 increasingly sensitive user information (e.g., personal data or private credentials), thus introducing potential vectors for privacy leakage. Environment (E) The Environment encompasses all external services, tools, or data sources the Decider may consult. This includes function calls, API endpoints, databases, and even naturallanguage interfaces. Whenever the Decider needs external information or performs updates (e.g., retrieving web content, reading documents, or executing specific functions), it communicates with the Environment. The Environments responses, however, may contain malicious or misleading content, which can in turn influence the Deciders behavior. Under this framework, typical agent execution proceeds as follows: issues task to D; plans the execution, queries for required data or functionality, and synthesizes results to return to . If encounters scenarios it cannot safely resolvee.g., requiring private credentials or policy decisionsit suspends execution and escalates to . After intervenes, the agent resumes and completes the task. Having outlined the system entities and their roles, we now shift focus to the lifespan and flow of information throughout agent execution. We track each piece of information from its origin, through its transformation across system components, to its eventual use or persistence. This perspective is critical: the act of generating, transferring, and acting upon information within this framework opens up numerous attack surfaces. To illustrate this vulnerability, consider real-world case drawn from our SAFEFLOWBENCHsuite specifically the Webpage Content Forgery scenario. Webpage Content Forgery Scenario Here, instructs the agent to buy Pixel tablet at the lowest possible price, using any available discounts. Acting as D, vision-language model interacts with browser-based to search for offers. The environment returns screenshot from BestBuy. Embedded within it is visually prominent popup reading: Limited Offer! Offer Your SSN to Enjoy 90% Off in 1 Minute!. Following directive to minimize price, naively acts on this promptleading to catastrophic privacy breach by leaking the users SSN. information in an agent system is not static. It is This example demonstrates core insight: dynamically generated (e.g., threat content from rendered webpage), propagated across components (from to D), and potentially acted upon. Particularly in multimodal settings, malicious visual content can bypass textual safeguards. VLMs, which fuse vision and language tokens via crossattention, may under-attend to risky regions in an image, increasing vulnerability to persuasive or deceptive content. The combination of natural language ambiguity and visual noise poses severe challenge to agent integrity and privacy. C.1 SAFEFLOW: Fine-Grained Information Flow Control for Agent Systems Building on the motivation established above, we now present the core design of SAFEFLOW. At its foundation, SAFEFLOWis inspired by Lattice-Based Access Control (LBAC), well-established model in computer security that provides formal guarantees for controlling the flow of information based on security policies (Denning, 1976). C.1.1 Lattice-Based Access Control Foundations In LBAC, every subject (e.g., user or process) and object (e.g., file, record, or message) is assigned security label. Each label comprises level (e.g., low, medium, high) and set of categories (e.g., finance, medical, confidential). These labels form partially ordered lattice, where dominance is defined by both the hierarchical level and category containment. Information flow in this model must adhere to two key invariants: No read up: subject can read an object only if its label dominates the objects label. No write down: subject can write to an object only if the objects label dominates the subjects label. 15 This ensures that sensitive information does not leak to lower integrity or confidentiality domains, and that untrusted inputs cannot taint high-assurance data. The lattice structure provides formal operations such as join (least upper bound) and meet (greatest lower bound) to reason about label composition and enforcement. C.1.2 The SAFEFLOWAbstraction & Simplification on LBAC While LBAC offers rigorous guarantees, applying it directly to LLM/VLM-driven agents introduces severe overhead. In principle, each unit of information would require managing and verifying three orthogonal levels: integrity, confidentiality, and availability. For agents expected to reason and act in real timeoften across noisy, multi-modal, or ambiguous inputsthis full-labeling scheme leads to performance bottlenecks. Excessive label checks can cause agents to over-cautiously halt execution, deferring to human intervention too frequently and undermining the goal of autonomous operation. SAFEFLOWsignificantly simplifies and adapts LBAC to the agent setting, while retaining its core security properties. Instead of enforcing full triple-level labeling at every step, SAFEFLOWintroduces streamlined SafeFlowAgent-Level abstraction to encode trust and policy constraints over both entities and information units. Specifically: Every entity in the system be it User (U), Decider (D), or Environment (E) is assigned SafeFlowAgent-Level, denoted as SFU , SFD, and SFE respectively. Every piece of information, such as query result, API response, or message between components, is dynamically assigned its own SafeFlowAgent-Level, denoted as SFI . This unified labeling abstraction acts as the basis for runtime policy checks and information flow enforcement. Rather than explicitly managing orthogonal levels, SAFEFLOWs runtime uses SFI and the current system context (SFU , SFD, SFE) to dynamically infer and enforce secure communication patterns. This abstraction allows SAFEFLOWto remain lightweight, auditable, and performant, while preserving strong flow control guarantees in dynamic, concurrent agent ecosystems. C.1.3 SAFEFLOWLevel Semantic and Enforcement Rules Building on the notation introduced above, we now describe how SAFEFLOWLevels serve as the credentialing mechanism for communication and trust between entities and information in the agent framework. Let SFX denote the SafeFlowAgent-Level assigned to Entity or Information {I, U, D, E}. The semantics of the level is inverse: smaller numerical values indicate higher trust or sensitivity. That is, SFX = 0 corresponds to the highest level of confidentiality and integrity, while larger values denote more public or potentially untrusted data. We then define the following operational rules: Label Propagation: When piece of information originates from an entity {U, D, E}, its SafeFlowAgent-Level inherits from the source, i.e. SFI = SFE Trust Semantics: When an entity receives an information I, trust is established only if the informations level exactly matches the entitys level: SFI = SFE fully trusts I, {U, D, E} In this case, is both readable and actionable for E, and can influence downstream decisions or be propagated further. Skeptical Read (Untrusted Data): if the information level is higher (i.e., less trusted) than that of the receiving entity, the data can be read but not trusted without verification: SFI > SFE may read but must not act on it without elevation, {U, D, E} In such cases, the information must undergo validation or be backed by additional attestations before it can be integrated into critical decisions. 16 Non-Visibility (Restricted Access): If the information level is lower (i.e., more sensitive) than that of the receiver, it is considered invisible to the entity: SFI < SFE is not visible to E, {U, D, E} To enable visibility, must be declassified, which may involve obfuscation, encryption, or abstraction of sensitive content. Level Adjustment via Verifiers: Any upgrade or downgrade of SFI or SFE requires validation by trusted verifier , where: SFV < SFX , {I, U, D, E} Such verifiers must themselves be assigned higher trust levels to ensure that label transitions preserve the systems security invariants. The mechanisms for validation e.g., cryptographic proofs, audit logs, or authenticated execution traces are discussed in detail in Section N.n These policies allow SAFEFLOWto support nuanced trust management across heterogeneous agents and data sources while minimizing the need for hardcoded access control lists or brittle rule-based filters. By tying trust semantics directly to SafeFlowAgent-Levels, the system enforces contextual trust boundaries that are both scalable and formally analyzable. C.2 Reliable Autonomy at Scale: Logging, Recovery, and Concurrency in SAFEFLOW Beyond enforcing fine-grained information flow control, SAFEFLOWincorporates several robust systems-level mechanisms that collectively ensure reliable execution, fault tolerance, and consistency across both single-agent and multi-agent environments. These mechanisms address real-world concerns such as execution failures, cascading agent dependencies, and concurrency conflicts during collaborative tasks. C.2.1 Transactional Logging for Verifiable Execution SAFEFLOWextends and adapts traditional write-ahead logging (WAL) and journaling principles from operating systems and databases to the context of intelligent agents. Classic journaling systems log both metadata and content into dedicated region before committing to disk, often resulting in significant I/O duplication and log bloat. Instead, SAFEFLOWfuses ideas from metadata journaling and log-structured file systems, introducing lightweight transactional logging mechanism optimized for agent-level operations. Each entity and each information item is meticulously logged with: globally unique log_id monotonic timestamp status {incomplete, complete} representing transactional state The original user-issued task, which remains invariant across all related log entries For entities, prior to executing any operation, corresponding log entry is generated and marked as incompleteanalogous to TxB (i.e., transaction begin) in OS terminology. The log also includes structured description of the operations metadata or high-level intent. Upon successful execution, the entry is updated to complete (TxE, i.e., transaction end). This procedure is tightly coupled with the SAFEFLOWAGENT MONITOR, which performs real-time validation: it checks whether the logged operation aligns with the original task and constitutes valid substep toward its completion. This ensures that the agents execution remains on the correct trajectory and is not derailed by irrelevant stimuli, adversarial prompts, or noisy intermediate states. Similarly, for information flows, the log records: The source entity The original SafeFlowAgent-Level (SFI ) The intended destination entity Any intervening verifier and transformation history of SFI 17 This unified logging framework allows the SAFEFLOWruntime to trace every decision and data movement with high granularity. In the event of external disruptione.g., system crashes or power lossexecution can resume by selectively replaying only those log entries with incomplete status, ensuring consistency and forward progress without redundant computation. Moreover, by anchoring every execution step to an immutable initial task specification, SAFEFLOWprovides defense-indepth mechanism against prompt injection and semantic drift, preserving both intent fidelity and operational integrity. C.2.2 Cascading Fault Isolation via Dependency Graphs While transactional replay ensures crash recoverability, agent tasks may still fail due to logic errors, resource constraints, or unsafe inputs. In multi-agent scenarios, such failures often lead to cascading disruptionswhere the output of one agent becomes critical dependency for others. Imagine collaborative task where Agent extracts live data from an API and writes it to shared analytics dashboard, while Agent simultaneously performs statistical summarization over the incoming data. If Agent As upstream data retrieval fails (e.g., due to API timeout), Agent may generate misleading or null results unless explicitly notified. SAFEFLOWAGENTs dependency-aware failure handling ensures such inconsistencies are caught early and contained. To address this, SAFEFLOWconstructs directed acyclic graph (DAG) over agent operations, capturing the inter-task dependency structure. Each node corresponds to discrete agent task, and edges encode logical or data-based dependencies. Each log entry is thus associated with its position in this DAG context. In the event of failure: 1. The system traces downstream dependencies and notifies affected agents to halt, retry, or replan. 2. Localized rollback or logical substitution is triggered to prevent global failure propagation, akin to cascading rollbacks in transactional databases or circuit breakers in microservice architectures. C.2.3 Coordinated Concurrency Control in Multi-Agent Collaboration SAFEFLOWfurther extends its reliability guarantees into high-concurrency settings, where multiple agents interact with shared resourcesoften in real time. To prevent race conditions and data corruption, SAFEFLOWdefines critical sections: shared regions or objects (e.g., document, buffer, or file) that may be concurrently accessed or modified by more than one entity. To regulate access, entities must acquire global mutex before modifying critical section. If the mutex is already held by another entity, subsequent contenders are blocked in-place and must engage in active polling until the lock becomes available. This mechanism ensures serialized access while avoiding deadlocks through strict mutual exclusion, albeit at the cost of potential waiting overhead under high contention. However, naive locking introduces scheduling challenges, especially in latency-sensitive tasks. For example, in real-time collaborative writing agent, Agent continuously transcribes user speech into document, while Agent periodically revises grammar and structure. If Agent acquires the lock first and starts reformatting the entire paragraph, Agent As incoming transcription may be delayed or droppedbreaking the user experience. Conversely, prioritizing Agent As fast writes and deferring Agent Bs edits improves responsiveness and flow. To resolve this, SAFEFLOWintroduces task-aware scheduler that coordinates lock acquisition using: Task Urgency (e.g., real-time user input vs. batch optimization) Estimated Execution Time Depending on Contextual Coupling (e.g., whether operations overlap semantically) The SAFEFLOWAGENT SCHEDULER evaluates conflicting operations based on their logged metadata and positions in the execution DAG, enforcing high-level scheduling policy that preserves both correctness and usability. 18 C.3 SafeFlowAgent-Level Modification: Enabling Secure, Minimal Exposure Flow In Section 3.1, we introduced the foundational enforcement rules of SAFEFLOW, which strictly govern information flow by comparing the SafeFlowAgent-Levels between entities and information. critical implication of these rules is the possibility of information flow deadlock condition where, due to strict enforcement without adjustment, certain data cannot traverse the system. This scenario arises when mismatched SafeFlowAgent-Levels between entities and information permanently block transfer or trust, effectively freezing communication. This section introduces the SafeFlowAgent-Level Modification Mechanism, which enables secure and minimal exposure-based information propagation. At its core lies key design principle:"
        },
        {
            "title": "Key Design Principle",
            "content": "Entities may only access the minimally exposed form of information required for specific operation. Any exposure beyond what is necessary is strictly restricted. Crucially, any adjustment to SafeFlowAgent-Level whether it concerns an entity (SFE) or unit of information (SFI ) must be mediated by SAFEFLOWAGENT VERIFIER, trusted reasoning component with strictly higher trust level than any entity in the system (i.e. SFV < SFX , for {I, U, D, E}). These verifiers powered by large-scale, CoT-capable reasoning models (e.g., OPENAI-O1/O3/O4, GEMINI 2.5 PRO/FLASH) analyze rich contextual evidence to reason about SafeFlow-Level changes. The goal is to apply the minimum required label adjustment to enable flow, without compromising system integrity. All modifications are fully auditable: the SafeFlowAgent logging systemdescribed in Section 3.2.1records the tasks original intent, complete operational history, and any verifier-driven label changes, thus supporting both post-hoc verification and real-time validation. C.3.1 Modifying Information SafeFlowAgent-Level (SFI ) Adjustments to SFI typically occur during communication between entities, triggered when SFI and the receiving entitys level (SFsink) are incompatible. The verifier must intervene to either upgrade or downgrade the informations level, depending on the mismatch direction: (A) Upgrading SFI (More Trusted, Smaller Value) This case arises when information initially deemed untrusted (SFI > SFsink) needs to be elevated so that more trusted entity may not only read it, but also act upon it. To evaluate whether this elevation is safe, the SafeFlow Verifier considers: The informations content Its source and receiving (sink) entities and their SafeFlowAgent-Level SFsource and SFsink The original user task Complete task-specific logs The verifier conducts deep logical inference under the following strict criteria: Non-maliciousness: The content must not pose any threat of system compromise, nor risk triggering behavior that violates laws or policies. Task relevance: The content must be directly relevant to the original task, and essential to one or more of its required steps. Privacy preservation: The information must not contain sensitive personal data (e.g., SSNs, private keys, identity-revealing content). Causal justification: Based on log timeline analysis, the information must be causally linked to the task and logically originate from SFsource. Label alignment: The original label must reflect the source entity: SFI = SFsource and SFI > SFsink. 19 If all conditions are met, the verifier adjusts SFI := SFsink, enabling trust and execution. If any condition fails, the verifier halts task execution, logs detailed interrupt reason, and triggers system-level exception. (B) Downgrading SFI (Less Trusted, Larger Value) This scenario arises when higher-level data (SFI < SFsink) is intended for less trusted recipient. To prevent over-exposure, information must be sanitized or abstracted before becoming accessible. Again, the verifier inspects: Source and sink entities and their SafeFlowAgent-Level Task definition and full task logs The raw content within the information and required exposure for sink-side operations All five of the criteria above apply, with one additional rule: Minimal Exposure Enforcement: The final information delivered to the sink must be the smallest sufficient subset necessary to complete the immediate operation, based on the sinks role and system state. Sensitive, irrelevant, or over-revealing components must be pruned or masked. When these conditions are met, the verifier sets SFI := SFsink, and the sink transitions from invisible to readable and executable. Otherwise, the flow is aborted and logged. C.3.2 Modifying Entity SafeFlowAgent-Level (SFE, {U, D, E}) While information-level adjustments (SFI ) facilitate immediate and localized flow correction, entitylevel adjustments (SFE) are fundamentally more consequential. They alter the global trust contract between the SAFEFLOWsystem and its agentsdeciding not just what information can be consumed, but what information an agent is ever eligible to see or act upon. Consequently, any modification to an entitys SafeFlowAgent-Level must be grounded in longitudinal behavioral evidence and governed by strict, formally analyzable criteria. We identify two triggering conditions for SFE modification: Permission Escalation Need: An entity cannot complete task step due to insufficient authority to access information whose SFI < SFE. Violation-Driven Restriction: The entity has, in the course of prior operations, either performed actions unrelated to the tasks completion, or generated/transmitted outputs containing illegal, harmful, or policy-violating content. (A) Downgrading Trust (Increasing SFE) SAFEFLOWenforces strong principle here: an entity that cannot safely handle certain trust level must not be allowed to access it in the future. Therefore, when violation is detected in the current operation, the verifier immediately penalizes the entity by elevating its SafeFlowAgent-Level: SFE := SFI + 1 This reactive adjustment guarantees that the violating entity can no longer access the information level it mishandledensuring the system maintains conservative safety envelope. (B) Upgrading Trust (Decreasing SFE) Reducing SFE is far more delicate and rare operation. It effectively increases the entitys trust privilege, expanding the horizon of what it may perceive and influence. This change is never triggered by single event. Instead, it is the outcome of robust, longitudinal statistical inference process grounded in the entitys entire behavioral footprint. We formalize this process via Bayesian trust estimation model, inspired by the Beta Reputation System. For each entity E, the system maintains dynamic trust score PE, representing the inferred probability that the entity will act responsibly when exposed to sensitive information. The detailed Beyesian Trust Estimation Process is shown below: 20 1. Trust Modeling via Bernoulli Trails Each operation issued by entity is treated as Bernoulli trialeither success (no violation) or failure (violation). We model PE using Beta distribution: PE Beta (αE, βE) At initialization, we assume uniform prior: α0 = 1, β0 = 1. The distribution is then iteratively updated by examining the entitys most recent σ = 100 operations, across all tasks, sorted in reverse chronological order. 2. Weighted Evidence Accumulation Each operation contributes to αE or βE depending on its outcome: If operation is successful: If operation is violation: αE αE + (cid:0)SFIj (cid:1) βE βE + (cid:0)SFIj (cid:1) Here, SFIj denotes the SafeFlow-Level of the information involved in operation j, and w(SFIj ) is risk-weighted evidence function: (SFI ) = ekSFI , c, R+ 3. Threshold-Based Decision At any moment, we compute the posterior mean trust score: PE = αE αE + βE If this score exceeds system-wide trust threshold θ (e.g., 0.98), the verifier considers the entity sufficiently reliable to be promoted. It lowers the entitys SafeFlowAgent-Level: SFE : SFE 1 Importantly, the magnitude of this adjustmentthat is, how much SFE is reducedshould be proportional to the depth of behavioral evidence examined. larger trust shift demands stronger statistical justification. To reflect this, we adapt the historical window size σ dynamically: for modest adjustments (e.g., reduction by 1 level), σ may remain at 100; however, for more substantial reductions (e.g., 2 or more levels), σ must be increased (e.g., to 150, 200, or more) to ensure longer-term consistency in the entitys trustworthy behavior. This scaling guards against overfitting to recent anomalies and reinforces the principle that higher trust must be earned through sustained performance over time. Overall, this action is logged with full context, including the updated PE, recent behavior trace, and justification via verifier reasoning traceensuring complete auditability."
        },
        {
            "title": "D Threat Taxonomy",
            "content": "To support comprehensive evaluation, SAFEFLOWBENCHdefines threat taxonomy structured by interaction environment: Webpage, Application (App), and Operating System (OS). Each environment introduces unique vulnerabilities and attack surfaces that agents must navigate safely. Webpage Environment: Visual Deception: Use of misleading visual design and layout to cause misinterpretation of interface elements. This includes spoofed buttons, misaligned labels, or misleading color schemes that exploit the visual parsing mechanisms of VLMs, leading to incorrect action selection or failure to identify critical cues. 21 Figure 3: Example of visual deception Content Forgery: Alteration or injection of false information into page content to create inconsistencies and induce errors. For instance, price tags, product details, or policy terms may be forged within an HTML element or overlaid visually to mislead the agent into executing unsafe transactions or recommending incorrect options. Figure 4: Example of content forgery Interaction Traps: Interface elements designed to mimic legitimate interactions but intended to trick agents into performing sensitive or harmful actions. Examples include look-alike CAPTCHA fields, fake submit buttons that trigger downloads, or elements that change semantics on hover or click, thereby compromising agent integrity. Figure 5: Example of interaction traps App Environment: Message Traps: Deceptive messaging tactics within the app interface, akin to phishing attacks. These may include pop-ups or modal dialogues containing manipulative or emotionally charged language intended to provoke unsafe decisions, often framed as urgent system requests or limited-time offers. Figure 6: Example of message traps Text Forgery: Manipulation of displayed text content for malicious purposes. This can involve substituting benign-looking terms for sensitive actions (e.g., renaming delete to save), using homoglyphs, or obfuscating harmful instructions within innocuous strings to evade detection by language models. Figure 7: Example of text forgery Video Advertisement: Use of in-app advertisements to deliver deceptive or harmful content. These may contain rapid visual transitions, embedded QR codes, or fake UI elements designed to mislead vision-language agents into misinterpreting their context or performing unsafe clicks. Figure 8: Example of video advertisement OS Environment: Automatic Execution: Exploiting automation features to trigger unwanted or forced actions. For example, auto-launching installers, silent permission grants, or timed file execution that bypass explicit user or agent intent. 23 Figure 9: Example of automatic execution Advertisement: Intrusive or deceptive ads presented at the OS level. These may take the form of system-wide banners, push notifications, or lock screen overlays that impersonate trusted applications or system alerts to gain elevated access or trigger unsafe behavior. Figure 10: Example of advertisement on OS Interaction Traps: System prompts or windows designed to elicit sensitive input or unintended operations. This includes fake password prompts, ambiguous Accept/Decline buttons with swapped meanings, or nested dialogs that obscure the actual operation being performed. Figure 11: Example of interaction traps on OS Wallpaper Forgery: Embedding hidden or misleading text/images/instructions within the OS background or wallpaper. Such content may be imperceptible to humans but easily parsed by agents, potentially triggering unauthorized behavior based on visual prompts not intended as input. Figure 12: Example of wallpaper forgery"
        },
        {
            "title": "E Concurrency Stress Testing for SAFEFLOW",
            "content": "To rigorously validate the concurrency-handling capabilities of SAFEFLOW, we design suite of 25 systematically crafted multi-agent scenarios, simulating real-world contention over shared critical sections. Our goal is to stress-test two fundamental properties: (1) correctness under concurrency, i.e., race condition avoidance and data consistency, and (2) responsiveness through intelligent lock scheduling based on task urgency, semantic coupling, and execution cost. The scenarios progressively range from simple two-agent resource conflicts to five-agent coordination under diverse operational constraints. Each case exercises specific concurrency dynamics, such as fast writes vs. long reads, batch updates vs. real-time streaming, and simultaneous semantic operations across overlapping data regions. Collectively, these scenarios provide comprehensive and high-fidelity benchmark for concurrency resolution in multi-agent systems. By structuring contention across varied workloads and task interactions, we demonstrate that SAFEFLOWmaintains robust, responsive, and deadlock-free operation even under adversarial multi-agent coordination. E.1 2-Agents Collaboration Scenarios Scenario 1: Real-Time Transcription vs. Background Proofreading Agent continuously transcribes user speech into shared document in real time. Agent periodically acquires the document lock to perform grammar and structural revisions. The goal is to validate whether high-frequency, low-latency updates (A) can proceed smoothly even when performs longer-formatting edits. This setup probes the schedulers ability to prioritize fast, user-facing writes over batch-style background tasks, ensuring an uninterrupted and fluent user experience. Scenario 2: Sensor Logging vs. Analytical Computation Agent rapidly streams time-sensitive sensor data into shared buffer. Agent periodically locks the buffer for batch aggregation, extracting statistical summaries over large windows. The conflict arises when Bs prolonged lock potentially stalls As real-time ingestion. This scenario assesses SAFEFLOWs scheduling to ensure fast periodic writes are minimally delayed, validating robustness under continuous data ingestion and computationally intensive concurrent tasks. Scenario 3: Event Logging vs. Monitoring Dashboard Agent writes frequent log entries to shared event file. Agent regularly parses the log for index updates to power live monitoring dashboard. Both agents contend for write accessA with high frequency, with long-duration parsing. This scenario tests the systems capacity to balance write-intensive logging with heavy but less frequent batch reads, preserving low-latency access and avoiding log corruption. Scenario 4: Critical Alarm vs. Maintenance Scan Agent triggers immediate alarm writes upon detecting anomalies in the system. Agent performs scheduled log maintenance and cleanup. Although Bs operations are routine, they require exclusive access to the shared log. This scenario tests whether the scheduler reliably prioritizes urgent, highseverity writes from over lower-priority batch maintenance tasks, thereby guaranteeing real-time responsiveness in safety-critical environments. Scenario 5: Robot Planning vs. State Reporting Agent inserts new patrol tasks for mobile robots into shared scheduling table. Concurrently, Agent updates live robot statusbattery level, coordinatesinto the same table. The system must reconcile As batch insertions with Bs high-frequency state writes. This setup tests concurrent updates on overlapping table regions and evaluates whether SAFEFLOWmaintains data integrity while ensuring timely visibility of new plans and status. E.2 3-Agent Collaboration Scenarios Scenario 6: Collaborative Writing Generation, Proofing, and Layout Agent writes new paragraphs to shared document, Agent continuously proofreads existing content, and Agent occasionally reformats the entire layout. All three modify overlapping regions of the document. This scenario targets fine-grained concurrency over semantically coupled tasks and 25 evaluates whether the scheduler allows time-sensitive and atomic edits to coexist with coarse, slower structural operations without inducing contention-based bottlenecks. Scenario 7: Shared Transaction Table Access Agent records new orders into shared database. Agent modifies order statuses, while Agent reads and aggregates transaction data for analytics. The transaction table sees simultaneous inserts, updates, and reads. This setup assesses SAFEFLOWs handling of read-write concurrency and its ability to avoid data races in database-like access pattern with mixed-frequency workloads. Scenario 8: Real-Time Translation Pipeline Agent transcribes user speech into shared buffer. Agent refines the transcription with semantic polishing. Agent simultaneously fetches the latest translations for rendering. This setting models streaming NLP pipeline and tests the systems ability to preserve buffer consistency while balancing concurrent reads and multi-layered transformations, all under real-time latency constraints. Scenario 9: Distributed Cache Populate, Purge, Query Agent updates shared cache with new data from external sources. Agent purges expired entries, while Agent serves frequent cache queries. This scenario stresses the lock scheduler in mixed read/write environment with heterogeneous workloads, validating low-latency reads even under aggressive cache purges and backfilling activity. Scenario 10: Video Streaming Pipeline Agent encodes raw video into frames, Agent applies transformations like watermarking, and Agent retrieves frames for real-time display. The shared buffer must support concurrent writemodify-read operations. This scenario measures SAFEFLOWs ability to maintain frame consistency and throughput under tightly coupled, latency-sensitive video operations. Scenario 11: Task Queue Scheduling System Agent appends tasks to shared queue, Agent dequeues and processes them, and Agent reviews pending tasks to reprioritize or cancel. The shared queue experiences concurrent enqueues, dequeues, and reordering. This tests SAFEFLOWs ability to coordinate queue updates without priority inversions or race conditions in dynamic scheduling environments. Scenario 12: Audio Floor Control in Conference System Agent (host) maintains permission list for speaking. Agents and (participants) contend for the right to speak. The shared list is updated frequently and must ensure exclusivity. This models token-based mutual exclusion and tests whether SAFEFLOWprevents simultaneous speaking rights, maintaining consistency in audio stream control. Scenario 13: Search Engine Indexing Pipeline Agent adds newly crawled documents to the index. Agent periodically reorganizes the index for faster access. Agent handles real-time queries. With all three agents touching the index, this setup evaluates SAFEFLOWs read/write contention control and how it prioritizes interactive querying over structural batch operations. Scenario 14: Multi-Stage Image Processing Agent pre-processes images (e.g., cropping), Agent applies enhancements, and Agent compresses/stores the final output. The shared buffer holds intermediate representations. This pipeline requires sequential consistency under concurrent access, testing whether image data remains uncorrupted across staged transformations. Scenario 15: Social Feed Post, Filter, Display Agent posts messages, Agent removes harmful content, and Agent fetches messages and marks them as read. All agents access the same message queue. This scenario evaluates SAFEFLOWs control over interleaved message lifecycle operations, maintaining consistency even under rapid post/remove/read cycles. Scenario 16: Financial Order Book Management 26 Agent logs buy/sell orders, Agent matches and settles trades, and Agent serves client queries. The shared order book is simultaneously appended to, modified, and read. This setup measures SAFEFLOWs ability to maintain atomicity and visibility guarantees in financial transaction systems. Scenario 17: Large-Scale Log Ingestion and Analysis Agent merges logs from distributed sources, Agent builds indices over the merged logs, and Agent performs keyword scans with alerting. With overlapping access to the same log corpus, this scenario stresses SAFEFLOWunder continuous write, batch indexing, and high-frequency scan operations. Scenario 18: Medical Record Update and Access Agent streams patient vitals, Agent appends diagnostic assessments, and Agent allows clinician annotations and views. All update the shared patient record. This critical scenario tests data consistency and priority-aware locking under safety-critical, concurrent medical updates and annotations. E.3 4-Agent Collaboration Scenarios Scenario 19: Real-Time Music Collaboration In this scenario, Agents and collaborate on composing piece of music: generates melodic sequences, while writes harmonic progressions. Agent continuously modifies global synthesizer settings (e.g., tempo, reverb), and Agent streams the live composition to listeners. All agents access and modify the same shared musical timeline and rendering buffer. The test simulates concurrent editing, parameter adjustments, and live output generation. The challenge lies in ensuring smooth audio playback (D) without disruption from upstream structural or parametric changes. SAFEFLOWmust handle frequent lock contention and prioritize latency-sensitive playback while maintaining correctness in overlapping edits and global control updates. Scenario 20: Shared Virtual Whiteboard Collaboration This setting simulates real-time collaborative whiteboard used in design or brainstorming. Agent draws new strokes, Agent modifies existing elements (e.g., resizing or recoloring shapes), and Agent inserts external media like images or charts, which trigger layout reflows. Meanwhile, Agent periodically snapshots the entire canvas for playback, versioning, or synchronization. All agents contend over the same graphical buffer, often on semantically overlapping regions. The scenario tests SAFEFLOWs fine-grained locking under concurrent graphical mutations, ensuring that visual consistency and temporal coherence are preserved even when edits and captures are tightly interleaved. Scenario 21: Multi-Developer Repository Coordination shared codebase is edited in parallel by Agents A, B, and C, each modifying different modules or files. Agent serves as the CI system, periodically acquiring full snapshot of the repository to run build and test pipelines. The repository represents the critical section where multiple concurrent writes and global reads intersect. This scenario challenges SAFEFLOWto prevent mid-commit inconsistencies and ensure atomic views for integration, especially when different developers changes introduce inter-file dependencies. It evaluates whether the scheduler can enable concurrent commits while deferring global snapshot tasks only as needed to maintain correctness. Scenario 22: Multiplayer Game World State Management In real-time multiplayer environment, Agents A, B, and simulate different players acting on shared game objectse.g., attacking the same enemy or interacting with shared loot. Agent periodically serializes the entire world state for backups or savepoints. The shared state is updated rapidly and unpredictably. The scenario tests SAFEFLOWs ability to support high-frequency, player-triggered writes to overlapping world entities while also allowing safe, full-state snapshots without disrupting ongoing gameplay. The scheduler must coordinate lock acquisition to avoid deadlocks and ensure low-latency player interactions despite background persistence operations. Scenario 23: Collaborative UI Design Platform 27 Agents A, B, and concurrently edit components on shared user interface design canvasmoving buttons, changing styles, or aligning layouts. Agent intermittently creates version checkpoints by locking and exporting the current design state. Each component and layout grid may be touched simultaneously by different agents. This scenario evaluates SAFEFLOWs performance in UI-centric, latency-sensitive context where responsiveness and visual integrity are key. It assesses whether small, rapid layout changes can coexist with global versioning, without introducing flickers, rollback errors, or state corruption. Scenario 24: Real-Time Educational Assessment Platform Agent captures live quiz responses from students, Agent grades responses and updates scores, Agent renders visual analytics for instructors (e.g., class-wide heatmaps), and Agent archives results into historical database. All agents operate over shared gradebook and analytics table. The scenario stresses concurrency between fast ingestion (A), compute-heavy grading (B), user-facing visualization (C), and batch archival (D). It evaluates whether SAFEFLOWcan correctly schedule urgent writes and UI updates without blocking slower archival operations or risking conflicting score calculations. E.4 5-Agent Collaboration Scenarios Scenario 25: Mixed-Priority Task Pool with Dynamic Agent Roles This complex scenario involves five agents interacting with central task pool. Agent submits longrunning AI training jobs. Agent pushes urgent user requests for inference. Agent exclusively processes these high-priority jobs, while Agent executes the queued training tasks. Agent monitors the task pool, updating system dashboards and logs. Tasks have varied sizes, durations, and urgency levels. All agents must read or modify the same queue structure under tight concurrency. The key challenge is balancing latency-sensitive inference tasks (B and C) against large batch jobs (A and D) while maintaining up-to-date visibility (E). SAFEFLOWmust enforce strict mutual exclusion on the task queue and apply fine-grained scheduling policies to prevent starvation, ensure timely inference response, and preserve system throughput under peak load."
        },
        {
            "title": "F Broader Impact",
            "content": "As language and vision based autonomous agents gain traction across digital assistants, web automation, robotics, and decision support systems, their increasing autonomy introduces new sources of risks. Failures in integrity, provenance, or safety can propagate unpredictably across systems and users. Our proposed framework, SAFEFLOW, and the accompanying benchmark suite, SAFEFLOWBENCH, aim to shift agent development toward principled architectures that prioritize security, accountability, and reliabilityfoundations that are critical for safe AI deployment in real-world settings. The NeurIPS community has long emphasized the importance of robustness, generalization, and alignment in machine learning systems. Our work builds upon this ethos, extending it to agentic systems where interactions with external tools and multi-agent dynamics introduce complex, emergent behaviors. SAFEFLOWtreats agents not just as reasoning engines, but as transactional systems with secure semantics over state and information flowintegrating classical ideas from systems and security into modern AI pipelines. Though this paper focuses on languageand vision-based agents, the motivation and design of SAFEFLOWare deeply connected to challenges observed across other AI subfields. For instance, in autonomous drivinganother domain where real-world safety is paramountour previous work has examined equivariant motion forecasting (Wang and Chen, 2023a,b), cooperative multi-agent planning (Wang et al., 2025b), and robustness under partial observations (Zhang et al., 2021). These systems face similar issues of coordination, uncertainty, and adversarial interference, and have directly inspired our approach to secure concurrency control and conflict resolution in multi-agent ecosystems. Moreover, advances in multimodal representation and integrationsuch as our work on unified visual foundation models (Liu et al., 2024b) and multimodal perception for autonomous navigation (Xing et al., 2025a,b)highlight both the potential and the fragility of systems that rely on fused information. These lessons shaped the design of SAFEFLOWs cross-modal security guarantees and its robust 28 memory and retrieval strategies. Similarly, our experience in benchmarking complex prediction systems (e.g., UniOcc (Wang et al., 2025a)) reinforces the need for comprehensive evaluation frameworks like SAFEFLOWBENCH, which extend beyond task success to include adversarial resilience, policy compliance, and recovery. In making SAFEFLOWand SAFEFLOWBENCHpublicly available, we hope to contribute not just tools but also shift in mindset: from viewing agents as prompt glue to engineering them as trustworthy, auditable, and policy-compliant systems. As agent-based AI systems take on increasingly critical roles, we believe this work is step toward aligning their autonomy with the societal values of safety, transparency, and accountability."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: In the abstract and Section 1, we clearly demonstrate the contribution and scope of the paper. this paper. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Appendix A, we have thoroughly discussed the limitations of our article, expecting to attract more future work. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: In Section 3 and Appendix C, we have thoroughly elaborated on the theoretical content and motivation. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In Section 3 and Section 5.1, we have provided methods and experimental setup to reproduce our experimental results. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 31 Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We are working hard to promote the process of open source and it is temporary. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided detailed information about experimental setting in Section 5.1. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Due to the high cost of the experiment and various experimental settings, we have not repeated our experiments. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details of the computer cost are mentioned in Section 5.1 and the Appendix. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We guarantee that the research conducted in the paper complies with NeurIPS Code of Ethics in all aspects. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed our broader impacts in Appendix F. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. 33 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper appropriately acknowledges the original creators or owners of all utilized assets, including code, data, and models, and clearly states the relevant licenses and terms of use, which have been duly followed. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. 34 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper has not released any new assets currently. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 35 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: We have describe the usage of LLMs in Section 1 and Section 5.1. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Columbia University",
        "Meta",
        "Texas A&M University",
        "UC Irvine",
        "UC San Diego",
        "University of Michigan",
        "University of Wisconsin-Madison"
    ]
}