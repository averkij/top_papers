{
    "paper_title": "SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System",
    "authors": [
        "Zhiyu Xu",
        "Weilong Yan",
        "Yufei Shi",
        "Xin Meng",
        "Tao He",
        "Huiping Zhuang",
        "Ming Li",
        "Hehe Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 3 4 9 7 1 . 1 1 5 2 : r SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System Zhiyu Xu1, Weilong Yan2, Yufei Shi3, Xin Meng4, Tao He5, Huiping Zhuang6, Ming Li7,, Hehe Fan8 1Jinan University 2National University of Singapore 3Nanyang Technological University 4Peking University 5University of Electronic Science and Technology of China 6South China University of Technology 7Guangming Laboratory 8Zhejiang University"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educatinga domain that demands external professional knowledge integration and rigorous step-wise reasoningexisting approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its PlanDoStudyAct philosophy into self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-ofthe-art video agents on the benchmark, establishing new paradigm for the community. 1. Introduction Existing MLLMs for video have achieved significant progress by focusing on enabling large models to understand videos through the integration of visual encoders, large language models, and temporal modeling, thereby allowing them to perceive, reason about, and discuss dynamic visual content [3, 24, 25]. However, they suffer from key Figure 1. SciEducator, conducting video comprehension and delivering science education, can generate multimodal educational e-booklets that provide comprehensive, detailed, and engaging guidance. limitation: inadequate ability to leverage external resources and integrate tools [7, 23], which substantially constrains their functionality and application scenarios. Recent agentic systems, capable of integrating and effectively utilizing external tools to ultimately achieve complex and powerful functionalities, thus attracting significant attention [11, 29, 31]. For instance, [16] proposed PostAgent that can transform research paper into wellstructured poster, while [17] introduced an agentic system integrating multiple agents capable of fully autonomous software development. These examples demonstrate the substantial application potential of agentic systems. However, they still face several issues at the current stage. Their performance is affected by inherent problems of large language models, such as hallucinations [27] and unstable capabilities [4, 12, 30]. Furthermore, for scientific video tasks that require the integration of external professional knowledge and rigorous step-by-step reasoning, these systems often struggle to generate effective and feasible plans initially and still lack the systematic mechanism to self-evolve and self-optimize their workflows based on previous execution results. To address the limitations of existing agentic systems, we propose SciEducator, novel multi-agent system for scientific video understanding and educating, where external professional knowledge integration, complex task planning, and rigorous reasoning are essential. Rooted in the celebrated Deming Cycle [14, 21] from management science, SciEducator introduces an iterative optimization mechanism that enables self-evolving reasoning and continuous enhancement. By repeatedly planning, executing, evaluating, and refining its workflows, the system progressively converges toward high-confidence and accurate interpretations as well as reliable educational content generation of complex scientific activities. Our SciEducator operates in two stages: understanding and educating. In the understanding stage, an LLM-based planner acts as the central controller. It retrieves domain knowledge from both internal and external sources, formulates multiple candidate workflows involving various agents (e.g., Video Content Acquisition Agent, Web or Academic Paper Search Agent), and stores them in solution pool. An LLM-based evaluator then integrates diverse metrics to assess each candidate and select the optimal workflow for execution. The execution results undergo confidence evaluation: if the planner deems the current results sufficiently convincing, it synthesizes the available information to produce the final output; otherwise, SciEducator proceeds with further processing. The execution outcomes and feedback, especially summarized failure reasons and newly acquired knowledge, are subsequently used to refine and update the solution pool, thereby embodying the Deming Cycles iterative improvement loop. In the educating stage, SciEducator generates multimodal e-learning materials tailored to the scientific content interpreted in the understanding stage. It first retrieves relevant experimental procedures, safety precautions, required equipment images, and shopping links, and then produces textual instructions, visual guides, audio narrations, and interactive e-booklets that present scientific knowledge in an engaging and accessible manner. To evaluate the superiority of our system, we introduce SciVBench, new benchmark containing 500 expertvalidated and literature-grounded questionanswer (QA) pairs across three scientific domains: physics experiments, chemistry experiments, and daily life phenomena. The QA pairs are further categorized into five types, i.e., terminology, principle, prediction, reading, and design. We also propose comprehensive set of metrics for comparative evaluation of both understanding and educating capabilities. Experimental results show that SciEducator consistently outperforms leading commercial MLLMs and state-of-the-art multi-agent systems in both aspects. Our main contributions are summarized as follows: We propose SciEducator, the first multi-agent system for scientific video understanding and educating, which not only comprehends complex and fine-grained scientific videos but also generates multimodal educational ebooklets that integrate diverse information to deliver engaging guidance and stimulate scientific curiosity. Inspired by the Deming Cycle, we equip SciEducator with an iterative workflow optimization mechanism that progressively refines solutions through failure analysis and newly acquired knowledge, substantially enhancing its capability to handle meticulous scientific activities and offering novel paradigm for agentic system design. We construct SciVBench, the first benchmark for scientific-phenomenon video analysis, featuring diverse questionanswer pairs across physics, chemistry, and daily life phenomena, along with comprehensive set of evaluation metrics. 2. Related Work 2.1. Multimodal Large Language Models Recent years have witnessed substantial progress in vision-language models applied to video understanding. VTimeLLM introduces boundary-aware three-stage training strategy [10], TimeSuite employs temporal adaptive position encoding [28], and Slowfast-LLava designs lowspeed feature sampler to enhance temporal awareness in visual representations [25]. Closed-source models such as GPT-4o [15], Gemini [9], and Claude [1] have also been widely adopted as foundational MLLMs in downstream video tasks. However, these systems are critically limited by their inherent inability to effectively leverage external resources and integrate specialized tools. This fundamental shortfall severely curtails their functional capacity and practical applicability in various scenarios. 2.2. Multi-Agent Systems With the widespread application of multi-agent systems(MASs) across numerous domains, computer vision researchers have also begun exploring MASs for video understanding [8, 26]. VideoAgent [23] proposed framework capable of interactive reasoning and planning through confidence evaluation, focusing particularly on long-form video comprehension. In [7], memory-enhanced MAS for video understanding was introduced, emphasizing structured spatiotemporal information. VideoAgent2 [31] simulated the cognitive process of human video understanding by designing an uncertain chain-of-thought reasoning approach. However, they still face several challenges. Their performance is limited by the inherent weaknesses of LLM, like hallucinations and instability. For scientific video tasks requiring external expertise and rigorous step-by-step reasoning, they often fail to produce viable initial plans and lack systematic mechanisms that enable them to self-evolve and optimize their workflows based on past results. 3. SciEducator Our pipeline is illustrated in Fig. 2, where we introduce SciEducator, multi-agent system for scientific video comprehension and education. We begin by giving the problem definition in Sec. 3.1, and introduce the specialized tools & agents integrated into the system in Sec. 3.2, followed by description of SciEducators video understanding workflow in Sec. 3.3, and the educational booklet generation process in Sec. 3.4. These designs enable the system to effectively analyze problems, provide feasible solutions, and ultimately produce accurate and robust responses, with an optional education e-booklet generation. 3.1. Problem Definition and Overview The goal of our multi-agent system is to return an accurate, self-consistent answer given user query and scientific video ; whats more, it is optional to generate an educational booklet after video understanding. The core workflow of our approach can be formally represented as follows: = S(Q, ; P, E, ), (1) where and represent the planner and evaluator, which act as the leading roles inside the Deming (PDSA) cycle. denotes the dynamically configured set of tools and agents aiming at different stages inside our system. For example, in the Deming cycle, includes TPlan, TDo, TStudy; and in the educational booklet generation stage, refers to TBooklet. Speaking of the PDSA iteration, which is the core of our pipeline, it conducts the plan-do-study-act process iteratively to obtain the most refined and reasonable answer, as shown in Fig. 2. Specifically, plan refers to the process of retrieving the corresponding knowledge and building up solution pool from our systems internal knowledge base. The do stage targets at assessing to find the most appropriate solution from the pool, and provides confidence to decide either to stop cycles and give well-structured response to users, or step into the study stage. In study, our system aims to find the reasons for being unable to give answers, and seeks useful new information. After updating the solution pool with the act stage, it will have better confidence to provide good reply to the query. We will illustrate more details in Sec. 3.3. Another key aspect of our system is the extension of scientific video understanding toward educational content generation, enabling the automatic creation of structured ebooklets that illustrate and guide the replication of scientific phenomena with high relevance, clarity, and appeal to young learners. Further details are provided in Sec. 3.4. 3.2. Tools & Agents Configuration Our system integrates total of sixteen specialized components, including ten agents and six tools, each tailored for specific subtasks with well-defined inputoutput interfaces. These components are organized into two categories: (i) dynamically invocable tools/agents, which are adaptively called based on the systems reasoning context; and (ii) fixed-execution tools/agents, which operate automatically at predetermined stages of the workflow. The dynamically invocable components primarily support functions such as task planning, content acquisition, evaluation, web or literature retrieval, and safety prompting. The fixed-execution components focus on knowledge-base maintenance, multimodal synthesis, and e-booklet generation. Detailed descriptions of all tools and agents, including their configurations and interdependencies, are provided in the Supplementary Material. 3.3. Deming Cycle within SciEducator instantiate as controller SciEducator (PDSA) closed-loop We PlanDoStudyAct to tailored scientific video understanding and its downstream educational content creation. The same loop is later reused for e-booklet generation (Sec. 3.4) with slightly different toolset and, when applicable, without video input. Each of the four stages in the i-th cycle is described as follows. Plan stage. Given user query and video sampled at one fps, we prompt captioner to obtain temporally grounded description Vcontent. retrieval-augmented agent then extracts salient entities and keywords and retrieves domain knowledge from an internal corpus. Noted that, this is only retrieved in the first cycle for once, and will be updated just by the study stage. The planner composes diverse pool of candidate plans by combining Vcontent and Figure 2. SciEducator architecture overview. We design multi-agent system capable of implementing the PDSA cycle, which iteratively optimizes output responses through cyclic iterations. K, using few-shot exemplars to stabilize call signatures and parameter settings. This stage can be summarized as: Mi, = (cid:0)Q, V, TPlan (cid:1), (2) where Mi is the candidate plan pool in the i-th cycle and TPlan denotes the planning toolset. Do stage. Given the solution pool Mi, the evaluator scores each plan with composite of objective Aobj and LLM-based perceptual criteria Apercep and selects the best trade-off in time and token efficiency, success likelihood, feasibility, and overall performance. To estimate time and token costs, we build an empirical prior by issuing 20 randomized probe calls per tool/agent, from which we obtain average latency, average token usage, and success probability. For relevance and feasibility, we weight plans [18], keywords using inverse document frequency (IDF) reflecting their ability to retrieve discriminative knowledge for the current video: IDF(k) = log (cid:18) f(k) + 1 (cid:19) , (3) where is the corpus size and f(k) is the number of documents containing k; we use an internal corpus of 84 physics In parallel, subjective judge and chemistry documents. compares plans on coverage, logical coherence, scientific soundness, and clarity. The selection of the best solution is defined as: = arg max sMi (cid:2) Aobj(s; E, IDF) + λ Apercep(s) (cid:3), (4) which is then executed step-by-step to produce the stage output Ri. We summarize the do stage as (cid:1), Ri = E(cid:0)Mi, V, TDo (5) where E() is the evaluator and TDo refers to the do-stage toolset. After obtaining Ri, the planner estimates confidence score using the query Q, the video context , and the executed plan: Ci = (Ri, Q, ). (6) This score indicates whether the available evidence suffices to address the query and yield convincing answer; if high, SciEducator synthesizes the information and produces well-structured response. Study and Act Stage. If confidence is low, the system enters the study phase: the planner diagnoses why the answer is insufficient (e.g., tool failures, overly broad and irrelevant retrieval, or insufficient detail in video captions) and aggregates any useful evidence discovered this round into the knowledge base. Formally, Fi, Ki+1 = (cid:0)Ri, Ki, Q, V, TStudy (cid:1), (7) where Fi is the failure analysis and Ki+1 = Ki Knew is the updated knowledge. Guided by Fi, Ki, the act function ΓAct replans the next pooladjusting structure, granularity, or query specificity (e.g., apply video super-resolution for blurry frames, increase captioning fps for missed actions, and refine queries with more specific entities): (cid:1). (cid:0)Fi, Ki+1, Mi Mi+1 = ΓAct (8) The system executes the next iteration with the updated solution pool Mi+1 and repeats the confidence check, iterating until Ci τ or maximum number of cycles we choose. 3.4. Educational E-booklet Generation After video understanding, SciEducator identifies the specific scientific phenomenon in the video and its underlying principles. When required, it triggers multimodal retrieval and generation pipeline to produce child-friendly e-booklet that includes: (a) experimental guidance; (b) required equipment with purchase links; (c) step-by-step procedures with instructional diagrams; (d) safety precautions for both equipment and operations with accompanying audio prompts; and (e) concise summary of the experiment and its principles. The booklet adopts progressive, logically organized layout to support rapid comprehension and faithful replication by young readers. To assemble content, SciEducator uses an Entity Recognition agent to extract key entities, then the Procedure Search and Safety Alert agents to retrieve replicable steps and precautions. We target four properties: (i) relevance (avoiding content unrelated to the experiment), (ii) instructional quality (complete, detailed, and safety-aware), (iii) attractiveness (engaging, visually impressive replication), and (iv) educational value (steps that expose the governing principles). The process follows the same PDSA loop as Sec. 3.3, but here the input is only the query, confidence is assessed w.r.t. these four properties, while the available tools are different. After obtaining the procedures, an Equipment Search tool returns images and purchase links for the relevant equipment; an Illustration Generation tool produces stepwise guidance images; and Speech Generation tool synthesizes audio instructions. Finally, an E-booklet Generation agent integrates text, images, audio, hyperlinks, and layout into well-structured booklet in the style of childrens science literature. Compared with singleLLM baseline, SciEducator provides richer modalities with higher relevance and instructional quality, while improving engagement through integrated textimageaudio design. 4. Experiments 4.1. SciVBench Dataset To construct specialized, accurate, and reasoningintensive collection of scientific videos with correspondFigure 3. Proportional distribution of video and QA categories. SciVBench comprises three types of scientific videos and five categories of questions, enabling comprehensive evaluation of models ability to acquire diverse domain knowledge and tackle various complex scientific problems. ing QA pairs, we systematically gathered rich scientific video resources from major video platforms and science education websites, including 54 physics experiment videos, 54 chemistry experiment videos, and 103 daily life phenomenon videos as shown in Fig. 3. Domain experts in physics and chemistry systematically reviewed the subtitles, explanatory narrations, knowledge manuals or guides, as well as relevant reference materials and academic papers for each video. Two domain experts independently authored and validated each QA pair, and disagreements were adjudicated by third expert. Through rigorous cross-validation with additional sources, we developed 500 scientificallygrounded question-answer pairs based on the video content: 160, 148, and 192 pairs for physics, chemistry, and daily life phenomenon videos. Only the visual content of the videos was used as input, with all subtitles and audio narrations removed. Each QA pair was designed such that correct answers could only be derived by comprehending the video content, not from the question text alone. All answers maintain logical integrity, forming coherent reasoning chains from fundamental knowledge to final conclusions, while avoiding any semantic deviation from reference materials and papers. These highly specialized, accurate, and reasoningintensive QA pairs establish solid foundation for evaluating the performance of existing MLLMs and multi-agent systems (MASs) in scientific video comprehension and validating the effectiveness of our SciEducator. 4.2. Evaluation Metrics Evaluation Metrics for Understanding. We employ two metrics to evaluate the performance in scientific video understanding, using qwen3-max as the unified evaluator for all model-generated answers. Specifically, these metrics are: Relevance (Rel): We first provide the reference answer and extensive scientific background for the current question to Qwen3-Max [2], instructing it to assess how relevant the model-generated answer is to the queryfocusing on whether the response aligns with the scientific subdomain involved, regardless of correctness. Figure 4. Qualitative comparison between SciEducator and MLLMs in Education E-booklet Generation. The left is our generated e-booklet with comprehensive contents and well-organized structure, while other popular MLLMs (see the right) all fail in such generation. These examples demonstrate SciEducators ability to generate more comprehensive, better-structured, and more attractive Education E-booklet. The goal is to evaluate whether the answer provides misleading or irrelevant information. The scoring uses three discrete values: 0 for irrelevant responses, 0.5 for partially relevant, and 1 for fully relevant. Accuracy (Acc): We then use Qwen3-Max [2] to analyze the semantic similarity between the generated answer and the reference answer, scoring the correctness of the models response. Similarly, this metric employs three discrete scores: 0 for completely incorrect answers, 0.5 for partially correct, and 1 for fully correct. Detailed scoring strategies and prompts can be found in the supplementary materials. The relevance and accuracy metrics for each model are calculated as the average scores across all questions and are presented in the results table as percentages, with the percentage symbols omitted. Evaluation Metrics for Education. To assess the educational performance of SciEducator and various MLLMs, we decoupled this part from the understanding benchmark. All models were provided with the scientific terminology of the current experiment in video and asked to generate procedures and precautions for replicating it. Although SciEducator can generate more modalities than MLLMs, we focused specifically on comparing their shared textual modality for quantitative comparison. We employed four metrics and uniformly used QwenVL-Plus [2] to evaluate all model responses in comparative setting. Qwen-VL-Plus [2] was supplied with substantial background information about each experiment as reference. Specifically, the metrics are: Relevance: How well the generated experimental procedures and precautions align with the current experiment and its underlying principles. Instructional Quality (IQ): How effectively the generated procedures and precautions guide children in conducting the experiment, with emphasis on detail orientation, completeness, clarity, and safety warnings. Attractiveness: comprehensive assessment of how engaging the textual instructions are. For SciEducator, the aesthetic quality of its supporting illustrations is also incorporated into this evaluation to identify the most captivating response. Educational Value (EV): How well each models response stimulates childrens scientific interest and guides them to understand the principles through the experiment. Detailed scoring strategies and prompts can be found in the supplementary materials. We evaluate 40 videos (Education Subset). For each video, all model responses were collectively anonymized and input into the VLM for evaluation. The best response for each metric was selected, and Table 1. Quantitative Comparison between SciEducator and popular MLLMs and MASs for scientific video understanding on SciVBench. Compared with these SOTA models, our model exhibits higher relevance and accuracy. Model GPT-4o [15] Gemini 2.0 flash [9] Claude 3.7 Sonnet [1] VideoAgent[23] videoagent[7] SciEducator (Ours) Physics Rel Acc Chemistry Acc Rel Daily Life Rel Acc Commercial APIs 31.42 36.15 31.76 39.86 46.96 40.20 34.69 38.75 31.88 30.73 34.64 31.77 Multi-Agent Systems 36.56 35.31 65.31 45.61 46.62 73.97 34.80 37.16 64.86 30.47 31.51 64.58 27.86 31.25 28.65 27.34 28.13 62. 47.50 52.81 44.06 49.06 46.25 81.88 Table 2. Quantitative Comparison between SciEducator and popular MLLMs for scientific education on SciVBench Education Subset. Each metric is reported by win rate (%). Model Education Subset Relevance IQ Attractiveness EV Gemini 2.0 flash [9] GPT-4o [15] Claude 3.7 Sonnet [1] SciEducator(Ours) 10.00 7.50 5.00 77.50 2.50 5.00 5.00 87. 0.00 2.50 0.00 97.50 5.00 7.50 5.00 82.50 final win rates were calculated and presented in tables. Table 3. Ablations on different maximum iteration rounds for education. Each metric is reported by win rate (%). Model Max Round(s) Education Subset SciEducator SciEducator SciEducator 1 3 5 Relevance IQ Attractiveness EV 2.50 7.50 90.00 0 7.50 92.50 2.50 32.50 65.00 15.00 35.00 50.00 more organized and higher-quality answers. As shown in Fig. 1, SciEducator can generate e-learning materials incorporating four modalities: text, images, hyperlinks, and audio, featuring rich colors and aesthetically pleasing layouts. The text is structured with sequential sections including title, introduction, experimental procedures and precautions, and summary. As illustrated in Fig. 4, this structural arrangement effectively guides readers through understanding the principles and completing the experiment. The visual elements include images of required materials and model-generated diagrams illustrating experimental steps, providing clearer guidance than text alone and highlighting SciEducators significant advantage over conventional LLMs. 4.3. Experimental Results 4.4. Ablation Studies Quantitative Analysis. To verify SciEducators effectiveness, we evaluate SciEducator on SciVBench against three closed-source MLLMs (Claude 3.7 Sonnet [1], GPT-4o [15], Gemini 2.0 Flash [9]) and two state-of-the-art multi-agent systems (MASs) (VideoAgent [23] and videoagent [7]), and report results in Tab. 1 for scientific video understanding and Tab. 2 for educational content generation. For scientific video understanding, Tab. 1 shows that SciEducator consistently surpasses all baselines on the relevance and accuracy metrics across the physics, chemistry, and daily-life tracks, indicating tighter adherence to the visual evidence and underlying scientific principles than general-purpose MLLMs and prior MASs. Tab. 2 reports results for the scientific educational content generation task, comparing SciEducator with the closed-source MLLMs (MASs baselines do not support this task). SciEducator demonstrates clear advantage in creating high-quality instructional materials. Qualitative Analysis. As illustrated in Fig. 5, for specific user queries, SciEducator provides responses that are more comprehensive, detailed, and clearly explained compared to MLLMs such as GPT-4o [15]. This advantage stems from two core capabilities of SciEducator: first, the integration of multiple powerful search tools significantly expands the systems knowledge capacity; second, its robust planning and replanning capabilities enable it to better analyze complex video content and acquired information, resulting in Effectiveness of PDSA Cycles. To validate the progressive improvement of SciEducator in response relevance and accuracy through PDSA cycles, we configured the maximum number of cycles from 1 to 5. After reaching the cycle limit, the system was compelled to generate the most likely correct answer based on accumulated knowledge. Results in Fig. 6 show that performance improves significantly with more cycles, demonstrating SciEducators capacity for selfevolution and its ability to iteratively deepen understanding, acquire relevant knowledge, and solve unconventional, complex problems. To validate the progressive improvement of SciEducator in generating educational text via PDSA cycles, we compare three variants with maximum cycle counts of 1, 3, and 5 on the Education Subset of SciVBench, with results also presented as win rates across various evaluation aspects and summarized in Tab. 3. Results demonstrate that as the PDSA cycles increase, the models performance improves, yielding instructional text with stronger relevance and more educational value, which in turn leads to the generation of higher-quality guidance images. Effectiveness of Evaluator Agent. To validate that each evaluation element in the Evaluator Agent effectively reduces resource consumption and execution rounds, we compared the complete EA with ablated versions lacking these elements on 500 QA pairs. As shown in Tab. 4, metrics include average time, token consumption, number of exeFigure 5. Qualitative comparison between SciEducator and MLLMs. These examples demonstrate SciEducators ability to generate more comprehensive, better-structured, and more logically coherent answers than the other MLLMs. Table 4. Ablations on Evaluator Agent (EA). Note that denotes empirical knowledge and Apercep represents Perceptual Evaluation metrics. Time and token consumption values are presented as ratios, with the total consumption of the complete EA version normalized to 1.00. Figure 6. Ablations on different maximum iteration rounds for scientific video understanding. Performance rises with larger Max Rounds for three categories, visualizing the benefit of iterative PDSA cycles. Agent Time Token Average Rounds Acc EA w/o EA w/o IDF EA w/o Apercep EA 1.20 1.08 1.14 1.00 1.18 1.06 1.13 1.00 4.09 3.99 4.17 3. 57.50 59.90 54.50 64.00 cution rounds, and final accuracy, with consumption values shown as ratios. Note that the total consumption of the complete EA is normalized to 1.00. The maximum number of execution rounds was set to 5. Experimental results demonstrate that each evaluation element in the Evaluator Agent effectively increases the probability of selecting superior solutions, reduces resource consumption and identifies more feasible options, and consequently generates responses with higher accuracy within limited iteration cycles. Effectiveness of Knew & . To validate the effectiveness of new knowledge Knew and failure analysis of the Study Stage, which updates the solution pool and improves the quality of candidate solutions, we conducted ablations by removing these elements, as presented in Tab. 5. In the variant where both Knew and are removed, no feedback is available, and updates only discard executed solutions. The experimental results indicate that Knew and are critTable 5. Ablations of on the effectiveness of different elements in the Study Stage. Knew represents the new knowledge acquired during execution, and denotes failure analysis. Model Physics Chemistry Daily Life Rel Acc Rel Acc Rel Acc SciEducator w/o Knew & SciEducator w/o Knew SciEducator w/o 59.69 65.94 71.56 45.94 50.94 55. 53.04 61.82 66.55 45.27 54.05 57.09 35.94 38.28 48.95 32.55 34.64 45.83 SciEducator 81. 65.31 73.97 64.86 64.58 62.24 ical for optimizing existing solutions and generating new ones. Within the PDSA process, these elements substantially improve the quality of the new solution pool, thereby enhancing both the relevance and accuracy of the results. 5. Conclusion In this work, we present SciEducator, the first iterative selfevolving multi-agent system for scientific video comprehension and education. By reformulating the Deming Cycles PlanDoStudyAct philosophy into self-evolving reasoning and feedback mechanism, the system progressively enhances its interpretation of complex scientific activities to deliver accurate and reliable results. To ensure comprehensive performance assessment, we construct the SciVBench benchmark. Extensive experiments show that SciEducator significantly outperforms current popular models, demonstrating remarkable superiority and application potential, and is expected to offer meaningful inspiration for future multi-agent systems and broader domains."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.7 sonnet and claude code, 2025. Official model announcement and capabilities overview. 2, 7 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 5, 6 [3] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1 [4] Runpeng Dai, Run Yang, Fan Zhou, and Hongtu Zhu. Breach in the shield: Unveiling the vulnerabilities of large language models. arXiv preprint arXiv:2504.03714, 2025. 2 [5] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024. [6] Francesco Cardinale et al. Isr. https://github.com/ idealo/image-super-resolution, 2018. [7] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, pages 7592. Springer, 2024. 1, 3, 7 [8] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023. 3 [9] Google DeepMind. Introducing gemini 2.0: our new ai model for the agentic era, 2024. Official announcement; includes Gemini 2.0 Flash details. 2, [10] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427114280, 2024. 2 [11] Kunal Jha, Wilka Carvalho, Yancheng Liang, Simon Du, Max Kleiman-Weiner, and Natasha Jaques. Crossenvironment cooperation enables zero-shot multi-agent coordination. arXiv preprint arXiv:2504.12714, 2025. 2 [12] Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. Llms get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120, 2025. 2 [13] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [14] Ronald Moen, Clifford Norman, et al. Evolution of the pdca cycle, 2006. 2 [15] OpenAI. Gpt-4o system card. https://arxiv.org/ abs/2410.21276, 2024. System Card describing capabilities, limitations, and safety evaluations of GPT-4o. 2, [16] Wei Pang, Kevin Qinghong Lin, Xiangru Jian, Xi He, Paper2poster: Towards multimodal arXiv preprint and Philip Torr. poster automation from scientific papers. arXiv:2505.21497, 2025. 2 [17] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6(3):1, 2023. 2 [18] Juan Ramos et al. Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning, pages 2948. New Jersey, USA, 2003. 4 [19] Jiayuan Rao, Zifeng Li, Haoning Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Multi-agent system for comprehensive soccer understanding. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 36543663, 2025. [20] Yufei Shi, Weilong Yan, Gang Xu, Yumeng Li, Yucheng Chen, Zhenxi Li, Fei Yu, Ming Li, and Si Yong Yeo. Pvchat: Personalized video chat with one-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2332123331, 2025. [21] Michael Taylor, Chris McNicholas, Chris Nicolay, Ara Darzi, Derek Bell, and Julie Reed. Systematic review of the application of the plandostudyact method to improve quality in healthcare. BMJ quality & safety, 23(4):290298, 2014. 2 [22] Oguzhan Topsakal and Tahir Cetin Akinci. Creating large language model applications utilizing langchain: primer on developing llm apps fast. In International conference on applied engineering and natural sciences, pages 10501056, 2023. [23] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena YeungLevy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 5876. Springer, 2024. 1, 3, 7 [24] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for mulIn European Conference on timodal video understanding. Computer Vision, pages 396416. Springer, 2024. 1 [25] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Slowfast-llava: strong training-free baseDehghan. arXiv preprint line for video large language models. arXiv:2407.15841, 2024. 1, 2 [26] Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, and Yi Yang. Doraemongpt: Toward understanding dynamic scenes with large language models (exemplified as video agent). arXiv preprint arXiv:2401.08392, 2024. 3 [27] Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. Cognitive mirage: review of hallucinations in large language models. arXiv preprint arXiv:2309.06794, 2023. 2 [28] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, et al. Timesuite: Improving mllms for long video understanding via grounded tuning. arXiv preprint arXiv:2410.19702, 2024. [29] Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, and Xiang Wang. Multi-agent architecture search via agentic supernet. arXiv preprint arXiv:2502.04180, 2025. 2 [30] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. 2 [31] Zhuo Zhi, Qiangqiang Wu, Wenbo Li, Yinchuan Li, Kun Shao, Kaiwen Zhou, et al. Videoagent2: Enhancing the llm-based agent system for long-form video understanding by uncertainty-aware cot. arXiv preprint arXiv:2504.04471, 2025. 2, 3 SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Tools & Agents Our system integrates 10 agents and 6 tools, each designed to handle specific tasks with clearly defined input and output specifications to ensure precise execution and seamless integration. The set of tools and agents is categorized into two groups: (i) dynamically invocable components and (ii) fixed-execution components, described as follows. A.1. Dynamically Invocable Tools/Agents (i) Planning and Answer Generation (Planner Agent): Our system uses Planner Agent configured with GPT-4o as its core. Leveraging the powerful text processing and knowledge reasoning capabilities of the LLM, it is responsible for generating and adjusting the solution pool within the system, consolidating all acquired information, evaluating confidence level, and outputting the final answer. (ii) Video Content Acquisition (Captioner Agent): Our system uses Captioner Agent configured with Gemini 2.0 Flash to obtain the content of video frames and generate textual descriptions of the video. (iii) Solution Evaluation (Evaluator Agent): Our system uses an Evaluator Agent configured with GPT-4o to evaluate the various solutions in the solution pool. It selects the best solution from the current pool by considering both subjective and objective metrics and initiates its execution. (iv) Web Content Search (Web Search Agent): We use the open-source web content search model SegGPT, based on the Google Search engine, to search for web links related to input keywords and organize the web information for output. (v) Paper Search (Paper Search Agent): We develop Paper Search Agent configured with GPT-4o for searching academic paper content. It can search for papers related to input keywords across major scientific paper platforms, organize and summarize the found content, and finally output the results. (vi) Video Super-Resolution Tool (VideoSR Tool): We built tool for performing super-resolution on video frames based on an open-source model. This enhances lowresolution or blurry frames, ultimately improving the description of the video content. (vii) Experimental Procedure Search (Procedure Search Agent): We develop Procedure Search Agent based on the open-source web content search model searchGPT. It searches for corresponding experimental procedures based on input experimental terms or descriptions, organizes the web information, and outputs the results. (viii) Key Entity Recognition in Content (Entity Recognition Agent): SciEducator uses an Entity Recognition Agent configured with GPT-4o to identify key experimental instruments and materials involved in the current experimental steps, and outputs them in specified format. (ix) Experimental Precautions Prompter (Safety Alert Agent): We develop Safety Alert Agent based on the open-source web content search model searchGPT. It comprehensively searches for corresponding safety precautions based on the current experimental content and equipment, organizes the information, and outputs it to alert readers to avoid potential hazards when conducting the experiment. A.2. Fixed-Execution Tools/Agents (i) Knowledge Base Construction and Storage: We develop tools for knowledge base construction and storage. These tools archive knowledge texts, invoke an embedding model to create and store query vectors, thereby facilitating subsequent retrieval. The construction and storage of the knowledge base must be completed before the DCAgent operates. (ii) Knowledge Base Retrieval (RAG Agent): We construct small-scale knowledge base containing basic scientific knowledge and fundamental material properties (e.g., electromagnetic induction, properties of air). We configured Retrieval-Augmented Generation (RAG) Agent based on GPT-4o. Based on the video content, it can extract important keywords, retrieve the most relevant content from the knowledge base based on these keywords, and output an organized summary. If no relevant content exists in the knowledge base, it will output statement indicating the lack of relevant knowledge. (iii) IDF Value Calculation for Keywords in Solutions (IDF Calculator Tool): We independently develop tool capable of calculating the Inverse Document Frequency of each keyword within the knowledge base, indicating the uniqueness of each keyword and its importance to the video content. It is fixedly invoked each time the Evaluator Agent evaluates the solutions in the solution pool. The Evaluator Agent extracts keywords from each solution and inputs them into the IDF Calculator Tool for computation. (iv) Experimental Equipment Image and Purchase Link Search (Equipment Search Tool): We independently develop tool for searching images and purchase links of experimental equipment, typically returning one image and one most relevant link per equipment item. (v) Experimental Procedure Illustration Generation (Illustration Generation Tool): We utilize the Gemini 2.5 Flash Image (Nano Banana) API to generate illustrations of experimental procedures by inputting descriptive text of the steps. (vi) Text-to-Speech Conversion (Speech Generation Tool): We employ the open-source tool KittenTTS to convert experiment-related text into speech. (vii) Multi-modal Information Integration for Structurally Organized and Aesthetically Arranged Ebooklet Compilation (E-booklet Generation Agent): We independently developed an E-booklet Generation Agent for integrating multi-modal information related to experiments, such as text, images, links, and audio. Implemented in JavaScript/CSS,the agent composes an HTML e-booklet with clear hierarchy and an aesthetically pleasing layout to stimulate readers scientific interest in vivid and engaging manner. B. Details about Empirical Prior To obtain the resource consumption and the probability of returning expected results for dynamically invocable tools/agents, we build an empirical prior by issuing 20 randomized probe calls per tool/agent, from which Evaluator Agent obtains average latency, average token usage, and success probability. Token usage is directly converted into the corresponding platforms API money cost. Results: Web Search Agent: On average, each call takes 21.89s, costs $0.0139, and 88% success rate. Paper Search Agent: On average, each call takes 17.41s, costs $0.010, and 75% success rate. Captioner Agent: On average, each call takes about 25s if the frame rate is 1 FPS, costing $0.0001; both time and financial cost increase in direct proportion to the FPS, 93% success rate. VideoSR Tool: Approximately 53s if the frame rate is 1 FPS, costing $0.0001; both time and financial cost increase in direct proportion to the FPS, 100% success rate. Procedure Search Agent: On average, each call takes 28.17 seconds, costs $0.0178, and 87% success rate. Entity Recognition Agent: On average, each call takes 10.45 seconds, costs $0.0064, and 99% success rate. Safety Alert Agent: On average, each call takes 26.29 seconds, costs $0.0153, and 72% success rate. Based on our practical considerations, we combine time consumption (t) and financial cost (c) into total cost: + 1000c. This is provided to the Evaluator Agent as reference. The Evaluator Agent is instructed to give equal weight to resource consumption and feasibility considerations, and ultimately to select the overall optimal solution. C. Evaluation Metrics C.1. Understanding We employ Qwen3-Max to uniformly evaluate all modelgenerated responses. We first provide the reference answers and substantial scientific background regarding the query questions, instructing Qwen3-Max to assess the relevance of the model-generated answers to the query, focusing on how well the response aligns with the scientific subdomain involved in the question, regardless of correctness. The objective is to determine whether the model provides misleading or irrelevant information. The detailed scoring strategy is as follows: 1.If the answer is entirely relevant to the subfield of the question, it receives relevance score of 1. 2.If an answer contains partially irrelevant content or is only broadly related to the field, it receives score of 0.5. 3.If it is completely irrelevant, it receives score of 0. Prompt: Evaluating Relevance You are professional scoring teacher and evaluation expert. Your task is to evaluate the relevance of responses based on the reference answer and the scientific background related of question, using three scoring options: 1 point, 0.5 points, or 0 points. Please follow these scoring criteria in order of priority from highest to lowest: 1.If the answer is entirely relevant to the specific subfield of the question, award 1 point. 2.If the answer contains partially irrelevant content or is only broadly related to the field of the question, assign 0.5 points. 3.If the answer is completely irrelevant, assign 0 points. Please provide the points directly, only the number, no other output. the models We then use Qwen3-Max to analyze the semantic similarity between generated answers and reference answers, scoring the accuracy of model responses. The detailed scoring strategy is as follows: 1.If errorsincluding numerical inaccuracies (e.g., the correct answer is 3, but the model answers 2) or terminological mistakes (e.g., the correct answer is Magnus Effect, but the model answers Bernoullis Principle)it receives score of 0 directly. response contains absolute 2. Qwen3-Max is prompted to analyze the key points of the reference answer and determine whether the models response covers all of them. If no key point is covered, the score remains 0. If some key points are covered and the remaining content contains no absolute errors, score of 0.5 is assigned. 3. If all key points are covered and no absolute errors are present in the remaining content, full score of 1 is awarded. Prompt: Evaluating Accuracy You are professional scoring teacher and evaluation expert. Your task is to evaluate the quality of responses based on reference answers, using three scoring options: 1 point, 0.5 points, or 0 points. Please follow these scoring criteria in order of priority from highest to lowest: 1. First check for any absolute errors that contradict the reference answers, such as instances where the reference answer deems something correct but the response considers it incorrect, or numerical mistakes; if such errors occur, award 0 points. 2. Then analyze the key points of the reference answers and compare them to the responses, noting that wording can differ but the meaning must be strictly identical to count as match; if the response fully covers all key points of the reference answers, give 1 point, and additional correct content without errors can still warrant 1 point. 3. Finally, if the response matches only part of the key points and contains no absolute errors, assign 0.5 points, but exercise caution when awarding this score. Dont be too rigid, you should fully refer to the different expressions of the standard answer. If there are more details than the standard answer, it should be considered correct Please provide the points directly, only the number, no other output. C.2. Educating We employ four metrics and uniformly use Qwen-VL-Plus to evaluate all model responses in comparative setting. Qwen-VL-Plus receives supplied with substantial background information about each experiment as reference. Specifically, the metrics are: Relevance: How well the generated experimental procedures and precautions align with the current experiment and its underlying principles. Instructional Quality (IQ): How effectively the generated procedures and precautions guide children in conducting the experiment, with emphasis on detail orientation, completeness, clarity, and safety warnings. Attractiveness: comprehensive assessment of how engaging the textual instructions are. For SciEducator, the aesthetic quality of its supporting illustrations is also incorporated into this evaluation to identify the most captivating response. Educational Value (EV): How well each models response stimulates childrens scientific interest and guides them to understand the principles through the experiment. Prompt: Evaluating Performance in Education You are fair and professional evaluation expert. Several models have generated experimental procedures and safety precautions for specific scientific phenomenon. Your task is to compare the responses produced by these different models from four aspects and select the bestperforming model for each aspect. You will be provided with description of the scientific phenomenon and substantial background information about it as reference. The four aspects are as follows: Relevance: How well each models generated experimental procedures and precautions align with the current experiment and its underlying principles. Instructional Quality: How effectively each models generated procedures and precautions guide children in conducting the experiment, with emphasis on detail orientation, completeness, clarity, and safety warnings. Attractiveness: How engaging the responses are. If any response contains images, the images should also be included in the evaluation. Educational Value: How well each models response stimulates childrens scientific interest and guides them to understand the principles through the experiment. Please select the best-performing model for each aspect, referring to the models by their names, and output the result in the following format: [ { Relevance: model name, Instructional Quality: model name, Attractiveness: model name, Educational Value: model name } ] D. Main Prompts D.1. Plan Stage Prompt: Plan Stage User Query: {user query} Video Path: {video path} Video Content Description: {video description} RAG Search Results: {rag result} You are scientific video understanding task planning expert. Your responsibilities are: 1. Generate multiple possible solutions based on video information, user queries, and retrieved knowledge 2. Each solution should include specific descriptions, tool call sequences, and parameters for each tool input Please output list of solutions directly in JSON format without any additional text: [ { Number: Solution number (integer), description: Solution description (detailed explanation of the solution process), steps: [ { tool: Name of the tool to call (must use one of the following recognized tool names: WebSearch, PaperSearch, Captioner, VideoSR), input: Tool input parameters (clear and specific input content) } ] } ] Tool Description: - WebSearch: Web search tool, can input query sentence - PaperSearch: Academic paper search tool, can input query sentence - VideoProcessor: Video understanding tool, inputs include video path, number of segments to divide the video into, frames per second to process, and information to search for in the video - VideoSR: Video super-resolution tool, input parameters are the same as VideoProcessor, performs super-resolution on input frames before understanding Do not include any other fields or text, output only JSON objects. Note: 1. Your solutions should be as logical and reasonably sequenced as possible, be relevant to user queries, striving to preserve the substances, environment, and specific movements of phenomena occurring in the video. Including the experimental objects and environment in tool input parameters may increase the success probability of the solution. 2. You should output as many possible solutions as possible to facilitate unified evaluation and improve success rate. Your output solutions may have more details, be more complete, and have longer call chains than our examples. 3. Please carefully check whether each step of your solution is conducive to solving the problem, such as querying the miraculous movements, changes, or interactions of objects, the environment or solution in which phenomena occur, etc. Make good use of each tool to obtain information and reduce ineffective calls. 4. Do not rashly conclude what phenomenon the video describes before you have obtained sufficient information. The parameters input to all tools in your solution should be as specific as possible. For example, if the video phenomenon occurs in mate tea, and you want to query the interaction between solid and liquid, or the peculiar movement of solids in liquid, we recommend that you always change the liquid here to mate tea, i.e., peculiar movement of solids in mate tea. 5. Note that you are working on scientific phenomenon understanding task. Your solutions should aim to clarify the scientific phenomena in the video content description. Please strictly adhere to the requirements D.2. Do Stage Prompt: Do Stage You are professional solution evaluation expert. Please evaluate multiple solutions and select the best one based on the following information: User Query: {user query} RAG Search Results: {rag result} Previous Solution Results: {previous results} Here are the solutions to evaluate along with their keyword IDF values (higher IDF values indicate more unique keywords): {plans with idf} Please follow these evaluation steps: 1. Analyze the feasibility and relevance of each solution. Evaluate feasibility and relevance according to the following criteria: 2. Consider the IDF values of keywords - higher IDF keywords may be more representative. Then consider the frequency of keywords in the video content (TF values) - higher TF keywords may be more representative. 3. Evaluate the completeness and logical coherence of each solution. Evaluate whether each solution contains scientific phenomena such as Why can person swing higher despite no energy being added? 4. Comprehensively consider the time consumption and financial cost of tools called in each solution, along with the probability of returning expected results of them. Below is the tool information list: WebSearch: 21.89s, $0.0139, 88% success rate PaperSearch: 17.41s, $0.010, 75% success rate VideoProcessor: 25s if the frame rate is 1 FPS, costing $0.0001; both time and financial cost increase in direct proportion to the FPS, with 93% success rate. VideoSR: 53s if frame per second = 1, $0.0001, both time and financial cost increase in direct proportion to the FPS, with 100% success rate. We roughly combine time consumption (t) and financial cost (c) into total consumption: + 1000c. You can reference this value for judgment 5. Considering all steps, giving equal weight to resource consumption and feasibility considerations, and ultimately to select the overall optimal solution. Please output the best solution directly in JSON format without any additional text: { best plan: { Number: solution number, description: solution description, Why: reasons you choose it as the best plan and reasons other plans are not good steps: [ { tool: tool name, input: input parameters } ] } } D.3. Study & Act Stage Prompt: Study & Act Stage User Query: {user query} Video Path: {video path} Video Content: {video descriptions} RAG Search Results: {rag result} Historical Execution Results and Current Execution Result: {history.get(previous results, [])} All Available Solutions: sure ascii=False, indent=2)} executing plan number:{Number} {json.dumps(all plans, enPlease analyze the reasons for the failure or poor outcome of this execution, summarize the existing execution results and new knowledge/information, adjust previous solutions, and attempt to generate completely new solutions. Finally, compile them into new solution collection called new plans. Return JSON object containing: { failure analysis: Analysis of failure reasons, knowledge summary: Summary of knowledge and information contained in existing execution results, new plans: List of new solutions (same format as the original solution list) } Tool Description: - WebSearch: Web search tool, can input query sentence - PaperSearch: Academic paper search tool, can input query sentence - VideoProcessor: Video understanding tool, inputs include video path, number of segments to divide the video into, frames per second to process, and information to search for in the video - VideoSR: Video super-resolution tool, input parameters are the same as VideoProcessor, performs super-resolution on input frames before understanding Here are some examples you can reference for adjusting or generating solutions based on failure reasons: 1.If you find that video information is insufficient, you can adjust or generate solutions by calling the VideoProcessor tool, increasing the input frame rate, and reducing the number of segments to obtain more information. However, you should not let VideoProcessor continue searching for things in existing solution as you might be going off track 2.If you need specific video information to determine the final answer, you can adjust or generate solutions by calling the VideoProcessor tool and specifying the questions you want to query 3.If the VideoProcessor tool returns that the video is blurry and affecting answer generation, you can call VideoSR to clarify the video and then understand. 4.If WebSearch or PaperSearch did not return expected information or timeout, you can try adjusting your search keywords 5.If the search returns information that is too broad, you can try using more precise keywords for searching 6.If the search returns several completely different scenarios and you need more details to determine the final answer, you can call appropriate tools to obtain them 7.If particular tool suddenly fails, you should reduce the use of that tool Note: You dont necessarily have to generate new solutions every time; its also acceptable to simply discard failed solutions. E. More Visualization Result In this section, we provide additional visualization results to further demonstrate SciEducators capabilities in both scientific video understanding and educational content generation. E.1. Scientific Video Understanding SciEducator leverages unique iterative self-evolving mechanism rooted in the Deming Cycle (Plan-Do-StudyAct) to achieve rigorous step-wise reasoning Fig. 7. Unlike standard MLLMs that may hallucinate or provide superficial answers, our system integrates external professional knowledge and performs failure analysis to refine its understanding. This capability serves as the foundation for the subsequent educational stage, ensuring that the scientific principles identified (e.g., light refraction, chemical reactions) are accurate before being translated into educational materials. E.2. Educational E-booklet Generation core innovation of SciEducator is its ability to generate comprehensive, child-friendly educational E-booklets, which is shown in Fig. 8. The system organizes multimodal contentincluding text, diagrams, and safety alertsinto structured format that fosters engagement and safety. The full E-booklet is visualized in five segments: The booklet begins with an engaging title and an Interesting Introduction designed to capture the learners curiosity. As shown in the visualization, the system uses evocative language (e.g., tiny scientist, exploring the wonders) to transform complex scientific concepts into an accessible narrative, setting the stage for the experiment. The E-booklet describes the detailed experimental materials and corresponding pictures and shopping links, which are helpful for the rapid commencement of the experiment. The E-booklet provides step-by-step, detailed description of the specific experimental procedures, along with corresponding illustrations, to assist in conducting the experiments. The E-booklet also provides important notes, which highlight some dangerous actions and operations that may lead to experimental failures, thereby ensuring the safety of experiments. The E-booklet concludes with concise summary that reinforces core concepts and takeaways. Figure 7. Qualitative comparison between SciEducator and MLLMs. These examples demonstrate SciEducators ability to generate more comprehensive, better-structured, and more logically coherent answers than the other MLLMs. Figure 8. Our generated e-booklet with comprehensive contents and well-organized structure. These examples demonstrate SciEducators ability to generate more comprehensive, better-structured, and more attractive Education E-booklet. (a) About Title and Interesting Introduction Figure 8. Our generated e-booklet (Continued). (b) Experiments Materials List Figure 8. Our generated e-booklet (Continued). (c) Experiments steps Figure 8. Our generated e-booklet (Continued). (d) Important Notes Figure 8. Our generated e-booklet (Continued). (e) Summary F. Extra Information F.1. Knowledge Base The knowledge base contains fundamental scientific concepts and detailed explanations in physics and chemistry. For instance, in physics, it covers topics such as Newtons second law, electromagnetic induction, and thermal expansion and contraction. In chemistry, it includes the combustion of metals and the properties of gases in the air. It also incorporates essential physics formulas and chemical equations. The scope of knowledge spans basic science from middle school to high school levels. Beyond this, it strictly excludes any advanced knowledge or uncommon scientific phenomena. The knowledge base is structured into 84 chapters, each with specific theme. Its primary purpose is to serve as foundational reference document corpus for IDF (Inverse Document Frequency) value calculation. Additionally, it aims to mitigate hallucinations produced by large language models during the plan stage by providing them with fundamental knowledge context. We utilize the textembedding-3-large model to compute and store embeddings for each chapter of the knowledge base. This allows for rapid vector retrieval in subsequent system runs, eliminating the need to recompute the knowledge base document embeddings each time. The concept behind this knowledge base can be applied to other fields as well, and is not limited to the domain of scientific video understanding. F.2. Average Cost Per Question We measure the average time consumption and token (monetary) cost per question for SciEducator when the maximum number of PDSA Cycle iterations during video understanding is set to 1, 3, and 5. The results are shown below. Note that in addition to the time and tokens consumed by the PDSA Cycle itself, fixed call to the Captioner Agent is required beforehand to obtain an initial video description. Maximum PDSA rounds = 1: Average time consumption per question is about 105s, with money cost of $0.0542. Maximum PDSA rounds = 3: Average time consumption per question is about 158s, with money cost of $0.0783. Maximum PDSA rounds = 5: Average time consumption per question is about 206s, with money cost of $0.1051. All money costs are automatically calculated by the platform of the APIs we call. F.3. SciVBench Dataset We provide some statistical analyses of videos and QA pairs in SciVBench. Fig. 9 presents statistics on the average video duration. Fig. 10 and Fig. 11 show statistical analyses of question and answer lengths. All three figures are presented separately for physics, chemistry, and daily life. Figure 9. Statistics of average duration for three video categories in SciVBench. Figure 10. Statistics of average question character length for three video QA categories in SciVBench. Figure 11. Statistics of average answer character length for three video QA categories in SciVBench."
        }
    ],
    "affiliations": [
        "Guangming Laboratory",
        "Jinan University",
        "Nanyang Technological University",
        "National University of Singapore",
        "Peking University",
        "South China University of Technology",
        "University of Electronic Science and Technology of China",
        "Zhejiang University"
    ]
}