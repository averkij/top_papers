{
    "paper_title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
    "authors": [
        "Xi Chen",
        "Mingkang Zhu",
        "Shaoteng Liu",
        "Xiaoyang Wu",
        "Xiaogang Xu",
        "Yu Liu",
        "Xiang Bai",
        "Hengshuang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 3 4 2 2 . 6 0 5 2 : r MiCo: Multi-image Contrast for Reinforcement Visual Reasoning Xi Chen1 Mingkang Zhu3 Shaoteng Liu3 Xiaoyang Wu1 Xiaogang Xu3 Yu Liu2 Xiang Bai4 Hengshuang Zhao1 1HKU 2 Tongyi Lab, Alibaba Group 3CUHK 4HUST"
        },
        {
            "title": "Abstract",
            "content": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine-grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and third, similar but distinct image. During training, the model is prompted to generate reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rulebased reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks."
        },
        {
            "title": "Introduction",
            "content": "Making visual analysis with multiple images is crucial in many real-world applications. For example, we understand actions through sequential images or videos, gain 3D awareness by recognizing multiview images, and analyze events by observing differences between states, etc. Although Vision Language Models (VLMs) [2, 8, 19, 15, 1] demonstrate promising capabilities in understanding single images, we find them struggle to link visual cues across multiple images. Multi-image understanding requires not only identifying fine-grained visual cues but also performing logical reasoning to uncover correspondences and differences among images. Recently, reasoning in language models [12, 16, 31, 27] has been significantly improved through the use of Chain-ofThought (CoT) prompting, especially when combined with rule-based reinforcement learning [28]. Therefore, straightforward idea to improve multi-image understanding is to extend this reinforcement learning paradigm to the visual domain. However, GRPO [28] requires constructing question-answer pairs with standard answers to compute rewards, which is particularly challenging for tasks involving fine-grained visual details and complex logic across images. Instead of focusing on constructing QA pairs, we explore how to incentivize VLMs to perform multi-image reasoning with minimal data preparation cost. Modern VLMs already possess strong perceptual and multimodal capabilities. Meanwhile, recent advances in RL-based single image reasoning [34, 23, 5] suggest that reasoning ability can be effectively acquired with limited data. Preprint. Under review. However, most of these methods still rely on task-specific supervision, such as hand-crafted QA pairs. To reduce the reliance on manual annotations, we draw inspiration from self-supervised visual representation learning [7, 14, 13, 3], where images are used as their own source of supervision. Contrastive learning methods [14, 7], for instance, learn discriminative representations by pulling together features from different views of the same image and pushing them away from those of different images. Guided by this principle, we exploit inherent constraints in images as supervision signal for reward calculation, and present novel method, MiCo (Multiple image Contrast). Specifically, we construct training triplets consisting of two augmentations of the same image and third, different but similar image with its own augmentation. We prompt the VLM to output the thinking process and make comparisons among these images to answer same/different. Multiple trajectories are sampled per example, and reinforcement learning is applied using advantages computed from the correctness of the final answer. key aspect of our approach is the design of challenging image comparisons. If negative samples are too distinct, the reasoning is trivial. We address this by sampling frames from the same video or using image editing datasets to find similar images, ensuring subtle differences that require careful visual inspection and reasoning. Beyond this contrastive framework, we also introduce Augmented GRPO, training strategy that samples trajectories using weak augmentations and optimizes them under stronger augmentations. This design allows high-quality CoTs to generalize to more difficult images. Although the model is trained solely on the image comparison task, the learned ability to link visual cues across multiple images generalizes to wider scope of scenarios. For example, the model can predict plausible future actions by analyzing visual changes across frames, distinguish object identities by comparing fine-grained appearance details, or detect subtle camera movement in scene transformations. Moreover, the contrastive learning process encourages attention to fine-grained details, which also benefits certain single-image understanding tasks like fine-grained layout/attribute understanding. Experimental results show that MiCo achieves strong performance for multi-image understanding [40, 32, 10], and also brings improvements on general vision tasks [11, 6, 39]."
        },
        {
            "title": "2 Related Work",
            "content": "Vision language model reasoning. Recent studies show that reasoning-capable LLMs [16, 12, 31, 27] can be effectively guided to generate long CoT [36] reasoning processes through reinforcement learning, leading to significant progress on tasks involving complex logic. Building on these advances, surge of recent works [5, 29, 26, 25] has extended CoT reasoning into the vision-language domain. For example, MM-Eureka [23] expands training data coverage across domains and refines RL training strategies. NoisyRollout [20] introduces image augmentations to enrich the exploration space for policy optimization. LVAA-Thinking [4] provides detailed analysis of supervised fine-tuning and RL for visual reasoning, along with curated dataset for related tasks. ThinkLite [34] further improves data efficiency via sample selection with Monte Carlo Tree Search. While these methods rely heavily on curated training data generated by existing models or human annotations, our work explores an alternative: leveraging inherent constraints within visual data to naturally elicit reasoning abilitywithout explicit question-answer supervision. Multi-image understanding. Understanding multiple images is crucial in real-world scenarios that require comparing object states, tracking actions, or recognizing objects across views. Recent large VLMs [2, 18, 17, 1, 15] have begun to support multi-image inputs natively. LLaVA-Interleave [19] extends LLaVA [19] to process interleaved multimodal inputs. VISC [41] introduces focus-centric data to enhance visual reasoning. Meanwhile, numerous benchmarks [32, 10, 24, 21, 44, 37] have been proposed to evaluate multi-image understanding from various angles. Despite these developments, recent evaluations [40, 46] highlight persistent limitations: VLMs often fail to link fine-grained visual cues across images, such as identifying the same object under different views or detecting subtle state changes for predictive reasoning. Our work addresses this gap by incentivizing the model to compare the fine details across images and make logical analysis."
        },
        {
            "title": "3.1 Pilot Study for Multi-image Understanding",
            "content": "We begin with pilot study to assess how well current VLMs understand multiple images. As shown in Fig. 1, we present examples that highlight the capabilities of several state-of-the-art VLMs, Qwen2.52 Figure 1: Challenges for multi-image understanding. While recent works support multiple images as input, most of them focus on scenarios where each image can be interpreted independently (e.g., Example 1), which remains relatively easy for current state-of-the-art VLMs. However, many realworld tasks (e.g., Example 2-4) require models to compare subtle visual differences, align visual cues across images, and reason about object correspondencescapabilities that current VLMs still struggle with. We gather Example 1 from MuirBench [32], Example 3 from VLM2-bench [40], Example 2,4 from real world samples. VL [33], InternVL [8], and GPT-4o [15]. While many recent models and benchmarks [24, 32, 44] support multi-image or video inputs, they primarily focus on scenarios like Example 1, where each image can be understood in isolation. In Example 1, models correctly identify single person across images, reflecting solid basic perception. However, when we examine more complex cases in Fig. 1, we observe that VLMs often suffer from severe hallucinations. In Example 2, both models fail to infer the correct camera movement, showing weaknesses in spatial reasoning. In Example 3, VLMs cannot distinguish between different car toys, indicating difficulty with cross-image comparison. Example 4 further reveals failure in tracking semantic changes across images, with hallucinated object positions and misidentified labels. These cases highlight that current VLMs still lack robust visual comparison abilities essential for multi-image understanding. These examples typically require the model to explicitly link visual cues across images, analyze fine-grained differences, and reason about inter-image correspondences. As current VLMs already possess strong abilities in single-image perception (e.g., reading fine-grained text) and demonstrate solid commonsense knowledge, as evidenced by their performance on standard vision benchmarks. We hypothesize that their primary limitation in multi-image understanding lies in their inability to compare and connect visual information across images. To address this gap, we focus on enhancing the meta-cognitive ability of visual comparison, the core skill needed for effective multi-image reasoning."
        },
        {
            "title": "3.2 Multi-image Contrast",
            "content": "Rather than collecting data for each specific multi-image task, we aim to improve VLMs general capacity to analyze and reason over multiple images by targeting the core meta skill: visual comparison. Inspired by the principles of self-supervised learning, we design lightweight and scalable framework that encourages the model to distinguish similar yet distinct images. By simulating contrastive visual situations and prompting the model to generate structured reasoning trajectories, we aim to enhance its ability to perceive fine-grained differences, establish correspondences, and perform step-by-step comparisons across images. Here, we elaborate on the pipeline of MiCo. The overall framework consists of the following main steps. First, we identify and construct contrastive image samples that are visually similar yet different. Then, we apply data augmentation to build informative training triplets. Finally, we leverage Augmented GRPO to evaluate set of reasoning trajectories and optimize the VLM accordingly. 3 Figure 2: Demonstrations for contrastive samples. The first row shows two triplets from the video, and the second row demonstrates samples from image editing datasets. These samples are visually similar but contain subtle differences (marked with red circles), on which we apply random cropping and resizing. In each triplet, the first two images are the same, and the third image is different. Image selection. We begin by selecting image pairs that are visually similar but exhibit subtle differences, which serve as contrastive supervision signals. We denote such pair as (Ia, Ib), where Ia and Ib are distinct images sharing high structural similarity (e.g., similar layout or background), but with small detail variations. We leverage two types of data sources that naturally fulfill this requirement: video frames and image editing datasets. For video data, we randomly sample (Ia, Ib) from the same video with temporal gap of 2 seconds, and compute their Structural Similarity (SSIM) to filter out near-identical pairs. For image editing data, each (Ia, Ib) pair consists of \"before\" and \"after\" edited image. We compute the pixel-wise Mean Squared Error (MSE) to remove significantly different pairs. These constraints ensure that the collected pairs exhibit subtle but meaningful changes. Image augmentation. While the selected image pairs already exhibit subtle variations, directly learning to distinguish them may still lead to shortcut learning. To increase task complexity and encourage detailed reasoning, we apply data augmentation to create diverse image views. As the visualization examples provided in Fig. 2, given source image I, we generate two augmented versions via random cropping and resizing (they do not change the content of images). For each image pair (Ia, Ib), we thus construct contrastive triplet: = {T1(Ia), T2(Ia), T3(Ib)},"
        },
        {
            "title": "3.3 Augmented GRPO",
            "content": "Question-answer formulation. After getting the triplets that contain similar images and their augmentations. We construct QA pairs for reinforcement learning. Given an image triplet, we add reasnoning prompt and user questions as follows:"
        },
        {
            "title": "Reasoning Template of MiCo",
            "content": "Reasoning Prompt: First output the thinking process in <think> </think> and give the final answer in <answer> </answer> tags. User Question: Regardless of the augmentation, are image1 and image2 the same? How about image2 and image3, image1 and image3? Only return T(True) or F(False) in <answer> </answer>, for example <think> </think> <answer>TFT</answer>. To increase the diversity and balance the difficulties of questions, besides the image triplet, we also construct image pairs and design the corresponding prompts for comparing two images. In addition, we use GPT-4o [15] to expand the user question of the same meaning but with various expressions. Rollout augmentation. For each question with augmented images , the original GRPO samples group of outputs {o1, o2, , oG} from the old policy πθold and then optimizes the policy model πθ. To better leverage difficult samples that arise from strong augmentations, we sample trajectories using weakly augmented inputs w, which are easier to produce valid reasoning chains. These sampled trajectories are then used to optimize the policy on stronger augmented contexts s, effectively transferring reliable behavior to harder instances. 4 Algorithm 1 MiCo: Reinforcement Multi-image Reasoning 1: Input: Policy πθ, old policy πθold , image triplet dataset = {(I1, I2, I3)}, training steps Tmax, group size G, clip parameter ϵ, weak augment operators w, strong augment operators 1 , 1 , 2 , 2 , 2: for = 1 to Tmax do 3: 4: 5: 6: 7: 8: 9: 10: 3 ) = w(I1, I2, I3) 3 ) = s(I1, I2, I3) Sample triplet (I1, I2, I3) Apply weak augmentation: (I Apply strong augmentation: (I Construct prompts qw and qs from the weak and strong augmented triplets, respectively Sample CoT responses {oi}G Evaluate reward Ri = R(I w, qw, oi) for each = 1, . . . , Compute group baseline = 1 Optimize πθ on the strong prompt qs using the group rollouts: L(θ) = 1 θ θ θL(θ) θold θ i=1 Ri, and advantages ˆAi = Ri , where ri = πθ(oiqs) πθold (oiqs) ri ˆAi, clip(ri, 1 ϵ, 1 + ϵ) ˆAi i=1 from πθold( qw) i=1 min (cid:80)G (cid:80)G σ(R) 11: (cid:17) (cid:16) Rollouts from weak prompt 12: 13: 14: end for Training objective. The training objective of Augmented GRPO could be formulated as follows. This objective encourages the policy to assign higher likelihoods to responses with higher relative rewards within each group. (θ) = E[q (Q, {oi}G (cid:18) i=1 πθold(Oq)] ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 min (cid:18) πθ(oiq) πθold (oiq) Ai, clip (cid:18) πθ(oiq) πθold (oiq) (cid:19) (cid:19) Ai β DKL (πθπref ) (cid:19) (1) , DKL (πθπref ) = πref (oi q) πθ(oi q) log πref (oi q) πθ(oi q) 1, (2) where ϵ and β are hyperparameters. Following GRPO [28], Ai is the normalized advantage computed based on rewards {r1, r2, . . . , rG}. Ai = ri mean({r1, r2, , rG}) std({r1, r2, , rG}) . (3) Following DeepSeek-R1 [12], we leverage the binary format reward and the accuracy reward, which considers the matching of <think> </think> <answer> </answer> tags, and the correctness of the final answer. For the triplet comparisons, we get 1 for the accuracy reward only if we make correct comparisons for all three pairs. Overall algorithm. Our MiCo could be summarized in Algorithm 1. We first construct an image triplet consisting of two augmented views of the same image and third, visually similar but distinct image (with augmentations). The model is prompted to perform multi-image comparison and generate reasoning trajectories. During training, chain-of-thought responses are sampled from the weakly augmented views, and the policy is optimized on the strongly augmented ones using rule-based reinforcement learning. This process enables the model to learn fine-grained visual reasoning in self-supervised manner."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "Hyper-parameters. For the baseline model, we follow previous works [23, 20, 34, 4] and select Qwen2.5-VL-7B [2]. For the training data, we use OmniEdit [35] for image editing pairs and extract video frames from Vidgen-1M [30]. The part of reinforcement learning follows GRPO [28], we set 5 Table 1: Performance on VLM2-Bench [40], which evaluates the ability to compare and link finegrained visual cues across multiple images. Without relying on any humanor model-annotated data, MiCo achieves significant improvements and sets new state-of-the-art. Reasoning-based models (marked with ) are evaluated using their corresponding prompting strategies. Baselines or Models General Chance-Level Human-Level LLaVA-OneVision[17] LLaVA-Video-7B [43] LongVQA-7B [42] mPLUG-Owl2-7B [38] Qwen2-VL-7B [2] InternVL2.5-8B [8] InternVL2.5-26B [8] Qwen2.5-VL-7B [2] GPT-4o [15] MM-Eureka-7B [23] NoisyRollout-7B [20] ThinkLite-VL-7B [34] VLAA-Thinker-7B [4] Qwen2.5-VL-7B-CoT[2] MiCo-7B-CoT Improvement Mat 25.00 95.06 16.60 18.53 14.29 17.37 18.07 41.24 30.50 35.91 37.45 55.60 40.93 40.45 47.49 43.24 57.14 +13.90 Trk 25.00 98.11 13.70 12.79 12.98 18.26 19.18 26.53 30.59 43.38 39.27 47.03 43.83 46.58 63.03 42.92 67.12 +24.20 Object Cnt 34.88 94.23 56.17 62.47 49.47 62.97 61.84 67.65 51.48 41.72 80.62 52.50 50.83 62.50 61.40 50.56 56.67 +6.11 Grp 25.00 91.29 27.50 28.50 29.00 31.00 37.50 40.00 52.50 47.50 57.50 54.00 34.50 49.50 55.00 36.00 58.00 +22.00 Cpr 50.00 96.02 47.22 54.72 46.53 49.17 68.08 72.22 43.33 71.39 74.17 74.10 63.33 75.56 72.20 66.39 81.94 +15.55 Person Overall* Cpr 50.00 97.08 62.00 62.00 58.00 63.00 72.00 85.00 59.50 80.00 50.00 77.50 70.50 77.50 71.00 62.50 65.00 +2.50 Cnt 34.87 92.87 46.67 66.91 41.56 58.06 67.92 66.67 59.67 59.76 90.50 60.00 63.33 62.50 57.50 55.83 57.50 +1.67 Grp 25.00 91.17 37.00 25.00 25.00 29.00 47.00 52.25 61.25 69.00 47.00 51.00 47.00 51.00 51.00 39.00 62.00 +23.00 VID - 100.00 47.25 59.00 45.00 43.00 55.25 50.25 45.25 45.00 66.75 43.50 36.50 36.50 47.75 36.75 44.25 +7.50 Avg 32.73 95.16 39.35 45.65 37.10 40.87 49.76 55.41 45.59 54.82 60.36 57.24 50.08 55.79 58.49 48.91 61.06 +12. human -61.44 0.00 -55.81 -49.51 -58.06 -54.31 -45.40 -39.75 -49.57 -40.34 -34.80 -37.91 -45.08 -39.37 -36.67 -46.24 -34.09 +12.93 format reward and accuracy reward with the weight of 1:1, respectively. Besides, we also apply KL regularization with weight of 0.01. During training, we follow previous works [23] to skip the rollout group with all correct/false answers. During training, we use learning rate of 1e-6 and set the batch size of 16. For each training sample, we generate group of 8 rollouts. We train the model for 600 iterations on 8A100 GPUs. Evaluation protocols. For evaluation, we follow the default hyper-parameters of Qwen2.5-VL [33] and utilize the VLMEvalKit [9]. For reasoning baselines, we adopt their official prompting formats. Minor inconsistencies in results may occur due to differences in the implementation details of evaluation frameworks or answer parsing logic."
        },
        {
            "title": "4.2 Result Analysis for Multi-image Comparison",
            "content": "Evaluation metrics. We first report the model performance on VLM2-Bench [40]. This benchmark mostly aligns with our intention of linking fine-grained visual cues across images. Specifically, VLM2-Bench includes three tracks: General Cue (GC), Object-centric Cue (OC), and Person-centric Cue (PC). Each track consists of subtasks with specific metrics: Mat (Matching) and Trk (Tracking) use paired T/F accuracy; Cpr (Comparison) evaluates consistency by requiring the model to correctly answer both positive and its corresponding negative statement; Cnt (Counting) uses normalized error to measure numerical prediction accuracy; Grp (Grouping) is multiple-choice task assessing clustering ability; and VID (Video Identity Describing) is scored based on GPT-4o evaluation of open-ended descriptions. Result analysis. As shown in Tab. 1, we present the comparison results on VLM2-Bench. We observe that all existing openand closed-source models lag behind human performance by large margin. Among them, GPT-4o [15] demonstrated clear advantages over other models. Thanks to the strong generalization ability of reinforcement learning, recent reasoning VLMs [23, 20, 34, 4] have shown consistent improvements when built upon Qwen2.5-VL-7B [2]. We report the performance of MiCo in the final block. Trained with contrastive triplets, MiCo effectively learns the core ability to compare images, achieving substantial gains across multiple tasks and obtaining the best average performance overall. Notably, our 7B model even outperforms GPT-4o. However, we find that CoT reasoning does not benefit all sub-tasks equally. Specifically, for tasks involving human faces (person track), CoTbased models offer limited or even negative gains compared to no-CoT counterparts. We hypothesize that human identity representations, such as facial nuances, are difficult to verbalize, thus limiting the benefit of language-based reasoning. In contrast, object-level identity differences (e.g., logos, 6 Table 2: Ablation studies on key configurations. We conduct experiments on VLM2-Bench [40] and report the average accuracy across the general, object, and person tracks. For each ablation, all other settings are kept consistent with our final model to ensure fair comparisons. (a) Learning Paradigm General Object Person Qwen2.5-VL [2] SFT No-CoT RL CoT RL 39.64 42.90 45.36 62.13 53.53 51. 50.01 65.53 63.44 55.98 55.23 57. (b) Data Source General Object Edit Data1 [35] Edit Data2 [45] Video Data Edit1 + Video 61.23 60. 60.29 62.13 65.33 64.33 64.50 65. Person 56.35 55.27 55.68 57.18 (c) Rollout Augmentation (d) Sample Formulation General Object Person General Object Person Qwen2.5-VL [2] (Strong, Strong) (Weak, Weak) (Weak, Strong) 39. 59.41 55.58 62.13 53.53 64.00 62. 65.53 63.44 56.98 54.81 57.18 Qwen2.5-VL [2] Image Pairs Image Triplets Pairs + Triplets 39.64 56.41 60. 62.13 53.53 66.98 65.33 65.53 63. 55.68 55.81 57.18 (e) Prompt Diversity (f) Image Augmentations General Object Person General Object Person Qwen2.5-VL [2] Single Prompt 20 Variations 50 Variations 39.64 55.53 62. 63.13 53.53 64.16 65.53 65.29 63. 51.50 57.18 54.93 Base (Crop, Resize) Base + Flip Base + Rotat. Base + Color. 62.13 61.13 62.58 60.15 65. 63.98 65.03 64.26 57.18 56.77 55. 54.86 textures, shapes) are more readily describable, allowing CoT reasoning to help reduce hallucinations and improve distinction."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "We conduct series of ablation studies to validate the effectiveness of our core designs. As an initial exploration of visual reasoning, we also analyze the impact of some basic configurations. Training strategies. In Tab. 2 (a), we evaluate different training paradigms. We first apply supervised fine-tuning (SFT) on our contrastive dataset, allowing the model to directly predict the final answer. We observe that this leads to minor gains on the general track, which more closely aligns with the training task. However, the ability acquired through SFT does not generalize well to more diverse reasoning tasks. We also test no-CoT reinforcement learning baseline, where the model is trained to output answers directly using GRPO [28]. Due to the absence of intermediate reasoning steps, the resulting trajectories are short and behave similarly to SFT, yielding limited improvements. Data source. In Tab. 2 (b), we compare different training data sources. Both the image editing data (OmniEdit [35]) and video-derived frames (VidGen [30]) individually support effective learning. Combining these two heterogeneous sources further enhances performance. We also validate that our framework is not tied to specific editing styles, as models trained on either OmniEdit or UltraEdit [45] generalize well, demonstrating robustness to the editing domain variation. Rollout augmentation. In this work, we leverage weak augmentations for rollout sampling, and use these high-quality answers to optimize harder questions with stronger augmentations. In Tab. 2 (c), we report different combinations of augmentation in (sampling, optimization) process. We show that, strong augmentations are vital for contrastive learning compared with weak augmentations, and our rollout augmentation strategy gets the best performance. Sample formulation. As discussed in Sec. 3.3, we construct prompts based on either image pairs or image triplets. While we initially suspected that binary image-pair comparisons (with 50% guess probability) might result in low-quality CoTs, our experiments reveal that they still contribute positively to performance. In practice, we find that combining both formatspair-based and triplet-based leads to the best results. Other configurations. In Tab. 2 (e), (f), we explore the effects of prompt and augmentation diversity. We observe that increasing the variation of image prompts helps prevent overfitting, with performance 7 Table 3: Quantitative results on general vision benchmarks. We report performance for wide scenarios. Multi-image benchmarks are marked in bold. MiCo brings steady improvements compared with our baseline, and gets competitive results against other visual reasoning models. MuirBench [32] BLINK [10] Hallusion [11] MMStar [6] MMMU [39] MathVistas [22] MM-Eureka-7B [23] NoisyRollout-7B [20] VLAA-Thinker-7B [4] ThinkLite-VL-7B [34] Qwen2.5VL-7B [2] MiCo-7B Improvement 60.57 59.61 61.00 57.62 58.43 60.53 +2.10 54.39 56.07 54.81 55.81 55.54 57.23 +1. 68.45 66.66 69.08 72.97 69.50 69.61 +0.11 65.73 65.66 63.60 66.80 64.06 65.60 +1.54 54.11 54.55 54.44 53.55 54.11 54.77 +0.66 72.00 71.60 70.80 71.89 67.10 67.90 +0.80 Table 4: Task analysis for visual reasoning. We list representative sub-tasks from MuirBench [32] and BLINK [10] to analyze the generalization ability and limitations for MiCo. Visual retrieval Semantic Corr. Spatial Rela. Scene Under. Forensic Det. Relative Depth Qwen2.5VL-7B [4] MM-Eureka-7B [23] VLAA-Thinker-7B [4] MiCo-7B 63.69 57.19 + 68.83 + 71.23 ++ 33.09 33.09 + 34.53 + 41.72 ++ 88.81 82.51 - 86.71 - 90.20 + 61.82 67.74 ++ 69.89 ++ 63.97 + 48.48 50.00 + 47.72 - 47.72 - 81.45 75.80 - 76.61 - 78.22 - saturating at around 50 distinct prompt templates. For image augmentations, we experimented with various techniques and ultimately selected random cropping and resizing as the default setting based on empirical performance."
        },
        {
            "title": "4.4 Analysis on General Vision Tasks",
            "content": "In this section, we evaluate the generalization ability and capacity boundaries of MiCo on broader range of vision tasks. We first report quantitative results on additional benchmarks and analyze performance across more diverse task types. Results on additional benchmarks. As shown in Tab. 4, we evaluate MiCo on MuirBench [32] and BLINK [10], both of which are representative multi-image understanding benchmarks. To further assess generalization, we also include several single-image benchmarks, including MMStar [6], MMMU [39], HallusionBench [11], and MathVista [22]. Compared to methods trained with manually curated supervision, our contrastive learning framework exhibits strong performance on multi-image understanding tasks, where relational reasoning across images is crucial. While MiCo also improves over standard baselines on single-image tasks, its performance remains behind models trained with task-specific guidance, particularly in complex scenarios like visual mathematics, where symbolic reasoning and structured representation are essential but not explicitly modeled in our current training paradigm. Task-wise analysis. We further analyze the performance of MiCo on specific sub-tasks from MuirBench [32] and BLINK [10] to better understand its strengths and limitations. Our contrastive learning framework demonstrates clear advantages on correspondence-style tasks, such as Visual Retrieval and Semantic Correspondence, where MiCo outperforms other reasoning models. These results highlight the models strength in aligning multimodal signals through relational comparisons. In addition, Spatial Relation taskswhich evaluate the models understanding of image layout and object positioningalso benefit from contrastive training. By encouraging attention to relative positions among visual entities, MiCo achieves the highest accuracy in this category. On the other hand, MiCo lags behind models trained with manually curated reasoning datasets on tasks such as Scene Understanding and Forensic Detection, which typically rely on single-image question answering. These tasks often demand domain-specific priors or curated logic patterns that are less emphasized in our data construction process. We also observe that Relative Depth, representative spatial reasoning task, remains challenging for all models and exhibits noticeable performance drop. This suggests that depth-aware understanding is not sufficiently captured by current training signals. Incorporating explicit spatial or geometric cues remains promising direction for future work toward building spatially grounded vision-language models. Figure 3: Demonstrations for visual reasoning. Given question, MiCo first examines the details of each image to identify answer-related visual cues, and then performs cross-image comparisons to derive the final answer. The reasoning processes are marked in gray, with key contents underlined."
        },
        {
            "title": "5 Conclusion\nIn this work, we propose a self-supervised framework that leverages inherent image constraints to\nincentivize multi-image reasoning in VLMs. We identify that the core challenge lies in linking visual\ncues across images. To address this, we adopt contrastive learning principles and construct image\ntriplets for reinforcement training. To further enhance reasoning, we introduce Augmented GRPO,\nwhich samples rollouts from simpler examples and optimizes the model on harder ones. Although\ntrained solely on image comparison tasks, our model generalizes well and achieves strong results\nacross multiple benchmarks.",
            "content": "Limitations and future directions. While our approach supports general reasoning through visual comparisons, it shows limited effectiveness on specialized tasks such as face verification, visual math, and spatial understanding, where structured priors or domain-specific knowledge are required. In future work, we plan to explore more efficient data construction strategies tailored to these domains."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774, 2023. 1, 2 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv:2502.13923, 2025. 1, 2, 5, 6, 7, 8 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 2 [4] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv:2504.11468, 2025. 2, 5, 6, [5] Liang Chen, Lei Li, Haozhe Zhao, and Yifan Song. Vinci. r1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. 1, 2 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? NeurIPS, 2024. 2, 8, 14 [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. 2 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 1, 3, 6 [9] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACMMM, 2024. [10] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV, 2024. 2, 8, 14 [11] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In CVPR, 2024. 2, 8 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948, 2025. 1, 2, 5 [13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 2 [14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 2 [15] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv:2410.21276, 2024. 1, 2, 3, 4, 6 [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv:2412.16720, 2024. 1, 2 [17] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. TMLR, 2025. 2, 6 [18] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llavanext-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv:2407.07895, 2024. [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 2 10 [20] Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv:2504.13055, 2025. 2, 5, 6, 8, 13 [21] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, et al. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. In NeurIPS, 2025. 2 [22] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. [23] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv:2503.07365, 2025. 1, 2, 5, 6, 8 [24] Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large visionlanguage models. In ICLR, 2025. 2, 3 [25] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, et al. Skywork r1v: Pioneering multimodal reasoning with chain-of-thought. arXiv:2504.05599, 2025. 2 [26] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv:2503.07536, 2025. 2 [27] ByteDance Seed, Yufeng Yuan, Yu Yue, Mingxuan Wang, Xiaochen Zuo, Jiaze Chen, Lin Yan, Wenyuan Xu, Chi Zhang, Xin Liu, et al. Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning. arXiv:2504.13914, 2025. 1, 2 [28] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv:2402.03300, 2024. 1, 5, 7, [29] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv:2503.20752, 2025. 2 [30] Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, and Hao Li. Vidgen-1m: large-scale dataset for text-to-video generation. arXiv:2408.02629, 2024. 5, 7 [31] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv:2501.12599, 2025. 1, 2 [32] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. In ICLR, 2025. 2, 3, 8, [33] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv:2409.12191, 2024. 3, 6 [34] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv:2504.07934, 2025. 1, 2, 5, 6, 8 [35] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In ICLR, 2024. 5, 7 [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. 2 [37] Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, et al. Towards open-ended visual quality comparison. In ECCV, 2024. [38] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In CVPR, 2024. 6 11 [39] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 2, 8 [40] Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, and Yi Fung. Vlm2-bench: closer look at how well vlms implicitly link explicit matching visual cues. arXiv:2502.12084, 2025. 2, 3, 6, 7 [41] Juntian Zhang, Yuhan Liu, Wei Liu, Jian Luan, Rui Yan, et al. Weaving context across images: Improving vision-language models through focus-centric visual chains. arXiv:2504.20199, 2025. 2 [42] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv:2406.16852, 2024. 6 [43] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv:2410.02713, 2024. 6 [44] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv:2406.12742, 2024. 2, 3 [45] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. In NeurIPS, 2024. [46] Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, and Lu Qi. Are they the same? exploring visual correspondence shortcomings of multimodal llms. arXiv:2501.04670, 2025."
        },
        {
            "title": "A More Implementation Details",
            "content": "Prompt expansion. In the main paper, we give an example of the user question as follows:"
        },
        {
            "title": "Reasoning Template of MiCo",
            "content": "Reasoning Prompt: First output the thinking process in <think> </think> and give the final answer in <answer> </answer> tags. User Question: Regardless of the augmentation, are image1 and image2 the same? How about image2 and image3, image1 and image3? Only return T(True) or F(False) in <answer> </answer>, for example <think> </think> <answer>TFT</answer>. As we verified in Tab.2 (c) that prompt diversity plays critical role in encouraging the model to generalize its reasoning ability. To this end, we design systematic prompt expansion strategy along two axes: question phrasing and comparison structure. First, we construct both forward questions (e.g., Are image1 and image2 the same?) and reverse questions (e.g., Are image1 and image2 different?), allowing the model to reason under varied semantic instructions. Second, we vary the combinations of image pairs being queried (e.g., image1 vs. image2, image2 vs. image3, image1 vs. image3), ensuring comprehensive coverage of possible relations. Through controlled sampling, we balance the resulting answer types (e.g., TFT, TTF, FFF, etc.), avoiding label bias and promoting robust multi-image understanding across diverse visual scenarios and logical outcomes. Data filtering. We apply data filtering When selecting similar but different image pairs from the image editing dataset and video frames. For image edting data, to ensure high-quality training samples for visual reasoning, we implement pixel-wise comparison strategy to filter out image pairs with overly large differences. Specifically, for each image pair, we compute the absolute difference across corresponding pixels. For RGB images, we first average the per-channel differences to obtain single grayscale difference map. pixel is considered different if its value exceeds predefined threshold (30). We then calculate the ratio of differing pixels across the entire image. If this difference ratio exceeds 0.8, the image pair is discarded. This simple yet effective rule ensures that the model focuses on learning from subtle and semantically meaningful variations, rather than from trivially dissimilar or noisy pairs. For video data, we calculate the SSIM between the selected two frames, and remove the samples with an SSIM value greater than 0.95 to filter nearly the same images. Differences with NoisyRollout [20]. Recently, NoisyRollout [20] also leverages image augmentation to enhance GRPO [28]. However, our Augmented GRPO is fundamentally different from their strategy. NoisyRollout introduces hybrid rollout strategy by mixing trajectories from both clean and moderately distorted images. Specifically, they add Gaussian noise on the images. Then, sample trajectories on noisy images and another trajectories on clean images. In this way, they get 2n trajectories in total with more diversity. Afterwards, NoisyRollout calculates the advantages based on the hybrid trajectories and optimizes the policy model on clean images. Differently, our Augmented GRPO is inspired by semi-supervised learning in computer vision, where we sample trajectories with weak image augmentation. We assume that it would be easier for the policy model to get more high-quality CoTs with weak image augmentation. Then, we calculate the advantages using these high-quality CoTs and optimize the model with stronger augmented images. This allows us to obtain correct and informative Chain-of-Thoughts (CoTs) even for samples that would otherwise be answered incorrectly under strong augmentation, thereby improving the models generalization to more challenging examples. 13 Table 5: Task analysis on MMStar. We report performance on different sub-tasks to evaluate the generalization ability of MiCo compared to the Qwen2.5VL baseline. Overall Coarse Perc. Fine Perc. Qwen2.5VL-7B 64.07 65.33 + MiCo-7B 72.00 73.20 + 60.40 59.60 - Inst. Reason. Logic Reason. Math 65.60 69.20 + 69.60 72.00 + 67.20 68.80 + Sci. & Tech. 49.60 49.20 - Table 6: Task analysis on MuirBench. Performance breakdown across 12 sub-tasks to evaluate the fine-grained generalization ability of MiCo. Qwen2.5VL-7B MiCo-7B Qwen2.5VL-7B MiCo-7B Action 40.85 40.85 Geo. Und. 49.00 53.00 + Diagram Diff. Spot. Attr. Sim. Cartoon 46.15 46. Counting 54.41 34.19 58.67 55.29 + 57.65 - 34.19 Img-Text Ordering Scene Und. Vis. Grnd. Vis. Ret. 61.83 63.98 + 14.06 20.31 + 33.33 35.71 + 77.89 79.90 + 72.63 74.14 + 63.70 71.23 + Table 7: Task analysis on BLINK. Performance comparison across 14 sub-tasks to evaluate the generalization ability of MiCo. Qwen2.5VL-7B MiCo-7B Qwen2.5VL-7B MiCo-7B 70.83 70.00 -"
        },
        {
            "title": "ArtStyle Counting",
            "content": "Forensic 48.48 47.27 - Jigsaw MultiView 59.33 69.23 72.65 + 69.33 + ObjLoc RelDepth RelReflect. SemCorr SpatialRel VisCorr 52.33 54.10 61.05 + 54.92 + 54.89 42.11 - VisSim 86.67 85.19 - FuncCorr 27.69 30.77 + IQTest 18.00 26.00 + 33.09 41.73 + 88.81 90.21 + 81.45 76.61 - 40.30 31.34 -"
        },
        {
            "title": "B More Experimental Results",
            "content": "B.1 More quantitative results To further evaluate the generalization ability of MiCo, we report its performance on diverse set of sub-tasks from three comprehensive benchmarks: MMStar [6], MuirBench [32], and BLINK [10]. As shown in Tables 5, 6, and 7, MiCo consistently improves upon Qwen2.5VL-7B across most reasoning-related tasks. On MMStar, MiCo achieves notable gains in instance reasoning, logical reasoning, and math, suggesting enhanced multi-step inference and abstraction capabilities. The performance on finegrained perception slightly decreases, indicating potential room for improvement in precise visual attribute understanding. For MuirBench, MiCo improves on 8 out of 12 sub-tasks, including Diagram Understanding, Geographic Understanding, and Visual Retrieval. These tasks involve complex spatial, contextual, or comparative reasoning, showing the effectiveness of our visual comparison objective. Tasks like Attribute Similarity and Counting show marginal drops, possibly due to their reliance on absolute visual matching rather than relational reasoning. On BLINK, MiCo shows strong improvements on Functional Correspondence, IQ Test, Jigsaw, Semantic Correspondence, and Visual Correspondenceall of which require visual logic, spatial matching, or multi-view inference. However, tasks such as Multi-view Reasoning and Relative Reflectance exhibit declines, suggesting future efforts could focus on making the model more robust to challenging viewpoint shifts and subtle appearance variations. Overall, these quantitative results demonstrate that MiCo is particularly effective at improving tasks involving reasoning, structure, and comparison, while fine-grained low-level perception remains direction for future enhancement. B.2 Unsuccessful Attempts Throughout our exploration, we experimented with several alternative approaches that ultimately did not lead to improved performance. For completeness and to facilitate future research, we summarize these unsuccessful attempts and provide insights into why they may have failed. 14 Confidence reweighting. Since our task is formulated as answering T/F questions, even when evaluating three comparisons simultaneously, there remains non-trivial chance (12.5%) of obtaining the correct answer purely by guessing. To reduce the impact of such randomness, we explored adding an additional reward or weight based on the models answer confidence. Specifically, we experimented with several approaches to compute confidence scores from the softmax probabilities of the output tokens. However, these confidence-based reweighting strategies did not yield any performance improvements. We analyze that this may be due to the fact that the softmax probability of the predicted token does not reliably reflect the models true certainty about the overall answer. In particular, the model may assign high confidence to tokens that are syntactically or semantically unrelated to the actual correctness of the reasoning (e.g., punctuation, or irrelevant words within the output). As result, the computed confidence can be misleading, making it an ineffective signal for reward shaping. Importance sampling. As in our Augmented GRPO, we sample the trajectories on simple examples with weak augmentations, but we use the trajectory to optimize harder exaples with strong augmentations. This might cause misalignment similar to offline reinforcement learning. In this way, we apply importance sampling, which calculates the probability gap between the trajectories for the simple and hard examples as weight to reweight the reward/advantages. This strategy could not bring improvements. We suspect that although importance sampling is theoretically justified, it may interfere with the core optimization dynamics of GRPO. Specifically, GRPO relies on the relative ranking of trajectories within group to compute structured advantages. Introducing importance weightsderived from distribution shiftsmay distort this internal ranking or inject instability into the reward signals. Additionally, the token-level probability changes caused by visual augmentations can be noisy or poorly calibrated, making the computed importance weights unreliable in practice."
        },
        {
            "title": "C Qualitative Analysis",
            "content": "We add more visual demonstrations for the reasoning ability of MiCo in Fig. 4 and Fig. 5. These qualitative examples demonstrate the strong reasoning capability of MiCo across various visual tasks. In Figure 4, the model exhibits detailed step-by-step analysis to distinguish visual differences, count distinct objects, and solve jigsaw-like problems. Rather than relying on superficial features, MiCo actively grounds its reasoning in object identity, pose, structure, and scene semantics. For example, in the toy comparison case, it not only detects the number of different objects but also considers subtle variations in assembly, model type, and color configuration. In the jigsaw task, it correctly identifies missing or manipulated segments by referencing spatial consistency and scene-level context. Figure 5 further highlights MiCo ability to tackle abstract reasoning challenges. In the IQ-style pattern recognition question, the model deduces complex symbol progression rule based on character groupings and positions. For functional correspondence and spatial matching, it accurately aligns image pairs by understanding object affordances and relative part placement. Additionally, in the visual similarity task, it discerns fine-grained geometric and design attributes to match images at structural level rather than based on superficial texture or color. Together, these examples reveal that MiCo does not merely perform image-text matching but is capable of systematic, multi-step reasoning grounded in visual understanding. This reflects its generalization ability across both low-level visual tasks and high-level abstract reasoning challenges."
        },
        {
            "title": "D Potential Social Impact",
            "content": "MiCo explores self-supervised and reinforcement learning-based approach to improve multi-image reasoning in vision-language models without relying on human-annotated question-answer pairs. By leveraging intrinsic visual constraints, such as consistency across augmented views and differences between similar images, MiCo significantly reduces the need for labor-intensive data curation. This has the potential to democratize the development of reasoning-capable AI systems, making them more accessible in low-resource settings or for underrepresented languages and domains where curated datasets are scarce. However, as with any powerful vision-language technology, there is risk of misuse, particularly in applications involving surveillance, misinformation, or unauthorized inference of user intent from visual data. MiCos improved ability to perform fine-grained comparisons across images could be 15 Figure 4: Demonstrations for detailed comparison and jigsaw solving. exploited in privacy-invading scenarios if deployed irresponsibly. To mitigate such risks, we advocate for deploying MiCo in alignment with responsible AI guidelines, ensuring transparency, consent, and clear boundaries in its application domains. In practice, this includes integrating robust sensitive content filtering, restricting deployment in high-stakes or privacy-sensitive scenarios, and establishing human-in-the-loop mechanisms for critical decision-making processes. Figure 5: Demonstrations for IQ test, functional correspondence, and visual similarity."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKU",
        "HUST",
        "Tongyi Lab, Alibaba Group"
    ]
}