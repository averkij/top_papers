{
    "paper_title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search",
    "authors": [
        "Xiaofeng Shi",
        "Yuduo Li",
        "Qian Kou",
        "Longbin Yu",
        "Jinxin Xie",
        "Hua Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, a challenging benchmark with expert-annotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 5 4 2 5 1 . 7 0 5 2 : r SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search Xiaofeng Shi1* Yuduo Li1,2* Qian Kou1* Longbin Yu1 Jinxin Xie1 Hua Zhou1 1Beijing Academy of Artificial Intelligence (BAAI) 2Beijing Jiaotong University (BJTU)"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, challenging benchmark with expertannotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR"
        },
        {
            "title": "Introduction",
            "content": "Effective academic paper retrieval is fundamental to research. As scientific literature continues to grow exponentially, researchers are increasingly challenged by the need to locate not just superficially relevant papers, but comprehensive and interconnected works that span multiple subtopics, time periods, and academic communities (Gusenbauer and Haddaway, 2020). While traditional academic search engines such as Google Scholar (Vine, 2006) support basic keyword queries well, they often fall short in supporting complex, multi-intent queries that require deeper contextual understanding or reference-based exploration. Consider the query: Show some cutting-edge technological advancements on how to improve the *Equal contribution. Corresponding author. Email: xfshi@baai.ac.cn Work done during internship at BAAI. Project leader. 1 generalization ability of machine learning models across multiple domains. This query implicitly demands up-to-date results, an understanding of generalization in machine learning context, and coverage across multiple subfields. Existing systems tend to either return overly generic results or fail to capture the full semantic scope of such queries, leading to time-consuming manual filtering by the user. Recent advances in large language models (LLMs) (Achiam et al., 2023; Team et al., 2023; Liu et al., 2024; Yang et al., 2025b) have enabled promising developments in information retrieval, including query rewriting, document retrieval, and ranking (Zhu et al., 2023). In the academic domain, these capabilities offer potential to support more intelligent, context-aware search experiences. However, academic research involves more than retrieving documents matching user query: researchers often explore citation networks, follow references recursively, and synthesize insights across multiple papers. These behaviors, central to scholarly discovery, remain underexplored in current LLMbased retrieval systems. To address this gap, we focus on modeling academic search as recursive, citation-driven process we term the Reference Chain (RefChain). As illustrated in Figure 1, RefChain simulates how researchers follow references from one paper to another, expanding the scope of retrieval beyond direct query matches. PaSa (He et al., 2025) represents key step in this direction, leveraging reinforcement learning (RL) to train an LLM-based agent to control RefChain expansion. However, PaSa is limited by its heavy reliance on training resources, its single-source retrieval design, and its coarse query understanding, which restrict its generalization across domains. We propose SPAR (Scholar PAper Retrieval), modular and extensible framework for academic retrieval built upon multi-agent architecture. SPAR enhances RefChain-based exploration with five specialized components: (1) Query Understanding Agent that interprets domain-specific intent and refines queries accordingly; (2) Retrieval Agent that interfaces with multiple academic data sources; (3) Query Evolver Agent that performs iterative, citation-aware query reformulation; (4) Judgement Agent that evaluates and filters relevant papers; and (5) Reranker Agent that reorders retrieved results based on authority, recency, and publication quality to improve ranking effectiveness. Together, these agents support comprehensive and dynamic academic search workflow that mirrors how human researchers conduct in-depth literature exploration (Figure 2). To systematically evaluate academic retrieval systems under realistic conditions, we also introduce SPARBench, new benchmark comprising diverse, expert-annotated queries spanning computer science and biomedicine. Unlike existing datasets with narrow scopes, SPARBench captures the multi-faceted nature of real-world academic search. Each query and its associated relevant documents were carefully reviewed and annotated by domain experts with strong academic backgrounds, ensuring high-quality and reliable ground-truth relevance labels. This rigorous construction process makes SPARBench robust testbed for developing and evaluating retrieval methods intended for practical academic use. Empirical results on both AutoScholar (He et al., 2025) and SPARBench demonstrate that SPAR significantly outperforms all compared methods. On AutoScholar, SPAR achieves an F1 score of 0.3843, surpassing the previous best method, PaSa (0.2449), by 56.92%. Notably, SPAR maintains strong balance between recall (0.4105) and precision (0.3612), while other methods often favor one at the expense of the other. On SPARBench, SPAR is the only method that consistently achieves meaningful scores across all metrics, with an F1 of 0.3015, recall of 0.3103, and precision of 0.2932, outperforming all baselines by clear margin. These results highlight SPARs robustness and generalization ability across both synthetic and real-world academic search scenarios. These findings underscore the importance of structured, agent-based retrieval frameworks for addressing the complexities of modern academic search. Our primary contributions are summarized as follows: Figure 1: The architecture of RefChain. We propose SPAR, training-free, modular, and extensible academic retrieval framework that leverages multi-agent architecture to perform fine-grained query understanding, multisource retrieval, RefChain-based exploration, and relevance-aware reranking. We introduce SPARBench, high-quality, multi-domain academic retrieval benchmark featuring realistic queries and expertannotated relevance labels across computer science and biomedicine. SPARBench enables rigorous and reproducible evaluation under practical academic search conditions. We conduct extensive experiments on both AutoScholar and SPARBench, demonstrating that SPAR consistently outperforms range of strong baselines, including manual search engines (e.g., Google Scholar, Semantic Scholar), LLM-assisted retrieval pipelines, and prior agent-based methods such as PaSa and PaperFinder."
        },
        {
            "title": "2 Related Work",
            "content": "Traditional Academic Search Engines Conventional academic search systems such as Google Scholar (Vine, 2006), Semantic Scholar(Kinney et al., 2023), OpenAlex (Priem et al., 2022), and PubMed (Canese and Weis, 2013) provide effective keyword-based retrieval for well-formed queries. However, these systems rely primarily on lexical matching and are limited in their ability to handle complex, multi-intent queries (Gusenbauer and Haddaway, 2020). They also lack support for citation-aware exploration or semantic reasoning, 2 Figure 2: The overview of SPAR. which are often essential for comprehensive literature review tasks. LLM-Enhanced Retrieval Recent advances in large language models have led to increasing interest in using LLMs to improve academic retrieval performance (Zhu et al., 2023; Ma et al., 2023). Techniques such as query rewriting, semantic expansion, and LLM-based document reranking have shown promise in improving precision and recall. However, most existing approaches operate in single-turn setting and do not support iterative, reference-driven exploration. Moreover, they rarely integrate domain-aware query understanding or multi-source retrieval strategies. Agent-Based Academic Search Existing agentbased frameworks such as PaSa (He et al., 2025) make notable progress in automated scholarly search but remain limited by their reliance on supervised training and low modularity. To address these issues, we introduce SPAR, training-free, modular agent framework designed for fine-grained query understanding and multi-source document exploration."
        },
        {
            "title": "3 Methodology",
            "content": "We introduce SPAR (Scholar PAper Retrieval), an agent-based framework for academic literature search. Given user query, SPAR first analyzes the input to identify search intent and perform query refinement ( 3.1). It then conducts iterative retrieval via multi-source search, reference chain expansion, and query evolution ( 3.2). Finally, it re-ranks the retrieved documents based on timeliness and authority ( 3.3). An overview of the framework is shown in Figure 2, with each component detailed in the following subsections."
        },
        {
            "title": "3.1 Query Interpretation and Refinement",
            "content": "The initial query presented by user often represents an incomplete articulation of complex, underlying information need, reflecting what (Belkin, 1980) termed an \"Anomalous state of knowledge.\" Users, shaped by their unique perspectives, prior knowledge, or specific roles, naturally approach the same topic with varying informational goals and lines of inquiry (Teevan et al., 2005). Therefore, effective information retrieval requires proactive strategy to discern latent user intent and to refine the initial query into more precise and targeted instructions (Carpineto and Romano, 2012; Croft et al., 2010). Recent studies have further emphasized the importance of query refinement in uncovering user intent, and the advent of LLMs has enabled more nuanced and context-aware query refinement techniques (Anand et al., 2023; Ma et al., 2023; Ye et al., 2023; Liu and Mozafari, 2024). SPAR incorporates Query Understanding agent to interpret the users search query and perform query refinement for subsequent precise academic paper retrieval. Given an academic search query q, the agent first performs intent classification, distinguishing whether the user seeks survey, recent advances, or methodological comparisons. It simultaneously conducts domain identification to anchor the query in specific field 3 of study (e.g., machine learning) and detects any temporal constraints expressed in the query (e.g., \"since 2020\"). These annotations help the system tailor downstream retrieval operations to the users true research goal. Next, the agent selects one or more appropriate academic sources from fixed set: ={Google, ArXiv, OpenAlex, Semantic Scholar, PubMed}. The selection is conditioned on both the query domain and intent, ensuring source-query alignment. The agent then determines whether the query requires multi-query refinement based on its specificity, domain clarity, and linguistic precision. If the query is broad, lacks technical terms, or includes ambiguous phrasing, the agent applies semantic disambiguation, correction, and intentaware expansion. Refinement is guided by the querys recognized intent and detailed refinement prompt is provided in Appendix A.1: For survey-focused queries, the agent generates refined queries targeting different perspectives, including methods, applications, historical developments, and future challenges. For complex or specialized domains, the agent generates refined queries using domainspecific terms and technical specifications while also targeting empirical studies and primary research. When temporal constraints are present, all refinements incorporate the specified date bounds to ensure time-sensitive relevance. Query Understanding Agent emphasizes coverage of diverse subfields and research methodologies from the initial query. The result of this stage is structured list of semantically enriched and disambiguated queries = {q1, q2, , qN }, each of which will be utilized to retrieve papers. This proactive query refinement lays the foundation for precise and context-aware academic paper search in SPAR."
        },
        {
            "title": "Query Evolution",
            "content": "After the Query Understanding Agent refines the query and identifies relevant sources, SPAR enters an iterative retrieval phase, coordinated by the Retrieval Agent, the Judgement Agent, and the Query Evolver Agent. The Retrieval Agent initiates this process by fetching academic papers using sourcespecific strategies and de-duplicating results. It also expands coverage through RefChain exploration, uncovering related work beyond the initial query matches. The Retrieval Agent executes source-adaptive querying for each qi Q. For sources such as Semantic Scholar or OpenAlex, it extracts keywords from each query; for Google, it submits the full query string. It then consolidates results across sources by merging retrieved papers, each annotated with metadata such as title, abstract, authorship, publication date, and source. The Judgement Agent evaluates the relevance of each retrieved paper by comparing it to the initial query and accompanying metadata. Papers scoring above the relevance threshold are added to the Related Pool = {r1, r2, , rm}. Prompts for judging relevance are provided in Appendix A.3. Subsequently, SPAR enhances knowledge expansion through RefChain. For each paper ri R, the Retrieval Agent extracts its list of references either by parsing PDFs or utilizing structured metadata from sources. Then these referred papers are scored using the same Judgement Agent. High-relevance papers are merged with the Related Pool. The most relevant papers from the expanded pool are selected as final results for the current query list and stored in Paper Cache = {p1, p2, , pK}. key design decision is to limit expansion to single RefChain layer. That is, while exploring the references of papers in the Related Pool, it does not recursively expand those references citations. This constraint is grounded in two considerations: Precision and relevance: Deeper RefChain often leads to tangential topics, reducing precision; Computational efficiency: Each layer significantly increases the retrieval and evaluation cost. Compared to PaSa, which uses RL training to determine expansion depth, SPARs deterministic, fixeddepth strategy ensures reliability, while iterative query evolution compensates by exploring new directions in controlled manner. To ensure depth and diversity in search results, the Query Evolver Agent then generates three new queries for pi P, focusing on its methodological insights, applications, and limitations. These queries are conditioned on the retrieval history trajectory, including the initial query, previous search 4 queries, and the metadata of the corresponding paper. random subset of the resulting queries is selected and added to the query list for further retrieval iterations. Prompt for evolving query is provided in Appendix A.2. This retrieval-expansion loop continues until the Paper Cache reaches predefined size or maximum depth. To avoid redundancy, SPAR filters out previously used queries and suppresses keyword overlaps across iterations, ensuring efficient and progressive exploration of the literature space. 3.3 Reranker After retrieving and scoring candidate papers, Re-ranking Module refines the final paper list. The reranking stage subsequently refines this candidate list by reordering the documents so that the most appropriate and informative items appear at the top. Besides the original relevance score, our reranker integrates two additional signals: Publication authority, estimated from metadata such as venue prestige and author reputation; Temporal relevance, determined by whether document satisfies explicit time constraints in the query or belongs to the most recent publications. The prompt template that combines these factors is provided in Appendix A.4. The final output is an ordered list of highly relevant, timely, and authoritative academic papers tailored to the users intent."
        },
        {
            "title": "4 SPARBench",
            "content": "Despite growing interest in scholarly information retrieval, the field still lacks robust and standardized benchmarks for systematic and realistic evaluation. This absence limits reproducibility and hinders progress in developing generalizable academic search systems. Existing resources remain limited in both scope and quality. For example, AutoScholar (He et al., 2025) is synthetic dataset constructed from AI conference papers between 2023 and 2024. Although it pairs GPT-4o-generated queries with relevant documents, only 100 query-document pairs were manually reviewed, raising concerns about label quality and applicability to real-world scenarios. Another benchmark, RealScholarQuery (He et al., 2025), contains 50 expert-written queries collected post-hoc from AI researchers, introducing potential evaluation bias toward models tuned for that specific setup. Most prior benchmarks focus on closed corpora (Ajith et al., 2024; Voorhees et al., 2021; Cohan et al., 2004), using static queries and documents. Such settings fail to capture key aspects of academic search, including query understanding, multi-source retrieval, and reference-based exploration. Despite efforts toward more comprehensive evaluation, no existing benchmark supports end-toend assessment encompassing ranking, reasoning, source selection, and iterative exploration. To address these limitations, we introduce SPARBench, benchmark for evaluating academic retrieval systems under realistic conditions. Unlike previous efforts, SPARBench draws from multiple academic sourcesincluding arXiv, PubMed, OpenAlex, and Semantic Scholarcovering diverse disciplines such as computer science and biomedicine. SPARBench reflects natural academic search behavior. Initial queries are generated by GPT-4o and then rigorously filtered by domain experts. The dataset includes multi-intent queries with incomplete grammar and minor spelling errors to simulate real-world user input. Relevance judgments follow multi-stage process combining automatic filtering, small and large language models, and manual validation by experts, ensuring high label quality and domain fidelity. Given the high cost of producing high-quality academic retrieval data, the current version includes 50 carefully curated queries, each undergoing expert review. This initial release prioritizes depth and reliability, providing solid foundation for future extensions to broader domains and larger query sets. SPARBench will be publicly released to support further research in academic search."
        },
        {
            "title": "4.1 Benchmark Characteristics",
            "content": "Realistic Queries: Simulate authentic academic search behavior through multi-intent, semantically rich queries with incomplete grammar and minor spelling errors. Cross-Domain Coverage: Supports evaluation across computer science and biomedicine, enabling assessment of domain-general and domain-specific retrieval capabilities. Multi-Source Corpus: Integrates documents 5 Figure 3: SPARBench construction pipeline. The process includes expert-curated seed queries, GPT-4o-based query expansion, multi-source document retrieval, and three-stage relevance filtering procedure combining language models and expert annotation. from arXiv, PubMed, OpenAlex, and Semantic Scholar to reduce source-specific bias and improve retrieval realism. The final benchmark comprises 50 queries and 560 expert-verified relevant documents. Stage-wise statistics are reported in Appendix (Figure 5). High-Quality Annotations: multi-stage labeling pipeline combines LLM-based filtering with expert validation, ensuring high-quality annotations and domain consistency."
        },
        {
            "title": "4.2 Construction Method",
            "content": "Figure 3 outlines the multi-stage pipeline used to construct SPARBench. set of seed queries was manually curated based on real academic research scenarios. These were expanded using GPT4o (Hurst et al., 2024) to introduce linguistic and semantic diversity. After expert screening, 50 queries were selected35 from computer science and 15 from biomedicine. Each query was submitted independently to arXiv, PubMed, OpenAlex, and Semantic Scholar, producing an initial candidate set of 198K documents. Relevance assessment proceeded in three stages: 1. Initial Pruning: Coarse relevance was estimated using Qwen2.5-7B-Instruct (Yang et al., 2024), reducing the set to 3K candidates. 2. Refinement: Qwen2.5-72B-Instruct (Yang et al., 2024) performed fine-grained filtering, yielding 2K documents. 3. Expert Validation: Graduate-level computer science annotators manually reviewed the remaining candidates, selecting approximately 560 relevant documents (averaging 12 per query). SPARBench fills critical gap in academic retrieval research by offering realistic, high-quality benchmark tailored for end-to-end evaluation of scholarly search systems."
        },
        {
            "title": "5.1 Evaluation Setup",
            "content": "We evaluated our method against diverse set of baselines, including traditional academic and web search engines, as well as LLM-enhanced retrieval systems. The evaluated baselines include: GOOGLE (G): Standard Google search using the original query. GOOGLE+GPT-4O (G+GPT):Query rewritten for clarity using GPT-4o (Hurst et al., 2024) before Google search. GOOGLE SCHOLAR (GS):Direct retrieval from Google Scholar without LLM intervention. CHATGPT SEARCH (CS):We Submit query to ChatGPT, which is powered by searchenabled GPT-4o. GOOGLE-ARXIV (GA):Google search restricted to arXiv.org. GOOGLE-ARXIV + LLM (GA+LLM): Query refined using LLM before Google search restricted by arXiv. 6 OPENALEX+LLM (OA+LLM):Keywords extracted by LLM for the retrieval of the OpenAlex API. SEMANTIC SCHOLAR+LLM(2S+LLM):LLMextracted keywords used for the Semantic Scholar search. PUBMED+LLM(PM+LLM):LLMgenerated keywords for PubMed searches. PASA:An LLM-driven academic search agent optimized through reinforcement learning (He et al., 2025). search PAPERFINDER:An assistant accessed LLM-powered academic at <https://paperfinder.allen.ai/chat>(accessed July 9, 2025). It mimics human-like iterative literature search by decomposing queries, tracking citations, and providing relevance explanations. (Allen Institute for AI, 2025) For all \"+LLM\" variants, we use Qwen332B (Yang et al., 2025a) for keyword extraction, relevance estimation or query refinement. For \"+GPT\" variants, GPT-4o is employed to rewrite the query for improved clarity before web search. Task-specific prompting strategies are detailed in Appendix A.1. We evaluate retrieval performance using three standard metrics: Precision, Recall, and F1, computed at the document level for each query. Importantly, each retrieval system operates over its own native search source (e.g., OpenAlex, Semantic Scholar, Google Scholar), rather than performing search on shared benchmark corpus. This setup reflects realistic usage scenarios and allows for endto-end evaluation of each systems full retrieval pipeline, including query understanding, source selection, search execution, and result ranking. Our goal is to assess the overall effectiveness of each system as holistic academic search solution. Let be the number of true positives (relevant documents correctly retrieved), the number of false positives (irrelevant documents retrieved), and the number of false negatives (relevant documents not retrieved). The metrics are defined as follows: 7 Precision = Recall = F1 = P + P + 2 Precision Recall Precision + Recall (1) (2) (3) Precision measures the proportion of retrieved documents that are truly relevant, reflecting retrieval accuracy. Recall measures the proportion of relevant documents that are successfully retrieved, reflecting coverage. F1 is the harmonic mean of Precision and Recall, providing balanced assessment of both accuracy and completeness. We report results on two benchmarks: AutoScholar: synthetic benchmark introduced in the PaSa paper, designed to evaluate retrieval precision on fine-grained AI domain queries. SPARBench: Our curated benchmark featuring real-world queries from computer science and biomedicine, with expert-validated relevance annotations."
        },
        {
            "title": "5.2 Main Result",
            "content": "As shown in Table 1, our proposed method SPAR consistently outperforms all baselines across both benchmarks. On the AUTOSCHOLAR dataset, SPAR achieves the highest F1 score of 0.3843 and the highest precision of 0.3612, while maintaining competitive recall (0.4105). This demonstrates its strong ability to retrieve relevant documents with high accuracy and balance. On SPARBENCH, SPAR also surpasses all other methods, achieving the best F1 score of 0.3015, recall of 0.3103, and precision of 0.2932. In contrast, prior methods such as GA+LLM, PASA, and PAPERFINDER exhibit either lower precision or significant performance imbalance (e.g., high recall but very low precision). Notably, while PAPERFINDER obtains the highest recall (0.8333) on AutoScholar, its precision (0.0261) is extremely low, leading to much lower F1 score. These results highlight SPARs superior capability in balancing precision and recall, thereby providing robust and effective academic document retrieval across diverse settings. Method G+GPT GS CS GA GA+LLM PM+LLM OA+LLM 2S+LLM PaSa PaperFinder AutoScholar SPARBench Recall Precision F1 Recall Precision - - - 0.0869 0.0400 0.0556 - 0.0045 0.0044 0.2449 0. 0.2015 0.2683 0.1130 0.3046 0.1571 0.1692 0.000 0.1083 0.0833 0.7931 0.8333 - - - 0.0507 0.0229 0.0333 - 0.0023 0.0023 0.1448 0.0261 - 0.0092 0.0043 0.0045 0.2451 0.1923 - 0.0242 0.0135 0.1041 0.0418 0.000 0.0082 0.0038 0.0038 0.2800 0.1613 0.000 0.0988 0.0449 0.1009 0.1474 - 0.0106 0.0050 0.0055 0.2180 0.2382 - 0.0138 0.0080 0.1076 0.0244 SPAR (ours) 0.3843 0.4105 0.3612 0.3015 0.3103 0. Table 1: Comparison of retrieval performance across different methods on the AutoScholar and SPARBench benchmarks. indicates metrics unavailable due to missing valid document. Recall Precision 6.2 Impact of RefChain Benchmark QInterp AutoScholar SPARBench w/o w/o 0.19 0.18 0.22 0.21 0.24 0.25 0.16 0.21 0.16 0.14 0.34 0. Table 2: Effect of query interpretation (QInterp) on retrieval performance across benchmarks."
        },
        {
            "title": "6.1 Effects on Query Interpretation",
            "content": "Query Interpretation (QInterp) enhances retrieval by analyzing the query intent, selecting appropriate sources, and performing intent-aware rewriting. As described in Appendix D, this module introduces structural awareness that enables better alignment between queries and target documents. Table 2 reports retrieval results with and without QInterp across two benchmarks. On both datasets, enabling QInterp improves overall F1 and precision. In SPARBench, precision increases substantially from 0.21 to 0.34, reflecting improved ranking relevance in complex, multi-source environment. However, recall tends to decrease (e.g., 0.25 to 0.24 in AutoScholar, 0.21 to 0.16 in SPARBench), likely due to more restrictive interpretations that favor precision over coverage. This trade-off is consistent with observations in baseline systems employing aggressive query rewriting  (Table 1)  . These results suggest that query interpretation is beneficial for precision-oriented retrieval, especially in settings requiring fine-grained query understanding and source selection. Future work may explore hybrid strategies that balance interpretation with recall-aware expansion. 8 The RefChain mechanism significantly improves document recall by expanding the set of candidates through citation-based traversal. As shown in Table 5 (Appendix C.3), RefChain enhances recall-oriented metrics on both the AutoScholar and SPARBench benchmarks. In AutoScholar, RefChain increases the recall after similarity filtering from 0.41 to 0.44 and raw recall from 0.58 to 0.77, while the average retrieved documents rise from 306.9 to 569.1. Similarly, on SPARBench, recall improves from 0.13 to 0.15, the raw recall from 0.26 to 0.31, and the retrieval volume from 367.8 to 504.9. However, this recall improvement reduces precision due to increased noise. In AutoScholar, precision drops from 0.29 to 0.19, and in SPARBench from 0.22 to 0.16. These results indicate that RefChain is most beneficial in recall-critical scenarios, such as retrievalaugmented generation (RAG) for academic synthesis. In contrast, precision-focused retrieval systems may prefer to disable RefChain to minimize noise and reduce downstream filtering overhead."
        },
        {
            "title": "6.3 Benefits of Query Evolution",
            "content": "Query Evolution refines search queries by leveraging retrieval history and high-relevance documents, enhancing search focus via semantic guidance from top-ranked results (see case study in Appendix C.2). Table 3 reports its impact on F1, recall, and precision across two academic search benchmarks: AutoScholar and SPARBench. Query Evolution consistently improves F1 in both benchmarks. Precision increases by 0.02 in each dataset, indicating more targeted retrieval. AlBenchmark Evolution AutoScholar SPARBench w/o w/o F1 0.34 0.33 0.26 0. Recall Precision 0.41 0.43 0.24 0.24 0.29 0.27 0.27 0. Table 3: Impact of query evolution on retrieval performance. Evolution denotes application of Query Evolution. though recall slightly decreases in AutoScholar, the overall shift toward higher precision demonstrates the effectiveness of focused querying. Prompts used for query evolution are provided in Appendix A.2. 6.4 Reranking Strategy and Its Advantages The reranking module reorders the top-10 retrieved documents to optimize Recall@5. Table 4 reports its effects on two benchmarks. On AutoScholar, Recall@5 increases from 0.3146 to 0.4015, representing 27.6% relative improvement. On SPARBench, Recall@5 improves from 0.1588 to 0.1662, 4.7% relative gain. These results demonstrate the modules effectiveness in prioritizing authoritative and contextually relevant documents, thereby enhancing retrieval quality and user experience. The larger improvement observed on AutoScholar suggests that reranking is particularly beneficial for datasets characterized by simpler query structures. Benchmark Reranking Recall@5 AutoScholar SPARBench w/o w/o 0.3146 0.4015 0.1588 0.1662 Table 4: Effect of reranking on Recall@5 for the top 5 retrieved documents across benchmarks."
        },
        {
            "title": "Prompt Selection",
            "content": "We examine how model and prompt choices affect relevance assessment performance. Larger models do not consistently outperform smaller ones. systematic evaluation reveals the most effective configuration. We compare two prompt stylesbrief and complexacross seven language models on the AutoScholar and SPARBench benchmarks (Appendix A.3; Table 6). On AutoScholar, Qwen332B with the brief prompt achieves the highest F1 score (0.38). On SPARBench, LLaMA3.370B (Grattafiori et al., 2024) with the same prompt performs best (F1: 0.30). Based on overall performance across both datasets, we select Qwen3-32B (brief) as the default configuration. Additional generalization experiments (Appendix C.4.2) confirm the robustness of this choice, underscoring the importance of prompt design and model selection in relevance assessment."
        },
        {
            "title": "7 Conclusion",
            "content": "We present SPAR, modular multi-agent framework for academic paper retrieval, designed to tackle the challenges of underspecified queries, fragmented sources, and evolving information needs. SPAR consists of four key stages: (1) Query Understanding agent that interprets user intent through intent classification, domain detection, and temporal constraint parsing, followed by intent-aware query refinement; (2) an Iterative Retrieval phase that integrates source-adaptive querying and RefChain-based citation expansion for recall-oriented exploration; (3) Query Evolver agent that diversifies search trajectories by generating follow-up queries based on previously retrieved papers; and (4) Reranker that ranks results using relevance, timeliness, and publication authority. To evaluate SPAR, we construct SPARBench, benchmark of semantically complex academic queries with expert-labeled relevance. Experiments on SPARBench and AutoScholar demonstrate that SPAR consistently outperforms strong baselines. It achieves an F1 score of 0.3843 on AutoScholar, +56% improvement over PaSa, and 0.3015 on SPARBench, the only method delivering balanced performance across all metrics. Our results validate the synergy of symbolic planning (RefChain), LLM-powered query evolution, and agent-based modular design in addressing the complexity of scholarly retrieval tasks. SPAR and SPARBench offer reproducible and extensible foundation for advancing intelligent academic search systems."
        },
        {
            "title": "Limitations",
            "content": "Despite the strong performance of SPAR, several limitations remain. First, SPAR limits citation-based expansion (RefChain) to single traversal depth. While this design reduces latency and suppresses noise, it may 9 miss deeply nested but highly relevant works, especially in long citation chains that characterize foundational research. Second, RefChain substantially improves recall but introduces noisy candidates, leading to lower precision. This trade-off, while acceptable for recall-oriented tasks, may be suboptimal in scenarios that demand high-precision retrieval, such as targeted literature reviews. Third, SPAR currently relies on static prompting and rule-based orchestration. The lack of feedbackdriven learning or user interaction modeling hinders personalization and adaptation over time. Incorporating reinforcement signals or retrievalbased supervision could make the system more robust in dynamic search environments. Finally, although SPARBench provides valuable testbed for semantically complex academic queries, it remains limited in scale and domain diversity. Future work should extend SPARBench to cover additional disciplines and query types, enabling broader generalization and facilitating standardized evaluation for next-generation academic search systems."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. 2024. Litsearch: retrieval benchmark for scientific literature search. arXiv preprint arXiv:2407.18940. Allen Institute for AI. 2025. Ai2 paper finder: LLMhttps:// powered academic search assistant. paperfinder.allen.ai/chat. Accessed: 2025-0709. Abhijit Anand, Vinay Setty, Avishek Anand, and 1 others. 2023. Context aware query rewriting for text rankers using llm. arXiv preprint arXiv:2308.16753. Nicholas Belkin. 1980. Anomalous states of knowledge as basis for information retrieval. Canadian journal of information science, 5(1):133143. Kathi Canese and Sarah Weis. 2013. Pubmed: the bibliographic database. The NCBI handbook, 2(1):2013. Claudio Carpineto and Giovanni Romano. 2012. survey of automatic query expansion in information retrieval. Acm Computing Surveys (CSUR), 44(1):1 50. Cohan, Feldman, Beltagy, Downey, and DS Weld. 2004. Specter: document-level representation learning using citation-informed transformers. 2020. arXiv preprint arXiv:2004.07180. Bruce Croft, Donald Metzler, and Trevor Strohman. 2010. Search engines: Information retrieval in practice, volume 520. Addison-Wesley Reading. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Michael Gusenbauer and Neal Haddaway. 2020. Which academic search systems are suitable for systematic reviews or meta-analyses? evaluating retrieval qualities of google scholar, pubmed, and 26 other resources. Research synthesis methods, 11(2):181217. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, and 1 others. 2025. Pasa: An llm agent for comprehensive academic paper search. arXiv preprint arXiv:2501.10120. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, and 1 others. 2023. The semantic scholar open data platform. arXiv preprint arXiv:2301.10140. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Jie Liu and Barzan Mozafari. 2024. Query rewritarXiv preprint ing via large language models. arXiv:2403.09060. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting in retrievalaugmented large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 53035315. Jason Priem, Heather Piwowar, and Richard Orr. 2022. Openalex: fully-open index of scholarly works, authors, venues, institutions, and concepts. arXiv preprint arXiv:2205.01833. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. 10 Jaime Teevan, Susan Dumais, and Eric Horvitz. 2005. Personalizing search via automated analysis of interests and activities. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 449 456. Rita Vine. 2006. Google scholar. Journal of the Medical Library Association, 94(1):97. Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021. Trec-covid: constructing pandemic information retrieval test collection. In ACM SIGIR Forum, volume 54, pages 112. ACM New York, NY, USA. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025b. Qwen3 technical report. arXiv preprint arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yilmaz. 2023. Enhancing conversational search: Large language model-aided informative query rewriting. arXiv preprint arXiv:2310.09716. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: survey. arXiv preprint arXiv:2308.07107."
        },
        {
            "title": "A Prompt Template",
            "content": "A.1 Prompt For Baseline Prompt for Query Refinement Generate search query suitable for Google based on the given academic paper-related query. Please adhere to the following instructions: 1. Understand the Query: Carefully read and comprehend the given academic query. 2. Identify Key Elements: Extract the main research domain, specific methods, or core concepts. 3. Formulate the Search Query: Construct concise and effective query that captures these components and is suitable for academic search engines. 4. Avoid Site Constraints: Do not include any site-specific filters (e.g., site:xxx). 5. Output Format: Only generate the refined query using the format below. [Users Query]: {UserQuery} [Generated Search Query]: <your query here>"
        },
        {
            "title": "Prompt for keywords extraction",
            "content": "Extract optimal search keywords from the given research question, specifically optimized for the {source} academic database. Your task is to generate concise, commaseparated query terms that will maximize relevant paper retrieval on this platform. Source-Specific Guidelines: Semantic Scholar: Focus on technical terminology and core concepts. Include methodological terms. Consider keywords if prominent researchers are known. author-centric Emphasize computer science and AI terminology where relevant. 11 OpenAlex: Prioritize terms. broader academic Include interdisciplinary connections. Balance specificity with coverage. Include field classifications where relevant. PubMed: Emphasize medical/biological terminology. Include relevant MeSH (Medical Subject Headings) terms. Consider clinical and biomedical contexts. Include chemical/drug names or biological processes where relevant. Response Format: [Start] keyword1, keyword2, keyword3, ... [End] Examples by Source: Semantic Scholar: [Start] transformer architecture, attention mechanism, language model fine-tuning [End] OpenAlex: [Start] neural networks, deep learning, artificial intelligence, pattern recognition [End] PubMed: [Start] CRISPR-Cas9, gene editing, genetic therapy, chromosomal modification [End] {searched_queries} - Relevant Document Title: {doc_title} - Document Abstract: {doc_abstract} - Document Field: {doc_field} ### TASK: Generate {N} NEW search queries that explore different aspects of this research area: 1. query exploring METHODOLOGICAL alternatives or comparisons 2. query focusing on APPLICATIONS or implementations 3. query addressing LIMITATIONS, challenges, or critiques Each query should be: - Clearly different from previously searched queries - Based on insights from the document - Relevant to the original research question - Specific enough to retrieve focused results ### IMPORTANT NOTE: information is missing or If document insufficient (e.g., empty abstract), generate queries based primarily on the original query and your knowledge of the research Focus on exploring compledomain. mentary aspects of the topic rather than requiring specific document details. ### OUTPUT FORMAT: Return JSON array of strings containing only the expanded queries: [Query 1,Query 2, Query 3] extract optimized search keyfor {source} from this quesNow, words tion:{user_query} A.3 Prompt for Relevance prompt for Relevance Brief A.2 Prompt For Query Evolution You are an academic search expert helping explore research topic more thoroughly. ### CONTEXT: - Original Query: {user_query} -"
        },
        {
            "title": "Searched",
            "content": "Queries: 12 You are an expert in academic research. Given query and document in the context of scholarly paper search, evaluate their relevance on scale from 0 to 1, where 0 means completely irrelevant and 1 means highly relevant. Base your evaluation on the querys intent, key concepts, and the documents content. Provide score and explain your reasoning consistently. Query: {UserQuery} Document: Title: {title} Abstract: {abstract} Score: [Your score between 0 and 1] Reasoning: [Your explanation] 0.50.6: Directly addresses primary subject with matching terminology 0.30.4: Related subfield but different focus area 0.10.2: Only tangential connection through peripheral terms 0.0: Fails Critical Relevance Check prompt for Relevance Complex B. Contextual Precision (00.3) You are rigorous and highly discerning academic search relevance evaluator. Your task is to critically assess the relationship between the users query and the provided scholarly article. Apply strict, highstandard academic lens to evaluate conceptual alignment, topical focus, and methodological relevance. Be skeptical of superficial keyword matches or loosely related themes. Only assign high relevance score (on 01 scale) when there is clear and substantial alignment in research purpose, methods, and contribution. Err on the side of conservatism in scoringprecision and selectivity are paramount. Input Format Query: Raw academic search query Article: Title: Academic article title Abstract: Abstract text summarizing the papers content"
        },
        {
            "title": "Hierarchical Evaluation Protocol",
            "content": "1. Critical Relevance Check (Binary Gate) If the document contains zero of the following, automatically score 0.0: 0.20.3: Explicitly addresses querys specific technical aspects 0.1: General thematic similarity without concrete details 0.0: No meaningful connection to query intent C. Depth Validation (00.1) 0.1: Provides experimental validation/novel theoretical framework 0.05: Mentions concept without substantive analysis 0.0: Superficial treatment of subject Scoring Matrix (Sum Components + + C) 0.000.19: Completely irrelevant / offtopic 0.200.39: Minimal relevance shares domain but different focus 0.400.59: Partial relevance addresses some aspects 0.600.79: Substantial relevance covers key elements Core subject keywords from 0.801.00: Optimal match comprequery Matching research domain Thematic alignment with query intent 2. Detailed Scoring Criteria (Only if passes Critical Check) hensive coverage Anti-Gaming Rules Penalize -0.3 for keyword stuffing without contextual relevance Penalize -0.2 for misleading titles/abA. Core Topic Alignment (00.6) stracts 13 If score < 0.4, round down to nearest 0.1 If score 0.7, require positive marks in all 3 criteria Machine learning for early Examples Example 1 (Low Score) Query: Alzheimers diagnosis using MRI Article: Statistical analysis of MRI machine calibration errors Reasoning: Fails Critical Relevance no ML or Alzheimers content Score: 0.15 Example 2 (High Score) Query: Federated learning optimization in IoT networks Article: Adaptive Gradient Compression for Energy-Efficient Federated Learning in Edge Computing Environments Reasoning: Directly addresses FL optimization (0.6) + technical specifics (0.25) + experimental validation (0.1) Score: 0.86 Input Data Query: {query} Article: {doc} Output Format Reasoning: tion] Score: [0.001.00] [Concise technical justificaThe query specifically asks for recent/current papers, so strongly prefer newer papers 3. Maintain reasonable relevance to the original query For each paper, provide: 1. new numerical rank (1 being the highest) 2. brief justification (1-2 sentences) 3. new relevance score between 0-1 that incorporates both relevance and the factors above List of papers with original relevance scores (title, year, venue, authors, relevance): {Doc List Here} Please provide your reranking with new scores and concise justifications in the following format for each document: Document [index]: [score] - [justification] For example: Document 1: 9.5 - Highly relevant as it directly addresses the query topic with empirical evidence. Document 2: 7.0 - Somewhat relevant but focuses on tangential aspect of the query. A.4 Prompt For Reranking"
        },
        {
            "title": "Reranking without Time Requirement",
            "content": "Please rerank the following {N} academic papers in response to the query:{Query} Please rerank the following {N} academic papers in response to the query: {Query} Consider these factors in your reranking: Consider these factors in your reranking: 1. Authority: 1. Authority: Publication venue prestige (top conferences/journals rank higher) Author prominence (authors with higher h-index or citation counts rank higher) Publication venue prestige (top conferences/journals rank higher) Author prominence (authors with higher h-index or citation counts rank higher) 2. Timeliness: 2. Timeliness: 14 Generally prefer more recent papers, but dont overly penalize influential older papers 3. Maintain reasonable relevance to the original query For each paper, provide: 1. new numerical rank (1 being the highest) 2. brief justification (1-2 sentences) 3. new relevance score between 0-1 that incorporates both relevance and the factors above List of papers with original relevance scores (title, year, venue, authors, relevance): {Doc List Here} Please provide your reranking with new scores and concise justifications in the following format for each document: Document [index]: [score] - [justification] For example: Document 1: 9.5 - Highly relevant as it directly addresses the query topic with empirical evidence. Document 2: 7.0 - Somewhat relevant but focuses on tangential aspect of the query."
        },
        {
            "title": "B BenchMark Information",
            "content": "B.1 SPARBench Example"
        },
        {
            "title": "An Example of SPARBench",
            "content": "Question: \"What are the potentials and ethical challenges of gene editing technologies (e.g., CRISPR) in treating genetic diseases? Provide specific explanations and recent research progress.\" Source Metadata: Search Time: 2025-04-10 Reference Answers: org/content/24/9/1526.full. pdf Title: \"Seamless gene correction of β-thalassemia mutations in patientspecific iPSCs using CRISPR/Cas9 and piggyBac\" Abstract: β-thalassemia, one of the most common genetic diseases worldwide, is caused by mutations in human hemoglobin beta (HBB) gene. Creation of induced pluripotent stem cells (iPSCs) from βthalassemia patients could offer an approach to cure this disease. Correction of disease-causing mutations in iPSCs can restore normal function and provide rich source for transplantation. In this study, we used the latest gene-editing tool, CRISPR/- Cas9 technology, combined with piggyBac transposon to efficiently correct patient-derived mutations without leaving any residual footprint. No off-target effects were detected in corrected iPSCs, which retain full pluripotency and normal karyotypes. When differentiated into erythroblasts using monolayer culture, gene-corrected cells restored HBB expression compared to the parental line. Our study provides an effective footprint-free correction method, thereby demonstrating critical step toward future application of cell-based gene therapy for monogenic diseases. Authors: Fei Xie, Lin Ye, Judy C. Chang, Ashley I. Beyer, Jiaming Wang, Marcus O. Muench, Yuet Wai Kan Year: 2014 Citation Count: 381 Source: OpenAlex Similarity Scores: * Small Model: 0.7 * LLM: 0.85 2. Paper ID: https://www. nature.com/articles/ s41392-019-0089-y.pdf 1. Paper ID: http://genome.cshlp. Title: \"Applications of genome edit15 ing technology in the targeted therapy of human diseases: mechanisms, advances and prospects\", Abstract: \"...\" ..."
        },
        {
            "title": "C Strategy Result",
            "content": "C.1 Query Interpretation Result Example of Query Interpretation User Query: What in improvements are needed development vaccine efficiency to respond to emerging Provide infectious diseases? multi-angle analysis. Query Intent: Literature review and multiangle analysis Domain: Biomedical sciences / Vaccine development Suitable Sources: pubmed, openalex Needs Expansion: true Expansion Reason: The query is somewhat broad and could be improved by incorporating specific technical terms or focusing on particular aspects of vaccine development. Terms like vaccine development efficiency are relatively general and may yield wide range of results, some of which might not be directly relevant to responding to emerging infectious diseases. The query could benefit from specifying methodologies (e.g., computational modeling, platform technologies, mRNA-based approaches), clarifying the scope (e.g., preclinical vs clinical stages), and defining the type of research sought (e.g., reviews, metaanalyses, case studies). Including terms such as platform vaccines, reverse vaccinology, or rapid antigen identification would enhance precision and relevance within the domain of biomedical sciences. Expanded Queries: Systematic methods for emerging infectious diseases Literature review of vaccine application strategies in global health emergencies (2021-2025) Historical survey of vaccine innovation in response to pandemics since Comparative analysis of future challenges in accelerating vaccine development post-pandemic Survey of multi-disciplinary approaches to improve vaccine design efficiency Time Requirement Description: NO Source Reason: PubMed is the most suitable source for this query due to its focus on biomedical and life sciences research, which directly aligns with vaccine development. OpenAlex can also be useful as it provides interdisciplinary context and broader metadata, supporting multi-angle analysis without time constraints. C.2 Query Evolution vs Native Method This example illustrates query-document semantic relationships via color coding. Tokens highlighted in the same color indicate shared or closely related concepts between queries and document content."
        },
        {
            "title": "Query Evolution vs Native method",
            "content": "Original Query: \"Can you identify any papers that analysed the use of target networks with linear function approximation, needed in theoretical properties of target networks?\" Previously Searched Queries: Survey of target networks with linear function approximation methods Systematic review of historical development of target networks with linear function approximation Literature review of target networks applications using linear function approximation review of vaccine development Can you identify any papers that analysed 16 the use of target networks with linear function approximation, needed in theoretical properties of target networks? State-of-the-art in theoretical properties of target networks using linear function approximation (2024-2025) Comparative analysis of future challenges in target networks with linear function approximation Relevant Document Title: \"A Unifying View of Linear Function Approximation in Off-Policy RL Through Matrix Splitting and Preconditioning\" Document Abstract: \"Traditionally, TD and FQI are viewed as differing in the number of updates toward the target value function: TD makes one update, FQI makes an infinite number, and Partial Fitted QIteration (PFQI) performs finite number, such as the use of target network in Deep Q-Networks (DQN) in the off-policy setting. This perspective, however, fails to capture the convergence connections between these algorithms and may lead to incorrect conclusions, for example, that the convergence of TD implies the convergence of FQI. In this paper, we focus on linear value function approximation and offer new perspective, unifying TD, FQI, and PFQI as the same iterative method for solving the Least Squares Temporal Difference (LSTD) system, but using different preconditioners and matrix splitting schemes. TD uses constant preconditioner, FQI employs data-feature adaptive preconditioner, and PFQI transitions between the two. Then, we reveal that in the context of linear function approximation, increasing the number of updates under the same target value function essentially represents transition from using constant preconditioner to datafeature adaptive preconditioner. This unifying perspective also simplifies the analyses of the convergence conditions for these algorithms and clarifies many issues. Consequently, we fully characterize the convergence of each algorithm without assuming specific properties of the chosen features (e.g., linear independence). We also examine how common assumptions about feature representations affect convergence, and discover new conditions on features that are important for convergence. These convergence conditions allow us to establish the convergence connections between these algorithms and to address important questions.\" - Document Field: \"cs.LG\" QueryEvolution: Practical implementation of Partial Fitted Q-Iteration with linear function approximation in off-policy reinforcement learning settings Challenges and convergence limitations of using fixed versus adaptive preconditioners in target network-based reinforcement learning algorithms NativeMethod: Real-world implementations and case studies of target networks using linear function approximation Critique of convergence and stability issues in target networks employing linear function approximation C.3 RefChain Effect Details As shown in Table 5, using RefChain can improve the final Recall by 7.32% and 15.38% on AutoScholar and SPARBench. C.4 Performance of Varies Relevance"
        },
        {
            "title": "Assesment",
            "content": "C.4.1 Performance on Benchmarks Table 6 shows the comparison of different models for relevance judgment. Qwen3-32B with brief instruction achieves the best F1 score of 0.38. C.4.2 Performance on OpenSource Dataset Table 7 shows the comparison of different models for relevance judgment on three open-source benchmarks. Qwen3-32B with brief instruction achieves the best average performance among all benchmarks."
        },
        {
            "title": "E SPARBench Stage Volume Change",
            "content": "17 Benchmark RefChain Recall Precision Raw Doc Num Valid Doc Num Recall (Raw) AutoScholar SPARBench w/o w/o 0.41 0.44 0.13 0.15 0.29 0. 0.22 0.16 306.90 569.08 367.81 504.94 3.95 6.58 10.77 15.00 0.58 0. 0.26 0.31 Table 5: Impact of RefChain on document retrieval metrics across two benchmarks. Enabling RefChain improves recall but introduces more noise, leading to drop in precision. Recall and Precision are computed based on documents retained after relevance filtering. Raw Doc Num refers to the total number of documents retrieved before filtering; Valid Doc Num indicates the number of relevant documents identified after filtering; Recall (Raw) is recall calculated over the full set of raw retrieved documents. Model and Inst Variant PaSa_Selector Llama3.1-8B (brief) Llama3.1-8B (complex) Llama3.3-70B (brief) Llama3.3-70B (complex) Qwen2.5-7B (brief) Qwen2.5-7B (complex) Qwen2.5-72B (brief) Qwen2.5-72B (complex) Qwen3-8B (brief) Qwen3-8B (complex) Qwen3-14B (brief) Qwen3-14B (complex) Qwen3-32B (brief) Qwen3-32B (complex) AutoScholar Recall 0.41 0.19 0.48 0.38 0.34 0.47 0.09 0.51 0.44 0.53 0.45 0.37 0.38 0.41 0.29 Precision 0.29 0.08 0.07 0.13 0.12 0.27 0.06 0.24 0.11 0.20 0.24 0.14 0.15 0.36 0.05 F1 0.34 0.11 0.13 0.20 0.18 0.34 0.08 0.33 0.17 0.29 0.31 0.21 0.22 0.38 0. SPARBench Recall 0.24 0.18 0.24 0.31 0.34 0.28 0.27 0.38 0.27 0.30 0.31 0.32 0.34 0.29 0.30 Precision 0.27 0.23 0.22 0.29 0.12 0.28 0.21 0.17 0.15 0.22 0.19 0.20 0.17 0.21 0.12 F1 0.26 0.20 0.23 0.30 0.17 0.28 0.24 0.24 0.19 0.25 0.23 0.25 0.23 0.24 0.18 Table 6: Performance Comparison of Different Models on AutoScholar and SPARBench Datasets(where brief refers to inst-brief and complex to inst-complex). Prompt details can be found in Appendix A.3. Model and Inst Variant TREC-Covid Scidocs LitSearch PaSa_Selector LLaMA3.1-8B (brief) LLaMA3.1-8B (complex) LLaMA3.3-70B (brief) LLaMA3.3-70B (complex) Qwen2.5-7B (brief) Qwen2.5-7B (complex) Qwen2.5-72B (brief) Qwen2.5-72B (complex) Qwen3-8B (brief) Qwen3-8B (complex) Qwen3-14B (brief) Qwen3-14B (complex) Qwen3-32B (brief) Qwen3-32B (complex) 0. 0.6967 0.6537 0.7047 0.6942 0.6930 0.6693 0.7163 0.6921 0.7143 0.6569 0.7170 0.6853 0.7256 0.6729 0.1291 0.7453 0.5251 0.7366 0.3278 0.3022 0.0751 0.7715 0. 0.3553 0.1203 0.4756 0.1238 0.6082 0.1651 0.4980 0.5695 0.5080 0.5737 0.5108 0.4808 0.3571 0.5830 0.4374 0.5224 0.4335 0.5338 0.4481 0.5566 0.4550 Table 7: F1 scores of various models on TREC-Covid (Voorhees et al., 2021), Scidocs (Cohan et al., 2004), and LitSearch-NLP-Class (Ajith et al., 2024) datasets. 18 Figure 4: The Overview of Query Interpretation Module Figure 5: Document volume at each filtering stage of the benchmark construction pipeline, showing the reduction from raw retrieval results to the final final set."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence (BAAI)",
        "Beijing Jiaotong University (BJTU)"
    ]
}