{
    "paper_title": "ModelCitizens: Representing Community Voices in Online Safety",
    "authors": [
        "Ashima Suvarna",
        "Christina Chance",
        "Karolina Naranjo",
        "Hamid Palangi",
        "Sophie Hao",
        "Thomas Hartvigsen",
        "Saadia Gabriel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation. The data, models and code are available at https://github.com/asuvarna31/modelcitizens."
        },
        {
            "title": "Start",
            "content": "ModelCitizens: Representing Community Voices in Online Safety Ashima Suvarna Christina Chance Karolina Naranjo Sophie Hao Thomas Hartvigsen Saadia Gabriel Hamid Palangi 5 2 0 2 ] . [ 2 5 5 4 5 0 . 7 0 5 2 : r University of California, Los Angeles, University of Virginia Google, New York University asuvarna31/modelcitizens"
        },
        {
            "title": "Abstract",
            "content": "Warning: This paper contains content that may be offensive or upsetting. Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPTo4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN8B and GEMMACITIZEN-12B, LLaMAand Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of communityinformed annotation and modeling for inclusive content moderation."
        },
        {
            "title": "Introduction",
            "content": "To accept ones pastones historyis not the same thing as drowning in it; it is learning how to use it. James Baldwin (1963) Perceptions of what is toxic or not are inherently subjective and vary significantly across communities and contexts (social or conversational) (Sap et al., 2022; Zhou et al., 2023; Pei and Jurgens, 2023; Lee et al., 2024; Pavlopoulos et al., 2020). This subjectivity is more pronounced in toxic language annotations as lived experience and community membership can influence annotator sensitivity to certain terms and phrases (Waseem, 2016; Goyal et al., 2022; Fleisig et al., 2024). For example, African American and LGBTQ+ annotators perceive and label toxicity targeted towards their community members differently compared to annotators outside these groups (Goyal et al., 2022). Thus, when these diverse annotations are collapsed by aggregation of data labels, we risk losing community-specific perspectives and further marginalizing voices from historically vulnerable communities (Fleisig et al., 2024). These annotation biases are not limited to data but manifest as tangible downstream harms. Specifically, automatic hate speech detection, and more broadly toxic language detection models trained on such data, risk introducing unintended consequences when moderating online spaces (Sap et al., 2019; Register et al., 2024). Artificial intelligence (AI)-based content moderation that follows US norms has been shown to be culturally insensitive when deployed in global contexts (Lee et al., 2023). AI models also inappropriately censor historical or legal documents that reflect outdated values (Grayer, 2018; Henderson et al., 2022). Furthermore, ignorance of dialectal variation and reclaimed language has ironically led to racially biased hate speech detectors that risk erasure of minorities (Sap et al., 2019). Similarly, AIs inability Target Group Example (cid:254) = Toxic (cid:10) = Non-Toxic Ingroup Outgroup Muslim C: recently heard someone talk about story where cultural clothing became problem, and it made me curious about why this happens. What are peoples experiences or thoughts on this? S: He was wearing muslim head scarf when he came to America and she asked him to take it off. C: Its often due to lack of understanding or fear of whats different. People sometimes react negatively without really thinking about cultural or religious significance. Gemini:(cid:254) Moderation:(cid:10) Muslim S: If you are woman you should not wear hijab because it is not your choice to wear it and it is symbol of oppression. Gemini:(cid:254) Moderation:(cid:254) Asian C: came across someone claiming that all Asians love kung fu movies, and Im curious about how true this is? S: All asians love kung fu movies. C: Kung fu movies are amazing. Whats not to like? Gemini:(cid:254) Moderation:(cid:254) (cid:10) (cid:254) (cid:254) (cid:10) (cid:10) (cid:254) Asian S: The best way to do this is to keep them from coming here in the first place. Gemini:(cid:254) Moderation:(cid:10) (cid:254) (cid:10) Table 1: Examples from MODELCITIZENS and the corresponding ingroup and outgroup labels. Here, refers to Context and refers to the Statement being annotated. We show the predicted labels from our baselines models: Gemini refers to Gemini-2.0-Flash and Moderation OpenAI Moderation API. We see that for Asians, ingroup annotators find an example non-toxic while both baseline models predict it to be toxic. to discern between hate speech and online recollections of hate crimes negatively affects victims mental health (Register et al., 2024). To mitigate such harms, sociotechnical approaches that incorporate community norms are widely recognized as essential for responsible content moderation (Costanza-Chock, 2020; Gordon et al., 2022; Delgado et al., 2023). However, so far, there is lack of scalable frameworks that centrally feature community perspectives in toxicity annotations. Prior work in pluralistic toxicity annotations has focused on limited identity groups (Goyal et al., 2022), single demographic attributes like country (Lee et al., 2023) or provide insufficient data for training (Pei and Jurgens, 2023). To address this gap, we introduce MODELCITIZENS, toxic language detection dataset that incorporates the social and conversational context in determining toxicity. MODELCITIZENS comprises 6, 822 posts and 40K total annotations that include perspectives from members of eight identity groups historically targeted by hate speech and toxicity (ingroup) - Asian, Black, Jewish, Latino, LGBTQ+, Mexican, Muslim, Women (RWJF, 2017). MODELCITIZENS includes 4,302 posts augmented with conversational context generated by large language model (LLM) to better model real-world online user data. We also collect outgroup annotations from individuals who do not identify with the target group in given post, enabling analyses to highlight annotation disparities between ingroup and outgroup annotators (see Table 1 for examples). We find that ingroup and outgroup annotators of MODELCITIZENS disagree on 27.5% of posts, and the outgroup annotators label the content more frequently as toxic (see Figure 1). We show that existing state-of-the-art toxicity detection systems (e.g. OpenAI Moderation) perform poorly on MODELCITIZENS with an average accuracy of 63.6%, highlighting their misalignment with annotators who identify as members of targeted groups (see Table 4). This can be caused by systemic over reliance on outgroup labels during toxicity annotation (Goyal et al., 2022; Fleisig et al., 2023). We also find that these systems perform worse on the context-augmented subset of MODELCITIZENS with an average accuracy of 59.6%. To improve alignment with ingroup annotations, we introduce LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA and Gemmabased toxicity classifiers finetuned on MODELCITIZENS. LLAMACITIZEN-8B achieves performance gain of 5.5% on the test set of MODELCITIZENS and 9% on the context-augmented subset of MODELCITIZENS outperforming all baselines. Our models demonstrate improved accuracy across all identity groups, validating the importance of incorporating community voices in AI system design. Our main contributions are as follows. We build MODELCITIZENS by (1) crowdsourcing ingroup and outgroup annotations for toxicity and (2) adding LLM-generated conversational contexts to model real-world social media posts. Through quantitative analyses on MODELCITIZENS, we highlight significant variations in perceptions of toxicity between ingroup and outgroup annotators, thus advocating for ingroup annotations as the gold standard for toxicity detection. We introduce LLAMACITIZEN-8B and GEMMACITIZEN-12B, toxicity detection models, finetuned on MODELCITIZENS to aid online content moderation. This lays the groundwork for future research to represent historically vulnerable communities in developing inclusive and equitable toxicity detection models."
        },
        {
            "title": "2 Related Work",
            "content": "Automatic Detection of Toxic Language. Toxic language1 detection is widely implemented by training classification models (Davidson et al., 2017; Founta et al., 2018). Popular training datasets source social media comments (Sap et al., 2020) or synthetically generate large-scale toxic data to train detection models (Hartvigsen et al., 2022). These datasets often lack conversational context (e.g., preceding comment) (Pavlopoulos et al., 2020) and situational context (e.g., speaker identity) (Zhou et al., 2023; Berezin et al., 2025). Recent research has shown that incorporating conversational context led to improved classifier performance for hate speech detection (Yu et al., 2022; Pérez et al., 2023). MODELCITIZENS incorporates both conversational context and community perspectives by adding LLMgenerated discourse and community-grounded annotations. Impact of Annotator Demographics. Prior work has shown that annotators background, such as gender, sex, race, nationality and age, significantly impacts their ratings and performance on NLP tasks (Biester et al., 2022; Pei and Jurgens, 2023; Santy et al., 2023; Bansal et al., 2025). For highly subjective tasks like hate speech or toxicity detection, annotator expertise, prior beliefs, and community membership also play key role (Waseem, 2016; Al Kuwatly et al., 2020; Sap et al., 2022; Goyal et al., 2022). Salminen et al. (2018) and Lee et al. (2024) collect country-specific labels and data that highlight the differences in toxicity interpretations across countries and cultures. We show how MODELCITIZENS improves upon existing work in Table 2. MODELCITIZENS con1We broadly focus on toxic language, which includes abuse, stereotyping, and hate speech. Datasets Aligned Annotators Context #Identity Groups Toxigen (Hartvigsen et al., 2022) HateBench (Shen et al., 2025) CREHate (Lee et al., 2024) Goyal et al. (2022) POPQUORN (Pei and Jurgens, 2023) MODELCITIZENS (ours) 13 34 5 - 8 Size 9K 7.8K 1.5K 25K 50 6.8K Table 2: Comparison of existing hate speech and toxicity datasets with MODELCITIZENS. Our dataset features 6.8K samples and 40K human annotations spanning 8 identity groups and incorporates community perspectives by aligning annotators with the identity group targeted in the sample. Additionally, MODELCITIZENS also contains samples with conversational context. tains sources annotations from individuals who selfidentify with the target group. Participation & Representation in AI. Designing equitable AI systems requires involving impacted communities (Sloane et al., 2022; Delgado et al., 2023; Suresh et al., 2024; Fleisig et al., 2024). There is long history of participatory design predating the LLM era (e.g, Kyng, 1991; Winschiers-Theophilus et al., 2012). Recently, research collectives like Masakhane and Queer in AI illustrate how community-driven participation can develop datasets and large-scale AI models to better reflect marginalized experiences (Nekoto et al., 2020; Queerinai et al., 2023). Kirk et al. (2024) demonstrates that community participation in the form of \"data labor\" can develop equitable preference datasets. While Sap et al. (2022) and Goyal et al. (2022) have collected diverse annotations of toxicity from various social groups, they do not systematically study this as active \"procedural participation\" of community members (Kelty, 2020). Through MODELCITIZENS, we demonstrate how to involve community perspectives in automatic toxicity detection."
        },
        {
            "title": "3 MODELCITIZENS Curation",
            "content": "The construction of MODELCITIZENS involves three-step process: (i) sampling posts containing references to diverse identity groups from Toxigen, (ii) generating conversational context using capable large language model (LLM), and (iii) crowd-sourcing community-specific annotations. Total: 6,822 Identity Group Count Toxicity (%) Asian Black Jewish Latino LGBTQ+ Mexican Muslim Women 690 788 828 796 945 859 882 1029 45.0 48.1 33.3 41.2 33.9 34.9 45.0 38.5 Type of Post Count Toxicity (%) Context-Augmented Single Post 4302 2520 40.0 40.0 Table 3: Statistics of the MODELCITIZENS dataset. Our dataset comprises of single statement posts and context-augmented posts spanning 8 identity groups. We show the percentage of toxic posts in our dataset (Toxicity (%)). Ingroup and Outgroup Annotators. Any social identity group that is targeted in given post is referred to as the target group. Annotators that self-identify with the target group are ingroup annotators while annotators that do not self-identify with the target group are outgroup annotators. We use the target groups associated with each post from Toxigen to stratify the sampled posts and recruit annotators accordingly. 3.1 Sampling from Toxigen We sample posts from the Toxigen dataset (Hartvigsen et al., 2022), which contains synthetic toxic language targeting minorities and vulnerable groups. Specifically, we sample 2,520 posts while balancing for 8 target group categories. Toxigen does not provide demographic details of annotators aligned to the target group, thus, we re-annotate the original posts with ingroup and outgroup annotators. We provide additional details of our sampling process in Appendix D. 3.2 Generating Synthetic Context Prior work have shown the importance of conversational context on toxicity detection (Pavlopoulos et al., 2020; Yu et al., 2022), thus, we augment the original posts with LLM-generated context. In particular, we prompt GPT-4o (Hurst et al., 2024) to generate previous comment and follow-up comment for the original post to mimic discourse on Reddit2. For each post we generate harmful context and benign context. We conduct human validation to assess the quality of the generated contexts and find that 86% of the posts had highquality contexts. After removing the low-quality contexts, we have 4,302 context augmented posts. We provide the prompts we use in Appendix D. 3.3 Collecting Annotations Toxigen includes 13 identity groups that are especially vulnerable to online hate. From these, we focus on 8 groups that are particularly likely to encounter online toxicity in North American context (since we recruit U.S.-based annotators) and are well-represented in the Prolific annotator pool. Specifically, we focus on posts targeting Asian, Black, Jewish, Latino, LGBTQ+, Mexican, Muslim, and Women identity groups. Future work may extend our annotation framework to include additional identity groups. Target Group Selection. Toxigen includes 13 identity groups that are especially vulnerable to online hate. From these, we focus on 8 identity groups that are most likely to face online hate in North American perspective (since we recruit U.S-based annotators) and are well-represented in the Prolific annotator pool. Thus, we focus on posts targeted towards Asian, Black, Jewish, Latino, LGBTQ+, Mexican, Muslim and Women. Future work may extend our annotation framework to include additional identity groups. Annotator Recruitment. We recruit annotators via Prolific.3 We apply two initial screening criteria: (i) participants must be fluent in English because MODELCITIZENS targets English language data, and (ii) participants must reside in the United States. Overall, we recruited 828 unique annotators for our annotation task. We provide the detailed demographic distribution of the annotators in Appendix Table 9. Annotation Process. We ask annotators whether the post would be toxic to the target group. Following prior work (Sap et al., 2020; Hartvigsen et al., 2022), annotators rate toxicity of the post on scale of 1-5 with 1 being benign and 5 being extremely toxic. To avoid biasing the annotators, we conduct our annotation in two phases: (i) annotators are first shown the original post (ii) annotators are shown the context-augmented posts. We collect 6 annotations per post balanced between ingroup and 2https://www.reddit.com/ 3https://www.prolific.com/ outgroup annotators. The annotation interface and guidelines are provided in Appendix A. Annotator Agreement. Now, we analyze the quality of our annotations using Krippendorffs α to calculate the inter annotator agreement (IAA). We find that our annotations show moderate agreement for Black (α = 0.32), Asian(α = 0.32), Muslim (α = 0.34), LGBTQ+ (α = 0.30), Latino(α = 0.34), Mexican (α = 0.40), Women (α = 0.47) and Jewish (α = 0.41). These are comparable to those achieved in prior work in toxic language detection (Sap et al., 2019) and demographically stratified annotations (Lee et al., 2024; Pei and Jurgens, 2023). MODELCITIZENS Statistics. We present the statistics of MODELCITIZENS in Table 3. Specifically, MODELCITIZENS contains 6,822 posts comprising of 2,502 single statement posts and 4,302 context-augmented posts. MODELCITIZENS covers eight identity groups and includes 40K human annotations, equally balanced between ingroup and outgroup annotations."
        },
        {
            "title": "4 Analysis on MODELCITIZENS",
            "content": "Here, we demonstrate that the community membership of annotators significantly impacts toxicity annotations. Additionally, we also show the impact of adding conversational contexts on toxicity. 4.1 Impact of Annotator Background To understand the influence of annotator identity on toxicity rating, we analyze the rating distribution and label disagreements between ingroup and outgroup annotations. We introduce two classes of disagreement between : (a) Missed Harm - when outgroup fails to recognize harm identified by the ingroup and (b) Amplified Harm - when when the outgroup perceives harm that the ingroup considers benign. Statistically Significant Differences between Ingroup and Outgroup. We show the distribution of ratings from ingroup and outgroup annotators for each target group in Appendix Figure 5. We conduct Wilcoxon Rank sum test on these ratings and find statistically significant differences in the annotations from ingroup and outgroup (p < 0.01).4 We find the largest differences in the median ratings for Asian, Black, LGBTQ+ and Women. Interestingly, outgroup annotators assigned lower toxicity 4Latino and Mexican target group showed low significant differences ratings than ingroup annotators for content targeting Asians. In contrast, for content targeting Black individuals, LGBTQ+ individuals, and women, outgroup annotators provided higher toxicity ratings compared to ingroup annotators. Higher Amplified Harm Disagreements. In Figure 1, we observe that amplified harm is more prevalent for content targeting women, LGBTQ+ individuals, and Jewish communities. This may reflect increased sensitivity toward these groups in the U.S., leading outgroup annotators to overestimate harm during annotation. Similar to findings from prior work (Sap et al., 2019), content targeting Black individuals also showed higher amplified harm rate. On the contrary, for content targeting Asians, outgroup annotators more frequently underestimate harm, resulting in higher missed harm rate. For Latino, Mexican, and Muslim target groups, disagreement rates for missed harm and amplified harm are more balanced. Overall, we observe the highest total disagreement rates for Black and LGBTQ+. 4.2 Impact of Context Augmentation Context Augmentation Increases Missed Harm Rate. From Figure 1, we observe that context augmentation reduced the disagreement rate for content targeting Asian and Black individuals while increasing the disagreement rate for all others. This is mainly attributed to the increase in missed harm rate on an average. However, for content targeting LGBTQ+ individuals and Asian individuals additional context reduced the missed harm rate. Overall, amplified harm rate was still higher than missed harm rate with context augmentation. This indicates that for some target groups adding context during annotations can lead to better agreement between outgroup and ingroup annotators. Adding context leads to change in toxicity ratings and labels. We project the collected toxicity ratings into binary classes of toxic and non-toxic with threshold of scores greater than 3 indicating toxic content. In Figure 2, we show that inclusion of context changes the label of posts from the original annotations of the posts without contexts. We find that additional context led to content that was labeled benign without context being labeled toxic for Muslim, women and black target groups. For all other identity groups, additional context led to posts being labeled non-toxic. This indicates that conversational context plays key role in contextuFigure 1: We present disagreements as missed harm and amplified harm on MODELCITIZENS. In particular, amplified harm rate is much higher than missed harm rate across most identity groups. Additionally, we observe that adding context to the posts lead to increased missed harm rate in majority of the groups. of 6,153 samples and MODELCITIZENS-test comprises of 669 samples. We ensure that there is no overlap between train and test set to eliminate contamination. Each instance of our dataset has ingroup and outgroup toxicity scores and we consider ingroup scores as gold for training and evaluation. We project the toxicity scores into binary labels of 1 and 0 by applying threshold of 3.5 Baselines. We evaluate 10 baseline models across diverse categories: closed proprietary models including GPT-4o (Hurst et al., 2024), Gemini-2.0-Flash (DeepMind, 2025); strong reasoning models including GPT-o4-mini (OpenAI, 2025); open-weights models including Qwen-2.57B-Instruct (Yang et al., 2024), LLaMA-3.1-8BInstruct (Grattafiori et al., 2024), Gemma-3-12BIT (Team et al., 2025), Qwen-2.5-32B-Instruct and content moderation models including Perspective API,6 OpenAI Moderation API7 and Llama-Guard3-8B (Inan et al., 2023). We share further details of the baseline implementations in Appendix C.8 Implementation Details. We utilize LLaMA3.1-8B-Instruct (Grattafiori et al., 2024) and 5This threshold yielded the highest IAA between annotators. Scores greater than 3 are considered toxic or 1. 6https://perspectiveapi.com/ 7https://platform.openai.com/docs/gui des/moderation 8Prior work have shown that these models can be used for content moderation (Weng et al., 2023). In our experiments, models typically respond when prompted as classifier. Figure 2: Percentage of posts where adding context leads to changes in toxicity labels. We compare the toxicity of the post and the context-augmented post. alizing toxicity; while it may reduce oversensitivity toward some minority groups (Asian, Jewish), it can also reveal previously overlooked toxicity (Muslim, Black, women), depending on the target group and the content involved (Yu et al., 2022)."
        },
        {
            "title": "MODELCITIZENS",
            "content": "Here, we study how to leverage MODELCITIZENS to benchmark and train toxicity detection models. 5.1 Setup Dataset. We sample 10% of MODELCITIZENS by balancing for identity groups to form our test set. Finally, MODELCITIZENS-train comprises Model Asian Black Jewish Latino LGBTQ+ Mexican Muslim Women Average GPT-4o Gemini-2.0-Flash GPT-o4-mini Perspective API OpenAI Moderation Llama-Guard-3-8B Qwen2.5-7B-Instruct LLaMA-3.1-8B-Instruct Gemma3-12B-Instruct Qwen-2.5-32B-Instruct GEMMACITIZEN-12B LLAMACITIZEN-8B 61.1 68.9 70. 63.3 70.0 67.8 65.6 66.7 65.6 71.1 77.8 77.8 66.2 66.2 74.6 50.7 53.5 63.4 50.7 59.2 69.0 67. 69.0 71.8 Proprietary Models 64.3 63.1 67.9 60.3 69.1 58.8 69.7 66.7 62.1 Toxicity Detection Models 52.4 59.5 67.8 63.2 63.2 60.3 56.1 68.2 65.2 Open-Weight Models 56.0 63.1 67.9 65.5 48.5 57.4 50.0 51. 56.1 53.0 63.6 66.7 CITIZEN Models 63.1 67.9 67.6 67.6 71.2 71.2 69.0 70.1 72. 56.3 56.3 69.0 56.3 67.8 65.5 72.4 82.8 79.3 Base LLaMA (%) +11.1 +12.7 +4. +10.3 +18.2 +11.5 75.0 74.0 72.9 57.3 64.6 72.9 60.4 67.7 78.1 75. 79.2 75.0 +7.3 73.8 72.9 73.8 69.2 73.8 76.6 67.3 72.0 71.0 72.0 81.3 85. 67.9 69.2 69.7 58.6 63.6 68.7 58.7 64.3 65.8 68.5 74.7 75.2 +13.1 +10. Table 4: Accuracy (%) of toxicity detection models on test set of MODELCITIZENS. The CITIZEN models were finetuned on our data. We show that LLAMACITIZEN-8B outperforms all baselines on average with gain of 10.9% over the base LLaMA. The highest numbers are highlighted in bold. Model Name Toxigen HM CC Avg. Model Accuracy (%) Perspective API OpenAI Mod API LLaMA-3.1-8B-Instruct LLAMACITIZEN-8B 50.6 43.5 70.1 74.2 35.1 55.4 74.3 76.0 20.5 14.7 49.7 53.8 35.0 37.9 64.7 68.0 Table 5: F1 scores of baselines and LLAMACITIZEN8B on content moderation datasets. We evaluate on Toxigen (Hartvigsen et al., 2022), HateModerate (HM) (Zheng et al., 2024b), and Counter-Context (CC) (Yu et al., 2022). The highest values are highlighted in bold. Gemma-3-12B-IT (Team et al., 2025) as the base models of our framework since they are highly capable and compute friendly. We fully finetune the models on MODELCITIZENS-train using LLaMAFactory (Zheng et al., 2024c). Additional details and hyperparameters are provided in Appendix C. Evaluation Sets. We evaluate LLAMACITIZEN8B and GEMMACITIZEN-12B against the baselines using MODELCITIZENS-test. To demonstrate robustness to unseen data distribution, we also evaluate LLAMACITIZEN-8B on toxicity detection datasets including HateModerate (Zheng et al., 2024a), which tests adherence to Facebooks existing content moderation policies, and Counter-Context, dataset for contextualized hate speech (Yu et al., 2022). Furthermore, we evaluate LLAMACITIZEN-8B on the unseen identity GPT-4o GPT-o4-mini Gemini-2.0-Flash Perspective API OpenAI Moderation Qwen-2.5-7B-Instruct LLaMA-3.1-8B-Instruct Gemma-3-12B-IT Qwen-2.5-32B-Instruct LLAMACITIZEN-8B 64.2 65.2 65.2 57.3 61.1 50.8 59.2 63.9 62.0 74.5 Table 6: Percentage accuracy of toxicity detection models on context-augmented subset of MODELCITIZENS. LLAMACITIZEN-8B outperforms all baselines while performance degrades for all models. The highest numbers are highlighted in bold. groups of Toxigen (Hartvigsen et al., 2022). 5.2 Results We present accuracy of the performance LLAMACITIZEN-8B, GEMMACITIZEN-12B and other baselines in Table 4. LLAMACITIZEN-8B outperforms all the baselines with gain of 5.5% over our best performing baseline on average and 10.9% over LLaMA-3.1-8B-Instruct. the base GEMMACITIZEN-12B outperforms Gemma-3-12B-IT model by 9.5%. Perspective API performs the worst among all toxicity detection models with an average accuracy of Model Name Label Choice Accuracy(%) LLaMA-3.1-8B-Instruct LLaMA-3.1-8B-Instruct LLaMA-3.1-8B-Instruct LLAMACITIZEN-8B - Outgroup Aggregated Ingroup 64.3 72.3 74.9 75.2 Table 7: Variations in performance of LLaMA-3.18B-Instruct on the test set of MODELCITIZENS with changes in annotation label choice. 5.3 Ablations Impact of Annotation Label Choice. Here, we study how the choice of annotation labels affects model performance. We fine-tune LLaMA-3.1-8BInstruct using three different annotation schemes: ingroup, outgroup, and aggregated (a majority vote of ingroup and outgroup annotations). In Table 7, we observe that all three finetuned models outperform the base model, indicating the value of supervised signal from human annotations. However, the model trained on ingroup labels consistently outperforms those trained on outgroup and aggregated labels. This suggests that ingroup annotations may provide more reliable signals for detecting toxicity which are not captured by outgroup labels and diluted in aggregated labels. These findings highlight the importance of considering the source of annotations when training models for toxicity detection (Fleisig et al., 2023). Impact of Data Scaling. Now, we explore how the benefits of MODELCITIZENS scale with the size of the training data. Specifically, we finetune LLaMA-3.1-8B-Instruct with three subsets of the MODELCITIZENS including 25%, 50%, and 100% of the data. We report the accuracy on our test set in Figure 3. We find that the accuracy scales monotonically with the size of the data. This highlights that the MODELCITIZENS dataset is of high quality, and further scaling has the potential to yield greater improvements on toxicity detection."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce MODELCITIZENS, toxic language dataset that incorporates conversational context and community grounded annotations from ingroup and outgroup annotators.MODELCITIZENS annotations reveal statistically significant disagreement between annotator groups. We show that most of these disagreements are amplified harm type where outgroup annotators label benign content as toxic. We furFigure 3: Model performance with data scale. We find that MODELCITIZENS is high quality dataset that enhances toxicity classification performance as it scales. 58.6% while Qwen2.5-7B-Instruct is the worst performing open-weights model across all identity groups. Despite not being specifically trained for content moderation, Gemini-2.0-Flash and GPT-o4-mini are the best performing baselines, even outperforming LLAMACITIZEN-8B for the Latino and Black identity group. We report F1 scores in Appendix as further analysis. In Table 4, we observe that most models including LLAMACITIZEN-8B have the highest performance for women indicating that these models are well-aligned to women. GPT-4o and Gemini-2.0Flash have the highest performance for Muslims. LLAMACITIZEN-8B has the lowest performance for Jewish and Latino, however, we observe that most models have very low performance on these groups highlighting the difficulty of detecting toxicity directed towards these groups. To further assess model performance on conversational context, we report the performance accuracy of all models on the context-augmented subset of MODELCITIZENS. We see that the performance of all models degrades on this subset indicating that detecting contextualized toxicity is harder problem. However, LLAMACITIZEN-8B still outperforms our best performing baselines Gemini-2.0Flash and GPT-o4-mini by 9%. Finally, in Table 5 we see that training on MODELCITIZENS generalizes well to out-of distribution toxicity datasets. LLAMACITIZEN-8B achives high F1 scores on all out-of-distribution datasets including unseen identity groups of Toxigen. This further highlights that training on MODELCITIZENS improves model robustness and generalization. Limited Conversational Context. Our context augmentations do not fully capture the various realworld scenarios that content moderators face on regular basis. However, we recognize that the context of toxic statement can be much longer. In this work, we have shown the significant effects immediately preceding and following context can have on toxicity detection. We believe that future research could explore the influence of richer contexts by including other discourse structures and modalities (e.g., audio, image, speech). LLM-Generated Context. We use LLMs to generate context for our examples due to the limitations of human annotation, which in turn affects the quality and realism of the generated contexts. Future work should consider leveraging real-world examples from online platforms or framing context generation as human annotation task."
        },
        {
            "title": "9 Ethical Considerations",
            "content": "Dataset Usage Caveat. While the goal of this work is to curate more context-aware and community-grounded dataset to support nuanced and socially-aware toxicity classification and analysis, we acknowledge that, in the hands of bad actors, our dataset could be misused in ways that harm the very communities we aim to support. We will make the intended use clear upon public release. Human Study. This research used human annotators to provide gold labels for the dataset. We provided content and trigger warnings prior to the annotators performing the task. Due to potential mental health risks, this study underwent review by an institutional human subjects research ethics review board (IRB) and was classified as IRB Exempt. To further support annotators, we provided mental health resource guide. No Personally Identifiable Information (PII) was collected; we only gathered demographic information such as race/ethnicity, gender and sexuality, and religion. All annotators were paid at least $16/hr and spend approximately 25-28 minutes on the annotations. Use of AI Assistants. We used AI assistants (ChatGPT, Gemini) to assist with grammar and proofreading in our paper writing. ther show that existing toxicity classifiers underperform on MODELCITIZENS, particularly on contextaugmented examples. To address this, we introduce LLAMACITIZEN-8B and GEMMACITIZEN12B, LLaMA and Gemma-based models finetuned on MODELCITIZENS, which outperforms existing baselines and better reflects the perspectives of targeted communities. Future work can explore extending LLAMACITIZEN-8B to include broader range of identity groups and social media contexts. Our work demonstrates way to center community perspectives in the development of equitable toxicity detection systems and provides resources to support future research in this direction."
        },
        {
            "title": "7 Acknowledgements",
            "content": "We thank Hritik Bansal and Arjun Subramonian for their constructive comments. We thank UCLA MARS Lab members and UCLA NLP Fairness Subgroup members for project discussions and support. We thank our annotators for their hard work. This work was supported by the UCLA Initiative to Study Hate and UCLA Racial and Social Justice Seed Grants program.."
        },
        {
            "title": "8 Limitations",
            "content": "We consider the following limitations of our work: Limited Identity Groups. MODELCITIZENS considers 8 identity groups that were wellrepresented on Prolific and in Toxigen. However, many identity groups face risks of online hate and censorship from biased content moderation systems. Future work can scale our framework to incorporate more identity groups as well as their intersections. Subjectivity of Toxicity. Although our intent is to amplify the voices of targeted groups through our assessment of ingroup versus outgroup labeling, we emphasize that aggregate scores for any demographic group fail to capture the range of individual perspectives and the diverse impact of hateful speech. Such aggregation also overlooks the role of intersectionality in shaping individual experiences. We recognize that further interdisciplinary collaboration among AI researchers, community partners, social scientists, industry practitioners and policymakers is necessary to provide the robust context needed to advance this discussion and responsible AI alignment."
        },
        {
            "title": "References",
            "content": "Hala Al Kuwatly, Maximilian Wich, and Georg Groh. 2020. Identifying and measuring annotator bias based on annotators demographic characteristics. In Proceedings of the fourth workshop on online abuse and harms, pages 184190. Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, and Aditya Grover. 2025. Comparing bad apples to good oranges: Aligning large language models via joint preference optimization. Sergey Berezin, Reza Farahbakhsh, and Noel Crespi. 2025. Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection. Laura Biester, Vanita Sharma, Ashkan Kazemi, Naihao Deng, Steven Wilson, and Rada Mihalcea. 2022. Analyzing the effects of annotator gender across nlp tasks. In Proceedings of the 1st Workshop on Perspectivist Approaches to NLP@ LREC2022, pages 1019. Sasha Costanza-Chock. 2020. Design Justice: Community-Led Practices to Build the Worlds We Need. The MIT Press. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Proceedings of the international AAAI conference on web and social media, volume 11, pages 512515. Google DeepMind. 2025. Introducing Gemini 2.0: our new AI model for the agentic era blog.google. https://blog.google/technology/goo gle-deepmind/google-gemini-ai-upd ate-december-2024/. Fernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. 2023. The participatory turn in ai design: Theoretical foundations and the current state of practice. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO 23, New York, NY, USA. Association for Computing Machinery. Eve Fleisig, Rediet Abebe, and Dan Klein. 2023. When the majority is wrong: Modeling annotator disagreement for subjective tasks. arXiv preprint arXiv:2305.06626. Eve Fleisig, Su Lin Blodgett, Dan Klein, and Zeerak Talat. 2024. The perspectivist paradigm shift: Assumptions and challenges of capturing human labels. NAACL. Antigoni Founta, Constantinos Djouvas, Despoina Chatzakou, Ilias Leontiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos, and Nicolas Kourtellis. 2018. Large scale crowdsourcing and characterization of twitter abusive beIn Proceedings of the international AAAI havior. conference on web and social media, volume 12. Mitchell L. Gordon, Michelle S. Lam, Joon Sung Park, Kayur Patel, Jeffrey T. Hancock, Tatsunori Hashimoto, and Michael S. Bernstein. 2022. Jury learning: Integrating dissenting voices into machine learning models. Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. Nitesh Goyal, Ian Kivlichan, Rachel Rosen, and Lucy Vasserman. 2022. Is your toxicity my toxicity? exploring the impact of rater identity on toxicity annotation. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, and et al. 2024. The llama 3 herd of models. Annie Grayer. 2018. Facebook apologizes after labeling part of declaration of independence hate speech. CNN Politics. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Annual Meeting of the Association for Computational Linguistics. Peter Henderson, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, and Daniel E. Ho. 2022. Pile of law: Learning responsible data filtering from the law and 256gb opensource legal dataset. NeurIPS. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. 2023. Llama Guard: LLMbased Input-Output Safeguard for Human-AI Conversations. ArXiv:2312.06674 [cs]. Christopher Kelty. 2020. The participant: century of participation in four stories. University of Chicago Press. Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, et al. 2024. The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. arXiv preprint arXiv:2404.16019. Morten Kyng. 1991. Designing for cooperation: cooperating in design. Commun. ACM, 34:6473. Nayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Jose Camacho-Collados, Juho Kim, and Alice Oh. 2024. Exploring cross-cultural differences in English hate speech annotations: From dataset construction to analysis. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 42054224, Mexico City, Mexico. Association for Computational Linguistics. Nayeon Lee, Chani Jung, and Alice Oh. 2023. Hate speech classifiers are culturally insensitive. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 3546, Dubrovnik, Croatia. Association for Computational Linguistics. Ryan* Marten, Trung* Vu, Charlie Cheng-Jie Ji, Kartik Sharma, Shreyas Pimpalgaonkar, Alex Dimakis, and Maheswaran Sathiamoorthy. 2025. Curator: tool for synthetic data creation. https://github.c om/bespokelabsai/curator. Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa, Tajudeen Kolawole, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddeen Hassan Muhammad, Salomon Kabongo, Salomey Osei, et al. 2020. Participatory research for low-resourced machine translation: case study in african languages. arXiv preprint arXiv:2010.02353. OpenAI. 2025. Introducing openai o3 and o4mini. https://openai.com/index/introducing-o3-ando4-mini/. AI Patronus. 2024. Llama guard is off duty. Patronus AI. John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain, and Ion Androutsopoulos. 2020. Toxicity Detection: Does Context Really Matter? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4296 4305, Online. Association for Computational Linguistics. Jiaxin Pei and David Jurgens. 2023. When do annotator demographics matter? measuring the influence of annotator demographics with the popquorn dataset. arXiv preprint arXiv:2306.06826. Juan Manuel Pérez, Franco M. Luque, Demian Zayat, Martín Kondratzky, Agustín Moro, Pablo Santiago Serrati, Joaquín Zajac, Paula Miguel, Natalia Debandi, Agustín Gravano, and Viviana Cotik. 2023. Assessing the Impact of Contextual Information in IEEE Access, 11:30575 Hate Speech Detection. 30590. Organizers Of Queerinai, Anaelia Ovalle, Arjun Subramonian, Ashwin Singh, Claas Voelcker, Danica Sutherland, Davide Locatelli, Eva Breznik, Filip Klubicka, Hang Yuan, et al. 2023. Queer in ai: case study in community-led participatory ai. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 18821895. Yim Register, Izzi Grasso, Lauren N. Weingarten, Lilith Fury, Constanza Eliana Chinea, Tuck J. Malloy, and Emma S. Spiro. 2024. Beyond initial removal: Lasting impacts of discriminatory content moderation to marginalized creators on instagram. Proc. ACM Hum.-Comput. Interact., 8(CSCW1). RWJF. 2017. Discrimination in america: experiences and views. Joni Salminen, Fabio Veronesi, Hind Almerekhi, SoonGvo Jung, and Bernard Jansen. 2018. Online hate interpretation varies by country, but more by individual: statistical analysis using crowdsourced ratings. In 2018 Fifth international conference on social networks analysis, management and security (SNAMS), pages 8894. IEEE. Sebastin Santy, Jenny Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. 2023. Nlpositionality: Characterizing design biases of datasets and models. arXiv preprint arXiv:2306.01943. Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The risk of racial bias in hate speech detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 16681678, Florence, Italy. Association for Computational Linguistics. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 54775490, Online. Association for Computational Linguistics. Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A. Smith. 2022. Annotators with attitudes: How annotator beliefs and identities bias toxic language detection. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 58845906, Seattle, United States. Association for Computational Linguistics. Xinyue Shen, Yixin Wu, Yiting Qu, Michael Backes, Savvas Zannettou, and Yang Zhang. 2025. HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns. In USENIX Security Symposium (USENIX Security). USENIX. Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. 2022. Participation is not design fix for machine learning. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, pages 16. Harini Suresh, Emily Tseng, Meg Young, Mary Gray, Emma Pierson, and Karen Levy. 2024. Participation in the age of foundation models. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, pages 16091621. Testing hate speech detectors against content moderation policies. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024c. Llamafactory: Unified efficient finetuning of 100+ language models. arXiv preprint arXiv:2403.13372. Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, and Maarten Sap. 2023. COBRA frames: Contextual reasoning about effects and harms of offensive statements. In Findings of the Association for Computational Linguistics: ACL 2023, pages 62946315, Toronto, Canada. Association for Computational Linguistics. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert BusaFekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, and Bryce Petrini. 2025. Gemma 3 technical report. Zeerak Waseem. 2016. Are you racist or am seeing things? annotator influence on hate speech detection on Twitter. In Proceedings of the First Workshop on NLP and Computational Social Science, pages 138 142, Austin, Texas. Association for Computational Linguistics. Lilian Weng, Vik Goel, and Andrea Vallone. Using gpt-4 for content moderation. 2023. https://openai.com/index/using-gpt-4-for-contentmoderation/. Heike Winschiers-Theophilus, Nicola J. Bidwell, and Edwin H. Blake. 2012. Community consensus: Design beyond participation. Design Issues, 28:89100. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2022. Hate Speech and Counter Speech Detection: Conversational Context Does Matter. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 59185930, Seattle, United States. Association for Computational Linguistics. Jiangrui Zheng, Xueqing Liu, Mirazul Haque, Xing Qian, Guanqun Yang, and Wei Yang. 2024a. HateModerate: Testing hate speech detectors against content moderation policies. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 26912710, Mexico City, Mexico. Association for Computational Linguistics. Jiangrui Zheng, Xueqing Liu, Guanqun Yang, Mirazul Haque, Xing Qian, Ravishka Rathnasuriya, Wei Yang, and Girish Budhrani. 2024b. Hatemoderate:"
        },
        {
            "title": "A Human Annotations",
            "content": "Model F1 Score Accuracy We use Prolific9 to recruit annotators. We prescreen annotators with (a) approval rate: 50-100% (b) fluency in English (c) must reside in the U.S. Additionally, we introduce attention checks in the HIT to ensure high quality annotations. We rely on Prolifics comprehensive prescreeners and demographic information to stratify annotators as ingroup and outgroup. Annotators were compensated at the rate of 16/hr and annotators spend an average of 25 minutes per HIT (see 9 for more details). Figure 4 shows the annotation framework. We present detailed demographics of the ingroup annotators in Table 9."
        },
        {
            "title": "B Additional Results on\nMODELCITIZENS",
            "content": "In Table 8, we report the F1 scores of all models and the average accuracy for comparison. We find that OpenAI Moderation, Perspective API and Llama-Guard-3 have lower F1 scores despite good accuracy (see Table 4) due to high false negative rates for these models. We see that GEMMACITIZEN-12B and LLAMACITIZEN-8B achieve good F1 scores on our dataset highlighting the robustness of our framework."
        },
        {
            "title": "C Additional Implementation Details",
            "content": "In this section, we provide additional details about our implementation, data preparation, prompts and hyperparameters of training LLAMACITIZEN-8B. C.1 General Implementation Details All of our experiments were conducted on an NVIDIA RTX H100 machine with support for 8 GPUs. Full fine-tuning runs took about 30 minutes to complete using distributed training on 2 GPUs. Our base LLM for all experiments was LLaMA-3.1-8B-Instruct (Grattafiori et al., 2024), specifically its Huggingface release 10. C.2 Fine-Tuning Details We utilize LLaMA-Factory (Zheng et al., 2024c) for finetuning the base LLM and curator (Marten et al., 2025) inference. We train LLAMACITIZEN-8B and GEMMACITIZEN-12B for 10 epochs on learning rate of 1e-5 with gradient accumulation of 2 and batch size 32. These fast for GPT-4o Gemini-2.0-Flash GPT-o4-mini Perspective API OpenAI Moderation Llama-Guard-3-8B Qwen-2.5-7B-Instruct LLaMA3.1-8B-Instruct Gemma-3-12B-IT GEMMACITIZEN-12B LLAMACITIZEN-8B 68.97 69.61 71.2 20.6 32.2 56.3 58.3 64.7 64.7 71.2 71.8 67.9 69.2 69.7 58.6 63.6 68.7 58.7 64.3 65.8 74.7 75.2 Table 8: F1 score of models on MODELCITIZENS. hyperparameters were finalized after systematically analyzing model performance across various learning rates (5e-7, 5e-6, 2e-6, 1e-4), batchsizes and epochs. C.3 Prompt Following prior work using LLMs for toxicity detections (Santy et al., 2023; Patronus, 2024; Weng et al., 2023), we use the following prompt for training and inference across all models: You will be provided with statement and some context. Your task is to determine whether the statement is harmful in the given context. If no context is provided, it will be marked as \"NA.\" In such cases, make your decision based solely on the statement. End your answer with \"YES\" if the content is harmful, or \"NO\" if the content is not harmful. CONTEXT: {context} STATEMENT: {statement} REPLY: {reply} C.4 Baseline Implementations For Perspective API11 and OpenAI Moderation API12, we collect toxicity scores by concatenating the context, statement and reply for samples with context and only on the statement when there is no context. We use threshold of 0.7 for Perspective API to convert toxicity scores to binary classes of toxic or non-toxic. We prompt LLaMA3.1-8B-Instruct, Qwen-2.5-7B-Instruct, Gemma-312B-IT, Qwen-2.5-32B-Instruct, GPT-4o, GPT-o4mini and Gemini-2.0-Flash with the prompt from 9www.prolific.com 10https://huggingface.co/meta-llama/Lla 11https://perspectiveapi.com/ 12https://platform.openai.com/docs/gui ma-3.1-8B-Instruct des/moderation C.3(temperature=0.0). For Llama-Guard-3-8B, we prepend our prompt with the safety taxonomy of LLama-Guard."
        },
        {
            "title": "MODELCITIZENS",
            "content": "D.1 Sampling from Toxigen We sample posts from the human validated subset of Toxigen comprising of 8.9K posts. We only use the train set to prevent contamination of the Toxigen test set. We find 5K posts spanning the selected 8 identity groups in Section 3. We then use GPT4o to identify well-formed and most informative samples from Toxigen using given prompt: You will be provided with statement and some context. Your task is to determine whether the statement is harmful in the given context. If no context is provided, it will be marked as \"NA.\" In such cases, make your decision based solely on the statement. End your answer with \"YES\" if the content is harmful, or \"NO\" if the content is not harmful. CONTEXT: {context} STATEMENT: {statement} REPLY: {reply} After GPT-4o filtering, we balance the dataset for equal toxic and non-toxic samples based on Toxigen toxicity scores. Finally, we are left with 2,520 high quality balanced samples from Toxigen that form MODELCITIZENS. D.2 Generating Contexts with GPT-4o We use GPT-4o to generate conversational context for the 2,520 collected from Toxigen. We generate an offensive context and non-offensive context by using the following prompt: Reddit discussions often feature strong opinions, either reinforcing harmful beliefs or challenging them. Given the following statement, generate realistic Reddit-style conversation where this statement appears as comment. The discussion should be framed as : 1. Offensive Framing: The context should make the statement appear as part of hostile, conspiratorial, or discriminatory discussion. It should be written in way that reinforces toxic beliefs, mimicking how such discussions unfold naturally on certain Reddit threads. Guidelines: - The conversation should follow offensive framing only. - Structure the response like real Reddit thread with: - brief post title (e.g., r/Politics , r/UnpopularOpinion, r/Conspiracy, r/ ChangeMyView) - context-setting introduction ( short post body) - The given statement as comment. Do not modify the statement. - At least one reply that align with the chosen framing. - Maintain natural language and Redditstyle interactions (e.g., casual tone, upvote/downvote references, sarcasm, or internet slang where appropriate). - Strcture your output as JSON Example Input: \"Immigrants are taking all our jobs.\" Example Output (Offensive Framing - r/ Conspiracy) - Post Title: \"The real reason wages are down...\" - Context: \"They tell us its just the economy, but lets be real walk into any job site, and guess whos working? Not Americans.\" - Comment: \"Immigrants are taking all our jobs.\" - Reply 1: \"Exactly. And the government just lets it happen because they want cheap labor.\" Statement: {statement} Reddit discussions often feature strong opinions, either reinforcing harmful beliefs or challenging them. Given the following statement, generate realistic Reddit-style conversation where this statement appears as comment. The discussion should be framed as : 1. Non-hostile, Non-hateful Framing : The context should make the statement appear as part of neutral or constructive discussion such as quoting it as an example of common misconception, sharing it as part of \" why do people say this?\" question, mentioning it from character in story or anecdote or asking for clarification on its meaning or implications. The goal is to make the statement appear in discussion without violating content moderation rules or sounding offensive, while still allowing for thoughtful discussion. Guidelines: - The conversation should follow Nonhostile, Non-hateful Framing only. - Structure the response like real Reddit thread with: - brief post title (e.g., r/Politics , r/UnpopularOpinion, r/Conspiracy, r/ ChangeMyView) - context-setting introduction ( short post body) - The given statement as comment. Do not modify the statement. - At least one reply that align with the chosen framing. - Maintain natural language and Redditstyle interactions (e.g., casual tone, upvote/downvote references, sarcasm, or internet slang where appropriate). - Structure your output as JSON object . Example Input: Statement: \"Immigrants are taking all our jobs.\" Target group: \"Asian\" Example Output (Non-hostile, Nonhateful Framing - r/ChangeMyView) - Post Title: \"CMV :Immigrants arent stealing jobs corporations are.\" - Context: \"I keep seeing this argument that immigrants take jobs, but thats huge oversimplification. Lets talk about it.\" - Comment: \"Immigrants are taking all our jobs.\" - Reply 1: \"Thats myth. The real issue is companies exploiting cheap labor while keeping wages low for everyone.\" Statement: {statement} Target group: {group} D.3 Human Validation of Generated Contexts We recruit ingroup annotators to validate the quality of the generated contexts. They are instructed to rate the generated context for a) naturalness - the generated context should follow the natural flow of the reddit thread and accurately mimic real reddit posts, and b) plausibility - the generated context sounds reasonable in the context of the given post and the target group : Is this post realistic or plausible in the context of given statement? 1. Very unrealistic/implausible 2. Moderately realistic/plausible 3. Very realistic/plausible 85We find that 85% of the generated contexts were rated realistic and plausible, thus, we are left with 4,302 context augmented posts. We combine the 2,520 single post samples and the 4,302 context augmented samples to form MODELCITIZENS. Figure 4: Annotation interface implemented using potato for toxicity annotation. Religious Affiliation Ethnicity LGBTQ+ Sex Asian Black Non-Religious - 56.7% Christianity - 43.3% Non-Religious - 3.1% Christianity - 96.9% Asian - 100% Black - 100% Jewish Judaism - 100% White - 100% Latino LGBTQ+ Mexican Non-Religious - 56.7% Christianity - 43.3% Non-Religious - 37.5% Christianity - 62.5% Islam - 2.5% Non-Religious - 48% Christianity - 52% Muslim Islam - 100% Women Non-Religious - 3.4% Christianity - 93.3% No - 80% Yes - 20% No - 65.6% Yes - 34.4% No - 83.4% Yes - 16.6% No - 75% Yes - 25% Yes - 100% Male - 66.7% Female - 33.3% Male - 43.7% Female - 56.3% Male - 90% Female - 10% Male - 65% Female - 35% Male - 76.6% Female - 53.4% Latino/Hispanic - 100% White - 62% Black - 32.5% Asian - 2.5% Latino/Hispanic - 100% No - 80% Yes - 20% Male - 66.7% Female - 33.3% White - 37.0% Black - 25.9% Asian - 18.5% Middle Eastern - 14.8% African - 3.7% White - 53.3% Black - 43.3% Asian - 3.4% No - 80% Yes - 20% Male - 66.7% Female - 33.3% No - 80% Yes - 20% Female - 100% Table 9: Demographic Distribution of ingroup annotators for each of the 8 identity groups. These attributes are based on the Prolific screeners and their corresponding response choices. Figure 5: Ingroup and Outgroup Annotators show statistically significant differences in rating distributions. MODELCITIZENS reveals that ingroup and outgroup rating distributions vary significantly across identity groups. For instance, outgroup annotators are more likely to rate content targeting black individuals as Extremely Harmful than ingroup as evident by the violin plots here."
        }
    ],
    "affiliations": [
        "Google",
        "New York University",
        "University of California, Los Angeles",
        "University of Virginia"
    ]
}