{
    "paper_title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation",
    "authors": [
        "Weipeng Tan",
        "Chuming Lin",
        "Chengming Xu",
        "FeiFan Xu",
        "Xiaobin Hu",
        "Xiaozhong Ji",
        "Junwei Zhu",
        "Chengjie Wang",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio's inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method's superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities."
        },
        {
            "title": "Start",
            "content": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation Weipeng Tan1 Chuming Lin2 Chengming Xu2 Feifan Xu2 Xiaobin Hu2 Xiaozhong Ji2 Junwei Zhu2 Chengjie Wang2 1Fudan University 2Youtu Lab, Tencent, China Yanwei Fu1 Abstract Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audios inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identityagnostic Gaussian distributions. Second, we introduce correlationenhanced emotion conditioning module with learnable emotion banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our methods superiority, outperforming state-of-theart approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our methods ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities. Code is available at GitHub. Keywords Talking head generation, emotion control, diffusion model 5 2 0 2 5 2 ] . [ 1 7 8 0 8 1 . 4 0 5 2 : r Equal contribution. Corresponding author. Figure 1: Our DICE-Talk, as the first emotional THG method based on diffusion models, can generate various emotions while well preserving the identity characteristic, thus largely benefiting the real-life application of digital human."
        },
        {
            "title": "1 Introduction\nRecent advancements in generative models have shed light on gen-\nerating high-quality and realistic videos under various controlling\nconditions such as texts [23], images [2], videos [38], etc. Among\nall the different subtasks of video generation, Talking Head Genera-\ntion (THG), as a human-centric task which aims to generate videos\nof talking heads guided by conditions such as speech and images,\nhas emerged as a significant problem due to its wide application in\nscenarios such as digital humans, film production, virtual reality.\nDespite importance, the extreme challenges result from its low tol-\nerance to artifacts in general and its demand of high fidelity in lip\nshapes, facial expressions, and head motions.",
            "content": "Following the commonly used generative models, GAN-based THG methods [21, 42] have achieved remarkable results in generating high-resolution videos through adversarial training between generators and discriminators, particularly in terms of visual quality and lip-sync accuracy. Diffusion model-based THG methods [25, 27], on the other hand, excel in generating high-quality and high-resolution images and videos, and it outperforms GANs in terms of the stability and consistency of the generated content, thus becoming the mainstream methods for THG. These methods xx, xx, xx largely facilitate THG by strengthening the explicit controlling conditions such as facial keypoints and head motion sequences. Yet, critical gap persists: none of these methods holistically model emotion as controllable, multimodal signal. Ignoring such condition forces models to generate \"emotionally flat\" outputs, limiting their applicability in fields like mental health support, where emotional resonance is critical. While it is straightforward to transfer the previous exploration on emotion control such as EAMM [13] and StyleTalk [19] to diffusion backbones, these methods are generally restricted with three main omissions leading to incomprehensive control. (1) Wrongly dominant visual emotion. EAMM and StyleTalk propose adopting videos as the emotion source. However, audio as the elementary input of THG contain rich affective emotion cues such as the prosody and spectral tilt. For example, one can easily tell apart if someone is happy or sad by only listening to him speaking. Lack of such information can severely haunt the comprehensiveness of the emotion representation. (2) Leaked identity characteristics. Existing methods often fail to disentangle emotion from speaker identity, allowing unintended facial features (e.g., unique expressions or micro-movements) to leak into the generated output. This issue stems from treating emotions as fixed, universal attributes while overlooking the identity-specific volatility. For instance, the same emotion (e.g., happiness) may manifest differently across individuals due to unique facial structures or habitual mannerisms. Without explicit modeling of this volatility, transferred emotions may retain source-specific artifacts, compromising both generalization and fidelity when applied to novel identities. Failure to take into account such fact degrades the generalizability of emotion transfer, especially when adapting to unseen identities. (3) Isolated emotion learning. Previous methods generally produce emotion representation based on single source video without extra knowledge. However, different emotions are actually correlated with each other. For example, learning emotions like sad and contempt can help model better understand other negative emotions such as disgusted. Such correlation, anyhow, is yet to be explored. To this end, we propose novel framework named DICE-Talk, which can effectively extract well-disentangled emotion features with the assistance of audio information through self-supervised method, and apply them to the generation of talking head videos in manner suitable for diffusion models. This approach not only improves the overall quality of the generated videos, ensuring better synchronization and control but also accurately transfers facial expressions and individualized details to new faces. Specifically, our DICE-Talk focuses on two main problems, i.e. extracting emotion embeddings and controlling diffusion models with such embeddings. For emotion embedding extraction, we propose the novel disentangled emotion embedder as an alternative to the previous methods such as StyleTalk based on the transformer backbone. Concretely, the audio and visual information of each video interacts with each other in the transformer emotion encoder, which models the emotion-related information of this video as Gaussian distribution with predicted mean and standard deviation, aiming to simultaneously learn the general emotion representation and the identity-specific volatility of this video. Through contrastive learning, the extracted features exhibit significant clustering across different identities and emotions, not only helping the model better understand the video content but also providing an effective way to capture and express the emotion of individuals. In order to control the denoising process with the well learned emotion representation, we propose leveraging an emotion bank module to fully explore the intrinsic correlation between different emotions to boost each other. Specifically, this emotion bank first imports knowledge from the extracted emotion representations, from which the informative ones are then aggregated and incorporated in the diffusion UNet through cross attention. This enables the model to actively utilize the emotion source from the training set, which leads to strong generalization ability of the trained model. To encourage the model to be better controlled by the emotion condition, we further utilize an emotion discrimination objective apart from the noise prediction loss. Our objective incorporates an emotion discriminator trained to classify affective states from intermediate noisy latents of the diffusion UNet, ensuring precise control without sacrificing lip-sync accuracy or visual fidelity. To validate the effectiveness of our proposed method, we conduct extensive experiments and comparisons on the MEAD [33], HDTF [43] and out-of-domain datasets. Our method significantly outperforms other competitors across multiple metrics, including FVD [28], FID [11], SyncNet (Sync-C and Sync-D) [8] and EmoScores. In addition to quantitative evaluation, we also perform comprehensive qualitative assessments. The results indicate that our method can generate highly natural and expressive talking videos and produce different emotions or even multiple changes in expressions within the same video according to user needs, achieving satisfactory visual effects. Overall, our contributions are summarized as follows: We propose DICE-Talk, the first audio-driven diffusion-based THG framework that disentangles speaker identity from emotional expressions. This enables realistic talking head generation with rich emotional expressions while preserving identity fidelity. We introduce cross-modal disentangled emotion embedder, representing emotions as identity-agnostic Gaussian distributions to address identity leakage and leverage audios inherent prosodic cues. correlation-enhanced emotion conditioning mechanism is proposed, which leverages learnable emotion bank to capture inter-emotion relationships."
        },
        {
            "title": "2 Related Work\nGAN-Based Talking Head Generation. There has been signif-\nicant research on GAN-based methods for person-generic audio-\ndriven talking head generation. Early methods [6, 21, 32] achieved\nlip synchronization by establishing a discriminator that correlates\naudio with lip movements. Other approaches [34, 35, 42, 44, 45]\ngenerated portrait videos by mapping audio to key facial infor-\nmation, such as landmarks, key points, or 3D Morphable Model\n(3DMM) [1] coefficients, before rendering the final frame. However,\ndue to the limitations of GANs in terms of generative capacity, the\nresults produced by these methods often suffer from artifacts like\npseudo-textures or restricted motion ranges.\nDiffusion Model-Based Talking Head Generation. Recently,\nthere has been a surge of research [9, 12, 15, 25, 27, 31, 36, 39â€“41]",
            "content": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation xx, xx, xx utilizing diffusion models to achieve high-quality portrait videos. Among these, X-Portrait [39] and MegActor [41] rely on the pose and expression from the source video to generate the target video, which limits their ability to produce videos based solely on audio. DiffTalk [25] was the first to modify lip movements using audio and diffusion models, but it does not extend to driving other head parts. EMO [27] was the first to leverage LDM [23] and audio features to achieve overall motion in portraits. V-Express [31] controls the overall motion amplitude by adjusting audio attention weights, while Hallo [40] designed hierarchical module to regulate the motion amplitude of different regions. Sonic [12] focus on global audio perception to obtain better talking head results. In summary, current audio-driven diffusion model approaches have not taken into account that each portrait should exhibit corresponding emotion while speaking, which is essential for generating higherquality portrait videos. Emotional Talking Head Generation. Previous research has explored several GAN-based methods [10, 13, 14, 18, 19, 26, 33, 37] for extracting style information to apply in talking head generation. MEAD [33] and Emotion [26] directly inject style labels into the network to drive the corresponding emotions. GC-AVT [18] and EAMM [13] map the facial expressions of each frame in the source video to each frame in the target video. LSF [37] and StyleTalk [19] employ 3D Morphable Models (3DMM) to extract facial information and construct style codes that drive the desired styles. EAT [10] achieves emotion control through parameter-efficient adaptations. Building on these approaches, our framework is first to disentangle identity and cooperate emotion to generate better emotion features into diffusion model and produce high-quality emotional talking portrait videos."
        },
        {
            "title": "3 Method\nProblem Formulation. The goal of THG is to generate a talking\nhead video under the control of a reference portrait image, audio,\nand emotion prior. Among these conditions, the reference portrait\nimage provides the background and facial identity, the audio guides\nthe overall head motion and lip movements. In addition, we propose\na disentangled emotion embedder to extract the emotion prior from\nthe visual and audio content of an emotion reference video, which\nis used to determine facial emotion and speaking habit.\nPreliminaries. In DICE-Talk, we employ a Stable Video Diffusion\n(SVD) [2] with EDM [16] framework to generate video frames. The\nSVD uses a diffusion and denoising process in the latent space via\na Variational Autoencoder (VAE). Given a video ğ‘¥ âˆˆ Rğ‘ Ã—3Ã—ğ» Ã—ğ‘Š ,\nit maps each frame into the latent space, encoding the video as\nğ‘§ = ğ¸ (ğ‘¥), which helps maintain visual quality while reducing\ncomputational cost. During the diffusion process, Gaussian noise\nğœ– âˆ¼ N (0, I) is gradually introduced into the latent ğ‘§, degrading\nit into complete noise ğ‘§ğ‘‡ âˆ¼ N (0, I) after ğ‘‡ steps. In the reverse\ndenoising process, the target latent ğ‘§ is iteratively denoised from\nthe sampled Gaussian noise using the diffusion model and then\ndecoded by the VAE decoder ğ· into the output video ğ‘¥ = ğ· (ğ‘§).\nDuring training, given the latent ğ‘§0 = ğ¸ (ğ‘¥0) and condition ğ‘, the\ndenoising loss is:",
            "content": "Lğ‘‘ğ‘’ğ‘›ğ‘œğ‘–ğ‘ ğ‘–ğ‘›ğ‘” = Ezğ‘¡ ,ğ,c,ğ‘¡ ğğœƒ (zğ‘¡ , c, ğ‘¡) ğğ‘¡ 2. (1) Among them, ğ‘§ğ‘¡ represents the noisy latent variables at timestep ğ‘¡ [1,ğ‘‡ ], and ğœ–ğ‘¡ is the added noise. ğœ–ğœƒ is the noise predicted by the UNet model, modified using an attention mechanism with parameters ğœƒ . This model employs cross-attention mechanism to fuse the condition ğ‘ with the latent features ğ‘§ğ‘¡ , thereby guiding the video generation. Overview. As depicted in Figure 2, our DICE-Talk consists of three important designs: (1) disentangled emotion embedder that extracts emotions from video and audio inputs, representing them as Gaussian distribution. (2) correlation-enhanced emotion conditioning module that stores emotion features in bank and injects them into diffusion model. (3) An emotion discrimination objective that improves emotion expression by classifying emotions during denoising. The diffusion process combines reference image, audio features, emotion data, and the emotion bank to guide facial animation. Audio features come from Whisper-Tiny [22], and we use an attention layer for processing following Ji et. al. [12]. Finally, after denoising, VAE decoder generates expressive portrait frames with the target emotion."
        },
        {
            "title": "3.1 Disentangled Emotion Embedder\nIn order to make full use of the related factors indicating emotion of\na speaker to achieve the well-disentangled emotion embeddings, we\nadopt a novel framework named disentangled emotion embedder.\nConcretely, for a ğ‘ -frame video clip V consisting of the frame set\nVğ›½ = {Ii}ğ‘\nğ‘–=1 and its corresponding audio Vğ›¼ , we first extract the\naudio and visual features respectively. For the visual information,\nwe follow StyleTalk to encode Vğ›½ into feature sequence ğ‘ ğ›½ âˆˆ Rğ‘ Ã—ğ‘‘ğ‘  .\nBeyond that, we further stack pretrained Whisper-Tiny [22] with\nadditional learnable transformer layers to extract sequential audio\nfeature ğ‘ ğ›¼ âˆˆ Rğ‘ Ã—ğ‘‘ğ‘  from Vğ›¼ .",
            "content": "After achieving features for each modality, our disentangled emotion embedder further merges audio information into the visual one through cross-attention mechanism. Specifically, for each layer, ğ‘ ğ›¼ is updated as follows: ğ‘„ğ›¼ = ğ‘Šğ‘„ (ğ‘ ğ›¼ ); ğ¾ğ›½ = ğ‘Šğ¾ (ğ‘ ğ›½ ); ğ‘‰ğ›½ = ğ‘Šğ‘‰ (ğ‘ ğ›½ ), Ë†ğ‘ ğ›¼ = ğ‘ ğ›¼ + ğœ†CA(ğ‘„ğ›¼, ğ¾ğ›½, ğ‘‰ğ›½ ), (2) (3) where CA(, , ) denotes cross attention. In this way, ğ‘ ğ›¼ is entitled with the awareness of dual modalities related to human emotion. With the fused Ë†ğ‘ ğ›¼ , we then propose modeling the emotion condition for each video as Gaussian distribution. Specifically, an attention-based aggregation strategy is employed on Ë†ğ‘ ğ›¼ as follows: (4) ğœ‡ğ‘  = softmax(ğ‘Šğ‘  Ë†ğ‘ ğ›¼ ) Ë†ğ‘ ğ‘‡ ğ›¼ , ğ›¼ ğœ‡ğ‘  )2, ğœ2 ğ‘  = softmax(ğ‘Šğ‘  Ë†ğ‘ ğ›¼ ) ( Ë†ğ‘ ğ‘‡ ğœ– (0, I), ğ‘  = ğœ‡ğ‘  + ğœğ‘  ğœ–, where ğ‘Šğ‘  R1ğ‘‘ğ‘  is trainable parameter, ğœ‡ğ‘ , ğœ2 and variance of the learned emotion prior ğ‘ . ğ‘  denotes the mean (5) (6) Compared with StyleTalk, our proposed model mainly enjoys the following merits: (1) Unified audio-visual emotion modeling. As mentioned in Sec. 1, audio should be taken as an important indicator of emotion. We design specific structure to handle these complex data, considering both visual and audio information, leading to stronger emotion embedding. (2) Identity-emotion disentanglement. Models such as StyleTalk cannot learn to fully disentangle xx, xx, xx Figure 2: Framework of DICE-Talk. Our method comprises three key components: disentangled emotion embedder, correlationenhanced emotion conditioning, and emotion discrimination objective. These architectural elements work synergistically to decouple identity representations from emotional cues while preserving facial articulation details, thereby generating lifelike animated portraits with emotionally nuanced expressions. identity information from emotion features due to insufficient capability via solely modeling deterministic emotion features. Therefore, such identity-related features would be leaked into the emotion embedding. Our method, on the other hand, model the emotion prior as Gaussian distribution that is more representative, thus learning better disentangled sequential embeddings. Embedder Training. Essentially, codes with similar emotions should cluster together in the emotion space. Therefore, we apply contrastive learning to the emotion priors by constructing positive pairs (ğ‘ , ğ‘ ğ‘ ) with the same identity and emotion, and negative pairs (ğ‘ , ğ‘ ğ‘›) with different identities or emotions. Then, the InfoNCE loss [4] with similarity metric ğœ is enhanced between positive and negative sample pairs: ğœ” ( ğ‘ ) = exp(ğœ (ğ‘ , ğ‘ )/ğœ), Lğ‘ğ‘œğ‘› = log (cid:18) ğœ” (ğ‘ ğ‘ ) ğœ” (ğ‘ ğ‘ ) + (cid:205)ğ‘ ğ‘› Sğ‘› ğœ” (ğ‘ ğ‘›) (cid:19) , (7) (8)"
        },
        {
            "title": "Conditioning",
            "content": "In order to control the emotion of talking head videos, straightforward way is to directly inject ğ‘  achieved in Eq. 6 into SVD through cross attention, following other conditional diffusion models such as Sonic [12]. However, while ğ‘  can well depict the emotion condition of each video clip, it still cannot well handle the internal correlation among different emotions, which leads to limited performance. To address the problem, we propose correlation-enhanced emotion conditioning process via an emotion bank attention layer. Rather than directly leveraging information contained in ğ‘  as condition, we instead utilize an emotion bank Rğ¾ ğ‘‘ğ‘  to first refine the emotion information. Specifically, during training, following previous works on discrete encoding, emotion memory ğ‘  is retrieved from based on the extracted emotion embedding ğ‘ , whose information is further aggregated according to ğ‘ : ğ‘  Cğ‘– 2 2, ğ‘  = Cğ‘˜ ; ğ‘˜ = arg min ğ‘– ğ‘„ğ‘  = ğ‘Šğ‘„ (ğ‘ ); ğ¾ğ‘  = ğ‘Šğ¾ (ğ‘ ); ğ‘‰ğ‘  = ğ‘Šğ‘‰ (ğ‘ ), ğ¸ğ‘† = CA(ğ‘„ğ‘ , ğ¾ğ‘ , ğ‘‰ğ‘  ), (9) (10) (11) where ğœ denotes temperature parameter, Sğ‘› denotes all negative samples for ğ‘ , and ğœ (ğ‘ ğ‘–, ğ‘  ğ‘— ) is the cosine similarity between sample pairs (ğ‘ ğ‘–, ğ‘  ğ‘— ). We additionally add fixed constant to stabilize the numerical range of the similarity and make the training process more stable. In the training of the disentangled emotion embedder, we directly train all parameters of this lightweight model. Meanwhile, we use random dropout trick when inputting emotion features ğ‘ ğ›½ and audio features ğ‘ ğ›¼ by setting some of the emotion features ğ‘ ğ›½ or audio features ğ‘ ğ›¼ to zero. This allows the model to obtain the emotion prior through single modality. By training with objectives such as commitment loss [29], it learns to memorize the shared emotion-related knowledge among the training set. Intuitively, embeddings with the same emotion would be clustered into similar ğ‘ , which in turn reduces the effect of identity leakage. Consequently, the retrieval process matches the appropriate feature distribution for the specific input, thereby expressing the corresponding emotion. Additionally, this method can express entirely new emotions through combinations of different emotional features, providing better generalization. Moreover, the training process can encourage ğ‘Šğ‘„,ğ‘Šğ¾ ,ğ‘Šğ‘‰ to discover latent subspaces where emotion correlations are salient, which further Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation xx, xx, xx benefits the generation process: After achieving well-trained C, the retrieval process can be skipped during inference, i.e. replacing ğ‘  in Eq. 10 with C, i.e. ğ‘„ğ‘  = ğ‘Šğ‘„ (ğ‘ ); ğ¾ğ‘ğ‘™ğ‘™ = ğ‘Šğ¾ (C); ğ‘‰ğ‘ğ‘™ğ‘™ = ğ‘Šğ‘‰ (C), ğ¸ğ‘† = CA(ğ‘„ğ‘ , ğ¾ğ‘ğ‘™ğ‘™ , ğ‘‰ğ‘ğ‘™ğ‘™ ). (12) (13) In this way, the cross attention as in Eq. 11 allowing ğ‘  to attend to multiple relevant prototypes (e.g., both \"anger\" and \"disgust\" for aggressive expressions). Emotion Projection. To utilize the emotion feature ğ¸ğ‘  obtained in Sec. 3.2 and guide the denoising process, then ğ¸ğ‘  is injected into the diffusion UNet through an additional emotion attention layer, where it interacts with other features via cross-attention mechanism to supplement additional facial details such as expressions and speaking habits. The information of the emotion feature ğ¸ğ‘  can be injected into the spatial and temporal cross-attention layers to provide spatial knowledge as follows: ğ‘§ğ‘  = ğ‘§ğ‘ + CA (ğ‘„ (ğ‘§ğ‘), ğ¾ (ğ¸ğ‘  ), ğ‘‰ (ğ¸ğ‘  )) , where ğ‘§ğ‘ is the spatial latent features after being injected with reference attention and audio attention, and ğ‘§ğ‘  is the adjusted spatial features guided by emotion prior spatial-aware level. (14)"
        },
        {
            "title": "3.3 Compositional Diffusion Adaptation\nTo finetune the newly introduced parameters in SVD, we adopt a\ncompositional objective as follows",
            "content": "(15) = Lğ‘‘ğ‘’ğ‘›ğ‘œğ‘–ğ‘ ğ‘–ğ‘›ğ‘” + ğœ†Lğ‘ğ‘™ğ‘  + Lğ‘£ğ‘, where Lğ‘‘ğ‘’ğ‘›ğ‘œğ‘–ğ‘ ğ‘–ğ‘›ğ‘” denotes the noise prediction loss as in Eq. 1, Lğ‘ğ‘™ğ‘  and Lğ‘£ğ‘ respectively denote emotion discrimination objective and emotion bank learning objective, which will be detailed below. Emotion Discrimination Objective. To promote the model to generate more precise emotions, we introduce the emotion discrimination objective. Formally, denoting ğ‘“ğ‘¡ R(ğ¶ ğ» ğ‘Š ) ğ‘‘ as the intermediate features extracted from the U-Net at timestep ğ‘¡, where ğ¶, ğ»,ğ‘Š represent channel, height, and width dimensions respectively. These features are first compressed by an adaptive pooling layer ğœ™ into fixed-dimensional vector ğ‘£ğ‘¡ = ğœ™ (ğ‘“ğ‘¡ ), which is then projected through an MLP classifier ğœ“ to obtain the emotion probability distribution ğ‘ğ‘¡ = softmax (ğœ“ (ğ‘£ğ‘¡ )) The auxiliary cross-entropy loss Lğ‘ğ‘™ğ‘  is computed as: Lğ‘ğ‘™ğ‘  = 1 ğ‘‡ ğ‘‡ ğ‘ ğ‘¦ğ‘ log ğ‘ğ‘¡,ğ‘ (16) ğ‘=1 ğ‘¡ =1 where ğ‘‡ is the total number of sampled timesteps, ğ‘ denotes the number of emotion categories, ğ‘¦ğ‘ is the ground-truth one-hot label for category ğ‘ , and ğ‘ğ‘¡,ğ‘ represents the predicted probability for category ğ‘ at timestep ğ‘¡. Lğ‘ğ‘™ğ‘  guides the model to focus on the accuracy of emotions during content generation, thus helping the model better capture emotion-related information in the noisy latent space. Emotion Bank Learning. The emotion bank is optimized through vector quantization loss Lğ‘£ğ‘, which jointly refines both the emotion prototypes and their association with input features. Given an input emotion prior and the codebook ğ¶ , the loss function operates bidirectionally: The codebook commitment loss sg[ğ‘ ] ğ¶ğ‘˜ 2 2 updates only the selected prototype ğ¶ğ‘˜ (where ğ‘˜ = arg minğ‘– ğ‘  ğ¶ğ‘– 2 2) toward the input feature using stop-gradient operation sg[], while the feature alignment loss ğ‘  sg[ğ¶ğ‘˜ ] 2 drives the input features toward their nearest prototype without modifying the codebook. The composite loss: Lğ‘£ğ‘ = sg[s] Cğ‘˜ 2 + ğ›½ sg[Cğ‘˜ ] 2 2, (17) where ğ›½ is the commitment weight."
        },
        {
            "title": "4 Experiments\n4.1 Experiments Setting\nDICE-Talk is implemented using PyTorch [20] and optimized with\nAdam [17]. The disentangled emotion embedder is trained on the\nMEAD [33] and HDTF [43] datasets. During training, we consider\nsamples with the same identity and emotion in MEAD as positive\nsamples, and segments from the same video in HDTF as positive\nsamples. Additionally, we will randomly dropout expression coeffi-\ncients or audios, but they will not be zeroed out simultaneously.",
            "content": "The training dataset for the diffusion model includes both emotion dataset and regular video dataset, which are incorporated into the training process at 1:1 ratio. We use MEAD dataset as the emotion dataset, while the regular video datasets include HDTF, VoxCeleb2 [7], and others from the Internet. The training of the diffusion model consists of only one stage, with the initialization of the spatial module and the temporal modules from stable-video-diffusion-xt-1-1 [3]. To separately enable different conditions during training, we manipulate the data such that 5% of it drops audio, 5% drops image, 5% drops emotion prior, and 5% drops all conditions. For training and testing set splitting, we select 5 identities out of 46 for testing on MEAD. For HDTF, we randomly select 20 videos for testing. Precautions are taken to ensure that there is no overlap of character identities between the training and testing sets. To further evaluate generalization performance, we collected an additional out-of-domain dataset comprising 10 reference images and corresponding audio samples from diverse online sources for testing. We selected reference images of speakers with neutral emotions from the MEAD dataset and used videos of the same person with different emotions from the dataset to generate corresponding emotional videos. For the out-of-domain dataset, we randomly selected reference videos for different emotions. For the HDTF dataset, we used the original videos as emotional references to generate videos for the reference images. To ensure fair comparison with EAT, we adopted similar approach by preparing fixed feature for each emotion. Specifically, we used the mean of the emotion priors from large number of videos with the same emotion as the prompt feature for the corresponding emotion. During inference, we utilize [16] as diffusion sampler with the denoising steps set as 25 for all diffusion-based methods."
        },
        {
            "title": "4.2 Quantitative Comparison\nWe compare our method with several previous works, including\nSadTalker [42], AniPortrait [36], V-Express [31], Hallo [40], Hallo2 [9],\nEchoMinic [5], Sonic [12], EAMM [13], StyleTalk [19], and EAT [10].\nAmong these, EAMM, StyleTalk, and EAT are also focused on emo-\ntion modeling.",
            "content": "xx, xx, xx Figure 3: Visual comparisons with recent state-of-the-art taking head generation methods. Our method can obtain more accurate emotional expressions. Besides, we propose the full video comparison in supplementary materials to represent the capability of our method on sync, naturalness and stability. Figure 4: Visual results of user and ablation studies. (a): Visual comparisons with StyleTalk and EAT. (b): Comparison of visualization results with different emotion embeddings. (c): Emotion embeddings visualization. (d): Interpolation results between sad and happy emotions by controlling the emotion embedding. To demonstrate the superiority of the proposed method, we evaluate the model using several quantitative metrics. We utilize the FrÃ©chet Inception Distance (FID) [11] to assess the quality of the generated frames, and further employ the FrÃ©chet Video Distance (FVD) [28] for video-level evaluation. To evaluate lip-sync accuracy, Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation xx, xx, xx Table 1: Quantitative comparisons with the state-of-the-arts on the MEAD and Out of Domain test set. The best results are bold, and the second are underlined. Ours-V represents the emotion prior coming from separate emotional reference video, while Ours-P represents the emotion prior as fixed prompt feature we set for the corresponding expression. MEAD Out of Domain Emotion Testset Method Condition FVD FID Sync-C Sync-D Emo-Score Neutral Non-neutral Sync-C Sync-D Emo-Score Neutral Non-neutral EAMM StyleTalk Ours-V audio + video audio + video audio + video 944.3041 122.2852 498.3980 104.0158 54.0682 382. 4.1973 4.9014 6.2182 EAT Ours-P audio + prompt 629.4533 117.4011 52.9348 audio + prompt 377.0500 6.9590 6.1158 SadTalker V-Express AniPortait Hallo Hallo2 EchoMinic Sonic audio audio audio audio audio audio audio 83.4242 697.1468 61.8953 537.5223 633.7429 60.4030 326.7183 55.9922 57.6818 478.0882 616.0920 74.9136 370.7403 52.2722 5.1855 3.1232 1.1205 6.0092 6.3825 5.7066 6.7164 9.5079 10.0728 8.7805 8.2743 8.9969 8.8352 10.6003 12.5940 8.6960 8.6008 9.0599 8.4195 0.2220 0.4323 0. 0.4673 0.4865 0.2198 0.2136 0.2179 0.2131 0.2130 0.2219 0.2586 0.4007 0.5119 0.4659 0.3210 0.5373 0.7447 0.5399 0.4755 0.5257 0.5133 0.5417 0.4832 0.1624 0.4058 0. 0.5160 0.4696 0.0448 0.1048 0.1320 0.1089 0.1130 0.1153 0.1837 3.4361 3.2781 6.2720 4.4362 6.1751 5.0071 2.9307 1.1171 6.1672 6.1309 4.8856 6.9257 10.1884 11.1976 8. 10.0130 8.3802 8.6663 10.8661 12.2194 8.1545 8.2896 9.2830 7.3133 0.2411 0.4722 0.5424 0.3865 0.5527 0.2430 0.2356 0.2356 0.2367 0.2394 0.2466 0.2444 0.6106 0.4200 0. 0.5323 0.6281 0.7225 0.5768 0.5486 0.6304 0.5748 0.7237 0.6014 0.1179 0.4895 0.5101 0.3379 0.5276 0.0831 0.1219 0.1313 0.1055 0.1276 0.0875 0.1255 Table 2: Quantitative comparisons with the state-of-the-arts on the HDTF test set. Table 3: User study comparison on out of domain dataset."
        },
        {
            "title": "Condition",
            "content": "SadTalker V-Express AniPortait Hallo Hallo2 EchoMinic Sonic EAMM StyleTalk Ours-V audio audio audio audio audio audio audio audio + video audio + video audio + video FVD 450.5424 429.8870 246.2741 188.8993 261.6602 410.4968 157.1348 887.3187 491.8348 258.3758 EAT Ours-P audio + prompt audio + prompt 586.7120 223.9416 FID 15.6347 11.2261 9.3376 9.9675 9.5827 38.1664 9.0076 68.4042 52.7492 10.3833 56.3571 10.0854 Sync-C 5.7955 1.3750 0.5623 7.0955 7.4964 6.0018 7.7676 3.9795 5.4025 6.8049 Sync-D 8.5125 12.5764 13.5954 7.7918 7.9497 9.2090 7.3520 9.6646 9.3879 8. 7.3534 7.3218 7.6736 7.8157 we use the Sync-C and Sync-D of SyncNet [8]. To assess the accuracy of the generated facial expressions, we use the Emo-Score [24]. Specifically, we selected four easily recognizable emotionsneutral, happy, surprised, and angry (emotions like sad and fear are inherently more easily confused with other emotions)to evaluate the effectiveness of emotion generation. The average of these scores will be used as the overall Emo-Score. Additionally, we will compare the average scores of neutral and non-neutral emotions. As shown in Table 1, our method ranks among the top in metrics on both the MEAD and out-of-domain datasets. Our method achieved the best emotion scores, reflecting that our approach generates vivid and accurate facial expressions through emotion priors. In the quality assessment of videos, single-frame images, and lipsync accuracy, our method is close to the current best methods, as evidenced by the FVD, FID, and SyncNet scores. The results in Table 2 further indicate that our method maintains high visual quality even on datasets like HDTF, which do not have obvious emotions. These results demonstrate that our method can ensure video generation quality while achieving diversity and accuracy in emotional expression."
        },
        {
            "title": "Ours",
            "content": "1.3482 2.2679 2.2321 2.0536 3.7768 1.4286 1.7857 2.0357 3.9911 3.9286 1.3036 1.5893 1.6071 3.9911 3.9375 1.4643 1.7321 1.8571 3.9732 4. rich expressions and natural movements on open datasets, demonstrating strong robustness. Analysis results show that our method exhibits significant advantages in emotional expressiveness and visual quality. Compared to recent THG methods, our model achieves diverse emotional expressions while maintaining facial details, whereas ordinary models often appear monotonous and lack subtlety in emotional expression. Compared to emotional models, our method has advantages in the accuracy and naturalness of emotional expression, enabling smooth transitions between emotions, while other models may appear stiff during emotional transitions. Additionally, our model supports generating higher-resolution images, resulting in significantly better clarity and visual quality."
        },
        {
            "title": "4.4 User Study\nWe conducted a subjective evaluation of the results on open datasets,\nassessing the comparative methods along four dimensions: emo-\ntional expression, lip-sync accuracy, video quality, and video smooth-\nness. 20 participants rated the results of five comparative methods\non a scale of 1 to 5 in a total of 40 video sets. As shown in Table 3, our\nDICE-Talk significantly outperformed others in the dimensions of\nemotional expression and video smoothness, with a notable 66.5%\nimprovement in emotional expression. Although StyleTalk and\nEAT achieved good results in objective metrics, they compromised\nthe original identity and reduced visual quality while generating\nemotions, leading to lower subjective emotion scores. Figure 4(a)\nillustrates two clear examples. In terms of lip-sync accuracy and\nvideo quality, our method is very close to the state-of-the-art Sonic\nand far surpasses other methods that model emotions. Several video\nresults from the user study are included in the supplementary ma-\nterials.",
            "content": "xx, xx, xx Figure 5: Our visual results with different emotions and identities on out of domain dataset. Table 4: Ablation studies on our method."
        },
        {
            "title": "Methods",
            "content": "F-SIM Emo-Score Neutral Non-neutral w/o emotion bank w/o Lğ‘ğ‘™ğ‘  learnable CA w/o emotion prior"
        },
        {
            "title": "Ours",
            "content": "0.8775 0.8502 0.8622 0.8523 0.8704 0.4781 0.4762 0.5293 0.4992 0.5424 0.6986 0.6636 0.6754 0.6539 0. 0.4047 0.4138 0.4806 0.4477 0.5101 Table 5: Comparison of clustering strength on different emotions. The higher value means the better clustering effects."
        },
        {
            "title": "Input",
            "content": "angry contempt disgusted happy audio video 2.44 6.25 video + audio 6.57 2.12 4.89 5.64 2.51 7.45 7.63 2.49 8.07 8."
        },
        {
            "title": "4.5 Ablation Study\nEmotion Conditioning. We analyzed the impact of different emo-\ntion control methods on emotion conditioning effectiveness. We\ncompared three approaches: without using emotion bank, without\nusing the emotion discrimination objective, and compute cross at-\ntention between learnable embeddings and the emotion prior. For a\nmore detailed evaluation, we additionally included the F-SIM [27]\nto assess the facial similarity between the generated video and the\nreference image. The experiments show that our method effectively\nenhances emotional expression capabilities while better preserving\nthe identity of the reference image. Table 4 presents the experimen-\ntal results of these schemes, indicating that both the emotion bank\nlearning and the emotion discrimination objective significantly\ncontribute to emotional expression and identity preservation, as\nreflected in the optimization of the F-SIM and Emo-Score metrics.\nDisentangled Emotion Embedder. Table 5 provides a quantita-\ntive evaluation of the clustering strength of emotions after incorpo-\nrating audio features. We define clustering strength ğ‘‘ğ‘ğ‘™ğ‘  =\nas\nthe ratio between inter-cluster distance ğ‘‘ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ and intra-cluster dis-\ntance ğ‘‘ğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘. A larger ratio indicates better clustering performance.\nWe calculated the clustering strength of the emotion prior obtained\nunder three conditions: using only video features, using only audio",
            "content": "ğ‘‘ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘‘ğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘ features, and using both together, with different emotions of the same identity as categories. The results show that visual information plays crucial role in the extraction of emotion prior, while audio features can serve as an auxiliary to enhance the clustering strength of emotion prior. However, using only audio features is insufficient to obtain effective emotions. When training the diffusion model in Sec. 3.3, if we do not employ an emotion prior sampling from Gaussian distribution in Eq. 6 and instead use deterministic mean ğœ‡ğ‘  for training, it may lead to overfitting of the training results and lost identity information. Figure 4(b) shows the results of different emotion prior acquisition methods. Using deterministic prior causes the model to transfer (eye reflections/facial contours) from the emotion reference video to the new face, leading to issues with identity deviation. The results in Table 4 confirm this. By employing the disentangled emotion embedder, the emotion prior obtained by the model from the same training video varies each time, thereby preventing the transfer of incorrect content to the generated video. What Can We Learn from Emotion Prior? We use t-SNE [30] to project the emotion priors into two-dimensional space. Figure 4(c) shows the emotion priors of two speakers from the MEAD dataset. Each prior is color-coded according to its corresponding emotion and intensity. The emotion priors with the same emotion first cluster together. Within each cluster, the emotion priors with the same intensity are closer to each other and there are noticeable transitions between emotion priors of different intensities. These observations indicate that our model can learn continuous distribution of emotion embedding. As shown in Figure 4(d), when performing linear interpolation between two emotion embeddings extracted from the test set, the facial expressions and details in the generated video transition smoothly."
        },
        {
            "title": "5 Conclusion\nIn summary, we propose DICE-Talk, a method for generating talking\nhead videos with emotion control. By designing and training the\ndisentangled emotion embedder, we obtain emotion priors that can\nfully represent the emotions and habits of the emotion reference\nvideo. Through the correlation-enhanced emotion conditioning\nmodule, we further capture the relationships between emotions,",
            "content": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation xx, xx, xx thus better representing various emotions. Finally, with the design of the compositional diffusion adaptation, we successfully transfer emotion priors to unseen faces. Experimental results show that DICE-Talk not only achieves vivid and rich emotional expressions but also ensures the overall quality of the generated video, providing new insights for more advanced and comprehensive talking head video generation. References [1] Volker Blanz and Thomas Vetter. 1999. morphable model for the synthesis of 3D faces. In ACM SIGGRAPH. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. simple framework for contrastive learning of visual representations. In ICML. [5] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. 2024. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. arXiv preprint arXiv:2407.08136 (2024). [6] Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, and Nannan Wang. 2022. Videoretalking: Audio-based lip synchronization for talking head video editing in the wild. In ACM SIGGRAPH Asia. [7] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018. VoxCeleb2: Deep Speaker Recognition. In Interspeech 2018. ISCA. doi:10.21437/interspeech.20181929 [8] Joon Son Chung and Andrew Zisserman. 2017. Out of time: automated lip sync in the wild. In ACCV Workshops. [9] Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, and Jingdong Wang. 2024. Hallo2: Long-duration and high-resolution audio-driven portrait image animation. arXiv preprint arXiv:2410.07718 (2024). [10] Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, and Yi Yang. 2023. Efficient emotional adaptation for audio-driven talking-head generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2263422645. [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS. [12] Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, et al. 2024. Sonic: Shifting Focus to Global Audio Perception in Portrait Animation. arXiv preprint arXiv:2411.16331 (2024). [13] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne Wu, Feng Xu, and Xun Cao. 2022. Eamm: One-shot emotional talking face via audio-based emotionaware motion model. In ACM SIGGRAPH. [14] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, and Feng Xu. 2021. Audio-driven emotional video portraits. In CVPR. [15] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. 2024. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634 (2024). [16] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems 35 (2022), 2656526577. [17] Diederik Kingma and Jimmy Ba. 2015. Adam: method for stochastic optimization. In ICLR. [18] Borong Liang, Yan Pan, Zhizhi Guo, Hang Zhou, Zhibin Hong, Xiaoguang Han, Junyu Han, Jingtuo Liu, Errui Ding, and Jingdong Wang. 2022. Expressive talking head generation with granular audio-visual control. In CVPR. [19] Yifeng Ma, Suzhen Wang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Zhidong Deng, and Xin Yu. 2023. Styletalk: One-shot talking head generation with controllable speaking styles. In AAAI. [20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS. [21] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. 2020. lip sync expert is all you need for speech to lip generation in the wild. In ACM MM. [22] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In ICML. [23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In CVPR. [24] Elena Ryumina, Denis Dresvyanskiy, and Alexey Karpov. 2022. In Search of Robust Facial Expressions Recognition Model: Large-Scale Visual Cross-Corpus Study. Neurocomputing (2022). doi:10.1016/j.neucom.2022.10.013 [25] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, and Jiwen Lu. 2023. Difftalk: Crafting diffusion models for generalized audio-driven portraits animation. In CVPR. [26] Sanjana Sinha, Sandika Biswas, Ravindra Yadav, and Brojeshwar Bhowmick. 2022. Emotion-controllable generalized talking face generation. In IJCAI. [27] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. 2024. Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. In ECCV. [28] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, RaphaÃ«l Marinier, Marcin Michalski, and Sylvain Gelly. 2019. FVD: new metric for video generation. In ICLR Workshops. [29] Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances in neural information processing systems 30 (2017). [30] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research (2008). [31] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. 2024. V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation. arXiv preprint arXiv:2406.02511 (2024). [32] Jiadong Wang, Xinyuan Qian, Malu Zhang, Robby Tan, and Haizhou Li. 2023. Seeing What You Said: Talking Face Generation Guided by Lip Reading Expert. In CVPR. [33] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. 2020. Mead: large-scale audiovisual dataset for emotional talking-face generation. In ECCV. [34] Suzhen Wang, Lincheng Li, Yu Ding, Changjie Fan, and Xin Yu. 2021. Audio2head: Audio-driven one-shot talking-head generation with natural head motion. In IJCAI. [35] Suzhen Wang, Lincheng Li, Yu Ding, and Xin Yu. 2022. One-shot talking face generation from single-speaker audio-visual correlation learning. In AAAI. [36] Huawei Wei, Zejun Yang, and Zhisheng Wang. 2024. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694 (2024). [37] Haozhe Wu, Jia Jia, Haoyu Wang, Yishun Dou, Chao Duan, and Qingshan Deng. Imitating arbitrary talking style for realistic audio-driven talking face 2021. synthesis. In ACM MM. [38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023. Tune-avideo: One-shot tuning of image diffusion models for text-to-video generation. In ICCV. [39] You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. 2024. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH. [40] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao Yao, and Siyu Zhu. 2024. Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation. arXiv preprint arXiv:2406.08801 (2024). [41] Shurong Yang, Huadong Li, Juhao Wu, Minhao Jing, Linze Li, Renhe Ji, Jiajun Liang, and Haoqiang Fan. 2024. MegActor: Harness the Power of Raw Video for Vivid Portrait Animation. arXiv preprint arXiv:2405.20851 (2024). [42] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. 2023. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In CVPR. [43] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. 2021. Flow-guided one-shot talking face generation with high-resolution audio-visual dataset. In CVPR. [44] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. 2021. Pose-controllable talking face generation by implicitly modularized audio-visual representation. In CVPR. [45] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. 2020. Makelttalk: speaker-aware talking-head animation. ACM TOG (2020)."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Youtu Lab, Tencent, China"
    ]
}