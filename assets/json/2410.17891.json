{
    "paper_title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
    "authors": [
        "Shansan Gong",
        "Shivam Agarwal",
        "Yizhe Zhang",
        "Jiacheng Ye",
        "Lin Zheng",
        "Mukai Li",
        "Chenxin An",
        "Peilin Zhao",
        "Wei Bi",
        "Jiawei Han",
        "Hao Peng",
        "Lingpeng Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions \\url{https://github.com/HKUNLP/DiffuLLaMA}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 1 9 8 7 1 . 0 1 4 2 : r a"
        },
        {
            "title": "SCALING DIFFUSION LANGUAGE MODELS\nVIA ADAPTATION FROM AUTOREGRESSIVE MODELS",
            "content": "Shansan Gong1, Shivam Agarwal2, Yizhe Zhang3, Jiacheng Ye1, Lin Zheng1 Mukai Li1, Chenxin An1, Peilin Zhao4, Wei Bi4, Jiawei Han2, Hao Peng2, Lingpeng Kong1 1The University of HongKong 2 University of Illinois at Urbana-Champaign 3 Apple 4 Tencent AI Lab sansa933@connect.hku.hk,shivama2@illinois.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion Language Models (DLMs) have emerged as promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release suite of DLMs (with 127M, 355M, and 7B parameters) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have ushered in new era of artificial intelligence, demonstrating remarkable capabilities in generating high-quality text, in-context learning, and following complex instructions (OpenAI, 2023; Touvron et al., 2023a). These advancements are primarily rooted in the scaling up of autoregressive (AR) language models. During both training and inference, these models leverage vast datasets and billions of parameters, employing strict left-to-right sequential process for memorization and generation. This approach has resulted in the emergence of intelligence capable of tackling diverse tasks (Wei et al., 2022a; Hoffmann et al., 2024). However, the ultimate upper limit of intelligence achievable through this paradigm remains an open question. While AR mechanisms form the foundation of current LLMs, they are not without limitations (Lin et al., 2021). Notable challenges include difficulties in future planning (Bachmann & Nagarajan, 2024; Hu* et al., 2024; Xie et al., 2024) and self-correction (Huang et al., 2024). These constraints have spurred researchers to explore alternative architectures for next-generation LLMs. compelling direction in current research focuses on the development of text diffusion models (Li et al., 2023b). Building upon the rapid evolution of diffusion models in various domains (Ho et al., 2020; Nichol & Dhariwal, 2021; Ramesh et al., 2021), innovative text diffusion models (Li et al., 2022; Lou et al., 2024) have opened up new possibilities for text generation. unifying insight across these models is the potential of diffusion language models (DLMs) for controllable (Venkatraman et al., 2024), any-order, and parallel text generation (Gong et al., 2023a). Notably, DLMs exhibit promising capabilities in intermediate token correction (Ye et al., 2024) and global planning (Zhang et al., 2023), thereby addressing key limitations inherent in the AR approach. Equal contribution 1https://github.com/HKUNLP/DiffuLLaMA 1 Despite the promising potential of text diffusion models, the relatively small model size limits the competitiveness of DLMs compared to AR models. Existing state-of-the-art DLMs such as Plaid 1B (Gulrajani & Hashimoto, 2023) and SEDD (Lou et al., 2024) are relatively small in scale (127M1B parameters) and under-trained, with less than 400B tokens of training data. This substantial gap in scale prevents fair comparisons with larger AR language models on many advanced capabilities and tasks, such as chain-of-thought reasoning abilities on complex mathematical benchmarks. However, pre-training at such scale is extremely resource-intensive, and the challenge is even more pronounced for diffusion models. These models lack the computational optimizations that have been developed for LLMs (Samragh et al., 2024) and require significantly more resources than their AR counterparts, as noted by Gulrajani & Hashimoto (2023). Given these scaling challenges, pre-trained LLMs emerge as an invaluable resource that we can leverage, considering the extensive computational efforts already invested in their development. This strategy aligns with recent trends where new models are scaled up or adapted to new architectures using existing LLMs (Wang et al., 2024; Zhang et al., 2024c). However, building DLMs through adaptation from AR models is non-trivial due to fundamental differences in their language modeling objectives. Two key distinctions present significant hurdles. First, AR models employ causal masking to prevent future information leakage, whereas diffusion models utilize bi-directional attention masks. Second, an AR LM processes clean inputs to predict subsequent tokens at each step, while diffusion model operates on noisy inputs to predict their denoised versions. To overcome these challenges, we propose simple adaptation approach that bridges these discrepancies. We unify their modeling objectives (3.2) and address the architectural differences by breaking the causal masking bias in AR models through attention mask annealing (3.3). Additionally, we inherit the shift operation from AR models (3.3). This streamlined adaptation recipe enables us to construct pre-trained DLM that can effectively compete in the arena of LLMs. Building on this approach, we leverage the FineWeb (Penedo et al., 2024) and SlimPajama (Soboleva et al., 2023) pre-training corpora to continue training small and medium-sized DLMs based on GPT2 (Brown et al., 2020), and further scale up to 7B model based on LLaMA2 (Touvron et al., 2023b). Our experiments provide comprehensive comparison between AR LMs and DLMs across language modeling, reasoning, and infilling tasks. The evaluation encompasses diverse settings, including zero-shot, few-shot, and fine-tuning scenarios, addressing the limitations of relying solely on perplexity in previous works (Shi et al., 2024). Our contributions and empirical findings include: We demonstrate that by narrowing the gap between AR models and DLMs, it is possible to convert 127M-7B AR models (GPT2 and LLaMA2) into DiffuGPT and DiffuLLaMA with training on less than 200B tokens. Notably, DiffuGPT outperforms GPT2 in most tasks. We scale DLMs to 7B parameters, greatly expanding the expertise compared to smaller-scale diffusion models. DiffuLLaMA emerges as the state-of-the-art DLM, exhibiting in-context learning, code generation, and strong infilling capabilities. Its generation speed is competitive with AR counterparts for unconditionally generating 1024 tokens using 256 diffusion timesteps. We provide comprehensive benchmark for DLMs and release our adapted diffusion models (127M, 355M and 7B) along with open-source adaptation code, efficient fine-tuning scripts, and evaluation toolkits."
        },
        {
            "title": "2 PRELIMINARY AND NOTATION",
            "content": "Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021b) are latent variable generative models characterized by forward and reverse Markov process. We denote x0 pdata(x0) as the variable following the data distribution, and xt q(xt) as the noisy variable of x0 at time t, where the maximum time is . The forward process q(x1:T x0) = (cid:81)T t=1 q(xtxt1) corrupts the initial data x0 into sequence of increasingly noisy variables x1:T . Accordingly, the backward Markov process models the joint probability as pθ(x0:T ) = pθ(xT ) (cid:81)T t=1 pθ(xt1xt), which gradually denoises xt to reconstruct the original data x0. Parameters θ are learned by minimizing the negative log-likelihood of x0, which can be optimized through the evidence lower bound (ELBO), log pθ(x0) Eq(x1x0)[ log pθ(x0x1)] + DKL(q(xT x0)pθ(xT )) + LT , (1) 2 Figure 1: The overview of our approach to adapt autoregressive (AR) models to diffusion models. Left: The shift operation in AR models enables the output layer hi to approximate the distribution of next tokens xi+1 in hidden representations through the cross entropy (CE) loss. Middle: We remove the causal mask gradually during training eventually making our model bi-directional. Right: inside the diffusion models we shift the logits to compute the loss with the next token (i.e., the loss on hi would be with respect to xi+1), while perceptually, the diffusion models are still functioning as recovering the original signals (since hi corresponds to xi+1 in AR loss). t=2 with LT = (cid:80)T Eq(xtx0)[DKL(q(xt1xt, x0)pθ(xt1xt))]. For continuous text diffusion (Li et al., 2022; Gong et al., 2023b), at each forward step, perturbations are applied according to q(xtxt1) = (xt; 1 βtxt1, βtI), where βt (0, 1) represents different scales across time steps such that xT (0, I). In the case of discrete denoising models (Ho et al., 2020; Austin et al., 2021; Zheng et al., 2024a), the forward process is defined as categorical distribution xt1), where each xt {0, 1}K is one-hot vector with vocabulary q(xtxt1) = Cat(xt; size K, Qt [0, 1]KK is the transition matrix, and each entry [Qt]ij denotes the probability of transition from the state to j. We build on the formulation of absorbing discrete diffusion (Austin et al., 2021), which specifies Qt = (1 βt)I + βt1m. We denote 1 as an all-one vector of size and as the one-hot encoding of special [MASK] token in the vocabulary. Therefore, the transition matrix, Qt indicates that with probability 1 βt, xt remains unchanged; otherwise, it transitions to m, becoming absorbed into [MASK]. Letting Qt := (cid:81)t i=1 Qi = αtI + (1 αt)1m and αt := (cid:81)t i=1(1 βt), the distribution of xt conditional on x0 is given by q(xtx0) = Cat(xt; x0) = αtIx0 + (1 αt)m1x0 = αtx0 + (1 αt)m, (2) since x0 is one-hot vector and thus 1x0 = 1. We expect αT to approach 0 such that the full noise data xT equals with probability 1. The discrete time representation of [0, ], restricts xt to fixed noise ratios. To avoid this bias and enable sampling from any noisy representation, we use continuous-time sampling, allowing to span any point within [0, 1] (Kingma et al., 2021; Shi et al., 2024; Zhao et al., 2024; Ou et al., 2024). Continuous-time sampling is equivalent to dividing [0, 1] into intervals and where . For any 0 < 1, the forward process generalizes to q(xtxs). We will use this continuous-time notation in the following sections."
        },
        {
            "title": "3 MODEL",
            "content": "We begin by formulating the continuous-time discrete diffusion process (3.1) and establishing connection between the discrete diffusion and autoregressive objectives (3.2). Based on this equivalence, we propose an adaptation approach (3.3) and sampling algorithm (3.4) for diffusion models adapted from AR models. The whole process is illustrated in Figure 1. 3.1 CONTINUOUS-TIME DISCRETE DIFFUSION PROCESSES Following Eq.2 and q(xtx0) = (cid:80) tween arbitrary points < can be derived as xs q(xtxs)q(xsx0), the forward transition distribution beq(xtxs) = Cat(xt; stxs) = αt αs xs + (1 αt αs )m, (3) 3 1 Qt = αt with Qst := αs conditional on x0 is also available in closed form, + (1 αt αs )1m. The corresponding backward transition distribution q(xsxt, x0) = q(xtxs)q(xsx0) q(xtx0) = (cid:40) αsαt 1αt x0 x0 + 1αs 1αt if xt = m, if xt = m. (4) In discrete diffusion processes, we aim to approximate the backward transition distribution q(xsxt, x0) using denoising model pθ(xsxt, fθ(xt)), where fθ(xt), an approximation of x0, is usually the output of neural networks such as transformer (Vaswani et al., 2017). We can define the denoising model to have similar form of backward transitions as pθ(xsxt) = αsαt m. According to the training objective in Eq.1, the KL-divergence of LT at 1αt each step can be simplified to reweighted cross-entropy function, fθ(xt) + 1αs 1αt DKL(q(xsxt, x0)pθ(xsxt)) = αs αt 1 αt δxt,mx 0 log fθ(xt), (5) where δa,b is the indicator function for = b. If we take the limit and let , the first two terms of Eq.1 will approach 0 and some constant, respectively. Thus the evidence lower bound (ELBO) effectively becomes LT and lim LT = (cid:90) 1 0 α 1 αt Eq(xtx0)[δxt,mx 0 log fθ(xt)] dt. (6) The full derivation is listed in Appendix A.2. The same form of ELBO which is invariant to noise schedule but related to the signal-to-noise ratio (SNR) is also introduced in Kingma et al. (2021); Shi et al. (2024). Following Austin et al. (2021), we choose the noise schedule αt = 1 t, then α = 1αt 1 . The previous discussion focused on the single token xt, and can be applied independently to text sequence of tokens xt = [x1 ]. During training, we do not compute integral loss in Eq.6 for efficiency consideration; instead, we sample for each data point. The final loss at is . . . , xN , L1:N = 1 (cid:34) Eq(xtx0) (cid:88) n=1 δxn ,m(xn 0 ) log fθ(x1:N (cid:35) )n , (7) where fθ(x1:N output token is indexed. )n denotes the whole input sequence is fed into the transformer model and the n-th 3.2 UNIFYING LANGUAGE MODELING OBJECTIVES The training objective of autoregressive (AR) language models is the negative log-likelihood of each ground-truth token provided the preceding tokens, (cid:88) L1:N AR = (xn 0 ) log fθ(x1:n1 0 )n1. (8) n=1 Comparing Eq.8 against Eq.7, we note that while both take the form of cross-entropy functions, Eq.7 includes an additional reweighting term 1 ,m. They result from the definition of discrete diffusion processes (3.1). The reweighting emphasizes smaller where xt contains fewer masked tokens, and this can be regarded as the importance sampling (Nichol & Dhariwal, 2021). The indicator specifies which tokens are masked for prediction. The AR training objective Eq.8, on the other hand, constrains the context to be unidirectional via attention masking and shifts the targets so that each token predicts the next token instead of itself. These discrepancies form the basis of our adaptation framework, which is detailed in 3.3. and an indicator function δxn In fact, an alternative way to understand AR modeling, through the lens of diffusion models, is to consider diffusion process where the forward pass deterministically masks right-to-left and tokenby-token (Austin et al., 2021; Hoogeboom et al., 2022). This yields backward process generating one token at time from left to right, running with = denoising steps in total. As discussed in Austin et al. (2021), the loss objective of this diffusion process is equivalent to standard crossentropy (Eq.8) commonly used to train AR language models. This crafted diffusion process for AR models represents special case of discrete diffusion (3.1), yet it is limited to unidirectional context and sequential token generation. In contrast, general discrete diffusion processes can leverage bidirectional context and support parallel generation in arbitrary orders."
        },
        {
            "title": "3.3 ADAPTATION",
            "content": "Building on the connection between AR modeling and discrete diffusion processes, we construct an adaptation recipe next. Figure 1 shows an overview of our adaptation approach. We use attention mask annealing, shift operations, and time-embedding free architecture to narrow the differences between AR and DLMs. Algorithm 1 Adaptation Training Algorithm 2 Sampling 1: Input: network fθ initialized by existing models, 0 ), mask token m. 1: Input: Trained diffusion model fθ, sampling algorithm τ , mask token m, start token s. 0 pdata and set labels x1:N 0 training corpus pdata(x1:N 2: Output: model parameters θ. 3: repeat 4: 5: 6: 7: 8: 9: 10: 11: 12: until end training Draw x1:N Sample Uniform(0, 1) Sample x1:N q(xtx0) Anneal the attention mask attn mask Forward logits fθ(x1:N Right shift logits by one position Lt = 1 Backprop with Lt and update θ δxt,mCE(logits, labels) Eq. ) with attn mask 0 Categorical(τ (logits)) ) = m. Forward logits fθ(x1:N Sample x1:N for = 1, . . . , do t1xn t1 = q(xn 2: Output: generated sample x0. 3: Initialize x1:N 4: for = T, . . . , 1 do 5: 6: 7: 8: 9: 10: 11: end for 12: Return x2:N xn end for Right shift x1:N , xn 0 t1 = [s, x1:N 1 ] 0 ) Eq.4 Attention Mask Annealing The prediction of the n-th token, given all preceding tokens, fθ(x1:n1 ), is usually implemented by causal attention masking in transformer-based AR language 0 models. As shown in Figure 1, causal attention masks set all entries in the upper triangle of the selfattention matrices to zero, so each token cannot attend to its respective future tokens. Such causal masking prevents the model from learning right-to-left dependencies for more general diffusion processes. To address this limitation while preserving left-to-right conditionals during adaptation, we introduce an incremental annealing process from causal masks to full attention matrices. During annealing, the causal mask is not immediately removed; instead, it is retained at controlled ratio, as shown in the middle part of Figure 1. At each training step, we sample the amount of context from the right side and progressively increase this amount till we obtain the full attention mask. Shift Operation AR models also apply shifting operation, where the target output is the input sequence shifted left by one position. In other words, the prediction target of the (n1)-th token is the n-th token, contrasting with typical diffusion models that try to predict masked tokens at their original positions. When initializing text diffusion models with AR model parameters, the model would tend to output the hidden representations of the shifted input sequence. If we continue to optimize the cross-entropy objective based on the original token positions, the model struggles to adapt due to misalignment between input and output. Instead, we maintain the shift operation (Algo.1, line 9), treating the output logits at each position as corresponding to the next token. When calculating the objective, we align prediction targets so that the diffusion model learns to recover the original signals. This process is illustrated in the right panel of Figure 1. Time-Embedding-Free Architecture Many diffusion models for text generation (Li et al., 2022; Dieleman et al., 2022; Gulrajani & Hashimoto, 2023; Lou et al., 2024; Shi et al., 2024) incorporate time embedding layers to represent the information of current timesteps t, which can explicitly indicate the noise scale of the input noisy data. While inferring these timesteps can be challenging for image diffusion models (Ho et al., 2020; Li et al., 2024), some discrete text diffusion models (He et al., 2023) assert that timesteps can be easily learned implicitly based on the number of mask tokens. Since AR models are not equipped with time embedding layers, we also choose not to use the time embedding, resulting in no additional parameters compared to previous diffusion models. 3.4 SAMPLING Following Shi et al. (2024), we initialize xT with all [MASK] tokens and then sample tokens according to the time reversal q(xsxt, x0) in Eq.4. At each timestep, if xt is mask, it will jump to the predicted x0 at time with probability αsαt . After iterations, the model generates the full 1αt sequence. Since our adapted models are trained with the shift operation, at each sampling iteration, 5 we shift back the generated sentence and prepend start token before the next forward pass (Algo.2, line 10). Usually larger requires more interactions of computation, and can yield texts in higher quality, and this trade-off can be controlled easily through . Through experiments, we find that the output generated by diffusion models is diverse and scattered. Therefore, for conditional generation tasks, we improve the sampling procedure to ensure that only tokens with high probabilities from neural networks are denoised (Ghazvininejad et al., 2019; Chang et al., 2022; Zheng et al., 2024a), so that the model could predict tokens mostly relevant to the input. In addition, existing sampling techniques for AR language models, including top-k and nucleus sampling (Holtzman et al., 2020), can be seamlessly applied to diffusion models as well."
        },
        {
            "title": "4.1 ADAPTATION SETUP",
            "content": "DiffuGPT We use the 30 billion tokens2 random split from the FineWeb dataset (Penedo et al., 2024), an improved corpus than OpenWebText (Gokaslan & Cohen, 2019) used in prior DLMs (Lou et al., 2024), to continue training GPT2 base (Radford et al., 2019). We use sequence packing, logits shifting, and 10K-step attention mask annealing to transform GPT2 to DiffuGPT. DiffuLLaMA We continue pre-training LLAMA-2-7-HF (Touvron et al., 2023a) on mixture of SlimPajama (70%) (Soboleva et al., 2023) and Starcoder (30%) (Li et al., 2023a) data following TinyLLaMA (Zhang et al., 2024a). We randomly sample 65 billion tokens from this mixture and use sequence packing with context length of 2048. For efficient implementation we enable flash-attention 2 (Dao, 2024) and directly use bi-directional attention without attention mask annealing. For both adaptation settings, we employ full parameter finetuning with bf16. Please refer to Appendix B.2 for details. We plot the training loss curve in Figure 2. We train DiffuLLaMA on 60B tokens and achieve lower loss compared to 127M and 335M models, suggesting scaling trend similar to that of AR LLMs (Kaplan et al., 2020). We also note that there is still scope for training more, since the model does not show signs of saturation. Figure 2: Training loss over tokens for different scales of our adapted diffusion models. 4.2 EVALUATION SETUP Previously developed diffusion language models (Gulrajani & Hashimoto, 2023; Lou et al., 2024; Shi et al., 2024; Ou et al., 2024) evaluate model performance using zero-shot perplexity on benchmark datasets. However, this metric alone does not fully capture models capabilities for several reasons. First, lower perplexity does not always correlate with human-like content, even in autoregressive models (Kuribayashi et al., 2021). Additionally, the loss from text diffusion models only indicates an upper bound on negative log-likelihood. While Kingma et al. (2021); Shi et al. (2024) demonstrate that the ELBO is invariant to the noise scheduler, discrepancies between continuous diffusion, discrete diffusion, and autoregressive loss still hinder fair comparisons across different model types. Given the ample evaluation benchmarks (Gu et al., 2024) for LLMs, we propose more comprehensive evaluation for diffusion models. Tasks and Metrics We consider TriviaQA (Joshi et al., 2017) to test the reading comprehension of models and last word completion task Lambada (Paperno et al., 2016) to test how models capture long-range dependencies in text. These two tasks are measured by exact match accuracy. We also test for common sense reasoning tasks HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2020), SIQA (Sap et al., 2019) and PIQA (Bisk et al., 2020), all of which involve multiplechoice questions assessed by accuracy. On grade school math problems GSM8K (Cobbe et al., 2021), we follow Ye et al. (2024) in finetuning setting using the augmented symbolic data to test the CoT (Wei et al., 2022b) math reasoning abilities of diffusion models. Following Shen et al. (2023), 2This is the total number of tokens used; however, our effective training tokens exceed this count, meaning that we train for more than one epoch. 6 Table 1: Comprehensive evaluation of different diffusion language models and the same scale pretrained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models. Model Size Type QA Word CommonSense Reasoning Math TriQA Lamb. HSwag Wino. SIQA PIQA GSM8K ROCStories Code Infilling 127M AR GPT2-S SEDD-S 170M DD DiffuGPT-S 127M DD 355M AR GPT2-M SEDD-M 424M DD DiffuGPT-M 355M DD Plaid1B 1.3B CD 4.0 1.5 2.0 6.7 1.8 3.8 1.2 7B LLaMA2 DiffuLLaMA 7B AR 45.4 DD 18. 25.9 12.4 45.0 37.7 23.1 60.5 8.6 68.8 70.9 29.9 30.2 33.4 38.3 31.5 37. 39.3 74.9 58.7 48.5 50.1 50.8 50.7 49.0 52.6 35.7 34.4 37.0 37.7 35.4 39. 62.1 55.6 57.7 67.4 56.1 59.6 51.3 32.3 54.5 67.1 56. 44.8 43.2 78.3 63.3 44.8 45.3 50.2 45.6 53.5 61.8 32.6 58.6 63. (7.8/0.8/7.4) 11.9/0.7/10.9 13.7/1.4/12.6 (8.6/0.9/8.2) 13.1/1.4/12.2 18.7/2.7/17.0 (1.6) 0.7 0.3 (2.6) 0.5 2.9 12.1/1.1/11.2 0. (11.6/2.1/10.5) (1.7) 15.5 23.3/5.5/21.2 we also test the story infilling tasks using ROCStories (Mostafazadeh et al., 2016) and evaluate using ROUGE score (Lin, 2004). To test the code infilling, we adopt Humaneval (Bavarian et al., 2022) single line infilling task, which is evaluated by pass@1 rate. We evaluate DiffuLLaMAs math reasoning and in-context learning ability by evaluating on MAWPS (Koncel-Kedziorski et al., 2016) and SATMATH (Zhong et al., 2024). We re-implement all tasks to ensure fair comparison. Implementation Details For pre-trained diffusion language models, we mainly use continuous diffusion (CD) model Plaid 1B (Gulrajani & Hashimoto, 2023), discrete diffusion (DD) model SEDD (Lou et al., 2024) with different scales as baselines. For autoregressive (AR) baselines, we consider the base models from which our models adapt. We implement infilling tasks for AR models by feeding the prefix and cutting off the generation length using the oracle length, considering that these AR models are not supporting infilling. For the sentence completion task, is the exact number of ground truth tokens for DD and 32 for CD. For 4 multi-choices tasks from commonsense reasoning, we compute the loss (Eq.6) of each choice (averaged by token) and choose the one with lowest loss (perplexity). For GSM8K finetuning, we use parameter-efficient LoRA tuning (Hu et al., 2022) for DiffuLLaMA. The decoding are set to 32 by default. The detailed settings are in Appendix B.3. 4.3 LANGUAGE MODELING CAPACITIES Benchmark performance According to Table 1, the results on diverse tasks demonstrate that our adapted diffusion models achieve the state-of-the-art results among all existing diffusion language models (DLMs). Notably, by observing different scales of our adapted diffusion models, we can draw the conclusion that scaling diffusion language models results in improved performance. When comparing with their respective AR counterparts, both DiffuGPTs outperform the GPT2 on most tasks. DiffuLLaMAs performance still falls short of the LLaMA2 model. This discrepancy is presumably attributed to the extensive amount of training tokens processed by the DiffuGPT series models, whereas the 7B model has not undergone equivalent training. TriviaQA and PIQA are significant challenging for DLMs, probably because they require specific physical knowledge, such as the capital of city or the boiling point of water; while our models are trained on 30B-70B tokens, which may be insufficient to preserve the general knowledge in the original LMs (Ke et al., 2023). In tasks that require more extensive global reasoning, such as complex mathematics and coding, DLMs consistently exhibit better performance compared to AR models that rely solely on left-toright modeling capabilities. Remarkably, DLMs demonstrate their strengths in infilling tasks. Regular LLMs like LLaMA2 are not trained for filling-in-the-middle (FIM) tasks like those in Roziere et al. (2023), making them incapable of handling infilling. Considering this, we do not provide the suffix information to the model, which might result in an unfair comparison. But the FIM requires 7 re-arranging the order of pre-training/inference sequence with special tokens (Zheng et al., 2024b), while diffusion training naturally supports this in its objective modeling. Tasks in Table 1 mainly measure conditional modeling abilities, where Plaid 1B performs unsatisfactorily for conditional generation tasks even though with 1B parameters. We attribute this result to the gap between the continuous diffusion modeling and discrete text representation; in contrast, discrete diffusion models align more closely with AR modeling, naturally supporting conditional generation. Despite this, as illustrated in Figure 3, Plaid 1B demonstrates its strength in unconditional generation, highlighting its language modeling capabilities as generative model. These findings reveal that the previous evaluation based on the perplexity of test data is too general to accurately assess the models true capabilities, while our evaluation offers more nuanced benchmark. Unconditional Generation We evaluate the quality of text unconditionally generated by DLMs in Figure 3. The perplexity is measured using GPT2 large, consistent with the prior work (Lou et al., 2024), where the data of MD4 (Shi et al., 2024) is sourced from its original paper. To make sure low perplexity is not brought by repeated content, we assess the distinct 2-gram diversity of the generated text. Our model achieves low perplexity while maintaining high level of diversity, validating that the DiffuGPT series excels in fluent text generation. As the number of decoding steps increases, thereby extending the test computing time, the fluency of unconditional generation improves. Similarly, scaling model size also contribute to better performance. An increase in generation perplexity is often associated with slight decrease in diversity, which is common phenomenon. Notably, DiffuGPT outperforms both SEDD and MD4 models, particularly at lower step counts (e.g., 64 steps), while as the continuous diffusion models, Plaid 1B needs more decoding steps to generate more fluent texts. DiffuGPT thus exhibits significant advantage on less sampling time. We outline the decoding hyperparameters and show the diversity changes across different settings in Appendix C.1, which also includes generation cases. Figure 3: Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2gram diversity. 4.4 ANALYSIS ON DIFFULLAMA Table 2: Performance on math/QA benchmarks (). We compare of DiffuLLaMA with zero-shot (ZS), few-shot (FS), self-consistency (SC), hit@k and chain-of-thought (CoT) prompts. We validate that scaling of DLMs significantly enhances the performance of downstream tasks in Table 1. Further, we aim to assess if the scaled 7B model demonstrates in-context learning and reasoning capabilities similar to AR LLMs. Table 2 presents the exact match accuracy between gold labels and predictions generated by DiffuLLaMA across zero-shot (ZS), few-shot (FS), and FS with chain-of-thought (CoT) scenarios. Besides, we deploy the selfconsistency approach (Wang et al., 2023), considering that small DLMs can indeed benefit from this technique (Ye et al., 2024). We use majority vote to choose the best answer from 3 individual predictions, and also report the hit rate @k with = 3, which counts if any of the predictions hit the correct answer. For in-context learning (ICL) evaluations, we give 4-shot on math tasks and 2-shot on TriviaQA. LLaMA2 DiffuLLaMA-ZS DiffuLLaMA-FS DiffuLLaMA-SC DiffuLLaMA-@k DiffuLLaMA-CoT MAWPS SATMath TriviaQA 45.4 18.5 20.9 26.0 34.1 - 63.5 9.7 31.3 33.1 40.8 28. 24.5 <1 23.6 27.7 57.7 9.5 Models The performance improvement from zero-shot to few-shot settings suggests that DiffuLLaMA can learn from ICL examples, particularly in following to the format of answers as we observe. We hypothesize that the adapted model retains some of the abilities from the base AR model. We randomly select the ICL demonstration here and anticipate that advanced ICL strategies in LLMs (Wu et al., 2023) could yield potentially higher results. The self-consistency offers LMs with an effective ap8 proach to test-time scaling (Snell et al., 2024), and DiffuLLaMA shows that it can also leverage this method. Furthermore, we report the hit rate results in generated candidate answers, highlighting the models potential to produce the correct answer. This reveals that the current model exhibits high uncertainty about its responses, leading to temporarily suboptimal performance. We also observe that adding step-wise solutions in the in-context example (CoT) leads to drop in performance, likely due to the absence of instruction tuning, similar to the findings in LLMs (Ouyang et al., 2022). We will leave instruction tuning as the future work as Ye et al. (2023) show that text diffusion model can benefit from instruction tuning. In summary, we show the potential capabilities of DiffuLLaMA, which motivates us to further investigate the scaling of diffusion models."
        },
        {
            "title": "4.5 DISCUSSIONS",
            "content": "Table 3: Ablation test for adaptation approaches on GSM8K symbolic dataset. CD is for continuous diffusion and DD is for discrete diffusion. Ablation Test on GSM8K-symbolic Direct ablation on adaptation training is costly; hence, we conduct preliminary experiments to determine the adaptation recipes. Following Ye et al. (2024), we finetune models on the augmented GSM8K symbolic dataset using various base models and training objectives. The models are either trained from scratch (random initialization) or initialized with GPT2-S/M weights. Training objectives includes autoregressive training with causal mask, continuous diffusion loss (CD), and discrete diffusion loss (DD). As shown in Table 3, different training objectives yield comparable results when training from scratch. However, when using GPT2 as the base model, the CD loss performs worse than both the DD and AR losses. We attribute this to the better alignment of DD and AR losses as discussed in 3.2. Previous continuous diffusion models (Dieleman et al., 2022; Gulrajani & Hashimoto, 2023) has reparameterized the estimation of embeddings into the CE loss. However, adapting diffusion models from an AR model in continuous space necessitates an additional projection from the embedding to categorical distribution, increasing the difficulty of adaptation. Autoregressive CD DD-w/o shift DD-w/o anneal DD Random GPT2-S GPT2-M 45.6 20.2 34.5 47.2 49.7 44.8 19.2 33.5 43.3 45. 30.5 27.9 - - 28.0 Base models For DD loss, removing attention mask annealing and shift operations both degrade performance, indicating the efficacy of our approaches. The mask annealing has minimal impact, so we choose to omit it for 7B adaptation to simplify implementation using flash-attention 2. Direct DD loss finetuning on GPT2 achieves accuracy of 45.4 and 49.7 for small and medium models, respectively, outperforming GPT2 AR finetuning. However, finetuning from already adapted diffusion language models (DiffuGPT) yields accuracy of 50.2 and 61.8  (Table 1)  . This demonstrates the superiority of DiffuGPT as the current best diffusion base model at this scale and highlights that better base model leads to improved results. Even with the same DD loss, DiffuGPTs finetuning converges faster and achieves lower loss, as shown in Appendix C.2. Inference Speed AR models usually utilize key-value caching (incremental decoding; Ott et al. 2019) to enhance throughput during decoding. However, due to the nature of sequential token generation, they are highly memorybound and cannot fully exploit modern accelerators (Chen et al., 2023). In contrast, diffusion models, despite not having concept of caching and requiring self-attention over the entire sequence at each iteration, can operate with fewer iterations than the sequence length and exhibit less memory-bound behavior. Their performance can be further boosted with hardware-aware optimizations like flashattention (Dao et al., 2022; Dao, 2024; Shah et al., 2024). In Figure 4, we evaluate the decoding latency with batch size 1 using flash-attention 2 and illustrate that our DiffuLLaMA achieves better inference efficiency using = 256 when generating sequences of length 1024 or longer. This underscores the significant potential of diffusion models for Figure 4: Single batch decoding speed (seconds) for different models using flash-attention 2. efficient inference. Further decreasing can lead to faster decoding but may sacrifice quality. Additional analysis and latency comparisons are provided in Appendix C.3."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Continue Pre-training Continue pre-training is commonly used in adapting an existing language model (LM) to domain-specific LM (Ke et al., 2023) or enabling new abilities of LM, such as for longer context (Chen et al., 2024) or code generation (Xu et al., 2024). Pre-training LMs is nontrivial and expensive (Samragh et al., 2024), thus in exploring of new architectures of LMs such as Mamba (Gu & Dao, 2023) and gated attention, Wang et al. (2024); Zhang et al. (2024c) choose to transfer from LMs to save the training cost. However, all these continue pre-training works follow the autoregressive (AR) language modeling, while adapting LMs into diffusion language model is more challenging due to discrepancies between their modeling objectives. Text Diffusion Models Diffusion models have demonstrated significant diversity and controllability in image generation (Ho et al., 2020; Song et al., 2021a; Ramesh et al., 2022). Building on this success, line of research (Li et al., 2022; Gong et al., 2023b;a; Dieleman et al., 2022) build continuous diffusion models for text generation tasks. Among them, Lin et al. (2023) experiment with pre-training and finetuning framework under small scale; Gulrajani & Hashimoto (2023) highlight the scaling law of continuous diffusion models, revealing that the compute-optimal requires longer training than their AR counterparts. To address the discrete nature of text, Austin et al. (2021); Hoogeboom et al. (2021); Zheng et al. (2024a) incorporate an absorbing [MASK] state as noise, laying the foundation for discrete diffusion models, which are further developed by Lou et al. (2024); Shi et al. (2024); Ou et al. (2024); Zhao et al. (2024). By connecting text diffusion models with pre-trained masked language models (MLMs; Devlin et al. 2019), Ye et al. (2023); He et al. (2023) initialize discrete diffusion models using MLMs. Besides, the unification between diffusion and AR generation is also discussed in image generation (Li et al., 2024). However, the adaptation of diffusion models from AR LLMs remains unexplored. Non-autoregressive Generation Non-autoregressive (NAR) models, introduced by Gu et al. (2018), break free from the left-to-right generation constraint, allowing for new capabilities like planning with future tokens (Wu et al., 2024). Current diffusion language models are notable part of the NAR family (Gong et al., 2023b). Given the challenges of developing NAR models, researchers often seek to find trade-off. For instance, SSD-LM (Han et al., 2023) leverages diffusion models to iteratively generate text blocks, facilitating semi-NAR generation process. Similarly, CLLM (Kou et al., 2024) enhances LLMs by enabling the parallel generation of tokens, thereby improving decoding speed. FiLM (Shen et al., 2023) adapts language models to generate tokens in any order, which is particularly useful for infilling tasks. Additionally, Gloeckle et al. (2024) focus on training models to achieve better and faster multi-token predictions as they scale up. These semi-NAR approaches provide compelling alternatives to traditional AR LLMs, yet few have thoroughly explored the scalability of fully-NAR models."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Building on existing DLMs, we present recipe for scaling DLMs by continuing training on off-theshelf autoregressive LLMs. Our adaptation technique involves using 1) attention mask annealing to enable bidirectional modeling and 2) shift operation to allow similar training dynamics like AR models. By unifying the language modeling objectives of autoregressive and diffusion models, we scale diffusion models up to 7B parameters. Through experiments on common sense reasoning, language modeling, math reasoning and code generation, we show that DiffuGPT and DiffuLLaMA have better performance compared to existing DLMs. We find that DiffuLLaMA is capable of following in-context demonstrations to some extent on math problems. In the future, we aim to instruction tune our DLMs and explore inference time planning methods. We release DiffuLLaMA and DiffuGPT for further exploration of diffusion models as an alternative language modeling method. AUTHOR CONTRIBUTIONS Shansan Gong: Project lead, methodology development, DiffuGPT training and model evaluation, major writing. Shivam Agarwal: Discussion, DiffuLLaMA training, writing. Yizhe Zhang: Dis10 cussion, DiffuLLaMA training, writing suggestions. Jiacheng Ye: Initial methodology exploration. Lin Zheng: Discussion, writing. Mukai Li & Chenxin An: Discussion, writing suggestions. Others: Mentorship and supervision."
        },
        {
            "title": "REFERENCES",
            "content": "Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 1798117993, 2021. Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. In Forty-first International Conference on Machine Learning, ICML, 2024. Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. ArXiv preprint, abs/2207.14255, 2022. Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about In The Thirty-Fourth AAAI Conference on Artifiphysical commonsense in natural language. cial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 74327439. AAAI Press, 2020. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1130511315. IEEE, 2022. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. ArXiv preprint, abs/2302.01318, 2023. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. LongloRA: Efficient fine-tuning of long-context large language models. In The Twelfth International Conference on Learning Representations, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Tri Dao, Daniel Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. 11 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. ArXiv preprint, abs/2211.15089, 2022. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel In Proceedings of the 2019 Conference on decoding of conditional masked language models. Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 61126121, Hong Kong, China, 2019. Association for Computational Linguistics. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Roziere, David Lopez-Paz, and Gabriel Synnaeve. In Forty-first International Better & faster large language models via multi-token prediction. Conference on Machine Learning, 2024. Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. DiffuSeq-v2: Bridging discrete and continuous text spaces for accelerated Seq2Seq diffusion models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 98689875. Association for Computational Linguistics, 2023a. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. DiffuSeq: Sequence In International Conference on Learning to sequence text generation with diffusion models. Representations, ICLR, 2023b. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv preprint, abs/2312.00752, 2023. Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. NonIn 6th International Conference on Learning Repautoregressive neural machine translation. resentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh Hajishirzi. Olmes: standard for language model evaluations, 2024. Ishaan Gulrajani and Tatsunori Hashimoto. Likelihood-based diffusion language models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. SSD-LM: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1157511596, Toronto, Canada, 2023. Association for Computational Linguistics. Zhengfu He, Tianxiang Sun, Qiong Tang, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. DiffusionBERT: Improving generative masked language models with diffusion models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 45214534, Toronto, Canada, 2023. Association for Computational Linguistics. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 1245412465, 2021. Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Edward Hu*, Moksh Jain*, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In International Conference on Learning Representations, 2024. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, In The Tenth Interand Weizhu Chen. Lora: Low-rank adaptation of large language models. national Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, In The Twelfth and Denny Zhou. Large language models cannot self-correct reasoning yet. International Conference on Learning Representations, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, 2017. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv preprint, abs/2001.08361, 2020. Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual pretraining of language models. In The Eleventh International Conference on Learning Representations, 2023. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 11521157, San Diego, California, 2016. Association for Computational Linguistics. Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. In International Conference on Machine Learning, ICML, 2024. Tatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki Asahara, and Kentaro Inui. Lower perplexity is not always human-like. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 52035217, Online, 2021. Association for Computational Linguistics. Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries. Starcoder: may the source be with you! Transactions on Machine Learning Research, 2023a. ISSN 2835-8856. Reproducibility Certification. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. ArXiv preprint, abs/2406.11838, 2024. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. In Conference on Neural Information Processing Systems, NeurIPS, 2022. Yifan Li, Kun Zhou, Wayne Xin Zhao, and Ji-Rong Wen. Diffusion models for non-autoregressive text generation: survey. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 23, 2023b. ISBN 978-1-956792-03-4. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, 2004. Association for Computational Linguistics. Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. Limitations of autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 51475173, Online, 2021. Association for Computational Linguistics. Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, and Weizhu Chen. Text generation with diffusion language models: pre-training approach with continuous paragraph denoise. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. In International Conference on Machine Learning, ICML, 2024. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 839849, San Diego, California, 2016. Association for Computational Linguistics. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 81628171. PMLR, 2021. OpenAI. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 4853, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. 14 Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. ArXiv preprint, abs/2406.03736, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15251534, Berlin, Germany, 2016. Association for Computational Linguistics. Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 88218831. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. ArXiv preprint, abs/2204.06125, 2022. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. ArXiv preprint, abs/2308.12950, 2023. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 87328740. AAAI Press, 2020. Mohammad Samragh, Iman Mirzadeh, Keivan Alizadeh Vahid, Fartash Faghri, Minsik Cho, Moin Nabi, Devang Naik, and Mehrdad Farajtabar. Scaling smart: Accelerating large language model pre-training with small model initialization. ArXiv preprint, abs/2409.12903, 2024. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, Hong Kong, China, 2019. Association for Computational Linguistics. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. ArXiv preprint, abs/2407.08608, 2024. Tianxiao Shen, Hao Peng, Ruoqi Shen, Yao Fu, Zaid Harchaoui, and Yejin Choi. Film: Fill-in language models for any-order generation. ArXiv preprint, abs/2310.09930, 2023. 15 Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. ArXiv preprint, abs/2406.04329, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ArXiv preprint, abs/2408.03314, 2024. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, 2023. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pp. 22562265. JMLR.org, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1189511907, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and In 9th Ben Poole. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 59986008, 2017. Siddarth Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, et al. Amortizing intractable inference in diffusion models for vision, language, and control. ArXiv preprint, abs/2405.20971, 2024. Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. ArXiv preprint, abs/2408.15237, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856. 16 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022b. Wilson Wu, John Xavier Morris, and Lionel Levine. Do language models plan ahead for future tokens? In First Conference on Language Modeling, 2024. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14231436, Toronto, Canada, 2023. Association for Computational Linguistics. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents. ArXiv preprint, abs/2402.01622, 2024. Yiheng Xu, Hongjin SU, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu. Lemur: Harmonizing natural language and code for language agents. In The Twelfth International Conference on Learning Representations, 2024. Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Zhenguo Li, Wei Bi, and Lingpeng Kong. Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models. ArXiv preprint, abs/2402.07754, 2024. Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Quanquan Gu. Diffusion language models can perform many tasks with scaling and instruction-finetuning. ArXiv preprint, abs/2308.12219, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, Florence, Italy, 2019. Association for Computational Linguistics. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024a. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024b. Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Joshua M. Susskind, and Navdeep Jaitly. In ThirtyPLANNER: Generating diversified paragraph via latent language diffusion model. seventh Conference on Neural Information Processing Systems, 2023. Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, et al. Gated slot attention for efficient linear-time sequence modeling. ArXiv preprint, abs/2409.07146, 2024c. Lingxiao Zhao, Xueying Ding, Lijun Yu, and Leman Akoglu. Improving and unifying discrete&continuous-time discrete denoising diffusion. ArXiv preprint, abs/2402.03701, 2024. Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. reparameterized discrete diffusion model for text generation. In Conferenec on Language Modeling, COLM, October 7-9, 2024, Philadelphia, PA, 2024a. Lin Zheng, Jianbo Yuan, Zhi Zhang, Hongxia Yang, and Lingpeng Kong. Self-infilling code generation. In Forty-first International Conference on Machine Learning, 2024b. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: human-centric benchmark for evaluating foundation models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 22992314, Mexico City, Mexico, 2024. Association for Computational Linguistics."
        },
        {
            "title": "A OBJECTIVE DERIVATIONS",
            "content": "This section provides detailed preliminary and loss derivations of 2 and 3.1 in the main paper. A.1 BACKGROUND OF DIFFUSION MODELS We denote x0 pdata(x0) as the variable following the data distribution, and xt q(xt) as the noisy variable of x0 at time t, where the maximum time is . The forward process q(x1:T x0) = (cid:89) t= q(xtxt1) (9) corrupts the initial data x0 into sequence of increasingly noisy variables x1:T . Accordingly, the reverse Markov process models the joint probability as pθ(x0:T ) = pθ(xT ) (cid:89) t= pθ(xt1xt), (10) which gradually denoises xt to reconstruct the original data x0. Parameters θ are learned by minimizing the negative log-likelihood of x0, which can be optimized through the variational lower bound (VLB): log pθ(x0) Eq(x1x0)[ log pθ(x0x1)] + DKL(q(xT x0)pθ(xT )) + LT , with LT = (cid:88) t=2 Eq(xtx0)[DKL(q(xt1xt, x0)pθ(xt1xt))]. (11) (12) For continuous text diffusion (Li et al., 2022; Gong et al., 2023b), at each forward step, perturbations are applied according to q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI), where βt (0, 1) represents different scales. In the end, xT (0, I). In the case of discrete denoising models (Ho et al., 2020; Austin et al., 2021; Zheng et al., 2024a), xt follows categorical distribution which naturally aligns with discrete text data. Let be the one-hot encoded sample of variable and xt Cat(xt; p) represent categorical distribution over vector with probabilities given by p. Here, represents the vocabulary size, {e1, . . . , eK}, and ek {0, 1}K is the one-hot encoding of the k-th word category. The forward process can be formulated through transition matrix Qt [0, 1]KK such that (13) q(xtxt1) = Cat(xt; xt1); Qt = (1 βt)I + βt1e K, (14) with 1 as an all-one vector of size and we assume eK as the special [mask] state, also defined as the absorbing state in discrete diffusion m. Each entry in [Qt]ij denotes the probability of transition from the state ei to ej, and thus the previously defined Qt means with probability 1 βt, xt will stay unchanged and otherwise it will jump to the mask state eK. Starting from x0, the t-step marginal distribution and the posterior at previous time 1 is respectively q(xtx0) = Cat(xt; = x0); q(xt1xt, x0) = q(xtxt1, x0)q(xt1x0) q(xtx0) (15) where cumulative products Qt = (cid:81)t i=1(1 βt). We expect αT approaches 0 such that the full noise data xT is equal to eK with probability 1. In the following sections, we primarily takes the discrete diffusion formulation. i=1 Qi = αtI + (1 αt)1m, and αt = (cid:81)t A.2 LOSS DERIVATION Previous discrete time [0, ] restricts xt to fixed time points whereas the continuous-time sampling allows for more flexibility covering any point in the range (Kingma et al., 2021; Shi et al., 18 2024; Zhao et al., 2024; Ou et al., 2024). In this case, runs from 0 to 1, corresponding to dividing [0, 1] into intervals and let . For any two arbitrary time points, 0 < 1, the forward modeling can be generalized from q(xtxt1) to q(xtxs). We uniformly adopt the notation of continuous-time in following sections. Following the previous definition, after simplification, we have q(xtx0) = αtx0+(1αt)m, referring to the probability of transition to absorbing mask state. Given q(xtx0) = q(xtxs)q(xsx0), we can derive the transition distribution between two arbitrary times and t: q(xtxs) = Cat(xt; stxs), with Qst = 1 Qt = αt αs + (1 αt αs )1m. (16) Similarly, after simplification, q(xtxs) = αt αs xs + (1 αt αs )m. (17) Following Zheng et al. (2024a); Shi et al. (2024) and extend the formulation to continuous time, we have the backward transition probability: q(xsxt, x0) = q(xtxs)q(xsx0) q(xtx0) = (cid:40) 1(1αs) 1αt (1 αt αs 1αt )αs = 1αs 1αt = αsαt 1αt = 1 αsαt 1αt if xt = xs = m, if xt = = xs. (18) For xt = m, the q(xsxt, x0) will stick to the observed data. For xt = m, we get the simplified q(xsxt, x0) = αs αt 1 αt In diffusion process, the generative model aims to approximate the reverse transitions using denoising model pθ(xsxt, fθ(xt)) q(xsxt, x0), where fθ(xt) represents the probability vector obtained from the softmax applied to the logits generated by the neural network, usually using transformer networks (Vaswani et al., 2017) in text domain. We can similarly have 1 αs 1 αt x0 + (19) m. pθ(xsxt) = αs αt 1 αt fθ(xt) + 1 αs 1 αt m. Given Eq.19 and Eq.20, the KL-divergence loss is optimized by DKL(q(xsxt, x0)pθ(xsxt)) = (cid:40) αsαt 1αt 0, DKL(x0fθ(xt)), for xt = m; for xt = m. (20) (21) We can use the indicator function δxt,m to unify the conditional cases. In addition, given x0, we have DKL(x0fθ(xt)) = 0 log fθ(xt) which corresponds to the cross-entropy widely used in the classification. Therefore, we have αs αt 1 αt Following Eq.12, if we set small timestep = = 1 DKL(q(xsxt, x0)pθ(xsxt)) = (0, 1), δxt,mx 0 log fθ(xt). (22) LT = (cid:88) [ t=2 αs αt (t s)(1 αt) δxt,mx 0 log fθ(xt)t]. (23) By taking the limit as , we have α = αtαs ts , and the sum is transformed into an integral: lim LT = (cid:90) 1 α 1 αt Eq(xtx0)[δxt,mx 0 log fθ(xt)] dt. (24) Also, the first two terms in Eq.11 are 0 and constant, respectively. Thus we can formulate the evidence lower bound (ELBO) of log pθ(x0) as Eq.24. The same form of ELBO which is invariant to noise schedule but related to the signal-to-noise ratio (SNR) is also introduced in Kingma et al. (2021); Shi et al. (2024). Following Austin et al. (2021); Zheng et al. (2024a), we choose the noise schedule αt = 1 t, then α 1αt = 1 . 19 The previous discussion focused on the single token xt, and can be easily extended to text sequence of length represented as xt = [x1 ]. The final loss of the whole sequence is , x2 L1:N = 1 Eq(xtx0) . . . , xN (cid:34) (cid:88) n=1 δxn ,m(xn 0 ) log fθ(x1:N (cid:35) )n , (25) where fθ(x1:N )n denotes the whole input sequence is fed into the transformer model and the n-th output token is indexed. During training, we sample for each data point to optimize the expectation in L1:N instead of the integral LT , while for evaluation, we use integral LT ."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 TRAINING DATA DiffuGPT Previous diffusion language models such as Plaid 1B (Gulrajani & Hashimoto, 2023), SEDD (Lou et al., 2024) and MD4 (Shi et al., 2024) use OpenWebText (Gokaslan & Cohen, 2019) to pre-train from scratch, referring to GPT2 (Radford et al., 2019). We choose the advanced FineWeb3 corpus (Penedo et al., 2024), which is also derived from Common Crawl. We randomly sample 30 billion tokens from subset sample-100BT. DiffuLLaMA Following Zhang et al. (2024b)4 we construct the training data for DiffuLLaMA by mixing SlimPajama (Soboleva et al., 2023) and Starcoder data (Li et al., 2023a). We randomly sample 65 billion tokens in the ratio of 7:3 from SlimPajama and Starcoder, respectively. We use sequence packing and pre-tokenize the dataset for efficient computing. B.2 MODEL OPTIMIZATION AND HYPERPARAMETERS DiffuGPT We implement DiffuGPT using LLaMA-Factory5 with DeepSpeed Zero-2 parallelization (Rajbhandari et al., 2020). The hyperparameter setting compared with previous work is listed in Table 4. The global batch size is calculated by multiplying the single GPU batch size, the number of gradient accumulation steps, and the number of GPUs, where we use 8 A100 80G. We use learning rate of 3e 4 with cosine scheduler. The warm up steps are set to 2K and attention mask annealing steps are 10K. As shown in Table 4, our effective training tokens are less or equal than SEDD and MD4, while DiffuGPT exhibits better performance according to Table 1 and Figure 3. Table 4: Training settings for different diffusion language models. Models Training steps Global batch size Context length SEDD (Lou et al., 2024) MD4 (Shi et al., 2024) DiffuGPT-S DiffuGPT-M 400k 1000k 1000k 160k 512 512 256 1280 1024 1024 512 DiffuLLaMA For more efficient pre-training, we implement DiffuLLaMA using huggingface6. We use DeepSpeed Zero-3 parallelization with CPU offloading (Rajbhandari et al., 2020) to efficiently scale DiffuLLaMA to multiple GPUs and nodes. Furthermore, we use flash-attention 2 and fused cross-entropy loss for optimized GPU memory usage and compute time (Dao, 2024). With these settings, we get set batch size of 60 per GPU with context length 2048 on GH200 96GB GPU. We use AdamW (Loshchilov & Hutter, 2019) to optimize our models with constant learning rate of 2e 5 and accumulate gradients every 4 steps. We train our model for 65 billion tokens on 16 4xGH200 nodes. 3https://huggingface.co/datasets/HuggingFaceFW/fineweb 4https://github.com/jzhang38/TinyLlama 5https://github.com/hiyouga/LLaMA-Factory 6https://github.com/huggingface/transformers 20 Tokenizer During adaptation, we do not change the tokenizer of the base model. In theory, we should expand the original vocabulary by adding an additional dimension to include special token as [MASK] token. However, considering practical issues on implementation, we can alternatively select an existing word from the vocabulary to serve as the [MASK] token. It is preferable that this chosen word has particularly low frequency of occurrence in corpus. For DiffuGPT-S we use tokenid=10541 and for DiffuGPT-M we set new [MASK] token with tokenid=50257. For DiffuLLaMA, we set tokenid=811. B.3 EVALUATION DETAILS Generation tasks For the TriviaQA and Lambada sentence completion tasks, we generate ntokens for continue-writing. In triviaQA, we set to the oracle length plus an additional 10 tokens, and we only evaluate the first 2000 cases in this dataset for efficiency. For Lambada, which requires the completion of the last word, we set to oracle length of that words tokens, which might be larger than 1 based on the tokenizer. For DLMs, we set the diffusion timesteps to the required generation length. For AR baselines, we cut off maximum new tokens. For SATMATH and MAWPS, we integrate our model into math-evaluation-harness7. CommonSense Reasoning tasks The 4 commonSense reasoning tasks are multiple-choices questions with 4 options. Instead of open generation, we calculate the diffusion loss for each Question+choice pair using Eq.25. lower loss (perplexity) indicates the model thinks that choice most suitable. This approach is commonly employed in ICL of LLMs (Wu et al., 2023). We also use this for AR baselines. Finetune GSM8K-symbolic The setting of finetune GSM8K-symbolic dataset is following Ye et al. (2024)8, which enables the diffusion model to perform chain-of-thought reasoning. For DiffuLLaMA, we use parameter-efficient-finetune: LoRA Tuning (Hu et al., 2022). We set rank to 8 and enable the finetuning of the word embedding layer, with 151 million (2%) parameters involved. For this task, we use = 64 for the decoding of DLMs. Infilling tasks For ROCstories, where each case is 5-sentence story, we setup evaluation referring Shen et al. (2023).The model is tasked with infilling the third sentence based on the first two and last two sentences. We evaluate the first 1000 cases in this dataset for efficiency. For code infilling, we use humaneval-single-line infilling 9 and their evaluation toolkit, which contains 1033 test cases. We implement infilling tasks for AR models by feeding the prefix and cutting off the generation length using the oracle length, considering that these AR models are not supporting infilling. We also try to feed the suffix information using the instruction like Given prefix and suffix please infill the middle, however, LLaMA2 can not follow this instruction. For AR LLMs, to perform infilling tasks requires additional FIM training (Roziere et al., 2023) or carefully instruction tuning. Unconditional Generation For unconditional generation in Figure 3, we set the temperature of top-k to 0.98 and top-p to 0.9 for the medium-sized model, while using top-k of 1.0 and top-p of 0.9 for the small model. We generate 64 samples and evaluate the perplexity using the GPT-2 large model, aligning with Lou et al. (2024); Shi et al. (2024)."
        },
        {
            "title": "C ADDITIONAL RESULTS",
            "content": "C.1 UNCONDITIONAL GENERATION The generation quality is different for different hyperparameters, shown in Figure 5. Lowering the temperature increases fluency but reduces diversity, leading to noticeable repetition in sentences. We randomly selected samples generated by our DiffuGPT-M models with 1024 tokens for various , as shown in Table 6, Table 7, and Table 8. Lower values result in less fluent text. 7https://github.com/ZubinGou/math-evaluation-harness 8https://github.com/HKUNLP/diffusion-of-thoughts 9https://github.com/openai/human-eval-infilling 21 Figure 5: The unconditional generation quality for different diffusion time steps and sampling algorithms. We annotate the temperature of top-k sampling and top-p sampling. Figure 6: Finetune GSM8K data with discrete diffusion objectives, using base model of either GPT2-S/M or DiffuGPT-S/M. DiffuGPT converges faster and attains lower loss. C.2 ABLATION ON GSM8K-SYMBOLIC Using the same discrete diffusion loss, if we direct finetune on GPT2 achieves accuracy of 45.4 and 49.7 for small and medium models, respectively. In contrast, Finetuning from DiffuGPT yields accuracy of 50.2 and 61.8  (Table 1)  . Comparing with GPT2, DiffuGPT, as the base model, converges faster and attains lower loss, as shown in Figure 6. This indicates that better base model leads to improved results and also demonstrates the superiority of DiffuGPT as the current best diffusion base model. C.3 DECODING SPEED TESTING We evaluate the inference time of LLaMA2 and DiffuLLaMA for unconditional text generation across various lengths. Our tests include vanilla attention, flash attention 2, and the torch version of flash attention SDPA, as shown in Table 5. Table 5: Single batch inference time for different attention implementation and generation lengths. Length Attention DiffuLLaMA (sec) LLaMA (sec) 512 1024 1024 1024 2048 2048 2048 flash-attention SDPA flash-attention 2 vanilla SDPA flash-attention 2 vanilla 12.5 13.2 13.3 16.2 28.5 23.5 38.1 9.2 16.3 17.5 17.2 29.5 35.7 32.8 Table 6: Generation examples of DiffuGPT-M (T = 1024). If youre considering applying to the JBCC school, Im confident you will find something there! What exactly do the schools have required? HAVE DREAMED (I know its not in my DNA). In fact, have my life time. What do you need anyway? - MA or PhD degree in non-religious education and/or MA or PhD degree. - Our Schools ensure that you have strong academic background in the non-religious field of your choice and passion for teaching in literature, research, writing, reading, English as second language, or teaching. - The concentration may help to provide world-class emphasis or offer unique opportunities for teaching of new skills or in dynamic contexts. - Practical Program coursework enables students to study in specific fields and areas in their career, this may be an essential part of education if you work as tutor or if your goal will include working in an office, as high school social science teacher, or classroom. What makes JBCC unique? The National International Baccala Practical Programs are located locally and offer JBCCs scope and the preparation to complete an advanced masters degree. How do students go to graduate from JBCC? Get in now! Congratulations for applying for the JBCC program. Become part of this community. What does JBCC achieve? JBCC prepares leaders to be catalyst for an educational and emotional enriching environment for service. Leaders are within their capacity to call others to service. Want to know more? To find additional information, please use this form. You can reach us to find your application materials for JBCC here. Christine Callender Educator and Director School of Christian Education Hello there everyone! Welcome to our tutorials section. We have everything you need to know what and how to make quality t-shirts apparel especially for those who are new to t-shirts. If you are not sure, they are very popular item. As you might imagine, there are wider audience of people than others. - Tt shirt how to make yourself? - T-shirt to wear? - What can use to make my shirt? ... 4. Do need to reuse my tee shirt? There is no need to reuse your tee shirt. This means you can spend few minutes putting it on another pair of clothing. Polyester is great for quick clean, allowing you to get out the stains on the front of your tee shirt, without damaging them. Since polyester is very breathable, which means it is able to be used on both your skin and anything else 23 Table 7: Generation examples of DiffuGPT-M (T = 256). Use of the Service: Cookies may enable your internet site or your computer or device to access information using our Services, so as to allow us to work together to provide the website that you use. Cookies may provide Personal Information in certain cases. Google Digital Advertising Cookies Advertising partners may use internet analytics with information on your websites visit, how far you come, through our different advertising networks in order to tailor them for you. This may provide your personal information to our advertising partners and may be shared with third-party advertisers. Google may also use tracking cookies that analyse visitor preferences, such as the type of device used, visits to pages the user has access to most frequently, or how frequently visitors visit particular pages, to analyse how people find what sort of information most interest to them. They can keep track of all such visits as they collect more information, using cookies and analytics to improve the content of our website. For more information on how to control Googles cookies on our websites, see https://geo.com/sies to learn moreLogan has been very popular among travelers in the area since 1845. The early explorers had place and thats the best place to stay after long journey. Fast-forward the years, and the ones that stayed in Logan migrated before even crossing over into the mountains to the north. These changes have made it from midwestern city to postmodern city of sorts where everyone lives. This is what global locations mean. Its easy enough to get lost after day spent traveling, but every city offers different experiences and fun when it comes to being on the road. The most important parts of Logan experience are the historic districts of Logan Square. Discover the history of so many different places and gather with people who enjoy exploring something new, or even if with children or pets in tow. Discover the Broadway complex of Logan Square and the rest of the neighborhood, as its charm sprinkled with charm. Sitting the North of Telegraph Hill one mile from Loomers you will find the Logan mountain trail. The 18-mile-long trail is one of the finest in the state and enjoy day on your bike or biking, surrounded by all sorts of cafes, unique eateries and art galleries. Lawsony Park has the Home Park Beer Cellar and the Home Park Bike Shop, and great gift shop for that. Explore the neighborhood on Memorial weekend and warm up with delicious bite from Logan Friendly Brews. week on Memorial Weekend is new tradition for the area of Loomers. Heres the excitement of heading on the road on one of the weekends of the year! The summer at Logan is one of the most popular times of year to explore the quirky neighborhoods. Our parks are vibrant and busy. We are downtown and our restaurant and bars stay open to have drink and coffee. Stop in at the rooftop patio to enjoy really nice evening with quick bite. And every week on Memorial Weekend, youll find really nice indoor pool! For kids with weekend, Logan has playground and good for two and on August 4 and 8 is the Logan Fun Day. few spots are available (and were always open to both men and women) and your group allows you to just catch up and season with friends while enjoying pint of beer. John Moody Brews is about to bring their Labor Day fun to the streets and your backyard on Labor Day Weekend. Logan Friendly Brews has brewery cruise, street vendors and live music on Thursday night and theyll have giveaways on Sunday. Its fun 24 Table 8: Generation examples of DiffuGPT-M in 1024 tokens (T = 32). work in near-zero conditions. In the meantime, employees are planning to avoid travel, and lack of traveling. The United States will not pay or pay for travel, Grand Tech said. But the company plans to continue to take its staff to Asia and other destinations. The plans for the next trip will vary. The Philippine office is investigating the outbreak and is considering further expanding to more but still non-fue areas. There may also be more traveling than they had previously been doing before. The company reports, confirmed first by ETOYs Sulayo Suan, denied to local media. Grand Tech has its headquarters in Cincinnati, Ohio. Delta variant virus caused plant disaster nearly two months ago. The the genotype 3 virus of the virus hit an employee while on the job there in April. The woman reported that she needed to remain home confinement until August. But succumbed continued serious illness to test results for weeks. The Delta variant is currently targeting companies in the European Union, with the virus occurring in Italy. French workers have also been reported suffering from the outbreak. Employees have been traveling to work due to some journeys to Asia and respiratory complications to travel to Asia. The company also reported significant risks to their herbal products containing Chinese plant material, including some hogweed. Hogweed alone is responsible for the loss of 1 U.S stem to each year, Grand Tech said. The U.S. is offering an ongoing vaccination program available to 72 million all Americans, including those most at risk from the virus, State Street announced.Globular epilepsy is genetic condition that is at risk.. ORIK can occur in as high as 15% of people living in Indonesia. However, can discuss information that exists that ORIK. There no factors that could be MeanESK 2 is class II agent with teratogenic activity as determined by the MSL. We propose not treatment of ORIK on patients that exceed the 2.0 threshold. Both biological factors predispose risk of the. ORISA-WEVINB8: at leastISMRC IX. Non-Mouse type 8a(d12). About both factors that promote the development of ORIK. To specify reasonable regulation that would affect p.ss.The United States Juvenile Court as an Authority for Administration of the Bureau of Corrections by Arthur Whyte, Jr. Chair: John F. Bronz Board: Buck Morgan Jr. Rep: John Little Reader: David Gervis. THURY Fisher, WOOD and her son. DATE: March 23, 1959 REV: July 3, 1949 By resolution which is passed: Either person uses nothing more than boat within warehouse, apartment or barn. Department of the Interior or Department of Labor or system of it existed as whole under the laws of Michigan three. through December one of such it would not. Each $100 person shall be fined and the division shall collect all damages of each two hundred dollars and dollars the total of such amount and enter a. Rights reserved: also trustee thereof may order the delinquent bonds. : If person convicted if that has committed any act of attachment. Assisting duties shall deemed to have ceased within fifteen days, then the Commissioner to be appointed shall pay the district the sum of four percent of the value of the money recovered from these costs. Pursuant in this section shall be revoked without notice. Revocation: also the equivalent of instructions accompanied with. Fifteen days of the entry of such order. : The blight or place known to be in person has an occupancy.k enables. He can throw in an about picture. How he loves it so much he is the greater these online webit himself right from scuba dive and wishes to be removed by the government of America, thats much sleazier story. From time social circles, the members of MoolahFast, dating site focusing on the planet of different scuba divers,"
        }
    ],
    "affiliations": [
        "Apple",
        "Tencent AI Lab",
        "The University of HongKong",
        "University of Illinois at Urbana-Champaign"
    ]
}