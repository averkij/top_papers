{
    "paper_title": "Whole-Body Conditioned Egocentric Video Prediction",
    "authors": [
        "Yutong Bai",
        "Danny Tran",
        "Amir Bar",
        "Yann LeCun",
        "Trevor Darrell",
        "Jitendra Malik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 2 5 5 1 2 . 6 0 5 2 : r Whole-Body Conditioned Egocentric Video Prediction Yutong Bai 1 Danny Tran 1 Amir Bar 2 Yann LeCun 2,3 Trevor Darrell Jitendra Malik 1,2 1UC Berkeley (BAIR) 2FAIR, Meta 3New York University"
        },
        {
            "title": "Abstract",
            "content": "We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, large-scale dataset of real-world egocentric video and body pose capture. We further design hierarchical evaluation protocol with increasingly challenging tasks, enabling comprehensive analysis of the models embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of human."
        },
        {
            "title": "Introduction",
            "content": "Human movement is rich, continuous, and physically grounded (Rosenhahn et al., 2008; Aggarwal and Cai, 1999). The way we walk, lean, turn, or reachoften subtle and coordinateddirectly shapes what we see from first-person perspective. For embodied agents to simulate and plan like humans, they must not only predict future observations (Von Helmholtz, 1925), but also understand how visual input arises from whole-body action (Craik, 1943). This understanding is essential because many aspects of the environment are not immediately visiblewe need to move our bodies to reveal new information and achieve our goals. Vision serves as natural signal for long-term planning (LeCun, 2022; Hafner et al., 2023; Ebert et al., 2018; Ma et al., 2022). We look at our environment to plan and act, using our egocentric view as predictive goal (Sridhar et al., 2024; Bar et al., 2025). When we consider our body movements, we should consider both actions of the feet (locomotion and navigation) and the actions of the hand (manipulation), or more generally, whole-body control (Nvidia et al., 2025; Cheng et al., 2024; He et al., 2024b; Radosavovic et al., 2024; He et al., 2024a; Hansen et al., 2024). For example, when reaching for an object, we must anticipate how our arm movement will affect what we see, even before the object comes into view. This ability to plan based on partial visual information is crucial for embodied agents to operate effectively in real-world environments. Building model that can effectively learn from and predict based on whole-body motion presents several fundamental challenges. First, representing human actions requires capturing both global body dynamics and fine-grained joint articulations, which involves high-dimensional, structured data with complex temporal dependencies. Second, the relationship between body movements and visual perception is highly nonlinear and context-dependentthe same arm movement can result in * Equal contribution; Equal advising. 1Project page: https://dannytran123.github.io/PEVA. Figure 1: Predicting Ego-centric Video from human Actions (PEVA). Given past video frames and an action specifying desired change in 3D pose, PEVA predicts the next video frame. Our results show that, given the first frame and sequence of actions, our model can generate videos of atomic actions (a), simulate counterfactuals (b), and support long video generation (c). different visual outcomes depending on the environment and the agents current state. Third, learning these relationships from real-world data is particularly challenging due to the inherent variability in human motion and the subtle, often delayed visual consequences of actions. To address these challenges, we develop novel approach PEVA that combines several key innovations. First, we design structured action representation that preserves both global body dynamics and local joint movements, using hierarchical encoding that captures the kinematic tree structure of human motion. This representation enables the model to understand both the overall body movement and the fine-grained control of individual joints. Second, we develop novel architecture based on conditional diffusion transformers that can effectively model the complex, nonlinear relationship between body movements and visual outcomes. The architecture incorporates temporal attention mechanisms to capture long-range dependencies and specialized action embedding component that maintains the structured nature of human motion. Third, we leverage large-scale dataset of synchronized egocentric video and motion capture data (Ma et al., 2024), which provides the necessary training signal to learn these complex relationships. Our training strategy includes random timeskips to handle the delayed visual consequences of actions and sequence-level training to maintain temporal coherence. For evaluation, we first assess the models ability to predict immediate visual consequences by evaluating its performance on single-step predictions at 2-second intervals, measuring both perceptual quality (LPIPS (Zhang et al., 2018)) and semantic consistency (DreamSim (Fu et al., 2023)). Second, we decompose complex human movements into atomic actionssuch as hand movements (up, down, left, right) and whole-body movements (forward, rotation)to test the models understanding of how specific joint-level movements affect the egocentric view. This fine-grained analysis reveals whether the model can capture the nuanced relationship between individual joint movements and their visual effects. Third, we examine the models capability to predict long-term visual consequences by evaluating its performance across extended time horizons (up to 16 seconds), where the effects of actions may be delayed or not immediately visible. Finally, we explore the models ability to serve as world model for planning by using it to simulate actions and choose the ones that lead to predefined goal. This layered approach allows us to systematically analyze the strengths and limitations of our model, revealing both its capacity to simulate embodied perception and the open challenges that remain in bridging the gap between physical action and visual experience. To conclude, we introduce PEVA, model that predicts future egocentric video conditioned on whole-body human motion. By leveraging structured action representations derived from 3D pose trajectories, our model captures the intricate relationship between physical movement and visual 2 perception. We develop diffusion-based architecture that can be trained auto-regressively over sequence of images in parallelized fashion. Our model utilizes random time-skips that enable covering long-term videos efficiently. Trained on Nymeria (Ma et al., 2024), large-scale realworld dataset of egocentric video and synchronized motion, PEVA advances embodied simulation with physically grounded, visually realistic predictions. Our comprehensive evaluation framework demonstrates that whole-body control significantly improves video quality, semantic consistency, and simulating counterfactual."
        },
        {
            "title": "2 Related Works",
            "content": "World Models. The concept of world model, an internal representation of the world used for prediction and planning, has rich history across multiple disciplines. The idea was first proposed in psychology by Craik (1943), who hypothesized that the brain uses small-scale models of reality to anticipate events. This principle found parallel development in control theory, where methods like the Kalman Filter and Linear Quadratic Regulator (LQR) rely on an explicit model of the system to be controlled (Kalman, 1960). The idea of internal models became central to computational neuroscience for explaining motor control, with researchers proposing that the brain plans and executes movements by simulating them first (Jordan, 1996; Kawato et al., 1987; Kawato, 1999). With the rise of deep learning, the focus shifted to learning these predictive models directly from data. Early work in computer vision demonstrated that models could learn intuitive physics from visual data to solve simple control tasks like playing billiards or poking objects (Fragkiadaki et al., 2015; Agrawal et al., 2016). This paved the way for modern, large-scale world models that predict future video frames conditioned on actions, enabling planning by imagining future outcomes (Ha and Schmidhuber, 2018; Hafner et al.; Liu et al., 2024; Li et al., 2022; Zhou et al., 2024; Yang et al., 2023, 2024; Assran et al., 2025). In reinforcement learning, models like Dreamer have shown that learning world model improves sample efficiency (Hafner et al., 2023). Recent approaches have used diffusion models for more expressive generation; for example, DIAMOND generates multi-step rollouts via autoregressive diffusion (Alonso et al., 2024). In the egocentric domain, Navigation World Models (NWM) used conditional diffusion transformers (CDiT) to predict future frames from planned trajectory (Bar et al., 2025). However, these models use low-dimensional controls and neglect the agents own body dynamics. We build on this extensive line of work by conditioning video prediction on whole-body pose, enabling more physically-grounded simulation. Human Motion Generation and Controllable Prediction. Human motion modeling has advanced from recurrent and VAE-based methods (Rempe et al., 2021; Petrovich et al., 2021; Ye et al., 2023) to powerful diffusion-based generators (Tevet et al., 2022; Zhang et al., 2024). These models generate diverse, realistic 3D pose sequences conditioned on text (Hong et al., 2024; Guo et al., 2022; Dabral et al., 2023), audio (Ng et al., 2024; Dabral et al., 2023; Ao et al., 2023), and head pose (Li et al., 2023; Castillo et al., 2023; Yi et al., 2025). Recent works like Animate Anyone (Hu et al., 2023) and MagicAnimate (Xu et al., 2023) generate high-fidelity human animations from reference image and pose sequence. Physically-aware extensions like PhysDiff (Yuan et al., 2023) incorporate contact into the denoising loop. While prior works treat pose as the target, our model uses it as input for egocentric video prediction, reversing the typical motion generation setup. This enables fine-grained visual control, bridging pose-conditioned video generation (Wu et al., 2023; Zhang et al., 2023) with embodied simulation. Unlike Make-a-Video (Singer et al., 2022) or Tune-A-Video (Wu et al., 2023), which focus on text/image prompts, we condition directly on physically realizable body motion. Egocentric Perception and Embodied Forecasting. Egocentric video datasets such as Ego4D (Grauman et al., 2022), Ego-Exo4D (Grauman et al., 2024) and EPIC-KITCHENS (Damen et al., 2018) were used to study human action recognition, object anticipation (Furnari and Farinella, 2020), future video prediction (Girdhar and Grauman, 2021), and even animal behavior (Bar et al., 2024). To study pose estimation, EgoBody (Zhang et al., 2022) and Nymeria (Ma et al., 2024) provide synchronized egocentric video and 3D pose. Unlike these works, we treat future body motion as control signal, enabling visually grounded rollout. Prior works in egocentric pose forecasting (Yuan and Kitani, 2019) and visual foresight (Finn and Levine, 2017) show that predicting future perception supports downstream planning. Our model unifies these lines by predicting future egocentric video from detailed whole-body control, enabling first-person planning with physical and visual realism. 3 Figure 2: Design of PEVA. To train on an input video, we choose random subset of frames and encode them via fixed encoder (a). They are then fed to CDiT that is trained autoregressively with teacher forcing (b). During the denoising process, each token attends to same-image tokens and cross-attends to clean tokens from past image(s). Action conditioning is done via AdaLN layers."
        },
        {
            "title": "3 PEVA",
            "content": "In this section we describe our whole-body-conditioned ego-centric video prediction model. We start by describing how to represent human actions (Section 3.1), then move on to describe the model and the training objective (Section 3.2). Finally, we describe the model architecture in Section 3.3."
        },
        {
            "title": "3.1 Structured Action Representation from Motion Data",
            "content": "To effectively capture the relationship between human motion and egocentric visual perception, we define each action as high-dimensional vector encoding both global body dynamics and detailed joint articulations. Rather than relying on simplified or discrete controls, our framework uses fullbody motion information, including global translation (via the root joint) and relative joint rotations structured by the kinematic tree. This design ensures that the action space richly represents human movement at both coarse and fine levels. To construct this representation, we synchronize motion capture data with video frames based on timestamps, then transform global coordinates into local frame centered at the pelvis. This transformation makes the data invariant to initial position and orientation. Global positions are converted to local coordinates, quaternions to relative Euler angles, and joint relationships are preserved using the kinematic hierarchy. We normalize all motion parameters for stable learning: positions are scaled to [1, 1] and rotations bounded within [π, π]. Each action reflects the change between consecutive frames, capturing motion over time and enabling the model to learn how physical actions translate into visual outcomes."
        },
        {
            "title": "3.2 Ego-Centric Video Prediction for Whole-Body Control",
            "content": "Next, we describe our formulation of PEVA from the perspective of an embodied agent. Intuitively, the model is an autoregressive diffusion model that receives an input video and corresponding sequence of actions describing how the agent moves and acts. Given any prefix of frames and actions, the model predicts the resulting state of the world after applying the last action and considering other environment dynamics. More formally, we are given dataset = {(x0, a0, ..., xT , aT )}n i=1 of agents videos from egocentric view and their associated body controls, such that every xj RHW 3 is video frame and aj Rdact an action in the Xsens skeleton ordering (Movella, 2021) for the upper body (everything above the pelvis), representing the change in translation, together with the delta rotation of all joints relative to the previous joint rotation. We represent motion in 3D space, thus we have 3 degrees of freedom for root translation, 15 joints for the upper body and represent relative joint rotations as Euler angles in 3D space leaving dact = 3 + 15 3 = 48. We start by encoding each individual frame si = enc(xi) into corresponding state representation, through pre-trained VAE encoder (Rombach et al., 2022). Given sequence of controls a0, . . . aT , 4 our goal is to build generative model that captures the dynamics of the environment: (sT , . . . s0aT , . . . a0) = (s0) 1 (cid:89) t=0 (st+1st, . . . , s0, aT , . . . a0) (1) To simplify the model, we factorize the distribution and make Markov assumption that the next state is dependent on the last states and single past action: (st+1st, . . . , s0, aT , . . . a0) = (st+1st, . . . , stk+1, at1) (2) We aim to train model parametrized by θ that minimizes the negative log-likelihood: (cid:34) ˆθ = arg min θ log Pθ(s0) 1 (cid:88) t=0 log Pθ(st+1st, . . . , stk+1, at) (cid:35) We model each transition Pθ(st+1st, . . . , stk+1, at) using Denoising Diffusion Probabilistic Model (DDPM) (Ho et al., 2020), which maximizes the (reweighted) evidence lower bound (ELBO) of the log-likelihood. For each transition, we define the forward diffusion process q(zτ st+1) = (zτ ; ατ st+1, (1 ατ )I), where zτ is the noisy version of st+1 at noise timestep τ , and ατ is the cumulative product of noise scales. The reverse process is learned by training neural network ϵθ to predict the noise given zτ and the conditioning context ct = (st, . . . , stk+1, at). Then denoising loss term for transition is given by: (cid:0) Lsimple, = Eτ,ϵN (0,I) (cid:104)(cid:13) (cid:13)ϵ ϵθ ατ st+1 + 1 ατ ϵ, ct, τ (cid:1)(cid:13) (cid:13) 2(cid:105) (3) Where Lsimple, 0 is the loss term corresponding to the unconditional generation of s0. Additionally, we also predict the covariances of the noise, and supervise them using the full variational lower bound loss Lvlb,t as proposed by (Nichol and Dhariwal, 2021). Hence the final objective yields (weighted) version of the ELBO for each term in the sequence: = 1 (cid:88) t= Lsimple,t + λLvlb,t (4) Despite not being lower bound of the log-likelihood, the reweighted ELBO works well in practice for image generation with transformers (Nichol and Dhariwal, 2021; Peebles and Xie, 2023). The advantage of our formulation is that it allows training in parallelized fashion using causal masking. Given sequence of frames and actions, we can train on every prefix of the sequence in single forward-backward pass. Next, we elaborate on the architecture of our model."
        },
        {
            "title": "3.3 Autoregressive Conditional Diffusion Transformer",
            "content": "While prior work in navigation world models (Bar et al., 2025) focuses on simple control signals like velocity and heading, modeling whole-body human motion presacents significantly greater challenges. Human activities involve complex, coordinated movements across multiple degrees of freedom, with actions that are both temporally extended and physically constrained. This complexity necessitates architectural innovations beyond standard CDiT approaches. To address these challenges, we extend the Conditional Diffusion Transformer (CDiT) architecture with several key modifications that enable effective modeling of whole-body motion: Random Timeskips. Human activities often span long time horizons with actions that can take several seconds to complete. At the same time, videos are raw signal which requires vast amounts of compute to process. To handle video more efficiently, we introduce random timeskips during training (see Figure 2a), and include the timeskip as an action to inform the models prediction. This allows the model to learn both short-term motion dynamics and longer-term activity patterns. Learning long-term dynamics is particularly important for modeling activities like reaching, bending, or walking, where the full motion unfolds over multiple seconds. In practice, we sample 16 video frames from 32 second window. Sequence-Level Training. Unlike NWM which predicts single frames, we model the entire sequence of motion by applying the loss over each prefix of frames following Eq. 4. We include an example of 5 this in Figure 2b. This is crucial because human activities exhibit strong temporal dependencies - the way someone moves their arm depends on their previous posture and motion. We enable efficient training by parallelizing across sequence prefixes through spatial-only attention in the current frame and past-frame-only attention for historical context (Figure 2c). In practice we train models with sequences of 16 frames. Action Embeddings. The high-dimensional nature of whole-body motion (joint positions, rotations, velocities) requires careful handling of the action space. We take the most simple strategy: we concatenate all actions in time into 1D tensor which is fed to each AdaLN layer for conditioning (see Figure 2c). These architectural innovations are essential for modeling the rich dynamics of human motion. By training on sequence prefixes and incorporating timeskips, our model learns to generate temporally coherent motion sequences that respect both short-term dynamics and longer-term activity patterns. The specialized action embeddings further enable precise control over the full range of human movement, from subtle adjustments to complex coordinated actions. 3."
        },
        {
            "title": "Inference and Planning with PEVA",
            "content": "Sampling procedure at test time. Given set of context frames (xt, ..., xtk+1), we encode these frames to get (st, ..., stk+1) and pass the encoded context as the clean tokens in Figure 2b and pass in randomly sampled noise as the last frame. We then follow the DPPM sampling process to denoise the last frame conditioning on our action. For faster inference time, we employ special attention masks where we change the mask in Figure 2c for within image attention to only be applied on the tokens of the last frame and change the mask for cross attention to context so that cross attention is only applied for the last frame. Autoregressive rollout strategy. To follow set of actions we use an autoregressive rollout strategy. Given an initial set of context frames we (xt, ..., xtk+1) we start by encoding each individual frame to get (st, ..., stk+1) and add the current action to create the conditioning context ct = (st, ..., stk+1, at). We then sample from our model parameterized by θ to generate the next state: st+1 = Pθ(st+1ct). We then discard the first encoding and append the generated st+1 and add the next action to produce the next context ct+1 = (st+1, st, ..., stk+1, at+1). We then repeat the process for our entire set of actions. Finally, to visualize the predictions, we decode the latent states to pixels using the VAE decoder Rombach et al. (2022)."
        },
        {
            "title": "4.1 Experiment Setting",
            "content": "Dataset. We use the Nymeria dataset (Ma et al., 2024), which contains synchronized egocentric video and full-body motion capture, recorded in diverse real-world settings using an XSens system (Movella, 2021). Each sequence includes RGB frames and 3D body poses in the XSens skeleton format, covering global translation and rotations of body joints. We sample body motions at 4 FPS. Videos are center-cropped and resized to 224224. We split the dataset 80/20 for training and evaluation, and report all metrics on the validation set. Training Details. We train variants of Conditional Diffusion Transformer (CDiT-S to CDiT-XXL, up to 32 layers) using context window of 315 frames and predicting 64-frame trajectories. Models operate on 22 patches and are conditioned on both pose and temporal embeddings. We use AdamW (lr=8e5, betas=(0.9, 0.95), grad clip=10.0) and batch size 512. Action inputs are normalized to [1, 1] for translation and [π, π] for rotation. All experiments use Stable Diffusion VAE tokenizer and follow NWMs hardware and evaluation setup. Metrics are averaged over 5 samples per sequence."
        },
        {
            "title": "4.2 Comparison with Baselines",
            "content": "To comprehensively evaluate our model, we compare PEVA with (CDiT Bar et al. (2025) and Diffusion Forcing (Chen et al., 2024)) along two key dimensions. First, to assess whether the model faithfully simulates future observations conditioned on actions, we use LPIPS (Zhang et al., 2018) and DreamSim (Fu et al., 2023), which measure perceptual and semantic similarity to ground truth. Second, to evaluate the overall quality and realism of the generated samples, we report FID (Heusel 6 et al., 2017). As shown in Table 1, our model achieves better results on both action consistency and generative quality. Furthermore, Figure 3 shows that our models tend to maintain lower FID scores than the baselines as the prediction horizon increases, suggesting improved visual quality and temporal consistency over longer rollouts. Qualitative results for 16 second rollouts can be seen in Figure 1c and Figure 5. We implement Diffusion Forcing (DF) on top of PEVA by applying the diffusion forward process to the entire sequence of encoded latents, then predicting the next state given the previous (noisy) latents. At test time, we autoregressively predict the next state as in PEVA, without injecting noise into previously predicted frames, like Chen et al. (2024). Table 1: Baseline Perceptual Metrics. Comparison of baselines on single-step prediction 2 seconds ahead. Model LPIPS DreamSim FID DF CDiT PEVA 0.3520.003 0.3130.001 0.3030.001 0.2440.003 0.2020.002 0.1930.002 73.0521.101 63.7140.491 62.2930. Figure 3: Video Quality Across Time (FID). Comparison of generation accuracy and quality as function of time for up to 16 seconds. Qualitative results for 16 second rollouts can be seen in Figure 1c and Figure 5."
        },
        {
            "title": "4.3 Atom Actions Control",
            "content": "To better evaluate PEVAs ability to follow structured physical control, we decompose complex motions into atomic actions. By analyzing joint trajectories over short windows, we extract video segments exhibiting fundamental movementssuch as hand motions (up, down, left, right) and whole-body actions (forward, rotate)based on thresholded positional deltas. We sample 100 examples per action type to ensure balanced coverage, and evaluate single-step prediction 2 seconds ahead. Qualitative results are shown in Figure 1a and Figure 4, and quantitative results in Table 2. Table 2: Atomic Action Performance. Comparison of models in generating videos of atomic actions. Model Navigation Left Hand Right Hand Forward Rot.L Rot.R Left Right Up Down Left Right Up Down 0.3930.011 0.3140.006 0.2790.005 0.2920.009 0.3060.005 0.3320.008 0.3230.006 0.3040.006 0.3150.007 0.3050.005 0.2960.008 DF 0.3480.004 0.2840.003 0.2490.004 0.2580.005 0.2650.009 0.2790.008 0.2670.004 0.2860.007 0.2730.004 0.2770.004 0.2680.002 CDiT Ours (XL) 0.3370.006 0.2770.006 0.2420.007 0.2440.005 0.2570.004 0.2720.008 0.2630.003 0.2710.005 0.2670.003 0.2680.004 0.2560.009 Ours (XXL) 0.3250.006 0.2690.005 0.2340.004 0.2360.003 0.2410.003 0.2510.004 0.2470.005 0.2560.007 0.2540.005 0.2520.004 0.2450."
        },
        {
            "title": "4.4 Ablation Studies",
            "content": "We conduct ablation studies to assess the impact of key design choices in PEVA, summarized in Table 3. First, increasing the context window from 3 to 15 frames consistently improves performance across all metrics, highlighting the importance of temporal context for egocentric prediction. Second, model scale plays significant role: larger variants from PEVA-S to PEVA-XXL show steady gains in perceptual and semantic fidelity. Lastly, we compare two action embedding strategiesMLP-based encoding versus simple concatenationand find that the latter performs competitively despite its simplicity, suggesting that our structured action representation already captures sufficient motion information. The gray-highlighted row denotes the default configuration in main experiments."
        },
        {
            "title": "4.5 Long-Term Prediction Quality",
            "content": "We evaluate the models ability to maintain visual and semantic consistency over extended prediction horizons. As shown in Figure 5, PEVA generates coherent 16-second rollouts conditioned on full-body motion. Table 3 reports DreamSim scores at increasing time steps, showing gradual degradation from 0.178 (1s) to 0.390 (16s), indicating that predictions remain semantically plausible even far into the future. 7 Figure 4: Atom Actions Generation. We include video generation examples of different atomic actions specified by 3D-body poses. 8 Figure 5: Generation Over Long-Horizons. We include 16-second video generation examples. Figure 6: Planning with Counterfactuals. We demonstrate planning example by simulating multiple action candidates using PEVA and scoring them based on their perceptual similarity to the goal, as measured by LPIPS (Zhang et al., 2018). In the first case, we show that PEVA enables us to rule out action sequences that leads us to the sink in the top row, and outdoors in the second row. In the second case we show PEVA allows us to find reasonable sequence of actions to open the refrigerator in the third row. PEVA enables us to rule out action sequences that grab the nearby plants and go to the kitchen and mix ingredients. PEVA allows us to choose the most correct action sequences that grab the box from the shelf. 10 Table 3: Model Ablations. We evaluate the impact of different context lengths, action embedding methods, and model sizes on single-step prediction performance (2 seconds into the future). Configuration Context Length 3 frames 7 frames 15 frames Action Representation Metrics LPIPS DreamSim PSNR FID 0.3040.002 0.3040.001 0.3030. 0.1990.003 0.1950.002 0.1930.002 16.4690.044 16.4430.068 16.5110.061 63.9660.421 62.5400.314 62.2930.671 Action Embedding (d = 512) Action Concatenation 0.3170.003 0.3030.001 0.2020.002 0.1930. 16.1950.081 16.5110.061 63.1010.341 62.2930.671 Model Size PEVA-S PEVA-B PEVA-L PEVA-XL PEVA-XXL 0.3700.002 0.3370.001 0.3080.002 0.3030.001 0.2980.002 0.3270.002 0.2460.002 0.2020.001 0.1930.002 0.1860.003 15.7430.060 16.0130.091 16.4170.037 16.5110.061 16.5560. 101.380.450 74.3381.057 64.4020.496 62.2930.671 61.1000."
        },
        {
            "title": "4.6 Planning with Multiple Action Candidates.",
            "content": "We demonstrate sample in which PEVA enables planning with multiple action candidates in Figure 1b and Figure 6. We start by sampling multiple action candidates and simulate each action candidate using PEVA via autoregressive rollout. Finally, we rank each action candidates final prediction by measuring LPIPS similarity with the goal image. We find that PEVA is effective in enabling planning through simulating action candidates."
        },
        {
            "title": "5 Failure Cases, Limitations and Future Directions",
            "content": "While our model demonstrates promising results in predicting egocentric video from whole-body motion, several limitations remain that suggest directions for future work. First, our planning evaluation is preliminarywe only explore simulation-based selection over candidate actions for only the left or right arm. While this provides an early indication that the model can anticipate visual consequences of body movement, it does not yet support long-horizon planning or full trajectory optimization. Extending PEVA to closed-loop control or interactive environments is key next step. Second, the model currently lacks explicit conditioning on task intent or semantic goals. Our evaluation uses image similarity as proxy objective. Future work could explore combining PEVA with high-level goal conditioning or integrating object-centric representations."
        },
        {
            "title": "5.1 Some planning attempts with PEVA",
            "content": "Here we describe how to use trained PEVA to plan action sequences to achieve visual target. We formulate planning as an energy minimization problem and perform standalone planning in the same way as NWM (Bar et al., 2025) using the Cross-Entropy Method (CEM) (Rubinstein, 1997) besides minor modifications in the representation and initialization of the action. For simplicity, we conduct two experiments where we only predict moving either the left or right arm controlled by predicting the relative joint rotations represented as euler angles. For each respective arm we control only the shoulder, upper arm, forearm, and hand leaving our actions space as 4 3 = 12 where we have (ϕshoulder, θshoulder, ψshoulder, ..., ϕforearm, θforearm, ψforearm). We initialize mean (µϕshoulder , µθshoulder , µψshoulder, ..., µϕforearm, µθforearm, µψforearm) and variance (σ2 ) as the mean and variance of the next ψshoulder action across the training dataset for these segments. , ..., σ2 ϕshoulder ψforearm θshoulder ϕforearm θforearm , σ2 , σ2 , σ2 , σ We assume the action is straight continuous motion. Thus we repeat this action for our sequence length, in our case = 8 and optimize the delta actions. The time interval between steps is fixed at = 0.25 seconds. All other hyperparameters remain the same as in NWM (Bar et al., 2025). 11 Table 4: Mean and Variance of relative rotation as euler angles (ϕ, θ, ψ) for arm segments computed across the training dataset. Segment Right Arm Left Arm Statistic Shoulder Upper Arm Forearm Hand Mean Variance Mean Variance Mean Variance Mean Variance (0.0027, 0.0012, 0.0015) (0.0010, 0.0006, 0.0003) (0.0624, 0.0687, 0.1494) (0.0625, 0.0697, 0.1496) (0.0107, 0.0011, 0.0020) (0.0062, 0.0004, 0.0013) (0.1119, 0.1647, 0.1791) (0.0991, 0.1593, 0.1611) (0.0068, 0.0035, 0.0077) (0.0036, 0.0063, 0.0002) (0.1937, 0.2107, 0.2261) (0.1791, 0.2012, 0.2186) (0.0065, 0.0001, 0.004, ) (0.0024, 0.0032, 0.0001) (0.2417, 0.229, 0.2631) (0.2126, 0.2237, 0.2475)"
        },
        {
            "title": "5.2 Qualitative Results",
            "content": "Due to time constraints, we focus our investigation on arm movementsarguably the most complex among body actions. While this remains an open problem, we present preliminary results using PEVA with CEM for standalone planning. This setting simplifies the high-dimensional control space while still capturing key challenges of full-body coordination. Figure 7: In this case, we are able to predict sequence of actions that pulls our left arm in, similar to the goal. Figure 8: In this case, we are able to predict sequence of actions that lowers our left arm, but not the same amount as the groundtruth sequence as we can see in the pose and hand at the bottom of our rollout. Figure 9: In this case, we are able to predict sequence of actions that lowers our left arm that lowers the tissue. However, the goal image still has the tissue visible. 12 Figure 10: In this case, we are able to predict sequence of actions that raises our right arm to the mixing stick. We see limitation with our method as we only predict the right arm so we do not predict to move the left arm down accordingly. Figure 11: In this case, we are able to predict sequence of actions that moves our right arm toward the left but not quite enough. We see limitation with our method as we only predict the right arm so we do not predict any necessary additional body rotations. Figure 12: In this case, we are able to predict sequence of actions that reaches toward the kettle but does not quite grab it as in the goal."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced PEVA, model that predicts egocentric video conditioned on detailed 3D human motion. Unlike prior work that uses low-dimensional or abstract control, PEVA leverages full-body pose sequences to simulate realistic and controllable visual outcomes. Built on conditional diffusion transformer and trained on Nymeria, it captures the link between physical movement and first-person perception. Experiments show that PEVA improves prediction quality, semantic consistency, and finegrained control over strong baselines. Our hierarchical evaluation highlights the value of whole-body conditioning across short-term, long-horizon, and atomic action tasks. While our planning results are preliminary, they demonstrate the potential for simulating action consequences in embodied settings. We hope this work moves toward more grounded models of perception and action for physically embodied intelligence."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors thank Rithwik Nukala for his help in annotating atomic actions. We thank Katerina Fragkiadaki, Philipp Krähenbühl, Bharath Hariharan, Guanya Shi, Shubham Tsunami and Deva Ramanan for the useful suggestions and feedbacks for improving the paper; Jianbo Shi for the discussion regarding control theory; Yilun Du for the support on Diffusion Forcing; Brent Yi for his help in human motion related works and Alexei Efros for the discussion and debates regarding world models. This work is partially supported by the ONR MURI N00014-21-1-2801."
        },
        {
            "title": "References",
            "content": "Jake Aggarwal and Quin Cai. Human motion analysis: review. Computer vision and image understanding, 73(3):428440, 1999. Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. Advances in neural information processing systems, 29, 2016. Eloi Alonso et al. Diamond: Diffusion as model of environment dreams. arXiv preprint arXiv:2401.02644, 2024. Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip: Gesture diffusion model with clip latents. ACM Transactions on Graphics (TOG), 42(4):118, 2023. Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. Amir Bar, Arya Bakhtiar, Danny Tran, Antonio Loquercio, Jathushan Rajasegaran, Yann LeCun, Amir Globerson, and Trevor Darrell. Egopet: Egomotion and interaction data from an animals perspective. In European Conference on Computer Vision, pages 377394. Springer, 2024. Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1579115801, 2025. Angela Castillo, Maria Escobar, Guillaume Jeanneret, Albert Pumarola, Pablo Arbeláez, Ali Thabet, and Artsiom Sanakoyeu. Bodiffusion: Diffusing sparse observations for full-body human motion synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42214231, 2023. Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, and Xiaolong Wang. Expressive whole-body control for humanoid robots. arXiv preprint arXiv:2402.16796, 2024. Kenneth JW Craik. The Nature of Explanation. Cambridge University Press, 1943. Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: framework for denoising-diffusion-based motion synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97609770, 2023. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epickitchens dataset. In Proceedings of the European conference on computer vision (ECCV), pages 720736, 2018. Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 27862793. IEEE, 2017. Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, and Jitendra Malik. Learning visual predictive models of physics for playing billiards. arXiv preprint arXiv:1511.07404, 2015. Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. Antonino Furnari and Giovanni Maria Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE transactions on pattern analysis and machine intelligence, 43(11):40214036, 2020. Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1350513515, 2021. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51525161, 2022. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Nicklas Hansen, Jyothir SV, Vlad Sobal, Yann LeCun, Xiaolong Wang, and Hao Su. Hierarchical world models as visual whole-body humanoid controllers. arXiv preprint arXiv:2405.18418, 2024. Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Omnih2o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning. arXiv preprint arXiv:2406.08858, 2024a. Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Learning human-to-humanoid real-time whole-body teleoperation. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 89448951. IEEE, 2024b. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), pages 68406851, 2020. Fangzhou Hong, Vladimir Guzov, Hyo Jin Kim, Yuting Ye, Richard Newcombe, Ziwei Liu, and Lingni Ma. Egolm: Multi-modal language model of egocentric motions. arXiv preprint arXiv:2409.18127, 2024. Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023. Michael Jordan. Computational aspects of motor control and motor learning. Handbook of perception and action, 2:71120, 1996. Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. Mitsuo Kawato. Internal models for motor control and trajectory planning. Current opinion in neurobiology, 9 (6):718727, 1999. Mitsuo Kawato, Kazunori Furukawa, and Ryoji Suzuki. hierarchical neural-network model for control and learning of voluntary movement. Biological cybernetics, 57:169185, 1987. Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1): 162, 2022. Jiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose estimation via ego-head pose estimation. In CVPR, pages 1714217151, 2023. Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, and Antonio Torralba. 3d neural scene representations for visuomotor control. In Conference on Robot Learning, pages 112123. PMLR, 2022. Huihan Liu, Yu Zhang, Vaarij Betala, Evan Zhang, James Liu, Crystal Ding, and Yuke Zhu. Multi-task interactive robot fleet learning with visual world models. arXiv preprint arXiv:2410.22689, 2024. 15 Qianli Ma et al. Nymeria: massive collection of multimodal egocentric daily motion in the wild. arXiv preprint arXiv:2406.09905, 2024. Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022. Movella. MVN User Manual. Movella, 2021. https://www.movella.com/hubfs/MVN_User_Manual.pdf. Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, and Alexander Richard. From audio to photoreal embodiment: Synthesizing humans in conversations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10011010, 2024. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Proceedings of the 38th International Conference on Machine Learning, pages 81628171. PMLR, 2021. Bjorck Nvidia, Castaneda, Cherniadev, Da, Ding, Fan, Fang, Fox, Hu, Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205, 2023. Mathis Petrovich, Michael Black, and Gül Varol. Action-conditioned 3d human motion synthesis with transformer vae. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1098510995, 2021. Ilija Radosavovic, Sarthak Kamat, Trevor Darrell, and Jitendra Malik. Learning humanoid locomotion over challenging terrain. arXiv preprint arXiv:2410.03654, 2024. Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas Guibas. Humor: 3d human motion model for robust pose estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1148811499, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. Bodo Rosenhahn, Reinhard Klette, and Dimitris Metaxas. Human motion. Understanding, Modeling, Capture, 2008. Reuven Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operational Research, 99(1):89112, 1997. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Ajay Sridhar, Dhruv Shah, Catherine Glossop, and Sergey Levine. Nomad: Goal masked diffusion policies for navigation and exploration. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 6370. IEEE, 2024. Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. Hermann Von Helmholtz. Helmholtzs treatise on physiological optics. Optical Society of America, 1925. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-tovideo generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. arXiv preprint arXiv:2311.16498, 2023. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023. 16 Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024. Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion In Proceedings of the IEEE/CVF conference on computer vision and pattern from videos in the wild. recognition, pages 2122221232, 2023. Brent Yi, Vickie Ye, Maya Zheng, Yunqi Li, Lea Müller, Georgios Pavlakos, Yi Ma, Jitendra Malik, and Angjoo Kanazawa. Estimating body and hand motion in an ego-sensed world. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 70727084, 2025. Ye Yuan and Kris Kitani. Ego-pose estimation and forecasting as real-time pd control. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1008210092, 2019. Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1008210092, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE transactions on pattern analysis and machine intelligence, 46(6):41154128, 2024. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Egobody: Human body shape and motion of interacting people from head-mounted devices. arXiv preprint arXiv:2204.06953, 2022. Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning. arXiv preprint arXiv:2411.04983, 2024."
        },
        {
            "title": "More Qualitative Results",
            "content": "In the main paper, we provide three types of visualization: PEVA can simulate counterfactuals, generate videos of atomic actions, and long video generation. Here, we show more qualitative results following the settings in main paper: Figure 13: Generation Over Long-Horizons. We include 16-second video generation examples. 18 Figure 14: Generation Over Long-Horizons. We include 16-second video generation examples. Figure 15: Generation Over Long-Horizons. We include 16-second video generation examples. 20 Figure 16: Generation Over Long-Horizons. We include 16-second video generation examples. 21 Figure 17: Generation Over Long-Horizons. We include 16-second video generation examples. Figure 18: Generation Over Long-Horizons. We include 16-second video generation examples. 23 Figure 19: Generation Over Long-Horizons. We include 16-second video generation examples. 24 Figure 20: Generation Over Long-Horizons. We include 16-second video generation examples. Figure 21: Generation Over Long-Horizons. We include 16-second video generation examples. 26 Figure 22: Generation Over Long-Horizons. We include 16-second video generation examples. 27 Figure 23: Generation Over Long-Horizons. We include 16-second video generation examples. Figure 24: Generation Over Long-Horizons. We include 16-second video generation examples. 29 Figure 25: Generation Over Long-Horizons. We include 16-second video generation examples."
        }
    ],
    "affiliations": [
        "FAIR, Meta",
        "New York University",
        "UC Berkeley (BAIR)"
    ]
}