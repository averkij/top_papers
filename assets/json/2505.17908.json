{
    "paper_title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback",
    "authors": [
        "Litao Guo",
        "Xinli Xu",
        "Luozhou Wang",
        "Jiantao Lin",
        "Jinsong Zhou",
        "Zixin Zhang",
        "Bolan Su",
        "Ying-Cong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of generative models, general-purpose generation has gained increasing attention as a promising approach to unify diverse tasks across modalities within a single system. Despite this progress, existing open-source frameworks often remain fragile and struggle to support complex real-world applications due to the lack of structured workflow planning and execution-level feedback. To address these limitations, we present ComfyMind, a collaborative AI system designed to enable robust and scalable general-purpose generation, built on the ComfyUI platform. ComfyMind introduces two core innovations: Semantic Workflow Interface (SWI) that abstracts low-level node graphs into callable functional modules described in natural language, enabling high-level composition and reducing structural errors; Search Tree Planning mechanism with localized feedback execution, which models generation as a hierarchical decision process and allows adaptive correction at each stage. Together, these components improve the stability and flexibility of complex generative workflows. We evaluate ComfyMind on three public benchmarks: ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and reasoning tasks. Results show that ComfyMind consistently outperforms existing open-source baselines and achieves performance comparable to GPT-Image-1. ComfyMind paves a promising path for the development of open-source general-purpose generative AI systems. Project page: https://github.com/LitaoGuo/ComfyMind"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 0 9 7 1 . 5 0 5 2 : r ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback Litao Guo HKUST(GZ) Xinli Xu HKUST(GZ) Luozhou Wang HKUST(GZ) Jiantao Lin HKUST(GZ) Jinsong Zhou HKUST(GZ) Zixin Zhang HKUST(GZ)"
        },
        {
            "title": "Bolan Su\nBytedance",
            "content": "Ying-Cong Chen HKUST(GZ), HKUST Figure 1: Overview of generative and editing capabilities supported by ComfyMind."
        },
        {
            "title": "Abstract",
            "content": "With the rapid advancement of generative models, general-purpose generation has gained increasing attention as promising approach to unify diverse tasks across modalities within single system. Despite this progress, existing opensource frameworks often remain fragile and struggle to support complex real-world applications due to the lack of structured workflow planning and execution-level feedback. To address these limitations, we present ComfyMind, collaborative AI system designed to enable robust and scalable general-purpose generation, built on the ComfyUI platform. ComfyMind introduces two core innovations: Indicates Equal Contribution Indicates Corresponding Author Preprint. Under review. Semantic Workflow Interface (SWI) that abstracts low-level node graphs into callable functional modules described in natural language, enabling high-level composition and reducing structural errors; Search Tree Planning mechanism with localized feedback execution, which models generation as hierarchical decision process and allows adaptive correction at each stage. Together, these components improve the stability and flexibility of complex generative workflows. We evaluate ComfyMind on three public benchmarks: ComfyBench, GenEval, and ReasonEdit, which span generation, editing, and reasoning tasks. Results show that ComfyMind consistently outperforms existing open-source baselines and achieves performance comparable to GPT-Image-1. ComfyMind paves promising path for the development of open-source general-purpose generative AI systems. Project page: https://github.com/LitaoGuo/ComfyMind"
        },
        {
            "title": "Introduction",
            "content": "The rapid development of visual generative models has demonstrated remarkable performance across multiple generative tasks, including text-to-image generation [13], image editing [46], and video generation [79]. In recent years, research has gradually shifted towards end-to-end general-purpose generative models [1013], aiming to handle these diverse tasks within single unified model. However, existing open-source general-purpose generative models [1012] still face series of challenges, including instability in generation quality and lack of structured planning and composition mechanisms required to handle complex, multi-stage visual workflows. These limitations affect the models performance and robustness in real-world applications. In contrast, the newly released OpenAIs GPT-Image-1 [13] has garnered widespread attention for its remarkable capabilities in unified image generations [14]. However, the closed-source nature of GPT-Image-1 and its primary focus on image generation tasks limit its applicability and scalability across broader multimodal generation tasks. The ComfyUI platform provides potential path toward achieving an open-source general-purpose generative approach. ComfyUI is an open-source platform designed to create and execute generative workflows, offering node-based interface that allows users to construct visual generative workflows represented as JSON according to their needs. The platforms modular design offers high flexibility in constructing workflows. However, despite its flexibility, building complex workflows from scratch remains challenge, particularly when dealing with customized or intricate task requirements, which demand substantial expertise and considerable time for trial and error. To address this, recent research [15, 16] has begun exploring the use of large language models (LLMs) to construct customized workflows, thereby enabling general-purpose visual generation. Building on prior work based on ComfyUI [17], such as ComfyAgent [16], an automated solution for generating workflows from natural language instructions was proposed. ComfyAgent employs multi-pronged mechanism to convert natural language instructions into executable workflows. This approach involves pseudo-code translation to convert JSON structures into Python-like code, dynamic retrieval [18] of node documentation to standardize outputs, and refinement at the workflow level. While this has made significant progress, ComfyAgent also reveals two core issues in lowlevel workflow generation. First, it treats workflow construction as flat, token-based decoding task, making it difficult to effectively model modularity and hierarchy. This leads to node omissions, semantic mismatches, and fragile compositions that are challenging to generalize across tasks. Second, the system lacks an execution-level feedback mechanism. Once the workflow is constructed, the system cannot obtain any feedback or error information during generation, hindering incremental corrections and reducing overall robustness. Figure 2: Structural comparison between ours and ComfyAgent. To address the challenges faced by ComfyAgent, we drew inspiration from human user workflow construction paradigms. We observed that human users typically do not build complex workflows from scratch but instead decompose tasks into smaller subtasks and select appropriate template workflows for each subtask based on higher-level semantics. This modular, step-by-step planning process, combined with localized feedback strategies, allows for incremental refinement and adaptation. When failure occurs, adjustments are made locally rather than globally. This hierarchical planning and reactive feedback strategy enhances the ability to solve complex problems and increases robustness. In this work, we simulate this human-like strategy and propose novel framework called ComfyMind. As shown in Figure 1, our framework exhibits strong generality, supporting wide range of image and video generation and editing tasks. The framework represents workflow generation as the semantic combination of template workflows, rather than the token-based synthesis of node configurations. Specifically, ComfyMind treats template workflows as atomic semantic modules, each with clearly defined function, input/output interfaces, and natural language descriptions. By reasoning over these higher-level components, ComfyMind achieves more stable and controllable task compositions. In addition, the semantic-level abstraction also enables effortless integration of new workflows, allowing ComfyMind to quickly adapt to emerging community-contributed models and tasks. ComfyMind consists of two core mechanisms. The first is the Semantic Workflow Interface (SWI), which abstracts low-level node graphs into callable semantic functions annotated with structured inputs, outputs, and natural-language captions. This abstraction allows language models to operate on workflows at the semantic level, reducing exposure to platform-specific syntax and minimizing structural errors. The second mechanism is Search Tree Planning with Local Feedback Execution, which models task execution as hierarchical decision process. Each node in the planning tree represents sub-task, and each edge corresponds to selected SWI module. During execution, failures trigger localized corrections at the current tree layer, avoiding full-chain regeneration and significantly improving robustness. The comparison with previous work is shown in Figure 2. We evaluate ComfyMind on three public benchmarks: ComfyBench, GenEval, and Reason-Edit. On ComfyBench, ComfyMind improves the workflow pass rate from 56.0% to 100%, and the task resolution rate from 32.5% to 83.0%. On GenEval, it achieves an overall performance score of 0.90, outperforming all baseline methods including GPT-Image-1. On Reason-Edit, it reaches GPT-score of 0.906, surpassing all open-source systems and matching the performance of proprietary models. Our contributions are summarized as follows: We introduce ComfyMind, general-purpose generation framework that conceptualizes visual content creation as modular, semantically structured planning process, and can be instantiated on node-based execution systems (e.g., ComfyUI) that support modular composition and hierarchical task planning. We propose unified control mechanism that integrates Semantic Workflow Interface for high-level modular abstraction with Search Tree Planning with Local Feedback Execution, enabling semantically grounded composition, adaptive correction, and robust realization of complex multi-stage workflows. We validate ComfyMind on three public benchmarks: ComfyBench, GenEval, and ReasonEdit, covering tasks in generation, editing, and reasoning. The results demonstrate strong performance, broad task generality, and the potential of semantic workflow composition as foundation for general-purpose visual generation."
        },
        {
            "title": "2.1 General-Purpose Visual Generation",
            "content": "Traditional visual generative models typically design task-specific architectures [1921] to address various generation tasks. With the rapid development of generation models, many studies have shifted towards general-purpose generation models, aiming to solve wide range of generation tasks using single model [2224, 11, 2529]. These approaches generally unify image understanding and image generation within one model, enabling more efficient and consistent performance across different tasks. Recently, OpenAIs closed-source model GPT-Image-1 [13] has gained significant attention due to its remarkable capabilities in image generation, editing, and modification, achieving top performance across multiple domains. 3 This study aims to leverage the open-source platform ComfyUI to design collaborative AI system. By constructing flexible workflows for each task, we strive to achieve general-purpose visual generation. Compared to current general-purpose generation methods, our approach offers distinct advantages: it not only encompasses video modalities but also effectively handles the structured planning and integration of complex, multi-stage visual workflows."
        },
        {
            "title": "2.2 Planning and Feedback Mechanisms in Collaborative AI Systems",
            "content": "In collaborative AI systems, planning and feedback mechanisms are essential for efficiently managing complex tasks. Early systems like AutoGPT [30] generate static goal lists with minimal intermediate feedback, often leading to inefficiencies when sub-goals fail. In contrast, more dynamic systems like AutoGen [31] allow for iterative plan adjustments through dialogue, enhancing adaptability. OpenAgents [32] provides structured execution pipelines tailored to domain-specific tasks but at the cost of flexibility. ReAct [33] introduces reasoning-acting loop that offers fine-grained control, though it may not be ideal for more complex workflows that require extensive coordination across multiple agents. MetaGPT [34] models human-team roles to standardize collaboration, but it lacks the flexibility needed for dynamic, evolving tasks. Despite these advancements, existing systems still face challenges in localized error handling and recovery. Many systems lack mechanisms for evaluating intermediate outcomes, making them prone to failure in dynamic, non-linear workflows, such as those encountered in generative tasks like ComfyUI [16]. Motivated by these limitations, we propose hierarchical search-tree planning strategy combined with local feedback execution to ensure more robust adaptation and fine-grained error recovery during task execution."
        },
        {
            "title": "3.1 ComfyUI Platform",
            "content": "ComfyUI is an open-source platform for designing and executing generative pipelines. User-defined workflows are encoded as JSON-based directed acyclic graphs (DAGs) and interpreted node-by-node on the server to produce images or videos. This node-level abstraction significantly lowers the entry barrier for human artists by modularizing the execution process into visual, interpretable components. ComfyUI, with the efforts of the open-source community, has integrated support for wide range of advanced models, covering various modalities from image generation to video production. Notable models include Flux [3], Flux Redux [35], ACE++ [36], Segment Anything [37, 38], Hunyuan [8], and Wan [9], among others. This broad support enables users to experiment with state-of-the-art tools for more diverse and sophisticated creative processes. In contrast, building complex workflows from scratch remains significant challenge. Constructing workflows that meet highly customized or complex task requirements not only requires deep technical knowledge but also demands significant time and trial-and-error, creating considerable barriers for users trying to efficiently develop new workflows or address specific needs."
        },
        {
            "title": "3.2 Semantic Workflow Interface",
            "content": "ComfyAgent aims to construct demand-customized ComfyUI workflows through LLMs to achieve general approach to visual generation. It has meticulously built large-scale Node document dataset, designed an intricate collaborative AI system, employed pseudocode to replace JSON for enhanced workspace representation, and utilized RAG technology. However, it still faces issues such as syntax errors in workflows and missing key Nodes, which result in pass rate of only 56% on ComfyBench. The fundamental reason for this low pass rate lies in the vulnerability of low-level representations to LLMs and the conceptualization of workflow construction as planar, token-based decoding task, which makes it difficult to effectively model modularity and hierarchy. In contrast to ComfyAgents paradigm of building entire workflows at the low-level, we adopt human-like approach to workflow construction by decomposing generation tasks into modular subtasks, each handled independently by planning agent. Within each subtask, the planning agent selects the most appropriate atomic workflow from the workflow library as tool. Unlike complex workflows, each atomic workflow is responsible for simple, single-step generation process, such 4 as text-to-image generation or mask generation. In other words, we replace the single token in ComfyAgent with atomic workflows as the minimal unit in workflow construction. Following this approach, we introduce semantic workflow interface, which uses natural language functions instead of low-level JSON specifications as an intermediate representation for workflow construction. Each atomic workflow, encapsulating function, is annotated with simple natural language description outlining its purpose, required parameters, and usage. Based on this metadata, the planning agent in ComfyMind selects the most appropriate function for invocation. During the invocation, required parameters (e.g., prompts or reference images) and optional high-level constraints are passed. The execution agent then maps the selected function to its corresponding JSON representation, injecting the parameters. Finally, LLMs perform adaptive parameter-level tuning on the JSON to meet additional constraints. The resulting workflow is executed via the ComfyUI platform, thus completing the generation of the individual subtask. This abstraction allows the LLM to operate entirely at the semantic level, bypassing the complexities of low-level syntactic grammar and the difficulty of effectively modeling modularity and hierarchy. By eliminating this bottleneck, ComfyMind significantly enhances the robustness of execution. The SWI also minimizes the reliance on fine-grained node documentation. While ComfyAgents operation depends on meticulously crafted dataset containing 3,205 distinct node descriptions, ComfyMind only requires single unified document to describe the available atomic workflows. Without the need for RAG, ComfyMind can directly inject workflow metadata into the LLMs context window, ensuring full visibility and eliminating the dependency on external lookups. Ultimately, this reduction in documentation facilitates the seamless integration of newly developed or task-specific workflows. This design enables ComfyMind to quickly integrate emerging workflows from the broader ComfyUI community, while allowing users the flexibility to customize workflow documentation and repositories to meet specific needs. Figure 3: Overview of ComfyMind pipeline. Given user instruction, the system first parses the task and delegates it to Planning Agent. The Agent incrementally explores semantic search tree, where each node proposes candidate workflow and receives local feedback based on execution results."
        },
        {
            "title": "3.3 Search Tree Planning with Local Feedback Execution",
            "content": "As introduced in Section 3.2, SWI enables LLMs to invoke community-validated atomic workflow using natural-language function calls. However, the system must still determine: how to compose multiple SWI calls into coherent and task-completing program. To address this, as sketched in Figure 3, ComfyMind introduces mechanism we term Search Tree Planning with Local Feedback Execution, which formulates workflow construction as search process over semantic planning tree. In this structure, each node represents local planning agent responsible for specific sub-task, while each edge denotes an execution agent that invokes an SWI function and propagates the result. complete path from the root to leaf yields the final visual output that satisfies the user instruction. At each planning node, the agent examines the current hierarchical workspace stateincluding text, images, context, and the available workflow documentation. Based on this information, it generates 5 chain of SWI functions aimed at advancing the current task. Only the first function in the chain is executed at this stage, with its arguments passed to the execution agent. This transition is equivalent to following an edge in the planning tree. The execution agent translates the selected function into its canonical JSON form as defined by the SWI, applies lightweight parameter adjustments based on higher-level constraints, and executes the workflow using the ComfyUI platform. Throughout the process, the underlying DAG structure is preserved to ensure syntactic correctness. After execution, Vision-Language Model (VLM) parses and annotates the generated visual content. The resulting artifact, its semantic description, and the updated task specification collectively define the workspace for the next planning node. If the planning agent determines that its sub-task can be completed with single operation, it issues termination signal and invokes the evaluation agent to assess the final output in terms of semantic alignment and perceptual quality. If the result passes evaluation, the search ends successfully. Otherwise, failure signal and diagnostic feedback are passed to the parent node, which records the outcome and revises its planning strategy accordingly. If no viable options remain at the current level, the error signal propagates upward. Crucially, all feedback is strictly confined to the current hierarchy level, preventing global rollback and preserving valid partial results. Compared to the step-by-step observe-then-act execution style of ReAct [39] planners, our method offers complete history tracking and structured backtracking capabilities. This allows the system to roll back only to the most recent viable decision point upon failure, rather than restarting the entire processthus avoiding redundant recomputation. At the same time, it improves planning stability by preventing repeated re-planning cycles caused by the lack of stable intermediate states, which can otherwise lead to strategy oscillations and failure to converge."
        },
        {
            "title": "4 Experiments",
            "content": "To assess our systems generative capabilities, we conduct three-pronged evaluation. ComfyBench [16] quantifies the systems ability to autonomously build workflows and general-purpose generation; GenEval [40] evaluates the systems T2I generation capabilities; Reason-Edit [6] measures how well complex editing instructions are executed. Experiments demonstrate that our method surpasses the strongest open-source baselines by substantial margin across all three benchmarks and achieves performance comparable to GPT-Image-1. An ablation study further confirms the contribution of each design component. More experimental results are presented in Appendix A."
        },
        {
            "title": "4.1 Autonomous Workflow Construction",
            "content": "Table 1: Evaluation of Autonomous Workflow Construction on ComfyBench [16]. Agent Vanilla Complex Creative Total %Pass %Resolve %Pass %Resolve %Pass %Resolve %Pass %Resolve GPT-4o + Zero-shot GPT-4o + Few-shot [41] GPT-4o + CoT [42] GPT-4o + CoT-SC [43] Claude-3.5-Sonnet + RAG [18] Llama-3.1-70B + RAG GPT-4o + RAG o1-mini + RAG o1-preview + RAG ComfyAgent [16] Ours 0.0 32.0 44.0 45.0 27.0 58.0 62.0 32.0 70.0 67.0 100.0 0.0 27.0 29.0 34.0 13.0 32.0 41.0 16.0 46. 46.0 92.0 0.0 16.7 11.7 11.7 23.0 23.0 45.0 21.7 48.3 48.3 100.0 0.0 8.3 8.3 5.0 6.7 10.0 21.7 8.3 23.3 21.7 85.0 0.0 7.5 12.5 15.0 7.5 15.0 40.0 12.5 30. 40.0 100.0 0.0 0.0 0.0 0.0 0.0 5.0 7.5 7.5 12.5 15.0 57.5 0.0 22.5 28.0 29.0 22.0 39.0 52.0 25.0 55.5 56.0 100.0 0.0 16.0 17.0 18.5 8.5 20.0 23.0 12.0 32. 32.5 83.0 We assessed the autonomous workflow construction capacity of our method in ComfyBench [16]. ComfyBench comprises 200 graded difficulty generative and editing tasks that span image and video modalities. For each task, the agent must synthesize workflows that can be executed by ComfyUI. The benchmark reports (i) pass rate, reflecting whether the workflow is runable, and (ii) resolve rate, reflecting whether the output satisfies all task requirements. As shown in Table 1, powered by SWI, our system achieves 100% pass rate across all difficulty tiers. Our methods eliminate JSON-level failures that still impede the strongest baseline, ComfyAgent. 6 More importantly, the proposed Search-Tree Planning with Local-Feedback Execution delivers substantial gains in task resolution: relative to ComfyAgent, Resolve rate increases by 100%, 292%, and 283% on the Vanilla, Complex, and Creative subsets, respectively. Appendix further demonstrates that our system successfully addresses wide spectrum of user instructions. The strong generalization ability and output quality evidenced here point to multi-agent systems based on ComfyUI as promising avenue toward general-purpose generative AI."
        },
        {
            "title": "4.2.1 Quantitative Results",
            "content": "We evaluate our systems capability in T2I generation using GenEval [40]. GenEval measures compositional fidelity across six dimensions, including single or two objects, count, color accuracy, spatial positioning, and attribute binding. We compare our method against three strong categories of baselines: (i) Frozen Text Encoder Mapping Methods, represented by SD3; (ii) LLM/MLLM-enhanced methods, such as Janus and GoT; and (iii) OpenAIs recently released GPT-Image-1. As shown in Table 2, our system achieves an overall score of 0.90, benefiting from its integration of prompt optimization workflows and local feedback execution. This result surpasses all baselines by +0.16 over SD3 and +0.10 over Janus-Pro-7B. Moreover, our system exceeds GPT-Image-1 in five out of six dimensions and the overall score. These results demonstrate that our ComfyUI-based system not only offers strong generality but also is capable of consolidating the strengths of diverse open models, achieving state-of-the-art performance in image synthesis. Table 2: Evaluation of T2I generation on GenEval [40]. Obj.: Object. Attr.: Attribution. Method Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding Frozen Text Encoder Mapping Methods 0.97 SDv1.5 [44] 0.98 SDv2.1 [44] 0.98 SD-XL [1] 0.94 DALLE-2 [45] 0.99 SD3-Medium [2] 0.43 0.50 0.55 0.52 0.74 LLMs/MLLMs Enhanced Methods LlamaGen [46] Chameleon [23] LWM [47] SEED-X [10] Emu3-Gen [24] Janus [11] JanusFlow [22] Janus-Pro-7B [29] GoT [12] 0.32 0.39 0.47 0.49 0.54 0.61 0.63 0.80 0.64 GPT-Image-1 [13] 0. Collaborative AI Systems ComfyAgent [16] Ours 0.32 0.90 0.71 - 0.93 0.97 0.98 0.97 0.97 0.99 0.99 0.99 0.69 1."
        },
        {
            "title": "4.2.2 Qualitative Results",
            "content": "0.38 0.51 0.74 0.66 0.94 0.34 - 0.41 0.58 0.71 0.68 0.59 0.89 0.69 0.92 0.30 1.00 0.35 0.44 0.39 0.49 0.72 0.21 - 0.46 0.26 0.34 0.30 0.45 0.59 0. 0.85 0.33 1.00 0.76 0.85 0.85 0.77 0.89 0.58 - 0.79 0.80 0.81 0.84 0.83 0.90 0.85 0.92 0.50 0. 0.04 0.07 0.15 0.10 0.33 0.07 - 0.09 0.19 0.17 0.46 0.53 0.79 0.34 0.75 0.04 0.62 0.06 0.17 0.23 0.19 0.60 0.04 - 0.15 0.14 0.21 0.42 0.42 0.66 0. 0.61 0.04 0.80 Figure 4 showcases representative and challenging cases from GenEval. Our method follows prompts, outperforming existing models on core constraints like counting, color, position and Attribution Binding. In the counting task, only our system generates exactly four keyboards with clear visual separation. For atypical color and position, we demonstrate superior image quality and instruction consistency. Regarding attribute binding, models like SD3 and Janus-Pro often entangle attributes and fail to localize them correctly. GPT-Image-1, while generally instruction-following, often produces fragmented and visually incoherent compositions. In contrast, our method not only satisfies finegrained directives but also integrates them into aesthetically coherent, contextually grounded scenes. These qualitative results corroborate the quantitative gains reported earlier. 7 Figure 4: Qualitative comparison on challenging GenEval [40] cases. Under constraints such as counting, color, position and attribute binding, only our method successfully satisfies all instructions, clearly outperforming SD3, Janus-Pro, and GPT-Image-1. 4."
        },
        {
            "title": "4.3.1 Quantitative Results",
            "content": "We further evaluate our systems image editing capability on the Reason-Edit [6]. Following the setting in benchmark, we adopt the GPT-score [6] as the evaluation metric. This score quantifies both the semantic fidelity to the editing instruction and the visual consistency of non-edited regions. We compare our method against the most advanced open-source baselines, including GoT [12], SmartEdit [6], CosXL-Edit [6], SEED-X [10], MGIE [48], MagicBrush [49] and IP2P [4], as well as the strongest closed-source model, GPT-Image-1. As shown in Figure 5, our method achieves score of 0.906the highest among all open-source frameworks. This result represents substantial improvement of +0.334 over the previous open-source SOTA SmartEdit (0.572). Moreover, our method achieves performance competitive with GPT-Image-1 (0.929), narrowing the gap between open and closed models. This gain arises from our systems planning and feedback mechanism, which enables it to synthesise and combine the most effective editing workflows contributed by the ComfyUI community. Through reasoning and iterative correction, our agent can adaptively select diverse workflows, improving the stability and precision of edits across varied scenarios. These results highlight the reasoning-driven editing capability of our system, and suggest strong potential for future performance gains through integration with more powerful workflows and models."
        },
        {
            "title": "4.3.2 Qualitative Results",
            "content": "Figure 5: Quantitative Comparison on Reason-Edit [6] benchmark. We further present qualitative results to assess the semantic understanding and visual fidelity of our system under challenging editing instructions. We select two representative tasks from the Reason-Edit [6] benchmark. As shown in Figure 6, our method consistently demonstrates the most faithful and visually coherent results across both tasks. Compared to existing open-source baselines, our system not only identifies the correct semantic target (e.g., apple vs. bread vs. orange juice) but also executes edits with minimal disruption to adjacent regions. Although GPT-Image-1 succeeds in executing editing instructions, it struggles to maintain visual consistency in non-edited regions. As illustrated in Figure 6, GPT-Image-1 loses details in non-edited areas (e.g., patterns on the juice box, yogurt container, and jam jar in the zoom-in views), alters color tones and image style, inaccurately preserves materials (e.g., wood texture), and changes the original aspect ratio. These flaws are also mentioned in GPT-eval [14]. 8 In contrast, our method accomplishes the instructions with minimal edits, effectively preserving visual details, image style, material properties, and proportions. These observations highlight our methods superior capability to perform precise and coherent edits. Figure 6: Qualitative Comparison on Reason-edit [6] benchmark."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "To isolate the contributions of our key design components, we conduct an ablation study on the ComfyBench benchmark  (Table 3)  . We evaluate three variants: the full system, version without search tree planning, and version without feedback execution. Benefiting from our Semantic Workflow Interface, all variants achieve 100% pass rate; the main differences lie in the resolve rate. Table 3: Ablation results of ComfyMind with different architectures on ComfyBench. Agent Vanilla Complex Creative Total %Pass %Resolve %Pass %Resolve %Pass %Resolve %Pass %Resolve Ours Ours w/o Tree Planning Ours w/o Feedback 100.0 100.0 100.0 92.0 86.0 68.0 100.0 100.0 100. 85.0 43.4 55.0 100.0 100.0 100.0 57.5 50.0 17.5 100.0 100.0 100.0 83.0 66.0 54.5 Removing the search tree planning module results in notable drop in task resolution, particularly on complex tasks (85.0% to 43.4%), underscoring its role in decomposing multi-step instructions and selecting suitable workflows. Similarly, disabling the local feedback mechanism significantly degrades performance, especially on creative tasks (57.5% to 17.5%), highlighting its importance for iterative correction and adaptive refinement. The results confirm that key components are essential for achieving high success rates in autonomous workflow construction."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose ComfyMind, novel framework built on the ComfyUI platform that addresses key challenges in general-purpose visual generative AI. By conceptualizing visual content creation as modular, semantically structured planning process and incorporating tree-based planning with local feedback execution, ComfyMind improves the stability and robustness of multi-stage workflows. Our framework outperforms previous open-source methods and achieves results comparable to GPT-Image-1 on benchmarks ComfyBench, GenEval, and Reason-Edit. ComfyMind offers promising path towards scalable, open-source solutions for complex generative tasks. Discussion This work focuses on developing collaborative AI system with general-purpose visual generation capabilities, rather than generating complete workflow JSON files. The goal is to explore 9 how such system can plan, execute, and provide feedback for complex tasks through semantic planning, modular composition, and localized corrections. While we use ComfyUI workflow modules for execution, the workflow itself is component of the system, not the research focus. By operating at higher semantic level with the SWI, our approach reduces the complexity and uncertainty of workflow generation, offering more robust and practical solution for real-world applications."
        },
        {
            "title": "References",
            "content": "[1] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [2] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [3] Black Forest Labs. Flux, 2024. URL https://github.com/black-forest-labs/flux. [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [5] Stability Ai. Cosxl, 2024. URL https://huggingface.co/stabilityai/cosxl. [6] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83628371, 2024. [7] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [8] Tencent. Hunyuanvideo, 2024. URL https://github.com/Tencent/HunyuanVideo. [9] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [10] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [11] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [12] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. [13] OpenAI. gpt-image-1, 2025. URL https://platform.openai.com/docs/models/gpt-image-1. [14] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. [15] Rinon Gal, Adi Haviv, Yuval Alaluf, Amit Bermano, Daniel Cohen-Or, and Gal Chechik. Comfygen: Prompt-adaptive workflows for text-to-image generation. arXiv preprint arXiv:2410.01731, 2024. [16] Xiangyuan Xue, Zeyu Lu, Di Huang, Zidong Wang, Wanli Ouyang, and Lei Bai. Comfybench: Benchmarking llm-based agents in comfyui for autonomously designing collaborative ai systems. URL https://arxiv. org/abs/2409.01392, 2024. [17] ComfyUI Contributors. ComfyUI: powerful and modular stable-diffusion gui. https://github.com/ comfyanonymous/ComfyUI, 2023. Accessed: 2025-05-14. 10 [18] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [19] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [20] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [21] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [22] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [23] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [24] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [25] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [26] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [27] Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280, 2024. [28] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [29] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [30] Toran Bruce Richards. AutoGPT. https://github.com/Significant-Gravitas/AutoGPT, 2023. [31] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. [32] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023. [33] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [34] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 3(4):6, 2023. [35] Black Forest Labs. Flux.1-redux-dev, 2024. URL https://huggingface.co/black-forest-labs/ FLUX.1-Redux-dev. [36] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 11 [37] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 39924003. IEEE, 2023. [38] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [39] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. [40] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [41] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [43] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [46] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [47] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv e-prints, pages arXiv2402, 2024. [48] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instructionbased image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. [49] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [50] Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. [51] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [52] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [53] Siqi Kou, Jiachun Jin, Zhihong Liu, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, and Zhijie Deng. Orthus: Autoregressive interleaved image-text generation with modality-specific heads. arXiv preprint arXiv:2412.00127, 2024."
        },
        {
            "title": "A Supplementary Experiments",
            "content": "A.1 Supplementary Ablation Study Table 4: Ablation results of ComfyMind with different LLMs. Agent Vanilla Complex Creative Total %Pass %Resolve %Pass %Resolve %Pass %Resolve %Pass %Resolve ComfyMind with GPT-4o ComfyMind with Deepseek-V3 100.0 100.0 92.0 90.0 100.0 100. 85.0 71.7 100.0 100.0 57.5 60.0 100.0 100.0 83.0 78.5 To further demonstrate the robustness of our system, we conducted evaluations on the ComfyBench benchmark using different LLMs in ComfyMind. As shown in Table 4, both Deepseek-V3 and GPT-4o achieve strong performance when employed as the primary LLMs. Specifically, both models reached 100% task pass rate and approximately 80% overall task completion rate. These results further confirm the stability and reliability of our system across different underlying LLMs. A.2 World knowledge-Informed Semantic Synthesis Table 5: Evaluation of World Knowledge-Informed Semantic Synthesis on WISE [50] Benchmark."
        },
        {
            "title": "Space Biology Physics Chemistry Overall",
            "content": "Dedicated T2I Models FLUX.1-dev [3] FLUX.1-schnell [3] PixArt-Alpha [51] playground-v2.5 [52] SDv1.5 [44] SDv2.1 [44] SD-XL-base-0.9 [1] SD3-Medium [2] SD3.5-Medium [2] SD3.5-Large [2] Unify MLLM Models Emu3 [24] Janus-1.3B [11] JanusFlow-1.3B [22] Janus-Pro-1B [29] Janus-Pro-7B [29] Orthus-7B-base [53] Orthus-7B-instruct [53] show-o-demo [26] show-o-demo-512 [26] vila-u-7b-256 [25] GPT-Image-1 [13] 0.48 0.39 0.45 0.49 0.34 0.30 0.43 0.42 0.43 0.44 0.34 0.16 0.13 0.20 0.30 0.07 0.23 0.28 0.28 0.26 0. 0.58 0.44 0.50 0.58 0.35 0.38 0.48 0.44 0.50 0.50 0.45 0.26 0.26 0.28 0.37 0.10 0.31 0.36 0.40 0.33 0.71 0.62 0.50 0.48 0.55 0.32 0.35 0.47 0.48 0.52 0.58 0.48 0.35 0.28 0.45 0.49 0.12 0.38 0.40 0.48 0.37 0. 0.42 0.31 0.49 0.43 0.28 0.33 0.44 0.39 0.41 0.44 0.41 0.28 0.20 0.24 0.36 0.15 0.28 0.23 0.30 0.35 0.83 0.51 0.44 0.56 0.48 0.29 0.34 0.45 0.47 0.53 0.52 0.45 0.30 0.19 0.32 0.42 0.15 0.31 0.33 0.46 0.39 0."
        },
        {
            "title": "Collaborative AI Systems\nOurs",
            "content": "0.90 0.79 0.92 0.77 0.79 0.35 0.26 0.34 0.33 0.21 0.21 0.27 0.29 0.33 0. 0.27 0.14 0.11 0.16 0.26 0.10 0.20 0.22 0.30 0.23 0.74 0.82 0.50 0.40 0.47 0.49 0.32 0.32 0.43 0.42 0.45 0.46 0.39 0.23 0.18 0.26 0.35 0.10 0.27 0.30 0.35 0.31 0. 0.85 To assess our systems capabilities in complex semantic understanding, reasoning, and world knowledge integration for text-to-image generation, we conduct evaluation on the recent WISE [50] benchmark. This benchmark contains three primary categories: cultural commonsense, spatiotemporal reasoning (including Space and Time subcategories), and natural sciences (comprising Physics, Chemistry, and Biology subfields), totaling 25 specialized domains with 1,000 challenging prompts. The evaluation metric WiScore combines Consistency, Realism, and Aesthetic Quality through weighted normalization, with maximum score of 1. Higher WiScore indicates stronger capability in accurately depicting objects and concepts using world knowledge. As shown in Table 5, our method achieves superior score of 0.85, surpassing all models, including GPT-Image-1 (0.80). Our approach significantly enhances world knowledge integration for open-source solutions, outperforming FLUX.1-dev (0.50) by 0.35 points and enabling 13 open-source models to match GPT-Image-1s performance. The exceptional performance on WISE confirms our systems generalizability and high-quality output in generative tasks."
        },
        {
            "title": "B Additional Results and Generation Examples",
            "content": "As an extended demonstration, we present ComfyMinds exemplary outputs across diverse generative tasks in Figures 7, 8, and 9, substantiating its versatile adaptability and superior performance in cross-domain generation scenarios."
        },
        {
            "title": "C Implementation Details",
            "content": "C.1 System Architecture Logically, the system is divided into two independent subsystems: the ComfyUI server and the automation control system. Both are deployed on the same physical server but operate within isolated runtime environments and listen on separate ports. This design ensures robust separation and scalability in task concurrency handling and resource management. The server is equipped with an NVIDIA RTX A6000 GPU with 48GB of VRAM, providing sufficient computational capacity for generation tasks. Within the system architecture, the ComfyUI platform is responsible for executing atomic workflows. It is deployed as local service that listens on designated port and provides dual interaction interfaces. On one hand, it exposes web interface through which users can access the frontend via browser to edit nodes, configure parameters, and perform real-time executions. On the other hand, ComfyUI incorporates both HTTP and WebSocket protocols, supporting standardized RESTful API task submissions and result retrieval, while enabling real-time progress tracking via WebSocket. In this study, the latter interface is primarily utilized. Technically, the client loads an adapted workflow in JSON format, submits generation request via the HTTP API, and establishes persistent WebSocket connection to receive real-time execution updates and final results. Built atop the ComfyUI execution layer, ComfyMind functions as the high-level planning and task management system, responsible for process orchestration, scheduling, and result evaluation. It is developed based on the ComfyAgent framework. C.2 Compilation of Atomic Workflow Library and Descriptive Documents To ensure adaptability across multimodal generation and editing tasks, diverse set of atomic workflows was systematically collected, curated, and organized to form foundational resource for general-purpose content generation. These workflows were sourced through combination of selective acquisition and custom development, aiming to balance stability and broad applicability. First, standard workflows from the ComfyBench benchmark suite were systematically tested. Suboptimal examples were discarded to ensure overall quality. Second, curated selection of high-quality workflows was extracted from the official ComfyUI website and documentation to meet common generation needs. Additionally, workflows with strong user ratings and proven effectiveness were sourced from popular communities such as Civitai and OpenArt, enhancing the practicality and robustness of the workflow library. Beyond these standardized resources, functional workflows tailored for complex reasoning tasks were developed by leveraging the flexibility of ComfyMind, in conjunction with LLMs and VLMs. These workflows include capabilities for reason generation, reason editing, and prompt enhancement, supplementing limitations in the standard resource base. C.3 Supplementary Description of the ComfyMind Method C.3.1 Semantic Workflow Interface Details To enable SWI, unified semantic description schema was developed for all atomic workflows. The system only requires single, consistent atomic workflow description document to support all reasoning tasks. This document employs hybrid format combining natural language and standardized parameter interfaces to specify each workflows core functionality, input/output requirements, and optional extensions. The core structure includes: the required workflow name, the corresponding prompt and visual input. Optional components include supplementary semantic constraints such as resolution, frame rate, or upscaling ratio. This modular, uniform documentation enables semantic-level workflow selection and parameter matching during reasoning, without requiring parsing of underlying node structuresgreatly improving planning efficiency and ease of expansion or maintenance. Figure 7: More examples generated by ComfyMind 15 Figure 8: More examples generated by ComfyMind 16 Figure 9: More examples generated by ComfyMind C.3.2 Execution Agent Details The execution agent in the ComfyMind system plays crucial role in bridging high-level planning and low-level execution. Once the planning agent completes the decomposition of subtask and outputs set of invocation instructions, the execution agent is activated to handle parameter processing, workflow adaptation, execution, and result feedback. The overall logic adheres to the fundamental principle: \"if execution succeeds, proceed to the next subtask; if it fails, return to the current node for local backtracking.\" This ensures tight integration between planning and execution. Before initiating the execution process, certain specialized semantic workflow interface functions may require preprocessing step. In such cases, the execution agent invokes LLMs to perform deep reasoning and semantic expansion on the users input prompt or initial task description. For example, if the user provides command like light passes through prism, the system leverages the LLMs knowledge to infer related phenomena such as spectral dispersion and colorful light bands, thereby optimizing the prompt and providing richer and more accurate input conditions for the subsequent generation process. Next, the execution agent parses the invocation parameters passed down from the planning agent, loads the corresponding atomic workflow template, and fills in placeholders with information such as the prompt text and input visual data paths. This results in fully instantiated and executable workflow. To better adapt to the tasks specific requirements, the execution agent further adjusts workflow parameters, such as image resolution, sampling steps, and output frame rate. This adaptive tuning is performed at the parameter level without altering the underlying DAG structure of the atomic workflow, thus preserving the correctness of node connections and ensuring execution stability. Once adaptation is complete, the execution agent submits the workflow to the ComfyUI platform through standardized interface and establishes real-time connection to monitor generation progress and status updates. After task execution finishes, the system automatically parses the generated results. It uses visual-language understanding module to extract semantic descriptions, detailed information, and scene characteristics from the outputs, synchronizing them with the current workspace state. This update provides the most recent context for subsequent subtask reasoning, maintaining continuity and coherence in the reasoning chain. If the entire process completes successfully, the execution agent returns success signal and hands off the updated workspace state to the next planning node. If any exceptions occur during executionsuch as parameter mismatches, insufficient generation quality, or timeoutsthe execution agent returns detailed failure reason and log information. This allows the planning agent to perform localized search backtracking and re-planning. Through this design, the execution agent effectively ensures seamless integration from high-level semantic instructions to concrete execution, while offering strong adaptability and robust error handling, significantly enhancing the systems overall stability and reasoning flexibility. C.3.3 Evaluation Agent Details The Evaluation Agent is responsible for assessing the quality of generated results and their consistency with the given instructions, ensuring that system outputs meet expected requirements. When the overall process concludes, the system invokes the Evaluation Agent to inspect the generated content against unified standard and relay the results back to the higher-level planning module. In its design, the Evaluation Agent adopts the classification-based assessment strategy used by ComfyAgent, dividing workflows according to task types such as text-to-image, image-to-image, text-to-video, image-to-video, and video-to-video, with tailored evaluation criteria for each category. Evaluation is conducted along two main dimensions: the quality of the generationsuch as clarity, richness of detail, and visual naturalnessand adherence to instructions, verifying whether the output accurately reflects the users intended theme, setting, or stylistic directives. Each evaluation returns two components: binary judgment indicating whether the current task is considered complete, which guides the system on whether to proceed or backtrack; and detailed failure analysis when evaluation is unsuccessful, explicitly identifying reasons such as lack of detail, semantic deviation, or stylistic inconsistency, thereby aiding the Planning Agent in refining subsequent inference steps. To accommodate varying tasks and application requirements, the Evaluation Agent supports dynamic threshold adjustment. The system can raise or lower evaluation standards depending on task complexity, quality expectations, or usage scenarios, enabling flexible balance between content quality and execution efficiency."
        },
        {
            "title": "D Limitation",
            "content": "Although ComfyMind supports modular workflow composition and automated planning, the current system lacks user-friendly interface for manually customizing or modifying the sequence of atomic workflow invocations. Users have limited ability to adjust planning strategies, override intermediate steps, or specify task-specific preferences through the UI. This may hinder broader adoption among non-technical users or practitioners 18 with domain-specific needs. Enhancing the interface to support more flexible and user-controllable planning customization is an important direction for future development."
        },
        {
            "title": "E System Prompt",
            "content": "This section outlines the important system prompt in ComfyMind, defining agent behavior, response style, and task boundaries, as demonstrated by Figures 10 to Figures 19. 19 Figure 10: System Prompt for Preprocessing, Part 1 20 Figure 11: System Prompt for Preprocessing, Part 2 Figure 12: System Prompt for Preprocessing, Part 3 22 Figure 13: System Prompt for Planning, Part 1 23 Figure 14: System Prompt for Planning, Part 2 Figure 15: System Prompt for Planning, Part 3 25 Figure 16: System Prompt for Tools Definition 26 Figure 17: System Prompt for Updating Workspace Figure 18: System Prompt for Workflow Adaptive Adjustment, Part 1 28 Figure 19: System Prompt for Workflow Adaptive Adjustment, Part"
        }
    ],
    "affiliations": [
        "HKUST(GZ)"
    ]
}