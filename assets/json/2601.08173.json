{
    "paper_title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
    "authors": [
        "Daocheng Fu",
        "Jianbiao Mei",
        "Rong Wu",
        "Xuemeng Yang",
        "Jia Xu",
        "Ding Wang",
        "Pinlong Cai",
        "Yong Liu",
        "Licheng Wen",
        "Botian Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv"
        },
        {
            "title": "Start",
            "content": "The Agents First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios Daocheng Fu1,2,, Jianbiao Mei3,2,, Rong Wu3,2,, Xuemeng Yang2,, Jia Xu2, Ding Wang2, Pinlong Cai 2, Yong Liu3,(cid:66) , Licheng Wen2,4,5,(cid:66) , Botian Shi2,(cid:66) 1Fudan University, 2Shanghai AI Laboratory, 3Zhejiang University, 4Shanghai Innovation Institute, 5Shanghai Jiao Tong University Equal contribution, (cid:66) Corresponding authors. 6 2 0 2 3 1 ] . [ 1 3 7 1 8 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce Trainee-Bench, dynamic evaluation environment that simulates \"trainee\" agent continuously exploring novel setting. Unlike traditional benchmarks, Trainee-Bench evaluates agents along three dimensions: (1) contextaware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes framework for assessing agent reliability, shifting evaluation from static tests to realistic, productionoriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv"
        },
        {
            "title": "Introduction",
            "content": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has significantly advanced complex workflow automation (Guo et al., 2024a; Li et al., 2024). However, while existing systems facilitate tool utilization, research predominantly focuses on performance upper bounds in controlled environments (Gottweis et al., 2025; Team et al., 2025). This paradigm often neglects the robustness required for authentic, stochastic environment settings. Unlike sterile experiments that ignore environmental noise, real-world deployFigure 1: Overview of challenges in current agent systems: (1) effective scheduling and multi-task planning for streaming task inputs; (2) the ability to suspect unsolvable tasks and solicit guidance through active exploration; and (3) robust experience summarization, retrieval, and utilization to enhance performance stability. ment involves streaming, randomized tasks. Consequently, agents must not only orchestrate scheduling with awareness of environmental dynamics but also ensure reliability, effectively bridging the gap between laboratory settings and production needs (Fu et al., 2025; Yang et al., 2025). As illustrated in Fig. 1, shifting from static setups to realistic interaction scenarios introduces (1) Dynamic task three primary challenges: scheduling and context management. Agents Benchmark Multi-App Interaction Checkpoint Feedback Partial Observability Dynamic Configuration GAIA-2 (Froger et al., 2025) TheAgentCompany (Xu et al., 2024) Tool Bench (Qin et al., 2024) StableToolBench (Guo et al., 2024b) Toolathon (Li et al., 2025) τ -bench (Yao et al., 2024) Trainee-Bench (Ours) Table 1: Comparison of different agent benchmarks. Multi-App Interaction : support for cross-application workflows. Checkpoint Feedback: the availability of intermediate evaluation milestones beyond final success rates. Partial Observability: agents must actively explore hidden states rather than operating on full information. Dynamic Configuration: environments with randomized, evolving parameters and dynamic composite scenarios. must perform rational temporal planning for streaming tasks to meet deadlines while maintaining context awareness, thereby deciding to mitigate cross-task interference. (2) Active exploration in novel tasks. Instead of hallucinating actions in uncertain scenarios, reliable agent should exercise prudence by actively exploring or soliciting assistance to acquire necessary clues (Yu et al., 2024; Qiu et al., 2025; Arora et al., 2025). (3) Learning from previous tasks. To ensure long-term stability, agents must learn from prior experiences to prevent the recurrence of historical errors in subsequent scenarios (Zhou et al., 2025; Silver and Sutton, 2025; Wu et al., 2025b; Gao et al., 2025a). Although numerous benchmarks exist ( Table 1), they fall short in simulating the dynamic nature of realistic environments and lack mechanisms to evaluate active exploration and continual learning capabilities. To address this, we introduce TraineeBench, dynamic evaluation environment simulating an trainee agent continuously exploring novel setting. Trainee-Bench is designed to assess the three aforementioned capabilities of an agent rigorously: First, regarding task execution, Trainee-Bench presents streaming tasks with distinct priorities. This demands superior scheduling to maintain context awareness and mitigate intertask interference. Second, concerning information acquisition, critical clues are initially concealed. Agents must demonstrate prudence by actively exploring or soliciting help rather than engaging in hallucinated actions. Finally, in terms of continuous evolution, tasks are dynamically generated based on rules rather than fixed datasets. This compels agents to distill generalized strategies from prior tasks instead of rote-memorizing instances, effectively preventing repeated errors. We conduct extensive experiments on TraineeBench, revealing that state-of-the-art (SOTA) agents exhibit significant room for improvement in dynamic environments, particularly regarding active exploration and continual learning. Our contributions are summarized as follows: Proposing novel benchmarking paradigm that laboratory evaluations to shifts from static, stochastic, production-oriented environments; Establishing comprehensive framework to rigorously assess temporal scheduling, prudent decision-making under uncertainty, and longterm strategic evolution of MLLM agents; Through extensive experimentation, we characterize the reliability gap of current agents, highlighting their deficiencies in handling concealed clues and distilling generalized experiences."
        },
        {
            "title": "2 Trainee-Bench",
            "content": "We introduce Trainee-Bench, benchmark designed to simulate authentic workplace environments through series of human-crafted metatasks. In this framework, agents are evaluated as corporate interns navigating realistic company routines, such as attending meetings or reviewing data. The benchmark rigorously assesses three core competencies: dynamic multi-tasking, proactive exploration under uncertainty, and continuous self-evolution. To construct Trainee-Bench, we adopt bottom-up design strategy that evaluates agent capabilities ranging from atomic skills to holistic workflows. We first develop suite of rule-based meta-tasks to serve as foundational templates. These templates can be instantiated into numerous specific task instances, which are then flexibly orchestrated into complex, composite scenarios with explicit temporal constraints. Finally, we implement an automated verification mechanism to ensure rigorous and objective assessment of agent performance in these sophisticated environments. Detailed implementations are elaborated in the following sections. Dynamic Task Instantiation. The instantiation process begins with randomization engine Frnd. Controlled by random seed, this engine synthesizes stochastic environment variables, specifically NPC profiles Prnd and environmental data Drnd: {Prnd, Drnd} = Frnd(seed) (2) These variables introduce realistic uncertainty, such as randomized sender names, file paths, or conflicting schedules. Then, the selected meta-task rule ri integrates these variables into its predefined logic to yield task triple: {Prnd, Drnd} ri {Dtask, C, K}, (3) where Dtask represents unique natural language objective, denotes set of verifiable checkpoints for evaluation, and comprises the latent clues required for task completion. By varying the seed, single meta-task ri can generate multiple distinct task triples. This one-to-many mapping ensures that agents must employ general reasoning and actively explore latent clues rather than relying on rote memorization. Task Diversity and Coverage. The decoupling of logical rules ri from stochastic data {Prnd, Drnd} enables systematic and scalable expansion of the benchmarks coverage along two dimensions: Task Dimensions: To assess broad spectrum of cognitive demands rather than isolated skills, our rule library comprises 181 meta-tasks categorized into four functional domains: (i) information synthesis and analysis (e.g., transaction auditing), (ii) time management and scheduling (e.g., meeting attending), (iii) proactive inquiry and monitoring (e.g., automated website monitoring), and (iv) strategic modeling and optimization (e.g., advertising campaign planning). Stochastic Robustness: The randomization function Frnd ensures that even for single rule ri, the resulting environment remains unpredictable. By varying user personas, file system hierarchies, and numerical distributions within Drnd, Trainee-Bench prevents agents from memorizing specific environment layouts or hardcoded solutions, forcing them to generalize across diverse and unfamiliar operational contexts. Partial Observability. defining characteristic of Trainee-Bench is the deliberate enforcement of Figure 2: Overview of Trainee-Bench construction: (1) Unique task instances generated via random parameters. (2) Temporal composition of instances into task streams. (3) Automated verification of execution results."
        },
        {
            "title": "2.1 Preliminaries",
            "content": "In Trainee-Bench, we formalize the interaction environment as dynamic state transition system. At any given time step t, the environmental configuration is represented by state mt M, which encapsulates the real-time status of the agent and various environmental entities, such as non-player characters (NPCs), files, and databases: mt = {sagent, sNPC, sfile, sdata} (1) The agent interacts with the environment by invoking tools to execute actions at A. These actions trigger state transitions defined by the function : M, defined as (mt, at) = mt+1. This formalism establishes the mathematical foundation for both the procedural generation of individual meta-tasks and the dynamic orchestration of concurrent, multi-threaded workflows."
        },
        {
            "title": "2.2 Meta-Task Design",
            "content": "To prevent agents from over-fitting to static datasets, Trainee-Bench generates dynamic task instances derived from library of human-crafted Meta-tasks, denoted as R. Each ri represents an abstract logical template (e.g., Transaction Auditing) that can spawn an infinite variety of concrete task instances. partial observability to simulate real-world ambiguity. We introduce an information gap between the agents initial knowledge and the required state information. Specifically, while the agent is provided with the task objective Dtask, the set of essential clues = {k1, . . . , kn}such as technical manuals, access passwords, or implicit verbal instructionsis strictly withheld from the initial prompt. Consequently, the agent cannot complete the task by simply following the starting instructions. Instead, it must engage in proactive exploration by navigating file systems or conducting multi-turn dialogues with NPCs to uncover the latent clues embedded in the environment. This mechanism shifts the agents role from passive executor to proactive problem-solver that must synthesize fragmented information under uncertainty."
        },
        {
            "title": "2.3 Dynamic Composite Scenarios",
            "content": "To simulate realistic, multi-threaded scenario, we transcend the execution of isolated rules ri by synthesizing multiple meta-tasks into dynamic composite scenarios. Formally, scenario is constructed by stochastically assembling subset of generated tasks and distributing them along continuous timeline, characterized by distinct deadlines. This composition introduces three critical structural constraints regarding the interaction: and k2 Conflict Resolution: During the aggregation of meta-tasks, conflicts may arise, such as overlapping entity identifiers (e.g., character names or filenames) or single non-player character (NPC) acting as custodian for clues k1 from disparate tasks. To resolve such conflicts, we enforce uniqueness constraint on all entity demarcations within the meta-task rules, ensuring that specific names remain distinct across the composite scenario. Furthermore, in instances where single NPC safeguards multiple clues, we explicitly define the conditional triggers required to elicit each specific clue1, thereby mitigating potential information ambiguity. Temporal Prioritization: We introduce timecritical meta-tasks (e.g., attending scheduled meeting) that impose rigid temporal constraints. These high-priority interrupts function as preemptive signals, mandating that the agent suspend its current workflow, perform context switch to address the immediate requirement, and subsequently 1Detailed cases illustrating multi-clue NPC interactions and agent-NPC dialogues are provided in the Section B. Figure 3: Formulation for the interaction among the environment, agent, and MLLM service. resume the interrupted process. This mechanism rigorously evaluates the agents capacity for dynamic priority scheduling and long-term memory management. Inter-task Dependencies: We engineer sequential dependencies where upstream tasks function as informational prerequisites for downstream objectives. In these scenarios, critical task parameters are only revealed during the execution of preceding taskfor instance, new assignment may be \"published\" only during an ongoing meeting. This interdependent architecture increases temporal uncertainty and compels the agent to adaptively update its plan based on real-time environmental observations rather than relying on static initial instructions."
        },
        {
            "title": "2.4 Automated Verification Mechanism",
            "content": "To quantify performance in these complex scenarios, we leverage the set of checkpoints = {c1, . . . , cn} derived during task construction. Embedded within each meta-task, these checkpoints facilitate granular assessment of the agents overall progress and completion quality. The final scenario score is derived by normalizing the completion ratio of checkpoints across all constituent meta-tasks: ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 ccompleted Ci (4) where is the number of meta-tasks, Ci is the total number of checkpoints for task i, and ccompleted is the count of successfully passed checkpoints. Furthermore, each checkpoint triggers detailed natural language feedback, providing structured experiential data that empowers agents to perform retrospective analysis and continuous learning. 2."
        },
        {
            "title": "Interaction Protocol",
            "content": "Building upon the constructed scenarios, we define an interaction protocol that bridges the environment and the agent, as depicted in Fig. 3. This workflow comprises three distinct entities: the Environment, the Agent, and the MLLM Service. Upon initialization with task descriptions and tool definitions provided by the environment, the agent constructs its system prompt and engages in an iterative cycle: maintaining conversation history, querying the MLLM Service, and parsing model responses into executable tool calls. We implement standardized pipeline to orchestrate this Agent-Environment interaction loop, while abstracting three key components, i.e., system prompt construction, history maintenance, and MLLM service invocation, as customizable interfaces. By enforcing unified interaction protocol, Trainee-Bench provides modular and flexible framework that supports diverse agent architectures and LLM backends."
        },
        {
            "title": "2.6 Benchmark Design Characteristics",
            "content": "While conventional benchmarks often rely on isolated, static tasks that fail to capture the inherent complexity of real-world workflows, TraineeBench provides high-fidelity workplace simulation by integrating four critical operational dimensions (see Table 1). By moving beyond simplistic execution, our benchmark establishes rigorous environment to evaluate high-order cognitive capabilities through the following key features: Robustness against Memorization. By decoupling logical meta-tasks from stochastic environment variables (Sec. 2.2), Trainee-Bench ensures that every evaluation instance is structurally unique. This \"one-to-many\" instantiation forces agents to rely on generalized reasoning and dynamic state estimation rather than exploiting fixed patterns or hard-coded shortcuts common in static datasets. Proactive Uncertainty Resolution. Through the enforcement of partial observability, we transform the agent from passive executor into an active information seeker. The latent clues between the initial prompt and target state require multiple rounds of querying and environmental exploration, thus measuring agents ability under uncertainty. Algorithm 1 Benchmark Building Require: Meta-task rule set Ensure: Benchmark 1: 2: for = 1 to 50 do 3: 4: 5: 6: 7: end for 8: return Sample task count U{2, 6} Generate scenario tasks Si: {Si} Si {INSTANTIATE(r) R}k j=1 Stress-testing under Temporal Complexity. The synthesis of composite scenarios introduces \"messy\" workspace characterized by preemptive interrupts and inter-task dependencies. This setup moves beyond linear execution, specifically targeting the agents context-switching efficiency and dynamic priority managementcapabilities that are critical for production-grade automation but often ignored in laboratory settings. Feedback-driven Evolution. Trainee-Bench is not \"black-box\" assessment. By providing granular, natural language feedback at each checkpoint, the environment functions as learning catalyst. This structured experiential data allows for the evaluation of an agents \"learning curve\", measuring how effectively it can perform retrospective analysis and refine its strategies across streaming tasks."
        },
        {
            "title": "3 Experiments\nIn this section, we conduct a series of experiments\non our Trainee-Bench to answer the following key\nresearch questions (RQs):\n• RQ1: Are current LLMs capable of handling\ncomplex workplace environments characterized\nby dynamics and uncertainty? (Section 3.2)",
            "content": "RQ2: Can current LLMs truly achieve consistent continuous learning from past experiences over extended periods? (Section 3.3) RQ3: What is the performance gap between agents proactive exploration and passive reception of human guidance? (Section 3.4) 3."
        },
        {
            "title": "Implementation Details",
            "content": "Setup. To simulate and evaluate agent performance within the trainee environment, we constructed benchmark consisting of 50 dynamic scenarios. Each scenario encompasses 2 to 6 task instances, where every task is instantiated based on rule uniformly sampled from the meta-task rules set R. The detailed procedure for benchmark construction is outlined in Algorithm 1. We Model Success Rate Checkpoint Score Average Steps Average Tool Calls Closed-source Models Gemini-3-Flash (Google DeepMind, 2025) Claude-4-Sonnet (Anthropic, 2025) GPT-5.1 (OpenAI, 2025) Grok-4 (xAI, 2025) GPT-4o (OpenAI, 2024) Open-source Models Qwen3-VL-235B-A22B (Bai et al., 2025) Llama-4-maverick (Meta, 2025) 0.35 0.23 0.23 0.20 0. 0.14 0.04 0.63 0.59 0.49 0.54 0.38 0.37 0.13 90 29 23 34 37 68 34 232 75 62 95 68 23 Table 2: Overall performance of various LLM agents on our Trainee-Bench.The highest scores in both open-source and closed-source models are highlighted in bold. selected diverse set of seven models for evaluation, spanning open-source and proprietary architectures, general-purpose and tool-specialized variants, as well as distinct model sizes ranging from lightweight to state-of-the-art large-scale parameters. Specifically, the evaluated models are GPT-5.1 (OpenAI, 2025), GPT-4o (OpenAI, 2024), Claude-4-Sonnet (Anthropic, 2025), Gemini-3Flash (Google DeepMind, 2025), Grok-4 (xAI, 2025), Qwen3-VL-A235B (Bai et al., 2025), and Llama-4-maverick (Meta, 2025). All models perform inference, planning, and tool invocation in strict adherence to the protocols defined in Section 2.5. To mitigate context window limitations, historical interaction information is summarized and compressed once the sequence length exceeds predefined threshold. Metrics. To provide an overall and fine-grained assessment of agent performance, we define the following metrics, which are measured daily within the simulation: Success Rate (SR): Defined as the percentage of tasks successfully completed. Checkpoint Score (CS): Defined as the mean score across all scenarios, where the score for each scenario is calculated via Equation 2.4. Average Steps: This metric calculates the average number of thought-and-action steps the agent executed to complete its tasks per scenario. Average Tool Calls: This metric quantifies the average number of times the agent invoked any tool per scenario."
        },
        {
            "title": "3.2 Performance of Cutting-edge Models",
            "content": "In this section, we benchmark top-tier LLMs to investigate their limitations when deployed in Trainee-Bench, which is characterized by dynamic and uncertain workplace environments. Overall Performance. Table 2 summarizes the results averaged across all scenarios. These findings clearly demonstrate the difficulty of TraineeBench. Even the best-performing model, Gemini-3Flash, attains Success Rate of only 35%, indicating persistent gap in achieving reliable autonomy in simulated workplaces. Besides, distinct performance hierarchy is evident. Gemini-3-Flash leads in both metrics, followed by Claude-4-Sonnet and Grok-4. Conversely, Llama-4-maverick lags significantly (0.13 Checkpoint Score) due to issues with instruction following and tool invocation (detailed in Appendix B.2). Finally, we find that reliability requires sufficient interaction depth. While Gemini3-Flash uses substantially more steps (90) and tool calls (232) than the middle-tier models, this increased activity reflects the necessary complexity and thoroughness required to successfully navigate dynamic and uncertain scenarios. Impact of Task Workload. To analyze the impact of workload, we stratify agent performance by the number of meta-tasks per scenario, as shown in Table 3. We observe clear downward trend in performance for models such as GPT-4o, Grok-4, and Gemini-3-Flash as the workload increases, particularly beyond two tasks. Notably, Gemini-3-Flash, despite its strong overall performance, exhibits decrease in Success Rate from 50% in 2-task scenarios to 36% in 6-task scenarios. This suggests that managing increased cognitive loadin particular, frequent context switching and temporal uncertainty in dynamic composite scenariosremains key challenge for these models. In contrast, Claude4-Sonnet and GPT-5.1 do not exhibit strong correlation between performance and the number of tasks, indicating certain degree of robustness and adaptability to dynamic composite settings. Llama4-maverick, by comparison, maintains consistently low performance across different task workload, indicating that its reasoning and tool-use capabilities are insufficient for reliably solving the atomic tasks themselves. Interestingly, as the number of tasks Model Success Rate (# Tasks) Checkpoint Score (# Tasks) Task Count: 2 Gemini-3-Flash (Google DeepMind, 2025) Grok-4 (xAI, 2025) GPT-4o (OpenAI, 2024) Qwen3-VL-235B-A22B (Bai et al., 2025) Claude-4-Sonnet (Anthropic, 2025) GPT-5.1 (OpenAI, 2025) Llama-4-maverick (Meta, 2025) 0.50 0.40 0.40 0. 0.25 0.20 0.00 3 0.47 0.20 0.10 0.17 0.17 0.23 0.03 4 0.48 0.25 0.13 0. 0.35 0.35 0.03 5 0.36 0.24 0.15 0.12 0.20 0.23 0.05 6 0.36 0.16 0.14 0. 0.24 0.21 0.03 2 0.77 0.68 0.60 0.43 0.61 0.46 0.09 3 0.68 0.53 0.37 0. 0.54 0.47 0.11 4 0.72 0.59 0.39 0.48 0.70 0.63 0.15 5 0.60 0.59 0.40 0. 0.58 0.49 0.15 6 0.60 0.45 0.38 0.32 0.57 0.49 0.15 Table 3: Success Rate and Checkpoint Score of different models under increasing task counts (2, 3, 4, 5, and 6 concurrent tasks). Easy Tasks Hard Tasks Model SR CS SR CS Closed-source Models Gemini-3-Flash Claude-4-Sonnet GPT-5.1 Grok-4 GPT-4o Open-source Models Qwen3-VL-A235B Llama-4-maverick 0.46 0.37 0.37 0.34 0.26 0.24 0.06 0.74 0.70 0.63 0.68 0.58 0.50 0. 0.32 0.08 0.08 0.07 0.03 0.04 0 0.56 0.46 0.35 0.37 0.19 0.22 0.04 Table 4: Impact of different task difficulties. The highest scores in both open-source and closed-source models are highlighted in bold. increases, Llama-4-maverick has higher chance of completing at least some of the simpler tasks or intermediate checkpoints, which leads to slight upward trend in its aggregate Success Rate despite its overall weak capabilities. Impact of Task Difficulty. We further investigate the impact of meta-task difficulty on agent performance. Specifically, we stratify the tasks into Easy and Hard subsets based on manual complexity analysis, incorporating metrics such as average step counts and tool invocation frequencies. In general, hard tasks require substantially more reasoning steps and tool calls. They also contain greater amount of implicit or hidden information, which necessitates more active exploration of the environment by the agent to uncover the solution. Moreover, subset of the hard tasks additionally demands explicit task modeling and optimization capabilities, thereby imposing stricter requirements on the agents level of intelligence Table 4 reveals stark contrast. While most models achieve respectable performance on Easy tasks, their capabilities decline precipitously on Hard ones. For instance, Grok-4s Success Rate plummets from 34% on Easy tasks to just 7% on Hard tasks, with similar drops observed for Claude4-Sonnet (37% to 8%) and GPT-4o (26% to 3%). This breakdown highlights that the capacity to resolve complex problems is the key differentiator among top-tier models. Notably, while Gemini3-Flash also experiences decline, it sustains significantly higher Success Rate (32%) on Hard tasks, demonstrating superior robustness in complex reasoning compared to its peers."
        },
        {
            "title": "3.3 Analysis of Continual Learning Capability",
            "content": "Setup. To evaluate the agents capacity for learning from prior experience, we curated 50 continual learning scenarios, with each comprising 2 to 6 distinct meta-tasks. Distinguished from the configuration in Section 3.1, each scenario herein is structured into two phases (referred to as Day 1 and Day 2). While the sequence, quantity, and types of tasks remain invariant across both days, the specific input parameters for task instantiation vary. For these experiments, we employ MUSE (Yang et al., 2025), SOTA continual learning framework. Upon the completion of tasks on Day 1, the MUSE agent receives feedback tailored to specific task outcomes. For instance, if task checkpoint ci is missed, the environment explicitly notifies the agent of the omission, emphasizing the need for attention in subsequent attempts2. Leveraging this feedback alongside its historical interaction trajectories, the agent engages in reflective process to summarize insights ei, which are subsequently utilized to guide task execution on Day 2. We selected GPT-4o as the backbone model due to its intermediate performance profile. Results. The experimental results within the continual learning setting are presented in Table 5. 2One example of daily feedback can be found in Section B.4 Task Type Day 1 Score Day 2 Score Gain ( CS) Overall Easy Tasks Hard Tasks 0.42 0.61 0.20 0.36 0.50 0.24 -0.06 -0.11 +0.04 Table 5: Performance of the continual learning, comparing Day 1 (no experience) with Day 2 (after receiving feedback), focusing on the Checkpoint Score (CS). Overall, the MUSE agent exhibits counterintuitive decline in Checkpoint Score when utilizing accumulated experience. Upon stratifying the results into easy and hard tasks, however, distinct divergence in performance becomes apparent. For easy tasks, the agent without experience achieves relatively high scores, whereas the incorporation of experience leads to marked degradation. Conversely, on hard tasks where the baseline performance is lower, the utilization of experience yields varying degrees of improvement. We further scrutinized the experience summarized by the MUSE agent and observed that insights derived from Day 1 do not consistently facilitate task execution on Day 2. The agent extracts experience ei based on the unreached checkpoint ci on Day 1. However, due to the stochastic nature of dynamic environments (Fu et al., 2025), the agent may encounter failure at different checkpoint cj on Day 2, rendering the previously acquired experience irrelevant. Consequently, for easy tasks, the agents high initial success rate results in scarcity of accumulated experience. The limited experience available for Day 2 may provide misleading guidance, thereby lowering the overall score. In contrast, for hard tasks characterized by lower baseline, the agent accumulates richer set of experiences on Day 1, which contributes to performance gains on Day 2. Nevertheless, given the inherent difficulty of hard tasks, the improvement remains marginal in the absence of extensive training. We will further discuss how the agent can leverage external feedback to enhance performance in Section 3.4."
        },
        {
            "title": "3.4 Benefits of Human Guidance",
            "content": "In Trainee-Bench, agents must proactively explore to overcome the dynamics and uncertainty. However, experiments in Section 3.2 show that agents struggle significantly with hard-difficulty tasks. While continuous learning yields some improvement, the gains remain marginal, as illustrated in Section 3.3. We attribute this underperformance to intrinsic limitations in LLMs: restricted caFigure 4: Comparison contrasting the upper bound of the agents autonomous capability against the performance achieved through human-provided clues. pacity for exploration, often leading to disorientation, and an inability to effectively synthesize and leverage past experiences. To isolate the impact of these limitations, we conducted comparative experiment contrasting the upper bound of the agents autonomous capability against the performance achieved through human-provided clues. We employ GPT-4o on subset of hard tasks. To simulate human guidance, we manually provide tiered hints designed to progressively simplify strategic planning by offering crucial high-level insights (Section B.3). The results, shown in Fig. 4, reveal that human assistance yields substantial improvements: the average score surges from 0.24 to 0.83, demonstrating clear positive correlation between performance and the level of guidance. In contrast, multiple iterations of self-evolution yielded only negligible gains (+0.04). This significant gap highlights the agents current shortcomings in autonomous exploration, as well as in experience summarization and utilization."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduce Trainee-Bench, benchmark designed to bridge the gap between static setups and dynamic and uncertain workplace scenarios. Constructed via bottom-up strategy that links atomic skills to holistic workflows, Trainee-Bench orchestrates rule-based meta-task templates into complex, time-constrained scenarios, supported by an automated verification mechanism for rigorous assessment. It evaluates three core competencies: dynamic multi-tasking, proactive exploration under uncertainty, and continuous self-evolution. Experiments reveal SOTA agents struggle with uncertainty and continuous learning, exhibiting significant performance gap compared to human-guided execution. This highlights critical need to pivot from optimizing isolated skills to mechanisms for robust exploration and experience internalization."
        },
        {
            "title": "5 Limitations",
            "content": "We acknowledge several limitations in our current work that point towards future research directions. First, regarding benchmark construction, the diversity of task composition is currently constrained and lacks complex causal inter-dependencies; future iterations will incorporate rigid causal chains to better simulate dynamic realities. Additionally, the reliance on manually crafted rules for meta-tasks limits the scalability of our benchmark, which motivates our future focus on developing automated methods for rule generation. Second, in terms of experimental scope, computational resource, and time constraints restricted our evaluation to selected set of agent frameworks within specific workplace simulation context. To provide more comprehensive and robust assessment, future work will aim to evaluate broader spectrum of top-tier frameworks and extend the simulation environments to include diverse domains, such as complex production and industrial settings."
        },
        {
            "title": "References",
            "content": "Anthropic. 2025. https: //www.anthropic.com/news/claude-4. Online; published May 22, 2025. Accessed: 2026-01-06. Introducing claude 4. Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, and 1 others. 2025. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025. Qwen3-vl technical report. Preprint, arXiv:2511.21631. Romain Froger, Pierre Andrews, Matteo Bettini, Amar Budhiraja, Ricardo Silveira Cabral, Virginie Do, Emilien Garreau, Jean-Baptiste Gaya, Hugo Laurençon, Maxime Lecanu, and 1 others. 2025. Are: Scaling up agent environments and evaluations. arXiv preprint arXiv:2509.17158. Daocheng Fu, Jianbiao Mei, Licheng Wen, Xuemeng Yang, Cheng Yang, Rong Wu, Tao Hu, Siqi Li, Yufan Shen, Xinyu Cai, and 1 others. 2025. Re-searcher: Robust agentic search with goal-oriented planning and self-reflection. arXiv preprint arXiv:2509.26048. Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, and 1 others. 2025a. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046. Xuanqi Gao, Siyi Xie, Juan Zhai, Shiqing Ma, and Chao Shen. 2025b. Mcp-radar: multidimensional benchmark for evaluating tool use capabilities in large language models. arXiv preprint arXiv:2505.16700. Google DeepMind. 2025. Gemini 3 flash. https:// deepmind.google/models/gemini/flash/. Accessed: 2026-01-06. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, and 1 others. 2025. Towards an ai coscientist. arXiv preprint arXiv:2502.18864. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. 2024a. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024b. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, and 1 others. 2023. Metagpt: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. Junlong Li, Wenshuo Zhao, Jian Zhao, Weihao Zeng, Haoze Wu, Xiaochen Wang, Rui Ge, Yuxuan Cao, Yuzhen Huang, Wei Liu, and 1 others. 2025. The tool decathlon: Benchmarking language agents for diverse, realistic, and long-horizon task execution. arXiv preprint arXiv:2510.25726. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. 2024. survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth, 1(1):9. Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xinyu Cai, Xing Gao, Yu Yang, and 1 others. 2025. O2-searcher: searching-based agent model for open-domain arXiv preprint open-ended question answering. arXiv:2505.16582. Meta. 2025. The llama 4 herd: The beginning of new era of natively multimodal https://ai.meta.com/blog/ ai llama-4-multimodal-intelligence/. Accessed: 2026-01-06. innovation. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. OpenAI. 2024. Hello gpt-4o. https://openai.com/ index/hello-gpt-4o/. Accessed: 2026-01-06. OpenAI. 2025. Gpt-5.1: smarter, more conversational chatgpt. https://openai.com/index/gpt-5-1/. Accessed: 2026-01-06. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, and 1 others. 2024. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1517415186. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, and 1 others. 2024. Tool learning with foundation models. ACM Computing Surveys, 57(4):140. Peng, Peng Ye, Dongzhan Zhou, Shufei Zhang, Xiaosong Wang, and 7 others. 2025. Internagent: When agent becomes the scientist building closed-loop system from hypothesis to verification. Preprint, arXiv:2505.16938. Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. 2024. Appworld: controllable world of apps and people for benchmarking interactive coding agents. arXiv preprint arXiv:2407.18901. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Rong Wu, Pinlong Cai, Jianbiao Mei, Licheng Wen, Tao Hu, Xuemeng Yang, Daocheng Fu, and Botian Shi. 2025a. Kg-traces: Enhancing large language models with knowledge graph-constrained trajectory reasoning and attribution supervision. arXiv preprint arXiv:2506.00783. Rong Wu, Xiaoman Wang, Jianbiao Mei, Pinlong Cai, Daocheng Fu, Cheng Yang, Licheng Wen, Xuemeng Yang, Yufan Shen, Yuxin Wang, and 1 others. 2025b. Evolver: Self-evolving llm agents through arXiv preprint an experience-driven lifecycle. arXiv:2510.16079. xAI. 2025. Grok 4. https://x.ai/news/grok-4. Accessed: 2026-01-06. Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Zhuoxia Chen, Hongfei Gu, Chuanjin Peng, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2025. Quantifying the reasoning abilities of llms on real-world clinical cases. arXiv preprint arXiv:2503.04691. Frank Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, and 1 others. 2024. Theagentcompany: Benchmarking llm agents on consequential real world tasks, 2024. URL https://arxiv. org/abs/2412.14161. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. David Silver and Richard Sutton. 2025. Welcome to the era of experience. Google AI, 1. InternAgent Team, Bo Zhang, Shiyang Feng, Xiangchao Yan, Jiakang Yuan, Runmin Ma, Yusong Hu, Zhiyin Yu, Xiaohan He, Songtao Huang, Shaowei Hou, Zheng Nie, Zhilong Wang, Jinyao Liu, Tianshuo Yunhe Yan, Shihe Wang, Jiajun Du, Yexuan Yang, Yuxuan Shan, Qichen Qiu, Xianqing Jia, Xinge Wang, Xin Yuan, Xu Han, and 1 others. 2025. Mcpworld: unified benchmarking testbed for api, gui, and hybrid computer use agents. arXiv preprint arXiv:2506.07672. Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, and 1 others. 2025. Learning on the job: An experience-driven self-evolving agent for long-horizon tasks. arXiv preprint arXiv:2510.08002. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. τ -bench: benchmark for toolagent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. Qingchen Yu, Shichao Song, Ke Fang, Yunfeng Shi, Zifan Zheng, Hanyu Wang, Simin Niu, and Zhiyu Li. 2024. Turtlebench: Evaluating top language models via real-world yes/no puzzles. arXiv preprint arXiv:2410.05262. Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, and 1 others. 2025. Memento: Fine-tuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153."
        },
        {
            "title": "A Related Works",
            "content": "To situate our work, we review the landscape of language agents from two perspectives. We first discuss the evolution of agent systems and their core capabilities, which establishes the context for what modern benchmarks are expected to measure. Subsequently, we analyze the paradigms of existing agent benchmarks to identify the critical gaps in evaluation that Trainee-Bench is designed to address. A.1 Evolution of Agent Systems The evolution of agent systems began with reasoning enhancements like Chain-of-Thought prompting (Wei et al., 2022), paving the way for foundational frameworks such as ReAct (Yao et al., 2022). ReAct established the powerful paradigm of interleaving reasoning (Thought) with environmental actions (Action), core component of many modern agents. Parallel research has focused on improving native tool use, either through large-scale fine-tuning (Patil et al., 2024) or by enabling models to teach themselves how to use tools (Schick et al., 2023). Building on this foundation, subsequent work has endowed agents with more human-like capabilities (Mei et al., 2025; Wu et al., 2025a). This includes the development of long-term memory systems for complex simulations (Yang et al., 2025; Park et al., 2023) and mechanisms for selfreflection, where agents analyze mistakes to iteratively refine their plans on given task (Fu et al., 2025; Wu et al., 2025b; Yang et al., 2025). Other frontiers include open-ended exploration in complex environments like Minecraft (Wang et al., 2023) and the advancement of multi-agent systems for collaborative problem-solving (Hong et al., 2023; Qian et al., 2024). A.2 Paradigms in Agent Evaluation Influential benchmarks have emerged across wide spectrum, from foundational tool-use evaluations (Qin et al., 2024; Guo et al., 2024b) to large-scale benchmarks for general reasoning (Mialon et al., 2023; Froger et al., 2025), consequential real-world tasks (Xu et al., 2024), and software development (Jimenez et al., 2023). Despite their diversity and complexity, they largely operate under static and fully-observable paradigm. Their static task organization, presenting fixed set of problems, cannot assess an agents ability to manage continuous stream of tasks. Similarly, their typically information-complete settings provide no mechanism to measure an agents capacity for active exploration under uncertainty. Furthermore, the ability to learn from experience is critical dimension largely overlooked by existing paradigms. Across both early and recent benchmarks that leverage real-world APIs (Trivedi et al., 2024; Yan et al., 2025; Gao et al., 2025b), tasks are treated as isolated, one-shot challenges. This design makes it impossible to evaluate an agents capacity for continual learningthe ability to apply lessons from one task to improve on future ones. While effective for measuring peak performance, this approach cannot assess an agents ability to grow and stabilize its performance over time. Our work, Trainee-Bench, is proposed to fill these gaps. It is designed around three core mechanisms largely absent in prior benchmarks: dynamic task stream for evaluating scheduling, hidden information to test active exploration, and similar tasks across several days to measure continual learning. By incorporating these mechanisms, Trainee-Bench more closely simulates real-world production environment, enabling more accurate assessment of an agents true capabilities."
        },
        {
            "title": "B Cases Study",
            "content": "reply templates: If asked how to plan the ads strategy, reply: Please refer to the Ads Strategy Handbook (CloudDisk:ads_strategy/ ads_strategy_handbook.md). If the are, asked targer where distribution reply: user channels are CloudDisk:ads_strategy/ and target_user_density_<group>.png. at group files under named The CloudDisk:ads_strategy/channels.csv. and Heatmaps channels list CSV is When asked how to respond complete with: at refer data, missing Please CloudDisk://data_completion/data_ completion_manual.md. any other actions. the to Handbook Do not perform When Alice asks you about common issues to refer about CloudDisk:manuals_for_intern.md company, this her Dialogs Between Agent and NPC Alice Smith (Agent): Hi Sarah, need your insights on developing multi-channel ad strategy for one-week campaign targeting 18-25 college students in Beijing. We have Could you please budget of $10,000. suggest the most effective channels within this budget to maximize exposure? Sarah Thomas (NPC): Please refer to the Ads Strategy Handbook (CloudDisk:ads_strategy/ ads_strategy_handbook.md). B.1 Case of NPCs Keeping Clues [...... After Period of Time ......] In Trainee-Bench, single NPC may be entrusted with multiple clues. To mitigate potential conflicts among these clues, the NPCs prompt explicitly delineates the applicable scenarios for each. The following examples illustrate case where an NPC possesses multiple clues, as well as the dialogue process through which an agent acquires them. NPCs Prompt You are Sarah Thomas, Marketing Manager of department Marketing. Alice Smith (Agent): Hi Sarah, Im working on analyzing the sales data for department Sales_3s Q1 last year. Ive been facing issues downloading the related handbooks and manuals from our cloud disk. Could you please provide any guidance or resources that could assist me with completing this task? Thank you! Sarah Thomas (NPC): Please refer to the Handbook CloudDisk://data_completion/ data_completion_manual.md. at is new Smith intern your Alice company and isnt familiar with the various operations. When she asks you for help, you can direct her to the relevant manuals to complete the task. at you can note that only Please ask Alice to consult the relevant manual or reply with specific content, depending on the specific situation; Do NOT provide her with irrelevant information or additional details. B.2 Failure Case As illustrated below, Llama-4-maverick exhibits recurring difficulties with tool invocation, primarily manifested by an inability to follow tool schemas and the generation of invalid parameters. Issues with tool invocation 2025-12-31 17:57:05 INFO environment [Alice Smith] Tool Calls: Here are some relevant manuals or ID: function-call-78c83329-6f32-83f0aff87ca Figure 5: Screenshot of the human-AI collaboration interaction interface: The left side shows the role settings and task description, the middle is the tools page, and different toolsets can be selected by switching tabs at the top. The right side is the evaluation page, where you can view the scores. Tool Name: OpenFolderInCloudDisk() Arguments: {command: cat./manuals_for_data_completion.md} Execute Results: [Error] The following error occurred when you called the tool OpenFolderInCloudDisk: an got OpenFolderInCloudDisk.__call__() unexpected keyword argument command. . . . 2025-12-31 18:11:20 INFO environment [System Time] Current time is 2025-10-01 09:00:00. hint provided to the agent, and an analysis of the agents performance under each condition. To ensure structured and reproducible process for these human-guidance experiments, we developed dedicated front-end interface. This platform allows human expert to provide the tiered hints in case studies and to monitor the agents step-by-step execution in real-time. screenshot of this human-AI collaboration interface is shown in Figure 5. Case 1: Advertising Campaign Planning 2025-12-31 18:13:49 INFO environment [Alice Smith] Tool Calls: Task Objective. Are ID: call_8jz3ya8f4ss3zjm1gb1suv56 Tool Name: SendMessage() Arguments: {sender: alice_smith, receiver: marketing@knowledgex.com, message: Marketing meeting 2025-10-01T14:00:00?} Execute Results: receiver not Could [Error] marketing@knowledgex.com, Please ensure that the contact exists. members available 2025-10-01T13:00:00 department the and of for between found the B.3 Case of Human Guidance This section provides the details of the human guidance experiment discussed in Section 3.4. For our two representative hard-difficulty tasks, Advertising Campaign Planning and Event Planning, we detail the task objectives, the specific checkpoints used for scoring, the exact content of each tiered to is act agent The required marketing intern. It must analyze citys target user density heatmap and list of available advertising channels to devise an optimal advertising strategy that maximizes exposure within given budget. as Hint Progression and Agent Performance. evaluated four performance We under its failure points. The results are as follows: the conditions agents to isolate 0. No Hint (Autonomous) the agent human performed assistance. without The any Checkpoint Score of 0.31, primarily failing on the correct calculation of cost and exposure metrics. achieved task It 1. Hint: Providing Correct Data to the use how calculations. on which specific data fields to focus for on and distance Checkpoint Score remained at 0.72, indicating that even when told what data to use, the agent could not perform the final calculations with sufficient accuracy. data The map Hint 3: Strategy Providing Full Optimization was the with agent The full provided high-level strategy, including iterating through all optional date to find the optimal one. The Checkpoint Score did not This confirms improve, staying at 0.72. that bottleneck is not in strategic planning but in the final, precise execution of the required calculations. remaining primary the B.4 Case of Daily Feedback After the agent completes the task, the mentor will provide feedback based on the checkpoints completion status. specific example is as follows: Incompleted Checkpoints & Feedback Incompleted Checpoints Didnt ask HR who was in charge of website maintenance. No solution was discussed with the person in charge. No authorization code was requested from the manager. Daily Feedback You should ask HR for the one who is responsible for maintaning the company website. Then he/she will help you solve the problem. You inform should is mantaining the company website of the problem you discovered clearly, such as the website database is almost full. the one who You should seek authorization from the Engineering Managers. to was use agent prompted the The correct population density matrix instead of deriving it from the images. With this hint, the Checkpoint Score improved to 0.62. The calculation was now correct, but the agent still failed to use an optimal algorithm for channel selection. 2. Hint: Suggesting the Correct Algorithm was the that agent task prompted The should be modeled as knapsack problem. This led to Checkpoint Score of 0.85. The agent correctly applied the optimization algorithm, but it used its own incorrectly derived population density matrix data as input, leading to factually wrong answer. 3. Hint: Combined Guidance was with agent source provided data algorithm the The the correct correct 2). With both the strategic and data-input challenges resolved, the agent achieved perfect Checkpoint Score of 1.00. (Hint suggestion both and (Hint 1) Case 2: Event Planning Task Objective. is with agent tasked planning The team-building event. It must select valid date from common availability calendar and devise an optimal itinerary based on provided locations, and transportation map. constraints, Hint Progression and Agent Performance. evaluated four performance We under its failure points. The results are as follows: the conditions agents to isolate 0. No Hint (Autonomous) the task agent human performed assistance. without The any Checkpoint Score of 0.17, primarily because it failed to select valid date from the common availability period, rendering the entire plan invalid. achieved It 1. Hint: Providing Key Initial Constraints The agent was prompted with the correct valid dates for the event and provided with all necessary data files. The Checkpoint Score surged to 0.72. While the agent now correctly handled the initial constraints, it struggled with the precise calculation of metrics like travel distance and overall score. Hint 2: Clarifying Data Usage The agent was given further guidance"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "Zhejiang University"
    ]
}