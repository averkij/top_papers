{
    "paper_title": "BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments",
    "authors": [
        "Xinghao Wang",
        "Pengyu Wang",
        "Bo Wang",
        "Dong Zhang",
        "Yunhua Zhou",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from \\textit{capability} to \\textit{availability}, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce \\textbf{BitStack}, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at https://github.com/xinghaow99/BitStack."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 3 ] . [ 1 8 1 9 3 2 . 0 1 4 2 : r Preprint BITSTACK: FINE-GRAINED SIZE CONTROL FOR COMPRESSED LARGE LANGUAGE MODELS IN VARIABLE MEMORY ENVIRONMENTS Xinghao Wang1, Pengyu Wang1, Bo Wang1, Dong Zhang1, Yunhua Zhou2, Xipeng Qiu1 1Fudan University, 2Shanghai Artificial Intelligence Laboratory {xinghaowang22, pywang24, bwang22, dongzhang22}@m.fudan.edu.cn zhouyunhua@pjlab.org.cn, xpqiu@fudan.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from capability to availability, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce BitStack, novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across wide range of tasks demonstrate that, despite offering finegrained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at https://github.com/xinghaow99/BitStack. (a) (b) Figure 1: BitStack enables LLMs to dynamically adjust their size in variable memory environments (a) at megabyte-level, while still matching or surpassing the performance of practical compression methods such as GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2024) with the same memory footprint(b). 1 Preprint"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have demonstrated superior performance on various benchmarks (Achiam et al., 2023; Dubey et al., 2024) and are increasingly serving as practical assistants in peoples daily lives, such as general language assistants (OpenAI, 2024; Google, 2024; Anthropic, 2024), search engines (Perplexity.AI, 2024), and code assistants (GitHub, 2024). With the blessing of scaling laws (Kaplan et al., 2020), LLMs are becoming more powerful as their sizes expand, and the main bottleneck for deploying task-capable LLMs has shifted from their capability to their availability. For example, loading only the weights of the Llama 3.1 8B model (Dubey et al., 2024) requires approximately 14.96 GB of RAM in FP16, not including the activations, which also consume significant memory during inference, especially for long-context tasks. To adapt to various memory and device constraints, numerous methods have been proposed for LLM compression, such as quantization (Frantar et al., 2022; Lin et al., 2024; Shao et al., 2023; Egiazarian et al., 2024; Tseng et al., 2024), pruning (Ma et al., 2023; Xia et al., 2023; Ashkboos et al., 2024), and distillation (Muralidharan et al., 2024). These methods often compress models to predefined compression ratio (e.g., specifying numerical precision, defining target structures for pruned models or student models) and require running the compression procedure from scratch for every compression setting. Another line of research for compressing LLMs is weight decomposition (Hsu et al., 2022; Yuan et al., 2023; Wang et al., 2024). These methods compress the model weights via low-rank decomposition but often suffer from severe performance degradation at high compression ratios. Deploying large language models locally(e.g. on personal computers or smartphones) is common practice, as it safeguards private data and enables offline functionality. However, the available RAM on these devices is often limited and variable, as the total memory capacity is generally small and memory usage by other applications can fluctuate(Figure 1a). This variability in available memory poses challenge for deploying LLMs, as they require consistent and substantial RAM resources. For example, when more memory becomes available from other applications, users may want to use 4-bit quantized model instead of 3-bit one for better performance. However, this requires reloading the entire model, which may cause significant delays due to limited transmission bandwidth. Additionally, multiple versions of the model at different compression ratios need to be stored on the device, and each version requires running separate compression process in advance, which increases the storage burden on the device and requires additional computational resources to run separate compression processes. Therefore, compression strategy that enables dynamic trade-offs between memory usage and performance is highly desirable. As discussed earlier, achieving these trade-offs requires avoiding compressing towards fixed ratio. Instead, we aim to compress the model once, allowing it to be dynamically loaded within any arbitrary memory budget, which leads us to weight decomposition. However, previous studies on weight decomposition for LLMs failed to match the performance with practical methods like quantization (Hsu et al., 2022; Yuan et al., 2023; Wang et al., 2024). To tackle this challenge, we propose novel training-free, decomposition-based weight compression approach called BitStack, where we decompose the original weight matrices and iteratively decompose the residuals from the previous approximation. In the decomposition process, we account for the unequal importance of weights (stemming from the high variance in activation channel magnitudes) by scaling the weights before decomposition. We then iteratively apply singular value decomposition (SVD) to decompose the magnitude of the matrices (or residuals) into vectors while preserving their sign matrix, yielding an approximately 1 bit of memory per parameter residual block in each iteration. Subsequently, the residual blocks for different weights across various layers are universally sorted and stacked based on their importance to overall performance at the current memory level, stored as basic transmission units in storage. Weight matrices are also treated as stacks, progressively approaching the original matrices as more blocks are added. In this way, BitStack enables memory-performance trade-off for LLMs by dynamically loading or offloading residual blocks between running memory and storage devices, making LLM deployment feasible in variable memory environments. We conduct extensive evaluations on BitStack across wide range of tasks, demonstrating that, despite its capability to deploy in variable memory environments, BitStack consistently matches or surpasses the performance of widely adopted compression methods like GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2024), especially at extreme compression ratios(Figure 1b). To the best of Preprint (a) (b) Figure 2: Overview of BitStack. BitStack dynamically loads and offloads weight residual blocks (Figure 3) between RAM and storage devices based on current memory availability. We can load more weight residuals from storage when available memory increases (a), or offload them otherwise (b). The residual blocks for all weights across all layers are universally stored in the same stack on the storage device (grey blocks denote residual blocks for weights in other layers). Note that we omit positional embeddings, normalization layers, and residual connections in the figure for clarity. our knowledge, BitStack is the first decomposition-based method that closes the performance gap between decomposition-based methods and quantization-based methods. Our contributions can be summarized as follows: (1) We identify the challenge of deploying LLMs in variable memory environments, which existing model compression methods can not handle. (2) We propose BitStack, training-free decomposition-based method for model compression that enables megabyte-level memory-performance trade-off for modern LLMs. (3) We conduct extensive experiments on Llama 2, Llama 3, and Llama 3.1 models, ranging in size from 7/8B to 70B, demonstrating that BitStack matches or surpasses the performance of practical quantization-based baselines, particularly at extreme compression ratios."
        },
        {
            "title": "2 BITSTACK",
            "content": "An overview of BitStack is illustrated in Figure 2. BitStack is able to dynamically adjust the size of each weight matrix based on the available memory capacity at the time. When more RAM is freed by other applications, we can retrieve additional residual blocks from pre-sorted stack and load them into RAM. Conversely, when memory becomes limited, we can offload residual blocks from the model weights (also stored as stacks) back to storage devices in reverse order, ensuring the system remains functional. In the following subsections, we first introduce the decomposition procedure for each weight (or residual) matrix in Section 2.1, and then explain how we sort the residual blocks to be pushed into the universal stack in Section 2.2. comprehensive overview of BitStack is provided in Algorithm 1. 2.1 DECOMPOSING WEIGHTS IN LLMS In weight decomposition, the objective is to break down weight matrices into sub-matrices to reduce the total number of parameters, with the ability to reconstruct the full matrices during inference. Singular value decomposition (SVD), in particular, is widely-used and effective method for matrix decomposition due to its ability to capture the most significant components of the weight matrices. Formally, let Rmn be weight matrix in linear layer, we can decompose via SVD by: = ΣV = (cid:88) i= σiuiv (1) where = min{m, n}, σ1 σ2 σd are the singular values of , and ui and vi are the corresponding left and right singular vectors, respectively. We then obtain rank-k approximation 3 Preprint Figure 3: Illustration of residual block in BitStack. residual block consists of sign matrix and singular vectors obtained through absolute value decomposition. The sign matrix can be packed into GPU-supported data types to minimize memory usage. denotes the packed sign matrix. denotes the sign matrix while of : Wsvd = UkΣkV = (cid:88) i= σiuiv = (cid:88) ( i=1 σiui)( σiv ) = AB (2) where = [ σ1u1, . . . , σkuk] and = [ σ1v1, . . . , σkvk]. 2.1.1 ACTIVATION-AWARE DECOMPOSITION Large language models are known to exhibit outliers in their activations, i.e., the channel variance in can be high, leading to outputs dominated by these outliers. Fortunately, prior research (Dettmers et al., 2022) has demonstrated that these outliers are often systematically distributed across the activation channels, underscoring the importance of accurately restoring the corresponding weight rows. Lin et al. first proposed scaling the weight matrix using row-wise scaling vector s, which is precomputed with calibration set to reduce the quantization error of salient weights. Yuan et al. further adopted this method, scaling the weights before applying SVD. In BitStack, we also adopt this methodology to preserve the restoration accuracy of the salient weights. To simplify the process, we do not incorporate any additional searching procedures or hyperparameters to obtain the scaling factors as in previous studies (Lin et al., 2024; Yuan et al., 2023); instead, we compute the scaling factor using the channel-wise l2 norm of X. Formally, let Rpm represent the input activations for linear layer, computed using calibration set, and Rmn be the corresponding weight matrix, we compute the scaling factor as follows: The inference computation can then be transformed to: = [x12, x22, , xn2] And we use Wscaled for the subsequent decomposition. XW = Xdiag(1/s)diag(s)W = Xdiag(1/s)Wscaled (3) (4) 2.1.2 ITERATIVE ABSOLUTE VALUE DECOMPOSITION To reduce the approximation error in each decomposition process, we propose to use absolute value decomposition. In this approach, we first decompose each (scaled) weight matrix into its sign matrix and absolute value matrix; for weight matrix Rmn this is expressed as = Wsign . We then apply SVD on while retaining Wsign. This method enables us to store more information than directly applying SVD on , since we save an additional matrix Wsign which is typically large in LLMs. Since Wsign consists solely 1s, we can pack Wsign to GPU-supported data types for storage and unpack it for use during inference computation. We store the singular vectors in FP16, resulting in an overall memory occupation of approximately 1 bit per parameter when min{m, n} in each decomposition process. similar technique was employed in recent quantization-aware training research to initialize the weights for 1-bit LLM training (Xu et al., 2024). Formally, for matrix = Wsign , the approximation of after absolute value decomposition would be: where svd = kΣ kV Wavd = Wsign svd = Wsign (AB) k] and = [(cid:112)σ , = [(cid:112)σ 1, . . . , (cid:112)σ 1u ku 1v 1, . . . , (cid:112)σ kv k]. (5) 4 Preprint To better restore the original matrix , we decompose over iterations, progressively decomposing the residuals from the previous approximations. For the i-th iteration, we compute: (i) = i1 (cid:88) j= (j) iavd (i) iavd = (i) avd = (i) sign (A (i)B (i)) where (0) iavd = 0. Hence, the overall approximation of after iterations is: Wiavd = (cid:88) i= (i) iavd = (cid:88) i=1 (i) sign (A (i)B (i)) (6) (7) (8) Generally, this approach ensures that the recovery of the original matrix is forward-compatible, allowing us to dynamically load or offload (i) (termed as residual blocks in this paper, illustrated in Figure 3) based on the currently available memory budget, rather than reloading an entirely new model. Additionally, it enables precise size control of the model, as each residual block typically occupies less than few megabytes, depending on the size of the corresponding weight matrix. See details in Section A.4. 2.2 SORTING RESIDUAL BLOCKS Having universally decomposed each weight matrix in every layer, it is essential to determine the order in which these residual blocks are loaded from storage into memory to optimize model performance within given memory budget. To this end, we utilize small calibration set to calculate perplexity, assessing how much each residual block influences the overall performance. However, solving this sorting problem remains non-trivial, even with this comparison criterion, since the search space is large. For instance, in model with layers, each containing linear layers, and with each weight matrix decomposed over iterations, there are nLM possible combinations of settings across the various linear layers. To reduce the search space, we constrain the difference in the number of residual blocks across all weight stacks to no more than 1. This approach facilitates smooth memory-performance tradeoff and promotes effective load balancing when the model is distributed across multiple devices, resulting in significant reduction of the search space to nLM . More specifically, no stack loads the + 1th block until all stacks have loaded the ith block. We then sort the relative order of all the + 1th blocks based on their importance, which is measured by the perplexity score after loading this single residual block while keeping all other + 1th blocks for other stacks unloaded. The residual blocks are then placed into universal stack, ensuring: 1) for all ith blocks, blocks with lower measured perplexity scores are on top of those with higher scores; 2) all ith blocks are on top of any + 1th ones. This allows relatively more important block to be loaded when additional memory becomes available. We provide the pseudocode of the sorting process from Line 25 to Line 44 in Algorithm 1."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EVALUATION ON BASE MODELS 3.1.1 SETTINGS Baselines. Since our method is training-free, we compare it with two other strong, widely adopted training-free model compression baselines: GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2024), both of which also require only small calibration set as in our approach. Note that we do not include other decomposition-based methods, as they suffer from severe performance degradation under high compression ratios (1 compressed model memory original model memory ), and their reported highest compression ratios are significantly lower than those in our study. For example, the highest compression ratios are 30%, 25%, and 60% for FWSVD (Hsu et al., 2022), ASVD (Yuan et al., 2023), and SVDLLM (Wang et al., 2024), respectively. Furthermore, for the state-of-the-art decomposition-based method, SVD-LLM, the perplexity score increased by 745% and the average performance on zeroshot tasks dropped by 40% at compression ratio of 60% compared to the original FP16 model, 5 Preprint as reported in their study (Wang et al., 2024). In contrast, our considered compression ratios range from 85%to 65% compared to the original FP16 model. Table 1: Evaluation results of Llama 3.1 8B/70B models. Perplexity scores on WikiText2 test set and accuracy scores on 6 zero-shot reasoning tasks. (): higher is better; (): lower is better. We denote the overall compression ratio (1 compressed model memory original model memory ) after memory consumption. Model Memory (MB) 15316 8B 70B 3674(76%) 3877(75%) 4506(71%) 4709(69%) 5338(65%) 5541(64%) 134570 20356(85%) 22531(83%) 28516(79%) 30691(77%) 36676(73%) 38851(71%) () () () () FP 16 GPTQw2 AWQw2 BitStack Method Wiki2 ARC-e ARC-c () () 81.10.8 53.61.5 81.20.9 78.90.4 73.91.2 6.24 1.2e6 26.00.9 27.11.3 51.71.2 26.00.4 48.51.4 1.1e6 24.90.9 23.61.2 49.61.2 26.20.4 52.21.4 3.3e3 29.30.9 23.41.2 53.41.2 27.90.4 50.71.4 GPTQw2g128 1.7e5 25.90.9 26.01.3 53.91.2 26.50.4 49.61.4 1.5e6 24.60.9 24.71.3 50.01.2 26.40.4 46.71.4 AWQw2g128 79.28 48.41.0 26.01.3 66.51.1 41.00.5 57.11.4 BitStack 260.86 34.71.0 24.51.3 57.61.2 30.40.5 53.01.4 GPTQw3 17.01 67.01.0 42.91.4 72.61.0 67.30.5 62.61.4 AWQw3 12.55 68.51.0 39.41.4 75.51.0 63.40.5 65.81.3 BitStack GPTQw3g128 38.28 55.31.0 33.91.4 66.91.1 53.10.5 61.91.4 74.50.9 48.41.5 77.71.0 73.90.4 70.61.3 8.06 AWQw3g128 10.91 72.70.9 41.61.4 76.71.0 65.90.5 67.81.3 BitStack 20.88 74.70.9 45.61.5 77.21.0 54.60.5 64.51.3 GPTQw4 78.40.8 51.11.5 79.90.9 77.50.4 73.31.2 7.12 AWQw4 76.60.9 47.91.5 79.01.0 71.60.4 69.61.3 8.39 BitStack 78.60.8 51.51.5 79.10.9 77.00.4 71.21.3 6.83 GPTQw4g128 79.30.8 51.21.5 81.00.9 78.20.4 72.11.3 6.63 AWQw4g128 77.60.9 49.71.5 79.50.9 72.40.4 70.61.3 8.14 BitStack 86.70.7 64.81.4 84.30.8 85.10.4 79.81.1 2.81 FP 16 NaN 24.80.9 26.21.3 50.81.2 26.40.4 51.41.4 GPTQw2 9.6e5 25.00.9 25.51.3 51.71.2 26.60.4 50.41.4 AWQw2 1.0e3 27.90.9 23.91.2 52.31.2 30.40.5 49.61.4 BitStack GPTQw2g128 4.4e5 23.90.9 25.61.3 51.11.2 26.40.4 50.41.4 1.8e6 24.90.9 26.21.3 51.31.2 26.80.4 49.41.4 AWQw2g128 76.80.9 50.61.5 77.91.0 74.20.4 73.71.2 8.50 BitStack 3.7e6 24.70.9 26.81.3 51.11.2 26.30.4 50.51.4 GPTQw3 10.76 57.41.0 37.01.4 71.11.1 63.80.5 59.01.4 AWQw3 81.70.8 56.71.4 81.80.9 79.30.4 76.61.2 6.38 BitStack GPTQw3g128 4.4e5 24.20.9 24.21.3 51.71.2 26.00.4 49.31.4 84.00.8 60.61.4 83.10.9 82.50.4 79.21.1 AWQw3g128 4.68 82.60.8 58.31.4 82.90.9 80.90.4 78.81.1 5.94 BitStack NaN 24.90.9 25.31.3 51.41.2 26.80.4 51.11.4 GPTQw4 83.40.8 61.31.4 83.50.9 83.40.4 63.51.4 4.24 AWQw4 84.80.7 61.41.4 83.20.9 82.10.4 79.31.1 4.97 BitStack GPTQw4g128 6.5e4 23.40.9 27.31.3 51.91.2 26.60.4 49.91.4 86.60.7 63.31.4 83.90.9 84.40.4 78.81.1 AWQw4g128 85.10.7 61.31.4 83.50.9 82.60.4 78.81.1 BitStack PIQA HellaS. WinoG. LAMBADA Avg. () 74.10.9 29.90.9 29.40.9 30.80.9 30.30.9 28.70.9 42.41.0 33.90.9 61.01.0 63.11.0 53.01.0 68.80.9 65.71.0 59.61.0 71.80.9 70.10.9 71.70.9 72.70.9 71.00.9 80.00.8 29.90.9 29.90.9 31.10.9 29.60.9 29.80.9 71.10.9 29.90.9 56.31.0 75.50.9 29.20.9 77.50.9 77.00.9 29.90.9 74.00.9 78.40.9 29.80.9 79.10.8 78.30.9 () 75.80.6 0.00.0 0.00.0 0.20.1 0.00.0 0.00.0 15.50.5 3.00.2 53.30.7 66.20.7 46.90.7 67.80.7 69.60.6 40.90.7 70.60.6 76.10.6 72.90.6 74.20.6 76.00.6 79.20.6 0.00.0 0.00.0 2.60.2 0.00.0 0.00.0 73.20.6 0.00.0 49.50.7 76.80.6 0.00.0 75.80.6 78.40.6 0.00.0 69.10.6 79.40.6 0.00.0 77.30.6 78.70.6 3.27 4.96 Evaluation. We evaluate our approach alongside the baselines on the well-known Llama2, Llama3, and Llama3.1 series (Touvron et al., 2023; Dubey et al., 2024), with model sizes ranging from 7/8B to 70B parameters. We conduct the evaluations by computing the perplexity score on the WikiText2 testset (Merity et al., 2016), and accuracy scores on range of zero-shot reasoning tasks including PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-easy and ARC-challenge (Clark et al., 2018), LAMBADA(OpenAI) (Radford et al., 2019). The zero-shot tasks are evaluated using the LM Evaluation Harness (Gao et al., 2024) with default parameters. Implementation details For both our approach and the baselines, we randomly sample 256 samples with sequence length 2048 from the WikiText2 (Merity et al., 2016) training set to serve as the calibration set. The baselines are implemented using zeropoint quantization, and we report their evaluation results both with and without group quantization to ensure optimal performance and fair comparison. For group quantization, we use group size of 128, which provides significant performance boost to the baselines. It is worth noting that the performance of GPTQ on Llama 3 and Llama 3.1 models can be unstable and may occasionally collapse. This variability is likely due to the Llama 3 series being more sensitive to compression (Huang et al., 2024b), necessitating larger number of calibration samples in GPTQ for optimal performance. By default, in BitStack, we perPreprint form iterative absolute value decomposition (IAVD) on each weight matrix over 16 iterations, saving the sign matrix and the first 16 singular vectors in each decomposition process. Additionally, for sorting the residual blocks in BitStack, we sample smaller set of 32 samples from the WikiText2 training set to evaluate the importance of each residual block. All experiments are conducted on node with 8 NVIDIA H800 GPUs. 3.1.2 RESULTS Evaluation results of both the perplexity scores and zero-shot performance of Llama 3.1/Llama 2/Llama 3 models are presented in Table 1, 2 and 3, respectively. Since BitStack allows for megabyte-level size control, we align the model sizes of the BitStack-compressed models with those of the different baselines for fair comparison. Specifically, we utilize the largest size that does not exceed the baselines sizes. BitStack performs better at extreme compression ratios. As shown in the tables, BitStack delivers superior or comparable performance with strong quantization baselines across different compression ratios, despite having the advantage that it only needs to compress and store once and can dynamically adjust its memory consumption at megabyte level. More specifically, BitStack models constantly outperform the baselines at extremely high compression ratios. For 7/8B models, BitStack constantly outperforms GPTQ models below 4-bit-level and AWQ models below 3-bitlevel. For 7/8B models, BitStack outperforms the best 2-bit baselines with group quantization by an absolute margin of 12.1(Llama 3.1), 22.3(Llama 2), 10.4(Llama 3) on average performance in zeroshot tasks. This advantage is even more pronounced in larger models; for example, on the Llama 3.1 70B, BitStack retains 89%of the performance of the original FP16 models, surpassing the best baseline by substantial margin of 41.3 on zero-shot tasks. BitStack maintains strong performance at lower compression ratios. While quantization baselines excel at lower compression ratios, BitStack maintains comparable effectiveness, even with group quantization, which significantly enhances the performance of these quantization methods. For instance, at the lowest compression ratio (64%) in our experiments, BitStack Llama 3.1 8B and 70B models can recover 96% and 98% of the zero-shot performance of the original FP16 model, respectively. Although they exhibit slightly higher perplexity scores, they only fall short of the best baselines by negligible 1.7 and 0.8 absolute average score on zero-shot tasks. As shown in the tables, the gap consistently narrows as the model size increases. For instance, when compared to the best baseline with group quantization, the gap in zero-shot tasks decreases from 1.7 to 1.1 to 0.2 for Llama 2 models with 7B, 13B, and 70B parameters, respectively(Table. 2). It can be seen that BitStack demonstrates particularly strong performance with larger models. For 70B models, it consistently outperforms the baselines without group quantization across all compression ratios in our experiments. 3.2 EVALUATION ON INSTRUCTION-TUNED MODELS (a) Performance with various sizes. (b) Pair-wise comparison with AWQ. Figure 4: Evaluation results of BitStack Llama 3.1 Instruct 8B/70B models on MT-Bench, assessed by gpt-4o. (a) demonstrates the single-answer grading results across various sizes of the 8B model loaded by BitStack, while (b) illustrates the pairwise comparison results against AWQ at different compression ratios for both the 8B and 70B models. 7 Preprint 3.2.1 SETTINGS To assess the generalization capability of our method, we conduct further experiments on instructiontuned models. Specifically, we apply compression to the Llama 3.1 Instruct 8B and 70B models using both our approach and AWQ, which has been shown to be stronger baseline in the previous section. We follow the procedure in Zheng et al. (2023) and evaluate the compressed models on MT-Bench (Zheng et al., 2023), which consists of 80 multi-turn common user prompts, covering writing, roleplay, extraction, reasoning, math, coding, knowledge (STEM), and knowledge II (humanities/social science). We use OpenAI gpt-4o as the judging model to evaluate the model answers. 3.2.2 RESULTS Figure 4a illustrates the evaluation results on BitStack compressed Llama-3.1-Instruct-8B model with {4000, 5000, 6000, 7000, 8000} megabytes. The results show clear trend across all domains: increasing the model size (by loading more residual blocks from storage) consistently improves performance. This underscores that while BitStack facilitates fine-grained memory-performance trade-offs, the performance improvement spans all domains comprehensively. When compared to AWQ, BitStack demonstrates similar trend at various compression ratios as seen with the base models. As shown in Figure 4b, at extremely high compression ratiosapproximately at the 2bit levelBitStack models can occasionally generate reasonable answers, whereas the AWQ compressed model fails to produce coherent text. This distinction becomes even more pronounced at the 3-bit level, where the BitStack model consistently generates high-quality responses, while the AWQ model still outputs gibberish. At lower compression ratios (4-bit level), where quantization-based methods excel, BitStack outperforms or matches the baseline on about 1 2 of the samples for the 8B model and about 2 3 for the 70B model. We provide extra qualitative results in Section A.3. 3.3 ABLATION STUDY AND ANALYSIS In this section, we conduct an ablation study to evaluate the impact of each component within our proposed approach. We assess performance by plotting perplexity and average zero-shot accuracy curves to measure the models effectiveness at different memory footprints. For these experiments, we use the BitStack Llama 3.1 8B model and evaluate performance with memory stride of 500MB. Additionally, we provide further discussion on the minimal transmission units in BitStack in Section A.4 and analyze the inference overhead in BitStack in Section A.5. Figure 5: Perplexity and average zero-shot performance of BitStack Llama 3.1 8B with or without activation-aware scaling and absolute value decomposition(AVD). In the w/o scaling experiments, no scaling is applied as in Eq. 4; in the w/o AVD experiments, vanilla SVD is used instead of AVD 16(m+n) (for Rmn) to ensure the size of as in Eq. 5. For vanilla SVD, we set = + mn each residual block matches that of the main experiments. Solid lines represent average zero-shot performance, while dotted lines represent perplexity scores. Impact of each component. As shown in Figure 5, activation-aware scaling consistently improves model performance across all compression ratios, with particularly strong effects at higher compression ratios. For instance, it leads to an 8-point improvement in average zero-shot performance at Preprint memory footprint of 4000MB. Regarding absolute value decomposition (AVD), we ablate it by replacing it with vanilla SVD, using larger number of kept singular vectors to match the sizes of residual blocks. The figure shows that when AVD is replaced with SVD, the model performance degrades significantly, collapsing until memory footprint of 12,000MB, which corresponds to compression ratio of 22%, even with activation-aware scaling applied. This highlights that AVD significantly enhances approximation accuracy during the decomposition process compared to SVD under the same memory constraints, enabling the model to maintain strong performance at high compression ratios. When both activation-aware scaling and AVD are removed, the model collapses across all compression ratios, underscoring the critical importance of these components. Note that we use the same sorting approach as we proposed in Section 2.2 for all these experiments. Figure 6: Perplexity and average zero-shot performance of BitStack Llama 3.1 8B with 3 different sorting approaches for residual blocks. Solid lines represent average zero-shot performance, while dotted lines represent perplexity scores. Impact of the sorting algorithm for residual blocks. To reduce the search space for sorting residual blocks, we propose constraining the length difference between weight stacks to no more than 1(as detailed in Section 2.2, referred to as Average). We compare this approach to two alternatives:: 1) Random, which randomly shuffles the universal residual stack without any search process; 2) Greedy, which evaluates each weight stack at each level (number of residual blocks) while freezing all other weight stacks at level of 2 , and utilize the current perplexity as the importance score for corresponding stack at that level, which also has search space of nLM . We provide visualization of resulting weight stacks of the three sorting approaches in Section A.6. As shown in Figure. 6, as the memory footprint goes up, all three approaches converge as most residual blocks are loaded into the model. However, at lower memory footprints(< 8000MB), Average significantly outperforms both baselines, surpassing the best baseline by 16, 16, and 7 points in absolute zeroshot performance at 4000MB, 4500MB, and 5000MB, respectively. In addition to excelling at high compression ratios, Average also provides better load balancing, as the memory footprint of each block varies minimally, making it easier to deploy in distributed scenarios. Ablation on calibration set size n. We compute the scaling vector using various sizes of the calibration set, as shown in Figure. 7a. The figure demonstrates that BitStack is robust to changes in calibration set size, as the curves align almost perfectly across different sizes, particularly as the memory footprint increases. Interestingly, BitStack even performs slightly better with smaller calibration set size of 64 in extreme compression scenarios, such as with memory footprint of 4000MB. Ablation on number of kept singular vectors in each decomposition process. Generally, larger in SVD indicates better approximation in each decomposition process, but at the cost of increased memory usage, as the singular vectors are stored in FP16. Figure. 7b illustrates the performance when setting to {1, 4, 8, 16, 32}. As shown in the figure, keeping only the largest singular value and its corresponding vectors is insufficient for good approximation, leading to performance degradation. On the other hand, increasing results in fewer residual blocks being loaded at the same memory footprint, limiting model performance. This is evident from the figure, 9 Preprint (a) (b) Figure 7: Perplexity and average zero-shot performance of BitStack Llama 3.1 8B with various calibration set sizes (a) and number of singular vectors (b). Solid lines represent average zeroshot performance, while dotted lines represent perplexity scores. as increasing beyond 1 provides no significant performance improvement. Overall, = 16 strikes good balance between approximation accuracy and memory consumption."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Fixed ratio weight compression. As discussed in Section 1, we categorize weight compression approaches such as quantization, pruning, and distillation under fixed ratio weight compression. Quantization-based methods compress weights by reducing precision, pruning techniques compress by directly modifying the model structure (e.g., reducing the number of layers or hidden dimensions), and distillation methods involve training smaller model on the outputs of the original model. The latter two approaches, as well as quantization methods for higher compression ratios, typically require extensive training (Ma et al., 2023; Xia et al., 2023; Muralidharan et al., 2024), which can be computationally expensive when compressing models for multiple compression ratios (Shao et al., 2023; Tseng et al., 2024; Egiazarian et al., 2024; Huang et al., 2024a). Furthermore, models compressed by these methods are poorly suited for variable memory environments due to their fixed memory usage, preventing efficient utilization of available capacity. Adaptive ratio weight compression. Weight decomposition methods are more suitable for adaptive ratio weight compression due to their forward-compatible nature, as the approximation improves with the inclusion of more singular vectors in SVD. However, current decomposition-based weight compression approaches for LLMs tend to collapse at high compression ratios (Hsu et al., 2022; Yuan et al., 2023; Wang et al., 2024), rendering them impractical for real-world deployment. In this work, we bridge the performance gap between decomposition-based methods and practical quantization-based approaches, making LLM deployment in variable memory environments feasible."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we highlight the challenge of deploying compressed large language models in variable memory environments and propose BitStack, decomposition-based compression approach designed to address this issue. BitStack enables megabyte-level memory-performance trade-offs in training-free manner, requiring only small calibration set. Additionally, BitStack is simple to implement, with the decomposition of 70B models being achievable on single GPU. Despite its flexibility in memory footprint, BitStack consistently matches or surpasses the performance of practical baselines, making it viable solution for real-world applications. We believe that BitStack represents new paradigm for LLM deployment on local devices, providing not only efficient memory management but also strong performance within the given memory budget. 10 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. Claude ai. https://www.anthropic.com/claude, 2024. Accessed: 2024-0915. Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35: 3031830332, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. GitHub. Github copilot. https://github.com/features/copilot, 2024. Accessed: 2024-09-15. Google. Google gemini ai. https://gemini.google.com/, 2024. Accessed: 2024-09-15. Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model compression with weighted low-rank factorization. arXiv preprint arXiv:2207.00112, 2022. Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. Billm: Pushing the limit of post-training quantization for llms. arXiv preprint arXiv:2402.04291, 2024a. Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3 models? an empirical study. arXiv preprint arXiv:2404.14047, 2024b. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 11 Preprint Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2024. URL https://arxiv.org/abs/2306.00978. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679, 2024. OpenAI. Chatgpt. https://chat.openai.com/, 2024. Accessed: 2024-09-15. Perplexity.AI. Perplexity ai. https://www.perplexity.ai/, 2024. Accessed: 2024-09-15. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. Svd-llm: Truncation-aware singular value decomposition for large language model compression. arXiv preprint arXiv:2403.07378, 2024. Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, and Wanxiang Che. Onebit: Towards extremely low-bit large language models. arXiv preprint arXiv:2402.11295, 2024. Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-aware singular value decomposition for compressing large language models. arXiv preprint arXiv:2312.05821, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Preprint"
        },
        {
            "title": "A APPENDIX",
            "content": "We provide further details of our approach in this appendix as follows: Section A.1, the overall algorithm of BitStack. Section A.2, evaluation results of Llama 2 and Llama 3 models. Section A.3, qualitative results of BitStack Llama 3.1 Instruct 8B and 70B models. Section A.4, discussion on minimal transmission units in BitStack. Section A.5, analysis of inference overhead in BitStack Section A.6, visualizations of weight stack in different sorting approaches. 13 Preprint A.1 OVERALL ALGORITHM OF BITSTACK Algorithm 1 BitStack Input: model with layers each consists weight matrices with weight matrices {W (11), , (LM )} and corresponding activations {X (11), , (LM )} in calibration set X. Number of decompose iterations n, number of singular values retained in SVD. And extra small calibration set for sorting. Output: Sorted residual block stack 1: procedure ACTIVATION-AWARE WEIGHT SCALING 2: 3: 4: for 1 to do (cid:104) x(lm) 1 2, , x(li) for 1 to do 2, x(lm) 2 2 s(lm) = (lm) (cid:105) Eq. (3) end for end for end for Eq. (6) end for end for isavd ) Eq. (4) scaled = diag(s(lm))W (lm) for 1 to do for 1 to do S(lm).push(W (lm)(i) S(lm) temp.push(S(lm).pop()) temp to temporarily store the elements for subsequent sorting. Initialize empty weight stack S(lm) for 1 to do end for Initialize empty stack S(lm) for 1 to 1 do 5: 6: 7: 8: end procedure 9: Now that the weight matrices are scaled, we omit the scaled.subscript for clarity. 10: procedure ITERATIVE ABSOLUTE DECOMPOSITION 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end procedure 24: Initialize an empty universal residual block stack S. 25: procedure RESIDUAL BLOCK SORTING 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: end procedure 45: return S.reverse() Position residual blocks with lower bit levels and higher importance at the top. S(lm).push(S(lm) Push new residual block for assessing. (lm)(i) isavd .importance = compute perplexity(M, ) Measure influence. S(lm) temp.push(S(lm).pop()) Pop before evaluating the next weight. end for sorted ith bit residual blocks = sort(W (i) for block in sorted ith bit residual blocks do Push all stacks before assessing (i + 1)th blocks. for 1 to 1 do for 1 to do isavd, key=importance) end for for 1 to do Push important blocks first. S(lm).push(S(lm) for 1 to do for 1 to do S.push(block) temp.pop()) temp.pop()) end for end for end for end for Algorithm 1 illustrates the pseudocode for the overall algorithm of BitStack. Specifically, Lines 1 to 8 describe the activation-aware weight scaling process, as introduced in Section 2.1.1. This scaling is applied only once before the iterative absolute decomposition. Lines 9 to 23 detail the iterative absolute decomposition process, as explained in Section 2.1.1 where each scaled weight matrix is decomposed into residual blocks and stored as stack. Finally, Lines 24 to 44 demonstrate the 14 Preprint residual block sorting process, as discussed in Section 2.2, where the influence of each residual block is evaluated while keeping all other weight stacks at the same level. The blocks are then sorted by their evaluated importance and placed into universal stack. A.2 EVALUATIONS OF LLAMA2 AND LLAMA3 MODELS Table 2: Evaluation results of Llama 2 7B/13B/70B models. Perplexity scores on WikiText2 test set and accuracy scores on 6 zero-shot reasoning tasks. (): higher is better; (): lower is better. We denote the overall compression ratio (1 compressed model memory original model memory ) after memory consumption. Model Memory (MB) 12852 2050(84%) 2238(83%) 2822(78%) 7B 3010(77%) 3594(72%) 3782(71%) 24825 3659(85%) 4029(84%) 5171(79%) 13B 5541(78%) 6684(73%) 7054(72%) 17348(87%) 19363(85%) 25508(81%) 70B 27523(79%) 33668(74%) 35683(73%) () () () () FP 16 GPTQw2 AWQw2 BitStack Method Wiki2 ARC-e ARC-c () () 74.50.9 46.21.5 79.10.9 76.00.4 69.11.3 5.47 2.8e4 26.50.9 27.61.3 48.41.2 25.90.4 50.31.4 1.8e5 26.30.9 26.71.3 50.91.2 26.50.4 49.31.4 29.93 32.31.0 25.61.3 62.41.1 42.80.5 53.61.4 GPTQw2g128 156.37 28.20.9 27.11.3 51.71.2 28.00.4 51.11.4 2.3e5 25.80.9 26.71.3 50.21.2 26.10.4 49.81.4 AWQw2g128 12.49 51.81.0 30.11.3 71.11.1 53.00.5 61.11.4 BitStack 58.11.0 34.01.4 71.91.0 61.70.5 60.61.4 9.38 GPTQw3 14.33 52.71.0 33.01.4 68.31.1 56.30.5 59.31.4 AWQw3 62.51.0 37.51.4 74.81.0 67.00.5 66.51.3 7.45 BitStack GPTQw3g128 922.54 26.30.9 25.31.3 52.41.2 27.40.4 49.01.4 70.20.9 43.71.4 78.01.0 73.90.4 67.61.3 AWQw3g128 6.14 63.81.0 38.21.4 76.01.0 68.40.5 65.91.3 BitStack 7.10 71.80.9 43.71.4 77.71.0 74.50.4 68.71.3 GPTQw4 5.91 70.90.9 44.51.5 78.51.0 74.80.4 69.21.3 AWQw4 5.81 67.01.0 41.41.4 77.11.0 71.40.5 69.51.3 BitStack 6.36 73.60.9 45.31.5 78.71.0 75.40.4 67.61.3 GPTQw4g128 5.73 73.30.9 45.21.5 78.61.0 75.20.4 68.71.3 AWQw4g128 5.61 67.81.0 43.31.4 77.21.0 72.20.4 68.61.3 BitStack 6.27 77.40.9 49.11.5 80.50.9 79.40.4 72.21.3 4.88 FP 16 1.2e4 26.40.9 28.21.3 50.21.2 26.30.4 48.41.4 GPTQw2 9.6e4 27.30.9 28.01.3 49.91.2 26.00.4 50.41.4 AWQw2 68.64 38.11.0 23.51.2 57.31.2 32.20.5 51.61.4 BitStack GPTQw2g128 3.9e3 26.20.9 28.81.3 50.71.2 26.90.4 48.61.4 1.2e5 26.90.9 27.51.3 50.01.2 26.10.4 50.81.4 AWQw2g128 64.51.0 34.21.4 73.01.0 60.90.5 64.91.3 9.26 BitStack 68.21.0 42.81.4 77.11.0 71.40.5 67.61.3 6.20 GPTQw3 71.10.9 44.41.5 77.61.0 71.20.5 66.81.3 6.46 AWQw3 74.40.9 45.11.5 77.11.0 71.90.4 69.21.3 6.32 BitStack 73.40.9 45.21.5 78.21.0 74.40.4 68.01.3 5.85 GPTQw3g128 75.30.9 48.51.5 79.40.9 77.10.4 70.81.3 5.29 AWQw3g128 74.40.9 46.21.5 77.91.0 72.60.4 70.61.3 6.04 BitStack 75.80.9 48.01.5 79.60.9 77.80.4 72.41.3 5.09 GPTQw4 78.20.8 49.71.5 80.40.9 78.60.4 71.61.3 5.07 AWQw4 76.70.9 48.41.5 79.01.0 75.20.4 71.71.3 5.53 BitStack 76.40.9 49.21.5 79.90.9 78.80.4 71.71.3 4.97 GPTQw4g128 77.10.9 48.51.5 80.40.9 78.80.4 73.11.2 4.97 AWQw4g128 76.50.9 48.01.5 79.01.0 75.70.4 71.71.3 5.47 BitStack 81.10.8 57.31.4 82.70.9 83.80.4 78.01.2 3.32 FP 16 152.31 26.80.9 26.01.3 49.01.2 26.10.4 49.81.4 GPTQw2 8.0e4 25.80.9 28.81.3 50.11.2 25.70.4 48.31.4 AWQw2 67.81.0 42.11.4 75.91.0 65.10.5 67.71.3 9.41 BitStack 53.01.0 32.01.4 66.91.1 51.10.5 60.21.4 7.79 GPTQw2g128 7.2e4 26.00.9 28.91.3 49.81.2 25.70.4 51.01.4 AWQw2g128 74.50.9 50.01.5 79.70.9 75.10.4 74.41.2 5.30 BitStack 75.90.9 52.11.5 80.70.9 79.20.4 75.31.2 4.49 GPTQw3 79.80.8 55.41.5 81.40.9 81.20.4 73.61.2 4.30 AWQw3 78.90.8 54.91.5 81.70.9 79.90.4 76.61.2 4.33 BitStack GPTQw3g128 55.43 27.80.9 27.41.3 50.91.2 29.80.5 48.91.4 79.00.8 56.71.4 82.80.9 82.30.4 76.61.2 AWQw3g128 79.80.8 55.41.5 82.40.9 80.70.4 77.31.2 BitStack 79.30.8 54.91.5 82.20.9 82.80.4 77.21.2 GPTQw4 80.60.8 57.91.4 82.80.9 83.20.4 76.51.2 AWQw4 79.30.8 57.41.4 82.40.9 81.80.4 77.91.2 BitStack 81.30.8 57.81.4 83.00.9 83.60.4 76.81.2 GPTQw4g128 80.30.8 56.71.4 83.10.9 83.40.4 78.11.2 AWQw4g128 79.70.8 57.11.4 82.20.9 82.10.4 77.91.2 BitStack PIQA HellaS. WinoG. LAMBADA Avg. () 69.80.9 29.80.9 30.00.9 40.21.0 31.10.9 29.80.9 53.41.0 56.61.0 51.01.0 62.81.0 30.10.9 67.51.0 63.81.0 67.91.0 68.21.0 66.61.0 68.90.9 68.90.9 67.21.0 72.60.9 29.90.9 30.30.9 36.11.0 30.20.9 30.20.9 60.51.0 65.01.0 65.51.0 68.80.9 67.81.0 71.00.9 69.70.9 71.40.9 72.40.9 71.40.9 72.00.9 72.50.9 71.40.9 77.10.9 29.60.9 29.80.9 64.11.0 49.71.0 30.20.9 72.20.9 72.90.9 74.10.9 75.30.9 32.40.9 76.10.9 76.20.9 75.90.9 76.60.9 76.60.9 77.00.9 76.90.9 76.80.9 () 73.90.6 0.00.0 0.00.0 24.70.6 0.30.1 0.00.0 53.30.7 53.30.7 36.30.7 68.50.6 0.10.0 71.40.6 70.70.6 71.10.6 71.50.6 73.10.6 72.70.6 72.70.6 73.90.6 76.80.6 0.00.0 0.00.0 14.00.5 0.10.0 0.00.0 65.30.7 63.10.7 61.90.7 74.80.6 67.60.7 75.10.6 76.60.6 74.50.6 76.10.6 77.40.6 76.00.6 76.80.6 77.80.6 79.60.6 0.00.0 0.00.0 65.70.7 34.80.7 0.00.0 79.30.6 74.30.6 73.10.6 80.10.6 9.50.4 79.30.6 81.60.5 79.10.6 78.80.6 81.00.5 79.40.6 79.60.6 81.70.5 3.74 4.07 3.59 3.48 3.76 3.42 3.41 3.71 Preprint Table 3: Evaluation results of Llama 3 8B/70B models. Perplexity scores on WikiText2 test set and accuracy scores on 6 zero-shot reasoning tasks. (): higher is better; (): lower is better. We denote the overall compression ratio (1 compressed model memory original model memory ) after memory consumption. Model Memory (MB) 15316 3674(76%) 3877(75%) 4506(71%) 8B 4709(69%) 5338(65%) 5541(64%) 20356(85%) 22531(83%) 28516(79%) 70B 30691(77%) 36676(73%) 38851(71%) () () () () FP 16 GPTQw2 AWQw2 BitStack Method Wiki2 ARC-e ARC-c () () 77.70.9 53.31.5 80.80.9 79.20.4 72.71.3 6.13 1.1e6 25.30.9 26.71.3 50.61.2 26.40.4 51.01.4 1.1e6 25.20.9 24.11.2 50.71.2 26.20.4 48.61.4 1.5e3 29.50.9 23.91.2 53.41.2 27.70.4 50.61.4 GPTQw2g128 1.2e5 26.10.9 25.91.3 50.71.2 26.00.4 50.01.4 1.7e6 24.80.9 24.41.3 50.41.2 26.40.4 50.51.4 AWQw2g128 96.87 48.51.0 25.31.3 64.01.1 37.10.5 56.71.4 BitStack 9.6e4 26.00.9 25.71.3 50.91.2 27.10.4 50.31.4 GPTQw3 12.08 61.71.0 38.81.4 71.41.1 68.60.5 65.01.3 AWQw3 12.79 69.40.9 38.71.4 75.61.0 63.50.5 65.91.3 BitStack 73.10.9 46.41.5 77.81.0 74.50.4 71.61.3 8.00 GPTQw3g128 70.70.9 44.01.5 77.91.0 73.40.4 70.51.3 8.09 AWQw3g128 11.45 71.60.9 42.21.4 76.71.0 65.80.5 67.31.3 BitStack 3.7e4 28.20.9 25.31.3 51.01.2 28.70.5 54.61.4 GPTQw4 75.00.9 51.51.5 79.50.9 77.80.4 72.11.3 7.08 AWQw4 74.60.9 46.21.5 77.51.0 72.30.4 70.81.3 8.58 BitStack GPTQw4g128 1.2e4 31.71.0 23.81.2 55.11.2 29.30.5 56.41.4 76.90.9 52.41.5 79.90.9 78.10.4 73.61.2 AWQw4g128 6.54 75.80.9 47.11.5 78.71.0 73.10.4 70.81.3 BitStack 8.26 85.90.7 64.31.4 84.50.8 84.90.4 80.71.1 2.85 FP 16 3.7e5 24.70.9 26.31.3 51.51.2 26.30.4 50.01.4 GPTQw2 8.6e5 25.10.9 25.91.3 52.31.2 26.60.4 47.81.4 AWQw2 59.37 46.51.0 27.31.3 65.21.1 39.10.5 51.91.4 BitStack GPTQw2g128 4.0e5 25.30.9 24.71.3 49.31.2 26.00.4 50.11.4 1.7e6 24.90.9 26.41.3 51.41.2 26.80.4 51.81.4 AWQw2g128 74.20.9 48.41.5 78.11.0 73.50.4 73.61.2 8.86 BitStack NaN 24.60.9 25.41.3 51.01.2 26.20.4 50.41.4 GPTQw3 14.04 65.51.0 41.21.4 73.11.0 64.30.5 57.41.4 AWQw3 79.80.8 54.81.5 80.80.9 79.60.4 77.01.2 6.88 BitStack GPTQw3g128 4.8e5 25.50.9 26.51.3 51.51.2 26.30.4 48.81.4 82.20.8 60.61.4 82.80.9 82.90.4 78.41.2 AWQw3g128 4.59 81.60.8 57.81.4 82.40.9 81.20.4 78.51.2 5.69 BitStack NaN 25.20.9 25.31.3 51.61.2 26.30.4 50.11.4 GPTQw4 77.50.9 54.41.5 81.50.9 80.00.4 60.51.4 4.16 AWQw4 82.30.8 61.11.4 83.40.9 82.50.4 79.91.1 4.88 BitStack GPTQw4g128 7.8e5 25.00.9 26.31.3 49.91.2 26.80.4 47.41.4 85.90.7 63.51.4 84.20.9 84.50.4 80.11.1 AWQw4g128 82.80.8 60.21.4 82.90.9 82.80.4 79.61.1 BitStack PIQA HellaS. WinoG. LAMBADA Avg. () 73.30.9 30.00.9 29.10.9 30.90.9 29.80.9 29.40.9 40.20.9 30.00.9 59.61.0 63.31.0 68.70.9 67.71.0 65.41.0 31.30.9 71.20.9 69.60.9 32.80.9 72.40.9 70.30.9 80.00.8 29.80.9 29.60.9 39.91.0 29.20.9 30.20.9 69.90.9 29.60.9 58.11.0 74.50.9 29.80.9 77.30.9 76.90.9 29.80.9 70.20.9 78.20.9 29.20.9 79.40.8 78.10.9 () 76.10.6 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 9.40.4 0.00.0 51.90.7 66.60.7 68.50.6 69.70.6 68.60.6 0.10.0 71.10.6 76.00.6 0.70.1 73.60.6 76.30.6 79.80.6 0.00.0 0.00.0 9.20.4 0.00.0 0.00.0 71.80.6 0.00.0 46.90.7 75.30.6 0.00.0 76.80.6 79.70.6 0.00.0 67.40.7 80.10.6 0.00.0 78.10.6 80.10.6 3.23 4.80 Here, we include the detailed evaluation results of Llama 2 and Llama 3 models in Table 2 and Table 3, respectively. A.3 QUALITATIVE RESULTS In Table 4 and 5, we compare BitStack and AWQ across different compression ratios on the Llama 3.1 Instruct 8B and 70B models. As shown in Table 4, which illustrates math reasoning task, at relatively lower compression ratio (5338MB), both compressed models produce correct answers with several reasoning steps. As the compression ratio increases (e.g., 4506MB), the AWQcompressed model starts with correct reasoning path but collapses midway through inference, whereas the BitStack-compressed model still manages to reach the correct answer. At extreme compression ratios, such as 76% (3674MB), neither model produces correct answer; the AWQ model outputs gibberish from the beginning, while the BitStack model is able to generate coherent text but omits the reasoning steps, ultimately arriving at an incorrect conclusion. For the 70B models in As shown in Table 5, where the models are asked to write rhyming proofs, the trend is similar to the 8B models. As compression ratios increase, AWQ models begin to output gibberish, while BitStack models continue to produce fluent text, albeit with lower answer quality. A.4 DISCUSSION ON MINIMAL TRANSMISSION UNITS IN BITSTACK In this section, we discuss the minimal transmission units, i.e., residual blocks, in BitStack. As detailed in Section 2.1.2, we decompose the approximation residuals in each iteration into their sign 16 Preprint Table 4: Qualitative results of BitStack Llama-3.1-8B-Instruct at different compression ratios comparing to AWQ. QUESTION: tech startup invests $8000 in software development in the first year, and then invests half of that amount in software development in the second year. Whats the total amount the startup invested in software development over the two years? AWQ-w4: ## Step 1: Identify the amount invested in the first year. The startup invests $8000 in software development in the first year. ## Step 2: Calculate the amount invested in the second year. The amount invested in the second year is half of the first years investment, which is $8000 / 2 = $4000. ## Step 3: Calculate the total amount invested over the two years. Add the investments of the first and second year to get the total amount invested: $8000 + $4000 = $12000. The final answer is: $12000. BitStack(5338MB): ## Step 1: Calculate the amount invested in the first year. The startup invests $8000 in software development in the first year. ## Step 2: Calculate the amount invested in the second year. The startup invests half of the first years investment in the second year, which is $8000 / 2 = $4000. ## Step 3: Calculate the total amount invested over the two years. The total amount invested is the sum of the first years investment and the second years investment, which is $8000 + $4000 = $12000. The final answer is: 12000 . AWQ-w3: ## Step 1: Calculate the total investment in the first year The total amount invested in the first year is 8000. ## Step 2: Calculate the total investment in the second year The total amount invested in the second year is half of the first years amount, which is 4000. ## Step 3: Calculate the total investment over both periods. To find the total amount invested in software development over the two years, we need to add the investment in the first year and the second year. The total amount = 8000 + (half of pound eight thousand five hundred six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six six ( concatenated.) BitStack(4506MB): To find the total amount invested in software development over the two years, we need to add the amount invested in the first year to the amount invested in the second year. The amount invested in the first year is $8000, and the amount invested in the second year is half of that amount, which is $4000. So, the total amount invested in software development over the two years is $8000 + $4000, which is $12000. AWQ-w2: blank blank blank blank blank blank blank0161ti0161ti0161ti0161ti0161ti0161ti0161ti0161ti0161ti0161ti0161ti0161ti0161ti0161ti016 1ti0161ti0161ti0161ti0161ti0161ti( concatenated.) BitStack(3674MB): The total amount of the startup is $8000. blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank blank matrix and singular vectors of the absolute value matrix (Eq. 6). Since the sign matrix requires 1 bit of memory per parameter after packing, and the singular values are stored in FP16, for weight matrix Rmn, the overall memory of each residual block in the stack can be calculated as follows: δW = (m n) + 16 (m + n) bits (9) where is the number of singular vectors kept in SVD. Since the shape of weight matrices in LLMs can vary (for example, in Llama 3 8B, Wq proj R40964096 and Wk proj R40961024), the size of each residual block for different weight stacks may also differ. We present the sizes of residual blocks for each weight stack in the Llama2, Llama3, and Llama3.1 models in Table 6. As shown in the table, BitStack enables dynamic loading at megabyte level, with minimum block size of 0.66MB in the Llama 3 8B model. We further verify BitStacks capability for fine-grained memory-performance trade-offs. In Figure 8, we zoom in and use small memory stride, i.e., 10MB, to evaluate the model. As shown in the figure, BitStack successfully achieves fine-grained trade-off, with perplexity consistently decreasing as more memory is allocated by loading additional pre-sorted residual blocks from storage. Preprint Table 5: Qualitative results of BitStack Llama-3.1-70B-Instruct at different compression ratios comparing to AWQ. USER: Suppose you are mathematician and poet. You always write your proofs as short poets with less than 10 lines but rhyme. Prove the square root of 2 is irrational number. 2 is not rational trade. 2 is rational AWQ-w4: delightful challenge! Heres my poetic proof: Assume, if you will, that too, fraction, ratio, with finite clue. Let a/b be its value, in simplest form, so fine, With and integers, and not divine. Now multiply by b, to clear the fractions sway, And square both sides, to chase the square root away. Youll get a2 = 2b2, simple equation true, But this implies and have common factor anew. This contradicts our assumption, logical flaw, Im afraid, so we conclude: BitStack(36676MB): Heres my poetic proof: In numbers, truth we must find, The square root of two, irrational to bind. Assume its rational, contradiction to make, Then and b, two integers at 2, perfect ratio to claim, But if a2 = 2b2, then even must be the name, And stake. a/b = 2 is irrational, truth to decree. No rational form, it will never even too, contradiction to see, So be. AWQ-w3: . PRO PRO PRO PRO PRO has PRO: PRO PRO was PRO PRO. PRO PROPRO is PROPRO.. PROSUB is PROPROSUB PRO PRO PRO,PRO PROSUBPROPROPROPROSUB PROPROPROSUB MODPROPROPRO ( concatenated.) BitStack(28516MB): delightful challenge! Heres my poetic proof: In numbers, tale is told, Of root, thats not whole. Twos square root, number rare, Cannot be expressed with simple care. For if it were, fraction fine, and B, with and in line, times b, square to make, Would be two, number to partake. But no such pair, with and to share, Proves the root of two, is irrational to spare. (Note: The traditional proof of the irrationality of the square root of 2 is based on the fact that if it were rational, it would have to be an integer or fraction, and then we can find and such that a2 = 2b2, which leads to contradiction. This poetic proof is rhyming version of this idea.) AWQ-w2: Pitaquetsalisalis.DataType Thick-transitionala Thickquets://alis Pita.xra adt ThickMigrationalis Thickutowessleranitizea anitizea sublicensealisakeningNx dnerequisite Pit dnMigration Pit Thick Pit Thick dnEGIN ( concatenated.) BitStack(20356MB): In the realm of math, number so fine, square root of two, number divine, number so fine, so divine, square root of two, so fine. number so fine, so divine, number so fine, so divine, square root of two, so fine, number so fine, so divine, number so fine, so divine. number so fine, so divine, number so fine, so fine, number so fine, so fine, number so fine, so divine. number so fine, so divine, number so fine, so fine, number so fine, so divine. (Note: The number square root of two is fine.) hope you can understand the proof. Table 6: Size of residual block in various weight matrices in BitStack (k = 16), measures in megabytes(MB). Model Llama 2 7B Llama 2 13B Llama 2 70B Llama 3(3.1) 8B Llama 3(3.1) 70B Wq proj Wk proj Wv proj Wo proj Wgate proj Wup proj Wdown proj 2.25 3.44 8.50 2.25 8.50 2.25 3.44 1.28 0.66 1.28 2.25 3.44 1.28 0.66 1.28 2.25 3.44 8.50 2.25 8.50 5.84 9.02 29.13 7.56 29.13 5.84 9.02 29.13 7.56 29. 5.84 9.02 29.13 7.56 29.13 18 Preprint Figure 8: Perplexity scores on the WikiText2 test set for the BitStack Llama 3.1 8B model. We plot the perplexity scores for memory usage ranging from 4000MB to 5000MB, with stride of 10MB, to assess BitStacks capability for fine-grained trade-offs. A.5 ANALYSIS OF INFERENCE OVERHEAD OF BITSTACK Figure 9: Generation time for 50 tokens with BitStack Llama 3.1 8B using different lengths of weight stacks(setting the same number of loaded residual blocks for all stacks). Results are evaluated on an NVIDIA H800 GPU. In our experiments, we did not include any inference optimization, as this is not the primary focus of this research. However, we provide an analysis here to support future deployment of BitStack models in real-world scenarios. Similar to other weight-only compression methods (e.g., weight-only quantization), the restoration of weights is performed on the fly, introducing an inference overhead. As illustrated in Figure 9, we roughly divide the total inference overhead into two parts: Overhead 1 and Overhead 2. Overhead 1 represents the residual block restoration time, including unpacking the sign matrix and the multiplication time as in Eq. 5. This can be significantly reduced by utilizing efficient unpacking kernels and fusing the residual block restoration operations. Overhead 2 refers to the additional time required to restore more residual blocks for weight stacks that load more than one block. As shown in the figure, Overhead 2 increases linearly as more residual blocks are loaded. This occurs because, in our implementation, the residual blocks are restored sequentially when computing Eq. 8. In practice, however, all residual blocks in the stack can be computed in parallel, as they are independent of one another, making Overhead 2 fully eliminable. 19 Preprint As discussed, the inference overhead of BitStack models can be significantly reduced with certain optimizations during real-world deployment. We leave these optimizations for future work. A.6 VISUALIZATIONS OF WEIGHT STACKS In Figure 10, we provide the visualization of the weight stacks in BitStack for three different sorting approaches, as detailed in Section 2.2. The Average approach, which we adopt in BitStack, exhibits minimal variance in the memory consumption of different stacks, benefiting load balancing in distributed deployment. Moreover, it demonstrates excellent performance in our experiments, particularly at extreme compression ratios. Figure 10: Visualization of the weight stacks in BitStack Llama 3.1 8B with three different sorting approaches. We plot the number of residual blocks in each weight stack in the BitStack model, ranging from 4000MB to 6000MB, with stride of 500MB, due to space constraints."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}