{
    "paper_title": "Goal Alignment in LLM-Based User Simulators for Conversational AI",
    "authors": [
        "Shuhaib Mehri",
        "Xiaocheng Yang",
        "Takyoung Kim",
        "Gokhan Tur",
        "Shikib Mehri",
        "Dilek Hakkani-Tür"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "User simulators are essential to conversational AI, enabling scalable agent development and evaluation through simulated interactions. While current Large Language Models (LLMs) have advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate goal-oriented behavior across multi-turn conversations--a critical limitation that compromises their reliability in downstream applications. We introduce User Goal State Tracking (UGST), a novel framework that tracks user goal progression throughout conversations. Leveraging UGST, we present a three-stage methodology for developing user simulators that can autonomously track goal progression and reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics for measuring goal alignment in user simulators, and demonstrate that our approach yields substantial improvements across two benchmarks (MultiWOZ 2.4 and {\\tau}-Bench). Our contributions address a critical gap in conversational AI and establish UGST as an essential framework for developing goal-aligned user simulators."
        },
        {
            "title": "Start",
            "content": "Goal Alignment in LLM-Based User Simulators for Conversational AI Shuhaib Mehri1 Xiaocheng Yang1 Takyoung Kim1 Gokhan Tur1 Shikib Mehri2 Dilek Hakkani-Tür1 1University of Illinois Urbana-Champaign, 2Contextual AI {mehri2, dilek}@illinois.edu 5 2 0 J 7 2 ] . [ 1 2 5 1 0 2 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "their User simulators are essential to conversational AI, enabling scalable agent development and evaluation through simulated interactions. While current Large Language Models (LLMs) have advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate goal-oriented behavior across multiturn conversationsa critical limitation that compromises reliability in downstream applications. We introduce User Goal State Tracking (UGST), novel framework that tracks user goal progression throughout conversations. Leveraging UGST, we present three-stage methodology for developing user simulators that can autonomously track goal progression and reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics for measuring goal alignment in user simulators, and demonstrate that our approach yields substantial improvements across two benchmarks (MultiWOZ 2.4 and τ -Bench). Our contributions address critical gap in conversational AI and establish UGST as an essential framework for developing goal-aligned user simulators."
        },
        {
            "title": "Introduction",
            "content": "To develop conversational agents that can handle real-world complexity and effectively address user needs, we must be able to simulate realistic user behaviors (Schatzmann et al., 2006; Pietquin and Hastie, 2013). In the era of experience, conversational agents learn most effectively by interacting with users in their environment (Silver and Sutton, 2025). Since large-scale human data collection is impractical, user simulators offer scalable alternative by modeling diverse user behaviorsfrom pursuing specific goals to exhibiting unique personas (Eckert et al., 1997a). Figure 1: The standard simulator (left) fails to adhere to their user goal and accepts refund in store credit. The goal-aligned user simulator response (right) considers their goal progression, and reasons to generate response that maintains alignment with the user goal. Large Language Models (LLMs) have enabled sophisticated user simulation that can generate contextually appropriate responses, however they also suffer from instruction drift and degraded performance over multi-turn conversations (Laban et al., 2025; Li et al., 2024). Current LLM-based user simulators exhibit what we refer to as the goal misalignment problemthey struggle to consistently adhere to their user goals throughout conversations (Zhang et al., 2020; Kim et al., 2025; Yao et al., 2024). Through principled analysis, we find that existing user simulators cannot reliably adhere to their assigned user profiles and behavioral constraints, or manage multiple objectives and complete them within the specified conversation limits, as detailed in Table 1. This misalignment leads to unexpected simulator behavior, which can produce inaccurate evaluations or misleading reward signals that compromises the effectiveness of reinforcement learning (RL) for conversational agents (Skalse et al., 2022; Amodei Figure 2: An illustration of the User Goal State Tracking framework. On the left side, we present user goal and conversation between user and conversational agent. The right side shows the latest user goal state, providing us with representation of the users goal progression. Below the goal state, we provide specific explanations that justify the status of each sub-component. et al., 2016; Carroll et al., 2019; Yao et al., 2024). These failures reveal fundamental limitation in user simulators that undermines their reliability for downstream tasks and highlights critical yet largely unexplored challenge in developing goalaligned user simulators. To address this challenge, we propose developing user simulators that track goal progression, and reason to ensure that each response progresses towards completing objectives while adhering to their user profiles and behavioral constraints, as depicted in Figure 1. We present User Goal State Tracking (UGST), framework that builds upon Dialog State Tracking principles (Henderson et al., 2014) and dynamically tracks users goal progression throughout conversation. UGST uses User Goal States to provide structured representations of goal progression. These states are created by decomposing the user goal into modular sub-components where each captures distinct aspect of the goal (e.g. finding restaurant to eat at, prefacing each request politely with please). Each sub-component is assigned corresponding status that is dynamically updated after every turn in conversation. Our framework is presented in Figure 2. We leverage UGST to systematically enhance in LLM-based user simulators goal alignment through three-stage methodology. First, we introduce inference-time steering, where we conduct UGST and provide simulators with their latest goal state before they generate each response, explicitly grounding them in their goal progression. Using inference-time steering, we generate conversations with explicit reasoning traces about goal progression and goal-aligned user responses. We conduct supervised fine-tuning (SFT) on the generated conversation data to foster intrinsic capabilities to autonomously track goal progression and generate goal-aligned responses without external guidance from inference-time steering. Further, inspired by recent findings on strong generalization capabilities of RL (Qian et al., 2025; Mukherjee et al., 2025; Gunjal et al., 2025), we apply Group Relative Policy Optimization (GRPO) (Shao et al., 2024) with composite reward derived from UGST to further refine reasoning and goal alignment capabilities. This approach addresses the fundamental limitations identified in current LLM-based user simulators, and helps develop more goal-aligned simulators that can autonomously track goal progression and reason to generate more goal-aligned responses. In our experiments, LLM-based user simulators are tasked to pursue diverse user goals by interacting with conversational agents across two benchmarks: MultiWOZ 2.4 (Ye et al., 2022), τ - Bench Airline, and τ -Bench Retail (Yao et al., 2024). We evaluate goal alignment by tracking the success rate of sub-components throughout each conversation using UGST. Our findings reveal that state-of-the-art LLM-based user simulators, including Llama-3.1-8B-Instruct, Qwen-2.572B-Instruct, Gemma-3-27B-Instruct, Llama-3.370B-Instruct, Qwen-2.5-72B-Instruct (Grattafiori et al., 2024; Qwen et al., 2025; Team et al., 2025), fail to successfully align with up to 40% of their assigned goals. Our three-stage methodology demonstrates substantial improvements, with inference-time steering yielding immediate gains of up to 5.4%, cold-start SFT achieving 11.0% absolute improvement, and GRPO with UGST rewards achieving the best performance with up to 14.1% absolute improvement in average success rate. Human evaluation validates both our UGST methodology and the quality of our improvements, with annotators confirming high agreement with automated assessments. Notably, our enhanced Llama-3.1-8B-Instruct and Qwen-2.5-7B-Instruct achieve performance competitive with or exceeding their much larger counterparts, Llama-3.370B-Instruct and Qwen-2.5-72B-Instruct. These improvements extend beyond goal alignment, with our approach also fostering more diverse user responses without compromising naturalness or coherence. The main contributions of our work are: that state-of-the-art LLM-based user simulators fail to consistently demonstrate goal-oriented behavior throughout multiturn conversations, undermining their reliability and highlighting the need for more goalaligned user simulators. We reveal We introduce User Goal State Tracking (UGST), novel framework that tracks users goal progression throughout conversation. We propose methodology for developing goal-aligned user simulators that leverages UGST. With our proposed methodology, we improve goal alignment in LLM-based user simulators and demonstrate that 8B parameter models can achieve competitive or superior performance to 70B+ parameter models. We establish comprehensive evaluation methods for goal alignment across two benchmarks (MultiWOZ 2.4 and τ -Bench), and validate our approach through both automated metrics and human evaluation. Overall, our work introduces UGST to track and improve goal alignment in user simulators, addressing critical limitation in existing user simulators and laying the groundwork for future advances in user simulation and conversational AI."
        },
        {
            "title": "2.1 User Simulation in Conversational AI",
            "content": "User simulation is well-established area of research in conversational AI, and has made significant advancements throughout the years. Early work relied on probabilistic approaches (Eckert et al., 1997b; Pietquin and Dutoit, 2006) that modeled user behavior through statistical patterns, and agenda-based methods (Schatzmann et al., 2007; Schatzmann and Young, 2009; Keizer et al., 2010) that used manually designed rules. As deep learning gained popularity, approaches started to leverage neural networks (Gür et al., 2018; Asri et al., 2016; Kreyssig et al., 2018) and transformers (Lin et al., 2021, 2022a; Cheng et al., 2022a; Sun et al., 2023), which enabled user simulators capable of generating natural and diverse responses. Most recently, LLMs have demonstrated highly sophisticated and effectiveness user simulator capabilities (Sekulic et al., 2024; Davidson et al., 2023)."
        },
        {
            "title": "2.2 Applications of User Simulators",
            "content": "Synthetic Data Generation. The costly and inefficient nature of manually collected user-agent conversation data (Acikgoz et al., 2025; Ye et al., 2022; Rastogi et al., 2020) motivates user simulators (Li et al., 2017). Recent works leverage LLMs to create synthetic training data for supervised fine-tuning (Prabhakar et al., 2025; Shim et al., 2025; Philipov et al., 2024; Niu et al., 2024; Chen et al., 2021), where simulators enable the efficient generation of vast amounts of high-quality conversation data with diverse user behaviors, domains, and scenarios. Reinforcement Learning for Conversational Agents. User simulators enable conversational agents to learn through trial-and-error interactions with RL (Algherairy and Ahmed, 2025; Scheffler and Young, 2002; Gür et al., 2018; Shah et al., 2018; Lin et al., 2022b; Liu et al., 2023). In these settings, agents interact with user simulators and learn through rewards based on the interactions. Conversational Agent Evaluation. Traditional methods for evaluating conversational agents require comparing generated responses with static responses. This leads to the one-to-many problem where many valid responses exist and some may be penalized when they do not match an expected response (Zhao et al., 2017; Mehri and Eskenazi, 2020; Cheng et al., 2022b; Davidson et al., 2023). Consequently, user simulators are used to interact with the agent and allow researchers to observe how the agent performs in various scenarios. The resulting conversations can be analyzed and evaluated based on metrics such as task completion or user satisfaction (Davidson et al., 2023; Cheng et al., 2022b; Sun et al., 2023; Yao et al., 2024; Lu et al., 2025; Kazi et al., 2024; Xiao et al., 2024; Pan et al., 2025). 2."
        },
        {
            "title": "Improving User Simulation",
            "content": "Existing work focuses on improving various aspects of LLM-based user simulators. For instance, Sun et al. (2023) makes user simulators more realistic by incorporating prior knowledge and experiences into response generation. Luo et al. (2024) improve user simulator quality by using verifier LLM to provide feedback for unsuitable responses. Meanwhile, Liu et al. (2023); Ahmad et al. (2025) improve the diversity of user simulator behavior by incorporating multiple personas. Despite these advances, existing works assume that LLMs possess the fundamental capabilities to simulate users. This assumption proves problematic, as recent evaluations reveal that even stateof-the-art LLM-based user simulators struggle to follow their user goal (Kim et al., 2025; Zhang et al., 2020; Yao et al., 2024). Our work addresses this critical gap by improving LLM abilities to simulate users and consistently demonstrate goaloriented behaviors."
        },
        {
            "title": "3 Problem Description",
            "content": "User simulators are designed to simulate realistic user behaviors and interact with conversational agents. They operate based on user goal which encompasses tasks to be completed along with behavioral guidelines, constraints, and contextual information. Formally, we define conversation as Cn = {u1, a1, ..., un, an}, where ui and ai denote the user simulator and agent utterance at turn i, respectively. The user simulator is provided with an initial system message s0 that provides general instructions for the user simulator and defines the user goal and generates each response ui based on the conversation history Ci1, aiming to progress towards achieving their user goal G. Conversations conclude upon reaching maximum length or when the user simulator explicitly ends the conversation by sending termination indicator (e.g., TERMINATE). Despite recent advances, current LLM-based user simulators suffer from the goal misalignment problem and demonstrate limitations in maintaining goal-aligned behavior throughout multi-turn conversations. To analytically understand these limitations, we conduct principled analysis of 52 randomly selected conversations, where LLMbased user simulators (Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct) interact with conversational agents to pursue user goal from the MultiWOZ 2.4, τ -Bench Airline, and τ -Bench Retail datasets. We identify systematic failures in goal alignment, where the user simulator fails to follow the specified user goal G. Table 1 categorizes these failures into five distinct patterns. These goal alignment failures can lead to unexpected behaviors from user simulators, which can adversely affect the reliability of evaluations, diminish the quality of generated synthetic data, and compromise the effectiveness of reinforcement learning. We highlight the critical need to address these shortcomings and improve the fundamental capabilities of LLMs to be goal-aligned user simulators. In the following sections, we introduce User Goal State Trackinga framework designed to systematically track users progress towards their goal and demonstrate how to leverage it to improve goal alignment in user simulators."
        },
        {
            "title": "4 User Goal State Tracking",
            "content": "In this section, we present User Goal State Tracking (UGST), framework that dynamically tracks users progress towards their goal throughout conversation. UGST maintains structured User Goal State, which contains status for each modular sub-component of user goal.The following subsections describe the user goal state structure and tracking process."
        },
        {
            "title": "4.1 User Goal State",
            "content": "The User Goal State is structured representation that captures users progress towards their goal at every turn in conversation. Given an initial user goal described in natural language, UGST decomposes it into distinct, modular subcomponents, where each represents an independent, self-contained part of the original user goal. There are several types of sub-components that can make up user goal. Task objectives and requirements represent the earliest established subcomponents (Schatzmann et al., 2007; Gür et al., 2018; Cheng et al., 2022b; Rastogi et al., 2020; Yao et al., 2024; Xiao et al., 2024). These are items that must be completed during an interaction, for instance, task objective might be \"book flight\", with associated requirements such as Goal Alignment Failure Frequency Confusion: The simulator forgets or confuses parts of the goal. For example, in shopping application, when instructed to return item and exchange item B, the simulator incorrectly requests to return both items. Contradiction: The simulator directly contradicts specified constraints or contextual information. For example, in flight booking scenario, the user simulator is instructed that they do not have their credit card details. However, they hallucinate fake credit card information when it is required to proceed with the booking. Wrongful termination: The simulator exhibits inadequate termination strategieseither terminating prematurely before receiving agent responses, or continuing indefinitely until reaching the maximum conversation length. Poor Length Management: The simulator fails to complete all parts of their goal within the conversation length limits, demonstrating poor management and awareness of conversation length. For example when tasked with booking multiple flights, the simulator exhausts the conversation length before completing all bookings. Misprioritization: The simulator overly prioritizes one part of their goal. They either get stuck on trying to achieve one part of their goal that is unachievable, rather than moving onto the remaining parts. Or, the user simulator terminates after completing one part of their goal, rather than completing the remaining parts of the goal. 33% 23% 21% 12% 11% Table 1: Categorization and frequency of goal alignment failures identified through manual analysis of 50 randomly selected conversations between LLM-based user simulators (Llama-3.1-8B-It and Qwen2.5-7B-It) and conversational agents. \"add one checked bag\" and \"get an aisle seat\". As user simulators have evolved to handle more complex scenarios, researchers have introduced additional dimensions to user goals. These include preferences that users must align with when pursuing task objectives (Yao et al., 2024; Cheng et al., 2022b), such as \"you prefer the cheapest available flight\" or \"you prefer an aisle seat, but you are also okay with window seat\". There are also user profile sub-components, which contain contextual information about the user that may influence their decision making and behavior. These profiles can include relevant facts about user, their persona, or emotional state (e.g. \"your payment information is stored online\", \"you currently live in New York\", \"you are hurried and always late\", \"you are emotional and bit angry\") (Yao et al., 2024; Xiao et al., 2024). Lastly, there are user policies that define behavioral constraints or guidelines that specify how user acts during interactions, such as \"you are private person and reveal little about yourself\" (Yao et al., 2024). We categorize user goal sub-components into these categories (user profile, user policy, task objective, requirement, or preference) for more granular representation of users goal progression (see Figure 2). In our representation of the user goal state, the different categories of sub-components have different success criteria. The user profile, user policy and preference sub-components can be either: ALIGNED: The user has demonstrated behavior consistent with the sub-component. MISALIGNED: The user has demonstrated behavior that contradicts or fails to align with this sub-component. objective Task requirements suband components can have status of: COMPLETE: The user has successfully accomplished the specific task or requirement. INCOMPLETE: The user has not yet accomplished the specific task or requirement. ATTEMPTED: The user has made sufficient attempt to complete the task or requirement, but cannot proceed due to external factors outside their control (e.g. agent-side failures, system constraints, or limitations). Unlike existing frameworks, this status ensures that users are not penalized for failures they did not cause, providing more fair representation of user performance."
        },
        {
            "title": "4.2 User Goal State Tracking Process",
            "content": "We track user goal progression throughout each conversation = {u1, a1...un, an}, using the user goal states. The process begins by decomposing the user goal into sub-components to create an initial user goal state S0. We employ an LLM to perform this, using the prompt provided in Appendix C. By default, each user profile and user policy sub-components are assigned status of ALIGNED, preferences are assigned status of MISALIGNED, and task objective and requirements sub-components are assigned status of INCOMPLETE. After each conversational turn, ti = (ui, ai), each sub-components status is individually updated via use an LLM, producing new user goal state Si that captures the goal progression up until turn (see Appendix for the prompt). The final user goal state Sn encapsulates the users overall goal alignment across the entire conversation."
        },
        {
            "title": "5 Methodology",
            "content": "The UGST framework serves as the foundation for enhancing goal alignment capabilities of LLMbased user simulators. In this section we present our method for leveraging UGST to systematically improve goal alignment in three stages."
        },
        {
            "title": "5.1 Stage 1: Inference-time Steering",
            "content": "Conventionally, user simulators are provided with user goal in the system prompt, and generate responses based solely on conversation history. At each turn i: ui = (Ci1), (1) where represents the user simulator, ui denotes the user simulators response at turn i, and Ci1 is the conversation history up until turn i. This formulation lacks explicit goal progression tracking, relying on the user simulators inherent ability to infer progress from the conversation history. We address this by conditioning the user simulator on the latest user goal state before generating each response. The goal state is provided as part of the instruction for each response, which helps ground the simulator on the progress made towards their goal and the remaining sub-components that they must complete and stay aligned with. We refer to this as inferencetime steering. In this process, UGST is conducted after every turn in the conversation. For each turn i, the user simulator is provided with both the conversation history and the latest user goal state to generate their next response: ui = (Ci1, Si1), (2) where represents the user simulator, ui denotes the user simulators response at turn i, Ci1 is the conversation history up until turn i, and Si1 represents the user goal state at turn 1. Providing the goal state before generating each response helps steer the user simulator towards generating responses that are better aligned with their user goals."
        },
        {
            "title": "5.2 Stage 2: Cold-Start Supervised",
            "content": "Fine-Tuning We generate goal-aligned conversation training data using inference-time steering with Llama3.3-70B-Instruct. We instruct the LLM to reflect on the user goal state at each turn, and provide reasoning trace that describes the users goal progression through explicit reasoning steps, ultimately distilling the goal progression tracking from inference-time steering. Specifically, the user simulator (1) reflects on the latest goal state and explicitly considers the progress made towards the goal, (2) analyzes the remaining task objectives, requirements, and preferences, and how to complete them within the conversation limit, and (3) considers how to generate response that stays aligned with the user profile and policy. To enable autonomous goal alignment without requiring external guidance from inference-time steering, we train LLM-based user simulators on user responses from the generated conversation data. We use the standard SFT objective: L(θ) = (cid:80) (Ci1,ui)D logPθ(uiCi1) where ui is the reasoning trace and response, and Ci1 is the conversation history. This cold-start SFT training process develops the intrinsic abilities of LLMs to (1) autonomously track goal progression throughout conversation, and (2) generate goal-aligned responses, eliminating dependency on computationally expensive inference-time steering. The user simulator now operates using the conventional formulation from Equation 1, receiving only conversation history Ci1 as input eliminating the need to do goal state tracking. After training, the simulators have learned to implicitly track and maintain goal alignment through reasoning patterns distilled from the inference-time steering process."
        },
        {
            "title": "5.3 Stage 3: GRPO with UGST Rewards",
            "content": "UGST provides structured reward signals that capture fine-grained goal-alignment across multiple dimensions. This characteristic makes it wellsuited for RL approaches, and enables us to design composite rewards that independently evaluate alignment across our different sub-component categories (Gunjal et al., 2025). We employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which has demonstrated effectiveness in developing emergent capabilities in LLMs, such as reasoning (Guo et al., 2025; Gunjal et al., 2025). We extend this approach to user simulation by leveraging UGSTs structured signals to further refine the goal-tracking and response generation capabilities developed in stage 2. Specifically, after each user response ui at turn i, we apply UGST to evaluate alignment across five conditions: (1) alignment with user profile, (2) alignment with user policy, (3) task objective attempt/completion, (4) requirement attempt/completion, (5) alignment with preferences. For each condition and response ui, we define an indicator function: Ij(ui) = 1 if response ui satisfies condition j, and 0 otherwise. Our composite reward aggregates these alignment signals: R(ui) = 5 (cid:88) j=1 αjIj(ui)} (3) where αj represents the weight for condition j. We assign equal weights of αj = 0.5. Using this reward function, we optimize the user simulator with GRPO to maximize the expected cumulative reward. This approach allows the simulator to learn policy that maintains alignment with the users profile, policies, and preferences while actively pursuing task objective and requirement completion."
        },
        {
            "title": "6.1 Experimental Setup",
            "content": "We evaluate user simulator goal alignment by examining how faithfully simulators adhere to their assigned user goals during interactions with conversational agents. Our evaluation methodology consists of the following steps: 1. Conversation Generation: We provide user simulators with goal and have them interact with GPT-4o mini (OpenAI, 2024) as conversational agent for maximum of 10 useragent turns. The conversational agent is provided domain-specific function calls and system prompt that describes its policy, and we use agents for three domains: MultiWOZ (Ye et al., 2022), τ -Bench Airline, and τ -Bench Retail (Yao et al., 2024). System prompts provided in Appendix A. components, and categorizing each into the sub-component categories, as detailed in Appendix C, 3. User Goal State Tracking: Then, we conduct UGST on the generated conversations with an LLM judge (Qwen-2.5-72B-Instruct (Qwen et al., 2025)). After each turn, the LLM judge individually updates the status of each subcomponent in the user goal state. It is provided with the sub-component description, the conversation history, and an evaluation criteria that provides instructions for how to update the status. An example is provided in Appendix D. 4. Evaluation Metrics: We measure user simulator performance by calculating the success rate for each sub-component category in the final user goal state, since it represents the users overall goal alignment across the entire conversation. For user profile, user policy, and preferences, we consider ALIGNED status as successful; for task objective and requirements, we consider both ATTEMPTED and COMPLETE status as successful. Evaluation Datasets. Our evaluation utilizes datasets made up of diverse and complex user goals. The τ -Bench Airline and τ -Bench Retail datasets provide comprehensive user goals that encompass diverse user scenarios, and naturally encompass all five sub-component categories (user profile, user policy, task objective, task requirement, preference), making them well-suited for our evaluation. On the other hand, the MultiWOZ datasets user goals predominantly focus in task objective and requirements, and preliminary experiments showed that Llama-3.1-8B-Instruct achieved over 95% success rate on these goals. To develop more demanding and comprehensive evaluation dataset for user simulators, we developed MultiWOZ Challenge. This dataset contains 150 carefully constructed user goals that incorporate all sub-component categories and feature more complex task objective and requirements. The generation process, that involved both LLMs and human annotations, is detailed in Appendix B."
        },
        {
            "title": "6.2 Human Evaluation",
            "content": "2. Initial User Goal State Generation: For each user goal, the corresponding user goal state is generated using GPT-4o by decomposing user goal into distinct modular subUser Goal State Tracking. We validate our evaluation method by conducting comprehensive human evaluation study with 10 graduate-level human annotators. For total of 30 randomly seModel Prof. Pol. T.O. Req. Pref. Avg Model Prof. Pol. T.O. Req. Pref. Avg Prompt-Based Prompt-Based Qwen-2.5-7B-It Llama-3.1-8B-It Gemma-3-27B-It Qwen-2.5-72B-It Llama-3.3-70B-It 88.3 90.9 98.7 89.3 97. 49.2 41.0 59.0 59.6 65.6 94.3 97.1 97.1 94.1 98.1 96.0 99.0 97.0 96.9 99.0 Inference-Time Steering Qwen-2.5-7B-It Llama-3.1-8B-It Gemma-3-27B-It Qwen-2.5-72B-It Llama-3.3-70B-It 77.9 92.2 94.8 93.3 93. 55.7 57.4 62.3 54.4 63.9 96.2 93.3 92.4 98.0 98.1 98.0 98.0 97.0 97.9 100.0 85.7 81.0 78.6 78.6 92.9 92.9 95.2 85.7 97.6 97.6 82.7 81.8 86.1 83.7 90. 84.1 87.2 86.4 88.3 90.6 Qwen-2.5-7B-It Llama-3.1-8B-It Gemma-3-27B-It Qwen-2.5-72B-It Llama-3.3-70B-It 82.0 84.8 91.0 88.6 91.7 40.8 36.8 39.5 40.8 46.1 96.0 97.1 97.8 98.2 99.6 99.5 98.9 99.5 97.9 100. 91.7 96.3 96.3 91.7 100.0 Inference-Time Steering Qwen-2.5-7B-It Llama-3.1-8B-It Gemma-3-27B-It Qwen-2.5-72B-It Llama-3.3-70B-It 73.0 84.8 94.5 85.8 92.7 47.4 52.6 56.6 48.7 59.2 94.9 95.3 97.1 97.1 96. 97.9 98.9 99.5 98.4 98.4 93.5 97.2 89.8 98.1 100.0 82.0 82.8 84.8 83.4 87.5 81.4 85.8 87.5 85.6 89.3 Cold-Start SFT Cold-Start SFT Qwen-2.5-7B-It Llama-3.1-8B-It 98.7 89.6 55.7 55.7 99.0 97.1 100.0 97.0 95.2 97. 89.7 87.4 Qwen-2.5-7B-It Llama-3.1-8B-It 85.8 84.8 43.4 47.3 92.4 95.5 94.7 97. 82.4 94.1 79.7 83.9 GRPO with UGST Rewards GRPO with UGST Rewards Qwen-2.5-7B-It Llama-3.1-8B-It 93.5 96. 63.9 100.0 62.3 100.0 100.0 100.0 100.0 97.6 91.5 91.2 Qwen-2.5-7B-It Llama-3.1-8B-It 85.5 84. 38.2 47.4 97.1 98.9 97.9 99.5 97.2 98.1 83.2 85.7 (a) τ -Bench Airline (b) τ -Bench Retail Table 2: User simulator goal alignment performance based on final user goal states from UGST. The table shows the average success rates for User Profile (Prof.), User Policy (Pol.), Task Objective (T.O.), Requirements (Req.) and Preferences (Pref.) sub-component categories, and the overall average performance on the τ -Bench Airline and Retail datasets. lected conversations, human annotators manually conduct UGST, resulting in total of 300 human annotated goal states. We measure the agreement between the human annotated goal states and LLM generated goal states by reporting the proportion of matching sub-component statuses. Table 4 presents both results per sub-component category and overall agreement, demonstrating that LLMs can reliably conduct UGST. User Goal State Generation. Due to its central role in our evaluation methodology, we also evaluate how well GPT-4o generates user goal states. We manually create user goal states for total of 30 randomly selected user goal states, and compare against the goal-states generated by GPT-4o. We report the precision (P), recall (R), and F1 scores for the sub-components, where true positives are when the generated goal state correctly contains sub-component, false positives are when the goal state contains sub-component it should not contain, and false negatives are when the generated goal state is missing subcomponent. Additionally, we report the accuracy (Acc.) for categorizing the sub-components. As shown in Table 5, GPT-4o achieves high performance across all metrics, indicating reliable goal state generation that ensures accurate and representative downstream evaluations."
        },
        {
            "title": "6.3 Experimental Results",
            "content": "Prompt-Based. As baseline, we evaluate several prompt-based LLMs for user simulators. We cover range of model sizes and architectures, including Qwen-2.5-7B-Instruct, Llama-3.1-8BInstruct, Gemma-3-27B-Instruct, Qwen-2.5-72BInstruct, and Llama-3.3-70B-Instruct (Grattafiori et al., 2024; Team et al., 2025; Qwen et al., 2025). Across the evaluation datasets, the results show general trend of improving average performance with increased model size, with Llama-3.370B-Instruct achieving the highest performance. However, closer analysis reveals mixed results across different sub-components, where larger models do not consistently demonstrate better performance. For example, while Llama-3.3-70BInstruct achieves the strongest average performance, Gemma-3-27B-Instruct achieves higher score for the user profile category. This variation in performance underscores the importance of evaluating user simulators across individual subcomponents rather than relying solely on aggregate metrics, as model capabilities vary across different types of user goal sub-components. Model Prof. Pol. T.O. Req. Pref. Avg Prompt-Based Qwen-2.5-7B-It Llama-3.1-8B-It Gemma-3-27B-It Qwen-2.5-72B-It Llama-3.3-70B-It 54.7 73.6 80.1 72.0 72. 18.0 35.0 50.3 31.0 31.0 78.4 86.1 95.0 91.0 97.9 81.9 89.0 97.7 93.3 98.6 Inference-Time Steering Qwen-2.5-7B-It Llama-3.1-8B-It Gemma-3-27B-It Qwen-2.5-72B-It Llama-3.3-70B-It 43.4 71.7 72.3 71.0 72. 26.3 38.0 45.7 46.3 64.3 88.1 85.2 87.0 94.9 97.0 89.8 88.6 89.7 96.9 97.8 73.5 80.9 92.1 90.2 93.2 71.1 74.3 74.8 92.9 91.7 61.3 72.9 83.0 75.5 83. 63.7 71.6 73.9 80.4 84.7 Cold-Start SFT Qwen-2.5-7B-It Llama-3.1-8B-It 60.3 57.1 38.9 40.1 88.0 92. 91.1 94.9 83.1 84.3 72.3 73.7 GRPO with UGST Rewards Qwen-2.5-7B-It Llama-3.1-8B-It 51.8 59. 34.0 44.0 98.3 99.7 98.8 99.8 93.9 97.8 75.4 80.0 Table 3: User simulator goal alignment performance based on final user goal states from UGST. The table shows the average success rates for User Profile (Prof.), User Policy (Pol.), Task Objective (T.O.), Requirements (Req.) and Preferences (Pref.), and the overall average performance on the Multiwoz Challenge dataset. Prof. Pol. T.O. Req. Pref. Avg 91.7 72.7 91.1 81.3 88.7 85. Table 4: Human Agreement scores with LLMbased UGST on random user-agent conversations. We report the overall scores, as well as scores for each sub-component category. Inference-time Steering. Next, we evaluate the same set of models with inference-time steering, where the user goal state Si1 is provided to the user simulator at every turn before they generate their response ui. Inference-time steering improves goal alignment in user simulators, increasing the average success rate up to 5.4%. However, while there is an increase in the average success rate, different models react differently to inference-time steering. For example, we observe F1 Acc. 98.04 95.60 96.63 91.97 Table 5: Precision (P), Recall (R), F1 score, and classification accuracy (Acc.) of GPT-4o for user goal state generation. drop in the user profile category for Qwen-2.57B-Instruct models, or drop in the preferences category for Gemma-27B-Instruct models. Cold-Start SFT. Our highest performing model stage is Llama-3.3-70Bfrom the previous Instruct, so we use it to generate cold-start SFT data with the process described in Section 5.2. We gather 500 user goals from the τ -Bench Retail training dataset and also generate 500 MultiWOZ user goals with our user goal generation pipeline (Appendix B). The resulting dataset consists of 1000 conversations. Using this dataset, we train Qwen-2.5-7B-Instruct and Llama-3.1-8B-Instruct with SFT. The hyperparameters we use include batch size of 32, learning rate of 1 106, and 4 epochs. This leads to significant increase in performance over the prompt-based and inferencetime steering methods. GRPO with UGST Rewards. Finally, we apply GRPO with UGST Rewards (Section 5.3) to train the models from the previous section. We use learning rate of 5 106, batch size of 16, 8 rollouts, and 350 training steps. Our dataset is made up of approximately 5000 training samples, constructed by taking subsets of each conversation in our cold-start SFT data that fit within 2048 tokens. The resulting models achieve even further improvements, and demonstrate either the highest or competitive average success rates among all models, including their larger counterparts, Gemma2.5-72B-Instruct and Llama-3.3-70B-Instruct. Our experimental results demonstrate the effectiveness of the proposed methodology for developing goal-aligned user simulators. Qwen-2.57B and Llama-3.1-8B-Instruct gain significant improvements that enable them to rival much more powerful models like Qwen-2.5-72B-Instruct and Llama-3.3-70B-Instruct, and demonstrate up to 14% increase in average success rates. Through systematic evaluation across diverse domains, we establish that our approach consistently and significantly improves user simulator goal alignment."
        },
        {
            "title": "7.1 Further Analysis of User Simulators",
            "content": "To further validate our findings and examine the broader impact of our methodology, we conduct further analysis of our user simulators and present results averaged across τ -Bench Airline, τ -Bench Retail, and Multiwoz Challenge in Table 6. Model F1 MTLD HDD Prompt-Based Qwen-2.5-7B-It Llama-3.1-8B-It Gemma-3-27B-It Qwen-2.5-72B-It Llama-3.3-70B-It 0.827 0.819 0.823 0.824 0.826 4.15 4.05 4.16 4.26 4.25 4.31 4.211 4.30 4.50 4.51 Inference-Time Steering Qwen-2.5-7B-It Llama-3.1-8B-It Gemma-3-27B-It Qwen-2.5-72B-It Llama-3.3-70B-It 0.829 0.820 0.818 0.828 0.831 3.94 4.04 4.16 4.23 4.20 4.29 4.19 4.28 4.58 4.63 Cold-Start SFT 71.9 67.0 85.4 70.1 75.2 83.1 66.4 81.9 77.7 70. 0.403 0.513 0.524 0.382 0.720 0.521 0.549 0.554 0.521 0.713 Qwen-2.5-7B-It Llama-3.1-8B-It 0.823 0.827 3.87 4.03 4.27 4. 78.8 75.3 0.748 0.745 GRPO with UGST Rewards Qwen-2.5-7B-It Llama-3.1-8B-It 0.827 0.836 4.05 3. 4.37 4.20 80.7 74.0 0.806 0.795 Table 6: Additional analysis of user simulators. We report BERTScore (F1) for semantic similarity between user utterances and user goals (higher scores indicate better goal alignment), naturalness (N) and coherence (C) scores, and diversity metrics including MTLD and HDD. Results are averaged across τ -Bench and MultiWOZ datasets. Goal Alignment. Following Zheng et al. (2024), we additionally evaluate goal alignment by using BERTScore (Zhang et al., 2019) to measure the semantic similarity between user utterances and their user goal, where higher F1 scores indicate better goal alignment. We observe that the semantic similarity scores generally show an increasing trend for each stage of our method, with the final stage achieving the highest scores. Naturalness and Coherence. key concern when improving goal alignment is ensuring that user simulators maintain their conversational abilities to generate realistic responses. To address this, we assess the naturalness and coherence of user-agent conversations by employing Qwen-2.572B-Instruct to rate conversations on scale from 1 to 5, as in Kazi et al. (2024). The evaluation prompts are provided in Appendix E. Throughout the different stages, naturalness and coherence scores show only minor variations and remain within similar range. This indicates that our approach achieves substantial goal alignment improvements without degrading the fundamental conversational qualities essential for realistic user simulation. Diversity. Lastly, we assess diversity using Measure of Textual Lexical Diversity (MTLD) (McCarthy and Jarvis, 2010) and Hypergeometric Distribution Function (HDD) (Wu, 1993) following established practices in user simulation evaluation (Luo et al., 2024; Pietquin and Hastie, 2013). MTLD quantifies lexical diversity at the token level, and HDD captures the diversity of vocabulary when randomly selecting fixed number of words from the user utterances. There is consistent increase in diversity of user simulator utterances, and user simulators from our final stage demonstrate significant improvements in diversity. These results provide additional validation that our methodology enhances goal alignment of user simulators, while also demonstrating that it promotes more diverse conversational behaviors, highly desirable trait for user simulation."
        },
        {
            "title": "8 Conclusion",
            "content": "This work introduces User Goal State Tracking to address the goal misalignment problem in LLMbased user simulators. This framework dynamically monitors user simulators goal progression through multi-turn conversations. We leverage this framework and present three-stage method that develops user simulators that autonomously track their goal progression and reason to generate goal-aligned responses. Our experiments across MultiWOZ, τ -Bench Airline, and τ -Bench Retail domains show significant improvements in goal alignment. By enabling more reliable user simulation, our work addresses fundamental challenge in conversational AI and establishes the foundation for developing agents that learn from user interactions through reinforcement learning."
        },
        {
            "title": "9 Limitations",
            "content": "While our work demonstrates improvements in user simulator goal alignment, several limitations remain. First, we use Qwen-2.5-72B-Instruct for reliable UGST, which is computationally expensive and limits the scalability of our framework. Secondly, when using UGST rewards for GRPO, we use equal weights across all conditions and do not incorporate other aspects such as response naturalness or coherence. Future work should address these limitations by developing smaller, specialized models for UGST that are more efficient, and investigating optimal reward functions that capture essential qualities for user simulation."
        },
        {
            "title": "References",
            "content": "Emre Can Acikgoz, Cheng Qian, Hongru Wang, Vardhan Dongre, Xiusi Chen, Heng Ji, Dilek Hakkani-Tür, and Gokhan Tur. 2025. desideratum for conversational agents: Capabilities, challenges, and future directions. Adnan Ahmad, Stefan Hillmann, and Sebastian Möller. 2025. Simulating user diversity in taskoriented dialogue systems using large language models. Atheer Algherairy and Moataz Ahmed. 2025. Prompting large language models for user simulation in task-oriented dialogue systems. Computer Speech & Language, 89:101697. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565. Layla El Asri, Jing He, and Kaheer Suleman. 2016. sequence-to-sequence model for user simulation in spoken dialogue systems. arXiv preprint arXiv:1607.00070. Micah Carroll, Rohin Shah, Mark Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. 2019. On the utility of learning about humans for human-ai coordination. Advances in neural information processing systems, 32. Moya Chen, Paul A. Crook, and Stephen Roller. 2021. Teaching models new apis: Domainagnostic simulators for task oriented dialogue. Qinyuan Cheng, Linyang Li, Guofeng Quan, Feng Gao, Xiaofeng Mou, and Xipeng Qiu. 2022a. Is multiwoz solved task? an interactive tod evaluation framework with user simulator. arXiv preprint arXiv:2210.14529. Qinyuan Cheng, Linyang Li, Guofeng Quan, Feng Gao, Xiaofeng Mou, and Xipeng Qiu. 2022b. Is MultiWOZ solved task? an interactive TOD evaluation framework with user simulator. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 12481259, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Sam Davidson, Salvatore Romeo, Raphael Shu, James Gung, Arshit Gupta, Saab Mansour, and Yi Zhang. 2023. User simulation with large language models for evaluating task-oriented dialogue. arXiv preprint arXiv:2309.13233. W. Eckert, E. Levin, and R. Pieraccini. 1997a. User modeling for spoken dialogue system evaluation. In 1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings, pages 8087. Wieland Eckert, Esther Levin, and Roberto Pieraccini. 1997b. User modeling for spoken dialogue system evaluation. In 1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings, pages 8087. IEEE. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. 2025. Rubrics as rewards: Reinforcement learning beyond verifiable domains. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Izzeddin Gür, Dilek Hakkani-Tür, Gokhan Tür, and Pararth Shah. 2018. User modeling for task oriented dialogues. In 2018 IEEE Spoken Language Technology Workshop (SLT), pages 900 906. Matthew Henderson, Blaise Thomson, and Jason D. Williams. 2014. The second dialog state tracking challenge. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 263272, Philadelphia, PA, U.S.A. Association for Computational Linguistics. Taaha Kazi, Ruiliang Lyu, Sizhe Zhou, Dilek Hakkani-Tur, and Gokhan Tur. 2024. Large language models as user-agents for evaluating task-oriented-dialogue systems. Simon Keizer, Milica Gasic, Filip Jurcicek, François Mairesse, Blaise Thomson, Kai Yu, and Steve Young. 2010. Parameter estimation for agenda-based user simulation. In Proceedings of the SIGDIAL 2010 Conference, pages 116123. Takyoung Kim, Janvijay Singh, Shuhaib Mehri, Emre Can Acikgoz, Sagnik Mukherjee, Nimet Beyza Bozdag, Sumuk Shashidhar, Gokhan Tur, and Dilek Hakkani-Tür. 2025. Pipa: for diagnosing interactive planning agents."
        },
        {
            "title": "A unified evaluation protocol",
            "content": "Florian Kreyssig, Iñigo Casanueva, Pawel Budzianowski, and Milica Gasic. 2018. Neural user simulation for corpus-based policy optimisation for spoken dialogue systems. arXiv preprint arXiv:1805.06966. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Llms get lost arXiv preprint and Jennifer Neville. 2025. in multi-turn conversation. arXiv:2505.06120. Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2024. Measuring and controlling instruction (in) stability arXiv preprint in language model dialogs. arXiv:2402.10962. Xiujun Li, Zachary C. Lipton, Bhuwan Dhingra, Lihong Li, Jianfeng Gao, and Yun-Nung Chen. 2017. user simulator for task-completion dialogues. Hsien-chin Lin, Christian Geishauser, Shutong Feng, Nurul Lubis, Carel van Niekerk, Michael Heck, and Milica Gašic. 2022a. Gentus: Simulating user behaviour and language in taskoriented dialogues with generative transformers. arXiv preprint arXiv:2208.10817. Hsien-chin Lin, Christian Geishauser, Shutong Feng, Nurul Lubis, Carel van Niekerk, Michael Heck, and Milica Gasic. 2022b. GenTUS: Simulating user behaviour and language in taskoriented dialogues with generative transformers. In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 270282, Edinburgh, UK. Association for Computational Linguistics. Hsien-chin Lin, Nurul Lubis, Songbo Hu, Carel van Niekerk, Christian Geishauser, Michael and Milica Gašic. Heck, Shutong Feng, 2021. Domain-independent user simulation with transformers for task-oriented dialogue systems. arXiv preprint arXiv:2106.08838. Yajiao Liu, Xin Jiang, Yichun Yin, Yasheng Wang, Fei Mi, Qun Liu, Xiang Wan, and Benyou Wang. 2023. One cannot stand for everyone! leveraging multiple user simulators to train task-oriented dialogue systems. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 121, Toronto, Canada. Association for Computational Linguistics. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming Pang. 2025. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. Xiang Luo, Zhiwen Tang, Jin Wang, and Xuejie Zhang. 2024. DuetSim: Building user simulator with dual large language models for task-oriented dialogues. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 54145424, Torino, Italia. ELRA and ICCL. Philip McCarthy and Scott Jarvis. 2010. Mtld, vocd-d, and hd-d: validation study of sophisticated approaches to lexical diversity assessment. Behavior research methods, 42(2):381 392. Shikib Mehri and Maxine Eskenazi. 2020. USR: An unsupervised and reference free evaluation In Proceedings metric for dialog generation. of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681707, Online. Association for Computational Linguistics. Sagnik Mukherjee, Lifan Yuan, Dilek HakkaniTur, and Hao Peng. 2025. Reinforcement learning finetunes small subnetworks in large language models. Cheng Niu, Xingguang Wang, Xuxin Cheng, Juntong Song, and Tong Zhang. 2024. Enhancing dialogue state tracking models through LLMbacked user-agents simulation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 87248741, Bangkok, Thailand. Association for Computational Linguistics. OpenAI. 2024. Gpt-4o mini: advancing costefficient intelligence. Jane Pan, Ryan Shar, Jacob Pfau, Ameet Talwalkar, He He, and Valerie Chen. 2025. When benchmarks talk: Re-evaluating code llms with interactive feedback. Daniel Philipov, Vardhan Dongre, gokhan tur, Simulating and Dilek Hakkani Tur. 2024. user agents for embodied conversational AI. In NeurIPS 2024 Workshop on Open-World Agents. Olivier Pietquin and Thierry Dutoit. 2006. probabilistic framework for dialog simulation IEEE Transacand optimal strategy learning. tions on Audio, Speech, and Language Processing, 14(2):589599. Olivier Pietquin and Helen Hastie. 2013. survey on metrics for the evaluation of user simulations. The Knowledge Engineering Review, 28(1):5973. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang, Silvio Savarese, and Caiming Xiong. 2025. Apigen-mt: Agentic pipeline for multiturn data generation via simulated agent-human interplay. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025. Toolrl: Reward is all tool learning needs. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 86898696. Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-based user simulation for bootstrapIn Human ping POMDP dialogue system. Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 149152, Rochester, New York. Association for Computational Linguistics. Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. 2006. survey of statistical user simulation techniques for reinforcementlearning of dialogue management strategies. The knowledge engineering review, 21(2):97 126. Jost Schatzmann and Steve Young. 2009. The hidden agenda user simulation model. IEEE transactions on audio, speech, and language processing, 17(4):733747. Konrad Scheffler and Steve Young. 2002. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proceedings of HLT, volume 2, page 0. Ivan Sekulic, Silvia Terragni, Victor Guimarães, Nghia Khau, Bruna Guedes, Modestas Filipavicius, André Ferreira Manso, and Roland Mathis. 2024. Reliable llm-based user simulator for task-oriented dialogue systems. arXiv preprint arXiv:2402.13374. Pararth Shah, Dilek Hakkani-Tür, Bing Liu, and Gokhan Tür. 2018. Bootstrapping neural conversational agent with dialogue self-play, crowdsourcing and on-line reinforcement learnIn Proceedings of the 2018 Conference ing. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages 4151, New Orleans - Louisiana. Association for Computational Linguistics. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Jeonghoon Shim, Gyuhyeon Seo, Cheongsu Lim, and Yohan Jo. 2025. Tooldial: Multi-turn dialogue generation method for tool-augmented language models. David Silver and Richard Sutton. 2025. Welcome to the era of experience. Google AI, 1. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. 2022. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471. Weiwei Sun, Shuyu Guo, Shuo Zhang, Pengjie Ren, Zhumin Chen, Maarten de Rijke, and Zhaochun Ren. 2023. Metaphorical user simulators for evaluating task-oriented dialogue systems. ACM Transactions on Information Systems, 42(1):129. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna KlimczakPlucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian BalIdan Szpektor, lantyne, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, JeanBaptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. 2025. Gemma 3 technical report. Trong Wu. 1993. An accurate computation of the hypergeometric distribution function. ACM Transactions on Mathematical Software (TOMS), 19(1):3343. Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, and Yongbin Li. 2024. FlowBench: Revisiting and benchmarking workflow-guided planIn Findings of ning for LLM-based agents. the Association for Computational Linguistics: EMNLP 2024, pages 1088310900, Miami, Florida, USA. Association for Computational Linguistics. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. τ -bench: benchmark for tool-agent-user interaction in realworld domains. Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz. 2022. Multiwoz 2.4: multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. ArXiv, abs/1904.09675. Zheng Zhang, Ryuichi Takanobu, Qi Zhu, MinLie Huang, and XiaoYan Zhu. 2020. Recent advances and challenges in task-oriented dialog systems. Science China Technological Sciences, 63(10):20112027. Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 654664, Vancouver, Canada. Association for Computational Linguistics. Zhonghua Zheng, Lizi Liao, Yang Deng, EePeng Lim, Minlie Huang, and Liqiang Nie. 2024. Thoughts to target: Enhance planning for target-driven conversation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21108 21124, Miami, Florida, USA. Association for Computational Linguistics."
        },
        {
            "title": "A System Prompts",
            "content": "Prompt A.1:"
        },
        {
            "title": "Agent Prompt",
            "content": "You are an advanced agent specializing in conversational dialogues. You can act both as an agent (providing services) and user (interacting with the database) to assist users in completing complex tasks. Each task may involve multiple sub-tasks, such as finding restaurants, making reservations, booking hotels, locating attractions, and arranging transportation by checking for trains and buying train tickets. # Task Information: If the user asks for some attributes of venue, then an API call is necessary. If you decide that more than one API calls are needed, you should call one API first and wait for the API result. call the next API or think and make response. After obtaining that result, you may think and If you decide that there is an API input slot that the user has never mentioned, please put \"any\" as the slot value as placeholder. # Objective: Ensure that each assistant utterance follows logical reasoning, determining whether an API call is needed and structuring the output accordingly. too many results returned by API results from database, you should ask the user for more constraints unless the user explicitly wants you to pick one or some. # General Procedure You should follow the workflow below every time you receive an input: item If there are 1. Use APIs to retrieve information from the database, make operations to the backend, or use tools to perform other tasks. 2. Repeat step 1 until you decide to make response to the user. 3. Use the response API to return the response to the user and end your turn. Prompt A.2:"
        },
        {
            "title": "User Simulator Prompt",
            "content": "You are user simulator interacting with an agent. and your task is to interact with the agent to achieve the goals as much as possible. You are provided with User Goals, # Conversation Guidelines: Work toward achieving specified user goals while maintaining natural conversation flow. Reveal information gradually and naturally as real person would. Do not hallucinate information that is not provided in the instruction. If asked about something not specified, respond honestly and tell the agent you do not have that information. User natural language that is appropriate to the context and your assigned personality, rather than copying the exact wording from the instructions. # Conversation Length: The maximum conversation length is 20 messages. Plan your approach accordingly to accomplish your goals within this limit. If youve achieved your goals before reaching the maximum length, you can end the To do so, you need to send \"Terminate.\" in separate, final conversation early. message. Make sure to allow the agent to respond before sending \"Terminate.\". NEVER include \"Terminate.\" in message where you are asking question or making request to the agent, because it will end the conversation before the agent can respond. Do NOT mention \"Terminate.\" at any other time, because that will end the conversation prematurely. Do not hallucinate information that is not provided in the instruction. If asked about something not specified, respond honestly and tell the agent you do not have that information. User natural language that is appropriate to the context and your assigned personality, rather than copying the exact wording from the instructions. # User Goals: {user_goals}"
        },
        {
            "title": "B Multiwoz User Goal Generation",
            "content": "We construct the MultiWOZ Challenge dataset to evaluate user simulator goal alignment. In this section, we present our user goal generation pipeline that we used to create this dataset. Step 1: Task Objective Generation For the attraction, hotel, restaurant, and train domains in the MultiWOZ database, we iterate over the entities. Each entity has set of key-value pairs (e.g. area= east, pricerange= moderate). To instantiate single task objective, we: (1) randomly sample requirement/preference keys to use their values to specify requirements or preferences about the entity the user is looking for, and (2) randomly sample request keys that will be used to specify information that the user wants about the entity. We provide this information to GPT-4o mini, and generate user goal in natural language. For example, given the attraction below: {\"name\": \"abbey pool and astroturf pitch\", \"type\": \"swimmingpool\", \"area\": \"east\", \"price range\": \"moderate\", \"address\": \"pool way ...\", \"phone\": \"01223 902088\", ... } we select the area and price range for requirement/preference keys, and select address and phone for request keys. The task objective would like like: Find swimming pool attraction in the east area. You prefer one with moderate price range. You want to know its address and phone number. Impossible Task Objectives. To also observe how user simulators perform under failure conditions, we repeat the process above, but assign random value to the requirement/preference keys. This helps create an entity that does not exist in our database. Conditional Task Objectives. We also include conditional task objectives. conditional task objective will require the user to search for one specific entity, but if it satisfies some conditional requirement, the user must look for another entity. For example: You are looking for swimming pool with cheap price range. If it is in the east area, then change your mind and look for another swimming pool in the west area that has moderate price range. Step 2: User Profile and Policy Generation Next, we automatically generate diverse set of about 50 user profiles and 50 user policies with GPT4o mini. An example user profile looks like: You are Maria García, travel blogger documenting her adventures across Southeast Asia. Your experiences focus on local cuisine and cultural festivals. An example user policy looks like: Always ask the agent to restate booking details (date, time, price) before confirming. Verify critical details by repeating them back and asking Is that correct?. Then, we conduct manual annotations of the user profiles and policies, where we confirmed the relevance and quality. When necessary, some elements were removed or modified. Step 3: User Goal Generation Finally, we generate user goal by randomly select user profile, user policy and multiple task objectives. These were combined into single user goal. We conducted more manual annotations at this step, to ensure the quality of user goals."
        },
        {
            "title": "C User Goal State Generation",
            "content": "Our user goal state generation process consists of three main steps. First, we decompose each user goal into distinct, modular sub-components, where each represents an independent, self-contained part of the original goal. Second, we classify each sub-component into one of five categories: user profile, user policy, task objective, requirement or preference. Finally, we assign each sub-component its default value as described in Section 4.2. For the decomposition and classification steps, we evaluated several LLMs, including DeepSeek-V3, Llama-3.3-70B-Instruct, GPT 4o-mini, and GPT 4o. Through manual validation, we determined that GPT-4o provided the highest quality results, and therefore used it to generate all the user goal states. The complete prompt used for this process is provided below. Prompt C.1: Sub-Component Decomposition Prompt You are an expert annotator tasked with dividing users goal into sub-components. # User Goal: {user_goal} user goal can be split into multiple sub-components. classified into one of the following categories: Each sub-component can be User Profile: persona, background, contextual information (e.g. You are software engineer with 5 years of experience) User Policy: general constraints and guidelines that must be consistently followed throughout the conversation (e.g. personal information) Always go with the cheapest option, dont share Task Objectives: tasks that must be accomplished (e.g. Book flight) Requirements: conditions and constraints associated with this task objective (e.g. Purchase 3 carry-on bags) Preferences: user preferences associated with this task objective (e.g. You prefer direct flight, youd like an aisle seat) Analyze the user goal and divide it into sub-components: Maintain the same wording as in the original goal when possible Feel free to split up sentence into multiple sub-components if it makes sense or if they fight in different categories Ensure that no part of the original goal is left out It is okay if category is empty, if no sub-components are relevant # Output Format { \"user_profile\": [ # List of user profile sub-components ], \"user_policy\": [ # List of user policy sub-components ], \"task_objectives\": [ # List of task objectives { } ] \"task_objective\": \"task objective description\", \"requirements\": [ # List requirements for this task ], \"preferences\": [ # List of preferences for this task ] } Output only JSON object, in the format specified. Sub-Component Status Update Prompt Prompt D.1: User Profile Sub-Component Status Update Prompt You are an expert evaluator analyzing user simulators behaviour. is given user profile component, which specifies an aspect of their persona, background, or contextual information. determine if the user simulator behavior is aligned with this user profile component. # User Profile Component: {user_profile_component} # Conversation History: {conversation_history[:-2]} Your task is to evaluate the latest turn to The user simulator # Latest Turn: {conversation_history[-2:-1]} # Evaluation Criteria: Evaluate the latest turn, and determine if the user simulator is aligned with the user profile component ({user_profile_component}). You must provide status, the value of which should be: ALIGNED - Choose if ALL these conditions are met: 1. The user simulator behavior does NOT directly contradict the user profile component 2. IF the context clearly calls for demonstrating this profile component, then the user simulator demonstrates it appropriately (explicitly or implicitly) 3. IF the context does NOT clearly call for demonstrating this profile component (neither demonstrating nor contradicting) MISALIGNED - Choose if ANY of these conditions are met: 1. The user simulator behavior DIRECTLY contradicts the user profile component 2. The context clearly and obviously calls for demonstrating the profile component, AND the user simulator fails to do so in way that would be unrealistic for someone with this profile # Important Guidelines: Focus on the latest turn, but use the conversation history for context Unless theres clear contradiction, or obvious missed opportunity, lean towards ALIGNED Most turns will not require demonstrating the profile component user goal can be split into multiple sub-components. Each sub-component can be classified into one of the following categories: User Profile: persona, background, contextual information (e.g. You are software engineer with 5 years of experience) User Policy: general constraints and guidelines that must be consistently followed throughout the conversation (e.g. personal information) Always go with the cheapest option, dont share Task Objectives: tasks that must be accomplished (e.g. Book flight) Requirements: conditions and constraints associated with this task objective (e.g. Purchase 3 carry-on bags) Preferences: user preferences associated with this task objective (e.g. You prefer direct flight, youd like an aisle seat) # Output Format: {{ \"status\": # ALIGNED MISALIGNED, \"reasoning\": # Brief explanation of your decision }} Output properly formatted JSON response, as specified by the Output Format, and nothing else."
        },
        {
            "title": "E Naturalness and Coherence Evaluation Prompts",
            "content": "Prompt E.1:"
        },
        {
            "title": "Naturalness Evaluation Prompt",
            "content": "You are an expert evaluator analyzing user simulators behavior while they are pursuing user goal. throughout conversation. Your task is to evaluate the naturalness of the user simulator # User Goal {user_goal} # Conversation History {conversation_string} # Evaluation Task Rate the naturalness of the user responses on scale from 1 to 5, based on three key dimensions: 1. **Grammar and coherence**: Grammatically correct sentences that flow logically and coherently. 2. **Context Relevance** Responses are strongly related to the conversation context and previous turns, as well as the user goal. 3. **Conversational Style**: conversation style. The rating scale is as follows: Responses are natural, and appropriate for the 5 (Highly Natural): Excels in all three dimensions - perfect grammar/coreference, fully contextually appropriate, and natural conversational 4 (Mostly Natural): Strong in all dimensions with minor lapses - occasional awkward phrasing or slightly unnatural language 3 (Moderately Natural): Adequate in all dimensions - functional but noticeably artificial in style or occasional context mismatches 2 (Somewhat Unnatural): Weak in 1-2 dimensions - grammar errors, loses context, or overly formal/robotic style 1 (Very Unnatural): Fails multiple dimensions - poor grammar, irrelevant responses, or mechanical/repetitive patterns # Output Format: {{ \"naturalness_score\": # number from 1 to 5 \"reasoning\": # Brief explanation of your evaluation decision }} Output properly formatted JSON response, as specified by the Output Format. Prompt E.2:"
        },
        {
            "title": "Coherence Evaluation Prompt",
            "content": "You are an expert evaluator analyzing user simulators behavior while they are pursuiing user goal. in pursuing their goal in conversation. Your task is to evaluate the coherence of user simulator # User Goal {user_goal} # Conversation History {conversation_string} # Evaluation Task Rate the coherence of the users dialogue on scale from 1 to 5, based on three key dimensions: 1. **Goal Progression**: User consistently works toward their stated goal with appropriate persistence and adaptation 2. **Topic Continuity**: between turns Responses maintain topical relevance and logical connections 3. **Response Appropriateness**: User responds appropriately to system prompts, questions, and clarification requests The rating scale is as follows: 5 (Highly Coherent): Clear goal pursuit throughout, all utterances topically connected, appropriate responses to all system turns 4 (Mostly Coherent): Strong goal focus with minor deviations, well-connected utterances, occasionally misses response opportunities 3 (Moderately Coherent): Generally pursues goal with some inconsistencies, mostly connected utterances, some inappropriate responses 2 (Somewhat Incoherent): Weak goal focus with major deviations, frequent topic jumps, often misses or misinterprets prompts 1 (Very Incoherent): No clear goal pursuit, disconnected responses, fails to engage appropriately with system # Output Format: {{ \"coherence_score\": # number from 1 to 5 \"reasoning\": # Brief explanation of your evaluation decision }} Output properly formatted JSON response, as specified by the Output Format."
        }
    ],
    "affiliations": [
        "Contextual AI",
        "University of Illinois Urbana-Champaign"
    ]
}