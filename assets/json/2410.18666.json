{
    "paper_title": "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation",
    "authors": [
        "Yuang Ai",
        "Xiaoqiang Zhou",
        "Huaibo Huang",
        "Xiaotian Han",
        "Zhengyu Chen",
        "Quanzeng You",
        "Hongxia Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models are available at: https://github.com/shallowdream204/DreamClear."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 2 6 6 6 8 1 . 0 1 4 2 : r DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation Yuang Ai, Xiaoqiang Zhou, Huaibo Huang,,(cid:66) Xiaotian Han Zhengyu Chen Quanzeng You Hongxia Yang MAIS & NLPR, Institute of Automation, Chinese Academy of Sciences School of Artificial Intelligence, University of Chinese Academy of Sciences ByteDance, Inc University of Science and Technology of China shallowdream555@gmail.com, huaibo.huang@cripac.ia.ac.cn Code and models: https://github.com/shallowdream204/DreamClear Figure 1: We present DreamClear, high-capacity image restoration model that delivers photorealistic restoration of real-world LQ images, outperforming SOTA diffusion-based models in handling diverse degradations."
        },
        {
            "title": "Abstract",
            "content": "Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing cost-effective, privacy-safe solution for IR dataset construction. The result is 38th Conference on Neural Information Processing Systems (NeurIPS 2024). large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the models adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClears superior performance, underlining the efficacy of our dual strategy for real-world image restoration."
        },
        {
            "title": "Introduction",
            "content": "Image restoration (IR), vital field in computer vision, targets transforming degraded low-quality (LQ) images into high-quality (HQ) counterparts. While IR has achieved significant advancements under predefined conditions, such as super-resolution [92, 10] and denoising [12, 40] tasks, realworld IR remains formidable challenge due to the diversity and complexity of degradation types. The disconnect between training data and real-world scenarios is substantial, as existing datasets inadequately encapsulate the intricacies of real-world degradations. Efforts to bridge this gap include domain adaptation [5, 74, 23, 81], dataset collection [68, 8, 11, 91], and degradation simulation [64, 85, 49, 71]. However, in contrast to the leaps in Natural Language Processing (NLP) [1] and AIGenerated Content (AIGC) [59] enabled by large-scale models and extensive data, IRs progress is not as pronounced. Real-world challenges persist, and the potential of large-scale data and high-capacity models remains largely untapped. This leads us to two critical questions: how can we obtain large-scale dataset that accurately represents real-world IR, and based on this, how can we construct powerful models tailored for real-world IR scenarios? Addressing the first question, considerable efforts have been made to curate IR datasets. Given the challenge of collecting real-world paired IR data, these datasets are typically constructed by acquiring HQ images and then simulating degradations to generate corresponding LQ images. While many works [64, 85, 49, 71] have refined the degradation simulation process, this paper focuses on the acquisition of HQ images and the associated challenges of copyright and privacy protection. The predominant method for obtaining HQ images is web scraping. Current open-source IR datasets, such as DIV2K [44] and Flickr2K [2], contain only few thousand images, insufficient for covering broad spectrum of real-world scenarios. Larger collections like SUPIR [80], with 20 million images, highlight the labor-intensive nature of large-scale dataset curation. Moreover, images sourced from the internet often involve copyright issues and privacy concerns, particularly with identifiable human faces. To advance the IR field effectively, there is an urgent need for dataset curation method that is privacy-safe and cost-effective. In response, we present an under-explored approach in the image restoration (IR) field: creating high-quality, non-existent images to enhance dataset curation efficiency, while evading copyright and privacy issues. We unveil GenIR, privacy-conscious, automated data curation pipeline that repurposes the generative prior in pretrained text-to-image (T2I) models for IR tasks, and uses multimodal large language models (MLLMs) to generate text prompts, thereby improving data synthesis quality. GenIR operates in three stages: (1) image-text pairs construction, (2) dual-prompt fine-tuning, and (3) data generation & filtering. Initially, GenIR utilizes existing IR datasets and the advanced MLLM, Gemini-1.5-Pro [62], to create image-text pairs, while generating negative samples via an image-to-image pipeline [50]. Subsequently, we apply dual-prompt learning strategy to adapt pretrained T2I models to the IR task, generating suitable prompts for data synthesis. In the final stage, MLLMs create various scene descriptions and synthesize images using the adapted image prior, with focus on ensuring no identifiable individuals are included. MLLMs also assess and filter the synthesized data based on quality, producing high-quality images that are privacy-safe and copyright-free. Through GenIR, we generate dataset of one million high-quality images, proving its efficacy in training robust real-world IR model. Armed with large-scale, high-quality image dataset, our focus shifts to the construction of high-capacity IR model that can robustly generalize to real-world scenarios. Recent state-of-theart approaches [77, 70, 80] employ the generative priors in pretrained Stable Diffusion [59] (SD) for realistic image restoration, underlining the power of rich generative prior in SD. As Fig.1 2 illustrates, SD-based methods outperform GAN-based ones. However, these strategies often neglect the degradation priors in input low-quality images, critical element in blind IR [72]. This insight leads us to investigate the integration of degradation prior into diffusion-based IR models, and how to optimize its synergy with large models. In this paper, we introduce DreamClear, high-capacity real-world image restoration model, grounded on large dataset. DreamClear is based on Diffusion Transformer (DiT) [53], the cornerstone of modern diffusion-based systems (e.g., Sora [7], Stable Diffusion 3 [18]). Our model employs dual-branch framework with textual guidance from multi-modal large language models (MLLMs) for photorealistic restoration. DreamClear first processes the low-quality image through lightweight network to produce reference image. We propose ControlFormer to enhance the control over DiT-based T2I models, thereby better utilizing the low-quality and reference images to guide the content of the generated image. To further improve the models generalization across diverse and complex degradations, we incorporate implicit prior degradation information to refine the solution space. Specifically, we suggest Mixture of Adaptive Modulator (MoAM), which extracts token-wise degradation representations and dynamically integrates various restoration experts for each token based on the Mixture-of-Experts (MoE) [61] structure, thereby enhancing the models adaptability to different degradation severities (See Fig. 1). The main contributions of this work can be summarized as follows: We propose GenIR, pioneering automated data curation pipeline for image restoration. It addresses the urgent need for privacy-safe and cost-effective methods in image restoration, yielding dataset of one million high-quality images. We present DreamClear, robust, high-capacity IR model that incorporates degradation priors into diffusion-based frameworks. This model improves control over content generation, adapts to various degradations, and generalizes well across diverse real-world scenarios. Extensive experiments across both low-level (synthetic & real-world) and high-level (detection & segmentation) benchmarks have demonstrated DreamClears state-of-the-art performance in handling intricate real-world scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Image Restoration. Image Restoration aims at restoring high-quality image from the low-quality input image. Over the past decade, different approaches have been proposed for image superresolution [95, 31, 65, 96], denoising [87, 84, 67], deblurring [57, 69, 56], deraining [14, 34, 32, 33], inpainting [97, 3], etc. Recently, researchers have increasingly focused on enhancing the generalization ability to diverse degradations in real-world applications [74, 68, 63]. The degradation simulation is improved from simple degradations to complex degradation processes, such as BSRGAN [85], Real-ESRGAN [64] and AnimeSR [71]. With the improved degradation simulation process, many recent methods can deal with diverse degradation types and achieve promising performance in realworld scenarios [43, 8]. With the paired data, training randomly initialized restoration model from scratch is one way to improve generalization ability [41]. The other way is to exploit the generative prior in the pre-trained generative model, such as GAN or diffusion models [52, 63, 4, 37, 66, 9]. In this work, we propose data synthesis pipeline and introduce real-world image restoration model with high generalizability. Generative Prior. Generative models learn the image synthesis process and embed the image prior in the model weights. The image prior in high-quality image generator, such as StyleGAN [36] and Stable Diffusion [59], can be adapted to other visual restoration tasks [43, 8, 38, 20]. To use the image prior in GANs, an additional encoder is often applied to convert the input image to the latent space [76, 48]. For the diffusion models, the forward process adds noise to the image gradually and finally converts the image to the latent noise space [15, 50]. By manipulating in the latent feature space, the input image is integrated into the generation process as conditional input, and the synthesis process exploits the image prior in the pre-trained models. The generative prior in the pre-trained models can also serve as good initialization for downstream synthesis tasks [63, 78, 35]. We exploit the generative prior in the pre-trained diffusion models to synthesize datasets for image restoration tasks and train restoration model for real-world applications. 3 Figure 2: An overview of the three-stage GenIR pipeline, which includes (a) Image-Text Pairs Construction, (b) Dual-Prompt Based Fine-Tuning, and (c) Data Generation & Filtering. Synthetic Dataset. Data size and data quality are widely recognized as essential for many vision tasks. large-scale high-quality dataset can facilitate the large model training and improve the model ability greatly [83, 22, 21, 24, 26]. Existing large-scale datasets are often manually collected with laborious human efforts [16, 39]. More importantly, the data crawled from the internet may leak privacy information [58, 24], raising concerns related to AI security. The synthesized datasets can not only reduce the laborious human efforts, but also avoid the privacy information leakage. High-quality synthesized datasets are verified to be effective in many vision tasks [28, 25, 6]. Our work is the first to explore the dataset synthesis in the image restoration field."
        },
        {
            "title": "3 Privacy-Safe Dataset Curation Pipeline",
            "content": "Traditionally, IR datasets are created by scraping web images and simulating degradations to generate low-quality (LQ) counterparts. This process is labor-intensive and rife with copyright and privacy issues, especially with identifiable human faces. To advance the IR field, privacy-safe and costeffective dataset curation method is needed. Drawing inspiration from the success of text-to-image (T2I) models in synthesizing high-quality images, we introduce the GenIR pipeline. This novel approach leverages the generative priors of pre-trained T2I models to construct extensive, privacysafe datasets. However, the efficacy of T2I models is contingent upon carefully crafted prompts for generating high-quality images fitting for IR tasks. To tackle this, GenIR, as illustrated in Fig. 2, employs streamlined three-stage process. Initially, we construct positive and negative samples, each paired with corresponding text prompts. Subsequently, dual-prompt based finetuning strategy concurrently learns both positive and negative prompts. Finally, we utilize LLMs to generate diverse array of text prompts, leading to the creation and filtering of data. Throughout this process, we maintain stringent privacy standards, ensuring no specific personal information is embedded in the text prompts or the generated images. Image-Text Pairs Construction. We use high-resolution, texture-rich images in existing IR datasets [44, 2, 22, 39] as positive samples. Given the unavailability of corresponding text prompts, we employ the sophisticated MLLM, Gemini-1.5-Pro [62], to generate necessary prompts via language instructions. Moreover, to identify and eliminate unwanted content during the T2I models fine-tuning and enhance image quality, we generate negative samples representing undesirable outcomes using the T2I model. As depicted in Fig. 2 (a), we adopt the image-to-image pipeline proposed in [50], using the T2I model and manually designed prompts such as cartoon, painting, ... , over-smooth, dirty, to directly generate negative samples. Dual-Prompt based Fine-Tuning. Rather than relying on complex, labor-intensive prompts with limited applicability, we propose an innovative dual-prompt based fine-tuning approach to refine the T2I model for our data needs. As illustrated in Fig. 2 (b), we employ positive and negative samples to learn their corresponding prompts. Specifically, we use positive tokens {p1 , , pM } and 4 negative tokens {n1 , , nN } to represent desired and undesired attributes, respectively, and subsequently learn their embeddings. We initialize these new positive and negative tokens using frequently used positive (e.g., 4k, highly detailed, professional ...) and negative text prompts (e.g., deformation, low quality, over-smooth ...). As the text condition is integrated into the diffusion model via cross-attention, we also refine the attention block to better comprehend these new tokens. After fine-tuning the T2I model with our curated image-text pairs, we can efficiently employ the learned prompts and refined diffusion model to readily generate the needed data. Data Generation & Filtering. In addition to the quality of images, the diversity of scenes within the IR dataset is of paramount importance. To address this, we leverage Gemini to generate one million text prompts, describing varied scenes under carefully curated language instructions. These instructions explicitly proscribe the inclusion of personal or sensitive information, thereby ensuring privacy. As depicted in Fig. 2 (c), we employ the fine-tuned T2I model in conjunction with the learned positive and negative prompts to generate HQ images. Classifier-free guidance (CFG) [30] provides mechanism to effectively utilize negative prompts, thereby mitigating the generation of undesired content. During the sampling phase, the denoising model ϵθ anticipates two outcomes: one associated with the positive prompt pos and the other with the negative prompt neg. The final CFG prediction is formulated as ϵθ(zt, t, pos, neg) = ω ϵθ(zt, t, pos) + (1 ω) ϵθ(zt, t, neg), (1) where ω denotes the CFG guidance scale. Post-sampling, the generated images are evaluated by quality classifier, which decides whether to retain the images based on the predicted probabilities. This binary classifier is trained on positive and negative samples. Gemini is subsequently used to ascertain whether the images exhibit blatant semantic errors or inappropriate content. Contrasted with direct web crawling, our GenIR provides more cost-effective and privacy-preserving approach to data acquisition. It circumvents the potential infringement of personal privacy information prevalent on the web, thereby ensuring our research is both ethical and secure - crucial aspect in the current artificial intelligence landscape characterized by extensive data usage. Ultimately, we gather one million high-resolution (2040 1356) images, each of superior quality."
        },
        {
            "title": "4 High-Capacity Image Restoration Model",
            "content": "The complex and varied degradation of real-world images presents major challenge to the practical applicability of restoration models. We introduce DreamClear, high-capacity image restoration model that dynamically integrates various restoration experts, guided by prior degradation information. DreamClear is built upon on PixArt-α [13], pre-trained T2I diffusion model based on the Diffusion Transformer (DiT) [53] architecture, which has proven its powerful generative capabilities [93]. Architecture Overview. Fig. 3 illustrates that DreamClear features dual-branch architecture, encompassing an LQ Branch and Reference Branch. LQ images Ilq are processed by SwinIR [41], lightweight degradation remover, resulting in smoother, albeit less detailed, reference images Iref . Considering potential detail loss in Iref , we employ both Ilq and Iref to direct the diffusion model. Moreover, we utilize the open-source MLLM, LLaVA [47], to generate detailed captions for training images using the prompt Describe this image and its style in very detailed manner, supporting the T2I diffusion model in attaining more realistic restoration. ControlFormer. ControlNet [88], prevalent structure for managing diffusion models, is tailored for the U-Net [60] structure in SD. It is unsuitable for DiT, stemming from the architecture difference. To address this, we present ControlFormer, which inherits ControlNets core features (trainable copy blocks and zero-initialized layers) but is adapted for the DiT-based T2I model, as shown in Fig. 3. ControlFormer, duplicating all DiT Blocks from PixArt-α, employs the MoAM block to combine LQ features xlq and reference features xref . This DiT-optimized ControlFormer maintains ControlNets essential components, providing effective spatial control within DiT. Mixture-of-Adaptive-Modulator. To enhance our models robustness to real-world degradations, we propose degradation-aware Mixture-of-Adaptive-Modulator (MoAM) for effective LQ and reference feature fusion. As shown in Fig. 3, MoAM consists of adaptive modulators (AM), 5 Figure 3: Architecture of the proposed DreamClear. DreamClear adopts dual-branch structure, using Mixture of Adaptive Modulator to merge LQ features and Reference features. We utilize MLLM to generate detailed text prompt as the guidance for T2I model. cross-attention layer, and router block. AM employs AdaLN [54] to learn dimension-wise scale γ and shift β parameters, embedding conditional information into input features. MoAM operates in three steps: 1) For DiT features xin, we calculate the cross-attention output xattn RN between xlq RN and xref RN C, where and denote the number of visual tokens and hidden size. xin is then modulated using xattn followed by zero linear layer. token-wise degradation map RN is generated through the linear mapping of xattn. 2) Features are further modulated using AM, with xref as the AM condition to extract clean features. 3) We adopt mixture of degradation-aware experts to adapt to diverse degradations, detailed below. Given varying degradations in real-world images, our method dynamically processes tokens using degradation priors. Each MoAM block consists of restoration experts (i.e., AM) {E1, , EK}, each specialized for specific degradation scenarios. routing network R() dynamically merges expert guidance for tokens, based on D. The routing network, two-layer MLP followed by softmax, yields token-wise expert weights = R(D) RN K. The dynamic mixture of restoration experts is formulated as γ(i) = (cid:88) k= w(i, k) etγ k[xlq(i)], β(i) = (cid:88) k=1 w(i, k) etβ [xlq(i)], (2) xout = (1 + γ) xin + β, where and index tokens and experts respectively, etγ and etβ map within an expert to γ and β, and denotes element-wise multiplication. MoAM dynamically fuses expert knowledge, leveraging degradation priors to tackle complex degradations. (3)"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Datasets. We adopt combination of DIV2K [44], Flickr2K [2], LSDIR [39], DIV8K [22], and our generated dataset to train DreamClear. We employ the Real-ESRGAN [64] degradation pipeline to generate LQ images, using the same degradation settings as SeeSR [70] to ensure fair comparison. All experiments are conducted on scaling factor 4. For testing datasets, following previous works [63, 80, 70], we evaluate our method on both synthetic and real-world benchmarks. For synthetic benchmarks, we randomly crop 3,000 patches from the 6 Table 1: Quantitative comparison with state-of-the-art real-world IR methods on both synthetic and real-world benchmarks. Best and second best performance are highlighted in red and blue, respectively. Datasets Metrics BSRGAN [85] Real- [64] ESRGAN SwinIRGAN [41] DASR [43] StableSR [63] DiffBIR [46] ResShift [82] SinSR [65] SeeSR [70] SUPIR [80] DreamClear PSNR SSIM LPIPS DISTS FID NIQE MANIQA MUSIQ CLIPIQA PSNR SSIM LPIPS DISTS FID NIQE MANIQA MUSIQ CLIPIQA PSNR SSIM LPIPS DISTS FID NIQE MANIQA MUSIQ CLIPIQA PSNR SSIM LPIPS DISTS FID NIQE MANIQA MUSIQ CLIPIQA NIQE MANIQA MUSIQ CLIPIQA DIV2K-Val LSDIR-Val RealSR DrealSR RealLQ250 19.88 0.5137 0.4303 0.2484 54.42 3.9322 0.3514 63.93 0.5589 18.27 0.4673 0.4378 0.2539 53.25 3.6885 0.3829 65.98 0.5648 25.01 0.7422 0.2853 0.1967 84.49 4.9261 0.3660 64.67 0. 27.09 0.7759 0.2950 0.1956 84.26 5.5866 0.3420 61.22 0.5385 4.5229 0.3523 63.66 0.5695 19.92 0.5334 0.3981 0.2304 48.44 3.8762 0.3854 64.50 0.5804 18.13 0.4867 0.3986 0.2278 46.46 3.4078 0.4381 68.25 0.6218 24.22 0.7401 0.2901 0.1892 90.10 5.0069 0.3656 62.06 0.4872 26.95 0.7812 0.2876 0.1857 83.79 5.7422 0.3423 58.37 0. 4.1091 0.3592 62.74 0.5465 19.66 0.5253 0.3992 0.2253 49.17 3.7468 0.3654 64.54 0.5682 17.98 0.4783 0.4020 0.2253 45.31 3.3715 0.3991 67.10 0.5983 24.89 0.7543 0.2680 0.1734 80.07 4.9475 0.3432 60.97 0.4548 27.00 0.7815 0.2789 0.1787 84.22 5.5749 0.3269 57.33 0.4819 4.0912 0.3632 63.63 0. 19.73 0.5122 0.4350 0.2606 59.62 3.9725 0.3110 59.66 0.5565 18.15 0.4679 0.4503 0.2615 60.60 3.6432 0.3315 60.96 0.5681 25.51 0.7526 0.3201 0.2056 91.16 5.9659 0.2819 50.94 0.3819 28.19 0.8051 0.3165 0.2072 94.96 6.9663 0.2754 46.49 0.3828 4.7486 0.2782 53.39 0.4671 19.73 0.5039 0.4145 0.2162 29.64 4.4255 0.2942 58.60 0. 18.11 0.4508 0.4152 0.2159 31.26 4.0218 0.3098 59.37 0.5190 24.60 0.7387 0.2736 0.1761 88.89 5.6124 0.3465 61.07 0.5139 27.39 0.7830 0.2710 0.1737 80.23 6.1699 0.3171 56.43 0.5344 4.6349 0.2939 57.11 0.5208 19.98 0.4987 0.3866 0.2396 37.00 4.5659 0.4268 64.77 0.6527 18.42 0.4618 0.4049 0.2439 35.91 4.3750 0.4551 65.94 0. 24.77 0.6902 0.3436 0.2195 69.94 6.1294 0.4182 61.74 0.6202 27.31 0.7140 0.3920 0.2443 76.89 6.3433 0.3801 55.14 0.6005 4.8160 0.4017 62.18 0.6420 19.80 0.4985 0.4450 0.2383 46.12 5.9852 0.3782 62.67 0.6498 18.24 0.4579 0.4524 0.2436 43.25 5.5635 0.3995 63.25 0.6501 24.94 0.7178 0.3864 0.2467 88.91 6.6044 0.3781 60.28 0. 27.16 0.7388 0.4101 0.2553 91.82 7.5616 0.3350 55.27 0.5788 5.9727 0.3816 61.55 0.6298 19.37 0.4613 0.4383 0.2175 37.84 5.7320 0.4206 65.27 0.6961 17.94 0.4302 0.4523 0.2265 36.01 5.4240 0.4309 65.35 0.6900 24.47 0.6710 0.4208 0.2432 70.83 6.4662 0.4009 60.36 0.6587 26.15 0.6564 0.4690 0.2540 85.26 6.8770 0.3890 58.50 0. 5.7768 0.4229 64.09 0.7044 19.59 0.5045 0.3662 0.1886 24.98 4.1320 0.5251 72.04 0.7181 18.03 0.4564 0.3759 0.1966 25.91 4.0590 0.5700 73.00 0.7261 24.66 0.7209 0.2997 0.2029 71.92 4.9102 0.5189 69.38 0.6839 27.10 0.7596 0.3117 0.2103 75.07 5.7696 0.4974 67.42 0.7022 4.4126 0.4992 70.57 0. 18.68 0.4664 0.3976 0.1818 28.11 3.4014 0.4291 69.34 0.6035 16.95 0.4080 0.4119 0.1838 30.03 2.9820 0.4683 70.98 0.6174 22.67 0.6567 0.3545 0.2185 71.63 4.5368 0.4296 66.09 0.5371 24.41 0.6696 0.3844 0.2264 90.78 5.1115 0.4174 64.53 0.5800 3.6336 0.3926 66.03 0.5800 18.69 0.4766 0.3657 0.1637 20.61 3.2126 0.4320 68.44 0. 17.01 0.4236 0.3836 0.1656 22.06 3.0707 0.4811 70.40 0.6914 22.56 0.6548 0.3684 0.2122 65.37 4.4381 0.4337 65.33 0.6895 24.48 0.6508 0.3972 0.2145 74.78 4.6295 0.3676 59.83 0.6620 3.5556 0.4351 66.76 0.7116 Table 2: Evaluation on COCO val2017 (object detection & instance segmentation) and ADE20K (semantic segmentation). Metrics GT Zoomed LQ BSRGAN RealESRGAN SwinIRGAN DASR StableSR DiffBIR ResShift SinSR SeeSR SUPIR DreamClear Object Detection (AP b) 49.0 Object Detection (AP 70.6 50) Object Detection (AP 53.8 75) Instance Segmentation (AP m) 43.9 Instance Segmentation (AP 67.7 50 ) Instance Segmentation (AP 47.3 75 ) Semantic Segmentation (mIoU) 50.4 7.4 12.0 7.5 6.4 11.2 6.3 11. 11.0 17.6 11.4 9.6 16.4 9.7 18.6 12.8 20.7 13.1 11.3 19.3 11.5 17.3 11.8 18.9 12.1 10.2 17.5 10.2 14.3 10.5 17.0 10.7 9.3 15.9 9.4 30.4 16.9 26.7 17.6 14.6 24.6 14.9 19.6 18.7 29.9 19.4 16.2 27.5 16.6 23. 15.6 25.0 15.9 13.6 23.3 13.7 29.7 13.8 22.3 14.2 12.0 20.6 12.1 19.6 18.2 29.1 18.9 15.9 26.6 16.1 26.9 16.6 27.2 17.0 14.1 24.5 14.0 27.7 19.3 30.8 19.8 16.7 28.2 16.8 31.9 validation sets of DIV2K and LSDIR, and degrade them using the same settings as training. We name these two benchmarks as DIV2K-Val and LSDIR-Val respectively. For real-world benchmarks, we conduct experiments on commonly used RealSR [8] and DRealSR [68] datasets. Besides, we establish another real-world benchmark, called RealLQ250, which includes total of 250 LQ images of size 256 256 used in previous works [70, 42, 64, 86, 80] or sourced from the Internet, without corresponding GT images. For all testing datasets with GT images, the resolution of the HQ-LQ image pairs is 1024 1024 and 256 256, respectively. Metrics. Following SeeSR [70], we adopt PSNR and SSIM (calculated on the channel of transformed YCbCr space) as reference-based distortion metrics, LPIPS [90] and DISTS [17] as reference-based perceptual metrics, NIQE [89], MANIQA [75], MUSIQ [75] and CLIPIQA [75] as no-reference metrics. FID [29] is used to evaluate the image quality. These metrics can achieve comprehensive evaluation of the restoration effects. Implementation Details For training GenIR and DreamClear, we both use the original latent diffusion loss [59]. The proposed GenIR framework is built on SDXL [55] and trained over 5 days using 16 NVIDIA A100 GPUs. The training is conducted on 1024 1024 resolution images with 7 LQ Input Real-ESRGAN [64] DiffBIR [46] SinSR [65] SeeSR [70] SUPIR [80] DreamClear (Ours) Figure 4: Qualitative comparisons on both synthetic (the first row) and real-world (the last two rows) benchmarks. Please zoom in for better view. batch-size of 256. For data generation, we use 256 NVIDIA V100 GPUs and spend 5 days to generate the large-scale dataset. Our DreamClear is built upon PixArt-α [13] and LLaVA [47]. The SwinIR model in DiffBIR [46] is used as the lightweight degradation remover. We use the AdamW optimizer with learning rate of 5e5 to train our model. The training is conducted on 1024 1024 resolution images, running for 7 days on 32 NVIDIA A100 GPUs with batch-size of 128. The number of experts in Eq. (2) is set to 3. For inference of DreamClear, we adopt iDDPM [51] with 50 sampling steps, CFG guidance scale ω = 4.5. Figure 5: User study. Vote percentage denotes average user preference per model. The Top-K ratio indicates the proportion of images preferred by top users. Our model is highly preferred, with most images being rated as top quality by the majority. 5.2 Comparison with State-of-the-Art Methods We compare our method with state-of-the-art GAN-based methods (BSRGAN [85], RealESRGAN [64], SwinIR-GAN [41], and DASR [43]) and recent diffusion-based methods (StableSR [63], DiffBIR [46], ResShift [82], SinSR [65], SeeSR [70], and SUPIR [80]). Quantitative Comparisons. Tab. 1 presents quantitative results on various benchmarks. Our method consistently excels in perceptual metrics (LPIPS, DISTS, FID) on synthetic datasets, signifying high perceptual quality. On real-world benchmarks, our method performs strongly across most no-reference metrics (NIQE, MANIQA, MUSIQ, CLIPIQA), attesting to the high quality of our restorations. Our diffusion-based method prioritizes photorealistic restoration. Despite lower PSNR/SSIM scores, recent works [79, 80] argue these metrics inadequately represent visual quality, and it is necessary to reconsider the reference values of existing metrics and propose more effective methods to evaluate modern image restoration methods. We believe that as the field of image quality assessment (IQA) evolves, more suitable metrics will emerge to adequately measure the performance of advanced IR models. 8 (a) (d) (b) (e) (c) (f) Figure 6: Impact of synthetic training data. As data size increases, performance improves on LSDIR-Val. Table 3: Ablation results on DIV2K-Val, COCO val2017 and ADE20K for DreamClear. LPIPS DISTS FID MANIQA MUSIQ CLIPIQA AP AP mIoU Mixture of AM AM Cross-Attention Zero-Linear Dual-Branch w/o Reference Branch Detailed Text Prompt Null Prompt 0.3657 0.3981 0.4177 0.4082 0.3657 0.4207 0.3657 0.3521 0.1637 0.1843 0.2016 0.1976 0.1637 0. 0.1637 0.1607 20.61 25.75 29.74 29.89 20.61 30.91 20.61 20.47 0.4320 0.4067 0.3785 0.4122 0.4320 0. 0.4320 0.4230 68.44 66.18 63.21 66.11 68.44 64.04 68.44 67.26 0.6963 0.6646 0.6497 0.6673 0.6963 0. 0.6963 0.6812 19.3 18.0 17.2 17.6 19.3 15.9 19.3 18.8 16.7 15.6 15.1 15.3 16.7 14. 16.7 16.2 31.9 28.6 26.3 27.2 31.9 24.7 31.9 29.8 Qualitative Comparisons. We provide qualitative comparisons in Fig. 4. When handling severe degradations (the first row), only our DreamClear can not only reason the correct structure but also generate clear details, while other methods may generate deformed structure and blurry results. When it comes to real-world images, our method can achieve results that are rich in detail and more natural (the third row). More real-world visual comparisons are in Apendix A.4. User Study. We conducted user study to evaluate our models restoration quality using 100 low-quality images, restored by our method and five others. Users were asked to rank the restorations considering visual quality, naturalness, and accuracy, among others. The study, involving 256 evaluators, was designed for fairness and wide participation. Two metrics, vote percentage and Top-K ratio, were used to analyze the results. As shown in Fig. 5, our model led on both metrics, receiving over 45% of total votes and being the top choice for 80% of the images, demonstrating its consistent superiority in producing high-quality images. Refer to Appendix A.2 for more details. 5.3 Evaluation on Downstream Benchmarks We assess the benefits of image restoration for downstream high-level vision tasks by conducting detection and segmentation experiments on the COCO 2017 [45] and ADE20K [94] datasets using various restoration models. Low-quality images are generated and restored under the same conditions as in training. We use the robust visual backbone RMT [19] (with Mask R-CNN [27] 1 for COCO, with UperNet [73] for ADE20K) for these tasks. Tab. 2 shows that our model obtains the best performance, implying its superiority in benefiting downstream tasks. Despite its superior performance in semantic restoration and fine-grained image recognition tasks, theres still substantial room for improvement. 5.4 Ablation Study Analysis of Generated Dataset for Real-World IR. Due to the extensive time required to train diffusion-based models, we use SwinIR-GAN to investigate the impact of generated datasets on 9 real-world image restoration (IR). SwinIR is trained on varying quantities of generated data for comparison with the DF2K-trained model. Fig. 6 shows that the model trained on an equivalent number of generated images exhibits marginally lower perceptual but higher no-reference metrics than the DF2K model. As the dataset size increases, all metrics improve, reinforcing our belief that larger datasets enhance model generalizability and restoration performance. Notably, the model trained with 100,000 generated images outperforms the DF2K model, underscoring the advantages of utilizing large-scale synthetic datasets for real-world IR. More ablations of GenIR are provided in Appendix A.3. Ablations of DreamClear. We conduct ablation studies to scrutinize the contribution of each component within DreamClear. Evaluating perceptual fidelity via LPIPS, DISTS, and FID metrics, and assessing the image quality of restoration results with MANIQ, MUSIQ, and CLIPIQA, we find that DreamClear outperforms its ablated versions across most metrics, substantiating the importance of these components (Tab. 3). Notably, null prompts slightly outperform detailed prompts on perceptual metrics. However, superior results on no-reference and high-level vision metrics suggest that the MLLM-provided detailed text prompts better preserve semantic information. Visual comparisons detailed in Appendix A.3 further reinforce the benefits of semantic guidance in image restoration provided by text prompts."
        },
        {
            "title": "6 Limitations and Broader Impact",
            "content": "Our model leverages the generative prior of pre-trained diffusion models for image restoration, with diverse synthesized datasets used during training to enhance model performance. In situations of severe image degradation, while our method could predict reasonable and realistic results, the synthesized texture details may not exist in the ground-truth image. high-quality reference image or explicit human instruction may compensate such limitation in some degree. Another limitation lies in the deployment in practical applications. Our model is diffusion-based model and it needs multiple inference steps to restore the input low-quality image. While our model can predict more plausible results than existing methods, it can not meet the requirement of real-time inference speed in many practical applications. Model distillation and model quantization may compensate the limitation of inference speed. This paper is purely academic study of real-world image restoration (IR). However, considering image restorations vital role in many practical applications, this work may not only bring some positive societal influence, e.g., improving the quality of images captured by smartphones, but also lead to some potential risks like privacy information leakage from photos on social media. However, the positive societal effects of image restoration far exceed the potential negative impacts, and people may make use of some other techniques, such as inpainting and watermarking, to remove the private information in images."
        },
        {
            "title": "7 Conclusion",
            "content": "To address the challenges in real-world image restoration (IR), we develop GenIR, privacy-safe automated pipeline that generates large-scale dataset of one million high-quality images, serving as robust training resource for IR models. Additionally, we introduce DreamClear, potent IR model that seamlessly integrates degradation priors into diffusion-based IR models. It introduces the novel Mixture of Adaptive Modulator (MoAM) to adapt to diverse real-world degradations. Our comprehensive experiments underline its outstanding performance in managing complex real-world situations, marking substantial progression in IR."
        },
        {
            "title": "Acknowledgements",
            "content": "This research is partially funded by Beijing Nova Program (20230484276), and Youth Innovation Promotion Association CAS (Grant No. 2022132)."
        },
        {
            "title": "References",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] E. Agustsson and R. Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In CVPRW, pages 126135, 2017. 2, 4, 6 [3] Y. Ai, H. Huang, and R. He. Lora-ir: Taming low-rank experts for efficient all-in-one image restoration. arXiv preprint arXiv:2410.15385, 2024. 3 [4] Y. Ai, H. Huang, X. Zhou, J. Wang, and R. He. Multimodal prompt perceiver: Empower adaptiveness generalizability and fidelity for all-in-one image restoration. In CVPR, pages 2543225444, 2024. [5] Y. Ai, X. Zhou, H. Huang, L. Zhang, and R. He. Uncertainty-aware source-free adaptive image super-resolution with wavelet augmentation transformer. In CVPR, pages 81428152, 2024. 2 [6] S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023. 4 [7] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video generation models as world simulators, 2024. 3 [8] J. Cai, H. Zeng, H. Yong, Z. Cao, and L. Zhang. Toward real-world single image superresolution: new benchmark and new model. In ICCV, pages 30863095, 2019. 2, 3, 7 [9] J. Cao, Y. Shi, K. Zhang, Y. Zhang, R. Timofte, and L. Van Gool. Deep equilibrium diffusion restoration with parallel sampling. In CVPR, pages 28242834, 2024. [10] M. Cao, C. Mou, F. Yu, X. Wang, Y. Zheng, J. Zhang, C. Dong, G. Li, Y. Shan, R. Timofte, et al. Ntire 2023 challenge on 360deg omnidirectional image and video super-resolution: Datasets, methods and results. In CVPRW, pages 17311745, 2023. 2 [11] C. Chen, Z. Xiong, X. Tian, Z.-J. Zha, and F. Wu. Camera lens super-resolution. In CVPR, pages 16521660, 2019. 2 [12] H. Chen, J. Gu, Y. Liu, S. A. Magid, C. Dong, Q. Wang, H. Pfister, and L. Zhu. Masked image training for generalizable deep image denoising. In CVPR, pages 16921703, 2023. 2 [13] J. Chen, Y. Jincheng, G. Chongjian, L. Yao, E. Xie, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. 5, 8 [14] X. Chen, H. Li, M. Li, and J. Pan. Learning sparse transformer network for effective image deraining. In CVPR, pages 58965905, 2023. 3 [15] J. Choi, S. Kim, Y. Jeong, Y. Gwon, and S. Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. In ICCV, pages 1436714376, 2021. 3 [16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. 4 [17] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli. Image quality assessment: Unifying structure and texture similarity. TPAMI, 44(5):25672581, 2020. 7 [18] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 3 [19] Q. Fan, H. Huang, M. Chen, H. Liu, and R. He. Rmt: Retentive networks meet vision transformers. In CVPR, 2024. 9 [20] B. Fei, Z. Lyu, L. Pan, J. Zhang, W. Yang, T. Luo, B. Zhang, and B. Dai. Generative diffusion prior for unified image restoration and enhancement. In CVPR, pages 99359946, 2023. 3 [21] C. Fu, X. Wu, Y. Hu, H. Huang, and R. He. Dvg-face: Dual variational generation for heterogeneous face recognition. TPAMI, 44(6):29382952, 2021. [22] S. Gu, A. Lugmayr, M. Danelljan, M. Fritsche, J. Lamour, and R. Timofte. Div8k: Diverse 8k resolution image dataset. In ICCVW, pages 35123516, 2019. 4, 6 [23] Y. Guo, J. Chen, J. Wang, Q. Chen, J. Cao, Z. Deng, Y. Xu, and M. Tan. Closed-loop matters: Dual regression networks for single image super-resolution. In CVPR, pages 54075416, 2020. 2 [24] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: dataset and benchmark for large-scale face recognition. In ECCV, pages 87102, 2016. 4 [25] H. A. A. K. Hammoud, H. Itani, F. Pizzati, P. Torr, A. Bibi, and B. Ghanem. Synthclip: Are we ready for fully synthetic clip training? arXiv preprint arXiv:2402.01832, 2024. 4 [26] X. Han, Y. Jian, X. Hu, H. Liu, Y. Wang, Q. Fan, Y. Ai, H. Huang, R. He, Z. Yang, et al. Infimmwebmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. arXiv 11 preprint arXiv:2409.12568, 2024. 4 [27] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn. In ICCV, pages 29612969, 2017. [28] R. He, S. Sun, X. Yu, C. Xue, W. Zhang, P. Torr, S. Bai, and X. Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022. 4 [29] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, pages 66266637, 2017. 7 [30] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [31] H. Huang, R. He, Z. Sun, and T. Tan. Wavelet-srnet: wavelet-based cnn for multi-scale face super resolution. In ICCV, pages 16891697, 2017. 3 [32] H. Huang, M. Luo, and R. He. Memory uncertainty learning for real-world single image deraining. TPAMI, 45(3):34463460, 2022. 3 [33] H. Huang, A. Yu, Z. Chai, R. He, and T. Tan. Selective wavelet attention learning for single image deraining. IJCV, 129(4):12821300, 2021. 3 [34] H. Huang, A. Yu, and R. He. Memory oriented transfer learning for semi-supervised image deraining. In CVPR, pages 77327741, 2021. 3 [35] J. Karras, A. Holynski, T.-C. Wang, and I. Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In ICCV, pages 2262322633, 2023. 3 [36] T. Karras, S. Laine, and T. Aila. style-based generator architecture for generative adversarial networks. In CVPR, pages 44014410, 2019. 3 [37] B. Kawar, M. Elad, S. Ermon, and J. Song. Denoising diffusion restoration models. In NeurIPS, pages 2359323606, 2022. 3 [38] H. Lee, K. Kang, H. Lee, S.-H. Baek, and S. Cho. Ugpnet: Universal generative prior for image restoration. In WACV, pages 15981608, 2024. 3 [39] Y. Li, K. Zhang, J. Liang, J. Cao, C. Liu, R. Gong, Y. Zhang, H. Tang, Y. Liu, D. Demandolx, et al. Lsdir: large scale dataset for image restoration. In CVPRW, pages 17751787, 2023. 4, 6 [40] Y. Li, Y. Zhang, R. Timofte, L. Van Gool, Z. Tu, K. Du, H. Wang, H. Chen, W. Li, X. Wang, et al. Ntire 2023 challenge on image denoising: Methods and results. In CVPR, pages 19041920, 2023. 2 [41] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte. Swinir: Image restoration using swin transformer. In ICCVW, pages 18331844, 2021. 3, 5, 7, 8 [42] J. Liang, H. Zeng, and L. Zhang. Details or artifacts: locally discriminative learning approach to realistic image super-resolution. In CVPR, pages 56575666, 2022. 7 [43] J. Liang, H. Zeng, and L. Zhang. Efficient and degradation-adaptive network for real-world image super-resolution. In ECCV, pages 574591, 2022. 3, 7, 8 [44] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, pages 136144, 2017. 2, 4, 6 [45] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755, 2014. 9 [46] X. Lin, J. He, Z. Chen, Z. Lyu, B. Fei, B. Dai, W. Ouyang, Y. Qiao, and C. Dong. Diffbir: Towards blind image restoration with generative diffusion prior. arXiv preprint arXiv:2308.15070, 2023. 7, 8, 19, 20, 21 [47] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In NeurIPS, 2023. 5, 8 [48] M. Liu, Y. Wei, X. Wu, W. Zuo, and L. Zhang. Survey on leveraging pre-trained generative adversarial networks for image editing and restoration. Science China Information Sciences, 66(5):151101, 2023. 3 [49] Z. Luo, Y. Huang, , S. Li, L. Wang, and T. Tan. Learning the degradation distribution for blind image super-resolution. In CVPR, 2022. 2 [50] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 2, 3, 4, 15 [51] A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In ICML, pages 81628171, 2021. 8 [52] X. Pan, X. Zhan, B. Dai, D. Lin, C. C. Loy, and P. Luo. Exploiting deep generative prior for versatile image restoration and manipulation. TPAMI, 44(11):74747489, 2021. 3 [53] W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 3, 5 [54] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with general conditioning layer. In AAAI, 2018. 6 12 [55] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2023. 7 [56] Y. Quan, Z. Wu, and H. Ji. Neumann network with recursive kernels for single image defocus deblurring. In CVPR, pages 57545763, 2023. [57] J. Rim, H. Lee, J. Won, and S. Cho. Real-world blur dataset for learning and benchmarking deblurring algorithms. In ECCV, pages 184201, 2020. 3 [58] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi. Performance measures and data set for multi-target, multi-camera tracking. In ECCV, pages 1735, 2016. 4 [59] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. 2, 3, [60] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234241, 2015. 5 [61] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017. 3 [62] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 4 [63] J. Wang, Z. Yue, S. Zhou, K. C. Chan, and C. C. Loy. Exploiting diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015, 2023. 3, 6, 7, 8, 19, 20, 21 [64] X. Wang, L. Xie, C. Dong, and Y. Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In ICCVW, pages 19051914, 2021. 2, 3, 6, 7, 8 [65] Y. Wang, W. Yang, X. Chen, Y. Wang, L. Guo, L.-P. Chau, Z. Liu, Y. Qiao, A. C. Kot, and B. Wen. Sinsr: Diffusion-based image super-resolution in single step. In CVPR, 2024. 3, 7, 8 [66] Y. Wang, J. Yu, and J. Zhang. Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490, 2022. 3 [67] Z. Wang, Y. Fu, J. Liu, and Y. Zhang. Lg-bpn: Local and global blind-patch network for self-supervised real-world denoising. In CVPR, pages 1815618165, 2023. 3 [68] P. Wei, Z. Xie, H. Lu, Z. Zhan, Q. Ye, W. Zuo, and L. Lin. Component divide-and-conquer for real-world image super-resolution. In ECCV, pages 101117, 2020. 2, 3, 7 [69] J. Whang, M. Delbracio, H. Talebi, C. Saharia, A. G. Dimakis, and P. Milanfar. Deblurring via stochastic refinement. In CVPR, pages 1629316303, 2022. 3 [70] R. Wu, T. Yang, L. Sun, Z. Zhang, S. Li, and L. Zhang. Seesr: Towards semantics-aware real-world image super-resolution. In CVPR, 2024. 2, 6, 7, 8, 19, 20, 21 [71] Y. Wu, X. Wang, G. Li, and Y. Shan. Animesr: Learning real-world super-resolution models for animation videos. In NeurIPS, pages 1124111252, 2022. 2, 3 [72] B. Xia, Y. Zhang, Y. Wang, Y. Tian, W. Yang, R. Timofte, and L. Van Gool. Knowledge distillation based degradation estimation for blind super-resolution. In ICLR, 2023. 3 [73] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun. Unified perceptual parsing for scene understanding. In ECCV, pages 418434, 2018. 9 [74] X. Xu, P. Wei, W. Chen, Y. Liu, M. Mao, L. Lin, and G. Li. Dual adversarial adaptation for cross-device real-world image super-resolution. In CVPR, pages 56675676, 2022. 2, 3 [75] S. Yang, T. Wu, S. Shi, S. Lao, Y. Gong, M. Cao, J. Wang, and Y. Yang. Maniqa: MultiIn CVPR, pages dimension attention network for no-reference image quality assessment. 11911200, 2022. 7 [76] T. Yang, P. Ren, X. Xie, and L. Zhang. Gan prior embedded network for blind face restoration in the wild. In CVPR, pages 672681, 2021. 3 [77] T. Yang, R. Wu, P. Ren, X. Xie, and L. Zhang. Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization. arXiv preprint arXiv:2308.14469, 2023. 2 [78] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3 [79] Z. You, Z. Li, J. Gu, Z. Yin, T. Xue, and C. Dong. Depicting beyond scores: Advancing image quality assessment through multi-modal language models. arXiv preprint arXiv:2312.08962, 2023. [80] F. Yu, J. Gu, Z. Li, J. Hu, X. Kong, X. Wang, J. He, Y. Qiao, and C. Dong. Scaling up to excellence: Practicing model scaling for photo-realistic image restoration in the wild. In CVPR, 2024. 2, 6, 7, 8, 19, 20, 21 [81] Y. Yuan, S. Liu, J. Zhang, Y. Zhang, C. Dong, and L. Lin. Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks. In CVPRW, pages 701710, 2018. 2 [82] Z. Yue, J. Wang, and C. C. Loy. Resshift: Efficient diffusion model for image super-resolution by residual shifting. In NeurIPS, 2023. 7, 8 [83] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In CVPR, pages 1210412113, 2022. 4 [84] J. Zhang, Y. Zhang, J. Gu, J. Dong, L. Kong, and X. Yang. Xformer: Hybrid x-shaped transformer for image denoising. In ICLR, 2024. 3 [85] K. Zhang, J. Liang, L. Van Gool, and R. Timofte. Designing practical degradation model for deep blind image super-resolution. In ICCV, pages 47914800, 2021. 2, 3, 7, 8 [86] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond gaussian denoiser: Residual learning of deep cnn for image denoising. TIP, 26(7):31423155, 2017. [87] K. Zhang, W. Zuo, and L. Zhang. Ffdnet: Toward fast and flexible solution for cnn-based image denoising. TIP, 27(9):46084622, 2018. 3 [88] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. 5 [89] L. Zhang, L. Zhang, and A. C. Bovik. feature-enriched completely blind image quality evaluator. TIP, 24(8):25792591, 2015. [90] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, pages 586595, 2018. 7 [91] X. Zhang, Q. Chen, R. Ng, and V. Koltun. Zoom to learn, learn to zoom. In CVPR, pages 37623770, 2019. 2 [92] Y. Zhang, K. Zhang, Z. Chen, Y. Li, R. Timofte, J. Zhang, K. Zhang, R. Peng, Y. Ma, L. Jia, et al. Ntire 2023 challenge on image super-resolution (x4): Methods and results. In CVPRW, pages 18641883, 2023. 2 [93] Z. Zheng, X. Peng, and Y. You. Open-sora: Democratizing efficient video production for all, 2024. 5 [94] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba. Scene parsing through ade20k dataset. In CVPR, pages 633641, 2017. 9 [95] X. Zhou, H. Huang, R. He, Z. Wang, J. Hu, and T. Tan. Msra-sr: Image super-resolution transformer with multi-scale shared representation acquisition. In ICCV, pages 1266512676, 2023. 3 [96] X. Zhou, H. Huang, Z. Wang, and R. He. Ristra: Recursive image super-resolution transformer with relativistic assessment. TMM, 26(8):64756487, 2024. [97] X. Zhou, J. Li, Z. Wang, R. He, and T. Tan. Image inpainting with contrastive relation network. In ICPR, pages 44204427, 2021."
        },
        {
            "title": "A Appendix",
            "content": "A.1 More Implementation Details For generating negative samples, the strength in SDEdit [50] is set to 0.6. To minimize the risk of generating images that contain private information in GenIR, we employ Gemini for both text prompt filtering and generated image filtering. The prompts for Gemini are set as You are an AI language assistant, and you are analyzing series of text prompts. Your task is to identify whether these text prompts contain any inappropriate content such as personal privacy violations or NSFW material. Delete any inappropriate text prompts and return the remaining ones in their original format. and You are an AI visual assistant, and you are analyzing single image. Your task is to check the image for any anomalies, irregularities, or content that does not align with common sense or normal expectations. Additionally, identify any inappropriate content such as personal privacy violations or NSFW material. If the image does not contain any of the aforementioned issues, it has passed the inspection. Please determine whether this image has passed the inspection (answer yes/no) and provide your reasoning., respectively. A.2 More Details of User Study To evaluate our approach, we conducted user study emphasizing restoration quality. We randomly selected 100 low-quality images from the test sets (Tab. 1), applying our model and five other leading methods to produce restored images, generating 100 groups of seven images each. Users, guided by the low-quality image, were asked to select the best-restored image from each group, considering factors such as visual quality, naturalness, detail accuracy, and absence of distortions or artifacts. Fairness was ensured by presenting each user with 10 randomly selected groups, randomizing image sequence within each group, and masking the methods. We widely disseminated the online questionnaire without restrictions, amassing feedback from 256 evaluators. We employed two metrics to quantify our studys results: vote percentage and Top-K ratio. The former represents the proportion of total votes each method received, while the latter measures the frequency method was among the top-k selections, indicating the proportion of most preferred images and models consistency in producing high-quality images. The Top-K ratio is defined as: 1(i Ftopk(sj, k)), where sj = {siji = 0, ..., 5} are the selection scores of the Rk j-th group from groups (with = 100), and Ftopk is the top-k operation. = 1 (cid:80)N j=1 As shown in Fig. 5, our model outperformed others on both metrics. Our model received over 45% of the total votes, indicating strong user preference. Additionally, our method was the top choice for 80% of the images, and it was ranked first or second for 98% of the images, highlighting our methods reliable ability to generate superior quality images compared to other methods. A.3 More Ablations More analysis of DreamClear Ablation. We give more detailed analysis in the following. (a) Mixture of Adaptive Modulator (MoAM). MoAM acts as an interaction module between the LQ branch and the reference branch, aiming to enhance the models robustness to intricate real-world degradations by explicitly guiding it through the introduction of token-wise degradation priors. It obtains the degradation map through cross-attention between the LQ features and reference features, guiding the dynamic fusion of expert knowledge to modulate LQ features. Tab. 3 presents the ablation studies of MoAM. Notably, when we substitute the Mixture of AM design with AM, all metrics undergo substantial decrease, underscoring the importance of the degradation prior guidance in MoAM for steering restoration experts. Moreover, we conducted experiments replacing AM with cross-attention and zero-linear layer. The use of cross-attention, in comparison to AM, leads to general reduction in model performance. While the zero-linear layer provides minor improvements in MANIQA and MUSIQ scores, it results in significant drop in perceptual fidelity. (b) Dual-branch framework. The incorporation of reference branch allows the model to focus less on degradation removal and more on enhancing image details through generative priors, ultimately producing more photorealistic images. The results in Tab. 3 indicate that our dual-branch structure significantly outperforms using only the LQ branch across all metrics. 15 Figure 7: Visual comparisons for ablation study on DreamClear. Table 4: Ablations for GenIR on LSDIR-Val using SwinIR-GAN. Training Data LPIPS DISTS FID MANIQA MUSIQ CLIPIQA Pre-trained T2I Model (3450images) Ours GenIR (3450images) 0.4819 0. 0.2790 0.2435 60.12 51.29 0.3271 0.3691 61.94 63.12 0.5423 0.5647 (c) Text prompt guidance. We evaluate the impact of using detailed text prompts generated by MLLMs versus null prompts. Despite null prompts slightly outperforming detailed prompts on perceptual metrics, the superior performance on no-reference and high-level vision metrics indicates that text prompts more effectively retain semantic information. Visual comparisons in Fig. 7 further underscore the advantages of text prompts in providing semantic guidance for image restoration. Visual results for DreamClear Ablation. Following the setting in Tab. 3, we provide more visual comparison results in Fig. 7. We find that when using null prompt instead of text prompt generated by MLLM, there are significant semantic errors in the eyes of the bear in the restoration results. This demonstrates that the semantic information provided by MLLM-generated detailed text prompts helps the model achieve more ideal restoration results. When using AM, zero-linear, and cross-attention instead of MoAM, the model tends to produce results that are either too smooth or contain semantic errors, proving the effectiveness of MoAM. Removing the reference branch results in significant deterioration of the restoration outcomes. Overall, our full model, DreamClear, achieves the best results in terms of fidelity and perception. Ablations for GenIR. We use the exact same prompts and sampling parameters to compare images generated by GenIR with those generated by the originally pre-trained T2I model. As shown in Fig. 10, the images generated by our proposed GenIR are more realistic and contain more texture details, while those generated by the pre-trained T2I model tend to have issues like being overly smooth and blurry. Intuitively, images generated by GenIR are likely to be more helpful for realworld IR. To verify this, following the setting of Fig. 6, we use manually designed prompts with the pre-trained T2I model to generate 3450 images for training SwinIR-GAN. As shown in Tab. 4, the model trained using images generated by our GenIR shows significant improvements across all metrics, quantitatively demonstrating the effectiveness of our approach. In addition, we provide visual comparisons in Fig. 8 to verify the effectiveness of dual-prompt learning in GenIR. It shows that the dual-prompt learning strategy can effectively enhance image texture details, making the generated images more suitable for image restoration training. Fig. 9 also demonstrates the effectiveness of our generated data in enhancing the visual effect of IR models. 16 Figure 8: Visual comparisons for ablation study on GenIR. Figure 9: Visual comparisons for ablation study on training datasets. A.4 More Real-World Visual Comparisons We provide more real-world visual comparisons with state-of-the-art diffusion-based real-world image restoration methods in Fig. 11, Fig. 12 and Fig. 13. 17 Figure 10: Visual comparison of images generated using the pre-trained T2I model and GenIR. Our proposed GenIR produces images with enhanced texture and more realistic details, exhibiting less blurring and distortion. This makes it better suited for training real-world IR models."
        },
        {
            "title": "LQ Input",
            "content": "StableSR [63] DiffBIR [46] SeeSR [70] SUPIR [80] DreamClear (Ours)"
        },
        {
            "title": "LQ Input",
            "content": "StableSR [63] DiffBIR [46] SeeSR [70] SUPIR [80] DreamClear (Ours) Figure 11: Visual comparisons on real-world benchmarks (1/3). Please zoom in for better view."
        },
        {
            "title": "LQ Input",
            "content": "StableSR [63] DiffBIR [46] SeeSR [70] SUPIR [80] DreamClear (Ours)"
        },
        {
            "title": "LQ Input",
            "content": "StableSR [63] DiffBIR [46] SeeSR [70] SUPIR [80] DreamClear (Ours) Figure 12: Visual comparisons on real-world benchmarks (2/3). Please zoom in for better view."
        },
        {
            "title": "LQ Input",
            "content": "StableSR [63] DiffBIR [46] SeeSR [70] SUPIR [80] DreamClear (Ours)"
        },
        {
            "title": "LQ Input",
            "content": "StableSR [63] DiffBIR [46] SeeSR [70] SUPIR [80] DreamClear (Ours) Figure 13: Visual comparisons on real-world benchmarks (3/3). Please zoom in for better view."
        }
    ],
    "affiliations": [
        "ByteDance, Inc",
        "MAIS & NLPR, Institute of Automation, Chinese Academy of Sciences",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "University of Science and Technology of China"
    ]
}