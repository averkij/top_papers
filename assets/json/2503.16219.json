{
    "paper_title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't",
    "authors": [
        "Quy-Anh Dang",
        "Chris Ngo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at https://github.com/knoveleng/open-rs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 9 1 2 6 1 . 3 0 5 2 : r Preprint. Under review. Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesnt Quy-Anh Dang1,2, Chris Ngo2 1VNU University of Science, Vietnam 2Knovel Engineering Lab, Singapore dangquyanh150101@gmail.com, chris.ngo@knoveleng.com"
        },
        {
            "title": "Abstract",
            "content": "Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gainse.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-previewusing only 7,000 samples and $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at https://github.com/knoveleng/open-rs."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) have significantly advanced the pursuit of artificial general intelligence (AGI), with models such as GPT-4o (OpenAI, 2024a), Claude 3.5 Sonnet (Anthropic, 2024), and Gemini 1.5 (Google, 2024) demonstrating unprecedented capabilities. pivotal aspect of this progress is the integration of post-training techniques into the training pipeline. These methodsincluding supervised fine-tuning (SFT) and reinforcement learning (RL)enhance reasoning accuracy, align models with societal values, and adapt them to user preferences, all while demanding fewer computational resources than pre-training (OpenAI, 2024b). notable innovation in this domain is OpenAIs o1 series, which leverages inference-time scaling through extended Chain-of-Thought (CoT) reasoning to achieve remarkable performance in mathematics, coding, and scientific reasoning tasks (OpenAI, 2024b). However, despite these breakthroughs, scaling reasoning capabilities at test time remains persistent challenge for the broader research community, largely due to limited access to proprietary methodologies and resources. Efforts to bolster LLM reasoning have explored diverse strategies. Process-based reward models (Uesato et al., 2022; Lightman et al., 2023a; Wang et al., 2023) guide models toward structured problem-solving, while RL approaches (Kumar et al., 2024) optimize performance through feedback-driven learning. Search algorithms, such as Monte Carlo Tree Search (MCTS) and Beam Search, have also been employed to enhance reasoning depth (Feng et al., 2024; Xin et al., 2024; Trinh et al., 2024). Although these methods have driven incremental gains, they fall short of the general reasoning prowess exhibited by the o1 series. Recently, the DeepSeek-R1 model (DeepSeek-AI, 2025) has emerged as competitive 1 Preprint. Under review. alternative, utilizing RL with the Group Relative Policy Optimization (GRPO) algorithm. Built on the 671-billion-parameter DeepSeek-V3, DeepSeek-R1 matches o1s reasoning performance (DeepSeek-AI, 2025). Yet, the sheer scale and computational demands of such modelsoften exceeding hundreds of billions of parametersrender them impractical for self-hosting by most organizations outside major technology firms, limiting their broader adoption. In contrast, small LLMs, typically ranging from 1 to 10 billion parameters, present resourceefficient alternative with potential for widespread deployment. Previous studies have demonstrated the feasibility of enhancing small LLMs through RL-based fine-tuning inspired by DeepSeek-R1 (Luo et al., 2025; Team, 2025b). However, these efforts often rely on expansive datasets (hundreds of thousands to millions of samples) or incur significant computational costs, undermining their accessibility for resource-constrained settings. This tension motivates two central research questions: 1. How do small LLMs behave when fine-tuned under strict resource constraints, such as limited computational power and training time? 2. Can their reasoning performance be elevated using an RL-based approach akin to DeepSeekR1s methodology, and if so, how? These questions naturally extend to practical inquiry: If viable, how should such an approach be implemented for small LLMs, and if not, what are the fundamental limitations? Addressing these, we investigate the reasoning capacity of 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under stringent constraints: training on cluster of 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24-hour window. Our methodology adapts the GRPO-based RL framework from DeepSeek-R1, tailoring it to the resourcelimited context of small LLMs. We assess performance on suite of mathematical reasoning benchmarks, domain requiring structured, logical problem-solving that serves as robust testbed for reasoning ability. Figure 1: Comparison of zero-shot pass@1 performance versus model size (left) and computational cost (right). Our Open-RS (red point) achieves the highest AIME24 score (46.7%), outperforming o1-preview (44.6%) and other models (green points). Additionally, Open-RS models exhibit the lowest computational cost at approximately $42. Our study yields three primary contributions: 1. We systematically analyze the reasoning potential of small LLMs under specific computational constraints, providing practical lens on their scalability and deployment feasibility. 2. We offer actionable insights into the efficacy and challenges of RL-based fine-tuning for small LLMs, bridging the gap between theoretical advancements and real-world applicability. 3. We release our source code and curated datasets as open-source resources, fostering reproducibility and encouraging further exploration by the research community. 2 Preprint. Under review. Our findings illuminate the promise of RL-based methods to enhance small LLMs reasoning capabilities, achieving competitive performance with minimal resources (Figure 1). Simultaneously, they reveal critical challengessuch as data efficiency, optimization stability, and length constraintsthat must be addressed to fully realize this potential. These insights lay the groundwork for developing lightweight, reasoning-capable LLMs suitable for resource-constrained environments, advancing the democratization of advanced AI technologies. The remainder of this paper is structured as follows: Section 2 details our methodology, including data curation, RL algorithm, and reward design; Section 3 presents three experiments, their results, and comparative analyses; and Section 4 summarizes key findings. Additional details, including related work, discussion, hyperparameter setups, and supplementary results, are provided in the Appendix."
        },
        {
            "title": "2 Methodology",
            "content": "In this section, we outline our approach to optimizing the reasoning capabilities of small large language models (LLMs) under computational constraints. Our methodology comprises two primary components: (1) the curation of high-quality, mathematics-focused dataset, and (2) the application of resource-efficient reinforcement learning (RL) algorithm. These components are designed to balance performance gains with practical limitations, such as reduced computational overhead and privacy considerations. 2.1 High-Quality Dataset Curation To minimize training costs while maximizing reasoning performance, we curate compact, high-quality dataset tailored to mathematical reasoning. This dataset is derived from two existing sources: the s1 dataset (Muennighoff et al., 2025) and the DeepScaleR dataset (DeepSeek-AI, 2025). By filtering and refining these datasets, we ensure that our training data is both relevant and challenging, enabling efficient learning for small LLMs. s1 Dataset The s1 dataset (Muennighoff et al., 2025) is general-purpose reasoning corpus comprising 59,029 questions sourced from diverse domains, including NuminaMATH (LI et al., 2024), AIME problems (19832021), OlympicArena (Huang et al., 2024), OmniMath (Gao et al., 2024), AGIEval (Zhong et al., 2023), probability questions from Stanford Universitys Statistics Department PhD Qualifying Exams (https://statistics. stanford.edu), and brain-teasers from PuzzledQuant (https://www.puzzledquant.com). Although the dataset spans multiple disciplinessuch as Astronomy, Biology, Chemistry, Computer Science, Geography, Mathematics, and Physicsour focus is exclusively on mathematical reasoning. To isolate mathematics-specific examples, we adopt filtering workflow inspired by (Muennighoff et al., 2025). First, we retain only questions with solutions containing the LaTeX command boxed{}, common indicator of mathematical answers, reducing the dataset to 31,323 examples. Next, we employ the distilled model DeepSeek-R1-Distill-Qwen-1.5B to eliminate trivial questions, yielding 21,533 examples. Finally, to ensure data quality, we use Qwen2.5-7B-Instruct to remove noisy or multi-part questions, resulting in final set of 18,615 high-quality mathematical reasoning examples open-s1 dataset. DeepScaleR Dataset The DeepScaleR dataset (Luo et al., 2025) contains 40,315 mathematics-specific questions drawn from AIME (19842023), AMC (prior to 2023), OmniMATH, and the Still dataset. Unlike the s1 dataset, DeepScaleR is pre-filtered to focus solely on mathematics, with redundant questions removed and solutions extracted from raw text using retrieval-augmented generation (RAG) and advanced LLMs like Gemini-1.5-Pro-002. To further refine this dataset, we apply Qwen2.5-Math-7B-Instruct to exclude easy questions, reducing the set to 21,044 examples open-deepscaler dataset. We opt for Qwen2.5-Math-7B-Instruct over DeepSeek-R1-Distill-Qwen-1.5Bused for the s1 datasetto introduce diversity in filtering criteria and avoid excessive overlap between the two datasets. 3 Preprint. Under review. Final Dataset Combining the refined open-s1 dataset (18,615 examples) and opendeepscaler (21,044 examples), we obtain final high-quality dataset of 39,659 mathematical reasoning questions. This curated corpus strikes balance between scale and specificity, enabling effective training of small LLMs under resource constraints. 2.2 Reinforcement Learning Algorithm To train small LLMs efficiently, we adopt the Group Relative Policy Optimization (GRPO) algorithm Shao et al. (2024), as utilized in DeepSeek-AI (2025). GRPO eliminates the need for separate critic modeltypically as large as the policy modelby estimating baselines from group scores, thereby reducing computational overhead. For each question q, GRPO samples group of outputs {o1, o2, . . . , oG} from the old policy πθold and optimizes the policy πθ by maximizing the following objective: JGRPO(θ) = (Oq)] [qP(Q),{oi}G (cid:18) i=1 πθold (cid:18) πθ(oiq) (oiq) πθold min 1 G i=1 Ai, clip (cid:18) πθ(oiq) (oiq) πθold , 1 ϵ, 1 + ϵ (cid:19) (cid:19) Ai βD KL(πθπref) (cid:19) , where the KL-divergence term is defined as: KL(πθπref) = πref(oiq) πθ(oiq) log πref(oiq) πθ(oiq) 1, and the advantage Ai is computed from group of rewards {r1, r2, . . . , rG}: Ai = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) . (1) (2) (3) Here, ϵ and β are hyperparameters controlling the clipping range and KL penalty, respectively. Reward Models The reward function is critical to guiding RL optimization. We employ rule-based reward system comprising three components, designed to balance correctness, efficiency, and structure without relying on resource-intensive neural reward models: Accuracy Reward: This evaluates whether the models response is correct, requiring the final answer to be presented in boxed{} format for reliable verification. binary score (1 for correct, 0 for incorrect) ensures simplicity and objectivity. Cosine Reward: This augments the accuracy reward by scaling it based on response length using cosine schedule. Shorter correct solutions receive higher rewards, while longer incorrect solutions are penalized less severely, incentivizing concise yet accurate reasoning. Format Reward: This enforces structural clarity by requiring the model to encapsulate its reasoning process within <think> and </think> tags, awarding positive score for compliance."
        },
        {
            "title": "3 Experiments",
            "content": "To address the research questions outlined in Section 1namely, how reinforcement learning (RL) can enhance the reasoning abilities of small large language models (LLMs) and what practical insights emerge under computational constraintswe design three experiments to analyze the training behavior of small LLMs. These experiments aim to provide empirical evidence of performance improvements and offer actionable guidance for future research and industrial applications. 4 Preprint. Under review. 3.1 Experimental Setup We select DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI, 2025) as our base model for training. This 1.5-billion-parameter model, distilled from larger architectures, is chosen for its balance of efficiency and reasoning potential. Notably, we bypass the supervised finetuning (SFT) phasetypically precursor to RL for performance enhancement (Chu et al., 2025)hypothesizing that the models pretraining is sufficient to leverage RL directly. For the RL phase, we employ the Group Relative Policy Optimization (GRPO) algorithm, as detailed in Section 2.2, due to its computational efficiency. Training is conducted on cluster of 4 NVIDIA A40 GPUs (48GB VRAM each), imposing constraints that limit us to sampling 6 outputs per step with maximum completion length of 4096 tokens. To facilitate this, we adapt open-r1 (Face, 2025), an open-source reproduction of DeepSeek-R1 by the Hugging Face team, customizing it to align with our objectives. The training phase is restricted to 1 epoch, completed within 24-hour window, reflecting realworld resource limitations. Hyperparameters and additional configurations are detailed in Appendix E. 3.2 Benchmark Datasets To evaluate the reasoning capabilities of our small LLM, we choose five mathematics-focused benchmark datasets: AIME24 1, MATH-500 (Lightman et al., 2023b; Hendrycks et al., 2021), AMC23 2, Minerva (Lewkowycz et al., 2022b) and OlympiadBench (He et al., 2024). Details of the datasets are provided in Appendix C. 3.3 Baseline Models Llama-3.1-70B-Instruct To contextualize our results, we compare our trained model against range of baselines: 2024b), Qwen-2.5-Math-7B-Instruct (Yang et al., 2024), rStar-Math-7B (Guan et al., 2025), Eurus-2-7B-PRIME, Qwen2.5-7B-SimpleRL (Zeng et al., 2025) (Cui et al., 2025), DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI, 2025), DeepScaleR-1.5B-Preview (Luo et al., 2025), Still-3-1.5B-Preview (Team, 2025b). o1-preview 2024a), (AI, (AI, This selection enables robust comparison across model sizes, training methodologies, and reasoning strategies, highlighting the efficacy of our approach for small LLMs. Details of the baselines are provided in Appendix D."
        },
        {
            "title": "3.4 Evaluation Metric",
            "content": "We adopt the zero-shot pass@1 metric to measure performance, defined as the proportion of problems correctly solved on the first attempt without prior examples. This metric emphasizes the models ability to reason independently, aligning with our goal of enhancing intrinsic reasoning capabilities in small LLMs. Final answers are required in boxed{} format for consistent automated evaluation. 3.5 Process and Results In this subsection, we present three experiments designed to enhance the reasoning abilities of small LLMs using reinforcement learning (RL), follow the methodology in Section 2. We analyze training progress, evaluate performance across benchmarks, and compare our models against baselines, highlighting key insights and their implications for future work. 1https://huggingface.co/datasets/AI-MO/aimo-validation-aime 2https://huggingface.co/datasets/AI-MO/aimo-validation-amc 5 Preprint. Under review. Figure 2: Performance of the model on AMC23 (left) and MATH-500 (right) across global training steps. The red dashed line indicates the baseline score at the start of training."
        },
        {
            "title": "3.5.1 Experiment 1: Impact of High-Quality Data",
            "content": "In Experiment 1, we train the DeepSeek-R1-Distill-Qwen-1.5B model using the open-s1 dataset (18,615 samples) from Section 2.1, with maximum completion length of 4096 tokens. We employ accuracy and format rewards, as described in Section 2.2. Although the full dataset corresponds to approximately 1500 global steps for one epoch, computational constraints (24-hour limit on 4x A40 GPUs) restrict training to 500 global steps. Performance on AMC23 improves from 63% to 70% and on MATH-500 from 83% to 84% within the first 50100 steps (see Figure 2). However, after 200 steps, accuracy degrades significantly, dropping below 60% on AMC23 and to 80% on MATH-500. Figure 3 illustrates this trend, showing unstable accuracy rewards and completion lengths fluctuating near 4000 tokens initially, then decreasing to around 3000 tokens by 100 global steps (approximately 3000 local steps on single GPU). Post-200 steps, lengths increase again, accompanied by unreadable content and non-English outputs. Figure 3: Accuracy reward (left) and completion length (right) of outputs in Experiment 1 across local steps. Note that global steps are distributed across 4 GPUs, with 100 global steps approximating 3000 local steps. This degradation suggests that the model struggles with the complexity of open-s1, often exceeding the 4096-token limit before producing final answer. The initial length reduction reflects adaptation to the format reward, but the subsequent increase and language drift indicate reward misalignment. We derive the following insight: Insight 1 Small LLMs can achieve rapid reasoning improvements with limited high-quality data within 50100 steps, but performance degrades with prolonged training under strict length constraints. 3.5.2 Experiment 2: Balancing Easy and Hard Problems Building on Experiment 1, we hypothesize that mixing easier problems with challenging ones could stabilize training and reduce completion lengths. We construct dataset of 7000 6 Preprint. Under review. samples: 3000 from open-s1, 3000 from open-deepscaler, and 1000 easier problems from the raw DeepScaleR dataset (Section 2.1). The maximum completion length is reduced to 3584 tokens, retaining accuracy and format rewards. Initial completion lengths drop to approximately 2800 tokens, and performance improves significantly: AMC23 rises from 63% to 80%, and MATH-500 from 83% to 85% within 50100 steps (Figure 2). However, after 150200 steps (approximately 4000 local steps), performance declines, and KL divergence becomes unstable (Figure 4), with mixed-language outputs reemerging. Figure 4: KL divergence (left) and completion length (right) of outputs in Experiment 2 across local steps. The improved initial performance validates our hypothesis, suggesting that easier problems encourage concise reasoning, while harder ones maintain complexity. However, the latestage instability highlights persistent challenges with length constraints and multilingual tendencies. We note: Insight 2 Incorporating mix of easy and hard problems under reduced length constraints enhances early performance and stabilizes reasoning behavior, though long-term stability remains elusive. 3.5.3 Experiment 3: Controlling Length with Cosine Reward Experiment 3 uses the same 7000-sample dataset as Experiment 2, but replaces the accuracy reward with cosine reward to better control output length, as outlined in Section 2.2. We also append an instruction to the system prompt: Reply in English only, do not use other languages, avoiding computationally expensive language reward function. The maximum completion length remains 3584 tokens. Completion lengths stabilize between 1000 and 3500 tokens (Figure 5), marked improvement over Experiment 2s 20003500 range. Performance on AMC23 and MATH500 increases modestly compared to the baseline (63% to 72.5% and 83% to 84.4%, respectively) within 50 steps, though it lags behind Experiment 2s peak (Figure 2). After 200 steps, mixed-language content persists, reflecting the multilingual nature of DeepSeek-R1-Distill-Qwen-1.5B. The cosine reward effectively regulates length, but the language issue suggests need for explicit language constraints or extended completion lengths for complex tasks. We conclude: Insight 3 Cosine rewards stabilize completion lengths, improving training consistency, but extending length limits is necessary for extremely hard tasks, particularly with multilingual base models. 7 Preprint. Under review. Figure 5: KL divergence (left) and completion length (right) of outputs in Experiment 3 across local steps."
        },
        {
            "title": "3.5.4 Overall Comparison",
            "content": "We select checkpoints at 100, 50, and 50 global steps from Experiments 1, 2, and 3, naming them Open-RS1, Open-RS2, and Open-RS3 (R for Reasoning, for Small), respectively. These are evaluated against baselines from Section 3.3 across benchmarks from Section 3.2, using zero-shot pass@1  (Table 1)  . Model General Models Llama-3.1-70B-Instruct o1-preview 7B Models Qwen-2.5-Math-7B-Instruct rStar-Math-7B Eurus-2-7B-PRIME Qwen2.5-7B-SimpleRL 1.5B Models DeepSeek-R1-Distill-Qwen-1.5B Still-3-1.5B-Preview DeepScaleR-1.5B-Preview Our Models Open-RS1 (100 steps) Open-RS2 (50 steps) Open-RS3 (50 steps) AIME24 MATH-500 AMC23 Minerva OlympiadBench Avg. 16.7 44. 13.3 26.7 26.7 26.7 28.8 32.5 43.1 30.0 30.0 46.7 64.6 85.5 79.8 78.4 79.2 82.4 82.8 84.4 87. 83.8 85.4 84.4 30.1 50.6 47.5 57.8 62.5 62.9 66.7 73.6 70.0 80.0 72.5 35.3 34.6 38.6 39.7 26.5 29.0 30.2 29.0 30.5 26.8 31.9 40.7 47.1 42.1 43.3 43.3 45.4 50. 52.4 52.4 51.3 35.7 43.8 48.9 50.9 48.9 51.6 57.0 53.0 55.7 56.3 Table 1: Zero-shot pass@1 performance across benchmarks. Bold indicates the highest score per benchmark. Dashes () denote unavailable official scores. Scores for o1-preview are sourced from AI (2024b); others from Zeng et al. (2025); Luo et al. (2025). Our models are evaluated using the lighteval package Fourrier et al. (2023). Our models outperform most baselines, with average scores of 53.0% (Open-RS1), 55.7% (Open-RS2), and 56.3% (Open-RS3), compared to 57.0% for DeepScaleR-1.5B-Preview. Notably, Open-RS3 achieves the highest AIME24 score (46.7%), surpassing o1-preview (44.6%) and DeepScaleR-1.5B-Preview (43.1%). Open-RS2 excels on AMC23 (80.0%) and ties with Open-RS1 on OlympiadBench (52.4%), both outperforming DeepScaleR-1.5B-Preview. MATH-500 scores remain competitive, though Minerva performance lags behind 7B models, reflecting the complexity of cross-disciplinary reasoning. We further compare training costs3 and data efficiency (Tables 2 and 3, and Figure 1). Our approach, using 7000 samples with 6 outputs per step (42,000 total samples), costs approxi3Cost estimates are calculated based on pricing from https://www.runpod.io/pricing. 8 Preprint. Under review. rStar-Math-7B Eurus-2-7BPRIME Qwen2.5-7BSimpleRL Open-RS Base Model Qwen2.5-Math-7B Qwen2.5-Math-7B Qwen2.5-Math-7B DeepSeek-R1SFT Data RM Data RM RL Data Hardware Time 7.3M 7k None 3.647M 16 10x 8 H100 80GB, 15x 4 A100 40GB Cost Est. 230k 0 Eurus-2-7B-SFT 150k 4 1x 8 A100 80GB 0 0 None 8k 8 4x 6 A100 80GB Distill-Qwen-1.5B 0 0 None 7k 6 1x 4 A40 48GB 72h $1088 36h $1633 24h $ Table 2: Comparison of data usage and training costs for 7B models. Data are sourced from original papers or GitHub issues addressing authors resource constraints. mately $42 on 4x A40 GPUs over 24 hours. In contrast, 7B models like Qwen2.5-7B-SimpleRL ($1633) and Eurus-2-7B-PRIME ($1088) and 1.5B models like DeepScaleR-1.5B-Preview ($3629) and Still-3-1.5B-Preview ($2268) require significantly more resources and data (e.g., 40k 16 samples for DeepScaleR). DeepScaleR-1.5BPreview Still-3-1.5B-Preview Open-RS Base Model DeepSeek-R1-DistillSFT Data RM Data RM RL Data Hardware Time Qwen-1.5B 0 0 None 40k 16 8x A100 80GB 240h DeepSeek-R1-DistillQwen-1.5B 0 0 None 30k 8 1x 8 A100 80GB 150h DeepSeek-R1-DistillQwen-1.5B 0 0 None 7k 6 1x 4 A40 48GB 24h Cost Est. $ $2268 $42 Table 3: Comparison of data usage and training costs for 1.5B models. Data are sourced from original papers or GitHub issues addressing authors resource constraints. Our approach demonstrates that small LLMs can achieve competitive reasoning performance with minimal data and cost, offering scalable alternative to resource-intensive baselines."
        },
        {
            "title": "4 Conclusion",
            "content": "Our study investigated enhancing the reasoning abilities of small LLMs using RL, focusing on the 1.5-billion-parameter DeepSeek-R1-Distill-Qwen-1.5B under strict constraints. Adapting the GRPO algorithm and compact mathematical reasoning dataset, we conducted three experiments to assess behavior and performance under resource limitations. Our findings show small LLMs can achieve significant reasoning gains with minimal resourcese.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-previewat cost of $42 versus thousands for baselines. Open-RS variants averaged 53.0%56.3% on benchmarks, demonstrating RLs viability for small LLMs. Releasing our code and datasets, we provide framework for lightweight, reasoning-capable models, despite challenges like optimization stability, laying foundation for future work. 9 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Meta AI. Introducing llama 3.1: Our most capable models to date, 2024a. URL https: //ai.meta.com/blog/meta-llama-3-1/. Published on July 23, 2024. Open AI. Introducing openai o1-preview, 2024b. URL https://openai.com/index/ introducing-openai-o1-preview/. Published on Dec 12, 2024. Anthropic. Claude 3.5 sonnet, 2024. claude-3-5-sonnet. URL https://www.anthropic.com/news/ Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. URL https://arxiv.org/abs/2501.17161. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. URL https://arxiv.org/abs/2502.01456. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/huggingface/open-r1. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https://arxiv.org/abs/2309.17179. Clementine Fourrier, Nathan Habib, Hynek Kydlıˇcek, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/ huggingface/lighteval. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/ technology/ai/google-gemini-next-generation-model-february-2024. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.211. Preprint. Under review. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, and Pengfei Liu. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai, 2024. URL https://arxiv.org/abs/2406.12753. Large language models are zero-shot reasoners. Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2219922213. Curran Associates, URL https://proceedings.neurips.cc/paper files/paper/2022/file/ Inc., 2022. 8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022a. URL https://openreview.net/forum?id=IFXTZERXdM7. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b. URL https://openreview.net/forum?id=IFXTZERXdM7. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath, URL https://github.com/project-numina/aimo-progress-prize/blob/main/ 2024. report/numina dataset.pdf. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023a. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023b. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://github.com/ agentica-project/deepscaler, 2025. Github. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir YazdanIn bakhsh, and Peter Clark. Self-refine: A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4653446594. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper files/paper/2023/ file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf. Iterative refinement with self-feedback. 11 Preprint. Under review. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114. OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/ learning-to-reason-with-llms/. Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain In Anna Koyourself! leveraging language models for commonsense reasoning. rhonen, David Traum, and Lluıs M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 49324942, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1487. URL https://aclanthology.org/P19-1487/. Laria Reynolds and Kyle McDonell. Prompt programming for large language models: In Extended Abstracts of the 2021 CHI Conference on Beyond the few-shot paradigm. Human Factors in Computing Systems, CHI EA 21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380959. doi: 10.1145/3411763.3451760. URL https://doi.org/10.1145/3411763.3451760. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025a. URL https: //arxiv.org/abs/2501.12599. RUCAIBox STILL Team. Still-3-1.5b-preview: Enhancing slow thinking abilities of small models through reinforcement learning, 2025b. URL https://github.com/RUCAIBox/ Slow Thinking with LLMs. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper files/paper/ 2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf. Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https: //arxiv.org/abs/2408.08152. Preprint. Under review. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/ 2409.12122. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. Edward Yeo, Yuxuan Tong, Xinyao Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in LLMs. In Scaling Self-Improving Foundation Models without Human Supervision, 2025. URL https://openreview.net/forum?id=6A861u4Crm. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id= 3ELRdg2sgI. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models, 2023. URL https://arxiv.org/abs/2304.06364. 13 Preprint. Under review."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Reasoning in Large Language Models substantial body of research has investigated methods to enhance the reasoning capabilities and factual accuracy of large language models (LLMs). Early approaches predominantly relied on prompting techniques to elicit structured reasoning. For instance, scratchpad-style prompting encourages models to break down problems into intermediate steps (Nye et al., 2021), while verification mechanisms assess the correctness of generated outputs (Cobbe et al., 2021). Chain-of-thought (CoT) prompting has emerged as particularly effective strategy, leveraging demonstrations of step-by-step reasoning to improve performance on complex tasks (Wei et al., 2022; Kojima et al., 2022; Reynolds & McDonell, 2021). More recently, techniques such as intermediate self-reflection have been proposed to enable models to iteratively refine their reasoning processes (Shinn et al., 2023; Madaan et al., 2023). In parallel, supervised fine-tuning (SFT) has been employed to embed reasoning capabilities directly into LLMs. Studies such as (Lewkowycz et al., 2022a) and (Rajani et al., 2019) demonstrate that fine-tuning on high-quality datasets can enhance problem-solving abilities. Notably, integrating CoT reasoning into SFT has shown significant promise; works like (Zelikman et al., 2022; Muennighoff et al., 2025; Ye et al., 2025) illustrate that fine-tuning with small, carefully curated datasets of CoT examples can yield substantial performance gains. However, these efforts have predominantly focused on large-scale LLMs, typically ranging from 7 billion to over 100 billion parameters. This reliance on massive models limits accessibility and practicality for resource-constrained settings, motivating the exploration of alternative approaches for smaller LLMs. A.2 Reasoning with Reinforcement Learning Reinforcement learning (RL) has emerged as powerful paradigm for improving reasoning in LLMs, particularly for tackling complex, multi-step problems. Unlike SFT, which often optimizes for imitation of training data, RL enables models to learn from feedback, enhancing generalization to both in-distribution and out-of-distribution tasks (Chu et al., 2025; Yeo et al., 2025). Recent advancements underscore the efficacy of RL in this domain. For example, OpenAI (2024b) and DeepSeek-AI (2025) demonstrate that RL-based training can significantly boost reasoning performance, while Team (2025a) explores scaling laws for RL-driven LLMs. These studies highlight RLs ability to refine decision-making processes by optimizing for task-specific rewards, such as correctness or logical coherence. Despite these advances, RL-based methods are not without limitations. They typically demand substantial computational resources, often exceeding those required for SFT, and are predominantly applied to large LLMs. This focus on scale renders RL impractical for smaller models and restricts its adoption outside well-resourced organizations, such as major technology firms. Furthermore, privacy concerns arise when deploying such models, as self-hosting becomes infeasible for most academic or industrial entities with limited infrastructure. Consequently, there remains critical gap in the literature: the application of RL to enhance reasoning in small LLMs under resource and privacy constraints. Limitations & Discussion While our study demonstrates the promise of RL-based fine-tuning for enhancing the reasoning abilities of small LLMs, several limitations and broader implications warrant discussion. These insights not only contextualize our findings but also highlight avenues for future research. B.1 Limitations First, our experiments were constrained by 24-hour training window on modest cluster of 4 NVIDIA A40 GPUs (48 GB VRAM each), limiting the number of global steps (e.g., 500 in Experiment 1 versus potential 1500 for one epoch). This restriction curtailed our 14 Preprint. Under review. ability to fully explore the long-term behavior of the model, particularly beyond 200 steps, where performance degradation and multilingual outputs emerged. Second, the maximum completion length (4096 tokens in Experiment 1, reduced to 3584 in Experiments 2 and 3) proved insufficient for extremely hard problems in the open-s1 dataset, forcing the model to truncate reasoning processes prematurely. This suggests that our methodology may underexploit the potential of small LLMs on complex tasks requiring extended reasoning chains. Third, the multilingual nature of the base model, DeepSeek-R1-Distill-Qwen-1.5B, introduced unintended language drift after 150200 steps, despite efforts to enforce English-only outputs via prompts in Experiment 3. This limitation reflects trade-off in using pretrained, multilingual foundation, which, while efficient, complicates monolingual optimization. Finally, our evaluation focused exclusively on mathematical reasoning benchmarks, leaving the generalizability of our approach to other domainssuch as scientific reasoning or codingunexplored. These constraints highlight the need for cautious interpretation of our results within the specified scope. B.2 Discussion Our findings reveal nuanced trade-off between efficiency and reasoning depth in small LLMs. The rapid performance gains observed in the first 50100 steps across all experiments (Insight 1) suggest that small, high-quality datasets can effectively bootstrap reasoning capabilities, aligning with prior work on data efficiency in RL (Chu et al., 2025). However, the subsequent degradation underscores sensitivity to over-optimization under fixed length constraints, challenge also noted in larger models like DeepSeek-R1 (DeepSeekAI, 2025). Experiment 2s success with mixed difficulty levels (Insight 2) indicates that curriculum-like strategies could mitigate this. Meanwhile, the cosine rewards stabilizing effect in Experiment 3 (Insight 3) suggests promising direction for controlling reasoning verbosity, though it sacrifices peak accuracy compared to Experiment 2. Comparatively, our Open-RS variants achieved performance rivaling or exceeding state-ofthe-art 1.5B models (e.g., DeepScaleR-1.5B-Preview) and even some 7B models, at fraction of the cost and data volume. This efficiency challenges the prevailing reliance on massive datasets and computational resources in reasoning enhancement (OpenAI, 2024b; Luo et al., 2025), offering scalable alternative for resource-constrained environments. However, the persistent multilingual drift and length limitations point to inherent challenges in adapting multilingual base models and optimizing for complex tasks within tight bounds. B.3 Future Directions These limitations suggest several research avenues. First, extending training duration or employing multi-stage length schedules could address truncation issues, allowing the model to handle harder problems without compromising stability. Second, incorporating lightweight language reward or monolingual pre-filtering of the base model might mitigate language drift, enhancing output consistency. Third, expanding the benchmark suite to include non-mathematical domains would test the generalizability of our approach, aligning with broader AGI goals. Finally, exploring hybrid methodssuch as combining GRPO with search algorithms like MCTS (Feng et al., 2024)could further deepen reasoning capacity without significantly increasing resource demands. In conclusion, our work demonstrates that RL-based fine-tuning can unlock substantial reasoning potential in small LLMs, even under stringent constraints. By identifying key tradeoffs and offering practical insights, we pave the way for developing efficient, reasoningcapable models that balance performance and accessibilitya critical step toward democratizing advanced AI technologies."
        },
        {
            "title": "C Datasets",
            "content": "Detail datasets are used in Section 3.2. 15 Preprint. Under review. AIME24 4: 30 problems from the 2024 American Invitational Mathematics Examination, emphasizing advanced high-school-level reasoning. AMC23 5: 40 problems from the 2023 American Mathematics Competition, testing foundational mathematical skills. MATH-500 (Lightman et al., 2023b; Hendrycks et al., 2021): subset of 500 problems from the MATH benchmark, sourced from various mathematics competitions and spanning algebra, calculus, and geometry. Minerva (Lewkowycz et al., 2022b): 272 undergraduate-level problems across physics, biology, chemistry, economics, and other sciences, requiring quantitative reasoning (mathematics subset used). OlympiadBench (He et al., 2024): 675 Olympiad-level problems in mathematics and physics, designed to challenge advanced reasoning abilities. Table 4 summarizes the datasets and their sample sizes. This diverse collection ensures comprehensive assessment of the models reasoning generalization across problem types and difficulty levels. Table 4: Benchmark Datasets and Sample Sizes for Evaluation Dataset # Samples AIME24 MATH-500 AMC23 Minerva OlympiadBench 30 500 40"
        },
        {
            "title": "D Baseline Models",
            "content": "The description of baseline models in Section 3.3. General-Purpose Large Models: Llama-3.1-70B-Instruct (AI, 2024a): 70B-parameter model optimized for instruction-following. o1-preview (AI, 2024b): high-performing reasoning model from OpenAI. Mathematics-Focused 7B Models: Qwen-2.5-Math-7B-Instruct (Yang et al., 2024): An RL-trained model with reward model for mathematical reasoning. rStar-Math-7B (Guan et al., 2025): Uses Monte Carlo Tree Search (MCTS) for deep reasoning and self-evolution. Eurus-2-7B-PRIME (Cui et al., 2025): Employs the PRIME method with online RL and process rewards. Qwen2.5-7B-SimpleRL (Zeng et al., 2025): Applies Proximal Policy Optimization (PPO) with rewards based on final answers. Mathematics-Focused 1.5B Models: DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI, 2025): The original untrained baseline. DeepScaleR-1.5B-Preview (Luo et al., 2025): Fine-tuned with GRPO on 40, math problem-answer pairs across multiple RL stages. Still-3-1.5B-Preview (Team, 2025b): RL-trained with focus on slow thinking (e.g., tree search) using 30,000 curated math examples. 4https://huggingface.co/datasets/AI-MO/aimo-validation-aime 5https://huggingface.co/datasets/AI-MO/aimo-validation-amc 16 Preprint. Under review."
        },
        {
            "title": "E Hyperparameter Setup",
            "content": "Table 5 show parameters that used in training phase. Table 5: Hyperparameter Setups for GRPO Trainer Parameter Value General Settings bf16 use vllm vllm device vllm enforce eager vllm gpu memory utilization vllm max model len do eval output dir overwrite output dir true true auto true 0.7 4608 false data/OpenRS-GRPO true 4 true Training Configuration gradient accumulation steps gradient checkpointing gradient checkpointing kwargs use reentrant: false learning rate lr scheduler type lr scheduler kwargs warmup ratio max steps num train epochs per device train batch size per device eval batch size 1.0e-06 cosine with min lr min lr rate: 0.1 0.1 500 1 6 6 Generation Settings max prompt length max completion length num generations temperature seed Logging and Saving log completions log level logging first step logging steps logging strategy save strategy save steps report to Reward Configuration reward funcs reward weights Hub Settings hub model id hub strategy push to hub 512 3584 or 4096 6 0.7 42 true info true 1 steps steps 50 wandb format, accuracy (cosine) 1.0, 2.0 OpenRS-GRPO every save true"
        }
    ],
    "affiliations": [
        "Knovel Engineering Lab, Singapore",
        "VNU University of Science, Vietnam"
    ]
}