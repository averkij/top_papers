{
    "paper_title": "ZeroSep: Separate Anything in Audio with Zero Training",
    "authors": [
        "Chao Huang",
        "Yuesheng Ma",
        "Junxuan Huang",
        "Susan Liang",
        "Yunlong Tang",
        "Jing Bi",
        "Wenqiang Liu",
        "Nima Mesgarani",
        "Chenliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio source separation is fundamental for machines to understand complex acoustic environments and underpins numerous audio applications. Current supervised deep learning approaches, while powerful, are limited by the need for extensive, task-specific labeled data and struggle to generalize to the immense variability and open-set nature of real-world acoustic scenes. Inspired by the success of generative foundation models, we investigate whether pre-trained text-guided audio diffusion models can overcome these limitations. We make a surprising discovery: zero-shot source separation can be achieved purely through a pre-trained text-guided audio diffusion model under the right configuration. Our method, named ZeroSep, works by inverting the mixed audio into the diffusion model's latent space and then using text conditioning to guide the denoising process to recover individual sources. Without any task-specific training or fine-tuning, ZeroSep repurposes the generative diffusion model for a discriminative separation task and inherently supports open-set scenarios through its rich textual priors. ZeroSep is compatible with a variety of pre-trained text-guided audio diffusion backbones and delivers strong separation performance on multiple separation benchmarks, surpassing even supervised methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 2 6 3 2 . 5 0 5 2 : r ZeroSep: Separate Anything in Audio with Zero Training Chao Huang1, Yuesheng Ma2, Junxuan Huang3, Susan Liang1, Yunlong Tang1, Jing Bi1, Wenqiang Liu3, Nima Mesgarani2, Chenliang Xu1 1University of Rochester, 2Columbia University, 3Tencent America"
        },
        {
            "title": "Abstract",
            "content": "Audio source separation is fundamental for machines to understand complex acoustic environments and underpins numerous audio applications. Current supervised deep learning approaches, while powerful, are limited by the need for extensive, task-specific labeled data and struggle to generalize to the immense variability and open-set nature of real-world acoustic scenes. Inspired by the success of generative foundation models, we investigate whether pre-trained text-guided audio diffusion models can overcome these limitations. We make surprising discovery: zero-shot source separation can be achieved purely through pre-trained text-guided audio diffusion model under the right configuration. Our method, named ZeroSep, works by inverting the mixed audio into the diffusion models latent space and then using text conditioning to guide the denoising process to recover individual sources. Without any task-specific training or fine-tuning, ZeroSep repurposes the generative diffusion model for discriminative separation task and inherently supports openset scenarios through its rich textual priors. ZeroSep is compatible with variety of pre-trained text-guided audio diffusion backbones and delivers strong separation performance on multiple separation benchmarks, surpassing even supervised methods. Our project page is here: https://wikichao.github.io/ZeroSep/."
        },
        {
            "title": "Introduction",
            "content": "At the heart of acoustic scene perception lies the fundamental task of source separation, which aims to isolate individual sound sources from complex audio mixture. Accurate source separation is crucial for wide range of applications, including media production, surveillance systems, automatic speech recognition in noisy environments, and analysis of complex soundscapes. The dominant approach to audio separation in recent years has relied heavily on supervised learning: deep neural networks are trained on large datasets of paired mixtures and clean sources [Luo and Mesgarani, 2019, Subakan et al., 2021]. While these methods have achieved impressive performance on specific, well-represented source types and datasets, they often fall short when faced with the open-set variability of real-world acoustic scenes. Consequently, training foundational separation model becomes exceptionally challenging due to the need for vast amounts of labeled data, the difficulty of defining training objectives and mixing strategies, and the design of effective conditioning mechanisms. Recent efforts, such as LASS-net Liu et al. [2022a], AudioSep Liu et al. [2023a], and FlowSep Yin et al. [2024], have explored leveraging natural language queries for more flexible separation. Despite these advances, they still contend with the same core challenges: vast data requirements, complex task-specific training regimes, and limited generalization to unseen acoustic scenes. Inspired by the transformative success of large language models in unifying diverse NLP tasks under generative framework [Brown et al., 2020], we pose central question: Can generative foundation model Preprint. Under review. similarly emerge for audio tasks? In this work, we explore this question by investigating the capabilities of pre-trained text-guided audio diffusion models. We discover that text-guided audio diffusion model can, out of the box, separate mixture into its sources no training or fine-tuning, relying solely on latent inversion and conditioned denoising: (i) Given mixed audio signal, we can find corresponding point in the diffusion models latent space through an inversion process. This latent representation captures the composite information from all the sound sources present in the mixture. (ii) Subsequently, by guiding the generative denoising process from this latent state using text prompts corresponding to individual sources in the mixture, the model can be steered to reconstruct each source in isolation. Surprisingly, even though this is generative process, the separated sources are highly faithful to the original sources, especially with classifier-free guidance 1, which prevents hallucination. This effectively repurposes the generative model for discriminative task, offering fundamentally different approach to separation. Based on the above observations, we introduce ZeroSep, zero-training framework for audio source separation that repurposes pretrained text-guided diffusion models. By casting separation as twostep generative inference, latent inversion followed by text-conditioned denoising, ZeroSep offers three key advantages: Open-set Separation: As the core of ZeroSep is pre-trained text-guided audio diffusion model, which has learned to generate realistic audios from diverse, open-domain descriptions and mixing styles, ZeroSep naturally handles open-set queries and is able to separate from diverse mixtures. Model-agnostic Versatility: The inversion plus denoising pipeline is generic to diffusion architectures, allowing ZeroSep to leverage different pre-trained audio diffusion backbones. Interestingly, we observe trend that the better the audio diffusion model can generate, the better it can separate, which could suggest continuous improvement whenever there is more advanced audio generation model available. Training-Free Efficacy: Without any fine-tuning or task-specific data, ZeroSep matches or exceeds the performance of existing training-based generative separators, overturning the assumption that high-quality separation requires dedicated training. In summary, our contributions to the community includes 1. We introduce ZeroSep, the first training-free audio source separation framework that repurposes pre-trained text-guided audio diffusion models, representing fundamental shift away from supervised separation paradigms. 2. We demonstrate that pure generative inferencelatent inversion followed by textconditioned denoisingyields state-of-the-art separation performance, outperforming existing training-based generative methods. 3. We establish ZeroSep versatility and open-set capability: it seamlessly handles diverse mixtures and textual queries and can be applied to many pre-trained audio diffusion backbones, improving separation quality as the underlying models generative fidelity increases."
        },
        {
            "title": "2 Related Works",
            "content": "Audio Diffusion Models. Diffusion probabilistic models have rapidly emerged as leading paradigm for generating high-quality and diverse audio content. Early works like DiffWave [Kong et al., 2021] and WaveGrad [Chen et al., 2021a] demonstrated the potential of applying denoising diffusion to synthesize raw audio waveforms, achieving unconditional audio generation. Building on this foundation, diffusion models were successfully extended to conditional audio generation tasks. In text-to-speech (TTS), models such as Diff-TTS [Jeong et al., 2021] and Grad-TTS [Popov et al., 2021] showed that diffusion processes could generate high-fidelity mel-spectrograms conditioned on text input. Researchers also focused on improving the efficiency and controllability of diffusion sampling; for instance, Guided-TTS Kim et al. [2022] introduced classifier guidance for TTS, and PriorGrad [Lee et al., 2022] addressed sampling speed in vocoders through data-dependent priors. Diffusion models have also been applied to other audio synthesis tasks, including singing voice synthesis with DiffSinger [Liu et al., 2022b] and waveform super-resolution with NU-Wave [Lee and Han, 2021]. More recently, the focus has shifted towards latent-space diffusion models and text-conditioned generation of general audio. AudioLDM [Liu et al., 2023b] pioneered combining diffusion with CLAP embeddings to enable text-conditioned generation of diverse sounds and music. AudioLDM2 [Liu et al., 2024] and Tango [Ghosal et al., 2023] further advanced in this direction, 2 providing enhanced control and quality. These text-conditioned latent diffusion models, capable of generating complex audio scenes from natural language, form the technological foundation for our training-free separation method ZeroSep. Audio Separation. The problem of source separation has long been tackled by both classic signal-processing techniques and, more recently, deep learning. Traditional methods such as NMFMFCC [Stöter et al., 2021] decompose mixtures under assumptions about timbral or spectral structure. While training-free, they often fail on complex or heavily overlapping sources that lack clear distinguishing features. Deep learning revolutionized the field by learning representations directly from data. Deep Clustering [Hershey et al., 2016] trains embeddings for clustering source-specific timefrequency bins, and Permutation-Invariant Training (PIT) [Yu et al., 2017] resolves the labelpermutation problem during training. Conv-TasNet [Luo and Mesgarani, 2019] further advanced performance with end-to-end waveform separation, frequently surpassing traditional masking approaches. However, these models remain blind to user intent: once trained, they separate every detectable component rather than targeting specific source. To introduce controllability, recent works condition separation on auxiliary modalities. Video-guided methods [Huang et al., 2024a] use visual cues, while language-based frameworks, such as LASS-Net [Liu et al., 2022a], AudioSep [Liu et al., 2023a], and FlowSep [Yin et al., 2024], leverage text prompts to guide mask estimation. Although more flexible, they still require large supervised corpora of synthetic mixtures, inheriting closedworld biases. Zero-shot diffusion editors like AUDIT [Wang et al., 2023] and AudioEdit [Manor and Michaeli, 2024] fine-tune or invert latent trajectories to delete components, but focus on editing rather than explicit separation. In contrast, ZeroSep repurposes pre-trained text-guided audio diffusion model as universal, training-free prior for open-set separation. By (i) inverting an audio mixture into the models latent space and (ii) re-denoising under user-provided text prompts with unit classifier-free guidance, ZeroSep generates one isolated waveform per prompt, achieving comprehensive, zero-shot source separation without fine-tuning."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first review the foundational knowledge of text-guided diffusion models and diffusion inversion techniques, which form the basis of our method. Next, we discuss the separation task setup with generative diffusion models. Lastly, we introduce ZeroSep, zero-shot separation adaptation of existing text-guided audio diffusion models. 3.1 Preliminary: Text-Guided Audio Diffusion and Inversion Text-guided audio diffusion models typically operate in learned latent space: An initial audio signal is first encoded into latent representation, denoted as x0; the forward diffusion process progressively adds Gaussian noise to this latent vector, transforming it into pure noise vector xT . neural network, parameterized by θ, learns to predict and remove the noise added at each step t, effectively reversing the diffusion and generating mel-spectrograms which are then converted to waveforms using vocoder. In text-guided models, this denoising process is driven by text condition c, derived from text encoder, ensuring the generated audio aligns with the text prompt. DDIM Inversion. To enable manipulation of existing audio content, inversion techniques are used to map real audio sample back into the noisy latent space. common approach is DDIM inversion, which leverages the deterministic nature of DDIM sampling [Song et al., 2020]. The standard DDIM sampling process iteratively denoises noisy latent xt to produce less noisy version xt1: xt1 = (cid:113) αt1 αt xt + (cid:16)(cid:113) 1 αt1 1 (cid:113) 1 αt (cid:17) (cid:0)xt, c, t(cid:1), ϵθ (1) t=0 defines the noise schedule and ϵθ(, c, t) is the models noise prediction conditioned where {αt}T on c. DDIM inversion reverses this process, estimating the noisy latent xt+1 from xt: (cid:16)(cid:113) 1 αt+1 (cid:113) αt+1 αt (cid:0)xt, c, t(cid:1), (cid:113) 1 αt xt+1 = 1 xt + ϵθ (2) (cid:17) so that iterating from x0 recovers an estimate of the pure noise xT . Cumulative errors can, however, cause deviations from the true noise trajectory. 3 Figure 1: The overview of ZeroSep, which includes (a) an inversion process to obtain latent representation for the mixture, and (b) separation denoising process to effectively extract the target source with text conditions. We show the choice of inversion prompt cinv and reverse prompt crev in (c), and demonstrate the valid separation region defined by ω in (d). (3) (4) DDPM Inversion. In contrast to DDIM inversion, DDPM inversion [Huberman-Spiegelglas et al., 2024] leverages the probabilistic forward diffusion to obtain an exact noise path. Given clean latent x0, one constructs an auxiliary sequence of noisy latents xt = and then extracts the per-step noise vectors αt x0 + 1 αt ϵt, ϵt (0, I), = 1, . . . , T, zt = xt1 µt(xt) σt , = T, . . . , 1, where µt(xt) and σt follow the DDPM [Ho et al., 2020] reverse-step definitions. Reconstruction simply re-injects xT and {zt} via xt1 = µt(xt) + σt zt, (5) exactly recovering x0. By scaling or replacing {zt} (e.g. using text embeddings at select timesteps), DDPM inversion offers probabilistic framework for precise, text-guided edits. From high-level perspective, both DDIM and DDPM inversion can be viewed as single mapping, which we denote by xT = Finv(x0, c). This operator Finv encapsulates the step-wise recovery of the noise trajectory corresponding to given clean latent. Whether implemented via the deterministic updates of DDIM or the probabilistic steps of DDPM, Finv produces the pure noise xT that, when re-injected into the standard diffusion sampler, exactly reconstructs the original sample x0. 3.2 Task Setup In real-world scenarios, an audio stream can be mixture of individual sound sources: = (cid:80)N i=1 s(i), where each source s(i) can be of various categories. To work in the diffusion latent space, we first convert to mel-spectrogram and encode it with Variational Autoencoder (VAE), yielding latent features RCT , where is the number of channels, the numbers of frequency bins, and the number of time frames. Let xmix denote the VAE encoding of the mixture and x(i) the encoding of source i. Our goal is to find separation mapping (cid:0)xmix, c(i)(cid:1) x(i), where c(i) is conditioning signal (e.g., text description) that specifies which source to extract. x(i) is then fed to the VAE decoder and Vocoder to convert latent features back to waveform level to obtain ˆs(i). 3.3 From Generation to Separation: The ZeroSep Principle The core of ZeroSep lies in repurposing pre-trained text-guided audio diffusion model, originally designed for generating audio from text, to perform the discriminative task of audio source separation. 4 Let cinv be the text prompt used during the inversion process (mapping the mixed audio xmix to noisy latent xT ), and crev be the text prompt used during the subsequent reverse denoising process (reconstructing clean source from the noisy latent). Diffusion models typically employ classifier-free guidance during denoising, where the noise prediction ϵt at step is combination of an unconditional prediction and conditional prediction guided by crev: ϵt = ϵθ(xt, , t) + ω (ϵθ(xt, crev, t) ϵθ(xt, , t)). (6) Here, ϵθ(xt, , t) is the unconditional noise prediction, ϵθ(xt, crev, t) is the prediction guided by crev, and ω is the classifier-free guidance weight controlling the influence of the text condition crev. While this formulation is typically used to amplify the presence of the desired content during generation, we discover that specific choices of cinv, crev, and ω, enable effective source separation. This shifts the models function from synthesizing new audio to dissecting existing mixtures. Here are the key principles for transforming the generative process into separation tool: The Reverse Prompt crev: To isolate specific source i, the reverse denoising prompt crev must explicitly describe that target source: crev := c(i), if separating source i. (7) This directs the denoising process to reconstruct the audio components associated with the target source described by c(i). Using any other prompt would result in guided generation, not separation. The Inversion Prompt cinv: The inversion prompt cinv influences how the mixed audio is mapped to the noisy latent space. We found flexibility here, with effective choices including null prompt or prompts describing the other sources present in the mixture (c(j) for = i). While describing other sources can potentially refine the latent representation by emphasizing non-target components, it requires prior knowledge of the mixtures contents, yet can be achieved with user query or by prompting Vision-Language Models or Audio Language Models (as shown in Fig. 1(c)). simpler and often effective approach is to use null prompt (cinv = ) as the default. This inverts the mixed signal based on the models general audio understanding without imposing specific content constraints during the inversion phase. The effect of cinv and crev can be found in Tab. 4. The Crucial Role of Guidance Weight ω: key discovery is that achieving separation hinges on setting the classifier-free guidance weight ω appropriately, specifically ω 1. This is counterintuitive to typical generative usage where high ω values (e.g., ω = 3.5 for AudioLDM2 [Liu et al., 2024]) amplify the conditional signal for strong generation. In our context, when using cinv = : Setting ω = 0 removes the conditional influence, effectively leading to reconstruction of the original mixed audio. Setting ω = 1 removes the unconditional noise estimation from the combined prediction in Eq. (6), leaving only the component aligned with the target source described by crev. This effectively isolates the target source during denoising. Setting ω > 1, as in standard generation, overly amplifies the conditional signal, leading to the synthesis of new content rather than the separation of existing audio components. We empirically find that ω = 1 yields the best separation results (as shown in Fig. 3(a)). This finding reveals that controlling the balance between conditional and unconditional predictions via ω is critical for steering the diffusion process from generation towards faithful separation. By carefully selecting cinv, crev, and setting ω (in practice, we set ω = 1), we effectively repurpose the pre-trained audio diffusion models generative capabilities to perform high-quality source separation without requiring any task-specific training."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Baselines. To evaluate our training-free diffusion-based separation method, we compare it against two categories of existing approaches: (i) Training-based methods. These methods rely on large-scale supervised training and leverage text queries for targeted separation. We include: LASS-Net [Liu et al., 5 Table 1: Main audio separation results comparing ZeroSep with training-based and training-free baselines on the AVE [Tian et al., 2018] and MUSIC [Zhao et al., 2018] datasets. Metrics are reported on the test sets. indicates higher is better, indicates lower is better. The best results are bold. Method MUSIC AVE FAD LPAPS C-A C-T FAD LPAPS C-A C-T Require Separation Training LASS-Net AudioSep FlowSep Separation Training Free NMF-MFCC AudioEdit ZeroSep (Ours) 1.039 0.725 0. 1.286 0.568 0.377 5.602 5.209 5.578 0.204 0.450 0.564 0.014 0.204 0.245 0.626 0.446 0.258 6.062 5.733 4. 0.232 0.457 0.493 0.011 0.167 0.082 5.618 4.869 4.669 0.239 0.055 0.196 0.453 0.271 0.615 1.246 0.372 0.269 5.851 4.959 4. 0.211 0.174 0.341 0.074 0.442 0.001 Figure 2: Qualitative visualization of audio separation results. The figure shows the input mixture (containing speech and dog barking) and the separated dog barking source produced by different baselines and ZeroSep. ZeroSep, guided by the text prompt dog bark, successfully isolates the target sound, demonstrating its effectiveness compared to baseline methods. More separation results can be found in the supplementary materials. 2022a], which conditions mask estimator on text queries; AudioSep [Liu et al., 2023a], scaled-up version of LASS-Net trained on massive multimodal data for zero-shot capabilities across diverse sources; and FlowSep [Yin et al., 2024], which enhances query-based separation using rectified continuous normalizing flows. We note that AUDIT [Wang et al., 2023] also uses audio diffusion models for instruction-guided audio editing (including source manipulation) but is not included as direct baseline due to the lack of public code and data release for comparison. (ii) Training-free methods. These methods perform separation without requiring task-specific training data. We compare against: NMF-MFCC [Stöter et al., 2021], classical non-negative matrix factorization approach operating on MFCC features for blind source separation; and AudioEditor [Manor and Michaeli, 2024], which achieves unsupervised separation by discovering principal components within the denoising process of pre-trained diffusion model. In summary, training-based baselines require extensive annotated data for training, whereas other training-free baselines employ different underlying principles from our generative diffusion-based approach. Datasets. We evaluate the open-set separation capabilities of our training-free method on two benchmark multimodal datasets with paired audio and text labels: The AudioVisual Event (AVE) [Tian et al., 2018] dataset contains 4,143 video clips, each 10 seconds long, covering 28 distinct sound categories (e.g., church bell, barking, frying). AVE is valuable for evaluating separation in complex, real-world scenarios due to the presence of background noise, off-screen sounds, and varying event durations. The MUSIC dataset [Zhao et al., 2018] consists of clean solo performances from 11 musical instruments, thereby offering controlled environment to assess the separation of individual, 6 Table 2: Evaluation of AudioLDM [Liu et al., 2024], AudioLDM2 [Liu et al., 2024], and Tango [Ghosal et al., 2023] on the MUSIC and AVE benchmarks. We compare two U-Net sizes for each AudioLDM variant-S (181 M) / (739 M) and AudioLDM2-S (350 M) / AudioLDM2-L (750 M), and Tangos 866 M-parameter U-Net. Results are reported for both DDIM and DDPM inversion methods, and for AudioLDM2 we include full vs. music-only training data. Model Size Data MUSIC AVE FAD LPAPS C-A C-T FAD LPAPS C-A C-T DDIM Inversion AudioLDM AudioLDM2 Tango S L DDPM Inversion AudioLDM AudioLDM2 Tango S Full Full Full Music Full Full Full Full Full Music Full Full 0.460 0.470 0.421 0.439 0.377 0.606 0.417 0. 0.390 0.384 0.397 0.539 4.690 4.625 4.630 4.620 4.669 0.562 0.577 0.575 0.584 0. 0.284 0.260 0.261 0.259 0.271 0.275 0.253 0.251 0.325 0.269 4.821 4.742 4.560 4.666 4. 0.484 0.490 0.114 0.102 0.039 0.477 0.424 0.106 0.442 0.001 4.511 0.544 0. 0.724 4.451 0.437 0.077 4.580 4.536 4.586 4.596 4. 0.605 0.626 0.595 0.609 0.598 0.300 0.283 0.238 0.259 0.239 0.239 0.266 0.272 0.238 0. 4.681 4.629 4.546 4.628 4.523 0.504 0.496 0.133 0.108 0.041 0.488 0.467 0.126 0.445 0.008 4. 0.581 0.189 0.723 4.471 0.451 0. isolated sources with minimal interference. To facilitate comparison with prior research and ensure reproducibility, we use the official separation data splits for both AVE and MUSIC as provided by the DAVIS repository [Huang et al., 2024a]. Evaluation Metrics. Traditional separation metricsSignal-to-Distortion Ratio (SDR), Signal-toInterference Ratio (SIR), and Signal-to-Artifact Ratio (SAR) [Raffel et al., 2014]quantify samplelevel differences between separated output ˆs and the ground truth s. These metrics assume the output lies on the same waveform manifold as s, an assumption violated by generative models that may produce perceptually accurate but sample-wise divergent signals. Tab. 3 demonstrates how VAEVocoder reconstruction of the mixture yields misleadingly poor SDR/SIR/SAR scores. SIR SDR Method 149.90 0.31 Original VAE + Vcoder 23.07 0.79 19. To capture perceptual and semantic fidelity of generative separation, we adopt metrics in embedding spaces: Frechét Audio Distance (FAD) [Kilgour et al., 2018]: measures the distance between embedding distributions of separated and ground-truth audio. Learned Perceptual Audio Patch Similarity (LPAPS) [Manor and Michaeli, 2024]: evaluates perceptual audio similarity in learned embedding space. CLAPA and CLAP-T [Yin et al., 2024]: CLAP-A is the cosine similarity between audio embeddings of the separation output and the ground-truth source; CLAP-T is the cosine similarity between audio embeddings and the text embedding of the target class. These feature-based metrics better reflect perceptual quality and semantic alignment, addressing the shortcomings of waveform-level reference metrics for generative audio separation. Table 3: Breakdown of SDR/SIR/SAR (with respect to the individual target source), for generative reconstruction of the mixture versus the original mixture. SAR 0.31 4.2 Main Comparison Tab. 1 presents the core results of our evaluation, comparing the performance of our training-free method, ZeroSep, against representative training-based and other training-free baselines on the AVE and MUSIC datasets. Remarkably, ZeroSep demonstrates performance that surpasses the leading supervised methods, effectively challenging the necessity of large-scale supervised training for state-of-the-art audio separation. On the MUSIC dataset, ZeroSep outperforms the strongest 7 Figure 3: (a) Impact of guidance weight ω: increasing ω from 0 to 1 improves separation metrics (LPAPS and CLAP-A), whereas ω > 1 degrades performance below the mixture baseline (ω = 0), underscoring the critical role of ω. (b)(c) Positive correlation between separation quality (normalized all scores from Tab. 2) and generative capability (normalized FAD scores on AudioCap [Liu et al., 2023b], [Liu et al., 2024]) across AudioLDM variants, indicating that stronger generation can potentially lead to better separation. training-based baseline FlowSep across all metrics. On the more complex and open-domain AVE dataset, ZeroSep achieves performance comparable to FlowSep. The training-based baselines show clear improvement trend with increasing model size and data: LASS-Net is surpassed by AudioSep, which in turn is surpassed by FlowSep, underscoring the benefits of extensive supervised training data and better models. The other training-free methods evaluated, NMF-MFCC and AudioEditor, yield substantially lower performance than the top supervised methods, highlighting the difficulty of achieving high-quality separation without leveraging separation training, which ZeroSep has addressed. Beyond quantitative scores, the qualitative visualization in Fig. 2 provides further evidence of ZeroSeps effectiveness, illustrating the successful separation of target sound (e.g., dog barking) from complex mixture containing other sources like human speech. These results collectively indicate that pre-trained text-guided diffusion models possess powerful inherent capabilities that can be effectively harnessed for audio separation without the need for task-specific training. 4.3 Ablation Studies In this section, we analyze the influence of various components on ZeroSeps separation performance to identify factors contributing to its effectiveness. We investigate aspects including the choice and capacity of the base generative model, the impact of its training data domain, inversion strategies, guidance weight effects, and prompt selection. How Does the Base Generative Model Affect Separation? Since ZeroSep is built upon pretrained diffusion model, understanding how this base model affects separation is crucial. First, we compare separation performance using different base model architectures, including models from the AudioLDM [Liu et al., 2023b], AudioLDM2 [Liu et al., 2024], and Tango [Ghosal et al., 2023] families, as shown in Tab. 2. The results indicate that various base models can yield separation performance comparable to the best training-based baseline, FlowSep, demonstrating versatility in base model selection. Specifically, models from the AudioLDM and AudioLDM2 families generally outperform Tango in this separation task. Second, we analyze the effect of model capacity by comparing different sizes within the AudioLDM and AudioLDM2 families (e.g., AudioLDM-S vs. AudioLDM-L, AudioLDM2-S vs. AudioLDM2-L). As shown in Tab. 2, increasing model size consistently leads to improved separation performance. This suggests positive correlation between the generative power of the base model and its effectiveness for separation. We further visualize this trend by plotting the correlation between generative performance (measured by FAD) and separation metrics in Fig. 3(b) and (c), which confirms that stronger generative models tend to yield better separation results. Third, we investigate the impact of the base models training data domain. Tab. 2 includes results for model trained specifically on MUSIC data compared to the same model trained on broader data corpora. We observe that the MUSIC-data-trained model achieves performance on the MUSIC dataset that is similar to or even better than the full-training model for certain metrics (e.g., LPAPS 8 Table 4: Effect of cinv and crev on separation metrics. Triangles indicate change relative to the baseline, with denoting improvement and denoting degradation. cinv crev c(i) random c(j) c(i) MUSIC AVE FAD LPAPS C-A C-T FAD LPAPS C-A C-T 0.377 4. 0.615 0.271 0.269 4.537 0.442 0.001 0.577 0. 0.454 0.077 4.900 0.325 0.231 0.252 0.146 0.056 0.363 0.125 4. 0.321 0.122 0.034 0.017 0.052 0.581 0.254 4.749 0.019 0.289 0.212 0.153 0.020 4. 0.055 0.496 0.062 0.054 0.056 and CLAP-A). This finding suggests that if the target separation domain is narrow, base generative model trained on domain-specific data can be sufficient or even advantageous, potentially increasing the separation flexibility. Inversion Strategy. As shown in Tab. 2, both DDIM and DDPM inversion methods enable competitive separation performance relative to the baselines. Analyzing their behavior across different base model capacities and training data domains, we observe that DDPM inversion tends to yield more stable metrics, exhibiting less fluctuation with respect to changes in model size and training data. In contrast, DDIM inversion shows larger variations under these different conditions. This analysis indicates that ZeroSeps effectiveness is not strictly tied to single inversion technique, offering flexibility in implementation. Effect of ω. As detailed in Sec. 3.3, setting ω = 0 effectively reduces the process to an unconditional reconstruction, while higher values increase the adherence to the text prompt crev. We analyze the impact of ω on separation performance by evaluating values in the set {0, 0.5, 1, 1.5, 2}. Fig. 3(a) presents the results for LAPAS and CLAP-A metrics. It can be observed that as ω increases from 0 to 1, both separation metrics generally improve, indicating that conditioning on the target source prompt effectively guides the separation. However, beyond ω = 1, performance deteriorates sharply, suggesting that excessively strong guidance can lead to suboptimal reconstructions or introduce artifacts. Based on this analysis, we empirically set ω = 1 for our main experiments to achieve the best balance between adherence to the target prompt and reconstruction quality. Effect of crev and cinv. Tab. 4 summarizes the separation performance under different prompt configurations. First, replacing the prompt crev specifying the target sound source with random, mixture-unrelated prompt results in drastic performance drop across all metrics. This highlights the essential role of accurate text conditioning towards separating target source. For cinv, we explore replacing the null prompt with prompt for different source present in the mixture but not the target. This substitution leads to slight degradation in performance, which demonstrates that while cinv provides some contextual information, the method is less sensitive to its precise content."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper demonstrates new paradigm for audio source separation, moving away from reliance on extensive supervised training. In particular, we introduce ZeroSep, novel training-free approach that leverages the power of pre-trained text-guided audio diffusion models. Our evaluation reveals that ZeroSep achieves performance on par with or exceeds leading supervised separation baselines across benchmark datasets, and our analysis further illuminates the factors critical for successful transformation from generation to separation. The effectiveness of ZeroSep showcases new application for the growing family of audio diffusion models and offers compelling alternative direction for developing open-set audio source separation models. Limitations. While we have demonstrated the efficacy of ZeroSep on popular audio diffusion models (e.g., AudioLDM families and Tango), how it works on larger models and alternative architectures remains untested. In addition, our reliance on latent inversion can introduce approximation errors that may impair separation fidelity. Due to computational constraints, we did not include these experiments in this work. We will explore how to scale evaluations to diverse, high-capacity diffusion models and develop more accurate inversion techniques in future work. 9 Beyond Basic Separation. ZeroSeps inherent mechanism unlocks diverse application scenarios beyond simple text-to-sound separation. First, text prompts can be automatically generated. Audio event detection [Mesaros et al., 2021] or audio language models [Gong et al., 2023, Ghosh et al., 2024] can derive labels or free-form descriptions from mixtures, enabling automated separation. Second, ZeroSep facilitates cross-modal applications; for instance, leveraging audio-visual localization [Huang et al., 2023, Chen et al., 2021b] and vision-language models [Liu et al., 2023c], users could separate sounds in video by visually describing sounding objects. Third, recognizing the importance of spatial audio understanding and rendering [Gao and Grauman, 2019, Liang et al., 2023a,b, Huang et al., 2024b] for human-level acoustic perception, ZeroSep can be directly extended to spatial audio separation using diffusion models that support multi-channel input, such as Stable Audio Open [Evans et al., 2025]. Finally, our method naturally enables continuous transition from mixture reconstruction to sound highlighting [Gandikota et al., 2024a, Huang et al., 2024c, 2025a,b], allowing scaling of target sound elements from full presence to complete separation [Gandikota et al., 2024b]."
        },
        {
            "title": "References",
            "content": "Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation. IEEE/ACM Trans. Audio Speech Language Process., 27(8):12561266, 2019. doi: 10.1109/TASLP.2019.2915167. Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan Zhong. Attention is all you need in speech separation. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021. Xubo Liu, Haohe Liu, Qiuqiang Kong, Xinhao Mei, Jinzheng Zhao, Qiushi Huang, Mark D. Plumbley, and Wenwu Wang. Separate what you describe: Language-queried audio source separation. In Proc. Interspeech, pages 18011805, 2022a. Xubo Liu, Qiuqiang Kong, Yan Zhao, Haohe Liu, Yi Yuan, Yuzhuo Liu, Rui Xia, Yuxuan Wang, Mark D. Plumbley, and Wenwu Wang. Separate anything you describe. arXiv preprint arXiv:2308.05037, 2023a. Han Yin, Jisheng Bai, Yang Xiao, Hui Wang, Siqi Zheng, Yafeng Chen, Rohan Kumar Das, Chong Deng, and Jianfeng Chen. Flowsep: Language-queried sound separation with rectified flow. arXiv preprint arXiv:2409.07614, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. In International Conference on Learning Representations (ICLR), 2021. Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations (ICLR), 2021a. Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-tts: denoising diffusion model for text-to-speech. In Proc. Interspeech 2021, pages 36053609, 2021. doi: 10.21437/Interspeech.2021-469. Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pages 85998608. PMLR, 2021. Heeseung Kim, Sungwon Kim, and Sungroh Yoon. Guided-tts: diffusion model for text-to-speech via classifier guidance. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, and Tie-Yan Liu. Priorgrad: Improving conditional denoising diffusion models with data-dependent adaptive prior. In International Conference on Learning Representations (ICLR), 2022. Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis via shallow diffusion mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022b. Junhyeok Lee and Seungu Han. Nu-wave: diffusion probabilistic model for neural audio upsampling. In Proc. Interspeech 2021, pages 16341638, 2021. doi: 10.21437/Interspeech.2021-36. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D. Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 2145021474. PMLR, 2023b. Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:28712883, 2024. doi: 10.1109/TASLP.2024.3399607. Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731, 2023. Fabian-Robert Stöter, Zahra Hafida Benslimane, et al. Separation by timbre via non-negative matrix factorization with mfcc clustering, 2021. Implementation in the nussl Python library. John R. Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe. Deep clustering: Discriminative embeddings for segmentation and separation. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016. Dong Yu, Morten Kolbæk, Zheng-Hua Tan, and Jesper Jensen. Permutation invariant training of deep models for speaker-independent multi-talker speech separation. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 241245, 2017. doi: 10.1109/ICASSP.2017.7952154. Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. High-quality visuallyguided sound separation from diverse categories. In Proceedings of the Asian Conference on Computer Vision (ACCV), pages 3549, December 2024a. Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu, Jiang Bian, and Sheng Zhao. Audit: Audio editing by following instructions with latent diffusion models. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Hila Manor and Tomer Michaeli. Zero-shot unsupervised and text-based audio editing using DDPM inversion. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 34603 34629. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/manor24a. html. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1246912478, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 11 Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 247263, 2018. Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In Proceedings of the European conference on computer vision (ECCV), pages 570586, 2018. Colin Raffel, Brian McFee, Eric Humphrey, Justin Salamon, Oriol Nieto, Dawen Liang, Daniel PW Ellis, and Colin Raffel. Mir_eval: transparent implementation of common mir metrics. In ISMIR, pages 367372, 2014. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018. Annamaria Mesaros, Toni Heittola, Tuomas Virtanen, and Mark Plumbley. Sound event detection: tutorial. IEEE Signal Processing Magazine, 38(5):6783, 2021. Yuan Gong, Hongyin Luo, Alexander Liu, Leonid Karlinsky, and James Glass. Listen, think, and understand. arXiv preprint arXiv:2305.10790, 2023. Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, Sakshi, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha. Gama: large audio-language model with advanced audio understanding and complex reasoning abilities. arXiv preprint arXiv:2406.11768, 2024. Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Egocentric audio-visual object In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern localization. Recognition, pages 2291022921, 2023. Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1686716876, 2021b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023c. Ruohan Gao and Kristen Grauman. 2.5 visual sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 324333, 2019. Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Av-nerf: Learning neural fields for real-world audio-visual scene synthesis. Advances in Neural Information Processing Systems, 36:3747237490, 2023a. Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Neural acoustic context field: Rendering realistic room impulse response with neural fields. arXiv preprint arXiv:2309.15977, 2023b. Chao Huang, Dejan Markovic, Chenliang Xu, and Alexander Richard. Modeling and driving human body soundfields through acoustic primitives. In European Conference on Computer Vision, pages 117. Springer, 2024b. Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models. In European Conference on Computer Vision, pages 172188. Springer, 2024a. Chao Huang, Susan Liang, Yunlong Tang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Scaling concept with text-guided diffusion models. arXiv preprint arXiv:2410.24151, 2024c. 12 Chao Huang, Ruohan Gao, JMF Tsang, Jan Kurcius, Cagdas Bilen, Chenliang Xu, Anurag Kumar, and Sanjeel Parekh. Learning to highlight audio by watching movies. arXiv preprint arXiv:2505.12154, 2025a. Chao Huang, Susan Liang, Yunlong Tang, Li Ma, Yapeng Tian, and Chenliang Xu. Fresca: Unveiling the scaling space in diffusion models. arXiv preprint arXiv:2504.02154, 2025b. Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Erasing concepts from diffusion models. In Proceedings of the 2024 IEEE European Conference on Computer Vision, 2024b. arXiv preprint arXiv:2311.12092."
        },
        {
            "title": "A Demo Page",
            "content": "Weve prepared demo page, included in the supplementary materials, to illustrate our method and showcase our results. We strongly encourage you to visit this webpage and experience our results. For the best viewing experience, we recommend using Google Chrome, as the page may not be fully compatible with Safari. On the demo page, youll find: Interactive Interface Demo: Weve built Gradio interface, making it easy to use our separation method. It supports both video and audio uploads, and allows for selecting different base models, inversion methods, and hyperparameters. The output is the separated audio, enabling you to effortlessly try out our approach. Separation Results from Various Methods: We present separation results across diverse scenarios, including musical instrument sources, daily events, and more. Figure 4: Failure case analysis of ZeroSep. Mixture: Man speech (stem 1) + Shofar (stem 2)."
        },
        {
            "title": "B Failure Case Analysis",
            "content": "While generally effective, ZeroSep can sometimes fail to fully isolate the target source. This typically occurs when an interfering source possesses significant energy that the model cannot eliminate in single iteration. An illustrative example of such failure is presented in Fig. 4. We postulate that, given the inherent progressive operation of diffusion models, the removal of interfering sources also proceeds incrementally. Consequently, this performance limitation may be tied to the number of inference steps utilized. Potential avenues for improvement include increasing the inference steps or iteratively applying the separation process."
        },
        {
            "title": "C More Separation Results",
            "content": "These figures present mel-spectrograms that visualize the audio separation performance on two-source mixtures. For each figure, the rows are ordered from top to bottom as follows: the first sources Ground Truth, followed by its separation results from LASS-Net, FlowSep, AudioEdit, AudioSep, and Ours. This sequence is then repeated for the second source: Ground Truth 2, LASS-Net 2, FlowSep 2, AudioEdit 2, AudioSep 2, and Ours 2. You might notice some white or empty areas on the right side of the mel-spectrograms; these are simply due to the varying lengths of the audio samples. 13 Figure 5: Mixture: Cello (stem 1) + Erhu (Stem 2) Figure 6: Mixture: Acoustic Guitar (stem 1) + Tuba (Stem 2) 14 Figure 7: Mixture: Accordion (stem 1) + Flute (Stem 2) Figure 8: Mixture: Cello (stem 1) + Erhu (Stem 2) 15 Figure 9: Mixture: Tuba (stem 1) + Cello (Stem 2) Figure 10: Mixture: Xlyophone (stem 1) + Trumpet (Stem 2) 16 Figure 11: Mixture: Truck (stem 1) + Banjo (Stem 2) Figure 12: Mixture: Chainsaw (stem 1) + Accordion (Stem 2) 17 Figure 13: Mixture: Train Horn (stem 1) + Bark (Stem 2) Figure 14: Mixture: Male Speech (stem 1) + Airplane (Stem 2) 18 Figure 15: Mixture: Truck (stem 1) + Ukulele (Stem 2) Figure 16: Mixture: Bark (stem 1) + Toilet Flush (Stem 2)"
        }
    ],
    "affiliations": [
        "Columbia University",
        "Tencent America",
        "University of Rochester"
    ]
}