{
    "paper_title": "Knowledge Transfer Across Modalities with Natural Language Supervision",
    "authors": [
        "Carlo Alberto Barbano",
        "Luca Molinaro",
        "Emanuele Aiello",
        "Marco Grangetto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a way to learn novel concepts by only using their textual description. We call this method Knowledge Transfer. Similarly to human perception, we leverage cross-modal interaction to introduce new concepts. We hypothesize that in a pre-trained visual encoder there are enough low-level features already learned (e.g. shape, appearance, color) that can be used to describe previously unknown high-level concepts. Provided with a textual description of the novel concept, our method works by aligning the known low-level features of the visual encoder to its high-level textual description. We show that Knowledge Transfer can successfully introduce novel concepts in multimodal models, in a very efficient manner, by only requiring a single description of the target concept. Our approach is compatible with both separate textual and visual encoders (e.g. CLIP) and shared parameters across modalities. We also show that, following the same principle, Knowledge Transfer can improve concepts already known by the model. Leveraging Knowledge Transfer we improve zero-shot performance across different tasks such as classification, segmentation, image-text retrieval, and captioning."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 1 1 6 5 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Knowledge Transfer Across Modalities with Natural Language Supervision",
            "content": "Carlo Alberto Barbano University of Turin carlo.barbano@unito.it Luca Molinaro University of Turin l.molinaro@unito.it Emanuele Aiello Politecnico di Torino emanuele.aiello@polito.it Marco Grangetto University of Turin marco.grangetto@unito.it"
        },
        {
            "title": "Abstract",
            "content": "We present way to learn novel concepts by only using their textual description. We call this method Knowledge Transfer. Similarly to human perception, we leverage cross-modal interaction to introduce new concepts. We hypothesize that in pre-trained visual encoder there are enough low-level features already learned (e.g. shape, appearance, color) that can be used to describe previously unknown high-level concepts. Provided with textual description of the novel concept, our method works by aligning the known low-level features of the visual encoder to its high-level textual description. We show that Knowledge Transfer can successfully introduce novel concepts in multimodal models, in very efficient manner, by only requiring single description of the target concept. Our approach is compatible with both separate textual and visual encoders (e.g. CLIP) and shared parameters across modalities. We also show that, following the same principle, Knowledge Transfer can improve concepts already known by the model. Leveraging Knowledge Transfer we improve zero-shot performance across different tasks such as classification, segmentation, image-text retrieval, and captioning. 1. Introduction Can blind person who gained sight recognize the objects he previously knew only by touch? This is philosophical riddle posed by William Molyneux in 1668 to John Locke [37], which has been relevant in vision neuroscience for decades. Recent research has shown that, while this does not happen immediately after sight restoration, cross-modal mappings develop rapidly in human subjects, within days [14]. While recent research in multimodal neural networks has focused on this cross-modal interaction [50], in this paper we aim to answer slightly revisited version of Molyneuxs riddle, in which our model already has some previous visual knowledge of the world. We postulate that prior knowledge about low-level visual features is enough to produce reasonable visual representation of an unknown concept if an explicative textual description is provided. This prior knowledge can be obtained with multimodal pre-training, for example by employing image-text alignment as done in CLIP and other similar works [11, 26, 45, 61]. Leveraging natural language supervision to learn novel visual concepts is process we call Knowledge Transfer. An illustrative example of the goal of Knowledge Transfer is shown in Fig. 1, where CLIP-based zero-shot classifier is presented with unknown concepts. We propose two possible ways of achieving Knowledge Transfer, either explicitly or implicitly. In Explicit Knowledge Transfer, starting from the textual description of the novel concept, we synthesize matching imaging via model inversion [24] that can be later used to fine-tune the model with visual-text matching loss. Implicit Knowledge Transfer, on the other hand, could rely on multimodal neurons [50] by employing only textual captions and fine-tuning the model with masked language modeling. This, however, would require parameters to be shared across visual and textual encoders. In this work, we focus on Explicit Knowledge Transfer as it has less strict requirement for model architecture. Our findings show that: 1. We can successfully introduce novel concepts in pretrained visual models with only textual descriptions. 2. Knowledge Transfer can also improve the visual accuracy on already existing concepts. 3. Knowledge Transfer can improve zero-shot downstream tasks such as classification, segmentation, and image-text retrieval and shows potential for out-of-domain generalization. 2. Related Works Research in multimodal representation learning aims to bridge the gap between different modalities (e.g. visual and textual representations), enabling models to process 1 (a) CLIP (B) Top-3 zero-shot predictions: (Triumphal Arch, Stone Wall, Steel Arch Bridge). CLIP (B) w/ Knowledge Transfer Top-3 zero-shot predictions: (Moongate, Triumphal Arch, Stone Wall) (b) CLIP (L) Top-3 zero-shot predictions: (Cocktail Shaker, Odometer, Dragonfly). CLIP (L) w/ Knowledge Transfer Top-3 zero-shot predictions: (Tonometer, Cocktail Shaker, Espresso Maker) Figure 1. Knowledge Transfer can introduce novel concepts in multimodal model, by leveraging prior visual knowledge of the visual encoder and textual description of the target concept. In the example, CLIP model [45] learns the concepts Moongate and Tonometer, without using any real image, while retaining good accuracy on general zero-shot classification (58.10% vs 56.43% and 70.79% vs 70.61% on ImageNet-1k). them jointly. Vision-Language Models (VLMs) like CLIP [45], CoCa [61], Flamingo [3] and ImageBind [11], align visual and textual features in shared embedding space, empowering zero-shot and few-shot learning in various visual tasks. Understanding how VLMs internally process cross-modal information has been advanced by the study of multimodal neurons [12, 43, 50]. Schwettmann et al. [50] identified specific neurons responsible for integrating visual and textual modalities, enhancing model interpretability. This insight drives the intuitions behind our method, by highlighting the presence of multimodal neurons. Cross-Modal Knowledge Transfer Cross-modal knowledge distillation [13, 18, 53] is strategy for transferring knowledge between different modalities to enrich representations. Methods like VidLANDK [53] and C2KD [18] employ modality-bridging techniques to improve generalization in zero-shot and few-shot scenarios. These approaches typically require substantial multimodal data and complex training procedures. In contrast, our method utilizes textual descriptions to introduce new visual concepts with minimal data, focusing on efficient knowledge integration without extensive multimodal datasets. Alternative approaches [7, 20, 65] involve synthetic data generation to train discriminative models. For example [65] leverages Stable Diffusion [47] to create diverse training samples, addressing data scarcity. While effective, this approach depends on the quality and diversity of generated data. Our approach differs by integrating novel concepts into existing models without relying on external knowledge of text-to-image generative model and computationally expensive data generation pipeline. Additionally, text-only training methods have been proposed to enhance visual understanding. For example, CapDec [41] leverages the alignment between visual and text encoder in CLIP in order to improve captioning by only using textual captioning data. Our approach, on the other hand, aims at achieving such alignment on unknown concepts, by leveraging free-form textual descriptions. More recently, few-shot cross-modal learning has been explored [35], demonstrating that integrating cues from multiple modalities can enhance concept learning, mirroring human learning. Their approach leverages few-shot examples of paired multimodal data to enhance unimodal downstream tasks. Differently from them, in this work, we use single-modal text data to introduce new visual knowledge. 3. Method In this section, we present our proposed method for Knowledge Transfer. We focus on Explicit Knowledge Transfer, which is based on inversion, as it is more broadly applicable to different architectures1. Fig. 2 illustrates graphical overview of the method. 3.1. Explicit Knowledge Transfer Let fT : RL Rn be text encoder (where is the sequence length) and fV : Rwh Rn be visual encoder (with and being the size of the image), our goal is to introduce new concepts into fV through fT using only the text modality. Let XT be set of unpaired captions pertaining to novel concept that we want to learn, and set of ideal ground truth images corresponding to that concept. What we would like to achieve is: sim(fv(x v), ft(xt)) (cid:123)(cid:122) (cid:125) (cid:124) st sim(fv(x v), ft(xk)) (cid:123)(cid:122) (cid:125) sk (cid:124) > (1) , xt XT , xk XK where XK is the set of all other captions pertaining to other concepts (XK XT = ). The condition in Eq. 1 means that all ideal visual samples should be mapped closer to the true corresponding captions then all other captions. In practice, if is available, we can satisfy Eq. 1 by optimizing its approximation [4]: min fV 1 (cid:88) 1 XT (cid:88) log xt exp(st) exp(st) + (cid:80) exp(sk) xk (2) which corresponds to the InfoNCE loss [6, 25, 45]. In our setting, however, is not available, thus we can try estimating it (e.g. with model inversion [24]) and then use the estimated values to jointly train fV and fT with the contrastive approach using InfoNCE [11, 45]. 1For the sake of completeness, we provide brief presentation of implicit transfer in the supplementary material. 2 Figure 2. Graphical overview of Knowledge Transfer. Starting from textual description of the target concept, we synthesize images via model inversion (left) then, using an image-text matching loss, we fine-tune the visual encoder to match the concept (right). In this way, we leverage prior knowledge contained in the model (from pre-training) to learn novel concepts. As practical example, caption xt for the concept Moongate could be perfectly circular archway built from uniformly cut stones or bricks, set into larger wall [...] as illustrated in Fig. 3. As stated in the introduction, this method works if the visual encoder already has some prior knowledge about the low-level visual attributes contained in the caption, for example, it has already been jointly pre-trained with the text encoder. An interesting point to analyze could be how much prior is needed for successful Knowledge Transfer: we leave this question for future research. by inversion 3.1.1. Estimating The most straightforward way to estimate would be to compute it by inverting the visual encoder fV starting from the textual embeddings of XT , in order to obtain an approximation ˆX . To do so, we solve the following optimization problem, starting from random noise: (fT (XT )) ˆX = 1 max ˆX sim(A(fV ( ˆX )), fT (XT )) + αR( ˆX ) (3) where, as in [24], 1 is the inversion operator, is random augmentation operation applied at each step (e.g. random affine) and is regularization term based on Total Variation (TV) [40], weighted by α. Augmentation and regularization help in producing more naturally-looking images. One question that may arise is why we do not employ generative models (e.g. DALL-E [46]) to synthesize training images based on textual descriptions. The reason is twofold: i.) we do not have control over the training dataset of such generative models, hence we do not know if they already include the concepts we target; ii.) using external generative models for augmenting the training set [7, 20, 48, 3 63, 65] is out of the scope of our research question, which is transferring the knowledge of single model from textual to visual modality. 3.1.2. Finetuning on the new concepts After images ˆX have been synthesized via model inversion, we can use them to train fv and fT with an image-text alignment objective such as InfoNCE (Eq. 2). In order to successfully match visual features to the desired concepts, we augment XT by prepending the corresponding concept name to each caption. In the example presented earlier, the fine-tuning caption would be represented by moongate is perfectly circular archway built from uniformly cut stones [...]. This step is required in order to finally map the lowlevel visual features to the high-level concept itself. 4. Experiments With an extensive evaluation on different datasets and domains, we aim to rigorously evaluate the potential of Knowledge Transfer. This section is divided into two parts: in the first we focus on learning completely novel concepts that were previously unknown, while in the second we focus on improving the performance of zero-shot downstream tasks. 4.1. Datasets We employ variety of datasets in different domains and for different downstream tasks. Here we provide complete list, divided by task. Note that we do not use any training data from these datasets, as we only use them for testing. All improvements come from the textual description. Natural images classification 1.) RareConcepts is collection of images of rare concepts gathered from the web. We release the dataset as part of (a) Moongate. Caption: perfectly circular archway built from uniformly cut stones or bricks, set into larger wall. It forms smooth circle, framing views of gardens or landscapes beyond, creating picturesque portal. (b) Tonometer. Caption: slender, pen-like probe attached to small base equipped with precise dials and gauges. This tool is often part of larger medical apparatus, featuring metallic finish and refined, professional appearance. Figure 3. Example of inverted images (top) and real images (bottom) from rare concepts that CLIP struggles to classify correctly. this work. In our experiments, we focus on three concepts that are relatively unknown to different large multi-modal architectures: Moongate, Gyroscope and Tonometer. For each concept, we collect 10 images. 2.) ImageNet-1k [8] is large-scale benchmark for visual recognition, with 1000 classes and 3.2M natural images. Medical images classification 3.) CheXpert-2x500c [17] is dataset of Chest X-Rays obtained from the large-scale CheXpert dataset [19] by considering 200 examples for the classes Atelectasis, Cardiomegaly, Edema, Consolidation, and Pleural Effusion. 4.) JSRT [52] is Chest X-Ray dataset containing 154 conventional chest radiographs with lung nodule of different types (malignant and benign nodules). Medical images segmentation 5.) UnitoChest [5] is collection of 306,440 chest CT slices coupled with nodules segmentation masks. We consider slices where nodules are present, for total of 4179 images. 6.) UDIAT [59] is dataset of breast masses in ultrasound images, containing 110 benign and 54 malignant cases. 7.) SIIM Pneumothorax [62] is Chest X-ray dataset for pneumothorax segmentation, released as challenge in 2019. We consider total of 500 images. 8.) BraTS23 Glioma [1] is brain MRI dataset of adult patients with brain gliomas. We consider all slices where tumor is present for total of 14,746 images. Image-Text retrieval and captioning 9.) Flickr30k [60] is dataset of 31,783 images from Flickr, each one associated with 5 captions provided by human annotators. For our experiments, we used Karpathys test split [23], which contains 1000 images and 5000 captions. 10.) MSCOCO [34] is large-scale dataset of more than 330k images with textual captions. We use Karpathys test split [23], containing 5000 images. 4.2. Setup Captioning To produce descriptive captions for the new concepts, we employ LLM-based approach. Specifically, for natural images, we use Llama-3 Instruct (with 8B parameters) [2] with the following prompt: Generate small description of the ImageNet class <class name> without using the word itself. The description must contain visual cues useful for recognizing the subject with low-level and accurate details. Please dont insert anything else in the response except the description., where we insert the appropriate class name for each new concept. Note that we employ an LLM only for the sake of convenience (e.g. captioning all 1000 ImageNet classes), but this is not requirement. For medical data, we actually employ mix of hand-crafted captions based on Radiopaedia [54] augmented with some elements from ChatGPT-4 [42]. All captions can be found in the supplementary material. Inversion We run inversion for 5k steps, using cosine learning rate annealing schedule. For the regularization term, we use the default value α = 0.005 [24]. The augmentation we employ is composed of random affine transformations (rotation comprised between -30 and +30 degrees, translation of 10%, and scaling comprised between 70% and 100% of the image size), with probability of 0.5. An example of inverted images can be found in Fig. 3. For each concept, we generate ten inverted samples. Fine-tuning Fine-tuning is performed using the InfoNCE loss (Eq. 2) to achieve alignment between the inverted images and the textual descriptions. We only fine-tune the visual encoder, while keeping the text encoder frozen. The motivation is that we wish to align features extracted from the visual encoder to those extracted from the text encoder. For most experiments, we perform quick fine-tuning consisting of only one single epoch, with small learning rates between 106 and 104. For CLIP-based models, we generally employ weight decay of 0.2 as in [45]. More details are provided in the description of each experiment. 4.3. Learning novel concepts In this first experimental section, we focus on learning novel concepts that are not known by the models. As first demonstration of Knowledge Transfer, we employ the RareConcepts dataset, comprising three rare classes: Moongate, 4 Model Concept CLIP ViT-B/32 [45] Moongate Tonometer Target Acc. ImageNet 0-shot Target Acc. ImageNet 0-shot Gyroscope Target Acc. ImageNet 0-shot Baseline 1e-5 2e-5 3e-5 4e-5 5e-5 Learning Rate 0% 100% 58.10% 57.78% 56.43% 53.95% 50.37% 42.30% 100% 90% 10% 60% 50% 100% 58.10% 57.52% 55.62% 51.98% 42.80% 23.73% 100% 100% 80% 80% 90% 100% 58.10% 57.86% 56.84% 53.96% 48.28% 34.48% 100% 100% 100% 100% CLIP ViT-L/14 [45] Moongate Target Acc. ImageNet 0-shot 78.95% 78.95% 100% 100% 100% 70.79% 70.74% 70.51% 69.96% 68.57% 62.35% 100% Tonometer Target Acc. ImageNet 0-shot 31.58% 52.63% 78.95% 100% 100% 70.79% 70.74% 70.61% 70.08% 69.06% 66.92% 100% Gyroscope Target Acc. ViLT [26] Moongate Tonometer ImageNet 0-shot Target Acc. ImageNet* 0-shot Target Acc. ImageNet* 0-shot 90% 100% 100% 100% 70.79% 70.65% 70.42% 69.84% 69.39% 68.35% 100% 90% 0% 0% 0% 0% 0% 0% 23.74% 23.90% 24.02% 24.16% 24.18% 24.16% 10% 30% 30% 30% 40% 40% 23.74% 23.88% 24.02% 24.04% 24.22% 23.94% Gyroscope Target Acc. 50% 60% 50% 50% 40% 30% ImageNet* 0-shot 23.74% 23.80% 23.88% 23.72% 23.38% 23.12% Table 1. Knowledge Transfer on novel and rare concepts (CLIP and ViLT). * for VilT, we employ ImageNet-100 [22] due to the computational requirements of evaluating every possible image-caption pair for zero-shot classification. Tonometer, and Gyroscope. These classes were selected by manually probing CLIP-based models on different and potentially rare concepts, gathered from the web. We target Knowledge Transfer on two variants of CLIP base and large (based on ViT-B/32 and ViT-L/14) and also sharedparameter architecture ViLT [26] based on ViT-B/32. For CLIP, we use the official pre-trained models released by OpenAI 2 and the inversion-finetuning setup is as described in Sec. 4.2. For ViLT we use slightly different approach, in order to accommodate the different architecture. To run inversion, we start from pair of input < xt, ˆx (0; 1) > composed by the textual caption and random noise. We then optimize ˆx by maximizing the image-text matching (ITM) score computed on the ITM head of ViLT [26]. The rest of the setup is the same as CLIP. More details are provided in the supplementary material. The results are presented in Tab. 1. We evaluate the zeroshot classification accuracy before and after applying Knowledge Transfer. Some of the captions we used for inversion are reported in Fig. 3; all captions are listed in the supplementary material. First, we notice that pre-trained models, indicated as baseline, show different unknown concepts: CLIP ViT-B/32 struggles on Moongate (0% accuracy) and Tonometer, CLIP ViT-L/14 struggles on Tonometer, while 2https://github.com/openai/CLIP ViLT exhibits the lowest accuracy overall, struggling more on Moongate and Tonometer. We report Knowledge Transfer results with different fine-tuning learning rates. Overall, models successfully learn novel concepts as demonstrated by the increase of zero-shot classification accuracy on each concept. We additionally report the accuracy of each finetuned model on ImageNet to assess whether the fine-tuning leads to catastrophic forgetting of previous knowledge [28]. With proper choice of learning rate, we achieve an increase in target accuracy while retaining similar accuracy on ImageNet. Notably, for some concepts, we achieve 100% while maintaining comparable results on ImageNet (e.g. for CLIP base and large). The only instance in which the target accuracy is not improved is represented by ViLT on Moongate, which remains at 0%; however, we notice slight increase on ImageNet on all concepts, hinting that Knowledge Transfer may lead to improved representations of existing concepts. 4.3.1. Ablation study We perform an ablation study on our fine-tuning strategy. In our experiments, during fine-tuning, we freeze the text encoder and only train the visual encoder. Here we evaluate fine-tuning with different configurations. The results are illustrated in Fig. 4. When fine-tuning both encoders, we observe rapid collapse of target accuracy and ImageNet accuracy for all concepts. We also observe similar trend 5 Figure 4. Comparison of fine-tuning strategies. Fine-tuning both the text and the visual encoders, or just the text encoder leads to collapse in accuracy. Fine-tuning only the visual encoder correctly aligns prior visual features to the novel concept. good choice of learning rate leads to higher accuracy on the novel concept (target) while limiting catastrophic forgetting on previous tasks (imagenet). when fine-tuning the text encoder only, while leaving the visual encoder frozen. This is, however, expected as our assumption is that the knowledge contained in the text encoder is already good enough to represent the target concept, and we just wish to align visual features to it. Moreover, if we alter the text encoder weights, correspondence between captions and inverted images may be lost, leading to degenerate cases. We perform additional ablation studies in the supplementary material. 4.3.2. Experiments with MedCLIP Next, we target Knowledge Transfer on medical imaging. Medical images are perfect task for Knowledge Transfer, as we can leverage existing medical knowledge in the form of text (e.g. from medical textbooks and encyclopedias) to accurately describe concepts and visual appearance of different pathologies on images such as Chest X-rays (CXR), Computed Tomography (CT) scans, Magnetic Resonance Images (MRI), and Ultrasound images. Our experiments are based on the MedCLIP architecture [56], which is CLIP-based model employing BioClinicalBERT3 as backbone for the text encoder and Swin Transformer [36] for the visual encoder. MedCLIP is pre-trained on the largescale MIMIC-CXR [21] and CheXpert [19] datasets, which contain CXR images along with radiological reports. The different concepts that can be found in the dataset are Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung Opacity, Pleural Effusion, Pneumonia, Pneumothorax, Support Devices. In our experiments, we aim to introduce the concept of benign and malignant nodules in CXRs in MedCLIP. We follow the same experimental protocol as with CLIP, and we measure the performance achieved by Knowledge Transfer on the external dataset JSRT [52]. The zero-shot classification results on JSRT are reported in Tab. 2. The captions used for inversion are listed in the sup3https : / / huggingface . co / emilyalsentzer / Bio _ ClinicalBERT plementary material. As before, we measure the zero-shot accuracy on previous knowledge using CheXpert-5x200c, to spot instances of catastrophic forgetting. We achieve an improved detection accuracy of 92.86% for malignant nodules, compared to the baseline accuracy of 83.93%, while retaining comparable results on the source dataset CheXpert5x200c. On benign nodules, we struggle to improve the accuracy, probably due to the less discriminative features of benign nodules on CXR images, compared to malignant ones. However, as also noticed in the previous experiments, we observe slight increase in accuracy on the source dataset, hinting an improvement in the models representations. 4.3.3. Out of domain Knowledge Transfer Lastly, we assess the potential of Knowledge Transfer to introduce novel concepts outside of the training domain. Specifically, we aim to introduce medical concepts into model trained on natural images. For this purpose, we finetune CLIP model on all five CheXpert classes (atelectasis, cardiomegaly, consolidation, edema, and pleural effusion). The results are reported in Tab. 3. We report the performance of MedCLIP as reference for model trained on CheXpert. Looking at the top-1 accuracy, we achieve improved results with both versions of CLIP, with the large variant scoring higher increase from 22.40% to 25.90%. However, breaking down the accuracy per class reveals that i.) classes with starting accuracy of 0% did not improve, and ii.) performance in some classes got worse (i.e. pleural effusion and atelectasis). This may be due to the domain gap between the prior knowledge of the model (natural images) and the features specific to the medical domain. Nevertheless, considering this limitation, Knowledge Transfer shows potential in zero-shot out-of-domain generalization. 4.4. Improving zero-shot downstream tasks In the second experimental part, we focus on improving the performance of zero-shot downstream tasks, focusing on both novel and known concepts. Namely, we target segmen6 Concept Baseline 2 3 4 5 Benign Nodule Target Acc. (base lr 1e-5) CheXpert-5x200c 0-shot 54.55% 54.55% 54.55% 54.55% 54.55% 54.55% 62.10% 61.80% 62.30% 62.10% 62.20% 62% Lung Cancer Target Acc. (base lr 1e-4) CheXpert-5x200c 0-shot 83.93% 87.50% 92.86% 94.64% 92.86% 92.86% 62.10% 62.20% 61.50% 53.70% 48.20% 44.50% Learning Rate (multiplier) Table 2. Knowledge Transfer on MedCLIP on the JSRT dataset. The model successfully learns the novel concept of malignant nodules (lung cancer) on CXR images. Benign nodules, on the other hand, are harder to visually differentiate from other findings in CXRs. Model Atelectasis Cardiomegaly Consolidation Edema Pleural Effusion Top-1 MedCLIP (ViT) Reference 49% CLIP ViT-B/ CLIP ViT-L/14 Baseline Transfer Baseline Transfer 0% 0% 59.50% 4% 69.50% 2.5% 21.5% 16.50% 32.5% 32.50% 75.50% 84% 0% 0% 0% 0% 0% 0% 0% 0% 94.50% 85% 35.50% 92.5% 62.10% 19.40% 21.30% 22.40% 25.90% Table 3. Learning novel concepts in different domain (from natural images to medical images) shows potential. Tested on CheXpert-5x200c. tation, image-text retrieval, and captioning. 4.4.1. Segmentation For segmentation, we employ the zero-shot method MedCLIP-SAMv2 [29, 30]. MedCLIP-SAMv2 works by computing activation maps from pre-trained CLIP model, and using them as query for the Segment Anything Model (SAM) [27]. Activation maps are computed using MultiModal Information Bottleneck Attribution (M2IB) [55], using target image and query prompt. Here, we aim at improving the quality of the activation maps on different concepts by leveraging Knowledge Transfer. This, in turn, should result in higher accuracy of the final segmentation. We target four different segmentation tasks: lung nodules segmentation on CT images (UnitoChest), lung pneumothorax segmentation on CXR images (SIIM Pneumothorax), breast nodule segmentation on ultrasound images (UDIAT), and glioma segmentation in MRIs (BraTS23). The overall results across all segmentation tasks are presented in Tab. 4. The captions used for inversion are reported in the supplementary material. To compute the M2IB activation maps on the fine-tuned models, we employ descriptive prompts as suggested in [30]. The prompts are reported in Tab. 4 as P1 to P4 for each task. We also report reference results of MedCLIP-SAMv2 on each task. Compared to the original setting of MedCLIP-SAMv2, lung nodules and lung pneumothorax are completely novel concepts. There is also slight difference in the brain glioma class compared to the original brain tumor task, explained in the supplementary file. We employ three metrics to assess the segmentation quality, namely the Dice-Sørensen Coefficient (DSC), Normalized Surface Distance (NSD), and Intersection over Union (IoU). We report results with different values of fine-tuning learning rate. We can observe an increase in segmentation metrics across all tasks, notably in breast ultrasound (NSD 59.44% to 61.56%) and brain MRIs (NSD 20.97% to 22.26%). For lung nodules and pneumothorax, the improvement is less pronounced, probably because the novelty of the task makes improving more difficult in the MedCLIP-SAM setting. 4.4.2. Text and image retrieval We perform experiments on text and image retrieval on the Flickr30k dataset. For these experiments, we employ the huggingface version4 of ViLT [26]. To fine-tune ViLT with Knowledge Transfer, we employ captions of common concepts that may help improve the models general knowledge. For this purpose, we use the 80 object categories from MSCOCO as target concepts, using the method presented in Sec. 4.2, using ChatGPT-4. All captions are reported in the supplementary material. For each caption, we generate 10 inverted images, for total of 800 inverted images. Finetuning is performed as in Sec. 4.3, by maximizing the ITM score for positive pairs and minimizing it for negative ones. The results of zero-shot text and image retrieval, before and after Knowledge Transfer, are reported in Tab. 5. For comparison, we also report the original results of ViLT from [26], alongside other relevant baselines. Results are shown in terms of recall (marked as R) computed at different levels (top-1, top-5, and top-10 recall). As we observe from the results, Knowledge Transfer consistently improves the results across all metrics for both image and text retrieval tasks. Notably, we score an improvement of almost 1% from 73.8% to 74.6% on the text retrieval task. Additional configurations are available in the supplementary material. 4https://huggingface.co/dandelin/viltb32mlmitm Model DSC NSD IoU DSC NSD IoU DSC NSD IoU DSC NSD IoU Lung Nodules Lung Pneumothorax Breast Ultrasound Brain MRI MedCLIP-SAMv2 Transf. (1e-5) Transf. (2e-5) Transf. (1e-4) 14.83% 17.30% 8.64% 6.30% 7.61% 3.75% 56.25% 59.44% 47.81% 17.20% 20.97% 12.05% 13.95% 17.45% 8.75% 6.28% 7.59% 3.77% 58.23% 61.56% 49.52% 15.90% 19.36% 11.10% 14.10% 17.65% 8.83% 6.41% 7.76% 3.83% 54.36% 57.30% 46.30% 18.13% 22.26% 12.62% 14.35% 18.03% 9.04% 6.02% 7.29% 3.59% - - - - - - Table 4. Improvements in zero-shot segmentation. denotes novel concepts that are not included in the original MedCLIP-SAMv2 training data [29]. Prompts used for segmentation are reported here: P1 medical chest CT scan showing circular spots of varying size within the lungs, suggesting either benign or malignant nodules; P2 medical chest x-ray showing an abnormal collection of air within the pleural cavity, suggesting pneumothorax; P3 medical breast mammogram showing an irregularly shaped, spiculated mass suggestive of malignant breast tumor; P4 brain MRI showing bright or dark mass with irregular edges suggestive of brain tumor or glioma. Model R@1 R@5 R@ R@1 R@5 R@10 Flickr30k (1K) Text Retrieval Image Retrieval ViLBERT [38] Unicoder-VL [31] ImageBERT [44] ViLT-B/32 (original) [26] - - - 31.9% 61.1% 72.8% 64.3% 85.8% 92.3% 48.4% 76.0% 85.2% 70.7% 90.2% 94.0% 54.3% 79.6% 87.5% 73.2% 93.6% 96.5% 55.0% 82.5% 89.8% ViLT-B/32 (huggingface) ViLT-B/32 (transf. 9e-7) ViLT-B/32 (transf. 2e-6) 73.8% 93.5% 96.5% 57.3% 83.9% 90.4% 74.6% 93.8% 96.4% 57.8% 84.0% 90.5% 74.6% 93.7% 96.5% 57.8% 84.0% 90.5% Table 5. Text and image retrieval on Flickr30k. Recall scores are shown at top 1, 5 and 10 levels. Our results are based on huggingfaces ViLT. Original results and other comparisons from [26]. Model BLEU@4 METEOR CIDEr SPICE MSCOCO (5K) CLIP-ViL [51] BLIP [32] VinVL [64] SimVLM [57] LEMON [16] CoCa [61] (proprietary) CoCa CoCa (transf. 8e-5) CoCa FT CoCa FT (transf. 5e-6) 40.2 40.4 41.0 40.6 41.5 40.9 6.9 17.9 34.9 35. 29.7 - 31.1 33.7 30.8 33.9 12.8 19.4 29.7 29.8 134.2 136.7 140.9 143.3 139.1 143.6 31.1 60.8 123.1 124. 23.8 - 25.4 25.4 24.1 24.7 9.1 13.7 23.5 23.3 Table 6. Image captioning on MSCOCO. CoCa refers to the baseline model pre-trained on LAION-2B [49], while CoCa FT refers to the model fine-tuned for captioning on MSCOCO. We highlight in bold the best results and the improvements by Knowledge Transfer. 4.4.3. Captioning We perform experiments on captioning on the MSCOCO dataset. For this task, we employ the CoCa architecture [61], which is state-of-the-art captioner. Specifically, we employ the open-source version released by LAION5 as the original one is proprietary. CoCa is built by adding an autoregressive text decoder to CLIP model, thus when fine-tuning we apply the InfoNCE loss jointly with captioning loss [61] which aims at predicting the next token yt given the previous tokens y<t and the image x. As captions, we utilize simple set of templates such as photo of X, containing different alterations. They are listed in the supplementary material. Results are presented in Tab. 6. We report different evaluation metrics (BLEU, METEOR, CIDEr, SPICE) computed using the standard pycocoevalcap package6. We perform experiments with two variants of CoCa: one pre-trained on LAION-2B [49], and one further fine-tuned for captioning on MSCOCO. We also report reference results from proprietary CoCa [61] for comparison, along with other methods. With Knowledge Transfer, we improve on CoCa FT across almost all metrics, reaching BLEU@4 of 35.2. notable result is achieved with the pre-trained only CoCa, where we improve all metrics by large margin, sometimes even doubling them (e.g. BLEU@4 from 6.9 to 17.9). We want to point out again that this model is not originally trained for captioning on MSCOCO, and the improvement is introduced by Knowledge Transfer alone, without using any real image at all. Even though the performance of open CoCa does not match the proprietary results reported in the original paper, thus preventing us from reaching state-of-the-art results, the improvement introduced by Knowledge Transfer is notable. 5. Conclusions and Future Works We present way to learn novel visual concepts by only using their textual descriptions, with method we call knowledge transfer. Through extensive evaluation, we show that knowledge transfer can successfully introduce novel concepts in pre-trained models, without hurting performance on previous tasks. We also show that knowledge transfer can improve the results of downstream zero-shot tasks, such as segmentation, text-image retrieval, and captioning, and also shows potential for out-of-domain generalization, for example on medical images. The proposed method is based on model inversion to synthesize ideal images for target concept, that are later used to fine-tune the model in an image-text matching fashion, such as CLIP [45]. Our method lever5https://github.com/mlfoundations/open_clip 6https://github.com/salaniz/pycocoevalcap 8 ages prior knowledge in pre-trained models, with the aim of aligning known low-level visual features to novel highlevel concepts. While in this paper we focused on Explicit Knowledge Transfer, we hypothesize that Implicit Knowledge Transfer is also achievable by exploiting multi-modal neurons. Future research will focus on this topic."
        },
        {
            "title": "References",
            "content": "[1] Maruf Adewole, Jeffrey Rudie, Anu Gbdamosi, Oluyemisi Toyobo, Confidence Raymond, Dong Zhang, Olubukola Omidiji, Rachel Akinola, Mohammad Abba Suwaid, Adaobi Emegoakor, et al. The brain tumor segmentation (brats) challenge 2023: glioma segmentation in sub-saharan africa patient population (brats-africa). ArXiv, 2023. 4 [2] AI@Meta. Llama 3 model card. 2024. 4 [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [4] Carlo Alberto Barbano, Benoit Dufumier, Enzo Tartaglione, Marco Grangetto, and Pietro Gori. Unbiased supervised contrastive learning. In The Eleventh International Conference on Learning Representations, 2023. 2 [5] Hafiza Ayesha Hoor Chaudhry, Riccardo Renzulli, Daniele Perlo, Francesca Santinelli, Stefano Tibaldi, Carmen Cristiano, Marco Grosso, Giorgio Limerutti, Attilio Fiandrotti, Marco Grangetto, et al. Unitochest: lung image dataset for segmentation of cancerous nodules on ct scans. In International Conference on Image Analysis and Processing, pages 185196. Springer, 2022. 4 [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning In International conference on of visual representations. machine learning, pages 15971607. PMLR, 2020. 2 [7] Yunhao Chen, Zihui Yan, and Yunjie Zhu. unified framework for generative data augmentation: comprehensive survey. arXiv preprint arXiv:2310.00277, 2023. 2, 3 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171 4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. 5 [10] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24142423, 2016. 1 [11] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 1, 2 [12] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 6(3):e30, 2021. 2, 1 [13] Saurabh Gupta, Judy Hoffman, and Jitendra Malik. Cross modal distillation for supervision transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28272836, 2016. [14] Richard Held, Yuri Ostrovsky, Beatrice de Gelder, Tapan Gandhi, Suma Ganesh, Umang Mathur, and Pawan Sinha. The newly sighted fail to match seen with felt. Nature neuroscience, 14(5):551553, 2011. 1 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 1, 2 [16] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up visionIn language pretraining for image captioning. 2022 ieee. CVF Conference on computer vision and pattern recognition (CVPR), pages 1795917968, 2021. 8, 5 [17] Shih-Cheng Huang, Liyue Shen, Matthew Lungren, and Serena Yeung. Gloria: multimodal global-local representation learning framework for label-efficient medical image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 39423951, 2021. 4 [18] Fushuo Huo, Wenchao Xu, Jingcai Guo, Haozhao Wang, and Song Guo. C2kd: Bridging the modality gap for cross-modal In Proceedings of the IEEE/CVF knowledge distillation. Conference on Computer Vision and Pattern Recognition, pages 1600616015, 2024. 2 [19] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, pages 590597, 2019. 4, 6 [20] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. Generative models as data source for multiview represenIn International Conference on Learning tation learning. Representations, 2022. 2, [21] Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Roger Mark, and Steven Horng. Mimic-cxr, deidentified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. 6 [22] Kaggle. ImageNet100. https://www.kaggle.com/ datasets/ambityga/imagenet100. [Accessed Nov. 2024]. 5 [23] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 31283137, 2015. 4 9 [24] Hamid Kazemi, Atoosa Chegini, Jonas Geiping, Soheil Feizi, and Tom Goldstein. What do we learn from inverting clip models? arXiv preprint arXiv:2403.02580, 2024. 1, 2, 3, 4 [25] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:1866118673, 2020. 2 [26] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-andlanguage transformer without convolution or region supervision. In International conference on machine learning, pages 55835594. PMLR, 2021. 1, 5, 7, 8, [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 7 [28] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13): 35213526, 2017. 5 [29] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Medclip-sam: Bridging text and image towards universal medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 643653. Springer, 2024. 7, 8 [30] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Medclip-samv2: Towards universal text-driven medical image segmentation. arXiv preprint arXiv:2409.19483, 2024. 7, 2 [31] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: universal encoder for vision and language by cross-modal pre-training. Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):1133611344, 2020. 8 [32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionIn International language understanding and generation. conference on machine learning, pages 1288812900. PMLR, 2022. 8, 5 [33] Liunian Harold Li, Haoxuan You, Zhecan Wang, Alireza Zareian, Shih-Fu Chang, and Kai-Wei Chang. Unsupervised vision-and-language pre-training without parallel images and captions. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 53395350, Online, 2021. Association for Computational Linguistics. 1 [34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [35] Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, and Deva Ramanan. Multimodality helps unimodality: Crossmodal few-shot learning with multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1932519337, 2023. 2 [36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 6 [37] John Locke. An essay concerning human understanding, 1690. 1948. 1 [38] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2019. 8 [39] Shentong Mo and Pedro Morgado. Unveiling the power of audio-visual early fusion transformers with dense interactions through masked modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2718627196, 2024. 1 [40] A. Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural networks. 2015. 3 [41] David Nukrai, Ron Mokady, and Amir Globerson. Text-only training for image captioning using noise-injected clip. arXiv preprint arXiv:2211.00575The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. 2 [42] OpenAI. Chatgpt, 2024. Nov 2024 Version. 4 [43] Haowen Pan, Yixin Cao, Xiaozhi Wang, and Xun Yang. Finding and editing multi-modal neurons in pre-trained transformer. arXiv preprint arXiv:2311.07470, 2023. 2 [44] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data, 2020. 8 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 2, 4, 5, 8 [46] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. 2022 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2 [48] Veit Sandfort, Ke Yan, Perry Pickhardt, and Ronald Summers. Data augmentation using generative adversarial networks (cyclegan) to improve generalizability in ct segmentation tasks. Scientific reports, 9(1):16884, 2019. 3 [49] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia 10 captioners are image-text foundation models. Transactions on Machine Learning Research, 2022. 1, 2, 8, 5 [62] Anna Zawacki, Carol Wu, Shih George, Julia Elliott, Mikhail Fomitchev, Hussain Mohannad, Paras Lakhani, Phil Culliton, and Shunxing Bao. Siim-acr pneumothorax segmentation challenge. Kaggle, 2019. [63] Huijuan Zhang, Zongrun Huang, and Zhongwei Lv. Medical image synthetic data augmentation using gan. In Proceedings of the 4th International Conference on Computer Science and Application Engineering, pages 16, 2020. 3 [64] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55795588, 2021. 8, 5 [65] Yongchao Zhou, Hshmat Sahak, and Jimmy Ba. Training on thin air: Improve image classification with generated data. arXiv preprint arXiv:2305.15316, 2023. 2, 3 Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 8, 5 [50] Sarah Schwettmann, Neil Chowdhury, Samuel Klein, David Bau, and Antonio Torralba. Multimodal neurons in pretrained In 2023 IEEE/CVF International text-only transformers. Conference on Computer Vision Workshops (ICCVW), pages 28542859, 2023. 1, 2, 5 [51] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can CLIP benefit vision-and-language tasks? In International Conference on Learning Representations, 2022. 8, 5 [52] Junji Shiraishi, Shigehiko Katsuragawa, Junpei Ikezoe, Tsuneo Matsumoto, Takeshi Kobayashi, Ken-ichi Komatsu, Mitate Matsui, Hiroshi Fujita, Yoshie Kodera, and Kunio Doi. Development of digital image database for chest radiographs with and without lung nodule: receiver operating characteristic analysis of radiologists detection of pulmonary nodules. American journal of roentgenology, 174(1):7174, 2000. 4, 6 [53] Zineng Tang, Jaemin Cho, Hao Tan, and Mohit Bansal. Vidlankd: Improving language understanding via video-distilled knowledge transfer. Advances in Neural Information Processing Systems, 34:2446824481, 2021. [54] Radiopaedia Team. Radiopaedia. https : / / radiopaedia.org/. [Accessed Nov. 2024]. 4 [55] Ying Wang, Tim G. J. Rudner, and Andrew Gordon Wilson. Visual explanations of image-text representations via multimodal information bottleneck attribution. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 7 [56] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. MedCLIP: Contrastive learning from unpaired medical images and text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 38763887, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. [57] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. SimVLM: Simple visual language model pretraining with weak supervision. In International Conference on Learning Representations, 2022. 8, 1, 5 [58] Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, JiLiang Tang, and Anil Jain. Adversarial attacks and defenses in images, graphs and text: review. International journal of automation and computing, 17:151178, 2020. 1 [59] Moi Hoon Yap, Gerard Pons, Joan Marti, Sergi Ganau, Melcior Sentis, Reyer Zwiggelaar, Adrian Davison, and Robert Marti. Automated breast ultrasound lesions detection using convolutional neural networks. IEEE journal of biomedical and health informatics, 22(4):12181226, 2017. 4 [60] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 4 [61] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Impact and Limitations 7.2. Implicit Knowledge Transfer We believe that Knowledge Transfer has the potential to be an impactful technique for introducing novel concepts in pretrained models. Overall, Knowledge Transfer is quite cheap in terms of computational requirements, as it works by only fine-tuning on just handful of synthesized samples. Thus, it is very quick and does not need large amount of memory. In this sense, it may be comparable to parameter-efficient fine-tuning (PEFT) techniques, such as low-rank adaptation (LoRA) [15], which minimize the amount of memory required for fine-tuning. However, compared to PEFT, Knowledge Transfer does not require any real data besides single textual description for each novel concept. The main limitation of Explicit Knowledge Transfer lies in the inversion step, which takes the most time compared to fine-tuning. If this step could be avoided, we could achieve near real-time learning of novel concepts with minimal computational requirements. This could enable the development of rapidly improving intelligent agents in many real-world applications. We hypothesize this is possible with Implicit Knowledge Transfer, for example by using Masked-Language Modeling (MLM) as proxy for knowledge transfer. However, in this work, we do not focus on this topic, as preliminary experiments (shown in Sec. 8.9) did not achieve satisfactory results compared to Explicit Knowledge Transfer. Another limitation lies in the limited comparison with state-of-the-art approaches; however, to the best of our knowledge, we are not aware of other works sharing the same goal as ours. 7. Knowledge Transfer 7.1. Possible improvements of Explicit Transfer 7.1.1. Relaxation of Eq. 3 Computing ˆX as in Eq. 3 might produce images that are widely different from the training distribution of natural images, as shown in Fig. 3. So, instead of inverting the whole visual encoder fV , we can invert just subset of layers ΨV fV , starting from the top of the model: ˆZ = Ψ1 (fT (XT )) max ˆZ sim(ΨV ( ˆZ ), fT (XT )) +R( ˆZ ) (4) where could be regularization similar to style transfer [10] to encourage ˆZ to be similar to the intermediate representations of natural images. Although in this work we focus on explicit knowledge transfer, we briefly present the idea behind Implicit Knowledge Transfer for the sake of completeness. It has been shown how multi-modal neurons can be found in multi-modal models [12, 50]. These neurons exhibit high activation on the same concepts in either modality, meaning that they are able to capture cross-modal representations. We hypothesize that in shared-parameter architecture (e.g. early-fusion transformers [26, 39]) it should be possible to exploit these neurons for knowledge transfer, for example with simple masked language modeling on the novel concept description, effectively eliminating the need for model inversion. For this purpose, early-fusion architectures that can process single modalities independently would be required. However, to the best of our knowledge, we are not aware of many large pre-trained models satisfying these requirements at the time being, hence we leave an in-depth exploration of this path for future research. Hints that training on different modalities independently can help can be found in the literature, for example during pre-training of U-VisualBERT [33]. Even more relevant to our research, authors in [57] report some capabilities of cross-modal transfer on SimVLM, however, the model is proprietary and we are unable to reproduce their claims. Thus, here we focus on ViLT and we report some preliminary analysis in Sec. 8.9. 7.3. Open questions Q1 Domain Gap. Inverted images, as shown in Fig. 3, appear widely different from natural images. However, as shown by the results in the paper, fine-tuning the models on them leads to improved results. Is domain gap present between inverted and real images? Or is it indicative of fundamental difference in which deep models process visual information? This phenomenon may be linked with adversarial attacks [58]. Q2 Generalizability of inversion An interesting point to analyze, which could provide some insights for Q1, is the generalizability of the inverted images. For example, can images inverted with certain model (e.g. CLIP) be used for training some other model from scratch? Or are they fitted to only work with the specific model used for inversion? Q2 Catastrophic Forgetting To what extent can we prevent catastrophic forgetting when applying Knowledge Transfer? In this work, we show that lower learning rates generally achieve good trade-off between learning novel con1 cepts and preserving previous information. However, there is still room for improvement. For example, LoRA [15] has been shown to help in avoiding catastrophic forgetting during fine-tuning, hence applying it during Knowledge Transfer could further improve the results. Also, Implicit Transfer (on shared-parameter models) might avoid catastrophic forgetting better than Explicit Transfer, for example by focusing on multi-modal neurons. 8. Experiments 8.1. CLIP on rare concepts We train using the Adam optimizer, with batch size of 4, weight decay of 0.2, and learning rates between 1e-5 and 5e-5 as reported in the table in the main text. We train using 10 inverted images for each concept. The captions used for inversion can be found in Tab. 12. 8.2. Details about image inversion for ViLT For ViLT, we deviate from the image inversion approach used for CLIP by disabling the random affine augmentation, as it produced noisy inverted images. Additionally, we use weight decay value of 0.01, which is consistent with the one used by the authors of ViLT. As explained in the main paper, our inversion process for ViLT maximizes the image-text matching (ITM) score, which is computed by the ITM head of ViLT [26]. This head outputs two values: one indicating no match and the other indicating match. To optimize this, we use the cross-entropy loss during inversion, aiming to maximize the output corresponding to match while minimizing the output for no match. The captions used for inversion for ViLT can be found in Tab. 13. 8.3. Ablation study We report additional ablation studies here. We focus on the construction of the captions for fine-tuning. As explained in Sec. 3.1.2, during fine-tuning we prepend each caption with the name of the concept, for example moongate is [...]. Here we motivate why this is necessary by comparing captions prepended with the name and captions without the name. The results are shown in Fig. 5. As we can observe, using the name of the concept during fine-tuning is necessary in order to map visual features to its textual description. 8.4. MedCLIP For MedCLIP, we use the same setup as CLIP on rare concepts, see Sec. 8.1. Namely, we employ Adam with batch size of 4 and weight decay of 0.2, using 10 inverted images for each concept. The descriptions used for inversion with MedCLIP can be found in Tab. 14. 8.5. CLIP on medical images (out of domain) For ViT-B/32, we use learning rate of 5e-5 with batch size of 8, and we train for 5 epochs; for ViT-L/14 we use learning rate of 1e-5, batch size of 4, and we train for 2 epochs. The captions used for inversion are reported in Tab. 15. 8.6. Segmentation Results of knowledge transfer on MedCLIP-SAMv2 with different values of learning rate are shown in Tab. 7. We report illustrative examples of the improvements achieved by knowledge transfer in Fig. 6 and Fig. 7. The captions used for inversion for segmentation can be found in Tab. 16. Differences in downstream tasks As said in the main text, lung nodules and pneumothorax segmentations are novel tasks on which MedCLIP-SAMv2 was not pre-trained. Regarding brain tumors, we employ the BraTS 2023 glioma dataset, which contains brain gliomas in adult patients. With respect to the original performance reported in [30] on brain tumors, we notice significant gap. However, the preprocessing of the images is quite different, as data from BraTS 2023 is more heavily preprocessed (e.g. skull stripping) than in [30]. We were not able to compare MedCLIP-SAMv2 on the original data, as, at the time of writing, details about the data split are missing. 8.7. Text-image retrieval In this section, we show the full results for text and image retrieval tasks on Flickr30k with ViLT. Tab. 8 is the extended version of Tab. 5 in which we report the huggingfaces pretrained baseline, along with the results of the experiments we performed while tuning the learning rate and the batch size. We report the best batch size for each learning rate. As can be seen our method works best with smaller learning rates in this setting. The captions used for inversion (mscoco) can be found in Tab. 17. 8.8. Captioning In these experiments, we deal with two types of captions: the first is the concept caption, that we use for inversion and fine-tuning with InfoNCE as in all other experiments (listed in Sec. 10), the second is the target caption that we use to fine-tune the autoregressive captioning decoder of CoCa with Lcap. Captioning Loss When fine-tuning on inverted images, we apply an autoregressive captioning loss, as defined in [61]: Lcap = (cid:88) t=1 log Pθ(yty<t, x) (5) which aims at predicting the next token yt given the previous tokens y<t and the image x. The final objective function that we optimize is the combination of the InfoNCE loss and the captioning loss: Figure 5. Ablation study on caption construction for finetuning. Model DSC NSD IoU DSC NSD IoU DSC NSD IoU DSC NSD IoU Lung Nodules Lung Pneumothorax Breast Ultrasound Brain MRI MedCLIP-SAMv2 Transf. (1e-5) Transf. (2e-5) Transf. (3e-5) Transf. (4e-5) Transf. (5e-5) Transf. (1e-4) Transf. (2e-4) 14.83% 17.30% 8.64% 6.30% 7.61% 3.75% 56.25% 59.44% 47.81% 17.20% 20.97% 12.05% 13.95% 17.45% 8.75% 6.28% 7.59% 3.77% 58.23% 61.56% 49.52% 15.90% 19.36% 11.10% 14.10% 17.65% 8.83% 6.41% 7.76% 3.83% 54.36% 57.30% 46.30% 18.13% 22.26% 12.62% 14.10% 17.65% 8.85% 6.25% 7.55% 3.73% 55.70% 59.00% 47.49% 15.47% 18.85% 10.78% 14.25% 17.85% 8.94% 6.24% 7.57% 3.71% 53.86% 56.82% 45.61% 15.26% 18.63% 10.62% 14.20% 17.78% 8.92% 6.20% 7.51% 3.70% 54.90% 57.97% 46.09% 16.22% 19.81% 11.34% 14.35% 18.03% 9.04% 6.02% 7.29% 3.59% 10.74% 13.64% 6.66% 4.71% 5.54% 2.86% - - - - - - - - - - - - Table 7. Full results on zero-shot segmentation with MedCLIP-SAMv2. = λ1LCLIP + λ2Lcap (6) where λ1, λ2 0. In our fine-tuning, we use λ1 = 1 and λ2 = 0.1. Target captions template We use set of 26 different templates as target captions during fine-tuning. At each optimization step, we select random template for each sample in the following manner: 1 TEMPLATES = ( 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 lambda c: fa bad photo of {c}., lambda c: fa low resolution photo of the {c}., lambda c: fa rendering of {c}., lambda c: fa bad photo of the {c}., lambda c: fa cropped photo of the {c}., lambda c: fa photo of hard to see {c}., lambda c: fa bright photo of {c}., lambda c: fa photo of clean {c}., lambda c: fa photo of dirty {c}., lambda c: fa dark photo of the {c}., lambda c: fa photo of my {c}., lambda c: fa bright photo of the {c}., lambda c: fa cropped photo of {c}., lambda c: fa photo of the {c}., lambda c: fa good photo of the {c}., lambda c: fa rendering of the {c}., lambda c: fa photo of one {c}., lambda c: fa close-up photo of the {c}., lambda c: fa photo of {c}., lambda c: fa low resolution photo of {c}., lambda c: fa photo of large {c}., lambda c: fitap of the {c}., lambda c: fa jpeg corrupted photo of the {c}., lambda c: fa good photo of {c}., lambda c: fitap of {c}., lambda c: fa photo of the large {c}., 23 24 25 26 27 28 ) 29 30 template_idx = torch.randint( 0, len(TEMPLATES), (1,) 31 32 ).item() 33 template = TEMPLATES[template_idx] 34 return tokenize(template(class_name)) These templates are inspired by OpenAI prompt ensembling for zero-shot classifiers7. We use these captions although they are not in the exact style of MSCOCO, as we do not want to leverage information from MSCOCO besides the concept classes. Target captions crafted specifically for MSCOCO might further improve the results. Results We report additional results of captioning, with and without the use of captioning loss in Tab. 9. Even without captioning loss, we are achieve improvements over both version CoCa (pre-trained on LAION-2B) and CoCa FT (fine-tuned on MSCOCO). By applying Lcap we achieve further increase in the reported metrics. We showcase some improvements in captioning on the CoCa model pre-trained 7https : / / github . com / mlfoundations / open _ clip / blob/main/src/open_clip/zero_shot_metadata.py 3 (a) Image (b) Ground Truth (c) M2IB Map (Baseline) (d) Final Segmentation (Baseline) (e) M2IB Map (Knowledge Transfer) (f) Final Segmentation (Knowledge Transfer) Figure 6. Qualitative evaluation of knowledge transfer on breast tumor segmentation (UDIAT dataset). We report the top ten most illustrative examples in which knowledge transfer improved segmentation, in terms of DSC. Model LR Batch Size R@1 R@5 R@10 R@1 R@5 R@ Flickr30k (1K) Text Retrieval Image Retrieval ViLT-B/32 (huggingface) - ViLT-B/32 ViLT-B/32 ViLT-B/32 ViLT-B/32 ViLT-B/32 ViLT-B/32 ViLT-B/32 ViLT-B/32 ViLT-B/32 ViLT-B/32 ViLT-B/ 8e-7 9e-7 1e-6 2e-6 3e-6 4e-6 5e-6 8e-6 1e-5 2e-5 3e-5 - 32 32 16 128 256 32 256 32 128 32 32 73.8% 93.5% 96.5% 57.3% 83.9% 90.4% 74.5% 93.8% 96.4% 57.7% 84.0% 90.4% 74.6% 93.8% 96.4% 57.8% 84.0% 90.5% 74.4% 93.8% 96.5% 57.7% 84.1% 90.5% 74.6% 93.7% 96.5% 57.8% 84.0% 90.5% 74.5% 93.9% 96.5% 57.7% 83.9% 90.5% 73.8% 93.6% 96.5% 57.4% 84.0% 90.5% 74.5% 93.9% 96.5% 57.6% 84.0% 90.5% 73.2% 93.7% 96.1% 57.4% 83.7% 90.4% 74.4% 93.8% 96.8% 56.8% 83.7% 90.6% 71.8% 93.2% 96.4% 56.7% 83.6% 90.4% 70.8% 92.1% 95.7% 56.0% 82.9% 90.2% Table 8. Full results for text and image retrieval on Flickr30k with ViLT. The first section reports baseline results, while the second shows the outcome of each tested learning rate and its optimal batch size (chosen among 16, 32, 64, 128, and 256). Recall scores at top 1, 5, and 10 are reported. 4 (a) Image (b) Ground Truth (c) M2IB Map (Baseline) (d) Final Segmentation (Baseline) (e) M2IB Map (Knowledge Transfer) (f) Final Segmentation (Knowledge Transfer) Figure 7. Qualitative evaluation of knowledge transfer on brain tumor segmentation (BraTS 2023 glioma dataset). We report the top ten most illustrative examples in which knowledge transfer improved segmentation, in terms of DSC. Model BLEU@4 METEOR CIDEr SPICE"
        },
        {
            "title": "Transfer",
            "content": "MSCOCO (5K) 8.9. Preliminary results with Implicit Knowledge CLIP-ViL [51] BLIP [32] VinVL [64] SimVLM [57] LEMON [16] CoCa [61] (proprietary) CoCa CoCa (transf. 6e-5) CoCa (transf. 9e-5) CoCa FT CoCa FT (transf. 2e-5) CoCa FT (transf. 5e-6) 40.2 40.4 41.0 40.6 41.5 40. 6.9 13.6 17.9 34.9 35.2 35.2 29.7 - 31.1 33.7 30.8 33.9 12.8 18.5 19.4 29.7 29.8 29.8 134.2 136.7 140.9 143.3 139.1 143. 31.1 47.3 60.8 123.1 123.1 124.0 23.8 - 25.4 25.4 24.1 24.7 9.1 13.6 13.7 23.5 23.2 23.3 Table 9. Image captioning on MSCOCO. means the decoder is also fine-tuned. CoCa refers to the baseline model pre-trained on LAION-2B [49], while CoCa FT refers to the model fine-tuned for captioning on MSCOCO. We highlight in bold the best results overall and the improvements achieved by Knowledge Transfer. on LAION-2B in Fig. 10. The concept captions used for inversion can be found in Tab. 17. 5 In this section, we show preliminary results about Implicit Knowledge Transfer, presented in Sec. 7.2. In Implicit Knowledge Transfer, the objective is to teach the model novel concept by only training it on text, without using inverted images. To do so with ViLT, we no longer use the image-text matching objective as it requires images, instead, we employ masked language modeling (MLM) [9], using as input pair composed of the textual description of the concept and random noise instead of the image. The assumption is that, in model with parameters shared between modalities, fine-tuning on one modality (text), will also benefit the other modality. Here, our hypothesis is that during finetuning, multi-modal neurons [50] can help in transferring knowledge across modalities. 8.9.1. Implicit Knowledge Transfer with MLM For Implicit Knowledge Transfer we used the same masked language modeling setup as in ViLT [26], which means that we use whole-word masking and masking probability of 15%. We use 10 examples for fine-tuning, each of which is Sample MSCOCO Actual shot of clock in the train station. pianist in suit and glasses playing keyboard. baseball player hitting ball in professional game. baseball holding baseball bat during baseball game. Two people that are sitting on table. Baseline grand - central - station - new - york.jpg (METEOR 0.0) Paul Kagasoff, president and chief executive officer of Intel corp., speaks during the 2012 Computex trade show in Taipei, Taiwan (METEOR 8.6) Aaron judge 2016 new york Yankees (METEOR 5.0) 20080419 _ mariners _ 0001 by Mike. Smith (METEOR 4.2) Dinner in our tiny studio apartment in Amsterdam. (METEOR 0.0) Knowledge Transfer black and white photo of clock at Grand Central terminal. (METEOR 92.9) photo of man in suit sitting at keyboard. (METEOR 86.1) baseball player swings his bat at batter. (METEOR 83.6) baseball player takes swing at pitch. (METEOR 86.9) photo of man and woman sitting at kitchen table. (METEOR 80.8) Sample Actual baseball batter up at the plate that just hit ball pair of red scissors sitting on newspaper. young man sitting at table with pizza. man in black suit kicking around soccer ball. Baseline david - ortiz _ display _ image _ display _ image _ display _ image _ display _ image _ display _ image _ display _ image (METEOR 3.9) Your scissors are now digitized (METEOR 9.5) in Pizza and beer Illinois. Chicago, Photo Flickr: via David J. Abbott M.D. (METEOR 8.3) foot blatter - - ball.jpeg (METEOR 5.6) The man in black tie sits in chair with his shirt sleeves rolled up. Avatar for Marc Anthony (METEOR 5.3) Knowledge Transfer baseball player swings his bat at pitch. (METEOR 80.3) photo of pair of red scissors on piece of paper. (METEOR 84.9) photo taken of man sitting at table with plate of pizza. (METEOR 83.5) man in suit kicking soccer ball on field. (METEOR 80.7) man in white shirt and black tie sits on chair. (METEOR 78.4) Table 10. Visual example of captioning on MSCOCO. We report the top ten most illustrative examples in which knowledge transfer improved captioning, in terms of METEOR score. composed by random noise image and masked caption. The masked captions are generated starting from the same caption by masking differently each time. For the caption we use the template is , where is the name of the concept and is the concepts description (from Tab. 13). We use batch size of 4 with different learning rates, for total of 3 train steps. Weight decay is set, as in the other experiments, to 0.01. Explicit Knowledge Transfer baseline with MLM For comparison, we also evaluate the results of explicit knowledge transfer with the masked language modeling objective instead of the image-text matching objective. We use the same setup as the implicit one, with the only exception that instead of random noise images, we use inverted images. In particular, we use the same inverted images we used for the explicit knowledge transfer with the image-text matching objective. 6 Learning Rate Type Concept Implicit Moongate Tonometer Target Acc. ImageNet* 0-shot Target Acc. ImageNet* 0-shot Baseline 0% 1e-5 0% 2e-5 0% 3e0% 4e-5 0% 5e-5 0% 23.74% 23.82% 23.90% 23.98% 23.94% 23.86% 10% 10% 10% 10% 10% 0% 23.74% 23.84% 23.86% 23.70% 23.64% 23.60% Gyroscope Target Acc. 50% 50% 60% 60% 60% 50% ImageNet* 0-shot 23.74% 23.74% 23.62% 23.42% 23.44% 23.46% Explicit Moongate Tonometer Target Acc. ImageNet* 0-shot Target Acc. ImageNet* 0-shot 0% 0% 0% 0% 0% 0% 23.74% 23.80% 24.08% 24.02% 24.10% 24.20% 10% 10% 10% 10% 10% 10% 23.74% 23.80% 23.74% 23.72% 23.70% 23.56% Gyroscope Target Acc. 50% 50% 50% 50% 40% 30% ImageNet* 0-shot 23.74% 23.74% 23.84% 23.84% 23.84% 23.82% Table 11. Knowledge Transfer on novel and rare concepts using masked language modeling with ViLT. In the Implicit Knowledge Transfer, we pass noise images along with corresponding masked caption to ViLT; in the explicit one, we replace noise images with inverted images. 8.9.2. Results discussion Tab. 11 reports the results for both implicit and explicit In knowledge transfer with masked language modeling. both cases, no improvements are observed for the moongate concept, whose accuracy stays at 0%. For tonometer, explicit knowledge transfer seems to work better since with the implicit one, there is loss of performance, while for gyroscope the opposite is true. In all cases, we observe an increase in the accuracy over the ImageNet-100 classes, as observed when using image-text matching objective. The only improvement is registered for the gyroscope concept in the implicit transfer setting, from 50% to 60%. Overall we can say that implicit knowledge transfer with masked language modeling does not work for the ViLT model, this is probably due to the fact that ViLT was pre-trained on image-text pairs, which means that it expects both modalities in input. Regarding explicit knowledge transfer with MLM, more experiments are needed to determine the correct algorithm and set of hyperparameters to make it work, for example, we may have to use more examples generated from different textual descriptions. 9. Code Code will be publicly released upon paper acceptance. 7 10. List of captions"
        },
        {
            "title": "Gyroscope",
            "content": "Moongate Tonometer Gyroscope Table 12. Descriptions for rare concepts (generated with Llama-3-8B-Instruct). perfectly circular archway built from uniformly cut stones or bricks, set into larger wall. It forms smooth circle, framing views of gardens or landscapes beyond, creating picturesque portal. slender, pen-like probe attached to small base equipped with precise dials and gauges. This tool is often part of larger medical apparatus, featuring metallic finish and refined, professional appearance. series of gleaming silver rings, each nested perfectly within the next, surrounds central disk that spins smoothly. The rings are connected by intersecting axes, allowing the disk to tilt and rotate freely while maintaining sophisticated, mechanical look. Table 13. Manually shortened descriptions for rare concepts (to fit into ViLTs 40 token input) perfectly circular archway built from uniformly cut stones or bricks, set into larger wall. It forms smooth circle, framing views of gardens, creating picturesque portal. slender, pen-like probe attached to small base equipped with precise dials and gauges. This tool is often part of larger medical apparatus. series of rings each nested within the next, surrounds central disk that spins. The rings are connected by intersecting axes allowing the disk to rotate freely. Table 14. Descriptions for medical classes for JSRT (Mix with Radiopaedia and ChatGPT-4). Benign Nodule Lung Cancer small, round spots appearing in Chest X-Ray, typically well-defined with smooth, regular borders. These spots are often uniformly dense and do not cause distortion of surrounding structures. dense and irregular mass on Chest X-Ray images often with spiked or uneven edges. It may appear in the lungs periphery or near the airways. Table 15. Descriptions for medical classes for CheXpert-5x200c (obtained with mix of Radiopaedia and ChatGPT-4). Atelectasis Cardiomegaly Pleural Effusion Consolidation Edema small areas of collapsed lung. It is usually seen on Chest X-Rays as small volume linear shadows, usually peripherally or at lung bases, appearing more opaque and shrunken. Enlargement of the heart usually seen in Chest X-Rays. The central shadow of the chest appears enlarged, extending beyond half the width of the entire chest cavity. collection of fluid between the lungs and the chest, which makes the area appear white and smooth in Chest X-Ray images. The area does not present visible lung markings. An area inside the lungs that appears as branching low attenuating (lucent) bronchi surrounded by high attenuating (dense) consolidated/opacified alveoli on Chest X-Ray images. An abnormal accumulation of fluid in the extravascular compartments of the lung, which makes the area whiter in Chest X-Ray images. It is usually present on both lungs. Table 16. Descriptions for medical classes for segmentation (Mix with Radiopaedia and ChatGPT-4). Lung Nodules Circular spots appearing within the lung fields, with clear and defined edges in CT images. They are denser than the surrounding tissue, often appearing in shades of gray or white, with varying size."
        },
        {
            "title": "Brain Tumor",
            "content": "A dark, irregularly shaped area is visible against the lighter surrounding tissue. The borders may appear uneven or spiculated, and the area is typically less uniform in texture. Shadowing can often be seen beneath the mass. An abnormal collection of air in the pleural space, which allows the parietal and visceral pleura to separate and the lung to collapse. The pleura edge is thin and no lung markings are visible. An irregular bright mass in brain MRI, often with thick and irregular margins, surrounded by vasogenic-type edema or fluid accumulation. It may also have hemorrhagic component. Table 17. Descriptions for MSCOCO classes used for text and image retrieval experiments (With ChatGPT-4). person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove human figure, typically with visible head, torso, arms, and legs, in various postures. two-wheeled vehicle with frame, handlebars, and pedals, usually ridden by person. four-wheeled enclosed vehicle with windows and doors, commonly seen on roads. two-wheeled motorized vehicle with seat and handlebars, typically ridden by one or two people. large flying vehicle with wings and tail, often seen with windows along the sides for passengers. large, rectangular vehicle with many windows and seating rows, designed to carry multiple passengers. long, linked series of vehicles running on tracks, often with locomotive at the front. large vehicle with separate cab and an open or enclosed cargo area for transporting goods. small to medium-sized watercraft with hull and often visible sails or an engine. vertical or horizontal post with red, yellow, and green lights, used to control vehicle flow at intersections. small, red, metal cylinder with nozzles on the side, often found on sidewalks for fire emergencies. red, octagonal sign with the word \"STOP\" in white, used to indicate where vehicles must halt. tall, narrow post with small display and slot, used to pay for parking time. long seat, often with backrest, typically found in parks or public areas. small animal with feathers, wings, and beak, often shown perched or flying. small, furry animal with pointed ears, whiskers, and long tail, often seen sitting or grooming. furry, four-legged animal with tail, usually seen with collar or leash. large, four-legged animal with mane and tail, often depicted standing or galloping. woolly animal with round body, small head, and short legs, often seen in groups in fields. large animal with boxy body, horns, and long face, often shown grazing or with an udder. massive, gray animal with long trunk, large ears, and tusks. large, sturdy animal with thick fur, rounded ears, and short tail, often shown standing or walking on all fours. horse-like animal with black and white stripes across its body. very tall animal with long neck and legs, spotted coat, and small horns on its head. bag with shoulder straps, typically worn on the back and used for carrying personal items. foldable, rounded canopy on stick, used for protection from rain or sun. small to medium-sized bag with handles, often carried by hand and used to hold personal items. long, narrow piece of fabric worn around the neck, often knotted at the collar of shirt. rectangular, boxy container with handle, used for carrying clothes and personal items when traveling. flat, round disc often made of plastic, used for throwing and catching. Long, narrow pieces of equipment attached to boots, used for gliding on snow. flat, wide board attached to boots, used for sliding on snow. round object of varying sizes, such as soccer ball or basketball, used in sports. lightweight object with string, often shaped like diamond or triangle, designed to fly in the wind. smooth, cylindrical wooden or metal stick used to hit baseball. padded, leather glove worn on one hand, used to catch baseballs. 9 skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush narrow board with wheels, used for rolling and performing tricks. long, flat board used for riding waves in the ocean. An oval-shaped frame with strings and handle, used to hit tennis ball. narrow-necked container with cap, often used to hold liquids like water or soda. stemmed glass with wide bowl at the top, used for drinking wine. small, handleless vessel used for drinking, usually made of ceramic or plastic. utensil with multiple prongs, used to pick up food. utensil with long, sharp blade, used for cutting food. utensil with shallow bowl at the end of handle, used for eating or serving food. round, deep dish, often used to hold soup or other foods. long, yellow fruit with curved shape and soft interior. round fruit, typically red or green, with stem at the top. Two slices of bread with filling in between, such as meat, cheese, or vegetables. round, orange-colored fruit with thick, textured peel. green vegetable with tree-like shape, featuring thick stalk and small florets. long, orange vegetable with pointed end, often with green leaves at the top. sausage in bun, often with condiments like ketchup or mustard. round, flatbread topped with cheese, sauce, and various toppings, often cut into slices. round, fried pastry with hole in the middle, often glazed or topped with sprinkles. sweet, layered dessert, often decorated with frosting or fruit. piece of furniture with backrest and four legs, designed for sitting. large, cushioned seat with backrest and arms, designed for multiple people. plant growing in container, often with green leaves or flowers. large, rectangular piece of furniture for sleeping, with mattress and pillows. flat, often rectangular surface with legs, designed for eating meals. porcelain fixture with seat and flushing mechanism, used in bathrooms. rectangular screen on stand or wall, used for viewing shows and movies. portable computer with hinged screen and keyboard. small, handheld device used to control cursor on computer screen. small, rectangular device with buttons, used to control electronics like TVs. flat, rectangular panel with keys, used for typing on computers. handheld electronic device with screen and buttons or touchscreen, used for communication. box-like appliance with door, used for heating food quickly. large appliance with door and interior racks, used for baking or roasting. small appliance with slots, used to toast bread. basin with faucet, used for washing hands, dishes, or food. large, box-like appliance with doors, used to store perishable food at low temperatures. collection of pages bound together with cover, containing text or images. circular or rectangular device with hands or digital display, showing the current time. decorative container, often made of glass or ceramic, used to hold flowers. handheld tool with two blades, used for cutting paper or fabric. soft, stuffed toy shaped like bear, often used by children. handheld device that blows warm air, used to dry hair. small brush with handle, used for cleaning teeth."
        }
    ],
    "affiliations": [
        "Politecnico di Torino",
        "University of Turin"
    ]
}