{
    "paper_title": "Reinforcement Learning via Self-Distillation",
    "authors": [
        "Jonas Hübotter",
        "Frederike Lübeck",
        "Lejs Behric",
        "Anton Baumann",
        "Marco Bagatella",
        "Daniel Marta",
        "Ido Hakimi",
        "Idan Shenfeld",
        "Thomas Kleine Buening",
        "Carlos Guestrin",
        "Andreas Krause"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts."
        },
        {
            "title": "Start",
            "content": "Reinforcement Learning via Self-Distillation Reinforcement Learning via Self-Distillation Jonas Hübotter1 Frederike Lübeck Marco Bagatella1, 2 Daniel Marta1 Thomas Kleine Buening1 Carlos Guestrin4 Andreas Krause1 1ETH Zurich 2Max Planck Institute for Intelligent Systems 3MIT 4Stanford , 1, 2 Lejs Behric Ido Hakimi1 , 1 , 1 Anton Baumann Idan Shenfeld 6 2 0 2 8 2 ] . [ 1 2 0 8 0 2 . 1 0 6 2 : r https://github.com/lasgroup/SDPO"
        },
        {
            "title": "Abstract",
            "content": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from scalar outcome reward per attempt, creating severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the models ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability fewer attempts. as best-of-k sampling or multi-turn conversations with"
        },
        {
            "title": "Introduction",
            "content": "Progress in deep reinforcement learning has shown that iterating on experienceacting, receiving feedback, and updating policy can unlock capabilities that are difficult to obtain from static supervision alone (Mnih et al., 2015; Silver et al., 2016; 2017; Berner et al., 2019). The same theme now appears in large language models (LLMs): large-scale post-training with reinforcement learning (RL) has substantially improved performance on reasoning-heavy tasks, especially in settings with programmatic or otherwise verifiable evaluation (Jaech et al., 2024; Guo et al., 2025; Kimi et al., 2025; Olmo et al., 2025). Nevertheless, the dominant RL recipe for LLM post-training remains bottlenecked by Figure 1: SDPO substantially outperforms an improved version of Group Relative Policy Optimization (GRPO) on LCB v6 with Qwen3-8B. Further, SDPO achieves GRPOs fifewer generations. Claude nal accuracy in 4 Sonnet 4 is the strongest instruct model on the public LCBv6 leaderboard. Shaded regions show the standard deviation across 3 seeds. Equal second authorship. Correspondence to jonas.huebotter@inf.ethz.ch. Reinforcement Learning via Self-Distillation Figure 2: Comparison of RLVR and RLRF settings. In Reinforcement Learning with Verifiable Rewards (RLVR), the agent learns from scalar reward r, which often acts as In contrast, an information bottleneck by masking the underlying environment state. Reinforcement Learning with Rich Feedback (RLRF) utilizes tokenized feedback. This provides significantly richer signal than scalar reward, as the feedback can encapsulate both the reward as well as detailed observations of the state (such as runtime errors from code environment or feedback from an LLM judge). πθ( x) and receives scalar reward credit assignment. Most current approaches operate in the setting of reinforcement learning with verifiable rewards (RLVR): given question x, the model samples an answer R, often binary (e.g., unit-tests pass/fail in code generation). Modern policy gradient RLVR methods such as Group Relative Policy Optimization (GRPO; Shao et al., 2024) estimate advantages from these sparse outcome rewards. Furthermore, when all rollouts in group receive the same (often zero) reward, GRPO advantages collapse to zero and learning stalls. To overcome this sparsity, one might prefer distillation from strong teacher (Guo et al., 2025; Yang et al., 2025; Lu & Thinking Machines Lab, 2025; Guha et al., 2026), which provides dense, token-level supervision. However, strong teachers are often unavailable in online learning, where the goal is to raise the capability ceiling beyond existing models. In this work, we argue that the key limitation is not RL per se, but the information bottleneck imposed by scalar outcome rewards. Many verifiable environments expose rich tokenized feedback beyond scalar rewards r, such as runtime errors, failing unit tests, or evaluations from an LLM judge. This feedback not only reveals whether rollout was wrong, but also what went wrong. We formalize this more general setting as Reinforcement Learning with Rich Feedback (RLRF) and illustrate its difference to RLVR in Figure 2. Here, feedback can be any tokenized representation of any state reached by an agentic system. The central question becomes: how can we convert rich feedback into effective credit assignment without requiring external supervision from strong teacher? Last Executed Input [[26,30,2],[11,23,1]] Runtime Error ZeroDivisionError: division by zero Line 73 in separateSquares (Solution.py) Our starting point is the observation that LLMs already possess powerful mechanism for using feedback: in-context learning (Brown et al., 2020; Wei et al., 2022). When conditioned on feedback, the same model can often identify plausible mistakes and propose corrected approach. common example of such feedback is the summary of failed test cases on coding platforms like LeetCode (Figure 3). Many recent works leverage this capability to iteratively generate corrections (Chen et al., 2021a; Madaan et al., 2023; Shinn et al., 2023; Yao et al., 2024; Yuksekgonul et al., 2025; Lee et al., 2025). In contrast, we use the current policy as self-teacher that, rather than sampling new response, re-evaluates the existing rollout after receiving rich feedback. Including the feedback in-context transforms the models next-token distribution, allowing the self-teacher to agree or disagree with the students original choices at specific tokens. This yields dense, logit-level credit assignment. Figure 3: Example of feedback from our code environment, inspired by LeetCode. Listings 5, 6, and 7 in the appendix show examples of feedback in case of wrong answer, memory error, and an index error. 2 Reinforcement Learning via Self-Distillation Method Sampling Signal off-policy rich strong teacher SFT / Distillation (Hinton et al., 2015) On-Policy Distillation (Agarwal et al., 2024) on-policy rich strong teacher RLVR (such as GRPO) (Lambert et al., 2025) on-policy weak environment environment RL via Self-Distillation (SDPO) (ours) on-policy rich Feedback Table 1: Comparison of self-distillation to alternative methods for post-training LLMs. For example, when provided with the feedback from Figure 3, the self-teacher can identify how the initial attempt should be modified to avoid the runtime error. Crucially, this mechanism incurs no sampling overhead: we simply re-compute the log-probabilities of the original attempt under the self-teachers feedback-augmented context. Building on this idea, we introduce Self-Distillation Policy Optimization (SDPO), an on-policy algorithm that performs RL via self-distillation. SDPO samples rollouts from the current policy, obtains rich environment feedback, and then minimizes logit-level distillation loss that matches the current policys next-token distribution to that of the self-teacher. Conceptually, SDPO addresses the central limitation of applying distillation to online learning: the absence of stronger external teacher. Instead of relying on fixed teacher, SDPO leverages the models ability to recognize its own mistakes in hindsight. By conditioning the current policy on the rich feedback it just received, we construct selfteacher that provides the dense supervision of distillation while retaining the exploration benefits of on-policy RL. Table 1 summarizes how this positions SDPO relative to RLVR and distillation baselines. We include comprehensive summary of related work in Section 6. We show that SDPO is policy gradient algorithm whose advantages are estimated using the self-teacher. This enables the implementation of SDPO with minor changes to standard RLVR pipelines, simply by swapping out the advantages. Summary of evaluation results. We evaluate SDPO in three online RL settings: Learning without rich feedback (3): We evaluate standard RLVR environments that do not return any feedback beyond scalar rewards. Here, SDPO treats successful attempts sampled in the current batch as feedback for failed attempts on the same question. We perform training runs on scientific reasoning and tool use, starting with Qwen3-8B and Olmo3-7B-Instruct. We find that SDPO outperforms strong GRPO baseline that integrates recent improvements: 68.8% vs. 64.1% final accuracy on aggregate. SDPO achieves higher accuracy with up to 7 shorter generation lengths compared to GRPO, demonstrating that effective reasoning need not be verbose. Learning with rich feedback (4): We evaluate competitive programming problems from LiveCodeBench v6 with LeetCode-style feedback. As shown in Figure 1, SDPO substantially improves over GRPO, reaching higher final accuracy (48.8% vs. 41.2%) and achieving GRPOs final accuracy in 4 fewer generations. SDPOs gains grow with model scale, suggesting that the ability for self-teaching emerges as models become stronger in-context learners. Discovering novel solutions to hard tasks at test-time (5): Finally, we demonstrate that SDPO can accelerate the discovery of solutions to difficult binary-reward questions. This contrasts with RLVR methods, which only begin learning once the first solution has been found. We leverage SDPO for Test-Time Self-Distillation, form of test-time training where the model specializes to an individual test question. We consider very difficult LiveCodeBench questions, for which the base models pass@64 is below 0.03, and show that SDPO accelerates the discovery of solutions by 3 ."
        },
        {
            "title": "2 SDPO: Self-Distillation Policy Optimization",
            "content": "We propose an algorithm that uses the in-context learning ability of the current policy x, ), which refers to the for assigning credit. Our key object is the self-teacher, πθ( 3 Reinforcement Learning via Self-Distillation Figure 4: Example of self-teaching with Qwen3-8B. The answer is generated by the model before seeing the feedback. Then, we re-evaluate the log-probs of the original attempt with the self-teacher after seeing the feedback. We show the per-token log(P(self-teacher)/P(student)), with red indicating negative values (self-teacher disagrees) and white indicating values around zero. Notably, in this example, Qwen3-8B identifies the error through retrospection without an explicit solution. Further, the activation is sparse, identifying where mistakes happen and adjusting to the students response distribution. current policy (the student) prompted with the question and the rich feedback . Next to the students original attempt y, may incorporate two key kinds of feedback: any environment output (such as runtime errors from code environment) and sample solution if was already solved with another attempt in the rollout group.1 As discussed before, the self-teacher πθ( x) since it sees additional information in-context. This leads us to observe: x, ) should have higher accuracy than the student πθ( We can use the same policy in two different roles: As the student for the initial attempt and as the teacher to determine the value of actions in hindsight. We introduce Self-Distillation Policy Optimization (SDPO) which repeatedly distills the self-teacher into the student. Given question x, we first sample rollouts from the student πθ and obtain corresponding environment feedback. We then use the KL-divergence, p(i) log p(i)/q(i), as distance measure for the next-token distributions of KL(p student and teacher, and optimize standard logit distillation loss: q) = LSDPO(θ) := KL(πθ( x, y<t) stopgrad(πθ( x, , y<t))) (1) where the stopgrad operator blocks gradients from flowing through the teacher, and thus prevents it from regressing towards the student and ignoring . The intuitive role of the teacher is to determine where and how the students original attempt was wrong through retrospection based on the feedback . Figure 4 shows an example of self-teaching with Qwen3-8B as student and self-teacher. We summarize SDPO in Algorithm 1 and display the teachers reprompt template in Table 2. Algorithm 1 SDPO Input: Language model πθ; dataset with questions x; number of rollouts per question; environment to obtain feedback for attempts. 1: repeat 2: 3: 4: 5: Sample question from dataset. πθ( Sample responses: yi} i=1 Evaluate responses to obtain feedback fi. x). { Self-distillation: Compute log-probs of self-teacher log πθ(yi,t x, fi, yi,<t). Update θ with gradient descent on 6: 7: until converged LSDPO(θ). We can derive the SDPO gradient as follows (see Appendix B.1 for details): Proposition 2.1. Let θ LSDPO(θ) = be the set of tokens in the vocabulary. The gradient of πθ ( x) (cid:34) t=1 ˆytV θ log πθ( ˆyt x, y<t) log πθ( ˆyt πθ( ˆyt LSDPO is x, y<t) x, , y<t) (cid:35) . (2) 1In standard RLVR implementations rollout group contains multiple simultaneous attempts for x. 4 Reinforcement Learning via Self-Distillation User: prompt Correct solution: successful_previous_rollout The following is feedback from your unsuccessful earlier attempt: environment_output Assistant: Correctly solve the original question. original_response Table 2: Template for self-teacher. prompt is replaced with the question. sample solution previously generated by the student is substituted for successful_previous_rollout (if available for this question; otherwise the paragraph is skipped). environment_output is replaced with the environment output (see, e.g., Figure 3) from the models original attempt (if it was not successful and there is no solution; otherwise the paragraph is skipped). If the models original attempt was successful, this attempt is passed as the correct solution. original_response is replaced with the models original attempt to re-evaluate its log-probabilities under the self-teacher. 2.1 Comparison to RLVR Note that the SDPO gradient is (negated) logit-level policy gradient where the advantages are estimated using the self-teacher.2 We can therefore reuse standard RLVR implementations and simply swap out the advantages. Let yi be the i-th rollout from rollout group of size for question x, then we have: AGRPO i,t ( ˆyi,t) := 1 yi,t = ˆyi,t} { (cid:16) mean ri i=1 ri} { (cid:17) , ASDPO i,t ( ˆyi,t) = log πθ( ˆyi,t πθ( ˆyi,t x, fi, yi,<t) x, yi,<t) . The GRPO advantages are zero on any non-generated token and constant within rollout yi.3 In contrast, the SDPO advantages are zero only for tokens where student and teacher perfectly agree. The SDPO advantage is positive for tokens which are more likely under the teacher while being negative for tokens which are less likely under the teacher. Thus, SDPO can be seen as direct extension of standard RLVR methods in two ways: 1. from 1-bit feedback to allowing arbitrary sequences of tokens as feedback, and 2. leveraging this rich feedback to estimate dense logit-level advantages. This tight connection to RLVR methods also enables straightforward extension of the SDPO gradient from Equation (2) to off-policy data via PPO-style clipped importance sampling (Schulman et al., 2017), see Appendix A.3. 2.2 Compute time & memory The only computational overhead of SDPO compared to GRPO is the additional computation of log-probs from the self-teacher, which can be effectively parallelized and is substantially faster than sequential generation. Figure 5 compares the compute time of SDPO and GRPO. As expected, the compute overhead of SDPO is relatively small. Here, we use micro batch size of 2;4 compute time can be further reduced by using larger micro batch sizes. Naively computing the KL divergence between student and teacher requires holding full logits of both models in memory. To avoid this, we approximate the KL divergence Figure 5: Time per step for SDPO vs GRPO (solid: without code environment, light: with code environment). 2See Appendix A.3 for detailed comparison of the SDPO gradient to the standard policy gradient. 3We use the GRPO (Shao et al., 2024) advantage without normalization (Liu et al., 2025b). 4The micro batch size corresponds to # rollouts we train on at time while accumulating gradients. Reinforcement Learning via Self-Distillation Figure 6: Training progression of Olmo3-7B-Instruct on Chemistry. We report the average accuracy across 16 samples per question and rolling average of response lengths over 5 steps. We report GRPO with the optimal hyperparameters for this model and task. in the SDPO loss by performing top-K distillation (i.e., only computing the top-K logits of the student and the corresponding logits of the teacher alongside term capturing the tail probability; cf. Appendix A.2). With reasonable choice of (e.g., = 100), this avoids virtually any memory overhead while capturing most of the information. 2.3 Stability improvements We find that two practical modifications significantly enhance the training stability of SDPO. First, we employ regularized self-teacher, implemented either via an exponential moving average (EMA) of the student parameters or by interpolating the current teacher with the initial teacher (cf. Appendix A.1). As detailed later, both strategies effectively stabilize learning. Second, we adopt the symmetric Jensen-Shannon divergence for the distillation loss; this formulation has similarly been shown to improve stability in on-policy distillation from external teachers (Agarwal et al., 2024)."
        },
        {
            "title": "3 Learning without Rich Environment Feedback",
            "content": "We first evaluate SDPO in standard RLVR environments, where feedback is limited to scalar rewards. Instead of using the scalar reward, SDPO treats successful attempts sampled in the current batch as feedback for failed attempts on the same question. By comparing the students attempt with correct solution, the self-teacher can identify where the student was wrong and provide dense credit assignment. 3.1 Experimental setting We evaluate tasks on which the model has not been explicitly fine-tuned: Science Q&A (Chemistry, Physics, Biology, Materials science): Undergraduate-level scientific reasoning using reasoning subsets (L3) from SciKnowEval (Feng et al., 2024). Tool use: Mapping tool-API specification and user request to the correct tool call, using ToolAlpaca (Tang et al., 2023). We perform train-test split to test in-domain generalization. We use Qwen3-8B (Yang et al., 2025) and Olmo3-7B-Instruct (Olmo et al., 2025) as initial checkpoints and report avg@16 relative to wall-clock training time, excluding initialization & validation. 6 Reinforcement Learning via Self-Distillation Chemistry Physics Biology Materials Tool use 1h 5h 1h 5h 1h 5h 1h 5h 1h 5h 35. 59.2 27.9 58.9 57.5 54.7 54.2 60.0 60.0 69.6 70. 63.8 63.6 66.6 72.7 63.6 75.6 34.3 44.4 51.5 51.8 44.4 52.9 74.3 73.9 72.1 77.1 74.1 78. 64.9 60.2 68.0 67.7 65.7 68.5 18.8 37.7 18.1 36. 39.3 32.7 48.8 59.2 46.8 54.3 76.8 55.3 62.7 59.9 63.3 62.7 66.1 47.8 54.2 56. 62.0 63.8 58.3 70.9 73.3 73.7 75.0 73.5 79.1 56.4 56.8 60.8 65.0 60.6 62.1 Qwen3-8B + GRPO + GRPO (on-policy) + SDPO (on-policy) Olmo3-7B-Instruct + GRPO + GRPO (on-policy) + SDPO (on-policy) Table 3: Comparison of SDPO and GRPO on reasoning-related benchmarks. We report the highest achieved avg@16 within 1 hour and 5 hours of wall-clock training time, respectively. Both SDPO and on-policy GRPO perform one gradient step per generation batch, while GRPO performs 4 off-policy mini batch steps. We select optimal hyperparameters for SDPO and baselines based on 5h accuracy. Each run is performed on node with 4 NVIDIA GH200 GPUs. Together with initialization and validation, each run takes approximately 6 hours. Baselines. We compare SDPO to an improved variant of GRPO (Shao et al., 2024), which incorporates several recent modifications (Olmo et al., 2025; Khatri et al., 2026) such as asymmetric clipping (Yu et al., 2025), avoiding biased normalization (Liu et al., 2025b), and correcting for off-policy data when using efficient inference frameworks (Yao et al., 2025). We integrate these modifications into GRPO implementation that represents strong baseline, as detailed in Equation (8) in Appendix A.3. GRPO enables off-policy training through PPOs clipped importance weighting (Schulman et al., 2017). We additionally report the special case of on-policy GRPO (matching the hyperparameters of vanilla SDPO). For both baselines, we perform hyperparameter sweep and report results for the models that achieve the highest validation performance across all target tasks. Hyperparameters and training details are provided in Appendix E. We use the verl library (Sheng et al., 2025) for fast multi-GPU training. 3.2 Results Table 3 summarizes our results. We find that SDPO outperforms GRPO across almost all runs, often leading to substantial improvements. SDPO learns notably faster than GRPO, performing close to 5 hours of GRPO training after only 1 hour of training with SDPO in several cases. SDPO achieves particularly substantial improvement over GRPO on the Chemistry task, as is displayed in Figure 6 (left). With Olmo3-7B-Instruct, SDPO achieves the speedup. Moreover, SDPOs 5h GRPO accuracy in 30 minutes of wall-clock training time, 10 5h accuracy is more than 20%-points higher than that of GRPO. We remark that our results with SDPO use strictly on-policy training (i.e., one gradient step per generation batch). Given the known efficiency gains of off-policy methods that perform multiple gradient updates per generation batch, we believe that studying SDPO with off-policy updates is an exciting direction for future work. Takeaway 1 We demonstrate that SDPO can learn to reason effectively, generalizing to challenging reasoning tasks. Without requiring any modification to existing RLVR environments, SDPO outperforms GRPO substantially in several cases. 3.3 Self-distillation learns to reason concisely We consistently observe that SDPO produces substantially shorter generations than GRPO shorter on average while achieving higher accuracy. SDPOs responses are more than 3 7 Reinforcement Learning via Self-Distillation . . . Alternatively. . . Closer to D? No. . . Wait Im going in circles. . . Wait, perhaps the correct answer is B. . . 101.85 Ah, this works. . . Wait think messed up. . . Hmm. . . 101.85 69.3. . . Thus, the correct answer is likely B: 1.85. <answer> </answer> 69.3. . . . . . At pH 7.4, all functional groups are neutral. . . maintaining balance between hydrophobic and hydrophilic character. . . [The] overall polarity. . . keeps logD from being very high. . . or very low. . . [typically falling] in the 2.0-3.0 range, with 2.61 (C) being reasonable estimate. . . <answer> </answer> (a) GRPO (5,549 tokens) (b) SDPO (764 tokens) Figure 7: Example responses from GRPO and SDPO after 50 training steps to the following question: What is the correct octanol/water distribution coefficient logD under the circumstance of pH 7.4 for the molecule O=C1O[C@@H](COc2ccon2)CN1c1ccc(C2=CCOCC2)c(F)c1? The answer options are A: 1.32, B: 1.85, C: 2.61, D: 3.76. The correct answer is C. GRPOs answer contains 5 Wait. Further, GRPOs answer No., and 25 repeats calculations such as 101.85 69.3, which appears four times, and the model even explicitly generates Wait Im going in circles. SDPOs answer avoids any circular reasoning and is more than 7 shorter. The base model is Qwen3-8B. Hmm., across tasks (cf. Table 8 in Appendix D). On Chemistry with Olmo3-7B-Instruct, SDPO even achieves 7 reduction in response length relative to GRPO while maintaining higher accuracy (Figure 6 (right)). While recent progress in RLVR has demonstrated that scaling response length is powerful driver of emergent reasoning capabilities (Jaech et al., 2024; Guo et al., 2025; Muennighoff et al., 2025), our results suggest that effective reasoning need not always be verbose. We find that SDPO improves the efficiency of reasoning. Qualitatively, we observe that the longer responses from GRPO often stem from superficial reasoning rather than necessary cognitive steps. GRPO frequently generates filler phrases like Hmm and Wait or enters circular logical loops that repeat previous steps verbatim. Figure 7 displays representative example of this phenomenon. Remarkably, SDPOs generations remain concise and avoid these superficial patterns. This may be explained by SDPOs dense credit assignment, which assigns specific advantage to each next-token prediction, leading to sparse advantages (cf. Figure 21 in Appendix F). By improving the efficiency of reasoning, SDPO reduces inference generation time and demonstrates that reasoning performance can be improved by refining how the model reasons, not just how long it reasons."
        },
        {
            "title": "4 Learning with Rich Environment Feedback",
            "content": "We next evaluate SDPO on coding tasks. Coding is canonical example of an RL environment that provides rich feedback, such as runtime errors and failed unit tests. Learning to solve these coding problems requires strong credit assignment since the student must identify its precise mistakes to avoid repeating them in the future. LiveCodeBench (LCB; Jain et al., 2025) provides set of contest-style coding problems, ranging from simple to competition-level. We restrict our evaluation to the most recent LCBv6 subset of LCB, which contains 131 questions released between February and May 2025. We consider setting with public and private unit tests, common for code contests and coding platforms like LeetCode, where the public tests are used for evaluation during training and the private tests are used for validation (Chen et al., 2022; Le et al., 2022; El-Kishky et al., 2025; Samadi et al., 2025).5 We use the Qwen3 (Yang et al., 2025) model family for our experiments, with Qwen3-8B as default unless otherwise specified. We report the average accuracy over 4 rollouts and use the same GRPO baseline as outlined in Section 3.1. 5We select public tests as 50% random subset of private tests. 8 Reinforcement Learning via Self-Distillation Figure 8: SDPO improves with model size. We compare the final LCBv6 validation accuracy of SDPO and GRPO at train step 80, across model sizes from Qwen3. The ability of SDPOs teacher to perform accurate retrospection appears to be an emergent phenomenon with scale. We include an additional scaling study with Qwen2.5-Instruct in the appendix (cf. Figure 17) which further supports this finding. Error bars indicate the standard error across 3 seeds. Results. Figure 1 compares the learning curves of SDPO and GRPO on LCBv6. We find that SDPO achieves substantially higher final accuracy (48.8%) than GRPO (41.2%) while also outperforming the strongest instruct models on the public LCBv6 leaderboard:6 Claude Sonnet 4 (40.5%) and Claude Opus 4 (39.7%). Furthermore, SDPO reaches the final accuracy of GRPO in 4 fewer generations. We include an extended comparison to other RLVR baselines that perform similarly to GRPO in Table 9 in the appendix. Differentiating between the easy, medium, and hard questions of LCB, we find that SDPO particularly improves over GRPO in solving medium and hard questions (cf. Figure 15 in the appendix). 4.1 Self-distillation benefits from stronger models central question for our work is whether SDPO is sensitive to the in-context learning ability of the base model. Intuitively, we expect that SDPO benefits from strong in-context learner, since this enables the teacher to perform more accurate retrospection. To answer this question, we perform scaling study with different model sizes from the Qwen3 (Yang et al., 2025) family. As shown by extensive prior work, the ability to learn in-context increases with model size (e.g., Brown et al., 2020). As depicted in Figure 8, SDPO significantly outperforms GRPO on larger models while only slightly improving over GRPO on smaller models. To determine whether SDPO can also underperform GRPO on model weaker than Qwen3-0.6B, we performed an additional scaling study with Qwen2.5Instruct (Qwen et al., 2024). While outperforming GRPO with Qwen2.5-7B and performing similarly with Qwen2.5-8B, we find that SDPO underperforms GRPO on Qwen2.5-1.5B, as seen in Figure 17 in Appendix D. Takeaway Our results suggest that the marginal improvement of SDPO over GRPO is tightly coupled with the strength of the base model, and motivates future study on models stronger than Qwen3-8B. In the same way that in-context learning is an emergent phenomenon with scale, the self-teachers ability to perform accurate retrospection in SDPO appears to be emergent with scale. 6On the public leaderboard, the LCBv6 subset can be obtained by selecting February to May 2025. 9 Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "4.2 Self-distillation performs dense credit assignment",
            "content": "V is the vocabulary. Whereas GRPO assigns constant advantage to each generated token, SDPO assigns an individual advantage to each possible next token along the generated sequence based on the agreement of student and teacher. At each position in the generated sequence y, there possible next tokens where are In distillaV tion, this level is typically called the logit-level since it corresponds to the logits of the model. In practice, we approximate the full nexttoken distribution by the top-K tokens, and as such, SDPO assigns unique advantages per sey quence. This is illustrated in Figure 9 and allows SDPO to perform dense credit assignment. Figure 9: Dense credit assignment in SDPO in the example from Figure 4. Shown in blue are tokens which become more likely under the self-teacher. The selfteacher identifies how the returned range has to be modified so that it does not contain n. natural question is whether the performance gains of SDPO are due to leveraging rich feedback in RLRF or due to the dense credit assignment of SDPO. To answer this question, we ablate the performance of SDPO in three configurations: Logit-level SDPO: credit assignment over the 100 most likely tokens (under the student) at each position. Token-level SDPO: credit assignment over the most likely token at each position. Sequence-level SDPO: We compute SDPO advantages for all generated tokens and average them to produce single scalar advantage per sequence (as in GRPO). This does not perform denser credit assignment than GRPO but still leverages the rich feedback . As shown in Figure 10 (left), the dense credit assignment of logit-level SDPO leads to significant performance gains over token-level SDPO and sequence-level SDPO. Nevertheless, even sequence-level SDPO outperforms GRPO, indicating that leveraging rich feedback in RLRF can lead to substantial gains over RLVR methods even without dense credit assignment. 4.3 The self-teacher improves during training Teacher Accuracy Avg accuracy qθ qθref Trust-region EMA Contrary to standard distillation, the selfteacher in SDPO is not frozen, but updated throughout training. This is critical component of SDPO, since it enables the teacher to improve over time, which means that the student can learn from stronger target. To investigate whether the self-teacher Table 4: Best/average accuracy until step 90 improves during training, we plot the avof various methods for teacher regularization. erage accuracy when generating using the Trust-region and EMA teachers use α = 0.01. self-teacher in Figure 10 (right). We find Training of the qθ eventually diverges. Error that the self-teacher improves significantly ranges indicate standard errors across 3 seeds. during training. Most notably, the students accuracy surpasses the initial teachers accuracy in later stages of training. This demonstrates that SDPO enables true bootstrapping of weak model to strong model, without the initial self-teachers performance limiting the final student. 29.8 44.4 45.6 45.3 36.1 48.8 50.6 49.3 1.3 0.2 0.2 0. 1.6 0.7 0.9 0.3 10 Reinforcement Learning via Self-Distillation Figure 10: Left: Rich feedback in RLRF and dense credit assignment of SDPO are complementary. We compare logit-level, token-level, and sequence-level SDPO advantages to GRPO. While denser credit assignment in SDPO is beneficial (logit-level > token-level > sequence-level), even sequence-level SDPO significantly outperforms GRPO due to leveraging the rich feedback. Error bars indicate the standard error across 3 seeds. Right: The self-teacher improves during training. We display the generative accuracy of the selfteacher compared to student on the current training batch (with rolling average over 5 steps). The final student score is taken at step 80. Notably, the performance of the student significantly surpasses the initial teachers accuracy. Error bars indicate the standard deviation across 3 seeds. As described in Section 2.3, SDPO uses regularized teacher to stabilize training. As can be seen in Table 4, non-regularized teacher significantly underperforms the regularized teachers. Furthermore, trust-region and EMA teachers outperform the teacher frozen at the initial teachers parameters, showing that the teacher improves through parameter sharing with the student. Yet, SDPO performs well even with frozen teacher. 4.4 On-policy self-distillation avoids catastrophic forgetting Prior work has shown that key benefit of on-policy algorithms, such as GRPO, is that models tend not to forget previously obtained capabilities (Shenfeld et al., 2026; Chen et al., 2025b; Lu & Thinking Machines Lab, 2025). This is practically desirable since it enables continual training pipelines where model is trained sequentially on diverse tasks without the need to retrain from scratch. To evaluate forgetting, we test the final checkpoints of GRPO and SDPO on diverse holdout tasks: IFEval (Zhou et al., 2023), which tests the ability of model to follow precise format instructions; ArenaHard-v2 (Li et al., 2025), which is an LLM-judged benchmark of real-world instruction-following prompts derived from LMArena (Chiang et al., 2024); and MMLU-Pro (Wang et al., 2024b), which tests broad multi-task knowledge and reasoning. As displayed in Table 5, SDPO learns the new task while mitigating degradation of initial capabilities, overall achieving better performanceforgetting tradeoff than GRPO. Off-policy self-distillation baseline. As an additional baseline, we consider training the student via supervised fine-tuning (SFT) on successful generations from the selfteacher (Scheurer et al., 2023; Dou et al., 2024).7 This requires 2 the generations of SDPO for the same number of steps, since we have to generate from both the student and the teacher. We report SFT on the successes of the self-teacher, which achieves higher accuracy than also including initial successes from the student in the SFT data. As shown in Table 5, SFT on the self-teacher significantly underperforms SDPO on LCBv6, while leading to worse forgetting of prior capabilities. This mirrors prior findings on the instability of off-policy imitation (see, e.g., Agarwal et al., 2024). 7SFT on teachers predictions is standard off-policy distillation approach (Kim & Rush, 2016). Reinforcement Learning via Self-Distillation Task: LCBv6 IFEval ArenaHard-v2 (hard prompt) Holdout tasks: ArenaHard-v2 (creative writing) MMLU-Pro Avg. (holdout) Base SFT on self-teacher GRPO SDPO 27.9 42.7 41.2 48.8 83.9 83.7 82.2 83.2 14. 11.2 12.0 12.3 13.7 8.9 10.8 11.1 62.5 61.9 62.3 62.9 43. 41.4 41.8 42.4 Table 5: On-policy methods do not suffer from catastrophic forgetting. We compare the accuracy of the final checkpoint on the training task LCBv6 and on holdout tasks IFEval, ArenaHard-v2, and MMLU-Pro. We compare to baseline that trains directly on responses generated by the initial self-teacher with SFT. Overall, SDPO achieves the best performance forgetting tradeoff. We include additional baseline results in Table 9 in the appendix."
        },
        {
            "title": "4.5 Can GRPO and SDPO be combined?",
            "content": "GRPO utilizes Monte Carlo advantages, which are unbiased with respect to the objective of maximizing expected reward J(θ) := x)]. In contrast, SDPO advantages πθ ( are inherently biased with respect to J(θ) due to being computed from rich feedback and self-teacher. This dichotomy parallels the fundamental distinction between Monte Carlo and bootstrapped advantages in RL: while the latter are biased, they typically yield lower variance (Sutton & Barto, 1998; Schulman et al., 2016). This motivates hybrid approach that combines reward-derived GRPO advantages with feedback-derived SDPO advantages: x)[r(y ASDPO+GRPO i,t ( ˆyi,t) := λAGRPO i,t ( ˆyi,t) + (1 λ)ASDPO i,t ( ˆyi,t), λ [0, 1]. (3) As shown in Figure 11, SDPO+GRPO appears to be more robust to weaker models than SDPO. Intuitively, in weaker model such as Qwen3-0.6B, the SDPO advantages are less reliable, and hence including the GRPO advantage helps to stabilize training. In contrast, we find that SDPO+GRPO slightly underperforms SDPO on stronger models such as Qwen3-8B. This suggests that the signal of GRPO, only informed by scalar reward, can be actively harmful with strong initial model. 4.6 Which feedback is most informative? To understand which type of rich feedback is most informative, we ablate the three types of feedback present in verifiable environment like code generation: the sample solution (if successful rollout is available in the current rollout group), the environment output (such as runtime errors), and the students original attempt. Figure 11: We compare the LCBv6 validation accuracy at step 80, across model sizes from Qwen3. SDPO+GRPO significantly outperforms SDPO on the weaker Qwen3-0.6B, while slightly underperforming SDPO on stronger models. We use λ = 0.9. Error bars indicate the standard error across 3 seeds. Including sample solution from failed attempts rollout group (if Sample solutions. available) closely mirrors the group-relative advantages of GRPO. We emphasize that these sample solutions are always generated by the student, as in GRPO, and do not require an expert model. They allow for disincentivizing unsuccessful approaches if the model is already able to solve the question. However, unlike GRPO where all tokens receive the same negative advantage, the self-teacher can identify specific mistakes and provide feedback on how to fix them. 12 Reinforcement Learning via Self-Distillation Teacher before training Student trained with SDPO = output 32.5 = solution 42.4 = output + solution 42.5 = + output + solution 39.3 Acc. (%) 0.5 1.0 1.2 0.8 Same output (%) 13.7 12.1 10.1 30.0 0.6 0.7 0.2 0.9 39.8 36.8 48.9 44. 0.2 2.7 0.9 1.8 0.40 0.07 0.37 0.23 0.0 0.0 0.0 0.0 Acc. (%) Avg. entropy Table 6: Performance of varying kinds of feedback. We evaluate informativeness of feedback based on SDPO training (until step 70) as well as the direct impact on the self-teacher. Same output measures the percentage of cases where the teacher receives the same environment output as the students initial attempt (i.e., not exploring alternative approaches). We observe that environment output and sample solutions are complementary and each provide informative feedback. Naively including only solutions or initial attempts significantly reduces diversity in the teacher and student. We remark that the sample solutions are generated by the student, enabling similar group-relative advantage estimation to GRPO. Error bars indicate standard deviation across 3 seeds. Environment output. The environment output describes the state of the environment after the students attempt. This is complementary to sample solutions since it can provide useful signal even if the student has never solved the question before (a setting we explore extensively in Section 5). Leveraging environment output is key differentiating factor between RLRF and RLVR settings. Students original attempt. The students original attempt does not have to be included in the reprompting template of the teacher. Indeed, we find that including it biases the teacher towards the students attempt (cf. Table 6). This reduces the entropy of the students distribution (particularly for initially uncertain tokens), thereby reducing exploration. We summarize results in Table 6 where we evaluate the effect on SDPO training as well as the direct impact on the self-teacher. We find that environment output & sample solutions are complementary, each providing informative feedback. Generally, we observe that performance is not sensitive to syntactic variations of the reprompting template from Table 2."
        },
        {
            "title": "5 Solving Hard Questions via Test-Time Self-Distillation",
            "content": "In Sections 3 and 4, we have demonstrated that SDPO can substantially improve over RLVR methods when performing train-time RL for reasoning tasks. We now turn to test-time setting where the model is given only single hard (binary-reward) question and must discover solution as quickly as possible: Definition 5.1 (Discovery time). The discovery time is the number of trials needed until solution is found (i.e., the smallest with the k-th attempt yk receiving reward 1). Based on this notion, we can define measure of the efficacy of discovery: discovery@k := P(discovery time = P(r(y1 k) x) = 1 or r(y2 x) = 1 or . . . or r(yk x) = 1), (4) where the probability is over any randomness in the algorithm producing yk and the rewards. Thus, the discovery@k metric quantifies the probability of discovering the solution within steps.8 While prior work has studied discovery with continuous rewards (e.g., Novikov et al., 2025; Yuksekgonul et al., 2026), discovery with language models in sparse or binary-reward settings does not allow hill-climbing continuous reward and has remained less well understood. The most naive approach to discovery in binary-reward tasks is to sample repeatedly i.i.d. from the base model, also known as best-of-k. The canonical pass@k metric for 8Our proposed discovery@k metric is canonical metric in the study of runtime speedup (i.e., time until termination, Dolan & Moré (2002)). Reinforcement Learning via Self-Distillation Figure 12: Compressing context into model weights via self-distillation. We illustrate the process of distilling the interaction history (context c) into the model parameters θ. The model πθ repeatedly attempts fixed hard question x, generating an answer and receiving feedback . Rather than appending this history to the context window, the model updates its weights θt θt+1 with SDPO (batch size 1) based on the feedback, effectively fixing mistakes by encoding πθ( x, c) directly into the policy πθ x). ( best-of-k sampling is exactly the probability of discovering at least one solution within independent samples from fixed model, coinciding with discovery@k. The discovery@k metric generalizes pass@k to algorithms that sample attempts sequentially. common sequential approach re-prompts the base model with additional context from previous attempts (Madaan et al., 2023; Shinn et al., 2023). We refer to this as multi-turn sampling. Here, the model itself does not change, only its context evolves over time. Performing RLVR on the question does not improve over best-of-k sampling from the base model, since binary reward provides no signal until the first solution has already been found.9 An RLRF method like SDPO does not face the same limitation, as it receives rich feedback from the environment after each attempt. This rich feedback enables the model to repeatedly correct its mistakes as it encounters them and receives feedback, even before ever discovering solution. In contrast to multi-turn sampling, SDPO repeatedly x) as we compresses context = (yk, fk) by distilling πθ( illustrate in Figure 12. This self-distillation enables SDPO to continually learn over long contexts, whereas the memory bottleneck of transformers inherently limits the context length of multi-turn sampling (Vaswani et al., 2017). In this section, we seek to answer the question: x, c) into model πθ ( Can repeatedly compressing context into model weights via self-distillation accelerate discovery for hard questions? 5.1 Experimental setting We consider particularly challenging subset of questions from LCBv6 that are at Qwen38Bs performance ceiling and require significant test-time sampling to find any solution. Concretely, we define two groups using Qwen3-8Bs pass@k: Hard tasks with pass@64 < 0.5 and very hard tasks with pass@64 < 0.03. Among these, we retain questions for which any of best-of-k, multi-turn, or SDPO find at least one solution within 512 steps across 5 seeds. This results in 19 hard and 9 very hard questions. For best-of-k sampling under the base model, we report the standard pass@k estimate (Chen et al., 2021b) from 2944 independent rollouts. As multi-turn sampling, we sequentially reprompt the model in-context using the concatenated feedback from previous attempts. To remain within Qwen3-8Bs 40k-token context limit, we employ first-in, first-out sliding window, discarding the earliest feedback once the maximum prompt length (32k tokens) is reached. We ablate the multi-turn reprompting strategy in Figure 19 in Appendix and find that retaining only past feedback while forgetting earlier attempts significantly outperforms the baseline that additionally retains past attempts. We evaluate SDPO with batch size of 16. We ablate this choice in Figure 19 in Appendix and find that overall performance differences are marginal, yet smaller batch sizes are beneficial for improvements at low generation budgets, while larger batch sizes result in more stable updates that still learn to solve questions at later stages into the run. 9For this reason, several works consider explicitly constructing curricula of solvable questions (e.g., Zhao et al., 2025; Huang et al., 2026; Diaz-Bone et al., 2025; Hübotter et al., 2025b), which self-distillation avoids. Other work found that RLVR yields limited improvement on hard questions (Yue et al., 2025). 14 Reinforcement Learning via Self-Distillation Figure 13: Self-distillation at test-time solves LiveCodeBench questions that neither the base model nor multi-turn conversations can solve. Left: Very hard questions (9 total) from LCBv6 where the base model achieves pass@64 < 0.03, i.e., in less than 3% cases, sampling 64 responses yields any success. Right: Hard questions (19 total) from LCBv6 where the base model achieves pass@64 < 0.5. We report the discovery@k metric, representing the probability of discovering at least one solution within total generations. Across both difficulty levels, SDPO achieves higher discovery@k rates at almost all generation budgets, compared to the base model and multi-turn conversation baseline that receives the feedback in-context. We report the mean and bootstrapped 90% confidence intervals of the mean across 5 random seeds per question. 5.2 Results Figure 13 compares discovery@k for SDPO, multi-turn sampling, and best-of-k sampling on very hard (left) and hard (right) questions from LCBv6. Across both difficulty levels, SDPO achieves substantially higher discovery@k rates at almost all generation budgets. On very hard tasks, multi-turn and best-of-k largely fail to solve questions within the available generation budget, achieving discovery@2750 of only 35.6% and 41.5%, respectively, whereas SDPO discovers solution in 53.2% of cases. SDPO not only solves more questions overall but also does so with substantially fewer attempts. Notably, to reach 22% discovery probability on very hard questions, SDPO requires approximately 3 fewer generations than best-of-k and multi-turn sampling. On hard tasks, SDPO reaches 78% discovery@2750 probability while achieving 67% discovery probability with roughly 2.4 fewer generations than best-of-k and multi-turn sampling. Overall, multi-turn and best-of-k sampling solve only 68.4% and 72.3% of questions, respectively. The context window length for multi-turn sampling is reached after 837 ( 466) steps for hard questions and after 1007 ( 349) steps for very hard questions, offering possible explanation for its diminishing gains at high generation budgets. Question 3 is only solved by SDPO. SDPO solves all questions that are solved by best-of-k and multi-turn sampling. Beyond that, SDPO uniquely discovers solution for Q3, which is neither solvable with multi-turn sampling nor with best-of-k sampling within 2750 attempts. In contrast, SDPO first discovers solution for Q3 after 321 attempts, which corresponds to 20 iteration steps of self-distillation based on feedback with batch size of 16. We include detailed per-question results in Table 10 in Appendix D. The initial self-teacher does not solve hard questions. Notably, the self-teachers initial accuracy is < 1% for almost all questions, and even exactly 0% on 78% of them (Table 11 in Appendix D). This shows that single turn of in-context feedback is insufficient to solve the 15 Reinforcement Learning via Self-Distillation problem. Despite this, the self-teachers credit assignment is sufficiently effective for SDPO to iteratively refine the policy and eventually solve these questions. Takeaway 3 We demonstrate that rich environment feedback enables SDPO to significantly accelerate discovery for hard questions. This is in contrast to RLVR methods, which only receive binary reward signal, and therefore only begin learning once the first solution has already been found."
        },
        {
            "title": "6.1 Reinforcement Learning with LLMs",
            "content": "Recently, large-scale RL training on diverse tasks has significantly improved the performance of LLMs on general reasoning tasks (Guo et al., 2025; Kimi et al., 2025; Olmo et al., 2025; Jaech et al., 2024; Lambert et al., 2025). This progress is primarily enabled by RLVR methods that use Monte Carlo estimates of rewards, such as STaR or GRPO (Zelikman et al., 2022; Shao et al., 2024), similar to the classical REINFORCE algorithm (Williams, 1992). While several traditional RLVR algorithms rely on learning separate value networks (Schulman et al., 2017), they incur substantial memory costs and retain the information bottleneck of scalar rewards. In the RLVR setting, it is common for an (outcome) reward to be given only at the end of sequence. To improve credit assignment, several works learn so-called process reward models (PRMs) that estimate rewards for each step in the sequence (Lightman et al., 2023; Wang et al., 2024a; Setlur et al., 2025). Unlike our RLRF setting, PRMs are typically trained on scalar rewards, either on value estimates for intermediate states or on outcome rewards (Cui et al., 2025). Unlike the self-teacher in SDPO, PRMs are distinct model from the student, introducing significant memory overhead. Our work shows that each language model is implicitly PRM through retrospection if given rich feedback. Conceptually, our work is related to expert iteration (Anthony et al., 2017) where student is bootstrapped by repeatedly imitating an improved version of itself (called the expert). Canonically, the expert combines the student with test-time search, such as tree search (Anthony et al., 2017) or majority voting (Zuo et al., 2025). In contrast, SDPO leverages the students ability to learn from rich feedback provided in-context. 6.2 Learning from Rich Feedback and through Retrospection Beyond scalar outcome rewards, recent works have leveraged rich execution or verbal feedback to guide generation (Gehring et al., 2025; Yuksekgonul et al., 2025). primary line of research focuses on translating verbal feedback into reward functions for RL. This is often achieved by mapping feedback to discrete token-level rewards using an external frozen model (Wang et al., 2026), or by employing strong external LLMs to explicitly construct state-wise reward functions (Goyal et al., 2019; Xie et al., 2024; Urcelay et al., 2026). Alternatively, feedback can be utilized without explicit reward modeling. Several approaches focus on in-context improvement without integrating the process into the RL optimization loop (Chen et al., 2021a; Madaan et al., 2023; Shinn et al., 2023; Yao et al., 2024; Yuksekgonul et al., 2025; Lee et al., 2025). Others manually curate preference datasets by pairing responses before and after feedback to train with direct preference optimization (Stephan et al., 2024; Lee et al., 2024), though this requires additional generation and lacks the direct credit assignment of SDPO. Various recent works bootstrap thinking traces from known answers, using these answers as rich feedback (Zhou et al., 2026; Hatamizadeh et al., 2026; Zhang et al., 2025). central object in several recent works is feedback-conditioned policy πθ(y x, ), which learns answers that lead to feedback (Liu et al., 2023; Zhang et al., 2023; Luo et al., 2025), typically through supervised objectives. The idea behind these approaches is to deploy 16 Reinforcement Learning via Self-Distillation policy conditioned on desirable (i.e., positive) feedback for deployment. This approach is conceptually related to goal-conditioned RL (Schaul et al., 2015; Liu et al., 2025a), where one can learn from negative examples through goal relabeling (Andrychowicz et al., 2017). Feedback-conditioned policies view feedback as goal, whereas RLRF views feedback as state that can be used to determine whether the goal is achieved. Unlike SDPO, these methods do not use feedback for credit assignment in negative trajectories, but rather as data transformation for goal relabeling."
        },
        {
            "title": "6.3 Distillation",
            "content": "Distillation is frequently employed as an alternative to supervised fine-tuning (SFT) when strong teacher model is available. This approach transfers capabilities by training student to mimic the output distribution or intermediate representations of the teacher (Hinton et al., 2015; Romero et al., 2015; Kim & Rush, 2016; Sanh et al., 2019; Xie et al., 2020). Distillation is typically performed on fixed off-policy datasets. To address the distribution shift between training and inference, recent works explore on-policy distillation, where the student learns from feedback of an external teacher on its own generations (Agarwal et al., 2024; Gu et al., 2024; Yang et al., 2025; Lu & Thinking Machines Lab, 2025). This mitigates the train-test mismatch, which relates closely to earlier work on online imitation learning (Ross et al., 2011). 6.4 Self-Distillation The concept of self-distillation was first proposed by Snell et al. (2022) in setting akin to supervised learning, introducing the idea of sampling from model provided with extra context and training the same model to mimic these predictions without that context. This mechanism has proven effective for compressing behavior (Bai et al., 2022; Choi et al., 2022) and factual information (Eyuboglu et al., 2026; Kujanpää et al., 2025) into model weights. Beyond compressing fixed context into model weights, recent works have used self-distillation to learn from environment feedback (Scheurer et al., 2023; Dou et al., 2024; Mitra & Ulukus, 2025). These approaches use an off-policy self-distillation objective, which substantially underperforms SDPOs on-policy learning. Off-policy self-distillation trains the student on generations from the teacher, whereas SDPO trains the student to avoid mistakes in its own generations. In concurrent work, Chen et al. (2025c) apply on-policy self-distillation to grid world settings where feedback is scalar reward, and reflection stage in the self-teacher diagnoses possible mistakes, showing improved credit assignment compared to learning value networks for advantage estimation."
        },
        {
            "title": "7 Conclusion, Limitations, and Future Work",
            "content": "We introduced Reinforcement Learning with Rich Feedback (RLRF), paradigm where environments provide tokenized feedback beyond scalar rewards, and argued that this removes key information bottleneck of RLVR. We then proposed Self-Distillation Policy Optimization (SDPO), which uses the current policy as feedback-conditioned self-teacher and distills its corrected log-probabilities into the student. This leverages the models ability to learn from context for dense credit assignment. We further demonstrated that SDPO can be implemented as minimal, drop-in modification to standard RLVR pipelines. Empirically, SDPO demonstrates superior sample efficiency and wall-clock convergence compared to GRPO on reasoning tasks, even when training in standard RLVR environments without rich feedback. SDPOs gains grow with model scale, suggesting that the capacity for self-correction scales with the models in-context learning capabilities. Moreover, we show that performing SDPO at test-time on individual hard binary-reward tasks accelerates the discovery of solutions compared to strong baselines. SDPO enables learning from rich feedback in way that parallels human cognition: utilizing precise outcomes rather than just binary rewards. By allowing the model to determine retrospectively how it should have acted, we demonstrate that language models can convert diverse tokenized feedback into effective self-supervision. 17 Reinforcement Learning via Self-Distillation Limitations. Our findings show that SDPOs performance depends on models in-context learning ability, suggesting that SDPO is primarily applicable for RL-training stronger base models, while it can underperform GRPO on weaker models. Moreover, performance depends on the quality of the environment feedback. If the environment provides uninformative or misleading feedback, model may not be able to learn from it through SDPO. Finally, SDPO adds small computational overhead compared to GRPO for computing the log-probs of the retrospective model. While often negligible, this may be larger overhead for smaller models with shorter generation lengths, where generation time is comparatively small. Future Work. Our work highlights several exciting directions for future research: Long-horizon and agentic settings. RLRF is particularly appealing when trajectories are long or expose information about intermediate states. Evaluating SDPO in agentic environments is natural next step. Training dynamics at scale. Beyond our evaluation on LiveCodeBench, it would be particularly interesting to scale SDPO to large multi-task RL training runs and further study its scaling properties with frontier base models. Beyond verifiable rewards. While we focused on verifiable code generation, many tasks provide textual feedback without ground-truth verifier. Investigating whether SDPOs retrospection mechanism can improve alignment in open-ended text generation or continuous-reward tasks remains an open empirical question. Behavioral differences in reasoning. We observed that SDPO induces qualitatively different reasoning patterns than GRPO, notably avoiding the latters tendency toward verbosity and superficial reasoning. Future work should systematically study how individual aspects, such as the reprompt template, influence behavior."
        },
        {
            "title": "Author Contributions",
            "content": "Jonas Hübotter conceived of the project in summer 2025 and has been working on it full-time since then, leading the team. Jonas proposed the conceptual framework of self-distillation for credit assignment with input from Lejs, implemented the algorithm with help from others, led the quantitative experiments on LCBv6, and led the writing of the paper. Frederike Lübeck led the design of the code environment, led the design and evaluation of the TTT setting in Section 5 with input from Jonas, contributed to the project direction in discussions, and contributed significantly to the writing of the paper. Lejs Behric noted the dense credit assignment of knowledge distillation with strong teacher models in discussions with Jonas, inspiring the idea of self-distillation. Further, Lejs led the evaluation of different teacher templates, co-led the development of tool for qualitative analysis of runs with Marco and Daniel, helped implement parts of the algorithm, and contributed to the project direction in discussions. Anton Baumann joined in December 2025 and led the evaluation of SDPO without rich feedback in Section 3 with input from Jonas, and contributed to the writing of the paper. Marco Bagatella and Daniel Marta co-led the development of tool for qualitative analysis of runs with Lejs, contributed to the training infrastructure, and contributed to the project direction in discussions. Ido Hakimi significantly contributed to the initial codebase and experimental setup, contributed early algorithmic ideas, and contributed to the project direction in discussions. Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, and Andreas Krause supported this project, with Idan and Carlos joining in December 2025. They made significant contributions to the project direction in discussions and gave valuable advice on our presentation. Thomas and Idan, in particular, significantly contributed to the development of core algorithmic ideas and design of experiments. Thomas further evaluated checkpoints on holdout benchmarks. Carlos suggested the qualitative analysis of reasoning traces in Figure 7 and the presentation of TTT results in Section 5. Andreas pointed out valuable connections to existing work in RL which shaped the direction of the project. 18 Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Akira Yoshiyama, Yassir Akram, Parnian Kassraie, Jonathan Thomm, Roman Vorushin, Afra Amini, Imanol Schlag, Yu Sun, and Moritz Hardt for helpful discussions. We thank Eduard Durech for helpful conversations regarding the scaling of RL fine-tuning and for his technical guidance on distributed infrastructure and long-context optimization. Furthermore, we would like to thank Leander Diaz-Bone for supporting dataset generation. This project was supported through the Swiss AI compute grant a156 and, in part, compute grant infra01. JH was supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545. FL and MB were supported by the ETH-MPI Center for Learning Systems. TKB and IH were supported by an ETH AI Center Postdoctoral Fellowship. DM was supported by the Knut and Alice Wallenberg Foundation."
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In ICLR, 2024. Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning. In ICML, 2025. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In NeurIPS, 2017. Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In NeurIPS, 2017. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. In NeurIPS, 2025. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165, 2020. Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 2015. Meng Cao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Scar: Shapley credit assignment for more efficient rlhf. arXiv preprint arXiv:2505.20417, 2025. Alex Chan, Hao Sun, Samuel Holt, and Mihaela Van Der Schaar. Dense reward for free in reinforcement learning from human feedback. In ICML, 2024. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025a. 19 Reinforcement Learning via Self-Distillation Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. In ICLR, 2022. Howard Chen, Noam Razin, Karthik Narasimhan, and Danqi Chen. Retaining by doing: The role of on-policy data in mitigating forgetting. arXiv preprint arXiv:2510.18874, 2025b. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In NeurIPS, 2021a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b. Wentse Chen, Jiayu Chen, Fahim Tajwar, Hao Zhu, Xintong Duan, Ruslan Salakhutdinov, and Jeff Schneider. Retrospective in-context learning for temporal credit assignment with large language models. In NeurIPS, 2025c. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In ICML, 2024. Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo. Prompt injection: Parameterization of fixed inputs. arXiv preprint arXiv:2206.11349, 2022. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Leander Diaz-Bone, Marco Bagatella, Jonas Hübotter, and Andreas Krause. Discover: Automated curricula for sparse-reward reinforcement learning. In NeurIPS, 2025. Elizabeth Dolan and Jorge Moré. Benchmarking optimization software with performance profiles. Mathematical programming, 91(2), 2002. Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, and Nanyun Peng. Re-rest: Reflection-reinforced self-training for language agents. In EMNLP, 2024. Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. Sabri Eyuboglu, Ryan Ehrlich, Simran Arora, Neel Guha, Dylan Zinsley, Emily Liu, Will Tennien, Atri Rudra, James Zou, Azalia Mirhoseini, et al. Cartridges: Lightweight and general-purpose long context representations via self-study. In ICLR, 2026. Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. Sciknoweval: Evaluating multi-level scientific knowledge of large language models. arXiv preprint arXiv:2406.09098, 2024. Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. In ICML, 2025. Prasoon Goyal, Scott Niekum, and Raymond Mooney. Using natural language for reward shaping in reinforcement learning. In IJCAI, 2019. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. 2024. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. In ICLR, 2026. 20 Reinforcement Learning via Self-Distillation Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with stochastic actor. In ICML, 2018. Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models. In ICLR, 2024. Ali Hatamizadeh, Syeda Nahida Akter, Shrimai Prabhumoye, Jan Kautz, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, and Yejin Choi. Rlp: Reinforcement as pretraining objective. In ICLR, 2026. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. In ICLR, 2026. Jonas Hübotter, Patrik Wolf, Alexander Shevchenko, Dennis Jüni, Andreas Krause, and Gil Kur. Specialization after generalization: Towards understanding test-time training in foundation models. In ICLR, 2026. Jonas Hübotter, Sascha Bongni, Ido Hakimi, and Andreas Krause. Efficiently learning at test-time: Active fine-tuning of llms. In ICLR, 2025a. Jonas Hübotter, Leander Diaz-Bone, Ido Hakimi, Andreas Krause, and Moritz Hardt. Learning on the job: Test-time curricula for targeted reinforcement learning. arXiv preprint arXiv:2510.04786, 2025b. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In ICLR, 2025. Leslie Pack Kaelbling, Michael Littman, and Anthony Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2), 1998. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Refining credit assignment in rl training of llms. In ICML, 2025. Devvrit Khatri, Lovish Madaan, Rishabh Tiwari, Rachit Bansal, Sai Surya Duvvuri, Manzil Zaheer, Inderjit Dhillon, David Brandfonbrener, and Rishabh Agarwal. The art of scaling reinforcement learning compute for llms. In ICLR, 2026. Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In EMNLP, 2016. Kimi, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Kalle Kujanpää, Pekka Marttinen, Harri Valpola, and Alexander Ilin. Efficient knowledge injection in LLMs via self-distillation. TMLR, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In PSIGOPS, 2023. Reinforcement Learning via Self-Distillation Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. In COLM, 2025. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In NeurIPS, 2022. Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, and Moontae Lee. Reinforcement learning from reflective feedback (rlrf): Aligning and improving llms via fine-grained self-reflection. arXiv preprint arXiv:2403.14238, 2024. Yoonho Lee, Joseph Boen, and Chelsea Finn. Feedback descent: Open-ended text optimization via pairwise comparison. arXiv preprint arXiv:2511.07919, 2025. Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arenahard and benchbuilder pipeline. In ICML, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In ICLR, 2023. Grace Liu, Michael Tang, and Benjamin Eysenbach. single goal is all you need: Skills and exploration emerge from contrastive rl without rewards, demonstrations, or subgoals. In ICLR, 2025a. Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv:2302.02676, 2023. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. In COLM, 2025b. Kevin Lu and Thinking Machines Lab. On-policy distillation. Thinking Machines Lab: Connectionism, 2025. URL https://thinkingmachines.ai/blog/on-policy-distillation. Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, and Tianyu Pang. Language models can learn from verbal feedback without scalar rewards. arXiv preprint arXiv:2509.22638, 2025. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. In NeurIPS, 2023. Purbesh Mitra and Sennur Ulukus. Semantic soft bootstrapping: Long context reasoning in llms without reinforcement learning. arXiv preprint arXiv:2512.05105, 2025. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540), 2015. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. In EMNLP, 2025. Andrew Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In ICML, 2000. Alexander Novikov, Ngân u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. 22 Reinforcement Learning via Self-Distillation Team Olmo, Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, et al. Olmo 3. arXiv preprint arXiv:2512.13961, 2025. Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted arXiv preprint regression: Simple and scalable off-policy reinforcement learning. arXiv:1910.00177, 2019. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015. Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011. Mehrzad Samadi, Aleksander Ficek, Sean Narenthiran, Siddhartha Jain, Wasi Uddin Ahmad, Somshubra Majumdar, Vahid Noroozi, and Boris Ginsburg. Scaling test-time compute to achieve ioi gold medal with open-weight models. arXiv preprint arXiv:2510.14232, 2025. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In ICML, 2015. Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In ICLR, 2016. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. In ICLR, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less. In ICLR, 2026. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In EuroSys, 2025. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In NeurIPS, 2023. 23 Reinforcement Learning via Self-Distillation David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529 (7587), 2016. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context. arXiv preprint arXiv:2209.15189, 2022. Moritz Stephan, Alexander Khazatsky, Eric Mitchell, Annie Chen, Sheryl Hsu, Archit Sharma, and Chelsea Finn. Rlvf: Learning from verbal feedback without overgeneralization. In ICML, 2024. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. In ICML, 2025. Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 1998. Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, et al. End-to-end test-time training for long context. arXiv preprint arXiv:2512.23675, 2025. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023. Belen Martin Urcelay, Andreas Krause, and Giorgia Ramponi. From words to rewards: Leveraging natural language for reinforcement learning. In TMLR, 2026. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Martin Wainwright and Michael Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 2008. Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, and Dongmei Zhang. Text2grad: Reinforcement learning from natural language feedback. In ICLR, 2026. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In ACL, 2024a. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. In NeurIPS, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In NeurIPS, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. 24 Reinforcement Learning via Self-Distillation Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3), 1992. Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc Le. Self-training with noisy student improves imagenet classification. In CVPR, 2020. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Reward shaping with language models for reinforcement learning. In ICLR, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, 2025. URL https: //fengyao.notion.site/off-policy-rl. Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. In ICLR, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. In NeurIPS, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? In NeurIPS, 2025. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature, 639:609616, 2025. Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, et al. Learning to discover at test time. arXiv preprint arXiv:2601.16175, 2026. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In NeurIPS, 2022. Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, et al. Agent learning via early experience. arXiv preprint arXiv:2510.08558, 2025. Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph Gonzalez. The wisdom of hindsight makes language models better instruction followers. In ICML, 2023. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. In NeurIPS, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025a. Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, et al. First return, entropy-eliciting explore. arXiv preprint arXiv:2507.07017, 2025b. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. 25 Reinforcement Learning via Self-Distillation Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. In ICLR, 2026. Brian Ziebart, Andrew Maas, Andrew Bagnell, Anind Dey, et al. Maximum entropy inverse reinforcement learning. In AAAI, 2008. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. In NeurIPS, 2025. 26 Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "2 SDPO: Self-Distillation Policy Optimization",
            "content": "2.1 Comparison to RLVR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Compute time & memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Stability improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3 Learning without Rich Environment Feedback",
            "content": "3.1 Experimental setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Self-distillation learns to reason concisely . . . . . . . . . . . . . . . . . . . . 4 Learning with Rich Environment Feedback 4.1 Self-distillation benefits from stronger models . . . . . . . . . . . . . . . . . 4.2 Self-distillation performs dense credit assignment . . . . . . . . . . . . . . . 4.3 The self-teacher improves during training . . . . . . . . . . . . . . . . . . . . 4.4 On-policy self-distillation avoids catastrophic forgetting . . . . . . . . . . . 4.5 Can GRPO and SDPO be combined? . . . . . . . . . . . . . . . . . . . . . . . 4.6 Which feedback is most informative? . . . . . . . . . . . . . . . . . . . . . . . 5 Solving Hard Questions via Test-Time Self-Distillation 5.1 Experimental setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Related Work 6.1 Reinforcement Learning with LLMs . . . . . . . . . . . . . . . . . . . . . . . 6.2 Learning from Rich Feedback and through Retrospection . . . . . . . . . . . 6.3 Distillation . . . . 6.4 Self-Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Conclusion, Limitations, and Future Work Implementation of SDPO A.1 Regularized teacher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Approximate Logit Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Off-Policy Training: Generalization to Logit-Level Losses . . . . . . . . . . . Theoretical Analysis B.1 Gradient Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 1 5 5 6 6 6 7 8 9 10 10 12 12 13 14 15 16 16 17 17 17 29 30 30 31 31 Reinforcement Learning via Self-Distillation B.2 Trust-region Teacher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 EMA Teacher as an Implicit Trust Region . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Additional Related Work",
            "content": "C.1 SDPO as Maximum Entropy RL . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Results & Ablations D.1 Learning without rich environment feedback . . . . . . . . . . . . . . . . . . D.2 Learning with rich environment feedback . . . . . . . . . . . . . . . . . . . . D.3 Test-time self-distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Experiment Details",
            "content": "E.1 Technical setup . . E.2 Hyperparameters . E.3 User Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Qualitative Examples F.1 Visualization of Advantages . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Environment Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Illustrative Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 32 34 34 36 37 37 42 42 44 46 46 46 49 28 Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "A Implementation of SDPO",
            "content": "The following pseudocode in Figure 14 outlines the implementation of SDPO: 1 def compute_sdpo_loss(batch, teacher_context, loss_mask): 2 \"\"\" Computes probabilities of response under the self-teacher 3 4 6 7 8 9 10 12 13 14 15 16 and the per-logit SDPO loss. \"\"\" # Compute model probabilities for response logprobs_student = compute_log_prob(batch) # (T,V) probs_student = logprobs_student.exp() # (T,V) # Compute self-teacher probabilities for response teacher_batch = reprompt(batch, teacher_context) logprobs_teacher = compute_log_prob(teacher_batch).detach() # (T,V) # Compute SDPO loss: per-token divergence per_token_loss = divergence(logprobs_student, logprobs_teacher) # (T,) return agg_loss(per_token_loss, loss_mask, loss_agg_mode=\"token-mean\") Figure 14: The pseudo-code of SDPO within standard RL training pipeline. Omitted here is the filtering to top-K logprobs for student and teacher (including tail term) as described in Appendix A.2. Further, we omit here any importance sampling weights to correct for off-policy data. reprompt modifies the batch to incorporate teacher context (i.e., rich feedback). divergence implements any per-token divergence such as reverse-KL, forward-KL, or Jensen-Shannon. In the following, we provide further details on: Teacher regularization (Appendix A.1) Approximating logit-distillation with the top-K logits for saving GPU memory (Appendix A.2) Generalizing PPO-style policy gradient algorithms to logit-level advantages (Appendix A.3) To disambiguate the notation of the self-teacher, we use qθ( in the following. Here, reprompt denotes the reprompt template of the self-teacher. x, ) := πθ( reprompt(x, )) A.1 Regularized teacher In contrast to standard distillation, the teacher in SDPO changes throughout training. This bootstrapping enables the teacher to improve, but it may also lead to training instability. To stabilize training, we seek to prevent the teacher from quickly diverging from the initial teacher qθref. We can achieve this by placing an explicit trust-region constraint on (Schulman et al., 2015; Peng et al., 2019), that is: q(yt This trust-region can be implemented in two ways: x, , y<t)(cid:1) qθref (yt x, , y<t) ϵ > 0. KL(cid:0) ϵ, (5) 1. Explicit trust-region: We can define the teacher as the policy closest to qθ while satisfying the trust-region constraint. This teacher can be expressed as x, , y<t) + α log qθ(yt x, , y<t) exp(cid:0)(1 q(yt (0, 1) the inverse Lagrange multiplier for the trust-region constraint. We with α include full derivation in Appendix B.2. We can plug this explicitly constrained teacher directly into the SDPO objective. α) log qθref (yt x, , y<t)(cid:1), (6) 29 Reinforcement Learning via Self-Distillation 2. Exponential moving average (EMA): Alternatively, we can stabilize the teachers α)θ + αθ (0, 1). Under mild smoothness assumptions, this EMA teacher remains parameters directly; parameterizing qθ with α implicitly within trust-region around the initial teacher (cf. Appendix B.3). by θ and updating as θ (1 Note that each implementation has different practical advantage: The EMA teacher requires additional GPU memory for θ yet does not introduce any runtime overhead. In contrast, the trust-region teacher requires an additional log-prob computation with qθref yet does not require additional GPU memory if θref is used for explicit KL regularization. A.2 Approximate Logit Distillation To save GPU memory, we perform distillation only on the top-K tokens predicted by the student: LSDPO(θ) = πθ ( x) Ey πθ ( x) t=1 t=1 KL(πθ(yt x, y<t) stopgrad(qθ(yt x, , y<t))) topK(πθ ) yt topK(πθ ) πθ(yt yt πθ(yt x, y<t) log (cid:17) x, y<t) log (cid:16) 1 + (cid:124) x, y<t) πθ(yt stopgrad(qθ(yt 1 x, , y<t)) topK(πθ ) πθ(yt yt topK(πθ ) qθ(yt (cid:16) yt 1 stopgrad (cid:123)(cid:122) tail x, y<t) (cid:17) (cid:125) x, , y<t) (7) Here, the top-K is with respect to student. Without top-K distillation, we would have to keep two copies of logits in memory: one for teacher and student each. Top-K distillation avoids virtually any memory overhead without impacting performance significantly, since most tokens of the vocabulary are not informative at given time. A.3 Off-Policy Training: Generalization to Logit-Level Losses PPO-style clipping (Schulman et al., 2017) with truncated importance sampling (Yao et al., 2025), clip-higher (Yu et al., 2025), fixed length normalization (Liu et al., 2025b): Ltoken(θ) := i=1 with wi,t := πθ (yi,t (yi,t tage. πθold 1 yi i=1 t=1 yi x,yi,<t) x,yi,<t) , wTIS i,t min (cid:16) wTIS i,t , ρ (cid:17) min (cid:16) wi,t Ai,t, clip(wi,t, 1 εlow, 1 + εhigh)Ai,t (cid:17) , := πθold πrollout θold (yi,t (yi,t x,yi,<t) x,yi,<t) , and Ai,t denotes the per-token advan- (8) We extend this to logit-level loss: 1 yi (cid:16) i=1 min Llogit(θ) := i=1 yi t= ˆyi,t (cid:16) min πθold ( ˆyi,t x, yi,<t), ρπrollout θold ( ˆyi,t x, yi,<t) (cid:17) wi,t( ˆyi,t)Ai,t( ˆyi,t), clip(wi,t( ˆyi,t), 1 εlow, 1 + εhigh)Ai,t( ˆyi,t) where ˆyi,t sums over all possible tokens at position for rollout (or the most likely under πθold, cf. Appendix A.2). The TIS changes since we explicitly weight each logit by its probability under πθold rather than relying on Monte Carlo estimate of the expectation over next-token predictions. Here, Ai,t( ˆyi,t) is per-logit advantage. In our experiments for SDPO, we apply the TIS term on token-level rather than logit-level. 30 (9) (cid:17) , Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "B Theoretical Analysis",
            "content": "This section is organized as follows: Appendix B.1 derives the SDPO gradient from Theorem 2.1. Appendix B.2 derives the trust-region regularized teacher discussed in Appendix A.1. Appendix B.3 shows that the EMA teacher, as discussed in Appendix A.1, implements trust-region constraint. To disambiguate the notation of the self-teacher, we use qθ( in the following. Here, reprompt denotes the reprompt template of the self-teacher. x, ) := πθ( reprompt(x, )) B.1 Gradient Estimator Proof of Proposition 2.1. In the following, we derive the gradient of LSDPO. θ LSDPO(θ) = θ = θ t=1 t=1 KL(πθ(yt x, y<t) x, y<t) log yt πθ(yt (cid:17) stopgrad(qθ(yt (cid:18) πθ(yt stopgrad(qθ(yt x, , y<t))) x, y<t) (cid:19) x, , y<t)) Let At,k := log (cid:16) stopgrad(qθ (yt πθ (yt x,y<t) x, ,y<t)) . Then, = θ t=1 yt πθ(yt x, y<t)At,k = t=1 yt πθ(yt x, y<t)θ At,k + At,kθ πθ(yt x, y<t). We have that θ At,k = score trick, πθ(yt simplifies to θ log πθ(yt x, y<t)θ log πθ(yt x, y<t) is the negative score function. Using the x, y<t) = θ πθ(yt x, y<t). Hence, the first term t=1 yt πθ(yt x, y<t)θ At,k = t= yt θ πθ(yt x, y<t) = t=1 θ yt (cid:124) πθ(yt (cid:123)(cid:122) = x, y<t) = 0. (cid:125) Thus, the gradient of LSDPO is yt t=1 θ LSDPO = At,kθ πθ(yt x, y<t) = t= yt πθ(yt (cid:34) = πθ ( x) (cid:16) x, y<t) At,kθ log πθ(yt x, y<t) (cid:17) t=1 yt θ log πθ(yt x, y<t) At,k (cid:35) = πθ ( x) (cid:34) t= yt θ log πθ(yt x, y<t) log πθ(yt qθ(yt x, y<t) x, , y<t) (cid:35) . Notably, the above implies that the gradient of loss if At,k = stopgrad (cid:16) log πθ (yt qθ (yt x,y<t) x, ,y<t) (cid:17) ."
        },
        {
            "title": "LSDPO is equivalent to the gradient of the",
            "content": "Reinforcement Learning via Self-Distillation B.2 Trust-region Teacher To stabilize training, we seek to prevent the teacher from diverging from the initial teacher qθref. We can achieve this by placing an explicit trust-region constraint on the teacher (Schulman et al., 2015; Peng et al., 2019), that is: q(yt x, , y<t)(cid:1) qθref (yt x, , y<t) ϵ > 0. KL(cid:0) (10) ϵ, In the following, we derive teacher which satisfies the trust-region constraint while staying close to the target qθ. The following optimization problem characterizes such (Peng et al., 2019): arg max s.t. yt KL(cid:0) q(yt q(yt x, , y<t) log x, , y<t) qθ(yt qθref (yt qθref (yt x, , y<t) x, , y<t) x, , y<t)(cid:1) (11) ϵ, where denotes the probability simplex. Intuitively, the solution is the satisfying the trust-region constraint, which is closest to qθ (i.e., has minimal cross-entropy to qθ) while being farthest from qθref (i.e., has maximal cross-entropy to qθref). Proposition B.1. The solution to Equation (11) can be expressed in closed form as x, , y<t) exp(cid:0)(1 q(yt α) log qθref (yt x, , y<t) + α log qθ(yt x, , y<t)(cid:1). (12) Proof. To simplify notation, we omit the conditioning in the following. The Lagrangian (with λ 0 for the KL constraint and ν for normalization) is (q, λ, ν) = q(yt) log yt qθ(yt) qθref (yt) (cid:16) λ yt q(yt) log q(yt) qθref (yt) (cid:17) ϵ (cid:16) + ν yt (cid:17) . 1 q(yt) Stationarity gives, for all yt, 0 = q(yt) = log qθ(yt) qθref (yt) (cid:16) λ log q(yt) qθref (yt) (cid:17) + 1 + ν. Let α := 1/λ. Then, the solution to Equation (11) can be characterized in closed form as (cid:16) (cid:17) q(yt) qθref (yt) exp exp(cid:0)(1 α log qθ (yt) (yt) qθref α) log qθref (yt) + α log qθ(yt)(cid:1). Chen et al. (2025c) perform similar derivation, but use reference πθref, which we observe to underperform compared to the reference qθref. B.3 EMA Teacher as an Implicit Trust Region To stabilize training, an alternative to the explicit trust-region teacher in Appendix B.2 is to parameterize the teacher as an exponential moving average (EMA) of the students parameters θk: α α)θk (0, 1), θk = ( 1 + αθk, (13) with initialization θ0 = θ0 = θref. While Equation (13) constrains the teacher in parameter space (and thus does not, in general, impose an explicit KL trust region in distribution remains space), under mild smoothness condition we can show that the EMA teacher qθ within an approximate trust region relative to the reference teacher qθref. Let qθ(yt x, , y<t) be softmax distribution with logits zθ(x, , y<t) x, , y<t) = softmax(cid:0) zθ(x, , y<t)(cid:1) (14) : . yt qθ(yt Reinforcement Learning via Self-Distillation Assume the logits are L-Lipschitz in parameters: (cid:13) (cid:13)θ1 (cid:13) (cid:13)zθ1 (x, , y<t) zθ2 (x, , y<t)(cid:13) (cid:13)2 θ2 (cid:13) (cid:13)2, (x, , t, θ1, θ2). Proposition B.2 (EMA yields an approximate trust region). Let θ0 = θ0 = θref, update θk by Equation (13). We assume: The logits are L-Lipschitz in parameters (cf. Equation (15)). (15) 1, and The student does not diverge from the initial model, i.e., θi Then, for any (x, c) and any sequence length T, the EMA teacher qθk region constraint θ02 Rref for all k. implicitly satisfies the trust- (cid:16) KL t=1 ( qθk x, , y<t) qθref ( x, , y<t) (cid:17) ϵk, ϵk = L2 4 R2 ref ( (1 α)k)2. (16) Proof. Fix (x, , t) and define = softmax(a) and = softmax(b) with = zθk (x, , y<t) and = zθref (x, , y<t). standard smoothness bound for softmax distributions10 implies 1 4 KL(p (17) 2 2. q) By Equation (15), this gives (cid:16) KL ( qθk x, , y<t) qθref ( x, , y<t) (cid:17) L2 4 θk θref 2 2. (18) Unrolling the EMA recursion with θ0 = θref yields θk i=1 By CauchySchwarz, θref = wi(θi θref), wi = α(1 α)k i, i=1 wi = 1 (1 α)k. 2 (cid:13) (cid:13) (cid:13) i=1 wi(θi (cid:17)(cid:16) i=1 Combining Equations (18) and (19) and summing over = 1, . . . , gives (cid:16) i=1 i=1 θref θi wi θref) α)k) = (1 2 (cid:13) (cid:13) (cid:13) wi (1 (cid:17) 2 2 wi θi θref 2 2. (19) (cid:16) KL t= ( qθk x, , y<t) qθref ( x, , y<t) (cid:17) L2 4 ( (1 α)k) i=1 α( Finally, if (1 θref2 α)k), which yields Equation (16). Rref for all θi k, then i=1 wi θi θref 2 2 R2 ref α)k (cid:13) (cid:13)θi i=1 wi = R2 θref (cid:13) (cid:13) 2 2. ref(1 (cid:1) qθref To summarize, Equation (16) recovers the same form as the explicit trust-region constraint KL(cid:0) ϵ (cf. Appendix B.2), with an effective radius ϵk controlled by the EMA rate αk, α and the cumulative deviation from the reference. For fixed and small α, 1 so the reference divergence scales as ϵk = O(α2k2) when θi stays within bounded neighborhood of θref. α)k (1 10Let = softmax(a) and = softmax(b), and define the log-partition A(z) = log exp(zi). For the categorical exponential family, the KL divergence admits the (primal) Bregman form A(a) KL(p (Wainwright & Jordan, 2008, Eq. (5.10)). Moreover, A(z) = softmax(z) and 2 A(z) = diag(p) , pp (Boyd & Vandenberghe, 2004). For any q) = A(b) A(a), v2 A(z)v = piv2 (cid:17)2 pivi (cid:16) = Vari p(vi) (maxi vi 4 mini vi)2 2 2 , 2 A(z) op 1 2 and hence is (1/2)-smooth in so inequality for β-smooth functions (Bubeck, 2015, Lemma 3.4) yields A(b) β 2 2; setting β = 1 2 2 gives KL(p 1 4 2 2. q) a 2. Applying the standard smoothness + A(a) + A(a), Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "C Additional Related Work",
            "content": "Value networks and Monte Carlo advantage estimation. Several prior approaches aim to improve credit assignment but face the same information bottleneck as GRPO. Classical RL frequently trains value networks which provide token-level advantages, but themselves are learned from scalar rewards (Schulman et al., 2016; 2017). Furthermore, value networks incur significant computational and memory overhead and are therefore typically not used to train LLMs. Other recent work estimates token-level advantages by performing additional generations starting from various positions in the original attempt (Kazemnejad et al., 2025; Zheng et al., 2025b). While this can learn with fewer gradient steps than GRPO it still uses only scalar rewards as signal and requires costly additional generations. Dense credit assignment with reward model. Several recent works have explored assigning dense (per-token) rewards given access to an external reward model, leveraging internal structure of the reward model (Chan et al., 2024; Cao et al., 2025). Partial observability. From the perspective of classical RL, many verifiable domains for LLMs are naturally partially observable: executing proposed solution induces latent environment state (e.g., failing tests or states of an agentic system) that is revealed only through rich feedback. This aligns with the formalism of partially observable Markov decision processes (POMDPs), where agents must act under incomplete observations of state (Kaelbling et al., 1998; Sutton & Barto, 1998). By contrast, RLVR and RLHF pipelines typically discard this observation channel and learn only from terminal scalar rewards or pairwise preferences. Relation to test-time training. Our setting from Section 5 can be seen as special case of test-time training where the model itself is updated at test-time using self-distillation. Updating the model at test-time is known as test-time training (Sun et al., 2020; 2025; Hardt & Sun, 2024; Hübotter et al., 2025a;b; Akyürek et al., 2025; Behrouz et al., 2025; Tandon et al., 2025; Hübotter et al., 2026). Unlike prior work, self-distillation uses the in-context learning ability of the current model to attribute credit after receiving feedback. This can be seen as simulating long-context reasoning with periodic compression of context into the model weights. C.1 SDPO as Maximum Entropy RL The SDPO objective resembles the objective in maximum entropy RL (e.g., Levine, 2018; Haarnoja et al., 2018) with particular choice of reward function. Maximum Entropy RL Consider optimizing arg max θ πθ ( x) (cid:34) r(yt (cid:35) x, y<t) + λH[πθ( x)], λ > 0 (20) x) = x)] is the where πθ(y x, y<t) is an arbitrary reward function, possibly dense entropy of the policy. Here, r(yt (i.e., per-token). Equation (20) is known as maximum entropy RL. It is known that this objective is equivalent to solving variational inference problem which discuss next. x, y<t) and H[πθ( t=1 πθ(yt log πθ(y x)] = x)[ πθ ( which is 1 if the attempt is correct To this end, we define Bernoulli random variable and 0 otherwise. We then define its distribution as p( x, y<t)). = 1 Further assuming w.l.o.g. that the prior over responses is uniform, we can express the posterior conditioned on the event of correctness as x, y) exp( 1 λ r(yt π(y x) := p(y x, = 1) p( = 1 x, y) exp (cid:32) 1 λ r(yt (cid:33) x, y<t) . (21) Then, Equation (20) is equivalent to minimizing the KL divergence with respect to π: arg min θ KL(πθ(yt x, y<t) π(yt x, y<t)). (22) 34 Reinforcement Learning via Self-Distillation SDPO optimizes an implicit reward defined by the teacher Note that Equation (22) is equivalent to the SDPO objective (Equation (1)) with implicit reward r(yt x, y<t) = x, , y<t) and λ = 1. In this sense, SDPO can be seen as maximum entropy RL log q(yt algorithm with dense rewards constructed implicitly through the retrospective model. This also points to connection of SDPO to inverse RL (Ng et al., 2000; Ziebart et al., 2008; Rafailov et al., 2023), where the goal is to recover an unknown reward function. In SDPO, the student learns an implicit reward function defined by the retrospective model. 35 Reinforcement Learning via Self-Distillation Additional Results & Ablations This section is organized as follows: Appendix D.1 contains results and ablations for Section 3. Appendix D.2 contains results and ablations for Section 4. Appendix D.3 contains results and ablations for Section 5. D.1 Learning without rich environment feedback Table 7 reports results when optimal hyperparameters are selected for each model/task combination. Table 8 compares average response lengths of SDPO and GRPO. Chemistry Physics Biology Materials Tool use 1h 5h 1h 5h 1h 5h 1h 5h 1h 5h 35.6 59.2 27.9 58.9 57. 54.2 54.2 59.9 69.6 69.6 70.1 62.9 62.9 70.6 74.5 74.8 80.6 34.3 30.3 53.1 51.8 49.4 53. 74.3 73.3 72.1 77.1 75.8 78.3 61.7 61.7 56.4 68.1 68.1 68.5 18.8 37. 18.1 36.7 39.3 42.7 48.8 59.2 54.3 54.3 76.8 55.3 62.7 60. 63.3 62.7 71.4 54.2 54.2 56.1 63.8 63.8 58.3 73.8 67.9 75.3 78.1 74.4 79.2 56.4 56.0 57. 65.0 61.3 62.5 Qwen3-8B + GRPO + GRPO (on-policy) + SDPO (on-policy) Olmo3-7B-Instruct + GRPO + GRPO (on-policy) + SDPO (on-policy) Table 7: Comparison of SDPO and GRPO on reasoning-related benchmarks. We report the highest achieved avg@16 within 1 hour and 5 hours of wall-clock training time, respectively. Both SDPO and on-policy GRPO perform one gradient step per generation batch, while GRPO performs 4 off-policy mini batch steps. We select optimal hyperparameters for SDPO and baselines based on 5h accuracy. We perform this selection independently for each model and dataset. Each run is performed on node with 4 NVIDIA GH200 GPUs. Together with initialization and validation, each run takes approximately 6 hours. As opposed to Table 3 which selects globally optimal hyperparameters per method, this table selects optimal hyperparameters individually for each model/task combination based on 5h accuracy. The hyperparameter grid is described in Section E.2.1. Model GRPO SDPO Reduction of SDPO Qwen3-8B Olmo3-7B-Instruct 820.8 1095.4 255.8 343.9 3.2 3.2 Table 8: Average response lengths of SDPO and GRPO (averaged across tasks from Section 3). Both algorithms are evaluated in the on-policy setting. 36 Reinforcement Learning via Self-Distillation Figure 16: Accuracy (pass@1) for varying train batch sizes (4, 8, 16, 32) and number of rollouts (4, 8) for training SDPO and GRPO with Qwen3-8B (Yang et al., 2025) on LCBv6, stderr across 3 seeds. Different shades of the same color correspond to different runs. D.2 Learning with rich environment feedback D.2.1 Additional Results Figure 15 shows the average accuracy of SDPO and GRPO stratified by question difficulty. LCB differentiates between easy, medium, and hard questions. As displayed, SDPO significantly improves over GRPO in solving medium and hard questions, highlighting the importance of rich feedback for challenging tasks. Note that this categorization of questions is different from the one in Section 5. In Figure 16, we compare different train batch sizes and number of rollouts for training GRPO and SDPO on LCBv6. Complementing the results shown in Figure 8, we show additional results using Qwen2.5-Instruct (Qwen et al., 2024) in Figure 17. D.2.2 Training Stability Figure 15: Average accuracy during training until step 80, stratified by difficulty. Error bars show standard deviation across 3 seeds. Figure 18 shows diverse metrics logged during training, including the loss, entropy, average gradient norm, and average response length. D.2.3 Baselines Table 9 compares the performance on LCBv6 of various baselines, including two variants of GRPO, GSPO, and CISPO to SDPO. D.3 Test-time self-distillation Complementing the results shown in Section 5, we show the discovery@k curves for all hard question in Figure 20, and report the mean number of generations until the first discovery in Table 10. Further, Table 11 shows the per-question accuracy of the self-teacher at the initial training step of SDPO. In Figure 19, we ablate the choice of batch size for SDPO and the in-context reprompting strategy for multi-turn sampling. 37 Reinforcement Learning via Self-Distillation Figure 17: Average validation accuracy by model size, std across 3 seeds. With Qwen2.5Instruct (Qwen et al., 2024) and Qwen3 (Yang et al., 2025) on LCBv6. Until step 65 for Qwen2.5 and until step 80 for Qwen3. Figure 18: Loss, entropy, avg. gradient norm and avg. response length during training of SDPO on LCBv6 (Section 4 . In the selection of hard questions, we have discarded one malformed question (Q9) where the coding environment did not correctly validate the solution due to rounding inaccuracies, which led to failures even with correct logic. Reinforcement Learning via Self-Distillation GRPO + only high-entropy tokens (Wang et al., 2025) GSPO (Zheng et al., 2025a) CISPO (Chen et al., 2025a) SDPO Accuracy Avg accuracy 41.2 37.8 40.1 41.2 48.8 0.8 2.2 2.3 1.8 0.6 38.2 35.9 37.7 37.8 43.8 0.0 0.1 0.1 0.1 0.0 Table 9: Performance on LCBv6 at/until training step 80 with std over 3 seeds. We compare to GSPO (Zheng et al., 2025a) and CISPO (Chen et al., 2025a). With Qwen3-8B. Question SDPO Best-of-k Multi-turn Best-of-k 1 3* 10* 43 46* 59 69 74* 86 91* 92* 95* 100 103* 111 120 125* 127 129 Hard tasks Very hard tasks 104 1987 938 111 1852 172 280 1948 85 1360 1575 1948 277 2246 85 24 1795 28 168 894 1739 98 2750 2750 109 1466 123 134 1466 421 2750 2750 1466 294 2750 95 327 1466 368 173 1145 2180 59 2750 1706 111 1315 76 134 2405 335 2384 2203 1794 1596 2210 39 70 2320 61 1141 2121 Speedup SDPO 0.9 1.4 2.9 1.0 0.8 0.7 0.5 0.8 5.0 2.0 1.8 0.8 1.1 1.2 1.1 13.6 0.8 13.1 1.0 1.3 1.2 Table 10: Mean number of generations until first success per question for SDPO, best-of-k sampling, and the multi-turn sampling. For the mean calculation, values are truncated at the maximum budget of 2750 generations. Very hard tasks (pass@64 < 0.03) are marked with an asterisk (*). Averaged over all questions, SDPO achieves successes faster than the baselines, reaching speedup of up to 13.6 on individual questions compared to best-of-k sampling. 39 Reinforcement Learning via Self-Distillation Question Initial Teacher Accuracy (%) 1 3 10 43 46 59 69 74 86 91 92 95 100 103 111 120 125 127 129 0.00 0.00 0.00 6.25 0.00 0.00 3.12 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.23 0.06 Table 11: Average accuracy of the retrospective teacher at the first step for each question. These scores represent the percentage of successful solutions generated when the base model is reprompted with feedback in single-turn interaction. For the majority of these hard and very hard tasks, the teacher accuracy is near or exactly 0%. Despite this, the self-distilled token-level advantages are sufficiently rich for SDPO to iteratively refine its policy and solve these questions over successive updates. Figure 19: Ablations self-distillation at test-time on hard tasks. Left: Impact of SDPO batch size on pass@k curves. While smaller batch sizes (8 and 16) can lead to slightly earlier discoveries at very low generation budgets (k < 26), larger batch sizes (16, 32) result in more stable updates that significantly improve the discovery rate as the budget scales. Right: Comparison of multi-turn reprompting templates on subset of hard questions. The Only feedback template concatenates the feedback from previous attempts using first-in, first-out sliding window. The Attempts + Feedback template concatenates the full turn, also using sliding window. Including only the feedback substantially outperforms concatenating full conversations. Reinforcement Learning via Self-Distillation Figure 20: Individual task results self-distillation at test-time. Discovery@k for each of the 19 questions evaluated in Section 5. In most cases, SDPO finds successful solution significantly earlier than both the base model and the multi-turn baseline. Notably, for one question (Q3) where the base model and the multi-turn baseline maintain discovery@k of zero for the entire budget up to 2750 , SDPO discovers solution after 321 attempts. Curves represent the mean and 90% confidence intervals across 5 random seeds per question. 41 Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "E Experiment Details",
            "content": "E.1 Technical setup All experiments were conducted on single node equipped with four NVIDIA GH200 GPUs, for total of 378GB VRAM. Our environment is built on top of the NVIDIA PyTorch container nvcr.io/nvidia/pytorch:25.02-py3, with CUDA 12.8 and PyTorch v2.7.0. Our implementation is based on the verl library (Sheng et al., 2025). We use PyTorch Fully Sharded Data Parallel (FSDP2) for distributed training. For rollout generation, we employ vLLM (Kwon et al., 2023), which enables efficient batched inference on the multi-GPU node. E.2 Hyperparameters We summarize hyperparameters used for SDPO in Table 12 and those used for GRPO in Table 13. Parameters General Model Thinking Data Max. prompt length Max. response length Batching Question batch size Mini batch size Number of rollouts Rollout Inference engine Temperature Validation Number of rollouts Temperature Top-p Without Feedback Section With Feedback Section 4 TTT Section 5 Qwen/Qwen3-8B allenai/Olmo3-7B-Instruct False 2048 8192 32 32 8 vllm 1. 16 0.6 0.95 Qwen/Qwen3-8B Qwen/Qwen3-8B False 2048 8192 32 1 8 vllm 1. 4 0.6 0.95 False 2048 8192 1 1 16 vllm 1.0 - - - SDPO loss Top-K distillation Distillation divergence Clip advantages Teacher-EMA update rate Rollout importance sampling clip 2 100 JensenShannon 0.05 20 Reverse-KL 0.01 2 20 Reverse-KL 5.0 0.01 2 Training Optimizer Learning rate Warmup steps Weight decay Gradient Clip Norm 5 (constant) AdamW 1 10 10 0.01 1.0 AdamW 10 6 (constant) AdamW 1 10 0 0.01 1.0 6 (constant) 1 0 0.01 1.0 Table 12: Hyperparameters used for SDPO for each experimental setup. 42 Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "General\nModel",
            "content": "Thinking Data Max. prompt length Max. response length"
        },
        {
            "title": "Rollout\nInference engine\nTemperature",
            "content": "Validation Temperature Top-p Number of rollouts Experiment 1 Section 3 Qwen/Qwen3-8B allenai/Olmo3-7B-Instruct False 2048 8192 32 8 (default) / 32 (on-policy) 8 vllm 1. 0.6 0.95 16 Loss ϵ-high Rollout importance sampling clip 2 KL coefficient (λ) 0.0 0.28 Training Optimizer Learning rate Warmup steps Weight decay Gradient Clip Norm 6 (default) / AdamW 1 10 10 0.01 1.0 10 5 (on-policy) Table 13: Hyperparameters used for GRPO. E.2.1 Details on Hyperparameter Selection (Section 3) 6 { } 8, 32 5, 10 and minibatch sizes For GRPO in the experiments in Section 3, we perform grid search over learning rates . For on-policy GRPO, we search over the same 10 { learning rates while fixing the minibatch size to 32. For SDPO, we grid-search over KL , and minibatch sizes variants (forward KL, JensenShannon), learning rates . For each method (GRPO, on-policy GRPO, and SDPO), we select single hyperpa- { rameter configuration that achieves the highest validation accuracy within the first 5 hours of training, evaluated across all datasets and models used in Section 3. We further report results obtained by selecting the optimal hyperparameter configuration separately for each model and dataset in Table 3. 5, 10 8, 32 10 } } { } 6 43 Reinforcement Learning via Self-Distillation E.3 User Templates For multiple-choice questions and tool use, the model must be prompted in task-specific manner. We therefore provide the prompt templates used for these settings below. Given question and four options, please select the right answer. Respond in the following format: <reasoning> ... </reasoning> <answer> ... </answer> For the answer, only output the letter corresponding to the correct option (A, B, C, or D), and nothing else. Do not restate the answer text. For example, if the answer is \"A\", just output: <answer> </answer> Listing 1: System prompt: Multiple Choice Questions {question} Please reason step by step. Listing 2: User prompt: Multiple Choice Questions You are helpful function-calling AI assistant. You are provided with function signatures within <functions></functions> XML tags. You may call one or more functions to assist with the user query. Output any function calls within < function_calls></function_calls> XML tags. Do not make assumptions about what values to plug into functions. Listing 3: System prompt: Tool use Your task is to answer the user's question using available tools. You have access to the following tools: Name: Axolotl Description: Collection of axolotl pictures and facts Documentation: getRandomAxolotlImage: Retrieve random axolotl image with information on the image source. Parameters: {} Output: Successful response. - Format: application/json - Structure: Object{url, source, description} searchAxolotlImages: Search for axolotl images based on specific criteria such as color, gender, and size. Parameters: {\"color\": \"string. One of: [wild, leucistic, albino]. The color of the axolotl (e.g., 'wild', 'leucistic', 'albino', etc.).\", \"gender\": \"string. One of: [male, female]. The gender of the axolotl ('male', 'female').\", \"size\": \" string. One of: [small, medium, large]. The size of the axolotl ('small', ' medium', 'large').\", \"page\": \"integer. The page number for pagination purposes .\"} Output: Successful response. - Format: application/json 44 Reinforcement Learning via Self-Distillation - Structure: Object{results: Array[Object{url, source, description}], pagination: Object{current_page, total_pages, total_results}} getAxolotlFacts: Retrieve interesting facts about axolotls such as their habits, habitats, and physical characteristics. Parameters: {\"category\": \"string. One of: [habits, habitat, physical characteristics]. The category of facts to retrieve (e.g., 'habits', 'habitat', 'physical characteristics').\", \"limit\": \"integer. The maximum number of facts to return.\"} Output: Successful response. - Format: application/json - Structure: Array[Object{fact, source}] Use the following format: Thought: you should always think about what to do Action: the action to take, should be one of the tool names. Action Input: the input to the action, must be in JSON format. All of the action input must be realistic and from the user. Begin! Question: Hey, can you show me random picture of an axolotl? Listing 4: Example user prompt: Tool use 45 Reinforcement Learning via Self-Distillation"
        },
        {
            "title": "F Qualitative Examples",
            "content": "F.1 Visualization of Advantages Figure 21 compares the advantages of SDPO and GRPO in representative example. Figure 21: Visualization of advantages in SDPO and GRPO with Olmo3-7B-Instruct in batch from the Chemistry task of Section 3. Each row corresponds to the beginning of response. The color indicates the advantage value at that token position, with positive advantages shown in blue and negative advantages shown in red. F.2 Examples Below, we show an example from training SDPO on LCBv6 using Qwen3-8B. [ Prompt ] You are coding expert . You will be given coding problem , and you need to write correct Python program that matches the specification and passes all tests . The time limit is 1 second . You may start by outlining your thought process . In the end , please provide the complete code in code block enclosed with ``` ```. You are given binary string of length , where : '1' represents an active section . '0' represents an inactive section . You can perform at most one trade to maximize the number of active sections in s. In trade , you : 46 Reinforcement Learning via Self-Distillation Convert contiguous block of '1's that is surrounded by '0's to all '0's. Afterward , convert contiguous block of '0's that is surrounded by '1's to all '1's. Return the maximum number of active sections in after making the optimal trade . Note : Treat as if it is augmented with '1' at both ends , forming = '1' + + '1 '. The augmented '1's do not contribute to the final count . Example 1: Input : = \"01\" Output : 1 Explanation : Because there is no block of '1's surrounded by '0's , no valid trade is possible . The maximum number of active sections is 1. Example 2: Input : = \"0100\" Output : 4 Explanation : String \"0100\" -> Augmented to \"101001\". Choose \"0100\" , convert \"101001\" -> \"100001\" -> \"111111\". The final string without augmentation is \"1111\". The maximum number of active sections is 4. Example 3: Input : = \"1000100\" Output : 7 Explanation : String \"1000100\" -> Augmented to \"110001001\". Choose \"000100\" , convert \"110001001\" -> \"110000001\" -> \"111111111\". The final string without augmentation is \"1111111\". The maximum number of active sections is 7. Example 4: Input : = \"01010\" Output : 4 Explanation : String \"01010\" -> Augmented to \"1010101\". Choose \"010\" , convert \"1010101\" -> \"1000101\" -> \"1111101\". The final string without augmentation is \"11110\". The maximum number of active sections is 4. Constraints : 1 <= == s. length <= 10^5 s[i] is either '0' or '1' Your solution should have the following signature : ``` python def maxActiveSectionsAfterTrade (s: str ) -> int : ``` [ Response ] < think > </ think > To solve this problem , we need to understand the structure of the string and the trade operation . Here 's the plan : ### Key Insights : 1. We are given binary string `s `, and we are allowed ** at most one trade **. 2. ** trade ** consists of two steps : - Convert contiguous block of `'1'` that is ** surrounded by `'0'` into `'0'` (i.e., flip it ). - Then convert contiguous block of `'0'` that is ** surrounded by `'1'` into `'1'` (i .e., flip it ). 3. The actual count of active sections is the number of `'1'` characters ** after ** the trade . The augmented `'1'` at the beginning and end do not count in the final result . Reinforcement Learning via Self-Distillation ### Strategy : - We simulate the trade by considering each possible way to flip block of `'1'` ( bounded by `'0'`) and then block of `'0'` ( bounded by `'1'`). - For each possible valid block , simulate the trade and compute the resulting number of `'1'` characters . - We only need to consider the original string `s `, not the augmented one ( since we can simulate the trade directly on `s ` by checking boundaries ). We ' ll use sliding window technique to find contiguous blocks of `'1'` and `'0'` and simulate the trade . --- ### Implementation Steps : 1. Simulate the trade by checking for block of `'1'` surrounded by `'0'` or at the edges ( we simulate the augmented `1` at both sides ). 2. For each such block , simulate converting it to `'0'`, then convert `'0'` block ( again , surrounded by `'1'`) to `'1'`. 3. Count the number of `'1'` in the final string and track the maximum . --- ### Python Code : ``` python def maxActiveSectionsAfterTrade (s: str ) -> int : if not s: return 0 max_ones = 0 = len (s) # We are simulating the augmented '1's on both ends , so we treat the boundaries as '1's # So '0' at index 0 or -1 is considered surrounded by '1's # Step 1: Find all candidate positions for the first part of the trade # ( convert block of 1s surrounded by 0s into 0 s) for in range (n): if s[i] == '1 ': start = while < and s[i] == '1 ': += 1 end = - 1 # Check if this block is surrounded by '0's left_boundary = ( start == 0 or s[ start - 1] == '0 ') right_boundary = ( end == - 1 or s[ end + 1] == '0 ') if left_boundary and right_boundary : # This is candidate block of '1's to flip # Now , look for block of '0's surrounded by '1 's in the remaining string # Replace the '1' block with '0 's # Now find '0' block surrounded by '1's # We can simulate the remaining string temp = list (s) for in range ( start , end + 1) : temp [j] = '0' # Now find contiguous block of '0's surrounded by '1 's max_trade_ones = 0 for in range (n) : if temp [k] == '0 ': start0 = while < and temp [k] == '0 ': += 1 end0 = - 1 # Check if this block is surrounded by '1's left_0 = ( start0 == 0 or temp [ start0 - 1] == '1 ') right_0 = ( end0 == - 1 or temp [ end0 + 1] == '1 ') if left_0 and right_0 : # Flip this block of '0's to '1's for in range ( start0 , end0 + 1) : temp [j] = '1' # Count the number of '1's count = sum ( temp ) max_ones = max ( max_ones , count ) break # Only take the first valid block for this trade # After trying this '1' block , restore the original string # No need , we just used copy return max_ones ``` 48 Reinforcement Learning via Self-Distillation --- ### Optimization Note : The above approach is ** not optimal ** for very large strings . more efficient method would involve precomputing the positions of blocks of `'1'` and `'0'`, and checking valid trades between these blocks in linear time . But with the constraints given ( up to 10^5) , we need to ensure linear or near - linear performance . The current code is correct but ** not efficient ** for large inputs . Would you like me to provide more optimized version that runs in ** O(n) ** time and uses only one pass through the string ? [ Feedback ] ' Runtime Error nTypeError : unsupported operand type (s) for +: ' int ' and ' str ' nLine 48 in maxActiveSectionsAfterTrade ( Solution . py )n nLast Executed Input \"11000\" ' F.3 Environment Feedback We show three examples of feedback in our coding environment, inspired by LeetCode. Test Case 3: Wrong Answer Input [[3,6,1],[4,21,4]] Output 6 Expected 22.87500 Test Case 6: Wrong Answer Input [[12,25,3],[3,14,2]] Output 14 Expected 25.83333 Listing 5: Example of feedback Wrong Answer from our code environment in case of wrong answer, inspired by LeetCode Runtime Error MemoryError: Line 91 in <module> (Solution.py) Line 25 in solve (Solution.py) Last Executed Input 10 633 9312 1314 8548 8857 1062 6410 3289 8594 1263 8549 733 3858 5973 ... (3 more lines) 49 Reinforcement Learning via Self-Distillation Listing 6: Example of feedback Memory Error from our code environment in case of wrong answer, inspired by LeetCode Runtime Error IndexError: list index out of range Line 28 in sortMatrix (Solution.py) Last Executed Input [[-1,-1,-1,-1,-1,-1,-1,-1,... Listing 7: Example of feedback Index Error from our code environment in case of wrong answer, inspired by LeetCode F.4 Illustrative Example Figure 22 shows an illustrative example of the dense credit assignment in SDPO. Figure 22: Dense credit assignment through self-teaching in SDPO. The answer is generated by then model (Qwen3-8B) before seeing the feedback. Then, we re-evaluate the log-probs of the original attempt with the self-teacher after seeing the feedback. We show the per-token log(P(self-teacher)/P(student)), with red indicating negative values (self-teacher disagrees), blue indicating positive values (teacher reinforces), and white indicating values around zero. Using binary rewards, GRPO would assign the same, negative advantage to all tokens in the sequence. In contrast, SDPO turns the feedback into dense credit assignment across the sequence. The first row shows the tokens of the generated response. The 3 other rows show the top-k logits of the self-teacher that are used during self-distillation, suggesting alternative tokens. Notably, in this example, the self-teacher identifies the error through retrospection without an explicit solution. The credit assignment on the generated sequence, and the alternative top-k logits correctly show that replacing set with dict maintains the order of elements. Further, in the seventh shown position, the model also identifies an alternative solution path which starts with the seen token, instead of directly returning the output. The activation is sparse, identifying where mistakes happen and adjusting to the students response distribution for specifically these few tokens."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "MIT",
        "Max Planck Institute for Intelligent Systems",
        "Stanford"
    ]
}