{
    "paper_title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
    "authors": [
        "Tong Wu",
        "Yang Liu",
        "Jun Bai",
        "Zixia Jia",
        "Shuyi Zhang",
        "Ziyong Lin",
        "Yanting Wang",
        "Song-Chun Zhu",
        "Zilong Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning."
        },
        {
            "title": "Start",
            "content": "2025-12-9 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Tong Wu Yang Liu* Jun Bai* Zixia Jia Shuyi Zhang Ziyong Lin Yanting Wang Song-Chun Zhu and Zilong Zheng(cid:0) NLCo Lab, Beijing Institute for General Artificial Intelligence (BIGAI) Code: Model: Website: https://github.com/bigai-nlco/Native-Parallel-Reasoner https://huggingface.co/bigai-NPR https://bigai-nlco.github.io/Native-Parallel-Reasoner"
        },
        {
            "title": "Abstract",
            "content": "We introduce Native Parallel Reasoner (NPR), teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) self-distilled progressive training paradigm that transitions from cold-start format discovery to strict topological constraints without external supervision; 2) novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing new standard for self-evolving, efficient, and scalable agentic reasoning. 5 2 0 2 8 ] . [ 1 1 6 4 7 0 . 2 1 5 2 : r Figure 1 Native Parallel Reasoner (NPR) transforms base model from sequential chain-of-thought (CoT) to native parallel reasoning via self-distilled progressive training paradigm. Compared with previous SoTA, NPR achieves high reasoning accuracy, genuine parallelism and token acceleration. The illustrated results are evaluated on the AIME25 benchmark. Core contributors. Project leaders. (cid:0) Correspondence to: zlzheng@bigai.ai Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning 1. Introduction The advent of super-scale Large Language Models (LLMs), exemplified by Gemini 3 (Pichai et al., 2025), GPT-5 (OpenAI, 2025), and DeepSeek-V3.2 (DeepSeek, Inc., 2025), has shifted the frontier of AI from semantic fluency to deep, multi-step agentic reasoning. Despite the excitement of deeper test-time scaling that enables models to solve complex problems (Muennighoff et al., 2025), the wider reasoning capacity to explore diverse trajectories in parallel emerges as the dominant requirement toward agentic AI (Shen et al., 2025; Comanici et al., 2025). The MapReduce paradigm has long underpinned distributed computing by separating task decomposition from trajectory aggregation (Dean and Ghemawat, 2008; Yang et al., 2025a; Wang et al., 2025a), yet its application to agentic language modeling remains critical missing link in the evolution of open-source LLMs. Ideally, the model should internalize the collaborative breadth of multi-agent systems directly into an efficient, natively parallel architecture. Despite this clear imperative, existing implementations remain fragmented and present three critical deficiencies. First, Algorithmic and Architectural Incompatibility. Prevalent inference engines (Zheng et al., 2024; Kwon et al., 2023) and Reinforcement Learning (RL) algorithms (Shao et al., 2024) are ill-equipped for native branching: The former fails to control parallel branching and aggregation; the latter often clips the gradients of the special tokens that trigger those operations, preventing the model from learning strict structure. Second, Inefficient Hand-Crafted Parallelism. Although the intuitive advantage of parallel sampling lies in efficiency, early attempts on internalizing the parallelism (Zheng et al., 2025a; Wen et al., 2025; Zhao et al., 2025) resort to hand-crafted divide-and-conquer rules via independent sampling. These methods fail to leverage shared Key-Value (KV) states, necessitating redundant recalculations for every branch and resulting in prohibitive linear latency costs (O(N)) that render the model impractical for real-time deployment. Third, Reliance on Supervised Distillation. Framework such as Multiverse (Yang et al., 2025a) successfully operationalizes native parallelism but depends heavily on supervised data distilled from stronger teacher models. While effective for compressing capabilities into smaller models, this dependence restricts the student to mimicking the teachers sequential reasoning topology force-fitted into parallel format, imposing an \"Intelligence Ceiling\" that prevents it from novel, model-intrinsic parallel strategies necessary for super-intelligence. To address these challenges, we take the first step to explore the potential of LLMs to self-evolve parallel reasoning capabilities without reliance on external supervision and introduce Native Parallel Reasoner (NPR). Specifically, NPR employs three-stage progressive training paradigm designed to transition the model from sequential emulation to genuine parallel cognition. In Stage 1 (2.2), we warm up seed instruction-tuned model (e.g., Qwen3-4B-Instruct) to spontaneously discover valid parallel structures by applying standard DAPO (Yu et al., 2025) with format-aware reward function, yielding structured trajectory generator, NPR-ZERO, which produces parallel-formatted outputs but still relies on sequential visibility, i.e., simulated parallelism. In Stage 2 (2.3), we bridge the gap to native parallel architecture by performing rejection sampling on NPR-ZERO and conducting parallel warmup, which instills strict topological constraints using parallel positional encoding and attention mask as in Yang et al. (2025a), converting the sequential behavior of models into real parallel execution. Finally, Stage 3 (2.4) generalizes these capabilities beyond the initial distribution via Native-Parallel RL. This stage implements collision-free parallel rollouts using novel Parallel-Aware Policy Optimization (PAPO) algorithm, which optimizes branching policies directly within the parallel execution graph, allowing the model to learn adaptive decomposition strategies through trial and error rather than imitation. To support this algorithmic breakthrough, we re-engineered the rollout infrastructure with robust NPR Engine (2.5). Specifically, we observed several stability issues unique to parallel RL, e.g., GPU memory leaks caused by radix-cache mechanism; excessively long generation due to the incorrect parallel token calculations; potential runtime failure because of adaptive parallel inference logic, etc. NPR Engine redesigns memory management and flow control, providing the first stable rollout backend capable of supporting large-scale parallel RL. We experiment NPR on Qwen3-4B-Instruct-2507 and Qwen3-4B (Non-Thinking) across broad suite of eight reasoning benchmarks, demonstrating consistent performance improvements on both original model and previously RL-tuned version up to 24.5% and inference speedup up to 4.6. The results 2 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning reveal three consistent superiorities of NPR (3.3). Self-Distilled Data Efficacy: Our self-distilled datasets outperform previous teacher-generated trajectories in Yang et al. (2025a) by an average of 10.1 points, validating the hypothesis to learn from native distributions. Parallelism Effectiveness and Efficiency: Both NPR-BETA and NPR-RL yields significant performance gains over direct sequential RL baselines (e.g., DAPO), confirming that adaptive parallel policies provide superior search mechanism compared to single-path rollouts. 100% Genuine Parallelism: We observed 30%+ AR fallback on test cases when running previous baselines (4.2), where models choose run vanilla AR generation to reach better performance. In contrast, NPR performs 100% genuinely parallel reasoning, with no instances of hidden AR fallbacks or pseudo-parallel behavior in all evaluated test cases. Finally, we conduct comprehensive analyses of NPR spanning inference acceleration (4.1), test-time scalability (4.3), pseudo-parallelism (4.2), evolution dynamics (4.4), and qualitative case studies (4.5). NPR achieves task-dependent speedups, reaching up to 4.6 over autoregressive (AR) decoding. Using best@8 as the metric for test-time scalability, we find that both parallel SFT and parallel RL consistently boost the performance of best-case exploitation across most benchmarks. Our qualitative studies highlight how NPR adapts its degree and style of parallelism across problem types, illustrating how structured parallel exploration leads to both faster inference and higher solution reliability. 1.1. Contributions unified and teacher-free paradigm for constructing Native Parallel Reasoner. We introduce teacher-free framework that unifies self-distilled data construction, parallel SFT, and parallel RL to build truely NPR. Our method intrinsically learns adaptive decomposition, diverse parallel plans, and KV-reusing execution policiesestablishing reusable cognitive primitive. We introduce Parallel-Aware Policy Optimization (PAPO), an RL algorithm specifically designed to optimize parallel decoding policies. Our analysis confirms that PAPO induces genuine parallel behaviors, where the model actively leverages independent attention branches for exploration and self-correction. Through case studies, we observe that NPRs parallelism manifests not only through task decomposition but also through diverse exploratory branches and reflective crosschecks across different reasoning angles. robust and scalable parallel inference engine enabling reliable NPR training. We re-engineered an NPR Engine, which redesigns core components of parallel execution, including radix-cache memory management, output-length control, and parallel-state branching based on Multiverse-Engine (Yang et al., 2025a). This eliminates the instability inherent in standard engines, providing practical backend for native parallel RL training. Robust accuracy and efficiency gains across diverse reasoning benchmarks. Using self-distilled parallel data, NPR reached even better performance gains. NPR-4B finetuned on Qwen3-4B-Instruct achieves 50.4% on AIME 25, 63.3% on AIME 24, surpassing the powerful baselines including Multiverse-4B(+14.9%) and Multiverse-32B(+12.5%) trained on distilled data s1.1. When training on Qwen3-4B (Non-thinking)1, NPR reaches average performance gain of over 24.5%. Beyond accuracy, we demonstrate that NPR fundamentally transforms the inference landscape. We conduct comprehensive analysis spanning inference acceleration (4.1), parallelism analysis (4.2) and test-time scalability (4.3). NPR offers up to 4.6 wall-clock speedup over AR decoding and achieves genuinely parallel reasoning, while qualitative case studies reveal that NPR evolves distinct cognitive strategies, utilizing parallelism for broad exploration in creative tasks and rigorous cross-verification in logical tasks. 1We technically observe Qwen3-4B (Thinking) to be challenging finetuning its thinking tokens into desired formats. (4.6) 3 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Figure 2 An overview of the NPR training framework. 2. Native Parallel Reasoner In this study, we propose Native Parallel Reasoning (NPR), framework that enables language models to generate and evaluate multiple reasoning branches in parallel. As shown in Figure 2, NPR is developed through three-stage curriculum that progressively induces, grounds, and amplifies this capability. First, NPR-ZERO uses reinforcement learning to induce structured parallel format without relying on external annotations. Next, NPR-BETA stabilizes these emerging parallel primitives through supervised fine-tuning on self-distilled trajectories. Finally, NPR applies parallel-aware reinforcement learning procedure that directly optimizes the models ability to perform native parallel reasoning. Together, these stages establish cohesive path from initial format induction to fully optimized parallel inference. 2.1. Preliminaries Parallel Reasoning. Parallel Reasoning (PR) relaxes the strict left-to-right dependency of AR reasoning, allowing the model to generate multiple reasoning steps independently whenever possible. Formally, the joint probability of reasoning sample ˆy consisting of reasoning steps {st}T t=1 can be factorized according to dependency graph defined over the steps: P( ˆy q; θ) = t=1 P(st Pa(st), q; θ), where Pa(st) denotes the set of parent steps that st directly depends on in G, and θ are the model parameters. This formulation enables the model to process reasoning steps that do not have mutual dependencies concurrently. Policy Optimization for Language Models To optimize the policy model within our reinforcement learning framework, we adopt objective functions based on DAPO (Yu et al., 2025). We first introduce the original DAPO update procedure. For each question-answer pair (q, y) D, the policy model πθold 4 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning first generates group of responses { ˆyi}G i=1. The objective function (θ) is then formulated as: (θ) = (q,y)D, { ˆyi}G i=1πθold (cid:34) ˆyi t=1 i=1 1 i=1 ˆyi 0 < (cid:12) s.t. (cid:12){ ˆyi is_equivalent(y, ˆyi)}(cid:12) (cid:12) < (q) min (cid:32) ri,t(θ) ˆAi,t, clip(cid:0)ri,t(θ), 1 ϵlow, 1 + ϵhigh (cid:1) ˆAi,t (cid:33)(cid:35) . (1) where ri,t(θ) denotes the probability ratio between the current and the old policy for the t-th token in response ˆyi, and ˆAi,t represents the standardized advantage of that token computed from the rewards {R1, R2, . . . , RG} of all generated responses in the group: ri,t(θ) = πθ( ˆyi,t q, ˆyi,<t) πθold( ˆyi,t q, ˆyi,<t) , ˆAi,t := Ri mean({R1, R2, , RG}) std({R1, R2, , RG}) . (2) This formulation ensures stable policy updates by clipping extreme probability ratios while encouraging exploration through group-wise normalization of advantages. It effectively balances between exploiting high-reward responses and maintaining diversity among generated outputs. Table 1 Structured schema of NPR. The Output Format Example of Parallel Reasoning [One-sentence independent strategy]</plan> [One-sentence independent strategy]</plan> [Self-contained detailed analysis for plan 1]</step> [Self-contained detailed analysis for plan 2]</step> <guideline> <plan>1: <plan>2: ... </guideline> <step>1: <step>2: ... <takeaway>[Compare steps, synthesize findings, determine next action]</takeaway> <guideline> <plan>1: ... </guideline> <step>1: ... <takeaway>[Final synthesis and conclusion]</takeaway> [Final user-facing summary. [Self-contained detailed analysis]</step> [One-sentence strategy]</plan> Include boxed{answer} for definitive short answers.] 2.2. Stage 1: Format-follow Reinforcement Learning To support adaptive decomposition and parallel reasoning during generation, we adopt simplified MapProcessReduce schema inspired by Multiverse (Yang et al., 2025a) but with leaner structure. Each parallel block begins with <guideline> ... </guideline>, which contains set of <plan> ... </plan> entries that define the Map stage. The Process stage follows: each <step> ... </step> block executes one mapped subtask independently and in parallel. After all <step> blocks complete, Reduce stage consolidates their outputs into final summary wrapped by <takeaway> ... </takeaway>. This explicit tag-based format makes the decomposition, independent processing, and final aggregation easy to parse and verify in downstream training and evaluation. While this schema provides clear, learnable format for parallel reasoning, obtaining large-scale, high-quality training data for it remains challenging. Prior work such as Multiverse (Yang et al., 2025a) constructs large, multi-step synthetic pipelines and aggregates outputs from several state-of-the-art teacher models (e.g., Deepseek R1 (Guo et al., 2025) and Gemini 2.5 Pro (Google, 2025)) to overcome data scarcity. While effective, these multi-teacher pipelines add operational complexity, require access to strong external teachers, and incur substantial maintenance costs. 5 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning We adopt simpler, self-improving approach. Starting from single pretrained LLM, we apply DAPO (Yu et al., 2025) to induce the target native parallel-reasoning generation format without paired supervision or external teachers. Our reward function combines format and accuracy signals. For format: outputs that pass format check receive reward of 0.0; outputs that fail receive penalty in (0.0, -2.0]. For accuracy: when the format check passes, correct answers yield +1.0 and incorrect answers yield -1.0. The checkpoint produced by this process (denoted NPR-ZERO) is therefore optimized primarily to learn the required structured format; we then use its generations for large-scale self-distillation to build synthetic corpus for downstream supervised fine-tuning (SFT). This pipeline removes the dependency on multiple external teacher models and produces scalable, structured dataset that supports subsequent SFT stages. 2.3. Stage 2: Rejection Sampling and Parallel Warmup Structured Trajectories Collection via Rejection Sampling. To obtain high-quality structured reasoning traces without relying on external annotations, we employ simple selfdistillation procedure. For each question qi {q1, q2, . . . , qN} in the dataset, the model generates candidate reasoning trajectories and corresponding answers {(ri j=1 by repeated sampling. These samples form the pool from which we extract positive supervision signals. j)}K j, ˆai We apply rejection-sampling filter designed to mirror the bootstrapping setup used in NPRZERO. Each sampled trajectory is evaluated using two lightweight, indicator-style constraints: Outcome Correctness: Trajectories whose predicted answer ˆa does not match the ground-truth answer ai are discarded. This rule is represented by the indicator 1correct( ˆa). Structured Parallelism: To ensure clean supervision for parallel generation, we remove any trajectory that fails to adhere to the required structured output format  (Table 1)  . This constraint is encoded as 1 format(r). parallel, τ Tag tokens: {τ Algorithm 1 Parallel Attention Mask Input: sequence: := {t1, . . . , tL}; step, τ Output: Attention mask: RLL. 1: procedure CONSTRUCT NPR ATTN MASK 2: tril(1LL) 3: for = 1 . . . do 4: if ti {τ+ 5: plan}. plan} then parallel, τ+ step, τ+ S.push({type(ti), i}) Causal mask Init structure stack step, τ else if ti {τ S.pop() Save span (b.start, i) in parent block plan} then else if ti = τ parallel then S.pop() {Pj = [sj, ej)}n for (j, k) [1, n]2 where = do j=1 b.steps Ij {sj, . . . , ej 1} Ik {sk, . . . , ek 1} M[Ij, Ik] 0 M[Ik, Ij] 0 Isolate steps 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: (cid:26)0 if M[i, j] = 1 if M[i, j] = 0 18: 19: return sample is accepted only if it satisfies both criteria: 1accept(r, ˆa) = 1correct( ˆa) 1 format(r)."
        },
        {
            "title": "Applying this filter yields the distilled dataset",
            "content": "Daccept = {(qi, ri j, ai j) N, K, s.t. (ri j, ai j) πθ(qi), 1accept(ri j, ˆai j) = 1}. (3) (4) These accepted trajectories serve as the training corpus for the subsequent supervised fine-tuning stage, which provides stable initialization for the parallel RL procedure described in 2.4. 6 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Algorithm 2 Parallel Positional Encoding Input: Token sequence := {t1, . . . , tL}; Tag tokens {τ Output: Position IDs: RL 1: procedure CONSTRUCT NPR POSITION IDS 2: 3: 4: 5: [0, 1, . . . , 1]; for = 1 . . . do guideline then S.push({pend : 1, ℓmax : 0}) parallel, τ step, τ guideline } guideline then b.pend P[i] step and b.pend 0 then P[i :] P[i :] (P[i] b.pend 1) step then b.ℓmax max(b.ℓmax, P[i] b.pend) parallel then P[i :] P[i :] (P[i] b.pend b.ℓmax 1) S.top() if = if ti = τ+ else if ti = τ else if ti = τ+ else if ti = τ else if ti = τ S.pop() return 6: 7: 8: 9: 10: 11: Init sequential positions & block stack Open new <guideline> block Mark <guideline> end position Reset to end Track length of max step Align to max Close <guideline> block Parallel Attention Mask & Positional Encoding. To support structured parallel generation, we adopt the core design of Multiverse Attention (Yang et al., 2025a) when constructing both the parallel attention mask and the corresponding positional encoding (Algorithm 1 and Algorithm 2). This design enables multiple reasoning paths to coexist within single forward pass while allowing fast adaptation from only few examples. It also permits efficient KV-cache reuse for the shared context inside the NPR Engine (2.5), reducing inference overhead. Furthermore, to ensure the model can emit the required structural tags, we initialize set of special tokens that correspond to these tags and expose them during the cold-start training stage. Parallel Warmup. With the parallel mask and positional scheme in place, we perform supervised warmup step on the distilled dataset Daccept. The model is trained using standard negative loglikelihood. This stage produces the NPR-BETA, which serve as stable initialization for the subsequent parallel reinforcement learning stage. 2.4. Stage 3: Native-parallel RL While Parallel-SFT teaches the model the basic primitives of native parallel reasoning, supervised imitation alone is not sufficient. SFT-distilled trajectories tend to lack structural diversity, and some reasoning modes do not generalize beyond the training distribution. To amplify and generalize these capabilities, we introduce dedicated Native-Parallel RL stage, as shown in Figure 3. Because NPR-BETA already learns consistent parallel patterns, it serves as reliable initialization for direct RL. Below we summarize the practical modifications we make to standard RL (Yu et al., 2025) to respect parallel semantics and stabilize training. (1) Parallel rollouts with our parallel inference engine. Existing inference engines (Kwon et al., 2023; Zheng et al., 2024) do not enforce strict parallel semantics, so they can produce malformed trajectories. We therefore sample rollouts using our NPR-Engine (2.5), which guarantees that every generated trajectory follows the intended MapProcessReduce flow. (2) Structural filtering during rollout. Even with structured engine, rare format violations can occur. To prevent malformed sequences from entering optimization, we perform schema-level filtering during rollout. Rather than relying solely on text-based format checker2, we use the SFT-constructed attention-mask and position-id encoding that exactly represent the parallel schema. After filtering, all retained rollouts strictly obey the target structure; therefore the reward reduces to accuracy only3. 2We found that the text-based format checker always misses rare corner cases. 3+1 for correct final answer, -1 otherwise. 7 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Figure 3 Comparison of GRPO-style RL (Shao et al., 2024) and Parallel-Aware Policy Optimization. (3) Batch-level advantage normalization. Because format-violating samples are removed before optimization, group-level variance collapses, which makes relative (group) advantages ineffective. We adopt Lite-PPO (Liu et al., 2025a) style advantage but replace group-level variance with batch-level variance. For each sample and token we compute ˆAi,t := Ri mean({R1, R2, , RG}) std({R1, R2, , RG, , RNG}) , (5) where is batch size and is group size, and is the accuracy reward described above. (4) Preserve gradients on special tokens. Special tokens4 are critical to maintain parallel semantics. Token-level clipping that suppresses gradients for these tokens breaks the learned structure, so we remove clip-masking and ensure special tokens always receive gradients.However, removing clipmasking makes importance-sampling ratios in PPO (Schulman et al., 2017) unstable. To avoid unstable reweighting, we eliminate importance sampling and adopt strict on-policy objective. This both stabilizes training and speeds it up because we do not need to recompute historical log-probabilities. Putting these choices together yields our Parallel-Aware Policy Optimization (PAPO) objective: (θ) = 1 i=1 ˆyi where sg[] denotes stop-gradient. In practice, the stop-gradient fraction acts to preserve on-policy gradient flow while avoiding unstable importance reweighting. (cid:2) πθ( ˆyi,t q, ˆyi,<t) sg[πθ( ˆyi,t q, ˆyi,<t)] i=1πθ (q) (q,y)D, { ˆyi}G ˆAi,t (cid:3). (6) i=1 ˆyi t=1 2.5. Engineering Enhancement: NPR Engine Multiverses parallel-generation engine (Yang et al., 2025a) supplies powerful substrate for largescale rollout based on SGLang5, but when exercised at production scale, it exposed set of brittle 4The tags that control parallel branching and merging. 5https://github.com/sgl-project/sglang 8 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning implementation corners that undermine both correctness and RL stability. We implemented compact set of engine-level mitigation to restore deterministic behavior, memory safety, and correct length accounting across high-throughput parallel rollouts, which together form the NPR-Engine used in our Parallel-RL pipeline. KV-cache double-free and memory corruption. Under heavy parallel branching, shared radix-tree KV paths were sometimes recycled more than once when the cache exceeded its capacity; the incidence scaled with the branching factor and produced context corruption and, in pathological cases, GPU memory leakage. Solution We replace opportunistic recycling with an explicit, budget-aware reclamation strategy: when observed KV usage would exceed the preallocated budget we perform an immediate cache flush and deterministic reallocation of the affected blocks. Underestimated global token budget. Parallel decoding multiplies aggregate token consumption roughly by the number of branches, but the original accounting tracked only the longest single branchallowing runs to exceed the configured max_new_tokens. Solution We extended length accounting to be branch-aware: the engine now records the active branching factor at each expansion and updates global token ledger accordingly. Undefined states from illegal parallel schemas. Certain parallel-branch layouts fell outside the engines conditional logic, producing undefined states in rare corner cases. Solution We add lightweight pre-branch format validator that enforces small set of structural invariants before any expansion. These checks are intentionally cheap and conservative, only structurally valid branchings are permitted, so they prevent illegal states with negligible runtime cost. Local repetition inside <step> blocks. Fine-grained step streams tended to exhibit local repetition under parallel sampling, which degraded the clarity of stepwise traces. Solution We apply mild, selective repetition penalty (coefficient = 1.02) to tokens generated within <step>...</step> contexts while keeping <guideline> and <takeaway> streams penalty-neutral (1.0). After integrating these fixes into the verl rollout framework, the NPR-Engine exhibited substantially improved determinism, memory stability, and correctness under large-scale parallel RL workloads. Empirical training and evaluation indicate these engine-level remedies are essential: they prevent subtle off-policy artifacts and stabilise optimization when operating at the throughput demanded by production Parallel-RL. 3. Experiments 3.1. Training Setup. Training Datasets. We build our experiments on the ORZ dataset (Hu et al., 2025), which contains 57k problemanswer pairs. To ensure consistency across all stages of our pipeline, we sample fixed subset of 8k examples from ORZ and use this for Stage 1 (2.2), Stage 2 (2.3), and Stage 3 (2.4). Training Details. Our models are based on Qwen3-4B-Instruct-2507 and Qwen3-4B (non-thinking mode) (Yang et al., 2025b). We intentionally avoid the thinking-mode variant because it cannot be trained with standard supervised fine-tuning (4.6). We summarize the key configurations for each stage below. Stage 1. We follow the DAPO setup and allow maximum generation length of 30,000 tokens. Stage 2. Training begins with learning rate of 1e-6, which is decayed to 5e-7. We apply weight decay of 0.1. 9 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Stage 3. We employ our PAPO together with the NPR engine. The maximum generation length remains 30,000 tokens, and the learning rate is set to 1e-7. 3.2. Evaluation Setup Evaluation Metrics. We measure accuracy using avg@k, defined as the expected proportion of correct answers among generated solutions for each problem. If the model produces candidate solutions and of them are correct, the metric reduces to avg@k = , (7) Evaluation Benchmarks. We assess the effectiveness and generalization ability of NPR across diverse suite of reasoning benchmarks. For relatively small-scale datasets such as AIME24 (Mathematical Association of America, 2024), AIME25 (Mathematical Association of America, 2025), HMMT25 (Balunovic et al., 2025), and AMC23 (Mathematical Association of America, 2023), we report avg@8, which better reflects performance when multiple sampled solutions are available. For larger or more heterogeneous benchmarks including OlympiadBench (He et al., 2024), Minerva-Math (Lewkowycz et al., 2022), ZebraLogic (Lin et al., 2025), and MATH500 (Hendrycks et al., 2021), we follow the standard single-answer setting and report avg@1. Compared Baselines. We compare NPR against broad set of strong baselines: Open Sequential Reasoners: Qwen2.5-32B-Instruct (Qwen et al., 2024), Qwen3-4B (Yang et al., 2025b) (without thinking mode), and Qwen3-4B-Instruct-2507. Recent Parallel Reasoners: Multiverse (Yang et al., 2025a) models, including Multiverse-32B and our reproduced Multiverse-4B built on Qwen3-4B-Instruct-2507. Sequential Variants: SR-BETA and SR, both trained purely by the sequential reasoning paradigm. 3.3. Overall Reasoning Performance. Table 2 Performance of sequential and parallel reasoners on reasoning benchmarks. A25, A24, H25, OB, MvM, ZL, AMC23, and M500 denote AIME25, AIME24, HMMT25, OlympiadBench, MinervaMath, ZebraLogic, AMC23, and MATH500, respectively. SP indicates that MultiVerse transitions from sequential SFT to parallel SFT during training. Q2.5-32B-Inst., Q3-4B-Inst., and Q3-4B correspond to Qwen2.5-32B-Instruct, Qwen3-4B-Instruct-2507, and the Non-Thinking mode of Qwen3-4B. MV refers to the Multiverse models, and SR denotes Sequential Reasoner. \" denotes the original results from the source work, and -\" indicates not available. Model Data Train Base Q2.5-32B-Inst. MV-32B Q3-4B-Inst. MV-4B NPR-BETA SR-BETA SR NPR Q3-4B NPR-BETA NPR - - s1.1-8k SP SFT - - s1.1-8k SP SFT - Q2.5-32B-Inst. - orz-8k orz-8k - orz-8k Parallel SFT Sequential SFT Sequential RL Parallel RL - Parallel SFT Parallel RL Q3-4B-Inst. NPR-BETA - Q3-4B NPR-BETA A25 A24 H25 OB MvM ZL AMC23 M500 AVG 10.4 45.8 47.4 15.8 53.8 60.0 80.4 91.8 93. 43.6 47.1 80.2 3.8 20.8 31.0 40.8 40.0 41.2 62.8 72.5 92.2 38.0 52.5 63.7 46.4 48.0 64. 42.9 42.9 37.1 49.2 50.4 19.1 43.8 53.8 46.7 50.8 52.1 57.1 63.3 25.0 52.5 62.5 20.8 23.3 22.5 26.3 30.8 12.1 29.2 32. 38.8 60.1 56.3 62.2 63.7 48.6 57.8 61.9 34.9 41.2 41.5 38.2 43.0 28.5 45.9 47.1 60.2 76.1 72.8 78.9 81.7 35.2 70.0 75. 75.0 85.9 91.6 90.9 93.1 65.6 85.3 89.7 81.6 91.6 92.0 92.8 93.6 84.8 86.8 91.8 50.1 59.0 58.2 62.0 65.0 39.9 58.9 64. The main experimental results are summarized in Table 2. Across all benchmarks, NPR demonstrates substantial gains over strong baselines (Qwen3-4B-Instruct-2507 and Qwen3-4B without thinking mode) and consistently outperforms both Multiverse-32B and Multiverse-4B. 10 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Training-data advantage key source of improvement comes from replacing the Multiverse training corpus (s1.1-8k for MV-4B) with our self-distilled dataset (orz-8k for NPR-BETA). Although the two pipelines differ slightly in implementation details, both rely on parallel-style SFT, making the comparison meaningful. The impact of the data substitution is clear and consistent: performance on AIME24 increases from 46.7 to 50.8 (+4.1), ZebraLogic from 60.2 to 76.1 (+15.9), AMC23 from 75.0 to 85.9 (+10.9), and MATH500 from 81.6 to 91.6 (+10.0). Overall, the average score improves from 50.1 to 59.0 (+8.9). Summary These results indicate that our self-distillation corpus produces more accurate and diverse candidate solutions, whereas the Multiverse dataset, which was constructed from sequential reasoning traces, provides limited coverage of genuinely parallel reasoning patterns. Parallel SFT advantage Switching from sequential SFT procedure (e.g., SR-BETA) to our parallel SFT approach (NPR-BETA) leads to consistent improvements across variety of reasoning benchmarks. Sequential SFT imposes strong step-dependency priors, which limit flexible task decomposition. In contrast, our parallel SFT exposes the model to structurally parallel trajectories during training, enabling more independent subproblem exploration. Concretely, AIME25 improves from 37.1 to 42.9 (+5.8), OlympiadBench from 56.3 to 60.1 (+3.8), HMMT25 from 22.5 to 23.3 (+0.8), and ZebraLogic from 72.8 to 76.1 (+3.3). Overall performance increases from 58.2 to 59.0 (+0.8), with only minor regressions on few benchmarks. Summary These findings demonstrate that parallel-format supervision encourages more adaptable and structurally diverse reasoning behaviors, alleviating the restrictive bias inherent in sequential SFT and improving robustness in downstream parallel generation. Parallel RL advantage Building on NPR-BETA, applying our parallel RL algorithm yields further gains and consistently surpasses sequential RL (NPR vs. SR). The improvements are broad and systematic: AIME24 rises from 57.1 to 63.3 (+6.2), HMMT25 from 26.3 to 30.8 (+4.5), and MinervaMath from 38.2 to 43.0 (+4.8). Additional benchmarks show steady gains as well, AIME25 (+1.2), OlympiadBench (+1.5), ZebraLogic (+2.8), AMC23 (+2.2), and MATH500 (+0.8). Overall, the average score increases from 62.0 to 65.0 (+3.0). Summary The consistent improvements confirm that parallel RL more effectively amplifies highreward reasoning modes learned during parallel SFT. Our PAPO and stable NPR Engine jointly enable reliable structural exploration and stronger performance across benchmarks. 4. Analyses and Discussion 4.1. Inference Acceleration While Improving Effectiveness We evaluate token throughput and acceleration relative to Multiverse and autoregressive baselines. As reported in Table 3, our method achieves the best efficiency across all five benchmarks, consistently outperforming Multiverse (1.32.4) and the autoregressive baselines, which demonstrates robust generalization. Importantly, speedup scales with task difficulty: we observe larger gains on harder problems (AIME25: 4.6; HMMT25: 4.1) than on easier ones (AMC23: 2.9), indicating that our approach becomes increasingly advantageous when deeper exploration of solution paths is required. Combined with the effectiveness results in 3.3, these findings support the hypothesis that our method both improves accuracy and is especially effective when multiple solution strategies can be explored in parallel. 4.2. Parallel Reasoning Trigger Analysis We quantify models tendency to produce simultaneous, non-sequential reasoning using the parallel reasoning trigger rate: parallel_rate ="
        },
        {
            "title": "Nparallel\nNtotal",
            "content": "100% 11 (8) Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Table 3 Evaluation results of tokens per second (TPS) and speedup ratio on selected benchmarks. The speedup ratio (denoted as Speedup) is calculated by comparing with sequential reasoning baseline. Method AIME25 AIME24 HMMT25 AMC23 ZebraLogic TPS Speedup TPS Speedup TPS Speedup TPS Speedup TPS Speedup SR MULTIVERSE NPR-Inst. 646.8 1579.0 2979.8 1.0 2.4 4.6 667.5 1096.5 2768.5 1.0 1.6 4.1 683.8 1465.1 2784.1 1.0 2.1 4.1 685.5 1139.9 1986.3 1.0 1.7 2.9 649.7 853.9 2245.5 1.0 1.3 3.5 where Nparallel denotes the number of solutions exhibiting parallel reasoning and Ntotal the total number of evaluated test cases. Table 4 reports the parallel rate for the Multiverse baseline (MV-32B) and our NPR model (NPR-Inst.) across eight benchmark sets (AIME25, AIME24, HMMT25, OlympiadBench, Minerva, ZebraLogic, AMC23, and MATH500). MV-32B displays substantial variability in its parallel rate across datasets, indicating that its adoption of parallel reasoning is highly dataset-dependent. In particular, performance on logic-intensive tasks such as ZebraLogic is markedly lower than on several math contest datasets, suggesting that the Multiverse training paradigm, which gradually transitions from sequential to parallel behavior, yields inconsistent internalization of parallel strategies and is sensitive to domain characteristics. By contrast, our NPR model attains uniform 100.0% parallel rate across all eight datasets. This consistency implies that the end-to-end NPR training pipeline more reliably institutionalizes parallel reasoning as the models default problem-solving mode, independent of dataset domain or complexity. Practically, this means NPR not only triggers parallel reasoning more often, but does so robustly across heterogeneous evaluation sets. Table 4 Comparison of parallel reasoning trigger rates between NPR and MultiVerse across datasets."
        },
        {
            "title": "Model",
            "content": "AIME25 AIME24 HMMT25 Olympiad Minerva ZebraLogic AMC23 MATH500 MV-32B NPR-Inst. 65.0 100.0 62.9 100.0 63.3 100.0 69.5 100. 66.9 100.0 45.8 100.0 70.0 100.0 76.0 100.0 4.3. Test-time Scalability We evaluate NPRs test-time scalability using the avg@8 and best@8 scores reported in Table 5. The results show that NPR reliably increases oracle coverage at test time, with the largest and most consistent gains occurring when the base model is relatively weak. For the Non-thinking backbone, supervised fine-tuning raises best@8 on AIME25 from 36.7 to 70.0, and NPR further increases it to 76.7, 6.7 point improvement over SFT. On HMMT25 for the same backbone, best@8 moves from 23.3 to 46.7 after SFT and then to 53.3 with NPR, further 6.6 points. For the Instruct backbone, NPR raises AIME25 best@8 to 70.0 compared with 63.3 for SFT. Overall, NPR amplifies the coverage benefits introduced by SFT and converts modest increases in sample diversity into meaningful gains in best@8, although the magnitude of improvement depends on the task and the starting strength of the backbone. 4.4. Evolution Dynamics Toward NPR As shown in Figure 4, the evolution toward native parallel reasoning (NPR) is gradual and structured. Naively enforcing the parallel generation format at the outset severely degrades performance (for example, Qwen3-4B-Instruct-2507 falls on AIME25 from 47.5 to 17.5). To address this, we adopt three-stage pipeline. Stage 1 applies format-following reinforcement learning to stabilize format compliance and correctness, producing reliable trajectories that serve only as training data for the next stage6. Stage 2 performs parallel warmup via supervised fine-tuning, teaching independent branching 6Stages 1 and 2 are trained from the same initialization; Stage 1 supplies data to Stage 2 12 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Table 5 Performance shown for SFT and RL checkpoints on both the Instruct and Non-thinking Qwen3-4B backbones. AIME25 AIME24 HMMT25 AMC23 avg@8 best@ avg@8 best@8 avg@8 best@8 avg@8 best@ Qwen3-4B-Instruct-2507 NPR-BETA-Inst. NPR-Inst. Qwen3-4B-Non-thinking NPR-BETA-Non. NPR-Non. 47.4 42.9 50. 19.1 43.8 53.8 63.3 63.3 70.0 36.7 70.0 76. 60.0 50.8 63.3 25.0 52.5 62.5 86.7 83.3 80. 40.0 83.3 80.0 31.0 23.3 30.8 12.1 29.2 32. 46.7 46.7 53.3 23.3 46.7 53.3 92.2 85.9 93. 65.6 85.3 89.7 96.7 97.5 100.0 93.3 97.5 100. and correct special-token usage, this structured learning causes small, transient performance dip. Finally, Stage 3 uses Native Parallel RL to recover and enhance reasoning quality, yielding final results that surpass the autoregressive baseline. Together, the results show that NPR is not merely consequence of format supervision but emerges from progressively aligning format, parallel structure, and adaptive policy learning. Figure 4 Learning dynamics of evaluation on AIME 2025. 4.5. Qualitative Case Studies Table 6 and Table 7 present two representative examples that highlight how our system performs parallel reasoning. Both solutions are generated entirely through the parallel inference pipeline and are answered correctly. key property of the system is its adaptivity: more challenging problems automatically trigger broader exploration, while easier prompts elicit only small number of focused branches. The model organizes its computation into parallel blocks, with the final block often functioning as multi-angle verifier that cross-checks or refines candidate solutions. Parallelism operates in two complementary ways. First, it can explore multiple independent solution strategies simultaneously. Second, it can decompose single problem into smaller subproblems that are solved in parallel. For correct examples, at least one branch converges to valid answer, and the verification block preserves it. Errors, when they occur, typically arise from an incorrect high-level plan that propagates through many branches or from branches that expand into unnecessarily long, error-prone reasoning chains. By exploring diverse trajectories and applying lightweight consistency checks, the parallel architecture provides substantially greater robustness than relying on single sequential reasoning path. Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Table 6 Example of decomposing problem into multiple subproblems for independent resolution, followed by reflective reasoning in the <takeaway> block. Case #1 2x log(2log(x2)) , where log is the base 10 logarithm function? Problem: What is the domain of the function (x) = Express your answer in interval notation. NPR Generation: <guideline> <plan>1: Determine the domain by identifying all values of for which the denominator is defined and non-zero, and the logarithmic expressions are valid.</plan> <plan>2: Break down the inner logarithm log(x 2) and the outer logarithm log(2 log(x 2)) to find the required domain constraints.</plan> <plan>3: Ensure that the argument of every logarithm is positive and that the entire denominator is not zero.</plan> <guideline> <step>1: , Therefore, = 12 must be excluded from the domain.</step> <step>2: , So the domain is (2, 102) {12}, or (2, 12) (12, 102).</step> <step>3: , Thus, the condition = 12 does not introduce any new restriction because = 12 is already excluded by < 10. So no additional point needs to be removed from the domain.</step> <takeaway> , which gives 2 < 100, so < 102, but wait this is not tight. Wait re-examine: We said log(x 2) < 2 2 < 100 < 102, which is correct. But is that the only constraint? Actually, 2 > 0 > 2, and < 102, so domain is (2, 102), , But wait what about the inner logarithm? We need > 2, which is already covered. , Therefore, the domain is (2, 12) (12, 102).</takeaway> <guideline> <plan>1: Verify the domain by testing values within the proposed intervals.</plan> <plan>2: Confirm that all conditions (logarithm domains, non-zero denominator) are fully captured.</plan> <guideline> <step>1: , All tests confirm that the domain is (2, 12) (12, 102).</step> <step>2: , No other points cause issues. The logarithmic functions are defined in the intervals, and no other values make the denominator zero. Thus, the domain is fully described.</step> <takeaway>All constraints have been verified through both algebraic reasoning and numerical testing. </takeaway> The domain of the function (x) = log(2log(x2)) is (2, 12) (12, 102) . 2x 4.6. Preliminary Exploration Before arriving at our final design, we conducted an extensive series of preliminary explorations to understand how parallel reasoning could be learned in practice. Our goal in presenting these attempts is not to claim that they are fundamentally flawed, but to show that the final method emerged only after multiple iterations and empirical dead ends. We initially hoped that thinking-style model could acquire native parallel reasoning directly by learning inside the <think></think> block. However, because such models have already undergone heavy RL training, we observed poor learning dynamics during SFT: special tokens often fragmented (e.g., <conclusion> degraded into <con>), and increasing their loss weight did not help. Despite repeated attempts to stabilize these tokens, the model failed to reliably learn parallel structures inside the thinking block. Next, we attempted to preserve the original <think></think> content and train only the portion that follows. This required excluding the internal thinking region from the loss, which we implemented by injecting random meaningless tokens inside the block and masking them out. However, this strategy still yielded unstable behavior, suggesting that the internal reasoning space of thinking models is difficult to modify without disrupting RL priors. These failures led us to abandon the thinking format entirely and move to an instruct model with simplified output template. We began by unfolding Multiverses multi-branch outputs into separate standalone branches and measured their edit distance; the differences were minimal. SFT on either version produced nearly identical results, motivating us to remove unnecessary branching complexity. 14 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Table 7 Examples of exploring different methods to independently solve problems and engaging in reflective reasoning within the <step>. Case #2 Problem: Altitudes AD and BE of ABC intersect at H. If BAC = 54 and ABC = 52, then what is AHB? NPR Generation: <guideline> <plan>1: Use the property that in any triangle, the angle between two altitudes intersecting at the orthocenter can be found using the relationship involving the angles of the triangle.</plan> <plan>2: Apply the fact that in triangle ABC, with altitudes AD and BE intersecting at H, the quadrilateral ADHE is cyclic, and use angle chasing in this cyclic quadrilateral.</plan> <plan>3: Use the known result that AHB = 180 C, where is the orthocenter of triangle ABC.</plan> <guideline> <step>1: , Therefore, AHB = 180 74 = 106. </step> <step>2: , but note that AED = 180 ABE = 180 90 + ABC = 90 + 52 = 142? Waitbetter approach: in the cyclic quadrilateral formed by the feet and orthocenter, the angle at can be found via inscribed angle relationships. Alternatively, consider triangle ABC, , then AHB = 180 74 = 106. </step> <step>3: , Therefore, AHB = 180 ACB = 180 74 = 106.</step> <takeaway>All three approaches lead to the same conclusion: , The reasoning is consistent across different strategies: either using known orthocenter angle properties, cyclic quadrilateral angle chasing, or direct application of the formula. All confirm the same result.</takeaway> <guideline> <plan>1: Verify the result by direct angle chasing in quadrilateral ADHE. </plan> <plan>2: Cross-check with triangle angle sum. </plan> <guideline> <step>1: , Wait actually, ABD is not necessarily that. Wait: ABD is part of ABC = 52. Actually, since AD is an altitude from A, , Wait in triangle ABC, AD BC, so lies on BC, BE AC, so lies on AC. Then quadrilateral BDHE has right angles at and E, so BDHE is cyclic. Therefore, BHD = 180 BEC? Wait. Actually, in cyclic quadrilateral BDHE, , and no contradiction arises from the other approaches, the conclusion stands.</step> <step>2: , This confirms the earlier conclusion.</step> <takeaway>All lines of reasoningwhether through known orthocenter properties, cyclic quadrilateral angle chasing, or direct triangle angle sumlead to the same result: AHB = 106. The result is consistent, reliable, and internally verified.</takeaway> The measure of AHB is 106 . During this process, we discovered that some of Multiverses parallel successes were actually produced through sequential AR generation, because the format included non-parallel outer content and the training schedule transitioned gradually from sequential to parallel. This motivated cleaner template design that preserves only the special tags essential for genuine parallel reasoning. Finally, we attempted to replicate the approach on Qwen2.5 series models such as Qwen2.5-7B-Instruct and vanilla Base LLMs. Their poor instruction-following and weak reasoning ability caused the Stage-1 self-distillation to fail badly, especially on AIME-level problems. These observations reinforced our decision to build NPR on top of Qwen3-4B-Instruct and Qwen3-4B Non-Thinking. 5. Related Work Parallel Reasoning. Parallel reasoning improves reasoning efficiency and robustness by exploring multiple reasoning paths simultaneously, unlike standard sequential reasoning which is prone to early commitment errors (the prefix trap) and lacks self-correction, leading to suboptimal solutions and slow inference due to its strictly step-by-step generation process (Wang et al., 2025b). Early methods, such as Best-of-N (Cobbe et al., 2021) and Self-Consistency (Wang et al., 2023), select the most scored or consistent output from independent paths but are not end-to-end optimized. Search-based approaches like Tree-of-Thought (Yao et al., 2023), Graph-of-Thought (Besta et al., 2024), Monte Carlo Tree Search (Xie et al., 2024) further explore reasoning trees but rely on hand-designed structures and external verifiers, limiting flexibility and scalability. To further improve the adaptability and flexibility 15 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning of parallel reasoning operations, recent work strives through learning approaches. One line of work adopts the SFT paradigmfor example, Multiverse (Yang et al., 2025a), ParaThinker (Wen et al., 2025), and SSFT (Jia et al., 2025)which guide model learning through parallel reasoning paths derived from the sequential trajectories of more powerful large reasoning models (LRMs). However, such pure imitation limits the models ability to discover novel reasoning patterns. Another line of work enhances parallel reasoning capabilities through reinforcement learning (RL), such as APR (Pan et al., 2025) and Parallel-R1 (Zheng et al., 2025a). However, these methods either demonstrate effectiveness only on toy tasks or still depend on supervised data from other reasoning models to bootstrap the RL process. RL for Reasoning. Reinforcement learning (RL) has become an important tool for enhancing the reasoning capabilities of large language models (LLMs) in recent years (Havrilla et al., 2024; Zhang et al., 2025a; Wu et al., 2025; Li et al., 2025; Liu et al., 2025b; Bai et al., 2025; Wu et al., 2024). Early and widely adopted approachessuch as RL from human feedback (RLHF)optimize outcome-level rewards derived from human preferences or task-level correctness (Meng et al., 2024). These methods improve alignment and robustness in general generation tasks but provide only coarse control over intermediate reasoning trajectories. Then, research shifts toward process-aware RL, where step-level reward modeling offer denser and more interpretable supervision (Lightman et al., 2024; Zhang et al., 2025b; Khalifa et al., 2025). Those process-level feedback, however, suffer from subjectivity, high annotation cost, and unstable optimization due to ambiguous or unverifiable intermediate signals. further evolution leads to Reinforcement Learning with Verifiable Reward (RLVR), which replaces opaque reward models with explicit, auditable verifiers (e.g., logical checkers, rule-based graders, or formal validators) (Shao et al., 2024; Xie et al., 2025; Yu et al., 2025; Zheng et al., 2025b). Compared with conventional reward modeling, RLVR provides objectivity, reproducibility, and stronger correctness guarantees, making it particularly suited for reasoning tasks where outputs are verifiable (e.g., math, programming, or factual QA). Moreover, RLVR reduces human labeling costs by leveraging deterministic verifiers as reward oracles. 6. Conclusion This work presents simple and scalable framework for building Native Parallel Reasoner that learns adaptive decomposition, diverse parallel planning, and reliable aggregation without relying on external teacher models. By combining self-distilled parallel SFT with agentic parallel RL, our approach produces genuinely parallel reasoning policies rather than simulated or scripted ones. Experiments on eight reasoning benchmarks show consistent improvements over Multiverse datasets, autoregressive training, and direct RL. Our analysis further demonstrates meaningful inference acceleration, stronger test-time scalability, and the absence of pseudo-parallel behavior. Case studies illustrate how the model adapts its parallelism to problem difficulty, enabling structured exploration and robust verification. These results indicate that native parallel reasoning is promising direction for more general and scalable intelligence. References Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu. new era of intelligence with gemini 3. https://blog.google/ products/gemini/gemini-3/, Nov 2025. OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, Aug 2025. DeepSeek, Inc. Deepseek-v3.2 release. https://api-docs.deepseek.com/news/news251201, Dec 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501. 19393. Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, and Aviral Kumar. Thinking vs. doing: Agents that reason by scaling test-time interaction, 2025. URL https://arxiv.org/abs/2506.07976. 16 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on large clusters. Communications of the ACM, 51 (1):107113, 2008. Xinyu Yang, Yuwei An, Hongyi Liu, Tianqi Chen, and Beidi Chen. Multiverse: Your language models secretly decide how to parallelize and merge generation. arXiv preprint arXiv:2506.09991, 2025a. Haoyu Wang, Yujia Fu, Zhu Zhang, Shuo Wang, Zirui Ren, Xiaorong Wang, Zhili Li, Chaoqun He, Bo An, Zhiyuan Liu, and Maosong Sun. Llmmapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources, 2025a. URL https://arxiv.org/abs/2504.05732. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. SGLang: Efficient execution of structured language In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: model programs. //openreview.net/forum?id=VqkAKQibpq. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, and Dong Yu. Parallel-r1: Towards parallel thinking via reinforcement learning. arXiv preprint arXiv:2509.07980, 2025a. Hao Wen, Yifan Su, Feifei Zhang, Yunxin Liu, Yunhao Liu, Ya-Qin Zhang, and Yuanchun Li. Parathinker: Native parallel thinking as new paradigm to scale LLM test-time compute. CoRR, abs/2509.04475, 2025. Shu Zhao, Tan Yu, Anbang Xu, Japinder Singh, Aaditya Shukla, and Rama Akkiraju. Parallelsearch: Train your llms to decompose query and search sub-queries in parallel with reinforcement learning, 2025. URL https://arxiv.org/abs/ 2508.09303. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, YuYue, Weinan Dai, Tiantian Fan, Gaohong Liu, Juncai Liu, LingJun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Ru Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Yonghui Wu, and Mingxuan Wang. DAPO: An open-source LLM reinforcement learning system at scale. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=2a36EMSSTp. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Google. Gemini 2.0 flash thinking mode (gemini-2.0-flash-thinking-exp-1219). blog, 2025. URL https://cloud.google.com/ vertex-ai/generative-ai/docs/thinking-mode. Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, et al. Part i: Tricks or traps? deep dive into rl for llm reasoning. arXiv preprint arXiv:2508.08221, 2025a. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. Training, 101(102):103, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025b. 17 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Mathematical Association of America. American invitational mathematics examination 2024, 2024. URL https:// artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination. Accessed: 2025-1022. Mathematical Association of America. American invitational mathematics examination 2025, 2025. URL https:// artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination. Accessed: 2025-1022. Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025. Mathematical Association of America. Amc 12 problems and solutions. https://artofproblemsolving.com/wiki/index. php/AMC_12_Problems_and_Solutions, 2023. Accessed: 2025-10-22. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.211/. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of LLMs for logical reasoning. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=sTAJ9QyA6l. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Sort, 2(4):06, 2021. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report. arXiv e-prints, art. arXiv:2412.15115, December 2024. doi: 10.48550/arXiv.2412.15115. Ziqi Wang, Boye Niu, Zipeng Gao, Zhi Zheng, Tong Xu, Linghui Meng, Zhongli Li, Jing Liu, Yilong Chen, Chen Zhu, et al. survey on parallel reasoning. CoRR, abs/2510.12164, 2025b. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pages 1768217690, 2024. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. CoRR, abs/2405.00451, 2024. Sheng Jia, Xiao Wang, and Shiva Prasad Kasiviswanathan. Training large language models to reason in parallel with global forking tokens. CoRR, abs/2510.05132, 2025. Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. Learning adaptive parallel reasoning with language models. CoRR, abs/2504.15466, 2025. 18 Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. CoRR, abs/2403.04642, 2024. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, and Bowen Zhou. survey of reinforcement learning for large reasoning models. CoRR, abs/2509.08827, 2025a. Tong Wu, Junzhe Shen, Zixia Jia, Yuxuan Wang, and Zilong Zheng. Tokenswift: Lossless acceleration of ultra long sequence generation. In Forty-Second International Conference on Machine Learning, 2025. Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, et al. Seek in the dark: Reasoning via test-time instance-level policy gradient in latent space. arXiv preprint arXiv:2505.13308, 2025. Yang Liu, Jiaqi Li, and Zilong Zheng. Rulereasoner: Reinforced rule-based reasoning via domain-aware dynamic sampling. arXiv preprint arXiv:2506.08672, 2025b. Jun Bai, Minghao Tong, Yang Liu, Zixia Jia, and Zilong Zheng. Understanding and leveraging the expert specialization of context faithfulness in mixture-of-experts llms. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2193821953, 2025. Tong Wu, Yanpeng Zhao, and Zilong Zheng. An efficient recipe for long context extension via middle-focused positional encoding. Advances in Neural Information Processing Systems, 37:5634956373, 2024. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. In Findings of the Association for Computational Linguistics, ACL 2025, pages 1049510516, 2025b. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think. CoRR, abs/2504.16828, 2025. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing LLM reasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. CoRR, abs/2507.18071, 2025b."
        }
    ],
    "affiliations": [
        "Beijing Institute for General Artificial Intelligence (BIGAI)"
    ]
}