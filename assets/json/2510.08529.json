{
    "paper_title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
    "authors": [
        "Xiangyuan Xue",
        "Yifan Zhou",
        "Guibin Zhang",
        "Zaibin Zhang",
        "Yijiang Li",
        "Chen Zhang",
        "Zhenfei Yin",
        "Philip Torr",
        "Wanli Ouyang",
        "Lei Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents."
        },
        {
            "title": "Start",
            "content": "COMAS: CO-EVOLVING MULTI-AGENT SYSTEMS VIA INTERACTION REWARDS Xiangyuan Xue1,2 Yifan Zhou3 Guibin Zhang4 Zhenfei Yin6(cid:66) Chen Zhang2,8 Philip Torr6 Wanli Ouyang1,2 Zaibin Zhang5,6 Yijiang Li7 Lei Bai2(cid:66) 1The Chinese University of Hong Kong 2Shanghai Artificial Intelligence Laboratory 3University of Georgia 5Dalian University of Technology 4National University of Singapore 6University of Oxford 8University of Science and Technology of China 7University of California San Diego 5 2 0 O 9 ] . [ 1 9 2 5 8 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Self-evolution is central research topic in enabling large language model (LLM)- based agents to continually improve their capabilities after pretraining. Recent research has witnessed transition from reinforcement learning (RL)-free to RLbased methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agents policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves stateof-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as novel and effective paradigm for self-evolution in LLM-based agents. Our dataset and code are available at https://github.com/xxyQwQ/CoMAS."
        },
        {
            "title": "INTRODUCTION",
            "content": "Self-evolution has emerged as central research theme for large language model (LLM)based agents, aiming to endow agents with the capacity to continually enhance their capabilities through interaction with the environment (Tao et al., 2024; Gao et al., 2025b; Fang et al., 2025), rather than remaining stagnant after pretraining. Early explorations predominantly adopted RL-free strategies, such as expanding external knowledge bases (Zhang et al., 2023; Tang et al., 2025), ensembling multiple agents (Ong et al., 2024; Frick et al., 2025), optimizing task workflows (Hu et al., 2024b; Zhang et al., 2024b; 2025c), and incorporating symbolic learning (Zhuge et al., 2024; Zhou et al., 2024). Yet, the effectiveness of these approaches is inherently constrained by the fixed capabilities of the underlying foundation models. Although subsequent work has introduced agent fine-tuning techniques (Yuan et al., 2024b; Zhao et al., 2025a), such methods often fall short of supporting genuinely continual, bootstrapping forms of evolutionary improvement. Reinforcement Learning (RL) offers promising direction for overcoming these limitations. These methods can be broadly classified by the source of their reward signals. As illustrated in Figure 1, most existing approaches depend on external rewards, typically derived from rule-based verifiers (Wan et al., 2025; Estornell et al., 2025) or specialized reward models (Motwani et al., 2024; Park et al., 2025). More recently, researchers have begun to explore intrinsic rewards, which dispense with external supervision by leveraging internal signals. Such methods encourage agents to (cid:66)Corresponding authors, jeremyyin@robots.ox.ac.uk, baisanshi@gmail.com."
        },
        {
            "title": "CoMAS",
            "content": "Figure 1: comparison of our proposed CoMAS framework with existing RL-based self-evolution methods. The left column outlines methods utilizing external rewards from verifiers or reward models. The middle column outlines methods leveraging intrinsic rewards from metrics such as selfcertainty, confidence, semantic entropy, and pseudo-labels from majority voting. The right column outlines our CoMAS framework, which derives rewards from multi-agent interactions. avoid low-probability regions or reinforce high-confidence predictions, using mechanisms such as self-certainty (Zhao et al., 2025b), confidence (Prabhudesai et al., 2025), semantic entropy (Zhang et al., 2025e), or pseudo-labels from majority voting (Zuo et al., 2025). This direction opens path toward self-evolution where external rewards are no longer prerequisite. Despite notable advances in RL-based methods, their design remains largely centered on selfrewarding at the level of individual models, in contrast to human intelligence, which has evolved as collective phenomenon emerging from the diversity and interplay of individuals rather than from any single, perfect principle (Minsky, 1986). In human teams, individuals learn and improve through mutual discussion and collaboration, without an external oracle evaluating every contribution (Barron, 2003). This contrast motivates critical research question: Can LLM-based agents, akin to human beings, achieve self-evolution by learning purely from inter-agent interaction within multi-agent system, without relying on external reward signals? To address this question, we propose Co-Evolving Multi-Agent Systems (CoMAS), novel framework where agents autonomously improve by learning from their interactions (Section 3). CoMAS is built upon three core components. First, interaction generates rich conversational data through collaborative and critical discussions, structured around solution proposal, evaluation, and scoring. Second, reward formulation uses an LLM-as-a-judge mechanism to derive intrinsic reward signals directly from this discussion history. Finally, policy optimization employs an RL algorithm to update each agents policy, enabling them to effectively internalize the lessons from the interaction data. As new paradigm for self-evolution, CoMAS offers several distinct advantages: (1) It generates reward signals intrinsically from agent interactions, eliminating the need for verifiers or reward models. (2) The learning paradigm is generally effective for various tasks, including open-ended problems where solutions cannot be easily verified. (3) Agents are trained in decentralized manner, allowing for co-evolution of heterogeneous systems without the bottleneck of shared model. (4) It fosters skills that transfer to out-of-domain tasks and diverse multi-agent collaboration settings. We evaluate CoMAS across various benchmarks in both single-agent and multi-agent settings (Section 4). Results show that CoMAS delivers consistent, and in many cases, state-of-the-art performance, achieving absolute gains of up to 2.20%, 3.66%, 19.80%, and 6.10% over untrained agents in Vanilla, Consistency, AutoGen, and Debate setups, respectively. In contrast, baseline methods show instability and performance degradation in many cases. Ablation studies confirm that interactionbased reward formulation prevents training collapse and reward hacking, and further demonstrates that CoMAS has great scalability as the number and diversity of agents increase. These results establish CoMAS as novel and effective paradigm for the self-evolution of LLM-based agents."
        },
        {
            "title": "2.1 REINFORCEMENT LEARNING FOR LLMS",
            "content": "The success of ChatGPT (Ouyang et al., 2022) has established reinforcement learning from human feedback (RLHF) (Kaufmann et al., 2023) as one of the core post-training paradigms for LLMs, enabling alignment with human values through the classical RL algorithm PPO (Schulman et al., 2017). To address efficiency and stability concerns, subsequent works have proposed Direct Preference Optimization (DPO) (Rafailov et al., 2023) and numerous variants (Wu et al., 2024a; Meng et al., 2024; Ethayarajh et al., 2024). The development of sophisticated reward models (Zhang et al., 2025g; Zhong et al., 2025) and LLM-as-a-judge methodologies (Zheng et al., 2023; Gu et al., 2024) has further accelerated progress in this field. Recently, the success of DeepSeek-R1 (Guo et al., 2025) has introduced reinforcement learning from verifiable reward (RLVR) (Lambert et al., 2024) as new paradigm that effectively enhances LLM reasoning capabilities through GRPO (Shao et al., 2024). Follow-up research has explored the underlying principles and further improved the effectiveness of this approach (Liu et al., 2025b; Yu et al., 2025). Overall, the evolution of reward modeling demonstrates clear progression from coarse-grained, human-annotated rewards toward fine-grained, automatically generated rewards (Lightman et al., 2023; Luo et al., 2024)."
        },
        {
            "title": "2.2 SELF-EVOLVING AGENTS",
            "content": "Self-evolution represents fundamental paradigm in LLM-based agents (Tao et al., 2024; Gao et al., 2025b; Fang et al., 2025; Zhang et al., 2025b), referring to the capacity of agents to autonomously enhance their capabilities through environmental interaction. Mainstream approaches treat agents as compound systems and involve diverse methodologies, including enriching external knowledge bases (Zhang et al., 2023; Tang et al., 2025), ensembling multiple agents through intelligent routing mechanisms (Ong et al., 2024; Frick et al., 2025), optimizing task-specific agentic workflows (Hu et al., 2024b; Zhang et al., 2024b; 2025c), and leveraging symbolic learning to refine collaborative processes (Yuan et al., 2024a; Zhou et al., 2024; Ma et al., 2025a). Additionally, some works pursue agent evolution through fine-tuning on self-generated trajectories (Yuan et al., 2024b; Zhao et al., 2025a), while recent research has begun exploring reinforcement learning as mechanism for achieving agent self-evolution (Estornell et al., 2024; Wan et al., 2025; Estornell et al., 2025; Gao et al., 2025a; Motwani et al., 2024; Park et al., 2025)."
        },
        {
            "title": "2.3 MULTI-AGENT SYSTEMS",
            "content": "Multi-agent systems (MAS) represent another core research area within LLM-based agents (Guo et al., 2024; Li et al., 2024; Tran et al., 2025), enhancing reasoning capabilities through multi-agent interaction and collaboration. Early research primarily focused on static systems with pre-defined roles and protocols (Hong et al., 2023; Li et al., 2023; Wu et al., 2024b), structured argumentative debates (Du et al., 2023; Liang et al., 2023; Wang et al., 2024; Qian et al., 2024), and large-scale social simulations (Yang et al., 2024b; Piao et al., 2025). Recent work has addressed the limitations of these fixed-structure systems by developing dynamic, self-improving architectures that leverage graph-based or reward-driven mechanisms (Zhuge et al., 2024; Zhang et al., 2024a; 2025a; Zhou et al., 2025; Hu et al., 2025b; Zhang et al., 2025f; Liu et al., 2025a; Liao et al., 2025)."
        },
        {
            "title": "3 METHOD",
            "content": "To explore the potential that LLM-based agents, akin to human beings, achieve self-evolution by learning from inter-agent interaction within multi-agent system, we propose Co-Evolving MultiAgent Systems (CoMAS), multi-agent framework where agents learn and adapt by interacting within shared environment, mimicking collaborative and critical discussion. Figure 2 shows an overview of our proposed CoMAS pipeline. CoMAS is built upon an interactive multi-agent workflow: interaction, reward formulation, and policy optimization. Interaction represents the procedure that generates conversational data through collaborative and critical discussion, reward formulation is responsible for extracting reward signals from the conversation history and"
        },
        {
            "title": "CoMAS",
            "content": "Figure 2: An overview of our proposed CoMAS pipeline. CoMAS is built upon flexible and interactive multi-agent workflow, composed of three core components: interaction, reward formulation, and policy optimization. For given question, the agents conduct discussion through contributing solutions, evaluating the existing solutions, and scoring solutions based on their evaluations. The scores will be extracted and transformed into rewards for the corresponding solutions and evaluations. All the generated experiences will be collected to train the policies of the agents. allocating rewards to corresponding actions, and policy optimization utilizes an RL algorithm to update the weights of the agents, thus achieving self-evolution. 3."
        },
        {
            "title": "INTERACTION",
            "content": "Inspired by discussion formats in technical communities (e.g. Reddit, Github, and Stack Overflow), our framework facilitates hierarchical and decentralized interactions. The environment contains set of agents, = {u1, u2, . . . , ul}. The policy of each agent uk is πθk , parameterized by θk. This design allows for heterogeneity, meaning that agents can be based on different foundation models rather than sharing single backbone. From the action-level perspective, the policy πθk of agent uk acts as function that maps an input prompt to response o: = πθk (p) (1) Here, represents the full input context, which varies depending on the interaction type. From the token-level perspective, the output is sequence of tokens {ot}T t=1 generated in an autoregressive manner. Each token ot is sampled from the policys probability distribution, conditioned on the prompt and previously generated tokens o<t: ot πθk (p, o<t) (2) These two perspectives are equivalent. We use the high-level functional notation for clarity in describing interactions and revert to the token-level view for the policy optimization in Section 3.3. In CoMAS, we define three primary interaction patterns: 1. Solution: Given specific question and its discussion history hq, the agent generates solution si to the question: si = uk(q, hq) (3) where is the index of the solution for question q. 2. Evaluation: Given specific question q, its discussion history hq, and solution si to the question, the agent provides critical evaluation ei,j for the solution: ei,j = uk(q, hq, si) (4)"
        },
        {
            "title": "CoMAS",
            "content": "where is the index of the evaluation for solution si. The agent is explicitly prompted to identify potential flaws in si rather than simply agreeing. This helps mitigate the catering bias common in LLMs and aligns with our reward design. 3. Scoring: Given specific question q, solution si to the question, and an evaluation ei,j for the solution, the agent scores the solution based on the evaluation: τi,j = uk(q, si, ei,j) (5) The agent is explicitly prompted to output in specific format, so that the score can be correctly extracted. Different from the solution and evaluation, scoring is an independent interaction pattern specifically designed for reward generation, contributing nothing to the discussion history. Further details will be described in Section 3.2. For each interaction step, the acting agent uk is selected uniformly at random from the agent pool U, i.e., uk Uniform(U). This is based on the assumption that each agent has an equal chance to contribute to the discussion. Besides, uniform distribution ensures similar quantities of experiences for each agent, which balances the training load. Based on these defined interaction patterns, we can formulate the entire interaction process. Given question sampled from the dataset, the interaction process unfolds over consecutive rounds. In each round {1, . . . , m}, solution si is generated referring to the discussion history hq, followed by evaluations {ei,j}n j=1 trying to figure out the mistakes in the solution. Then the solution si together with its evaluations {ei,j}n j=1 are appended to the discussion history hq, enriching the context for subsequent rounds. Besides, the discussion history hq will be compressed to the last κ rounds to avoid the discussion history exceeding the context limitation. In addition, each solution and evaluation pair (si, ei,j) is processed with the scoring step to form τi,j. Finally, the entire interaction process yields total of solutions, evaluations, and scoring results."
        },
        {
            "title": "3.2 REWARD FORMULATION",
            "content": "The interaction process generates rich set of trajectories. To derive learning signal, we employ an LLM-as-a-judge approach (Gu et al., 2024) to assign rewards. This is based on the primary interaction pattern of scoring defined in Section 3.1. For each solution and evaluation pair (si, ei,j), we already have the scoring result τi,j through the scoring step. Then the scoring result is parsed to extract the score value: ˆτi,j = Extract(τi,j) (6) where Extract() predefines function to extract the score value from the formatted scoring result. The score value ˆτi,j should be an integer between 1 and 3, with the following semantics: 3: The solution is correct; the evaluation is unhelpful or incorrect. 2: The solution is mostly correct but has minor flaws pointed out by the evaluation. 1: The solution is incorrect with fatal mistakes identified by the evaluation. Then the score value is normalized to the range of [0, 1] and used to compute complementary rewards for the solution and the evaluation. This creates zero-sum game between the solver and the evaluator, encouraging both correctness and critical thinking: r(si) = ˆτi,j 1 2 , r(ei,j) = 1 r(si) = 3 ˆτi,j 2 (7) Additionally, penalty is applied to the agent that performs the scoring step if its output format is invalid. Given the scoring result τi,j, its penalty reward is defined as: r(τi,j) = (cid:26)0, 1, if τi,j {1, 2, 3} correctly extracted otherwise (8) where zero reward is assigned for successful scoring, which facilitates format following while encouraging the scoring agents to be neutral, thus improving the stability of the training process."
        },
        {
            "title": "3.3 POLICY OPTIMIZATION",
            "content": "Many existing works on RL for LLMs employ GRPO (Shao et al., 2024) and its variants (Liu et al., 2025b; Yu et al., 2025). However, our CoMAS framework features diverse interaction patterns rather than generating multiple rollouts from single prompt. For this reason, we adopt REINFORCE++ (Hu et al., 2025a), an effective RL algorithm that is naturally compatible with our approach and can be implemented with minimal modifications to the underlying architecture. All the agents within our framework are trained using the same procedure, so we only describe the update for single agent uk for simplicity. Interactions involving agent uk (as solver, evaluator, or scorer) are collected into replay buffer Dk = {(p, o, r(o))}, where is the context, is the generated output, and r(o) is its assigned reward. For each sample (p, o, r(o)) in Dk, the objective is based on token-level credit assignment. The advantage At is defined based on each token ot in the sequence. This advantage consists of the trajectory-level reward r(o) penalized by cumulative KL-divergence term to regularize the policy: At = r(o) β (cid:88) λ=t log πθk (oλp, o<λ) πref(oλp, o<λ) (9) Here, πref is fixed reference policy, typically the initial pre-trained model, to constrain the scale of the update step, and β controls the strength of this KL penalty. The advantages At are then normalized across the batch from the replay buffer Dk to stabilize updates: ˆAt = At Mean({At}) Std({At}) + ϵ We then use the surrogate objective to improve the policy, which is defined as: J(θk) = E(p,o,r(o))Dk (cid:88) (cid:16) ρt(θk) ˆAt, clip(ρt(θk), 1 ϵ, 1 + ϵ) ˆAt (cid:17) min (10) (11) t=1 Note that ρt(θk) = πθk (otp,o<t) πold(otp,o<t) is the importance sampling ratio, where πold is the policy before the current update. The gradient of the surrogate objective exactly points to the direction of the policy update, thus encouraging the action tokens that lead to higher advantages while constraining the policy update to be within trusted region, which ensures stable learning."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "4.1."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Infrastructure. We implement our CoMAS framework based on MARTI (Zhang et al., 2025d), an LLM-based multi-agent RL framework forked from OpenRLHF (Hu et al., 2024a). The training pipeline of MARTI consists of three phases: trajectory rollout, experience making, and agent training, which exactly aligns with our framework described in Section 3. Parameters. We adopt Qwen2.5-3B-Instruct (Yang et al., 2024a) as the base model under homogeneous setting to balance the model capacity and training budget. The agent number is set to = 4 for main experiments and = 2 for ablation studies. The interaction rounds are set to = 2 and = 1 with horizon of κ = 2. More details are provided in Appendix B. Datasets. Our training dataset comprises 2000 samples across three domains: 600 math tasks of level-4 or level-5 from MATH (Hendrycks et al., 2021), 600 coding tasks of medium and hard levels from KodCode (Xu et al., 2025), and 800 science tasks of physics, chemistry, and biology categories from WebInstruct-verified (Ma et al., 2025b). These datasets have proven effective for RL training and are completely distinct from our evaluation benchmarks."
        },
        {
            "title": "CoMAS",
            "content": "Table 1: The evaluation results of the agents trained by our framework on different benchmarks when employed in different setups, together with the comparison with the selected baselines. The results with performance improvements are highlighted in green, drops in red, and neutral changes in gray. CoMAS consistently improves the performance of the agents in almost all the settings. Method GSM8K MATH-500 HumanEval Dataset MBPP SciBench GPQA MMLU i 84.00 Untrained 84.80 (+0.80) MAPoRL 84.40 (+0.40) TTRL CoMAS (Ours) 85.40 (+1.40) 68. 51.40 52.60 (+1.20) 69.51 (+0.61) 53.40 (+2.00) 68.29 (-0.61) 52.80 (+1.40) 70.73 (+1.83) 85.40 Untrained 85.80 (+0.40) MAPoRL i 88.20 (+2.80) TTRL CoMAS (Ours) 87.20 (+1.80) 73.78 55.00 55.40 (+0.40) 75.61 (+1.83) 56.80 (+1.80) 73.78 (+0.00) 55.80 (+0.80) 77.44 (+3.66) 32.67 54.00 56.00 (+2.00) 34.07 (+1.40) 28.12 (+1.34) 61.40 (+0.00) 57.40 (+3.40) 34.47 (+1.80) 25.45 (-1.34) 61.60 (+0.20) 56.20 (+2.20) 34.67 (+2.00) 27.46 (+0.67) 62.40 (+1.00) 26.79 61.40 36.47 55.80 57.00 (+1.20) 39.08 (+2.61) 31.47 (+2.68) 63.20 (+0.00) 59.00 (+3.20) 38.48 (+2.00) 27.23 (-1.56) 63.80 (+0.60) 59.20 (+3.40) 37.68 (+1.20) 29.69 (+0.89) 65.60 (+2.40) 28.79 63. Untrained MAPoRL TTRL CoMAS (Ours) 72.40 (+19.80) 45.80 (+7.40) 50.61 (+10.98) 38.00 (+8.20) 22.85 (+2.61) 22.99 (+6.70) 50.60 (+13.20) 29.80 34.60 (+4.80) 20.64 (+0.40) 21.65 (+5.36) 40.40 (+3.00) 34.00 (-3.40) 22.80 (-7.00) 52.60 50.00 (-2.60) 41.00 (-11.60) 39.63 39.63 (+0.00) 23.17 (-16.46) 38.40 37.40 (-1.00) 37.80 (-0.60) 14.06 (-2.23) 19.64 (-0.60) 20.24 16.29 37.40 b 84.60 Untrained 85.40 (+0.80) MAPoRL 86.20 (+1.60) TTRL CoMAS (Ours) 85.20 (+0.60) 71.34 55.00 53.60 (-1.40) 74.39 (+3.05) 55.20 (+0.20) 73.78 (+2.44) 55.40 (+0.40) 77.44 (+6.10)"
        },
        {
            "title": "4.1.2 EVALUATION DETAILS",
            "content": "38.68 54.80 55.60 (+0.80) 39.88 (+1.20) 31.47 (+3.12) 64.80 (+2.00) 58.00 (+3.20) 37.88 (-0.80) 29.02 (+0.67) 64.00 (+1.20) 55.60 (+0.80) 39.08 (+0.40) 29.91 (+1.56) 65.20 (+2.40) 28.35 62.80 Baselines. We compare against three baselines: untrained agents as weak baseline, MAPoRL (Park et al., 2025), and TTRL (Zuo et al., 2025) as strong baselines. MAPoRL implements multi-agent RL within debate framework (Du et al., 2023; Liang et al., 2023), but we replace its specialized reward model with rule-based verifier. TTRL leverages majority voting to generate pseudo-labels for test-time training. For coding and science tasks where majority voting cannot produce reliable answers, we implement dummy rewards to ensure TTRL can be applied to our training dataset. Benchmarks. We conduct evaluation on multiple standard benchmarks, including GSM8K (Cobbe et al., 2021) and MATH-500 (Hendrycks et al., 2020) for math, HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for coding, SciBench (Wang et al., 2023) and GPQA (Rein et al., 2024) for science, and MMLU (Hendrycks et al., 2020) for general knowledge. For evaluation efficiency, we randomly retain 500 samples if the benchmark contains more than 500 tasks. Setups. We evaluate the trained agents on various inference setups, including Vanilla (i.e., direct inference) and Consistency (Wang et al., 2022) for single-agent setups, and AutoGen (Li et al., 2023; Wu et al., 2024b) and Debate (Du et al., 2023; Liang et al., 2023) for multi-agent setups. All the setups follow the standard implementation from MASLab (Ye et al., 2025)."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "Figure 3: Training dynamics of CoMAS. The left figure shows the curve of the average response length of each agent during training. The right figure shows the curve of the average normalized reward of each agent during training. These trends together indicate that CoMAS achieves stable and effective training process that improves the capabilities of agents."
        },
        {
            "title": "CoMAS",
            "content": "We follow the settings described in Section 4.1 to conduct the main experiments. Table 1 presents the performance of the agents trained by our CoMAS framework under different benchmarks and setups, together with comprehensive comparison with the selected baselines. In terms of the single-agent setups, CoMAS consistently improves over the untrained base model across all benchmarks and remains competitive with MAPoRL, which relies on external reward signals from the rule-based verifier. Under the Vanilla setup, CoMAS yields the best results on GSM8K (85.40%), HumanEval (70.73%), SciBench (34.67%), and MMLU (62.40%), while being close to the best on the remaining datasets. Under the Consistency setup, CoMAS again improves all benchmarks and reaches the best scores on HumanEval (77.44%), MBPP (59.20%), and MMLU (65.60%), with comparable performance on the remaining benchmarks. Although TTRL attains outstanding performance on GSM8K (88.20%) and MATH-500 (56.80%), it fails on HumanEval (73.78%) and GPQA (27.33%), which demonstrates its specialized effectiveness on math tasks and reveals its significant limitation on general tasks. For the multi-agent setups, CoMAS demonstrates clear advantages in both stability and absolute gains. Under the AutoGen setup, training with TTRL collapses markedly and MAPoRL brings mixed or negative changes, while CoMAS delivers large improvements over untrained agents on every benchmark, especially on GSM8K (72.40%), MATH-500 (45.80%), HumanEval (50.61%), MBPP (38.00%), and MMLU (50.60%). Under the Debate setup, all the methods benefit from the strong collaborative pattern, but CoMAS attains the best or near-best results overall, especially leading on MATH-500 (55.40%), HumanEval (77.44%), and MMLU (65.20%), while TTRL slightly edges out GSM8K and MBPP and MAPoRL tops SciBench and GPQA. These results indicate that CoMAS provides robust and generalizable gains across interaction regimes, particularly excelling in multi-agent collaboration where alternative methods can be fragile. Figure 3 provides insight into the training dynamics of CoMAS. The left figure tracks the average response length per agent, showing consistent growth throughout training. This increase reflects the agents improving capabilities in both solution generation and evaluation, aligning with the established patterns in LLM reasoning (Yeo et al., 2025). The right figure displays the average normalized reward for each agent. Despite mid-training fluctuations, rewards consistently hover around 0.5 and eventually converge to similar values across agents. This convergence demonstrates that our adversarial interaction reward design successfully creates stable training environment."
        },
        {
            "title": "4.3.1 REWARD FORMULATION",
            "content": "Figure 4: Results of the ablation study for reward formulation. The left figure compares the performance across the original CoMAS and two variants (without evaluation and without scoring). The right figure shows the average normalized rewards during the training process. These results indicate that the adversarial reward design is of key importance for the success of CoMAS. To validate the effectiveness of our adversarial reward design described in Section 3.2, we created two CoMAS variants by removing either the evaluation or scoring steps. In the first variant (without evaluation), rewards come directly from the agents themselves acting as judges. In the second variant (without scoring), evaluation steps provide direct judgments on solutions with supporting ratio as the reward signal, while evaluation rewards are based on mutual consistency. Figure 4 presents our"
        },
        {
            "title": "CoMAS",
            "content": "findings, with the left panel comparing performance across variants and the right panel showing normalized reward trajectories during training. We evaluated performance by averaging scores across all benchmarks under both Vanilla and Consistency setups. The left panel reveals that both variants underperform compared to the untrained base model, highlighting that our carefully designed adversarial reward formulation is critical to CoMASs success rather than any casual reward design. To understand the underlying mechanisms, we analyzed the reward dynamics shown in the right panel. The original CoMAS maintains stable rewards around 0.5, while both variants start with high rewards around 0.8 and then follow divergent trajectories. When evaluation is removed, the reward curve unexpectedly decreases over time, indicating that agents become increasingly strict judges rather than generating useful reward signals. When scoring is removed, rewards steadily increase toward the maximum value of 1.0, revealing reward hacking strategy where agents unanimously support all solutions, allowing both solution and evaluation steps to receive maximum rewards."
        },
        {
            "title": "4.3.2 FRAMEWORK SCALABILITY",
            "content": "Figure 5: Results of the ablation study for framework scalability. The left figure shows how the number of agents affects the performance of CoMAS across different setups. The right figure compares the performance between homogeneous and heterogeneous agent settings. These results demonstrate the underlying scalability of CoMAS with the number and diversity of agents. To investigate the scalability of CoMAS, we examined how performance changes with varying numbers of agents. Using identical base models for all agents, we conducted experiments with = 1, 2, 4 agents. The left panel of Figure 5 compares performance across these settings, with results averaged across all benchmarks. Our findings show that performance generally improves as the number of agents increases. This pattern is particularly pronounced in the Consistency and Debate setups, where single-agent CoMAS shows performance decreases of 0.19% and 0.16%, respectively, while four-agent CoMAS achieves substantial gains of 2.02% and 1.75%. Similar smooth performance improvements are observed in the Vanilla and AutoGen setups. These results highlight the critical role of multi-agent interactions and demonstrate the inherent scalability of the CoMAS framework. We also evaluated the impact of agent diversity within CoMAS. For this experiment, we maintained = 2 agents but used different base models: Qwen2.5-3B-Instruct for one agent and Llama-3.23B-Instruct for the other. The right panel of Figure 5 compares performance between homogeneous and heterogeneous settings, with results averaged across all benchmarks. Heterogeneous agents consistently outperformed their homogeneous counterparts, with particularly notable improvements in the Vanilla (2.21%), Consistency (2.78%), and Debate (2.13%) setups. These findings suggest that diverse knowledge and capabilities from different base models enhance overall performance, with CoMAS effectively encouraging agents to learn from each others strengths. This points to the potential for even greater performance gains with more diverse agent settings."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we address fundamental research question on self-evolution of LLM-based agents inspired by human intelligence: Can agents achieve self-evolution purely through inter-agent interactions within multi-agent system, without relying on external reward signals? To answer this question, we introduce CoMAS, novel framework that performs interactions composed of solu-"
        },
        {
            "title": "CoMAS",
            "content": "tion proposal, evaluation, and scoring, derives intrinsic rewards from discussion dynamics via an LLM-as-a-judge mechanism, and optimizes each agents policy through an RL algorithm. Across multiple benchmarks and collaboration settings, CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance in most evaluation scenarios. Our ablation studies demonstrate that interaction-based rewards are essential for preventing training collapse and reward hacking. Furthermore, we show that performance scales positively with both the number and diversity of agents, highlighting the frameworks scalability potential. These findings establish CoMAS as novel and effective paradigm for self-evolution in LLM-based agents and open promising avenues for future research in autonomous multi-agent learning systems."
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This work draws inspiration from the collaborative intelligence observed in human society to develop the CoMAS framework. While our approach is motivated by natural social dynamics, our implementation and experimental evaluation are strictly confined to standard LLM reasoning tasks across domains of math, coding, and science, rather than simulating real-world social interactions or scenarios. Given this limited scope, the research presented in this paper does not directly raise ethical concerns. However, we acknowledge that the multi-agent co-evolution paradigm introduced in this work could potentially be extended to real-world applications that may have broader implications for human welfare. We therefore encourage readers and future researchers to carefully consider the ethical implications when extending this paradigm beyond academic benchmarks."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "We have made every effort to ensure the reproducibility of our work. We provide comprehensive details throughout this paper and the appendices. Section 3 presents detailed description of our CoMAS framework, including theoretical foundations and implementation workflow. Section 4.1 covers experimental parameters, dataset construction, and evaluation settings, with additional specifications in Appendix B. Complete experimental results are documented in Appendix C. We also include prompt templates in Appendix and an example trajectory in Appendix to facilitate understanding and replication. Our dataset and code are released at https://github.com/xxyQwQ/CoMAS."
        },
        {
            "title": "REFERENCES",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Brigid Barron. When smart groups fail. The journal of the learning sciences, 2003. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023. arXiv preprint arXiv:2305.14325, 2023. Andrew Estornell, Jean-Francois Ton, Yuanshun Yao, and Yang Liu. Acc-collab: An actor-critic approach to multi-agent llm collaboration. arXiv preprint arXiv:2411.00053, 2024. Andrew Estornell, Jean-Francois Ton, Muhammad Faaiz Taufiq, and Hang Li. How to train leader: Hierarchical reasoning in multi-agent llms. arXiv preprint arXiv:2507.08960, 2025. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407, 2025. Evan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios Angelopoulos, and Ion Stoica. Prompt-to-leaderboard. arXiv preprint arXiv:2502.14855, 2025. Hongcheng Gao, Yue Liu, Yufei He, et al. Flowreasoner: Reinforcing query-level meta-agents. arXiv preprint arXiv:2504.15257, 2025a. Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025b. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Guo et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021."
        },
        {
            "title": "CoMAS",
            "content": "Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for In The Twelfth International Conference on Learning multi-agent collaborative framework. Representations, 2023. Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024a. Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. arXiv preprint arXiv:2501.03262, 2025a. Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, et al. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. arXiv preprint arXiv:2505.23885, 2025b. Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024b. Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hullermeier. survey of reinforcement learning from human feedback. arXiv preprint arXiv:2312.14925, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind exploration of large language model society. Advances in Neural Information Processing Systems, 2023. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth, 2024. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multiagent debate. arXiv preprint arXiv:2305.19118, 2023. Junwei Liao, Muning Wen, Jun Wang, and Weinan Zhang. Marft: Multi-agent reinforcement finetuning. arXiv preprint arXiv:2504.16129, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Shuo Liu, Zeyu Liang, Xueguang Lyu, and Christopher Amato. Llm collaboration with multi-agent reinforcement learning. arXiv preprint arXiv:2508.04652, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025b. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Improve mathematical reasoning in language models by Lei Shu, Yun Zhu, Lei Meng, et al. automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Xiaowen Ma, Chenyang Lin, Yao Zhang, Volker Tresp, and Yunpu Ma. Agentic neural networks: Self-evolving multi-agent systems via textual backpropagation. arXiv preprint arXiv:2506.09046, 2025a. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025b. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 2024."
        },
        {
            "title": "CoMAS",
            "content": "Marvin Minsky. Society of mind. Simon and Schuster, 1986. Sumeet Ramesh Motwani, Chandler Smith, Rocktim Jyoti Das, et al. Malt: Improving reasoning with multi-agent llm training. arXiv preprint arXiv:2412.01928, 2024. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 2022. Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim. Maporl: Multi-agent post-co-training for collaborative large language models with reinforcement learning. arXiv preprint arXiv:2502.18439, 2025. Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi Wang, Di Zhou, et al. Agentsociety: Large-scale simulation of llmdriven generative agents advances understanding of human behaviors and society. arXiv preprint arXiv:2502.08691, 2025. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, et al. Scaling large language model-based multi-agent collaboration. arXiv preprint arXiv:2406.07155, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, et al. Chemagent: Self-updating library in large language models improves chemical reasoning. arXiv preprint arXiv:2501.06590, 2025. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387, 2024. Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry OSullivan, and Hoang Nguyen. Multi-agent collaboration mechanisms: survey of llms. arXiv preprint arXiv:2501.06322, 2025. Ziyu Wan, Yunxiang Li, Xiaoyu Wen, et al. Rema: Learning to meta-think for llms with multi-agent reinforcement learning. arXiv preprint arXiv:2503.09501, 2025. Junlin Wang, Jue Wang, Ben Athiwaratkun, et al. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692, 2024. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023."
        },
        {
            "title": "CoMAS",
            "content": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. β-dpo: Direct preference optimization with dynamic β. Advances in Neural Information Processing Systems, 2024a. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multiagent conversations. In First Conference on Language Modeling, 2024b. Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen, Martz Ma, Bowen Dong, et al. Oasis: Open agent social interaction simulations with one million agents. arXiv preprint arXiv:2411.11581, 2024b. Rui Ye, Keduan Huang, Qimin Wu, Yuzhu Cai, Tian Jin, Xianghe Pang, Xiangrui Liu, Jiaqi Su, Chen Qian, Bohan Tang, et al. Maslab: unified and comprehensive codebase for llm-based multi-agent systems. arXiv preprint arXiv:2505.16988, 2025. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chainof-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. Evoagent: Towards automatic multi-agent generation via evolutionary algorithms. arXiv preprint arXiv:2406.14228, 2024a. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024b. Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv preprint arXiv:2410.11782, 2024a. Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-memory: Tracing hierarchical memory for multi-agent systems. arXiv preprint arXiv:2506.07398, 2025a. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for llms: survey. arXiv preprint arXiv:2509.02547, 2025b. Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. Darwin godel machine: Openended evolution of self-improving agents. arXiv preprint arXiv:2505.22954, 2025c. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762, 2024b."
        },
        {
            "title": "CoMAS",
            "content": "Kaiyan Zhang, Runze Liu, Xuekai Zhu, Kai Tian, Sihang Zeng, Guoli Jia, Yuchen Fan, Xingtai Lv, Yuxin Zuo, Che Jiang, Ziyang Liu, Jianyu Wang, Yuru Wang, Ruotong Zhao, Ermo Hua, Yibo Wang, Shijie Wang, Junqi Gao, Xinwei Long, Youbang Sun, Zhiyuan Ma, Ganqu Cui, Lei Bai, Ning Ding, Biqing Qi, and Bowen Zhou. Marti: framework for multi-agent llm systems reinforced training and inference, 2025d. URL https://github.com/TsinghuaC3I/ MARTI. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025e. Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang. Data-copilot: Bridging billions of data and humans with autonomous workflow. arXiv preprint arXiv:2306.07209, 2023. Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: hierarchical multi-agent framework for general-purpose task solving. arXiv preprint arXiv:2506.12508, 2025f. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025g. Wanjia Zhao, Mert Yuksekgonul, Shirley Wu, and James Zou. Sirius: Self-improving multi-agent systems via bootstrapped reasoning. arXiv preprint arXiv:2502.04780, 2025a. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 2023. Jialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, Yicheng Chen, Yang Zhang, Wei Zhou, Jinjie Gu, and Lei Zou. comprehensive survey of reward models: Taxonomy, applications, challenges, and future. arXiv preprint arXiv:2504.12328, 2025. Heng Zhou, Hejia Geng, Xiangyuan Xue, et al. Reso: reward-driven self-organizing llm-based multi-agent system for reasoning tasks. arXiv preprint arXiv:2503.02390, 2025. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jurgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning, 2024. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "A THE USE OF LLMS",
            "content": "We employ LLMs exclusively for manuscript refinement. Specifically, LLMs are utilized to correct typographical errors, address grammatical issues, and enhance linguistic expression. All content generated or refined through LLMs has been rigorously reviewed and validated by the authors. LLMs are not involved in any other aspects of this research, including conceptual development, data collection, code implementation, experimental design, or result interpretation."
        },
        {
            "title": "B DETAILS FOR PARAMETER SETTINGS",
            "content": "The core parameter settings have been described in Section 4.1.1. Here, we provide all the details for parameter settings when implementing our CoMAS framework. Table 2: The detailed parameter settings when implementing our CoMAS framework."
        },
        {
            "title": "Setting",
            "content": "Qwen2.5-3B-Instruct 4 8 1 2 28672 4096 1.0 0.7 1.0 0.2 0.0 1 4 64 2 64 2 AdamW 1e-6 0.0 1.0 True True True True True True"
        },
        {
            "title": "C DETAILS FOR EXPERIMENTAL RESULTS",
            "content": "Due to space constraints, our ablation studies in Section 4.3 only provide averages over all benchmarks across different setups. Here we present the details for experimental results. Table 3 shows the experimental results of using simplified reward formulations in CoMAS. Table 4 shows the experimental results of using different numbers of agents in CoMAS. Table 5 shows the experimental results of using homogeneous and heterogeneous agents in CoMAS. Table 3: Detailed experimental results for the ablation study on reward formulation. The results with performance improvements are highlighted in green, drops in red, and neutral changes in gray. Method GSM8K MATH-500 HumanEval Dataset MBPP SciBench GPQA MMLU i 84. Untrained CoMAS (Ours) 85.40 (+1.40) 52.80 (+1.40) 70.73 (+1.83) 56.20 (+2.20) 34.67 (+2.00) 27.46 (+0.67) 62.40 (+1.00) 55.00 (+1.00) 32.87 (+0.20) 27.01 (+0.22) 62.20 (+0.80) w/o Evaluation 82.60 (-1.40) 28.12 (+1.34) 60.60 (-0.80) 83.60 (-0.40) w/o Scoring 63.41 (-5.49) 68.90 (+0.00) 55.40 (+1.40) 31.66 (-1.00) 50.60 (-0.80) 50.60 (-0.80) 54.00 32.67 26. 51.40 61.40 68.90 Untrained t n 85.40 55. 73.78 55.80 36.47 28.79 63.20 CoMAS (Ours) 87.20 (+1.80) 55.80 (+0.80) 77.44 (+3.66) 59.20 (+3.40) 37.68 (+1.20) 29.69 (+0.89) 65.60 (+2.40) 63.20 (+0.00) w/o Evaluation 85.00 (-0.40) 29.91 (+1.12) 62.40 (-0.80) w/o Scoring 53.60 (-1.40) 85.80 (+0.40) 54.20 (-0.80) 55.20 (-0.60) 56.00 (+0.20) 36.27 (-0.20) 71.34 (-2.44) 71.95 (-1.83) 36.87 (+0.40) 28.35 (-0.45) Table 4: Detailed experimental results for the ablation study on agent number. The results with performance improvements are highlighted in green, drops in red, and neutral changes in gray."
        },
        {
            "title": "Method",
            "content": "GSM8K MATH-500 HumanEval"
        },
        {
            "title": "Dataset\nMBPP",
            "content": "SciBench GPQA MMLU i 84.00 Untrained CoMAS (1-Agent) 83.20 (-0.80) CoMAS (2-Agent) 83.40 (-0.60) CoMAS (4-Agent) 85.40 (+1.40) 51.40 68.90 69.51 (+0.61) 50.40 (-1.00) 52.80 (+1.40) 67.68 (-1.22) 52.80 (+1.40) 70.73 (+1.83) 54.00 32.67 33.07 (+0.40) 27.90 (+1.12) 62.80 (+1.40) 53.00 (-1.00) 54.00 (+0.00) 33.87 (+1.20) 27.46 (+0.67) 62.20 (+0.80) 56.20 (+2.20) 34.67 (+2.00) 27.46 (+0.67) 62.40 (+1.00) 26.79 61.40 85.40 Untrained t n CoMAS (1-Agent) 84.40 (-1.00) CoMAS (2-Agent) 85.80 (+0.40) CoMAS (4-Agent) 87.20 (+1.80) 73.78 55.00 55.20 (+0.20) 71.95 (-1.83) 55.40 (+0.40) 74.39 (+0.61) 55.80 (+0.80) 77.44 (+3.66) 36.47 63.20 55.80 66.20 (+3.00) 55.80 (+0.00) 37.88 (+1.40) 25.67 (-3.12) 56.40 (+0.60) 38.28 (+1.80) 29.02 (+0.22) 65.20 (+2.00) 59.20 (+3.40) 37.68 (+1.20) 29.69 (+0.89) 65.60 (+2.40) 28. 52.60 Untrained t 29.80 CoMAS (1-Agent) 73.40 (+20.80) 44.20 (+5.80) 46.34 (+6.71) 39.20 (+9.40) 21.64 (+1.40) 18.30 (+2.01) 46.60 (+9.20) CoMAS (2-Agent) 71.20 (+18.60) 45.80 (+7.40) 50.61 (+10.98) 37.40 (+7.60) 24.65 (+4.41) 22.54 (+6.25) 50.40 (+13.00) CoMAS (4-Agent) 72.40 (+19.80) 45.80 (+7.40) 50.61 (+10.98) 38.00 (+8.20) 22.85 (+2.61) 22.99 (+6.70) 50.60 (+13.20) 16.29 38.40 37. 20.24 39.63 b 84.60 Untrained CoMAS (1-Agent) 85.20 (+0.60) CoMAS (2-Agent) 85.60 (+1.00) CoMAS (4-Agent) 85.20 (+0.60) 71. 55.00 55.00 (+0.00) 70.73 (-0.61) 56.20 (+1.20) 72.56 (+1.22) 55.40 (+0.40) 77.44 (+6.10) 38.68 28.35 54.80 55.40 (+0.60) 36.67 (-2.00) 29.46 (+1.12) 62.00 (-0.80) 56.60 (+1.80) 38.88 (+0.20) 29.91 (+1.56) 64.00 (+1.20) 55.60 (+0.80) 39.08 (+0.40) 29.91 (+1.56) 65.20 (+2.40) 62.80 Table 5: Detailed experimental results for the ablation study on agent diversity. The results with performance improvements are highlighted in green, drops in red, and neutral changes in gray. Method GSM8K MATH-"
        },
        {
            "title": "HumanEval",
            "content": "Dataset MBPP"
        },
        {
            "title": "MMLU",
            "content": "a Untrained"
        },
        {
            "title": "84.00\nHomogeneous\n83.40 (-0.60)\nHeterogeneous 85.20 (+1.20)",
            "content": "l a Untrained"
        },
        {
            "title": "85.40\nHomogeneous\n85.80 (+0.40)\nHeterogeneous 87.00 (+1.60)",
            "content": "s o 68.90 51.40 52.80 (+1.40) 67.68 (-1.22) 53.20 (+1.80) 73.78 (+4.88) 73.78 55.00 55.40 (+0.40) 74.39 (+0.61) 57.40 (+2.40) 76.83 (+3.05) 32. 54.00 54.00 (+0.00) 33.87 (+1.20) 27.46 (+0.67) 62.20 (+0.80) 58.00 (+4.00) 34.47 (+1.80) 28.12 (+1.34) 61.80 (+0.40) 26.79 61.40 36.47 55.80 56.40 (+0.60) 38.28 (+1.80) 29.02 (+0.22) 65.20 (+2.00) 60.20 (+4.40) 38.48 (+2.00) 30.58 (+1.79) 67.40 (+4.20) 63. 28.79 Untrained t Homogeneous Heterogeneous 63.00 (+10.40) 40.40 (+2.00) 48.17 (+8.54) 52.60 71.20 (+18.60) 45.80 (+7.40) 50.61 (+10.98) 37.40 (+7.60) 24.65 (+4.41) 22.54 (+6.25) 50.40 (+13.00) 37.20 (+7.40) 24.65 (+4.41) 21.21 (+4.91) 46.20 (+8.80) 39.63 37. 16.29 38.40 20.24 29.80 Untrained e D"
        },
        {
            "title": "84.60\nHomogeneous\n85.60 (+1.00)\nHeterogeneous 86.20 (+1.60)",
            "content": "71.34 55.00 56.20 (+1.20) 72.56 (+1.22) 56.00 (+1.00) 73.78 (+2.44) 17 38.68 54.80 56.60 (+1.80) 38.88 (+0.20) 29.91 (+1.56) 64.00 (+1.20) 57.80 (+3.00) 40.28 (+1.60) 31.03 (+2.68) 65.40 (+2.60) 62. 28."
        },
        {
            "title": "D PROMPT TEMPLATES",
            "content": "In this section, we provide the prompt templates used in our CoMAS framework. Note that they may vary slightly across different task domains. Here we present the science tasks as an example."
        },
        {
            "title": "Prompt Template for Solution",
            "content": "The problem is presented as follows: {problem} Current discussion on the problem is presented as follows for your reference: {discussion} Provide your step-by-step solution to the problem. The final answer should be (cid:44) (cid:44) decimal number enclosed within boxed{{}}, e.g. boxed{{1}}, boxed{{0.1}}, or boxed{{0.01}}. The unit part given in the problem should not be enclosed."
        },
        {
            "title": "Prompt Template for Evaluation",
            "content": "The problem is presented as follows: {problem} Current discussion on the problem is presented as follows for your reference: {discussion} You are required to evaluate the following solution: {solution} You should point out every possible error and defect in the solution. Provide your (cid:44) (cid:44) (cid:44) evaluation by listing all the mistakes you find in the solution, specifying what is wrong and why. Keep your evaluation concise and clear. Avoid using lot of words to retell the reasoning process."
        },
        {
            "title": "Prompt Template for Scoring",
            "content": "The problem is presented as follows: {problem} You are required to score the following solution: {solution} The evaluation on the solution is presented as follows: {evaluation} evaluation is effective. You should consider the rationality of the evaluation and score the solution. The score should be an integer between 1 and 3 with the following standards: (cid:44) 3: The solution is completely correct, and none of the mistakes mentioned in the (cid:44) 2: Some minor mistakes mentioned in the evaluation do exist, but they do not affect (cid:44) 1: Some of the mistakes mentioned in the evaluation are fatal, which directly lead to (cid:44) the overall correctness. an incorrect answer. Your score should be enclosed within \"<score>\" and \"</score>\" tags. You should also briefly explain the reasons before providing your score. Keep your decision (cid:44) concise and clear. Avoid using lot of words to retell the reasoning process. (cid:44) For example: The calculation error mentioned in the evaluation cannot be ignored and leads to an (cid:44) <score>1</score> incorrect answer."
        },
        {
            "title": "E EXAMPLE TRAJECTORY",
            "content": "In this section, we present an example trajectory generated by our CoMAS framework to illustrate the complete pipeline and enhance readers understanding of the process."
        },
        {
            "title": "Example Trajectory",
            "content": "Question: Suppose copper wire (density = 8.96 g/cmˆ3) has diameter of 0.25 mm. If sample of (cid:44) The unit of the answer should be \"cm\". this copper wire has mass of 22 g, how long is the wire? Solution: To find the length of the copper wire, we need to follow these steps: 1. Calculate the volume of the copper wire: 8.96 g/cm³. The formula to find the volume of an object is: = frac{text{mass}}{text{density}} The mass of the copper wire is given as 22 g, and the density of copper is given as (cid:44) (cid:44) Substituting the given values: = frac{22 text{ g}}{8.96 text{ g/cm}ˆ3} Performing the division: = 2.457 text{ cm}ˆ3 2. Determine the volume of single segment of the wire: rˆ2 The wire is cylindrical in shape. The volume of cylinder is given by: = pi (cid:44) where is the radius of the cylinder and is the height (or length) of the cylinder. The diameter of the wire is 0.25 mm, so the radius is half of the (cid:44) diameter: = frac{0.25 text{ mm}}{2} = 0.125 text{ mm} = 0.125 text{ cm} (cid:44) Substituting and into the volume formula: 2.457 text{ cm}ˆ3 = pi (0.125 text{ (cid:44) cm})ˆ2 3. Solve for the length h: 0.015625 text{ cm}ˆ2 times frac{2.457 text{ cm}ˆ3}{pi times 0.015625 text{ cm}ˆ2} First, calculate (0.125 text{ cm})ˆ2: (0.125 text{ cm})ˆ2 = 0.015625 text{ cm}ˆ2 Now, substitute this into the volume equation: 2.457 text{ cm}ˆ3 = pi times (cid:44) Solve for by dividing both sides by pi times 0.015625 text{ cm}ˆ2: = (cid:44) Using the approximation pi approx 3.14159: = frac{2.457 text{ cm}ˆ3}{3.14159 (cid:44) Perform the multiplication in the denominator: 3.14159 times 0.015625 approx (cid:44) Now, divide: = frac{2.457}{0.05000} approx 49.14 text{ cm} times 0.015625 text{ cm}ˆ2} 0.05000 Therefore, the length of the wire is boxed{49.14} cm. Evaluation: 0.025 cm, so the radius should be 0.0125 cm, not 0.125 cm. but the mass of the wire. The correct volume should be calculated as = frac{22 text{ g}}{8.96 text{ g/cm}ˆ3} approx 2.457 text{ cm}ˆ3. The solution contains several errors: 1. The volume calculation is incorrect because the given mass (22 g) is not the volume, (cid:44) (cid:44) 2. The radius calculation is incorrect. The diameter is given as 0.25 mm, which is (cid:44) 3. The formula for the volume of cylinder is = pi rˆ2 h, but the solution (cid:44) 4. The final calculation for is incorrect. It should be = frac{2.457 text{ (cid:44) (cid:44) These errors lead to an incorrect final length of the wire. cm}ˆ3}{pi times (0.0125 text{ cm})ˆ2} approx 49.14 text{ cm}, not = frac{2.457}{0.05000} approx 49.14 text{ cm}. incorrectly uses = pi rˆ2 without the term. Scoring: The solution is fundamentally correct but contains critical error in the final step (cid:44) (cid:44) (cid:44) <score>1</score> due to the incorrect value of pi. This error directly leads to an incorrect final length of the wire. Therefore, the score is 1, indicating that some of the mistakes mentioned in the attack are fatal. Solution: wire. 2.457 text{ cm}ˆ Let's go through the problem step-by-step to find the correct length of the copper (cid:44) Step 1: Calculate the Volume of the Wire The volume of the wire can be calculated using the formula: = frac{m}{rho} Substituting the given values: = frac{22 text{ g}}{8.96 text{ g/cm}ˆ3} approx (cid:44) Step 2: Relate the Volume to the Cross-Sectional Area and Length The volume of cylinder (which the wire can be considered as) is given by: = (cid:44) where is the cross-sectional area and is the length of the wire. The cross-sectional area of circular wire is: = pi left( (cid:44) Given the diameter of the wire is 0.25 mm, convert it to cm: text{diameter} = 0.25 (cid:44) So, the radius is: = frac{0.025 text{ cm}}{2} = 0.0125 text{ cm} Now, calculate the cross-sectional area A: = pi left( 0.0125 text{ cm} right)ˆ2 = (cid:44) pi times 0.00015625 text{ cm}ˆ2 approx 0.000490873852123405 text{ cm}ˆ2 frac{text{diameter}}{2} right)ˆ2 text{ mm} = 0.025 text{ cm} times"
        },
        {
            "title": "CoMAS",
            "content": "Step 3: Solve for the Length of the Wire Using the volume formula = times h, solve for h: = frac{V}{A} Substitute the values of and A: = frac{2.457 text{ cm}ˆ3}{0.000490873852123405 text{ cm}ˆ2} approx 499.9999999999999 text{ cm} (cid:44) Step 4: Round to Reasonable Number of Significant Figures Given the precision of the provided values, we can round the length to reasonable (cid:44) (cid:44) Thus, the length of the wire is: boxed{5000} number of significant figures. The length is already very close to 5000 cm, so we can simply state: approx 5000 text{ cm} Evaluation: converting the diameter to cm is unnecessary and complicates the calculation. The diameter is 0.25 mm, which is 0.025 cm, and the radius is 0.0125 cm. The cross-sectional area should be calculated as = pi (0.0125)ˆ2 approx 4.91 times 10ˆ{-5} text{ cm}ˆ2. Here are the mistakes in the solution: 1. Volume Calculation: The volume calculation is correct, but the intermediate step of (cid:44) (cid:44) (cid:44) (cid:44) 2. Length Calculation: The length of the wire should be calculated as = frac{V}{A} = (cid:44) (cid:44) (cid:44) 3. Units Consistency: The final answer should be rounded to sensible number of (cid:44) (cid:44) frac{2.457 text{ cm}ˆ3}{4.91 times 10ˆ{-5} text{ cm}ˆ2} approx 49999.99999999999 text{ cm}. Rounding to reasonable number of significant figures, the length is approximately 50000 cm or 5000 cm. significant figures. Given the precision of the provided values, 5000 cm is reasonable and accurate answer. So, the correct length of the wire is: boxed{5000} Scoring: The solution contains minor mistakes that are not fatal to the overall correctness. The volume calculation and cross-sectional area calculation are correct, and the length (cid:44) calculation, while slightly rounded, is reasonable and does not affect the final (cid:44) answer. (cid:44) <score>2</score> Solution: 0.0125 , text{cm} text{mm} = 0.025 , text{cm} mass and diameter. Let's follow the steps to find the solution. area of circle: = pi rˆ2 = pi (0.0125)ˆ2 , text{cm}ˆ2 approx 0.00050685 , text{cm}ˆ2 To solve this problem, we need to determine the length of the copper wire given its (cid:44) Step 1: Determine the Volume of the Copper Wire First, convert the diameter from millimeters to centimeters: text{Diameter} = 0.25 , (cid:44) The radius (r) of the wire is half of the diameter: = frac{0.025}{2} , text{cm} = (cid:44) The cross-sectional area (A) of the wire can be calculated using the formula for the (cid:44) (cid:44) Step 2: Calculate the Length of the Wire Using the density formula: rho = frac{m}{V} Rearrange the formula to solve for the volume (V): = frac{m}{rho} Substitute the given values: = frac{22 , text{g}}{8.96 , text{g/cm}ˆ3} approx (cid:44) Step 3: Relate Volume to Length The volume of cylinder (wire) is given by: = times Solve for the length (L): = frac{V}{A} = frac{2.455 , text{cm}ˆ3}{0.00050685 , (cid:44) Final Answer: boxed{4840} text{cm}ˆ2} approx 4840 , text{cm} 2.455 , text{cm}ˆ3 Evaluation: Specifically: The solution contains significant error in the calculation of the volume of the wire. (cid:44) The volume calculation is incorrect because the cross-sectional area = pi rˆ2 should be squared, but the final volume calculation does not reflect this. The correct (cid:44) area is = pi (0.0125)ˆ2 approx 0.00049087 , text{cm}ˆ2, and the correct (cid:44) volume is = 0.00049087 times 2.455 approx 0.00121 , text{cm}ˆ3. (cid:44) The length should then be calculated as = frac{0.00121 , text{cm}ˆ3}{0.00050685 (cid:44) Therefore, the correct length of the wire is approximately 2.38 , text{cm}. , text{cm}ˆ2} approx 2.38 , text{cm}. Scoring: The original solution is completely correct, and none of the mistakes mentioned in the attack are effective. The volume and length calculations are accurate, and the (cid:44) final length of the wire is 4840 , text{cm}. (cid:44) <score>3</score>"
        }
    ],
    "affiliations": [
        "Dalian University of Technology",
        "National University of Singapore",
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong",
        "University of California San Diego",
        "University of Georgia",
        "University of Oxford",
        "University of Science and Technology of China"
    ]
}