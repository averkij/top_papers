{
    "paper_title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
    "authors": [
        "Yuhao Yang",
        "Zhen Yang",
        "Zi-Yi Dou",
        "Anh Nguyen",
        "Keen You",
        "Omar Attia",
        "Andrew Szot",
        "Michael Feng",
        "Ram Ramrakhya",
        "Alexander Toshev",
        "Chao Huang",
        "Yinfei Yang",
        "Zhe Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency."
        },
        {
            "title": "Start",
            "content": "UltraCUA: Foundation Model for Computer Use Agents with Hybrid Action Yuhao Yangh, Zhen Yanga, Zi-Yi Doua, Anh Nguyena, Keen Youa, Omar Attiaa, Andrew Szota, Michael Fenga, Ram Ramrakhyaa, Alexander Tosheva, Chao Huangh, Yinfei Yanga, Zhe Gana aApple, hThe University of Hong Kong Senior Authors Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, foundation model that bridges this gap through hybrid actionseamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) synthetic data engine producing 17,000+ verifiable tasks spanning real-world computer-use scenarios; (3) large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency. Through this work, we establish scalable paradigm that effectively bridges primitive low-level GUI interactions and high-level programmatic intelligence, paving the way for more robust, efficient, and unified computer-use agents that can adapt to diverse environments and complex user tasks in the real world. Date: October 21, 2025 5 2 0 O 0 2 ] . [ 1 0 9 7 7 1 . 0 1 5 2 : r (a) OSWorld and WindowsAgentArena (b) Existing Agents v.s. UltraCUA Figure 1 (a) UltraCUAs performance on OSWorld and WindowsAgentArena; (b) Comparison between existing GUI Agents and UltraCUA. Pure low-level actions lead to cascade errors, while UltraCUA is faster and stronger."
        },
        {
            "title": "Introduction",
            "content": "Computer-use automation has emerged as critical capability for enabling autonomous agents to interact with the vast ecosystem of desktop and web applications that humans use daily (Hong et al., 2023; Shaw et al., 2024; Zhang et al., 2023). However, current computer-use agents (CUAs) face fundamental limitation: 1 they operate exclusively through primitive actions such as clicking, typing, and scrolling (Rawles et al., 2024; Koh et al., 2024). This constraint creates significant performance gap compared to agents that leverage rich programmatic interfacesAPIs, MCP servers, and toolsto accomplish complex tasks efficiently (Qin et al., 2023b; Schick et al., 2023b). The reliance on primitive actions introduces critical challenges. First, lengthy execution chains accumulate errors that cascade into failuresa single misplaced click can derail an entire task (Zheng et al., 2024; Yan et al., 2023). Second, operations that could be accomplished with single programmatic call require dozens of GUI actions, creating performance bottlenecks. For example, extracting data from multiple spreadsheets requires traditional CUA to navigate menus, select cells individually, copy values, switch applications, and paste contenteach action potential failure point. In contrast, an agent with spreadsheet APIs could accomplish this reliably with far fewer operations. This efficiency gap is stark: while other agents leveraging programmatic interfaces exceed 80% success on benchmarks like GAIA (Mialon et al., 2024; Zhang et al., 2025), GUI-only computer-use agents remain fundamentally limited, motivating our unified approach that combines GUI generality with programmatic efficiency. In this paper, we bridge this capability gap through hybrid action, seamlessly integrating GUI primitives with high-level programmatic tool calls. Rather than treating these as mutually exclusive options, our approach enables strategic combination of both modes. Agents learn to leverage programmatic tool calls when they provide clear efficiency gains, while retaining GUI interactions for universal coverage and fine-grained control. To summarize, our technical contributions include: An automated pipeline for collecting programmatic tools that scales beyond manually curated sets (Qin et al., 2023a; Tang et al., 2023). Our system extracts tools from software documentation, integrates open-sourced implementations, and employs coding agents to generate new tools on demand. This scalable pipeline produces hundreds of tools across diverse environments, from OSWorlds Ubuntu applications to WindowsAgentArenas Windows ecosystem. dual-pipeline synthetic data engine for verifiable computer-use task generation. Large-scale task synthesis for CUA training is challenging due to the complexity of verifying task completion in dynamic environments. To address this, we develop two complementary pipelines producing 16,000+ verified tasks. The first pipeline employs an instruction-first strategy where agents explore computer environments and propose tasks based on observed states, with trajectories verified by evaluator agents. The second pipeline uses an evaluator-first strategy, collecting atomic verification functions (e.g., checking Chrome URLs, verifying file paths, validating image attributes) from environments, then reprogramming (e.g., modifying parameters) and composing (e.g., combining multiple checks) them to create complex evaluation criteria. LLMs generate tasks satisfying these pre-defined evaluators, ensuring reliable trajectory assessment for training. large-scale hybrid action trajectory collection. Existing computer-use datasets contain only pure GUI action sequences, lacking demonstrations of programmatic tool integration. We collect 20,000+ successful trajectories by combining powerful planner model (OpenAI o3) with state-of-the-art grounding model (GTA1-7B (Yang et al., 2025a))a simple yet effective agentic framework. The planner selects between programmatic tool calls and low-level GUI actions based on task context, while the grounder ensures accurate GUI execution. This dataset enables training models to seamlessly alternate between action modes for optimal task completion. foundation agent model with hybrid action trained using the programmatic tools, synthetic tasks, and rollout trajectories described above. We train models at two scales (7B and 32B) through supervised fine-tuning on the high-quality trajectories from our collection, followed by online reinforcement learning on our verifiable synthetic tasks. This two-stage approach produces agents that effectively select between GUI primitives and programmatic tool calls based on task context. Experiments demonstrate substantial improvements over state-of-the-art CUAs. On OSWorld (Xie et al., 2024), our models achieve an average 22% relative improvement over their base models across both scales. Notably, out-of-domain evaluation on WindowsAgentArena (Bonatti et al., 2024)without any Windows-specific trainingshows our 7B model reaches 21.7% success rate, outperforming baselines trained on Windows data. These results validate that hybrid action provides consistent benefits across model scales and platforms. Our 2 Figure 2 An overview of UltraCUAs design. The agent adaptively switches between visual grounding and programmatic tool call, establishing the hybrid action mechanism. code, models, and datasets will be released to facilitate future research."
        },
        {
            "title": "2 Methodology",
            "content": "Our methodology comprises three key components for developing foundation CUA model with hybrid action. First, we build comprehensive collection of programmatic tools through an automated extraction pipeline. Second, we design dual-pipeline synthetic data engine that generates verifiable tasks for complex real-world computer use. Finally, we train our model via supervised fine-tuning on collected trajectories followed by online reinforcement learning on synthetic tasks."
        },
        {
            "title": "2.1 Automated Tool Collection for Hybrid Action\nThe foundation of our approach is hybrid action—seamlessly integrating primitive GUI actions with high-\nlevel programmatic tools. We define a “tool” as a high-level interface encapsulating sequences of computer-use\nactions, typically implemented as Python functions, keyboard shortcuts, or combinations of primitive actions\n(e.g., type, key combinations)—but excluding actions requiring visual grounding like clicks. Each tool is\nexposed to the model through a Python function signature with descriptive docstrings specifying parameters\nand functionality.",
            "content": "While GUI-only agents suffer from cascading failures in lengthy action sequences, programmatic interfaces alone cannot handle all computer interactions. Our hybrid approach enables agents to leverage programmatic tools for efficiency when available, while retaining GUI actions to ensure generalization. To build hybrid action space where tools cover diverse applications and usage scenarios, we developed an automated pipeline collecting hundreds of tools from the following three complementary sources. Tool details are also present in Appendix A.3. Extraction from Software Documentation. Application documentation contains expert knowledge, particularly keyboard shortcuts, that bypass tedious GUI sequences. For example, changing VS Codes color theme requires navigating File Preferences Color Theme with GUI actions. Our pipeline extracts the shortcut (Ctrl+K, Ctrl+T) from documentation and converts it into programmatic tool: vscode.set_theme(). This transforms fragile multi-step sequences into single, reliable operations. Integration of Open-Source Implementations. We incorporate existing programmatic tools from open-sourced frameworks, particularly leveraging implementations from AgentS2 (Agashe et al., 2025) and AgentStore (Jia et al., 2024). These tools transform complex GUI sequences into efficient programmatic calls. For example, this AgentS2 tool for spreadsheet manipulation replaces dozens of manual clicks with single function: 3 (cid:7) def et_cell_values ( self , cell_values : dict , app_name : str , sheet_name : str ) : \" \" \" Set multiple cell values in spreadsheet . Args : cell_values : { \" A2 \" : \" hello \" , \" B3 \" : 123.45} \" \" \" return T_ L _ U _ . format ( (cid:6) cell_values = cell_values , app_name = app_name , sheet_name = sheet_name ) Inspired by CoACT-1 (Song et al., 2025), we adopt the Automatic Scaling with Coding Agents. multi-agent paradigm where an orchestrator dynamically delegates subtasks to either GUI operator or coding agent that executes Python/Bash scripts. This allows bypassing inefficient GUI sequences through direct programmatic execution. We extend this by mining the coding agents trajectories for reusable tools: when the coding agent solves subtasks programmatically, we employ an automatic LLM workflow to extract and refine these solutions into parameterized functions, with reflection steps and automated unit testing to ensure correctness. For example, from trajectory where the coding agent modifies VS Code settings via script, we extract: (cid:7) def _ _ e _ b i ( key : str , command : str , when : str = \" \" ) : \" \" \" Create or update VS Code keybinding . Args : key : \" ctrl + \" , command : \" workbench . action . u t E o o \" Returns : { \" path \" : \" ... \" , \" action \" : \" added \" , \" backup \" : \" ... \" } \" \" \" (cid:6) (cid:4) (cid:5) (cid:4) (cid:5)"
        },
        {
            "title": "2.2 Synthetic Data Engine for Hybrid Action Tasks",
            "content": "Large-scale synthetic training tasks for CUAs remain scarce, while existing resources are primarily test sets or complete trajectories with limited reproducibility. To address this, we developed dual-pipeline synthetic data engine producing 17,000+ verifiable tasks for real-world computer use. Our engine operates through two complementary strategies: evaluator-first generation, ensuring verifiability, and instruction-first generation, creating contextually relevant tasks with diversity. 2.2.1 Evaluator-First Generation This approach begins by collecting state-checking evaluators from computer environmentsscripts that verify specific system states (e.g., file existence, application settings, UI elements). We use the atomic evaluator functions in OSWorld (Xie et al., 2024) to reprogram these evaluators by modifying parameters and compose multiple evaluators to create complex verification conditions. For example, combining file-checker with URL-checker validator creates task requiring both file manipulation and browsing interaction. Given these evaluator configurations, we prompt LLMs to generate corresponding tasks that would satisfy the verification conditions. For instance, the file-URL checker combination might generate tasks like Navigate to the Python documentation page and download the PDF tutorial to your Documents folder, which requires both web browsing to reach the correct URL and file system operations to verify the download. This ensures every generated task has programmatic way to verify completion, critical for providing clear reward signals during RL training. This approach produces 4,000+ high-quality tasks with guaranteed verifiability. 2.2.2 Instruction-First Generation Following Ramrakhya et al. (2025), this approach generates tasks based on observed system states. Agents explore computer environments through exploratory walks, reaching diverse UI states. At each state, we analyze the current interface and generate contextually appropriate tasks (e.g., create new spreadsheet when in file manager). Task completion is verified by an evaluator agent rather than predefined scripts, allowing flexibility in execution paths. This approach generates 12,000+ tasks that naturally arise from real usage patterns, complementing the systematic coverage of evaluator-first generation. 2.2.3 Workspace Simulation realistic workspace is crucial for generating meaningful tasks. When synthetic tasks require interaction with specific content, our pipeline triggers content preparation workflow tailored to task requirements. For example, for code-related tasks, we fetch files from popular GitHub repositoriesextracting Python scripts 4 Table 1 Comparison of our two synthetic data generation strategies. Strategy Task Count Rollout SR (%) Avg. Difficulty Avg. Steps Total Samples Total Traj. Evaluator-First Instruction-First 4K 13K 29 45 Medium-Hard Easy-Medium 6.8 6.5 33K 149K 4.8K 22K from Hugging Face repos or configuration files from trending projects. For image tasks, we retrieve open-source images from Wikipedia Commons matching relevant categories. For document editing, we generate synthetic documents via LLMs with task-appropriate content. This targeted approach ensures realistic task contexts: image editing tasks receive actual photos, code refactoring tasks get real implementations, and document tasks operate on properly formatted files. By matching content types to task requirements, we create scenarios that accurately reflect real-world computer use. 2.2.4 Complementary Design Rationale In general, the two approaches serve distinct purposes. Evaluator-first generation produces complex, verifiable tasks ideal for RL trainingcode-based evaluators provide precise rewards without expensive trajectory verification. However, these tasks tend to be challenging due to evaluators design and multi-evaluator compositions. Instruction-first generation offers greater diversity through environment exploration, covering more real-world scenarios with naturally easier tasks. This complementary design ensures both reliable RL signals and broad task coverage. We further summarize detailed data statistics in Table 1."
        },
        {
            "title": "2.3 Training a Foundation Agent with Hybrid Action",
            "content": "We train our foundation model using two-stage approach: supervised fine-tuning on high-quality trajectory demonstrations followed by online reinforcement learning. This curriculum first establishes competency in hybrid action, then optimizes action selection between GUI primitives and programmatic interfaces through self-play. 2.3.1 Multi-Agent Rollout for Trajectory Generation To generate high-quality training data, we deploy multi-agent system comprising Planner agent and specialized Grounder agent. We use OpenAI o3 as the Planner, which operates in ReAct framework (Yao et al., 2022) with Agent-S2-style prompting (Agashe et al., 2025) to enhance reasoning capabilities. The Planner strategically chooses between programmatic calls and GUI actions based on task context and available tools. When GUI interaction is needed, we employ GTA1-7B (Yang et al., 2025a) as the Grounding agent for precise visual localization, ensuring accurate element targeting in complex interfaces. For each synthetic task, we expose relevant programmatic tools to the Planner and perform 8 rollouts to capture diverse solution strategies. This process generates 26.8K successful trajectories demonstrating effective hybrid action strategies across our synthetic tasks. 2.3.2 Working Memory Mechanism (cid:7) < memory > Task : Create folder Favorites on bookmarks bar . Progress : Chrome open , bookmarks bar visible . Next : Access bookmark manager via Ctrl + Shift + . </ memory > (cid:6) Complex hybrid execution paths risk losing context as agents alternate between programmatic tools and GUI actions. We address this through an integrated working memory system using <memory></memory> tags, inspired by Bonatti et al. (2024). The agent autonomously maintains this memoryrecording completed steps, extracted values, and intermediate resultsensuring coherent execution without external storage. The common memory content includes: (1) task objectives and constraints, (2) progress tracking across completed actions, and (3) information that must persist across steps (e.g., file paths, UI element states, intermediate values). For example, during bookmark management task, the agent maintains structured state information as shown. This mechanism proves crucial for multi-step tasks requiring information persistence across action modality switches. (cid:4) (cid:5) 5 2.3.3 Stage 1: Supervised Fine-Tuning We fine-tune multiple base models, including UI-TARS-1.5 (7B) (Qin et al., 2025) and OpenCUA (32B) (Wang et al., 2025b) on the 26.8K successful trajectories from the rollout system. To ensure balanced training across all trajectory steps, we create individual samples from each turn: for the i-th turn, we include messages up to that point but apply loss only to the i-th assistant response. This prevents overfitting to early trajectory steps while ensuring each action decision receives equal training weight, teaching the model proper hybrid action at every step of task execution. 2.3.4 Stage 2: Online Reinforcement Learning While SFT provides behavioral foundations, mastering strategic action selection requires learning from exploration. The hybrid action space creates numerous solution paths for each tasksome efficient, others suboptimal. Through online RL, agents can discover these optimal strategies via self-play. We begin by filtering our evaluator-first tasks (4,000+) through 8 rollouts per task with the SFT model, identifying 1,000 tasks where the model succeeds at least once. We define task difficulty as the average success rate across these 8 rollouts. During training, we randomly sample tasks with difficulty scores in [0.4, 0.8]avoiding tasks that are too easy or too challenging to maximize learning efficiency within the models zone of proximal development. For policy optimization, we employ variant of GRPO (Shao et al., 2024) inspired by DAPO (Rosset et al., 2024), with key modifications for our hybrid action setting. We remove KL regularization and implement clip-higher strategy to encourage exploration of diverse action sequences. To prevent regression toward GUI-only solutions, we design reward function that incentivizes tool usage. The total reward for trajectory τ is: where Renv(τ ) {1, 1} is the sparse environment reward (1 for task success, -1 for failure), and the tool-use reward is defined as: R(τ ) = Renv(τ ) + Rtool(τ ) , (1) Rtool(τ ) = (cid:40) 0.3 0 if Renv(τ ) = 1 and τ contains tool calls, otherwise. (2) This reward structure teaches the agent not just to succeed, but to succeed efficiently through strategic hybrid action. Notably, we exclude format rewards despite their common use in RL with LLMs. We found in empirical analysis that models struggle with complex tool syntax early in training, causing format penalties to dominate the learning signal and discourage outcome-based learning. By focusing solely on outcome and tool-use rewards, we enable the model to gradually master tool syntax through successful examples rather than punishment, leading to more robust learning. We propagate rewards to each action step and normalize by trajectory length for stable optimization."
        },
        {
            "title": "3.1 Experimental Setup\nBenchmarks. We use OSWorld-Verified (Xie et al., 2024) as our primary benchmark. It is a realistic\nbenchmark featuring a Ubuntu Desktop environment accessible through screen observations, comprising 369\ntasks. OSWorld contains diverse tasks spanning common office suites, IDEs, and web browsers, designed to\nrigorously test an agent’s long-horizon planning and visual grounding abilities. Each task is self-contained\nwith a deterministic starting state, a natural language goal, and an automated rule-based evaluator, en-\nsuring reproducible and reliable assessment. To evaluate cross-platform generalization, we also test on\nWindowsAgentArena (Bonatti et al., 2024), which contains 154 real-world tasks in Windows 11 environ-\nments. This provides an out-of-domain evaluation since our models are primarily trained on Ubuntu-based\ntasks, testing the transferability of learned hybrid action strategies across operating systems.",
            "content": "6 Table 2 Comparison of the state-of-the-art methods on the OSWorld benchmark. We split the results by steps and show the approach type in the second column. We report the success rate (%) as the evaluation metric in the fourth column. denotes our reproduced results, averaged across 4 independent runs. Same-colored rows share the same base model. Agent Method Model Category Open-Source Success Rate (%) Max Steps: 15 Max Steps: 50 General Model General Model Agentic Model o3 (OpenAI, 2025) Claude 3.7 Sonnet (Anthropic, 2025) OpenAI CUA (OpenAI, 2025) Jedi-7B w/ GPT-4o (Xie et al., 2025) Multi-Agent Framework Multi-Agent Framework Agent S2 (Agashe et al., 2025) General Model Qwen2.5-VL-72B (Bai et al., 2025) Agentic Model UI-TARS-72B-DPO (Qin et al., 2025) Agentic Model OpenCUA-7B (Wang et al., 2025b) Agentic Model UI-TARS-1.5-7B (Qin et al., 2025) Agentic Model OpenCUA-32B (Wang et al., 2025b) Agentic Model UI-TARS-1.5-7B (Qin et al., 2025) Agentic Model OpenCUA-32B (Wang et al., 2025b) UltraCUA-7B-SFT UltraCUA-7B-RL UltraCUA-32B-SFT UltraCUA-32B-RL Agentic Model Agentic Model Agentic Model Agentic Model 9.1 27.1 26.0 26.8 27.0 4.4 24.0 24.3 24.5 29.7 23.4 33.3 27.0 28.9 39.0 41.0 17.2 35.8 31.3 27.0 34.5 25.8 28.2 27.3 34.1 26.1 35.1 28.5 30.2 41.5 43.7 Baselines. To demonstrate the effectiveness of our approach, we compare our final model against several strong baselines that isolate different components of the agents capabilities. General Models: powerful, pre-trained vision-language models that are not specifically fine-tuned for GUI automation. We include leading models like Claude (Anthropic, 2025) and o3 (OpenAI, 2025) to establish baseline for generalist, out-of-the-box performance. Multi-Agent Frameworks: systems that orchestrate multiple components to solve computer-use tasks. These frameworks typically employ planner-grounder architecture and may be enhanced with additional capabilities such as memory, experience replay, or the integration of coding agent. Prominent examples include Agent-S2 (Agashe et al., 2025) and Jedi-7B (Xie et al., 2025). Specialized Agentic Models: models that have been specifically fine-tuned or purpose-built for computer control and GUI-centric scenarios. This includes models like OpenAI CUA (OpenAI, 2025), UI-TARS (Qin et al., 2025) and OpenCUA (Wang et al., 2025b), which are trained on large datasets of computer interaction trajectories to specialize their abilities for this domain. Training Details. Our models are fine-tuned for 3 epochs during the SFT stage with learning rate of 2e-5. For the SFT stage, we sample 66K steps from trajectories with evaluator-first and instruction-first synthetic data, each 33K. The subsequent online RL stage is trained for 150 steps with learning rate of 1e-6. All experiments are conducted on NVIDIA H100 GPUs. During training, we control the number of programmatic tools to limit the context length to 32K. Evaluation Metrics. We use the following metrics to measure effectiveness and efficiency: 1) Success Rate (SR): Our primary metric. It is the percentage of tasks the agent successfully completes in single attempt, as verified by the benchmarks automated evaluators. 2) Pass@4: To account for the stochastic nature of LLM inference, we also report Pass@4. task is marked as successful under this metric if the agent completes it correctly in at least one of four independent rollout attempts. 3) Trajectory Efficiency: We measure the number of steps an agent takes to complete task successfully. Each step is either GUI action or programmatic tool call. lower step count indicates higher efficiency."
        },
        {
            "title": "3.2 Main Results\nOSWorld Evaluation. Table 2 presents comprehensive results on the OSWorld benchmark across different\nstep budgets. Our UltraCUA-7B achieves 28.9% success rate at 15 steps, surpassing all comparable 7B models,\nincluding the strong UI-TARS-1.5-7B baseline (23.4%) with a 23.5% relative improvement. More remarkably,",
            "content": "7 Table 3 Detailed performance comparison of agent methods across different domains on OSWorld, measured by success rate (%) under the 15-step setting. Highlighted cells denote best results for each domain. Methods (avg. runs) UI-TARS-1.5-7B UltraCUA-7B-SFT UltraCUA-7B-RL OpenCUA-32B UltraCUA-32B-SFT UltraCUA-32B-RL Chrome Calc (47) (46) Impress Writer GIMP VSCode Multi Apps Thunderbird OS VLC (17) (101) (25) (23) (47) (15) (24) (23) 32.5 (4.0) 40.7 (4.0) 41.2 (4.0) 42.4 (4.0) 38.9 (4.0) 40.6 (4.0) 8.4 (4.0) 13.3 (4.0) 13.9 (4.0) 21.3 (2.1) 20.0 (2.1) 25.7 (2.1) 22.8 (4.0) 26.9 (4.0) 27.1 (4.0) 36.8 (2.2) 40.2 (2.2) 40.2 (2.2) 36.9 (4.0) 46.7 (4.0) 55.4 (4.0) 42.4 (2.2) 59.3 (2.2) 62.5 (2.2) 48.5 (4.0) 57.7 (4.0) 50.0 (4.0) 53.8 (4.0) 71.4 (4.0) 70.0 (4.0) 43.5 (4.0) 34.8 (4.0) 46.7 (4.0) 39.1 (3.0) 57.9 (3.0) 54.3 (3.0) 7.1 (4.0) 8.4 (4.0) 10.6 (4.0) 9.9 (1.9) 9.9 (1.9) 14.9 (1.9) 38.3 (4.0) 41.1 (4.0) 43.3 (4.0) 50.0 (2.0) 69.6 (2.0) 72.1 (2.0) 34.1 (4.0) 36.0 (4.0) 37.0 (4.0) 42.9 (1.8) 62.1 (1.8) 64.0 (1.8) 27.0 (4.0) 30.3 (4.0) 33.6 (4.0) 23.5 (1.8) 31.5 (1.8) 33.3 (1.8) Avg. 23.4 (4.0) 27.0 (4.0) 28.9 (4.0) 33.3 (2.5) 39.0 (2.5) 41.0 (2.5) UltraCUA-32B reaches 41.0% success rate, outperforming even closed-source systems like Claude 3.7 Sonnet (27.1%) and OpenAI CUA (26.0%). We also present the detailed evaluation results per domain in Table 3. Note that for OpenCUA-series models, due to the sub-optimal speed and infrastructure for inference, the overall average run is less than 4. The results validate our hybrid action approach across model scales. While general-purpose models struggle without specialized training (e.g., Qwen2.5-VL-72B at 4.4% despite 72B parameters), our models achieve superior performance through strategic integration of programmatic tool calls. The consistent improvements from base models (UI-TARS-1.5-7BUltraCUA-7B: +23.5%, OpenCUA-32BUltraCUA-32B: +23.1%) demonstrate that hybrid action provides orthogonal benefits to agent capabilities. Cross-Platform Generalization. To assess generalization beyond the training domain, we evaluate on WindowsAgentArena without any Windows-specific fine-tuning. Table 4 shows that UltraCUA-7B achieves 21.7% success rate, outperforming both Qwen2-VL-7B trained with OpenCUAs Windows data (13.5%) and UI-TARS-1.5-7B (18.1%). This 20% relative improvement over UI-TARS demonstrates that hybrid action strategies learned on Ubuntu effectively transfer to Windows environments, validating the domain-agnostic nature of our approach. Table 4 Out-of-domain evaluation on WindowsAgentArena. Qwen2-VL-7B (w/ OpenCUA Data) UI-TARS-1.5-7B UltraCUA-7B 13.5 18.1 21.7 SR (%) Model"
        },
        {
            "title": "3.3 Ablation Studies",
            "content": "We conduct series of ablation studies to dissect our framework and validate the contribution of its key components. These experiments isolate the impact of the hybrid action space, working memory, and reinforcement learning stage on agent performance. 3.3.1 The Impact of Hybrid Action To validate the effectiveness of hybrid action, we examine its impact on both specialized agentic models and powerful multi-agent frameworks. Impact on Specialized Models. We compare three configurations: (1) UI-TARS-1.5-7B (GUI-only baseline), (2) our model with tools disabled (UltraCUA-7B w/o Tools), and (3) our full model with hybrid action. Table 5 shows that hybrid action yields substantial improvements: success rate increases from 23.4% to 27.0% (+15.4% relative) while maintaining similar step counts. The addition of programmatic tools proves essential for effectiveness in complex automation tasks. Impact on Multi-Agent Frameworks. To test whether hybrid action benefits extend to state-of-the-art systems, we evaluate our GTA1-7B + o3 rollout framework with and without programmatic tools. As shown in Table 5, hybrid action provides even larger gains in this setting: success rate improves from 44.0% to Table 5 Impact of hybrid action on different agent architectures. Hybrid action benefits both specialized models and multi-agent frameworks."
        },
        {
            "title": "Model Configuration",
            "content": "Success Rate (%) Pass@4 Avg. Steps Agentic Models (Max Steps: 15) UI-TARS-1.5-7B (GUI-Only) UltraCUA-7B-SFT w/o Tools (GUI-Only) UltraCUA-7B-SFT (Hybrid Action) 23.4 25.1 27.0 Commercial Models & Multi-Agent Framework (Max Steps: 50) Claude-4-Sonnet GTA1-7B + o3 w/o Tools GTA1-7B + o3 (Hybrid Action) 43.9 44.0 48.2 33.3 34.3 37. 60.5 62.4 9.31 9.24 8.46 15.53 13.22 (a) Outcome Reward (b) Format Reward (c) Tool-call Pattern Figure 3 Evolution of the agents behavior during reinforcement learning. Rewards are increasing as the number of failed tasks with tool-calls drops after RL. 48.2% (+9.5% relative) and average steps decrease by 14.9%. This demonstrates that hybrid action becomes increasingly valuable as the underlying system becomes more capable. 3.3.2 The Importance of Reinforcement Learning We evaluate the impact of online RL by comparing models before and after this training stage, for UltraCUA7B. From Table 2, we can see that online RL brings 7% relative improvement (27.028.9). Figure 3 reveals how RL transforms agent behavior in three key ways. First, outcome rewards increase steadily during RL (Fig. 3a), confirming performance gains. Interestingly, format rewards also improve substantially (Fig. 3b) despite not being explicitly optimized. This suggests agents learn proper tool syntax naturally through successful task completion. Most significantly, RL reshapes tool-use strategy (Fig. 3c). Tool-related failures drop 46% (12266) while successes increase by 5%, indicating pre-RL models often make harmful tool calls. Correspondingly, overall tool usage decreases, showing agents learn to be selective rather than aggressive with tool deployment. These results demonstrate that while SFT teaches the mechanics of hybrid action, RL enables strategic decision-making about when to use each action typea crucial distinction for effective automation. 3.3.3 Impact of Working Memory We evaluate working memory by training models with and without <memory></memory> blocks in the SFT data, isolating the contribution of explicit state tracking. Table 6 shows consistent improvements from working memory: success rate increases from 25.4% to 27.0% (+6.3% relative) and average steps decrease slightly. While modest, these gains are meaningful for tasks requiring persistent statefile operations, form filling, and cross-application workflows. The efficiency improvement suggests memory helps agents avoid redundant actions like re-navigating to previously visited screens or re-extracting obtained information."
        },
        {
            "title": "3.4 Analysis on Tool Use Patterns",
            "content": "To understand how our model leverages the hybrid action space, we analyze tool usage patterns across different application domains and task types. 9 Table 6 Impact of working memory on model performance. Models are trained with identical data except for the presence of memory blocks. Model Configuration Success Rate (%) Pass@4 Avg. Steps UltraCUA-7B-SFT w/o Memory UltraCUA-7B-SFT w/ Memory 25.4 27.0 37.1 37.9 8.56 8.46 Relative Improvement +6.3% +2.1% -1.2% Tool Usage Scales with Model Capability. Figure 4 reveals clear correlation between model capability and tool usage sophistication. The multi-agent framework (GTA17B+o3) demonstrates extensive tool utilization with 60-80 calls and 8-10 unique tools per domain, while our single models show progressively conservative patternsUltraCUA32B uses tools moderately (20-40 calls) and UltraCUA-7B sparingly (0-20 calls). This pattern validates our hybrid action hypothesis: stronger models not only call tools more frequently but also leverage greater diversity, suggesting they better recognize when programmatic interfaces provide efficiency gains. The trend holds across all domains from office suites to development environments, confirming that effective hybrid action emerges naturally with increased model capability. Figure 4 Tool-call patterns across domains and models. Stronger models exhibit higher frequency and diversity. Table 7 OSWorld with OOD tools. Out-of-Distribution Tool Generalization. We evaluate the models ability to utilize tools not seen during training by introducing new programmatic tools at inference time. These tools are unseen during training due to the context length limit. Table 7 shows that models can adapt to unseen tools, achieving modest performance gains (+1.9% relative SR). However, the increased steps suggest adaptation challengesmodels may explore unfamiliar tools before selecting appropriate ones. This zero-shot tool generalization capability also extends beyond single-platform scenarios: Table 4 demonstrates that our model achieves 21.7% success rate on Windows tasks despite training exclusively on Ubuntu, outperforming baselines by leveraging its learned hybrid action strategies across platforms and tool ecosystems. UltraCUA-7B-SFT w/ OOD tools SR (%) Avg. Steps"
        },
        {
            "title": "Configuration",
            "content": "27.0 27.5 8.46 8."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduced UltraCUA as foundation CUA models that bridge the critical gap between general-purpose GUI agents and specialized API-based agents. We achieve this through novel hybrid action space that seamlessly integrates low-level GUI actions with high-level tool use. Our core contributions are scalable pipeline for automated tool acquisition, synthetic data engine for generating verifiable hybrid tasks, and two-stage training curriculum to optimize strategic agent behavior. UltraCUA achieves state-of-the-art performance on the real-world benchmarks. Ablation studies confirm that the hybrid action space is the essential driver of this success, demonstrating novel and more effective paradigm for building strong, robust and efficient agents for general computer control."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would like to thank Harsh Agrawal, Dongxu Li, and Yutong Dai for valuable suggestions and feedback. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions."
        },
        {
            "title": "References",
            "content": "Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025. Anthropic. Claude 3.7 sonnet and claude code. Technical report, Anthropic, 2025. URL https://www.anthropic. com/news/claude-3-7-sonnet. System Card. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. Tianle Cai, Xuezhi Wang, Yiming Zhan, Jiaming Chen, Yuan Wang, Yunchang Pan, Wayne Zhang, Da Li, Peilin Li, Yihui Wang, et al. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023. Xiang Deng, Lichend Vong, Shuyan Naga, Aohan Chen, Bowei Xia, Boyuan Wang, Jiaming Wang, Jing-Cheng Zhang, Kexuan Liu, Libo Li, et al. Mind2web: generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. Wenyi Hong, Weihan Zhang, Junkai Chen, Yutao Zheng, Yue Zhang, Bin Wang, Guo Zhao, Canyu Zhang, Aili Li, Yunchao Sun, et al. Cogagent: visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023. Chengyou Jia, Minnan Luo, Zhuohang Dang, Qiushi Sun, Fangzhi Xu, Junlin Hu, Tianbao Xie, and Zhiyong Wu. Agentstore: Scalable integration of heterogeneous agents as specialized generalist computer assistant. arXiv preprint arXiv:2410.18603, 2024. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Dua, Ming Liang, and Aniruddha Nayak. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In Association for Computational Linguistics (ACL), 2024. Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui 2: Mastering universal user interface understanding across platforms. arXiv preprint arXiv:2410.18967, 2024. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In International Conference on Learning Representations (ICLR), 2024. OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025. URL https://cdn.openai.com/pdf/ 2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. System Card. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Yujia Qin, Shihao Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023a. Yujia Qin, Shengding Liang, Yining Du, Wenqi Ye, Yan Cheng, Yaxi Lin, Yi Ruan, Jian Li, Xu Sun, Jie Fu, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023b. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Ram Ramrakhya, Andrew Szot, Omar Attia, Yuhao Yang, Anh Nguyen, Bogdan Mazoure, Zhe Gan, Harsh Agrawal, and Alexander Toshev. Scaling synthetic task generation for agents via exploration. arXiv preprint arXiv:2509.25047, 2025. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidworld: dynamic benchmarking environment for autonomous agents. In International Conference on Machine Learning (ICML), 2024. 11 Corin Rosset, Nan Jiang, and Alekh Agarwal. Direct action-policy optimization. arXiv preprint arXiv:2405.19553, 2024. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Tsvigun, Gael Cosculluela, Spencer SacerdotiCohen, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023a. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Tsvigun, Gael Cosculluela, Spencer SacerdotiCohen, Thomas Scialom, Stiegler, and R. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023b. Zhihong Shao, Peize Wang, Lirui Dou, Yuxian Wang, Yushan Liu, Dian Zhang, Shang-Yi Li, Nuo Zhou, Han Liu, Zaibin Zheng, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, et al. Coact-1: Computer-using agents with coding as actions. arXiv preprint arXiv:2508.03923, 2025. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025a. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, et al. Opencua: Open foundations for computer-use agents. arXiv preprint arXiv:2508.09123, 2025b. Tianbao Xie, Danyang Liu, Yutong Wang, Can Zhang, Zheyuan Li, Yichen Zhu, Tao Yu, Jeff Hoffman, Hui Su, and Mohit Bansal. Osworld: massively multitask and multilingual benchmark for general-purpose agents. arXiv preprint arXiv:2404.09852, 2024. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025. An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025a. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. Zhen Yang, Zi-Yi Dou, Di Feng, Forrest Huang, Anh Nguyen, Keen You, Omar Attia, Yuhao Yang, Michael Feng, Haotian Zhang, et al. Ferret-ui lite: Lessons from building small on-device gui agents. arXiv preprint arXiv:2509.26539, 2025b. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Graham Durrett, and Karthik Narasimhan. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. In European Conference on Computer Vision, pp. 240255. Springer, 2024. Chi Zhang, Zhao Huang, Boyu Li, Hongyu Li, Song-Chun Zheng, Limin Yu, Hualei Wang, Weichen Ma, and Han Zhang. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023. Wentao Zhang, Liang Zeng, Yuzhen Xiao, Yongcong Li, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: hierarchical multi-agent framework for general-purpose task solving. arXiv preprint arXiv:2506.12508, 2025. 12 Lianmin Zheng, Yujie Zhang, Shuyan Wang, and Long Chen. Gpt-4v(ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Li, Caiming Li, Yitao Liu, Mohammed Al-Tawil, Song-Chun Huang, Weizhen Wang, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Related Work Multimodal Agents for Computer Automation. The ambition to create agents that can operate GUIs is longstanding, but has seen remarkable progress with the advent of Vision-Language Models (VLMs). Early approaches often relied on structured data like HTML or accessibility trees. More recent and generalizable agents operate directly from pixels and high-level instructions. In web automation, benchmarks like WebArena (Zhou et al., 2023) and Mind2Web (Deng et al., 2023) have driven the development of agents capable of complex online tasks. Similarly, in general computer control, works like CogAgent (Hong et al., 2023), Ferret UI You et al. (2024); Li et al. (2024) and OSWorld (Xie et al., 2024) have demonstrated agents that can navigate desktop environments, and AppAgent (Zhang et al., 2023) has shown similar capabilities on mobile devices. Current approaches to GUI automation can be broadly categorized into two paradigms. Multi-Agent systems employ specialized models for different subtasksfor instance, GPT-4o+Aria-UI (Yang et al., 2024) and GTA-1 (Yang et al., 2025a) combine planner model with dedicated grounder model, leveraging the strengths of each component for strategic planning and precise visual grounding, respectively. In contrast, Foundation Agent Models like UI-TARS (Qin et al., 2025), UI-TARS-2 (Wang et al., 2025a), OpenCUA (Wang et al., 2025b) and Ferret-UI Lite (Yang et al., 2025b) adopt an end-to-end approach, where single unified model autonomously handles both planning and grounding tasks. While multi-agent systems benefit from modular design and specialized expertise, foundation models offer simpler deployment and potentially better coordination between planning and execution. common thread among these powerful agents is their reliance on primitive action space consisting of clicks, types, and scrolls. While this provides generality, it also leads to the brittleness and long-horizon planning challenges that our work directly addresses. Our contribution is the introduction of hybrid action space that retains this generality while adding the efficiency and robustness of high-level tools. Tool and API Augmentation for LLMs. Parallel to the development of GUI agents, another line of research has focused on augmenting LLMs with the ability to use external tools and APIs. The seminal work of ToolFormer (Schick et al., 2023a) showed that models could learn to call APIs to access information they lack. This paradigm was rapidly scaled up by frameworks like ToolLLM (Qin et al., 2023b) and the Gorilla benchmark (Patil et al., 2023), which enabled models to select from thousands of real-world APIs. Furthermore, the concept of tool-making (Cai et al., 2023) has explored agents that can write their own tools when needed, capability we incorporate into our tool acquisition pipeline. Recent advances have introduced reinforcement learning to tool-use training. ReTool (Feng et al., 2025) and ToolRL (Qian et al., 2025) pioneered the use of online RL for training end-to-end tool-use agents, demonstrating that reward signals alone can guide models to learn effective tool selection and usage strategies. These methods move beyond supervised learning on static datasets, allowing agents to discover optimal tool-use patterns through interaction and feedback. This RL-based paradigm aligns closely with our approach, where we employ online reinforcement learning to train agents that can strategically alternate between primitive GUI actions and high-level tool calls. While these tool-augmented agents are highly effective for structured, programmatic tasks, they typically operate in non-visual, text-based environment and lack the ability to interact with the vast number of applications that do not expose an API. Our work bridges this gap, bringing the power of rich tool ecosystem to the visually-grounded domain of GUI agents. A.2 The Use of Large Language Models We used large language models (LLMs) to assist with specific aspects of paper preparation. Specifically, LLMs were employed for: (1) language polishing and grammar checking to improve clarity and readability, (2) formatting suggestions to ensure compliance with conference style guidelines, and (3) recommendations for data visualization approaches to better present experimental results. All research ideas, experimental design, implementation, and core scientific contributions were developed by the authors without LLM assistance. The LLMs served purely as writing and presentation aids. 14 A.3 Details for Programmatic Tools Table 8 summarizes the programmatic tools available across 10 different application domains on OSWorld. The collection comprises 881 tools in total, with individual domains offering between 4 (System) and 135 (VS Code) tools. These tools provide fine-grained control over desktop applications, enabling agents to perform tasks ranging from basic navigation (e.g., jump_to_next_tab) to complex application-specific operations (e.g., batch_spreadsheet_numeric_formatter). The comprehensive tool coverage ensures that agents can effectively automate diverse desktop workflows across different software environments. Table 8 Overview of available tools across different domains."
        },
        {
            "title": "LibreOffice General",
            "content": "LibreOffice Calc LibreOffice Impress LibreOffice Writer System Thunderbird VLC VS Code Total"
        },
        {
            "title": "Tool Count Example Tools",
            "content": "jump_to_next_tab, chrome_domain_data_wiper, open_downloads_page save_image_as, undo_last_action, swap_foreground_background_colors open_find_and_replace, open_print_preview, open_hyperlink_dialog spreadsheet_column_formula_injector, batch_spreadsheet_numeric_formatter set_line_spacing_1, insert_non_breaking_space, apply_subscript select_to_start_of_next_page, select_to_start_of_paragraph open_system_terminal_and_execute, open_app_or_filename open_message_in_conversation, delete_message_permanently set_video_as_wallpaper, volume_up, jump_1_minute_forward add_vs_code_keybinding, vscode_exclude_folders, search_within_current_file 69 88 41 114 123 4 119 83 135 A.4 Details for Synthetic Tasks We generated comprehensive synthetic dataset of 17,864 tasks across 10 application domains using two complementary approaches. As shown in Table 9, the evaluator-first approach contributed 4,387 high-quality tasks with complex multi-step instructions, while the instruction-first approach generated 13,477 tasks to ensure broad coverage of application functionalities. The dataset spans diverse applications from productivity tools (LibreOffice suite with 5,885 combined tasks) to specialized software like GIMP (1,121 tasks) and development environments like VS Code (1,990 tasks). Chrome represents the largest single-domain category with 2,826 tasks, reflecting the importance of web interactions. The multi-apps category (2,113 tasks) specifically tests cross-application workflows. Task complexity varies from simple operations (e.g., Change the text alignment to Center) to sophisticated procedures requiring multiple coordinated actions (e.g., Convert video to MP4 and save with new filename), ensuring comprehensive evaluation of agents GUI navigation and task execution capabilities. A.5 Qualitative Examples To illustrate the practical advantages of our hybrid action paradigm, we present three representative examples in Figures 5, 6, and 7. These cases highlight how UltraCUA strategically selects between high-level programmatic tools and low-level GUI actions to enhance efficiency, tackle complex problems, and ensure robust execution. First, the email-starring task (Figure 5) exemplifies the agents capacity for intelligent and fluid alternation 15 Table 9 Overview of synthetic data generation across different domains."
        },
        {
            "title": "Domain",
            "content": "Eval-First Instr-First Example Instructions"
        },
        {
            "title": "Chrome",
            "content": "751 2,"
        },
        {
            "title": "GIMP",
            "content": ""
        },
        {
            "title": "LibreOffice\nCalc",
            "content": "LibreOffice Impress 651 501 1,496 1,397 LibreOffice Writer 851 989 OS/System 301 1,197 Thunderbird 351 1,084 VLC"
        },
        {
            "title": "VS Code",
            "content": "330 250 666 1,740 Multi-apps 2,113 Find hotels in Paris for 2 adults for three nights starting next Friday and sort the list by lowest price. Restore the previous session pages in Google Chrome. Please replace the current white backdrop with solid green color, but keep the black circle in the centre exactly as it is. In GIMP, navigate to the Display section and set the check style to Medium checks. Open the spreadsheet and make the entire header row (row 1) bold. Protect the sheet Sheet2 in LibreOffice Calc. Make every slide in this deck use solid dark-green background (RGB 0 128 0). Id like all the pages to share that exact colour so the presentation looks consistent. Add video from /videos/video3.mov to slide 3 in LibreOffice Impress. Change the default font in LibreOffice Writer to Calibri. Change the text alignment to Center in LibreOffice Writer. accidentally created file called draft.txt on my Desktop. Please delete it completely so its no longer there. View the partitioning table of the disk named {disk_name} in the Disks app. Create new folder named ToSort inside the Local Folders section. Import contacts from Windows Mail into Thunderbird. Open the cat photo in VLC and set it as my desktop wallpaper. Play the current video in VLC Media Player. Could you open VS Code and create new text file named meeting_notes.txt inside the folder /home- /user/notes ? Make sure to save the file before you finish. Search for the term Data Structure in the document and highlight it in LibreOffice Writer Change the desktop wallpaper to Desert on the Ubuntu desktop. Search for JavaScript in Brave settings and enable it."
        },
        {
            "title": "Total",
            "content": "2,826 1,121 2,147 1,898 1,840 1, 1,435 996 1,990 2,"
        },
        {
            "title": "Total",
            "content": "4,387 13,477 17,864 16 between control modes. The process begins with precise low-level GUI click to select the target Bills folder, effectively setting the context. Immediately following this, the agent switches to high-level, reliable tool callsselect_all and add_or_remove_starto execute the core bulk operation. This strategic handoff from specific GUI action to general-purpose tools ensures both precision and operational robustness. In the second example (Figure 6), the agent is asked to clear YouTube browsing history. Instead of relying on potentially brittle sequence of clicks through menus, it initiates the workflow with single programmatic tool call, open_history_page, to navigate directly to the correct settings page. Subsequently, it seamlessly transitions to primitive GUI actionstyping into search field and clicking buttonsto perform the more nuanced task of filtering and deleting the specific entries. This demonstrates practical fusion of programmatic speed for navigation and GUI flexibility for manipulation. Finally, more complex scenario in Figure 7 showcases the models ability to automate workflows that are intractable for purely GUI-based agents. When tasked with batch-processing images on the desktop, UltraCUA correctly identifies the need for scripted solution. It programmatically opens system terminal, installs the necessary software (imagemagick), and proceeds to write and execute multi-line shell script to automate the entire process. This ability to generate and utilize code represents significant leap in problem-solving capability. Figure 5 An example of UltraCUA-32B helping processing emails. The agent alternates between low-level actions and programmatic tool calls smartly, leading to efficient completion of the task. 17 Figure 6 An example of UltraCUA-32B helping clear certain Chrome history with hybrid action. The agent calls prothe grammatic tool at the first step to assist in going into the desired page directly. 18 Figure 7 An example of UltraCUA-32B helping processing images with hybrid action. The model starts coding at the very first step by calling the terminal tool, and finally writes bash script and executes it to make the task successful."
        }
    ],
    "affiliations": [
        "Apple",
        "The University of Hong Kong"
    ]
}