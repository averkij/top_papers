{
    "paper_title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
    "authors": [
        "Yongxin Guo",
        "Jingyu Liu",
        "Mingda Li",
        "Xiaoying Tang",
        "Qingbin Liu",
        "Xi Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents videos as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE processes visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling framework's formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are available at \\url{https://github.com/gyxxyg/TRACE}."
        },
        {
            "title": "Start",
            "content": "TRACE: TEMPORAL GROUNDING VIDEO LLM VIA CAUSAL EVENT MODELING 4 2 0 2 4 ] . [ 2 3 4 6 5 0 . 0 1 4 2 : r Yongxin Guo1,4, Jingyu Liu4, Mingda Li4, Xiaoying Tang1,2,3, Qingbin Liu4, Xi Chen4 1School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China 2The Shenzhen Institute of Artificial Intelligence and Robotics for Society 3The Guangdong Provincial Key Laboratory of Future Networks of Intelligence 4 Tencent PCG"
        },
        {
            "title": "ABSTRACT",
            "content": "Video Temporal Grounding (VTG) is crucial capability for video understanding models and plays vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents videos as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose novel taskinterleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE processes visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling frameworks formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-ofthe-art video LLMs. Our model and code are available at https://github.com/gyxxyg/TRACE."
        },
        {
            "title": "Introduction",
            "content": "Video Temporal Grounding (VTG) is an important ability for video understanding models (Lin et al., 2023b), and has becoming the base of series of downstream tasks like moment retrieval (Fabian Caba Heilbron & Niebles, 2015; Gao et al., 2017; Oncescu et al., 2021), dense video caption (Zhou et al., 2018; Tang et al., 2019), video highlight detection (Lei et al.; Liu et al., 2022), and video summarization (Song et al., 2015; Gygli et al., 2014). While nongenerative models excel in moment retrieval and video highlight detection (Lei et al., 2021; Han et al., 2024; Wang et al., 2024a), they are inflexible, task-specific, and demand substantial fine-tuning for optimal performance. To tackle these challenges, recent research employs video LLMs as versatile models, integrating timestamp information into visual inputs, and fine-tuning them on VTG tasks (Ren et al., 2023; Huang et al., 2023; Wang et al., 2024b; Qian et al., 2024) to enhance their performance and facilitate zero-shot prediction. Challenges posed by videos inherent structures. Despite reflecting human intent, current video LLM-based approaches rely on pure natural language generation, which, as illustrated in Figure 1(a), lacks clear structure and indiscriminately blends information like timestamps and text captions. In contrast, videos have an inherent structure, consisting of sequential events over time, each with distinct timestamps, captions, and salient scores (for tasks like video highlight detection and video summarization). Consequently, the gap between videos and current video LLMs undermines the ability of video LLMs to effectively model video events, potentially making video LLMs difficult to achieve satisfactory results (Figure 1(b)) on VTG tasks. (a) Video Structure (b) Performance Gap between Models Figure 1: Challenges posed by videos inherent structures. Figure 1(a) shows the difference between natural language and video structure, while Figure 1(b) highlights the performance gap between SOTA video LLMs (Ren et al., 2023; Guo et al., 2024) and TRACE. We present zero-shot performance results for video LLM approaches. Specifically, we report the performance of models using the CIDEr metric for the dense video captioning task on the Youcook2 dataset, R@1IOU=0.7 for the moment retrieval task on the Charades-STA dataset, and HIT@1 for the highlight detection task on the QVHighlights dataset. Causal event modeling as solution. In this paper, our primary goal is to develop novel video LLM approach for resolving the mismatch between language modeling of LLMs and videos inherent structure. Specifically, we concentrate on tackling two main challenges: (1) developing theoretical framework that shifts from causal language modeling to structured video modeling, and (2) constructing practical video LLM based on the theoretical framework to provide an effective solution. To accomplish this, we first introduce the causal event modeling framework, where videos are represented as sequences of events, each containing timestamps, salient scores, and textual captions. The next events are predicted based on video inputs, text instructions, and preceding events. To effectively implement the causal event modeling framework in practice, we present novel task-interleaved video LLM, TempoRAl grounding via Causal Event modeling (TRACE), as illustrated in Figure 2. The TRACE treats visual frames, timestamps, salient scores, and text as separate tasks, utilizing diverse encoders and decoding heads for each task, with task tokens sequenced in an interleaved manner. Furthermore, we develop an adaptive head-switching method for improved generation. Our numerical results across various VTG tasks reveal the superior performance of TRACE in comparison to state-of-the-art (SOTA) video LLMs. Our key contributions are summarized as follows: We model the videos by series of events, and propose causal event modeling framework to capture videos inherent structure. We then present novel task-interleaved video LLM model, TRACE, tailored to implement the causal event modeling framework through the sequential encoding/decoding of timestamps, salient scores, and textual captions. We conduct comprehensive experiments on multiple VTG tasks and datasets to verify the effectiveness of TRACE. The results reveal significant improvements of TRACE in comparison to SOTA video LLMs. Notably, TRACE improves zero-shot performance by 3.1 and 4.9% on Youcook2 (CIDEr and F1 Score), by 6.5% and 3.7% in Recall (IOU = {0.5, 0.7}) on Charades-STA, and by 10.3% and 9.2% for mAP and HIT@1 on QVHighlights. Moreover, surpassing existing video LLMs, TRACE achieves comparable performance to traditional non-generative and task-specific methods after fine-tuning, highlighting the potential of video LLMs to excel in VTG tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Video temporal grounding. Video Temporal Grounding (VTG) tasks aim to precisely identify the timestamps of events within given video (Lin et al., 2023b). This includes various tasks such as moment retrieval (Gao et al., 2017; Zala et al., 2023b; Oncescu et al., 2021; Hendricks et al., 2018a; Boris et al., 2024), dense video caption (Zellers et al., 2021; Zala et al., 2023b; Tang et al., 2019; Fabian Caba Heilbron & Niebles, 2015; Kim et al., 2024), video summarization (Song et al., 2015; Gygli et al., 2014; Hua et al., 2024), and video highlight detection (Lei et al., 2021; Xiao et al., 2023). For tasks such as moment retrieval, video summarization, and video highlight detection, traditional approaches primarily use large-scale video-text pre-training (Xu et al., 2021; Wang et al., 2022; Yan et al., 2022; Li 2 Figure 2: Overview of the training process of TRACE model. We employ variety of encoders and heads to handle time, score, and text inputs and outputs. The timestamps of the sampled frames are converted into time tokens and subsequently integrated into the visual tokens of each frame. In the answer section, time tokens, score tokens, and text tokens are inserted in sequential manner. The generation process of TRACE is summarized in Figure 4. et al., 2023c; Chen et al., 2024b; Tong et al., 2022; Zhao et al., 2024). Subsequently, they fine-tune the pretrained models by incorporating task-specific prediction heads. While these methods have demonstrated satisfactory results, they are resource-intensive for pre-training, lack zero-shot capabilities, can only handle one specific task per model, and often require additional fine-tuning for numerous downstream tasks. For the dense video caption task, Vid2Seq employs special time tokens to represent timestamps (Yang et al., 2023). Some approaches integrate additional input information, such as text queries from training datasets (Kim et al., 2024), while other models utilize different decoding heads to decode timestamps and textual captions (Wang et al., 2021, 2023a) in parallel. However, these architectures are specifically designed for the dense video caption task, cannot be easily adapted to LLM structures to fully harness the capacity of pretrained LLMs, and also lack zero-shot capabilities. Video LLMs for video temporal grounding. Large language models (LLMs) (Kaplan et al., 2020; Achiam et al., 2023; Touvron et al., 2023) have demonstrated significant potential in acquiring knowledge and addressing real-world challenges using zero-shot approach. Recent research has focused on integrating knowledge from other modalities, such as vision (Liu et al., 2024b; Li et al., 2023a) and audio (Ghosal et al., 2023), to bolster the capabilities of LLMs. Within the visual domain, video large language models (video LLMs) have emerged as crucial research area (Lin et al., 2023a; Maaz et al., 2023; Zhu et al., 2023). Traditional video LLMs (Zhang et al., 2023; Lin et al., 2023a; Li et al., 2023b, 2024a; Cheng et al., 2024; Yao et al., 2024) have made considerable performance improvements in tasks such as video question answering, reasoning, and video captioning. However, these methods encounter difficulties in precisely pinpointing event timestamps within videos. To address this issue, TimeChat (Ren et al., 2023), VTimeLLM (Huang et al., 2023), and Hawkeye (Wang et al., 2024b) have attempted to overcome this limitation by fine-tuning the video LLMs on VTG datasets. More recently, LITA (Huang et al., 2024) introduces fast-slow visual tokens and incorporates time tokens into LLM tokenizers. Momentor (Qian et al., 2024) suggests time encoder to address time token quantization errors. VTG-LLM (Guo et al., 2024) integrates special time tokens and time position embeddings to improve the ability of video LLMs in comprehending timestamps. However, these methods do not take into account the inherent structure of videos and still cannot achieve satisfactory performance. In this paper, we propose the causal event modeling framework to model video structure and design the TRACE model to address the proposed framework. Numerical results demonstrate significant performance gains of TRACE over existing video LLMs on VTG tasks."
        },
        {
            "title": "3 TRACE",
            "content": "In this section, we aim to develop novel video LLM that aligns well with video structures, addressing two questions: (1) how to model video structures, and (2) how to implement theoretical models. We start by proposing causal event modeling framework to tackle \"how to model\". Then, we introduce TRACE to address \"how to implement\". 3.1 Modeling the Inherent Structures of Videos Formulating videos by events. Given video V, we represent the video as series of events {e1, e2, , eK}, with each event ek = (tk, sk, ck) encompassing timestamps tk, salient scores sk, and textual captions ck. In summary, we have = {e1, e2, , eK} = {(tk, sk, ck)1 K} . (1) Causal event modeling framework. To effectively utilize the knowledge of pretrained LLMs, the design of causal event modeling shares the underlying intuition of causal language modeling, as formulated in the subsequent equation 1. P(eke1:k1, I, F) = P(tk, sk, cke1:k1, I, F) , = P(tke1:k1, I, F)P(sktk, e1:k1, I, F)P(cksk, tk, e1:k1, I, F) , (2) where indicates the textural instructions, and indicates the visual frame inputs. The next event ek is determined by textural instructions, visual inputs, and previous events. We can find that causal event modeling framework aligns well with the video structure (Figure 1(a)): (1) timestamps, salient scores, and textual captions are sequentially decoded within each event; (2) events are then ordered by timestamps. 3.2 TRACE: Task-Interleaved Temporal Grounding Video LLM In Eq. 2, we introduce formal causal event modeling framework to tackle the challenge of modeling video structures. This section illustrates the design of TRACE to implement the causal event modeling framework (Figure 2). Overview of TRACE. As illustrated in Eq. 2, the causal event modeling framework necessitates encoding/decoding of visual frames (F), text (I and ck), timestamps (tk), and scores (sk). Consequently, the TRACE considers these elements as distinct tasks and employs the following design to efficiently manage them. Separated multi-task processing. TRACE utilizes separate encoders and decoding heads for each task to convert task inputs into task tokens and decode task tokens back to outputs (Sec. 3.2.1). Task-interleaved sequence modeling. Task tokens are sequenced in an interleaved manner according to Eq. 2 in TRACE and fed into LLM backbones (Sec. 3.2.2). Adaptive head-switching mechanism for generation. During generation, we implement an adaptive head-switching mechanism to select the appropriate decoding head for producing the next token (Sec. 3.2.3). 3.2.1 Separated Multi-Task Processing TRACE consists of four unique tasks: visual frames, text, timestamps, and scores. Regarding text, we directly utilize the text tokenizer and LLM head of the LLM backbone (Mistral-7B-v0.2 (Jiang et al., 2023)). Moreover, we added special token sync for indicating the end of text tasks. The processing for the other tasks is detailed below. Timestamps and scores processing. For processing timestamps and score information, we employ two separate encoders and decoding heads, both of which share the same architecture. Specifically, each encoder is initialized with tokenizer containing 13 tokens: 11 number tokens 0, , 9, . for representing timestamps/scores, sep to mark the end of each timestamp/score, and sync to signify the end of the current task. Token embeddings are initialized using LLM token embeddings. In accordance with the research in VTG-LLM (Guo et al., 2024), we format each timestamp/score to the same length, comprising 4 whole-number parts, 1 dot, and 1 fractional part 2. Subsequently, sep is inserted between timestamps/scores, and sync is appended at the end of each timestamp/score input sequence. For instance, the timestamp inputs [10.23, 125.37] will be tokenized into the following sequence: 0010.2sep0125.4sync. 1Theoretically, the order of time, score, and text will not impact the results. We select one order here. 2Different from timestamps, scores will be encoded to 3 score tokens, including 1 whole-number parts, 1 dot, and 1 fractional part. 4 Figure 3: Illustration of token sequence of TRACE. Following Eq 2, the sequence begins with visual frame tokens (F) followed by instruction tokens (I). Event (e) tokens are structured in the sequence of time tokens (t), score tokens s, and text tokens c, with events ordered chronologically based on their occurrence time. Visual frames processing. Given -frame video, we initially encode the frames using the pretrained CLIP ViTL (Radford et al., 2021), with each frame being encoded into 576 visual tokens. Subsequently, we employ Slot-Based Compression (Guo et al., 2024) to reduce the number of visual tokens to 8 per frame. Moreover, to integrate temporal information into the visual inputs, we use time encoder to encode the timestamps of each sampled frame and remove the sync and sep tokens, resulting in 6 time tokens for each frame. Finally, we concatenate the 8 visual tokens with the 6 time tokens to form the visual inputs for each frame. 3.2.2 Task-interleaved sequence modeling Utilizing the processed task tokens, we construct the sequence following Eq. 2. The token sequence order is illustrated in Figure 3. Inter-event sequence order. The sequence commences with visual frame tokens followed by textual instruction tokens I. For the events section, event tokens are sequenced according to the events occurrence time to align with the causal event modeling formula P(eke1:k1, I, F). Intra-event sequence order. For each event, in accordance with Eq. 2, tokens are arranged sequentially by time tokens (P(tke1:k1, I, F)), score tokens (P(sktk, e1:k1, I, F)), and text tokens (P(cksk, tk, e1:k1, I, F)). Consequently, the causal event modeling framework (Eq. 2) emerges as specialized autoaggressive model, featuring unique sequence order that closely aligns with video structures. 3.2.3 Adaptive Head-Switching Mechanism for Generation Using sync token for adaptive head switching. Since TRACE employs distinct decoding heads for various tasks during training, selecting the appropriate decoding head during generation based on previously decoded tokens is crucial. This selection is facilitated by the sync token. As illustrated in Figure 4, TRACE generates tokens in the sequence of time, score, and text tokens. Detection of the sync token prompts TRACE to switch decoding heads accordingly. The heads are cycled switched in the order of time head - score head - text head. 3.3 Training Strategy and Data Preparation This section outlines the TRACE training process, which includes two stages. For the stage 1, task modules such as the vision compression layer, task encoder, and task heads are trained for initialization. For the stage 2, the LLM backbone is fine-tuned while keeping the task modules tuned. Detailed settings and datasets are presented below. Due to the page limitation, detailed annotation examples for each task, and details about data filtering and processing are provided in Appendix A. Stage 1: Initialization of task modules. In stage 1, task modules such as the vision compression layer, time encoder/head, and score encoder/head are trained while the vision encoder and LLM backbone remain fixed. As shown in Table 1, stage 1 primarily utilizes two groups of datasets. 5 Figure 4: Generation process of TRACE. The TRACE generate tokens following the order of time tokens, score tokens, and text tokens. The decoding heads are switched when sync tokens are generated. Table 1: Datasets used for TRACE training process. \"Compressed\" indicates that datasets are condensed by retaining only one sample for samples with identical videos but varying instructions. The transparent text denotes the setting of TRACE-uni, which involves the inclusion of additional training data pertaining to general video understanding tasks. Stage Stage Stage 2 Datasets Valley, LLaVA_Image, TextVR, ShareGPT4Video, VTG-IT Valley (Compressed), TextVR (Compressed), ShareGPT4Video (Compressed), VTG-IT, ActivityNet Captions, VideoChatGPT, InternVid, Next-QA, LLaVA-Video178K Quantity 1.9M 0.9M (1.77M) Image and video caption datasets for initializing the visual compression layer. This group of datasets including Valley (Luo et al., 2023b), LLaVA_Image (Liu et al., 2024b), TextVR (Wu et al., 2025), and randomly sampled subset of ShareGPT4Video (Chen et al., 2024a) datasets. VTG datasets for task encoder/head initialization. We use VTG-IT dataset in this group. For stage 1 training, we uniformly sample 128 frames from each video. The learning rate is set to 1e-3, and models are trained for one epoch with batch size of 128. Stage 2: Instruction tuning for enhancing VTG capacity. In Stage 2, the LLM backbone and task modules are fine-tuned, with only the vision encoder remaining fixed. As shown in Table 1, stage 2 primarily utilizes three groups of datasets. VTG instruction tuning datasets for enhancing VTG capacity. We use VTG-IT (Guo et al., 2024), ActivityNet Captions (Fabian Caba Heilbron & Niebles, 2015), and subset of InternVid (Wang et al., 2023b), resulting in total of 635K data samples. Low-quality samples were filtered out, and the VTG-IT-VHD and VTG-IT-VS datasets were re-annotated. Additional details can be found in Appendix A. Video caption datasets for maintaining the quality of the visual compression layers. We use parts of the video data from stage 1, such as Valley (Luo et al., 2023b), TextVR (Wu et al., 2025), and ShareGPT4Video (Chen et al., 2024a) datasets. These datasets are compressed by retaining only one sample for samples with identical videos but different instructions, yielding 284K data. Video question answering datasets to enhance TRACEs reasoning capabilities. We use VideoChatGPT (Maaz et al., 2023) and Next-QA (Xiao et al., 2021) in this part. For TRACE-uni, we have incorporated the YouTube and perception-test components from LLaVA-Video-178K (Zhang et al., 2024). For each video, the content is uniformly divided into 128 clips, with one frame randomly sampled from each clip. The learning rate is set to 5e-6, and the models are trained for two epochs using batch size of 128."
        },
        {
            "title": "4 Experiments",
            "content": "Detailed experimental settings and hyper-parameters can be found in Appendix B.1. Case studies can be found in Appendix B.3 6 4.1 Evaluation Datasets, Metrics, and Baseline Models. We evaluate the model performance on three different tasks: Dense video caption. We use Youcook2 (Zhou et al., 2018) and ActivityNet Captions (Fabian Caba Heilbron & Niebles, 2015) datasets as the evaluation datasets. The evaluation metrics include CIDEr (Vedantam et al., 2015), METEOR (Banerjee & Lavie, 2005), and SODA_c (Fujita et al., 2020) for assessing the quality of the captions. These metrics are averaged under different IoU thresholds {0.3, 0.5, 0.7, 0.9}, following previous studies (Ren et al., 2023; Huang et al., 2023). Additionally, we report the F1 score to measure the models ability to accurately locate timestamps. Moment retrieval. We utilize test set of Charades-STA (Gao et al., 2017) for the moment retrieval task and report the recall at IOU thresholds of 0.5 and 0.7. Additionally, we present the mIOU results. Video highlight detection. We employ the validation set of the QVHighlights dataset (Lei et al., 2021) and report the mean average precision (mAP) with IOU thresholds of 0.5 and 0.75, as well as the HIT@1, which represents the hit ratio of the highest scored clip. For baseline models, we select Valley (Luo et al., 2023b), VideoChat (Li et al., 2023b), Video-ChatGPT (Maaz et al., 2023), and Video-LLaMA (Zhang et al., 2023) as examples of traditional video LLMs. For video LLMs specifically designed for VTG tasks, we choose TimeChat (Ren et al., 2023), VTimeLLM (Huang et al., 2023), Momentor (Qian et al., 2024), HawkEye (Wang et al., 2024b), and VTG-LLM (Guo et al., 2024). 4.2 Performance of TRACE Superior zero-shot performance of TRACE over other video LLMs. of TRACE compare to SOTA video LLM baselines. The results show that In Table 2, we show the zero-shot performance Suprior zero-shot performance. As shown in Table 2, TRACE significantly outperforms other video LLMs by substantial margin across all three datasets. Notably, it achieves 3.1 and 4.9% performance improvement on the Youcook2 dataset using the CIDEr and F1 Score metrics; 6.5% and 3.7% performance increase in Recall with IOU = {0.5, 0.7} thresholds on the Charades-STA dataset; and 10.3% and 9.2% performance gain for the mAP and HIT@1 metrics on the QVHighlights dataset. Better performance than task-specific models and larger LLMs. As shown in Table 2, as generalist model capable of handling various tasks, the performance of TRACE surpasses that of task-specific models like HawkEye (Wang et al., 2024b). Furthermore, the 7B TRACE model outperforms the VTimeLLM (13B) model (Huang et al., 2023), further validating the advantages of the TRACE architecture. Effectiveness of the TRACE architecture in addressing general video understanding tasks. As illustrated in Table 6 and 2, we observe that: (1) despite not being trained on extensive multi-task datasets, TRACE is still effective in managing general QA tasks; (2) by incorporating additional training data for general video understanding tasks, the TRACE-uni achieves superior performance compared to TRACE in both VTG tasks and general video understanding tasks. Performance of TRACE on ActivityNet Captions dataset. In Table 4, we show the performance of TRACE on ActivityNet Captions dataset. All the reported algorithms except for Momentor (Qian et al., 2024) have incorporated the ActivityNet Captions dataset as part of the training data. Results show that the TRACE attains the best performance in both moment retrieval tasks and dense video caption tasks. 4.3 Ablation Studies of TRACE. The causal event modeling framework enhances model performance in VTG tasks. In the Ablation Studies on Architecture section of Table 3, we conducted experiments without utilizing the causal event modeling framework. The results indicate that employing the causal event modeling framework significantly improves model performance, and TRACE can achieve better results even when sampling fewer video frames. Using different encoders and decoding heads for different tasks is essential for TRACE to achieve the best result. In the \"w/o independent ecoder/heads\" part of Table 3, we performed ablation studies by not utilizing separate encoders and decoder heads for different tasks. Instead, we directly incorporated time tokens and score tokens into the text tokenizers. The results suggest that using shared encoder/decoding heads for causal event modeling framework significantly disrupts the prelearned knowledge of LLMs, leading to irrelevant and meaningless responses. The performance of TRACE improves with the increase in the number of frames. We conducted ablation studies on the number of sampled frames, as presented in Table 3. The results show that (1) the performance of TRACE 7 Table 2: Zero-shot performance of algorithms over various tasks. We evaluated the performance of VTG-LLM using the Youcook2, Charades-STA, and QVHighlights datasets. We highlight the best results for each block using bold font. The Valley, VideoChat-Embed, and Video-LLaMA results are elaborated from previous studies (Ren et al., 2023; Huang et al., 2023; Qian et al., 2024). The results with transparent text indicates unfair comparison (13B)."
        },
        {
            "title": "Model",
            "content": "Traditional Video LLMs Valley (7B) VideoChat (7B) Video-LLaMA (7B) Temporal Grounding Video LLMs TimeChat (7B) VTimeLLM (7B) VTimeLLM (13B) Momentor (7B) HawkEye (7B) VTG-LLM (7B) TRACE (7B) TRACE-uni (7B) Youcook2 Charades-STA"
        },
        {
            "title": "QVHighlights",
            "content": "SODA_c CIDEr F1 Score R@1(IOU=0.5) R@1(IOU=0.7) mAP HIT@1 0.1 0.2 0.0 0.0 0.6 0.0 1.5 3.4 0.1 1.2 3. 12.6 1.5 2.2 2.3 5.0 8.1 8.6 17. 22.4 22.4 4.7 3.2 2.7 32.2 27.5 34.3 26.6 31.4 33.8 40.3 43.7 1.6 1.4 1.2 13.4 11.4 14.7 11.6 14.5 15. 19.4 21.0 10.9 13.1 11.3 15.2 18.1 15.6 14.5 23.9 7. 16.5 33.5 26.8 27.5 42.7 43.9 Table 3: Ablation studies of TRACE. All the algorithms solely utilize VTG-IT (Guo et al., 2024) during fine-tuning for efficient evaluation. The \"w/o causal event modeling \" approach indicates the use of natural language-style inputs similar to previous studies (Guo et al., 2024; Ren et al., 2023). The \"w/o independent encoder/heads\" approach signifies directly adding new tokens to the LLM tokenizer instead of employing separate encoders/heads for different tasks. We highlight the best results using bold font for each block."
        },
        {
            "title": "Frame Number",
            "content": "Youcook2 Charades-STA SODA_c CIDEr F1 Score R@1(IOU=0.5) R@1(IOU=0.7) Ablation Studies on Architecture w/o causal event modeling w/o independent encoder/heads TRACE (VTG-IT) Ablation Studies on Frame Number TRACE (VTG-IT) TRACE (VTG-IT) TRACE (VTG-IT) 96 64 8 64 128 1.4 1.9 1.4 1.9 2.1 17.2 4.3 -Fail to Follow Instruction6. 29.7 37.0 21.4 5.0 6.9 7.5 18.6 21.4 21.4 28.8 37.0 41. 14.0 17.0 13.6 17.0 20.0 enhances as the number of sampled frames increases; (2) the performance of TRACE is comparable or even superior to SOTA video LLMs like VTG-LLM and TimeChat when sampling just 8 frames, demonstrating the effectiveness of the TRACE model architecture. Incorporating InternVid (Wang et al., 2023b) and ActivityNet Captions (Fabian Caba Heilbron & Niebles, 2015) datasets boost TRACE performance on long videos. As illustrated in Figure 5, we carried out ablation studies by exclusively using VTG-IT as the training data for VTG tasks. The results indicate that the performance of TRACE on long videos improves when incorporating internVid and ActivityNet Captions datasets, leading to enhanced performance on Youcook2, QVHighlights, and ActivityNet Captions datasets. Conversely, the performance of TRACE on short videos slightly decreases (Charades-STA), suggesting that the annotations in the internVid and ActivityNet Captions datasets may not be as accurate as those in short video annotations. (a) Performance on Youcook2 (b) Performance on Charades-STA (c) Performance on QVHighlights (d) Performance on ActivityNet Captions Figure 5: Ablation studies on data utilized while training TRACE. We conduct experiments solely utilizing VTG-IT and compare its performance with that of the original TRACE. 4.4 Fine-tuned Performance of TRACE. Competitive performance of TRACE to traditional methods after fine-tuning. for 3 epochs on Youcook2 and Charades-STA datasets 3. The results indicate that In Table 5, we fine-tune the TRACE TRACE significantly outperform generalist baselines. In contrast to TimeChat and VTG-LLM, which struggle to attain satisfactory performance even after fine-tuning, the TRACE derives significant benefits from fine-tuning and achieves notably better performance than generalist baselines. These results further substantiate that our enhancements to the model architecture are crucial for VTG tasks. TRACE achieve comparable performance to non-generative and task-specific SOTAs. As depicted in Table 5, the TRACE achieves new SOTA results on Youcook2 (without audio inputs). Furthermore, the performance of TRACE on the Charades-STA dataset is also competitive with non-generative models such as InternVideo2 and VDI. However, these methods cannot handle various tasks simultaneously and lack zero-shot capability the contribution of TRACE herein."
        },
        {
            "title": "5 Conclusion and Future Works",
            "content": "In this paper, our goal is to address the mismatch between video structure and video LLMs on VTG tasks, and propose causal event modeling framework and the TRACE model as solution. Numerical results indicate the superior zero-shot performance of TRACE compared to other video LLM baselines, and TRACE also achieves competitive performance relative to traditional non-generative and task-specific models after fine-tuning. By overcoming the inherent limitations of video LLM architectures, TRACE demonstrates the potential of video LLMs on VTG tasks, and we believe that the TRACE could be strong foundation for future research on video LLMs in VTG tasks. However, there are future works that can further enhance the capabilities of TRACE. For instance, incorporating additional training data on general tasks to improve the performance of TRACE in general video understanding tasks and expanding the VTG dataset. Furthermore, considering that current annotations for general video understanding tasks do not include timestamp and score information, expanding the annotation of these tasks by incorporating the 3Results on QVHighlights can be found in Appendix B.2. 9 Table 4: Performance of TRACE on ActivityNet Captions dataset. The evaluation of TimeChats and VTG-LLMs results was conducted using the official provided checkpoints. The indicates zero-shot evaluation. We highlight the best and the second best results using bold and underline."
        },
        {
            "title": "Models",
            "content": "VTimeLLM Momentor TimeChat VTG-LLM TRACE TRACE-uni"
        },
        {
            "title": "Moment Retrieval",
            "content": "METEOR SODA_c CIDEr F1 Score R@1(IOU=0.5) R@1(IOU=0.7) mIOU 6.8 4.7 5.7 5.9 6.4 6.9 5.8 2.3 4.7 5.1 6.0 6.4 27.6 14.9 19.0 20. 25.9 29.2 36.9 34.8 39.3 40.4 29.5 23.0 4.6 8.3 37.7 38.2 14.2 12.4 2.0 3. 24.0 24.7 31.4 29.3 6.9 12.0 39.0 39.4 Table 5: Fine-tuned performance of TRACE. We fine-tune the TRACE for 3 epochs on the Youcook2 and Charades-STA datasets. We emphasize the best and second best results using bold font and underline. For Youcook2 dataset, we choose Vid2Seq (Yang et al., 2023), PDVC (Wang et al., 2021), and CM2 (Kim et al., 2024) as task-specific baselines. The results depicted in gray indicate unfair comparisons due to additional audio inputs and different architectures. For charades-STA dataset, we choose InternVideo2-6B (Wang et al., 2024a), VDI (Luo et al., 2023a), and Moment-DETR (Lei et al., 2021) as examples of non-generative models."
        },
        {
            "title": "Model",
            "content": "Youcook"
        },
        {
            "title": "Model",
            "content": "SODA_c CIDEr F1 Score Charades-STA R@1(IOU=0.5) R@1(IOU=0.7) Task-Specific Models PDVC Vid2Seq (Audio Input) Vid2Seq CM2 Generalist Models TimeChat VTG-LLM TRACE 4.4 7.9 5.7 5. 3.4 3.6 6.7 22.7 47.1 25.3 31.7 11.0 13.4 35.5 27.3 23.5 28.4 19.5 20.6 31.8 Non-Generative Models InternVideo2-6B VDI Moment-DETR Generative Models HawkEye TimeChat VTG-LLM TRACE 70.0 52.3 55.7 58.3 46.7 57.2 61.7 49.0 31.4 34.2 28.8 23.7 33.4 41.4 Table 6: Performance of TRACE on general video understanding tasks."
        },
        {
            "title": "Models",
            "content": "MVBench (Li et al., 2024b) VideoMME (w/o subtitle) (Fu et al., 2024) Video-LLaVA (Lin et al., 2023a) LLaVA-NeXT-Video (Liu et al., 2024a) VideoChat2 (Li et al., 2024b) VideoLLaMA 2 (Cheng et al., 2024) TRACE TRACE-uni 41.0 46.5 51.1 54.6 48.1 53.8 39.9 33.7 33.7 44. 43.8 49.6 occurrence timestamps of QA pairs and the matching score between questions and answers could significantly improve the overall performance of TRACE."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. Meinardus Boris, Batra Anil, Rohrbach Anna, and Rohrbach Marcus. The surprising effectiveness of multimodal large language models for video moment retrieval. arXiv preprint arXiv:2406.18113, 2024. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024a. Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audiosubtitle-text omni-modality foundation model and dataset. Advances in Neural Information Processing Systems, 36, 2024b. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 961970, 2015. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Soichiro Fujita, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, and Masaaki Nagata. Soda: Story oriented dense video captioning evaluation framework. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VI 16, pp. 517531. Springer, 2020. Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pp. 52675275, 2017. Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instructiontuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731, 2023. Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Xi Chen, and Bo Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. arXiv preprint arXiv:2405.13382, 2024. Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. Creating summaries from user videos. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13, pp. 505520. Springer, 2014. Donghoon Han, Seunghyeon Seo, Eunhwan Park, Seong-Uk Nam, and Nojun Kwak. Unleash the potential of clip for video highlight detection. arXiv preprint arXiv:2404.01745, 2024. Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with temporal language. In Empirical Methods in Natural Language Processing (EMNLP), 2018a. Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with temporal language. In Empirical Methods in Natural Language Processing (EMNLP), 2018b. Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo Luo. V2xum-llm: Cross-modal video summarization with temporal prompt instruction tuning. arXiv preprint arXiv:2404.12353, 2024. Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. arXiv preprint arXiv:2311.18445, 2(3):9, 2023. De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. arXiv preprint arXiv:2403.19046, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 11 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, and Seong Tae Kim. Do you remember? dense video captioning with cross-modal memory retrieval. arXiv preprint arXiv:2404.07610, 2024. Lei, TL Berg, and Bansal. Qvhighlights: Detecting moments and highlights in videos via natural language queries.(2021). URL https://arxiv. org/abs/2107.09609. Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. Advances in Neural Information Processing Systems, 34:1184611858, 2021. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023b. Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1994819960, 2023c. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. In Proceedings of the IEEE/CVF Mvbench: comprehensive multi-modal video understanding benchmark. Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024b. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023a. Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified video-language temporal grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 27942804, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Ye Liu, Siyuan Li, Yang Wu, Chang-Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 30423051, 2022. Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, and Yang Liu. Towards generalisable video moment retrieval: Visual-dynamic injection to image-text pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2304523055, 2023a. Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023b. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. Andreea-Maria Oncescu, Joao Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. Queryd: video In ICASSP 2021-2021 IEEE International Conference on dataset with high-quality text and audio narrations. Acoustics, Speech and Signal Processing (ICASSP), pp. 22652269. IEEE, 2021. Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. arXiv preprint arXiv:2312.02051, 2023. 12 Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Summarizing web videos using titles. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 51795187, 2015. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12071216, 2019. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45664575, 2015. Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end dense video captioning with parallel decoding. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 68476857, 2021. Teng Wang, Jinrui Zhang, Feng Zheng, Wenhao Jiang, Ran Cheng, and Ping Luo. Learning grounded vision-language representation for versatile understanding in untrimmed videos. arXiv preprint arXiv:2303.06378, 2023a. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023b. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024a. Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos. arXiv preprint arXiv:2403.10228, 2024b. Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Hong Zhou, Mike Zheng Shou, and Xiang Bai. large cross-modal video retrieval dataset with reading comprehension. Pattern Recognition, 157:110818, 2025. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97779786, 2021. Yicheng Xiao, Zhuoyan Luo, Yong Liu, Yue Ma, Hengwei Bian, Yatai Ji, Yujiu Yang, and Xiu Li. Bridging the gap: unified video comprehension framework for moment retrieval and highlight detection. arXiv preprint arXiv:2311.16464, 2023. Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084, 2021. Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu. Videococa: Video-text modeling with zero-shot transfer from contrastive captioners. arXiv preprint arXiv:2212.04979, 2022. Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1071410726, 2023. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2305623065, 2023a. Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In CVPR, 2023b. 13 Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. In Advances in Neural Information Processing Systems 34, 2021. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. URL https://arxiv.org/abs/2410.02713. Long Zhao, Nitesh Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et al. Videoprism: foundational visual encoder for video understanding. arXiv preprint arXiv:2402.13217, 2024. Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        },
        {
            "title": "Contents of Appendix",
            "content": "A Dataset Preparation A.1 Details of data format . A.2 Processing InternVid . . A.3 Processing VTG-IT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiments B.1 Detailed Experimental Settings . B.2 Additional Experiment Results . . B.3 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 15 17 17 17"
        },
        {
            "title": "A Dataset Preparation",
            "content": "A.1 Details of data format In this section, we introduce the details of the data format we used while training TRACE. For the VTG datasets, in addition to ActivityNet Captions dataset and InternVid dataset, we directly use the annotation collected by VTG-IT (Guo et al., 2024). In detail, the annotations can be categories into the following four types: General tasks (Figure 6). For general tasks such as video captioning, image captioning, and video question answering, the answer component of the data does not include timestamps or scores. Consequently, we employ single token sync as placeholder for timestamps and scores, signifying an empty response for this part of response. This kind of annotation including LLaVA_Image (Liu et al., 2024b), Valley (Luo et al., 2023b), TextVR (Wu et al., 2025), ShareGPT4Video (Chen et al., 2024a), VideoChatGPT (Maaz et al., 2023), and Next-QA (Xiao et al., 2021) datasets. Dense video caption task (Figure 7). The Dense Video Captioning task solely comprises timestamps and textual captions responses. As result, we use single token sync as placeholder for scores. The datasets of this task include HiRESTstep (Zala et al., 2023a), COIN (Tang et al., 2019), ActivityNet Captions (Fabian Caba Heilbron & Niebles, 2015), VTG-IT-DVC (Guo et al., 2024), and InternVid (Wang et al., 2023b) datasets. Moment retrieval task (Figure 8). Similiar to dense video caption task, moment retrieval solely comprises timestamps and textual captions responses. The moment retrieval task including HiRESTgrounding (Zala et al., 2023a), QuerYD (Oncescu et al., 2021), DiDeMo (Hendricks et al., 2018b), VTG-IT-MR (Guo et al., 2024), and InternVid (Wang et al., 2023b) datasets. Video highlight detection task (Figure 9). For the video highlight detection task, we utilize the query as the textual response for all highlight moments. In this case, we employ the VTG-IT-VHD (Guo et al., 2024) dataset. Video summarization task (Figure 10). The video summarization task employs the caption of each event as the textual response. We use the VTG-IT-VS (Guo et al., 2024) dataset here. A.2 Processing InternVid Dense video caption data. The dense video caption data is constructed based on the InternVid-Full annotations. For each video, we opt not to use the annotation of that video as dense video caption data if any of the following checklist items are met: There exists caption with fewer than 5 words. There exists caption very similar to another caption within one video. We use fuzzywuzzy package here and set the threshold to 70. The number of events is less than 5 or greater than 50. There are special characters present, excluding letters, spaces, and dots. Moment retrieval data. We discovered that directly utilizing the InternVid-Full annotations to construct the moment retrieval data leads to suboptimal performance, likely due to the imprecise timestamp annotations. Consequently, we employ the InternVid-10M-FLT-INFO annotation to construct the moment retrieval data, which is filtered subset of the InternVid-Full annotation provided by the authors. Figure 6: Annotation example of video caption task. Figure 7: Annotation example of dense video caption task. 16 Figure 8: Annotation example of moment retrieval task. A.3 Processing VTG-IT In this section, we describe the processing of the VTG-IT dataset. For the dense video caption and moment retrieval tasks, we directly utilize the annotations provided by VTG-IT. However, the data for video highlight detection and video summarization tasks supplied by VTG-IT often have uniform salient scores within each video. As result, our goal here is to enhance the quality of data for these two tasks, specifically video highlight detection and video summarization. Video highlight detection task. For each event in the dense video caption task, we initially divide each event into maximum of 20 clips. Subsequently, we compute the similarity between the frames within each clip and the event captions using ViT-G/14 from EVA-CLIP (Sun et al., 2023). The similarity scores are then normalized to Gaussian distribution to serve as the highlight score. In detail, the clips with scores higher than [2.275%, 3.593%, 5.480%, 8.076%, 11.507%, 15.866%, 21.186%, 27.425%, 34.458%, 42.074%, 50.000%, 57.926%, 65.542%, 72.575%, 78.814%, 84.134%, 88.493%, 91.924%, 94.520%, 96.407%, 97.725%] of other clips will be assign scores [1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0, 4.2, 4.4, 4.6, 4.8, 5.0]. Video summarization task. The data for the video summarization task is built upon the foundation of the video highlight detection task and the dense video caption task. Specifically, for each event in the dense video caption task data, we select the clip with the highest score to serve as the summarization clip for that particular event."
        },
        {
            "title": "B Experiments",
            "content": "B.1 Detailed Experimental Settings We report the detailed model architecture design and training hyper-parameters in Table 7. The training takes about 5 days for stage 1 and 5 days for stage 2. B.2 Additional Experiment Results Ablation studies on training data. In Tables 8 and 9, we report the performance of TRACE using different training data. The results show that (1) the performance of TRACE on long videos increase while using the original TRACE setting. (2) The performance of TRACE slightly reduced on short videos (Charades-STA). 17 Figure 9: Annotation example of video highlight detection task. 18 Figure 10: Annotation example of video summarization task. Table 7: Detailed training setting and hyper-parameters. Setting Stage 1 Stage Computation Vision Encoder DeepSpeed Stage LLM Batch Size Num Frames Frame Sample Train Epochs Learning Rate LR Scheduler Model Max Length 16 ATN 910B openai/clip-vit-large-patch14-336 Zero3 Offload Mistral-7B-v0.2 128 128 Uniform 1 1e-3 Cosine 4096 16 ATN 910B openai/clip-vit-large-patch14-336 Zero3 Offload Mistral-7B-v0.2 128 128 Split to 128 clips then random within clip 2 5e-6 Cosine"
        },
        {
            "title": "Models",
            "content": "Table 8: Ablation studies on training data (Youcook2 and Charades-STA). Youcook2 Charades-STA SODA_c CIDEr F1 Score METEOR R@1(IOU=0.5) R@1(IOU=0.7) mIOU TRACE (VTG-IT) TRACE 2.1 2. 7.5 8.1 21.4 22.4 2.6 2.8 41.2 40.3 20.0 19.4 38.9 38. Fine-tuned performance on QVHighlights. In Table 10, we show the performance of TRACE on QVHighlights dataset after fine-tuning. The results indicate that TRACE significantly outperform other video LLMs by large margin. B.3 Case Studies We present the case studies of TRACE in Figure 11. The results demonstrate that TRACE can accurately identify the events within the given video and is also proficient in performing traditional video captioning tasks."
        },
        {
            "title": "Models",
            "content": "Table 9: Ablation studies on training data (ActivityNet Captions)."
        },
        {
            "title": "Moment Retrieval",
            "content": "SODA_c CIDEr F1 Score METEOR R@1(IOU=0.5) R@1(IOU=0.7) mIOU TRACE (VTG-IT) TRACE 5.8 6.0 24.7 25.9 38.9 39.3 6.0 6. 19.2 37.7 9.3 24.0 25.0 39.0 Table 10: Fine-tuned performance of algorithms on QVHighlights datasets. We fine-tune the Algorithm on QVHighlights datasets. Model QVHighlights mAP HIT@1 21.7 TimeChat VTG-LLM 24.1 31.8 TRACE 37.9 41.3 51.5 Figure 11: Case study of TRACE."
        }
    ],
    "affiliations": [
        "School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China",
        "Tencent PCG",
        "The Guangdong Provincial Key Laboratory of Future Networks of Intelligence",
        "The Shenzhen Institute of Artificial Intelligence and Robotics for Society"
    ]
}