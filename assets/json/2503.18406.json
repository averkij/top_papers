{
    "paper_title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning",
    "authors": [
        "Sherry X. Chen",
        "Misha Sra",
        "Pradeep Sen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to challenges in creating large, high-quality training datasets. Previous work has typically relied on text-toimage (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fail to align with the specified edit instructions due to the limitations of T2I models, which negatively impacts models trained on such datasets. To address this, we present Instruct-CLIP, a self-supervised method that learns the semantic changes between original and edited images to refine and better align the instructions in existing datasets. Furthermore, we adapt Instruct-CLIP to handle noisy latent images and diffusion timesteps so that it can be used to train latent diffusion models (LDMs) [19] and efficiently enforce alignment between the edit instruction and the image changes in latent space at any step of the diffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset and get over 120K refined samples we then use to fine-tune their model, guided by our novel Instruct-CLIP-based loss function. The resulting model can produce edits that are more aligned with the given instructions. Our code and dataset are available at https://github.com/SherryXTChen/Instruct-CLIP.git."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 6 0 4 8 1 . 3 0 5 2 : r Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning Sherry X. Chen"
        },
        {
            "title": "Pradeep Sen",
            "content": "University of California, Santa Barbara xchen774,sra,psen@ucsb.edu"
        },
        {
            "title": "Abstract",
            "content": "Original IP2P I-CLIP (Ours) Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to challenges in creating large, high-quality training datasets. Previous work has typically relied on text-toimage (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fail to align with the specified edit instructions due to the limitations of T2I models, which negatively impacts models trained on such datasets. To address this, we present Instruct-CLIP, self-supervised method that learns the semantic changes between original and edited images to refine and better align the instructions in existing datasets. Furthermore, we adapt Instruct-CLIP to handle noisy latent images and diffusion timesteps so that it can be used to train latent diffusion models (LDMs) [23] and efficiently enforce alignment between the edit instruction and the image changes in latent space at any step of the diffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset and get over 120K refined samples we then use to fine-tune their model, guided by our novel Instruct-CLIP-based loss function. The resulting model can produce edits that are more aligned with the given instructions. Our code and dataset are available at https://github.com/SherryXTChen/Instruct-CLIP.git. 1. Introduction Recent advances in instruction-guided image editing have introduced intuitive and powerful approaches requiring only single text instruction for image editing [1, 30, 32 In particular, InstructPix2Pix (IP2P) [1] pioneered 34]. diffusion-based instruction-driven image editing, serving as the foundation for numerous subsequent works [11, 16, 32]. Change the panda to monkey Let the old man be dressed in military uniform Change the background to classroom The cat needs to be black Make it chocolate cake Figure 1. Sample results showcasing the strength of our Instruct-CLIP method (I-CLIP), compared to the baseline InstructPix2Pix (IP2P) [1]. These methods leverage pre-trained text-to-image (T2I) models [3, 15, 20, 23, 25] and condition the generation process on the edit instructions instead of the usual prompts. 1 Original Edited Original Edited ORIGINAL: by andy warhol (0.130) REFINED: make the sky pink (0.263) ORIGINAL: make the waves into hurricane (0.090) REFINED: add lightning storm to the sky (0.289) ORIGINAL: make it morning (0.179) REFINED: make it foggy morning (0.324) ORIGINAL: make it landscape (0.090) REFINED: photograph (0.233) ORIGINAL: make it daydream (0.113) REFINED: make it spring (0.300) ORIGINAL: make it video game character (0.097) REFINED: as 3 sculpture (0.357) Figure 2. Problems with existing instruction-guided imageediting datasets [1]. As shown, there are many examples where the datasets original edit instruction does not match the actual changes in the images. Our I-CLIP approach refines edit instructions to match the visual change better and allows us to train system that produces better outputs. The values in parentheses are the cosine similarity between the visual change from the original to the edited image and the edit instruction from I-CLIP. However, these models still need to be fine-tuned on appropriate datasets to show them how to make instructionguided edits. Some work has created these datasets by using T2I models to approximate the desired behavior of an instructionguided image editing system. For example, several approaches [1, 33, 34] leverage Prompt-to-Prompt [7] to generate pair of images that look like one is an edited version of the other by using pair of appropriately modified prompts. separate, fine-tuned large-language model (LLM), such as GPT3 [2], is then used to generate plausible edit instructions from these prompts. Given the generality of Prompt-to-Prompt and LLMs, such datasets can cover wide range of instructions. However, the quality of image editing is often limited by the capabilities of the generative methods, resulting in problems when the changes between the original and the edited image are misaligned with the edit instruction  (Fig. 2)  . This affects the performance on models trained on these datasets, as we can see with the IP2P results in Fig. 1. Although improving the datasets would significantly enhance the performance of models trained on them, doing so is difficult. On one hand, manually creating edit instructions and corresponding outputs is extremely labor-intensive and thereby impractical. On the other hand, constructing an advanced pipeline for instruction-guided image editing dataset creation that is able of performing edits effectively is nontrivial. After all, if such pipeline existed, it would be successful instruction-guided, image-editing method itself. Furthermore, creating new dataset may not be necessary. Despite the inherent limitations of previous datasets, we observe that they still provide some signal for training image editing models. After all, as shown in Fig. 1, we see that models trained on them produce results that, while imperfect, show that the system is understanding something of the desired edit. Can we harness this signal to refine the dataset and address its issues? Inspired by CLIP [22], which learns rich semantic alignments through contrastive language-image pre-training, we embed original-edited image pairs and their corresponding edit instructions into the same feature space in similar manner with neural network, which we call it Instruct-CLIP, followed by separate text decoder to generate more precise instructions from the learned features [12]. Instruct-CLIP is equipped with modified DINOv2 [18] backbone capable of handling noisy latent images in Stable Diffusion (SD) [23] so it can not only enhance training data quality in the pre-processing stage but also serve as an efficient learning objective during training. We use Instruct-CLIP to refine instructions in the IP2P dataset [1], resulting in 120K+ unique, enhanced samples  (Fig. 2)  . The IP2P model is then fine-tuned on this dataset with an Instruct-CLIP-guided loss, enabling the trained instructionguided editing model to produce results more faithfully aligned with the instructions (Figure 5). In summary, our contributions are: dual-purpose model that refines dataset instructions and enhances editing model training. large-scale dataset with over 120K samples featuring more accurate and enriched instructions. An instruction-guided image editing method trained on above dataset. 2. Related Work 2.1. Text-to-image diffusion-based image editing Diffusion models [3, 4, 15, 20, 23, 25] have set new standard for image generation quality, and form the basis of various text-to-image (T2I) editing applications. range of methods [7, 17, 19, 2628] aim to edit given image to semantically align with target prompt, using the input prompt as an anchor. Traditional T2I models like Stable Diffusion (SD) [23] often yield inconsistent results even with similar prompts, where both subject and context can change significantly. Prompt-to-Prompt [7] addresses this by preserving attention maps for the shared components in the input and the target prompt to improve image visual alignment, but both images are synthesized by the approach and it cannot handle arbitrary input images. Null-text inversion [17] overcomes this by reconstructing an input image in SD based on the input prompt. They do this by optimizing the empty-text as the negative prompt, enabling controlled edits when combined with Prompt-to-Prompt. Other methods similarly leverage diffusion inversion techniques paired with unique editing mechanisms: Diffusion Disentanglement [28] aims to find the optimal blend of input and target prompt features to find the compromise between visual alignment with the input image and semantic alignment with the target prompt; EDICT [27] uses coupling inversion for improved reconstruction, which in turn enhance editing performance; pix2pix-zero [19] optimizes model attention maps for the target image to match the ones for the input image for visual consistency between the two; and Plug-and-Play [26] integrates self-attention maps from the input image to guide the edited image generation. While each method has its own advantages, major shared shortcoming is they require prompts for both input and output images, which can be cumbersome for users. 2.2. Instruction-guided image editing new line of work has emerged that offers more userfriendly approach to image editing, requiring only single edit instruction [1, 11, 16, 30, 32, 33]. Most of these methods leverage priors from the aforementioned SD model. For example, InstructPix2Pix (IP2P) [1] replaces the prompt with an instruction to condition the generative process of the edited image, where the input image is concatenated as an additional condition. Among these instruction-guided image editing methods, HIVE [33] collects user feedback on outputs to train reward model, which iteratively improves the models output. Watch Your Step [16] and ZONE [11] both use masking mechanism either by measuring the discrepancy between IP2P predictions with and without the instruction, or by employing region intersection-overunion (IoU) scheme with segmentation model. This helps them avoid editing instruction-irrelevant regions, resulting in better localized editing capabilities. 2.3. Instruction-guided datasets Regardless what additional control mechanism is introduced to improve instruction-guided editing results, the development of instruction-guided image editing models fundamentally relies on suitable training datasets. However, creating large-scale, high-quality datasets automatically presents significant challenge, as such data creation system would effectively constitute an editing method itself. Prior works have addressed this challenge by combining existing synthesis models to approximate the behavior of desired instruction-guided image editing methods, primarily following two distinct approaches. The first approach, adopted by several studies [1, 33, 34] utilize aforementioned Prompt-to-Prompt [7] to generate pairs of original and edited images using corresponding pairs of prompts that describe the content of each image. These systems typically fine-tune large language model (LLM) such as GPT-3 [2] is also fine-tuned to produce instructions from pairs of prompts. to generate instructions from the prompt pairs. While this approach can produce diverse samples, the dataset quality is inherently limited by both the Prompt-to-Prompt method and the LLMs accuracy, frequently resulting in misalignment between the provided instruction and the actual transformation from the original to the edited image. The second approach attempts to address these limitations by creating datasets through inpainting [30, 32]. In this method, edited ground truth images are generated by inpainting selected regions of original images. Although this technique improves the quality of localized image editing samples, it has two significant drawbacks: it cannot generate samples with global modifications (such as style transfer), and it often requires manual creation of inpainting regions. These limitations not only increase the cost of dataset creation but also substantially restrict the dataset size. 3. Instruct-CLIP Our goal is to improve previous instruction-guided image editing methods by enhancing the quality of edit instructions in available datasets [1, 9, 29, 33]. However, while some instructions in these datasets do not reflect the actual changes between the original the edited images, the datasets overall still provide enough signal to train editing models. The main challenge is therefore how to properly leverage this signal to refine the given edit instructions and consequently improve the overall dataset quality. To this end, we propose Instruct-CLIP (I-CLIP for short), which embeds the visual change between the original image and the edited result into the same feature space as the corresponding edit instruction through contrastive learning with neural network. Our approach is inspired by CLIP [22], which learns the semantic alignment between images and their captions. Likewise, Instruct-CLIP learns the relationship between the visual changes in the originaledit image pair and its edit instruction. Just like in CLIP, our approach has an image encoder that in our case encodes the visual change between the input and edited images (denoted as I-CLIPvis(I o, e)), and text encoder that encodes the edit instruction (I-CLIPtxt(p)). Note that unlike the original CLIP image encoder, ours takes both the original and edited images as input so that it can encode their visual difference. As shown in Fig. 3a, we fine-tune the two encoders by computing and minimizing the contrastive loss between them (Eq. 2). Furthermore, while the architecture of I-CLIPtxt is the (a) Overall Instruct-CLIP system architecture (b) Architecture of visual-change encoder I-CLIPvis(I o, e) Figure 3. Instruct-CLIP and instruction-refinement architecture. (a) Overview of our Instruct-CLIP approach (I-CLIP, for short), which embeds the visual change in the original/edited images and and the edit instruction into the same feature space through contrastive loss, Lcontrast (Eq. 2). To obtain refined instruction from its I-CLIP embedding ztxt, we fine-tune the text decoder DeCap [12] to decode ztxt back to using cross-entropy loss, LDeCap (Eq. 4). At run-time, the text decoder can take the embedded visual change from the original to the edited image (zvis) and decodes it to produce new instruction. Due to the significant cosine similarity gap between zvis and ztxt even when they are well aligned, directly decoding zvis leads to suboptimal results. To achieve representation of zvis closer to the text features that the instruction decoder learned during training, we compute (zvis) with Eq. 6 and decode it to obtain the refined instruction p, which is used to improve the dataset. (b) Image encoder I-CLIPvis, which includes two shared-weighted DINOv2 [18] modules in front of the standard CLIPvis encoder. same as CLIP, our I-CLIPvis adds two shared-weighted DINOv2 [18] modules in front of CLIPvis, as shown in Fig. 3b. These DINOv2 units extract rich and robust visual features from the input images and allow CLIPvis to focus on encoding the difference between the original and edited images. This is essential when we introduce Stable Diffusion (SD) latent image encoding and diffusion into the network, as explained in Sec. 3.2. To describe the architecture of I-CLIPvis, we write the output of DINOv2 for given image as = DINOv2(I), and the intermediate features from each of its layers as {di [1, l]}. So in I-CLIPvis, we first we compute do and de by passing the input image and the edited image through DINOv2, respectively, then compute the DINOv2 feature difference de do that we input to CLIPvis. The intermediate feature differences de are also added to input of the ith layer of CLIPvis before being processed by that layer. We find that this improves model performance in our experiments by providing additional information that may be lost from de do when being processed from the (i + 1)th to lth layer, similar to skip connections [6]. do To train Instruct-CLIP, we first initialize CLIPvis and I-CLIPtxt with the respective blocks from the pre-trained CLIP model. Then, given batch of data samples {(I , pi) [1, n]} from an existing instructionguided image-editing dataset like InstructPix2Pix [1], we first compute the Instruct-CLIP features for both the visual difference and edit instruction: , = I-CLIPvis(I zvis ztxt = I-CLIPtxt(pi), , ) (1) and compute the contrastive loss Lcontrast between them: Lcontrast = 1 (cid:32) (cid:88) i= log + log exp (cid:0)sim(zvis , ztxt j=1 exp (cid:0)sim(zvis exp (cid:0)sim(ztxt , zvis j=1 exp (cid:0)sim(ztxt )/τ (cid:1) , ztxt )/τ (cid:1) , zvis (cid:80)n (cid:80)n )/τ (cid:1) )/τ (cid:1) (2) (cid:33) , where scalar τ is learnable temperature parameter that controls the sharpness of the similarity distribution and function sim(, ) measures normalized cosine similarity: sim(z1, z2) = z1 z2 z1z2 . (3) We use this contrastive loss in this manner to fine-tune the I-CLIPvis and I-CLIPtxt modules in Fig. 3a first, and once trained we train the DeCap decoder [12] to translate features to actual text (Sec. 3.1). 3.1. Predicting instructions by decoding features Learning the alignment between original-edited image pairs and instructions does not automatically result in better instructions, since decoder is required to translate I-CLIP latent-space features into actual text instructions. To do this, we considered two different training strategies: either decoding zvis = I-CLIPvis(I o, e) or ztxt = I-CLIPtxt(p) to match the given edit instruction p. Since edit instructions in the original datasets are often sub-optimal, we want to avoid forcing the model to decode zvis directly to during training because they may Instead, we opt to decode ztxt back to p, usnot match. ing the same approach of the CLIP-based text-captioning model DeCap [12]. Formally, let = [w1, w2, , wn] be token-representation of the instruction with length n, where each token is represented by one-hot vector. Similarly, the decoded/predicted instruction is = [w 2, , n] (instructions can be represented by the same number of tokens through padding and/or truncation). This is essentially classification problem that requires identifying the correct element for each token, so the training objective of the text decoder is cross-entropy loss: 1, LDeCap = 1 n (cid:88) (cid:88) (w) log yp yp (w), (4) wW i=1 where is the set of all possible tokens, yp ground-truth for each token w, and yp of token at position for as predicted by DeCap. (w) is the (w) is the probability At runtime, our goal is to generate improved instructions that accurately capture the visual changes from the original image to the edited one by decoding zvis. However, since the average cosine similarity between visual and text CLIP features is relatively low (around 0.2) [12], text decoder that has been previously trained on text features ztxt will have hard time handling zvis due to such differences, producing sub-optimal decoded results. To address this, we would like to input to the decoder feature representation of zvis more similar to the text features the instruction decoder has learned on during training. straightforward approach would be to compute the cosine similarity between zvis and each instructions text feature in the dataset, selecting the feature with the highest similarity. However, this method would simply retrieve the most similar instruction, limiting both the diversity and accuracy of refined instructions. Instead, we follow the approach of DeCap [12] which leverages information from all text features by weighting their influence by their cosine similarity to zvis. Basically, instructions with text features more similar to zvis should contribute more to the refined instruction. Therefore, we calculate the probability of each instruction feature contributing to the refined representation using the softmax function over the cosine similarities between all text features and zvis. Formally, for dataset of size n, let {ztxt = I-CLIPtxt(pi) [1, n]} be the set of text features, each corresponding to an instruction pi in the dataset. The probability wi that instruction pis text feature influences the refined instruction is defined as: exp (sim(ztxt j=1 exp (sim(ztxt , zvis)) , zvis)) wi = (cid:80)n (5) . With these weights, we project zvis to the text feature space: Figure 4. Training our LD-DINOv2 model. To use I-CLIP as part of the training objective for Stable Diffusion [23], it needs to handle noisy latent images. Therefore, we replace the original DINOv2 backbone (Fig. 3b) with latent-diffusion version of it that we call LD-DINOv2. This takes both the noisy latent image Lk from SD VAE encoding and forward diffusion (FD) process with timestep tk. We then train LD-DINOv2 to ignore the noise and the latent-space compression and to extract the original DINOv2 features using the training objective LLD-DINOv2 (Eq. 7). 3.2. Working in the latent-diffusion domain With I-CLIP and DeCap, we can now generate datasets with improved semantic alignment between original/edited image changes and their edit instructions, thereby providing better signal for training instruction-guided, imageediting models. So one might be tempted to simply train models like InstructPix2Pix [1] directly as-is on our improved dataset. While this does lead to some improvement (see Sec. 4.5), we find we get even more improvement if we reinforce the alignment between the visual change and the edit instruction during the training of the stable diffusion (SD) model itself. This means using I-CLIP as an integral part of the training objective beyond just dataset refinement. To do this, I-CLIP must directly handle noisy, latent images in SD, defined as Lk = FDk(VAEenc(I), ). Here, FDk() is the kth step of the forward-diffusion process [25] which takes in image I, embeds it in the latent domain with SDs variational autoencoder VAEenc(I), and then adds randomly sampled noise (0, I) according to specific noise schedule based on k. Our DINOv2 feature backbone in I-CLIP, which we call LD-DINOv2 (see Fig. 4), works with Lk directly by also taking the corresponding timestep tk as input and being trained to ignore the noise and the latent-space compression to extract the original DINOv2 features with the following training objective: LLD-DINOv2 = 1 sim(dLD, d) + 1 (cid:88) (cid:16) i=1 1 sim(dLD , di) (cid:17) , (7) where dLD is the output of LD-DINOv2, is the original DINOv2 output, and the intermediate features from each of its layers are {dLD [1, l]}. The visual feature of the original-edited image pair (I o, e) then becomes: (cid:88) (zvis) = (wi ztxt ). (6) I-CLIPvis( Lo k1 , tk1 , Le k2 , tk2), i=1 In the end, the refined instruction for (I o, e) is = DeCap (cid:0)(zvis)(cid:1), where DeCap() denotes the instruction decoder, and so the refined dataset sample is (I o, e, p). where: Lo k1 Le = FDk1 (VAEenc(I o), N1) = FDk2 (VAEenc(I e), N2), 5 (8) (9) are generated using randomly sampled Gaussian noise terms N1 and N2. For further details on the LD-DINOv2 training process, please refer to the supplementary. Note that we set k1 = k2 = 0 during dataset refinement stage so LD-DINOv2 will process the pristine latent images. 3.3. Training our instruction-guided editing model At this point, we would like to leverage both the refined dataset and I-CLIP to enhance SD-based instruction-guided image editing methods, such as InstructPix2Pix (IP2P) [1]. Given data sample (I o, e, p), IP2P usually trains denoising UNet to predict the noise added to the edited image: Nk = UNetIP2P(Lo, Le k, tk, p), (10) which is conditioned on the original image in the latent domain (Lo) and the edit instruction p. The traditional training loss in this setup is the mean squared error (MSE) with respect to the actual noise added to the GT edited image: LMSE = (cid:13) (cid:13)N Nk (cid:13) Given the refined sample (I o, e, p), we design complementary loss function to incorporate our Instruct-CLIP guidance into the objective: (cid:13) 2 (cid:13) (cid:13) 2 (11) , LI-CLIP = 1 sim(cid:0)I-CLIPvis(Lo, t0, Le k1, tk1), I-CLIPtxt(p)(cid:1), (12) where the intermediate, denoised latent output using the predicted noise is calculated as: k1 = RDk,k1( Le k, Nk). Le k, Nk) is the reverse-diffusion process. (13) Here, RDk,k1( Le Thus, our final training objective is: = LMSE + λLI-CLIP, (14) where λ is set to 0.1 to balance LI-CLIP with the MSE loss. 4. Experiments We implemented and trained Instruct-CLIP (I-CLIP) as described above and used it to refine the InstructPix2Pix (IP2P) dataset [1] to get 120K+ refined instructions, which were then used to fine-tune its model. Please refer to the supplementary for more details. We now describe the experiments to test our method. 4.1. Benchmarks and baselines We evaluate methods on two instruction-guided imageediting benchmarks: MagicBrush (MagBr) [32] and ZONE [11]. The former contains multi-turn edits where multiple instructions are used to edit one image iteratively, as opposed to single-turn edit where the image is edited once. We also evaluate results using these metrics: 6 CLIP-T = sim(CLIPvis((I e)), CLIPtxt(pe)) CLIP-I = sim(CLIPvis((I e)), CLIPvis(I e)) DINO-I = sim(DINOv2((I e)), DINOv2(I e)) Here, (I e) denotes the method output corresponding to benchmark sample (I o, e, po, pe), where is the original image, is the ground-truth edited image (available for MagBr, not for ZONE), po is the caption for the original image, and pe is the intended caption for the edited image. The CLIP-T score assesses semantic alignment between the result and its intended caption in the benchmark. Finally, both the CLIP similarity score (CLIP-I) and the DINO similarity score (DINO-I) evaluate the visual alignment between the result and the ground-truth, if available. We compare our editing model with several baselines: Inst-Inpaint (I-Inp) [30], MagicBrush (MagBr), HIVE [33], InstructPix2Pix (IP2P) [1], Watch Your Steps (WYS) [16], and ZONE [11]. I-Inp, MagBr, and HIVE are all trained on (partially) manually created datasets, and WYS and ZONE both use masking mechanisms to explicitly locate the editing region. See Sec. 2 for more information. 4.2. Instruction refinement results We present samples from our refined dataset in Fig. 2 (see supplementary for more samples). As we can see, Instruct-CLIP is able to correct wrong instructions such as make the waves into hurricane to add lightning storm to the sky (1st row right). By correcting these samples, we reduce the noise due to instructions not reflecting the actual edits, which in turn helps improve the performance of the model trained on this dataset as we will see next. 4.3. Image-editing results We find diverse set of samples showcasing the strength of our method in Figs. 1 and 5 compared to the baselines (see supplemental for more). Our model not only addresses many issues present in IP2P such as unintended changes to regions irrelevant to the instructions but also generates results aligned more accurately with the edit instructions. This is particularly important for multi-turn editing to avoid results diverse further from the desired outcome across edit turns as shown in the supplementary. Notably, this is achieved without manually created datasets or masking mechanisms to explicitly locate the editing region, as used in other approaches. Furthermore, we present the CLIP-T value of each output in Fig.5, with the best value per sample underlined. As shown, these values - as well as the CLIP-I and DINO-I values provided in the supplementary material - are not indicative of visual quality, but are included for completeness. Table 1 provides quantitative comparison of our model in both single-turn and multi-turn edit scenarios. Original HIVE I-Inp WYS ZONE MagBr IP2P I-CLIP (Ours) 0. 0.303 0.325 0.225 0.312 0.224 0. Change the floor to carpets 0.278 0.276 0.290 0.299 0. 0.304 0.287 Change color of body of water to blue 0.241 0.362 0.369 0.369 0. 0.365 0.360 Add necklace 0.238 0.196 0. 0.280 0.267 0.243 0.240 What if he was holding laptop? 0. 0.284 0.381 0.375 0.395 0.351 0. Add carved pumpkin next to the sign 0.311 0.311 0.335 0.287 0. 0.295 0.332 Open her mouth. 0.318 0.315 0. 0.273 0.306 0.217 0.305 Let the strings be striped 0. 0.232 0.327 0.333 0.313 0.337 0. Add soup into the bowl 0.314 0.256 0.296 0.311 0. 0.308 0.315 Replace the roses with tulips Figure 5. Comparison with state-of-the-art approaches for instruction-guided image editing, including HIVE [33], Inst-Inpaint (I-Inp) [30], Watch Your Steps (WYS) [16], ZONE [11], MagicBrush (MagBr) [32], InstructPix2Pix (IP2P) [1] showcasing the strength of our approach. CLIP-T value of each output is shown at its top-left corner, with the best value per row underlined. Note that the image with the best CLIPT score is not necessarily the visually best result, underscoring the deficiencies of conventional metrics (including CLIP-I and DINO-I shown in the supplemental) for measuring the quality of image edits. 7 MagBr data (single-turn) ZONE data CLIP-T CLIP-I DINO-I CLIP-T CLIP-I DINO-I CLIP-T MagBr data (multi-turn) HIVE I-Inp MagBr WYS ZONE IP2P Ours 0.303 0.285 0.307 0.313 0.301 0.300 0.305 0.892 0.887 0.929 0.924 0.929 0.854 0.911 0.746 0.729 0.836 0.815 0.824 0.645 0. 0.299 0.277 0.303 0.313 0.307 0.298 0.301 0.852 0.859 0.896 0.887 0.896 0.824 0.871 0.659 0.654 0.759 0.727 0.750 0.573 0.721 (+1.67%) (+6.67%) (+24.5%) (+1.01%) (+5.70%) (+25.83%) 0.297 0.267 0.292 0.301 0.296 0.296 0.297 (+0.34%) Table 1. Quantitative comparison with baselines. An up arrow () indicates higher values are better. The percentage improvements over baselines are in parentheses. vs. MagBr Cnt % ours win tie MagBr win 311 29.76% IP2P win 124 11.87% MagBr win vs. IP2P Cnt % 566 54.16% ours win 310 29.67% IP2P win 168 16.08% tie 611 58.47% tie 485 46.41% 148 14.16% 412 39.43% IP2P vs. MagBr Cnt % Table 2. Method pairwise comparison responses from user study. ZONE data CLIP-T CLIP-I DINO-I CLIP-T MagBr data IP2P Ours (w/o data) Ours (w/o loss) Ours 0.300 0.299 0.303 0.305 0.854 0.862 0.903 0.911 0.645 0.671 0.782 0. 0.296 0.295 0.298 0.297 Table 3. Quantitative comparison for ablation study. An up arrow () indicates higher values are better. The best results are marked in bold. We trained two variants for our model: one trained on the original IP2P dataset without refined instructions but with the Instruct-CLIP guidance loss (Ours (w/o data)) and the other trained on our refined dataset without the Instruct-CLIP guidance loss (Ours (w/o loss)). 4.4. User study We also conducted one with 104 participants (60 male, 43 female, 1 non-binary) to compare our method with MagicBrush (MagBr) [32] and InstructPix2Pix (IP2P) [1] on the InstructBrush benchmark [35], ensuring fairness since no method was trained on similar data. Each person evaluated 33 randomly sampled output pairs based on the input image and edit instruction, and select which was better or if they were similar. We balanced the study across all pairwise comparisons, with 1,045 responses per pair  (Table 2)  . Participants overall found our method better than IP2P (17.8% more wins) and MagBr (24.4%). Original IP2P Ours (w/o data) Ours (w/o loss) Ours Place floor lamp next to the chair. Replace the truck with tractor Figure 6. Effect of refined instructions and Instruct-CLIP guidance loss. Compared to variants of our model trained without refined instructions (Ours (w/o data)) or without the guidance loss (Ours (w/o loss)), our model produce superior results. Ori IP2P MagBr Ours Ori IP2P MagBr Ours Turn orange shirt into blue Add an eagle above the plane Delete the wooden boat Remove the white plane Figure 7. Examples of failure cases compared with InstructPix2Pix (IP2P) [1] and MagicBrush (MagBr) [32] performed on the original (Ori) images. that refined data significantly improve visual alignment with ground truth, evidenced by higher CLIP and DINO scores, while the guidance loss further enhances this alignment. 4.6. Limitations Despite being able to correct lot of inaccurate edit instructions in the training set, our method is still heavily influenced by the limitation of the generative method [7] these training images come from, which include color bleeding outside the intended edit region (Fig. 7 top left), and incomplete object addition (Fig. 7 top right). While our model respects the original images better, it sometimes struggles to remove objects in the original images (Fig. 7 bottom). 4.5. Ablation study 5. Conclusion To assess the impact of refined instructions and our Instruct-CLIP guidance loss, we trained two model variants: one on the original IP2P dataset with the Instruct-CLIP guidance loss (Ours (w/o data)) and another on our refined dataset without the guidance loss (Ours (w/o loss)). We compare these with IP2P and our full method in Table 3, and visual examples are provided in Fig. 6. Results show We have presented Instruct-CLIP, self-supervised method for instruction-guided image editing that learns the semantic changes between original and edited images to refine edit instructions in datasets. Applying I-CLIP to the InstructPix2Pix dataset yields over 120K refined samples, which we use to fine-tune its model with our I-CLIP-guided loss function and generate better edit results."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Aleksander Holynski, and Alexei Efros. InstructPix2Pix: Learning to follow image editing instructions. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1839218402, 2023. 1, 2, 3, 4, 5, 6, 7, 8, 11, 12 [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:18771901, 2020. 2, 3 [3] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Advances in Neural Information Processing Systems (NeurIPS), 34:87808794, 2021. 1, 2 [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis, march 2024. URL http://arxiv. org/abs/2403.03206, 2024. 2 [5] Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, StyleGAN-NADA: Gal Chechik, and Daniel Cohen-Or. CLIP-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):113, 2022. 11 [6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778, 2016. 4 [7] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2, 3, 8 [8] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [9] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. HQ-Edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. 3 [10] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. VIEScore: Towards explainable metrics for conditional image synthesis evaluation. arXiv preprint arXiv:2312.14867, 2023. 12 [11] Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao Hu, Jianzhuang Liu, et al. ZONE: Zero-shot instruction-guided local editing. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 62546263, 2024. 1, 3, 6, 7, 13, 14, 17 [12] Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang. DeCap: Decoding CLIP latents for zero-shot captioning via text-only training. arXiv preprint arXiv:2303.03032, 2023. 2, 4, 5, 11 [13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 12 [14] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 12 [15] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent Consistency Models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 1, [16] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos Derpanis, and Igor Gilitschenski. Watch Your Steps: LoIn Eurocal image and scene editing by text instructions. pean Conference on Computer Vision (ECCV), pages 111 129. Springer, 2025. 1, 3, 6, 7 [17] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text Inversion for editing real images using guided diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 60386047, 2023. 2, 3 [18] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 4, 11 [19] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In SIGGRAPH, pages 111, 2023. 2, 3 [20] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion modarXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 2 [21] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 11 [22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International Conference on Machine Learning vision. (ICML), pages 87488763. PMLR, 2021. 2, 3, [23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684 10695, 2022. 1, 2, 5, 11 [24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. UNet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. 11 [25] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1, 2, 5 and Stefano Ermon. arXiv preprint [26] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-Play diffusion features for text-driven In Conference on Computer image-to-image translation. Vision and Pattern Recognition (CVPR), pages 19211930, 2023. 2, 9 [27] Bram Wallace, Akash Gokul, and Nikhil Naik. EDICT: Exact diffusion inversion via coupled transformations. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 2253222541, 2023. 3 [28] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in textIn Conference on Computer to-image diffusion models. Vision and Pattern Recognition (CVPR), pages 19001910, 2023. 2, 3 [29] Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. EditWorld: Simulating world dynamics for instruction-following image editing. arXiv preprint arXiv:2405.14785, 2024. 3 [30] Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut Erdem, and Aysegul Dundar. Instructing to remove objects with diffusion models. arXiv preprint arXiv:2304.03246, 2023. 1, 3, 6, 7 Inst-Inpaint: [31] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936, 2022. 12 [32] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. MagicBrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. 1, 3, 6, 7, 8, 13, 14, 17 [33] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. HIVE: Harnessing human feedback for instructional visual editing. arXiv preprint arXiv:2303.09618, 2023. 2, 3, 6, 7, 13, 14, 17 [34] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. UltraEdit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024. 1, 2, 3 [35] Ruoyu Zhao, Qingnan Fan, Fei Kou, Shuai Qin, Hong Gu, Wei Wu, Pengcheng Xu, Mingrui Zhu, Nannan Wang, and InstructBrush: Learning attention-based inXinbo Gao. arXiv preprint struction optimization for image editing. arXiv:2403.18660, 2024. 8, 13, 14, 17 10 In this supplementary, we briefly discuss the difference between our approach and CLIP directional similarity (Sec. A). Then, we provide more implementation details in Sec. B. We compare performance of edit instruction refinement between our approach and vision language models in Sec. D. Next, we show the limitation of LIP/DINO metrics in Sec. C. Finally, we include additional editing results, refined editing instructions, as well as more failure cases in Sec. E. A. CLIP Directional Similarity Comparison One key difference between our I-CLIP and the CLIP direction similarity [5] used in IP2P is that I-CLIP relies on edit instructions rather than individual image prompts, which require two prompts per image pair. This makes I-CLIP easily applicable across instruction-guided image-editing datasets, even when image prompts are unavailable. Also, individual prompts can be quite lengthy. In contrast, edit instructions are typically more concise, resulting in less irrelevant information in their text embeddings. For example, here is prompt pair from the IP2P dataset: Infinity walk by Marcelo Archila - Black & White Landscapes (contrast, monochrome, hdr, black and white, fine art, long exposure) and Infinity walk by Marcelo Archila - Black & White Landscapes (contrast, monochrome, hdr, black and white, commercial). At first, it might be challenging to infer the edit instruction, which is simply: make it commercial. B. Implementation and Dataset Refinement LD-DINOv2 is initialized from ViT-L/14 DINOv2 model [18]. The patch embedding projection layer is reTable 4. Metrics (CLIP/DINO-I if GT exists) for Fig. 5 outputs, with best bolded and shown. placed to accommodate for Stable Diffusion (SD) VAE [23] encoded images. We initialize the timestep embedding projection module that takes timestep inputs following the SD timestep encoding implementation. It is trained on InstructPix2Pix (IP2P) [1] dataset images, all resized to 256, with learning rate lr = 105, batch size of 32 and total training steps of 100K. For the first 10K steps, we fix the timesteps to be 0, so the latent image inputs are not noisified, this helps the patch embedding projection layer to learn to encode latent images. Then, we linearly increase the upper bound of the timestep in proportion to the training step number, till the maximum value 1000 at the 90K th step. There, we uniformly sample the timestep value between 0 and the corresponding upper bound for each training step. Gradually increase the timestep value sampling range help maintain the information learned by the patch embedding projection layer while tuning the timestep embedding projection module accordingly. For the last 10K steps, timesteps are randomly sampled uniformly. Instruct-CLIP (I-CLIP) is initialized from ViT-L/14 CLIP model [22]. We freeze the text decoder, aforementioned LD-DINOv2, and finetune the CLIP image encoder. The instruction decoder follows the architecture of DeCap [12] with pre-trained GPT-2 backbone [21] and is trained along with the image encoder on the IP2P dataset with learning rate lr = 105, batch size of 32, and total training steps of 100K. The advantage of training LD-DINOv2 ahead of time is that we can sample timesteps within its maximum possible range at random without repeating above training procedure since LD-DINOv2 already learns to ignore the noise added to the latent image. After training, we refine the IP2P dataset. For each data sample (I o, Ie, p) and its corresponding refined instruction p, we update the sample if the I-CLIP cosine similarity between the visual changes in the original/edited image and the refined instruction differs significantly from that with the original instruction: sim(cid:0)Instruct-CLIPvis(Lo, 0, Le, 0), Instruct-CLIPtext(pR)(cid:1) > sim(cid:0)Instruct-CLIPvis(Lo, 0, Le, 0), Instruct-CLIPtext(p)(cid:1) + ϕ, (15) where Lo = VAEenc(I o), Le = VAEenc(I e), (16) and ϕ is the margin we set at 0.1. This gives us over 120K new instructions out of 313,010 samples in the IP2P dataset. We keep the original instruction in the remaining samples. The image editing model is initialized from the IP2P model [1], where the UNet [24] is fine-tuned using LowRank Adaptation (LoRA) [8] with parameter = α = 32 on aforementioned new samples with lr = 104, batch size of 64, and total training step of 10K. The rest of the training configuration follows the original IP2P work. Figure 8. Top: VLM outputs with respect to prompt Provide the edit instruction that can transform the source image to the target image in one phrase: and image pairs in Fig. 2. Bottom: VLM outputs with respect to prompt Describe the image in one phrase: and the input (leftmost) image in the last row. computed on the MagBr test set, which has similar distribution as their training set giving MagBr an advantage. D. Comparison with Vision Language Models To compare our method with vision language models (VLMs) in terms of edit instruction refinement, we evaluate LLaVA [13] and LLaVA-Next [14], two widely used opensource VLMs, in Fig. 8 (top). Both VLMs fail to generate effective edit instructions compared to our method. In Fig. 8 (bottom), we use these VLMs to generate caption for the input image (last row), which serves as the input prompt for editing methods that require separate prompts for the input and target images. While the caption accurately describes the image, it fails to capture its watercolor style, crucial detail needed for style editing intended in this sample pair. As result, users still need to manually refine the input prompt and write the target image prompt, which is more cumbersome than using single edit instruction. E. Additional Results We include the multi-turn edit example mentioned in the paper in Fig. 9. Additionally, we include more instructionguided image editing results in Figs. 10 and 11, as well as samples from our dataset with refined instructions in Fig. 12 and 13. Lastly, we include additional failure cases in Fig. 14. Figure 9. Multi-turn edit comparison with InstructPix2Pix (IP2P) [1]. Note how IP2P (top row) gradually diverges from the desired result more and more, unlike our approach (bottom row) which produces results more consistent with the original. C. Limitation of CLIP/DINO Metrics There are several reasons for the gap between our qualitative and quantitative results, which we included for completeness. First, while these metrics are widely used, they have well-documented limitations and dont align with human judgment highlights their weak correlation with humans in VIEScore [10], Yuksekgonul et al. show CLIPs Bag-of-Words behavior is insensitive to word order [31]). Table 4 also illustrates this; despite the superior qualitative performance of our results in Fig. 5, our metrics are usually lower than baselines. Finally, Table 1 (MagBr data) was"
        },
        {
            "title": "Original",
            "content": "IP2P I-CLIP (Ours)"
        },
        {
            "title": "Original",
            "content": "IP2P I-CLIP (Ours) Remove airplane Change the tractor to bus Change color of suitcase to orange Change the snow into grasses Make gray sofa into floral print Change the donuts to sandwich Add ketchups What if the baseball bat was made of wood? Add tourist bus Change background color to bright neon color Change it to mars Change the color of the astronuts to yellow Change the color of train to red Remove the arch Make the sun have smiley face Change the orange to apple Figure 10. Additional results from our Instruct-CLIP image editing method on benchmarks [11, 32, 33, 35] (Part 1/2)"
        },
        {
            "title": "Original",
            "content": "IP2P I-CLIP (Ours)"
        },
        {
            "title": "Original",
            "content": "IP2P I-CLIP (Ours) Make the train into cartoon style Change the painting into the starry night Change the plant color to blue Add water bottle Change the panda to king kong Add panda Make it lemonade Make it heart-shaped light Add fishing poles What if the woman had cowboy hat Add lighthouse Replace the notebook with binder Change his outfit into tuxedo Make fireball Turn the birthday cake to oreo cake Turn the man red Figure 11. Additional results from our Instruct-CLIP image editing method on benchmarks [11, 32, 33, 35] (Part 2/2)"
        },
        {
            "title": "Edited",
            "content": "ORIGINAL: have it be christmas (0.164) REFINED: make it winter cottage (0.317) ORIGINAL: turn into modern castle (0.123) REFINED: make it modern architecture (0.224) ORIGINAL: make it nightmare (0.184) REFINED: make it moonlit night (0.300) ORIGINAL: make them look like alien homes (0.113) REFINED: make it starry night sky (0.329) ORIGINAL: its in the style of edward gorey (0.106) REFINED: as pencil drawing (0.291) ORIGINAL: make it photo of man (0.071) REFINED: on beach (0.248) ORIGINAL: make the sky orange (0.200) REFINED: make it fiery sunset (0.351) ORIGINAL: make it painting by famous artist (0.152) REFINED: make it painting by van gogh (0.359) ORIGINAL: make it high school (0.099) REFINED: make it look like childrens book (0.335) ORIGINAL: make it less pink (0.037) REFINED: make the picture into blue (0.157) ORIGINAL: add large fireball (0.187) REFINED: replace the snow with fire (0.320) ORIGINAL: make her look like bird (0.127) REFINED: make it magazine cover (0.321) ORIGINAL: turn it into christmas tree (0.184) REFINED: add lightbulb to the tree (0.335) ORIGINAL: make the birds ravens (0.127) REFINED: make the tree blue (0.321) ORIGINAL: make it snowstorm (0.248) REFINED: make it foggy (0.379) ORIGINAL: have her wear cowboy hat (0.178) REFINED: make the wearing glasses (0.279) ORIGINAL: have the tunnels be made of candy (0.124) REFINED: make the tree red instead (0.253) ORIGINAL: make it beach (0.132) REFINED: make it sunset in the desert (0.326) ORIGINAL: swap the church with castle (0.216) REFINED: turn the bridge into castle (0.318) ORIGINAL: replace the library with bank (0.101) REFINED: as pen and ink drawing (0.261) ORIGINAL: change her to cat (0.097) REFINED: make the drawing into cartoon (0.251) ORIGINAL: turn into drawing by bill watterson (0.072) REFINED: make it pencil sketch (0.284) ORIGINAL: make him wear an overcoat (0.201) REFINED: make the suit blue (0.340) ORIGINAL: turn into cat (0.021) REFINED: as vintage film photograph (0.270) ORIGINAL: make it an image from disney movie (0.039) REFINED: to be drawing (0.340) ORIGINAL: make her shark (0.164) REFINED: give sword in her hand (0.289) ORIGINAL: make the sky black (0.240) REFINED: make the dress black (0.353) ORIGINAL: make the tree violet (0.218) REFINED: the trail is purple (0.394) ORIGINAL: make him vampire (0.054) REFINED: make the photo black and white (0.332) ORIGINAL: have the railing have gold color (0.271) REFINED: give the lake golden glow (0.388) ORIGINAL: add head of bull (-0.024) REFINED: make the beach into desert (0.296) ORIGINAL: make it christmas village (0.090) REFINED: make the fog red (0.277) Figure 12. Additional refined instruction from our dataset (Part 1/2)"
        },
        {
            "title": "Edited",
            "content": "ORIGINAL: change to different ocean (0.020) REFINED: make her brunette (0.257) ORIGINAL: make it desert instead (0.033) REFINED: the trees are made of gold (0.368) ORIGINAL: turn into photo (0.133) REFINED: as black and white photograph (0.297) ORIGINAL: have the background be deep blue (0.227) REFINED: make the flowers blue (0.363) ORIGINAL: make it snow (0.055) REFINED: have the sky blue (0.288) ORIGINAL: turn into painting (0.006) REFINED: make it at night (0.296) ORIGINAL: convert into an anime (0.086) REFINED: replace the cafe with eiffel tower (0.246) ORIGINAL: turn into sakura tree (0.203) REFINED: have her in field of flowers (0.319) ORIGINAL: make the king child (0.061) REFINED: move it to modern city (0.199) ORIGINAL: make it mango (0.060) REFINED: make it drawing instead (0.256) ORIGINAL: change the lake to river (0.080) REFINED: make it tropical rainforest (0.340) ORIGINAL: move to phoenix (0.137) REFINED: make it modern apartment (0.271) ORIGINAL: make the picture more gritty (0.091) REFINED: in snow storm (0.282) ORIGINAL: make it contemporary photograph (0.186) REFINED: make it black and white photograph (0.295) ORIGINAL: add dinosaur (0.128) REFINED: have flock of birds flying overhead (0.321) ORIGINAL: turn into painting (0.094) REFINED: make the portrait of evil wizard (0.270) ORIGINAL: have it be in zoo (0.166) REFINED: make it tropical jungle (0.306) ORIGINAL: put the book on fire (0.137) REFINED: have golden background (0.366) ORIGINAL: add castle in the background (0.122) REFINED: make the barn into castle (0.248) ORIGINAL: dont like the hall (0.066) REFINED: have him in meadow (0.335) ORIGINAL: make it grey (0.180) REFINED: make it gloomy rainy (0.281) ORIGINAL: make it vintage photograph (0.128) REFINED: make it look like ukiyo - painting (0.308) ORIGINAL: make her zombie (0.185) REFINED: make her blue alien (0.326) ORIGINAL: add dragon (0.191) REFINED: add dragon flying above the lake (0.303) ORIGINAL: add an angel (0.082) REFINED: put in the grand canyon (0.279) ORIGINAL: take away the watercolor (0.134) REFINED: draw as pencil drawing (0.275) ORIGINAL: add rainbow (0.201) REFINED: make milky way look like rainbow (0.385) ORIGINAL: it is clawn suit (0.230) REFINED: make the suit orange (0.338) ORIGINAL: make the junk ship small plane (0.152) REFINED: make it in japanese garden (0.264) ORIGINAL: have the mountain be made of gold (0.140) REFINED: make it summer mountain (0.271) ORIGINAL: the furrow is made of glass (0.162) REFINED: as stained glass window (0.367) ORIGINAL: make him hipster (0.154) REFINED: make the woman have beard (0.268) Figure 13. Additional refined instruction from our dataset (Part 2/2)"
        },
        {
            "title": "Original",
            "content": "IP2P I-CLIP (Ours)"
        },
        {
            "title": "Original",
            "content": "IP2P I-CLIP (Ours) Put python on the girl Swap the raspberries into strawberries Remove the tie Wear the man pair of sunglasses Make the dog golden statue Change the statue to jesus status Remove the potted plant with green flower pots Turn the football to basketball Make the yellow umbrella red Put garlic bread on the plate Remove the eagel Delete the table Gid rid of his black hat Remove the ball Delete the whiteboard Remove the trees 17 Figure 14. Additional failure cases from our Instruct-CLIP image editing method on benchmarks [11, 32, 33, 35])"
        }
    ],
    "affiliations": [
        "University of California, Santa Barbara"
    ]
}