{
    "paper_title": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection",
    "authors": [
        "Xu Lin",
        "Jinlong Peng",
        "Zhenye Gan",
        "Jiawen Zhu",
        "Jun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available."
        },
        {
            "title": "Start",
            "content": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection Xu Lin1*, Jinlong Peng1*, Zhenye Gan1, Jiawen Zhu2, Jun Liu1 1Tencent Youtu Lab 2Singapore Management University {gatilin, jeromepeng, wingzygan, juliusliu}@tencent.com jwzhu.2022@phdcs.smu.edu.sg 5 2 0 2 9 2 ] . [ 1 3 7 2 3 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as overallocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, novel YOLOlike framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through an Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, lightweight dynamic routing network guides expert specialization during training through diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code: isLinXu/YOLO-Master 1. Introduction Real-time object detection stands as critical task in computer vision, finding extensive applications in autonomous *Equal contribution. Figure 1. Accuracy-latency trade-off on MS COCO. YOLOMaster-N achieves 42.4% AP at 1.62ms latency, outperforming baselines on the Pareto frontier. driving, video surveillance, and robotic systems [28, 31, 34]. The YOLO series has established itself as the dominant paradigm in this domain, achieving an effective balance between detection accuracy and inference speed through its one-stage detection framework [15, 16, 18, 36, 39]. Recent advancements in YOLO-like architectures have primarily focused on two directions: enhancing feature representation through improved backbone designs [36], and optimizing multi-scale feature fusion via refined neck architectures [25]. For instance, YOLOv5 introduces C2f modules for better multi-scale feature learning, while YOLOv11 incorporates selective attention mechanisms to enhance global representation capability. However, these improvements remain constrained by fundamental limitation: all existing YOLO architectures employ static dense computation where every input, regardless of its complexity, is processed through identical network pathways with uniform computational resources. This one-size-fits-all paradigm leads to significant inefficiency. Specifically, simple scenes with sparse and large objects consume the same computational budget as complex scenarios densely packed with small objects, resulting in both resource wastage and suboptimal feature purity [7, 27]. Moreover, the YOLO series has long faced critical challenge in balancing the accuracy-speed trade-off. From YOLOv1 to the latest iterations, each generation attempts to push the Pareto frontier of this trade-off through architectural innovations and training strategies. Yet these improvements are fundamentally static and predefined. The computational budget and network capacity are fixed at design time, lacking adaptive mechanisms to dynamically allocate resources based on input characteristics. This limitation becomes particularly evident when dealing with diverse realworld scenarios: detector optimized for complex urban scenes may be over-parameterized for simple highway environments, while one tuned for efficiency may lack sufficient capacity for challenging cases. Research in modern large language models has revealed that sparse activation patterns can dramatically improve both efficiency and adaptability, in which different inputs selectively activate distinct subsets of model parameters [7, 35]. This insight motivates us to explore whether similar dynamic computation paradigms can fundamentally reshape the accuracy-efficiency landscape in real-time object detection. To address these limitations, we introduce YOLOMaster, novel architecture that pioneers conditional computation for real-time object detection by integrating Mixture of Experts (MoE) framework within the YOLO pipeline. Our approach enables the detector to dynamically activate subset of expert networks based on input content, thereby breaking the traditional static trade-off between model capacity and computational cost. The MoEbased design incorporates three core mechanisms: (1) Dynamic routing with soft Top-K activation during training for gradient flow and hard Top-K sparsity during inference for efficiency; (2) Efficient expert groups employing depthwise separable convolutions with varying receptive fields (33, 55, 77 kernels) to capture distinct multi-scale patterns; (3) Load balancing supervision ensuring uniform expert utilization during training while maintaining genuine sparsity during deployment. Evaluated on the MS COCO dataset, YOLO-Master achieves superior performance, surpassing YOLOv12 [36] by 1.8% mAP and YOLOv13 [18] by 0.8% mAP while maintaining competitive inference speed. This validates that adaptive capacity allocation successfully establishes new state-of-the-art for real-time object detection, which expands resources for challenging cases while preserving efficiency on typical inputs. We summarize our contributions as follows: We propose the first MoE-based conditional computation framework for real-time object detection, fundamentally breaking the static accuracy-efficiency trade-off by enabling dynamic expert activation that adapts model capacity to input complexity. We design an Efficient Sparse MoE block with multiscale experts and dynamic routing network. We use soft Top-K experts during training for gradient flow and hard Top-K experts during inference for genuine sparsity, achieving training stability and deployment efficiency. We introduce load balancing supervision mechanism tailored for object detection that prevents expert collapse while maintaining uniform utilization, proving critical for stable MoE training without sacrificing inference sparsity. Extensive experiments across five diverse benchmarks (MS COCO, PASCAL VOC, VisDrone, KITTI, SKU110K) demonstrate state-of-the-art performance. Consistent improvements across varying object densities and visual domains validate the generalizability of adaptive computation over static architectures. 2. Related Work 2.1. Real-time Object Detectors The YOLO series has established itself as the dominant paradigm for real-time object detection, evolving through continuous architectural refinements [1518, 30, 32, 34, 36, 38]. Representative improvements include multi-scale feature pyramids [30], efficient layer aggregation [15], NMSfree training [38], selective attention mechanisms [16], and adaptive visual perception [18]. These methods primarily focus on backbone architecture optimization, feature fusion strategies, and training paradigm enhancements. However, they all employ static dense computation, in which every input, regardless of complexity, is processed through identical network pathways with uniform computational resources. This fundamental limitation prevents adaptive capacity allocation based on input characteristics. Beyond YOLO, other real-time detectors such as RTDETR [43] adopt transformer-based architectures with similar static computation patterns. While these methods achieve competitive accuracy-efficiency trade-offs through architectural innovations, they lack mechanisms for dynamic resource allocation. Our YOLO-Master addresses this gap by introducing conditional computation through Mixture of Experts framework, enabling adaptive expert activation that fundamentally breaks the static trade-off inherent in existing architectures. 2.2. Mixture of Experts Mixture of Experts (MoE) was originally proposed to improve model capacity through conditional computation, where gating network routes inputs to specialized expert sub-networks [14]. This sparse activation strategy has achieved remarkable success in scaling language models to trillions of parameters while maintaining manageable comFigure 2. The framework of YOLO-Master. The architecture integrates ES-MoE modules into the Backbone and Neck for enhanced feature extraction and fusion. Input features are processed through Dynamic Routing Network with Softmax Gating, selecting top-K experts for weighted aggregation. The framework adaptively switches between Standard, Soft Top-K (training), and Hard Top-K (inference) routing strategies for efficient multi-scale object detection across P3, P4, and P5 prediction layers. putational cost [7, 19]. Recent works have extended MoE to computer vision, primarily focusing on image classification tasks with Vision Transformers [4, 29, 33] and multi-task learning [2]. However, applying MoE to dense prediction tasks like object detection remains largely unexplored. Unlike classification where routing operates on global image representations, object detection requires handling multiscale spatial features with varying object densities and scale distributions. Preliminary efforts integrated MoE into ViTbased detectors [40], but they often incur substantial computational overhead unsuitable for real-time scenarios. Our YOLO-Master addresses this gap by introducing the first MoE framework tailored for lightweight CNN-based real-time detectors. We design dynamic routing mechanism that operates on feature pyramid hierarchies, enabling adaptive expert activation based on spatial characteristics. The training-inference decoupled routing strategy (soft TopK training for gradient flow, hard Top-K inference for genuine sparsity) ensures both optimization stability and deployment efficiency, making conditional computation practical for real-time detection. 2.3. Adaptive Feature Processing Attention mechanisms have been widely adopted in object detection to dynamically recalibrate features by focusing on informative regions [10, 12, 13, 41, 42]. While effective, these mechanisms apply the same computation to all inputs, including channel attention (SE [13]), spatial attention (CBAM [42]), and Transformer-based self-attention [1, 37] all operate through static, input-independent architectures. Recent efficient attention variants [3, 26] reduce computational complexity but remain fundamentally dense, processing every spatial location with uniform capacity. Our MoE-based approach differs fundamentally: rather than adaptively weighting features through attention scores, we achieve adaptive computation through conditional expert activation. This paradigm shift enables inputdependent capacity allocationsimple regions activate fewer experts while complex regions access greater model capacityfundamentally breaking the static computation constraint inherent in attention-based methods. 3. Methodology 3.1. Overview of YOLO-Master In this work, we propose YOLO-Master, novel YOLOlike framework for real-time object detection (RTOD). YOLO-Master builds upon recent YOLO architectures (e.g., YOLOv12 [36]) and introduces an Efficient Sparse Mixture-of-Experts (ES-MoE) module to enable sparse, instance-conditional adaptive computation. As illustrated in Fig. 2 (Top-left), YOLO-Master follows the standard YOLO design with Backbone, Neck, and Detection Head. Our ES-MoE module is inserted into both the Backbone and the Neck: in the Backbone, it dynamically enhances feature extraction across varying object scales and scene complexities; in the Neck, it enables multi-scale adaptive fusion and information refinement. The ES-MoE module follows the information flow illustrated in Fig. 2 (bottom-left). Specifically, ES-MoE comprises three key components: i) Dynamic Routing Network that produces instance-dependent routing signals; ii) Softmax Gating Mechanism that selects the most relevant experts; and iii) Weighted Aggregation unit that fuses the activated expert outputs into refined representation. The core Dynamic Routing Network employs phased routing strategy, using soft routing during training to encourage expert specialization and hard Top-K activation during inference to select the most relevant experts, as illustrated in Fig. 2 (right). Next we describe each component in detail. Specifically, given an input feature map RCHW , where C, H, and denote the number of channels, height, and width, respectively, the module first employs Dynamic Routing Network to extract routing features. These features are then fed into the Softmax Gating mechanism to compute weight distributions for expert selection. Let denote the total number of experts, and = {w1, w2, . . . , wE} represent the gating weights assigned to each expert. The gating weights are computed as: wi = exp(gi(X)) j=1 exp(gj(X)) (cid:80)E , = 1, 2, . . . , (1) where gi() denotes the gating function for the i-th expert. Based on the computed weights w, the top-K experts with the highest weights are selected, where to ensure sparse activation. The outputs from the selected experts are then combined through Weighted Aggregation to produce the enhanced feature map [14, 35]: = Norm (cid:32) (cid:88) iTK (cid:33) wi Experti(X) (2) where TK denotes the set of indices of the top-K selected experts, and Norm() represents the normalization operation applied to stabilize the aggregated features [20, 21]. This design enables dynamic allocation of computational resources based on the local characteristics and complexity of input features. The key innovation of ES-MoE lies in its phased routing strategy, as shown in Figure 2 (right panel). During training, Soft Top-K routing mechanism ensures gradient continuity by assigning smooth, differentiable weights to all experts while emphasizing the top performers. During inference, the module switches to Hard Top-K strategy, activating only experts (K E) to achieve practical computational sparsity and acceleration [7]. This adaptive mechanism effectively resolves the computational redundancy inherent in traditional dense models, enabling efficient expert selection across different deployment phases. The detailed design and analysis of this routing strategy will be presented in Section 3.4. 3.2. Dynamic Routing Network The expert network comprises independent feature transformation modules Experti, each designed to perform distinct nonlinear transformations on the input feature X. The core design objectives are to achieve high computational efficiency and diverse receptive fields, enabling the model to adaptively select the most suitable feature processing paths. Efficient Expert Architecture. To meet the stringent computational constraints of real-time detection, each expert Experti employs Depthwise Separable Convolution (DWconv) as its fundamental building block instead of standard convolution [11]. The DWconv significantly reduces both parameter count and FLOPs by decoupling spatial filtering (depthwise convolution) from channel-wise information integration (pointwise convolution): Experti(X) = DWconvki,CinCout(X). (3) This design ensures that the total parameter count and computational cost of the entire expert network remain manageable even with large E, which is crucial for maintaining the lightweight nature of YOLO-Master. Diverse Receptive Fields. To equip the model with the capability to process local features at different scales and complexities, each experts DWconv is designed with different convolutional kernel sizes ki. Specifically, we configure the expert group with varying odd kernel sizes ki 3, 5, 7, . . . to cover spectrum of receptive fields, inspired by multi-kernel approaches in Inception networks [? ]. Guided by the routing mechanism, Experti can be dynamically activated, allowing the model to adaptively aggregate contextual information across different spatial ranges. This diversified architecture enhances the expressive power of the ES-MoE module, enabling it to handle multi-scale features more effectively than convolutional blocks with single fixed kernel size. Expert Output and Aggregation. Each expert Experti produces an output Yi RCoutHW that maintains the same spatial dimensions as the input feature X, with predefined output channel count Cout. All expert outputs Y1, . . . , YE are subsequently aggregated using the routing weights Ω = [ω1, . . . , ωE] computed by the dynamic routing mechanism: YMoE = (cid:88) i=1 ωi Yi, (4) where YMoE RCoutHW is the final aggregated output of the ES-MoE module. 3.3. Gating Network Design The gating network plays critical role in the ES-MoE module, responsible for generating the raw logits Λ RE11 that activate the experts. Its design adheres to lightweight principles to ensure the routing decision process itself does not become computational bottleneck [33]. Information aggregation. First, for global information aggregation, the routing weights should be derived from global contextual information rather than local features to provide unified guidance for the entire input feature map RCHW . We therefore employ Global Average Pooling (GAP) to compress the input feature map into compact global descriptor RC11 [13]: = GAP(X). [13, 23] Logits computation. Subsequently, for lightweight logits computation, the aggregated descriptor is fed into parameter-efficient gating network G. This network consists of two 1 1 convolutional layers (Cin Cred E) with nonlinear activation function. We introduce channel reduction ratio γ = 8 to define the intermediate channel dimension Cred = max(C/γ, 8), thereby constraining the computational overhead of the gating network. The computational flow is defined as: Λ = Convout=E 11 (cid:16)"
        },
        {
            "title": "SiLU",
            "content": "(cid:16) Convout=Cred 11 (cid:17)(cid:17) (P ) (5) directly select the top-K largest logits ΛK from Λ, apply Softmax normalization to them ΩK = Softmax(ΛK), and strictly set the weights of the remaining experts to zero [35]. where Convout=C denotes 1 1 convolution with output channels. The output Λ represents the unified logits scores for the experts across the spatial dimensions. 11 Expert logits. Finally, the computational complexity of generating expert logits Λ = Λ1, Λ2, . . . , ΛE depends solely on the channel dimension and the number of experts E, remaining independent of the spatial dimensions of the input feature map. This design ensures efficient operation even when processing high-resolution feature maps in both the Backbone and Neck components of the architecture. 3.4. Phased Routing Strategy The routing paradigm design pursues the fundamental objective of the ES-MoE framework, which ensures comprehensive expert learning during training while enforcing strictly sparse activation during inference to achieve computational acceleration. This dual objective is implemented through phased dynamic routing mechanism [7, 45]. Computation of Expert Weights Ω. The gating network outputs raw logits Λ RE11. First, Λ is normalized via the Softmax function to obtain initial weights: Ω = exp(Λi) j=1 exp(Λj) (cid:80)E (6) where Ω represents the probability of each selected expert. Soft Top-K Strategy (Training Mode). Maintaining gradient flow is crucial during training. We adopt the Soft Top-K strategy to enforce sparsity while preserving gradients for non-zero weights [7]. First, we identify the index set IK of the top-K largest weights in Ω. Then, we construct binary hard mask MK based on IK: MK,i = (cid:40) 1 0 if IK otherwise . (7) The soft Top-K weights Ωtrain are obtained by elementwise multiplication of Ω and MK, followed by renormalization of the non-zero entries: Ωtrain = Ω MK j=1(Ω)j (MK)j + ϵ (cid:80)E (8) where ϵ is minimal value to prevent division by zero. This approach ensures only experts are activated, while maintaining continuous gradients of the weights with respect to logits Λ due to the involvement of Ω in the computation. Hard Top-K Strategy (Inference Mode). During inference, we pursue true computational sparsification. We Ωinf er,i = (cid:40) exp(Λi) jIK (cid:80) exp(Λj ) 0 if IK otherwise . (9) The Hard Top-K strategy ensures that during sparse forward propagation, only expert modules are invoked for computation, thereby achieving significant acceleration on actual hardware. Dynamic Switching. The forward propagation logic of the model is based on the current operational mode (self.training): Ω = (cid:40) Ωtrain Ωinf er if Training if Inference . (10) Through this dynamic switching, we achieve an optimal balance between training effectiveness and inference speed. 3.5. Loss Function Design Our optimization objective is to minimize the total loss function LT otal, which comprises two key components: the standard YOLOv8 detection loss LY OLO and specifically designed load balancing loss LLB for the MoE architecture. This combined loss formulation ensures the model achieves high detection accuracy while effectively addressing the issue of imbalanced expert utilization: LT otal = LY OLO + λLB LLB where λLB > 0 is hyperparameter controlling the contribution weight of the load balancing term to the total loss. Detection Loss LY OLO. : The detection loss LY OLO follows the standard YOLOv8 formulation, evaluating the models performance in object classification and localization [17]. It consists of three core components: classification loss Lcls measuring the discrepancy between predicted and ground-truth categories, localization loss Lloc typically implemented using CIoU or DIoU loss to assess the overlap and positional deviation between predicted and ground-truth bounding boxes [44], and Distribution Focal Loss LDF that optimizes the distribution representation of bounding boxes [22]: LY OLO = Lcls + Lloc + LDF L. (11) Load Balancing Loss LLB. The load balancing loss is introduced to mitigate the prevalent expert collapse issue in MoE training, where the routing network tends to allocate most input tokens to small subset of stronger or betterinitialized experts [7]. LLB encourages balanced utilization of all experts by penalizing the deviation between each experts average utilization frequency µi and the ideal uniform Dataset Method Table 1. Comparison with state-of-the-art Nano-scale detectors across five benchmarks. COCO PASCAL VOC VisDrone KITTI SKU-110K Efficiency mAP mAP50 mAP mAP50 mAP mAP50 mAP mAP50 mAP mAP50 (%) (%) (%) (%) (%) (%) (%) (%) (%) (%) YOLOv10-N [38] YOLOv11-N [16] YOLOv12-N [36] YOLOv13-N [18] 38.5 39.4 40.6 41.6 YOLO-Master-N 42.4 53.8 55.3 56.7 57.8 59. 60.6 61.0 60.7 60.7 62.1 80.3 81.2 80.8 80.3 81.9 18.7 18.5 18.3 17.5 19. 32.4 32.2 31.7 30.6 33.7 66.0 67.8 67.6 67.7 69.2 88.3 89.8 89.3 90.6 91. 57.4 57.4 57.4 57.5 58.2 90.0 90.0 90.0 90.3 90.6 Latency (ms) 1.84 1.50 1.64 1. 1.62 distribution 1/E. First, we define the average utilization frequency µi of expert over the current batch and all spatial positions as: µi = (cid:34) (cid:80)H (cid:80)E h=1 (cid:80)H (cid:80)W j=1 h=1 w=1(Ωtrain)i,h,w (cid:80)W w=1(Ωtrain)j,h,w (cid:35) (12) where Ωtrain denotes the Soft Top-K weights computed during the training phase. The load balancing loss LLB adopts the mean squared error (MSE) form to measure the discrepancy between µi and the target uniform utilization rate 1/E: LLB ="
        },
        {
            "title": "1\nE",
            "content": "E (cid:88) i=1 (cid:18) µi (cid:19)2 ."
        },
        {
            "title": "1\nE",
            "content": "(13) By minimizing LLB, we ensure that the model fully leverages all experts during training, thereby enhancing its overall generalization capability and robustness. 4. Experiment 4.1. Experimental Setup Datasets. We evaluate on five diverse benchmarks: MS COCO 2017 [24] (118k training images, 80 categories), PASCAL VOC 2007+2012 [6] (16.5k images, 20 categories), VisDrone-2019 [5] (6.5k images, 10 categories), KITTI [8] (7.5k images, 3 categories), and SKU-110K [9] (8.2k images, 1 category). Implementation. We use YOLOv12-Nano [36] (width scaling factor 0.5) as baseline with MoE modules integrated. All models are trained for 600 epochs at 640 640 resolution using SGD optimizer with cosine learning rate scheduling. The total batch size is 256. Data augmentation includes Mosaic (p=1.0), Copy-Paste (p=0.1), and MixUp (disabled for Nano variant). Standard augmentations (random affine, HSV color jittering) are also applied. All training and testing are performed on 4 high-performance compute. Metrics. We report mAP50 : 95 and mAP50 across all benchmarks. Efficiency metrics include Params (M) with activated experts, latency (ms), and FPS measured on dedicated inference accelerator, following the standard hardware configuration of the YOLOv12 baseline (FP16, batch size=1), emphasizing real-time deployment feasibility. 4.2. Main Results demonstrates that YOLO-Master-N achieves state-of-theart performance across all five benchmarks while maintaining real-time inference speed. YOLO-Master-N outperforms recent YOLOv13-N by +0.8% (COCO), +1.4% (VOC), +2.1% (VisDrone), +1.5% (KITTI), and +0.7% (SKU-110K) in mAP. The largest gains appear on VisDrone (+2.1%) and KITTI (+1.5%), validating our design for small object detection and precise localization. Despite accuracy improvements, YOLO-Master-N is 18% faster than YOLOv13-N and only 8% slower than the fastest YOLOv11-N, demonstrating optimal efficiency-accuracy balance. On SKU-110K with 147 objects / image, our method achieves 58.2% mAP, proving effectiveness in crowded scenes. These results validate that our MoE-based architecture with selective feature processing enables both higher accuracy and practical inference speed across diverse detection scenarios. 4.3. Ablation Studies 4.3.1. Effectiveness of ES-MoE Module. We investigate the optimal placement strategy for ES-MoE modules in Table 5. Backbone-only integration achieves the best performance at 62.1% mAP with 2.66M parameters, representing +1.3% improvement over the baseline (60.8%). This validates that expert specialization in earlystage feature extraction is criticalthe backbones ES-MoE can effectively learn scale-adaptive and semantic-diverse representations that benefit downstream detection. Neckonly integration fails with 58.2% mAP (-2.6%), as the routing mechanism cannot effectively specialize without diverse input features from the backbone. The vanilla backbone produces homogeneous features that limit the necks ability to discover complementary expert patterns. Surprisingly, full integration (both backbone and neck) severely degrades performance to 54.9% mAP (-5.9% compared to baseline). We attribute this to gradient interference between cascaded routing mechanisms: the backbone and neck ES-MoE modTable 2. Comparison of detection performance across Small scales on MS COCO. Model Params GFLOPs mAPbox YOLOv11-S YOLOv12-S YOLOv13-S YOLO-Master-S 9.40 9.13 9.00 9. 19.7 19.7 20.8 28.6 47.0 48.0 48.0 49.1 Table 3. Comparison of classification performance on ImageNet dataset. Model Dataset Size Top-1 Top-5 ImageNet YOLOv11-cls-N YOLOv12-cls-N ImageNet YOLO-Master-cls-N ImageNet 224 224 3.38 70.0 71.7 76.6 89.4 90.5 93.4 Table 4. Comparison of segmentation performance on MS COCO at 640640. Model Size mAPbox mAPmask 50-95 (%) 50-95 (%) 640 YOLOv11-seg-N YOLOv12-seg-N 640 YOLO-Master-seg-N 640 38.9 39.9 42.9 32.0 32.8 35.6 Table 5. Ablation study on ES-MoE placement. Configuration Params (M) mAP (%) Baseline (no ES-MoE) Neck Only Backbone Only Both (Full) 2.63 2.49 2.66 2. 60.8 58.2 62.1 54.9 Table 6. Ablation study on expert numbers. #Experts #Params mAP50 mAP (M) 2.51 2.76 3.68 (%) 81.4 82.2 82.0 (%) 61.0 62.3 62.0 2 4 ules produce conflicting routing gradients during backpropagation, destabilizing training and preventing expert specialization. This finding reveals an important design principle: more ES-MoE modules do not guarantee better performance and careful placement is essential to avoid negative interactions. Based on these results, we adopt backboneonly ES-MoE as our default configuration, balancing accuracy and training stability. 4.3.2. Number of Experts Table 6 investigates the impact of expert count on the performance-efficiency trade-off. Four experts achieve the Table 7. Ablation study on top-K selection with 4 experts. Sparsity mAP50 (%) mAP (%) 1 2 3 75% 50% 25% 0% 81.1 81.9 81.6 81.6 61.3 61.8 61.8 61.9 Figure 3. Loss ablation study across configurations. (a) DFL loss Comparison. (b) MoE Loss Comparison. (c) Validation mAP. (d) Total loss. (e) MoE loss Evolution. (f) mAP convergence. optimal balance at 62.3% mAP and 82.2% mAP50 with 2.76M parameters. Using only 2 experts results in - 1.3% mAP drop (61.0%), indicating insufficient capacity to model diverse object patterns across different scales and semantic categories. Scaling to 8 experts yields no improvement (62.0% mAP, -0.3%) while increasing parameters by 33% (3.68M), suggesting over-parameterization where redundant experts provide diminishing returns. This validates that moderate expert diversity is sufficient for capturing multi-scale variations in object detection, and we adopt 4 experts as our default configuration. 4.3.3. Top-K Selection Strategy Given 4 experts, we analyze the effect of top-K routing in Table 7. Top-2 routing achieves optimal performance (61.8% mAP) with 50% sparsity. Top-1 routing suffers from -0.5% mAP degradation (61.3%), indicating insufficient representational capacity. Activating 3 or 4 experts yields no% respectively. The sweet spot at K=2 validates our design: two complementary experts provide sufficient feature diversity while preserving computational efficiency. This finding aligns with recent MoE literature [7, 33] showing diminishing returns beyond K=2 for vision tasks. 4.3.4. Loss Function Configuration Table 8 and Figure 3 analyze five loss configurations. Surprisingly, removing DFL loss entirely and using MoE-only loss (weight=1.5) yields the best performance at 62.2% mAP (+0.3% over baseline). The training dynamics (Figure 3) explain this: Config 4 (DFL + strong MoEλ=1.5) exhibits severe oscillations, while Config 5 (MoE-only) conFigure 4. Qualitative comparison across four challenging scenarios. All test images are from the MS COCO [24] ane PASCAL VOC. 2007+2012 [6] test set Table 8. Ablation study on DFL and MoE loss configurations. 4.3.5. Generalization to Downstream Tasks Configuration Loss DFL MoE Config 1: DFL + MoE (baseline) Config 2: No DFL, weak MoE Config 3: DFL + strong MoE Config 4: DFL + stronger MoE Config 5: MoE only (Ours) 0.5 0.5 1.0 1.5 1. mAP (%) 61.9 61.9 61.9 61.4 62.2 verges smoothly. We hypothesize that DFL and MoE losses create conflicting gradients. Specifically, DFL enforces uniform distribution-based refinement, whereas MoE loss encourages instance-adaptive expert specialization. When both have significant weights, they compete for gradient dominance, causing training instability (Config 4: 61.4% mAP, worst). Removing DFL eliminates this conflict, allowing MoE loss to guide both regression and expert specialization. This suggests that MoE loss effectively subsumes DFLs role in mixture-of-experts architectures. We adopt Config 5 (MoE-only, λ = 1.5) as our default. To further evaluate the versatility of YOLO-Master, we extend the optimal configuration derived from the ablation studies to image classification and instance segmentation. Classification. As shown in Table 3, YOLO-Master-clsN achieves 76.6% Top-1 accuracy on ImageNet, yielding substantial gains of 6.6% and 4.9% over YOLOv11 and YOLOv12, respectively. This highlights the robust feature representation of our backbone. Segmentation. In Table 4, YOLO-Master-seg-N delivers 35.6% mAP mask, surpassing YOLOv12-seg-N by 2.8% and demonstrating simultaneous improvements in both localization and mask prediction. Detection Summary. Complementing these results, our detection variant  (Table 2)  achieves 49.1% mAP box, setting new state-of-the-art for small-scale models. These consistent cross-task improvements confirm that YOLO-Master serves as powerful and general-purpose architecture, effectively elevating performance across diverse visual recognition paradigms. 4.4. Qualitative Analysis Figure 4 presents qualitative comparisons across four representative challenging scenarios. YOLO-Master-N demonstrates consistent improvements over baseline methods:"
        },
        {
            "title": "References",
            "content": "Small Object Detection (Row 1). In the outdoor scenario featuring small animals on grass, earlier versions (v10-v11) fail to detect the distant object. YOLOv12-N begins detection with low confidence 0.47, YOLOv13-N improves to 0.53, while YOLO-Master-N achieves confident detection (0.65-0.82) with accurate localization, validating the effectiveness of scale-adaptive expert routing for smallscale objects in challenging backgrounds. Category Disambiguation (Row 2). The coastal scene with bird near rocks presents challenging background camouflage. While YOLOv10-N through v12-N fail to detect the occluded person, YOLOv13-N achieves marginal detection. YOLO-Master-N produces accurate detection with precise localization (cyan box), demonstrating that expert specialization enables better discrimination of occluded objects from complex backgrounds. Complex Scene (Row 3). In the challenging sheep shearing scene with overlapping animals and human interaction, YOLO-Master-N achieves clean detection with accurate localization (avg confidence 0.85 vs. 0.77 for v13), demonstrating effective handling of complex scenes. Dense Scene (Row 4). In the challenging dining scenario with numerous overlapping objects (bottles, cups, utensils) and person, earlier versions miss many small items. YOLO-Master-N achieves comprehensive detection with high confidence (0.87-0.97), demonstrating superior capability in dense, cluttered environments. Across all scenarios, YOLO-Master-N achieves higher average confidence and more complete detection coverage, demonstrating the effectiveness of ES-MoEs adaptive expert specialization for diverse real-world challenges. 5. Conclusion In this paper, we present YOLO-Master, novel realtime object detection framework that introduces Efficient Sparse Mixture-of-Experts (ES-MoE) to the YOLO architecture. Our approach addresses the fundamental tradeoff between model capacity and computational efficiency through lightweight dynamic routing network. We employ soft top-K routing during training to maintain gradient flow, and switch to hard top-K routing during inference to achieve genuine computational sparsity. Comprehensive experiments on five large-scale benchmarks demonstrate that YOLO-Master achieves state-of-the-art performance with superior efficiency. It proves that sparse MoE architectures can be successfully adapted to dense prediction tasks, demonstrating that dynamic expert selection improves both accuracy and efficiency simultaneously. Looking forward, our approach can be extended to other vision tasks beyond detection, paving the way for efficient real-time vision systems on resource-constrained devices through adaptive neural architectures with conditional computation. [1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. European Conference on Computer Vision, pages 213229, 2020. [2] Yi Chen, Hongchen Tan, Tianqi Wang, et al. Modalityagnostic mixed-expert training for vision-language models. In International Conference on Machine Learning (ICML), 2023. [3] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, et al. Cswin transformer: general vision transformer backbone with In Proceedings of the IEEE/CVF cross-shaped windows. Conference on Computer Vision and Pattern Recognition, pages 1212412134, 2022. [4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [5] Dawei Du, Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Lin, Qinghua Hu, Tao Peng, Jiayu Zheng, Xinyao Wang, Yue Zhang, Liefeng Bo, Hailin Shi, Rui Zhu, Aashish Kumar, Aijin Li, Almaz Zinollayev, Anuar Askergaliyev, Arne Schumann, Binjie Mao, Byeongwon Lee, Chang Liu, Changrui Chen, Chunhong Pan, Chunlei Huo, Da Yu, DeChun Cong, Dening Zeng, Dheeraj Reddy Pailla, Di Li, Dong Wang, Donghyeon Cho, Dongyu Zhang, Furui Bai, George Jose, Guangyu Gao, Guizhong Liu, Haitao Xiong, Hao Qi, Haoran Wang, Heqian Qiu, HongLiang Li, Huchuan Lu, Ildoo Kim, Jaekyum Kim, Jane Shen, Jihoon Lee, Jing Ge, Jingjing Xu, Jingkai Zhou, Jonas Meier, Jun Won Choi, Junhao Hu, Junyi Zhang, Junying Huang, Kaiqi Huang, Keyang Wang, Lars Sommer, Lei Jin, and Lei Zhang. Visdrone-det2019: The vision meets drone object detection in image challenge results. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2019. [6] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge. International Journal of Computer Vision, 88(2):303338, 2010. [7] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [8] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. [9] Eran Goldman, Roei Herzig, Avital Eisenschtat, Jacob Goldberger, and Tal Hassner. Precise detection in densely packed In Proceedings of the IEEE/CVF Conference on scenes. Computer Vision and Pattern Recognition (CVPR), pages 32543263, 2019. [10] Qibin Hou, Daquan Zhou, and Jiashi Feng. Coordinate attention for efficient mobile network design. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1371313722, 2021. [11] Andrew Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [12] Jie Hu, Li Shen, Samuel Albanie, et al. Gather-excite: Exploiting feature context in convolutional neural networks. In Advances in neural information processing systems, pages 94239433, 2018. [13] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 71327141, 2018. [14] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural Computation, 3(1):7987, 1991. [15] Glenn Jocher. Ultralytics yolov5, 2020. [16] Glenn Jocher and Jing Qiu. Ultralytics yolo11, 2024. [17] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics yolov8, 2023. [18] Mengqi Lei, Siqi Li, Yihong Wu, and et al. Yolov13: Realtime object detection with hypergraph-enhanced adaptive visual perception. arXiv preprint arXiv:2506.17733, 2025. [19] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. [20] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, et al. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. [21] Mike Lewis, Shruti Bhosale, Tim Dettmers, et al. Base layers: Simplifying training of large, sparse models. International Conference on Machine Learning, pages 62656274, 2021. [22] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yilong Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, et al. Yolov6: single-stage object detection framework for industrial applications. arXiv preprint arXiv:2209.02976, 2022. [23] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In International Conference on Learning Representations, 2014. [24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In Computer Vision ECCV 2014, pages 740755. Springer International Publishing, 2014. [25] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming Feature arXiv preprint He, Bharath Hariharan, and Serge Belongie. pyramid networks for object detection. arXiv:1612.03144, 2017. [26] Ze Liu, Yutong Lin, Yue Cao, et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [27] Tuan Nguyen, Thanh Mai, Hai Tran, Tuan Nguyen, and Duc Nguyen. Small object detection: comprehensive survey on challenges, techniques and real-world applications. Array, 25:100421, 2025. [28] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In European conference on computer vision, pages 145161. Springer, 2020. [29] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. arXiv preprint arXiv:2308.00951, 2023. [30] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. [31] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. arXiv preprint arXiv:1506.02640, 2015. [32] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 779788, 2016. [33] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Julian Menick, Yee Whye Xu, Jasper Snoek, Tao Yang, et al. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. [34] Ranjan Sapkota, Rizwan Mishra, Meenu Yu, and Han Yu. Yolo advances to its genesis: decadal and comprehensive review of the you only look once (yolo) series. Artificial Intelligence Review, 2025. [35] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. arXiv preprint arXiv:1701.06538, 2017. [36] Yunjie Tian, Qixiang Ye, and David Doermann. Yolov12: Attention-centric real-time object detectors. arXiv preprint arXiv:2502.12524, 2025. [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. [38] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time endto-end object detection. arXiv preprint arXiv:2405.14458, 2024. [39] Haoxuan Wang, Qingdong He, Jinlong Peng, Hao Yang, Mingmin Chi, and Yabiao Wang. Mamba-yolo-world: marrying yolo-world with mamba for open-vocabulary detection. In ICASSP 2025-2025 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 15. IEEE, 2025. [40] Lemeng Wang, Soyoung Yoon, Trung Jin, James Li, Xuwang Wang, and Rose Yu Chen. Residual mixture of experts. arXiv preprint arXiv:2204.09636, 2022. [41] Qilong Wang, Banggu Wu, Pengfei Zhu, et al. Eca-net: Efficient channel attention for deep convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1153411542, 2020. [42] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. Proceedings of the European Conference on Computer Vision (ECCV), pages 319, 2018. [43] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. 2023. [44] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou loss: Faster and better learning for bounding box regression. Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):12993 13000, 2020. [45] Yanqi Zhou, Tao Lei, Hanxiao Liu, et al. Mixture-of-experts with expert choice routing. In Advances in Neural Information Processing Systems, pages 71037114, 2022."
        }
    ],
    "affiliations": [
        "Singapore Management University",
        "Tencent Youtu Lab"
    ]
}