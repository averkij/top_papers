{
    "paper_title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
    "authors": [
        "Hongchi Xia",
        "Xuan Li",
        "Zhaoshuo Li",
        "Qianli Ma",
        "Jiashu Xu",
        "Ming-Yu Liu",
        "Yin Cui",
        "Tsung-Yi Lin",
        "Wei-Chiu Ma",
        "Shenlong Wang",
        "Shuran Song",
        "Fangyin Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 6 1 1 0 1 . 2 0 6 2 : r SAGE: Scalable Agentic 3D Scene Generation for Embodied AI Hongchi Xia1,2* Xuan Li1 Zhaoshuo Li1 Qianli Ma1 Jiashu Xu1 Ming-Yu Liu1 Yin Cui1 Tsung-Yi Lin1 Wei-Chiu Ma3 Shenlong Wang2 Shuran Song1,4 Fangyin Wei1 1NVIDIA 2University of Illinois Urbana-Champaign 3Cornell University 4Stanford University Figure 1. Overview and example outputs of SAGE. Given an open-ended user request, our system autonomously composes realistic, diverse, and simulation-ready 3D environments. The generated scenes are directly deployable in modern simulators, supporting embodied tasks such as Mobile Manipulation and Pick-and-Place. Through agent-driven reasoning, generator orchestration, and multi-level augmentation, the framework produces interactive environments at scale for robot policy learning."
        },
        {
            "title": "Abstract",
            "content": "Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scenegeneration systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given user-specified embodied task (e.g., pick up bowl and place it on the table), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative rea- *This work was done while Hongchi Xia was an intern at NVIDIA. soning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here. 1. Introduction Embodied AI is on the hunt for data. Unfortunately, while the web has powered large vision and language models [2, 39], neither the web nor the real world can resolve the 1 Framework Method Rule-based Infinigen [40], ProcTHOR [7] 3D DataDriven DiffuScene [46], Text2Room [15] ATISS [37], PhyScene [61] CommonScenes [66], EchoScene [67] LLM/VFMs LayoutGPT [9], LayoutVLM [43], I-Design [3] Static Pipeline Agentbased Holodeck [62], ArtiScene [14], Architect [54] AnyHome [10], Scenethesis [26], GenUSD [25] SceneWeaver [63] SAGE (Ours) Input Methodology Simulation-Ready Output OpenVocabulary Modality Text Built-in Scene Grounded Generation in 3D SelfImprovement Fine-grained Collision w/ Physical SimulatorAttributes Validated Control Check Text / 3D Scene Floor Plan Scene Graph Text Text Text (+Image) (pose-only) Table 1. Comparison of scene generation methods. Our agent-based method uniquely fulfills all criteria, enabling scalable generation of simulator-validated data essential for robotic applications. pressing data needs of embodied agents. Real-world embodied data collection is slow and costly [52], and is fundamentally constrained by the need for interactive environments [41]. Simulation thus emerges as the natural alternative, providing scalable, interactive, and low-cost embodied environments with safety guarantees [30]. To effectively support embodied AI, simulated data must satisfy four desiderata: (i) Realism: the geometry, appearance, semantic structure, and physics of the simulation should resemble the real world closely enough that policies learned in simulation can reliably transfer to reality; (ii) Diversity: simulation should encompass wide range of assets, environments, and tasks to prevent overfitting and support generalization; (iii) Simulation-readiness: objects and scenes must be physically stable, interactable, and directly compatible with modern simulators for large-scale training; (iv) Task-awareness: simulation environments should adapt to and facilitate the training of various targeted embodied tasks. For instance, data engine should provide various kitchens for household robots, fire-hazard scenes for rescue robots, surgical rooms for medical robots, etc. Unfortunately, existing methods for generating 3D simulated data fall short on one or more of these criteria. While Real2Sim approaches [17, 5658] prioritize realism by reconstructing digital twins from the real world, the high costs of data capture and reconstruction make them difficult to scale. In contrast, generative approaches offer more scalable alternative, but still face significant bottlenecks. For example, rule-based systems [7, 40] ensure physical plausibility but sacrifice flexibility and diversity. Data-driven approaches [15, 37, 46] improve realism, but their limited 3D training data prevents them from generalizing to new room types, handling open-vocabulary prompts, or supporting fine-grained layout control. Pipelines based on foundation models [9, 43, 62] enable text-driven generation but lack 3D grounding, yielding physically invalid scenes. Perhaps most importantly, these systems are static: their computational graph is fixed, preventing adaptive reasoning and selfcorrection. Recently, concurrent work, SceneWeaver [63], takes step toward agentic scene generation. Yet, it is not directly deployable in simulation due to missing physical attributes and the absence of simulator-in-the-loop verification. There are still major gaps in (1) physical grounding for interaction and (2) compatibility with robot simulators, as shown in Tab. 1. In this work, we present SAGE, novel agentic framework for scalable 3D scene generation that produces simulation-ready environments directly from arbitrary user prompts. The agent, operating over Model Context Protocal (MCP) [31], adaptively orchestrates generators for floor plans, structured layouts, and text-to-3D assets, while two complementary critics provide continuous feedback: visual critic for semantic/spatial coherence and physics critic with simulator-in-the-loop validation (Isaac Sim [34]) for stability under gravity and collisions. This closed loop enables selfcorrection without hard-coded tool order and yields scenes that are both realistic and interactable. We scale these scenes for embodied AI by applying multilevel augmentation (object configuration, object category, and layout) to generate diverse yet task-consistent environments. On top of these scenes, we automatically synthesize action data with grasp pose proposals [65], collision-aware Inverse Kinematics (IK) [44], and navigation planning, and train Diffusion Policy [4]. Experiments show clear scaling trends with increasing scene diversity and demonstration count, and improved generalization to unseen objects and layouts. In summary, our contributions are twofold: An agentic scene-generation framework that unifies multiple generators with visual and physics critics under MCP, enabling adaptive tool use, self-improvement, and simulator-validated stability. It generates open-vocabulary scenes with state-of-the-art realism and physical validity. An embodied AI data pipeline that automatically generates unlimited scenes and teacher demonstrations for imitation learning, yielding clear scaling benefits and stronger generalization to unseen environments. Figure 2. Overview of SAGE scene generation. Our system converts open-vocabulary text prompts into simulation-ready 3D scenes by orchestrating multiple generator tools and critics. The agent dynamically calls generators (Scene Init, Asset Placer/Mover/Remover) to construct and refine layouts, while visual and physics critics provide iterative feedback for self-improvement. The visual critic suggests semantic corrections (e.g., missing or misplaced objects), and the physics critic validates stability via Isaac Sim. For example, after applying physics critic in the bottom image, the newly added pillows on the bed fall flat. This self-improvement process ends when the agent considers that the generated scene meets the input user requirements. The resulting scenes can be further scaled via augmentation and used for embodied policy learning. 2. Related Works 3D Indoor Scene Synthesis Early work synthesized indoor scenes with procedural systems such as ProcTHOR [7] and Infinigen Indoors [40], assembling layouts via rules and grammars. This paradigm grounds geometry and scales well but fixed recipes constrain open vocabulary, fine-grained control, and self-improvement, and rarely expose physicsrich attributes beyond hand-coded rules. Data-driven methods such as ATISS [37] and DiffuScene [46], along with graph-aware CommonScenes [66] and EchoScene [67], learn strong spatial priors and yield realistic layouts with neural 3D generators; nevertheless they inherit closed taxonomies, offer limited object-level levers, and do not attach explicit physical properties or run simulator-side validation. Motivated by advances in long-horizon reasoning for LLM agents [12, 55, 64], recent methods shift to languageor vision-assisted scene synthesis. Holodeck [62] and ArtiScene [14] leverage LLMs or image intermediaries to widen semantic coverage and stylistic control; however, they usually follow static, pre-ordered modules, provide little self-improvement, have sparse 3D grounding, and omit physics validation such as collision or stability. Some works [10, 26] combine LLM planning with vision-guided refinement under constraints, but the refinement is only on asset pose. Building further on agent capabilities for iterative decision making and self-reflection [16, 42], agentic pipelines [25, 63] generate complete layouts and refine them through feedback. SceneWeaver [63] further leverages tool use for relation/collision-aware placement. Despite these advances, orchestration often outweighs end-to-end simulation deliverables: physical attributes are not systematically attached and simulator-validated outputs are not the default artifact. Our method SAGE pushes from semantically plausible to task-aligned, simulation-ready: we support openvocabulary prompts (with optional images), generate scenes natively, attach object-level physical properties, and perform in-simulator validation (collision and stability), so outputs are directly deployable for embodied policy training. Simulation Environment for Embodied AI In recent years, numerous simulation benchmark environments have been developed to provide safe, scalable, and controllable platforms for evaluating robot learning algorithms before real-world deployment. LIBERO [27], CALVIN [29] and SimplerEnv [24] focus on tabletop manipulation tasks such as pick-and-place and object rearrangement using simple robotic arm setups. iGibson [22], Habitat [45], BEHAVIOR1K [23], ManiSkill [47], RoboCasa [32], AI2Thor [18], VirtualHome [38], ThreeDWorld [11] extend the scope to homescale embodied activities that combine object interaction and motion planning within complex 3D indoor scenes. Some recent works also aim to unify and scale up embodied AI benchmarking. RoboGen [53] combines automatic object generation and task generation. RoboVerse [13] integrates multiple simulators and benchmarks to support seamless transitions across environments. Underlying these platforms are diverse physics engines such as PyBullet [5], MuJoCo [49], Isaac Sim [34], SAPIEN [59], Genesis [1], which are specifically developed for robotics simulation, and Unity [50], Unreal [8], which are designed for game developments. While many of these frameworks provide rich, realistic, and ready-to-use environments for embodied AI, they often require substantial manual effort for scene and asset creation. Others rely on procedurally retrieved assets, which can limit diversity when scaling up. And the simulation is often only enabled when the creation is complete. In contrast, SAGE integrates 3D generation frameworks, enabling openvocabulary asset creation with high flexibility and minimal manual intervention. Furthermore, by incorporating simulation directly into the generation loop, SAGE supports iterative self correction and improvement. 3. Method Given user demand with robot task description, our goal is to generate diverse 3D scenes ready to run embodied agents in simulation for scalable policy learning. At the core of SAGE is an agent-driven scene generation framework, detailed in Sec. 3.1. Based on the feedback from visual and physical critics, the agent effectively self-improves the generation with multiple editing operations supported by scene generator tools. In Sec. 3.2, we describe how scene generation can be easily scaled up with object-level and scene-level augmentation, which can then be used for action generation to train embodied AI policy. 3.1. Agent-driven Scene Generation SAGE operates under the Model Context Protocol (MCP), standardized protocol for seamless interaction with external tools [31]. In this setup, the agent acts as the MCP client, while each tool (e.g., layout generator, physics simulator) is hosted behind an MCP server. The agent takes chain of actions to improve the scene until the scene is considered visually realistic and physically stable. In each iteration, when the agent identifies the need for specific capability, e.g., generating floor plan or validating physical stability, it sends structured request through MCP. The server executes the corresponding operation, returns the result, which the agent incorporates as feedback to decide the next action. This setup enables adaptive, tool-driven scene generation without hard-coded logic. Fig. 2 summarizes our approach. 3.1.1. Generator The scene is constructed through set of generator tools that the agent dynamically invokes via MCP. Each tool performs specific operation initializing layouts, adding new assets, or adjusting existing ones and can be flexibly composed based on the agents reasoning and critic feedback. This allows for iterative, adaptive scene construction with finegrained control over content and structure. Scene Initializer takes the scene specification as input and is responsible for generating an empty 3D room (with only floor and walls). It also outputs list of proposed objects, each with text description, estimated physical attributes, and placement constraints (e.g., relationship to other objects, boundaries). The textures of floor and wall are generated by MatFuse [51], with size assigned from the LLM prediction. The object list reflects the scene demands or robot task descriptions. For example, if the user asks about learning task of pick an apple and place it to bowl, the generator will include the required apple and bowl in the object list to be returned to the agent. Asset Placer takes in text string describing the placement requirement for few objects. Its task is to generate and place these objects into the 3D scene. We generate objects with TRELLIS [60] given text description. To make the object simulation-ready, we also leverage VLM to estimate physical properties, including height to rescale the unit-sized object, mass for physical simulation, and metallic/roughness values for physics-based rendering (PBR). For each object, we use an LLM to analyze the input placement condition and choose one of the three categories: floor, wall, and on-top. This further guides the placement sequence and constraints of each placement. We adopt depth first search with collision avoidance to place the objects to possible location candidates following the sequence, and choose the placement the best satisfies the constraints. Asset Mover locates and moves the object specified in the input text string. This is achieved by first removing the object and then reusing the placement planner from the Asset Placer to reloate the object. Movement instructions typically come from the visual critic. Asset Remover removes the object described in the input text, using LLM-based reasoning to locate the object in the scene. It is usually called when the critics (described later) gives feedback to remove an object. 3.1.2. Critic for Self-Improvement Naively stacking different generators causes error accumulation from individual imperfections. Errors generally fall into two types: visual artifacts (e.g., missing or misplaced objects) and physical violations (e.g., instability or collisions). To mitigate this, we introduce visual critic that assesses semantic and spatial coherence, as well as physics critic that enforces stability and simulator readiness. Together, they guide iterative self-correction high-quality scene generation. Visual Critic To evaluate and improve the layout quality and scene completeness, we introduce visual critic. The critic takes the current scene configuration as input, i.e., object placements and multi-view renderings (top-down and four corner views), and proposes new objects to place as well as adjustments to existing placements. Its feedback is then incorporated to aid the agents decision-making of the most appropriate generator to invoke next. By integrating these visual feedback signals, the agent gains more comprehensive understanding of the current scene state and can decide which generator to invoke next for optimal refinement. 4 Physics Critic Beyond visual realism, generated scene must also be physically stable and simulation-ready to support embodied learning. To achieve this, we employ simulation-in-the-loop validation during generation. After each object addition, movement, or removal. the scene is loaded into Isaac Sim [34] to test its physical stability. We measure the pose change after simulation and reject placements that cause instability or collisions, retaining only candidates that preserve global stability. If the generator fails to find stable configuration, the critic reports the failure to the agent, suggesting alternative actions such as using smaller objects or adjusting placement target locations. Through this iterative validation loop, SAGE maintains near-perfect physical stability, ensuring that the resulting scenes are directly deployable for downstream embodied learning tasks. 3.2. Scaling the Scene for Embodied AI To train robust and generalizable policy, it is not sufficient to use single generated scene; instead, we must generate diverse set of simulation-ready scenes that follow the users task specification. To this end, we introduce scene augmentation strategy that systematically expands each generated environment into numerous variants while preserving task semantics. These augmented scenes are then used to generate corresponding robot actions via motion planning, followed by imitation learning to train task-specific embodied policies. 3.2.1. Scene Augmentation To scale one generated scene into diverse yet taskconsistent set of variants, we apply (1) task-relevant object augmentation (configuration, category) that diversifies key objects while preserving task semantics, and (2) taskirrelevant scene augmentation to enrich the rest of the scenes. Object Configuration-level The pose of each taskrelevant object (e.g., the target to be picked, placed on, or navigated toward) is resampled within the current scene to create variations in object placement. This process increases spatial diversity while preserving overall scene semantics. Object Category-level Given the text description of each task-relevant object from the generation stage, we employ an LLM-based text augmentation to produce variations in geometry and texture (e.g., shape, color, material, or finish) while maintaining the original object category. We then use TRELLIS [60] to synthesize corresponding 3D assets from these augmented descriptions, which are placed into the scene to enrich visual and physical diversity across instances. background scene, including room geometry and all taskirrelevant objects, is regenerated through the agent-driven scene generation. This process produces diverse scene layouts sharing the same task specification, enabling learning policies that generalize across spatial configurations. Simulation-ready Validation After each augmentation step, we call the physics critic to ensure the stability and physical plausibility of all placements. Object-level physical properties such as mass and PBR parameters are estimated by VLM as before. This guarantees that every augmented scene remains physically valid and immediately deployable for policy training. 3.2.2. Action Generation The generated environments are now ready for downstream embodied AI tasks. We demonstrate their utility on two representative tasks, i.e., Pick-and-Place and Mobile Manipulation, and automatically generate large-scale action demonstrations using established motion planning techniques. This enables scalable policy learning and highlights the benefits of simulation-ready scene generation. Pick-and-Place For grasping actions, we use M2T2 [65] to generate grasp pose candidates from rendered depth images. Collision-free trajectories are computed by integrating Curobo [44] into the motion planning and inverse kinematics pipeline, ensuring feasible and stable grasp execution. Mobile Manipulation This task is composed of navigation with object pick-and-place in between. For the navigation motions, we adopt RRT [21] for robot path planning, generating collision-free trajectories between designated start and target positions. Failure Modes Motion planning is not always successful, often due to inaccurate grasp pose predictions, unreachable target configurations, or unexpected collisions during trajectory execution. We filter out failed examples by performing collision checks and verifying whether each manipulated object reaches its expected location. 3.2.3. Policy Learning Given the large-scale action data generated from motion planning, we employ imitation learning to train generalizable policies from these demonstrations. Specifically, we use Diffusion Policy [4] for policy learning. The model takes as input RGB and depth images from multiple camera views along with corresponding end-effector trajectories, and outputs continuous actions for next-step execution. Scene Layout-level While the above augmentations modify task-related objects, the background environment remains unchanged. For tasks requiring full-scene exploration or navigation, we introduce layout-level augmentation, where the 4. Experiments We evaluate scene generation in Sec. 4.1. In Sec. 4.2, we demonstrate how scalable scene and action data generated by our framework improves policy generalization. ] 2 6 [ d H ] 3 6 [ a n ) O ( S Bedroom Living room Fairy-tale princess room Rusty and dusty restroom Figure 3. Common and open-vocabulary scene generation comparison. Compared with baselines, SAGE produces more complete scenes with more realistic layouts on common room types, while following the style prompts more faithfully on open-vocabulary queries. Room Type Method Visual Physics #Obj Real. Func. Lay. Comp. Coll. % Stab. % Bedroom Kitchen Holodeck[62] 28.5 SceneWeaver[63] 17.5 48.3 SAGE (Ours) Holodeck[62] 28.5 SceneWeaver[63] 37.5 47.6 SAGE (Ours) Living Room Holodeck[62] 34.0 SceneWeaver[63] 18.1 48.8 SAGE (Ours) Average Holodeck[62] 30.3 SceneWeaver[63] 24.4 48.2 SAGE (Ours) 7.4 9.0 9.0 6.7 8.2 8.5 8.3 8.5 8.8 7.5 8.6 8.8 6.8 9.7 10.0 6.1 7.7 9. 7.3 9.3 9.5 6.7 8.9 9.5 5.0 7.8 8.0 4.4 6.8 7.8 5.7 7.2 7.8 5.0 7.3 7. 6.1 7.5 9.5 6.2 7.2 7.6 7.3 6.8 7.4 6.5 7.2 8.2 29.1 31.0 2.3 16.0 28.0 0. 20.8 39.5 2.7 22.0 32.8 1.9 51.0 58.8 99.8 73.8 67.0 100.0 66.5 77.2 100.0 63.8 67.7 99. Table 2. Scene generation evaluation on common scene types. Scores averaged across 10 scenes per room type. SAGE consistently outperforms prior methods across all categories. 4.1. Scene Generation 4.1.1. Setup Implementation Details SAGE integrates series of large foundation models, including LLMs, VLMs, and specialized generators for 3D objects and background textures. We use open-source models hosted via self-managed APIs to ensure reproducibility. Specifically, we adopt gpt-oss-120b [36] as both the agent LLM and the integrated LLM, and Qwen3VL-30B-A3B-Instruct [48] for visionlanguage reasoning. Metrics Following [63], we evaluate the visual quality and physical plausibility with scores averaged across 10 generated scenes per room type. Visual metrics cover Realism, Functionality, Layout, and Completeness based on GPT-4.1 [35]. Physical validity includes the collision ratio of 3D object meshes using trimesh [6], and the ratio of stable objects within Isaac Sim [34]. An object is considered unstable if its relative translation exceeds 0.2 meter or its rotation exceeds 8 degrees after 120 simulation steps. Baselines We compare against two state-of-the-art methods using their official codebases. Holodeck [62] is LLMdriven but lacks self-improvement due to its fixed generation pipeline. SceneWeaver [63] is agent-based yet omits simulator validation, yielding nonsimulation-ready scenes. 4.1.2. Experiment Results Common Types We evaluate SAGE and the baselines on three common indoor scene types: Bedroom, Kitchen, and Living Room. Quantitative results are reported in Tab. 2, with qualitative comparisons in Fig. 3. SAGE achieves the best results across all metrics, in both visual quality and physical stability. Holodeck [62] generates fewer objects with lower realism and functionality, and exhibits frequent collisions due to its rigid, predefined generation pipeline. SceneWeaver [63] achieves moderate visual quality but exhibits high collision rates and low stability, mainly due to its absence of simulator-based validation. As illustrated in Fig. 5, baseline scenes show displaced or fallen objects after simulation, whereas SAGE remains fully stable. 6 Gym Office Cyberpunk game den Starry-night bedroom Meeting room Children room Golden and luxury bedroom Muddy and dirty dining room Figure 4. Additional open-vocabulary generation. SAGE produces diverse, semantically coherent scenes spanning various styles and functionalities, from Gym and Office spaces to creative themes like Cyberpunk game den and Starry-night bedroom. Holodeck [62] SceneWeaver [63] SAGE (Ours) physically stable scenes. f e A Figure 5. Stability verification. Generated scenes are loaded into IsaacSim for physical validation. Both baselines exhibit displaced objects due to instability, whereas SAGE preserves scene stability before and after simulation. Open-Vocabulary Types We further demonstrate the open-vocabulary generation capability of SAGE across wide range of scene types. As shown in Fig. 3 and Fig. 4, our method successfully generates highly diverse and stylized spaces (e.g., Gym, Office, Cyberpunk game den, Starry-night bedroom). Unlike retrieval-based methods, our text-to-3D object synthesis enables the creation of long-tail, semantically coherent scenes with faithful adherence to user prompts and distinctive visual styles. Ablation Study We conduct an ablation study to assess the impact of each critic design. As shown in Tab. 3, adding the visual critic substantially improves visual quality, while the physics critic greatly reduces collisions from 7.8% to 1.9% and raises stability to 99.6%. Combining both critics yields the best overall performance across all metrics, confirming that visual feedback and simulation-in-the-loop validation are complementary and critical for generating realistic and 7 SAGE-10k Dataset To support community research at scale, we pre-generated 10k-scene dataset with our approach, named SAGE-10k Dataset. Its across 50 room types in diverse styles, including 565K uniquely generated 3D objects. The preview image and statistics of the dataset are shown in Fig. 6. 4.1.3. Additional Capabilities and Extensions In this subsection, we present several straightforward extensions of SAGE that demonstrate its flexibility across diverse generation settings. While these capabilities are not the primary focus of this paper, they arise naturally from the modular design of our framework and require minimal adaptation. These results highlight the generality of SAGE and suggest that further improvements in generation quality can be achieved by strengthening individual components, which we leave as promising direction for future work. Multi-room Layout We showcase the multi-room generation capability of SAGE across different kinds of scene layouts, as Fig. 7 shows. Connected floor plans are created in the Scene Initializer. Additionally, the agent can execute parallel updates with Asset Placer/Mover/Remover, and the generators are updated to accept multiple room IDs and conditions within single MCP tool call. Image-conditioned Scene Generation We demonstrate that SAGE can be extended with image-conditioned generation without architectural changes by utilizing Qwen3-VL [48] to extract style and object attributes from reference Figure 6. SAGE-10k Dataset: We pre-generated 10k-scene dataset named SAGE-10k Dataset across 50 room types and 50 styles, including 565K uniquely generated 3D objects. We include the statistics of room types, room examples, and objects per scene in the figure as well. The dataset can be accessed via this link. Multilingual teachers apartment student apartment with one bedroom Mid-century modern family home Figure 7. Multi-room open-vocabulary generation. SAGE can be extended to generate multi-room scenes at scale easily by generating the floor plan and then calling generator MCP tools to fill in multiple rooms in parallel. m f n d r G Figure 8. Image-conditioned scene generation. Using Qwen3VL [48], SAGE extracts style and object attributes from reference images to enable image-conditioned scene generation without architectural modifications. The generated scenes are not pixel-aligned but remain semantically consistent with the reference images. images, as shown in Fig. 8. While not pixel-aligned, the generated scenes remain semantically coherent. 8 Articulated Objects In addition to rigid-body objects, our modular design can easily extend the current text-to-3D generation with object retrieval. As Fig. 9 shows, SAGE can be integrated with articulated assets from PartNet-Mobility [59] in the generated scenes. Also, robot actions can be generated by grasp pose prediction and motion planning. 4.2. Embodied AI Learning We evaluate whether the diversity of our generated scenes enables effective scaling by measuring policy improvements under increasing scene and demonstration counts. 4.2.1. Setup Implementation Details We study two representative tasks: (1) Pick-and-Place with Franka Emika Panda robot, (2) Mobile Manipulation using Franka Emika Panda arm mounted on an Omron LD-60 mobile base [33, 68]. All action data generation and policy rollout are conducted in Isaac Lab [30] on NVIDIA L40S GPUs with parallel simulation. Policies are trained via the imitation learning framework Robomimic [28]. Pick-and-Place uses single model, whereas the long-horizon Mobile Manipulation is decomposed into four sequential policies. Each policy is responsible for one stage of the task (see task description) and also Figure 9. Articulated Objects: SAGE can be extended with articulated objects using retrieval from PartNet-Mobility [59]. Top: we show two scenes with multiple articulated objects at closed and open states. Bottom: an action sequence generated with grasp pose prediction and motion planning( Sec.3.2.2) for pick up the bowl, place it in the drawer, and close the drawer. Please visit website for full video. Critics Setting Visual Physics Visual Physics #Obj Real. Func. Lay. Comp. Coll. % Stab. % 35.3 50.1 36.8 53.7 8.5 8.9 8.8 8. 9.2 9.5 9.3 9.6 7.5 8.1 7.7 8.0 7.3 8.1 7.8 8.2 7.8 3.7 1.9 0.8 80.3 84.1 99.6 100.0 Table 3. Ablation study on critics. Results are averaged over five scenes for each of three room types. Adding both visual and physics critics leads to the best results, confirming their joint importance in scene generation. Best is in bold, second best is underlined. predicts termination signal for stage transition. We report success rates averaged across three held-out scenes, each evaluated with 90 random robot spawning poses and object configurations. Task Description For Pick-and-Place, the robot must pick up mug from the table and place it into bowl. For Mobile Manipulation, the robot is initialized at random position in the scene. It needs to navigate to table with coke can, pick it up, move to another desk, and place the can on it. Scene and Action Generation SAGE can automatically generate large-scale scene and action data starting from the task descriptions. Object-level and scene-level augmentation is applied to Pick-and-Place and Mobile Manipulation, respectively. We collect over 28k demonstrations for Pick-andPlace with 264 unique objects and nearly 50k demonstrations for Mobile Manipulation across 50 diverse scenes. 4.2.2. Policy Learning Scaling Trend with SAGE-generated Data As shown in Fig. 10 and Fig. 11, policy success rates increase with the number of scenes. Despite that the learned policy only has access to partial visual observations, it gradually converges towards the privileged agent which has full access to the entire 3D scene information. To contextualize performance relative to task difficulty, we also compare the success rate between policy rollout and motion planning in Tab. 4. Our results show that the learned policy achieves success rates approaching those of the underlying motion planner, confirming both the fidelity of the generated data and the effectiveness of imitation learning at scale. Comparison with Baselines Since existing methods rely on heterogeneous simulation setups and implementations, directly integrating them into unified environment is nontrivial. To enable fair comparison, we design two baselines that mimic the methodology of prior works by removing components unique to our framework and substituting them with their counterparts. As shown in Fig. 11, both baselines improve slowly with scaleachieving less than one-third of the final success rate of the policy trained on SAGE-generated data, even with the same number of demonstrations and scenes. To further demonstrate the generalizability of our trained policy, we test on baseline-generated held-out scenes as in Fig. 11. As the results in Tab. 5, despite distribution shift favoring baselines, our trained policies achieve higher success rates, indicating robust generalization. This comparison highlights that the unique design of SAGE, including agentic orchestration, physics validation, and text-to-3D synthesis, is critical for generating effective training data. Our method not only converges significantly faster than the baselines but also approaches the performance of the privileged agent, demonstrating that high-quality, simulation-validated scenes can efficiently drive policy learning. These results 9 Figure 10. Examples and scaling curve on Pick-and-Place. Top left: diverse generation. Bottom left: example trajectory. Right: success rate w.r.t. demo/object counts. More diverse object augmentations improve policy success, narrowing the gap to the privileged agent. Figure 11. Examples and scaling curve on Mobile Manipulation. Left: task overview. Mid-top: diverse generation. Mid-bottom: example trajectory. Right: success rate w.r.t. demo/scene counts. Both baselines omit physics critic and replace text-to-3D object synthesis with retrieval: Baseline 1 mimics SceneWeaver [63]; Baseline 2 further replaces the agent with fixed pipeline, resembling Holodeck [62]. Diverse SAGE-augmented scenes boost the learned policys success and close the gap to the privileged agent, while removing physics critic and object synthesis degrades performance (Baseline 1). Replacing the agent with static pipeline further reduces success rate (Baseline 2). Task Pick-and-Place Mobile Manipulation Motion Planning Policy Rollout Train 65.3 68.4 Test 57.7 52. Train 63.4 54.6 Test 50.0 46.0 Table 4. Comparison of training and test success rates for policy rollout and motion planning. The learned policy generalizes well from training to testing, and policy rollout performance approaches that of the privileged motion planning. Training Source Test Source Succuss Rate (%) Baseline 1 [63] Baseline 2 [62] SAGE Baseline 1 [63] Baseline 2 [62] SAGE (Ours) 13.2 13.5 39.1 9.3 16.2 24.7 14.4 13.1 46. Table 5. Cross-Evaluation. SAGE policies generalize better on out-of-distribution scenes. SAGE even achieves higher success rates on baseline-generated scenes. suggest promising path toward scalable, simulation-driven learning for embodied AI. 5. Conclusion We present SAGE, an agentic framework that turns openvocabulary text prompts into simulation-ready indoor environments by orchestrating layout and asset generators with visual and physics critics via MCP. The same pipeline scales them for embodied learning through multi-level scene augmentation and automatic action synthesis. On two representative tasks, i.e., Pick-and-Place and Mobile Manipulation, policies trained purely on our generated data show clear scaling with scene diversity and demonstration count, improving generalization to unseen objects and layouts. Limitations and future work. Our current scope emphasizes indoor scenes and rigid-body physics, and extending to outdoor settings as well as articulated and deformable objects is promising. Action generation presently targets flexible compositions of pick, place, and navigation. Incorporating additional tasks is natural next step. Beyond imitation, coupling the generator with online RL and real-robot closedloop validation could further boost performance."
        },
        {
            "title": "References",
            "content": "[1] Genesis Authors. Genesis: generative and universal physics engine for robotics and beyond, 2024. 3 [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, 10 Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. 1 [3] Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, and Xi Wang. I-design: Personalized llm interior designer. In European Conference on Computer Vision, pages 217234. Springer, 2024. [4] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion, 2024. 2, 5, 14, 15 [5] Erwin Coumans and Yunfei Bai. Pybullet, python module for physics simulation for games, robotics and machine learning, 2016. 3 [6] Dawson-Haggerty et al. trimesh. 6 [7] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, and Roozbeh Mottaghi. ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. In NeurIPS, 2022. Outstanding Paper Award. 2, 3 [8] Epic Games. Unreal engine. 3 [9] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. 2 [10] Rao Fu, Zehao Wen, Zichen Liu, and Srinath Sridhar. Anyhome: Open-vocabulary generation of structured and textured In European Conference on Computer Vision, 3d homes. pages 5270. Springer, 2024. 2, 3 [11] Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, et al. Threedworld: platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020. [12] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: arXiv preprint On path to artificial super intelligence. arXiv:2507.21046, 2025. 3 [13] Haoran Geng, Feishi Wang, Songlin Wei, Yuyang Li, Bangjun Wang, Boshi An, Charlie Tianyue Cheng, Haozhe Lou, Peihao Li, Yen-Jen Wang, et al. Roboverse: Towards unified platform, dataset and benchmark for scalable and generalizable robot learning. arXiv preprint arXiv:2504.18904, 2025. 3 [14] Zeqi Gu, Yin Cui, Zhaoshuo Li, Fangyin Wei, Yunhao Ge, Jinwei Gu, Ming-Yu Liu, Abe Davis, and Yifan Ding. Artiscene: Language-driven artistic 3d scene generation through image intermediary. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 28912901, 2025. 2, 3 [15] Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 79097920, 2023. 2 [16] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models In Proceedings of the 2023 conference can self-improve. on empirical methods in natural language processing, pages 10511068, 2023. [17] Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, and Yunzhu Li. Phystwin: Physics-informed reconstruction and simulation of deformable objects from videos, 2025. 2 [18] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. 3 [19] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 16 [20] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 16 [21] Steven LaValle. Rapidly-exploring random trees: new tool for path planning. Research Report 9811, 1998. 5, 18 [22] Chengshu Li, Fei Xia, Roberto Martín-Martín, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, et al. igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. arXiv preprint arXiv:2108.03272, 2021. 3 [23] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 8093. PMLR, 2023. [24] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 3 [25] Tsung-Yi Lin, Chen-Hsuan Lin, Yin Cui, Yunhao Ge, Seungjun Nah, Arun Mallya, Zekun Hao, Yifan Ding, Hanzi Mao, Zhaoshuo Li, et al. Genusd: 3d scene generation made easy. In ACM SIGGRAPH 2024 Real-Time Live!, pages 12. 2024. 2, 3 [26] Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu Zeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera, and Zhaoshuo Li. Scenethesis: language and vision agentic framework for 3d scene generation, 2025. 2, 3 [27] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. 3 11 [28] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martín-Martín. What matters in learning from offline human demonstrations for robot manipulation. In arXiv preprint arXiv:2108.03298, 2021. 8, [29] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. 3 [30] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, and Animesh Garg. Orbit - Unified Simulation Framework for Interactive Robot Learning Environments. IEEE Robotics and Automation Letters, 8 (6), 2023. 2, 8, 15, 18 [31] Model Context Protocol (MCP). Model Context Protocol: Open protocol that standardizes how applications provide context to llms. https://modelcontextprotocol. io/introduction, 2025. Accessed: 2025-11-11. 2, 4, 15 [32] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523, 2024. 3 [33] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots, 2024. 8 [34] NVIDIA Corporation. Nvidia isaac sim. https:// github.com/isaac-sim/IsaacSim, 2025. Version 5.0.0. 2, 3, 5, 6, 14, 17, 18 [35] OpenAI. Openai api. https://platform.openai. com/docs/api-reference, 2024. Accessed: 2025-1108. 6 [36] OpenAI. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. 6 [37] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregressive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems, 34:1201312026, 2021. 2, 3 [38] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 84948502, 2018. 3 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 1 [40] Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, and Jia Deng. Infinigen indoors: Photorealistic indoor scenes using procedural generation. In CVPR, 2024. 2, 3 [41] Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia Pérez-DArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: simulation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 75207527. IEEE, 2021. [42] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. 3 [43] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun Wu. Layoutvlm: Differentiable optimization of 3d layout via vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 29469 29478, 2025. 2 [44] Balakumar Sundaralingam, Siva Kumar Sastry Hari, Adam Fishman, Caelan Garrett, Karl Van Wyk, Valts Blukis, Alexander Millane, Helen Oleynikova, Ankur Handa, Fabio Ramos, Nathan Ratliff, and Dieter Fox. curobo: Parallelized collisionfree minimum-jerk robot motion generation, 2023. 2, 5, 18 [45] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in neural information processing systems, 34:251266, 2021. 3 [46] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nießner. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2050720518, 2024. 2, 3 [47] Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, et al. Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. arXiv preprint arXiv:2410.00425, 2024. 3 [48] Qwen Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 6, 7, 8 [49] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 50265033. IEEE, 2012. 3 [50] Unity Technologies. Unity, 2023. Game development platform. 3 [51] Giuseppe Vecchio, Renato Sortino, Simone Palazzo, and Concetto Spampinato. Matfuse: Controllable material generation with diffusion models. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 44294438. IEEE, 2024. 4, 16 [52] Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, and Xingang Wang. Embodiedreamer: Advancing real2sim2real transfer for policy training via embodied world modeling, 2025. [53] Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, 12 [66] Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, and Benjamin Busam. Commonscenes: Generating commonsense 3d indoor scenes with scene graph diffusion. In Advances in Neural Information Processing Systems, 2023. 2, 3 [67] Guangyao Zhai, Evin Pınar Örnek, Dave Zhenyu Chen, Ruotong Liao, Yan Di, Nassir Navab, Federico Tombari, and Benjamin Busam. Echoscene: Indoor scene generation via information echo over scene graph diffusion. In European Conference on Computer Vision (ECCV), 2024. 2, 3 [68] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto MartínMartín, Abhishek Joshi, Kevin Lin, Abhiram Maddukuri, Soroush Nasiriany, and Yifeng Zhu. robosuite: modular simulation framework and benchmark for robot learning, 2025. 8 and Chuang Gan. Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. arXiv preprint arXiv:2311.01455, 2023. [54] Yian Wang, Xiaowen Qiu, Jiageng Liu, Zhehuan Chen, Jiting Cai, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, and Chuang Gan. Architect: Generating vivid and interactive 3d scenes with hierarchical 2d inpainting. Advances in Neural Information Processing Systems, 37:6757567603, 2024. 2 [55] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research. 3 [56] Hongchi Xia, Zhi-Hao Lin, Wei-Chiu Ma, and Shenlong Wang. Video2game: Real-time, interactive, realistic and browser-compatible environment from single video, 2024. 2 [57] Hongchi Xia, Chih-Hao Lin, Hao-Yu Hsu, Quentin Leboutet, Katelyn Gao, Michael Paulitsch, Benjamin Ummenhofer, and Shenlong Wang. Holoscene: Simulation-ready interactive 3d worlds from single video, 2025. [58] Hongchi Xia, Entong Su, Marius Memmel, Arhan Jain, Raymond Yu, Numfor Mbiziwo-Tiapo, Ali Farhadi, Abhishek Gupta, Shenlong Wang, and Wei-Chiu Ma. Drawer: Digital reconstruction and articulation with environment realism, 2025. 2 [59] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: simulated part-based interactive environment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1109711107, 2020. 3, 8, 9 [60] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation, 2025. 4, 5, 14, 15, 16, [61] Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for emIn Proceedings of the IEEE/CVF Conference bodied ai. on Computer Vision and Pattern Recognition, pages 16262 16272, 2024. 2 [62] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, and Christopher Clark. Holodeck: Language guided generation of 3d embodied ai environments, 2024. 2, 3, 6, 7, 10, 17 [63] Yandan Yang, Baoxiong Jia, Shujie Zhang, and Siyuan Huang. Sceneweaver: All-in-one 3d scene synthesis with an extensible and self-reflective agent, 2025. 2, 3, 6, 7, 10 [64] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022. 3 [65] Wentao Yuan, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. M2t2: Multi-task masked transformer for object-centric pick and place, 2023. 2, 5, 18 SAGE: Scalable Agentic 3D Scene Generation for Embodied AI"
        },
        {
            "title": "Appendix",
            "content": "A. Additional Experiment Results In this section, we present additional visualization results and implementation details for both scene generation (Sec. A.1) and robot learning (Sec. A.2), followed by runtime analysis (Sec. A.3). We encourage readers to visit our project webpage for additional visualization videos. A.1. Scene Augmentation In this section, we will show more results related to our proposed scene augmentation method, including Object Category-level Augmentation and Scene Layout-level Augmentation. A.1.1. Object Category-level Augmentation For previously generated single base scene produced by our agentic scene generation framework, we can augment the task relevant objects with different shapes and textures while keeping the same object category, so that robot can learn generalizable policy to unseen scenarios. Here we showcase the capability of our category augmentation method. To illustrate the capacity of our augmentation method, we randomly select part of the objects in the scene for category augmentation. Following the method we described in the main paper method section, given the text description of the selected object from the generation stage, we employ an LLM-based text augmentation to produce variations in geometry and texture (e.g., shape, color, material, or finish) while maintaining the original object category. We then use TRELLIS [60] to synthesize corresponding 3D assets from these augmented descriptions, which are placed into the scene to enrich visual and physical diversity across instances. We show the object category-level augmentation results in different scene categories in Fig. 1. A.1.2. Scene Layout-level Augmentation In addition to object category-level augmentation, to diversify the background environment as well besides the taskrelevant objects, we proposed scene layout-level augmentation. The background scene, including room geometry and all task-irrelevant objects, is regenerated through the agentdriven scene generation, while we keep the task-relevant objects unchanged but repositioned according to the new scene layouts. This process produces diverse scene layouts sharing the same task specification, enabling learning policies that generalize across spatial configurations. We show the scene layout-level augmentation results in different scene categories in Fig. 2. A.2. Robot Action Generation and Policy Inference A.2.1. Robot Action Generation We also showcase our parallel robot action generation process with the scaled training data in our project webpage. The collected robot action data is composed of robot endeffector poses with multiple camera views. For Pick-andPlace task, the camera views input includes 3 perspective cameras located at left, right, and wrist of the Franka Emika Panda robot. For Mobile Manipulation task, the camera views input includes 5 cameras. Three of them are perspective cameras located at left, right, and wrist of the composed mobile manipulator robot. The extra two cameras are fisheye cameras located on top of the robot, looking front and back separately for navigation purposes. We input both RGB and depth from each view with the resolution of 128 128 to the policy network [4]. Parallelized action generation is performed with 8 environments for Pick-and-Place task and 2 environments for Mobile Manipulation task in parallel per GPU in the IsaacSim [34] simulator. A.2.2. Policy Inference For policy inference, we feed the trained policy network with the observed camera views and execute the inferred robot actions. We showcase the policy inference demos in our project webpage, for both the Pick-and-Place task and the Mobile Manipulation task. Failure cases are mostly due to the randomness of the policy inference, far-away objects, or difficult grasp pose location which is hard to reach and grasp by the robot. A.3. Runtime Analysis Scene Generation Generation time for single scene with SAGE varies according to user demands. The most timeconsuming components are object generation and simulationin-the-loop validation for physical stability, while agent reasoning and LLM/VLM inference are faster. Object generation with TRELLIS [60] takes 15 seconds per object. We parallelize this across 8 GPUs, reducing average generation time to 2-3 seconds per object. Simulation with Isaac Sim [34] takes 1-2 seconds per placement candidate for stability validation. To minimize simulation overhead, we simulate once after placing all floor and wall objects, then remove any unstable objects. This significantly improves efficiency by consolidating multiple simulations into one. For on-top objects, however, we simulate each placement individually since these objects are typically smaller and more prone to instability. We adopt an early-stopping strategy that accepts the first stable, collision-free placement validated by Isaac Sim [34]. This greatly reduces computation time, as we 14 Base Scene Aug. Scene Aug. Scene 2 Aug. Scene 3 Figure 1. Object Category-Level Augmentation. Here we showcase the capability of our category augmentation method. We randomly select part of the objects in the scene for category augmentation. Given the text description of the selected object from the generation stage, we employ an LLM-based text augmentation to produce variations in geometry and texture (e.g., shape, color, material, or finish) while maintaining the original object category. We then use TRELLIS [60] to synthesize corresponding 3D assets from these augmented descriptions, which are placed into the scene to enrich visual and physical diversity across instances. typically find stable placements within few trials despite having 30-50 candidate locations. Overall, while generation time spans broad range, scene with 20 objects takes approximately 10 minutes, with time scaling linearly for additional objects. Action Generation Using Isaac Lab [30] as our simulation platform, we leverage its parallelism to significantly reduce data collection time. For Pick-and-Place tasks, motion planning for each demonstration takes 8-10 seconds per environment when run sequentially. By parallelizing across 8 environments, the total planning time increases to 15-20 seconds, but this yields an average of 2-3 seconds per demonstration. For Mobile Manipulation tasks, which involve longer trajectories, the parallelized per-demonstration time is 8-10 seconds. Our simulations scale readily across GPU clusters, enabling proportional speedup in data generation based on available GPU resources. This scalability facilitates efficient collection of large-scale datasets for training generalizable policies. Policy Training We train diffusion policy [4] using the Robomimic [28] framework, which takes several hours to converge on our robot action data. Note that diffusion policy is one approach to convert our generated actions into an executable policy, chosen here for its simplicity. Other methods, such as fine-tuning VLA, may prove more efficient and we leave for future work. B. Additional Implementation Details In this section, we will elaborate the implementation details of our proposed agent-driven scene generation and robot action generation as well as policy learning. B.1. Scene Generation B.1.1. Overview Our scene generation framework uses carefully designed agent-driven system. The SAGE operates under the Model Context Protocol (MCP) [31], standardized protocol for seamless interaction with external tools [31]. For the agent to understand each tool, we provide descriptions that include Base Scene Aug. Scene 1 Aug. Scene 2 Aug. Scene 3 Figure 2. Scene Layout-Level Augmentation. Here we show more results of Scene Layout-Level augmentation, where the background scene, including room geometry and all task-irrelevant objects, is regenerated through the agent-driven scene generation. This process produces diverse scene layouts sharing the same task specification, enabling learning policies that generalize across spatial configurations. Bedroom: We keep the objects of desk, nightstand, and mug on the nightstand as the same. Livingroom: We keep the objects of sideboard, coffeetable, and vase on the coffeetable as the same. Office: We keep the objects of sofa, desk, and pen on the desk as the same. Meeting room: We keep the objects of table, cabinet, and cup on the meeting table as the same. the tools function, input argument types (specified as Python strings), and output format. We formulate all tool outputs as JSON dictionary strings, which the agent can easily parse and interpret. At each iteration, the agent either specifies the next tool and its input arguments, or indicates that scene generation is complete. The server executes the requested operation and returns the result, which the agent uses as feedback to determine the next action. This setup enables adaptive, tool-driven scene generation without hard-coded logic. For the sake of spaces in supplementary, we will include all the prompts we used in our code release. B.1.2. Generators The scene is constructed using set of generator tools that the agent dynamically invokes via MCP. Each generator is an MCP tool the agent can call at each iteration. Below we describe the implementation of each generator. Scene Initializer The scene initializer generates the floor plan and materials for the floor and walls. Floor plan generation proceeds in two steps. First, we prompt the LLM with the scene description, which returns the room types and sizes. For multi-room layouts, second step generates connectivity between rooms and places connecting doors. We generate materials from text descriptions of the floor and walls provided by the LLM using Matfuse [51]. In the supplementary material, we also use Flux.1-dev [19, 20] to generate more detailed textures. We then assemble the floor plan with the generated materials to create the scene layout. Asset Placer The asset placer takes text string describing placement requirements for objects, generates them with TRELLIS [60], and places them into the 3D scene. After the text-to-3D inference with TRELLIS [60], we perform series of mesh post-processing, including decimation, nonmanifold correction, and hole filling, to ensure the watertight property of each generated mesh. After asset generation, we use VLM to estimate physical properties, including height, mass, and metallic/roughness values. For each object, an LLM analyzes the input conditions and categorizes the placement as floor, wall, or on-top. The placement logic varies by category. For floor objects, we first use the LLM to generate constraints, including global position in the room and relative position/orientation to existing objects. These constraints guide our placement scoring. We sample available positions using grid-based approach, then apply depthfirst search to find optimal positions and orientations while checking for collisions. For wall objects, we similarly use depth-first search with grid-based sampling and collision checking against both floor and wall objects. For on-top placement, unlike [62] which supports only single-layer relationships, our method enables multi-layer scene graphs. We sample available locations by computing surface normals on the supporting objects mesh, selecting faces with normals close to the rooms up axis. This allows placement on shelves and surfaces, not just object tops as in [62]. After collision checking, we obtain multiple candidate locations. Finally, we validate stability using Isaac Sim [34]. In the simulation, all the wall objects are set as static since they are attached on the wall. We simulate each candidate placementif unstable (e.g., pillow standing on bed), we record the post-simulation pose and re-simulate with this adjusted pose. If the second simulation is stable, we accept the placement; otherwise, we reject it. If all candidates fail, the physics critic will report the failure to the agent and suggest alternatives such as smaller objects or different support surfaces. Note that unlike [62], which supports only singleiteration placement, our agentic framework enables iterative, adaptive placements across multiple tool calls to fully realize the scene and satisfy user requirements. Asset Mover The Asset Mover locates and moves objects specified in the input text string. It first uses the integrated LLM to parse the target object and its destination. The object is temporarily removed from the current room, then repositioned using the same placement logic applied to other objects. If placement fails due to insufficient space, the object is restored to its original location and the failure is reported to the agent. The agent can then choose alternative actions, such as moving the object elsewhere or removing other objects to free up space. Movement instructions typically come from the visual critic. Asset Remover The Asset Remover deletes objects described in the input text, using LLM reasoning to locate them in the scene. It is typically called when the critic provides feedback to remove an object. B.1.3. Critics Visual Critic The visual critic evaluates the current scene configuration and suggests new objects to place or adjustments to existing placements. When proposing new objects, it considers several factors. First, it identifies natural object combinationsfor example, placing chairs around tables or books on bookshelvesusing example combinations we provide to guide its reasoning. Second, when the room appears sparse, it suggests background elements like floor plants or decorative objects to fill empty spaces. Third, it verifies whether task-relevant objects specified by the user have been placed, notifying the agent if any are missing. For adjustments to existing objects, the critic analyzes rendered images to determine which items should be moved or removed. This feedback guides the agent in selecting the most appropriate generator to invoke next. Physics Critic The physics critic operates at every stage of each generator, explicitly validating scene stability after object operationsaddition, movement, and removal. During physical simulation, wall-mounted objects are treated as static since they attach to walls, while all other objects remain dynamic and respond to collisions and gravity. When operations fail due to physics instability, the physics critic detects these failures and returns them to the agent as feedback. B.2. Scene Augmentation We apply two types of augmentation to our agent-generated scenes: object category-level and scene layout-level. Implementation details are provided below. Object Category-Level Augmentation We adopt TRELLIS [60] to generate augmented objects given the augmented text descriptions of objects. The generated objects are then placed into the scene with the same supporting relationships as their originals, and physics validation is performed using Isaac Sim [34]. Wall and floor textures are also able to be augmented during this process. Scene Layout-Level Augmentation In layout-level augmentation, the background sceneincluding room geometry and all task-irrelevant objectsis regenerated through agentdriven scene generation, while previously generated taskrelevant objects are preserved and reused. To achieve this, we add stage in the Asset Placer generator that excludes pre-generated objects from the generation list using LLMbased reasoning, preventing them from being regenerated. This ensures that task-relevant objects are preserved while all task-irrelevant objects and the room layout are regenerated, producing diverse spatial configurations that enable policies to generalize better. Note that scenes from layout-level augmentation can undergo further augmentation. For example, we can combine them with another round of category-level augmentation to create even greater diversity. B.3. Robot Action Generation Pick-and-Place We use M2T2 [65] to generate grasp pose candidates from rendered depth images. These are then transformed using the camera pose to obtain the actual 3D grasp pose in world coordinates for motion planning with inverse kinematics (IK). Curobo [44] is integrated into the motion planning and IK pipeline by incorporating mesh geometries into its collision checking, ensuring feasible and stable grasp execution. Once we have the grasp pose, we divide the grasp into multiple steps. First, we use IK to calculate the end-effector trajectory from the start pose to position directly above the grasp pose. Next, the gripper lowers and closes to grasp the object. Finally, the gripper lifts to raise the object. For placement, we use IK with Curobo collision avoidance. We simplify placement to drop, which could be improved with additional motion planning steps to gently place the object down. Mobile Manipulation This task is composed of few subtasks including object grasping, navigation, and placement. Object grasping follows the same motion planning as Pick-and-Place. For navigation, we use RRT [21] to plan collision-free trajectories. The algorithm explores possible paths from both start and target positions, ending when the paths meet. Collision checking is implemented using 2D occupancy grid, which is faster and saves memory compared to 3D explicit mesh collision checking. Parallelism We leverage parallelism features in Isaac Sim and Isaac Lab [30, 34] to simulate multiple environments in parallel per GPU (8 for Pick-and-Place and 2 for Mobile Manipulation). This scales easily to multiple GPUs in cluster. We use NVIDIA L40S GPUs in our experiments."
        }
    ],
    "affiliations": [
        "Cornell University",
        "NVIDIA",
        "Stanford University",
        "University of Illinois Urbana-Champaign"
    ]
}