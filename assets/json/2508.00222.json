{
    "paper_title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization",
    "authors": [
        "Yihong Dong",
        "Xue Jiang",
        "Yongding Tao",
        "Huanyu Liu",
        "Kechi Zhang",
        "Lili Mou",
        "Rongyu Cao",
        "Yingwei Ma",
        "Jue Chen",
        "Binhua Li",
        "Zhi Jin",
        "Fei Huang",
        "Yongbin Li",
        "Ge Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem."
        },
        {
            "title": "Start",
            "content": "Preprint, July 2025 RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization Yihong Dong1,2, Xue Jiang1,2, Yongding Tao1, Huanyu Liu1, Kechi Zhang1, Lili Mou3,4, Rongyu Cao2, Yingwei Ma2, Jue Chen2, Binhua Li2, Zhi Jin1, Fei Huang2, Yongbin Li2, Ge Li1 1 School of Computer Science, Peking University 3 Department of Computing Science, University of Alberta dongyh@stu.pku.edu.cn 2 Tongyi Lab, Alibaba Group 4 Canada CIFAR AI Chair lige@pku.edu.cn 5 2 0 2 6 ] . [ 3 2 2 2 0 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLMs immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLMs problem-solving scope. To address this problem, we propose RL-PLUS, novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2%. Moreover, the analysis of Pass@k curves indicates that RLPLUS effectively resolves the capability boundary collapse problem."
        },
        {
            "title": "Introduction",
            "content": "The paradigm of Reinforcement Learning with Verifiable Reward (RLVR) has significantly propelled the improvement of reasoning performance in Large Language Models (LLMs), particularly in solving complex tasks involving math and coding (OpenAI, 2024; Guo et al., 2025; KimiTeam, 2025). RLVR optimizes LLMs performance via reinforcement learning (RL) process guided by verifiable reward computation, e.g., determining whether an output matches ground-truth math answer or passes unit tests for coding. This method enables LLMs to scale their computation at test time by extending Chain-of-Thought (CoT) processes and spontaneously exhibit sophisticated cognitive behaviors such as reflection and exploration. Thus, RLVR is believed to be promising way for LLMs to achieve continuous self-evolution toward more powerful AI (Guo et al., 2025). Despite the empirical successes, some work (Havrilla et al., 2024; Shao et al., 2024; Yue et al., 2025a) points out that current RLVR cannot enable LLMs to acquire novel reasoning abilities, but rather simply utilize reasoning patterns already in the base model. As shown in Figure 1(a), although the pass@1 performance of RLVR-trained models surpasses that of the base model, its pass@1282 is substantially lower. This trend suggests that the underlying capability distribution of the base 0Work done during Yihong Dong and Xue Jiangs internship at Tongyi Lab. 1Our code is available at https://github.com/YihongDong/RL-PLUS. 2The pass@k calculates the proportion of problems the model can potentially solve within finite (k) number of attempts metric is commonly used to gauge models capability boundary. 1 Preprint, July (a) Collapse problem of capability boundaries in LLMs after RLVR training. (b) Benefits of our RL-PLUS. Figure 1: (a) The commonly used RLVR methods can lead to the collapse problem of capability boundaries in base LLMs. (b) RL-PLUS can overcome capability boundary collapse of LLMs in RLVR, consistently showing larger pass@k than base model. model is broader and that existing RLVR can collapse the base models capability boundary, thus fundamentally limiting the acquisition of new reasoning pathways. This limitation stems from an essential challenge when applying RLVR to LLMs: the potential solution space of LLMs is extremely immense with sparse reward that current RLVR techniques cannot effectively guide the model to explore new and unknown pathways, i.e., outward exploration. The challenge is particularly acute in long reasoning tasks where rewards are contingent upon the successful completion of an entire inferential chain. single erroneous step can nullify the reward for the entire trajectory, thus failing to provide positive signal for acquiring new knowledge. Consequently, the model is compelled to focus on inward exploitation, meaning that it refines and optimizes the knowledge and reasoning methods it already possesses, which results in contraction of the models exploratory range and shrinking of its capabilities. This phenomenon not only prevents the model from acquiring new information or abilities that surpass its base model, but also significantly impedes any sustained enhancement of its overall performance. The ancient educational principle that If one learns from others but does not think, one will be bewildered. If, on the other hand, one thinks but does not learn from others, one will be in peril3 offers crucial lens through which to view the limitations of current methodologies for enhancing LLM reasoning. Current RLVR can be viewed as the latter case, which excels at thinking through inward exploitation but demonstrates inadequate outward exploration due to its inherently on-policy strategy coupled with LLMs immense action space and sparse reward, i.e., hard to continuous learning of new knowledge. Conversely, approaches like Supervised Fine-Tuning (SFT) represent the former case, focusing on imitating solutions but failing to internalize the underlying reasoning principles, leading to brittleness when encountering novel problems. This motivates us to develop novel RLVR approaches with effective external learning, but there are two key challenges that need to be addressed. First, distributional mismatch between the models policy and the external data source is inevitable. Standard importance sampling corrections for RL are inadequate, i.e., employing the proxy with on-policy introduces systematic bias, whereas direct using off-policy usually suffers from high variance and bias due to their significantly divergent distributions. Second, there is challenge of efficiently extracting valuable information from this external data. Models are naturally inclined to favor high-probability tokens, thus reinforcing existing knowledge. However, the key to discovering novel reasoning often lies in exploring low-probability tokens that the model would otherwise ignore. In this paper, we propose RL-PLUS, novel hybrid-policy optimization approach designed to synergize internal exploitation with external data during RL process. Specifically, RL-PLUS has two core techniques. ❶ To resolve the issue of distributional mismatch, we employ Multiple Importance Sampling, which provides lower bias and variance estimation of importance by combining information from multiple policies. ❷ To promote the discovery of new knowledge, we introduce an Exploration-Based Advantage Function, which reshapes the learning objective by prioritizing 3A principle from the philosopher and educator Confucius. 2 Preprint, July 2025 advantages for reasoning paths that are correct but are hard to explore (i.e., low probability) under the current policy. We also provide theoretical analysis demonstrating that our approach achieves lower bias and variance compared with mainstream RLVR methods when leveraging external data. Extensive experiments show the effectiveness and generalization of RL-PLUS. On six challenging math reasoning benchmarks, RL-PLUS achieves state-of-the-art (SOTA) performance, outperforming existing RLVR methods and improving upon SFT+GRPO by 5.2 average points. RL-PLUS also demonstrates superior generalization to six out-of-distribution (OOD) tasks. RL-PLUS exhibits clear and stable improvements across diverse model families, with the average relative improvements of GRPO up to 69.2%. Moreover, the analysis of Pass@k curves across multiple benchmarks indicates that RL-PLUS effectively transcends the inherent capability ceiling of the base model, thus addressing capability boundary collapse observed in prior RLVR approaches."
        },
        {
            "title": "2 Background and Related Work",
            "content": "In this section, we first establish the theoretical preliminaries necessary to understand our work, and then provide critical review of the most related work, identifying key limitations in existing methods and thereby motivating the design of our proposed RL-PLUS. 2.1 Preliminary Knowledge LLM-based Reasoning as Markov Decision Process. We frame the task of generating reasoning sequence (e.g., solution to math problem) as Markov Decision Process (MDP) (Puterman, 2014). At each timestep t, the state st consists of the initial prompt concatenated with the sequence of previously generated tokens, y<t. The action at is the selection of the next token yt from the vocabulary. The model, or policy πθ, maps state to distribution over actions. reward R(q, y) is provided only upon completion of the entire sequence y. In the context of RLVR, this reward is typically sparse and binary. For example, score is 1 if the final answer is correct and 0 otherwise. The objective is to learn policy πθ that maximizes the expected cumulative reward J(θ) = Eyπθ [R(q, y)]. Policy Gradient Optimization. Policy gradient methods are the standard for optimizing LLMs in on-policy RLVR settings. Group Relative Policy Optimization (GRPO) (Shao et al., 2024) shows exceptional performance in various tasks, especially to enable effective scaling within the RLVR paradigm. Compared to Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017), GRPO leverages group-normalized rewards to estimate advantages, eliminating the need for value model and thereby improving computational efficiency. The standard GRPO objective is: JRL(θ) = E(q,y)Don (cid:88) min (ri,t(θ)Ai, clip(ri,t(θ), 1 ϵ, 1 + ϵ)Ai) βDKL[πθπref] (1) t=1 ri,t(θ) = πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) , Ai = Ri mean({R1, R2, . . . , RG}) std({R1, R2, . . . , RG}) , (2) (3) where ri,t(θ) is the importance sampling ratio and Ai is the estimated advantage for an on-policy trajectory yi. Recent work, such as Simple-rl (Zeng et al., 2025) and DAPO (Yu et al., 2025), has proposed either setting the KL coefficient β to very small value or omitting the KL term in Equation 1 entirely. The rationale is that during the training of model for long CoT reasoning, the models distribution is expected to diverge significantly from the initial policy, rendering this constraint unnecessary. Evaluating Reasoning Boundaries with pass@k. To accurately assess models true problemsolving capabilities, we utilize the pass@k metric (Chen et al., 2021). It measures the probability of obtaining at least one correct answer within independent samples for given problem. Unlike mean accuracy (i.e., pass@1), pass@k provides more comprehensive view of the models Preprint, July 2025 reasoning potential and is critical for evaluating whether method expands the set of solvable problems (Yue et al., 2025a). This on-policy RLVR paradigm, while powerful, leads to two fundamental challenges when the goal is to surpass base models intrinsic capabilities: 1) an inability to effectively integrate novel, external knowledge due to the high variance and bias associated with off-policy data, and 2) the tendency for on-policy exploration to collapse into known, high-probability reasoning paths, thereby shrinking the models reasoning boundary. These challenges directly motivate our approach."
        },
        {
            "title": "2.2 Related Work",
            "content": "We position RL-PLUS by critically examining two primary lines of research: on-policy RLVR for reasoning and hybrid SFT-RL methods. On-Policy RLVR and Its Intrinsic Limitations Reinforcement learning has become cornerstone for enhancing LLM reasoning (Yue et al., 2025b; Liu et al., 2025c; Wang et al., 2025). Seminal works have shown that RLVR can significantly improve performance on complex reasoning tasks by rewarding correct final answers (Guo et al., 2025; Zeng et al., 2025; Hu et al., 2025). Subsequent research has refined this paradigm; for instance, PRIME-Zero (Cui et al., 2025a) uses implicit process rewards, and Oat-Zero (Liu et al., 2025b) simplifies the advantage calculation in GRPO. However, growing body of evidence reveals critical flaw in these on-policy methods: they primarily optimize existing knowledge rather than discovering new reasoning capabilities. This leads to two well-documented issues. First is the Capability Boundary Collapse problem. While RLVR models often show superior pass@1 performance, their advantage diminishes as increases in pass@k evaluations, with base models eventually surpassing them (Yue et al., 2025a). This strongly suggests that RLVR refines the probability of known correct paths but fails to expand the overall set of solvable problems. Second, these methods suffer from Entropy Collapse, where policy entropy sharply decreases during training, making the model overly deterministic and hindering further exploration (Cui et al., 2025b). This indicates that on-policy RLVR, by its nature, is prone to inward exploitation that reinforces existing biases and limits the models potential. Hybrid SFT-RL Methods To overcome knowledge limitations of pure RL, researchers explored hybrid methods that combine RL with SFT on external demonstration data (Cai et al., 2025). Early approaches employed sequential, multi-stage training (SFT then RL), as seen in models like InstructGPT (Ouyang et al., 2022). While conceptually simple, this often leads to catastrophic forgetting of the SFT-learned knowledge and suffers from computational inefficiency. More recent work has focused on unified or interleaved training frameworks. For example, ReLIFT (Ma et al., 2025) alternates between RL and online fine-tuning on difficult problems, while LUFFY (Yan et al., 2025) selectively imitates high-quality external trajectories using mixed policy. In another example, TAPO (Wu et al., 2025) enhances RL by integrating external, high-level guidance in the form of thought patterns abstracted from prior data. Other methods, such as SASR (Chen et al., 2025) and SuperRL (Liu et al., 2025a), employ adaptive switches to dynamically balance SFT and RL objectives based on training state. While these methods are more sophisticated, they often rely on complex, potentially unstable heuristics for balancing the two learning signals. Moreover, simply adding an SFT loss to the RL objective, as explored in GRPO w/ SFT Loss, can degrade performance, highlighting the difficulty of effective integration. Even advanced frameworks like UFT (Wang et al., 2024b), which aim to unify SFT and RL to accelerate convergence, do not explicitly address how to stabilize off-policy updates while simultaneously directing exploration towards novel solutions. Motivation The foregoing analysis reveals persistent gaps in the related work. On-policy RLVR methods are constrained by the base models inherent knowledge, while existing hybrid SFT-RL methods lack principled mechanism to both stabilize learning from external, off-policy data and explicitly incentivize exploration of low-probability but correct reasoning pathways. RL-PLUS is designed to directly address these deficiencies. 4 Preprint, July"
        },
        {
            "title": "3 RL-PLUS",
            "content": "RL-PLUS overcomes the LLMs capability boundaries collapse problem in RLVR by integrating externally-guided exploration with the exploitation of internal reasoning pathways."
        },
        {
            "title": "3.1 Mitigating Distributional Mismatch with Multiple Importance Sampling",
            "content": "A central challenge in learning from static dataset De = {ei}N i=1 is the distributional shift between the target policy πθ and the unknown behavior policy πω. Standard importance sampling (IS) presents dilemma for correcting this mismatch. On-policy IS surrogates, which use proxy like πθold in the denominator, are systematically biased when applied to external data from πω (Lemma A.5). Conversely, the theoretically correct off-policy estimator, using weights (θ) = πθ(ete<t) re πω(ete<t) , suffers from support mismatch of πθ (Lemma A.6) and prohibitively high variance as the policies diverge (Lemma A.7), which destabilizes training. This issue is compounded by the fact that πω is usually unknown, rendering direct weight computation infeasible. To solve this, we introduce Multiple Importance Sampling to construct an estimator with lower Instead of directly estimating πω, we treat the generation of an variance and controllable bias. external sample as arising from mixture policy composed of the previous policy πθold and the external policy πω. Therefore, the Multiple Importance Sampling of each token can be defined as: rm i,t(θ) = 2πθ(ei,tq, ei,<t) πω(ei,tq, ei,<t) + πθold(ei,tq, ei,<t) , (4) where ei,t is the t-th token in the external data trajectory ei. It replaces the aforementioned explosive bias from poor proxy or support mismatch with controlled, bounded distortion error (Remarks A.8 and A.9), making the overall MIS estimator robust for stable learning from external data. The formal denominator acts as crucial variance guardrail. The presence of πθold, which is intentionally kept close to πθ, prevents the ratio from exploding even if πω is highly dissimilar, ensuring the estimators variance remains bounded. Theorem 3.1 (Variance Robustness of MIS). So long as there is at least one policy in the behavior pool {πβk } (e.g., πβ πθ), the variance of the MIS estimator will be low. The estimator is insensitive to other arbitrarily bad behavior policies in the pool. (See Proof in Appendix A.4) ) that is good approximation of the target policy πθ (i.e., πβ key challenge remains: the behavior policy πω is unknown. We require robust method to estimate it. Instead of naively using proxy, we derive an estimator for πω from principled Bayesian perspective. We frame the estimation as decision problem where we must balance our belief in our best available model, πθold , against state of maximal uncertainty, represented by non-informative uniform policy U. This allows us to hedge against model error, leading to the following Bayesoptimal estimator. Theorem 3.2 (Bayes-Optimal Policy Estimator). Let the model space for the unknown behavior policy πω be composed of two candidate models: (1) The specific proxy policy, πθold, represent- (2) non-informative uniform policy, U(τ ), representing our available, specific information. ing maximal uncertainty. Let the trajectory space have finite volume = (cid:82) dτ , such that U(τ ) = 1/V . Under the Principle of Indifference, we assign equal prior probabilities to these models, i.e., (πω = πθold) = (πω = U) = 1/2. Then, the estimator ˆπω that minimizes the Bayes risk (expected L2 error) is the Bayesian model average: ˆπ 2 U(τ ) (See Proof in Appendix A.5) 2 πθold(τ ) + ω(τ ) = 1 3.2 Efficient Exploration with Exploration-Based Advantage Function Merely incorporating external data stably is insufficient; we must also guide the model to focus on its most valuable information, especially the new knowledge that the model is unlikely to discover on its own. Models tend to favor high-probability tokens, whereas novel knowledge is often embedded in correct reasoning paths that the model considers to have low probability. To this end, we design an Exploration-Based Advantage Function, Ac i,t, which prioritizes encouraging the model to explore reasoning steps that are correct but hard to explore, which can be defined as: 5 Preprint, July 2025 Ac i,t = Ri mean({R1, R2, . . . , RG}) std({R1, R2, . . . , RG}) Ci,t (5) The first term is the standardized reward for all trajectories, including both internal exploration and external data, and the second term is the weight to encourage exploration. Inspired by focal loss (Lin et al., 2017), we define the weight Ci,t as: Ci,t = (1 detach(πθ(ei,tq, ei,<t)))γ, (6) where πθ(ei,tq, ei,<t) represents the models exploration probability in the correct token ei,t from the external data. When it is hard to explore (i.e., πθ is small), the weight Ci,t becomes large, amplifying the advantage signal for that timestep and compelling the model to attend to this overlooked region. γ is hyperparameter to control Ci,t. The detach function is standard operation in Torch that prevents gradients from backpropagating through the probability calculation, which enhances training stability. 3.3 The Composite RL-PLUS Objective To synergize internal exploitation Do with external data De, we formulate the final training objective of RL-PLUS as composite function JRL-PLUS(θ), which is defined as: JRL-PLUS(θ) = E(oi,Ai)Do [ri,t(θ)Ai] (cid:124) (cid:125) (cid:123)(cid:122) Internal Exploitation (Thinking) (cid:2)rm + E(ei,Ac i,t)De (cid:124) (cid:125) (cid:123)(cid:122) External data for Exploration (Learning) i,t(θ)Ac i,t (cid:3) (7) where the first term represents the standard policy gradient objective, which is responsible for stabilizing and improving upon the models existing reasoning capabilities. The second term constitutes the core of our contribution, which drives the policy to external exploration. It leverages our two primary innovations: 1) Multiple Importance Sampling rm i,t(θ), which provides low-variance, robust mechanism for integrating external data, and 2) Exploration-Based Advantage Function Ac i,t, which re-weights the learning signal to prioritize novel yet high-value reasoning paths. Moreover, we omit the clipping mechanism (e.g., clip(rt(θ), 1ϵ, 1+ϵ)), which would suppress the gradient signals corresponding to highly informative, low-probability events, i.e., the new knowledge we aim to acquire. By removing this constraint, RL-PLUS is empowered to take larger, more assertive optimization steps when it encounters valuable information in the external data, thus accelerating the assimilation of novel knowledge and more effectively expanding its capability boundaries in RLVR."
        },
        {
            "title": "4 Experimental Results",
            "content": "In this section, we conduct extensive experiments to demonstrate the effectiveness and generalization of RL-PLUS. Detailed experimental setups can be found in Appendix. Performence of RL-PLUS. As shown in Table 1, RL-PLUS comprehensively outperforms existing RLVR methods across all evaluated applications, achieving SOTA performance. comparison with several straightforward baselines clearly demonstrates the benefits of RL-PLUS. SFT can be viewed as means of learning from external knowledge, while GRPO enables the model to explore solutions on its own through reinforcement learning. The combined SFT+GRPO approach yields synergistic gains, illustrating the value of integrating both external knowledge and self-exploration. However, the GRPO w/ SFT Loss baseline, which simply adds an SFT loss to the RL training, shows decline in performance. This suggests that effectively merging these two learning paradigms is non-trivial challenge. RL-PLUS significantly improves upon SFT+GRPO by an average of +5.2 points, showcasing more potent strategy for this integration. Furthermore, when compared to concurrent methods like LUFFY and ReLIFT, which also incorporate external examples into their training process in some form, RL-PLUS also achieves superior performance, which indicates that RL-PLUS offers more effective way for learning from external knowledge. 6 Preprint, July 2025 Table 1: Performance of RL-PLUS against other baselines, where the best-performing result for each benchmark is highlighted in bold and the base model is Qwen2.5-Math-7B for all methods. Method AIME 24 AIME 25 AMC MATH-500 Minerva Olympiad Avg. Qwen2.5-Math-7B Qwen2.5-Math-7B-Instruct SimpleRL (Zeng et al., 2025) OpenReasoner (Hu et al., 2025) PRIME (Cui et al., 2025a) Oat (Liu et al., 2025b) DAPO (Yu et al., 2025) TAPO (Wu et al., 2025) LUFFY (Yan et al., 2025) ReLIFT (Ma et al., 2025) SFT GRPO (Shao et al., 2024) GRPO w/ SFT Loss SFT+GRPO RL-PLUS 11.5 12.5 27.0 16.5 17.0 33.4 23.4 33.3 29.4 28. 22.2 25.1 19.5 25.8 33.4 4.9 10.2 6.8 15.0 12.8 11.9 15.5 18.6 23.1 21.8 22.3 15.3 16.4 23.1 25.9 31.3 48.5 54.9 52.1 54.0 61.2 66.3 77.5 65.6 64. 52.8 62.0 49.7 62.7 68.1 43.6 80.4 76.0 82.4 81.4 78.0 86.0 83.4 87.6 86.8 82.6 84.4 80.4 87.2 90.2 7.4 32.7 25.0 33.1 39.0 34.6 40.1 38.2 37.5 40. 40.8 39.3 34.9 39.7 43.8 15.6 41.0 34.7 47.1 40.3 43.4 49.6 46.2 57.2 54.8 43.7 46.8 39.4 50.4 58.8 19.0 37.6 37.4 41.0 40.7 43.7 46.8 49.5 50.1 49. 44.1 45.5 40.1 48.2 53.4 Table 2: Out-of-Distribution performance on programming tasks (i.e., HumanEval, LiveCodeBench, Codeforces) and science QA (i.e., ARC-c, GPQA-diamond, MMLU-Pro). Method HumanEval LeetCode LiveCodeBench ARC-c GPQA-diamond MMLU-Pro Avg. Base Model SFT GRPO SFT+GRPO RL-PLUS 42.1 55.5 63.4 59.8 68. 22.8 8.3 21.1 8.34 27.8 14.9 8.1 15.3 9.7 19.2 18.2 75.2 81.7 72.4 82.3 13.1 24.7 40.4 24.2 40.4 30.2 42.7 47.5 37.7 54.7 23.6 35.8 44.9 35.4 48. Performance on OOD Tasks. The results on OOD tasks are presented in Table 2, which show that RL-PLUS achieves substantial improvements over all baselines, including the mainstream method SFT+GRPO. It surpasses the next best baseline by an average of +3.9 points. This indicates that RL-PLUS not only enhances capabilities within specific domain but also develops more fundamental reasoning abilities that generalize to other domains. In the domain of science QA, RL-PLUS consistently outperforms both GRPO and SFT+GRPO across all benchmarks. More notably, under significant domain shift to programming tasks, our approach maintains its strong performance and advantage. In contrast, the performance of SFT and SFT+GRPO deteriorates significantly in this area. Considering this alongside the in-domain results from Table 1, clear pattern emerges: while SFT-based methods provide strong boost for in-domain tasks, they fail to generalize and perform worse than RL-based methods in OOD scenarios. RL-PLUS resolves this trade-off. By effectively merging the external knowledge acquisition of SFT with the robust generalization of RL, it achieves superior performance in both in-domain and out-of-distribution settings, outclassing methods reliant on either paradigm alone. Training Dynamics. In Figure 2, we present the training dynamics of our proposed method and baselines on various benchmarks. As illustrated, RL-PLUS consistently outperforms the alternatives in terms of test accuracy and rewards throughout the training process. Notably, RL-PLUS continues to show clear upward trend in performance even after the baselines have plateaued. We further analyze the changes in actor entropy during training. We observe that directly incorporating external data during rollouts (the green line in Figure 2) leads to an entropy explosion, causing the models outputs to become chaotic. In contrast, the entropy of the baseline models collapses to nearly zero over the course of training, indicating loss of exploratory capability. The entropy of RL-PLUS, however, does not diminish to zero, which suggests that our trained model retains considerable capacity for exploration. Prior research (Cui et al., 2025b) has established that policy performance is achieved at the cost of policy entropy, and the depletion of entropy marks the upper limit of performance. This implies that RL-PLUS still possesses potential for further improvement. Additionally, the response length can reflect the test-time scaling performance of method. The 7 Preprint, July 2025 Figure 2: Training dynamics of RL-PLUS and other baselines. steadily increasing response length of RL-PLUS is the indicator of healthy and robust training state. In contrast, while directly incorporating external data also leads to long response lengths, its low accuracy and high policy entropy suggest that this length stems from unproductive exploration rather than meaningful reasoning. Table 3: The performance of RL-PLUS based on Different LLMs. Model AIME 24 AIME 25 AMC MATH-500 Minerva Olympiad Avg. LLaMA-3.1-8B SFT GRPO RL-PLUS Deepseek-Math-7B SFT GRPO RL-PLUS Qwen2.5-Math-1.5B SFT GRPO RL-PLUS Qwen2.5-Math-7B SFT GRPO RL-PLUS 4.7 2.6 3.5 11.7 1.1 3.8 2.5 4.1 7.2 11.7 11.8 20.4 11.5 22.2 25.1 33.4 0.4 0.9 0.5 2.1 0.3 0.3 0.2 0.4 3.6 13.2 7.7 13.6 4.9 22.3 15.3 25. 18.5 29.8 19.5 35.5 14.5 23.3 17.3 25.0 26.4 37.8 40.2 50.0 31.3 52.8 62.0 68.1 46.4 50.0 45.0 64.4 40.4 51.2 47.0 54.8 28.0 70.6 61.8 80.4 43.6 82.6 84.4 90.2 19.8 21.3 20.2 29.4 18.8 21.3 20.9 21.7 9.6 26.8 26.8 33.1 7.4 40.8 39.3 43.8 13.2 16.9 14.2 31.2 10.7 19.8 14.5 21.4 21.2 31.3 32.0 45.2 15.6 43.7 46.8 58.8 17.2 20.2 17.2 29.1 14.3 19.9 17.1 21.3 16.0 31.9 30.1 40.5 19.0 44.1 45.5 53.4 Application on Various LLMs. To validate the applicability of RL-PLUS on various LLMs, we conduct experiments on several mainstream open-source LLMs, including LLaMA-3.1-8B, Deepseek-Math-7B, and the 1.5B and 7B versions of Qwen2.5-Math. The detailed results are presented in Table 3. The results indicate that RL-PLUS achieves comprehensively superior performance, regardless of the base model. Notably, on Qwen2.5-Math-7B model, RL-PLUS elevates the average score to 53.4, significantly outperforming the base model of 9.0 and other methods such as SFT of 44.1 and GRPO of 45.5. Furthermore, on LLaMA-3.1-8B, where methods like GRPO struggled to yield improvements, RL-PLUS successfully trained the model to achieve an absolute gain of 11.9 points. These findings provide evidence that RL-PLUS can consistently enhance LLMs of varying architectures and scales, significantly boosting their reasoning capabilities. Acquiring Reasoning Abilities Beyond Base Model. The fundamental goal of incorporating an external policy into the RL-PLUS method is to expand the models capability boundary by continuously introducing knowledge. Following the experimental setup of (Yue et al., 2025a), we test whether RL-PLUS acquires superior reasoning abilities relative to the base model. Figure 3 displays the pass@k performance curves for different methods across multiple tasks. clear trend is observable where the performance curve of the GRPO method gradually converges with that of the base model as increases. In some instances, GRPOs performance even drops below the base model at larger k-values, finding consistent with that of (Yue et al., 2025a). In contrast, our approach maintains consistent performance advantage over both the base model and GRPO as k-values increase. 8 Preprint, July 2025 Figure 3: Pass@k curves of RL-PLUS compared with baselines across multiple benchmarks. This sustained outperformance provides strong evidence that RL-PLUS effectively breaks through the capability boundary of the base model, rather than merely optimizing performance within its inherent ability range. On the AMC and MATH-500 tasks, the accuracy of RL-PLUS eventually plateaus because its performance is approaching the maximum possible score of 1.0. Table 4: Ablation Study of RL-PLUS. Method AIME 24 AIME25 AMC MATH-500 Minerva Olympiad Avg. Variants with External Data πθ/πθold πθ/πθω πθ/πθω with Our Policy Estimation RL-PLUS - Exploration-Based Advantage Function - Multiple Importance Sampling 19.6 25.8 26.1 33.4 28.3 25.1 14.8 16.3 19.2 25.9 24.1 15.3 55.1 59.9 62.3 68.1 67.8 62. 81.0 83.8 86.8 90.2 88.8 84.4 33.5 32.4 38.6 43.8 40.4 39.3 46.2 49.3 52.0 58.8 56.0 46.8 41.7 44.6 47.5 53.4 50.9 45.5 Ablation Study. To analyze the sources of RL-PLUSs effectiveness, we conduct series of ablation studies, with the results presented in Table 4. We first ablate the two core components of our approach: Multiple Importance Sampling and the Exploration-Based Advantage Function. The experimental results show that removing the Exploration-Based Advantage Function causes the models average performance to decrease from 53.4 to 50.9, which demonstrates the importance of efficient exploration for reinforcement learning. Furthermore, removing Multiple Importance Sampling leads to more significant performance degradation, with the average score dropping substantially to 45.5, highlighting the significance of incorporating external knowledge. Additionally, we compare our method against three naive approaches for integrating external knowledge. The first variant approximates the external policy πθω using the old policy πθold. The second variant, an approach also seen in LUFFY (Yan et al., 2025), approximates the external policys probability as 1, treating it as perfect oracle. When using our policy estimation as the external policy, i.e., the third variant, the performance improves by 2.9 points, demonstrating the effectiveness of our policy estimation. Due to the improper integration methods, these variants all show significant performance gap compared to RL-PLUS."
        },
        {
            "title": "5 Training Stability of RL-PLUS",
            "content": "To validate the training stability of RL-PLUS, we extended the number of training steps on the Qwen2.5-Math-1.5B model to over 10 times the original setup. As shown in Figure 4, the models key metrics demonstrate excellent stability and continuous performance improvement as training progresses. Specifically, the Average Test Score and Critic Rewards Mean both show steady upward trend, while the Actor Entropy Loss rapidly converges and stabilizes in healthy, non-zero range. This reveals an ideal balance: the models policy, while becoming more effective (i.e., exploitation), also maintains the necessary policy stochasticity for exploration, thus avoiding premature convergence to local optimum. These results strongly demonstrate that the RL-PLUS framework possesses outstanding training stability and has the potential for further performance gains through extended training. 9 Preprint, July 2025 Figure 4: Training Stability of RL-PLUS."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we proposed RL-PLUS, novel hybrid-policy optimization approach designed to counter the capability boundary collapse observed in LLMs trained with RLVR. RL-PLUS addresses this problem by synergizing external data with internal exploitation through two core components: Multiple Importance Sampling to resolve distributional mismatch from external data, and Exploration-Based Advantage Function to incentivize the discovery of correct yet low-probability reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of RL-PLUS. Notably, Pass@k curves and training dynamics demonstrate that our method breaks through the reasoning capability boundary of base model, leading to further performance improvements."
        },
        {
            "title": "References",
            "content": "Hongyi James Cai, Junlin Wang, Xiaoyin Chen, and Bhuwan Dhingra. How much backtracking is enough? exploring the interplay of sft and rl in enhancing llm reasoning. arXiv preprint arXiv:2505.24273, 2025. Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, and Xiao Wang. Step-wise adaptive integration of supervised fine-tuning and reinforcement learning for task-specific llms. arXiv preprint arXiv:2505.13026, 2025. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025a. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025b. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 10 Preprint, July 2025 Alexander Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane DwivediYu, et al. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In ACL (1), pp. 38283850. Association for Computational Linguistics, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. KimiTeam. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In NeurIPS, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q. Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. https: //huggingface.co/datasets/Numinamath, 2024. Hugging Face repository, 13:9. Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In ICCV, pp. 29993007. IEEE Computer Society, 2017. Yihao Liu, Shuocheng Li, Lang Cao, Yuhang Xie, Mengyu Zhou, Haoyu Dong, Xiaojun Ma, Shi Han, and Dongmei Zhang. Superrl: Reinforcement learning with supervision to boost language model reasoning. arXiv preprint arXiv:2506.01096, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like training pilot study. https://oatllm.notion.site/ oat-zero, 2025b. Notion Blog. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025c. Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, et al. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025. OpenAI. Openai o1 system card and model report. 2024. Technical report, OpenAI o1 series, available online. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Martin Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. 11 Preprint, July 2025 David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024a. Zhichao Wang, Bin Bi, Zixu Zhu, Xiangbo Mao, Jun Wang, and Shiyu Wang. Uft: Unifying finetuning of sft and rlhf/dpo/una through generalized implicit reward function. arXiv preprint arXiv:2410.21438, 2024b. Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, and Jianhua Tao. Thought-augmented policy optimization: Bridging external guidance and internal capabilities. arXiv preprint arXiv:2505.15692, 2025. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? CoRR, abs/2504.13837, 2025a. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025b. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. 12 Preprint, July"
        },
        {
            "title": "A Theoretical Analysis of Multiple Importance Sampling",
            "content": "We provide rigorous theoretical analysis of the Multiple Importance Sampling (MIS) estimator for policy optimization. First, we dissect the bias and variance issues inherent to standard Importance Sampling (IS) when using data from single behavior policy. Subsequently, we prove that the MIS estimator is unbiased and analyze its superior variance properties. We show that MIS is robust to the inclusion of suboptimal behavior policies, establishing it as powerful tool for integrating diverse data sources in policy optimization. A.1 Preliminaries and Core Assumptions Our analysis is based on the following standard settings and assumptions. Let the objective function be J(θ) = Eτ πθ [R(τ )], where τ represents complete trajectory, R(τ ) is its corresponding cumulative return, and πθ is the target policy we aim to optimize. Assumption A.1 (Joint Support Coverage). The support of the target policy πθ is covered by the union of the supports of all behavior policies {πβk }K k=1. Formally, supp(πθ) (cid:91) supp(πβk ) k=1 This assumption ensures that any trajectory possible under πθ can be sampled with non-zero probability by at least one behavior policy. Assumption A.2 (Bounded Rewards). The trajectory returns are bounded, i.e., for all trajectories τ , there exists constant Rmax such that R(τ ) Rmax < . This ensures that all expectations and variances are well-defined. A.2 Analysis of Bias and Variance in Single-Strategy Importance Sampling When learning from data generated by single external behavior policy πω, the standard IS estimator can suffer from bias and variance problems. We analyze three primary failure modes. A.2.1 Importance Sampling Estimators We formally define the estimators central to our analysis. We consider dataset of trajectories. Definition A.3 (Standard Importance Sampling (IS) Estimator). When all data is sampled from single behavior policy πω (i.e., = 1, πβ1 = πω), the standard IS estimator for J(θ) is: ˆJIS(θ) = 1 (cid:88) i=1 πθ(τi) πω(τi) R(τi), where τi πω Definition A.4 (Proxy IS Estimator). biased variant of the IS estimator that uses proxy policy πθold in the denominator, while the data is sampled from different policy πω: ˆJproxy(θ) = 1 (cid:88) i= πθ(τi) πθold (τi) R(τi), where τi πω A.2.2 Bias from Proxy In practice, to mitigate the high variance that occurs when the data-generating policy πω is far from the target policy πθ, one might be tempted to use different policy, πθold, as the denominator for the importance ratio. This proxy policy is chosen to be closer to πθ (e.g., previous iterate of the policy). However, this introduces systematic bias, as it violates the fundamental principle of importance sampling. Lemma A.5 (Bias of the IS Estimator with Proxy). Assume trajectory data τi is sampled from an external policy πω, i.e., τi πω. If we construct an estimator using proxy policy πθold in the denominator of the importance weight: ˆJproxy(θ) = 1 N (cid:88) i=1 πθ(τi) πθold(τi) R(τi) 13 Preprint, July then this estimator is biased for the true objective J(θ) whenever the proxy policy πθold is not identical to the true sampling policy πω. The bias is given by: B(θ, ω, θold) = Eπω [ ˆJproxy(θ)] J(θ) = (cid:90) πθ(τ )R(τ ) (cid:18) πω(τ ) πθold(τ ) (cid:19) 1 dτ (8) Proof. We compute the expectation of the proxy estimator ˆJproxy(θ) under the true data distribution πω. The expectation is taken with respect to τ πω. Eπω [ ˆJproxy(θ)] = Eτ πω (cid:20) πθ(τ ) πθold(τ ) πθ(τ ) πθold(τ ) (cid:21) R(τ ) R(τ )dτ (cid:90) = πω(τ ) This is the expected value that the estimator will yield. Crucially, because the sampling distribution πω(τ ) in the integral does not cancel with the denominator πθold (τ ), this expression cannot be simplified to the true objective J(θ) = (cid:82) πθ(τ )R(τ )dτ . The bias of this estimator is its expectation minus the true objective: B(θ, ω, θold) = Eπω [ ˆJproxy(θ)] J(θ) = = = (cid:90) (cid:90) πω(τ ) (cid:90) (cid:18) πω(τ ) πθ(τ ) πθold(τ ) πθ(τ ) πθold(τ ) R(τ )dτ (cid:90) πθ(τ )R(τ )dτ R(τ ) πθ(τ )R(τ ) dτ (cid:19) πθ(τ )R(τ ) (cid:18) πω(τ ) πθold (τ ) (cid:19) 1 dτ The final expression for the bias is zero if and only if πω(τ ) = πθold (τ ) for all relevant trajectories. If the external data policy πω differs significantly from the proxy policy πθold, this ratio will deviate substantially from 1, leading to large, systematic bias. A.2.3 Bias from Support Mismatch Even when using the correct data-generating policy πω in the denominator, the standard IS estimator is biased if the support of πω does not fully cover the support of the target policy πθ. Lemma A.6 (Bias of the Standard IS Estimator from Support Mismatch). When using data sampled from an external policy πω to estimate the objective J(θ), if the support condition supp(πθ) supp(πω) is not met, the standard IS estimator ˆJIS(θ) = 1 πθ(τi) πω(τi) R(τi) (where τi πω) is biased. The bias relative to the true objective is: (cid:80)N i=1 B(θ, ω) = Eπω [ ˆJIS(θ)] J(θ) = (cid:90) τ supp(πθ)supp(πω) πθ(τ )R(τ )dτ (9) Proof. The expectation of the IS estimator is calculated as follows: Eπω [ ˆJIS(θ)] = Eτ πω (cid:20) πθ(τ ) πω(τ ) (cid:21) R(τ ) (cid:90) τ supp(πω) (cid:90) = = πω(τ ) πθ(τ ) πω(τ ) R(τ )dτ πθ(τ )R(τ )dτ τ supp(πω)supp(πθ) The true objective J(θ) can be decomposed over the same domains: (cid:90) J(θ) = πθ(τ )R(τ )dτ τ supp(πθ) (cid:90) = πθ(τ )R(τ )dτ + (cid:90) τ supp(πθ)supp(πω) τ supp(πθ)supp(πω) πθ(τ )R(τ )dτ 14 Preprint, July 2025 The bias is the difference between these two quantities. This term represents the expected return from trajectories possible under πθ but not under πω, and it is zero if and only if the support condition holds. A.2.4 Variance Divergence of the Importance Ratio Lemma A.7 (Variance of the IS Ratio). Even if the support condition is satisfied, the variance of the importance ratio rω(τ ) = πθ(τ ) πω(τ ) can become extremely large when the target policy πθ and behavior policy πω are dissimilar. Precisely, the variance is equal to the Chi-squared divergence between the two policies: Varπω (rω) = χ2(πθ, πω) Proof. The variance of the ratio is Varπω (rω) = Eπω [(rω)2] (Eπω [rω])2. Under the support coverage condition, the expectation of the ratio is Eπω [rω] = 1. We compute the second moment: (cid:18) πθ(τ ) πω(τ ) (cid:90) πθ(τ )2 πω(τ ) Eπω [(rω)2] = πω(τ ) dτ = dτ. (cid:19)2 (cid:90) By noting that χ2(πθ, πω) = (cid:82) (πθ(τ )πω(τ ))2 Eπω [(rω)2] 2 + 1 = Eπω [(rω)2] 1, we have: πω(τ ) dτ = (cid:82) πθ(τ )2 πω(τ ) dτ 2 (cid:82) πθ(τ )dτ + (cid:82) πω(τ )dτ = Therefore, the variance is: Eπω [(rω)2] = χ2(πθ, πω) + 1. Varπω (rω) = (χ2(πθ, πω) + 1) 12 = χ2(πθ, πω). Both the χ2-divergence and the more commonly known KL-divergence (DKL(πθπω)) are measures of dissimilarity between distributions (both are instances of f-divergences). large value in one typically implies large value in the other. Therefore, as the policies diverge, there are often regions where πθ(τ ) πω(τ ). In these regions, the ratio rω(τ ) becomes extremely large, causing the variance to explode. A.3 Bias Advantage of the MIS Estimator The standard MIS estimator is proven to be unbiased. In practice, common and highly practical scenario involves using external data collected from the behavior policy, πω, which may be far from the target policy πθ. To stabilize estimates, one can introduce proxy policy, πθold (e.g., previous iterate of πθ), into the denominator of the importance weight. This creates powerful estimator that deliberately accepts small, controlled bias in exchange for substantial reduction in variance. We now formally analyze the bias advantage of this practical MIS estimator compared to the aforementioned approaches. Remark A.8 (Controlled Bias vs. Explosive Bias of Proxy IS). This estimator is motivated by variance reduction. While biased, its bias is far more controlled than that of the proxy estimator from Lemma A.5, which uses only πθold in the denominator. comparison of their bias-inducing factors is revealing: Proxy IS Factor: fproxy(τ ) = πω(τ )πθold (τ ) πθold (τ ) Practical MIS Factor: fMIS(τ ) = πω(τ )πθold (τ ) πω(τ )+πθold (τ ) When πθold(τ ) 0 for trajectory that is plausible under πω, the proxy IS factor can become arbitrarily large, leading to an uncontrolled, potentially infinite bias. In contrast, the practical MIS factor is normalized difference and is strictly bounded within (1, 1). The presence of the true sampling distribution πω(τ ) in the denominator acts as crucial guardrail, preventing the weights from exploding and ensuring the bias remains bounded. 15 Preprint, July 2025 Remark A.9 (Overcoming Support Mismatch). The practical MIS estimator also offers robust solution to the critical problem of support mismatch (Lemma A.6), where supp(πθ) supp(πω). The practical MIS estimator mitigates this by relying on the weaker joint support assumption, supp(πθ) supp(πω) supp(πθold ). By including πθold, it explicitly covers the full support of πθ and eliminates the truncation error. In its place, it introduces distortion error, given by the bounded bias term derived above. In essence, this estimator replaces potentially infinite and unrecoverable truncation error, i.e, (cid:90) Bsupport = πθ(τ )R(τ )dτ τ supp(πθ)supp(πβ1 ) , with manageable and bounded distortion error, making it far more robust choice for real-world applications. A.4 Variance Advantage and Robustness of the MIS Estimator The core advantage of MIS lies in its variance control and robustness, and we formally analyze below. Theorem A.10 (Variance Robustness of MIS). So long as there is at least one policy in the behavior pool {πβk } (e.g., πβ πθ), the variance of the MIS estimator will be low. The estimator is insensitive to other arbitrarily bad behavior policies in the pool. ) that is good approximation of the target policy πθ (i.e., πβ k Proof. We qualitatively analyze the behavior of the MIS weight w(τ ) = nitude directly drives the variance. (cid:80) πθ(τ ) αj πβj (τ ) , whose magDilemma of Standard IS: Assume we only use bad policy πβm, for which the probability density approaches zero in some region Sbad (πβm(τ ) 0), while the target policy has non-negligible density there (πθ(τ ) > ϵ). In this case, the standard IS ratio πθ(τ ) πβm (τ ) would diverge in Sbad, causing the variance to explode. Advantage of MIS: Now, we add good policy πβ πθ. The denominator of the MIS weight is mixture density: (cid:80) αjπβj (τ ). Even in the problematic region Sbad, the denominator contains at least one term, αk πβk (τ ) αk πθ(τ ), which is positive and non-negligible. The MIS weight is therefore effectively bounded: to the pool, satisfying πβ w(τ ) = πθ(τ ) αk πβk (τ ) + (cid:80) j=k αjπβj (τ ) πθ(τ ) αk πθ(τ ) + . . . πθ(τ ) αk πβk (τ ) 1 αk = nk The weight is bounded from above by constant that does not depend on the ratio of policies. The summation in the denominator acts as variance guardrail, preventing the sampling deficiencies of any single policy from destabilizing the entire estimate. Remark A.11 (Practical Implications). The robustness of MIS is especially critical when combining internal data (from an old policy πold) and external data (from πω). The policy πold ensures that the KL-divergence from the current policy πθ is kept within controllable range. This ensures that there is always good policy in the pool. Therefore, even if the external policy πω is far from πθ, the MIS estimator can stabilize the variance through the presence of πold. MIS achieves soft, unbiased form of variance control by mixing policy densities in the denominator. This adaptive weighting mechanism makes MIS theoretically sound and highly effective choice for integrating heterogeneous data sources in policy optimization. A.5 Optimal Bayesian Estimation of the Behavior Policy under Model Uncertainty We need method to construct robust estimator for πω that acknowledges our uncertainty. We propose principled approach based on Bayesian decision theory to derive an optimal estimator for πω that explicitly balances our belief in the proxy model πθold with model of maximal uncertainty. We frame the task of selecting an estimator ˆπω as Bayesian decision problem. State of Nature: The true, unknown behavior policy πω. 16 Preprint, July 2025 Action: Our choice of an estimator ˆπω for πω. Model Space M: The set of candidate models for πω. Given our limited knowledge, we define minimal, discrete model space that captures the dichotomy between our specific knowledge and our uncertainty. Loss Function L(ˆπω, πω): function that quantifies the error of our estimator. standard choice is the squared L2-error, L(ˆπω, πω) = (cid:82) (ˆπω(τ ) πω(τ ))2 dτ . Our goal is to find the estimator ˆπω that minimizes the Bayes risk, which is the expected loss with respect to our prior beliefs about the state of nature. Theorem A.12 (Bayes-Optimal Policy Estimator). Let the model space for the unknown behavior policy πω be composed of two candidate models: The specific proxy policy, πθold, representing our available, specific information. non-informative uniform policy, U(τ ), representing maximal uncertainty. Let the trajectory space have finite volume = (cid:82) dτ , such that U(τ ) = 1/V . Under the Principle of Indifference, we assign equal prior probabilities to these models, i.e., (πω = πθold) = (πω = U) = 1/2. Then, the estimator ˆπω that minimizes the Bayes risk (expected L2 error) is the Bayesian model average: ˆπ ω(τ ) = πθold (τ ) + U(τ ) 1 2 1 2 Proof. The Bayes risk of an estimator ˆπω is the expectation of the loss function over the prior distribution of πω: R(ˆπω) = Eπω [L(ˆπω, πω)] = = (cid:88) L(ˆπω, π)P (πω = π) (cid:90) π{πθold ,U } 1 2 (ˆπω(τ ) πθold(τ ))2 dτ + (cid:90) 1 2 (ˆπω(τ ) U(τ ))2 dτ To find the optimal estimator ˆπ ω that minimizes this risk, we can use the calculus of variations or simply note that the integrand is sum of squared errors, which is minimized point-wise. For any given trajectory τ , we seek to minimize: (ˆπω(τ )) = (ˆπω(τ ) πθold(τ ))2 + (ˆπω(τ ) U(τ ))2 This is simple quadratic function of the scalar value ˆπω(τ ). We find the minimum by taking the derivative with respect to ˆπω(τ ) and setting it to zero: ˆπω(τ ) = 2 (ˆπω(τ ) πθold(τ )) + 2 (ˆπω(τ ) U(τ )) = 0 2ˆπω(τ ) πθold (τ ) U(τ ) = 0 1 2 ˆπω(τ ) = (πθold(τ ) + U(τ )) This result gives the point-wise minimizer. Integrating over all τ confirms that the optimal estimator function is: ˆπ ω(τ ) = πθold (τ ) + U(τ ) 1 2 1 2 This estimator is known as the Bayes estimator under quadratic loss for this prior. It is optimal in the sense that no other estimator has lower expected error, given our stated beliefs about the possible models for πω. It is straightforward to verify that ˆπ ω(τ ) is valid probability density function, as (cid:82) ˆπ (cid:82) U(τ )dτ = (cid:82) πθold (τ )dτ + 1 ω(τ )dτ = 1 2 2 (1) + 1 2 (1) = 1. 2 Assumption A.13 (Unit-Volume Trajectory Space). For analytical tractability, we assume the trajectory space is normalized to have unit volume, i.e., (cid:82) dτ = 1. Under this assumption, the maximum-entropy (uniform) distribution is U(τ ) = 1 for all τ . 17 Preprint, July 2025 Remark A.14 (Robustness and Connection to Regularization). Theorem A.12 provides rigorous justification for what is, in essence, form of regularization. The resulting estimator ˆπ ω is mixture model that hedges against the deficiencies of πθold . The uniform component U(τ ) acts as safety net or defensive distribution. By ensuring that ˆπ 2V > 0 for all τ , it guarantees that the importance sampling ratios denominator is strictly positive and bounded away from zero. This prevents the variance of the importance weights from exploding, critical property for stable off-policy learning. ω(τ ) 1 The assumption = 1/2 reflects state of maximal ambiguity between the specific information we have (πθold) and the general uncertainty we face (U). It is the most conservative and robust choice when we cannot quantify our confidence in πθold. Thus, forming the estimator as their mean is the theoretically optimal strategy to navigate this uncertainty. Theoretical Analysis of the Exploration-Based Advantage We provide theoretical justification for the proposed Exploration-Based Advantage function. We prove that it adaptively focuses the policy gradient updates on high-value, hard-to-explore actions. B.1 Gradient Analysis We now analyze the effect of this advantage function on the policy gradient. Lemma B.1 (Gradient Contribution of Single Timestep). The gradient update for the policy parameters θ induced by the action ei,t from correct, high-reward trajectory is given by: θi,t θ log πθ(ei,tq, ei,<t) Ai (1 πθ(ei,tq, ei,<t))γ Proof. The gradient update for θ log πθ(ei,t . . . ) Ac i,t. Substituting the definition of Ac i,t, we have: the policy objective at timestep is proportional to θi,t θ log πθ(ei,t . . . ) Ai Ci,t(θ) By the definition of Ci,t(θ) and the properties of the detach operator, the term Ci,t(θ) is treated as scalar weight during backpropagation. Substituting its definition yields the result. This lemma establishes the precise form of the gradient update. We now prove our main result: that this form adaptively focuses learning. Theorem B.2 (Adaptive Gradient Focusing). Given high-reward trajectory where Ai > 0, the gradient magnitude of the update induced by Ac i,t is inversely related to the policys confidence πθ(ei,t . . . ). The update is amplified for hard (low-probability) actions and suppressed for easy (high-probability) actions. Proof. We analyze the asymptotic behavior of the scaling factor on the gradient, based on Lemma B.1. Let pt = πθ(ei,t . . . ) denote the policys probability for the correct action at time step t. The gradient is scaled by the factor Ai (1 pt)γ. We consider two cases for the value of pt. Case 1: Hard-to-Explore Correct Action. In this case, the policy assigns low probability to the correct action, i.e., pt 0. The exploration weight becomes: lim pt0 Ci,t(θ) = lim pt (1 pt)γ = 1 The resulting gradient update, θi,t θ log pt Ai, retains its full magnitude. The learning signal from this valuable, unexplored action is preserved. Case 2: Easy-to-Explore Correct Action. In this case, the policy is already confident about the correct action, i.e., pt 1. The exploration weight becomes: lim pt1 Ci,t(θ) = lim pt1 (1 pt)γ = The resulting gradient update vanishes: θi,t 0. The model effectively ignores updates from examples it has already mastered. 18 Preprint, July 2025 Conclusion. It demonstrates that the optimization process is focused on the gradients from actions where the policy is incorrect or uncertain, thereby prioritizing the learning of new knowledge. This proves that the advantage function leads to adaptive gradient focusing. Figure 5: Detailed Training dynamics of RL-PLUS and other baselines. Effect of hyperparameter γ Our systematic investigation into the hyperparameter γ in RL-PLUS, illustrated in Figure 6, reveals two key findings. First, the model demonstrates considerable robustness, as its performance fluctuates only small across the tested range of γ. Second, RL-PLUS consistently surpasses the GRPO baseline across all math reasoning benchmarks, irrespective of the specific value of γ. Further analysis highlights distinct trend: the model uniformly achieves its peak performance when γ=0.5. This optimal value holds not only for the Average test score but also across all individual benchmarks, including AMC, Olympiad, AIME, Minerva, and Math. This suggests that an intermediate value for γ strikes an effective balance in the models learning process. While the model is not highly sensitive to this parameter, the clear peak establishes γ=0.5 as strong default, and there is still potential room for improvement with fine-grained tuning of γ. Figure 6: Effect of hyperparameter γ in RL-PLUS. 19 Preprint, July 2025 Figure 7: case of RL-PLUS compared with baselines GRPO and SFT+GRPO."
        },
        {
            "title": "D Case Study",
            "content": "Figure 7 presents typical case study that visually contrasts the performance of RL-PLUS with the baseline methods, GRPO and SFT+GRPO. In this case, RL-PLUS demonstrates significant advantage in both logical rigor and computational precision. Specifically, GRPO, while touching upon part of the core issue by identifying multiples of 5 as part of the losing positions, demonstrates an incomplete understanding. It fails to identify the other critical condition, thus arriving at an incorrect conclusion. SFT+GRPOs approach is fundamentally flawed. It completely misinterprets the game-theoretic model of the problem, erroneously applying an irrelevant modulo 3 logic, causing its reasoning to be incorrect from the outset. The performance of RL-PLUS is exemplary. It be20 Preprint, July 2025 gins by accurately identifying the problem as game of finding P-positions (second-player winning positions). Subsequently, through deductive reasoning, it successfully derives the complete pattern for the set of losing positions: when 0 or 2 mod 5. Finally, it proceeds with clear, step-bystep calculation for both conditions, sums them accurately, and arrives at the correct answer. This case provides compelling evidence that RL-PLUS possesses more profound and comprehensive multi-step reasoning capability."
        },
        {
            "title": "E Experimental Setup",
            "content": "Training Details. All experiments are conducted on 8 NVIDIA A100 80G GPUs. By default, we use the Qwen2.5-Math-7B model (Yang et al., 2024) as the base model in our experiments. For our training, we use the dataset from previous work (Yan et al., 2025), which contains 45,000 prompts from OpenR1-Math-220k with correct reasoning trajectories annotated by Deepseek-R1, and change the rope theta of Qwen2.5-Math-7B from 10000 to 40000 and extend the window size to 16384. In implementing the RL algorithm, we leverage the VeRL framework (Sheng et al., 2024). We set the batch size to 128, the mini-batch size to 64, and the maximum training epoch to 2. For each problem, we use 8 rollout trajectories, with maximum response length of 8192 tokens. For our approach, one of the model-generated rollouts is replaced with correct reasoning trajectory from the training dataset. It is important to note that we ensure all other RL algorithms maintain the same parameter settings as RL-PLUS to guarantee fair comparison. For the hyperparameter γ, we set it to 0.5 in all experiments by default. To validate the applicability of RL-PLUS on various base LLMs, we additionally extend RL-PLUS to other base models, including LLaMA-3.1-8B-instruct, Deepseek-Math-7B-instruct, and Qwen2.5-Math-1.5B. Evaluation. In line with established practices, we evaluate the performance of RL-PLUS on comprehensive suite of standard mathematical reasoning benchmarks, including GSM8K (Cobbe et al., 2021), MATH500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024), as well as on competition-level benchmarks such as AIME 2024 (Li et al., 2024) and AMC 2023 (Li et al., 2024). Additionally, although our training focuses on math, we extend our evaluation to out-of-domain (OOD) tasks to assess the robustness and generalization capabilities of our approach. The OOD datasets include ARC-c (Clark et al., 2018)(OpenDomain Reasoning), GPQA-diamond (Rein et al., 2024) (Science Graduate Knowledge), MMLUPro (Wang et al., 2024a) (Reasoning-focused Questions from Academic Exams and Textbooks), as well as three code generation datasets: HumanEval (Chen et al., 2021), LeetCode (Guo et al., 2024), and LiveCodeBench (Jain et al., 2024). During evaluation, we set the sampling temperature to 0.6 and report the average pass@1 score over 5 runs by default. Baselines. We compare our approach with two categories of baselines, all trained upon the same base model. The first category comprises eight recently proposed RLVR methods, including: 1) SimpleRL (Zeng et al., 2025) and 2) OpenReasoner-Zero (Hu et al., 2025) are two open-source RL implementations that train starting from the base model using rule-based rewards. SimpleRL employs token-level, length-rectified GRPO algorithm, while OpenReasoner-Zero utilizes the PPO algorithm. 3) PRIME (Cui et al., 2025a) introduces an implicit process reward based on outcome labels during RL. 4) Oat-Zero (Liu et al., 2025b) modifies the GRPO algorithm by removing the standard deviation from the advantage computation and eliminating token-level normalization in the policy loss calculation. 5) DAPO (Yu et al., 2025) optimizes GRPO algorithm by introducing four operations: Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss, and Overlong Reward Shaping. 6) LUFFY (Yan et al., 2025) leverages off-policy reasoning trajectories to augment GRPO. 7) TAPO (Wu et al., 2025) integrates reasoning templates into GRPO sampling process to enhance the models internal reasoning capabilities. 8) ReLIFT (Ma et al., 2025) performs RL and SFT alternately during training. The second category consists of four straightforward baselines: 1) SFT, supervised fine-tuning using external reasoning trajectory data. 2) GRPO (Shao et al., 2024), training with GRPO algorithm on question-answer pairs. 3) SFT+GRPO, common RL cold-start approach that performs SFT before RL training. 4) GRPO w/ SFT Loss, jointly optimizes the GRPO objective and SFT loss during training."
        }
    ],
    "affiliations": [
        "Department of Computing Science, University of Alberta",
        "School of Computer Science, Peking University",
        "Tongyi Lab, Alibaba Group"
    ]
}