{
    "paper_title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval",
    "authors": [
        "Xiaojun Wu",
        "Cehao Yang",
        "Xueyuan Lin",
        "Chengjin Xu",
        "Xuhui Jiang",
        "Yuanliang Sun",
        "Hui Xiong",
        "Jia Li",
        "Jian Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 1 7 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "THINK-ON-GRAPH 3.0: EFFICIENT AND ADAPTIVE LLM REASONING ON HETEROGENEOUS GRAPHS VIA MULTI-AGENT DUAL-EVOLVING CONTEXT RETRIEVAL Xiaojun Wu1,2,3, Cehao Yang1,2,3, Xueyuan Lin1,2,4, Chengjin Xu1,3, Xuhui Jiang1,3, Yuanliang Sun3, Hui Xiong2, Jia Li2, Jian Guo1 1IDEA Research, International Digital Economy Academy 2Hong Kong University of Science and Technology (Guangzhou) 3DataArc Tech Ltd. 4Hithink RoyalFlush Information Network Co., Ltd {xwu647,cyang289,xlin058,jialee}@connect.hkust-gz.edu.cn, xionghui@ust.hk sunyuanliang@dataarctech.com, {xuchengjin,jiangxuhui,guojian}@idea.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Thinkon-Graph 3.0 (ToG-3), novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of Chunk-TripletsCommunity heterogeneous graph index, which pioneeringly incorporates dualevolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses critical limitation of prior Graph-based RAG methods, which typically construct static graph index in single pass without adapting to the actual query. multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid advancement of both commercial (OpenAI, 2025; AI, 2025a; Comanici et al., 2025) and open-source Large Language Models (LLMs) (Yang et al., 2025; AI, 2025b; Liu et al., 2024; Zeng et al., 2025; Gan et al., 2023) has significantly enhanced the accessibility of generative AI capabilities for both end-users and developers. Notably, open-source models play crucial role in enabling *Equal contribution. Corresponding authors. Our code implementation are available at https://github.com/DataArcTech/RAG-Factory."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Performance Limitations of Graph-Based RAG systems under Resource-Constrained and LocallyDeployed Scenarios. In such scenarios, developers typically adopt open-source models such as Llama or Qwen as the backbone LLMs. Limitations like incomplete extracted triplets, insufficient extraction details and parsing failure may lead to insufficient knowledge provision, ultimately resulting in failure to adequately answer the query. AI applications in offline environments. However, current LLMs still face notable limitations, including issues with factual hallucinations and inadequate performance in complex reasoning tasks. Retrieval-augmented generation (RAG) (Gao et al., 2023) has become popular method for grounding Large Language Models (LLMs) with external knowledge, addressing issues like knowledge cutoff and hallucination. While traditional RAG systems rely on vector similarity to retrieve relevant text chunks, they often struggle with complex reasoning tasks that require integrating information across multiple documents or understanding structural relationships between entities. To address the above limitations, recent advancements have explored using Knowledge Graphs (KGs) or extracted Graph using LLMs to represent and retrieve structured information. ToG (Sun et al., 2023; Ma et al., 2024) pioneered an iterative hybrid RAG framework that tightly couples text and KGs retrieval, though their approach relies on pre-existing structured KGs such as Freebase and Wikidata. On the other hand, methods like GraphRAG (Edge et al., 2024) and LightRAG (Guo et al., 2024) address this issue by constructing graph directly from the input documents. They create an entity-based graph to enhance information retrieval and summarization. However, as shown in Figure 1, the quality of the generated graph is highly dependent on the LLMs ability to accurately extract entities and relationships, which can be bottleneck for lightweight models like Qwen2.5-7B72B (Yang et al., 2024), which is broadly deployed in private and offline environments. Moreover, these methods often separate the handling of local and global questions. To overcome these limitations, we introduce Think-on-Graph 3.0 (ToG-3), new RAG framework that integrates the strengths of both paradigms. Our core contribution lies in the introduction of novel Chunk-Triplets-Community heterogeneous graph architecture and novel MACER (Multi-Agent Context Evolution and Retrieval) mechanism, which pioneeringly incorporates dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. Figure 2 illustrates the key distinctions between ToG-3 and classical RAG paradigms such as NaiveRAG and GraphRAG. ToG-3 introduces novel dual-evolution mechanismcomprising Evolving Query and Evolving Subgraphthat dynamically refines both the query representation and the graph structure in an iterative manner. This approach addresses critical limitation of prior RAG methods, which typically construct static graph index in single pass without adapting to the actual query. The framework is particularly suited for resource-constrained and on-premises deployment scenarios, where lightweight open-source LLMs (e.g., Llama or Qwen) are often employed as the backbone of the RAG system. Extensive experiments on complex multi-hop reasoning benchmarks demonstrate that our method achieves the highest average Exact Match and F1 scores on HotpotQA, 2WikiMultihopQA, and Musique. For broad reasoning tasks, ToG-3 also achieves remarkable win rates over baselines across comprehensiveness, diversity, and empowerment dimensions."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Evolution of Retrieval-Augmented Generation Paradigms. (a) Naive RAG embeds raw documents and performs single-shot retrieval. (b) Graph-based RAG pre-builds static graph once and retrieves from it. (c) ToG-3 introduces four-agent loopRetriever, Constructor, Reflector, Responsewhere the graph and the query sub-tasks co-evolve at runtime, yielding dynamic, query-adaptive context that converges to minimal, sufficient subgraph. Our key contributions are summarized as follows: 1. We propose MACER (Multi-Agent Context Evolution and Retrieval), novel multi-agent framework that introduces dual-evolution mechanism integrating Evolving Query and Evolving Sub-Graph within graph-based RAG. This design significantly enhances retrieval performance and complex reasoning capabilities, especially when using lightweight opensource LLMs as the backbone of the RAG system. 2. We present ToG-3, unified reasoning system that effectively combines the complementary advantages of prior graph-based and ToG methods through ChunkTripletCommunity Heterogeneous Graph Index and Dual-Evolving Context Retrieval Loop Process. 3. We conduct extensive experiments on both Deep and Broad Reasoning Tasks, demonstrating that our approach consistently supports multi-hop inference and large-scale contextual integration, achieving competitive results across diverse benchmarks."
        },
        {
            "title": "2.1 GRAPH-BASED RETRIEVAL-AUGMENTED GENERATION",
            "content": "Recent advances in retrieval-augmented generation (RAG) have increasingly emphasized structural awareness to improve reasoning depth and contextual coherence. Edge et al. (2024) propose GraphRAG, which builds knowledge graph (KG) from documents via LLM-based entity and relation extraction, then applies community detection to generate hierarchical summaries for global sensemaking. Guo et al. (2024) introduce LightRAG, which employs dual-level retrieval system combining low-level fact retrieval and high-level semantic discovery using compact KG, improving both efficiency and coverage. Further building on this idea, Gutierrez et al. (2024; 2025) present non-parametric continual learning framework that uses Personalized PageRank over an open KG to enable associative, multi-hop reasoning. Other structure-augmented RAG methods include RAPTOR (Sarthi et al., 2024), Chen et al. (2023) enhance sense-making but often introduce noise through uncontrolled summarization or lack explicit support for multi-hop reasoning. 2.2 KNOWLEDGE GRAPHS IN RAG AND HYBRID APPROACHES The integration of structured knowledge into LLM reasoning has long been pursued to improve faithfulness and interpretability. Early KG-augmented RAG systems retrieve triples from static ex-"
        },
        {
            "title": "Preprint",
            "content": "ternal knowledge bases such as Wikidata or Freebase to ground model outputs (Sun et al., 2023). However, these sources are often incomplete, outdated, or misaligned with domain-specific content. To overcome this, hybrid RAG frameworks (Ma et al., 2024) combine unstructured text and structured KGs to balance breadth and precision. Chain-of-Knowledge (CoK) (Li et al., 2024) retrieves from multiple structured sources including Wikipedia, Wikidata, and Wikitable to ground LLM responses. HybridRAG (Sarmah et al., 2024) fuses vector-based and KG-based retrievers, demonstrating superior reasoning performance compared to either modality alone. 2."
        },
        {
            "title": "ITERATIVE AND REFLECTIVE REASONING IN LLMS",
            "content": "Enabling LLMs to reason iteratively has been shown to improve accuracy and faithfulness. ITERRETGEN (Shao et al., 2023) introduces an iterative loop that alternates between retrieval and generation, using generated hypotheses to guide further search. Trivedi et al. (2023) combine Chainof-Thought (CoT) with retrieval, interleaving reasoning steps with evidence gathering, significantly improving performance on multi-hop QA. Self-RAG (Asai et al., 2023) equips LLMs with reflection tokens to decide when to retrieve and whether the output is hallucinated. ReAct (Yao et al., 2023a) combines reasoning traces with external actions, enabling task decomposition and environment interaction. Other efforts focus on continual learning for LLMs, where RAG serves as nonparametric alternative to fine-tuning (Shi et al., 2024). Continual pretraining (Jin et al., 2022) and instruction tuning (Zhang et al., 2023) can update model parameters but suffer from catastrophic forgetting (Huang et al., 2024). Model editing methods (Yao et al., 2023b) offer fine-grained updates but struggle with generalization."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "Think-on-Graph 3.0 (ToG-3) introduces novel Multi-Agent Context Evolution and Retrieval (MACER) framework for open-domain question answering. 3.1 PROBLEM FORMULATION Let = {di}N is both accurate and faithful to the source corpus, derived from minimal, sufficient subgraph heterogeneous graph constructed from D: i=1 be text corpus. The objective is to answer user query with an answer that of = arg min GG subject to Suff(q, G) = 1, (1) where Suff(, ) {0, 1} is an function judging the sufficiency of subgraph for answering the query. Existing methods face critical dilemma: (1) Systems like ToG-1 or 2 rely on high-quality, preconstructed KGs, limiting their applicability to private or specialized domains. (2) Corpus-based GraphRAG methods (e.g., GraphRAG, LightRAG) build static graph from in one go. Their performance is bottlenecked by the quality of this initial graph, which in turn depends heavily on the capability of the LLM used for information extraction. 3.2 HETEROGENEOUS GRAPH INDEX: SCHEMA AND CONSTRUCTION 3.2.1 NODE AND EDGE SCHEMA The Constructor Agent builds heterogeneous graph = (V, E) with three node types: Chunks (C): Sentence-level text passages from the corpus. Triplets (T ): Semantic triples (s, p, o) extracted from chunks, annotated with entity and relation types (type s, type p, type o). Communities (M): Summaries of entity clusters obtained via Leiden clustering on the entity co-occurrence graph, each condensed into an abstract. Edges are defined by three type relations:"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Multi-Agent Dual-Evolving Context Retrieval-Response Loop. The Retriever fetches an initial chunktripletcommunity subgraph. The Response Agent produces an answer; the Reflector Agent judges sufficiency (reward=1/0). If insufficient (reward=0), the Reflector evolves the query into sub-queries while the Constructor evolves the subgraph (sub-graph refinement). The loop repeats until the context becomes sufficient or the horizon is reached, after which the Response Agent synthesizes the final answer from the full trajectory. OPENREL(s, p, o): Connects entities and via predicate extracted by the LLM, forming an open-domain semantic triple. MENTIONEDIN(t, c): Connects triplet to the chunk from which it was extracted. SUMMARYFOR(m, e): Connects community summary node to an entity that belongs to that community. This unified schema allows both fine-grained (chunk/triplet) and high-level (community) information to be retrieved seamlessly within single vector space, effectively addressing the local/global retrieval dichotomy of prior GraphRAG systems. 3.2.2 OFFLINE INDEX CONSTRUCTION Algorithm 1 in Appendix. details the one-time construction of the universal index G. key design choice is the use of single frozen encoder Eθ (e.g., jina-mebedding-v3 (Sturua et al., 2024)) to embed all nodesregardless of typeinto unified 1024-dimensional dense vector space. This enables efficient vector search across all node types during retrieval. 3.3 THE MACER PROCESS: MULTI-AGENT CONTEXT EVOLUTION AND RETRIEVAL The core of ToG-3 is the online MACER loop (Algorithm 2), an iterative process of retrieval, generation, and reflection that dynamically evolves the context subgraph Gk. We formalize this process as an episodic Markov Decision Process (MDP) = (S, A, P, r). State Space (S) : At each step k, the state sk = (q, Gk, Hk) captures the complete reasoning context, including the original query q, the current evidence subgraph Gk retrieved by Retriever Agent πret, and the trajectory history Hk = (q i=0 of all previous sub-queries, answers, rewards, and sub-graphs. i, ai, ri, Gi)k1 Action Space (A) sk is either to generate targeted refinement sub-query output the STOP action (to terminate the episode). : The Reflector Agent πref serves as the policy network. Its action ak at state (to continue the reasoning process) or to Reward Function (r) diately provides sparse, binary reward rk: : Upon the Response Agent generating an answer ak, the Reflector immerk = (cid:26)1 if Suff(q, Gk, ak) = 1 (sufficient context) 0 otherwise. (2) This reward signal directly dictates the termination condition and guides the refinement process."
        },
        {
            "title": "Preprint",
            "content": "Transition Dynamics (P ) Given the current state sk and an action ak (which corresponds to issuing sub-query qk), the transition to the next state sk+1 occurs deterministically according to the following update rules: The constructor agent πconst applies the transition operator using the generated sub-query and the current graph state Gk to produce an updated graph Gk+1. This step including iterative sequence of evolving queries and evolving sub-graphs reflects the structural evolution of the graph based on the agents reasoning action, formally defined by the recurrence: = πevolve ref const (q Gk+1 = πevolve (q, Gk), k, Gk), (3) (4) The action history Hk+1 is augmented with new tuple recording the executed sub-query k, the corresponding action ak, the reward rk received, and the resulting graph state Gk+1. This ensures comprehensive trace of the reasoning trajectory, which is essential for credit assignment and subsequent learning. Hk+1 = Hk (q πfinal resp (q, Hk) k, ak, rk, Gk+1) (5) (6) The complete MACER process, now cast as an MDP, is summarized in Algorithm 2. The loop continues until the Reflectors policy πref outputs the STOP action (via rk = 1) or maximum horizon is reached. The final answer is synthesized from the full trajectory Hk of states and actions, ensuring faithfulness to the evolved evidence. This MDP formulation provides the formal foundation for establishing the convergence of the MACER process under mild assumptions, as detailed in Appendix. I. This iterative refinement allows ToG-3 to start from potentially weak initial graph but specialize it towards the reasoning path of the specific query, converging on high-quality evidence subgraph . This evolving and refinement mechanism alleviate the three fundamental weaknesses of small LMs in static GraphRAG, including incomplete triplet recall, insufficient knowledge details and high parsing failure of LLMs output, as mentioned in Section 1."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENTAL SETUP Datasets To comprehensively evaluate the reasoning capabilities of RAG systems, we conduct experiments on two distinct categories of tasks: Deep Reasoning Tasks including HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020) and Musique (Trivedi et al., 2022) and Broad Reasoning Tasks including 4 subsets of UltraDomain (Qian et al., 2025) benchmark. Detailed statistics for all datasets are provided in Table 4 and Appendix. C. Evaluation Metrics For Deep Reasoning Tasks, we follow standard QA evaluation practices with Exact Match (EM) and F1 Score. For Broad Reasoning Tasks, we adopt multi-dimensional LLM-based evaluation approach including Comprehensiveness, Diversity and Empowerment following (Guo et al., 2024). Metrics detail are provide Appendix.E. Baselines We compare ToG-3 against the following state-of-the-art RAG methods across all datasets, including NaiveRAG (Gao et al., 2023), ToG-2 (Ma et al., 2024), GraphRAG (Edge et al., 2024), LightRAG (Guo et al., 2024), MiniRAG (Fan et al., 2025) and HippoRAG-2 (Gutierrez et al., 2025). Baselines details can be found in Appendix.D. For graph-based methods, we maintain identical chunk sizes (1024 tokens) and use the same LLM (Qwen2.5-32B-Instruct (Yang et al., 2024)) for all extraction and generation tasks to eliminate model capability variations. Implementation details are provide Appendix.A. 4.2 RESULT OF DEEP REASONGING BENCHMARK Result Analysis from Method Perspective. Results shown in Table 1 represent the average of three independent reasoning experiments. Previsou Graph-based methods like GraphRAG that rely"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Exact Match (EM) and F1 scores on Deep Reasoning datasets.We highlight the best , second-best , and third-best methods with different background color shades. HotpotQA 2WikiMultihopQA Musique Average Method NaiveRAG ToG-2 GraphRAG LightRAG MiniRAG HippoRAG-2 EM 0.634 0.308 0.337 0.308 0.213 0.612 F1 0.365 0.153 0.011 0.013 0.012 0.534 EM 0.382 0.401 0.439 0.420 0.125 0.491 Ours 0.639 0.516 0.500 0.189 0.194 0.018 0.023 0.018 0.254 0.267 EM 0.230 0.103 0.109 0.082 0.067 0.212 F1 0.143 0.105 0.008 0.009 0.007 0. EM 0.415 0.271 0.295 0.270 0.135 0.438 F1 0.232 0.151 0.012 0.015 0.012 0.311 0.221 0. 0.453 0.312 on LLM-based graph construction show limited performance. Their performance is the lowest, particularly in terms of F1 scores as shown in Figure 4b, which can be attributed to lack of focus on deep factual reasoning and tendency to produce verbose responses, resulting in low token-level recall. More detailed precision and recall results are provided in Appendix. F.1. ToG-2, without leveraging well-curated knowledge graphs like Freebase and Wikidata, demonstrates moderate performance in open-domain settings. NaiveRAG achieves competitive third-place results by avoiding graph construction limitations and relying solely on retrieved documents for response generation. HippoRAG-2 emerges as the strongest baseline, employing an efficient embedding model with Personalized PageRank algorithm and LLM-based triple filtering to achieve second-best performance. However, our proposed method consistently outperforms all competitors, achieving the highest average EM (0.453) and F1 (0.312) scores across all three benchmarks. This superior performance is attributed to our novel Chunk-Triplets-Community heterogeneous graph architecture and the MultiAgent Context Evolution and Retrieval (MACER) framework, which enables adaptive subgraph refinement and evolving query decomposition for complex reasoning tasks and overcomes the graph construction challenges that plague other graph-based RAG systems. Result Analysis from Dataset Perspective. As shown in Figure 4, the average performance of the baselines and our method across the HotpotQA, 2WikiMultiHopQA, and Musique datasets generally follows descending trend. This pattern can be attributed to the following reasons: HotpotQA (Yang et al., 2018): Although widely used, this dataset has been shown to provide weaker test of multi-hop reasoning due to the presence of numerous spurious cues and shortcut signals (Trivedi et al., 2022; Gutierrez et al., 2024). Musique (Trivedi et al., 2022): challenging multi-hop QA dataset comprising approximately requiring 24 hops, which emphasizes comprehensive evaluation of multi-step reasoning abilities. Musique is designed to feature diverse and complex reasoning paths, necessitating the integration of information across multiple hops to arrive at correct answers. 4.3 RESULT OF BROAD REASONING TASKS As shwon in Figure 5, The four heatmaps clearly demonstrate that the five methods can be distinctly divided into two clusters: the upper-right region (predominantly red, indicating superior performance) and the lower-left region (predominantly blue, indicating inferior performance). Specifically, ToG-3, GraphRAG, and LightRAG exhibit significantly higher win rates compared to NaiveRAG and HippoRAG-2. Detailed win rates (%) of baselines v.s. ToG-3 across four datasets are provided in Table 6 of Appendix. F. Our framework outperforms NaiveRAG by substantial margins (up to 88.8% overall win rate on Legal dataset and 72.9% average win rate on all four datasets), highlighting the limitations of chunk-based retrieval for complex queries. While GraphRAG shows competitive performance in comprehensiveness due to its extensive community summarization and retrival, ToG-3 achieves better balance across all metrics, particularly excelling in diversity and empowerment through its heterogeneous graph architecture that integrates chunk-level, triplet-level, and community-level information. Detailed ELO rating calculation for broad reasoning tasks can be found in Appendix. F.3. The multi-agent dual-evolving context retrieval mechanism enables both"
        },
        {
            "title": "Preprint",
            "content": "(a) Exact Match (EM) Score Comparison (b) F1 Score Comparison Figure 4: Performance comparison of different RAG methods on multi-hop QA datasets. (a) Exact Match scores measure the percentage of questions where the models answer exactly matches the ground truth. (b) F1 scores provide harmonic mean of precision and recall for token-level answer matching. Figure 5: ELO-based Pairwise Win Rate Matrices Across Four Benchmark Datasets. Each heatmap visualizes win probabilities derived from direct head-to-head experimental comparisons, transformed through the ELO framework to ensure transitive consistency. The diagonal of the heatmap is set to default value of 0.5, indicating self-comparison of the method. deep knowledge reasoning through entity-relation exploration and broad community reasoning. This balanced architectural approach makes ToG-3 particularly effective for real-world applications requiring both comprehensive coverage and precise, actionable insights. Our analysis reveals that, on average, 20% of the samples require one evolving-context iteration, 32% require two iterations, and 48% require three iterations. Case studies of ToG-3 retrieval and response output are provided in Appendix. G. 4.4 ANALYSIS OF COMPUTATION COST The Table 2 reveal consistent accuracy-efficiency trade-off across all datasets. We observed that during the indexing phase, GraphRAG required the longest processing time, averaging 13.10 hours. This is primarily due to its need to extract large number of triplets and generate community summaries. In comparison, both ToG-3 and LightRAG showed similar indexing times, at 10.13 and 10.06 hours respectively. Although ToG-3 also involves community summary generation, it constructs the graph more efficiently by extracting fewer relational structures during graph initial-"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Computational cost comparison across datasets between Graph-based methods. The best EM score of each dataset are marked in bold. ToG-3 achieves the best accuracy with efficient indexing and justified inference cost. Dataset Method HotpotQA 2WikiMultihopQA Musique Average ToG-3 GraphRAG LightRAG ToG-3 GraphRAG LightRAG ToG-3 GraphRAG LightRAG ToG-3 GraphRAG LightRAG Entities 37,358 94,376 94,578 19,311 50,556 50,177 32,842 106,042 94,621 29,837 83,658 79,792 Graph Statistics Relations Communities Indexing Time (h) Inference Time (s/q) 30,987 73,265 76,157 21,077 37,840 37,995 39,134 83,139 75,923 30,399 64,748 63, 5,041 10,981 - 3,417 6,261 - 6,258 9,407 - 4,905 8,883 - 12.5 15.8 12.1 8.2 10.3 7. 9.7 13.2 10.3 10.13 13.10 10.06 16.82 8.91 6.54 14.95 7.45 5.23 13.24 9.37 7.12 15.00 8.58 6. Avg. EM 0.639 0.337 0.308 0.500 0.439 0.420 0.221 0.109 0.082 0.453 0.295 0.270 Table 3: Ablation studies of MACER components and foundation model scaling. Standard ToG-3 settings incorporates all MACER components, employs the Qwen2.5-32B-instruct as the backbone LLM, and utilizes the Jina-v3-embedding model for representation encoding. Ablation Setting HotpotQA 2WikiMultihopQA Musique Average EM F1 EM F1 EM F1 EM F1 Standard ToG-3 settings 0.639 0.516 0.500 0. 0.221 0.153 0.453 0.312 MACER Components Ablation 0.598 w/o Evolving Query 0.613 w/o Evolving Sub-Graph 0.641 w/o Community Node 0.443 0.472 0. 0.412 0.458 0.487 Foundation Model Scaling Abalation LLM Model Qwen2.5-14B Qwen2.5-72B Embedding Model Qwen3-Embed-0.6B Qwen3-Embed-4B 0.573 0.668 0.638 0.643 0.469 0.538 0.517 0. 0.453 0.523 0.505 0.508 0.203 0.234 0.259 0.231 0.281 0.269 0.271 0.178 0.203 0. 0.121 0.138 0.148 0.396 0.425 0.448 0.256 0.281 0.309 0.198 0.235 0.224 0.227 0.134 0. 0.155 0.158 0.408 0.475 0.456 0.459 0.278 0.327 0.314 0.317 ization compared to both LightRAG and GraphRAG. While LightRAG achieve faster inference times, they suffer from lower accuracy due to redundant graph elements or simpler retrieval mechanisms. GraphRAGs expensive two-stage indexing yields suboptimal results despite longer processing times. ToG-3 demonstrates an effective balance: its efficient heterogeneous graph construction produces refined knowledge bases across all datasets, and while its multi-agent reasoning requires higher inference time, this cost is directly justified by its best performance on all benchmarks, making it ideal for quality-sensitive applications requiring reliable reasoning capabilities. Note that HippoRAG-2 was excluded from the comparative analysis due to its reliance on OpenIE-based extraction rather than pure LLM extraction, which differs fundamentally from the approaches under investigation. 4.5 ABALATION STUDY Abalation Study of MACER component Our ablation study reveals the relative importance of each MACER component for deep reasoning performance. The most significant performance degradation occurs when removing the evolving query mechanism (average performance drop of 12.6% in EM and 17.9% in F1), underscoring its critical role in complex question answering, expecially when using smaller LLMs. The iterative query decomposition enables the framework to break down multifaceted questions into tractable sub-problems, which is essential for navigating the heterogeneous graph structure. Removing subgraph refinement causes moderate performance decrease (average drop of 6.2% in EM and 10.0% in F1), indicating its importance in adapting the knowledge structure"
        },
        {
            "title": "Preprint",
            "content": "to the specific reasoning context. Interestingly, community nodes show the smallest impact on deep reasoning tasks (a slight drop in the average EM and F1 scores), suggesting that while they contribute to performance, the chunk and triplet representations carry most of the relevant information for precise answer generation. However, in broad reasoning tasks, community nodes are essential for comprehensive coverage and diversity, highlighting the complementary roles of different node types in our heterogeneous graph architecture. Abalation Study of used foundation model The foundation model scaling analysis reveals several important patterns. First, LLM capacity has substantially greater impact on performance than embedding model size. Scaling from Qwen2.5-14B to Qwen2.5-72B yields 16.4% average improvement in EM scores, highlighting the critical role of reasoning capability in complex QA tasks. Second, larger embedding models provide consistent but more modest improvements. Qwen3Embed-0.6B shows slight average EM improvement over jina-embeddings-v3, while Qwen3Embed-4B provides 1.6% improvement. This suggests that while retrieval quality matters and larger embedding models contribute to better performance, the LLMs reasoning capacity remains the primary bottleneck for complex reasoning tasks. These findings provide practical guidance for resource allocation in real-world deployments."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduced Think-on-Graph 3.0, novel framework that fundamentally rethinks the paradigm of RAG for complex reasoning. By proposing the Multi-Agent Context Evolution Retrieval (MACER) mechanism and dynamic Chunk-Triplets-Community heterogeneous graph architecture, we address critical limitations in both existing graph-based RAG methods and knowledge-graph-dependent approaches. Our comprehensive experimental evaluation demonstrates that ToG-3 achieves state-of-the-art performance across multiple challenging benchmarks. The frameworks core innovation is its dual-evolving mechanismcomprising Evolving Query and Evolving Subgraphwhich dynamically refines both the query representation and the underlying graph structure throughout the reasoning process. This iterative co-evolution enables deep, multihop inference while preserving broad coverage across complex queries. This adaptive capability proves particularly valuable for overcoming the quality constraints of static graph construction and the domain limitations of pre-existing knowledge bases. The frameworks ability to work with light LLMs also opens possibilities for more efficient and deployable AI systems. We believe the principles established in ToG-3dynamic graph evolution, multi-agent collaboration, and heterogeneous knowledge integrationprovide foundation for the next generation of RAG systems."
        },
        {
            "title": "6 LIMITATION AND FUTURE DIRECTIONS",
            "content": "Of course our work has several limitations. First, constrained by GPU resources, our experiments are primarily conducted with LLMs up to 72B parameters and embedding models up to 4B parametersthough these sizes are practical for most developers and small-to-medium enterprises for local deployment. Second, the evolving query and sub-graph refinement components increase inference latency, typically 23 slower than baseline methods, making our approach more suitable for accuracy-critical applications where sacrificing speed for improved knowledge fidelity is acceptable. Third, the same mechanisms result in longer context inputs, which demand larger GPU memory capacity for efficient processing. These limitations could be mitigated through model distillation, optimized graph traversal algorithms, and dynamic context pruning techniques in future improvement. Future work will explore three promising directions for further advancement. First, we plan to extend our multi-agent evolving framework to support larger-scale and more complex knowledgeintensive tasks, such as programming assistance, financial analysis, and clinical decision-making. Second, we aim to generalize our method from text to multimodal reasoning, integrating audio, image, and video modalities to construct world model that bridges textual knowledge with perceptual grounding. Third, inspired by human cognitive science and brain science, we will explore novel architectures that combine parametric memory with large language models to unify memory and reasoning in seamless framework, enabling more efficient knowledge retention and tool-use capabilities akin to human intelligence."
        },
        {
            "title": "Preprint",
            "content": "REFERENCES Anthropic AI. Introducing claude 4. 2025a. URL https://www.anthropic.com/news/claude-4. Meta AI. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. 2025b. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection, 2023. URL https://arxiv.org/abs/ 2310.11511. Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading, 2023. URL https://arxiv.org/ abs/2310.05029. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Benoit Dherin, Michael Munn, Hanna Mazzawi, Michael Wunder, and Javier Gonzalvo. Learning without training: The implicit dynamics of in-context learning. arXiv preprint arXiv:2507.16003, 2025. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024. Tianyu Fan, Jingyuan Wang, Xubin Ren, and Chao Huang. Minirag: Towards extremely simple retrieval-augmented generation. arXiv preprint arXiv:2501.06713, 2025. Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang, Kunhao Pan, Ping Yang, Qi Yang, Jiaxing Zhang, et al. Ziya2: Data-centric learning is all llms need. arXiv preprint arXiv:2311.03301, 2023. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrievalaugmented generation. 2024. Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=hkujvAPVsg. Bernal Jimenez Gutierrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. From rag to memory: Non-parametric continual learning for large language models, 2025. URL https://arxiv.org/ abs/2502.14802. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multiIn Donia Scott, Nuria Bel, hop QA dataset for comprehensive evaluation of reasoning steps. and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pp. 6609 6625. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020. COLING-MAIN.580. URL https://doi.org/10.18653/v1/2020.coling-main.580. Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, and Jinsong Su. Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14161428, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.77. URL https://aclanthology.org/2024.acl-long.77/."
        },
        {
            "title": "Preprint",
            "content": "Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. Lifelong pretraining: Continually adapting language models to emerging corpora. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 47644780, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.351. URL https: //aclanthology.org/2022.naacl-main.351/. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. In International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=cPgh4gWZlz. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. Think-on-graph 2.0: Deep and faithful large language model reasoning with knowledgeguided retrieval augmented generation, 2024. URL https://arxiv.org/abs/2407.10805. OpenAI. Introducing gpt-5. 2025. URL https://openai.com/index/introducing-gpt-5/. Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang. Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation. In Proceedings of the ACM Web Conference 2025 (TheWebConf 2025), Sydney, Australia, 2025. ACM. URL https://arxiv.org/abs/2409.05591. arXiv:2409.05591. Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction. arXiv preprint arXiv:2408.04948, 2024. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. ManIn The Twelfth ning. RAPTOR: recursive abstractive processing for tree-organized retrieval. International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=GN921JHCRw. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy, 2023. URL https://arxiv.org/abs/2305.15294. Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, and Hao Wang. Continual learning of large language models: comprehensive survey. arXiv preprint arXiv:2404.16789, 2024. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Gunther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Andreas Koukounas, Nan Wang, and Han Xiao. jina-embeddings-v3: Multilingual embeddings with task lora, 2024. URL https://arxiv.org/abs/2409.10173. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. arXiv preprint arXiv:2307.07697, 2023. Vincent Traag, Ludo Waltman, and Nees Jan Van Eck. From louvain to leiden: guaranteeing well-connected communities. Scientific reports, 9(1):112, 2019."
        },
        {
            "title": "Preprint",
            "content": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. doi: 10.1162/tacl 00475. URL https://aclanthology. org/2022.tacl-1.31/. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions, 2023. URL https://arxiv.org/abs/2212.10509. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. In International Conference on React: Synergizing reasoning and acting in language models. Learning Representations (ICLR), 2023a. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1022210240, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.632. URL https://aclanthology.org/2023.emnlp-main.632/. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Zihan Zhang, Meng Fang, Ling Chen, and Mohammad-Reza Namazi-Rad. CITB: benchmark for continual instruction tuning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 94439455, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.633. URL https://aclanthology.org/2023.findings-emnlp.633/."
        },
        {
            "title": "APPENDICES",
            "content": "Within this supplementary material, we elaborate on the following aspects: Appendix A: Implementation Details and Hyperparameters Appendix B: Detailed ToG-3 Algorithms Appendix C: Datasets Statistics and Details Appendix D: Baselines Details Appendix E: Evaluation Metrics Appendix F: More Experiment Results and Details Appendix G: Case Study for ToG-3 Appendix H: Graph Visualization Examples Appendix I: Theoretical Support for ToG-3 Appendix J: LLM Prompts"
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "We implement ToG-3 experiments with the following configuration: Data Processing: Chunk size is set to 1024 tokens with 20-token overlap between consecutive chunks to maintain contextual continuity. Multi-Agent hyperparameter: Constructor Agent extracts maximum of 2 knowledge triplets per chunk and employs hierarchical Leiden clustering (Traag et al., 2019) with maximum cluster size of 5 for community detection. Retriever Agent retrieves top-5 most relevant nodes using hybrid vector-graph similarity matching. Reflector/Responser Agent utilizes the top-5 retrieved passages as context for answer generation. Backend Infrastructure: LLM service is based on Qwen2.5-32B-Instruct (Yang et al., 2024) deployed with vLLM (Kwon et al., 2023) engine using bfloat16 precision and prefix caching enabled and greedy-search generation method, which is more stable than the Qwen3 model in mixed reasoning mode in our task; embeddings are generated using Jina-embeddings-v3 (1024-dimensional) (Sturua et al., 2024); Our server is equipped with 8 A100 40GB cards, AMD EPYC 256-core Processor, 2TB memory, and Ubuntu 20.04.1 system. and the hybrid vector-graph storage is implemented using Neo4j community edition 1 for efficient knowledge representation and retrieval, see Appendix.H for visualized graph example. TOG-3 ALGORITHMS Algorithms 1 and 2 present the two-stage pipeline of ToG-3. The first stage constructs heterogeneous graph index comprising chunks, triplets, and communities, while the second stage implements Multi-Agent Context Evolution and Retrieval (MACER) loop featuring novel dual-evolution mechanismEvolving Query and Evolving Subgraphthat dynamically refines both the query representation and the graph structure through iterative interaction."
        },
        {
            "title": "C DATASET DETAIL",
            "content": "This section presents comprehensive statistical overview of the Deep and Broad datasets we use in this paper, including detailed statistics metadata and licensing information, as summarized in Table 4. Additionally, we provide individual descriptions of each dataset to elucidate their respective characteristics and intended use cases. C.1 DEEP REASONING DATASETS HotpotQA (Yang et al., 2018): crowdsourced question answering dataset built on English Wikipedia, comprising approximately 113K questions. Each question is constructed to require the combination of information from the introductory sections of two Wikipedia 1https://neo4j.com/product/community-edition"
        },
        {
            "title": "Preprint",
            "content": "E {MENTIONEDIN(t, c)} i=1, lightweight LM Llight, encoder Eθ Algorithm 1 Offline Construction of Heterogeneous Index Graph Require: Corpus = {di}N Ensure: Heterogeneous graph = (V, E) 1: , 2: SplitIntoChunks(D) 3: 4: for each chunk do Tc Llight(c) 5: Tc 6: for each triplet Tc do 7: 8: 9: 10: end for 11: Ge BuildEntityCoOccurrenceGraph(T ) 12: {Mℓ}ℓ LeidenClustering(Ge) 13: for each community Mℓ do mℓ Llight(Mℓ) 14: {mℓ} 15: for each entity Mℓ do 16: 17: 18: 19: end for 20: Encode every node using Eθ 21: return = (V, E) {SUMMARYFOR(mℓ, e)} end for end for Sentence-level segmentation Extract semantic triplets (s, p, o, type s, type p, type o) is all triplets Generate community summary Unified dense encoding Algorithm 2 ToG-3: Multi-Agent Context Evolution and Retrieval (MACER) Loop Require: Query q, heterogeneous graph G, LLM L, max rounds Ensure: Final answer 1: 0, G0 Retriever(q, G) 2: H0 {(q, G0, init)} 3: repeat 4: 5: 6: 7: 8: 9: 10: 11: 12: until = 13: πfinal 14: return ak πresp(q, Gk, Hk) rk πsuff ref (q, Gk, ak) if rk = 1 then break end if πevolve ref Gk+1 πevolve Hk+1 Hk {(q + 1 (q, Gk) const (q k, ak, rk, Gk+1)} resp (q, Hk) k, Gk) Initial retrieval Initialize trajectory history Response Agent generates answer Reflector judges sufficiency Reflector evolves query Constructor evolves subgraph Synthesize answer from full trajectory articles for answering. The dataset provides two gold paragraphs per question, along with list of sentences identified as supporting facts necessary to answer the question. HotpotQA includes various reasoning strategies such as bridge questions (involving missing entities), intersection questions (e.g., what satisfies both property and property B?), and comparison questions (comparing two entities through common attribute). It is available in two settings: few-shot distractor setting where models are provided with 10 paragraphs including the gold ones, and an open-domain full-wiki setting where models must retrieve relevant passages from the entire Wikipedia corpus given only the question. 2WikiMultihopQA (Ho et al., 2020): multi-hop question answering dataset that contains complex questions requiring reasoning over multiple Wikipedia paragraphs. Each question is designed to necessitate logical connections across different pieces of information to arrive at the correct answer."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Statistics of Deep Reasoning and Broad Reasoning Datasets. Metrics abbreviations: Comp. (Comprehensiveness), Div. (Diversity), Emp. (Empowerment). Dataset Corpus Size Chunks Entities/Relations Communities Metrics License Deep Reasoning Tasks HotpotQA 2WikiMultihopQA Musique 9,809 6,119 11,254 Broad Reasoning Tasks CS Agriculture Legal Mix 10 12 94 9,812 6122 11,300 2,134 2,025 5,900 658 37,358/30,987 19,311/21,077 32,842/39,134 3,530/33,507 6,043/12,571 26,180/44,334 2,784/5,089 5,041 3,417 6,258 1,166 1,039 1,359 EM, F1 EM, F1 EM, F1 Apache-2.0 Apache-2.0 CC-BY-4.0 Comp., Div., Emp. Apache-2.0 Musique (Trivedi et al., 2022): challenging multi-hop QA dataset containing approximately 25K 24 hop questions, constructed by composing single-hop questions from five existing single-hop QA datasets. It is designed to feature diverse and complex reasoning paths, requiring models to integrate information from multiple hops to generate correct answers. The dataset emphasizes comprehensive evaluation of multi-step reasoning capabilities. C.2 BROAD REASONING DATASETS The following datasets are curated from the UltraDomain (Qian et al., 2025) benchmark. The benchmark construction leverages financial reports, legal contracts, and 428 college textbooks across 18 distinct domains to evaluate model versatility and adaptability in specialized and broad application scenarios: CS: Computer science domain focusing on data science, software engineering, and programming topics, requiring technical comprehension and analytical reasoning. Agriculture: Covers agricultural practices including beekeeping, crop production, and disease prevention, demanding domain-specific knowledge integration. Legal: Derived from legal contracts and documents, focusing on corporate legal practices, regulatory compliance, and governance, requiring precise interpretation of nuanced legal language. Mix: Contains diverse contexts from college textbooks spanning natural sciences, humanities, and social sciences, testing generalization capabilities across interdisciplinary topics."
        },
        {
            "title": "D BASELINES",
            "content": "This section presents the baseline methods evaluated in this paper, encompassing both classical algorithms such as NaiveRAG and GraphRAG, as well as recently proposed approaches including LightRAG, ToG-2, and HippoRAG-2. Baselines are as follows: NaiveRAG (Gao et al., 2023): standard chunk-based retrieval baseline that segments raw texts into chunks and stores them in vector database using text embeddings. For queries, it generates vectorized representations to directly retrieve text chunks based on semantic similarity. GraphRAG (Edge et al., 2024): graph-enhanced RAG system that utilizes an LLM to extract entities and relationships from text, representing them as nodes and edges. It generates community summaries through graph clustering and employs both local (entitybased) and global (community-based) retrieval strategies for comprehensive information access. LightRAG (Guo et al., 2024): graph-structured RAG framework that employs duallevel retrieval system combining low-level entity retrieval with high-level knowledge discovery. It integrates graph structures with vector representations for efficient retrieval of related entities and their relationships."
        },
        {
            "title": "Preprint",
            "content": "ToG-2 (Ma et al., 2024): knowledge graph-based framework implements tightcoupling hybrid RAG paradigm that iteratively retrieves information from both unstructured texts and structured knowledge sources. It alternates between graph retrieval and context retrieval for in-depth knowledge exploration. HippoRAG-2 (Gutierrez et al., 2025): non-parametric continual learning framework that leverages Personalized PageRank algorithm over an open knowledge graph constructed using LLM-extracted triples. It enhances multi-hop reasoning capabilities through sophisticated graph traversal and passage integration mechanisms."
        },
        {
            "title": "E METRICS",
            "content": "We employ different evaluation protocols for the two task categories: For Deep Reasoning Tasks, we follow standard QA evaluation practices as ToG (Sun et al., 2023; Ma et al., 2024) and HippoRAG (Gutierrez et al., 2024; 2025): Exact Match (EM): Measures the percentage of predictions that exactly match the ground truth answer. F1 Score: Computes word-level overlap between predictions and ground truth answers. For Broad Reasoning Tasks, we adopt multi-dimensional LLM-based evaluation approach due to the complexity and open-ended nature of these queries following LightRAG (Guo et al., 2024): Comprehensiveness (Comp.): Measures how thoroughly the answer addresses all aspects of the question. Diversity (Div.): Assesses the variety of perspectives and insights provided in the answer. Empowerment (Emp.): Evaluates how well the answer enables informed understanding and judgment. The LLM-based evaluation uses GPT-4o-mini as judge, with careful attention to prompt design and answer ordering to avoid positional bias. The LLM evaluation prompt is shown in Appendix.J"
        },
        {
            "title": "F MORE EXPERIMENT RESULTS AND DETAILS",
            "content": "This section presents extended experimental results, including detailed precision and recall metrics on Deep Reasoning tasks, as well as one-to-one win rates from Broad Reasoning tasks. The pairwise win rates are converted into unified ELO rating system, with the resulting ratings visualized in the heatmap shown in Figure 5. F.1 PRECISION AND RECALL RATE RESULTS Table 5 reveals the underlying reason for the relatively low F1 scores of GraphRAG and LightRAG: these methods are not specifically designed for deep reasoning tasks. By examining both precision/recall metrics and output cases, we observe that excessively long or unfocused responses tend to substantially reduce recall, thereby diminishing overall F1 performance. F.2 RESULT DETAIL IN BRAOD REASONING TASKS Table 6 presents the pairwise win rates (%) of baseline methods against ToG-3 across four datasets and four evaluation dimensions. The results demonstrate that ToG-3 consistently outperforms all compared baselines. F.3 ELO RATING CALCULATION FOR BROAD REASONING TASKS This appendix details the mathematical framework and computational process for deriving ELO ratings from pairwise comparison data across four benchmark datasets. The ELO rating system provides mathematically consistent approach to quantify relative performance differences between"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Comprehensive Evaluation Metrics of five RAG methods across three deep reasoning datasets. The best results of each dataset are marked in bold. Method HotpotQA 2WikiMultihopQA Musique P NaiveRAG 0.365 GraphRAG 0.011 0.013 LightRAG 0.012 MiniRAG 0.593 0.423 0.393 0.372 0.346 0.006 0.007 0.006 0.189 0.018 0.023 0.018 0.345 0.456 0.429 0.403 0.168 0.009 0.012 0.009 0.143 0.008 0.009 0.007 0.280 0.266 0.224 0.203 0.126 0.004 0.005 0.003 ToG0.516 0.595 0.454 0.267 0.485 0. 0.153 0.286 0.132 P: Precision, R: Recall. ToG-3 achieves best F1 while maintaining high precision-recall balance. Table 6: Win rates (%) of baselines v.s. ToG-3 across four datasets and four evaluation dimensions. The better results of each dataset are marked in bold. Metrics Agriculture Comprehensiveness Diversity Empowerment Overall Comprehensiveness Diversity Empowerment Overall Comprehensiveness Diversity Empowerment Overall NaiveRAG 28.4% 19.6% 29.4% 28.7% GraphRAG 46.8% 44.4% 25.2% 47.6% LightRAG 38.9% 32.0% 40.5% 39.6% ToG-3 71.6% 80.4% 70.6% 71.3% ToG-3 53.2% 55.6% 74.8% 52.4% ToG-3 61.1% 68.0% 59.5% 60.4% CS NaiveRAG 32.4% 32.0% 32.8% 33.3% GraphRAG 49.6% 48.4% 43.2% 49.2% LightRAG 45.6% 42.0% 46.0% 46.0% ToG67.6% 68.0% 67.2% 66.7% ToG-3 50.4% 51.6% 56.8% 50.8% ToG-3 54.4% 58.0% 54.0% 54.0% Legal NaiveRAG 12.4% 9.6% 12.4% 11.2% GraphRAG 49.6% 46.8% 29.6% 48.4% LightRAG 33.6% 28.0% 33.6% 32.4% ToG-3 87.6% 90.4% 87.6% 88.8% ToG-3 50.4% 53.2% 70.4% 51.6% ToG-3 66.4% 72.0% 66.4% 67.6% Mix NaiveRAG 34.8% 28.4% 38.8% 36.0% GraphRAG 51.6% 52.0% 38.4% 51.2% LightRAG 47.6% 39.2% 52.0% 49.6% ToG-3 65.2% 71.6% 61.2% 64.0% ToG-3 48.4% 48.0% 61.6% 48.8% ToG52.4% 60.8% 48.0% 50.4% HippoRAG-2 ToG-3 HippoRAG-2 ToG-3 HippoRAG-2 ToG-3 HippoRAG-2 ToG-3 Comprehensiveness Diversity Empowerment Overall 24.5% 18.8% 27.8% 25.6% 75.5% 81.2% 72.2% 74.4% 31.6% 28.0% 32.9% 32.0% 68.4% 72.0% 67.1% 68.0% 21.6% 17.2% 21.6% 20.4% 78.4% 82.8% 78.4% 79.6% 29.6% 23.6% 34.0% 31.6% 70.4% 76.4% 66.0% 68.4% retrieval-augmented generation methods. The ELO rating system transforms raw win rates into logarithmic scale that ensures transitive consistency in performance rankings. The core transformation is defined as follows: For given method with win rate wi against the reference method (ToG-3), the ELO rating difference is calculated as: Ri = 400 log10 (cid:19) 1 (cid:18) 1 wi The absolute ELO rating for method is then: where Rref = 1600 is the reference rating for ToG-3. Ri = Rref Ri The win probability between any two methods and with ratings Ri and Rj is given by: (i beats j) = 1 1 + 10(Rj Ri)/400 CASE STUDY FOR TOGThis section provides detailed case study of ToG-3 in deep reasoning task (Figure 6) and broad reasoning task (Figure 7 and Figure 8), offering an intuitive demonstration of the execution dynam-"
        },
        {
            "title": "Preprint",
            "content": "Question: What nationality is the performer of the song When The Stars Go Blue? Initial Evidence (Sub-Graph): When The Stars Go Blue -> performed by -> Ryan Adams Ryan Adams -> known for -> alternative country, rock, folk Ryan Adams -> also includes genre -> indie rock, Americana ... (no nationality information present) Evolving Query Decomposition 1 (Reflector Agent) Who is the performer of the song When The Stars Go Blue? Ryan Adams. Evolving Sub-Graph Refinement (Constructor Agent): Sammy Hagar -> is -> American rock vocalist Chuck Berry -> is -> American singer and songwriter Elvis Presley -> pioneered -> Rock and Roll (American origin) Ryan Adams -> nationality -> American Wee Willie Walker -> was -> American soul and blues singer [Added] Evolving Query Decomposition 2 (Reflector Agent) What is Ryan Adams known for besides the song When The Stars Go Blue? Ryan Adams is known for his work as singer-songwriter and musician, with musical style that encompasses alternative country, rock, and folk. Evolving Query Decomposition 3 (Reflector Agent) What is Ryan Adamss nationality? Ryan Adams is an American singer and musician. Final Answer (Response Agent): ryan adams is american. Ground Truth: america Explanation: The initial sub-graph contained no explicit information about Ryan Adams nationality. Through evolving query decomposition, the system identified missing link and triggered sub-graph refinement. New evidence from biographical triples (e.g., \"Ryan Adams -> nationality -> American\") was integrated, enabling definitive answer. Figure 6: Case Study: Evolving Query and Evolving Sub-Graph for Multi-Hop Reasoning. This example demonstrates how an initial knowledge sub-graph fails to answer complex question, prompting decomposition into sub-query. The Constructor Agent refines the sub-graph with additional biographical facts, enabling the Response Agent to generate the correct answer. The process highlights the dynamic, iterative nature of self-evolving context retrieval. ics of its dual-evolution mechanismcomprising Evolving Query and Evolving Subgraphacross multi-step reasoning processes."
        },
        {
            "title": "H GRAPH VISUALIZATION EXAMPLES",
            "content": "This section details two constructed graph used in our study: the 2WikiMultihopQA subset (exemplifying deep reasoning) and the computer science domain graph from UltraDomain (exemplifying broad reasoning), which are visualized with Neo4j community edition 2. 2WikiMultihopQA Dataset: Exemplar of Depth Reasoning 2WikiMultihopQA is designed to test depth reasoningthe ability to perform multi-step, sequential inference over entity-relation paths. Each question requires traversing at least two hops (e.g., first identifying persons profession, then linking that profession to historical event, and finally combining both to answer causal query). This structure forces models to engage in complex semantic chaining, where errors in early steps propagate, challenging robustness in long-range dependency handling. The datasets sparse yet densely connected knowledge graphs emphasize precision in step-by-step reasoning over 2https://neo4j.com/product/community-edition"
        },
        {
            "title": "Preprint",
            "content": "Question: How do different regression metrics (e.g., RMSE, MAE, R2) vary by model and dataset? Evidence (from ToG-3): Mean Squared Error -> is related to -> Root Mean Squared Error... Mean Squared Error -> is similar to -> Mean Absolute Error... summary 23 -> SUMMARY FOR -> RMSE.MSE emphasizes larger errors... summary 287 -> SUMMARY FOR -> MAE. ARIMA model performance evaluated using MAE... Answer 1(from ToG-3): Different regression metrics such as RMSE, MAE, and R2 can vary by model and dataset... RMSE penalizes large errors more heavily due to squared terms, while MAE is robust to outliers... R2 measures explained variance but can be misleading on small or overfitted datasets... Answer 2(from GraphRAG): The provided community summary does not detail specific variations...To evaluate how these metrics vary: 1. Train different regression models on your datasets... 2. Compute RMSE, MAE, R2 using MLlib.RegressionMetrics... 3. Compare performance across models and datasets... Evaluation Result (JSON): { \"Comprehensiveness\": { \"Winner\": \"Answer 1\", \"Explanation\": \"Answer 1 provides detailed explanation of various regression metrics...\" }, \"Diversity\": { \"Winner\": \"Answer 2\", \"Explanation\": \"Answer 2 presents different perspective by incorporating specific tools...\" }, \"Empowerment\": { \"Winner\": \"Answer 1\", \"Explanation\": \"Answer 1 empowers the reader by explaining what each metric means...\" }, \"Overall Winner\": { \"Winner\": \"Answer 1\", \"Explanation\": \"Answer 1 is the overall winner as it provides comprehensive understanding...\" } } Figure 7: Case Study: Comparing Regression Metrics Across Models and Datasets. This example illustrates how two reasoning systems answer technical ML question: GraphRAG emphasizes practical implementation (e.g., using Sparks MLlib), while ToG3 focuses on theoretical distinctions between RMSE, MAE, and R2. An evaluator selects the more comprehensive and empowering answer based on evidence from the knowledge graph."
        },
        {
            "title": "Preprint",
            "content": "Question: What policy changes are needed to foster equitable access to nutritious food? Evidence (from ToG-3): summary 216 -> SUMMARY FOR -> Food Security. **Supermarket and Food Security**: The reliance on supermarkets for food is viewed as threat... Victory Garden -> promoted -> Food Security... Supermarket -> threatens -> Food Security...\" White House Childhood Obesity Task Force -> recommended -> Farm to School... Food Works -> collaborates with -> Sauvie Island Organics... Michael Taylor -> interprets -> Food Modernization Act... background in representing Monsanto... Answer 1(from ToG-3): policy changes that could foster equitable access to nutritious food may include implementing subsidies... increasing funding for farm to school programs in underserved communities... establishing regulations that prioritize the availability of nutritious options in school meal programs... and creating educational initiatives to promote awareness about nutrition and healthy eating habits... these changes could help address disparities in access to nutritious food... Answer 2(from GraphRAG): 1. **strengthen conflict of interest policies**: prevent individuals with significant industry ties... 2. **enhance transparency and accountability**: implement transparent processes... 3. **support diverse food producers**: promote variety of food producers... 4. **expand nutrition education and assistance programs**: enhance nutrition education... 5. **ensure regulatory independence**: guarantee that regulatory bodies are independent... 6. **promote local food systems**: encourage local food production through grants, subsidies... 7. **support community gardens and urban agriculture**: provide funding and resources... 8. **promote sustainable farming practices**: offer incentives for sustainable farming... These measures aim to create more resilient and sustainable food system... Evaluation Result (JSON): { { \"Comprehensiveness\": { \"Winner\": \"Answer 2\", \"Explanation\": \"Answer 2 provides more detailed and structured set of policy changes... \" \"broader scope than Answer 1...\" }, \"Diversity\": { \"Winner\": \"Answer 2\", \"Explanation\": \"Answer 2 presents wider variety of perspectives... \" \"including regulatory, community, and economic solutions.\" }, \"Empowerment\": { \"Winner\": \"Answer 1\", \"Explanation\": \"Answer 1 empowers readers by offering comprehensive overview...\" \"facilitates informed judgment.\" }, \"Overall Winner\": { \"Winner\": \"Answer 2\", \"Explanation\": \"Answer 2 emerges as the overall winner due to its superior\" \"comprehensiveness, diversity, and empowerment.\" } } Figure 8: Case Study: Policy Recommendations for Equitable Food Access. This example illustrates the full reasoning pipeline: complex policy question is answered by two different systems (GraphRAG and ToG3), supported by retrieved knowledge snippets. An evaluator then compares both responses across multiple dimensions, selecting the more comprehensive, diverse, and empowering answer as the winner."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Structural overview of the 2WikiMultihopQA subset, exemplifying depth reasoning through multihop entity-relation paths (e.g., traversing person profession historical event to answer causal queries). Figure 10: Visualization of the computer science domain graph in UltraDomain, showcasing breadth reasoning via diverse node types (e.g., programming languages like Scala/Spark, frameworks like HDFS/Kafka) and relationship types (e.g., implements, runs on, contains). surface-level pattern matching. structural overview highlighting its multi-hop nature is shown in Figure 9. Computer Science Domain Graph in UltraDomain: Exemplar of Breadth Reasoning The computer science domain graph from UltraDomain represents breadth reasoningfocused on expansive coverage of concepts and their interrelations. It includes wide range of CS entities (from foundational data structures/algorithms to applied distributed systems/cloud services) and diverse relationship types (e.g., implements, runs on, contains). This breadth challenges models to navigate large, heterogeneous concept space, where connections span disparate subfields (e.g., linking programming language to database, or an algorithm to hardware). For instance, understanding how Spark relates to Hadoop, Kafka, and multiple programming languages requires integrating knowledge across multiple domains, reflecting the need for broad, cross-concept awareness. visualization of this graph, illustrating its extensive node and edge diversity, is provided in Figure 10. THEORETICAL SUPPORT: IMPLICIT DYNAMICS OF IN-CONTEXT"
        },
        {
            "title": "LEARNING",
            "content": "The iterative refinement process in MACER and dual-evolving mechanism is not merely heuristic but possesses theoretical grounding through the lens of implicit in-context learning dynamics. Recent work by (Dherin et al., 2025) demonstrates that transformer-based models can perform incontext learning by implicitly modifying their MLP weights through attention mechanisms. We extend this theoretical framework to explain the convergence properties of our multi-agent reasoning process. Implicit Weight Updates via Attention Dynamics The trajectory history Hk serves as an incontext prompt that induces implicit low-rank updates to the frozen LLMs parameters. Specifically,"
        },
        {
            "title": "Preprint",
            "content": "for transformer module with MLP layer weights , the context Hk generates an implicit weight update Wk through the attention mechanism: Wk = (W Ak)A(q) A(q)2 , where Ak = A(Hk, q) A(q). (7) Here, A() denotes the activation pattern from the attention layer, A(q) represents the baseline activation without context, and A(Hk, q) captures the contextualized activation with the full reasoning history. The term Ak quantifies the information injected by the evolving context Hk. The lowrank nature of Wk ensures efficient and targeted parameter updates without catastrophic forgetting of pre-trained knowledge. MDP Policy as an Implicit Function of Context Recall from Section 3.3 that the Reflector Agents policy πref maps states sk = (q, Gk, Hk) to actions (sub-queries or STOP). Under the implicit learning view, πref is not fixed network but an emergent policy πk shaped by Wk. Thus, the sequence {πk}K k=1 constitutes trajectory of implicitly adapted policies driven by the evolving context Hk. Convergence via Regret Minimization We analyze convergence through the lens of episodic regret minimization in the MDP = (S, A, P, r). Let π denote the sk value of policy π at state sk, and let sk be the optimal value. The cumulative regret sk over steps is: i=k γikri sk = maxπ π (cid:104)(cid:80)K = Eπ (cid:105) R(K) = (cid:88) k=1 (cid:0)V sk πk sk (cid:1) . (8) We establish sublinear regret growth R(K) = o(K) under the following mild assumptions: Assumption 1 (Realizability). There exists policy π such that Suff(q, sentable by the implicit policy class induced by in-context prompts of the form (H; q). Assumption 2 (Bounded Gradient Norm). The implicit gradient direction gk, defined as the rewardsensitive update signal from Hk, satisfies gk for some constant > 0. ) = 1, and π is repreUnder these assumptions, the following properties hold: Property 1 (Smooth Policy Evolution). The value function evolves smoothly with respect to implicit updates: πk+1 πk Lgk + O(gk2), for some Lipschitz constant > 0, ensuring stable policy transitions. (9) Property 2 (Expected Policy Improvement). Each refinement step yields non-negative expected improvement: (cid:2)V πk+1 sk πk sk Hk (10) where η > 0 and {σk} is martingale difference sequence with E[σk Hk] = 0. This follows from the fact that evolving sub-queries generated by the Reflector target knowledge gaps, and the Constructors evolving graph refinement increases the likelihood of sufficiency. (cid:3) ηgk2 σk, Property 3 (Vanishing Implicit Gradient). As the context becomes increasingly informative, the room for improvement diminishes: lim gk = 0 almost surely. (11) This is guaranteed by Assumption 1 (Realizability) and the finite horizon K, which ensures the process either reaches sufficient subgraph (rk = 1) or exhausts its budget. Together, these properties imply that the sequence {πk} converges to policy π satisfying π s1 s1 MACER reliably converges to sufficient context ϵ for arbitrarily small ϵ > 0 as . In practice, with reasonable horizon (e.g., = 3), for faithful answer synthesis."
        },
        {
            "title": "Preprint",
            "content": "This analysis establishes that the MACER loop performs an implicit form of policy gradient ascent on the reward landscape defined by context sufficiency, with convergence guarantees rooted in stochastic approximation theory and in-context learning dynamics, providing rigorous foundations for the empirical effectiveness of our reward-based evolving context mechanism."
        },
        {
            "title": "J PROMPT TEMPLATES",
            "content": "Our framework employs multi-stage, prompt-driven reasoning pipeline that integrates structured knowledge graph (KG) extraction, community-based summarization, iterative sub-query decomposition, sub-graph refinement, and faithful answer synthesis. Each stage is governed by specialized prompt template designed to ensure modularity, interpretability, and factual consistency. The complete sequence of prompts is as follows: 1. KG Triplets Extraction: As shown in Figure 11, given raw textual input, this prompt instructs the model to extract structured subject-relation-object triples (e.g., entity1 -> relation -> entity2) to construct fine-grained knowledge sub-graph. This step transforms unstructured text into queryable graph structure. 2. Generate Community Summary: As shown in Figure 12, based on densely connected sub-graphs (communities), this prompt synthesizes concise natural language summary that captures the core themes and relationships within each community, enabling high-level semantic indexing and retrieval. 3. Keyword Expansion for Retrieval Augmentation: As shown in Figure 13, to improve recall in the querying phase, this prompt generates set of synonyms and related terms from the original query, considering variations in capitalization, pluralization, and common phrasings, separated by delimiter symbols. 4. Evolving Sub-Query Decomposition: As shown in Figure 14, for complex multi-hop questions, this prompt recursively decomposes the current query into simpler, contextanswerable sub-questions, guided by previously retrieved information and reasoning traces, enabling stepwise information gathering. 5. Evolving Sub-Graph Refinement: As shown in Figure 15, this prompt cleans and enhances the retrieved or extracted sub-graph by removing irrelevant triples, normalizing entity names, and optionally filling in strongly supported missing links, thereby improving the signal-to-noise ratio for downstream reasoning. 6. Final Answer Synthesis: As shown in Figure 16, in the final stage, the model generates concise, context-grounded answer using only the refined evidence, with explicit instructions to avoid hallucination or reliance on prior knowledge. If the answer cannot be determined, it returns Unknown to maintain factual integrity. These prompts work in concert to enable structured, interpretable, and reliable reasoning over hybrid text-and-graph knowledge sources. And Figure 17 shows the LLM evaluation prompt in the broad reasoning task. Their modular design allows for independent tuning and auditing, making the overall system transparent and robust to noise and ambiguity."
        },
        {
            "title": "Preprint",
            "content": "-GoalGiven text document, identify all entities and their entity types from the text and all relationships among the identified entities. Given the text, extract up to {max knowledge triplets} entity-relation triplets. -Steps1. Identify all entities. For each, extract: entity name entity type entity description 2. Identify all related (source, target) pairs. For each, extract: source entity target entity relation relationship description 3. Output valid JSON only: { \"entities\": [...], \"relationships\": [...] } of Relativity\", \"entity name\": -An Output Example- { \"entities\": [ { \"entity name\": \"Albert Einstein\", \"entity type\": \"Person\", \"entity description\": \"...\" }, Theory\", \"Theory { \"entity description\": \"...\" }, { \"entity name\": \"Nobel Prize in Physics\", \"entity type\": \"Award\", \"entity description\": \"...\" } ], \"relationships\": [ { \"source entity\": \"Albert Einstein\", \"target entity\": \"Theory of Relativity\", \"relation\": \"developed\", \"relationship description\": \"...\" }, { \"source entity\": \"Albert Einstein\", \"target entity\": \"Nobel Prize in Physics\", \"relation\": \"won\", \"relationship description\": \"...\" } ] } \"entity type\": \"Scientific -Real Data- #################### text: {text} #################### output: ; Figure 11: KG Triplets Extraction Prompt Template. The template provides structured instructions for extracting entities and relationships from text, with clear formatting for both input requirements and JSON output format. role=\"system\" You are provided with set of relationships from knowledge graph, each represented as entity1 -> entity2 -> relation -> relationship description. Your task is to create summary of these relationships. The summary should include: Names of the entities involved, concise synthesis of the relationship descriptions. The goal is to capture the most critical and relevant details that highlight the nature and significance of each relationship. Ensure the summary is coherent and integrates information to emphasize key aspects. Avoid redundancy and maintain clarity. role=\"user\" #################### text: {community info} #################### assistant: % Generated summary based on {community info} will appear here. Figure 12: Community Summary Template. This template provides structured instructions for extracting entities and relationships from text, with clear formatting for input specifications and expected JSON-like output format."
        },
        {
            "title": "Preprint",
            "content": "role=\"system\" Given some initial query, generate synonyms or related keywords up to {max keywords} in total, considering possible cases of capitalization, pluralization, common expressions, etc. Provide all synonyms/keywords separated by ˆ symbols: keyword1ˆkeyword2ˆ.... Note: result should be in one line, separated by ˆ symbols. role=\"user\" ---- QUERY: {query str} ---- assistant: % Example: KEYWORDS: machine learningˆML learning machinesˆAI modelsˆneural networksˆdeep learning ... Figure 13: Keyword Expansion Prompt Template. This template instructs the model to generate up to {max keywords} synonyms or related terms for given query, formatted as single line separated by ˆ symbols. role=\"system\" The original question is as follows: {query str} We have an opportunity to answer some, or all of the question from knowledge source. Context information for the knowledge source is provided below, as well as previous reasoning steps. Given the context and previous reasoning, return question that can be answered from the context. This question can be the same as the original question, or represent subcomponent. It should not be irrelevant to the original question. If no further information can be extracted, return None. Examples: Question: How many Grand Slam titles does the winner of the 2020 Australian Open have? Knowledge source context: Provides names of the winners of the 2020 Australian Open Previous reasoning: None Next question: Who was the winner of the 2020 Australian Open? Question: How many Grand Slam titles does the winner of the 2020 Australian Open have? Knowledge source context: Includes biographical info for each winner Previous reasoning: - Who was the winner of the 2020 Australian Open? - The winner was Novak Djokovic. Next question: How many Grand Slam titles does Novak Djokovic have? Current Input: Question: {query str} Knowledge source context: {context str} Previous reasoning: {prev reasoning} assistant: % Output: <decomposed sub-question> OR None Figure 14: Step-wise Query Evolution and Decomposition Prompt Template. This template guides the model to recursively break down complex question into answerable sub-questions based on available context and prior reasoning, enabling multi-hop reasoning over knowledge sources."
        },
        {
            "title": "Preprint",
            "content": "role=\"system\" You are given sub-graph extracted from knowledge graph, represented as list of triples: entity1 -> relation -> entity2. This sub-graph may contain irrelevant, redundant, or incomplete information. Your task is to refine the sub-graph by: Removing irrelevant or noisy triples not related to the query, Filling in missing but inferable relationships (if strongly supported), Ensuring entity names are normalized (e.g., consistent capitalization, singular/plural). Return the refined sub-graph in the same triple format, one per line. If no refinement is needed, return the original sub-graph. If all triples are irrelevant, return None. Example Input: Query: What are the major achievements of Marie Curie? Sub-graph: Marie Curie -> won -> Nobel Prize in Physics Marie Curie -> born in -> Warsaw Marie Curie -> spouse -> Pierre Curie Apple Inc. -> founded by -> Steve Jobs Refined Output: Marie Curie -> won -> Nobel Prize in Physics Marie Curie -> won -> Nobel Prize in Chemistry Marie Curie -> spouse -> Pierre Curie (Note: Added Chemistry prize based on strong prior knowledge; removed birthplace and unrelated Apple fact) Current Input: Query: {query str} Sub-graph: {subgraph triples} assistant: Figure 15: Sub-Graph Evolution and Refinement Prompt Template. This template guides the model to clean, complete, and normalize noisy or incomplete knowledge sub-graph in response to given query, improving its relevance and coherence for downstream reasoning. role=\"system\" Context information is provided below. You must answer the query using only this context, and not any prior knowledge. Do not make assumptions or add information not present in the context. If the answer cannot be determined from the context, respond with Unknown. --------------------- {context str} --------------------- Query: {query str} Instructions: Extract or synthesize the answer strictly from the provided context. Keep the answer concise and factual. Avoid phrases like The context states that. . . \" just give the answer. assistant: % Final answer derived solely from context. Figure 16: Final Answer Synthesis Prompt Template. This template enforces faithful response generation based exclusively on retrieved context, core principle in Retrieval-Augmented Generation (RAG) systems. It suppresses model hallucination by explicitly forbidding the use of prior knowledge."
        },
        {
            "title": "Preprint",
            "content": "role=\"system\" You are an expert tasked with evaluating two answers to the same question based on three criteria: Comprehensiveness, Diversity, and Empowerment. Evaluation Criteria: Comprehensiveness: How much detail does the answer provide to cover all aspects and sub-questions implied by the original query? Diversity: How varied and rich is the answer in providing different perspectives, evidence sources, or reasoning paths? Empowerment: How well does the answer help the reader understand the topic and make informed judgments or decisions? Instructions: Compare Answer 1 and Answer 2 for each criterion. Choose the better answer and explain why. Select an overall winner based on balance across all three. Input: Question: {query} Answer 1: {answer1} Answer 2: {answer2} Output Format (JSON): { \"Comprehensiveness\": { \"Winner\": \"Answer 1 or Answer 2\", \"Explanation\": \"...\" }, \"Diversity\": { \"Winner\": \"Answer 1 or Answer 2\", \"Explanation\": \"...\" }, \"Empowerment\": { \"Winner\": \"Answer 1 or Answer 2\", \"Explanation\": \"...\" }, \"Overall Winner\": { \"Winner\": \"Answer 1 or Answer 2\", \"Explanation\": \"...\" } } Figure 17: Answer Evaluator Prompt Template. This template guides dedicated agent to compare two candidate responses along three dimensions: comprehensiveness, diversity, and empowerment, promoting highquality, informative, and user-centered answer selection in multi-agent systems."
        }
    ],
    "affiliations": [
        "DataArc Tech Ltd.",
        "Hithink RoyalFlush Information Network Co., Ltd",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "IDEA Research, International Digital Economy Academy"
    ]
}