{
    "paper_title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
    "authors": [
        "Qianhui Wu",
        "Kanzhi Cheng",
        "Rui Yang",
        "Chaoyun Zhang",
        "Jianwei Yang",
        "Huiqiang Jiang",
        "Jian Mu",
        "Baolin Peng",
        "Bo Qiao",
        "Reuben Tan",
        "Si Qin",
        "Lars Liden",
        "Qingwei Lin",
        "Huan Zhang",
        "Tong Zhang",
        "Jianbing Zhang",
        "Dongmei Zhang",
        "Jianfeng Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 3 4 1 3 0 . 6 0 5 2 : r GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents Qianhui Wu*1 Kanzhi Cheng*2 Rui Yang*3 Chaoyun Zhang1 Jianwei Yang1 Huiqiang Jiang1 Jian Mu2 Baolin Peng1 Bo Qiao1 Reuben Tan1 Si Qin1 Lars Liden1 Qingwei Lin1 Huan Zhang3 Tong Zhang3 Jianbing Zhang2 Dongmei Zhang1 Jianfeng Gao1 1Microsoft 2Nanjing University 3University of Illinois Urbana-Champaign"
        },
        {
            "title": "Abstract",
            "content": "One of the principal challenges in building VLM-powered GUI agents is visual groundinglocalizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment due to lack of explicit spatial supervision; inability to handle ambiguous supervision targets, as singlepoint predictions penalize valid variations; and mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in single forward pass. In line with this, we further design grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B achieves scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones, outperforming UI-TARS72B (38.1) on ScreenSpot-Pro, with significantly fewer parameters and training data. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths. Project page: https://aka.ms/GUI-Actor."
        },
        {
            "title": "Introduction",
            "content": "With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), there is increasing interest in building GUI (Graphical User Interface) agents that understand natural language instructions and autonomously interact with software interfaces across platforms such as desktops [1, 2], mobile devices [3], and web applications [4]. Effective GUI agents require two core capabilities: (i) multimodal perception to interpret visual and linguistic cues, and (ii) action *Equal contribution: qianhuiwu@microsoft.com, chengkz@smail.nju.edu.cn, ry21@illinois.edu. Leadership. Preprint. Figure 1: Left: Model performance vs. training data scale on the ScreenSpot-Pro benchmark. Higher and more left is better; larger points indicate models with more parameters. We only show GUI-Actor models built upon Qwen2-VL here for fair comparison. With Qwen2.5-VL as the backbone, GUIActor-3B/7B reaches scores up to 42.2/44.6 (without Verifier). Right: Illustration of action attention. GUI-Actor grounds target elements by attending to the most relevant visual regions. execution to interact with digital environments via mouse, keyboard, or touchscreen [5, 6]. While early systems relied on structured metadata (e.g.HTML, DOM trees, or view hierarchies) [1], such data is often noisy, inconsistent, or unavailable across platforms. Recent work thus emphasizes visual GUI agents that perceive interfaces directly from rendered screenshots, akin to human users [7]. central challenge in this paradigm is visual grounding: mapping natural language plans to screen regions. Most existing methods treat this as coordinate generation task, producing screen positions (e.g.x=0.125, y=0.23) through the same text generation mechanisms used by LLMs [8]. However, representing GUI actions through coordinate generation, where models output screen positions as text tokens (e.g., x=..., y=...) introduces several intrinsic limitations. First, spatialsemantic alignment is weak: generating discrete coordinate tokens requires the model to implicitly map visual inputs to numeric outputs via language modeling head, without any explicit spatial inductive bias. This process is inefficient, data-intensive, and prone to errors due to the lack of direct supervision linking visual features to action locations. Second, supervision signals are ambiguous: many GUI actions, such as clicking within button, allow for range of valid target positions. However, coordinate-based methods typically treat the task as single-point prediction, penalizing all deviationseven reasonable onesand failing to capture the natural ambiguity of human interaction. Finally, there is granularity mismatch between vision and action space: while coordinates are continuous and high-resolution, vision models like Vision Transformers (ViTs) [9] operate on patchlevel features. This mismatch forces the model to infer dense, pixel-level actions from coarse visual tokens, which undermines generalization to diverse screen layouts and resolutions. Although some recent approaches [10] attempt to enrich spatial grounding by predicting bounding boxes instead of single points, they still represent these regions as raw coordinate strings (e.g.x_min, y_min, x_max, y_max) that are detached from the visual features. Without architectural components such as ROI pooling [11] or spatial attention mechanisms [12], such methods fall short of bridging the gap between linguistic intent and grounded visual action. Rethinking how humans interact with digital interfaces reveals key insight: humans do not calculate precise screen coordinates before actingthey perceive the target element and interact with it directly. Motivated by this observation, we propose GUI-Actor, VLM augmented with an attention-based action head, enabling coordinate-free visual grounding that more closely mimics human behavior. Unlike prior methods that treat action grounding as coordinate prediction task, GUI-Actor learns to attend directly to relevant visual regions without relying on numeric coordinate generation. At the core of GUI-Actor is dedicated <ACTOR> token, which encodes the grounding context by jointly processing visual input and natural language instructions. An attention mechanism then learns to align this token with the most relevant GUI regions by attending over visual patch tokens from the screenshot. The resulting attention map naturally identifies actionable regions on the interface. To address the inherent ambiguity in GUI interactions, where multiple points within UI element (e.g.a button) may all be valid, GUI-Actor is trained using multi-patch supervision. All visual 2 patches overlapping with ground-truth bounding boxes are labeled as positives, while others are treated as negatives. This supervision strategy allows the model to tolerate spatial ambiguity and reduces over-penalization of reasonable action variations. Furthermore, because GUI-Actor grounds actions directly at the vision backbones native spatial resolution, it avoids the granularity mismatch of previous methods and generalizes more robustly across different screen sizes, resolutions, and layouts. Finally, to support decision refinement, we further enhance GUI-Actor by presenting lightweight grounding verifier that evaluates multiple candidate regions and selects the most plausible one for action execution. Our contribution can be summarized as follows: 1. We revisit recent coordinate generation-based approaches for visual grounding in GUI agents, identify their limitationssuch as weak spatial-semantic alignment, ambiguous supervision targets, and mismatched feature granularityand propose GUI-Actor, novel coordinate-free method that effectively addresses these issues. 2. We design an attention-based action head, which can generate multiple candidate regions in single forward pass, offering flexibility for downstream modules such as search strategies. 3. We introduce grounding verifier to select the most likely action region among the candidates proposed from the action attention map. We show that this verifier can be easily integrated with other grounding methods for further performance boost. 4. Extensive experiments demonstrate that GUI-Actor outperforms the state-of-the-art methods trained on similar scale of data across multiple GUI action grounding benchmarks, and exhibits greater robustness to unseen screen sizes and resolutions. Remarkably, the 2B version of GUI-Actor even surpasses several competing 7B models. Furthermore, by leveraging the verifier, GUI-Actor with lightweight training (i.e., freezing the backbone LLM and fine-tuning only the newly introduced 100M parameters in the action head) can effectively equip the underlying VLM with grounding capabilities without compromising its general-purpose strengths."
        },
        {
            "title": "2 Related Work",
            "content": "LLM/VLM-Powered GUI Agents. The advent of LLMs and VLMs has catalyzed the development of GUI agents that can understand natural language instructions and perform complex tasks across mobile [13], web [14, 15], and desktop environments [16, 1, 2]. Early research focused on designing autonomous agent frameworks [4, 17, 18] that prompt commercial models to interact with operating systems via code generation [19, 20] or tool use [21, 22]. With rising demand for open-source and customizable agents, parallel line of work focuses on training LLMs/VLMs for enhanced agentic capabilities, including GUI understanding, planning, and execution [7, 10, 23, 24]. The key to these efforts is collecting GUI-specific training data, such as OCR annotations [25], interface summaries [26], QA pairs [23], and large-scale task demonstrations [15, 2732]. central requirement of agent development is the ability to interact with realistic GUI environments deployed in virtual machines and Chrome-based browsers. While early agents operated over structured metadata like HTML or accessibility trees [14, 33], such representations are brittle and inconsistent across platforms [34, 7]. Consequently, recent trends have shifted toward vision-centric paradigm, where agents interact with raw screenshots using mouse and keyboard inputs [35, 36], closely mimicking human behavior. Within this setting, central challenge emerges: grounding natural language instructions to specific GUI regions, referring to as GUI Visual Grounding. GUI Visual Grounding. Given GUI screenshot and natural language instruction, GUI visual grounding aims to locate the target region for interaction. Although conceptually related to grounding in natural images, this task presents unique challenges due to the semantic density and structural regularity of GUI layouts [34, 8]. common approach frames GUI grounding as text-based coordinate prediction task, where models generate point positions (e.g., x=..., y=...) as output language tokens [37, 38]. This formulation has led to widespread adoption due to its simplicity and compatibility with existing LLMs/VLMs. To improve performance, prior works have scaled both models and training data [3943, 10, 44]. UGround [8] proposes data pipeline for synthesizing diverse GUI grounding examples, while OS-Atlas [45] offers multi-platform dataset and unified GUI action model. More recently, Xu et al. [46] introduced training-free approach that performs GUI grounding by leveraging the internal attention of VLMs. 3 (a) Illustration of attention-based action head. (b) Image Patch Labels Figure 2: Overview of GUI-Actor. (a) Illustration of how the action head works with VLM for coordinate-free GUI grounding. (b) Illustration of the spatial-aware multi-patch supervision for model training. We label all image patches that are partially or fully covered by the ground-truth bounding box as positive (1) and all others as negatives (0). Despite their success, coordinate-based methods suffer from key limitations, including weak spatial inductive bias, ambiguous point supervision, and resolution mismatches between visual features and action targets. This paper presents compelling alternative to the prevailing coordinate-based method: GUI-Actor, novel coordinate-free grounding framework for GUI agents. It introduces an <ACTOR> token that attends directly to relevant image patches via an attention-based action head, enabling more human-like grounding while mitigating the limitation of coordinate-based methods."
        },
        {
            "title": "3 The Design of GUI-Actor",
            "content": "Considering the limitations of text-based coordinate generation, e.g., weak spatial-semantic alignment and ambiguous supervision targets, we draw inspiration from how humans interact with GUIs. Rather than computing precise coordinates, humans typically visually identify the intended element and then directly act on it, by tapping with finger or positioning mouse cursor. Motivated by this, GUI-Actor explores novel architecture for GUI visual grounding: we first introduce special token <ACTOR> as the contextual anchor, and then train an action attention head to attend from this anchor to image patches corresponding to the target element. Finally, we present grounding verifier to select the most semantically appropriate target among the multiple candidates derived from the attention map. <ACTOR> Token as Contextual Anchor Given screenshot image and an instruction q, coordinate generation based methods typically train the VLM to produce sequence of {x1:i1, xi:i+m, xi+m+1:j1, xj:j+n, xj+n+1:N }, where m, > 0, is the total length of the output sequence, > 1, and 1 > + + 1. For example, in pyautogui.click(x=0.123, y=0.234) Xu et al. [7], the segments xi:i+m and xj:j+n correspond to the tokenized xand y-coordinates, respectively. The segment xi+m+1:j1 represents the separator between them, while the rest captures the surrounding context. In this work, we replace the coordinate span {xi:i+m, xi+m+1:j1, xj:j+n} with three special tokens to facilitate coordinate-free grounding and better context integration from both the visual input and textual instruction: VLM(I, q) = {x1:i1, <ACTOR_START>, <ACTOR>, <ACTOR_END>, xi+3:N }. (1) We use the final-layer hidden state of <ACTOR>, i.e., h<ACTOR>, as contextual anchor for computing action attention over the visual patch tokens. Attention-Based Action Head Let v1, . . . , vM denote the visual patch features extracted by the Vision Encoder of the VLM from the input screenshot, where each vi Rd. The action head computes an attention distribution from the <ACTOR> token over these visual patches to identify the target action region. 4 To incorporate GUI-aware contextual information, we first apply self-attention layer over the visual patch features. This allows the model to aggregate semantics across spatially related patches, enabling patches that belong to the same GUI element to share coherent representations: v1, . . . , vM = SelfAttn(v1, . . . , vM ), (2) where vi Rd denotes the contextualized feature for the i-th patch after self-attention module. Next, we project both the <ACTOR> token representation h<ACTOR> and the contextualized patch features v1, . . . , vM into shared embedding space via two separate MLPs, and obtain and zi Rd: = MLPT(h<ACTOR>), zi = MLPV(vi). (3) Finally, we compute attention scores between the <ACTOR> token and each visual patch. Let denote the total number of image features that are input to the LM backbone, the resulting attention weights a1, . . . , aM form an attention map over the screen, indicating the most relevant region for grounding the action. αi = zzi , ai = exp(αi) j=1 exp(αj) (cid:80)M , for = 1, . . . , M. (4) Spatial-Aware Multi-Patch Supervision key advantage of our approach is the ability to leverage dense and spatially structured learning signals from bounding-box supervision. Rather than relying on single, potentially ambiguous click point as in traditional coordinate-based methods, GUI-Actor treats all image patches that are partially or fully covered by the ground-truth bounding box as positive examples (yi=1) and all others as negatives (yi = 0), where yi denotes the label associated with vi. This allows the model to more effectively capture the full spatial extent of actionable elements. An illustration is provided in Figure 2b. For more details, please refer to Appendix B. We train the model using combination of next-token prediction (NTP) loss and action attention loss: The action attention loss is defined as: = LNTP + LAction_Attn, LAction_Attn = (cid:88) i= pi log pi ai , pi = yi j=1 yj + ϵ (cid:80)M , = 1, . . . , M, (5) (6) where ϵ is small constant for numerical stability."
        },
        {
            "title": "4 Grounding Verifier",
            "content": "A key advantage of our attention based action grounding model is its ability to produce multiple candidate action regions in single forward pass, without incurring additional inference cost. This is natural consequence of the attention-based design, where the model assigns scores to all visual patches simultaneously. This efficiency creates an opportunity: rather than relying solely on the top-scoring patch, we can introduce lightweight verification step to select the most semantically appropriate target among the candidates. With the insight that verification is often easier than generation [47], we propose Grounding Verifier, lightweight VLM module which takes as input the instruction and screenshot with visual marker placed at the proposed location, and predicts whether the marked region correctly fulfills the task intent. This verifier serves as decision refinement layer, allowing the system to reflect on its action before execution. Data & Training Training data for the verifier is constructed from the OS-Atlas dataset [45], which spans desktop, mobile, and web domains. This dataset provides triplets of the form (image, query, bounding box), where each image is associated with multiple queries and their corresponding bounding boxes. For each triplet, we generate positive example by placing visual marker (i.e., hollow red circle) at the center of the bounding box, treating it as the correct grounding point for the given query. To create negative examples, we apply two strategies: (1) selecting the 5 center of an incorrect bounding box from the same image; (2) randomly sampling point outside the target region. Each resulting training instance is formatted as tuple (I, x, y), where is an image with marked point, is the corresponding language instruction, and {True, False} is the ground-truth label indicating whether the point correctly satisfies the instruction. More details are deferred to Appendix F.1. We fine-tune the verifier from lightweight VLM using standard supervised learning. The model takes (I, x) as input and is trained to generate the correct token y. The training objective is the cross-entropy loss: where Pθv denotes the output probability from the verifier model with parameters θv. LVerifier = log Pθv (y I, x), Inference At inference time, GUI-Actor predicts the final action location by combining natural language generation with visual grounding. Given the current GUI state and user instruction, GUI-Actor first generates an agent response via standard decoding, for example, producing string like pyautogui.click(<ACTOR_START><ACTOR><ACTOR_END>) that includes the special <ACTOR> token. We then extract the hidden state corresponding to <ACTOR> and use the action head to compute attention over all visual patches. This attention distribution serves as spatial activation map, identifying the most relevant screen region for executing the predicted action. To identify the most semantically appropriate region among the top-K attention-weighted patches, we use the verifier θv to score each candidate by marking it on the image and evaluating its alignment with the instruction x. For each marked image, the verifier outputs probabilities for True and False tokens, and we define the selection score as: s(I, x) = Pθv (True I, x) Pθv (True I, x) + Pθv (False I, x) . (7) Candidates are evaluated in descending order of attention weights, and we return the first one exceeding confidence threshold (e.g., s(I, x) > γ) without further evaluation."
        },
        {
            "title": "5 Experiments",
            "content": "Implementation Details We implement GUI-Actor using PyTorch and Huggingface Transformers. Unless otherwise specified, we adopt Qwen-2-VL-7B-Instruct [38] as the backbone VLM for both GUI-Actor and the baselines to ensure fair comparison with previous state-of-the-art methods. For the re-implementation of baseline Aguvis-7B with both point supervision (point sup.) and bounding-box supervision (bbox sup.), we directly use the official source code provided by Aguvis [7]. We use the same dimensionality as the backbone VLM for all configurations of the action head. The grounding verifier is finetuned from UI-TARS-2B-SFT [10]. During inference, we construct pool of = 20 candidates and apply confidence threshold of γ = 0.95 for tasks in ScreenSpot-Pro and γ = 0.8 for ScreenSpot and ScreenSpot-v2. Following Aguvis [7], we structure our training data as sequences of pyautogui-style operations, but replace the original coordinates with the special tokens, as described in Section 3. Our full training recipe is built from several public GUI datasets, comprising 1M screenshots. Both GUI-Actor and the two baseline models are trained using the data recipe summarized in Table 7 for 1 epoch. Additional dataset details are provided in Appendix D. To train GUI-Actor, we begin by freezing all backbone VLM parameters and training only the newly introduced components of the action head (20M/100M parameters for 2B/7B backbone). After this warm-up phase, we finetune the entire model using standard supervised learning. Evaluation Benchmarks & Metric We evaluate GUI-Actor and other baseline methods on three well-established GUI visual grounding benchmarks: ScreenSpot [34], ScreenSpot-v2 [45], and ScreenSpot-Pro [48], with the last featuring higher-resolution interfaces and greater domain shift (e.g., industrial software, multi-window layouts), serving as practical testbed for generalization. For the evaluation metric, we use Element Accuracy, which measures the proportion of predictions where the click point falls within the ground-truth bounding box of the target element. Please refer to Appendix for more details on the benchmark information. 6 Table 1: Performance comparison on ScreenSpot-Pro, which features higher-resolution interfaces and greater domain shift (e.g., industrial software, multi-window layouts), serving as practical testbed for generalization. Dev Creative CAD Scientific Office OS Avg-Text Avg-Icon Avg GPT-4o Claude Compute OS-Atlas-4B ShowUI-2B UGround-V1-2B UI-TARS-2B GUI-Actor-2B GUI-Actor-2B + Verifier Qwen2-VL-7B SeeClick-9.6B Aria-UI-2-5.3B OS-Atlas-7B Aguvis-7B UGround-V1-7B UI-TARS-7B GUI-Actor-7B GUI-Actor-7B + Verifier CogAgent-18B UGround-72B-V1 UI-TARS-72B 0.7 12.6 3.7 9.4 27.4 26.4 34.8 41. 1.3 0.3 8.4 17.7 16.1 28.1 36.1 38.8 38.8 8.0 31.1 40.8 0.6 16.8 2.3 5.3 26.7 27.6 35.5 36.7 0.9 0.6 14.7 17.9 21.4 31.7 32.8 40.2 40.5 5.6 35.8 39. 1.5 11.9 1.5 1.9 14.6 14.6 28.4 34.5 0.4 1.9 6.1 10.3 13.8 14.6 18.0 29.5 37.2 6.1 13.8 17.2 1.2 25.8 7.5 10.6 34.3 39.8 38.2 41. 3.5 2.0 18.1 24.4 34.6 39 50.0 44.5 44.5 13.4 50.0 45.7 0.9 26.9 4.8 13.5 38.3 42.6 53.9 62.6 3.0 0.9 16.1 27.4 34.3 49.6 53.5 56.5 64.8 10.0 51.3 54. 0 8.1 3.1 6.6 17.9 14.3 30.6 36.2 0.5 1.5 2.6 16.8 19.4 24.5 24.5 36.2 43.9 3.1 25.5 30.1 1.3 23.4 5.0 10.8 - 39.6 50.7 57. 2.5 1.8 17.1 28.1 - - 47.8 55.8 60.7 12.0 - 50.9 0 7.1 1.7 2.6 - 8.4 14.1 16.1 0.2 0 2.0 4.0 - - 16.2 16.4 17.6 0.8 - 17. 0.8 17.1 3.7 7.7 26.6 27.7 36.7 41.8 1.6 1.1 11.3 18.9 22.9 31.1 35.7 40.7 44.2 7.7 34.5 38.1 Table 2: Performance comparison on ScreenSpot. Mobile-Text Mobile-Icon Desktop-Text Desktop-Icon Web-Text Web-Icon Avg GPT-4 GPT-4o Claude Computer Use Gemini 2. UGround-V1-2B UI-TARS-2B GUI-Actor-2B GUI-Actor-2B + Verifier Qwen2-VL-7B CogAgent-7B SeeClick-9.6B Magma-8B Aguvis-G-7B OS-Atlas-7B Aguvis-7B UGround-v1-7B UI-TARS-7B GUI-Actor-7B GUI-Actor-7B + Verifier UI-TARS-72B Aguvis-72B UGround-V1-72B 22.6 20.2 - - 89.4 93.0 93.0 93.8 75.5 67.0 78.0 60.4 88.3 93.0 95.6 93.0 94.5 94.9 96. 94.9 94.5 94.1 24.5 24.9 - - 72.0 75.5 79.9 81.2 60.7 24.0 52.0 58.5 78.2 72.9 77.7 79.9 85.2 82.1 83.0 82.5 85.2 83.4 20.2 21.1 - - 88.7 90.7 88.1 89.7 76.3 74.2 72.2 75.3 88.1 91.8 93.8 93.8 95.9 91.8 93.8 89.7 95.4 94.9 11.8 23.6 - - 65.7 68.6 78.6 80.7 54.3 20.0 30.0 52.9 70.7 62.9 67.1 76.4 85.7 80.0 82. 88.6 77.9 85.7 9.2 12.2 - - 81.3 84.3 90.9 91.3 35.2 70.4 55.7 69.1 85.7 90.9 88.3 90.9 90.0 91.3 92.2 88.7 91.3 90.4 8.8 7.8 - - 68.9 74.8 84.0 80.6 25.7 28.6 32.5 52.0 74.8 74.3 75.2 84.0 83.5 85.4 87.4 85.0 85.9 87.9 16.2 18.3 83.0 84.0 77.7 82.3 86.5 86.9 55.3 47.4 53.4 60.3 81.8 82.5 84.4 86.3 89.5 88.3 89. 88.4 89.2 89.4 Baselines We primarily compare our approach against models of comparable scale (7B parameters). The baselines include (i) closed-source models like GPT-4o [49], Claude for Computer Use [50], and Gemini 2.0 [51], as well as (ii) open-source models like SeeClick [34], ShowUI [39], and Magma [52]. We particularly highlight several baselines that share the same backbone as ours, including the backbone Qwen2-VL [38], Aguvis-7B [7], UGround-v1-7B [8], and UI-TARS-7B [10]. We also conduct performance comparison among Qwen-2.5-VL and models using it as backbone like Jedi [44]. Unless otherwise specified, all numbers are reported from the original paper or from the UI-TARS benchmark[10]. Main Results Table 1, 2, 3, and 4 present performance comparisons on ScreenSpot-Pro, ScreenSpot, and ScreenSpot-v2 benchmarks. Our models, GUI-Actor-2B and GUI-Actor-7B, consistently outperform existing state-of-the-art methods across all benchmarks, with the 2B model even surpassing many 7 Table 3: Performance comparison on ScreenSpot-v2. indicates results obtained from our own evaluation of the official model on Huggingface. Mobile-Text Mobile-Icon Desktop-Text Desktop-Icon Web-Text Web-Icon Avg Operator GPT-4o + OmniParser-v2 OS-Atlas-4B UI-TARS-2B GUI-Actor-2B GUI-Actor-2B + Verifier SeeClick-9.6B Magma-8B OS-Atlas-7B Aguvis-7B UGround-V1-7B UI-TARS-7B GUI-Actor-7B GUI-Actor-7B + Verifier UI-TARS-72B 47.3 95.5 87.2 95.2 95.0 95. 78.4 62.8 95.2 95.5 95.0 96.9 96.5 97.2 94.8 41.5 74.6 59.7 79.1 82.2 81.5 50.7 53.4 75.8 77.3 83.3 89.1 84.3 84.8 86. 90.2 92.3 72.7 90.7 92.2 94.3 70.1 80.0 90.7 95.4 95.0 95.4 91.7 94.3 91.2 80.3 60.9 46.4 68.6 81.8 82. 29.3 57.9 63.6 77.9 77.8 85.0 84.1 85.0 87.9 92.8 88.0 85.9 87.2 92.9 93.6 55.2 67.5 90.6 91.0 92.1 93.6 93.9 94.0 91. 84.3 59.6 63.1 78.3 82.7 82.8 32.5 47.3 77.3 72.4 77.2 85.2 82.3 85.2 87.7 Table 4: Performance comparison of models based on the Qwen-2.5-VL backbone. ScreenSpot-Pro: Qwen2.5-VL-3B Qwen2.5-VL-7B Jedi-3B Jedi-7B GUI-Actor-3B GUI-Actor-3B + Verifier GUI-Actor-7B GUI-Actor-7B + Verifier Dev 21.4 29.1 38.1 27.4 39.8 43.8 38.1 43.1 Creative CAD Scientific Office 25.8 24.9 34.6 34.0 36.7 37.8 41.4 41.9 18.4 13.8 23.0 32.2 34.1 43.3 38.3 48.3 29.5 31.1 38.6 52.4 49.6 48.4 50.8 47.2 40.9 45.7 57.0 68.7 61.3 63.5 63.0 65.7 OS 20.4 22.4 25.0 26.0 35.2 42.9 38.8 43.4 70.5 80.7 71.9 84.7 88.6 89.3 55.1 61.5 84.1 86.0 87.6 91.6 89.5 90.9 90.3 Avg 25.9 27.6 36.1 39.5 42.2 45.9 44.6 47.7 ScreenSpot-v2: Mobile-Text Mobile-Icon Desktop-Text Desktop-Icon Web-Text Web-Icon Avg Qwen2.5-VL-3B Qwen2.5-VL-7B Jedi-3B Jedi-7B GUI-Actor-3B GUI-Actor-3B + Verifier GUI-Actor-7B GUI-Actor-7B + Verifier 93.4 97.6 96.6 96.9 97.6 98.3 97.6 96.9 73.5 87.2 81.5 87.2 83.4 85.3 88.2 89. 88.1 90.2 96.9 95.9 96.9 96.9 96.9 97.4 58.6 74.2 78.6 87.9 83.6 87.9 85.7 86.4 88.0 93.2 88.5 94.4 94.0 95.3 93.2 95.7 71.4 81.3 83.7 84.2 85.7 86.7 86.7 84.7 80.9 88.8 88.6 91.7 91.0 92.4 92.1 92.5 competing 7B models. While there is one exception UI-TARS-7B achieving stronger performance, it benefits from training on significantly larger dataset that includes both public and proprietary data (see Figure 1). Additionally, it undergoes more extensive training pipeline, including continual pre-training, an annealing phase, and final stage of direct preference optimization (DPO). Although our models are trained solely with supervised fine-tuning, they achieve competitive or even superior results on ScreenSpot-Pro, demonstrating its strong capability and potential. Robust Out-of-Distribution Generalization As shown in Table 1, both GUI-Actor-2B and GUIActor-7B demonstrate strong performance on ScreenSpot-Proan out-of-distribution benchmark characterized by higher resolutions and substantial domain shifts from the training datasurpassing the previous state-of-the-art UI-TARS model by +9.0 and +5.0 points with 2B and 7B parameters, respectively. We attribute this gain to explicit spatial-semantic alignment: unlike coordinate-based approaches such as UI-TARS, GUI-Actor leverages an attention-based action head that grounds semantic cues directly in discrete visual regions. This design embeds stronger spatial inductive bias and naturally aligns with the patch-based representations of modern vision backbones. As result, GUI-Actor is better equipped to reason over localized visual content, enabling robust generalization across diverse screen resolutions and UI layouts. Further evidence of this robustness is shown in the Figure 3(c): as training progresses, GUI-Actor-2B and GUI-Actor-7B show no sustained overfitting on the out-of-domain ScreenSpot-Pro benchmark: its accuracy recovers from early dips, then gradually increases before stabilizing. In contrast, baseline performance steadily declines after peaking early in training. 8 Figure 3: Accuracy Progression Over Training Steps. Improved Sample Efficiency Figure 3 illustrates how GUI-Actor design leads to improved sample efficiency compared to coordinate-based baselines. GUI-Actor reaches its final accuracy on both ScreenSpot and ScreenSpot-v2 using only 60% of the training data, outperforming the point and box-supervised models of AGUVIS, which plateau after 80-90% of the data. This efficiency stems from GUI-Actor explicit spatial-semantic alignment through its action head, which enables grounding directly at the vision backbones native patch resolution, avoiding the granularity mismatch that hampers baseline methods. Additionally, our multi-patch supervision strategy gracefully handles the supervision ambiguity in coordinate generation based methods, offering dense, spatially structured supervision signals. Enabling backbone VLM grounding on GUIs without sacrificing general-purpose strengths. We introduce variant, GUI-Actor-LiteTrain, where we freeze all backbone VLM parameters and train only the newly introduced components for the action head and special tokens. This setup explores how GUI-Actor can impart GUI grounding capabilities without diminishing the VLMs general-purpose strengths. As shown in Table 5, GUI-Actor-LiteTrain yields substantial performance improvements over the unmodified backbone VLM. With the help of grounding verifier, it even rivals fully fine-tuned coordinate generation models. These results suggest that the backbone VLM already exhibits strong perceptual understanding of UI screenshots. As such, training the model to generate coordinates in text format may primarily focus on coordinate mapping, offering limited contribution to the semantic understanding of UI elements. More importantly, GUI-Actor-LiteTrain retains the backbones original language and vision-language capabilities, demonstrating that lightweight integration can enable grounding without compromising generality. Table 5: Analysis on lightweight training (-LiteTrain), where the backbone VLM (i.e., Qwen-2-VL) is frozen, and only the newly introduced parameters for the action head and special tokens are updated during training. Method Updated # of Params ScreenSpot-Pro ScreenSpot ScreenSpot-v2 GUI-Actor-2B-LiteTrain GUI-Actor-2B-LiteTrain + Verifier GUI-Actor-7B-LiteTrain GUI-Actor-7B-LiteTrain + Verifier 19M 19M 103M 103M 25.4 34.0 22.9 35.8 71.4 79. 73.5 81.3 73.9 82.3 74.9 83.8 Boosting Performance via Grounding Verifier The results in Tables 1, 2, 3, and 5 demonstrate that the grounding verifier consistently improves performance, highlighting its effectiveness in enhancing grounding accuracy. The improvement is especially significant on ScreenSpot-Pro, where it boosts GUI-Actor-7B by nearly 4 points and GUI-Actor-7B-LiteTrain by an impressive 13 points. Additionally, we investigate the benefits of the Verifier Self-Aggregation strategy in Appendix G.1 and evaluate the verifiers applicability to other baseline models in Appendix G.2. Our findings suggest that our verifier is highly robust and well-suited to GUI-Actor, as it requires only single forward pass to generate diverse region proposals. Ablation Study Table 6 present the results of our ablation study, where bbox sup. and point sup. denote models trained to predict the ground-truth bounding box or action point coordinates in natural language format, respectively. The results show that models trained with coordinate generation (both bounding box and point supervision) consistently underperform compared to GUI-Actor-7B across the benchmarks, highlighting the effectiveness and necessity of explicit spatial-semantic alignment achieved through our proposed action head. Interestingly, despite having access to more spatial information, Aguvis-7B (bbox sup.) performs similarly to or worse than Aguvis-7B (point sup.), suggesting that without architectural mechanisms or spatial inductive bias, these coordinate generation based methods remain disconnected from the underlying visual representation, limiting their generalization and grounding capabilities. Table 6: Ablation study on ScreenSpot-Pro, ScreenSpot, and ScreenSpot-v2. ScreenSpot-Pro: GUI-Actor-7B Aguvis-7B (bbox sup.) Aguvis-7B (point sup.) Dev 38.8 12.4 15. Creative 40.2 17.0 19.4 CAD 29.5 1.5 3.8 Scientific Office 44.5 18.1 17.3 56.5 21.7 24.4 OS 36.2 11.7 11.7 Avg 40.7 13.8 15. ScreenSpot: Mobile-Text Mobile-Icon Desktop-Text Desktop-Icon Web-Text Web-Icon Avg GUI-Actor-7B Aguvis-7B (bbox sup.) Aguvis-7B (point sup.) 94.9 92.3 92.3 82.1 73.4 79.0 91.8 92.3 93. 80.0 76.4 71.4 91.3 92.6 92.6 85.4 74.8 75.2 88.3 84.4 85.1 ScreenSpot-v2: Mobile-Text Mobile-Icon Desktop-Text Desktop-Icon Web-Text Web-Icon Avg GUI-Actor-7B Aguvis-7B (bbox sup.) Aguvis-7B (point sup.) 96.5 92.3 96.1 84.3 73.4 80.1 91.7 92.3 96.1 84.1 76.4 74.6 93.9 92.6 93. 82.3 74.8 74.3 89.5 84.4 87.0 Multi-Region Prediction Without Extra Inference Cost Due to its attention-based grounding mechanism, GUI-Actor can generate multiple candidate action regions in single forward pass, incurring no extra inference cost. To evaluate the effectiveness of these high-probability regions, we use the Hit@k metric, where represents the number of top-ranked predictions considered. Figure 4a shows that GUI-Actor exhibits substantial improvement from Hit@1 to Hit@3, whereas the gap for baselines is relatively marginal. In our analysis, we observed that for coordinate-generation-based baselines, even when multiple predictions are sampled, the outputs are mostly identical, e.g., shifting slightly from (0.898, 0.667) to (0.899, 0.666). In contrast, our model simultaneously produces multiple candidate regions from the attention distribution in single forward pass. These candidates are mutually exclusive, naturally promoting diversity and increasing the chance of capturing all valid action regions. Figure 4b provides qualitative example where our approach successfully identifies all ground-truth regions required for action execution. (a) Hit@1 and Hit@3 for different models. (b) GUI-Actor can capture multiple potential regions. Figure 4: (a) Hit@1 and Hit@3 for different methods. For Aguvis baselines, we run inference 3 times with temperature = 1.0, top_p = 0.95. (b) Illustration of multi-region prediction. In this example, the instruction is check shopping cart and the central shopping cart text is clickable, while the ground truth is only the top-right icon. Online Evaluation on OS-World-W To assess the real-world effectiveness of our proposed system, we conducted online evaluations on OS-World-W, curated subset of the OS-World benchmark focused on 49 Windows-specific tasks involving complex, multi-step interactions across office and multi-application scenarios. We use GPT-4o as the planner and use GUI-Actor-7B for action 10 grounding. Compared with the leading visual grounding baselines Aguvis-7B, NAVI [53], and OmniAgent [54], GUI-Actor-7B demonstrates promising performance with task success rate of 12. 2%, outperforming OmniAgent and NAVI (both 10. 2%) and significantly surpassing Aguvis-7B (point sup.) (4.0%). More details can be found in Appendix H."
        },
        {
            "title": "6 Conclusion",
            "content": "We present GUI-Actor, novel coordinate-free visual grounding framework for GUI agents. Departing from prevailing text-based coordinate generation paradigms, GUI-Actor introduces dedicated <ACTOR> token that attends to target visual patches to directly localize GUI elements on the screen. This mechanism explicitly aligns spatial visual features with the semantic signals from instructions, and naturally supports bounding-boxbased multi-patch supervision, which helps mitigate the ambiguity inherent in single-point predictions. Benefiting from its ability to propose multiple candidate regions in single pass, GUI-Actor further employs lightweight verifier to select the most plausible target at inference time. Experiments across diverse benchmarks demonstrate that GUI-Actor outperforms state-of-the-art methods and exhibits stronger generalization to unseen layouts and screen resolutions. We conduct extensive analyses to understand the effectiveness of each component in our framework, highlighting its promising potential for advancing visual GUI agents."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Boyu Gou for providing the bounding box training data, and Yiheng Xu, Qiushi Sun, Zichen Ding, and Fangzhi Xu for their valuable discussions and insightful suggestions."
        },
        {
            "title": "References",
            "content": "[1] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024. [2] Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao, Chao Du, et al. Ufo2: The desktop agentos. arXiv preprint arXiv:2504.14603, 2025. [3] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. [4] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [5] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024. [6] Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray Huang, Si Qin, et al. Large action models: From inception to implementation. arXiv preprint arXiv:2412.10047, 2024. [7] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [8] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=YicbFdNTTy. [10] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [11] Yuxuan Sun, Chong Sun, Dong Wang, You He, and Huchuan Lu. Roi pooled correlation filters for visual tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57835791, 2019. [12] Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, and Jifeng Dai. An empirical study of spatial attention mechanisms in deep networks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 66886697, 2019. [13] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. [14] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. [15] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [16] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37: 5204052094, 2024. [17] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024. [18] Chengyou Jia, Minnan Luo, Zhuohang Dang, Qiushi Sun, Fangzhi Xu, Junlin Hu, Tianbao Xie, and Zhiyong Wu. Agentstore: Scalable integration of heterogeneous agents as specialized generalist computer assistant. arXiv preprint arXiv:2410.18603, 2024. [19] Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, et al. survey of neural code intelligence: Paradigms, advances and beyond. arXiv preprint arXiv:2403.14734, 2024. [20] Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, et al. Scienceboard: Evaluating multimodal autonomous agents in realistic scientific workflows. arXiv preprint arXiv:2505.19897, 2025. [21] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 120, 2025. [22] Chaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang, Qingwei Lin, and Dongmei Zhang. Api agents vs. gui agents: Divergence and convergence. arXiv preprint arXiv:2503.11069, 2025. [23] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. [24] Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. [25] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. [26] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. In European Conference on Computer Vision, pages 240255. Springer, 2024. 12 [27] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36: 5970859728, 2023. [28] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024. [29] Jiani Zheng, Lu Wang, Fangkai Yang, Chaoyun Zhang, Lingrui Mei, Wenjie Yin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, and Qi Zhang. Vem: Environment-free exploration for training gui agent with value environment model. arXiv preprint arXiv:2502.18906, 2025. [30] Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. [31] Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. Vision-language models can self-improve reasoning via reflection. arXiv preprint arXiv:2411.00855, 2024. [32] Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, and Junxian He. Breaking the data barrierbuilding gui agents through task generalization. arXiv preprint arXiv:2504.10127, 2025. [33] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. [34] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [35] OpenAI. Computer-using agent. Available at: https://openai.com/index/computer-using-agent, 2025. [36] Anthropic. Claude computer use. Available at: https://www.anthropic.com/news/developing-computer-use, 2024. [37] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [39] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. [40] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. [41] Pawel Pawlowski, Krystian Zawistowski, Wojciech Lapacz, Marcin Skorupa, Adam Wiacek, Sebastien Postansque, and Jakub Hoscilowicz. Tinyclick: Single-turn agent for empowering gui automation. arXiv preprint arXiv:2410.11871, 2024. [42] Fei Tang, Yongliang Shen, Hang Zhang, Siqi Chen, Guiyang Hou, Wenqi Zhang, Wenqiao Zhang, Kaitao Song, Weiming Lu, and Yueting Zhuang. Think twice, click once: Enhancing gui grounding via fast and slow systems. arXiv preprint arXiv:2503.06470, 2025. [43] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [44] Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025. [45] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [46] Hai-Ming Xu, Qi Chen, Lei Wang, and Lingqiao Liu. Attention-driven gui grounding: Leveraging pretrained multimodal large language models without fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 88518859, 2025. 13 [47] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [48] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. [49] OpenAI. Introducing gpt-4o. Available at: https://openai.com/index/hello-gpt-4o, 2024. [50] Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: preliminary case study with claude 3.5 computer use, 2024. URL https://arxiv.org/abs/2411.10323. [51] Google. Introducing gemini 2.0. Available at: https://blog.google/technology/google-deepmind/googlegemini-ai-update-december-2024, 2024. [52] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. arXiv preprint arXiv:2502.13130, 2025. [53] Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. [54] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. [55] Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154, 2024. [56] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. [57] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731, 2021. [58] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776, 2020. [59] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. arXiv preprint arXiv:2010.04295, 2020."
        },
        {
            "title": "A Limitations",
            "content": "Our attention-based action generation is particularly well-suited for GUI environments, where interface elements such as icons and text lines often exhibit regular geometric patterns that align natually with patch-based detection. However, limitation stems from the base model we employ: the backbone VLM (e.g., Qwen2-VL) adopts Naive Dynamic Resolution strategy with fixed patch size of 28 28 pixels. This poses challenges when dealing with very small interface elements (e.g., icons smaller than 10 10 pixels), as such fine-grained details may be insufficiently represented. Although this challenge is not unique to our method, it can be pronounced in tasks demanding high-precision control, such as those encountered in professional software like CAD tools. While we introduce simple mitigation strategy in this work, fully addressing this limitation may require more substantial advancements in the future, such as improving the visual encoders perceptual resolution or incorporating offset-based spatial refinement. Details on Multi-Patch Supervision key advantage of our approach lies in its ability to leverage dense, spatially structured supervision from bounding boxes. Unlike traditional coordinate-based methods that depend on single, potentially ambiguous click point, GUI-Actor enables supervision over entire target regions, capturing the spatial extent of actionable elements more effectively. An example is illustrated in Figure 2b. 14 Specifically, we convert each ground-truth bounding box into binary mask over the image patch grid. Given normalized bounding box = [left, top, right, bottom] [0, 1]4, we scale the normalized coordinates to the patch grid resolution: (left , top H, right , bottom H) . (8) This rounding ensures that all patches partially or fully covered by the bounding box are included in the supervision region. All patches whose indices fall within the resulting grid-aligned region are labeled as positives (i.e., mask value 1), while all others are assigned value of 0. This yields binary vector {0, 1}M aligned with the image patch sequence, where = H. We define the action head loss as the KL divergence between the predicted attention distribution {a1, . . . , aM } and normalized target distribution derived from the binary mask {0, 1}M : pi = yi j=1 yj + ϵ (cid:80)M , for = 1, . . . , ; L<ACTOR> = (cid:88) i=1 pi log pi ai , (9) where ϵ is small constant for numerical stability. Visualization of Attention Maps from GUI-Actor To better understand the grounding behavior of GUI-Actor, we provide additional examples visualizing its attention maps overlaid on the original input images in Figure 5. The following Python code outlines the visualization process: starting from the raw attention scores, we normalize and reshape them to match the image dimensions, apply colormap for clearer interpretation, and finally blend the attention heatmap with the original image. This produces an intuitive overlay that highlights regions the model attends to when making decisions. 1 width , height = image . size 2 , = attention _ma p_ size # This may not equal width // patch_size due to image reshaping , and you may need // 2 and // 2 due to the Naive Dynamic Resolution operation in Qwen2 - VL 3 4 scores = np . array ( json_prediction [ attn_scores ][0]) . reshape (H , ) 5 6 # Normalize the attention weights for coherent visualization 7 scores_norm = ( scores - scores . min () ) / ( scores . max () - scores . min () ) 8 9 # Resize the attention map to match the image size . You can choose different resize strategies , such as NEAREST and BILINEAR . 10 score_map = Image . fromarray (( scores_norm * 255) . astype ( np . uint8 ) ) . resize (( width , height ) , resample = Image . BILINEAR ) 11 12 # Apply colormap 13 colormap = plt . get_cmap ( jet ) 14 colored_score_map = colormap ( np . array ( score_map ) / 255.0) # returns RGBA 15 colored_score_map = ( colored _score_m ap [: , : , :3] * 255) . astype ( np . uint8 ) 16 colored_overlay = Image . fromarray ( color ed_score_map ) 17 18 # Blend with original image 19 blended = Image . blend ( image , colored_overlay , alpha =0.3) Listing 1: Python code for overlaying the attention score map on the image. Training Datasets used for GUI-Actor We compile our training data from several publicly available, high-quality GUI datasets. Summary statistics are provided in Table 7. Note that we exclude samples from Wave-UI that overlap with downstream task test sets. 1https://huggingface.co/datasets/agentsea/wave-ui 15 (a) ScreenSpot: \"click the button to create new project\" (b) ScreenSpot-Pro: \"restart from CD\" (c) ScreenSpot-Pro: \"confirm sort\" (d) ScreenSpot-Pro: \"select the legend of the plot\" Figure 5: Example visualizations from (a) ScreenSpot and (b)(c)(d) ScreenSpot-Pro. Each image shows the original interface with an overlaid attention map indicating regions of interest of GUI-Actor. The attention maps largely overlap with the ground truth areas (red bounding boxes), demonstrating that the model can effectively capture the accurate UI elements. Table 7: Overview of training datasets used for GUI-Actor. Dataset # of Elements # of Screenshots Platform Uground Web-Hybrid [8] GUI-Env [23] GUI-Act [23] AndroidControl [55] AMEX [24] Wave-UI1 Total 8M 262K 42K 47K 1.2M 50K 9.6M 775K 70K 13K 47K 100K 7K 1M Web Web Web Android Android Hybrid -"
        },
        {
            "title": "E GUI Visual grounding Benchmarks",
            "content": "In these benchmarks, each screenshot is paired with natural language instruction written by human annotators, typically describing the content or function of the target element, e.g., the new project button or switch to weekly view in the calendar. The agent is required to identify the location of the corresponding element on the screen based on the given instruction. ScreenSpot is the first benchmark specifically designed for GUI visual grounding, containing 1,272 single-step instructions paired with corresponding target elements. It covers wide range of GUI platforms, including mobile (Android and iOS), desktop (macOS and Windows), and web environments, and categorizes elements into text-based or icon elements. ScreenSpot-v2 is corrected version of ScreenSpot that fixes annotation errors and ambiguous instructions, while keeping the total number of samples unchanged. ScreenSpot-Pro is recently introduced challenging benchmark tailored for high-resolution professional scenarios. It contains 1,581 tasks annotated by experts across 23 professional applications spanning three operating systems. Compared to ScreenSpot, ScreenSpot-Pro features higher-resolution 16 screenshots and larger domain gap from most grounding pretraining data, e.g., industrial software and multi-window interfaces. We view its performance as practical estimate of generalization for GUI visual grounding models."
        },
        {
            "title": "F More Detailed on Grounding Verifier",
            "content": "F.1 Data Construction We construct the verifier training dataset from the OS-Atlas dataset [45], which spans desktop, mobile, and web domains. The original data consists of triplets in the form of (image, query, bounding box), where each image is paired with multiple queries and their corresponding bounding boxes. For each triplet, we generate positive example by placing marker at the center of the bounding box, treating it as the correct grounding point for the given query. To create negative examples, we apply two strategies: (1) selecting the center of different bounding box from the same image to simulate semantically plausible but incorrect location; (2) randomly sampling point outside the correct bounding box to simulate an unrelated action. As shown in Figure 6, each proposed point is marked on the image with hollow red circle. This process produces two labeled examples per query: one positive and one negative, formatted as: {image with correct point, query, True}, {image with wrong point, query, False}. In total, we construct balanced training set containing 730K examples, equally split between positive and negative cases. The overview of our dataset is listed in Table 8. Table 8: Overview of the dataset used to train our Grounding Verifier, including both positive and negative examples. Since multiple positive and negative samples can be generated from single screenshot, the size of our dataset can exceed that of the original dataset. Dataset # of Screenshots Platform SeeClick [34] FineWeb [56] UIbert [57] AMEX [24] RICOSCA [58] Widget Captioning [59] Linux-desktop [45] Windows-desktop [45] MacOS-desktop [45] Total 147K 123K 17K 155K 30K 22K 9K 220K 7K 730K Web Web Mobile Android Android Mobile Linux Windows MacOS - F.2 Patch Selection Given the top candidate patches from the attention map, our goal is to select the one that best aligns with the user instruction in the image. straightforward approach is to draw marker at the center of each patch and use verifier to score how well each position satisfies the instruction x. Specifically, for each candidate image with marked point, we use the verifier θv to compute the probability of predicting tokens True or False: Ptrue = Pθv (TrueI, x) and Pfalse = Pθv (FalseI, x). We then define the score for each position as the normalized probability of the True token: s(I, x) = . Ptrue Ptrue+Pfalse key limitation of this approach is that each patch (typically 28 28 pixels) may miss small icons located between two neighboring patches, leading to inaccurate target localization. To address this, we introduce simple yet effective refinement. We first cluster 4-connected neighboring patches and compute weighted center based on the verifier scores of the individual patches. This enables the generation of candidate points that lie between adjacent patches and improves localization accuracy without directly modifying the patch size of the base model. In our implementation, we use up to = 20 top-scoring patches, filtering out those with attention weights below 20% of the maximum attention weight. We then apply clustering to the neighboring 17 Figure 6: Illustration of positive and negative examples used to train the grounding verifier. patches, compute the weighted centers of these clusters, and add them to the set of candidate positions. Each candidate position is scored using s(I, x), and we select the one with the highest score. Given candidate coordinate (x, y), we crop the image using square region of size lcrop lcrop centered at (x, y). This is implemented as: 1 image . crop (( 2 max (0 , - l_crop //2) , max (0 , - l_crop //2) , min ( image . size [0] , + l_crop //2) , min ( image . size [1] , + l_crop //2) 3 4 5 6 ) ) We set lcrop = 1000 pixels for all tasks. To reduce the computational cost, if candidate position achieves high confidence score (e.g., s(I, x) > 0.95), we immediately return that position without evaluating the remaining candidates. In our experiments, we set the threshold to 0.95 for tasks in ScreenSpot-Pro and 0.8 for ScreenSpot and ScreenSpot-v2. lower threshold reduces reliance on the verifier and instead trusts the grounding models output, which is suitable when the grounding model is highly accurate. In contrast, higher threshold prompts the verifier to more carefully assess each candidate, which is beneficial when the grounding model is less reliable, as in ScreenSpot-Pro."
        },
        {
            "title": "G Improving Grounding with Verifier",
            "content": "G.1 Enhancing Generation with Verifier Self-Aggregation In this section, we explore how to further leverage the verifiers capability through simple yet effective technique that we call Verifier Self-Aggregation (VS). The idea is to crop the input image at multiple scales and compute the verifier scores for each crop, then average these scores to obtain more robust final prediction. This approach balances the trade-off between capturing detailed local information (with smaller crops) and maintaining broader context (with larger crops). Specifically, we use two crop sizes in our experiments: lcrop = 1200 and lcrop = 1400 for ScreenSpot-pro. The results, shown in Table 9, demonstrate that verifier self-aggregation leads to improved performance on ScreenSpot-Pro. Verifier self-aggregation provides simple yet effective strategy to enhance verifier robustness, while also highlighting the need for more robust verifiers in the future. 18 Table 9: Verifier Self-aggregation on ScreenSpot-Pro. Dev Creative CAD Scientific Office OS Avg-Text Avg-Icon Avg GUI-Actor-7B + Verifier GUI-Actor-7B + Verifier Self-aggregation 40.1 39.5 39.0 40.2 37.2 38.3 47.2 44.9 63.5 63. 41.8 44.9 61.1 61.3 16.7 17.4 44.2 44.5 Figure 7: Comparison with AGUVIS using the verifier. AGUVIS inferences 21 times for verification. In contrast, GUI-Actor performs single inference step, requiring only about 5% of the computation during inference. G.2 Comparison with Baseline Using Verifier To further validate the effectiveness of our grounding verifier, we integrate it into the AGUVIS baseline by sampling multiple candidate positions during inference and selecting the one with the highest verifier score. Specifically, AGUVIS generates one deterministic output at temperature 0.0 and samples 20 additional candidate points using temperature of 0.7. While this approach explores broader range of plausible locations, it incurs substantial computational overhead for these models. In contrast, our GUI-Actor uses the attention map to propose multiple candidate points within single pass. This leads to much more efficient processrequiring only about 5% of the computation compared to AGUVISwhile achieving considerably higher grounding accuracy on ScreenSpot and ScreenSpot-v2, and significantly outperforming AGUVIS on the more challenging ScreenSpot-Pro benchmark. These results demonstrate both the efficiency of GUI-Actor and the general effectiveness of the verifier in improving action selection, even when applied to other models."
        },
        {
            "title": "H Online Benchmark Evaluation on OSWorld",
            "content": "To evaluate the real-world effectiveness of our proposed GUI-Actor, we conduct experiments on OSWorld [16] using GUI-Actor-7B for quick validation. OS-World is live benchmark designed to test GUI agents in realistic desktop environments. We focus on curated subset of 49 Windows-specific tasks, denoted as OSWorld-W, covering variety of multi-step office and multi-application scenarios. Each task is paired with handcrafted verification scripts to ensure reliable automatic evaluation. Following the standard evaluation pipeline, we adopt GPT-4o as the planner. At each step, the planner observes the current GUI screenshot and user instruction and generates natural language plan. This plan is then grounded into concrete actionseither via coordinate-based or coordinate-free mechanismsby the underlying grounding model, which plays critical role in determining the agents success. We compare GUI-Actor with several state-of-the-art visual grounding baselines: Aguvis-7B [7], NAVI [53], and OmniAgent [54]. As shown in Table 10, GUI-Actor achieves the highest task success rate at 12.2%, outperforming OmniAgent and NAVI (both at 10.2%) and substantially surpassing Aguvis-7B (4.0%). These results highlight the effectiveness and robustness of GUI-Actor in complex, real-world GUI environments. Despite having no exposure to OSWorld-W tasks during training, GUI-Actor generalizes well to unseen scenarios, delivering more accurate and reliable grounding performance than existing alternatives. 19 Table 10: Task Success Rate on the OSWorld-W subset (49 live Windows GUI tasks). GUI-Actor significantly outperforms existing grounding models in this real-world setting. #Tasks Completed Grounding Model Success Rate (%) Aguvis-7B (point sup.) NAVI OmniAgent GUI-Actor-7B (Ours) 4.00 10.2 10.2 12.2 2/49 5/49 5/49 6/"
        }
    ],
    "affiliations": [
        "Microsoft",
        "Nanjing University",
        "University of Illinois Urbana-Champaign"
    ]
}