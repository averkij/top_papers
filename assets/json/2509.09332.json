{
    "paper_title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning",
    "authors": [
        "Yuecheng Liu",
        "Dafeng Chi",
        "Shiguang Wu",
        "Zhanguang Zhang",
        "Yuzheng Zhuang",
        "Bowen Yang",
        "He Zhu",
        "Lingfeng Zhang",
        "Pengwei Xie",
        "David Gamaliel Arcos Bravo",
        "Yingxue Zhang",
        "Jianye Hao",
        "Xingyue Quan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io"
        },
        {
            "title": "Start",
            "content": "OMNIEVA: EMBODIED VERSATILE PLANNER VIA TASK-ADAPTIVE 3D-GROUNDED AND EMBODIMENTAWARE REASONING Yuecheng Liu*, Dafeng Chi*, Shiguang Wu*, Zhanguang Zhang*, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan Huawei Noahs Ark Lab"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) Task-Adaptive 3D Grounding mechanism, which introduces gated router to perform explicit selective regulation of 3D fusion based on contextual require- (2) ments, enabling context-aware 3D grounding for diverse embodied tasks. an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits strong ability across wide range of downstream scenarios. Evaluations of suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io 5 2 0 2 1 1 ] . [ 1 2 3 3 9 0 . 9 0 5 2 : r Figure 1: Performance Comparison Across 2D and 3D Embodied Reasoning Benchmarks. OmniEVA achieves state-of-the-art performance on 7 out of 8 benchmarks. *: Equal contribution. {liuyuecheng1, chidafeng1, wushiguang, zhanguang.zhang}@huawei.com; : Corresponding author. zhuangyuzheng@huawei.com"
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid advancement of multimodal large language models (MLLMs) has substantially improved both the performance and generalization capabilities of artificial intelligence, enabling systems to interpret and reason across diverse modalities, such as text, images, and video. This shift has opened new avenues to embodied intelligence Reed et al. (2022); Ahn et al. (2022); Driess et al. (2023), capable of perceiving, reasoning, and acting in physical environments. Spatial reasoning is core component of embodied cognition, serving as the bridge between perception and action. It transforms sensory inputs into structured scene representations that support rational generation, self-reflection, and long-horizon planning, where deeper reasoning is used to select the most effective immediate action in service of long-term embodied objectives. Early visionlanguage models addressed spatial reasoning primarily in two dimensions. SpatialVLM Chen et al. (2024a) introduced large-scale synthetic VQA grounded in real imagery, while RoboPoint Yuan et al. (2024a), RoboSpatial Song et al. (2025), and RoboRefer Zhou et al. (2025) incorporated fine-grained spatial grounding by predicting coordinates or bounding boxes from language prompts. RoboBrain Ji et al. (2025); Team et al. (2025a) further unified high-level planning with low-level spatial pointing, outperforming general MLLMs on embodied benchmarks. More recently, 3D LLMs have extended reasoning beyond 2D perception by incorporating point clouds, voxel grids, or 3D position embeddings into MLLMs Huang et al. (2023c); Zhu et al. (2024a); Hong et al. (2023); Zheng et al. (2025); Huang et al. (2025). Despite recent progress, two core challenges remain. First, the geometric adaptability gap: models trained solely on 2D inputs struggle with tasks that require strong spatial reasoning, such as object stacking, occlusion handling, or navigation in cluttered 3D scenes. This limitation arises from the absence of explicit 3D structural encoding, which restricts generalization in geometry-rich environments. Existing 3D-LLM approaches Zhu et al. (2024a); Zheng et al. (2025); Huang et al. (2025) often depend on hard-coded 3D injection strategies that ignore task relevance, resulting in unnecessary computation and noisy embeddings when 3D inputs are incomplete or nonessential. Second, the embodiment constraint gap: current methods often rely on labeled web-scale image or video datasets, or on rule-based synthetic simulations. Models trained on such data frequently overlook the constraints and capabilities of real robots, producing plans that may appear valid in theory but are infeasible in practice. In particular, neglecting object affordances, workspace limitations, and kinematic feasibility leads to action sequences that cannot be executed on physical platforms. Furthermore, the absence of embodied long-horizon planning benchmarks that explicitly incorporate embodiment constraints makes it difficult to systematically evaluate the unique challenges they pose. To address these limitations, we introduce OmniEVA (Embodied Versatile Planner), novel architecture that pioneers Task-Adaptive 3D Grounding and Embodiment-aware Reasoning. OmniEVA is the first framework to dynamically integrate 2D and 3D inputs via taskconditioned feature selection, enabling versatile and executable embodied reasoning through two key innovations: Task-Adaptive 3D Grounding: We introduce gated routing mechanism that dynamically modulates the infusion of 3D features into the visual-language backbone based on contextual task requirements. This allows for explicit, selective geometric grounding only when spatially essential, avoiding the drawbacks of static 3D fusion and enabling robust performance across both 2D and 3D reasoning tasks. Embodiment-Aware Reasoning: Moving beyond passive scene understanding, OmniEVA jointly incorporates task goals, environmental context, and physical constraints into its reasoning process. Through post-training with our proposed Taskand Embodiment-aware GRPO (TEGRPO) algorithm, the model learns to generate plans that respect object affordances, workspace boundaries, and kinematic limits, significantly improving executability and success rates on real robots. To comprehensively evaluate OmniEVAs capability and adaptability across diverse embodied scenarios, we conduct experiments on 8 public embodied reasoning benchmarks spanning image-, video-, and 3D-based question answering. These benchmarks cover broad spectrum of tasks ranging from basic spatial understanding to advanced geometric reasoning under multi-dimensional inputs. OmniEVA achieves state-of-the-art performance on 7 out of 8 benchmarks, demonstrating the effectiveness of the task-adaptive 3D-grounding mechanism. It also attains top results on ob2 ject navigation tasks in both the HM3D and MP3D datasets. To further probe embodiment-aware reasoning, we introduce four primitive benchmarksWhere2Go, Where2Grasp, Where2Approach, and Where2Fiteach targeting fundamental skill essential for long-horizon planning. OmniEVA achieves state-of-the-art performance across all primitive tasks, confirming its capability to master core embodied operations. These primitives form the foundation for more complex downstream applications, such as mobile manipulation. By excelling in these fundamental abilities, OmniEVA delivers significant improvements in downstream task performance, underscoring its robustness and versatility in embodied reasoning."
        },
        {
            "title": "2 RELATED WORK",
            "content": "MLLMs for Embodied Reasoning Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved spatial reasoning capabilities, particularly through synthetic datasets and spatially grounded visual question answering (VQA). SpatialVLM Chen et al. (2024a) pioneered large-scale spatial QA grounded in real-world imagery, laying the foundation for more precise spatial understanding. Building on this, models such as RoboPoint Yuan et al. (2024a), Robospatial Song et al. (2025) and RoboRefer Zhou et al. (2025) introduced fine-grained spatial outputs, including coordinate prediction and bounding box localization. RoboBrain Ji et al. (2025); Team et al. (2025a) further advanced this line by integrating high-level planning with low-level spatial pointing, outperforming general-purpose MLLMs on embodied reasoning benchmarks. To assess reasoning and planning in dynamic or large-scale environments, several video-based benchmarks have also emerged, such as VSI-Bench Yang et al. (2025b) and EgoPlan Chen et al. (2023). However, despite these developments, most embodied reasoning models remain limited by their reliance on 2D inputs, lacking the capacity to fully interpret environments with complex 3D geometric structures. 3D Large Language Models Efforts to extend LLMs to 3D modalities have explored representations such as point clouds Huang et al. (2023c); Zhu et al. (2024b); Chen et al. (2024d) and voxel grids Hong et al. (2023); Zhang et al. (2025). More rencent approaches inject 3D positional information into visual tokens, enabling pretrained MLLMs to perform spatial reasoning in three dimensions Zhu et al. (2024a); Zheng et al. (2025); Huang et al. (2025). While these methods have achieved state-of-the-art results on several 3D benchmarks, the hard-coded 3D injection methods can be problematic when 3D inputs are noisy, incomplete, or irrelevant to the task."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 OVERVIEW OmniEVA builds on pretrained MLLMs which typically comprises three principal components: 1) vision transformer encoder Eimg that converts each RGB image into sequence of discrete visual tokens, 2) lightweight network that maps visual embeddings into the language models latent space for seamless cross-modal interaction and 3) an autoregressive text decoder that generates output tokens. The model accepts natural language instruction , sequence of RGB images or video frames (I1, I2, . . . , IN ), and optionally, depth maps (D1, D2, . . . , DN ) for each view. To support cross-view spatial understanding, the model also ingests camera intrinsic parameters and extrinsic poses Mi corresponding to each frame. Conventional MLLMs such as QwenVL and InternVL split each frame into Hp Wp patches, augment them with 2D positional encodings, and feed the flattened sequence into Eimg. For frames, the encoder outputs RN HpWpdv , where dv denodes the embedding dimension. While effective for many vision-language tasks, this purely 2D approach omits direct 3D informationdepth values or world coordinateswhich is critical for complex geometric reasoning. Recent 3D LLMs, such as 3DRS(Huang et al., 2025), rely on static architecture to integrate 3D features, limiting their flexibility in tasks where such features are unnecessary. OminiEVA introduces Task-Adaptive Gated Router (TAGR) to dynamically fuse 3D features and two-stage training paradigm to enable Embodiment-aware Planning. 3 Figure 2: Model Architecture of OmniEVA. Left: The overall architecture of OmniEVA, featuring novel task-adaptive gated router that dynamically incorporates 3D positional embeddings. Middle: Detailed implementation of the gated router module. Right: Illustrative examples of the gated routers activation state across different tasks. 3.2 TASK-ADAPTIVE GATED ROUTER The module of Task-Adaptive Gated Router (TAGR) is illustrated in Figure 2. TAGR serves as dynamic mediator between task demands and spatial complexity, selectively regulating the injection of 3D positional encodings. We will introduce the details of the framework in the following sections. Patch-Level 3D Positional Encoding To encode spatial geometry, each depth image Di RHW is first projected into world coordinate matrix Pi RHW 3 using the camera parameters. Each pixel is represented by its 3D coordinate (x, y, z) in Pi. The 3D coordinate matrix Pi is then partitioned into patches aligned with the patch size of the RGB image processed by the ViT Encoder. For each patch, the 3D coordinates of all pixels are averaged, producing patched coori RHpWp3. Finally, sinusoidal encoding is applied to the 3D coordinates of dinate matrix each patch, mapping them into vectors of dimension dv. For frames, this process yields the 3D positional encoding features denoted as RN HpWpdv . Dynamic 3D Injection via Gated Routing Rather than applying 3D positional encoding uniformly for all tasks, we propose Task-Adaptive Gated Router (TAGR) that explicitly perform selective 3D integration based on task-specific requirements. TAGR determines whether to inject 3D positional priors based on two conditioning signals: 1) the task condition, reflecting the nature of the task to be performed, and 2) the scene condition, reflecting the structural complexity of the visual input. For task conditioning, lightweight sentence transformer Reimers & Gurevych (2019) encodes the instruction into latent vector Rdst . For scene conditioning, the vision encoder outputs RN HpWpdv , which is then aggregated via average pooling to obtain global scene descriptor avg Rdv : = SentenceTransformer(T ) avg =AvgPooling(V , dim = 0, 1, 2) (1) (2) The concatenated vector [V , avg] is passed through multi-layer perceptron (MLP) module to produce gate logits R2, which represent the probabilities corresponding to the activation and deactivation of the gate module. = MLPψ(Concatenate([V , avg])) R2 (3) The gate control variable is sampled from using the Gumbel-Softmax Jang et al. (2016) function to allow end-to-end gradient flow, with the temperature τ . This gate variable then controls whether to inject 3D positional embeddings into the visual stream. When = 1, the gate is activated, and the model augments 2D features with explicit 3D spatial cues; Conversely, when = 0, the gate remains inactive, and the model relies solely on 2D visual information. By conditioning 4 this gating mechanism on both the task and the scene, our module learns to allocate 3D reasoning capacity selectively - only whenand whereit is most beneficial. This mechanism can also be interpreted as Mixture-of-Experts (MoE) between pure visual tokens and the fused tokens (V + p), as shown in Equation 5. The resulting hybrid visual tokens and the text tokens are then passed to the LLM backbone llm θ () to generate the response tokens o. = GumbelSoftmax(V g, τ ) {0, 1} hybrid = + =(1 g)V + g(V + p) (4) (5) hybrid) = llm θ (T , We pretrain TAGR module on depth-aware datasets (see Appendix for detail)using crossentropy loss to align the predicted output with the ground truth label olabel. To encourage stable and interpretable gating behavior, we add KL divergence regularization term between the predicted gate distribution and prior Pprior (we use Bernoulli(0.5) as the prior over the binary outcomes), ψ,θ(olabel, o) + α LKL where ψ are the parameters of TAGR module, and θ are the parameters of LLMs, α = 0.01. After pretraining, the parameters of TAGR module are frozen during subsequent training stage. ψ (V gPprior) ψ,θ = LCE"
        },
        {
            "title": "Ltotal",
            "content": "(6) (7) 3.3 EMBODIMENT-AWARE TRAINING STRATEGY To unify perception, reasoning, and execution across heterogeneous embodied tasks, we introduce two-stage training paradigm that fosters omni-dimensional spatial cognition and embodiment-aware planning. This framework enables the model to comprehend rich multimodal inputs and transform them into context-aware, physically executable plans. Our contributions are twofold: (1) the curation of general and task-directed embodied reasoning datasets, and (2) the development of taskand embodiment-aware GRPO (TE-GRPO) that incorporates physical constraints and multimodal feedback for improved training in embodied settings. 3.3.1 OMINI-SUPERVISED FINE-TUNING FOR EMBODIED REASONING To establish robust reasoning backbone, we initiate training with hybrid dataset that combines general embodied reasoning corpora with suite of custom embodied task datasets. General Embodied Reasoning Dataset These datasets encompass diverse modalitiesincluding 2D images, video sequences, and 3D environmentsand support range of tasks such as spatial relation referring, temporal inference, visual grounding, scene captioning, and imagination. Collectively, they foster the models ability to perform spatial-temporal reasoning and multimodal comprehension. Full dataset specifications are provided in Appendix B. Custom Embodied Task Dataset While existing benchmarks like Where2Place (Yuan et al., 2024b) and PACO-LVIS (Ramanathan et al., 2023) focus narrowly on spatial affordance prediction, our dataset expands the scope to include navigation, manipulation, and composite tasks. These tasks challenge the model to reason about affordance prediction, grasp feasibility, active exploration, etc. Details are outlined in Appendix B. To further enhance reasoning capabilities, we annotate each task with chain-of-thought (CoT) cues that include task decomposition logic and decision rationale. These data serve as warm-start for the model to internalize structured planning strategies, laying the groundwork for embodiment-aware optimization in the subsequent training stage. 3.3.2 TASKAND EMBODIMENT-AWARE REINFORCED FINETUNING To foster robust and adaptable planning in dynamic, real-world environments, we introduce the Task and Embodiment-aware GRPO (TE-GRPO) algorithm. While prior methods have largely focused on improving semantic fidelity, they often neglect the physical feasibility of generated plans. TEGRPO seeks to bridge this gap by promoting outputs that are not only semantically aligned with task objectives but also executable within the constraints of robotic embodiment. Building on the original GRPO framework Shao et al. (2024), we retain the format reward rformat to incentivize the model to learn the think-answer reasoning pattern. To further guide the model Figure 3: Training Paradigm of OmniEVA. The two-stage cascade progressively enhances embodied intelligence: Stage 1 builds broad reasoning foundation, while Stage 2 grounds it in physical realityculminating in robust task execution across diverse real-world scenarios. toward generating both task-directed and physically feasible plans, we introduce two additional reward components: rtask (q, oi) = EvalTask(q, oi) [0, 1], rembod (q, oi) = EvalExec(q, oi) {0, 1} (8) where denotes the user prompt, oi is the i-th response of the model, EvalTask() evaluates whether the output semantically satisfies the task described by q, independent of physical constraints. For example, in pointing task, rtask is computed as the proportion of generated points that fall within the target region. In contrast, EvalExec() assesses embodiment feasibility by validating the plan against robotic constraints such as kinematics, reachability, and environment limitations within simulator. These two reward components reflect distinct optimization objectives: rtask emphasizes performance on offline evaluation benchmarks, while rembod targets end-to-end execution success in real-world robotic deployments. Progressive Embodiment Curriculum To accelerate convergence and promote physically grounded reasoning, we employ curriculum learning-inspired reward scheduling strategy. This approach gradually transitions the models optimization focus from semantic correctness to embodiment feasibility. At training step t, the composite accuracy reward racc i,t (q, oi) is defined as, racc i,t (q, oi) = rtask (q, oi) (cid:0)λt rembod (q, oi) + (1 λt)(cid:1) (9) where λt [0, 1] is scheduling coefficient that increases over time, gradually shifting the models focus from task completion to embodiment feasibility. Early in training, λt 0, allowing the model to receive positive reward even when embodiment constraints are not fully satisfied. As training progresses, λt 1, enforcing stricter adherence to physical constraints. The final reward for the i-th response is then computed as: ri,t(q, oi) = rformat (oi) + racc i,t (q, oi) (10) For group of responses with group size G, the normalized advantages of the i-th response at training step is calculated as: Ai,t = ri,t mean({r0,t, r1,t, , rG,t}) std({r0,t, r1,t, , rG,t}) (11) The final policy update objective is, i=1 Jt(θ) = (cid:20) 1 (cid:88) (cid:18) min (cid:18) πθ(oiq) πθold(oiq) Ai,t, clip (cid:18) πθ(oiq) πθold(oiq) (cid:19) (cid:19) , 1 ϵ, 1 + ϵ Ai,t βDKL(πθπθold) (cid:21) (12) where β is regularization coefficient that restricts the deviation degree between the current policy πθ and the reference policy πθold during optimization; ϵ is positive coefficient that limits the magnitude of policy updates, preventing training instability caused by excessive updates. Through this embodiment-aware training pipeline, OmniEVA evolves from perceptual understanding to physically grounded execution, enabling generalizable planning and reliable performance across diverse real-world scenarios."
        },
        {
            "title": "4 EXPERIMENTAL RESULTS",
            "content": "To assess the effectiveness of the proposed method, this section begins by detailing the benchmarks used for evaluation. Detailed implementation settingsincluding model architecture and training configurationsare provided in Appendix A. We then systematically address three core research questions that probe the capabilities and innovations introduced by our method: (1) How effectively does the dynamic 3D-grounding mechanism enhance multimodal reasoning, and how does it work? (2) Does the embodiment-aware reasoning lead to improved success rates in tasks requiring real-world robotic execution? How does it adapt to physical constraints? (3) Can OmniEVA solve long-horizon tasks by composing and sequencing primitive capabilities? 4.1 BENCHMARKS FOR EVALUATION 4.1.1 EMBODIED REASONING BENCHMARKS Embodied Reasoning Benchmarks with 2D Inputs To assess the models embodied reasoning capabilities across visual modalities, we employ four established benchmarks: Where2Place Yuan et al. (2024a), VSI-bench Yang et al. (2025b), PACO-LVIS Ramanathan et al. (2023), and RoboRefit Lu et al. (2023). 1 These datasets span both static images and dynamic video inputs, enabling comprehensive evaluation of spatial and temporal understanding and multimodal reasoning. To further evaluate the models capacities in versatile embodied tasks with physical constraints, we introduce four benchmarks that connect the primitive embodied capabilities with composite downstreamed tasks: Where2Go, Where2Fit, and Where2Approach, Where2Grasp. Compared to simulator-based online evaluation, this VQA-style approach substantially reduces evaluation overhead. Detailed examples and description can be found in Appendix C. Where2Go: The agent must select the most informative next view from multiple images to locate target object in partially observable environment. The setting closely aligns with the Large Space Object Seeking tasks, where agents must infer spatial layouts and make decisions under uncertainty. Where2Fit: The agent must identify the free space on the table by predicting set of 2D points. Physical constraints, including object location, size, collision potential, must be considered, making this task highly relevant to the Mobile Placement (Easy) tasks. Where2Approach: The agent must identify free space on the table that is not obstructed by any chairs. This task demands reasoning under occlusion as well as handling locomotion and manipulation constraints, making it closely aligned with the Mobile Placement (Hard) tasks in geometrically challenging scenarios. Where2Grasp: The agent must identify objects based on their color, size, location, and category. This task emphasizes object-centric recognition and directly aligns with the requirements of the Mobile Pick-up tasks. 1Since the original data annotations of RoboRefit and PACO-LVIS lack VQA pairs, we constructed minimal evaluation set suitable for VLM based on image distribution, object category, part category, etc. The evaluation code is consistent with Where2Place. 7 The relationship between primitive embodied capabilities and composite downstream tasks is demonstrated in Section 4.3 . Embodied Reasoning Benchmarks with 3D Inputs To extend evaluation into three-dimensional spatial contexts, we adopt four 3D benchmarks: Ma et al. (2022), ScanQA Azuma et al. (2022), Scan2Cap Chen et al. (2021) and ScanRefer Chen et al. (2020). These datasets challenge the models capacity for open-ended question answering, scene captioning, and 3D visual grounding within richly structured 3D environments. By incorporating depth and geometry, they serve as critical tests of the models ability to reason beyond planar representations."
        },
        {
            "title": "4.1.2 END-TO-END ONLINE EVALUATION WITHIN SIMULATORS",
            "content": "While previous works often evaluate the performance of the MLLMs on offline dataset, we also perform end-to-end evaluation to bridge the gap between planning and robot execution within simulators, on the following 3 introduced benchmarks. The benchmark is built based on 3000m2 office environment containing 8 core operation scenarios and 95 object categories representative of common workplace items. We categorize the benchmark into three progressive evaluation stages: Large-Space Object Seeking: It is also referred as object navigation in prior work. This task evaluates the agents capability to locate given object in large space. Local Mobile Manipulation: This evaluation set comprises over 30 representative scenarios featuring diverse background configurations, varing initial robot poses, and range of object types, sizes, and locations. The Mobile Pick-up task involves grasphing various objects across diverse scenes and tabletop configurations. The Moile Placement is divided into two difficulty tiers based on environment complexity. In the easy tier, the robot only needs to consider the immediate table surface condition (e.g., object occlusion) to determine the optimal placement location, as done in Where2Fit, before placing the object. For the hard tier tasks, the robot must fist determine the optimal chassis poses while accounting for environmental constraints imposed by the spatial arrangements of tabletop objects and surrounding chairs (same setting as Where2Approach). The evaluation involves navigating to target poses, followed by assessing trajectory planning for safe mug placement on the table, with success rates calculated based on task completion accuracy. comprehensive description of scenario design and task categorization is provided in AppendixD. End-to-End Delivery: This task evaluates the integration of embodied skills by requiring the robot to complete end-to-end object-delivery tasks across the entire office environment. We select two metrics, the overall success rates and the average task completion times, to evaluate the effectiveness of the pipeline. 4.2 TASK-ADAPTIVE 3D-GROUNDING: VALIDATION ACROSS MULTIMODAL BENCHMARKS How Effective Is the Task-Adaptive Gated Router? We compared our approach against two baselines. (1) Hard-coded 3D integration: The 3D features are integrated into visual tokens for all tasks, which is common strategy employed by prior 3D LLMs Zhu et al. (2024a); Zheng et al. (2025); Huang et al. (2025). (2) Without 3D integration: With 3D features disregarded, the model can be viewed as traditional 2D MLLM. As shown in Table 1, our method outperforms both baselines in three out of four tasks, yielding an average performance improvement of 1.22%. These results underscore the models superior adaptability and its capacity to leverage 3D information when contextually appropriate. Table 1: Results of Different 3D-Integration Methods. To eliminate the influence of 3D-free data, the experiments here are only trained on the training sets of SQA3D, ScanQA, Scan2Cap, and ScanRefer. Methods Benchmark Results SQA3D ScanQA Scan2Cap ScanRefer Average Baselines Hard-coded 3D Integration Without 3D Integration 61.21 61.15 Dynamic 3D Integration (Ours) 62.55 31.46 30.69 30. 95.49 75.46 97.86 41.19 4.30 43.06 57.34 42.90 58. 8 When Is the TAGR Module Activated? To illustrate the conditions under which the taskadaptive gated router (TAGR) activates, we conducted both quantitative and qualitative analysis. First, we examined the activation probabilities of prompt words across various tasks (Figure 4). Language signals related to geometric attributes (e.g., shape, square, rectangular) and spatial verbs (e.g., throwing, go, away) consistently elicited high activation scores. This pattern suggests that such linguistic cues implicitly signal the need for 3D spatial reasoning. Conversely, prompts centered on object counting or generic inquiries (e.g., many, nine) exhibited low activation, implying that these tasks rely predominantly on 2D visual features. Figure 4: Top 30 Words from Prompts Sorted by Gate Activation Rate: comparison of highest and lowest. To reduce the influence of statistical noise, the analysis was restricted to the 350 most frequent words. We further illustrate this behavior through qualitative case studies (Figure 5). In the first two examples, querying the shape of table and desk activates the 3D gate with differing probabilities: 0.73 for the rectangular table, indicating ambiguity between square and rectangular and thus reliance on 3D cues; and 0.52 for the round table, suggesting sufficient 2D visual information. In contrast, object counting and color identification in the two right-hand examples leave the 3D gate inactive, demonstrating the TGGR modules ability to omit 3D features when spatial reasoning is unnecessary. Figure 5: Case Study of Gate Activation State. Selected examples from the validation dataset illustrate the most prominently activated and deactivated words within the input prompts, highlighting the models sensitivity to specific language cues. Comparison Between OmniEVA and State-of-the-Art Models on 2D/3D Benchmarks Table 2 summarizes OmniEVAs performance across four 2D embodied reasoning benchmarks: Where2Place Yuan et al. (2024a), VSI-Bench Yang et al. (2025b), PACO-LVIS Ramanathan et al. (2023), and RoboRefit Lu et al. (2023). These tasks span both image and video modalities. Despite its relatively compact size (8B parameters), OmniEVA consistently achieves state-of-the-art performance across all benchmarks, surpassing significantly larger models including Robobrain-2.0-32B, GPT-4o, and Gemini-2.5-Pro. On average, it delivers performance gain of +10.45 compared with previous SOTARobobrain-32B. Extending to 3D embodied reasoning, we evaluated OmniEVA on four widely adopted benchmarks:: SQA3D Ma et al. (2022), ScanQA Azuma et al. (2022), Scan2Cap Chen et al. (2021), and ScanRefer Chen et al. (2020), which encompass 3D question answering, captioning, and 3D visual grounding tasks  (Table 3)  . OmniEVA again leads on three out of four benchmarks, outperforming state-of-the-art specialized 3D LLMs such as Video-3D-LLM Zheng et al. (2025) and 9 3DRS Huang et al. (2025) with notable improvements of +2.3, +0.3, and +8.5, respectively. While it slightly trails in 3D visual grounding (ScanRefer), OmniEVA sets new milestone by achieving 55.8 accuracy using purely text-based input and outputwithout relying on external detection modules or task-specific grounding heads. This result significantly exceeds the previous best of 44.4 by Spatial-3D-LLM Wang et al. (2025) in the same setting, highlighting the robustness and generality of OmniEVAs end-to-end reasoning capability. In addition to general embodied reasoning, OmniEVA demonstrates strong performance in downstream tasks such as Object Navigation, evaluated on the HM3D Ramakrishnan et al. (2021) and MP3D Chang et al. (2017) datasets. Here, the model is tasked to predict 3D subgoal location to guide exploration toward target object. As shown in Table 4, OmniEVA outperforms the stateof-the-art navigation model UniNavid Zhang et al. (2024a) in both success rate (SR) and path efficiency (SPL), achieving notable +5.4 improvement in SPL. Qualitative examples of exploration trajectories are provided in Appendix E.2. Table 2: 2D General Reasoning Benchmarks and In-house Benchmarks. [1] Hurst et al. (2024),[2] Team et al. (2025b),[3] Zhang et al. (2024b),[4] Li et al. (2024),[5] Zhu et al. (2025),[6] Bai et al. (2025),[7] Yuan et al. (2024a),[8] Azzolini et al. (2025),[9] Luo et al. (2025),[10] Yang et al. (2025a),[11] Team et al. (2025a) Models / Benchmarks General Models GPT-4o [1] Gemini-2.5-Pro [2] Llava-Next-Video 7B [3] Llava-OneVision 7B [4] InternVL3-8B [5] InternVL3-78B [5] Qwen2.5-VL-7B [6] Qwen2.5-VL-72B [6] Embodied Models RoboPoint [7] Cosmos-Reason1-7B [8] VeBrain-8B [9] Magma-8B [10] RoboBrain2.0-7B [11] RoboBrain-2.0-32B [11] OmniEVA 8B (Ours) Public Embodied Benchmarks In house Embodied Benchmarks Where2Place VSI-bench PACO-LVIS RoboRefit Where2Go Where2Fit Where2Approach Where2Grasp 20.41 28.60 4.76 5.87 12.68 21.74 10.99 39.92 46.80 5.51 11.34 10.89 63.59 73.59 74.95 43.60 48.83 35.62 32.57 42.89 48.48 37.51 39.41 - 25.64 26.30 12.65 36.10 42.69 57. 2.09 3.14 1.44 2.18 4.57 3.49 3.21 4.06 9.21 2.58 0.89 3.23 11.38 16.23 21.01 9.96 17.91 1.18 9.48 13.76 21.48 1.21 32.58 47.83 14.42 4.00 4.95 62.74 69.98 91. 50.72 55.07 31.88 0.00 41.06 51.69 38.16 49.76 - 40.10 28.98 0.00 38.64 41.06 86.96 37.15 41.82 61.34 63.32 33.07 41.16 38.59 41.49 56.64 38.86 28.47 28.45 32.99 59.23 78. 0.17 3.50 0.10 1.98 2.08 1.04 1.50 0.00 2.46 0.00 0.00 0.00 2.85 4.35 7.37 6.38 27.00 0.89 6.87 8.63 11.80 12.75 30.50 35.97 6.70 0.00 13.50 63.24 67.60 73. Table 3: 3D Reasoning Benchmarks. [1] Hong et al. (2023),[2] Zhu et al. (2024b),[3] Huang et al. (2023c),[4] Chen et al. (2024d),[5] Zhang et al. (2025),[6] Wang et al. (2025),[7] Huang et al. (2023b),[8] Huang et al. (2023a),[9] Chen et al. (2024c),[10] Zhu et al. (2024a),[11] Yu et al. (2025),[12] Deng et al. (2025),[13] Zheng et al. (2025),[14] Huang et al. (2025) Table 4: ObjNav Benchmarks [1] Wijmans et al. (2019),[2] Zhou et al. (2023),[3] Wu et al. (2024),[4] Yokoyama et al. (2024),[5] Huang et al. (2024),[6] Yin et al. (2024),[7] Yu et al. (2023),[8] Yin et al. (2025),[9] Ramrakhya et al. (2022),[10] Long et al. (2024),[11] Yadav et al. (2023b),[12] Yadav et al. (2023a),[13] Shah et al. (2023),[14] Ramrakhya et al. (2023),[15] Zhang et al. (2024a), Models SQA3D ScanQA Scan2Cap ScanRefer Methods EM EM CIDEr w.a. w/o.a. HM3D MP3D SR SPL SR SPL Baseline Models 3D-LLM(Flam) [1] 3D-LLM(blip2) [1] PQ3D [2] LEO [3] G-3D-LLM [4] SceneLLM [5] S-3D-LLM [6] ChatScene [7] Chat-3D v2 [8] LL3DA [9] LLaVA-3D [10] Inst3D-LMM [11] 3D-LLaVA [12] V-3D LLM[13] 3DRS [14] OmniEVA (Ours) 47.1 50.0 53.6 46.2 54.6 54.7 55.6 54.5 58.6 60.6 62.9 20.3 20. 21.5 27.2 21.6 27.0 24.6 30.1 30.3 30.6 72.4 70.6 72.2 77.1 63.9 62.9 79.2 79.7 62.9 83.8 86.1 94.6 57.0 47.9 55.5 42.5 54.1 57.8 58.1 62.9 21.2 30.3 44.3 55.8 Baseline Methods DD-PPO [1] ESC [2] VoroNav [3] VLFM [4] GAMap [5] SG-Nav [6] L3MVN [7] UniGoal [8] Habitat-Web [9] InstructNav [10] OVRL [11] OVRL-v2 [12] LFG [13] PIRLNav [14] UniNavid [15] OmniEVA (Ours) 27.9 39.2 42.0 52.5 53.1 54.0 54.2 54.5 57.6 58.0 62.0 62.8 68.9 70.4 73.7 74. 14.2 22.3 26.0 30.4 26.0 24.9 25.5 25.1 23.8 20.9 26.8 28.1 36.0 34.1 37.1 42.5 28.7 36.4 40.2 41.0 31.6 28.6 59.1 14.2 17.5 16.0 16.4 8.5 7.4 26."
        },
        {
            "title": "4.3 EMBODIMENT-AWARE REASONING: PERFORMANCE UNDER PHYSICAL CONSTRAINTS",
            "content": "Does task and embodiment-aware reasoning enhance success rates in real-world robotic tasks? To assess the effectiveness of task and embodiment-aware reasoning, we evaluates the performance of models trained with or without rtask and rembod. The evaluation spans both primitive skill benchmarks (Where2Approach, Where2Fit and Where2Grasp) and downstream tasks involving physical execution, namely Mobile Placement and Mobile Pickup. Quantitative results are summarized in Figure 6. Figure 6: Ablation Results of the proposed TE-GRPO Method on Local Mobile-Manipulation Tasks The results demonstrate that the TE-GRPO training method, which jointly optimizes the rtask and rembod, leads to significant performance improvements on both primitive skill benchmarks and downstream tasks. Specifically, Where2Approach and Where2Fit exhibit performance gains of 28.95% and 34.28%, respectively. These improvements are also directly reflected in the increased success rates of the Mobile Placement taskby 43% in the Easy variant and 50% in the Hard variant2. While both rtask and rembod contribute individually to performance enhancement, their combination yields maximized synergistic effects. However, we observe that the efficacy of rembod diminishes when the low-level control policy suffers from performance bottlenecks. For instance, although TE-GRPO improves the Where2Grasp benchmark score by 26.59%, the Mobile Pickup task exhibits only an 18.7% improvement with rtask, and no significant gain with rembod alone. Although our framework demonstrates improved object-pointing accuracy as reflected in the benchmark metrics, the ultimate execution performance remains constrained by the generalization capacity of the grasping policy. How does OmniEVA adapt to physical and embodiment constraints? To further understand the models embodiment awareness, we analyze OmniEVA trained with TE-GRPO. Figure 7 illustrates its reasoning process. OmniEVA begins by performing task-level analysis, including scenario interpretation, commonsense spatial analysis, and textual localization of vacant areas. It then incorporates physical constraintssuch as reachability and workspace boundariesto refine its predictions. Through such embodiment-aware slow thinking processes and reward reinforcement, the final output is placement region that satisfies all specified requirements. Prompt: Could you help me find vacant area on the table? Response: <think> ..., 1. Describing the Image: The image shows table with ... 2. Common-Sense Considerations: - vacant area must not overlap any existing objects ... large enough to accommodate ... avoid edges or corners where objects might fall off. 3. Vacant Area Identification: The left side of the table appears to have some empty space, particularly around the yellow cup and the green plate. 4. Embodiment Awareness: Considering the range of the robots executable area, it should be placed as close to the bottom edge of the table as possible. 5. Output: - Based on the analysis: ... </think> <answer> Points are <point>(319, 416), (317, 404), (328, 398), (329, 410)</point>.</answer> Figure 7: Case study illustrating OmniEVAs reasoning process under embodiment-aware constraints. 2The downstream task requires only feasible solution for local motion and manipulation, whereas the benchmark demands an optimal one. See Appendix for illustrative examples. 11 We also compare outputs from models trained with and without it for the same task, as shown in Figure 8. Results show that both models effectively identify unoccupied areas on the tabletop, indicating satisfactory task-level performance. However, the placement locations proposed by the model without TE-GRPO training frequently fall outside the operational range of the robotic arm or exhibit suboptimal execution efficiency. In contrast, OmniEVA trained with TE-GRPO consistently identifies vacant areas that are both feasible and executable, demonstrating enhanced alignment with physical and task constraints. w/o. TE-GRPO. w. TE-GRPO. w/o. TE-GRPO. w. TE-GRPO. Figure 8: Comparison of response w/o and embodiment-aware reasoning. Real World Experiments To evaluate generalization in physical environments, we deploy the model on wheeled dual-arm robotic platform. Examples of real-world executions are provided in Appendix F. OmniEVA demonstrates robust reasoning capabilities, effectively translating user instructions into physically executable plans. These results affirm the models ability to generalize embodiment-aware reasoning across diverse physical constraints."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper presents OmniEVA, an embodied versatile planner designed to perform robust crossdimensional reasoning across wide spectrum of embodied tasks. OmniEVA is the first to incorporate an explicit dynamic routing mechanism for 3D grounding, significantly enhancing its adaptability and reasoning performance under varied task demands. Furthermore, OmniEVA introduces an embodiment-aware fine-tuning strategy that effectively bridges the gap between semantic reasoning and robotic execution. This enables the generation of plans that are not only logically sound but also physically feasible in real-world environments. By unifying semantic embodied reasoning with actionable planning, OmniEVA marks substantial step forward in the development of generalpurpose embodied agents capable of reasoning, planning, and executing across diverse domains."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We would like to thank the following individuals for their valuable contributions and support during the course of this research: Zhanpeng Zhang, Pengxiang Xia, Jingzhou Luo, Yunpeng Ma, Yehai Yang, Guangyuan Zou, Miaomiao Ouyang, Zhirui Yan, Helong Huang, Tongtong Cao, Guowei Huang, Weichao Qiu, Haoting Qu, Weixing Chen, Shunbo Zhou, Junyi Dong, Yu Huang, Yang Xu, Lei Zhang, Bowen Shi, Bin Zou."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1912919139, 2022. Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. 12 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1445514465, 2024a. Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European conference on computer vision, pp. 202221. Springer, 2020. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pp. 370387. Springer, 2024b. Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2642826438, 2024c. Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models. CoRR, 2023. Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Runsen Xu, Ruiyuan Lyu, Dahua Lin, and Jiangmiao Pang. Grounded 3d-llm with referent tokens. arXiv preprint arXiv:2405.10370, 2024d. Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 31933203, 2021. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Jiajun Deng, Tianyu He, Li Jiang, Tianyu Wang, Feras Dayoub, and Ian Reid. 3d-llava: Towards generalist 3d lmms with omni superpoint transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 37723782, 2025. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. Andrew Guo, Bowen Wen, Jianhe Yuan, Jonathan Tremblay, Stephen Tyree, Jeffrey Smith, and Stan Birchfield. Handal: dataset of real-world manipulable object categories with pose annotations, affordances, and reconstructions. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1142811435. IEEE, 2023. 13 Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 53565364, 2019. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, and Zhou Zhao. Chat-3d v2: Bridging 3d scene and large language models with object identifiers. CoRR, 2023a. Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, and Zhou Zhao. Chat-3d v2: Bridging 3d scene and large language models with object identifiers. CoRR, 2023b. Hao Huang, Yu Hao, Congcong Wen, Anthony Tzes, Yi Fang, et al. Gamap: Zero-shot object goal navigation with multi-scale geometric-affordance guidance. Advances in Neural Information Processing Systems, 37:3938639408, 2024. Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023c. Xiaohu Huang, Jingjing Wu, Qunyi Xie, and Kai Han. Mllms need 3d-aware representation supervision for scene understanding. arXiv preprint arXiv:2506.01946, 2025. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17241734, 2025. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Yuxing Long, Wenzhe Cai, Hongcheng Wang, Guanqi Zhan, and Hao Dong. Instructnav: ZeroarXiv preprint shot system for generic instruction navigation in unexplored environment. arXiv:2406.04882, 2024. Yuhao Lu, Yixuan Fan, Beixing Deng, Fangfu Liu, Yali Li, and Shengjin Wang. Vl-grasp: 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 976983. IEEE, 2023. 14 Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, et al. Visual embodied brain: Let multimodal large language models see, think, and control in spaces. arXiv preprint arXiv:2506.00123, 2025. Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Learning affordance grounding from exocentric images. In CVPR, 2022. Ruiyuan Lyu, Jingli Lin, Tai Wang, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, et al. Mmscan: multi-modal 3d scene dataset with hierarchical grounded language annotations. Advances in Neural Information Processing Systems, 37: 5089850924, 2024. Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1648816498, 2024. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual In Proceedings of the IEEE/cvf question answering benchmark requiring external knowledge. conference on computer vision and pattern recognition, pp. 31953204, 2019. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual In 2019 international conference on document question answering by reading text in images. analysis and recognition (ICDAR), pp. 947952. IEEE, 2019. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903. IEEE, 2024. Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, et al. Habitat 3.0: co-habitat for humans, avatars and robots. arXiv preprint arXiv:2310.13724, 2023. Santhosh Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv preprint arXiv:2109.08238, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 71417151, 2023. Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das. Habitat-web: Learning In Proceedings of the embodied object-search strategies from human demonstrations at scale. IEEE/CVF conference on computer vision and pattern recognition, pp. 51735183, 2022. Ram Ramrakhya, Dhruv Batra, Erik Wijmans, and Abhishek Das. Pirlnav: Pretraining with imitation and rl finetuning for objectnav. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1789617906, 2023. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. Nils Reimers and Hugging Face. sentence-transformers/all-minilm-l6-v2. huggingface.co/sentence-transformers/all-MiniLM-L6-v2, 2021. cessed: 2025-09-08. https:// Ac15 Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv. org/abs/1908.10084. Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301, 2022. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. In European A-okvqa: benchmark for visual question answering using world knowledge. conference on computer vision, pp. 146162. Springer, 2022. Dhruv Shah, Michael Robert Equi, Błazej Osinski, Fei Xia, Brian Ichter, and Sergey Levine. Navigation with large language models: Semantic guesswork as heuristic for planning. In Conference on Robot Learning, pp. 26832699. PMLR, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1576815780, 2025. BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025a. Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025b. Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Nießner. Rio: 3d object instance re-localization in changing indoor environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 76587667, 2019. Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pp. 17231736. PMLR, 2023. Xiaoyan Wang, Zeju Li, Yifan Xu, Jiaxing Qi, Zhifei Yang, Ruifei Ma, Xiangde Liu, and Chao Zhang. Spatial 3d-llm: Exploring spatial awareness in 3d vision-language models. arXiv preprint arXiv:2507.16524, 2025. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. arXiv preprint arXiv:1911.00357, 2019. Pengying Wu, Yao Mu, Bingxian Wu, Yi Hou, Ji Ma, Shanghang Zhang, and Chang Liu. Voronav: Voronoi-based zero-shot object navigation with large language model. arXiv preprint arXiv:2401.02695, 2024. Karmesh Yadav, Arjun Majumdar, Ram Ramrakhya, Naoki Yokoyama, Alexei Baevski, Zsolt Kira, Oleksandr Maksymets, and Dhruv Batra. Ovrl-v2: simple state-of-art baseline for imagenav and objectnav. arXiv preprint arXiv:2303.07798, 2023a. 16 Karmesh Yadav, Ram Ramrakhya, Arjun Majumdar, Vincent-Pierre Berges, Sachit Kuhar, Dhruv Batra, Alexei Baevski, and Oleksandr Maksymets. Offline visual representation learning for embodied navigation. In Workshop on Reincarnating Reinforcement Learning at ICLR 2023, 2023b. Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1420314214, 2025a. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1063210643, 2025b. Hang Yin, Xiuwei Xu, Zhenyu Wu, Jie Zhou, and Jiwen Lu. Sg-nav: Online 3d scene graph prompting for llm-based zero-shot object navigation. Advances in neural information processing systems, 37:52855307, 2024. Hang Yin, Xiuwei Xu, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu. Unigoal: Towards universal zero-shot goal-oriented navigation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1905719066, 2025. Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, and Bernadette Bucher. Vlfm: Visionlanguage frontier maps for zero-shot semantic navigation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 4248. IEEE, 2024. Bangguo Yu, Hamidreza Kasaei, and Ming Cao. L3mvn: Leveraging large language models for visual target navigation. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 35543560. IEEE, 2023. Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, and Jianke Zhu. Inst3d-lmm: Instance-aware 3d scene understanding with multi-modal instruction tuning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1414714157, 2025. Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pp. 6985. Springer, 2016. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024a. Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. In Proceedings of the IEEE/CVF Osprey: Pixel understanding with visual instruction tuning. Conference on Computer Vision and Pattern Recognition, pp. 2820228211, 2024b. Hang Zhang, Zhuoling Li, and Jun Liu. Scenellm: Implicit language reasoning in llm for dynamic scene graph generation. Pattern Recognition, pp. 111992, 2025. Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang. Uni-navid: video-based vision-language-action model for unifying embodied navigation tasks. arXiv preprint arXiv:2412.06224, 2024a. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d llm: Learning position-aware video representation for 3d scene understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 89959006, 2025. Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, and Xin Eric Wang. Esc: Exploration with soft commonsense constraints for zero-shot object navigation. In International Conference on Machine Learning, pp. 4282942842. PMLR, 2023. Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024a. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Ziyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan Huang, and Qing Li. Unifying 3d vision-language understanding via promptable queries. In European Conference on Computer Vision, pp. 188206. Springer, 2024b."
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "A.1 MODEL ARCHITECTURE AND TRAINING CONFIGURATIONS Our experiments are built upon the pretrained InternVL3-8B model Zhu et al. (2025), which serves as the foundational backbone for our multimodal large language models (MLLMs). To encode user instructions within the task-adaptive gated routing (TAGR) module, we utilize the all-MiniLM-L6v2 Reimers & Face (2021), chosen for its efficiency and semantic fidelity. Instruction embeddings are further processed through lightweight two-layer multilayer perceptron (MLP) with hidden dimension of 256, enabling compact yet expressive representation learning. During training, we freeze the parameters of the vision transformer to preserve its pretrained visual semantics, while fine-tuning the LLM backbone to adapt to downstream multimodal tasks. Optimization is performed using the AdamW optimizer with batch size of 128 and warm-up ratio of 0.01. We employ cosine learning rate schedule, initializing the lr of LLM backbone at 1e5 and the TAGR module at 1e4. For video-based inputs, we uniformly sample 16 frames during training and 32 frames during inference, striking balance between temporal granularity and computational efficiency. To handle 3D spatial information, we voxelize both point clouds for positioning and 3D bounding boxes using fixed voxel size of 0.1 meters. This discretization facilitates consistent spatial reasoning across diverse environments and tasks. Detailed hyper-parameters as given in Table 5 Table 5: OmniEVA Training Hyper-parameter Configuration Hyper-parameters TAGR Pretraining Supervised Finetuning Reinforced Finetuning epochs batch size learning rate (LLM) learning rate (TAGR) learning rate (ViTs) lr schedular init τ (gumbel softmax) final τ (gumbel softmax) weight decay gradient clipping use bf16 use fp16 warmup ratio optimizer image resolution video frames (training) video frames (inference) 3D voxel size 1 256 5e-7 1e-4 - cosine 1.0 0.05 0.1 1.0 true false 1e-3 AdamW 448448 16 32 0.1 1 256 1e-5 - - cosine 1e-6 1e-6 0.1 1.0 true false 1e-3 AdamW 448448 16 32 0.1 1 128 1e-5 - - - 1e-6 1e-6 0.1 1.0 true false 0.0 AdamW 448448 16 32 0. A.2 INPUT MODALITIES AND OUTPUT REPRESENTATIONS OmniEVA is designed to accommodate wide range of input modalities and output formats, enabling versatile interaction across visual and textual domains. Below, we detail the supported configurations. A.2.1 VISUAL INPUT MODALITIES Single Image Ideal for static or minimally dynamic environments, single-frame inputs support 2D spatial reasoning tasks such as object recognition, scene description, and basic grounding. This modality is particularly effective when temporal context is unnecessary and spatial relationships are confined to single viewpoint. Multi-View Images or Video By aggregating information across multiple viewpoints or temporal frames, this modality facilitates both spatial and temporal reasoning. It is well-suited for dynamic 19 or large-scale environments where understanding motion, continuity, or cross-frame object relationships is essentialsuch as in navigation, tracking, or multi-step manipulation tasks. RGB-D Video This modality integrates RGB visual data with depth information to reconstruct full 3D scene geometry. It is indispensable for tasks requiring occlusion-aware reasoning, volumetric understanding, or precise spatial manipulation. To enable accurate position embedding in world coordinates, users must provide the intrinsic camera matrix and corresponding extrinsic poses for each frame. These parameters allow the model to transform depth maps into structured 3D representations, forming the foundation for geometry-aware decision-making. A.2.2 TEXTUAL AND COORDINATE-BASED OUTPUTS OmniEVA accommodates range of textual and spatial formats for both input queries and output responses, enabling flexible interaction across semantic and geometric dimensions. Natural Language Queries and Responses Natural language serves as the primary interface for user interaction, supporting expressive queries and interpretable model responses. This format aligns with standard benchmarks in VQA and facilitates rich semantic engagement, allowing users to specify tasks in intuitive, human-readable form. 2D Spatial Annotations For tasks such as 2D visual grounding and image captioning, inputs and outputs can be expressed using normalized pixel coordinates within the range [0, 1000]. This format enables precise object localization and descriptive annotation within single image frame. Examples: Question: Describe the object located at <point>(24, 312)</point>. Answer: It is brown book next to pencil. Question: Locate the apple on the left side of the book. Answer: <point>(122, 213)</point>. 3D Spatial Annotations For tasks involving 3D spatial reasoningsuch as object captioning, grounding, and navigationusers may specify coordinates manually or allow the model to infer them from RGB-D inputs. Coordinates are discretized using 0.1-meter grid to ensure consistency and precision across scenes. Question: What is the object located at <3dbox>(61,217,26,5,7,3)</3dbox>? Answer: It is brown wooden chair located at the center of the room. Question: Locate the second chair next to the table. Answer: <3dbox>(74,213,123,10,8,8)</3dbox>. This format empowers OmniEVA to reason about partially occluded objects and those outside the current frame, enabling robust interpretation and interaction within complex 3D environments.huozh"
        },
        {
            "title": "B TRAINING DATASET",
            "content": "B.1 DATASET OVERVIEW Figure 9 presents comprehensive breakdown of the training dataset utilized by OmniEVA. Designed to support omni-multimodal and cross-dimensional reasoning, the dataset integrates three major categories: general data, image-based reasoning data, and 3D reasoning data. This diverse composition enables OmniEVA to develop robust capabilities across wide spectrum of tasks, from basic object recognition to complex spatial and semantic understanding. In total, the dataset comprises approximately 5.2 million samples. Detailed descriptions and distributions of each data category are provided in the subsequent sections. 20 Figure 9: Overview of the Training Data used by OmniEVA B.2 GENERAL EMBODIED REASONING DATA General Visual Question Answering To maintain the foundational visual reasoning capabilities and generalization strength of the vision-language model (VLM), we integrated diverse set of general visual question answering (VQA) datasets. Notably, LLaVA-665K Liu et al. (2023) contributes broad spectrum of tasks, including VQA, optical character recognition (OCR), region localization, and instruction-following. To further enrich the dataset, we incorporated academic datasets such as GQA Hudson & Manning (2019), OKVQA Marino et al. (2019), and AOKVQA Schwenk et al. (2022), which emphasize compositional reasoning and external knowledge grounding. OCR-focused datasets like OCR-VQA Mishra et al. (2019), TextVQA Singh et al. (2019), and ScienceQA Saikh et al. (2022) enhance the models ability to interpret embedded textual content. Additionally, region-level vision-language understanding is supported through RefCOCO Yu et al. (2016) and Visual Genome Krishna et al. (2017), which provide fine-grained spatial and semantic annotations. To bolster language fluency and multimodal dialogue coherence, we also included approximately 40,000 pure text instruction samples from ShareGPT Chen et al. (2024b). 2D Visual Grounding To endow OmniEVA with robust object detection and geometric localization capabilities, we incorporated the LVIS Gupta et al. (2019) dataseta comprehensive benchmark for large-vocabulary instance segmentation. LVIS offers approximately 164,000 images annotated with nearly 2 million high-quality segmentation masks, spanning over 1,000 entry-level object categories such as chair and plate. Its rich diversity and precise annotations make it an ideal foundation for training models in spatially grounded object recognition. We utilize 140K samples from LVIS to train the 2D visual grounding module. Object Reference Object reference data plays crucial role in enabling OmniEVA to associate linguistic instructions with specific visual regions. We curated three complementary datasets to support this capability. (1) Osprey-724K Yuan et al. (2024b) is large-scale instruction tuning dataset specifically constructed to achieve pixel-level vision-language alignment. Designed to overcome the limitations of traditional multimodal modelswhich primarily operate at the image-level or bounding box-levelit incorporates regional masks linked with precise language descriptions to enhance model performance in fine-grained visual understanding tasks. (2) Robopoint Object Reference347K Yuan et al. (2024a) involves locating keypoints within given reference object in an image. For example: In the image, an object is marked with red box. Please indicate several points located in the area below this object. The models response would be in the form of normalized coordinates such as [(0.56, 0.69), ...]. Such data helps the model learn to accurately identify target positions that have spatial relationships with reference objects, making it suitable for applications like robotic grasping or object association scenarios. (3) RoboRefIt Lu et al. (2023) specifically de21 signed for visual grounding tasks in robot interaction. It aims to enhance robots ability to recognize and locate target objects based on language instructions in real-world scenarios. The dataset comprises 10,872 real RGB-D images collected from cluttered indoor environments in daily life. Each image is annotated with referring expressions (instruction sentences), totaling 50,758 entries, which describe object features or locations in robot-oriented language style. Approximately half of the images contain similar or distracting objects, increasing recognition difficulty to simulate challenges in real-world grasping scenarios. Together, these datasets provide 511K training samples for object reference grounding. Object Part Recognition Part recognition data is primarily used to visually highlight specific functional parts of objects in images, as referred to by corresponding linguistic instructions. These datasets include: (1) AGD20K Luo et al. (2022) is constructed by collecting and labeling over 20K images from 36 affordance categories, such as sit on, type on, and drink etc. Affordance grounding aims to locate objects action possibilities regions, an essential step toward embodied (2) HANDAL Guo et al. (2023) is used for category-level object pose estimation intelligence. and affordance prediction. Unlike previous datasets, it is focused on robotics-ready manipulable objects that are of the proper size and shape for functional grasping by robot manipulators, such as pliers, utensils, and screwdrivers. The dataset consists of 308k annotated image frames from 2.2k videos of 212 real-world objects in 17 categories. It focus on hardware and kitchen tool objects to facilitate research in practical scenarios in which robot manipulator needs to interact with the environment beyond simple pushing or indiscriminate grasping. (3) PACO Ramanathan et al. (2023) is large-scale dataset constructed for fine-grained image understanding tasks, designed to support objectand part-level instance segmentation as well as attribute recognition. It contains 57,643 images with 1,644,461 annotated object instances spanning 270 distinct categories (e.g., body, rim, handle). The dataset can be used for object detection, semantic segmentation, and instance segmentation tasks, and supports conversion from instance masks to semantic masks or bounding boxes for diverse downstream applications. In total, we utilize 408K samples from these datasets. Free Space Location Free space location data is primarily used to visually mark vacant placement areas in images, as indicated by corresponding linguistic instructions. These datasets include: (1) Robopoint Free Space Reference-320K Yuan et al. (2024a) In this dataset, the language instructions require the model to identify keypoints in free space near reference objectdespite the absence of clear visual cues. For example: Indicate several points in the empty space to the left of the pizza box. This type of data enables the model to understand where is suitable region to perform an action, even if no object is visually present, making it highly applicable to robotic navigation or assisted placement tasks. (2) RefSpatial 3D Vacant Zhou et al. (2025) RefSpatial is large-scale dataset created to support Visual Language Models (VLMs) in performing 3D multi-step reasoning for spatial referencing tasks. It aims to enhance models ability to understand complex spatial instructions in real-world environments. The dataset contains approximately 20 million (20M) question-answer (QA) pairs, covering 31 spatial relation categories, and supports multi-step reasoning of up to 5 steps. It includes locate empty space: define point in an empty area on surface based on its spatial relationships with surrounding objects, and ask to confirm this empty location (e.g., Please provide point in the vacant area on the desktop that simultaneously satisfies the following spatial conditions: ...). (3) Open-X-Embodiment ONeill et al. (2024) In current datasets involving actions performed by robots/robotic arms, skills typically include pick, pick and place, and more. We extract keyframes and manipulated objects from the corresponding trajectory data to construct object placement data. For example: RT-1 Brohan et al. (2022) dataset comprises over 130,000 real-world robotic demonstrations (episodes), covering more than 700 different tasks. These were collected by 13 robots over period of 17 months. The trained actions include diverse skills such as grasping/placing objects, opening/closing drawers, extracting items, standing objects upright, knocking them over, pulling tissues, and opening jars. BridgeData V2 Walke et al. (2023) dataset contains 60,096 trajectories spanning 24 different environments, including toy kitchens, sinks, microwaves, desktops, washing machines, toolboxes, and other diverse settings. It encompasses 13 types of manipulation skills, ranging from basic pick-and-place, pushing/pulling, and sweeping, to more complex tasks such as stacking blocks, folding cloth, and manipulating granular media. Together, these datasets provide 530K training samples. 22 Video-based Spatial Reasoning Despite the emergence of benchmarks such as OpenEQA Majumdar et al. (2024) and VSI-Bench Yang et al. (2025b), large-scale training datasets for video-based spatial reasoning remain limited. To bridge this gap, we construct comprehensive dataset by harnessing high-fidelity indoor scene sources from ScanNet Dai et al. (2017), Matterport3D Chang et al. (2017), and 3RScan Wald et al. (2019). From these sources, we extract egocentric video sequences and generate questionanswer pairs aligned with the task taxonomy defined in VSI-Bench Yang et al. (2025b), encompassing: (1) object count, (2) relative distance, (3) appearance order, (4) relative direction, (5) object size, (6) absolute distance, (7) room size, and (8) route planning. Each QA pair is produced through hybrid pipeline combining automated template generation with manual verification to ensure spatial coherence and semantic precision. For route planning tasks, we first convert point clouds into x-y navigation mesh maps. Navigable waypoints are selected based on three independently defined anchors: the start object, its orientation, and the end object. Using the A* algorithm, we compute the shortest path while merging trajectory points of adjacent objects belonging to the same entity. Steering directions are then derived from angular changes along the path, enabling fine-grained spatial reasoning. We use 281K samples for training. Active Exploration Prior approaches typically assume fully observable environments, limiting their applicability to real-world scenarios. We propose novel task that enhances spatial reasoning under partial observability. Given multiple images from an indoor scene, the model must select the most informative view to locate specified object. For instance: From the provided visual input, identify the most informative image frame that offers the best chance of locating the bed. Format your response as: Frame ID: [Selected Frame ID]. This task strengthens the models decisionmaking in incomplete environments and is crucial for downstream embodied tasks such as object navigation. To support this, we curated 18K training samples from HM3D Ramakrishnan et al. (2021) and MP3D Chang et al. (2017), covering 6 and 21 object categories respectively. 3D Visual Question Answering To advance spatial reasoning in 3D environments, we integrate three complementary datasets, each contributing unique challenges and perspectives. (1) SQA3D Azuma et al. (2022): This dataset emphasizes situational awareness, requiring agents to interpret their position, orientation, and context within 3D scene before answering questions. It simulates real-world embodied cognition, where understanding ones spatial state is prerequisite for reasoning. We collected approximately 79K samples, covering diverse indoor layouts and object configurations. (2) ScanQA Ma et al. (2022): Focused on general spatial understanding, ScanQA includes questions about object alignment, relative direction, and localization. It challenges models to parse nuanced spatial relationships from textual queries and visual cues. Our training set includes 23K samples, offering rich variety of spatial scenarios. (3) MMScan-QA Lyu et al. (2024): As the largest and most comprehensive resource, MMScan provides over 1.28M QA samples built on ScanNet Dai et al. (2017), Matterport3D Chang et al. (2017), 3RScan Wald et al. (2019), and Arkitscenes Baruch et al. (2021). It features hierarchical grounded language annotations spanning object-level and region-level semantics, enabling multi-granular reasoning. In total, we utilize approximately 1.4M samples for 3D VQA training. 3D Captioning In this task, the model generates descriptive captions for objects given 3D position or bounding box, detailing attributes such as color, shape, and spatial relations. For instance, Question: Describe the object located at (155,72,23,15,13,3). Answer: It is light brown, wooden chair, located in front of white table in the room. This task bridges geometric localization with natural language generation. We train on the Scan2Cap dataset Chen et al. (2021), which comprises 37K annotated samples across diverse indoor scenes. 3D Visual Grounding As the inverse of 3D captioning, this task requires the model to localize objects in 3D space based on natural language descriptions. It poses significant challenge for MLLMs, which often struggle to generate accurate 3D bounding boxes without priors from off-theshelf 2D or 3D detectors. For example, Question: Detect the bounding box of chair in the corner of the room, opposite to brown desk. Answer: (78, 23, 135, 5, 5, 7). We leverage ScanRefer Chen et al. (2020) and MMScan-VG Lyu et al. (2024), totaling 1.1M samples. 3D Scene Imagination To push the boundaries of spatial reasoning, we introduce task set in partially observable environmentswhere some objects are occluded or outside the agents field of 23 view. The model must infer the contents of unobserved regions based on contextual cues and spatial layout, given 3D location within the scenario. For example, Question: Based on the currently observed environment, when the agent walks to position (384, 42, 15), what new objects might become visible? Only consider objects not currently seen. Answer: You may see various cookers, cabinets, kitchen counters, kettles, .... This task probes the models understanding of object cooccurrence and spatial regularities. We collect 45K samples using the Habitat simulator Puig et al. (2023), drawing from MP3D Chang et al. (2017) and HM3D Ramakrishnan et al. (2021) with randomized walk policy to ensure diverse and unbiased scene coverage. 3D Subgoal Prediction Existing spatial reasoning methods for large-scale navigationimagebased pointing Yuan et al. (2024a), marker selection, or direct command outputs (e.g., move forward or turn left) often struggle in complex, occlusion-heavy scenes under partial observability. To overcome these limitations, we introduce 3D-aware planning framework that ingests sequential RGB-D observations and directly generates subgoals in continuous 3D coordinate space. By formulating intermediate objectives as 3D waypoints, the model leverages the geometric structure of the environment, avoids local optima caused by relying on single image views, and supports explicit long-term trajectory planning rather than making only myopic action predictions. Moreover, it accounts for occluded or unseen regions, enabling the agent to propose subgoals that guide exploration around obstacles and through partially observed areas. Our training set includes approximately 113K samples, supporting robust learning of spatial planning under uncertainty. EXAMPLES OF THE IN-HOUSE PRIMITIVE EMBODIED BENCHMARKS C.1 WHERE2GO The Where2Go benchmark is constructed using the validation splits of the HM3D Chang et al. (2017) and MP3D Chang et al. (2017) datasets. Each sample presents partially observable environment in which the model must select the most informative view to locate specified target object. frame is designated as the ground truth if it contains visible segment of the shortest navigable path from the agents current position to the target object. In total, the benchmark comprises 207 samples, forming diverse and challenging validation set for evaluating view selection under uncertainty. Prompt: From the provided visual input, identify the most informative image frame (with IDs starting from 1) that offers the best chance of locating the sofa. Format your response as: Frame ID: [Selected Frame ID] Ground Truth: Frame ID: 5, 6 24 Prompt: From the provided visual input, identify the most informative image frame (with IDs starting from 1) that offers the best chance of locating the tv monitor. Format your response as: Frame ID: [Selected Frame ID] Ground Truth: Frame ID: 4 Prompt: From the provided visual input, identify the most informative image frame (with IDs starting from 1) that offers the best chance of locating the plant. Format your response as: Frame ID: [Selected Frame ID] Ground Truth: Frame ID: 1, 2, C.2 WHERE2FIT The Where2Fit Benchmark addresses the task of identifying free space on tables by predicting set of 2D points. These tables are drawn from variety of real-world scenessuch as offices, conference rooms, pantries, and workstationsand exhibit different levels of clutter, ranging from blank and sparse to densely occupied. The benchmark systematically increases the number of objects across these clutter conditions, presenting progressive challenge. In addition, it incorporates critical physical constraints, including object dimensions, fit within the available space, and collision avoidance with other objects. The entire benchmark consists of 464 samples, including 200 generation tasks that require the model to output corresponding points, and 264 judgment tasks where the model must determine whether given point would cause collision. 25 Prompt: Locate some free space for me on the table. Prompt: Find me an empty spot on the table, thanks! Prompt: Would you be able to place the red plug on the conveyor belt? Prompt: Could you help me find vacant area on the table? C.3 WHERE2APPROACH The Where2Approach benchmark is required to identify unobstructed free space on table while accounting for potential occlusions caused by surrounding chairs. The testbed features long table cluttered with objects and encircled by randomly arranged chairs, simulating geometrically complex and occlusion-rich environments. This task necessitates advanced spatial reasoning under substantial visual occlusion, as well as the integration of locomotion and manipulation constraints. Specif26 ically, the agent must determine feasible chassis positions that offer sufficient unobstructed area for successful placement operations. These requirements closely align with the challenges posed by Mobile Placement (Hard) tasks, which emphasize operation in highly constrained and visually disordered scenarios. The entire test set consists of 200 samples, each covering completely different perspectives, robot positions, tabletop object configurations, and chair arrangements. Prompt: Find the nearest free space on the table with no chairs around. Prompt: Locate the closest empty spot on the table that isnt surrounded by chairs. Prompt: Look for the nearest available area on the table where no chairs are placed nearby. Prompt: Search for nearby open space on the table that has no chairs in its vicinity. 27 C.4 WHERE2GRASP The Where2Grasp benchmark requires the identification of objects based on key attributes including color, size, location, and category. The evaluation set consist 200 samples and encompasses over 40 common object categories sourced from variety of household and office environments, with diverse backgrounds. This task emphasizes object-centric cognitive capabilities, focusing on the perception and interpretation of object characteristics under real-world conditions. Prompt: Locate the orange on the counter. Prompt: Please locate the glasses. Prompt: Locate the cola bottle on the table. Prompt: Find the junction box on the conveyor belt."
        },
        {
            "title": "D DOWNSTREAM TASK DESCRIPTION",
            "content": "D.1 MOBILE PLACEMENT EASY For the Mobile Placement Easy benchmark, we constructed scenes with 8 tables in an office environment, with various items randomly scattered on the tabletops. There are total of 40 types of items to enhance the diversity of the scenes. The robots initial position is 1 to 1.5 meters away from the edge of the table, with an angular deviation of -15 to 15 degrees, to observe the environment and objects. The scenes are divided into three levels based on the number of randomly scattered items on the tabletop: no-objects, sparse, and dense, with 0, 4, and 8 objects on the tabletop, respectively. The models performance is evaluated in 200 simulation scenes, with 50 no-object scenes, 50 sparse scenes, and 100 dense scenes. We use the success rate of placing objects as the evaluation metric. D.2 MOBILE PLACEMENT HARD In the Mobile Placement Hard benchmark, we construct long-table environment with chairs positioned around the perimeter and varied objects distributed on the table surface. The robot is required to identify approachable regions unobstructed by chairs or densely placed tabletop items. The table periphery is systematically divided into 12 candidate zones (three per long side, one per short side, and four corners), each classified as either occupied (by chairs or dense objects, excluding corners) or vacant (clear spaces or sparse-object areas permitting approach). We additionally distribute random objects in the central table area, which improves visual realism and increases scene diversity. Each simulation scenario involves randomized assignment of occupancy states to candidate regions, with objects sampled from 70-category office inventory (e.g., monitors, books, mugs). The robot is initialized 0.9 to 1.5 meters from the table edge (15 deg angular deviation) to observe the table and surroundings. The evaluation requires the model to predict feasible placement points, followed by robot execution attempts. Performance is assessed in 100 simulated scenarios, where we measure the success rates of planning placement trajectories. D.3 MOBILE PICKUP For the Mobile Pick-up benchmark, we constructed scenes with 8 tables in an office environment, with various items randomly scattered on the tabletops. The objects to be picked up include: pen holders, cans, bottles, coffee cups, oranges, pears, apples, and lemons. There are total of 40 background objects to enhance the diversity of the scenes. The robots initial position is 1 to 1.5 meters away from the edge of the table, with an angular deviation of -15 to 15 degrees, to observe the environment and objects. The scenes are divided into three levels based on the number of randomly scattered items on the tabletop: easy, medium, and hard. In the easy level, only the objects to be picked up are on the tabletop, while in the medium and hard levels, there are 3 and 7 additional objects besides the ones to be picked up. The models performance is evaluated in 150 simulation scenes, with 60 easy scenes, 50 medium scenes, and 40 hard scenes. We use the success rate of picking up objects as the evaluation metric."
        },
        {
            "title": "E QUALITATIVE RESULTS OF OMNIEVA",
            "content": "Here we present sample inference results generated by OmniEVA, including 2D visual grounding, object reference, object part recognition, free space location, activate exploration, 3d questionn answering, 3d captioning, 3d visual grounding and 3d scene imagination. All examples shown were not included in the training dataset. 30 E.1 QUALITATIVE RESULTS ON 2D SPATIAL REASONING TASKS 2D Visual Grounding Prompt: In the photo, where can find the shirt? Response: <point>(457, 117), (428, 125)</point>. You can find the shirt at Prompt: Highlight the location of the tarp. region: Response: <point>(331, 219), (248, 215) </point>. Highlighted Prompt: Locate the scarf in this image. Response: <point>(336,425),(331,450) </point>. scarf The is found at Prompt: Please specify the coordinates for the handbag. coordinates: Specifying Response: <point>(331, 409), (256, 490) </point>. Object Reference Prompt: Please hand me the orange on the left. Response: Points are <point>(302, 271), (343, 260)</point>. 31 would like to grab the teddy Prompt: bear in shirt. are Center Response: <point>(347, 241), (374, 245) </point>. points Prompt: What object is in <point>(213, 261), (218, 239), (216, 220)</point>? Response: Scissors. The image features an item Prompt: encased within red rectilinear shape and another item surrounded by green rectangular boundary. Pinpoint several spot son any item situated between the objects framed by these rectangles. Response: Locations are <point>(250, 212), (199, 214), (179, 226) </point>. Object Part Recognition Prompt: Can you locate the frame on this mirror? Response: Points are <point>(301, 140), (318, 140)</point>. Prompt: How do you interact with the strainers? By Response: <point>(1310, 880)</point>. interacting with the Prompt: If wanted to interact with the base of this glass, where would find it? Response: it on the <point>(249, 456)</point>area. You should interact with (272, 458), Prompt: Could you point out the page of the book? Response: <point>(556, 254)</point>. 32 Free Space Location In the image, theres region Prompt: outlined by red rectangle. Locate several points within the vacant area on this marked surface. Response: Locations are <point>(396, 435), (380, 440)</point>. Prompt: Point to free spot behind white textured radiator at lower right. Response: Points are <point>(394, 345) </point>. Prompt: Point to free spot in front of white painted wood chest of drawers at center. Response: 439)</point>. Points are <point>(196, Prompt: What is the location of free spot to the right of wooden trolley at left? Please provide its 2D coordinates. Response: 258)</point>. Points are <point>(173, Active Exploration Prompt: From the provided visual input, identify the most informative image frame (with IDs starting from 1) that offers the best chance of locating the fireplace. Format your response as: Frame ID: [Selected Frame ID] 33 Ground Truth: Frame ID: 5, 6 Prediction: Frame ID: Prompt: From the provided visual input, identify the most informative image frame (with IDs starting from 1) that offers the best chance of locating the sofa. Format your response as: Frame ID: [Selected Frame ID] Ground Truth: Frame ID: 3, 5 Prediction: Frame ID: 5 Prompt: From the provided visual input, identify the most informative image frame (with IDs starting from 1) that offers the best chance of locating the sink. Format your response as: Frame ID: [Selected Frame ID] Ground Truth: Frame ID: 2, 3 Prediction: Frame ID: 1 34 Prompt: From the provided visual input, identify the most informative image frame (with IDs starting from 1) that offers the best chance of locating the gym equipment. Format your response as: Frame ID: [Selected Frame ID] Ground Truth: Frame ID: 5 Prediction: Frame ID: 5 E.2 QUALITATIVE RESULTS ON 3D SPATIAL REASONING TASKS 3D Question Answering am using the clothes dryer Prompt: under another clothes dryer. What object is beneath the table to my left? ... Ground Truth: box Prediction: box Prompt: am sitting on chair while facing the whiteboard and there is chair on the same side of the table on my left. What is the shape of the table in front of me? ... Ground Truth: rectangular Prediction: rectangular 35 Prompt: What color is the tool box to the left of the piano? Answer the question simply. Ground Truth: red green Prediction: red Prompt: Where is the bathroom vanity located? Answer the question simply. Ground Truth: to right of bathroom door below window Prediction: left of toilet am making some copies and Prompt: there are some pictures on my left. What is brown and square to my left? ... Ground Truth: table Prediction: table Prompt: am throwing trash with chair on my right within reach. What is the color of the chair that is within reach on my right? ... Ground Truth: black Prediction: black am opening the window with Prompt: the shoe rack on my right. What is on top of shelf behind me that can place on my back? ... Ground Truth: bag Prediction: pillow Prompt: am standing by the poster printer and there is table behind me. Can see the box if turn around? ... Ground Truth: no Prediction: yes 36 3D Captioning (red box: 3D box given in prompt) located at Prompt: Given an object <3dbox>(150,155,51,5,10,5)</3dbox>, describe ... Ground Truth: The coffee table is white. The coffee table is in front of the orange couch. Prediction: The coffee table is in the middle of the room. The coffee table is white rectangle. Prompt: Given an object located at <3dbox>(132,151,54,7,6,7)</3dbox>, describe ... Ground Truth: The chair is the color black. It is facing the table along with other chairs. It is spaced beside other chairs. Prediction: This is black chair. It is at the corner of the table. located at Prompt: Given an object <3dbox>(153,152,55,8,7,6)</3dbox>... Ground Truth: The chair is red and green. It is to the right of the black chair. Prediction: This is chair with no arms. It is in front of desk. located at Prompt: Given an object <3dbox>(140,166,58,4,5,4)</3dbox>... Ground Truth: black computer with blue screen. Its located on wooden desk with wooden chair in front of it. Prediction: This is black laptop. It is on desk. 37 located at Prompt: Given an object <3dbox>(137,158,51,4,4,5)</3dbox>... Ground Truth: The black backpack is directly in front of the nightstand. The backpack is also directly to the left of the bed. Prediction: The backpack is black. It is on the floor, to the left of the bed. located at Prompt: Given an object <3dbox>(159,133,53,6,7,9)</3dbox>... Ground Truth: This is sturdy chair, made of wood and with green cushioning. It is in contact with wall surface and brown table. It is oriented about 120 degrees in relation to the viewer. Prediction: The chair is the one closest to the door. the chair has green seat and four legs. 3D Visual Grounding (red Box: ground truth; green box: prediction) ... There is black office chair. Prompt: placed in the side of the wall. Response: <3dbox>(130, 155, 55, 6, 5, 4)</3dbox>. Result: Success Prompt: ... It is next to the tall floor lamp. it is snack machine and it is full of tasty snacks. it is over by the door ... Response: <3dbox>(143, 135, 59, 10, 8, 17)</3dbox>. Result: Success 38 Prompt: ... In the corner of the room are boxes. the first two book shelves in the corner to the right of the boxes are the bookshelves we are looking for. Response: <3dbox>(127, 139, 61, 4, 21, 20)</3dbox> Result: Success Prompt: ... The keyboard is sitting on top of the desk. it is in front of the left-side monitor. Response: <3dbox>(141, 125, 59, 9, 1, 20)</3dbox> Result: Success Prompt: ... This is green tool box. . the green tool box is in front of red tool box on the floor next to piano. Response: <3dbox>(147, 127, 52, 5, 3, 3)</3dbox>. Result: Success Prompt: ... This is black trash can. it is under the counter top. Response: <3dbox>(159, 143, 51, 3, 4, 4)</3dbox>. Result: Success Prompt: ... This is black leather loveseat in library. if you were sitting in it, the long, short bookshelf would be on your right and the tall book shelf, opposite you across the room. Response: <3dbox>(145, 168, 57, 15, 9, 7)</3dbox>. Result: Fail Prompt: ... The black chair, in front of the chair with brown table, on the right has 2 black chairs. on the left is black chair with the left to right it is the first chair. Response: <3dbox>(154, 159, 54, 5, 6, 10)</3dbox>. Result: Fail 3D Scene Imagination (from HM3D and MP3D validation set) ..., when the agent walks to Prompt: position <3dbox>(122,129,21,10,10,10) </3dbox>, what new objects might become visible? ... You may see various Ground Truth: pictures, chairs, vases, trays, plates, coffee mugs, washing machines, lamps, curtains, rugs, vents, balustrades, sensors, tables, books, candlesticks, and pillows. Prediction: You may see various lamps, ventilations, and cabinets. ..., when the agent walks Prompt: to position <3dbox>(153,164,26,10,10,10) </3dbox>, what new objects might become visible? ... Ground Truth: You may see few motion detectors, blinds, wall toilet papers, bathroom towels, trashcans, toilets, decorations, decorative plants, and bathroom shelves. Prediction: You may see various lamps, pictures, and beds. ..., when the agent walks Prompt: to position <3dbox>(146,157,21,10,10,10) </3dbox>, what new objects might become visible? ... Ground Truth: You may see various bathroom cabinets, pillows, benches, handbags, seats, and pictures. Prediction: You may see various lamps, cabinets, pillows, and beds. ..., when the agent walks Prompt: to position <3dbox>(125,155,49,10,10,10) </3dbox>, what new objects might become visible? ... Ground Truth: You may see various boxes, pictures, lamps, ceiling lamps, drawers, stools, appliances, beds, fire alarms, pillows, fireplace walls, racks, and tables. Prediction: You may see various pictures, chairs, tables, and plants. ..., when the agent walks to Prompt: position <3dbox>(130,100,34,10,10,10) </3dbox>, what new objects might become visible? ... : You may see various Ground Truth: shelves, tables, mirrors, decorations, chairs, sinks, faucets, liquid soaps, door windows, TVs, curtains, carpets, wine racks, closet areas for hanging clothes, clothes, bathtubs, bath towels, shower soap shelves, soap bottles, shower walls, shower floors, shower glasses, shower mats, towel bars, towels, shower handles, ceiling lamps, and shower doors. Prediction: You may see various pictures, tables, chairs, plants, cabinets, counters, sinks, and towels. ..., when the agent walks to Prompt: position <3dbox>(166,186,24,10,10,10) </3dbox>, what new objects might become visible? ... You may see various Ground Truth: ceiling lamps, smoke alarms, doorposts, headboards, wall nightstands, radios, lounge chairs, pillows, dog beds, window curtains, bathtubs, shower doors, bathroom cabinets, baskets, stair steps, and towels. Prediction: You may see various shelves, chairs, flower vases, cabinets, lamps, pictures, curtain rails, curtains, and carpets. lamps, tables, Object Navigation in Partially Observable Environments (examples are sampled from HM3D validation set): green boxpredicted 3D subgoal at each planning step; red line: past trajectory; red box: target object to be found Prompt: Based on the currently observed environment, determine the best position to go to find the toilet. Exploration Trajectory: OmniEVA first explored the doorway of the room in the upper right corner. After mistakenly entering the kitchen area, it quickly adjusted its path, selected new area to explore, and directly located the sofa in the living room. 41 Prompt: Based on the currently observed environment, determine the best position to go to find bed. Exploration Trajectory: The model first moves to the hallway at the intersection of multiple rooms to gather more comprehensive environmental information. It then determines that the room in the lower right corner is more likely to be bedroom, proceeds to that location, and ultimately finds the bed. Prompt: Based on the currently observed environment, determine the best position to go to find the chair. Response: The model first walks to the center of the corridor, sees the sofa in the upper left, then heads to that room area and finds the chair. E.3 EXAMPLES OF EMBODIMENT-AWARE REASONING Embodiment-aware Reasoning Prompt: Could you help me find vacant area on the table? Response: <think>1. **Describing the Image**: The image shows wooden table with various objects placed on it, including mug, highlighter, small fruit, notebook, white object resembling bottle, and chair positioned nearby. The table has some visible 42 empty spaces, but the objects occupy certain areas.2. **Common-Sense Considerations**: - The goal is to identify free space on the table where additional objects can be placed without causing collisions or instability. - Free space should be large enough to accommodate typical objects like the ones present on the table. - The placement should avoid overlapping with existing objects or causing them to tip over.3. **Relevant Factors**: - **Object Dimensions**: The size of the objects already on the table should be considered when determining free space. - **Table Dimensions**: The overall dimensions of the table should be taken into account to ensure that the identified free space is realistic. - **Collision Avoidance**: Any suggested free space should not interfere with the existing objects or lead to potential collisions with the chair or other furniture.4. **Free Space Identification**: - The bottom-left corner of the table appears to have some free space, as there are no objects occupying that area. - The top-right corner also seems to have some free space, though it is farther than the bottom-left. - The middle-right section of the table has limited free space due to the presence of the notebook and the white bottle.5. **Embodiment Awareness**: The operational range of the robot necessitates that it be positioned as close as practicable to the tables bottom edge.6. **Answer Formulation**: - Based on the analysis, the bottomleft corner of the table is the most suitable for placing additional objects, as it offers the largest and most unobstructed free space.</think><answer>Locations are <point>(66, 359), (56, 360), (76, 357)</point>.</answer> Prompt: Find the nearest free space on the table with no chairs around. Response: <think>To answer the question, we need to analyze the image comprehensively:1. **Describing the Image**: The image shows large wooden table in room with multiple chairs placed around it. The table is cluttered with various objects, including books, containers, tools, and other miscellaneous items. There are also some empty spaces on the table.2. **Common-Sense Considerations**: - The question asks for nearest free space on the table, meaning we need to identify areas that are not occupied by objects and are close to the center of the table. - Free space should be large enough to place something without causing collision with existing objects. - The tables edges and corners are potential candidates for free space, but they may not be ideal due to limited usable area.3. **Relevant Factors**: - Identify all objects currently occupying the table. - Determine the tables layout and identify any visible gaps or open areas. - Assess the proximity of these gaps to the center of the table. - Ensure the identified space is large enough to accommodate new object.4. **Analysis**: - The table has several visible gaps, particularly near the center-left and top-right sections of the table. - These gaps appear to have enough space to place small to medium-sized objects without causing collision with existing items. - The center area of the table is less cluttered than the top-right edges, but is obscured by chairs in front.5. **Embodiment Awareness**: To ensure the robot can cover its entire potential work area, placement near the top-right edge of the table is critical.6. **Conclusion**: - The nearest free space on the table can be identified as the central-left section of the table, specifically around the coordinates (460, 250) to (470, 250), which is relatively clear area with no objects obstructing it.</think><answer>Points are <point>(467, 249), (458, 249), (478, 249) </point>. </answer> 43 Prompt: Locate some free space for me, please. Response: <think>1. **Describing the Image**: The image shows table with various objects placed on it, including tissue box, cup, book, and other small items. The table is situated in what appears to be living room or office setting, with chairs and other furniture visible in the background. 2. **Common-Sense Considerations**: - Free space refers to areas on the table that are not occupied by existing objects.- The placement of new objects should avoid causing clutter or imbalance on the table.- The dimensions of the table and the size of the objects already present should be considered when determining free space. 3. **Relevant Factors**:- **Object Dimensions**: The size of the existing objects on the table will determine how much free space is available.- **Table Dimensions**: The overall size of the table will help assess whether there is enough space for additional objects.- **Free Space Placement**: The placement of new objects should consider the stability of the table and ensure no objects are placed too close to the edges to prevent them from falling off. 4. **Embodiment Awareness**: For optimal performance, we should place the robot near the bottom of the table to ensure it can access its entire intended work area.5. **Answering the Question**:- Based on the image, there seems to be some free space on the right side of the table near the edge, away from the existing objects.- This area appears to have sufficient distance from the existing items and could accommodate small object without causing collision or imbalance. provides ample room and is less likely to cause interference with other objects. </think><answer>Locations are <point>(450, 346), (455, 348), (467, 352) </point>.</answer>"
        },
        {
            "title": "F REAL WORLD DEPLOYMENT EXAMPLES",
            "content": "44 Figure 10: Example for Deployment of OmniEVA on Real World Robots. Prompt: Place the paper cup in the empty space on the table at the back right. Figure 11: Example for Deployment of OmniEVA on Real World Robots. Prompt: Place the cup on the long table next to the meeting room."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab"
    ]
}