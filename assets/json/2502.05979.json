{
    "paper_title": "VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer",
    "authors": [
        "Xinyu Liu",
        "Ailing Zeng",
        "Wei Xue",
        "Harry Yang",
        "Wenhan Luo",
        "Qifeng Liu",
        "Yike Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Crafting magic and illusions is one of the most thrilling aspects of filmmaking, with visual effects (VFX) serving as the powerhouse behind unforgettable cinematic experiences. While recent advances in generative artificial intelligence have driven progress in generic image and video synthesis, the domain of controllable VFX generation remains relatively underexplored. In this work, we propose a novel paradigm for animated VFX generation as image animation, where dynamic effects are generated from user-friendly textual descriptions and static reference images. Our work makes two primary contributions: (i) Open-VFX, the first high-quality VFX video dataset spanning 15 diverse effect categories, annotated with textual descriptions, instance segmentation masks for spatial conditioning, and start-end timestamps for temporal control. (ii) VFX Creator, a simple yet effective controllable VFX generation framework based on a Video Diffusion Transformer. The model incorporates a spatial and temporal controllable LoRA adapter, requiring minimal training videos. Specifically, a plug-and-play mask control module enables instance-level spatial manipulation, while tokenized start-end motion timestamps embedded in the diffusion process, alongside the text encoder, allow precise temporal control over effect timing and pace. Extensive experiments on the Open-VFX test set demonstrate the superiority of the proposed system in generating realistic and dynamic effects, achieving state-of-the-art performance and generalization ability in both spatial and temporal controllability. Furthermore, we introduce a specialized metric to evaluate the precision of temporal control. By bridging traditional VFX techniques with generative approaches, VFX Creator unlocks new possibilities for efficient and high-quality video effect generation, making advanced VFX accessible to a broader audience."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 9 7 9 5 0 . 2 0 5 2 : r VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer XINYU LIU, Hong Kong University of Science and Technology, China AILING ZENG, Tencent AI Lab, China WEI XUE, Hong Kong University of Science and Technology, China HARRY YANG, Hong Kong University of Science and Technology, China WENHAN LUO, Hong Kong University of Science and Technology, China QIFENG LIU, Hong Kong University of Science and Technology, China YIKE GUO, Hong Kong University of Science and Technology, China Fig. 1. VFX Creator is an efficient framework based on Video Diffusion Transformer, enabling spatial and temporal control for visual effect (VFX) video generation. With minimal training data, plug-and-play mask control module allows precise instance-level manipulation, while the integration of tokenized start-end motion timestamps with text space provides fine-grained temporal control over the VFX rhythm. Project Page: https://vfx-creator0.github.io/ Crafting magic and illusions stands as one of the most thrilling facets of filmmaking, with visual effects (VFX) serving as the powerhouse behind unforgettable cinematic experiences. While recent advances in generative artificial intelligence have catalyzed progress in generic image and video synthesis, the domain of controllable VFX generation remains comparatively underexplored. More importantly, fine-grained spatial-temporal controllability in VFX generation is critical, but challenging due to data scarcity, complex dynamics, and precision in spatial manipulation. In this work, we propose novel paradigm for animated VFX generation as image animation, where dynamic effects are generated from user-friendly textual descriptions and static reference images. Our work makes two primary contributions: i) Authors addresses: Xinyu Liu, Hong Kong University of Science and Technology, China, xliugd@connect.ust.hk; Ailing Zeng, Tencent AI Lab, China, ailingzengzzz@gmail.com; Wei Xue, Hong Kong University of Science and Technology, China, weixue@ust.hk; Harry Yang, Hong Kong University of Science and Technology, China, yangharry@ust. hk; Wenhan Luo, Hong Kong University of Science and Technology, China, whluo@ust. hk; Qifeng Liu, Hong Kong University of Science and Technology, China, liuqifeng@ ust.hk; Yike Guo, Hong Kong University of Science and Technology, China, yikeguo@ ust.hk. Open-VFX, the first high-quality VFX video dataset spanning 15 diverse effect categories, annotated with textual descriptions, instance segmentation masks for spatial conditioning, and startend timestamps for temporal control; This dataset features wide range of subjects for the reference images, including characters, animals, products, and scenes. ii) VFX Creator, simple yet effective controllable VFX generation framework based on Video Diffusion Transformer. The model incorporates spatial and temporal controllable LoRA adapter, requiring minimal training videos. Specifically, plug-and-play mask control module enables instance-level spatial manipulation, while tokenized start-end motion timestamps embedded in the diffusion process accompanied by the text encoder, allowing precise temporal control over effect timing and pace. Extensive experiments on the Open-VFX test set with unseen reference images demonstrate the superiority of the proposed system to generate realistic and dynamic effects, achieving state-of-the-art performance and generalization ability in both spatial and temporal controllability. Furthermore, we introduce specialized metric to evaluate the precision of temporal control. By bridging traditional VFX techniques with generative techniques, the proposed VFX Creator unlocks new possibilities 2 Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo for efficient, user-friendly, and high-quality video effect generation, making advanced VFX accessible to broader audience."
        },
        {
            "title": "1\nVisual effects (VFX) video generation is paramount in video produc-\ntion, particularly in cinema, gaming, and virtual reality, where it\nenhances visual impact and improves creative efficiency [Adobe nd].\nVisual effects combine live action footage with generated imagery\nto create realistic environments, objects, animals, and creatures that\nwould be dangerous, expensive, impractical, or impossible to cap-\nture on film. While early visual effects involved experimentation\nwith film stock, modern techniques include animation, computer-\ngenerated imagery (CGI), and other post-production methods [Cha-\nbanova 2022]. However, these approaches often involve high compu-\ntational costs, long production cycles, and significant manual inter-\nvention. With the rapid development of diffusion models [Blattmann\net al. 2023; Ho et al. 2020], visual effects generation is progressively\ntransitioning from traditional techniques to generative models.",
            "content": "Recent emerging models [Lin et al. 2024; Polyak et al. 2024] have impressive video generation capabilities, showcasing strong temporal consistency and visually appealing effects. These advancements offer great potential for artists, enabling the creation of stunning videos with minimal input, such as images or text prompts. However, the field of VFX generation for video remains underexplored, with existing open-source models struggling to produce complex effects and effectively control motion generation from text prompts. In contrast, closed-source products such as Pika [Pika 2023] and PixVerse [Pixverse 2023] have proven their ability to generate wide array of striking visual effects using diffusion-based generative models. These platforms can create effects such as realistic explosions, anti-gravity phenomena, and cinematic character transformations without manual modeling or lengthy production timelines. Despite their power, these proprietary platforms are limited by restricted access to their visual effects resources, which hinders broader development and exploration within this domain. In this work, we propose novel paradigm for animated VFX generation, such as image animation, where dynamic effects are generated from user-friendly textual descriptions and static reference images. We introduce two primary contributions to address the challenges and limitations of VFX video generation. First, we present Open-VFX, the first high-quality generated VFX video dataset comprising 675 videos across 15 distinct effect categories, sourced from two commercial platforms: Pika and PixVerse. The dataset includes diverse range of subjectscharacters, animals, products, and sceneswith minimum resolution of 700x1000 pixels. Additionally, it contains 245 static images from Pexels [Pexels 2024], annotated with textual descriptions, frame-level masks, and normalized start-end timestamps. This wide-ranging dataset provides extensive reference material for the generation of VFX across various domains. Second, we propose VFX Creator, simple yet effective controllable VFX generation framework based on Video Diffusion Transformer [Yang et al. 2024], as shown in Fig. 1. The model incorporates spatial and temporal controllable LoRA adapter, enabling high-quality video generation with minimal training data. For spatial control, we integrate video mask sequences as conditions with the latent video noise, facilitating instance-level spatial manipulation. For temporal control, we tokenize the start-end motion timestamps and integrate them into the diffusion process alongside the text encoder, allowing for precise control over the timing and pacing of the effects. Finally, we conduct comprehensive experiments on the Open-VFX dataset to demonstrate the models generation capabilities. We also introduce specialized evaluation metric to assess the precision of temporal control, showcasing the models ability to generate dynamic, temporally consistent VFX. Due to the data efficiency of VFX Creator, our framework can be easily adapted for fine-tuning across different categories of visual effects. This flexibility enables the rapid generation of broad array of visual effects videos, significantly reducing the cost and time typically associated with traditional VFX production in the film industry. In summary, our contributions are as follows: (1) We present the Open-VFX, the first high-quality VFX video dataset spanning 15 diverse effect categories, annotated with text prompt, instance segmentation masks for spatial conditioning, and startend timestamps for temporal control. (2) We propose the VFX Creator, simple yet effective controllable VFX generation framework based on Video Diffusion Transformer. The model incorporates spatial and temporal controllable LoRA adapter, enabling precise manipulation. (3) We perform comprehensive evaluation on the Open-VFX dataset, showcasing that the proposed system surpasses existing methods in generating visual effects. Additionally, we introduce novel metric specifically designed to assess the precision of temporal control in the generated effects."
        },
        {
            "title": "2 RELATED WORK\n2.1 General Video Generation\nThe rapid advancement of video generative models is driven by\ndiffusion models (DMs) [Ho et al. 2020; Nichol et al. 2021; Rombach\net al. 2022; Sohl-Dickstein et al. 2015; Song et al. 2020], which en-\nable innovative approaches in video generation. A key architecture\nis the Diffusion Transformer [Peebles and Xie 2023], leveraging\ntransformer designs to capture long-range dependencies, enhancing\ntemporal consistency and dynamics, and multi-resolution synthe-\nsis [Brooks et al. 2024; Chen et al. 2023; Kuaishou 2024; Ma et al.\n2024b; Shao et al. 2024; Team 2024; Yang et al. 2024]. For example,\nCogVideoX [Yang et al. 2024] uses a 3D full attention mechanism\nfor spatial and temporal coherence, while Hunyuan-DiT [Li et al.\n2024] introduces large pre-trained models for rich contextual detail.\nFurthermore, controllable video generation has garnered consider-\nable attention due to its promising applications in video editing and\ncontent creation. For instance, LAMP [Wu et al. 2023a] focuses on\ntransferring information from the first frame to subsequent frames,\nensuring the consistency of the initial image throughout the video\nsequence; however, it is constrained by fixed motion patterns in\na few-shot setting. Recent efforts have sought to enhance control\nover generative models by integrating additional neural networks\ninto diffusion frameworks. ControlNet [Zhang et al. 2023a] directs\nimage generation based on control signals by replicating specific",
            "content": "VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer 3 Fig. 2. Overview of our proposed Open-VFX Dataset. (a) demonstrates diverse input inference images in the dataset, including humans, animals, objects, and various scenes across single and multiple components. (b) shows the text descriptions of the proposed 15 VFXs, and (c) presents an example (Explode it) VFX. layers from pre-trained models and connecting them with zero convolutions [Wang et al. 2024a]. However, the field of controllable visual effect video generation has yet to be explored."
        },
        {
            "title": "3 DATASET\n3.1 Definition of Visual Effects\nVisual effects (VFX) can create realistic environments and charac-\nters that are difficult or impossible to capture during filming. For\nexample, VFX involves compositing techniques to combine different\nvisual elements into a single scene, often through green screen or",
            "content": "digital background replacement. Additionally, digital effects such as explosions, smoke, and weather simulations help create dynamic and immersive environments. Motion capture technology is used to animate characters or creatures, while digital makeup effects transform actors into fantastical characters. In light of these various considerations, we have curated set of 15 distinct visual effects, including Cake-ify, Crumble, Crush, Decapitate, Deflate, Dissolve, Explode, Eye-pop, Inflate, Levitate, Melt, Squish, Ta-da, Transformer into Venom, and Transformer into Harley Quinn. As shown in Fig. 2, we demonstrate the overview of our Open-VFX Dataset, and these effects are designed to deliver more immersive and visually compelling experience for the audience. Detailed descriptions of these effects can be found in the supplementary materials."
        },
        {
            "title": "3.3 Data Annotation\nTo accomplish the spatial-temporal controlled VFX video generation\ntask, we adhered to the following process to acquire timestamp and\nmask annotations for our Open-VFX dataset:",
            "content": "Start-end motion timestamps. To accurately capture the start and end timestamps of motion, traditional optical flow methods fall short 4 Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo Fig. 3. More examples of our Open-VFX dataset, including 10 VFXs and diverse reference images. in tracking the dynamic movement of visual effects due to their inherent limitations. Consequently, we employ Co-Tracker [Karaev et al. 2024] for timestamp labeling. By monitoring the displacement of tracked points, motion is deemed to begin when the coordinates start to shift, and it concludes when the displacement ceases, providing precise temporal boundaries for the motion."
        },
        {
            "title": "4 NETWORK\n4.1 Preliminary\nWe introduce the preliminary of CogVideoX architecture(Rombach\net al., 2022), the baseline diffusion transformer Network used in our\nwork, and Low-Rank Adaptation (LoRA) (Hu et al., 2021), which\nhelps understand the spatial and temporal controllable LoRA adapter.",
            "content": "Instance-level mask sequences. To enable the model to generate visual effects specifically tailored to the selected object, we utilize SAM2 [Ravi et al. 2024] to semantically annotate the motion of the chosen object, producing corresponding mask sequences. We generate several times to obtain diverse generated videos with different animated objects, and we annotate them via masks. During inference, SAM2 is employed to generate binary region masks based on the user-defined area, which serve as spatial conditions to guide precise control over the spatial manipulation of the object."
        },
        {
            "title": "4.1.1 Baseline Diffusion Transformer Network. VFX Creator builds\nupon the CogVideoX architecture [Yang et al. 2024] and leverages a\ncausal 3D Variational Autoencoder (VAE) [Kingma 2013] for video\ncompression, achieving temporal and spatial factors of 4 and 8, re-\nspectively. Latent variables are structured as sequential inputs, while\ntextual information is encoded into embeddings using the T5 En-\ncoder [Raffel et al. 2020]. These inputs are processed jointly within\na stacked Expert Transformer network, which integrates Adaptive\nLayer Normalization for better alignment and 3D Rotary Positional",
            "content": "VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer 5 video clip is subject to random shift, with the start and end timestamps constrained within the ranges of [0,𝑇 𝐷] and [𝐷,𝑇 ], where 𝑇 represents the maximum frame count in the training video, and 𝐷 denotes the duration of the visual effects motion. In this section, we explore two strategies for integrating timestamp control signals and conduct experiments on three different effects to analyze their effectiveness and accuracy. Strategy I: Integrate Temporal Mask with Timestep. Current temporal condition representation typically takes two forms: digitized start-end timestamps and temporal masks. The former normalizes timestamps directly, while the latter applies temporal mask to the frame sequence, designating moving frames as 1 and static frames as 0. Existing methods for temporal condition injection can be categorized into two strategies: one integrates temporal mask with Timestep embeddings and injects them into the diffusion blocks; the other utilizes timestamp encoder to interact with the text space and computes cross-attention with the noisy latent representation. In the following, we first focus on the former approach. We project the temporal mask of motion in videos into timestep embedding and add it to each frame to ensure uniform application of the motions timing and pace to every frame. As illustrated in Module of Fig. 4, given the normalized timestamp, we introduce timestamp encoder network that projects the temporal masks into the timestep embedding space, which is then incorporated into the DiT blocks [Peebles and Xie 2023]. Strategy II: Integrate Timestamps with Text Space. Inspired by [Fang et al. 2024], we first map the start and end timestamps of the visual effects to the prompt space, converting them into timestamp tokens. These tokens are then concatenated with the original text prompt tokens. By leveraging the cross-attention mechanism in the DiT block, we generate VFX videos conditioned on the temporal control signals. As shown in Module II of Fig. 4, given the normalized timestamps 𝑦timestamp, the text prompt 𝑦text, and textdomain specific encoder 𝜏𝜃 , we introduce timestamps encoder network 𝜏𝜙 , that projects timestamps 𝑦timestamp to the intermediate text representation space R𝑀 𝑑𝜏 such that 𝜏𝜙 (𝑦𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝 ) R𝑀 𝑑𝜏 We concatenate 𝜏𝜙 (𝑦timestamp) and𝜏𝜃 (𝑦text) and input the crossattention layer Attention (𝑄, 𝐾, 𝑉 ) = softmax (cid:16) 𝑄𝐾𝑇 𝑉 ,with (1) (cid:17) 𝑑 = 𝑊 (𝑖 ) = 𝑊 (𝑖 ) = 𝑊 (𝑖 ) 𝑉 𝑄 𝜑𝑖 (𝑧𝑡 ), 𝐾 (𝜏𝜙 (𝑦timestamp) 𝜏𝜃 (𝑦text)), (𝜏𝜙 (𝑦timestamp) 𝜏𝜃 (𝑦text)). (2) where 𝑧𝑡 denotes the latent representation 𝑧 at the 𝑡-th diffusion time step. 𝜑𝑖 (𝑧𝑡 ) R𝑁 𝑑𝑖 𝜖 denotes flattened intermediate representation of the Transformer implementing 𝜖𝜃 . 𝑊 (𝑖 ) 𝜖 and 𝑉 𝑊 (𝑖 ) 𝑄 R𝑑 𝑑𝜏 and 𝑊 (𝑖 ) 𝐾 R𝑑 𝑑𝜏 are learnable projection matrices. is the operator for tensor concatenation. R𝑑 𝑑𝑖"
        },
        {
            "title": "4.2.2 Spatial Controllable LoRA Adapter. Currently, we com-\nmonly employ three methods to incorporate spatial conditions into",
            "content": "Fig. 4. Pipeline of VFX Creator. We introduce two novel modules: (a) Spatial Controlled LoRA Adapter. This module integrates mask-conditioned ControlNet with LoRA, injecting mask sequences into the model to enable instance-level spatial manipulation. (b) Temporal Controlled LoRA Adapter. We explore two strategies for incorporating temporal control: module involves tokenizing start-end motion timestamps and embedding them into the diffusion process alongside the text space, while module II integrates temporal mask with timestep embeddings to guide the diffusion process. Embeddings (RoPE) [Narvekar and Karam 2011] to enhance the models ability to capture temporal dynamics and long-range dependencies in video frames."
        },
        {
            "title": "4.2.1 Temporal Controlled LoRA Adapter. To enable rhythm-\ncontrolled visual effect generation, we first employ temporal video\naugmentation by utilizing the start and end timestamps of the ef-\nfect’s motion to maximize data utilization. Specifically, the moving",
            "content": "6 Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo the diffusion model: i) concatenating the reference image with the first frame mask before inputting them into the diffusion model [Ma et al. 2024a]; ii) combining the latent mask and noisy latent along the channel dimension for spatial control [Lei et al. 2024]; iii) introducing spatial ControlNet [Zhang et al. 2023a] to extract mask sequence that guides the generation process. Our experimental observations reveal that the first two methods do not successfully facilitate spatial condition-based video generation tasks. While effective in U-Net-based diffusion models, they are not suitable injection techniques for transformer-based diffusion models. Therefore, to enable instance-level spatial manipulation, we introduce plug-andplay mask control module, leveraging the mask guidance to precisely control the desiring instance. During training, we utilize SAM2 to obtain the mask sequences of the moving instances. We retain the mask sequences preceding the start timestamp, while padding the remaining frames with zeros, and combine them to form the spatial condition. These spatial conditions are then injected into learnable spatial ControlNet [Zhang et al. 2023a] for fine-tuning. As shown in Fig. 4, we extract mask conditions and integrate them into the main network. This branch shares trainable parameters initialized as copy of the original half branch and operates in parallel, using zero convolution as bridge to integrate the conditional controls. Specifically: (3) 𝒚𝑐 = F𝑚 (𝒙) + Z(F𝑐𝑛 (𝒙, 𝒄; Θ𝑐𝑛); Θ𝑧), where (; Θ) denotes neural model with learnable parameters Θ, Z(; Θ𝑧) indicates the zero convolution layer, and 𝑥, 𝑦𝑐 Rℎ𝑤𝑐 and 𝑐 are the 2D feature maps and conditional controls, respectively. This trainable spatial ControlNet branch is connected to the partly frozen main branch with zero-initialized convolution layer, ensuring the integration of spatial conditions while minimizing interference with the base model."
        },
        {
            "title": "5.1\nDuring the training phase, we incorporate low-rank matrices with a\nrank of 128 into the 3D Transformer module of the baseline network.\nWe randomly sample 49 frames with a resolution of 480 × 720. We\nemploy the AdamW [Loshchilov 2017] optimizer with a constant\nlearning rate of 1e-4 for training all models. All experiments are\nconducted on the NVIDIA H800 GPU. We froze the gradients of most\nweights in the original base network and trained for 3000 steps with\na learning rate of 1e-4, implementing both learning rate warm-up\nand decay mechanisms. Our VFX dataset is partitioned into training",
            "content": "and testing sets in 9:1 ratio, both containing 15 distinct types of visual effects. The training set includes an average of 40 videos per effect, while the testing set contains 5 videos per effect."
        },
        {
            "title": "5.2 Evaluation Metrics\nIn this experiment, we adopt three metrics following prior works:\nFID-VID [Unterthiner et al. 2018], FVD [Balaji et al. 2019] and Dy-\nnamic Degree [Huang et al. 2024] to evaluate the general quality and\ndegree of dynamics of the synthesized videos. As shown in Table 2,\nwe aim to focus on the generative performance of the motion in the\nvideo. We design three metrics to evaluate the accuracy of temporal\ncontrol: frame-level errors Ef and second-level errors Es, as well as\nTemporal Intersection over Union (𝑇IoU). Specifically, temporal error\nquantifies the difference between the start and end timestamps of\nthe predicted and ground truth segments. The frame-level temporal\nerror and second-level temporal error are related through the frames\nper second (FPS):",
            "content": "Ef = 1 𝑁 𝑁 𝑖=1 (cid:0)(cid:12) (cid:12)ˆ𝑡start,𝑖 𝑡start,𝑖 (cid:12) (cid:12) + (cid:12) (cid:12)ˆ𝑡end,𝑖 𝑡end,𝑖 (cid:1) (cid:12) (cid:12) Es = 1 𝑁 𝑁 𝑖=1 ˆ𝑡start,𝑖 FPS (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) 𝑡start,𝑖 FPS (cid:12) (cid:12) (cid:12) (cid:12) + ˆ𝑡end,𝑖 FPS (cid:12) (cid:12) (cid:12) (cid:12) 𝑡end,𝑖 FPS (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) (4) (5) where 𝑁 is the number of video segments, and 𝑡start,𝑖 and 𝑡end,𝑖 are the normalized start and end timestamps of the 𝑖-th video clip. To evaluate the accuracy of the predicted timestamps, we randomly sample five pairs of start and end ground truth timestamps for reference sample. The start ground truth is constrained within the range of [0, 2/𝑇 ], while the end ground truth falls within the range of [2/𝑇 ,𝑇 ], where 𝑇 represents the total number of frames."
        },
        {
            "title": "5.3 Quantitative Comparison Results\nFor quantitative evaluation, we conducted experiments on 15 types\nof visual effects from our dataset and compared VFX Creator with\nthree state-of-the-art open-source methods, with detailed results\nshown in Table 1. VFX Creator outperforms the other methods\nacross all metrics, particularly in generating visual effects with large\nmotion patterns, indicating superior video quality and more accu-\nrate motion generation. As seen in Table 1, DynamiCrafter [Xing\net al. 2025] and CogVideoX [Yang et al. 2024] are less responsive to\nvisual effect prompts, producing videos with minimal or no motion,\nas reflected by their lower dynamic degrees. While LTX-Video [Ha-\nCohen et al. 2024] exhibits a higher dynamic degree, this is due to\nthe generation of large, incorrect motions, which does not corre-\nspond to the semantic accuracy of the motion. These results align\nwith the limitations observed in the quantitative evaluation. To fur-\nther validate these findings, we conducted extensive user studies\nto assess the correspondence accuracy between generated visual\neffects and text prompts. VFX Creator consistently outperforms\nother state-of-the-art methods, confirming its exceptional ability\nto generate high-quality, semantically accurate visual effects, even\nwhen dealing with large and complex abstract motions.",
            "content": "VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer 7 Fig. 5. Qualitative comparisons of VFX video generation on two different visual effects between our method, DynamiCrafter, LTX-Video, CogVideoX-5B, and Pika. Table 1. Quantitative comparisons of VFX video generation for 15 visual effects in our dataset."
        },
        {
            "title": "Method",
            "content": "FID-VID FVD Dynamic Degree DynamiCrafter LTX-Video CogVideoX VFX Creator (Ours) 119.78 82.93 90.82 29.92 1515.28 1563.73 1624. 752.95 0.27 0.51 0.14 0."
        },
        {
            "title": "5.4 Qualitative Comparison Results\nWe present qualitative comparison results of VFX Creator with four\nrepresentative models on three effects: \"Deflate it,\" \"Dissolve it,\"\nand \"Eye-pop it,\" as shown in Fig. 5. For comparison, results from\nPika are used as the benchmark. We found that VFX Creator con-\nsistently generates more reliable videos, even when Pika struggles\nto produce satisfactory outputs. For example, Pika fails to generate",
            "content": "correct effects in \"Defalte it\" and \"Eye-pop it\", while VFX Creator successfully generates the abstract effect video. Pikas failure may stem from the substantial increase in complexity associated with accurately locating and generating the eye in smaller, more distant contexts. In contrast, DynamicCrafter exhibits significant challenges in generating visual effect motions that correspond to text prompts. LTX-Video and CogVideoX-5B, on the other hand, generate either small-scale motions or large, erroneous ones, which leads to lack of temporal consistency and semantic accuracy. These limitations are not present in VFX Creator, which maintains better alignment with the intended motion and effect characteristics. This ensures improved temporal consistency and visual fidelity. These results demonstrate that VFX Creator outperforms other models, delivering visually accurate and temporally consistent visual effects, even in cases where ground truth struggles with reliability. 8 Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo Fig. 6. Qualitative results of spatial controllable VFX video generation of our method on two different visual effects. Users can precisely specify the animated instance by clicking points or dropping boxes to obtain the mask."
        },
        {
            "title": "5.5 Ablation Study\nThe proposed VFX Creator is pivotal for generating high-quality\ncontrollable VFX videos. To assess the contributions of the spatial\nand temporal control modules, we conduct a series of ablation stud-\nies. First, we compare the results of two temporal control injection\nstrategies. The impact of start and end timestamp guidance on an-\nimation results is presented in Table 3. Our observation reveals\nthat integrating timestamps with the text space yields superior re-\nsults. This approach integrates timestamps with textual prompts to\nperform a cross-attention mechanism with video latent representa-\ntions, enhancing the precise alignment of visual effects with spec-\nified temporal cues. Additionally, the transformer-based diffusion\nmodel is more effective in handling implicit representation injec-\ntions compared to explicit conditions. Additionally, we present\nthe quantitative results of integrating the spatial control module. As\ndemonstrated in Fig. 6, VFX Creator effectively achieves accurate\nobject manipulation, generating photorealistic videos with strong\nconsistency between the visual effects and user interactions. Lastly,\nwe analyze the impact of different sample sizes during training by\ncomparing the model’s performance across varying shot numbers:\n1-shot, 10-shot, and 40-shot. As shown in Table 3, we observe that\nthe number of shots plays a significant role in the model’s perfor-\nmance. The results indicate that increasing the number of shots\ngenerally improves performance, with the 10-shot configuration\noften yielding balanced results. This highlights the model’s data\nefficiency, demonstrating its ability to learn abstract and complex\nvisual effect motions without the need for large amounts of data.",
            "content": "Table 2. Ablation results of two temporal control integration strategies."
        },
        {
            "title": "Visual Effect",
            "content": "Temporal Strategy 𝑇IoU Ef Es 𝑇IoU Ef"
        },
        {
            "title": "Temporal Strategy II",
            "content": "Ta-da it Explode it Levitate it 0.69 0.68 0.69 12.52 11.30 12.88 1.56 1.49 1.61 0.85 0.88 0.80 5.04 3.76 5. Es 0.63 0.47 0."
        },
        {
            "title": "Average",
            "content": "0.69 12.23 1.56 0.84 4.72 0. Table 3. Ablation results of different sample sizes during training across varying shot numbers."
        },
        {
            "title": "Effect",
            "content": "Ta-da it"
        },
        {
            "title": "Squish it",
            "content": "Shots 1-shot 10-shot 40-shot 1-shot 10-shot 40-shot 1-shot 10-shot 40-shot FID-VID 52.31 47.91 54.73 96.48 57.71 50.97 140.42 44.62 44.35 FVD 1432.40 2861.18 726.83 2667.72 2829.00 2394.20 3297.11 1409.98 1644.69 Dynamic Degree 0.6 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1."
        },
        {
            "title": "5.6 User Study\nTo further validate the effectiveness of our method, we conducted a\nhuman evaluation, comparing our approach with four existing ap-\nproaches, without using additional data for guidance. We invited 20\nusers to assess 30 sets of generated comparing results. We evaluated\nthe quality of the generated videos across four dimensions: Text",
            "content": "VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer 9 Alignment, Subject Fidelity, Motion Fluency, and Overall Quality. Text Alignment measures how well the generated video aligns with the text prompt; Subject Fidelity evaluates how closely the generated object matches the reference image; Motion Fluency assesses the smoothness and quality of the motions in the generated video; and Overall Quality reflects whether the overall quality of the generated video meets user expectations. As shown in Fig. 7, both our method, VFX Creator, and Pika& PixVerse achieved superior user preference across all metrics, with VFX Creator slightly outperforming Pika& PixVerse, demonstrating the effectiveness of our approach. Specifically, we leverage minimal training videos and enabling finegrained spatial and temporal control, our system bridges the gap between traditional VFX techniques and modern generative models. The extensive experiments demonstrate its ability to produce realistic, dynamic effects with state-of-the-art performance in both spatial and temporal controllability. With the innovative integration of instance-level spatial manipulation and precise temporal control, VFX Creator paves the way for efficient and user-friendly VFX generation, making advanced visual effects more accessible to broader audience and expanding creative possibilities in filmmaking. REFERENCES Adobe. n.d.. From pyrotechnics to prosthetics: guide to special effects in movies. https://www.adobe.com/creativecloud/video/discover/a-guide-to-specialeffects-in-movies.html Accessed: 2023-01-17. Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. 2019. Conditional GAN with Discriminative Filter Generation for Text-to-Video Synthesis.. In IJCAI, Vol. 1. 2. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. 2024. Video generation models as world simulators. https: //openai.com/research/video-generation-models-as-world-simulators (2024). Fig. 7. User Study. Our VF Creator demonstrates superior human preference compared to other methods"
        },
        {
            "title": "7 CONCLUSION\nIn conclusion, this work presents significant advancements in the\nfield of controllable visual effects (VFX) generation, addressing crit-\nical challenges associated with fine-grained spatial and temporal\nmanipulation. First, we propose the Open-VFX dataset establishes\na foundational resource for future explorations in VFX generation,\noffering a diverse array of effect categories and detailed annotations\nthat enhance the training of VFX generation models. Furthermore,\nwe introduce VFX Creator, a simple yet effective controllable VFX\ngeneration framework based on a Video Diffusion Transformer.",
            "content": "Anastasia Chabanova. 2022. VFXA New Frontier: The Impact of Innovative Technology on Visual Effects. Ph. D. Dissertation. University of Westminster. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. 2023. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512 (2023). Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. 2022. Towards Understanding Mixture of Experts in Deep Learning. arXiv:2208.02813 [cs.LG] https://arxiv.org/abs/2208.02813 I-Sheng Fang, Yue-Hua Han, and Jun-Cheng Chen. 2024. Camera Settings as Tokens: Modeling Photography on Latent Diffusion Models. In SIGGRAPH Asia 2024 Conference Papers. 111. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2023. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023). Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. 2024. LTX-Video: Realtime Video Latent Diffusion. arXiv preprint arXiv:2501.00103 (2024). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. 2024. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2180721818. Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. 2024. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 92129221. Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. 2024. CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos. In Proc. arXiv:2410.11831. Auto-encoding variational bayes. arXiv preprint Diederik Kingma. 2013. arXiv:1312.6114 (2013). Kuaishou. 2024. Keling. https://kling.kuaishou.com/z Accessed: 2025-01-19. Guojun Lei, Chi Wang, Hong Li, Rong Zhang, Yikai Wang, and Weiwei Xu. 2024. Animateanything: Consistent and controllable animation for video generation. arXiv preprint arXiv:2411.10836 (2024). Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai 10 Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo for video generation. In ACM SIGGRAPH 2024 Conference Papers. 111. Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. 2024. Dreamvideo: Composing your dream videos with customized subject and motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 65376549. Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. 2024a. Motionbooth: Motion-aware customized text-to-video generation. arXiv preprint arXiv:2406.17758 (2024). Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023b. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 76237633. Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. 2023a. Lamp: Learn motion pattern for few-shot-based video generation. arXiv preprint arXiv:2310.10769 (2023). Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. 2024b. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv preprint arXiv:2408.13239 (2024). Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. 2025. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision. Springer, 399417. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024. Cogvideox: Textto-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072 (2024). Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023a. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. Yuxin Zhang, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. 2023b. Motioncrafter: One-shot motion customization of diffusion models. arXiv preprint arXiv:2312.05288 (2023). Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. 2025. Motiondirector: Motion customization of text-to-video diffusion models. In European Conference on Computer Vision. Springer, 273290. Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. 2024. Hunyuan-DiT: Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding. arXiv:2405.08748 [cs.CV] https://arxiv.org/abs/2405.08748 Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. 2024. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131 (2024). Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. 2024b. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048 (2024). Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, et al. 2024a. Follow-your-click: Opendomain regional image animation via short prompts. arXiv preprint arXiv:2403.08268 (2024). Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, and Bryan Russell. 2023. Customizing motion in text-to-video diffusion models. arXiv preprint arXiv:2312.04966 (2023). Niranjan Narvekar and Lina Karam. 2011. no-reference image blur metric based on the cumulative probability of blur detection (CPBD). IEEE Transactions on Image Processing 20, 9 (2011), 26782683. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021). Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. 2024. I2VEdit: FirstFrame-Guided Video Editing via Image-to-Video Diffusion Models. In SIGGRAPH Asia 2024 Conference Papers. 111. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. Pexels. 2024. Free Stock Photos. https://www.pexels.com/ Accessed: 2024-01-19. Pika. 2023. Pika: platform for creative AI art. https://pika.art/ Accessed: 2025-01-11. Pixverse. 2023. Pixverse: AI-powered Image and Video Editing Platform. https://app. pixverse.ai/ Accessed: 2025-01-11. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. 2024. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720 (2024). Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 167. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. 2024. SAM 2: Segment Anything in Images and Videos. arXiv:2408.00714 [cs.CV] https://arxiv.org/abs/2408.00714 Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. 2024. Customize-a-video: One-shot motion customization of text-to-video diffusion models. arXiv preprint arXiv:2402.14780 (2024). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. arXiv:2112.10752 [cs.CV] https://arxiv.org/abs/2112.10752 Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. 2024. Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer. arXiv preprint arXiv:2405.17405 (2024). Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning. PMLR, 22562265. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 (2020). Genmo Team. 2024. Mochi 1. https://github.com/genmoai/models. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2018. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717 (2018). Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. 2024a. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 93269336. Zhao Wang, Aoxue Li, Lingting Zhu, Yong Guo, Qi Dou, and Zhenguo Li. 2024b. Customvideo: Customizing text-to-video generation with multiple subjects. arXiv preprint arXiv:2401.09962 (2024). Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. 2024c. Motionctrl: unified and flexible motion controller VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer 11 after mixed training, such as \"Ta-da it.\" In the future, implementing strategies such as Mixture of Experts (MOE) [Chen et al. 2022] may improve the performance of mixed training approaches. MORE CONTROLLED QUALITATIVE RESULTS In this section, we present additional results on spatial and temporal controlled VFX video generation using VFX Creator. Fig. 8 illustrates qualitative findings for two distinct effects across four cases, showcasing VFX Creators ability to perform instance-level object manipulation with precision. Additionally, the temporal results depicted in Fig. 10 demonstrate VFX Creators capacity to control the rhythm of VFX video generation over time. This underscores the accuracy and effectiveness of the control mechanisms in our method. MORE QUALITATIVE COMPARISON RESULTS In this section, we present additional qualitative comparison results of VFX Creator with other video generation models, including LTXVideo [HaCohen et al. 2024], and CogVideoX [Yang et al. 2024]. As illustrated in Fig. 9, LTX-Video and CogVideoX demonstrate failure to accurately interpret the visual effect prompt, leading to the generation of only minimal actions. In contrast, VFX Creator and Pika (ground truth) exhibit significantly better comprehension and generation of the corresponding effect videos. Notably, for the \"Crumble it\" effect, our methodology yields results that are not only more complete but also superior in quality when compared to the ground truth. DEFINATIONS OF VISUAL EFFECTS The Open VFX dataset encompasses 15 distinct categories of visual effects, featuring wide array of subjects, including characters, animals, products, and scenes. As illustrated in Table 4, we offer comprehensive explanation of the specific meanings associated with each visual effect, facilitating deeper understanding for users. Table 4. Visual Effect Types and Corresponding Definitions in the Open VFX Dataset."
        },
        {
            "title": "Definition",
            "content": "Cake-ify it Transform the subject into hyper-realistic prop cakes."
        },
        {
            "title": "Crumble it",
            "content": "Break apart the subjects into fragments."
        },
        {
            "title": "Crush it",
            "content": "Apply hydraulic press to flatten the subject."
        },
        {
            "title": "Decapitate it",
            "content": "Simulate the decapitation of subjects."
        },
        {
            "title": "Explode it",
            "content": "Eye-pop it"
        },
        {
            "title": "Levitate it",
            "content": "Similar to balloon losing air, cause subjects to shrink and flatten. Cause the object to disintegrate into nothingness. Burst the subject into fragments. Make the eyes of subjects bulge or pop out. Puff up the still subject like balloon. Make static objects or subjects appear to float or hover."
        },
        {
            "title": "Melt it",
            "content": "Turn objects into fluid, gooey forms."
        },
        {
            "title": "Squish it",
            "content": "Ta-da it Compress the subject as though under immense pressure. With flourish, subjects disappear behind cloth."
        },
        {
            "title": "Transform into\na black Venom",
            "content": "Characterize the static subject, transforming it into black Venom."
        },
        {
            "title": "Transform into\nHarley Quinn",
            "content": "Characterize the static subject, transform it into Harley Quinn. QUANTITATIVE RESULTS OF MIXED TRAINING As data-efficient system capable of achieving visual effect personalization, we attempted to explore unified VFX generation model. Specifically, we tuned 15 effects using 600 effect videos in combined manner. We then evaluated the results of the unified model, calculating their FID-VID [Unterthiner et al. 2018], [Balaji et al. 2019], and Dynamic Degree [Huang et al. 2024]. Additionally, we compared the results of the 15 effects from both category-specific training and unified and mixed training. As shown in Table 5, experimental results indicate that the quality of unified training falls short compared to category-specific training, as direct unification tends to confuse multiple visual effects. Most of the effect results indicate that mixed training leads to decline in overall video quality, although about one-third of the effects exhibit slight improvements 12 Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo Table 5. Comparison results of mix and single training for visual effect generation. t FID-VID FVD Dynamic Degree t Single Mix Single Mix Single Mix - C 54.48 67.22 1140 1503 0.8 0.8 m C 65.11 65.06 1690 1738 0.8 0.8 r 46.71 44.52 1000 0.0 0.0 t c 43.76 44.52 1263 1054 0.6 0.6 fl e s e p o - y a 103.90 111.28 76.14 87. 50.97 84.19 34.87 54.43 94.62 117.34 2034 2133 0.0 0.0 1463 2394 2612 1547 1641 3566 3811 0.8 0.8 1.0 1.0 0.0 0. 1.0 1.0 fl 86.14 77.35 1946 2184 0.8 0.8 t L e 35.12 68.32 63.37 70.38 665 1018 0.0 0.0 1794 0.6 0.6 u 44.35 52.36 1644 1543 1.0 1.0 - m V 54.73 34.65 117.90 108.99 726 979 1.0 1.0 3668 1.0 1.0 Fig. 8. More spatial controlled VFX generation results of our method on two different visual effects. VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer 13 Fig. 9. More qualitative comparison results of VFX video generation on two different visual effects between our method, DynamiCrafter, LTX-Video, CogVideoX5B, and Pika. 14 Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo Fig. 10. More temporal controlled VFX generation results of our method."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology, China",
        "Tencent AI Lab, China"
    ]
}