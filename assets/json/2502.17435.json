{
    "paper_title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
    "authors": [
        "Chen-Wei Chang",
        "Cheng-De Fan",
        "Chia-Che Chang",
        "Yi-Chen Lo",
        "Yu-Chee Tseng",
        "Jiun-Long Huang",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in bi-directional evaluations. These results highlight our method's stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 5 3 4 7 1 . 2 0 5 2 : r GCC: Generative Color Constancy via Diffusing Color Checker Chen-Wei Chang1 Cheng-De Fan1 Chia-Che Chang2 Yi-Chen Lo2 Yu-Chee Tseng1 Jiun-Long Huang1 Yu-Lun Liu1 1National Yang Ming Chiao Tung University 2MediaTek Inc. https://chenwei891213.github.io/GCC/ Figure 1. Our method performs color constancy through diffusion-based color checker inpainting. (top left) Given an input image, we first inpaint color checker with Stable Diffusion, aligning the achromatic (gray) squares to accurately reflect the scene illumination (top middle). The RGB color extracted from the achromatic squares is then used to remove the color cast from the input image (top right). (Bottom) Our approach leverages the strong priors of pre-trained diffusion models to accurately estimate scene illumination without requiring physical color checkers during capture, enabling accurate white balance correction across diverse scenes."
        },
        {
            "title": "Abstract",
            "content": "acteristics without requiring sensor-specific training, making it versatile solution for real-world applications. Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) Laplacian composition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) maskbased data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-theart worst-25% error rates of 5.15 and 4.32 in bi-directional evaluations. These results highlight our methods stability and generalization capability across different camera char1. Introduction Color constancy is crucial aspect of computer vision, focused on determining the illumination of scene to ensure that colors are accurately represented under varying lighting conditions. This process is essential for maintaining consistent color appearance and for applications ranging from photography to autonomous driving. Traditional methods for color constancy, such as the Gray World [12], Gray Edge [58], and Shades-of-Gray [23] algorithms, rely on statistical assumptions about scene color distributions. While these methods are computationally efficient, they often struggle in challenging scenes with ambiguous color distributions[50]. More sophisticated statistical approaches, like Bright Pixels 1 [38] and Gray Index [50], have been proposed but remain sensitive to violations of their underlying assumptions. In contrast, learning-based methods have demonstrated superior performance by leveraging training data to learn complex illumination priors. Early approaches focused on gamut mapping [6, 15] or simple regression models [24]. With the advent of machine learning, methods such as Cheng Color Constancy (CCC) [17] employed regression-based learning, while Fast Fourier Color Constancy (FFCC) [9] utilized frequency-domain transformations to efficiently estimate illumination. Deep learning further advanced the field, with methods like C4 [8] and FC4 [37], the latter introducing confidence-weighted pooling to automatically identify important spatial regions for illumination estimation. However, significant challenge in learning-based color constancy is that models are often constrained to specific camera sensors due to variations in spectral sensitivities [1, 27]. Models trained on one camera frequently fail to generalize to others without retraining or calibration [43]. Recent works have approached this problem from various angles. For instance, IGTN [63] introduced metric learning to learn scene-independent illuminant features, while quasiunsupervised approaches [10] leverage semantic features of achromatic objects for better cross-sensor generalization. Several studies have attempted to address the multi-sensor challenge through domain adaptation techniques [20, 55] or by learning device-independent intermediate representations [1]. C5 [2] proposed an innovative approach that uses multiple unlabeled images from the target camera during inference to calibrate the model to new sensors. CLCC [45] further improved upon this by introducing contrastive learning to ensure that images of the same scene under different illuminants have distinct representations, while different scenes under the same illuminant have similar representations. In this paper, we present novel approach to color constancy that leverages inpainting techniques to integrate color checker directly into the image. Our method utilizes Laplacian decomposition, allowing us to preserve highfrequency structural details while reducing the influence of low-frequency color information from the inserted color checker. In summary, we make the following contributions: We are the first to utilize inpainting for color checker integration in color constancy tasks, providing new avenue for illumination estimation without the need for extensive camera-specific training data. By employing Laplacian decomposition, we enhance the models ability to generate color checker that is structurally consistent with the input image, thereby improving the accuracy of color extraction from the patches. Our method operates in deterministic manner, avoiding the introduction of noise during training and inference, which enables more reliable results with better computational efficiency. 2. Related Work 2.1. Color Constancy and White Balance Color constancy research spans statistical-based and learning-based approaches. Statistical methods like Gray World [12], Gray Edge [58], Shades-of-Gray [23], Bright Pixels [38], and Gray Index [50] make assumptions about scene color statistics but struggle with challenging scenes. Learning-based methods have proven more effective, evolving from gamut mapping [6, 15] and regression models [24] to deep learning approaches. Notable developments include CCC [8], FFCC [9], and FC4 [37]. key challenge is camera-specific spectral sensitivity [1, 27], requiring retraining or calibration for new sensors [43]. Recent solutions include IGTNs [63] metric learning, quasi-unsupervised learning [10], and domain adaptation approaches [20, 55]. C5 [2] uses unlabeled target camera images during inference, while CLCC [45] employs contrastive learning to improve feature representations. Our work leverages diffusion models for color checker inpainting, offering novel approach to illumination estimation without extensive camera-specific training data. 2.2. Image-Conditional Diffusion Models Denoising Diffusion Probabilistic Models (DDPMs) [57] achieve state-of-the-art generation by reversing noising process with UNet architectures [53], demonstrating excellence in density estimation and sample quality [22, 41]. Latent Diffusion Models (LDMs) [52] improved efficiency by operating in compressed latent space and introduced crossattention conditioning. This enabled powerful inpainting capabilities, demonstrated by Blended Diffusion [4, 5], Paintby-Example [64], ControlNet [69], and IP-Adapter [66]. Recent work identified that perceived limitations were often due to DDIM scheduler implementation issues [44] rather than fundamental constraints. Our work leverages these insights to effectively adapt diffusion models for color checker inpainting in illumination estimation. 2.3. Learning-based Lighting Estimation Lighting estimation methods traditionally use physical probes like mirror balls [21], 3D objects [46, 62], eyes [48], or faces [13, 67]. Early probe-free approaches used limited models like directional lights [39], sky models [34, 35], or spherical harmonics [30]. Modern methods focus on HDR environment maps, pioneered by Gardner et al. [29]. DeepLight [42] and EverLight [19] handle both indoor and outdoor scenes, while StyleLight [61] uses GANs for joint LDRHDR prediction. Some works explore panorama outpainting [3, 18] but struggle with HDR [19]. Recently, DiffusionLight [49] introduced virtual chrome ball synthesis using diffusion models. Our work follows similar direction but focuses on color checker inpainting for illumination estimation. 2 Figure 2. Overview of our training pipeline. Starting from pretrained stable-diffusion-2-inpainting, we enable color checker generation through end-to-end fine-tuning. Given ground truth color checker image and its mask, we apply color jittering in the masked region. The input image latent passes through Laplacian composition before being concatenated with the masked image latent and the resized mask for the SD Inpainting U-Net. The model is trained with an L2 loss between the inpainted output and ground truth image at fixed timestep . Figure 3. Overview of our inference pipeline for illumination estimation. neutral color checker is pasted onto the input image, which is then encoded into the latent space. The input latent is processed through Laplacian composition before being concatenated with the masked image latent and the resized mask. The modified U-Net generates an inpainted result at fixed timestep . After inverse gamma correction, we sample the color checker patches to obtain the final RGB illumination value. We highlight the steps and components that are different from the training pipeline. 2.4. Fine-tuning Strategies for Diffusion Models For personalization, DreamBooth [54] pioneered special token fine-tuning, while Gal et al. [25] and Voynov et al. [60] proposed learned word embeddings approaches. For geometry estimation, Marigold [40] demonstrated successful fine-tuning using synthetic data. Garcia et al. [28] revealed that simple end-to-end fine-tuning can outperform complex approaches once DDIM implementation issues are fixed. For efficiency, LoRA [36] introduced low-rank weight changes, while SVDiff [33] and orthogonal fine-tuning [51] proposed alternative parameterizations. Following Garcia et al. [28], we adopt simple full fine-tuning strategies for our color checker inpainting task. 3. Method Instead of directly predicting environmental RGB light, we propose to leverage diffusion models rich priors to inpaint color checker into the scene and extract illumination colors from it. As shown in Figs. 2 and 3, our pipeline consists of (1) During training, we fine-tune diffusion-based inpainting model at timestep t=T with images containing color checkers, optimizing for deterministic single-step inference (Sec. 3.1-3.2). (2) We introduce Laplacian decomposition to maintain the checkers high-frequency structure while allowing illumination-aware color adaptation (Sec. 3.3). (3) At inference time, we composite neutral color checker into given scene and use our fine-tuned model to inpaint it according to the scene illumination, from which we extract environmental colors (Sec. 3.4). 3.1. Network Architecture We base our model on stable-diffusion-2-inpainting [52] for its specialized local editing capability. The model consists of VAE encoder-decoder pair (E, D) and U-Net denoising backbone. Given an RGB image RHW 3 and binary mask {0, 1}HW indicating the color checker region, we first encode both the masked image and the original image into the latent space as zmasked = E(I (1 )) and = E(I), where denotes element-wise multiplication. The 3 Although SDEdit [47] could be applied to our task, it faces fundamental trade-off in noise level selection. On one hand, insufficient noise fails to effectively suppress the original chromatic information from the input image, making it difficult to adapt to the target scene illumination. On the other hand, excessive noise, while better at removing unwanted color information, can disrupt the structural consistency between the generated result and the input reference. Furthermore, for color constancy tasks, maintaining oneto-one correspondence between input and output is essential. While traditional diffusion models stochastic nature allows for ensemble improvements through multiple inferences, this comes at increased computational cost. Following [28], we adopt an end-to-end fine-tuning approach that enables single-step deterministic inference while maintaining high-quality color checker generation. Specifically, we fine-tune the inpainting U-Net at fixed timestep = as shown in Fig. 2. Given an input image and its corresponding mask M, we first obtain the augmented image Iaug by applying color jittering to the masked region. We then obtain its latent representation through the VAE encoder, = E(Iaug). The latent representation is processed through Laplacian decomposition to extract high-frequency components, zh = H(z). For single-step prediction, we directly set the noise term ϵ = 0 in the forward process: 1 αT ϵ. The denoised latent is then obzT = tained through ˆz0 = 1 αT ϵθ(zcombined, T, c), where zcombined = [zT , M, zmasked] Rhw(2d+1) represents the concatenated input features along the channel dimension, and denotes the text condition. Finally, we decode the latent to obtain the inpainted image: ˆI = D(ˆz0). The model is optimized using mean squared error loss: αT zT αT zh + ="
        },
        {
            "title": "1\nHW",
            "content": "(cid:88) (I i,j ˆIi,j)2, i,j (1) where (i, j) denotes the pixel coordinates, and and are the height and width of the image, respectively. Color checker misalignment issue. Existing color constancy datasets [16, 31] only provide rough bounding boxes for color checkers instead of precise corner point locations. This hinders our ability to accurately align the standard sRGB color checker with the one in the original image, affecting the models learning of the transformation from standard to harmonized colors. To overcome this limitation, we designed mask region-based data augmentation method. We first analyze two intuitive solutions: directly masking and allowing the model to perform inpainting. This approach results in generated color checkers with contours that do not meet our expectations, making accurate color extraction from the patches difficult (Fig. 4 (a)). The second solution involved overlaying the color checker template directly onto 4 Figure 4. Analysis of color checker alignment strategies. (a) Direct inpainting on masked regions leads to poor color checker structure. This is because we do not provide any guidance on the desired color checker structure, causing the model to generate contours that do not meet our expectations. (b) Using homography transform to overlay template suffers from pixel-level misalignment due to imprecise bounding box annotations. (c) Our mask color jittering approach overcomes corner point annotation limitations by allowing the model generate geometrically consistent color checker structures while accurately reflecting scene illumination. mask is downscaled by factor of 8 to match the latent resolution as Rhw, where = H/8, = W/8. During training, the U-Net denoiser ϵθ takes as input the concatenation of the noised latent zt, the downscaled mask , and the masked image latent zmasked along the channel dimension as zcombined = [zt, , zmasked] Rhw(2d+1), where is the latent dimension. Together with the timestep and text embedding c, the denoiser is trained to predict the noise as ϵθ(zcombined, t, c) Rhwd. At inference time, we obtain the final inpainted result by decoding the denoised latent ˆI = D(z0), where only the color checker region is modified while leaving the rest unmodified, making this architecture particularly suitable for color constancy task. 3.2. End-to-End Fine-Tuning Training. Although pre-trained diffusion models like SD and SD inpainting [52] models have been exposed to diverse image collections, additional fine-tuning crucial for generating precise color checkers that accurately reflect environmental illumination. As shown in our experiments Fig. 6, fine-tuning significantly impacts the models ability to generate color checkers that faithfully represent scene illumination. the original image (Fig. 4 (b)). However, due to the absence of precise corner point locations, alignment with the raw checker remains imperfect at per-pixel level even using homography transform. Masked color jittering. Therefore, we further explored third approach: directly applying strong color jittering to the mask region (Fig. 4 (c)). This seemingly counterintuitive method aims to destroy clues that may leak sensor-specific information, forcing the model to rely on information outside the mask region to reconstruct the original color checker that aligns with the ground truth. Random color jittering to the masked color checker region helps our model learn more robust mapping between the standard and scene-specific color spaces. The augmented image ˆI is obtained by: Iaug = (1 ) + (I), (2) where is the input image, is the binary mask, denotes element-wise multiplication, and () represents the color jittering function that randomly applies brightness, contrast, saturation adjustments and Gaussian noise to the masked region. By randomly perturbing the color checker region, we force the model to rely on contextual illumination cues rather than local color checker patterns. This approach overcomes the limitations of imprecise annotations in existing datasets and enhances the models ability to learn accurate illumination estimation from scene context. 3.3. Laplacian Decomposition Although mask color jittering addresses the imprecise corner annotation issue, the randomness in jittering may occasionally allow low-frequency information leakage from the masked region. This could cause the model to simply reconstruct the masked area rather than harmonize it with the scene illumination. To address this issue, we introduce the Laplacian decomposition technique. By extracting only the high-frequency components of the input image through Laplacian decomposition, our approach serves two purposes: First, it preserves the structural details needed to generate color checker that faithfully maintains the patch layout of our pre-pasted reference. Second, it minimizes the influence of low-frequency color information, encouraging the model to focus on harmonizing the generated color checker with the scene illumination rather than reconstructing the original colors. The detailed Laplacian decomposition process is presented in Algorithm 1. The key benefit of Laplacian decomposition, as shown in Fig. 6, allows the model to generate color checkers that maintain structural consistency while correctly reflecting scene illumination, enabling accurate illumination estimation. Algorithm 1: High-frequency Extraction via Laplacian Pyramid Input: Input latent RBCHW , pyramid levels Output: High-frequency components zh Initialize zh = 0 33 Gaussian kernel for each channel in do zcurr z[c] for = 0 to 1 do // Current level features // Gaussian blur // High-freq details zblur zcurr zhigh zcurr zblur if = 0 then zh[c] zhigh else zh[c] zh[c] + Upsample(zhigh) end zcurr AvgPool(zblur) // Downsample end end return zh 3.4. Inference The complete inference pipeline of our method is illustrated in Fig. 3, which consists of the following steps: Color checker generation. We first composite fixed-size neutral color checker centered at the mask region. The input image is then gamma-corrected with γ = 2.2 to transform it to the sRGB domain. This preprocessed image is processed through our model in single forward pass with fixed timestep = . The output is then inverse gamma-corrected to obtain the raw domain result. Illumination estimation. Since we have precise control over the initial color checker placement and the Laplacian decomposition ensures structural preservation, we can reliably extract color information from each patch. Specifically, we apply perspective transformation to align the generated checker into standardized rectangular grid, followed by applying fixed grid masks to sample colors from each patch. The scene illumination is then estimated from the achromatic patches of the color checker. 4. Experiments 4.1. Experimental Setup Dataset. We use two publicly available color constancy benchmark datasets in our experiments: the NUS 8-Camera dataset [16] and the re-processed Color Checker dataset [31] (referred to as the Gehler dataset). The Gehler dataset contains 568 original images captured by two different cameras, while the NUS 8-Camera dataset [16] contains 1736 original images captured by eight different cameras. Each image in both datasets includes Macbeth Color Checker (MCC) chart, which serves as reference for the ground-truth illuminant color. 5 For both datasets, we adopt three-fold cross-validation protocol for our experimental evaluation. This ensures that the training and testing data are completely separated, reflecting the models generalization capability to unseen scenarios. Since the pre-trained VAE was trained on sRGB images, we apply gamma correction of γ = 1/2.2 on linear RGB images before encoding to minimize the domain gap. Conversely, after VAE decoding, we apply inverse gamma correction to convert the output back to the linear domain for metric evaluation. Evaluation metrics. To evaluate the performance of color constancy methods, we use the standard angular error metric, which measures the angular difference between the estimated illuminant and the ground-truth illuminant. Specifically, the angular error θ between an estimated illuminant vector ˆy and the ground-truth illuminant vector is defined as: θ = arccos (cid:19) (cid:18) ˆy ˆyy (3) The angular error is measured in degrees, with smaller values indicating better estimation accuracy. Following previous works, we report the following statistics of the angular error. 4.2. Implementation Details Our implementation is based on the Stable Diffusion v2 framework [52] using PyTorch, following parameter settings from [28]. We train our models using the Adam optimizer with an initial learning rate of 5105 and apply exponential learning rate decay after 150-step warm-up period. For cross-validation experiments, we train for 6k iterations with batch size 4 on the NUS-8 dataset, and 13k iterations with batch size 8 on the Gehler dataset. For crossdataset evaluation, when training on the Gehler dataset and testing on NUS-8, we use batch size of 8 with no gradient accumulation for 12k iterations. When training on NUS-8 and testing on the Gehler dataset, we use batch size of 8 with gradient accumulation over 2 steps (effective batch size of 16) for 15k iterations. For data augmentation, we follow FC4 [37] to rescale images by random RGB values in [0.6, 1.4], noting that we only rescale the input images since our training does not require ground truth illumination. The rescaling is performed in the raw domain, followed by gamma correction. This is implemented through 33 color transformation matrix, where diagonal elements control the intensity of individual RGB channels (color strength), and off-diagonal elements determine the degree of color mixing between channels (color offdiag). With probability of 0.8, we randomly crop region containing the mask, where the crop size ranges from 80% to 100% of the original image dimensions while ensuring the mask remains fully visible. Additionally, we apply local transformations to masked regions only, including brightness adjustment ([0.8, 2.0]), saturation adjustment ([0.8, 1.3]), contrast adjustment ([0.8, 1.3]), and Gaussian noise (σ [0, 30]). For the Laplacian decomposition, we use two-level pyramid (L = 2) to balance the preservation of high-frequency structural details and the suppression of low-frequency color information. All experiments were conducted on an NVIDIA RTX 4090 GPU. We will make our source code and fine-tuned model weights publicly available for reproducibility. 4.3. Results and Comparisons For evaluation, we follow the standard protocol of three-fold cross-validation on both the NUS-8 Camera dataset [16] and the re-processed Color Checker dataset [31]. Due to space limitations, we present the complete cross-validation results in the supplementary material. As shown in Tab. 1, our method demonstrates superior robustness in cross-dataset evaluation, where we train on one dataset and test on another. In this challenging scenario, our method achieves state-of-the-art performance particularly in the worst-25% metric, obtaining 5.22 degrees when trained on NUS-8 and tested on Gehler, and 4.32 degrees in the reverse setting. This improvement in handling difficult cases demonstrates the stability and generalization capability of our approach, suggesting that our method effectively leverages the pretrained diffusion prior to learn robust illumination patterns. Fig. 5 demonstrates the robustness and novel capabilities of our proposed method compared to prior approaches. In contrast to prior approaches, our methods ability to perform sampling at different positions and generate result ensembles allows us to quantify model uncertainty, showcasing the precision and consistency of our approach. Despite utilizing diffusion model, our method maintains efficient inference times due to its single-step design. Using an NVIDIA RTX 4090 GPU, our method processes 512512 image in 180ms, making it practical for realworld applications. This is significantly faster than traditional diffusion-based methods that typically require multiple denoising steps, as shown in Tab. 2 while maintaining diffusion priors benefits for accurate color constancy estimation. 4.4. Ablation Studies We conducted series of ablation experiments to validate the importance of key design choices, including using Laplacian composition, noise prediction-based LoRA fine-tuning, and mask-based data augmentation in Tab. 3. Without Laplacian decomposition. In this experiment, we solely rely on the VAE encoders latent representation as model input without utilizing the high-frequency components from the Laplacian decomposition. As shown in Fig. 6, the generated color checker is adversely affected by the low-frequency information from the neutral reference 6 Table 1. Camera-agnostic evaluation. All results are in units of degrees. Training set Testing set NUS 8-Camera [17] Color Checker [56] Color Checker [56] NUS 8-Camera [17] Method Mean Median Tri-mean Best 25% Worst 25% Mean Median Tri-mean Best 25% Worst 25% Statistical Methods White-Path [11] Gray-World [12] 1st-order Gray-Edge [59] 2nd-order Gray-Edge [59] Shades-of-Gray [23] General Gray-World [7] Grey Pixel (edge) [65] Cheng et al. [17] LSRS [26] GI [50] Learning-based Methods Bayesian [32] Chakrabarti [14] FFCC [9] SqueezeNet-FC4 [37] C4 SqueezeNet-FC4 [68] SIIE [1] CLCC [45] C5 [2] Ours 7.55 6.36 5.33 5.13 4.93 4.66 4.60 3.52 3.31 3. 4.75 3.52 3.91 3.02 2.73 3.72 3.05 3.34 2.62 5.68 6.28 4.52 4.44 4.01 3.48 3.10 2.14 2.80 1.87 3.11 2.71 3.15 2.36 2.20 2.46 2.44 2.57 2.19 6.35 6.28 4.73 4.62 4.23 3.81 - 2.47 2.87 2.16 3.50 2.80 3.34 2.50 2.28 2.79 2.51 2.68 2.29 1.45 2.33 1.86 2.11 1.14 1.00 - 0.50 1.14 0. 1.04 0.86 1.22 0.81 0.72 1.02 0.89 0.78 0.88 16.12 10.58 10.43 9.26 10.20 10.09 - 8.74 6.39 7.62 11.28 7.72 7.94 6.36 5.69 8.51 6.30 7.39 5.15 9.91 4.59 3.35 3.36 3.67 3.20 3.15 2.92 3.45 2.91 3.65 3.89 3.19 2.40 2.28 4.24 3.42 2.65 2.35 7.44 3.46 2.58 2.70 2.94 2.56 2.20 2.04 2.51 1. 3.08 3.10 2.33 2.03 1.90 3.88 2.95 1.98 2.06 8.78 3.81 2.76 2.80 3.03 2.68 - 2.24 2.70 2.13 3.16 3.26 2.52 2.10 1.97 3.93 3.06 2.14 2.12 1.44 1.16 0.79 0.89 0.99 0.85 - 0.62 0.98 0.56 1.03 1.17 0.84 0.70 0.67 1.45 0.94 0.66 0.87 21.27 9.85 7.18 7.14 7.75 6.68 - 6.61 7.32 6. 7.33 7.95 7.01 4.80 4.60 7.66 6.70 5.72 4.32 Figure 5. Sensitivity to color checker placement. This figure demonstrates the robustness of our method across various color checker positions in the scene. The left part displays different placements of color checkers and their corresponding processed results, showing that our method remains effective under challenging warm color temperatures (regions with lower data distribution). The scatter plots on the right quantitatively validate this observation, where the estimated illumination values consistently cluster near the ground truth target, confirming the precision and consistency of our approach. color checker initially placed in the scene. These disharmonious colors in the synthesized color checker lead to inaccurate color source estimation, as we can no longer extract reliable color values for computing the environment color. With noise. In this experiment, we used LoRA [36] to fine-tune the SDXL inpainting model. While the model architecture is the same as stable-diffusion-2-inpainting, we adopted noise prediction-based training approach: we inject noise into the latent representation of the input image and train the model to predict this noise, optimizing the LoRA parameters by computing the L2 loss between the predicted and actual noise. During inference, we employ SDEdit with noise strength of 0.6 and 25 denoising steps and compute the median from 10 generated samples. However, this approach shows limited effectiveness compared to our final method. The main limitation stems from the tradeoff between maintaining the color checkers geometry and suppressing the low-frequency information from the neutral color checker. While we need to preserve the color checkers Table 2. Comparison between fine-tuned SDXL inpainting and our one-step model. All metrics are reported in degrees, and inference time is measured on single 512512 image using an NVIDIA RTX 4090 GPU. All models are trained on the NUS-8 Camera dataset [17] and evaluated on the re-processed Color Checker dataset [31]. Method Steps Ensemble Inference time (s) Mean Median Best-25% Worst-25% Metrics () SDXL Inpainting (SDEdit) Full Model 25 1 10 17.98 0.18 4.47 2.62 3.25 2. 1.07 0.88 10.01 5.15 Table 3. Ablation study on key components of our method. We evaluate the impact of different components: Laplacian composition (Lap.), color checker inpainting vs. direct RGB prediction, and masked color jittering (Mask DA). All models are trained on the NUS-8 Camera dataset [17] and evaluated on the re-processed Color Checker dataset [31]. The results show that our color checker inpainting approach outperforms direct RGB prediction, and the combination with other components (Laplacian composition, and masked color jittering) yields the best performance. All error metrics are reported in degrees, with lower values indicating better performance. Noise Lap. Inpaint Mask DA Mean Median Best-25% Worst-25% Zeros Zeros Zeros Zeros - - - - - 3.71 3.52 2.98 2. 2.86 2.76 2.53 2.19 1.31 1.25 1.26 0.88 7.68 6.78 6.14 5.15 Figure 6. Importance of fine-tuning and Laplacian composition. (a) Results without fine-tuning show poor color checker quality due to the domain gap between pretrained diffusion models training data (sRGB images) and our gamma-corrected raw images, leading to disharmonious inpainting results. (b) Results without Laplacian composition are biased by the low-frequency information from the neutral color checker, leading to inharmonious generation. (c) Our full method with both components produces well-harmonized color checkers that accurately reflect scene illumination. shape, the insufficient noise level fails to effectively suppress the low-frequency information from the neutral color checker, resulting in inferior color estimation. Without mask data augmentation. Initially, we use the color checker corner locations provided in the datasets and apply homography matrix to align the standard color checker to the original position. However, the inaccuracy of corner detection leads to pixel alignment issues. To overcome this limitation, we employ the mask-based data aug8 Figure 7. Failure cases. Our approach struggles when there is significant mismatch between the illumination of the original color checker and the ambient lighting in the scene. mentation approach, which avoids dependence on precise corner locations and effectively generates color checkers that harmonize with the scene. The reliance precise corner locations proves problematic, making our mask-based approach superior for generating scene coherent color checkers. Without inpainting color checker. In this experiment, we did not obtain the environment white balance color by inpainting color checker. Instead, we directly let the diffusion model predict the final output RGB. This direct prediction approach proves less effective than our inpainting-based method, highlighting the importance of using color checker references for accurate environmental color estimation. 5. Conclusion In this work, we have presented novel approach to color constancy that leverages image-conditional diffusion models to inpaint color checkers directly into images. This method not only enhances the accuracy of illumination estimation but also addresses significant limitations of traditional techniques, particularly their struggles with generalization across different camera sensors. By employing Laplacian decomposition, we effectively preserve high frequency structural details, ensuring that the inpainted color checkers harmonize with the original image context. Limitations. As shown in Fig. 7, our method struggles when there is significant mismatch between the inpainted color checker and the scenes ambient lighting. This typically occurs in challenging scenarios with multiple strong light sources of different colors or complex spatially-varying illumination. While diffusion models provide strong image priors, they sometimes prioritize visual plausibility over physical accuracy, especially in extreme lighting conditions."
        },
        {
            "title": "References",
            "content": "[1] Mahmoud Afifi and Michael Brown. Sensor-independent illumination estimation for dnn models. In BMVC, 2019. 2, 7, 12 [2] Mahmoud Afifi, Jonathan Barron, Chloe LeGendre, Yun-Ta Tsai, and Francois Bleibel. Cross-camera convolutional color constancy. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 2, 7 [3] Naofumi Akimoto, Seito Kasai, Masaki Hayashi, and Yoshimitsu Aoki. 360-degree image completion by two-stage conditional gans. In 2019 IEEE International Conference on Image Processing (ICIP), pages 47044708. IEEE, 2019. 2 [4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1820818218, 2022. 2 [5] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Trans. Graph., 42(4), 2023. [6] Kobus Barnard. Improvements to gamut mapping colour constancy algorithms. In European conference on computer vision, 2000. 2 [7] Kobus Barnard, Vlad Cardei, and Brian Funt. comparison of computational color constancy algorithms. i: Methodology and experiments with synthesized data. IEEE transactions on Image Processing, 11(9):972984, 2002. 7 [8] Jonathan Barron. Convolutional color constancy. In International Conference on Computer Vision, 2015. 2, 13 [9] Jonathan Barron and Yun-Ta Tsai. Fast fourier color constancy. In Computer Vision and Pattern Recognition, 2017. 2, 7, [10] Simone Bianco and Claudio Cusano. Quasi-unsupervised color constancy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 2 [11] David Brainard and Brian Wandell. Analysis of the retinex theory of color vision. JOSA A, 3(10):16511661, 1986. 7 [12] Gershon Buchsbaum. spatial processor model for object colour perception. Journal of the Franklin institute, 1980. 1, 2, 7 [13] Dan Calian, Jean-Francois Lalonde, Paulo Gotardo, Tomas Simon, Iain Matthews, and Kenny Mitchell. From faces to outdoor light probes. In Computer Graphics Forum, pages 5161. Wiley Online Library, 2018. 2 [14] Ayan Chakrabarti. Color constancy by learning to predict chromaticity from luminance. Advances in Neural Information Processing Systems, 28, 2015. 7 [15] Ayan Chakrabarti, Keigo Hirakawa, and Todd Zickler. Color constancy with spatio-spectral statistics. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011. 2 [16] Dongliang Cheng, Dilip K. Prasad, and Michael S. Brown. Illuminant estimation for color constancy: why spatial-domain methods work and the role of the color distribution. J. Opt. Soc. Am. A, 31(5):10491058, 2014. 4, 5, 6, 12, 13 [17] Dongliang Cheng, Dilip Prasad, and Michael Brown. Illuminant estimation for color constancy: why spatial-domain methods work and the role of the color distribution. JOSA A, 31(5):10491058, 2014. 2, 7, 8 [18] Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Jonathan Eisenmann, Siavash Khodadadeh, and Jean-Francois Lalonde. Guided co-modulated gan for 360 field of view extrapolation. In 2022 International Conference on 3D Vision (3DV), pages 475485. IEEE, 2022. 2 [19] Mohammad Reza Karimi Dastjerdi, Jonathan Eisenmann, Yannick Hold-Geoffroy, and Jean-Francois Lalonde. Everlight: Indoor-outdoor editable hdr lighting estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 74207429, 2023. 2 [20] Hal Daume III and Daniel Marcu. Domain adaptation for statistical classifiers. Journal of artificial Intelligence research, 2006. 2 [21] Paul Debevec. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, page 189198, New York, NY, USA, 1998. Association for Computing Machinery. 2 [22] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 2021. 2 [23] Graham Finlayson and Elisabetta Trezzi. Shades of gray In Color and Imaging Conference, and colour constancy. 2004. 1, 2, 7 [24] Brian Funt and Weihua Xiong. Estimating illumination chromaticity via support vector regression. In Color and Imaging Conference, 2004. 2 [25] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [26] Shaobing Gao, Wangwang Han, Kaifu Yang, Chaoyi Li, and Yongjie Li. Efficient color constancy with local surface reflectance statistics. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part II 13, pages 158173. Springer, 2014. 7 [27] Shao-Bing Gao, Ming Zhang, Chao-Yi Li, and Yong-Jie Li. Improving color constancy by discounting the variation of camera spectral sensitivity. JOSA A, 2017. 2 [28] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and Bastian Leibe. Finetuning image-conditional diffusion models is easier than you think. arXiv preprint arXiv:2409.11355, 2024. 3, 4, 6, 12 [29] Marc-Andre Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gambaretto, Christian Gagne, and JeanFrancois Lalonde. Learning to predict indoor illumination from single image. ACM Trans. Graph., 36(6), 2017. 2 [30] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-Francois Lalonde. Fast spatially-varying indoor lighting estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69086917, 2019. 9 [31] Peter Vincent Gehler, Carsten Rother, Andrew Blake, Tom Minka, and Toby Sharp. Bayesian color constancy revisited. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 18, 2008. 4, 5, 6, 8, 12, 13 [32] Peter Vincent Gehler, Carsten Rother, Andrew Blake, Tom Minka, and Toby Sharp. Bayesian color constancy revisited. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 18. IEEE, 2008. 7 [33] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact paarXiv preprint rameter space for diffusion fine-tuning. arXiv:2303.11305, 2023. 3 [34] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, and Jean-Francois Lalonde. Deep outdoor illumination estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 73127321, 2017. 2 [35] Lukas Hosek and Alexander Wilkie. An analytic model for full spectral sky-dome radiance. ACM Transactions on Graphics (TOG), 31(4):19, 2012. [36] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3, 7, 12 [37] Yuanming Hu, Baoyuan Wang, and Stephen Lin. Fc4: Fully convolutional color constancy with confidence-weighted pooling. In Computer Vision and Pattern Recognition, 2017. 2, 6, 7, 12, 13 [38] Hamid Reza Vaezi Joze, Mark Drew, Graham Finlayson, and Perla Aurora Troncoso Rey. The role of bright pixels in illumination estimation. In Color and Imaging Conference, 2012. 2 [39] Kevin Karsch, Varsha Hedau, David Forsyth, and Derek Hoiem. Rendering synthetic objects into legacy photographs. ACM Transactions on graphics (TOG), 30(6):112, 2011. 2 [40] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. 3 [41] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 2021. 2 [42] Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, and Paul Debevec. Deeplight: Learning illumination for unconstrained mobile mixed reality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. [43] Orly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks, Tianfan Xue, Nikhil Karnad, Qiurui He, Jonathan Barron, Dillon Sharlet, Ryan Geiss, et al. Handheld mobile photography in very low light. ACM Trans. Graph., 2019. 2 [44] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2024. 2 [45] Yi-Chen Lo, Chia-Che Chang, Hsuan-Chao Chiu, Yu-Hao Huang, Chia-Ping Chen, Yu-Lin Chang, and Kevin Jou. Clcc: Contrastive learning for color constancy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2, 7, 13 [46] Stephen Lombardi and Ko Nishino. Reflectance and illumination recovery in the wild. IEEE transactions on pattern analysis and machine intelligence, 38(1):129141, 2015. 2 [47] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations, 2022. 4 [48] Ko Nishino and Shree Nayar. Eyes for relighting. ACM Transactions on Graphics (TOG), 23(3):704711, 2004. 2 [49] Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Varun Jampani, Amit Raj, Pramook Khungurn, and Supasorn Suwajanakorn. Diffusionlight: Light probes for free by painting chrome ball. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98108, 2024. 2 [50] Yanlin Qian, Joni-Kristian Kamarainen, Jarno Nikkanen, and Jiri Matas. On finding gray pixels. In Computer Vision and Pattern Recognition, 2019. 1, 2, 7 [51] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Scholkopf. Controlling text-to-image diffusion by orthogonal finetuning. arXiv preprint arXiv:2306.07280, 2023. 3 [52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 2, 3, 4, 6, 12 [53] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, 2015. 2 [54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. [55] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In Computer VisionECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11, 2010. 2 [56] Lilong Shi. Re-processed version of the gehler color conhttp://www. cs. sfu. ca/ stancy dataset of 568 images. color/data/, 2000. 7 [57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, 2015. 2 [58] Joost van de Weijer, Theo Gevers, and Arjan Gijsenij. Edgebased color constancy. IEEE Trans. Image Process., pages 22072214, 2007. 1, 2 [59] Joost Van De Weijer, Theo Gevers, and Arjan Gijsenij. Edgebased color constancy. IEEE Transactions on image processing, 16(9):22072214, 2007. 10 [60] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. P+: Extended textual conditioning in text-to-image generation. 2023. 3 [61] Guangcong Wang, Yinuo Yang, Chen Change Loy, and Ziwei Liu. Stylelight: Hdr panorama generation for lighting estimation and editing. In European Conference on Computer Vision, pages 477492. Springer, 2022. 2 [62] Henrique Weber, Donald Prevost, and Jean-Francois Lalonde. Learning to estimate indoor lighting from 3d objects. In 2018 International Conference on 3D Vision (3DV), pages 199207. IEEE, 2018. 2 [63] Bolei Xu, Jingxin Liu, Xianxu Hou, Bozhi Liu, and Guoping Qiu. End-to-end illuminant estimation based on deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2 [64] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023. 2 [65] Kai-Fu Yang, Shao-Bing Gao, and Yong-Jie Li. Efficient illuminant estimation for color constancy using grey pixels. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 22542263, 2015. [66] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. 2023. 2 [67] Renjiao Yi, Chenyang Zhu, Ping Tan, and Stephen Lin. Faces as lighting probes via unsupervised deep highlight extraction. In Proceedings of the European Conference on computer vision (ECCV), pages 317333, 2018. 2 [68] Huanglin Yu, Ke Chen, Kaiqi Wang, Yanlin Qian, Zhaoxiang Zhang, and Kui Jia. Cascading convolutional color constancy. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1272512732, 2020. 7, 13 [69] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023."
        },
        {
            "title": "Overview",
            "content": "This supplementary material presents additional details and results to complement the main manuscript. In Section A, we provide comprehensive implementation details, including dataset preprocessing protocols and training configurations. Section presents an empirical analysis of the impact of different pyramid levels in our Laplacian decomposition technique. Section showcases qualitative results demonstrating our methods effectiveness across various datasets and real-world scenarios, while Section details our crossvalidation evaluation protocol and comparative analysis with state-of-the-art methods. We will release our complete training and inference code along with pre-trained weights to facilitate future research in this area. A. Implementation Details A.1. Datasets and Preprocessing We use two publicly available color constancy benchmark datasets in our experiments: the NUS 8-Camera dataset [16] and the re-processed Color Checker dataset [31] (referred to as the Gehler dataset). The Gehler dataset contains 568 original images captured by two different cameras, while the NUS 8-Camera dataset [16] contains 1736 original images captured by eight different cameras. Each image in both datasets includes Macbeth Color Checker (MCC) chart, which serves as reference for the ground-truth illuminant color. Following the evaluation protocol in [1], several standard metrics are reported in terms of angular error in degrees: mean, median, tri-mean of all the errors, the mean of the lowest 25% of errors, and the mean of the highest 25% of errors. Following the preprocessing protocol from [37], we process the raw image data before applying gamma correction for sRGB space conversion. Since the pre-trained VAE was trained on sRGB images, we apply gamma correction of γ = 1/2.2 on linear RGB images before encoding to minimize the domain gap. Conversely, after VAE decoding, we apply inverse gamma correction to convert the output back to the linear domain for metric evaluation. A.2. Training Details Figure 8. Flow diagram of Laplacian composition. Frequency component fusion through two-level (1/2 resolution) blur, downsample, and composition operations. and testing on NUS-8, we use batch size of 8 with no gradient accumulation for 12k iterations. When training on NUS-8 and testing on the Gehler dataset, we use batch size of 8 with gradient accumulation over 2 steps (effective batch size of 16) for 15k iterations. SDXL Inpainting (SDEdit) For the SDXL inpainting model [52] with LoRA fine-tuning experiments, we use learning rate of 5 105 and LoRA rank of 4. In the crossdataset experiment from the NUS-8 Camera dataset to the Gehler dataset, we train for 20,000 iterations with batch size 4. A.3. Inference Settings Full Model Following Garcia et al. [28], we employ DDIM scheduler with fixed timestep = and trailing strategy during inference for deterministic single-step generation. Our implementation is based on the stable-diffusion2-inpainting model [52]. SDXL Inpainting (SDEdit) For comparison, we also implement version using SDXL inpainting model [52] with LoRA [36] fine-tuning. During inference, we use the DDIM scheduler with 25 denoising steps and SDEdit with noise strength of 0.6, guidance scale of 7.5, and LoRA scale of 1. The final illumination estimation is obtained by computing the median from an ensemble of 10 generated samples. B. Laplacian Decomposition Full Model We train our models using the Adam optimizer with an initial learning rate of 5105 and apply exponential learning rate decay after 150-step warm-up period. For cross-validation experiments, we train for 6k iterations with batch size of 2 on the NUS-8 Camera dataset and 13k iterations with batch size of 4 on the Gehler dataset. For cross-dataset evaluation, when training on the Gehler dataset B.1. Laplacian Deomposition Visualization In Fig. 8, we visualize how our Laplacian decomposition technique preserves high-frequency structural details while allowing illumination-dependent color adaptation. The pyramid decomposition effectively separates the color checkers geometric pattern from its chromatic information, enabling 12 Table 4. Analysis of different pyramid levels in Laplacian composition. Results are trained on the NUS-8 Camera dataset and tested on Gehler dataset . Level Mean Median Best-25% Worst-25% = 1 = 2 = 3 3.53 2.67 3.16 3.27 2.25 2. 1.48 0.89 1.25 6.03 5.22 5.62 Table 5. Result on NUS-8 Camera dataset, with mean angular error in degrees. NUS-8 Camera dataset Mean Med. Tri. Best Worst 25% 25% CCC [8] AlexNet-FC4 [37] FFCC [9] C4 CLCC [45] Ours SqueezeNet-FC4 [68] 2.38 2.12 1.99 1.96 1.84 2.10 1.48 1.53 1.31 1.42 1.31 1.52 1.69 1.67 1.43 1.53 1.42 1.69 0.45 0.48 0.35 0.48 0.41 0. 5.85 4.78 4.75 4.40 4.20 4.38 Table 6. Result on Gehler dataset, with mean angular error in degrees. Gehler dataset Mean Med. Tri. Best Worst 25% 25% CCC [8] SqueezeNet-FC4 [37] FFCC [9] C4 CLCC[45] Ours SqueezeNet-FC4 [68] 1.95 1.65 1.61 1.35 1.44 1.91 1.22 1.18 0.86 0.88 0.92 1.80 1.38 1.27 1.02 0.99 1.04 1.84 0.35 0.38 0.23 0.28 0.27 0. 4.76 3.78 4.27 3.21 3.48 3.46 For evaluation, we follow the standard protocol of threefold cross-validation on both the NUS-8 Camera dataset [16] and the Gehler dataset [31]. The results are presented in Tab. 5 and Tab. 6. As FC4 notes, many scenes have multiple light sources with differences up to 10 degrees, so further reducing an error already under 2 degrees may not be strong comparison. Instead, our method inpaints physically plausible color checkersa different strategy than directly optimizing for ground truth illumination RGB, which can yield lower single-camera performance but enables strong cross-camera generalization, as shown in our cross-dataset experiments. our model to maintain structural consistency while accurately reflecting scene illumination. B.2. Analysis of Pyramid Level Selection We conduct experiments with different numbers of pyramid levels (L = 1,2,3) to analyze the effectiveness of our Laplacian composition. As shown in Tab. 4, using two-level decomposition (L = 2) achieves the best performance across all metrics. Adding more levels not only increases computational complexity but also leads to performance degradation, as the additional levels introduce more low-frequency information that can adversely affect the harmonious generation of color checkers. C. Additional Qualitative Results C.1. Benchmark Datasets On the NUS-8 Camera dataset and Gehler dataset, we utilize the original mask locations to place fixed-size neutral color checkers in our experiments. The results Fig. 9 and Fig. 10 demonstrate our methods ability to generate structurally coherent color checkers that naturally blend with the scene while accurately reflecting local illumination conditions, enabling effective color cast removal across diverse lighting scenarios. C.2. In-the-wild Images For in-the-wild scenes, we adopt center-aligned placement strategy to address camera vignetting effects, which can impact color accuracy near image edges. This consistent central positioning not only mitigates lens shading issues but also demonstrates our methods flexibility in color checker placement. The results Fig. 11 validate our approachs robustness in practical photography applications, showing consistent performance in white balance correction despite the fixed central placement strategy. C.3. Interactive Visualization We provide an interactive HTML interface that visualizes results with color checkers placed at different locations within scenes. The visualization demonstrates that our method produces accurate outputs with minimal variation across different placement positions. The results show that the estimated illumination values consistently cluster near the ground truth target regardless of the checkers position, confirming our methods reliability and position-independence in illumination estimation. D. Cross-validation Results 13 Figure 9. Qualitative results for the NUS-8 Camera dataset. 14 Figure 10. Qualitative results for the Gehler dataset. Figure 11. Qualitative results for in-the-wild images with center-placed color checkers."
        }
    ],
    "affiliations": [
        "MediaTek Inc.",
        "National Yang Ming Chiao Tung University"
    ]
}