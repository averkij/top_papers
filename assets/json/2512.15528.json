{
    "paper_title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "authors": [
        "Daiqing Wu",
        "Dongbao Yang",
        "Can Ma. Yu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 8 2 5 5 1 . 2 1 5 2 : r Pre-print. EMOCALIBER: ADVANCING RELIABLE VISUAL EMOTION COMPREHENSION VIA CONFIDENCE VERBALIZATION AND CALIBRATION Daiqing Wu1,3 Dongbao Yang1 Can Ma1 Yu Zhou2 1IIE, Chinese Academy of Sciences wudaiqing@iie.ac.cn 2Nankai University 3University of Chinese Academy of Sciences"
        },
        {
            "title": "ABSTRACT",
            "content": "Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as deterministic task, requiring the model to output single, definitive emotion label for each image. Such formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber."
        },
        {
            "title": "INTRODUCTION",
            "content": "Perceiving and interpreting emotional signals from visual stimuli is critical ability that enables humans to communicate empathetically, facilitating smoother information exchange and enhancing interpersonal trust (Konecki, 2019). Consequently, this ability is indispensable to emotionally intelligent agents, whose applications span mental health support (Abdollahi et al., 2022), affective companionship (Park & Whang, 2022), and embodied service robotics (Seyitoglu & Ivanov, 2024). Accompanying this need, Visual Emotion Comprehension (VEC) has attracted rapidly growing attention in recent years (Zhao et al., 2021). This task requires models to analyze sentiment polarities or recognize the underlying emotions of given images, serving as cornerstone for developing and evaluating visual emotional intelligence in agents. Prior research on VEC primarily focuses on small-scale, task-specific models, which typically rely on fine-tuning to recognize predefined set of emotion categories and learn the corresponding imageemotion mapping. Although these models have achieved notable success, their capabilities are tightly bound to the training objectives, resulting in limited generalizability even to closely related settings (Lin et al., 2020). In contrast, the recent emergence of Multimodal Large Language Models (MLLMs) offers potentially more flexible solution for VEC. By integrating multimodal encoders with powerful Large Language Models (LLMs) (Liu et al., 2023), MLLMs inherit strong instruction-following abilities that support the execution of diverse range of vision tasks through unified interface (Yin et al., 2024). This makes them intrinsically more adaptable to the complex and continually evolving scenarios in real-world applications. 1 Pre-print. Figure 1: In addition to structured Chain-of-Thought (CoT) and the derived answer, EmoCaliber also produces self-evaluated confidence level. It enables users to adopt the output selectively, significantly enhancing the reliability of the VEC system. Figure 2: Illustration of the models evolution across the three training stages. Through these stages, the model is successively endowed with structured reasoning, taught to verbalize confidence, and finally calibrated to express confidence accurately. Recognizing these advantages, recent research has gradually shifted its focus toward optimizing VEC capabilities within MLLMs. EmoVIT (Xie et al., 2024) represents seminal effort in this direction by introducing the first emotion-centric dataset for visual instruction tuning. After it, growing body of research has explored complementary perspectives, including training-free adaptations (Zhang et al., 2024b; Fang et al., 2025), tuning-based optimization (Yang et al., 2024; Xing et al., 2024), and benchmarking specific properties (Gao et al., 2025; Wu et al., 2025b). Despite the current prosperity, these approaches mainly require MLLMs to produce deterministic responses for each query, framing VEC as definitive task. This formulation overlooks the inherent uncertainties arising from the perceptual subjectivity of emotion (LeDoux & Hofmann, 2018), where single image may evoke distinct emotional responses from different viewers (Zhao et al., 2016), or even from the same viewer under varying circumstances. Such oversight significantly compromises their reliability in downstream applications, many of which are highly sensitive due to their human-centric nature. In this manuscript, we introduce feasible solution to this issue by teaching MLLMs to explicitly verbalize their confidence levels after generating responses. As shown in Figure 1, the model is enabled to assign high confidence for responses supported by clear emotional cues and self-evaluated as accurate, while assigning low confidence to responses involving emotionally ambiguous or beyond its competence. This confidence dimension serves as an auxiliary indicator alongside reasoning and categorization, allowing users to selectively trust or utilize model outputs and thereby mitigating the unreliability caused by the previous definitive formulation. 2 Pre-print. Figure 3: Task composition of VECBench. The training split comprises Visual Sentiment Analysis (VSA) and Visual Emotion Recognition (VER) tasks, with each subtask denoted as sourcegranularity (#sample). In addition to retaining corresponding subtasks for in-domain (ID) evaluation, the test split also includes out-of-domain (OOD) VER tasks to verify generalization ability. To instantiate this capability, we devise three-stage training framework, as illustrated in Figure 2. In the first stage, we endow the model with structured reasoning capabilities via Supervised FineTuning (SFT). Inspired by the human affective cognition, we formalize the process from visual perception to affective categorization into five structured steps: (1) identifying prominent visual elements in the image; (2) providing detailed descriptions of human subjects, if present; (3) describing contextual elements beyond the subjects; (4) discussing how these elements interact; and (5) deriving an emotional conclusion based on the preceding observations. By structuring the reasoning process, we encourage the model to integrate comprehensive visual evidence before producing well-grounded predictions. We denote the obtained model as the scaffold model. In the second stage, we further equip the model with the ability to verbalize confidence after the answer. Using set of data unseen in stage one, we first task the scaffold model to generate structured reasoning. Subsequently, we append each reasoning path with confidence value, quantified by the deviation between the predicted emotion and the ground-truth label. We then perform SFT of the scaffold model on this augmented data to teach it to verbalize confidence, yielding an advanced version that we name the cold-start model. In the third stage, we adopt Reinforcement Learning (RL) to calibrate the models confidence expression, while concurrently incentivizing more precise affective reasoning. Specifically, we design three-component reward function to jointly optimize response format, emotion prediction, and confidence verbalization. By applying Group Relative Policy Optimization (GRPO) (Shao et al., 2024) on another held-out dataset, we ultimately obtain EmoCaliber, an MLLM proficient for VEC that not only achieves accurate emotion predictions via structured reasoning, but also provides calibrated self-assessment through confidence verbalization. To ensure systematic and comprehensive evaluation, we group six widely recognized VEC datasets into VECBench. Based on their taxonomies and annotations, we divide them into non-overlapping training and test splits. The training split comprises data from FI (You et al., 2016), WebEmo (Panda et al., 2018), and EmoSet (Yang et al., 2023), covering Visual Sentiment Analysis (VSA) and Visual Emotion Recognition (VER) tasks. Meanwhile, the test split not only retains in-domain data for evaluation but also incorporates samples from Abstract (Machajdik & Hanbury, 2010), Artphoto (Machajdik & Hanbury, 2010), and UnbiasedEmo (Panda et al., 2018) for out-of-domain generalization evaluation. The overall task composition is illustrated in Figure 3. By constructing VECBench, we aim to provide fair and reproducible baseline to facilitate future research on MLLMs for VEC. In summary, our contributions are threefold: We introduce practical solution to mitigate unreliability in the current definitive formulation of VEC by enabling MLLMs to verbalize their self-estimated confidence for each response. To instantiate this capability, we devise three-stage training framework, which gradually endows structured reasoning, teaches confidence verbalization, and calibrates confidence Pre-print. expression. This ultimately leads to EmoCaliber, model capable of performing VEC reliably and accurately. Through comprehensive evaluation on VECBench, we demonstrate that EmoCaliber delivers not only significant advantages in accurately self-evaluating confidence, but also consistent superiority in both in-domain and out-of-domain VEC performance. The remainder of this paper is organized as follows. Section 2 reviews prior work closely related to this study. Section 3 describes the proposed training framework in detail. Section 4 presents extensive experimental results, including comparisons with state-of-the-art MLLMs and ablation studies. Finally, Section 5 concludes the paper."
        },
        {
            "title": "2.1.1 SMALL-SCALE SPECIALIZED VEC MODELS",
            "content": "As long-standing research field, the development of VEC has largely mirrored the broader evolution of Artificial Intelligence. Early VEC studies primarily rely on hand-crafted features, exploring cues from low-level textures to mid-level aesthetic attributes and high-level semantic representations (Zhao et al., 2014). With the rise of pretrained models, researchers naturally explore their application to VEC, where the remarkable performance improvements validate the clear benefits of general-purpose knowledge for perceiving emotions (Yang et al., 2017; Di et al., 2023). Subsequent studies further advance this direction, typically by introducing human priors through elaborating modules to better capture spatial clues and leverage pretrained knowledge (Xiuzhuang et al., 2018; Wu et al., 2025a). In recent years, the proliferation of visionlanguage pretraining has likewise stimulated progress in VEC, with researchers enhancing visual models through distilling emotional cues embedded in language models (Wu et al., 2024; Wang et al., 2025b). Despite achieving notable progress, their training paradigm imposes an implicit ceiling on generalization. Models trained on one emotion taxonomy cannot be directly applied to another, and even within the same taxonomy, performance degrades substantially when evaluated on images from different domains (Lin et al., 2020). This limitation constrains their applicability in scenarios that demand greater flexibility. 2.1.2 VEC SYSTEMS BASED ON MULTIMODAL LARGE LANGUAGE MODELS The recent emergence of MLLMs offers potential solution to this dilemma. At the cost of some computational efficiency, VEC systems based on MLLMs achieve unprecedented flexibility in comprehending diverse task formulations, while maintaining reasonable level of expertise supported by web-scale pretraining (Wu et al., 2025c). Recognizing these advantages, extensive research has emerged to investigate MLLM-based VEC. series of works focuses on tuning-based optimization, exploring how to inject emotional insights or sharpen emotion perception. For instance, EmoVIT (Xie et al., 2024) synthesizes the first emotion-centric instructiontuning dataset using an advanced proprietary LLM; EmoLLM (Yang et al., 2024) further scales up the size of fine-tuning data and designs modules to capture clues from content and relational viewpoints; EMO-LLaMA (Xing et al., 2024) targets facial expression recognition, adopting similar paradigm while augmenting it with expert feature extraction and contextual modeling. Another line of research avoids modifying MLLM parameters and instead leverages hierarchical prompting or MLLMs intermediate states to mirror human affective reasoning. SoV (Zhang et al., 2024b) breaks down emotion perception into subtasks and iteratively guides MLLMs through systematically organized set of prompts; SEPM (Fang et al., 2025) employs two-stage inference pipeline that first uncovers correlations among visual emotions and the MLLMs internal confidence, then applies tailored strategies to produce appropriate prompts. 2.1.3 HANDLING SUBJECTIVITY IN EMOTION PERCEPTION In MLLM-based VEC research, relatively limited effort has been devoted to handling the inherent subjectivity of emotion perception. This challenge stems from the fact that the same visual stimulus may evoke different emotional responses, depending on factors such as the viewers personality, cultural background, or current circumstances (Zhao et al., 2023). While mainstream VEC datasets Pre-print. use majority voting to reflect degree of perceptual consensus, cases where plausible interpretations are overlooked remain inevitable. As illustrated in the lower sample in Figure 2, the models explanation for sadness may resonate with some individuals while still contradicting the annotated label of awe. recent notable attempt to mitigate this issue during evaluation is the MVEI benchmark (Wu et al., 2025b), which reformulates the classification tasks originally designed for specialized models into an emotion statement judgement task tailored for MLLMs. This reformulation preserves evaluation depth through the expressive flexibility of statements, while reducing the impact of subjectivity by discretizing the answer space. This challenge also receives focus from an adjacent research field, multimodal sentiment analysis. OV-MER (Lian et al., 2025b) directly annotates videos with multiple open-vocabulary emotion labels via humanLLM collaboration strategy, thereby capturing the potential diversity induced by subjective perception. While inspiring, these approaches rely on training or evaluation protocols that differ from existing mature frameworks, which may limit the direct reuse of well-established baselines and subtask designs. In light of this, we propose teaching the model to verbalize its confidence level as an alternative perspective for handling subjectivity. This approach not only provides users with quantitative sense of potential subjective interpretations but also aligns with established frameworks. 2.2 CONFIDENCE VERBALIZATION AND CALIBRATION Since LLMs and MLLMs frequently generate hallucinations or fail to express uncertainty when faced with unfamiliar problems, eliciting the confidence of their responses has become crucial means to enhance their practical reliability. To achieve this, early attempts (Tian et al., 2023) rely on prompt engineering or consistency across sampled answers, which are limited by modest calibration effectiveness or significantly increased latency. Subsequent tuning-based methods introduce groupbased (Lin et al., 2022) or binary (Zhang et al., 2024a) confidence estimation, yet still fall short of capturing sample-wise, fine-grained uncertainty. The recent popularity of RL has further inspired new directions. For instance, SaySelf (Xu et al., 2024) establishes solid baseline by performing confidence calibration via RL on top of SFT; and RLCR (Damani et al., 2025) demonstrates the benefit of directly incorporating Brier score (Glenn et al., 1950) into the reward function. In VEC tasks, beyond their established role in enhancing reliability, confidence verbalization offers an additional intuitive advantage in reflecting the perceptual subjectivity of emotion. This motivates us to incorporate such capability into the MLLM-based VEC system."
        },
        {
            "title": "3 TRAINING FRAMEWORK",
            "content": "Building upon these insights, this section introduces three-stage training framework, with each stage detailed in Section 3.1, Section 3.2, and Section 3.3. 3.1 STAGE 1: ENDOWING WITH STRUCTURED REASONING Inferring emotions from visual cues is inherently complex, requiring the integration of salient elements, background contexts, and their interactions. This poses substantial challenge for generic MLLMs, whose Chain-of-Thoughts (CoTs) are typically unconstrained, leading to diffuse trajectories and neglect of critical clues. To enable structured reasoning with hierarchical clue modeling, we first perform SFT on curated CoT-annotated data. Due to the lack of suitable data, we begin by constructing the VEC-CoT dataset, with its overall pipeline illustrated in Figure 4. Inspired by human affective cognition (Ortony et al., 2022), we decompose VEC into five sequential steps: 1) Salient Element Identification. Detecting the primary visual elements that initially attract attention, which form the raw evidence for subsequent appraisal; 2) Human Subject Description. Detailing any present human figures, as they often convey the most explicit and socially anchored emotional signals; 3) Contextual Expansion. Extending the observation to broader environmental features, which modulate how identical elements may evoke differing interpretations under varying circumstances; 4) Interaction Modeling. Examining the relationships and causal dynamics among the observed elements, integrating how actions, objects, and settings reinforce or contradict each other; 5) Comprehensive Analysis. Consolidating the entire evidence 5 Pre-print. Figure 4: Construction pipeline of the VEC-CoT dataset from the training split of VECBench. Imagelabel pairs are first templatized and fed into proprietary MLLMs to synthesize structured CoTs, which are then subjected to strict quality evaluation. Finally, imagetext pairs with high-quality CoTs are retained and grouped into the VEC-CoT dataset. Table 1: Statistics of the VEC-CoT dataset used across training stages. The dataset is split by 6:3:1. VEC-CoT VER VSA Total EmoSet-6 WebEmo-25 WebEmo-7 FI-8 WebEmo-2 FI-2 1st Stage 2nd Stage 3rd Stage Total 30,022 14,921 4,996 49,999 12,371 6,095 2,052 20, 15,552 7,806 2,707 26,155 3,516 1,761 544 5,821 21,298 10,587 3,485 35,370 3,308 1,773 562 5,643 86,067 43,033 14,346 143,446 chain to derive the final emotional conclusion, thereby bridging low-level visual perception with high-level affective categorization. Building on this decomposition, we leverage advanced proprietary MLLMs to synthesize high-quality structured CoTs conditioned on both the image and its label. Concretely, we design dedicated prompt template that specifies the required CoT format and criteria, and apply it to imagelabel pairs from the VECBench training split. The proprietary MLLM (Seed1.5-VL (Guo et al., 2025)) then generates CoTs that adhere to the prescribed structure. To mitigate potential errors introduced during synthesis, we incorporate an additional filtering stage, which leverages another MLLM (GPT-5 (OpenAI, 2025)) to evaluate each synthesized CoT for issues such as hallucinations, label leakage, and logical inconsistencies. Only CoTs that pass this rigorous screening are retained as high-quality outputs, ultimately forming triplets together with their original imagelabel pairs. This procedure leads to the VEC-CoT dataset, which contains 143,446 imagelabelCoT triplets spanning diverse data sources and two major VEC task families. Detailed statistics are reported in Table 1. We subsequently split the dataset into three disjoint subsets, following 6:3:1 ratio, denoted VEC-CoT1/2/3, corresponding to the utilization across the three training stages. Let VEC-CoT-1 be denoted as {(xi, i=1, consisting of queries paired with high-quality responses. Each query xi contains an image and question, while the response comprises CoT enclosed within <think> tag pair and an emotion label enclosed within an <answer> tag pair. Each step in the CoT is further structured by tag pairs corresponding to its semantic role, namely, <element>, <human>, <context>, <interaction>, and <analysis>. In general, the response i,n]. We adopt Qwen2.5-VL (7B) (Bai et al., 2025b) as the base model, denoted by πθ, and perform SFT with cross-entropy objective. It encourages the model to learn reasoning paths from the synthesized CoTs, endowing it with structured reasoning that mirrors human cognition: is an n-length text sequence, which we denote as i,2, , = [y i,1, )}N Lstage1(θ) = Et (cid:104) log πθ(y i,txi, (cid:105) i,<t) , Et : expectation across variable t. (1) 6 Pre-print. Figure 5: Illustration of the 2nd training stage. The scaffold model first performs inference on unseen data, producing CoT and an emotion prediction. This prediction is then mapped onto VAD lexicon-based emotion loop to measure its normalized distance from the ground-truth label. The estimated confidence score, derived from this distance, is directly appended to the original CoT and prediction, forming the supervision data used for SFT. 3.2 STAGE 2: TEACHING TO VERBALIZE CONFIDENCE We refer to the resulting model as the scaffold model. Although it is equipped with structured reasoning, framing VEC as deterministic task still raises practical concerns regarding reliability. Therefore, in this stage, we further teach the model to verbalize its confidence level, as illustrated in Figure 5. For query xi from VEC-CoT-2, we first prompt the model to generate response yi and extract the content within the <answer> tag pair as its emotion prediction. Since the model has not encountered these samples during the first stage, some predictions may disagree with the annotated labels. Given the strong semantic correlation among emotion categories, we propose quantifying human prior knowledge about such deviations as reference for the models confidence estimation. For example, for an image labeled as amusement, the model should assign higher confidence estimation to an erroneous prediction of contentment than to one of anger. To quantify this, we leverage the NRC VAD lexicon (Mohammad, 2025), which characterizes words along three well-established affective dimensions, Valence, Arousal, and Dominance (VAD). The Euclidean distance in the VAD space thereby becomes an intuitive and psychology-grounded measurement. To normalize these distances, we design strategy that maps the VAD representations onto 2D circle while preserving their relative spatial relationships. For set of flat-structured categories, we compute the pairwise VAD distances and determine the optimal loop connecting them. This ring-shaped structure allows us to obtain perimeter-based normalized semantic distance between any two emotion categories by measuring their separation along its circumference. In cases where emotion categorization is hierarchical (i.e., Parrotts model (Parrott, 2001) of WebEmo), we put constraint on loop construction to ensure the adjacency of subcategories sharing the same parent, while keeping the rest of the procedure unchanged. The four emotion loops obtained through this procedure are shown in Figure 5, fulfilling all subtask requirements in VEC-CoT-2. We denote the normalized semantic discrepancy between predicted emotion ei and annotated emotion as d(ei, ) [0, 0.5]. Beyond this semantic prior, we further incorporate probability-based prior. Given an nlength response sequence yi = [yi,1, yi,2, , yi,n], we take the predicted token probabilities [pi,1, pi,2, , pi,n] derived from the model logits and average them over the sequence. Subse7 Pre-print. quently, we adopt heuristic computation to unify them into final confidence estimation response yi: of = (cid:26) 0.5 (1 + Ej[pi,j]) 0.5 d(ei, ) Ej[pi,j] if ei = , if ei = . (2) This computation guarantees that [0, 0.5] for incorrect ones. We then round this value to two decimal places and embed it within <confidence> tag pair, which is appended directly after response yi. Finally, the model πθ is still optimized by the cross-entropy objective, where the encapsulated confidence is simplified as : [0.5, 1.0] for correct predictions and Lstage2(θ) = Et (cid:104) log πθ(c i,txi, yi, (cid:105) i,<t) . (3)"
        },
        {
            "title": "3.3 STAGE 3: CALIBRATING CONFIDENCE EXPRESSION",
            "content": "We refer to the resulting model as the cold-start model. In this stage, we further optimize both confidence estimation and emotion prediction through RL. For query xi from VEC-CoT-3, we denote the models response as yi. The emotion prediction ei is extracted from the <answer> tag pair, and the confidence estimate ci is extracted from the <confidence> tag pair. Based on empirical exploration, we adopt the log-likelihood reward for confidence calibration. Let I(ei) denote binary indicator (0/1) of whether the prediction is correct. This reward is formulated as: Rconf(yi) = I(ei) log(ci) + (1 I(ei)) log(1 ci). (4) It encourages high confidence for correct predictions and low confidence for incorrect ones, while severely penalizing overconfident cases where confidence is misaligned with prediction correctness. For correctness, we adopt vanilla outcome-based reward Rcorrect(yi) = I(ei). We further incorporate binary (0/1) format reward Rformat(yi) to verify whether the model outputs properly structured reasoning and whether each component is correctly enclosed within its designated tag pair. The overall reward is their sum: R(yi) = Rconf(yi) + Rcorrect(yi) + Rformat(yi). Subsequently, we adopt GRPO (Shao et al., 2024) variant for policy optimization. Given query xi, the algorithm first samples group of responses [yi,1, yi,2, , yi,G] using previous policy model πold. According to Bereket & Leskovec (2025), we compute the advantage Ai,g of response yi,g without the normalization: Ai,g = R(yi,g) mean(R). The optimization objective of the cold-start model πθ in this stage is formulated as: Lstage3(θ) = Eg (cid:104) (cid:104) Et min(ρi,g,t(θ)Ai,g, clip(ρi,g,t(θ), 1 ϵ)Ai,g) βDKL(πθπref) (cid:105)(cid:105) . (5) ρi,g,t(θ) = πθ(yi,g,txi, yi,g,<t)/πold(yi,g,txi, yi,g,<t) serves as correction factor to simulate onpolicy sampled distribution. The KL-divergence term aims to constrain the model within trusted region around the reference model πref."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "To validate the effectiveness of the proposed method, we conduct comprehensive experiments in this section. Experimental preparations are described in Section 4.1, Section 4.2, and Section 4.3. The main performance comparisons are reported in Section 4.4 and Section 4.5, while analytical studies are presented in Section 4.6, Section 4.7, Section 4.8, and Section 4.9. 4.1 IMPLEMENTATION DETAILS In the first stage, we adopt batch size of 8, freeze the vision encoder, and fully fine-tune the LLM with learning rate of 1e-5. In the second stage, we lower the learning rate to 4e-6 while keeping all other configurations unchanged. In the third stage, we set the KL coefficient to 0.01 and fine-tune all parameters (including the vision encoder) under learning rate of 1e-6. 8 Pre-print. Table 2: Comparison of advanced MLLMs on emotion prediction on ID VER tasks of VECBench. Bold and underline indicate the best and second-best results, respectively. ID VER FI-8 WebEmo-7 WebEmo-25 EmoSetAverage Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Qwen2.5-VL (Bai et al., 2025b) InternVL3.5 (Wang et al., 2025a) Qwen3-VL (Bai et al., 2025a) GPT-5 (OpenAI, 2025) EmoVIT (Xie et al., 2024) Emotion-LLaMA (Cheng et al., 2024) EmoCaliber (Ours) 51.90 53.10 56.50 53. 59.70 23.50 69."
        },
        {
            "title": "4.2 EVALUATION ON VECBENCH",
            "content": "50.95 51.96 54.57 Open-Source MLLM 43.87 43.60 41.09 42.00 49.70 49.56 Proprietary MLLM 45.61 44.50 53.06 Emotion-Oriented MLLM 44.40 58.16 18.70 18.97 54.00 70.00 48.34 20.11 52.81 21.00 16.10 21.10 21.11 15.46 21.72 47.30 51.90 52. 45.01 50.47 50.64 40.73 40.78 44.98 40.24 39.75 44.12 19.50 19.37 54. 52.94 43.15 42.75 15.00 10.30 28.30 15.12 10.04 24.28 59.40 18.70 68. 59.17 13.26 67.72 44.63 17.80 55.03 45.20 15.60 53.70 The strong generalizability of MLLMs allows them to operate beyond fixed traintest dataset pairs. While this flexibility is desirable, it inevitably raises concerns about fairness during evaluation. This issue deserves particular attention in VEC, which is composed of diversified yet fragmented datasets, where data used to train one model may inadvertently appear in the test split of another. To establish unified and fair evaluation protocol, we consolidate set of popular datasets into VECBench. Based on it, we designate held-out portion of data exclusively for testing, mitigating potential data leakage. Specifically, VECBench unifies six datasets. 1). FI (You et al., 2016) is collected from Flickr and Instagram and annotated via crowdsourcing using Mikels eight emotions (Mikels et al., 2005). In line with previous implementations, we map half of its labels to binary emotion polarity (FI-2), and retain the remaining half as FI-8. From each subset, we sample 1,000 instances for testing. 2). WebEmo (Panda et al., 2018) is large-scale dataset built upon keyword retrieval, annotated according to Parrotts hierarchical model. Each image receives fine-grained 25-class label, which can be further grouped into 7 and subsequently 2 higher-level classes. This hierarchical structure provides unique advantage in evaluating MLLMs in distinguishing subtle emotional differences. Following 1:1:1 ratio across the three hierarchies, we construct WebEmo-25, WebEmo-7, and WebEmo-2, sampling 1,000 images from each subset for testing. 3). EmoSet (Yang et al., 2023) is recent large-scale dataset also annotated with Mikels eight emotions, characterized by its high quality and coverage. We use it directly as EmoSet-8 and sample 1,000 instances for testing. The training split of VECBench is constructed by integrating all previous unselected samples. While in-domain (ID) testing captures models most direct emotion comprehension, its ability to generalize to out-of-domain (OOD) samples is equally indispensable in real-world applications. With this in mind, we introduce three additional datasets from distinct domains. 4) Abstract (Machajdik & Hanbury, 2010) consists of 229 images composed purely of color-texture combinations; 5) Artphoto (Machajdik & Hanbury, 2010) includes 806 artistic works sourced from art-sharing platforms, both of which are annotated based on Mikels model and absent from the training split. We directly incorporate all their samples as Abstract-8 and Artphoto-8 for testing. 6) UnbiasedEmo (Panda et al., 2018) is carefully curated 6-class dataset designed to challenge models in recognizing different emotions with the same object/scene. We sample 1,000 instances from it (UnbiasedEmo-6) for evaluation. Consequently, we group all datasets into the two mainstream VEC tasks, VER and VSA, based on their classification granularity. The resulting training split contains four VER sub-tasks (FI-8, WebEmo-7, WebEmo-25, EmoSet-8) and two VSA sub-tasks (FI-2, WebEmo-2). The test split includes the corresponding four ID VER and two ID VSA sub-tasks, as well as three OOD VER sub-tasks (Abstract-8, ArtPhoto-8, UnbiasedEmo-8). Summary statistics for all components are shown in Figure 3. 4.3 BASELINES AND EVALUATION METRICS To ensure fair and meaningful comparisons, our baselines include state-of-the-art open-source MLLMs (Qwen2.5-VL (Bai et al., 2025b), InternVL3.5 (Wang et al., 2025a), and Qwen3-VL (Bai et al., 2025a)), as well as representative proprietary MLLM (GPT-5 (OpenAI, 2025)). For MLLMs that have undergone emotion-oriented optimization, we include EmoVIT (Xie et al., 2024) and 9 Pre-print. Table 3: Comparison of MLLMs on emotion prediction on ID VSA tasks of VECBench. ID VSA FI-2 WebEmo-2 Average Acc F1 Acc F1 Acc F1 Qwen2.5-VL (Bai et al., 2025b) InternVL3.5 (Wang et al., 2025a) Qwen3-VL (Bai et al., 2025a) GPT-5 (OpenAI, 2025) EmoVIT (Xie et al., 2024) Emotion-LLaMA (Cheng et al., 2024) EmoCaliber (Ours) Open-Source MLLM 84.82 84.90 85.30 85.60 87.10 87.04 Proprietary MLLM 88.60 88.40 Emotion-Oriented MLLM 85.50 56.40 88.10 85.60 63.15 88. 69.50 70.30 75.90 69.64 69.47 75.71 77.20 77.95 81.50 77.23 77.39 81.38 73.70 73. 81.15 80.96 74.40 44.90 75.80 74.67 48.21 75.78 79.95 50.65 81.95 80.14 55.68 81. Table 4: Comparison of MLLMs on emotion prediction on OOD VER tasks of VECBench. OOD VER UnbiasedEmo-6 Abstract-8 Artphoto-8 Average Acc F1 Acc F1 Acc F1 Acc F1 Qwen2.5-VL (Bai et al., 2025b) InternVL3.5 (Wang et al., 2025a) Qwen3-VL (Bai et al., 2025a) GPT-5 (OpenAI, 2025) EmoVIT (Xie et al., 2024) Emotion-LLaMA (Cheng et al., 2024) EmoCaliber (Ours) 77.80 76.80 85.40 Open-Source MLLM 28.38 77.76 27.95 76.66 85.47 23.58 Proprietary MLLM 64.20 63.02 Emotion-Oriented MLLM 78.24 78.50 46.41 40.50 80.41 79.90 28.81 10.48 29. 41.92 31.35 28.63 22.87 36.97 40.82 40.32 37.43 41.80 41.10 47.72 48.52 49.77 48.85 49.03 49. 43.48 45.53 45.33 50.55 50.61 28.80 7.78 29. 41.61 22.95 41.94 38.09 16.46 41.84 49.64 24.64 50.37 48.38 23.55 50.73 Emotion-LLaMA (Cheng et al., 2024). Notably, the reported EmoVIT results are reproduced version based on Qwen2.5-VL. Most other emotion-oriented MLLMs are either not publicly available or not aligned with our task setting. One exception is AffectGPT (Lian et al., 2025a), which is similar to Emotion-LLaMA and tailored for video input. Preliminary experiments indicate that AffectGPT performs comparably to Emotion-LLaMA, yet substantially underperforms the imagetargeted baselines on VEC. For clarity and conciseness, we therefore exclude it from the main comparisons. For emotion prediction, we report Accuracy (Acc) and macro-F1 (F1). Accuracy measures the overall proportion of correctly classified samples, while macro-F1 emphasizes balanced performance across all categories by averaging F1 scores equally. For confidence evaluation, we follow prior work and adopt three complementary metrics. Expected Calibration Error (ECE) groups predictions into confidence bins and computes the discrepancy between average confidence and empirical correctness; lower values indicate better calibration. We use 10 bins, each covering confidence interval of 0.1. The Brier score measures the mean squared error between predicted confidence and binary correctness, with lower values preferable. Area under ROC curve (AUC) evaluates the models ability to distinguish correct from incorrect predictions across varying confidence thresholds, with 0.5 indicating random performance and 1.0 representing ideal discrimination. 4.4 COMPARISON ON EMOTION PREDICTION Table 2, Table 3, and Table 4 report the emotion prediction performance of MLLMs on the ID VER, ID VSA, and OOD VER tasks of VECBench, respectively. On the ID VER tasks, our proposed EmoCaliber significantly outperforms all competitors, including advanced open-source, proprietary, and emotion-oriented MLLMs. In particular, EmoCaliber achieves an average accuracy of 53.70, exceeding the second-best model, Qwen3-VL, by 10.05. These results validate the effectiveness of our structured reasoning design. By mirroring human affective cognition, it enables the model to capture fine-grained visual cues and more accurately model affective correlations, thereby leading to proficient emotion comprehension. It is worth noting that Emotion-LLaMA exhibits substantially lower performance compared to the others. As it is originally designed for video input, we include it as representative baseline to highlight the necessity of image-oriented methods for VEC. 10 Pre-print. Table 5: Comparison of MLLMs on confidence estimation on VECBench. Verb denotes using the verbalized confidence; Ans denotes using the token probability at the answer position; Avg denotes using the average probability over all response tokens. Confidence Estimation Qwen2.5-VL (Bai et al., 2025b) InternVL3.5 (Wang et al., 2025a) Qwen3-VL (Bai et al., 2025a) ID VER ID VSA OOD VER ECE Brier AUC ECE Brier AUC ECE Brier AUC Open-Source MLLM Verb Ans Avg Ans Avg Verb Ans Avg 49.20 50.61 42.44 48.89 39.80 48.95 26.31 46.05 48.18 51.06 41.98 52.82 39.50 47.21 39.62 45.71 56.21 59.93 58.58 56.30 59.24 65.60 53.13 59. 13.81 16.85 2.94 27.12 2.47 12.13 0.37 11.18 17.03 18.56 16.06 29.55 16.88 15.24 17.77 16.14 59.35 61.69 64.30 67.09 60.49 67.54 51.42 60.75 34.15 35.23 25.59 36.69 23.33 30.05 21.95 30.66 34.93 38.58 29.90 39.93 29.53 30.68 32.38 32.73 60.93 60.55 67.57 47.50 57.81 69.47 42.35 69. Proprietary MLLM GPT5 (OpenAI, 2025) Verb 28.20 29.99 69. 3.39 13.69 74.28 19.35 25.82 70. Emotion-Oriented MLLM EmoVIT (Xie et al., 2024) Ans Avg Emotion-LLaMA (Cheng et al., 2024) Avg EmoCaliber (Ours) Verb 33.09 33.52 62.19 13.63 39.11 35.91 53.31 22.77 49.63 57.43 50.00 70. 16.27 3.62 29.35 4.76 17.85 15.01 33.61 14.68 49.76 62.81 50.00 66.09 22.70 24.47 49.83 12.17 29.91 29.45 45.90 22.41 57.43 67.29 50.00 72. Table 6: Evolvement of model capability along the three training stages. For fair comparison, we uniformly adopt verbalized confidence as the models estimated confidence. Evolvement ID VER ID VSA OOD VER Acc ECE Brier Acc ECE Brier Acc ECE Brier Base Model 40.73 Scarffold Model 54.47 Cold-Start Model 54. EmoCaliber 55.03 - - - - - Initialized from Qwen2.5-VL 77.20 Stage 1: Endowing with Structured Reasoning 81.75 Stage 2: Teaching to Verbalize Confidence 14.67 14.79 81.60 Stage 3: Calibrating Confidence Expression 14.68 81.95 13.63 22.77 23.87 7.28 4. - - - 47.72 50.04 - - - - 49.81 13.01 22. 50.37 12.17 22.41 On the ID VSA tasks, although the performance gains are less pronounced, EmoCaliber still achieves the highest average accuracy and ranks within the top two across all subtasks. We attribute this to the relatively simpler nature of VSA, where the advantages of stronger affective reasoning may be partially offset by the general comprehension capabilities of more advanced models. On the OOD VER tasks, EmoCaliber also delivers competitive performance with an average accuracy of 50.37, the second-highest among all models and only slightly behind GPT-5 (50.55). Since these test samples lie entirely outside the training domains, this result validates that the gains of our model are not merely due to memorizing ID patterns. Instead, EmoCaliber generalizes robustly to more diverse data, achieving emotion prediction performance comparable to proprietary models. 4.5 COMPARISON ON CONFIDENCE ESTIMATION Table 5 compares the confidence estimation performance of MLLMs on VECBench. For each baseline, we elicit confidence estimates using up to three strategies when applicable. Verb denotes explicitly prompting the model to output confidence score; Ans extracts the predicted probability of the first decisive token in the answer; and Avg computes the average token probability over the entire output sequence, including both the reasoning and the final answer. Overall, EmoCaliber significantly outperforms all competing models on both ID VER and OOD VER tasks. Notably, on ID VER, EmoCaliber achieves an ECE of 13.63, which is nearly half that of the second-best baseline, Qwen3-VL (Ans) at 26.31. similar performance gap is observed on OOD VER, highlighting EmoCalibers clear advantage in calibrated confidence estimation for VER tasks. On ID VSA, EmoCaliber performs slightly below GPT-5 while remaining broadly comparable to other state-of-the-art MLLMs. We attribute this behavior to the training strategy, as it is inherently challenging to simultaneously prevent overconfidence in fine-grained multi-class tasks and ensure sufficient confidence in coarse-grained binary tasks. We further analyze this trade-off in the reward 11 Pre-print. Table 7: Influence of different learning rate settings during the first stage training. 1st Stage Base Model Scaffold Model #1 Scaffold Model #2 Scaffold Model #3 Scaffold Model #4 Scaffold Model #5 Scaffold Model #6 Scaffold Model #7 Scaffold Model #8 Learning ID VER ID VSA OOD VER Average Rate - 1e-6 2e-6 4e-6 8e-6 1e-5 2e-5 4e-5 8e-5 Acc F1 Acc F1 Acc F1 Acc F1 40.73 51.54 52.80 53.84 52.84 54.47 53.17 53.17 53.87 40.24 50.74 51.71 52.94 51.72 53.58 52.23 52.31 53.10 77.20 81.65 81.30 81.15 81.45 81.75 81.95 81.30 80.15 77.23 81.79 81.36 81.23 81.50 81.78 81.99 81.30 80.12 47.72 50.20 50.18 49.51 50.22 50.04 49.92 49.20 47. 48.85 50.35 49.84 49.66 50.84 50.55 50.11 50.30 48.15 55.22 61.13 61.43 61.50 61.50 62.09 61.68 61.22 60.47 55.44 60.96 60.97 61.28 61.35 61.97 61.44 61.30 60.46 Table 8: Influence of different learning rate settings during the second stage training. Bolded results indicate the best performance excluding the scaffold model. 2nd Stage Scarffold Model Cold-Start Model #1 Cold-Start Model #2 Cold-Start Model #3 Cold-Start Model #4 Cold-Start Model #5 Cold-Start Model # Learning ID VER ID VSA OOD VER Average Rate - 1e-6 2e-6 4e-6 6e-6 8e-6 1e-5 Acc Brier Acc Brier Acc Brier Acc Brier 54.47 52.97 53.27 54.05 54.05 52.94 53.22 - 25.22 24.74 23.87 24.56 24.71 24.99 81.75 81.60 80.90 81.60 81.85 81.30 81.25 - 14.94 14.90 14.79 14.78 14.73 14. 50.04 49.28 50.05 49.81 48.55 49.68 49.61 - 23.60 22.88 22.94 23.05 23.24 23.28 62.09 61.28 61.41 61.82 61.48 61.31 61.36 - 21.25 20.84 20.53 20.80 20.89 20.96 ablation study. Collectively, EmoCaliber demonstrates strong confidence estimation capabilities, establishing solid, practical baseline for building reliable VEC systems. 4.6 ANALYSIS OF MODEL EVOLVEMENT Table 6 compares emotion prediction and confidence estimation performance before and after each training stage. As shown, the first stage training substantially improves emotion prediction accuracy across all tasks. This confirms the consistent benefits of structured affective reasoning across different data domains and task granularities. In the second stage, the model is taught to express confidence, which is accompanied by slight degradation in emotion prediction performance. Finally, in the third stage, both emotion prediction and confidence estimation performances improve simultaneously. The former not only reverses the decline observed in the second stage but also surpasses the scaffold model, while the latter shows consistent gains across all confidence metrics. These results validate the effectiveness of RL in calibrating confidence expression. Overall, the results demonstrate that each training stage fulfills its intended objective, jointly contributing to the final performance of EmoCaliber. 4.7 ABLATION OF LEARNING RATE Given the complexity of jointly optimizing the three-stage training framework, we adopt greedy strategy for hyperparameter tuning to pursue local optimum. The effects of varying learning rates for the first and second training stages are presented in Table 7 and Table 8, respectively. In the first stage, the model is supervised to acquire structured reasoning capabilities. From Table 7, we observe that wide range of learning rates consistently lead to stable improvements across all tasks, validating that the benefits of the structured reasoning design and constructed VEC-CoT dataset are robust to optimization settings. Nevertheless, due to the sensitivity of SFT to learning rate selection, different choices still exert non-negligible impact on performance. Specifically, lower learning rates lead to slightly inferior performance on ID VER and ID VSA tasks, suggesting potential underfitting. In contrast, higher learning rates result in degraded performance on OOD VER tasks, possibly due to overfitting. Balancing these trade-offs, we adopt learning rate of 1e-5 in the final implementation. Empirically, this setting achieves the highest average accuracy across tasks, providing favorable balance between ID performance and OOD generalization. 12 Pre-print. Table 9: Influence of different RL configurations during the third stage training. 3rd Stage Rconf Normalize ID VER ID VSA OOD VER Average Acc Brier Acc Brier Acc Brier Acc Brier Cold-Start Model EmoCaliber #1 EmoCaliber #2 EmoCaliber #3 EmoCaliber #4 - Brier-based Brier-based Log-Likelihood Log-Likelihood - 54.05 53.94 54.54 53.67 55.03 23.87 45.10 42.80 44.55 22.77 81.60 80.00 82.25 81.45 81.95 14.79 19.61 16.74 18.07 14.68 49.81 51.03 47.33 49.30 50.37 22.94 40.28 41.64 39.77 22. 61.82 61.66 61.37 61.47 62.45 20.53 35.00 33.73 34.13 20.02 Figure 6: Distribution of verbalized confidences of models derived from different RL configurations on ID VER tasks. Error bars represent 95% confidence intervals. In the second stage, the model is trained to verbalize confidence, with optimization focused on confidence-related tokens. As shown in Table 8, even though tokens directly responsible for emotion prediction are excluded from optimization, the resulting decline in prediction accuracy is minimal. This suggests that the optimization directions for confidence estimation and emotion prediction are not inherently conflicting, supporting their joint optimization in the subsequent RL stage. Compared to the first stage, accuracy and Brier score exhibit more complex variations across different learning rates in this stage. Taking into account overall performance, we select learning rate of 4e-6, which achieves top-2 performance on most tasks. Notably, this value is lower than the 1e-5 used in the first stage, which is intuitive given that fewer tokens are actively updated during this stage. 4.8 ABLATION OF REWARD DESIGN Although RL has proved to be effective for confidence calibration, our experiments reveal that this process is notably unstable in VEC, with minor configuration changes leading to divergent optimization outcomes. Table 9 compares the effects of different RL setups. Our initial exploration (#1) adopts the Rconf formulation from RLCR (Damani et al., 2025): Rconf(yi) = (I(ei) ci)2, (with variable yi, ei, ci consistent with Equation (4)), (6) together with the standard GRPO algorithm. While this reward provides supervision aligned with calibration metrics, it imposes relatively smooth constraints over different confidence levels. Empirically, this setting leads to severe overconfidence on VEC, with confidence estimates heavily concentrated above 0.9. similar phenomenon has been reported in prior work (Bereket & Leskovec, 2025), which attributes it to the standard advantage normalization in GRPO, potentially inducing positive feedback loop. Specifically, when reward values within group exhibit small variance, the normalization amplifies minor relative advantages, causing the model to repeatedly increase the probability of slightly preferred responses and drive toward extreme confidence estimations. Motivated by this analysis, we explore removing advantage normalization and replacing the reward formulation with an alternative log-likelihoodbased objective (Equation (4); #4). Acting jointly, these Pre-print. Figure 7: Representative cases of EmoCaliber performing ID VER, ID VSA, and OOD VER tasks on VECBench. two modifications effectively mitigate the overconfidence issue and achieve the intended calibration behavior. In contrast, applying either modification (#2, #3) in isolation is insufficient. These effects are visualized in Figure 6, where EmoCaliber exhibits well-calibrated confidence estimates. 4.9 CASE STUDY Figure 7 presents representative examples of EmoCaliber performing ID VER, ID VSA, and OOD VER tasks on VECBench. These cases qualitatively illustrate how EmoCaliber derives emotion predictions through structured affective reasoning, as well as how it produces explicit confidence estimates. Specifically, the structured reasoning process enables EmoCaliber to jointly consider diverse visual cues, including salient subjects (present in all cases), fine-grained objects (e.g., the two human figures in the top example), fague contextual backgrounds (such as the warm indoor atmosphere in the middle example), and artistic or stylistic elements (e.g., the apocalyptic aesthetic in the bottom example). The confidence estimates further reflect both the inherent subjectivity of emotion perception and the models self-assessed reliability. EmoCaliber assigns higher confidence to relatively clear and straightforward cases (the top two examples), while expressing lower confidence for more ambiguous and uncertain inputs, such as the abstract artwork in the bottom example. Overall, these case studies provide intuitive evidence supporting the effectiveness of EmoCaliber for accurate and reliable VEC."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Overall, this paper presents practical solution toward more reliable VEC systems through the introduction of EmoCaliber. By verbalizing confidence alongside emotion predictions, the model is empowered to reflect the inherent subjectivity of emotion perception in images, effectively alleviating long-standing challenge while remaining compatible with existing evaluation protocols. To support this goal, we develop three-stage training framework and complementary data construction pipeline that yields high-quality VEC-CoT dataset. We further unify six widely used VEC datasets into VECBench, enabling fair and systematic evaluation of VEC in MLLMs. Extensive experiments 14 Pre-print. demonstrate that EmoCaliber consistently delivers superior performances in both emotion prediction and confidence estimation, while also revealing insights into different optimization configurations. Through EmoCaliber and VECBench, we hope to establish solid baselines for future research and facilitate the development of reliable VEC systems."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work is supported by the National Natural Science Foundation of China (Grant No. 62376266 & 62406318)."
        },
        {
            "title": "REFERENCES",
            "content": "Hojjat Abdollahi, Mohammad Mahoor, Rohola Zandie, Jarid Siewierski, and Sara Qualls. Artificial emotional intelligence in socially assistive robots for older adults: pilot study. IEEE Transactions on Affective Computing, 14(3):20202032, 2022. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Michael Bereket and Jure Leskovec. Uncalibrated reasoning: Grpo induces overconfidence for stochastic outcomes. arXiv preprint arXiv:2508.11800, 2025. Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang Peng, and Alexander Hauptmann. Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning. Advances in Neural Information Processing Systems, 37:110805110853, 2024. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806, 2025. Wang Di, Guo Xutong, Tian Yumin, Liu Jinhui, He Lihuo, and Luo Xuemei. TETFN: text enhanced transformer fusion network for multimodal sentiment analysis. Pattern Recognition, 136:109259, 2023. Yiyang Fang, Jian Liang, Wenke Huang, He Li, Kehua Su, and Mang Ye. Catch your emotion: Sharpening emotion perception in multimodal large language models. In Forty-second International Conference on Machine Learning, 2025. Lancheng Gao, Ziheng Jia, Yunhao Zeng, Wei Sun, Yiming Zhang, Wei Zhou, Guangtao Zhai, and Xiongkuo Min. Eemo-bench: benchmark for multimodal large language models on image evoked emotion assessment. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 70647073, 2025. Brier Glenn et al. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):13, 1950. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. Krzysztof Konecki. Trust in symbolic interactionist research and in phenomenological investigation. Polish Sociological Review, 207(3):271288, 2019. Joseph LeDoux and Stefan Hofmann. The subjective experience of emotion: fearful view. Current opinion in behavioral sciences, 19:6772, 2018. Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, Rui Liu, Xiaojiang Peng, et al. Affectgpt: new dataset, model, and benchmark for emotion understanding with multimodal large language models. arXiv preprint arXiv:2501.16566, 2025a. 15 Pre-print. Zheng Lian, Haiyang Sun, Licai Sun, Haoyu Chen, Lan Chen, Hao Gu, Zhuofan Wen, Shun Chen, Zhang Siyuan, Hailiang Yao, et al. Ov-mer: Towards open-vocabulary multimodal emotion recognition. In Forty-second International Conference on Machine Learning, 2025b. Chuang Lin, Sicheng Zhao, Lei Meng, and Tat-Seng Chua. Multi-source domain adaptation for visual sentiment classification. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 26612668, 2020. Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. Transactions on Machine Learning Research, 2022. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Jana Machajdik and Allan Hanbury. Affective image classification using features inspired by psychology and art theory. In Proceedings of the 18th ACM international conference on Multimedia, pp. 8392, 2010. Joseph Mikels, Barbara Fredrickson, Gregory Larkin, Casey Lindberg, Sam Maglio, and Patricia Reuter-Lorenz. Emotional category data on images from the international affective picture system. Behavior research methods, 37(4):626630, 2005. Saif Mohammad. Nrc vad lexicon v2: Norms for valence, arousal, and dominance for over 55k english terms. arXiv preprint arXiv:2503.23547, 2025. OpenAI. Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025. Andrew Ortony, Gerald Clore, and Allan Collins. The Cognitive Structure of Emotions. Cambridge University Press, 2022. Rameswar Panda, Jianming Zhang, Haoxiang Li, Joon-Young Lee, Xin Lu, and Amit RoyChowdhury. Contemplating visual emotions: Understanding and overcoming dataset bias. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 579595, 2018. Sung Park and Mincheol Whang. Empathy in humanrobot interaction: Designing for social robots. International journal of environmental research and public health, 19(3):1889, 2022. Gerrod Parrott. Emotions in Social Psychology: Essential Readings. Psychology Press, 2001. Faruk Seyitoglu and Stanislav Ivanov. Robots and emotional intelligence: thematic analysis. Technology in Society, 77:102512, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 54335442, 2023. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025a. Zou Wang, Sun Xia, Lu Qiang, Wang Xuxin, and Feng Jun. vision and language hierarchical alignment for multimodal aspect-based sentiment analysis. Pattern Recognition, 162:111369, 2025b. Daiqing Wu, Dongbao Yang, Yu Zhou, and Can Ma. Bridging visual affective gap: Borrowing textual knowledge by learning from noisy image-text pairs. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 602611, 2024. 16 Pre-print. Daiqing Wu, Dongbao Yang, Huawen Shen, Can Ma, and Yu Zhou. Resolving sentiment discrepancy for multimodal sentiment detection via semantics completion and decomposition. Pattern Recognition, 172:112719, 2025a. Daiqing Wu, Dongbao Yang, Sicheng Zhao, Can Ma, and Yu Zhou. Customizing visual emotion evaluation for mllms: An open-vocabulary, multifaceted, and scalable approach. arXiv preprint arXiv:2509.21950, 2025b. Daiqing Wu, Dongbao Yang, Sicheng Zhao, Can Ma, and Yu Zhou. An empirical study on configuring in-context learning demonstrations for unleashing mllms sentimental perception capability. In Forty-second International Conference on Machine Learning, 2025c. Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, and Wen-Huang Cheng. Emovit: Revolutionizing emotion insights with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2659626605, 2024. Bohao Xing, Zitong Yu, Xin Liu, Kaishen Yuan, Qilang Ye, Weicheng Xie, Huanjing Yue, Jingyu Yang, and Heikki Kälviäinen. Emo-llama: Enhancing facial emotion understanding with instruction tuning. arXiv preprint arXiv:2408.11424, 2024. Zhou Xiuzhuang, Jin Kai, Chen Qian, Xu Min, and Shang Yuanyuan. Multiple face tracking and recognition with identity-specific localized metric learning. Pattern Recognition, 75:4150, 2018. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, and Jing Gao. Sayself: Teaching llms to express confidence with self-reflective rationales. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 59855998, 2024. Jingyuan Yang, Qirui Huang, Tingting Ding, Dani Lischinski, Danny Cohen-Or, and Hui Huang. Emoset: large-scale visual emotion dataset with rich attributes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2038320394, 2023. Jufeng Yang, Dongyu She, and Ming Sun. Joint image emotion classification and distribution learning via deep convolutional neural network. In IJCAI, pp. 32663272, 2017. Qu Yang, Mang Ye, and Bo Du. Emollm: Multimodal emotional understanding meets large language models. arXiv preprint arXiv:2406.16442, 2024. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12), 2024. Quanzeng You, Jiebo Luo, Hailin Jin, and Jianchao Yang. Building large scale dataset for image emotion recognition: The fine print and the benchmark. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. R-tuning: Instructing large language models to say dont know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 71067132, 2024a. Qixuan Zhang, Zhifeng Wang, Dylan Zhang, Wenjia Niu, Sabrina Caldwell, Tom Gedeon, Yang Liu, and Zhenyue Qin. Visual prompting in llms for enhancing emotion recognition. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 44844499, 2024b. Sicheng Zhao, Yue Gao, Xiaolei Jiang, Hongxun Yao, Tat-Seng Chua, and Xiaoshuai Sun. Exploring principles-of-art features for image emotion recognition. In Proceedings of the 22nd ACM international conference on Multimedia, pp. 4756, 2014. Sicheng Zhao, Hongxun Yao, Yue Gao, Rongrong Ji, Wenlong Xie, Xiaolei Jiang, and Tat-Seng Chua. Predicting personalized emotion perceptions of social images. In Proceedings of the 24th ACM International Conference on Multimedia, pp. 13851394, 2016. 17 Pre-print. Sicheng Zhao, Xingxu Yao, Jufeng Yang, Guoli Jia, Guiguang Ding, Tat-Seng Chua, Bjoern Schuller, and Kurt Keutzer. Affective image content analysis: Two decades review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):67296751, 2021. Sicheng Zhao, Xiaopeng Hong, Jufeng Yang, Yanyan Zhao, and Guiguang Ding. Toward labelefficient emotion and sentiment analysis. Proceedings of the IEEE, 111(10):11591197, 2023."
        }
    ],
    "affiliations": [
        "IIE, Chinese Academy of Sciences",
        "Nankai University",
        "University of Chinese Academy of Sciences"
    ]
}