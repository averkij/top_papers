{
    "paper_title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy",
    "authors": [
        "Teng Hu",
        "Zhentao Yu",
        "Guozhen Zhang",
        "Zihan Su",
        "Zhengguang Zhou",
        "Youliang Zhang",
        "Yuan Zhou",
        "Qinglin Lu",
        "Ran Yi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 2 9 7 5 1 2 . 1 1 5 2 : r Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy Teng Hu1** Zhentao Yu2* Guozhen Zhang2 Zihan Su1 Zhengguang Zhou2 Youliang Zhang2 Yuan Zhou2 Qinglin Lu2 Ran Yi1 1Shanghai Jiao Tong University 2Tencent Hunyuan Project page: https://sjtuplayer.github.io/projects/Harmony Figure 1. Harmony employs cross-task synergy training strategy to achieve robust audio-visual synchronization. This versatile framework supports multiple generation paradigms, including joint audio-video synthesis as well as audio-driven and video-driven generation, while also demonstrating strong generalizability to diverse audio types (e.g., music) and visual styles."
        },
        {
            "title": "Abstract",
            "content": "The synthesis of synchronized audio-visual content is key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome * Equal Contribution. Corresponding author. these challenges, we introduce Harmony, novel framework that mechanistically enforces audio-visual synchronization. We first propose Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization. 1. Introduction The unified synthesis of audio and video represents pivotal frontier in contemporary generative AI, with profound implications for content creation, digital avatars, and immersive virtual worlds. Industry-leading proprietary models, such as Veo 3 [8] and Sora 2 [37], have set high benchmark, delivering outputs with remarkable fidelity and demonstrating substantial practical utility. However, significant gap persists between these closed-source systems and the capabilities of existing open-source methods [5, 38, 43]. fundamental challenge, in particular, remains largely unsolved in the open-source community: achieving precise and harmonious audio-visual alignment. While recent open-source models have made strides in generation quality, they often struggle with robust audiovisual synchronization. Recent explorations into end-to-end joint audio-video generation [14, 26, 34, 35, 42, 50] underscore this limitation. Specifically, these methods often exhibit specialized limitations: many are confined to generating ambient sounds and fail to synthesize natural human speech [14, 26, 34, 42]; while others, such as JAMFlow [30], focus solely on speech generation but lack the capability to generate environmental sounds. Even among more general models, Ovi [35] exhibits deficiencies in robust alignment, while UniVerse-1 [50] suffers from poor audio-video synchronization. Table 1 presents capability comparison. These shortcomings reveal critical gap in current research: few methods investigate the root causes of audio-video misalignment from methodological standpoint. Consequently, there remains lack of highly generalizable and well-aligned audio-video joint generation methods. This leaves significant void in the open-source landscape for unified framework capable of generating comprehensive audio spectrumfrom ambient sounds to human speechwhile maintaining precise audio-visual harmony. In this work, we posit that the difficulty in achieving robust synchronization stems from three fundamental chal- (1) Durlenges inherent to the joint diffusion process. ing joint generation, both modalities are progressively denoised from pure noise. In the early, highly stochastic stages, attempting to align two concurrently evolving, highly noisy latents causes phenomenon we term Correspondence Drift, where the optimal mapping continuously shifts, impeding stable learning. (2) Audio-visual synchronization presents fundamental architectural tension between two competing objectives: precise, frame-level temporal alignment (e.g., lip movements) and holistic, global style consistency (e.g., emotional tone). Existing designs, often relying on single, monolithic mechanism like global cross-attention, conflate these distinct goals, forcing the model into suboptimal trade-off where neither objective (3) Conventional Classifier-Free Guidis fully achieved. ance (CFG) [16] operates by amplifying conditioning sigTable 1. Comparison of capabilities among existing joint audiovideo generation models. We evaluate their ability to generate different sound types and the quality of their temporal alignment with the video. (: Good, : Fair/Limited, : Poor/Unsupported) Model MM-Diffusion [42] JavisDiT [34] AnimateSI [52] JAM-Flow [30] UniVerse-1 [50] Ovi [35] Harmony (Ours) Human Speech Environmental Sound Speech-Video Alignment Sound-Video Alignment nals for each modality in isolation. Consequently, it does not inherently promote or enhance the crucial cross-modal correspondence between the generated audio and video. To overcome these challenges, we propose Harmony, novel joint audio-video generation framework designed to generate highly synchronized audio-video content with cross-task synergy. Harmonys design is centered on three core innovations, each targeting one of the aforementioned challenges. To mitigate Correspondence Drift, we employ Cross-Task Synergy training paradigm, co-training the joint generation task with auxiliary audio-driven video and video-driven audio generation tasks to leverage the latters strong supervisory signal for instilling robust alignment priors. To resolve the conflation of local and global synchronization objectives, we further propose Global-Local Decoupled Interaction Module that ensures both holistic style consistency and precise temporal synchronization through the decoupled global style attention and localized, RoPE-aligned frame-wise attention. Finally, to address the fact that conventional CFG lacks mechanism for enhancing audio-visual alignment, we propose SynchronizationEnhanced CFG (SyncCFG). This novel technique redefines the negative condition learned from the cross-task training stage to explicitly isolate and amplify the guidance vector corresponding to audio-visual alignment. newly proposed Harmony-Bench validate our framework on the challenging task of jointly generating human speech and ambient sounds. Harmony achieves the best audio-video alignment, maintaining fine-grained temporal synchronization in complex acoustic scenes, which confirms the efficacy of our approach. The main contributions of our work are summarized as follows: experiments"
        },
        {
            "title": "Extensive",
            "content": "our on We propose Harmony, novel joint audio-video generation framework built upon the principle of CrossTask Synergy to resolve the fundamental Correspondence Drift problem in joint diffusion models. We design Global-Local Decoupled Interaction Module that achieves comprehensive alignment in both overall style and fine-grained temporal details. We propose novel Synchronization-Enhanced CFG (SyncCFG) that guides the model towards better audiovisual correspondence during inference by using muteaudio and static-video condition as negative guidance. We establish new state-of-the-art in audio-visual generation, with extensive experiments validating Harmonys superior performance in cross-modal synchronization. 2. Related Work 2.1. Video Generation The field of video generation has rapidly advanced [13, 21, 29, 49, 55], transitioning from early Generative Adversarial Networks (GANs) [11] to the now-dominant diffusion models [17]. Building on their success in image synthesis [23, 39, 41], models like AnimateDiff [13] and SVD [1] extended diffusion to the temporal domain. Architectures have also evolved from UNets to more powerful Diffusion Transformers (DiT) [33, 56], with recent open-source models like HunyuanVideo [29] and Wan [49] achieving state-of-the-art visual quality, which inspired lot of downstream video generation methods, like video customization [19, 20], video editing [4, 32], and camera control [18, 22, 53]. However, critical limitation persists across this body of work: singular focus on the visual modality. By generating silent videos, these models produce content that feels incomplete and lacks the immersive quality of real-world experiences, underscoring the need for cohesive audio-visual synthesis. 2.2. Joint Audio-Video Generation Recently, growing body of research has begun to explore the simultaneous generation of audio and video within single, unified framework [34, 35, 42, 50, 51, 54, 57]. However, most of the early open-source approaches in this domain were restricted to synthesizing coarse environmental sounds and were unable to generate meaningful human speech [26, 34, 42]. This limitation began to be addressed by subsequent models like JAM-Flow [30] and UniAVGen [57], which started to incorporate speech-video joint generation. significant advancement came with models like UniVerse-1 [50] and Ovi [35], which integrated more powerful audio synthesis components to enable the joint generation of both ambient sounds and human vocals. Despite this progress, key challenge remains in the detailed alignment of the overall soundscape. These models often struggle to cohesively blend human speech with its surrounding environmental audio in manner that is acoustically and semantically consistent with the visual context, highlighting remaining gap in creating truly immersive audio-visual experiences. 3. Method In this section, we introduce Harmony, novel framework for joint audio-video synthesis designed to overcome the fundamental challenge of cross-modal alignment in diffusion models, capable of joint audio-video generation on speech, sound effects, and ambient audio. We introduce three core innovative designs: (1) Cross-Task Synergy training strategy that combines the standard joint audiovideo generation task with auxiliary audio-driven video and video-driven audio generation tasks, leveraging the strong, uni-directional supervisory signals from both to accelerate and stabilize the learning of audio-video alignment (2) Global-Local Decoupled Interaction Module that efficiently ensures both fine-grained temporal correspondence and holistic stylistic consistency; and (3) CrossTask Alignment-Enhanced CFG mechanism that repurposes guidance by designing more meaningful negative anchors, to explicitly amplify audio-video synchronization during inference. 3.1. Preliminary: Joint Audio-Video Diffusion Joint audio-video synthesis typically employs dual-stream Latent Diffusion Model. After encoding video and audio into latents (zv, za), denoising network ϵθ is trained to reverse standard Gaussian noising process. The network consists of parallel video and audio backbones that process their respective noisy latents, zv,t and za,t. Synchronization is learned through an interaction module (e.g., cross-attention) that couples the two streams. The model is optimized by minimizing the noise prediction error for both modalities: = ϵv ˆϵv(zv,t, za,t, t)2 + ϵa ˆϵa(za,t, zv,t, t)2. (1) However, this standard approach struggles to learn robust alignment from two concurrently noisy signalsa core challenge our work addresses. 3.2. Cross-Task Synergy for Enhanced Alignment 3.2.1. The Challenge of Correspondence Drift Problem Formulation. While recent advancements in joint audio-video generation have focused on novel architectures, fundamental challenge persists: achieving robust crossmodal alignment. We identify the root cause not as an architectural limitation, but as an inherent instability in the training paradigm, phenomenon we term Correspondence Drift. During the initial stages of joint training, both audio and video signals are heavily diffused with noise. Attempting to learn correspondence between two concurrently evolving, highly stochastic latent variables results in an unstable and inefficient learning target, causing the alignment process to drift and converge slowly. Empirical Motivation. To empirically validate this hypothesis, we present comparative analysis in Fig. 3. We Figure 2. (a) Mitigating Correspondence Drift with Cross-Task Synergy. Our training paradigm leverages supervised audioand video-driven task to provide strong alignment signal. This instills robust synchronization features in the model, stabilizing the otherwise stochastic joint generation process. (b) Overview of the Harmony Model. The architecture features parallel branches for multimodal inputs. The video stream is conditioned on reference image and descriptive prompt. The audio stream is conditioned on reference audio, an ambient sound description, and speech transcript. The model then generates single, synchronized audio-visual result. leveraging the high-quality, noise-free learning signal from the uni-directional supervisory task, our model efficiently learns intricate audio-video correspondences. This prelearned alignment knowledge then acts as powerful catalyst, accelerating the convergence and enhancing the final alignment quality of the primary joint generation task. Dual-Branch Model Architecture. Our model features dual-branch architecture for video and audio generation. The video branch adapts the pre-trained Wan2.25B model [49]. To ensure structural parity, we design symmetric audio generation branch that synthesizes an audio clip conditioned on speech transcript Ts (phonetic content), descriptive caption Ta (describing the acoustic scene, e.g., vocal emotion or ambient sounds), and reference audio Ar (timbre). We process these inputs with multi-encoder setup: an audio VAE [5] encodes and Ar into latents za and zr. Crucially, departing from prior works [34, 35], we use separate text encoders to preserve phonetic precision: dedicated speech-encoder [3] for the transcript Ts ( espeech) and T5 encoder [6] for the descriptive prompt Ta ( eprompt). During denoising, the reference latent zr is prepended to the noisy target latent za,t, forming composite input latent a,t. This composite latent, along with the speech and prompt embeddings, is then processed by Multi-Modal Diffusion Transformer (MMDiT) to predict the noise: ˆϵa = MM-DiT(concat(z a,t, espeech, eprompt), ta). (2) To facilitate effective cross-modal interaction between the two branches, we instantiate bidirectional global-local deFigure 3. Comparison of the audio-video alignment score among different training strategies. compare the lip-sync alignment scores of an audio-driven video generation task versus joint audio-video generation task, utilizing an identical network architecture (detailed in Sec. 3.3). The results reveal that the audio-driven model, conditioned on clean audio signal, rapidly converges to high alignment score; in contrast, the joint generation model exhibits markedly slower convergence. This disparity strongly indicates that anchoring one modality with deterministic, noise-free signal, as implemented in the audiodriven model, provides stable learning gradient, enabling the cross-modal interaction module to efficiently capture alignment cues. 3.2.2. Cross-Task Synergy Overview of Cross-Task Synergy. Based on this insight, we propose novel training framework, Cross-Task Synergy, which synergistically combines the standard joint audio-video generation task (the primary task) with audiodriven video and video-driven audio generation tasks. By coupled interaction module at each layer, with further details provided in Sec. 3.3. Cross-Task Synergy Training. We design hybrid training strategy that realizes the principle of Cross-Task Synergy. By concurrently training on both the joint generation task and and two deterministic, single-modality-driven tasks (audio-driven video gen and video-driven audio gen), we provide the model with stable alignment signal to counteract Correspondence Drift. The audio-driven task conditions video generation on the clean audio latent by setting the audio timestep ta to 0. Symmetrically, the video-driven task conditions audio generation on the clean video latent by setting the video timestep tv to 0. The total training objective is weighted sum of the three corresponding losses: = Ljoint + λvLaudio driven + λaLvideo driven, (3) where λv and λa are balancing hyperparameters, and represents the set of auxiliary conditions (e.g., text prompts and speech embeddings). The loss components are defined as: Ljoint =ϵv ˆϵv(zv,t, za,t, c, t)2 + ϵa ˆϵa(za,t, zv,tc, t)2,"
        },
        {
            "title": "Laudio\nLvideo",
            "content": "driven =ϵv ˆϵv(zv,t, za,0, c, t)2, driven =ϵa ˆϵa(za,t, zv,0, c, t)2. (4) This bidirectional, synergistic training approach enables our model to achieve faster convergence and superior degree of final audio-video alignment. 3.3. Global-Local Decoupled Interaction Module tension between two objectives: Robust audio-video synchronization presents fundamen- (1) precise, finetal grained temporal alignment and (2) holistic, global style consistency (e.g., emotional tone, ambient features). Prior works [34, 35] often attempt to address both with single, monolithic mechanism like global cross-attention, which conflates these goals and leads to suboptimal trade-off. To resolve this, we propose novel Global-Local Decoupled Interaction Module with two specialized components: (1) RoPE-Aligned Frame-wise Attention module for precise local synchronization, and (2) Global Style Alignment module for holistic consistency. This decoupled design allows each component to excel at its specific task, resolving the conflict between fine-grained temporal alignment and global style propagation. 3.3.1. RoPE-Aligned Frame-wise Attention To achieve precise temporal synchronization, we employ local frame-wise attention strategy, which is more computationally efficient and better suited for fine-grained alignment than global cross-attention. However, key challenge arises from the mismatched sampling rates of video and audio latents (Tv = Ta). This discrepancy means specific event in one modality can occur at timepoint that falls between two discrete frames in the other. standard attention mechanism, forced to operate on discrete set of keys, must attend to the nearest but temporally imperfect frame. This forced approximation introduces temporal jitter and fundamentally degrades fine-grained synchronization. Temporal Alignment via RoPE Scaling. To resolve this mismatch, we introduce an alignment step prior to the attention operation [5]. Our key insight is to unify the temporal coordinate spaces of both modalities by dynamically scaling their Rotary Positional Embeddings (RoPE) [45]. Before attention operation, we rescale the positional indices of the source modality to match the timeline of the target. For instance, in Audio-to-Video (A2V) attention, an audio frame at index is mapped to virtual position = (Tv/Ta) for its RoPE calculation. This ensures their positional encodings are directly comparable, establishing strong inductive bias for correct temporal correspondence. Frame-wise Cross-Attention Mechanism. With the latents now temporally aligned in the RoPE space, we apply symmetric, bidirectional cross-attention mechanism. Each frames attention is confined to small, relevant temporal window in the other modality. Taking A2V as an example, given video latent zv and an audio latent za, we first reshape zv to expose its temporal dimension (z v). For each video frame i, we construct local context window Ca,i from adjacent audio frames. Cross-attention is then applied independently for each video frame, attending to its corresponding audio context window: Qv,i = v[:, i, :, :]W v[:, i, :, :] = Cross-Attn(Qv,i, Ka,i, Va,i), [0, Tv-1], a,i, Va,i = Ca,iW v,i, Ka,i = Ca,iW a,i. (5) The Video-to-Audio (V2A) frame-wise alignment operates analogously. The updates are then integrated via residual connections: zupdated = zv + v, zupdated = za + za. (6) This RoPE-aligned frame-wise mechanism efficiently enleveraging the temporal synchronization, forces mutual benefits of local attention while correctly handling disparate timescales. 3.3.2. Global Style Alignment While frame-wise attention excels at establishing finegrained temporal correspondence, its localized nature inherently limits the propagation of holistic stylistic attributes, such as the overall emotional tone or ambient characteristics, which require global context to be consistently maintained. Prior methods often rely on single, monolithic global attention mechanism, which conflates the distinct novel scheme that repurposes the guidance mechanism to specifically target and enforce audio-video synchronization. Our approach leverages the dual capabilitiesjoint generation and audio&video-driven synthesisacquired during our cross-task training to enhance the alignment signal. 3.4.1. Analysis of Standard Guidance Limitations The standard CFG formulation in previous work [35] is used to strengthen the conditioning on text prompt c: (cid:16) (cid:17) ϵ = ˆϵθ(zv,t, za,t, c) + ˆϵθ(zv,t, za,t, c) ˆϵθ(zv,t, za,t, c) . (8) Here, the guidance pushes the denoising process away from an unconditional (null-text c) prediction and towards one that aligns with the text prompt c. The key limitation of standard CFG is that its guidance is oriented solely towards text-adherence. The guidance vector, computed by contrasting the text-conditioned output with text-unconditioned one, exclusively strengthens how well the output matches the prompt. This process, however, is agnostic to the internal consistency between audio and video. It provides no mechanism to isolate or amplify the crucial synchronization signal between the two streams. 3.4.2. SyncCFG Formulation for Video Guidance To explicitly compute an alignment-enhancing direction, we aim at isolating the visual dynamics caused by the audio. Our key insight is to design more meaningful Negative Anchor that represents static baselinehow the video should look in the absence of sound. For instance, for person speaking, the correct video for silent audio track would be still face with closed mouth. We achieve this by creating silent audio negative anchor. We leverage the audio-driven pathway of our model to predict the noise for the video latent zv,t conditioned on muted audio input, znull a,0 . The resulting predic- (zv,t, znull tion, ˆϵdriven a,0 ), represents the models expectation for this visually static scene. The guided prediction for the video noise ϵv is then formulated as: θ ϵv =ˆϵdriven θ (cid:16) (zv,t, znull a,0 )+ sv ˆϵjoint θ (zv,t, za,t) ˆϵdriven θ (zv,t, znull a,0 ) (cid:17) . (9) The subtraction term isolates the precise visual modificationssuch as mouth movements or object impactsthat are directly correlated with the audio. By amplifying this difference, SyncCFG specifically enhances the synchronization between sound and motion. 3.4.3. SyncCFG Formulation for Audio Guidance Similarly, for audio guidance, we design null video-based negative anchor to isolate motion-driven sounds. Using static video latent znull v,0 , we predict baseline audio sig- (znull nal ˆϵdriven v,0 , za,t) that represents the ambient sound of θ Figure 4. SyncCFG employs the mute audio and static video as the negative anchors to capture the synchronization feature, which can effectively enhance the audio-video alignment. tasks of temporal alignment and global style consistency, overburdening the module. To address this, we propose principled decoupling: our RoPE-Aligned Frame-wise Attention is exclusively responsible for precise temporal correspondence, while dedicated Global Style Alignment module handles holistic consistency. This separation allows each component to specialize, preventing interference between the two objectives. Our core insight for global alignment is to leverage the reference audio latent, zr (which provides speaker identity and timbre), as compact carrier for style information. Instead of directly modifying the target audio za and disrupting its fine-grained denoising, we modulate zr with the global context from the entire video latent zv. This is achieved by treating zr as the query and zv as the key and value within residual cross-attention block: zupdated Qr = zrW = zr + Cross-Attn(Qr, Kv, Vv), , Vv = zvW . , Kv = zvW (7) The resulting visually-informed reference audio latent zupdated is then prepended to the noisy audio latent za,t (as described in Sec. 3.2.2), allowing the audio generation to condition on visually-grounded global style. This decoupled design offers key advantage: by confining the global style injection to the reference latent, it prevents interference between holistic style consistency and precise framewise temporal alignment, preserving the stability and fidelity of the final audio generation. 3.4. Synchronization-Enhanced CFG While Classifier-Free Guidance (CFG) is powerful technique for conditional generation, its standard application in audio-video synthesis fails to explicitly amplify the crucial correspondence between modalities. To address this, we introduce Synchronization-Enhanced (SyncCFG), Figure 5. Qualitative Comparison between Harmony and the state-of-the-art methods, including Universe-1 [50] and Ovi [35]. motionless scene, as the video content is static. The guided prediction for audio noise ϵa is then formulated as: ϵa =ˆϵdriven θ (cid:16) (znull v,0 , za,t)+ sa ˆϵjoint θ (zv,t, za,t) ˆϵdriven θ (znull v,0 , za,t) (cid:17) (10) . This approach transforms CFG from generic conditional amplifier into targeted mechanism, effectively enforcing fine-grained audio-video correspondence. 4. Experiments 4.1. Experimental Settings Datasets and Training. Our model is trained on diverse corpus of over 4 million audio-visual clips, covering both human speech and environmental sounds. The data is aggregated from public sources like OpenHumanVid [31], AudioCaps [28], and WavCaps [36], and supplemented with our own curated high-quality collections. All data is uniformly annotated using Gemini [12]. Our training follows three-stage curriculum: (1) foundational audio pre-training on all audio data, (2) timbre disentanglement finetuning using multi-utterance speech data, and (3) final cross-task joint audio-visual training. The video branch is initialized from Wan2.2 [49]. The final joint stage is trained for 10,000 iterations with batch size of 128 and learning rate of 1e-5. comprehensive list of datasets, data processing details, and full training hyperparameters are provided in the supplementary material. Harmony-Bench and Metrics. To facilitate rigorous evaluation, we introduce Harmony-Bench, new benchmark of 150 test cases designed to assess core audio-visual generation capabilities. It is structured into three 50-item subsets with increasing complexity: 1. Ambient Sound-Video: Evaluates temporal alignment of non-speech sounds using AI-generated scenarios conditioned on audio and video captions. 2. Speech-Video: Assesses lip-sync and speech quality on mix of real-world and synthetic multilingual data, conditioned primarily on transcript. 3. Complex Scene (Ambient + Speech): Tests the models ability to generate and synchronize co-occurring speech and ambient sounds in complex scenes, using full set of multimodal prompts. We evaluate performance using comprehensive suite of automated metrics. For Video, we measure Aesthetic Quality (aesthetic-predictor-v2-5), Imaging Quality (MUSIQ), Dynamic Degree (RAFT), Motion Smoothness and Identity Consistency (DINOv3). For Audio, we report AudioBoxAesthetics (PQ, PC, CE, CU), WER (Whisper-large-v3), and IB-A Score. For AV-Sync, we use lip-sync metrics (Sync-C, Sync-D), and overall consistency (IB-score). Further details on the benchmark construction and metric implementations are available in the appendix. 4.2. Comparison on audio-video generation To evaluate the performance of our model, we compare it with state-of-the-art audio-video generation methods on the three types of datasets (Ambient Sound-Video, SpeechVideo, and Complex Scene), including MM-Diffusion [42], JavisDiT [34], UniVerse-1 [50], and Ovi [35]. The quantitative results are shown in Tab. 2. Our model, Harmony, demonstrates highly competitive performance, achieving state-of-the-art or comparable results in both video quality (e.g., AQ, DD, ID) and audio fidelity (e.g., PC, PQ). Most notably, its primary advantage lies in audio-visual synchronization. Harmony significantly outperforms all baselines on key synchronization metrics, achieving the highest SyncTable 2. Quantitative comparison with state-of-the-art methods averaging on ambient sound-video, speech-video, and complex scenevideo generation. We evaluate performance across three categories: video quality, audio fidelity, and audio-visual synchronization. Best results are in bold, second-best are underlined. For more comprehensive and detailed evaluations, please refer to the supplementary material. Some metrics could not be generated for MM-Diffusion, as it is an unconditional generation model. Method Video Quality & Coherence Audio Fidelity & Quality Audio-Visual Synchronization AQ IQ DD MS ID PQ PC CE CU WER IB-A Sync-C Sync-D DeSync IB MM-Diffusion [42] JavisDiT [34] UniVerse-1 [50] Ovi [35] Harmony (Ours) 0.32 0.34 0.52 0.57 0.59 0.43 0.53 0.67 0.65 0. 0.13 0.38 0.24 0.34 0.36 0.99 0.99 0.99 0.99 0.99 - 0.38 0.89 0.90 0.91 5.37 5.46 5.52 6.19 6.39 4.07 2.24 2.13 2.13 2.05 4.27 3.19 3.63 4.44 4. 5.89 4.54 4.84 5.84 5.67 - 1.00 0.24 0.49 0.15 - 0.14 0.07 0.12 0.12 - 0.89 0.97 4.04 5.61 - 11.62 10.71 9.62 7.53 - 1.13 1.10 1.14 0. 0.12 0.18 0.12 0.18 0.19 This capability extends to natural sounds, where the model accurately identifies the primary sound source (e.g., an animal) while also attending to ambient environmental sounds, such as the rain in the cat example and the birdsong in the crocodile case. Collectively, these visualizations underscore our models superior ability to achieve fine-grained and contextually aware audio-visual alignment. 4.4. Ablation Studies We conduct comprehensive ablation study to validate our core components, with the results presented in Tab. 3. In this study, we train all ablated models on the human speech dataset and evaluate their audio-visual alignment. Our baseline model replaces the proposed Global-Local Decoupled Interaction (GLDI) module with standard global crossattention mechanism, similar to Ovi [35], and is trained without cross-task synergy (CTS). As shown in the table, progressively integrating our contributions yields consistent improvements. First, introducing the GLDI module demonstrates the benefit of decoupling local and global interactions. This is further enhanced by the RoPE Alignment, which effectively resolves timescale mismatches and boosts fine-grained synchronization (Sync-C from 4.29 to 4.80). Subsequently, the Cross-Task Synergy (CTS) training strategy further refines the models alignment capabilities. Finally, applying the Synchronization-Enhanced CFG (SyncCFG) during inference provides the most substantial performance gain, catapulting Sync-C from 5.09 to 6.51. This systematic improvement validates that each component of Harmony is crucial for achieving the state-of-the-art audiovideo synchronization performance. Figure 6. Visualization of the audio-to-video frame-wise crossattention map, where the audio can accurately capture the sound source from the videos. score of 5.61 and the lowest (best) Sync-D score of 7.53. This substantial improvement in temporal alignment directly validates the effectiveness of our proposed cross-task synergy mechanism in enhancing cross-modal coherence. We provide qualitative comparisons with UniVerse-1 and In the talking head example (left), both Ovi in Fig. 5. competing methods fail to produce synchronized lip movements. For the music-driven case (right), their limitations persist: UniVerse-1 generates irrelevant noise, while Ovi produces audio that, while musically correct, is less dynamica fact reflected in its simpler waveform. Visually, both methods yield videos with minimal motion. In contrast, our Harmony generates fluid video of person playing the mandolin with motions that are dynamically synchronized with the rich, corresponding music, as evidenced by the more complex audio waveform. 4.3. Visualization of Cross-modal Attention 5. Conclusion To validate the effectiveness of our frame-wise crossattention mechanism, we visualize the attention maps from the audio-to-video module. As illustrated in Fig. 6, when synthesizing human speech, the model precisely localizes its attention on the speakers oral region. Notably, in scenarios with multiple individuals, our model can distinguish between them, focusing exclusively on the active speaker. In this work, we presented Harmony, novel framework addressing the audio-visual synchronization gap in generative models. We find this gap stems from key methodological flaws: Correspondence Drift, an architectural conflict between global style and local timing, and the limitations of standard CFG for cross-modal alignment. To address these issues, Harmony introduces three core components. Table 3. Ablation study on the core components of Harmony. We start with baseline and progressively add each module: Global-local decoupled interaction module (GLDI), RoPE Alignment (RoPE), Cross-Task Synergy (CTS) , and SynchronizationEnhanced CFG (SyncCFG). Note that this experiment is evaluated on the human-speech dataset; therefore, it has different synchronization results compared to Tab. 2."
        },
        {
            "title": "CTS",
            "content": "SyncCFG Sync-C Sync-D IB 4.20 4.29 4.80 5.09 6.51 10.93 10.67 10.30 10.16 8.63 0.13 0.14 0.14 0.15 0.18 Cross-Task Synergy training instills robust alignment priors to counteract drift. Global-Local Decoupled Module resolves the architectural conflict by handling style and timing separately. Finally, our novel Synchronization-Enhanced CFG (SyncCFG) provides an explicit mechanism to amplify the alignment signal during inference. Our experiments validate that Harmony establishes new state-of-theart in audio-video synchronization, proving more effective than simply scaling up models. We believe this work provides strong foundation for new generation of accessible and well-aligned audio-visual models."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported by Tencent Hunyuan. We thank our team lead, Yuan Zhou, and Qinglin Lu, for their guidance. We also thank Zhentao Yu, Guozheng Zhang, Zhengguang Zhou, Youliang Zhang, Yi Chen, Zixiang Zhou, and Sen Liang for their help with data and technique support."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [2] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. 12 [3] Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, and Xie Chen. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. arXiv preprint arXiv:2410.06885, 2024. 4 [4] Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin Ivebench: Modern benchmark Hu, and Shuicheng Yan. suite for instruction-guided video editing assessment. arXiv preprint arXiv:2510.11647, 2025. 3 [5] Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. Mmaudio: Taming multimodal joint training for high-quality video-toaudio synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2890128911, 2025. 2, 4, 5, 16 [6] Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. [7] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian conference on computer vision, pages 251263. Springer, 2016. 14 [8] Google DeepMind. Veo3. https : / / deepmind . google/models/veo/, 2025. 2 [9] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP 20202020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 736740. IEEE, 2020. [10] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1518015190, 2023. 14 [11] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and In NeurIPS, Yoshua Bengio. Generative adversarial nets. 2014. 3 [12] Google. Gemini. https://gemini.google.com/, 2025. 7, 12 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toIn ICLR, image diffusion models without specific tuning. 2024. 3 [14] Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, and Sergey Tulyakov. Av-link: Temporally-aligned diffusion features for cross-modal audio-video generation. arXiv preprint arXiv:2412.15191, 2024. 2 [15] Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, et al. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 885890. IEEE, 2024. [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 3 [18] Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma. Motionmaster: Training-free camera motion transfer for video generation. arXiv preprint arXiv:2404.15789, 2024. 3 [19] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. 3 [20] Teng Hu, Zhentao Yu, Zhengguang Zhou, Jiangning Zhang, Yuan Zhou, Qinglin Lu, and Ran Yi. Polyvivid: Vivid multisubject video generation with cross-modal interaction and enhancement. arXiv preprint arXiv:2506.07848, 2025. 3 [21] Teng Hu, Jiangning Zhang, Zihan Su, and Ran Yi. Ultragen: High-resolution video generation with hierarchical attention. arXiv preprint arXiv:2510.18775, 2025. 3 [22] Teng Hu, Jiangning Zhang, Ran Yi, Hongrui Huang, Yabiao Wang, and Lizhuang Ma. High-efficient diffusion model fine-tuning with progressive sparse low-rank adaptation. In 13th International Conference on Learning Representations, ICLR 2025, pages 9206692078. International Conference on Learning Representations, ICLR, 2025. [23] Teng Hu, Jiangning Zhang, Ran Yi, Jieyu Weng, Yabiao Wang, Xianfang Zeng, Zhucun Xue, and Lizhuang Ma. Improving autoregressive visual generation with clusteroriented token prediction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 9351 9360, 2025. 3 [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 13 [25] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 53255329. IEEE, 2024. 14 [26] Masato Ishii, Akio Hayakawa, Takashi Shibuya, and Yuki Mitsufuji. simple but strong baseline for sounding video generation: Effective adaptation of audio and video arXiv preprint diffusion models for joint generation. arXiv:2409.17550, 2024. 2, 3 [27] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 13 [28] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, 2019. 7, 12 [29] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [30] Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, and Youngjung Uh. Jam-flow: Joint audio-motion synthesis with flow matching. arXiv preprint arXiv:2506.23552, 2025. 2, 3 [31] Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77527762, 2025. 7, 12 [32] Sen Liang, Zhentao Yu, Zhengguang Zhou, Teng Hu, Hongmei Wang, Yi Chen, Qin Lin, Yuan Zhou, Xin Li, Qinglin Lu, et al. Omniv2v: Versatile video generation and editarXiv preprint ing via dynamic content manipulation. arXiv:2506.01801, 2025. 3 [33] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 3 [34] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, et al. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025. 2, 3, 4, 5, 7, 8, 12, 13, 14, 15 [35] Chetwin Low, Weimin Wang, and Calder Katyal. Ovi: Twin backbone cross-modal fusion for audio-video generation. arXiv preprint arXiv:2510.01284, 2025. 2, 3, 4, 5, 6, 7, 8, 13, 14, [36] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: chatgpt-assisted weaklylabelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:33393354, 2024. 7, 12 [37] Openai. Sora2. https://openai.com/zhHansCN/index/sora-2/, 2025. 2 [38] Ziqiao Peng, Jiwen Liu, Haoxian Zhang, Xiaoqiang Liu, Songlin Tang, Pengfei Wan, Di Zhang, Hongyan Liu, and Jun He. Omnisync: Towards universal lip synchronization via diffusion transformers. arXiv preprint arXiv:2505.21448, 2025. 2 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 3 [40] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning, pages 2849228518. PMLR, 2023. 14 [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3 [42] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. 2, 3, 7, 8, 13, 14, 15 [43] Sizhe Shan, Qiulin Li, Yutao Cui, Miles Yang, Yuehai Wang, Qun Yang, Jin Zhou, and Zhao Zhong. Hunyuanvideofoley: Multimodal diffusion with representation alignment [56] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [57] Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, and Limin Wang. Uniavgen: Unified audio and video generation with asymmetric cross-modal interactions. arXiv preprint arXiv:2511.03334, 2025. 3 [58] Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, and Xiu Li. Speakervid-5m: large-scale high-quality dataset for audiovisual dyadic interactive human generation. arXiv preprint arXiv:2507.09862, 2025. 12 for high-fidelity foley audio generation. arXiv:2508.16930, 2025. 2 arXiv preprint [44] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. [45] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [46] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. 13 [47] Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. 13 [48] Aesthetic Predictor V2.5. Aesthetic predictor v2.5. https://github.com/discus0434/aestheticpredictor-v2-5, 2024. 13 [49] Ang Wang, Baole Ai, and et al Bin Wen. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 4, 7, [50] Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. 2, 3, 7, 8, 12, 13, 14, 15 [51] Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. Av-dit: Taming image diffusion transformers for efficient joint audio and video generation. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 1048610495, 2025. 3 [52] Xihua Wang, Ruihua Song, Chongxuan Li, Xin Cheng, Boyuan Li, Yihan Wu, Yuyue Wang, Hongteng Xu, and Yunfeng Wang. Animate and sound an image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2336923378, 2025. 2 [53] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [54] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visualaudio generation with diffusion latent aligners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71517161, 2024. 3 [55] Zhucun Xue, Jiangning Zhang, Teng Hu, Haoyang He, Yinan Chen, Yuxuan Cai, Yabiao Wang, Chengjie Wang, Yong Liu, Xiangtai Li, et al. Ultravideo: High-quality uhd video dataset with comprehensive captions. arXiv preprint arXiv:2506.13691, 2025. A. Overview In this supplementary material, we provide more implementation details, experiment results, including: Implementation details (Sec. B); Benchmark settings (Sec. C); More quantitative comparisons (Sec. D); More qualitative comparisons (Sec. E); Details about voice clone (Sec. F); Audio-driven performance; (Sec. G); More qualitative results (Sec. H); We also provide demo video and project page in https://sjtuplayer.github.io/projects/Harmony, where the demo video shows the powerful and comprehensive ability of our model in audio-video generation and the project page provides comparison between the existing methods. B. Implementation Details Datasets. Our training corpus is curated from diverse range of public and newly collected sources to cover both human speech and environmental sounds. 1) Human Speech Data: We aggregate vocal data from multiple open-source datasets, including the TTS-specific Emilia dataset [15], as well as audio-visual corpora such as OpenHumanVid [31] and SpeakerVid [58]. To ensure high-quality alignment, we employed an audio-visual consistency scoring model to filter this collection, resulting in high-quality subset of 2 million video clips, each 3-10 seconds in duration. We then utilized the Gemini [12] for automated annotation, generating ASR transcripts, descriptive video captions, and captions for any background sounds present in the clips. 2) Environmental Sound Data: For environmental sounds, we leverage several established public datasets, including AudioCaps [28] (128 hours, manually captioned), Clotho [9] (31 hours, manually captioned), and WavCaps [36] (7,600 hours, automatically captioned). Recognizing the often-suboptimal visual quality of the VGGSound dataset [2], we supplemented our data by collecting an additional 2 million audio-visual clips rich in environmental sounds. These new clips were subsequently annotated using Gemini [12] to generate corresponding audio and video captions. Training Strategy. Our training protocol is structured in three distinct stages to ensure stable convergence and highfidelity generation. For the video branch, we initialize our model with the pre-trained weights of Wan2.2-5B [49]. The audio model undergoes dedicated two-stage pre-training process before the final joint training. Stage 1: Foundational Audio Pre-training. The audio model is first pre-trained on balanced 1:1 mixture of our human speech and environmental sound datasets. We train for 100,000 iterations with global batch size of 1536, using clips with maximum duration of 10 seconds. During this stage, the reference audio is randomly selected 1-3 second segment from the ground-truth clip. This phase enables the model to learn to replicate both the timbre and content from the provided reference audio. Stage 2: Timbre Disentanglement Finetuning. To enable the model to disentangle general acoustic characteristics from specific content, we finetune it using mismatched reference and target content. For human speech, we use cross-utterance data from the same speaker. For environmental sounds, we sample non-overlapping reference clip from the same long recording as the ground-truth target. This setup compels the model to extract the invariant acoustic signaturebe it speakers voice or an environmental ambiencefrom the reference and apply it to the new content dictated by the prompt or transcript. We finetune for an additional 20,000 iterations in this configuration. Stage 3: Cross-Task Audio-Visual Training. Finally, we proceed to the Cross-Task joint training stage. The full audio-visual model is trained for 10,000 iterations with batch size of 128, again using 1:1 mixture of human speech and environmental sound data. Across all training stages, we employ constant learning rate of 1e-5 for all model parameters. Hyperparameters. During the final cross-task training stage, the balancing weights for our synergistic loss (Eq. 3) are set to λv = 0.1 and λa = 0.3. The model is trained using Flow Matching objective with shift of 5. For inference, we use 40 integration steps with classifier-free guidance (CFG) scales of sv = 3 for video and sa = 2 for audio. The samplers shift parameter is also maintained at 5. C. Benchmark Settings C.1. The Harmony-Bench Dataset Existing benchmarks for audio-visual generation are inadequate for comprehensive evaluation. JavisBench [34] lacks evaluation for human speech, while Verse-Bench [50] is hampered by low-quality labels and limited focus on audio-visual synchronization. To enable more rigorous and holistic assessment, we construct and introduce Harmony-Bench. This new benchmark features 150 meticulously designed test cases, organized into three progressively challenging subsets (50 items each). It is specifically crafted to disentangle and systematically evaluate models semantic consistency and temporal synchronization across diverse and complex acoustic scenarios. Ambient Sound-Video Generation. This subset is designed to assess the models ability to generate nonspeech acoustic events that are precisely synchronized with corresponding visual dynamics. The 50 test cases feature synthetically constructed scenarios, enabling the creation of complex audio-visual interactions that are difTable 4. Human-speech set comparison with state-of-the-art joint audio-visual generation models. We evaluate performance across three categories: video quality, audio fidelity, and audio-visual synchronization. Best results are in bold, second-best are underlined. Method Video Quality & Coherence Audio Fidelity & Quality Audio-Visual Synchronization AQ IQ DD MS ID PQ PC CE CU WER IB-A Sync-C Sync-D DeSync IB MM-Diffusion [42] JavisDiT [34] UniVerse-1 [50] Ovi [35] Harmony (Ours) 0.32 0.30 0.47 0.48 0.48 0.43 0.54 0.67 0.65 0.63 0.13 0.28 0.15 0.17 0.20 0.99 0.99 0.99 0.99 1.00 - 0.35 0.89 0.88 0.93 5.37 5.34 4.28 6.19 6. 4.07 2.16 1.86 1.59 1.57 4.27 3.61 3.84 5.41 5.30 5.89 3.92 3.91 6.21 5.93 - 1.00 0.23 0.19 0.15 - 0.14 0.16 0.10 0.15 - 1.20 1.22 5.13 6. - 12.73 13.10 10.38 8.63 - - - - - 0.12 0.22 0.16 0.17 0.18 Table 5. Environment set comparison with state-of-the-art joint audio-visual generation models. We evaluate performance across three categories: video quality, audio fidelity, and audio-visual synchronization. Best results are in bold, second-best are underlined. Method Video Quality & Coherence Audio Fidelity & Quality Audio-Visual Synchronization AQ IQ DD MS ID PQ PC CE CU WER IB-A Sync-C Sync-D DeSync IB MM-Diffusion [42] JavisDiT [34] UniVerse-1 [50] Ovi [35] Harmony (Ours) 0.32 0.37 0.57 0.62 0.64 0.43 0.55 0.68 0.66 0.65 0.13 0.33 0.16 0.44 0.56 0.99 0.99 1.00 0.99 0.98 - 0.45 0.92 0.93 0. 5.37 5.64 6.14 6.45 6.53 4.07 2.29 2.30 2.46 2.68 4.27 3.06 3.20 3.78 4.12 5.89 5.14 5.46 5.98 6.22 - - - - - - 0.18 0.04 0.20 0. - - - - - - - - - - - 0.94 1.10 1.06 0.70 0.12 0.16 0.07 0.20 0.21 ficult to capture or isolate in real-world recordings. The model is conditioned on detailed audio caption and separate video caption. Evaluation centers on audio fidelity, temporal synchrony, and the semantic consistency between the generated audio and visual events. Speech-Video Generation. This 50-item subset assesses the fidelity of speech synthesis and lip synchronization. To test for robustness and multilingual generalization, it includes balanced mix of 25 real-world and 25 AIsynthesized samples, driven by transcripts in both English (spoken word en) and Chinese (spoken word zh). The video caption is deliberately kept minimal (e.g., man is speaking), compelling the model to derive lip movements and facial expressions directly from the transcripts content. Key evaluation criteria are speech intelligibility, naturalness, and the precision of lip-audio synchronization. Complex Scene: Ambient + Speech. Representing the most challenging scenario, this subset evaluates the models capacity to simultaneously generate and synchronize both speech and ambient sounds within unified, complex scene. Each of the 50 test cases is constructed to feature co-occurring audio-visual events, requiring the model to process combination of inputs: transcript (spoken word en), an ambient sound description (audio caption), and visual scene description (video caption). The evaluation critically examines the models ability for sound source separation and mixing (e.g., maintaining speech clarity over background door-closing sound). Furthermore, it assesses multi-modal temporal alignment: speech must synchronize with lip movements, while ambient sounds must align with their corresponding visual actions. To provide comprehensive evaluation on this benchmark, we adopt suite of automated metrics designed to assess three key aspects: 1) Visual Quality and Coherence, 2) Audio Fidelity, and 3) Audio-Visual Synchronization and Consistency. C.2. Evaluation Metrics To comprehensively assess model performance on Harmony-Bench, we employ suite of automated metrics targeting three core aspects of audio-visual quality. Visual Quality and Coherence. We evaluate the visual quality and temporal consistency of the generated videos using the following metrics: Aesthetic and Imaging Quality. We assess aesthetic quality (AQ) and imaging quality (IQ) using the pre-trained aesthetic-predictor-v2-5[48] and MUSIQ[27] models, respectively. Motion Dynamics. Temporal coherence is evaluated through Dynamic Degree (DD) and Motion Smoothness (MS)[24]. We employ RAFT[46] to quantify the magnitude of motion and pre-trained video frame interpolation model to evaluate motion smoothness. Identity Consistency (ID). For subject-specific generation, we measure ID by computing the mean DINOv3[44] feature similarity between reference image and all generated frames. Audio Fidelity and Quality. The quality of the generated audio is measured by: AudioBox-Aesthetics.[47] We employ this model to evaluate perceptual quality across four dimensions: Production Quality (PQ), Production Complexity (PC), Table 6. Complex set comparison with state-of-the-art joint audio-visual generation models. We evaluate performance across three categories: video quality, audio fidelity, and audio-visual synchronization. Best results are in bold, second-best are underlined. Method Video Quality & Coherence Audio Fidelity & Quality Audio-Visual Synchronization AQ IQ DD MS ID PQ PC CE CU WER IB-A Sync-C Sync-D DeSync IB MM-Diffusion [42] JavisDit [34] UniVerse-1 [50] Ovi [35] Harmony (Ours) 0.32 0.34 0.52 0.60 0. 0.43 0.50 0.65 0.63 0.66 0.13 0.54 0.42 0.41 0.32 0.99 0.98 0.99 0.99 1.00 - 0.33 0.85 0.88 0.91 5.37 5.40 6.14 5.94 6.43 4.07 2.26 2.23 2.33 1. 4.27 2.91 3.85 4.14 4.76 5.89 4.56 5.15 5.33 4.86 - 1.00 0.25 0.79 0.15 - 0.09 0.00 0.06 0.06 - 0.58 0.72 2.94 4.70 - 10.50 8.32 8.86 6. - 1.32 1.09 1.21 1.13 0.12 0.17 0.14 0.18 0.18 Figure 7. More comparison on human-speech video generation. Table 7. Chinese speech comparison with state-of-the-art models, focusing on audio fidelity (WER) and audio-visual synchronization. Best results are in bold, second-best are underlined."
        },
        {
            "title": "Method",
            "content": "WER Sync-C Sync-D IB JavisDiT [34] UniVerse-1 [50] Ovi [35] Harmony (Ours) 4.84 2.32 9.10 0. 1.27 0.91 4.45 5.05 12.63 11.02 10.79 9.38 0.20 0.22 0.20 0.22 Content Enjoyment (CE), and Content Usefulness (CU). Word Error Rate (WER). For speech synthesis, accuracy is measured by WER. We transcribe the generated audio using Whisper-large-v3[40] and compare it against the ground-truth transcript. IB-A Score. Semantic alignment between the generated audio and the text prompt is quantified using the IB-A Score[10]. Audio-Visual Synchronization. The critical capability of joint generation is assessed through synchronization metrics: Sync-C & Sync-D. Lip-sync accuracy is explicitly measured using these two established metrics[7]. DeSync Score. Predicted by Synchformer[25], this score quantifies the temporal misalignment (in seconds) between the audio and video streams. ImageBind (IB) Score. Following [10], we use the IB score to assess overall audio-visual consistency by computing the cosine similarity between their respective feature embeddings. D. More Quantitative Comparisons In this section, we present detailed quantitative comparisons against state-of-the-art methods for joint audio-video generation, including Ovi [35], UniVerse-1 [50], JavisDiT [34], and MM-Diffusion [42]. Our evaluation spans multiple challenging test sets, with results for environmental sounds and complex audio scenes presented in Tables 46. Across these diverse datasets, our model consistently demonstrates superior performance. key observation is our models superior video dynamism compared to competitors. For inFigure 8. More comparison on environment-sound video generation. stance, while UniVerse-1 and Ovi sometimes achieves favorable Identity Distance (ID) score, this is often consequence of generating static or nearly static videos, where frame-to-frame identity is trivially high but fails to capture the scenes intended motion. Crucially, our method consistently achieves the lowest Word Error Rate (WER) and the best scores on audio-visual synchronization metrics. This combination of high fidelity, strong dynamism, and precise alignment underscores our models robustness in generating coherent and realistic content for complex scenes. Furthermore, we specifically assess the cross-lingual capabilities of the models on dedicated Chinese speech test set, with key results summarized in Table 7. The results highlight significant performance gap. Our model achieves substantially lower WER and markedly better synchronization scores. It is worth noting that the standard WER metric is not perfectly optimized for the tokenization of the Chinese language; therefore, the relative performance between models serves as the most meaningful indicator. The pronounced improvement in both WER and synchronization metrics strongly validates the effectiveness and superiority of our approach for cross-lingual audio-visual speech generation. E. More Qualitative Comparisons In this section, we present further qualitative comparisons of our method against state-of-the-art approaches: Ovi [35], UniVerse-1 [50], and JavisDiT [34]. We focus on two challenging scenarios: synchronized human speech and dynamic environmental sounds. We exclude MMDiffusion [42] from this analysis as it is designed for unconditional generation and is therefore not directly comparable. Comparisons on Human Speech. As illustrated in Figure 7, our model demonstrates superior performance in audio-visual speech generation. Competing methods like Ovi and UniVerse-1 tend to produce static or minimally dynamic video frames, resulting in talking head effect with little natural movement. In contrast, our model generates high-fidelity video with fluid, naturalistic motion. The accompanying audio is clear and, most importantly, precisely synchronized with the lip movements, resulting in significantly more coherent and believable output. Comparisons on Environmental Sounds. We further evaluate performance on generating dynamic environmental sounds in Figure 8, where the shortcomings of other methods are even more pronounced. JavisDiT struggles in this domain, producing low-quality video and unstable audio; for instance, in the gunfire example, its generated audio waveform is highly irregular and fails to represent the acoustic event convincingly. UniVerse-1 and Ovi frequently generate static or partially static scenes. clear example is the ocean waves case, where the main waves remain frozen while only the water surface shows minimal movement. This lack of dynamism is compounded by poor audio-visual synchronization, where the sound of crashing waves does not align with the visual content. In stark contrast, our method excels in all aspects: it generates highquality, dynamic videos with realistic motion, and the synthesized audio is both high-fidelity and precisely synchronized with the visual events, delivering cohesive and immersive audio-visual experience. Figure 9. Visualization of the voice-clone results of our model. F. Details about Voice Clone In this section, we provide additional details on the voice cloning capability of our model, which is achieved through the use of reference audio input, Ar. The mechanism begins by processing short reference audio clip (typically 1-3 seconds) containing the desired voice timbre with our pre-trained audio VAE encoder [5]. This yields compact latent representation, zr, which effectively captures the unique, time-invariant characteristics of the speakers voice while discarding the original phonetic content. As described in our main methodology, this reference latent zr is then prepended to the noisy target audio latent za,t during each step of the denoising process. By conditioning the MM-DiT on this fixed reference latent, the model is guided to synthesize new speechbased on the phonetic content from the transcript Tsin the desired target voice. To qualitatively validate the effectiveness of this approach, we provide examples in Figure 9. The figure demonstrates that our model can successfully clone variety of distinct voice timbres onto newly generated speech Importantly, this high-fidelity voice cloning is content. achieved without degrading the visual quality of the generated video. The lip movements remain precisely synchronized with the cloned audio, and the overall facial expressions and video coherence are maintained at high level. This highlights the models ability to disentangle audio timbre from other generation aspects, enabling robust voice cloning within coherent audio-visual output. G. Audio-Driven Performance As detailed in our main paper, our Cross-Task Synergy Training strategy is fundamental to the models performance. key component of this strategy is the inclusion of deterministic, audio-driven video generation task, represented by the loss term Laudio driven. During training, this task explicitly requires the video branch to generate video conditioned on the clean, non-noisy audio latent za,0 (i.e., the audio latent at timestep ta = 0). By directly optimizing for this objective, our model is inherently equipped with the ability to perform high-fidelity audio-driven video synthesis at inference time, making it native capability rather than an emergent one. To demonstrate the effectiveness of this native capability, Figure 10. Visualization of the audio-driven results of our model. we present qualitative results for audio-driven video generation in Figure 10. The figure showcases examples where video is generated solely from target speech audio clip. The results exhibit high visual quality, characterized by natural facial expressions and coherent head movements. More importantly, the lip movements are precisely and accurately synchronized with the nuances of the input speech, validating the strong audio-visual alignment instilled by our training approach. This confirms that our Cross-Task Synergy strategy not only enhances joint generation but also directly enables high-fidelity, single-modality-driven applications. H. More qualitative results To further demonstrate the capabilities and robustness of our model, we present additional qualitative results organized into three key areas: generating high-quality human speech videos, rendering diverse artistic styles, and synthesizing complex ambient sounds. More results on human speech. First, we showcase additional results on generating human speech videos in Figure 11. These examples highlight the models ability to produce highly realistic talking heads with natural facial expressions and coherent movements. The synthesized speech is characterized by its clarity and natural prosody, capturing range of vocal tones. Crucially, we maintain precise lip synchronization across all examples, which is fundamental for creating believable human speech. These results reinforce our models core capability in generating high-quality, well-synchronized audio-visual speech content across various identities. Diverse visual styles. Beyond photorealism, key strength of our model is its capacity to generate video content across wide spectrum of artistic styles. As illustrated in Figure 12, our model can produce outputs in distinct aesthetics such as Disney-style animation and traditional ink wash painting. These stylized generations maintain high visual quality, characterized by sharp details, vibrant colors, and temporally coherent motion consistent with the target aesthetic. This demonstrates the models flexibility in capturing and rendering complex artistic attributes. Diverse Ambient Sounds. Our model demonstrates remarkable capability to generate wide spectrum of ambient sounds, extending beyond simple environmental noise. As illustrated in Figure 13, it can produce diverse and complex acoustic eventsfrom the sharp, percussive bursts of fireworks to the structured harmonies of music. Crucially, each sound is rendered with high fidelity and meticulously synchronized with its corresponding visual source. This ability to construct rich, thematically consistent auditory environments validates our models strength in enhancing the overall visual narrative. Collectively, these examples validate our models comprehensive generation capabilities. From producing highly synchronized human speech to rendering diverse artistic styles and creating rich, context-aware ambient soundscapes, our model demonstrates remarkable versatility. The ability to master these distinct yet complementary domains underscores its potential for creating highly expressive and immersive audio-visual content, pushing the boundaries beyond conventional generation methods. Figure 11. More results on human-speech video generation. Figure 12. Visualization of speech-video generation in diverse style. Figure 13. More results on ambient-sound video generation."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Tencent Hunyuan"
    ]
}