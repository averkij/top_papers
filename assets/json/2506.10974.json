{
    "paper_title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
    "authors": [
        "Yixin Ou",
        "Yujie Luo",
        "Jingsheng Zheng",
        "Lanning Wei",
        "Shuofei Qiao",
        "Jintian Zhang",
        "Da Zheng",
        "Huajun Chen",
        "Ningyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 4 7 9 0 1 . 6 0 5 2 : r AUTOMIND: Adaptive Knowledgeable Agent for Automated Data Science Yixin Ou*, Yujie Luo*, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang , Da Zheng, Huajun Chen, Ningyu Zhang Zhejiang University Ant Group Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph zhengda.zheng@antgroup.com {ouyixin,zhangningyu}@zju.edu.cn https://github.com/innovatingAI/AutoMind"
        },
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents have shown great potential in solving real-world data science problems. LLMdriven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AUTOMIND, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AUTOMIND delivers superior performance versus state-ofthe-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AUTOMIND as an efficient and robust step toward fully automated data science."
        },
        {
            "title": "Introduction",
            "content": "Data science agents aim to leverage LLM agents to automate data-centric machine learning tasks that begin with task comprehension, data exploration and analysis, advance through feature engineering, and culminate in model selection, training, and evaluation (Sun et al., 2024; Zheng et al., 2025a; Liu et al., 2025), serving as critical component for future AI agents to achieve autonomous scientific discovery. Many data science-related benchmarks * Equal Contributions. Corresponding Author. 1 (Huang et al., 2024a; Jing et al., 2024; Chan et al., 2025; Chen et al., 2024) have been proposed to provide structured tasks rooted in real-world data science challenges to assess performance across the entire problem-solving pipeline. Because of the great complexity of these tasks, most existing data science agent frameworks rely on pre-defined workflows and optimize on top of the specific workflow through search and refinement (Jiang et al., 2025; Hong et al., 2024a; Trirat et al., 2024) or extend to multi-agent framework to better stimulate the performance of each workflow node (Li et al., 2024b). However, current data science agents all overlook the fundamental limitations in model capabilities: despite being trained on massive code-based corpus, the agents inherently lack the rich empirical expertise accumulated by human practitioners in data science tasks (Zheng et al., 2025a). Moreover, existing data science agents largely employ an inflexible coding strategy, and tend to implement code only for relatively simple and classic tasks in practice (Guo et al., 2024; Li et al., 2024b). Yet the diversity and complexity of real-world problems require dynamic, context-aware coding strategy. Indeed, addressing truly complex or even cuttingedge tasks that require high levels of creativity and innovation, poses significant challenges for data science agents in generating high-quality code appropriately tailored to such complex tasks. To tackle these issues, we propose AUTOMIND, an adaptive knowledgeable LLM agent framework designed for automated data science challenges. As illustrated in Figure 1, AUTOMIND introduces three major innovations: an expert knowledge base for data science, an agentic knowledge tree search algorithm, and self-adaptive coding strategy. We evaluate AUTOMIND on two automated data science benchmarks with two different families of foundation models. Experimental results show that AUTOMIND achieves superior performance on Figure 1: The framework of our AUTOMIND. both two of the benchmarks compared to baselines. Specifically, on the official MLE-Bench leaderboard, AUTOMIND surpasses 56.8% of human participants, repesenting an improvement of 13.5% over the prior state-of-the-art (SOTA), AIDE. Moreover, we conduct in-depth analysis to examine the effectiveness and efficiency of AUTOMIND and find that AUTOMIND delivers 300% increase in efficiency and reduces token costs by 63% compared to prior SOTA."
        },
        {
            "title": "2 Preliminaries",
            "content": "Building on recent successes in integrating tree search strategies with workflows of LLM agents (Jiang et al., 2025; Chi et al., 2024; Yamada et al., 2025), we model LLM agent-driven automated data science as an optimization problem, and apply tree search algorithm to solve it. Formally, we denote possible soluton for data science task as tuple = (p, σ, η), where denotes textual plan outlining the proposed approach, σ is the corresponding Python code snippet, and η is the validation metric used to assess the execution results. Let be the space of all possible solutions, and the objective is to find the optimal solution: = arg max η sS where η = max{η (p, σ, η) S} (1) Unlike general agents (Yao et al., 2023; Hong et al., 2024b; Shinn et al., 2023; Chen et al., 2023), which conceptualize problem solving as longhorizon, sequential decision process aimed at maximizing cumulative reward through action choices based on all prior observations, our modeling approach significantly simplifies the objective by directly evaluating and comparing possible solutions for data science tasks."
        },
        {
            "title": "3 AUTOMIND",
            "content": "In this section, we introduce our AUTOMIND, an adaptive knowledgeable LLM agent framework designed for automated data science challenges. As illustrated in Figure 1, AUTOMIND introduces three major innovations: an expert knowledge base for data science (3.1), an agentic knowledge tree search algorithm (3.2), and self-adaptive coding strategy (3.3). First, the agents specialized knowledge retriever extracts multiple relevant papers and tricks from the expert knowledge base. Next, the agentic knowledgeable tree search module initiates an iterative loop in which it selects parent node according to the search policy, executes an action that synthesizes task information with retrieved knowledge to produce new solution, and integrates the resulting node into the solution tree. Concurrently, the self-adaptive coding strategy is invoked during the code implementation stage of each action, reconciling solution complexity with the inherent coding capabilities of LLMs. Once the iteration limit is reached or the time budget is exhausted, the best node in the solution tree identified by Equation (1) is selected, submitted, and evaluated as the final solution. The following sections delve into key implementation details of each component. 3.1 Expert Knowledge for Data Science The data science competitions are challenging due to the requirements of high-quality experience in 2 designing effective solutions (Chan et al., 2025; Trirat et al., 2024). Using LLMs alone to solve these competitions is challenging due to their reliance on static, pre-trained knowledge, which may lack domain-specific or up-to-date insights. To address this challenge, we construct knowledge base to effectively solve these problems. It is built based on domain-specific resources, including papers from top-tier conferences and journals, as well as expert-curated insights from top-ranked competition solutions. 3.1.1 Knowledge Base Construction Following the human expertise, we collect the topranked solution tricks from different competitions which expect to fully unleash the power of the existing method, as well as the papers to provide more expressive strategies when solving data science tasks. In machine learning tasks, minor yet effective tricks can significantly enhance model performance. To incorporate such human insights in our framework, we collect the discussions of topranked solutions. To be more specific, we identify all Kaggle competitions with publicly shared solutions1, and then archive both competition descriptions and the content of associated technical forum posts. After filtering out invalid competitions and posts, 455 Kaggle competitions with valid solutions have been collected, including 3,237 public forum posts containing top solutions. Then, different labels describing the competition-oriented technical categories are attached to each solution post for subsequent retrieval. Besides, the papers accepted after peer-review are high-quality knowledge in solving different machine learning tasks. To utilize such knowledge, we first collect papers published in top-tier conferences in the recent three years, like KDD, ICLR, NeurIPS, ICML, EMNLP, and domain-specific journal like Bioinformatics. For each paper, the meta information (including title, author, abstract, and keywords) and main content are preserved, from which we obtain the prepared paper knowledge. 3.1.2 Knowledge Retrieval Directly retrieving relevant knowledge using only task descriptions is challenging due to the weak correlation between real-world task descriptions and 1We collect all Kaggle competitions with solutions using the list from https://github.com/faridrashidi/ kaggle-solutions. the available technical approaches. Consequently, traditional retrieval methods relying solely on task description embeddings prove ineffective in our context. To address this limitation, we propose labeling system to facilitate knowledge retrieval, filtering, and re-ranking. We construct hierarchical labels to precisly describe the collection of tricks. To be more specific, we first construct hierarchy label set based on all collected ML tasks with the assistance of LLMs, and it contains 11 top-level categories and corresponding subcategories (e.g., category Computer Vision and subcategory Image Classification). Then, to label each trick, AUTOMIND first selects the most relevant top-level categories, then identifies the most appropriate labels from the corresponding subcategories. We ensure labeling quality through self-consistency strategy with five independent iterations and voting-based arbitration, on which we obtain precise labels for each trick. Compared with collected tricks for competitions, papers are much more diverse in data and techniques, bringing difficulties in designing hierarchical labels. Then, we use LLMs to generate brief summary for each paper from the perspective of data (including type, domain, and dataset name), ML tasks, the proposed techniques and key contributions. In this way, papers can be retrieved to solve the different competitions from different perspectives. In the retrieval stage, the input task is analyzed with the same labeling tricks. For each label, AUTOMIND performs similarity search in the knowledge base to retrieve associated knowledge. Then, after filtering out solutions or tricks of the same target task to avoid plagiarism, the retrieved results are re-ranked based on the aforementioned label priority order, on which we obtain the final retrieved knowledge. 3.2 Agentic Knowledgeable Tree Search Solution Node (N ) To facilitate the exploration of possible solutions, we model the search space as solution tree in which each node corresponds to distinct solution = (p, σ, η), annotated as either valid or buggy. Each solution node stores the following attribute information: Plan p: An end-to-end textual solution plan typically comprises sequential stages including data pre-processing, feature engineering, model training, and inference. Code σ: Python code snippet that imple3 Algorithm 1 Search Policy π in AUTOMIND Require: Require: Ninit Require: Hdebug Require: Hgreedy Ensure: (N , A) Current state of solution tree Hyper-parameter of the number of initial draft nodes Hyper-parameter of the probalitity of whether debug node Hyper-parameter of the probability of whether select the best node Select one parent node and specify the next action 1: Ndraft Get the number of draft nodes in 2: if Ndraft < Ninit then return (Nempty, Adraft) 3: end if 4: pdebug Get random floating-point number from 0 to 1 5: Nbuggy Get random buggy node in the solution tree 6: if pdebug < Hdebug and Nbuggy is not None then return (Nbuggy, Adebug) 7: end if 8: pgreedy Get random floating-point number from 0 to 1 9: Ngreedy Get the best node in the solution tree 10: Nnon-greedy Get random valid node in the solution tree 11: if pgreedy < Hgreedy and Ngreedy is not None then 12: 13: else if pgreedy Hnon-greedy and Ngreedy is not None then 14: 15: end if 16: return (Nempty, Adraft) return (Nnon-greedy, Aimprove) return (Ngreedy, Aimprove) Draft new solution Debug buggy solution Improve valid solution Mitigate getting trapped in local optima No solution to debug or improve, draft new solution ments the outlined solution plan. Metric η: The task-specific validation score extracted from the code execution output; if execution fails, no metric is recorded and the node is flagged as buggy. Output o: The terminal output generated during code execution, which can be used as feedback signal. Summary γ: brief report produced by an LLM-based verifier that, given the plan p, code σ, metric η, and output o, assesses each solution node and labels it as valid or buggy. Search Policy (π) At the start of each iteration of AUTOMIND, the search policy π receives the current state of the solution tree , selects one node as the parent, and specifies the action for the current iteration. The algorithm for search policy is driven by probabilistic hyperparameters and rule-based heuristics, and is outlined in Algorithm 1. Action (A) The action space of AUTOMIND consists of three distinct operations: Drafting Adraft, Improving Aimprove, and Debugging Adebug. As illustrated in Figure 1, each action goes through similar pipelinecomprising plan generation, code implementation, and execution, and output verificationwhile varying primarily in the specific input provided for the plan generation stage. In the drafting action, the agent synthesizes the task description with relevant papers retrieved from the expert knowledge base to formulate an initial plan. In the improving action, the agent is provided with the parent solutionconsisting of plan p, code σ and output oas well as tricks retrieved from the expert knowledge base, and is instructed to improve the plan accordingly. In the debugging action, the agent receives only the buggy parent solution and is instructed to modify the plan to resolve the bug. Procedures for code implementation, execution, and output verification are uniformly applied across all action types. Once an action completes, the resulting solution is encapsulated as node within the solution tree , after which the next iteration begins. 3.3 Self-Adpative Coding Strategy To cope with the spectrum of data science workloads from straightforward machine learning models to multi-stage, state-of-the-art architectures, we introduce within AUTOMIND self-adaptive coding mechanism, reconciling solution complexity with the coding capabilities of LLMs, as illustrated in Figure 1. During the code implementation stage of an action, we employ LLM-as-a-judge based on professional rubrics for scoring the overall com4 plexity of both the task and the solution plan on five-point scale. When this score falls below preset thresholdindicating that the agent regards the plan as straightforwardthe agent implements the entire code for the plan in one pass to maximize efficiency. Conversely, for plans exceeding the threshold, the agent adopts stepwise strategy by decomposing the plan into sequential substeps and incorporating execution feedback at each substep. Specifically, for each substep, the agent performs an Abstract Syntax Tree (AST) check and then executes the corresponding code in terminal session. If the tests pass, the agent advances to the next substep; otherwise, the agent regenerates the substeps implementation using the error messages as feedback. This loop repeats until either the tests succeed for all substeps, after which the agent integrates the substeps code into complete implementation; or predefined retry limit is reached for any substep, forcing the agent to abandon the current plan."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup 4.1.1 Foundation Models and Baselines Foundation Models In our main experiment, we evaluate the agents by switching the foundation model. Specifically, we evaluate two different models: o3-mini2 of OpenAI and deepseek-v33 of DeepSeek. Baseline Agents Considering the substantial computational resources required by baseline reproduction, the quantative comparison in our main experiment is confined to AIDE (Jiang et al., 2025), which outperforms MLAB (Huang et al., 2024a) and OpenHands (Wang et al., 2025) and represents the prior state-of-the-art (SOTA) on the MLEBench (Chan et al., 2025). We provide necessary detailed hyperparameters for reproducing our results of different agents in Appendix B. 4.1.2 Benchmarks and Metrics MLE-Bench We select MLE-Bench (Chan et al., 2025), which consists of 75 offline Kaggle competitions for evaluating LLM agents, as part of our test benchmarks. We further apply rule-based filtering the tasks in MLE-Bench as detailed in Appendix A. Consequently, we obtain lite version of 2o3-mini-2025-01-31 3DeepSeek-V3-0324 MLE-Bench conssiting of 16 tasks4, which are split into Easy, Medium and Hard tiers based on human experience and the performance of prior SOTA. We include full list of tasks in MLE-Bench used for evaluation in Appendix A. Top AI Competions An inspection of the original MLE-Bench reveals that most tasks were curated before 2023, with several classical machine learning tasks dating to 2018 or earlier. Considering the fact that foundation models are likely seen corresponding tasks during pre-training, we supplement our evaluation with two task drawn from recent top AI competitions. Specifically, we include the WhoIsWho-IND track of the Open Academic Graph (OAG) Challenge at KDD Cup 2024 (Zhang et al., 2024), evaluated by the area under the ROC curve (AUC), and the BELKA Challenge at the NeurIPS 2024 Competition (Blevins et al., 2024), assessed by average precision (AP). More details are in the Appendix A. Evaluation Metrics For MLE-Bench, we evaluate the performance of AUTOMIND and prior SOTA by comparing their submissions with those of human participants on the official leaderboard of each Kaggle competition, and averaging across all designated splits. We report metric denoted Beats (%), defined as the proportion of human participants whose scores are surpassed by the LLM agent. For the two Top AI Competitions, we employ the organizers official task metrics, evaluating agents directly by their raw scores. 4.1.3 Runtime Environment Settings In our experiments, LLM agents are loaded into an Ubuntu 20.04 Docker container containing the dataset prepared for each task, and an Anaconda environment pre-installed with standard Python packages for machine learning (e.g., PyTorch, scikitlearn), thereby providing all requisite dependencies for code implementation and execution. The container runs on compute node with 48 vCPUs, 448GB RAM, 9.6TB SSD storage, and single NVIDIA GeForce RTX 3090 GPU, all of which are fully accessible to the agents. For each of the task across MLE-Bench and Top AI Competitions, LLM agents are allotted 24-hour wall-clock budget to make their final submission. We repeat all experiments with two runs per task to report the mean and standard error of the performance. 4For expository convenience, we still refer to this lite version as MLE-Bench. 5 Benchmarks Split Metric Prior SOTA o3-mini deepseek-v3 Easy Medium Hard All OAG BELKA Beats (%) Submissions (#) Beats (%) Submissions (#) Beats (%) Submissions (#) Beats (%) Submissions (#) AUC Submissions (#) AP Submissions (#) 52.5 10.8 93.7 0.2 20.3 3.0 103.5 44.3 10.7 1.2 102.1 86.8 30.0 0.2 99.5 38.2 0.56 0.03 16.0 15.6 0.09 0.06 5.0 4.2 74.0 2.9 90.7 33.0 29.0 6.4 72.3 40.8 18.4 1.8 36.0 7.1 43.3 3.0 70.1 25. 0.52 0.03 14.5 13.4 0.33 0.03 2.0 0.0 MLE-Bench Top AI Competitions AUTOMIND w/o Knowledge deepseek-v3 81.8 0.7 7.8 25.8 12.4 39.2 5.3 10.2 15.9 2.2 23.6 6.4 5.2 14.1 6.2 52.1 1.0 8.8 19.9 2.8 0.50 0.00 0.02 48.5 41.3 0.19 0.03 0.14 2.5 2.1 AUTOMIND o3-mini deepseek-v3 71.8 4.3 19.3 14.6 0.2 24.9 6.1 4.6 8.7 1.6 36.6 9.4 25.9 18.9 7.2 45.4 1.7 15.4 13.4 2.3 0.55 0.08 0.01 4.5 2.1 0.44 0.02 0.35 9.0 1.4 81.2 0.1 7.2 18.2 3.3 44.3 0.4 15.3 18.1 9.2 38.7 14.3 20.3 6.3 0.7 56.8 1.4 13.5 15.2 4.7 0.58 0.04 0.06 4.0 1.4 0.39 0.05 0.06 8.5 0.7 Table 1: Main results on MLE-Bench and Top AI Competitions. Beats (%) represents the percentage of human participants surpassed by the agent on the officail leaderboard and serves as the principal evaluation metric for MLE-Bench. Submissions (#) reports the number of valid submissions the agent makes within 24-hour time budget; it is provided solely as descriptive statistic and not as an evaluative metric, as its magnitude does not directly reflect the agents competence. AUC and AP are the official evaluation metrics for the OAG and BELKA competitions, respectively. Performance gains relative to the baseline are denoted by , whereas declines are . 4.2 Main Results As shown in Table 1, AUTOMIND achieves better performance in comparison with prior SOTA on both MLE-Bench and Top AI Competitions, despite making smaller number of valid submissions. We find that AUTOMIND (o3-mini) and AUTOMIND (deepseek-v3) outperform 45.4% and 56.8% of human participants on the official leaderboard of MLE-Bench respectively, representing performance gains of 15.4% and 13.5% over the prior SOTA (AIDE). Moreover, AUTOMIND exhibits remarkable superiority on the Hard split of MLE-Bench, achieving Beats (%) improvements of 25.9% with o3-mini and 20.3% with deepseek-v3 over the prior SOTA. Notably, even running AUTOMIND without the expert knowledge base still outperforms the prior SOTA by 8.8% on the Beats (%) metric. Across both the OAG and BELKA challenges in Top AI Competitions, AUTOMIND delivers performance that is at least on par with, and in most cases exceeds prior SOTA. Particularly, AUTOMIND (o3-mini) achieves an average precision of 0.44 on the BELKA challenge, representing 0.35 absolute improvement over the prior SOTA."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Ablation Study To validate the effectiveness of our design, we conduct ablation experiments for AUTOMIND (deepseek-v3) on the Medium split of MLEBench, separately disabling two principal components in AUTOMIND: expert knowledge base and self-adaptive coding strategy. Figure 2: Abaltion studies on deepseek-v3 for AUTOMIND on the Medium split of MLE-Bench. Beats represents the percentage of human participants surpassed by the agent on the official leaderboard. Valids represents the percentage of valid submissions among all solutions the agent makes within 24-hour time budget. Expert knowledge provides additional effective supervision for agentic tree search. When AUTOMIND is run without access to the expert knowledge base, the agent is forced to rely exclusively on its internal knowledge to draft and improve the solutions. The results in Figure 2 demonstrate that ablating the expert knowledge base leads to respective declines of 5.0% and 1.3% in the Beats (%) and Valids (%) metrics. Figure 3 presents hourly snapshots of the Beats (%) metric over 24-hour time budget, demonstrating that AUTOMIND equipped with the expert knowledge base Figure 3: Test time scaling results on MLE-Bench. We record hourly snapshots of the percentage of human participants surpassed by the agents best solution over 24-hour time budget in experiments with deepseek-v3. consistently outperforms the variant without it, even within the initial hours of search progress. We attribute these performance gains to the integration of expert knowledge, which imposes additional constraints on the agents solution search space. By leveraging human-validated knowledge, AUTOMIND reduces its reliance on limited internal knowledge of foundation LLMs and focuses exploration within more promising search space of possible solutions. Self-adaptive coding provides robust support for the implementation of more complex plans. We ablate the self-adaptive coding mechanism by completely replacing it with one-pass coding strategy during the code implementation stage of AUTOMIND. The results in Figure 2 demonstrate that, replacing the self-adaptive coding mechanism with one-pass strategy leads to respective declines of 24.6% and 19.0% in the Beats (%) and Valids (%) metrics, highlighting its significant limitations in addressing complex tasks and plans. We attribute this decline to the limited coding capacity of the foundation LLMs, which proves insufficient to tackle complex tasks and plans in one-pass generation. By applying stepwise decomposition of complex plans and integrating AST check with execution feedback, error accumulation in the early segments of code generated by the one-pass strategy can be minimized, thereby preserving the efficient execution of subsequent code segments. As for simpler tasks and plans, the self-adaptive coding strategy inherently permits the utilization of one-pass generation, thereby striking balance between efficiency and robustness in AUTOMIND. 5.2 Efficiency Analysis Test-Time Scaling To assess the efficiency of different agent frameworks, we investigate test-time scaling by tracking the performance of both AUTOMIND and the prior SOTA AIDE over 24-hour time budget in experiments with deepseek-v3. As shown in Figure 3, both agents are able to progressively improve their solutions as the available test-time increases. Notably, on MLE-Bench, AUTOMIND achieves the prior SOTAs 24-hour performance in just 6 hours on average, representing three-fold improvement in time efficiency. Moreover, even when run without the expert knowledge base, AUTOMIND nearly doubles the time efficiency, requiring only 13 hours to achieve the prior SOTAs 24-hour performance. Agents Input Output Total AIDE (24h) 2.27 0. 0.22 0.03 2.49 0.31 AUTOMIND (6h) w/o knowlege (13h) 0.86 0.07 2.06 0.47 0.11 0.01 0.26 0. 0.90 0.04 2.32 0.50 Table 2: Token costs across all MLE-Bench tasks. We present the input, output, and total token costs for experiments with deepseek-v3, each quantified in millions of tokens. Token Costs We quantifiy the cumulative token costs at the time by which each agent framework achieves the prior SOTAs 24-hour performance. As shown in Table 2, owing to the efficiency improvements, AUTOMIND achieves 63% reduction in token costs, and even the variant without the expert knowledge base realizes 7% reduction in token costs. 5.3 Case Study As shown in Figure 4, we provide case study on the Belka dataset to verify the effectiveness of the proposed AUTOMIND. Belka competition is provided to predict the binding affinity of small molecules (provided the SMILES of this molecule and three building blocks) to specific protein tarFigure 4: running case on the BELKA challenge. We compare the proposed solution plans and corresponding code implementations generated by both AIDE and AUTOMIND. gets (the amino acid sequenceis available). AUTOMIND first retrieves papers MolTrans (Huang et al., 2021) and DeepDTA (Öztürk et al., 2018) from the constructed knowledge base. Then, it designs frequent subsequence minging strategy and dualCNN blocks from these papers. The code script is generated and executed following the designed plans. On the contrary, AUTOMIND (w/o knowledge) focuses on extracting the statistical features of molecules and only adopts the simple MLPs to predict the binding probability. For AIDE, the final solution used the gradient boosting model, which is too simple to solve this complex task. Compared with AIDE and AUTOMIND (w/o knowledge), AUTOMIND could retrieve the potential papers and design more expressive model for complex tasks, the higher performance could demonstrate the effectiveness of the constructed knowledge base and retrieval strategy."
        },
        {
            "title": "6 Related Work",
            "content": "LLM Agents. LLMs, with excellent reasoning (Qiao et al., 2023; Sun et al., 2025; Chen et al., 2025) and planning (Huang et al., 2024b; Wei et al., 2025a) abilities, are becoming the central control components of AI agents (Wang et al., 2024; Xi et al., 2023; Durante et al., 2024) and have been increasingly applied in software engineering (Qian et al., 2024; Hong et al., 2024b; Yang et al., 2024; Wei et al., 2025b), deep research (Li et al., 2025; Zheng et al., 2025b; Wu et al., 2025), GUI manipulation (Wu et al., 2024; Lai et al., 2024; Gou et al., 2024; Hu et al., 2024), scientific discovery (Chen et al., 2024; Hong et al., 2024a; Trirat et al., 2024), embodied intelligence (Ahn et al., 2022; Singh et al., 2023; Song et al., 2023), etc. Most current LLM agent frameworks rely on two paradigms. One is the training-free general architecture that depends on the strong capabilities of foundation models and carefully customized workflows (Hong et al., 2024b; Qian et al., 2024; Trirat et al., 2024; Li et al., 2024b). The other involves fine-tuning models in specific fields. Previous studies mainly focused on imitation learning based on large amount of trajectory data (Chen et al., 2023; Zeng et al., 2023; Wu et al., 2024; Qiao et al., 2024). However, with the emergence of GRPO-like algorithms (Shao et al., 2024; Yu et al., 2025; Yue et al., 2025), models can now learn to complete target tasks through self-exploration with rule-based rewards (Song et al., 2025; Jin et al., 2025; Wei et al., 2025b; Lu et al., 2025; Feng et al., 2025). 8 LLM Agents for Data Science. Data science agents aim to leverage LLMs to automate datacentric machine learning tasks, including data analysis, data modeling, and data visualization, serving as critical component for future AI agents to achieve autonomous scientific discovery. Most existing approaches decompose data science tasks into distinct subtasks with clear boundaries based on human expertise, executing them as workflows within single or multiple agents (Zhang et al., 2023; Li et al., 2024a; Guo et al., 2024; Li et al., 2024b). Furthermore, Hong et al. (2024a); Jiang et al. (2025); Trirat et al. (2024); Chi et al. (2024) employ reflection and search-based optimization. However, these methods overlook fundamental limitations in model capabilities: despite being trained on massive code datasets, the models inherently lack the rich empirical expertise accumulated by human practitioners in data science. To integrate human expertise, DS-Agent (Guo et al., 2024) adopts knowledge-based approach by collecting expert Kaggle solutions and applying case-based reasoning to adapt these legacy solutions to new tasks. Additionally, the inherent complexity of data science task types necessitates diverse problem-solving strategies, whereas current solutions predominantly apply uniform approaches across all tasks. To address these gaps, this paper proposes to enhance agent capabilities by incorporating human expertise (from research papers, Kaggle competitions, etc.) as an expert knowledge base, while implementing dynamic coding strategy selection mechanisms to adapt to different task requirements."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce AUTOMIND, an adaptive, knowledgedriven LLM-agent framework tailored for automated data science. By integrating an expert knowledge base, tree-based reasoning agent, and selfadaptive coding strategy, AUTOMIND surpasses the previous state-of-the-art (AIDE) on MLE-Bench and two recent AI competitions."
        },
        {
            "title": "Limitations",
            "content": "Benchmarks and Baselines Due to limited computational resources, rather than evaluating the full set of 75 MLE-Bench (Chan et al., 2025) tasks, we select representative subset of 16 tasks, chosen to span the entire spectrum of difficulty levels and task categories for our experiments. Furthermore, since MLE-Bench provides only aggregate performance metrics without per-task raw scores for baseline agents, we have to replicate the evaluation process on our 16-task subset, incurring substantial computational overhead. Consequently, we only conduct experiments on AIDE (Jiang et al., 2025) as the baseline, which outperforms MLAB (Huang et al., 2024a) and OpenHands (Wang et al., 2025) and represents the prior state-of-the-art on the MLEBench. Coding Capability of Foundation Models AUTOMIND relies heavily on the foundation models coding capability. If the coding capability of foundation models is insufficient for implementing complex solutions with high potential, our approach may underperform compared to traditional data science agents."
        },
        {
            "title": "References",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, and 24 others. 2022. Do as can, not as say: Grounding language in robotic affordances. CoRR, abs/2204.01691. Andrew Blevins, Ian Quigley, Brayden Halverson, Nate Wilkinson, Rebecca Levin, Agastya Pulapaka, Walter Reade, and Addison Howard. 2024. Neurips 2024 - predict new medicines with belka. https:// kaggle.com/competitions/leash-BELKA. Kaggle. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Aleksander Madry, and Lilian Weng. 2025. Mle-bench: Evaluating machine learning agents on machine learning engineering. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023. Fireact: Toward language agent fine-tuning. CoRR, abs/2310.05915. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025. Towards reasoning era: survey of long chain-ofthought for reasoning large language models. CoRR, abs/2503.09567. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, 9 Xuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan Sun. 2024. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. CoRR, abs/2410.05080. Kexin Huang, Cao Xiao, Lucas Glass, and Jimeng Sun. 2021. Moltrans: molecular interaction transformer for drugtarget interaction prediction. Bioinformatics, 37(6):830836. Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, and Chenglin Wu. 2024. SELA: tree-search enhanced LLM agents for automated machine learning. CoRR, abs/2410.17238. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2024a. Mlagentbench: Evaluating language agents In Fortyon machine learning experimentation. first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Li Fei-Fei, and Jianfeng Gao. 2024. Agent AI: surveying the horizons of multimodal interaction. CoRR, abs/2401.03568. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. Retool: Reinforcement learning for strategic tool use in llms. Preprint, arXiv:2504.11536. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. 2024. Navigating the digital world as humans do: Universal visual grounding for GUI agents. CoRR, abs/2410.05243. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. 2024. Ds-agent: Automated data science by empowering large language models with case-based reasoning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, and 6 others. 2024a. Data interpreter: An LLM agent for data science. CoRR, abs/2402.18679. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2024b. Metagpt: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shawn Wang, Xinchen Xu, Shuofei Qiao, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, and 9 others. 2024. Os agents: survey on mllm-based agents for general computing devices https://github.com/OS-Agent-Survey/ use. OS-Agent-Survey/. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024b. Understanding the planning of LLM agents: survey. CoRR, abs/2402.02716. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. 2025. AIDE: ai-driven exploration in the space of code. CoRR, abs/2502.13138. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. Preprint, arXiv:2503.09516. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. 2024. Dsbench: How far are data science agents to becoming data science experts? CoRR, abs/2409.07703. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. 2024. Autowebglm: large language model-based web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024, pages 52955306. ACM. Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. 2024a. Mlr-copilot: Autonomous machine learning research based on large language models agents. CoRR, abs/2408.14033. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025. Webthinker: Empowering large reasoning models with deep research capability. Preprint, arXiv:2504.21776. Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, and Ge Zhang. 2024b. Autokaggle: multi-agent framework for autonomous data science competitions. CoRR, abs/2410.20424. Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, and Siheng Chen. 2025. Ml-agent: Reinforcing llm agents for autonomous arXiv preprint machine learning engineering. arXiv:2505.23723. 10 Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. 2025. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. Preprint, arXiv:2503.21620. Hakime Öztürk, Arzucan Özgür, and Elif Ozkirimli. 2018. Deepdta: deep drugtarget binding affinity prediction. Bioinformatics, 34(17):i821i829. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Chatdev: Communicative In Proceedings agents for software development. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1517415186. Association for Computational Linguistics. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with language model prompting: survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5368 5393. Association for Computational Linguistics. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024. Autoact: Automatic agent learning from scratch for QA via self-planning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 30033021. Association for Computational Linguistics. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt: Generating situated robot task plans using large language models. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 11523 11530. IEEE. agents with large language models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 2986 2997. IEEE. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. Preprint, arXiv:2503.05592. Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Yuan Wu, Qi Liu, and 15 others. 2025. survey of reasoning with foundation models: Concepts, methodologies, and outlook. ACM Comput. Surv. Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, and Jian Huang. 2024. survey on large language model-based agents for statistics and data science. CoRR, abs/2412.14222. Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. 2024. Automl-agent: multi-agent LLM framework for full-pipeline automl. CoRR, abs/2410.02958. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024. survey on large language model based autonomous agents. Frontiers Comput. Sci., 18(6):186345. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, and 2 others. 2025. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, and Fei Liu. 2025a. Plangenllms: modern survey of LLM planning capabilities. CoRR, abs/2502.11221. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. 2025b. SWE-RL: advancing LLM reasoning via reinforcement learning on open software evolution. CoRR, abs/2502.18449. Junde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025. Agentic reasoning: Reasoning llms with tools for the deep research. CoRR, abs/2502.04644. Chan Hee Song, Brian M. Sadler, Jiaman Wu, Wei-Lun Chao, Clayton Washington, and Yu Su. 2023. Llmplanner: Few-shot grounded planning for embodied Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. 11 2024. OS-ATLAS: foundation action model for generalist GUI agents. CoRR, abs/2410.23218. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, and 10 others. 2023. The rise and potential of large language model based agents: survey. CoRR, abs/2309.07864. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. 2025. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Preprint, arXiv:2504.08066. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable automated software engineering. CoRR, abs/2405.15793. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. Preprint, arXiv:2503.14476. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, and 8 others. 2025. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. Preprint, arXiv:2504.05118. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for llms. CoRR, abs/2310.12823. Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song, and 3 others. 2024. Oagbench: human-curated benchmark for academic graph mining. arXiv preprint arXiv:2402.15810. Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang. 2023. Data-copilot: Bridging billions of data and humans with autonomous workflow. CoRR, abs/2306.07209. Da Zheng, Lun Du, Junwei Su, Yuchen Tian, Yuqi Zhu, Jintian Zhang, Lanning Wei, Ningyu Zhang, and Huajun Chen. 2025a. Knowledge augmented complex problem solving with large language models: survey. Preprint, arXiv:2505.03418. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025b. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. Preprint, arXiv:2504.03160."
        },
        {
            "title": "A Benchmarks",
            "content": "A.1 MLE-Bench The original MLE-Bench (Chan et al., 2025) consists of 75 offline Kaggle competitions for evaluating LLM agents. We apply rule-based filtering the tasks in MLE-Bench. Specifically, we first exclude the tasks for which no valid submission can be made from the prior SOTA, thereby eliminating tasks that might be excessively difficult or ill-posed for LLM agents. From the remaining tasks, we then sample balanced subset, retaining at least one and no more than two tasks per task category (e.g., image classification, training LLMs). Consequently, we obtain lite version of MLE-Bench conssiting of 16 tasks, which are splited into Easy, Medium and Hard tiers based on huma experience and the performance of prior SOTA. We include full list of tasks in MLE-Bench used for evaluation in Table 3. A.2 Top AI Competitions BELKA dataset collects from kaggle competition5, which predict the the binding affinity of small molecules to specific protein targets. It provides 2.9B molecule-protein pairs of training data. In this paper, we sample 2.2M training rows and 590K test rows from full training data following the label distribution, and uses the AP score to evaluate the methods. OAG dataset used in this paper is collected from KDD cup Who-is-Who incorrect assignment detection task6. Given the paper assignments of each author and the metadata of each paper, this task is to detect paper assignment errors for each author. In this task, each paper contains the title, abstract, author name and corresponding organization, keywords, venue and year. The training data contains several authors and corresponding paper assignments. For simplicity, we sample 10k papers from 5https://www.kaggle.com/competitions/ leash-BELKA 6https://www.biendata.xyz/kdd2024 12 Tasks Task Type Dataset Size (GB) Metric Type Original Complexity Split aptos2019-blindness-detection random-acts-of-pizza spooky-author-identification google-quest-challenge stanford-covid-vaccine predict-volcanic-eruptions-ingv-oe lmsys-chatbot-arena us-patent-phrase-to-phrase-matching mlsp-2013-birds statoil-iceberg-classifier-challenge petfinder-pawpularity-score Image Classification Text Classification Text Classification Training LLMs Tabular Signal Processing Text Classification Text Regression Audio Classification Image Classification Image Regression tensorflow-speech-recognition-challenge Audio Classification denoising-dirty-documents new-york-city-taxi-fare-prediction tgs-salt-identification-challenge ventilator-pressure-prediction Image to Image Tabular Image Segmentation Forecasting 10.22 0.003 0.0019 0.015 2.68 31.25 0.18 0.00214 0.5851 0.3021 1.04 3.76 0.06 5.7 0.5 0.7 Max Max Min Max Min Min Min Max Max Min Min Max Min Min Max Min Low Low Low Medium High High Medium Medium Low Medium Medium Medium Low Low Medium Medium Easy Easy Easy Easy Easy Easy Medium Medium Medium Medium Medium Medium Hard Hard Hard Hard Table 3: Full list of tasks in MLE-Bench used for evaluation in our work. the given paper list, and then preserve the corresponding authors related to these papers. These authors are classified into training and test data following the label distribution used in training data. The eveluation metric is (cid:80)M 1 wi AU Ci where is the test author number, wi = #ErrorsOf hisAuthor #T otalErrors ."
        },
        {
            "title": "B Hyperparameters",
            "content": "We list the detailed hyperparameters for AUTOMIND and AIDE in Table 4 and Table 5, respectively."
        },
        {
            "title": "C Prompts",
            "content": "In this section, we showcase some of the prompts used in the full pipeline of AUTOMIND, which serve as reference. 13 Hyperparameter Value agent.retriever.model agent.analyzer.model agent.planner.model agent.coder.model agent.improver.model agent.verifier.model agent.steps agent.search.num_drafts agent.search.max_debug_depth agent.search.debug_prob agent.search.trick_prob agent.search.greedy_prob agent.time_limit exec.timeout gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18 &TARGET_MODEL &TARGET_MODEL &TARGET_MODEL gpt-4.1-mini-2025-04-14 500 5 20 1 0.8 0.8 86400 Hyperparameters for AUTOMIND. 4: Table &TARGET_MODEL is the foundation model being evaluated. agent.search.num_drafts is the number agent.search.debug_prob of initial draft nodes. is the probalitity of whether debug node. agent.search.trick_prob probalitity of whether use tricks to improve node. agent.search.greedy_prob is the probability of whether select the best node. the is Hyperparameter Value agent.code.model agent.feedback.model agent.steps agent.search.num_drafts agent.search.max_debug_depth agent.search.debug_prob agent.time_limit exec.timeout &TARGET_MODEL gpt-4o-mini-2024-07-18 500 5 20 1 86400 32400 the Table 5: Hyperparameters for AIDE. &TARGET_MODEL is evaluated. agent.search.num_drafts of initial draft nodes. agent.search.debug_prob is the probalitity of whether debug node. foundation model is being the number Prompt for plan generation of drafting # Introduction You are an expert machine learning engineer attempting task. In order to complete this task, you need to come up with an excellent and creative plan for solution, which will be implemented by another engineer. We will now provide description of the task. # Task description {task_description} # Memory Take the Memory section into consideration when proposing the solution plan, don't propose the similar solution but keep the evaluation metric exactlty the same. {memory} # Knowledge Some of the tricks that have proved useful for the same type of task are provided as follows: {tricks} You should carefully consider these tricks when designing your solution. # Data Analysis {data_analysis} # Instructions ## Response format Your response should be detailed outline/sketch of your proposed solution in natural language. You do not need to implement the solution but you should provide enough detail for another engineer to implement it in Python code. There should be no additional headings or text in your response. Just natural language text followed by newline. 'xgboost==2.1.3', solution can use any relevant machine learning packages ## Installed Packages Your 'torchgeometric==2.6.1', 'transform- 'pandas==2.1.4', ers==4.44.2', 'torch==2.2.0', 'optuna==4.0.0', 'timm==0.9.7', 'scikit-learn==1.2.2', 'numpy==1.26.2', 'bayesianoptimization==1.5.1', 'seaborn==0.13.2'. Feel free to use any other packages too (all packages are already installed!). For neural networks please use PyTorch because of the unavailability of TensorFlow in the environment. 'statsmodels==0.14.4', 'torchvision==0.17.0', 'matplotlib==3.9.2', 'lightgbm==4.5.0', 'scipy==1.11.4', such as: ## Plan guideline - DO NOT CHEAT ON EVALUATION. The solution should calculate the evaluation metric described in the task description on hold-out validation set. - If the evaluation metric is not provided, you should propose reasonable evaluation metric for the task and calculate it. - The solution should print the evaluation metric computed on the hold-out validation set at the last step of the solution. - Try to come up with more modern and powerful methods to feature engineering and modelling and avoid using outdated methods. For example, if the task is classification task, you should use modern transformer-based models instead of traditional models like CNN or LSTM. 14 - The solution should adopt appropriate methods to prevent model overfitting, such as data augmentation, early stopping, regularization, dropout, and others. - Don't suggest to do model ensembling. - Don't suggest to do Exploratory Data Analysis. - Don't suggest to do hyperparameter tuning. - The data is already prepared and available in the './input' directory. There is no need to unzip any files. - The solution should use os.walk to get the paths of all available files in the '. /input' directory for data loading. - If 'sample_submission.csv' file existes, directly load it and use it as template for the 'submission.csv' file. The solution should save predictions on the provide unlabeled test data in the 'submission.csv' file in the ./submission/ directory. Prompt for plan generation of debugging # Introduction You are an expert machine learning engineer attempting task. You are provided with the plan, code and execution output of previous solution below that had bug and/or did not produce submission.csv, and should improve it in order to fix the bug. For this you should first propose an reasonanle improvement and accordingly outline detailed improved plan in natural language, which will be implemented by another engineer. We will now provide description of the task. # Task description {task_description} # Previous Solution ## Previous Plan {prev_plan} ## Previous Code {prev_code} ## Previous Execution Output {prev_output} # Data Analysis {data_analysis} # Instructions ## Response format First, provide brief explanation of your reasoning for the proposed improvement to the previous plan (wrapped in <think></think>). Then, provide detailed outline/sketch of your improved solution in natutal language based on the previous plan and your proposed improvement (wrapped in <plan></plan>). You do not need to implement the solution but you should provide enough detail for another engineer to implement it in Python code. ## Installed Packages 15 'xgboost==2.1.3', 'matplotlib==3.9.2', solution can use any relevant machine learning packages 'torchYour 'transformgeometric==2.6.1', ers==4.44.2', 'pandas==2.1.4', 'torch==2.2.0', 'optuna==4.0.0', 'timm==0.9.7', 'scikit-learn==1.2.2', 'numpy==1.26.2', 'bayesianoptimization==1.5.1', 'seaborn==0.13.2'. Feel free to use any other packages too (all packages are already installed!). For neural networks please use PyTorch because of the unavailability of TensorFlow in the environment. 'statsmodels==0.14.4', 'torchvision==0.17.0', 'lightgbm==4.5.0', 'scipy==1.11.4', such as: ## Improve guideline - You should pay attention to the execution output of the previous solution, and propose an improvement that will fix the bug. - The improved plan should be derived by adapting the previous plan only based on the proposed improvement, while retaining other details of the previous plan.Don't suggest to do Exploratory Data Analysis. - Don't suggest to do hyperparameter optimization, you should use the best hyperparameters from the previous solution. - If 'sample_submission.csv' file existes, directly load it and use it as template for the 'submission.csv' file. The solution should save predictions on the provide unlabeled test data in the 'submission.csv' file in the ./submission/ directory. - When describing your improved plan, do not use phrases like 'the same as before' or 'as in the previous plan'. Instead, fully restate all details from the previous plan that you want to retain, as subsequent implementation will not have access to the previous plan. Prompt for plan generation of improving with tricks # Introduction You are an expert machine learning engineer attempting task. You are provided with the plan, code and execution output of previous solution below and should improve it in order to further increase the test time performance. For this you should integrate integrate several useful tricks provided and accordingly outline detailed improved plan in natural language, which will be implemented by another engineer. We will now provide description of the task. # Task description {task_description} # Memory Take the Memory section into consideration when proposing the solution plan, don't propose the similar solution but keep the evaluation metric exactlty the same. {memory} # Previous Solution ## Previous Plan {prev_plan} ## Previous Code {prev_code} ## Previous Execution Output 16 {prev_output} # Knowledge Here are some tricks that have proved useful for the task: {tricks} You should carefully consider these tricks when designing your solution. # Data Analysis {data_analysis} # Instructions ## Response format First, provide brief explanation of your reasoning for the proposed improvement to the previous plan (wrapped in <think></think>). Then, provide detailed outline/sketch of your improved solution in natutal language based on the previous plan and your proposed improvement (wrapped in <plan></plan>). You do not need to implement the solution but you should provide enough detail for another engineer to implement it in Python code. 'xgboost==2.1.3', solution can use any relevant machine learning packages ## Installed Packages Your 'torchgeometric==2.6.1', 'transform- 'pandas==2.1.4', ers==4.44.2', 'torch==2.2.0', 'optuna==4.0.0', 'timm==0.9.7', 'scikit-learn==1.2.2', 'numpy==1.26.2', 'bayesianoptimization==1.5.1', 'seaborn==0.13.2'. Feel free to use any other packages too (all packages are already installed!). For neural networks please use PyTorch because of the unavailability of TensorFlow in the environment. 'statsmodels==0.14.4', 'torchvision==0.17.0', 'matplotlib==3.9.2', 'lightgbm==4.5.0', 'scipy==1.11.4', such as: ## Improve guideline - You should focus ONLY on integrating the provided tricks in the knowledge section into the previous solution to fully leverage their potentials. - Make sure to fully integrate these tricks into your plan while preserving as much details as possible. - Ensure that your plan clearly demonstrates the functions and specifics of the tricks. - Identify the key areas in the previous solution where the knowledge can be applied. - Suggest specific changes or additions to the code or plan based on the knowledge provided, and avoid unnecessary modifications irrelevant to the tricks. - If 'sample_submission.csv' file existes, directly load it and use it as template for the 'submission.csv' file. The solution should save predictions on the provide unlabeled test data in the 'submission.csv' file in the ./submission/ directory. - When describing your improved plan, do not use phrases like 'the same as before' or 'as in the previous plan'. Instead, fully restate all details from the previous plan that you want to retain, as subsequent implementation will not have access to the previous plan. Prompt for plan generation of improving without tricks # Introduction You are an expert machine learning engineer attempting task. You are provided with the plan, code and execution output of previous solution below and should improve it in order to further increase the test time performance. For this you should first propose reasonable improvement 17 and accordingly outline detailed improved plan in natural language, which will be implemented by another engineer. We will now provide description of the task. # Task description {task_description} # Memory Take the Memory section into consideration when proposing the solution plan, don't propose the similar solution but keep the evaluation metric exactlty the same. {memory} # Previous Solution ## Previous Plan {prev_plan} ## Previous Code {prev_code} ## Previous Execution Output {prev_output} # Data Analysis {data_analysis} # Instructions ## Response format First, provide brief explanation of your reasoning for the proposed improvement to the previous plan (wrapped in <think></think>). Then, provide detailed outline/sketch of your improved solution in natutal language based on the previous plan and your proposed improvement (wrapped in <plan></plan>). You do not need to implement the solution but you should provide enough detail for another engineer to implement it in Python code. 'xgboost==2.1.3', solution can use any relevant machine learning packages ## Installed Packages Your 'torchgeometric==2.6.1', 'transform- 'pandas==2.1.4', ers==4.44.2', 'torch==2.2.0', 'optuna==4.0.0', 'timm==0.9.7', 'scikit-learn==1.2.2', 'numpy==1.26.2', 'bayesianoptimization==1.5.1', 'seaborn==0.13.2'. Feel free to use any other packages too (all packages are already installed!). For neural networks please use PyTorch because of the unavailability of TensorFlow in the environment. 'statsmodels==0.14.4', 'torchvision==0.17.0', 'matplotlib==3.9.2', 'lightgbm==4.5.0', 'scipy==1.11.4', such as: ## Improve guideline - You should conduct only one expert-level actionable improvement to the previous solution. - This improvement should be atomic so that the effect of the improved solution can be experimentally evaluated. - The improved plan should be derived by adapting the previous plan only based on the proposed improvement, while retaining other details of the previous plan. - Dont suggest to do Exploratory Data Analysis. 18 - Dont suggest to do hyperparameter optimization, you should use the best hyperparameters from the previous solution. - If 'sample_submission.csv' file existes, directly load it and use it as template for the 'submission.csv' file. The solution should save predictions on the provide unlabeled test data in the 'submission.csv' file in the ./submission/ directory. - When describing your improved plan, do not use phrases like 'the same as before' or 'as in the previous plan'. Instead, fully restate all details from the previous plan that you want to retain, as subsequent implementation will not have access to the previous plan. Prompt for complexity scorer # Introduction You are an expert machine learning engineer attempting task. In order to complete this task, you are given discription of the task and solution plan proposed by another engineer and need to assess the complexity of the task and the proposed solution. We will now provide description of the task. # Task description {task_description} # Proposed Solution {proposed_solution} # Data Analysis {data_analysis} # Instructions ## Response format First, provide brief explanation of your reasoning for the assessment of the complexity of the task and the proposed solution (wrapped in <think></think>). Then, provide ONLY ONE average complexity score as floating point number between 1 and 5, which can contain 0.5 points (wrapped in <score></score>). ## Task complexity scoring criteria - 5 = Extremely complex and cutting-edge task with high levels of innovation required. This involves solving unique or highly specialized problem that may push the boundaries of existing knowledge or technology. - 4 = Complex task that involves advanced techniques or methodologies, requiring considerable expertise in the domain, such as building novel algorithms, optimization methods, or handling advanced data. - 3 = Moderately complex task that requires significant problem-solving, such as integrating different methods or creating custom algorithms for specific use cases. - 2 = Simple task with some level of complexity, such as basic algorithms that need some degree of fine-tuning or adjustment to meet the specific requirements of the project. - 1 = Very simple task that requires minimal effort, such as basic data manipulation or applying standard algorithms without any customization. ## Proposed solution complexity scoring criteria - 5 = groundbreaking or transformative solution that pushes the envelope in the field. It introduces novel approach that is scalable, efficient, and offers long-term value or sets new standard. - 4 = highly original and effective solution that shows deep understanding of the problem domain and offers significant contribution to the field. The solution is well-optimized and efficient. - 3 = An original and creative solution with reasonable level of complexity. It involves designing and implementing custom solutions or combining existing methods in new way. - 2 = somewhat original solution that involves adapting existing tools or methods with some customization to meet the needs of the project. There may be room for optimization or improvement. - 1 = Very basic solution, perhaps using standard, off-the-shelf tools with minimal adaptation, lacking originality or novel contributions. ## Complexity scoring guideline - Evaluate the complexity of the task and the proposed solution, and assign score between 1 and 5. - Assign an average score between 1 and 5, consider factors such as the task's complexity, the proposed solution, the dataset size, and the time and hardware resources required for implementation and execution. Prompt for code implementation through one-pass coding # Introduction You are an expert machine learning engineer attempting task. In order to complete this task, you are given discription of the task and solution plan proposed by another engineer and need to assess the complexity of the task and the proposed solution. We will now provide description of the task. # Task description {task_description} # Proposed Solution {proposed_solution} # Data Analysis {data_analysis} # Instructions ## Response format Your response should be single markdown code block (wrapped in ''') which implements this solution plan and prints out and save the evaluation metric. 'xgboost==2.1.3', solution can use any relevant machine learning packages ## Installed Packages 'torchYour 'transformgeometric==2.6.1', ers==4.44.2', 'pandas==2.1.4', 'torch==2.2.0', 'optuna==4.0.0', 'timm==0.9.7', 'scikit-learn==1.2.2', 'numpy==1.26.2', 'bayesianoptimization==1.5.1', 'seaborn==0.13.2'. Feel free to use any other packages too (all packages are already installed!). For neural networks please use PyTorch because of the unavailability of TensorFlow in the environment. 'statsmodels==0.14.4', 'torchvision==0.17.0', 'matplotlib==3.9.2', 'lightgbm==4.5.0', 'scipy==1.11.4', such as: 20 ## Code guideline - The code should **implement the proposed solution** and **print the value of the evaluation metric computed on hold-out validation set**, - **AND MOST IMPORTANTLY SAVE PREDICTIONS ON THE PROVIDED UNLABELED TEST DATA IN 'submission.csv' FILE IN THE ./submission/ DIRECTORY.** - The code should save the evaluation metric computed on the hold-out validation set in 'eval_metric.txt' file in the ./submission/ directory. - The code should be single-file python program that is self-contained and can be executed as-is. - No parts of the code should be skipped, don't terminate the code before finishing the script. - DO NOT WRAP THE CODE IN MAIN FUNCTION, BUT WRAP ALL CODE in the '__main__' module, or it cannot be executed successfully. - All class initializations and computational routines MUST BE WRAPPED in 'if __name__ == \"__main__\":'. - DO NOT USE MULTIPROCESSING OR SET 'num_workers' IN DATA LOADER, as it may cause the container to crash. - Your response should only contain single code block. - All input data is already prepared and available in the './input' directory. There is no need to unzip any files. - DO NOT load data from \"./data\" directory, it is not available in the environment. - Do not save any intermediate or temporary files through 'torch.save' or 'pickle.dump'. - Try to accelerate the model training process if any GPU is available. - DO NOT display progress bars. If you have to use function intergrated with progress bars, disable progress bars or using the appropriate parameter to silence them. - Don't do Exploratory Data Analysis. Prompt for stepwise decomposition # Introduction You are an expert machine learning engineer attempting task. In order to complete this task, you are given the proposed solution and supposed to decompose it into multiple steps. We will now provide description of the task. # Task description {task_description} # Proposed Solution {proposed_solution} # Instructions ## Response format - Your response should be single JSON code block (wrapped in ''') which contains the decomposition steps of the proposed solution. - The generated JSON should have the following format: { \"decomposed steps\": [ { }, \"step\": \"Name of the step\", \"details\": \"Detailed description of the step\", 21 ... ], } ## Solution decomposition guideline - You should decompose the proposed solution into multiple steps, and provide detailed descriptions of each step. - DO NOT MODIFY THE PROPOSED SOLUTION. In the description of each step, you should keep as many details of the proposed solution as possible, especially the exact hyperparameters and sample code. - DO NOT CHEAT ON EVALUATION. The solution should calculate the evaluation metric described in the task description on hold-out validation set. - If the evaluation metric is not provided, you should propose reasonable evaluation metric for the task and calculate it. - The solution should save the evaluation metric computed on the hold-out validation set in 'eval_metric.txt' file in the ./submission/ directory. - The solution should use os.walk to get the paths of all available files in the '. /input' directory for data loading. - If sample_submission.csv file existes, directly load it and use it as template for the 'submission.csv' file. The solution should save predictions on the provide unlabeled test data in the 'submission.csv' file in the ./submission/ directory. - You should **print the value of the evaluation metric computed on hold-out validation set** in the last step of the decomposed steps. - Don't do Exploratory Data Analysis in the decomposition steps. - If you find improvements suggestions in the plan, you should take them in serious consideration and include them in the decomposition steps. - You do not need to implement the code in the decomposed steps. - Note that the order of the decomposed steps determines the order in which the code is implemented and executed. Prompt for code implementation through stepwise coding # Introduction You are an expert machine learning engineer attempting task. In order to complete this task, you are given the code for previous steps and need to implement the current step of natural language solution plan proposed by another engineer in Python code. We will now provide description of the task. # Task description {task_description} # Current Step {current_step} # Previous Steps Code You should continue the following code for previous steps to implement the current step of the solution plan, but do not repeat it: {prev_steps} # Data Analysis {data_analysis} # Instructions ## Response format First, provide suggestions for the current step based on the previous steps and the failed last try step if provided (wrapped in <think></think>). Then, provide single markdown code block (wrapped in ''') which implements the current step of solution plan. 'xgboost==2.1.3', solution can use any relevant machine learning packages ## Installed Packages 'torchYour 'transformgeometric==2.6.1', ers==4.44.2', 'pandas==2.1.4', 'torch==2.2.0', 'optuna==4.0.0', 'timm==0.9.7', 'scikit-learn==1.2.2', 'numpy==1.26.2', 'bayesianoptimization==1.5.1', 'seaborn==0.13.2'. Feel free to use any other packages too (all packages are already installed!). For neural networks please use PyTorch because of the unavailability of TensorFlow in the environment. 'statsmodels==0.14.4', 'torchvision==0.17.0', 'matplotlib==3.9.2', 'lightgbm==4.5.0', 'scipy==1.11.4', such as: ## Code guideline - You should first provide suggestions for the current step based on the previous steps and the failed last try step if provided, and then implement the current step of the solution plan. - **You should ONLY implement the code for the current step of the solution plan, rather than the entire solution plan.** - DO NOT MODIFY THE CURRENT STEP. You should implement the current step exactly as it is. - You should **print the value of the evaluation metric computed on hold-out validation set** if it is calculated in the current step. - You should save the evaluation metric computed on the hold-out validation set in 'eval_metric.txt' file in the ./submission/ directory if it is calculated in the current step. - DO NOT PRINT ANYTHING ELSE IN THE CODE, except for the evaluation metric and completion message for the current step. - The code should be single-file python program that is self-contained and can be executed as-is. - DO NOT WRAP THE CODE IN MAIN FUNCTION, BUT WRAP ALL CODE in the '__main__' module, or it cannot be executed successfully. - All class initializations and computational routines MUST BE WRAPPED in 'if __name__ == \"__main__\":'. - DO NOT USE MULTIPROCESSING OR SET 'num_workers' IN DATA LOADER, as it may cause the container to crash. - No parts of the code should be skipped, don't terminate the code before finishing the script. - **DO NOT REPEAT the code for previous steps, you should only import them from prev_steps.py.** - DO NOT REPETITIVELY IMPORT THE SAME MODULES IN PREVIOUS STEPS, but you can import other modules if needed. - **AND MOST IMPORTANTLY SAVE PREDICTIONS ON THE PROVIDED UNLABELED TEST DATA IN 'submission.csv' FILE IN THE ./submission/ DIRECTORY.** if predictions are involved in the current step. - All input data is already prepared and available in the './input' directory. There is no need to unzip any files. - DO NOT load data from \"./data\" directory, it is not available in the environment. - Do not save any intermediate or temporary files through 'torch.save' or 'pickle.dump'. 23 - You can reference to the based code to implement the current step, but do not completely repeat it. - Try to accelerate the model training process if any GPU is available. - DO NOT display progress bars. If you have to use function intergrated with progress bars, disable progress bars or using the appropriate parameter to silence them. - Don't do Exploratory Data Analysis. Prompt for output veirification # Introduction You are an expert machine learning engineer attempting task. You have written code to solve this task and now need to evaluate the output of the code execution. You should determine if there were any bugs as well as report the empirical findings. # Task description {task_description} # Code {code} # Execution Output {execution_output} # Tool { \"type\": \"function\", \"function\": { \"name\": \"submission_verify\", \"description\": \"Verify the execution output of the written code.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"is_bug\": { \"type\": \"boolean\", \"description\": \"true if the output log shows that the execution failed or has some bug, otherwise false.\", }, \"is_overfitting\": { \"type\": \"boolean\", \"description\": \"true if the output log shows that the model is overfitting or validation metric is much worse than the training metric or validation loss is increasing, otherwise false. \", }, \"has_csv_submission\": { \"type\": \"boolean\", \"description\": \"true if the code saves the predictions on the test data in 'submission.csv' file in the './submission/' directory, otherwise false. Note that the file MUST be saved in the ./submission/ directory for this to be evaluated as true, otherwise it should be evaluated as false. You can assume the ./submission/ directory exists and is writable.\", }, \"summary\": { \"type\": \"string\", \"description\": \"write short summary (2-3 sentences) describing the empirical findings. Alternatively mention if there is bug or the submission.csv was not properly produced. You 24 do not need to suggest fixes or improvements.\", }, \"metric\": { \"type\": \"number\", \"description\": \"If the code ran successfully, report the value of the validation metric. Otherwise, leave it null.\", }, \"lower_is_better\": { \"type\": \"boolean\", \"description\": \"true if the metric should be minimized (i.e. lower metric value is better, such as with MSE), false if the metric should be maximized (i.e. higher metric value is better, such as with accuracy).\", }, }, \"required\": \"lower_is_better\"], }, }, } [\"is_bug\", \"is_overfitting\", \"has_csv_submission\", \"summary\", \"metric\","
        }
    ],
    "affiliations": [
        "Ant Group",
        "Zhejiang University",
        "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
    ]
}