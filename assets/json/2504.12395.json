{
    "paper_title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework",
    "authors": [
        "Jiale Tao",
        "Yanbing Zhang",
        "Qixun Wang",
        "Yiji Cheng",
        "Haofan Wang",
        "Xu Bai",
        "Zhengguang Zhou",
        "Ruihuang Li",
        "Linqing Wang",
        "Chunyu Wang",
        "Qin Lin",
        "Qinglin Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharacter, a scalable framework for character customization built upon a foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, the framework introduces a scalable adapter with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. This dual-data structure enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways. Qualitative experiments demonstrate the advanced capabilities of InstantCharacter in generating high-fidelity, text-controllable, and character-consistent images, setting a new benchmark for character-driven image generation. Our source code is available at https://github.com/Tencent/InstantCharacter."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 5 9 3 2 1 . 4 0 5 2 : r InstantCharacter: Personalize Any Characters with Scalable Diffusion Transformer Framework Jiale Tao1, Yanbing Zhang1, Qixun Wang12, Yiji Cheng1, Haofan Wang2, Xu Bai2, Zhengguang Zhou12, Ruihuang Li1, Linqing Wang12, Chunyu Wang1, Qin Lin1, Qinglin Lu1 1Hunyuan, Tencent 2InstantX Team Tech Lead, Corresponding Author"
        },
        {
            "title": "Abstract",
            "content": "Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharactera scalable framework for character customization built upon foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, the framework introduces scalable adapter with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. This dual-data structure enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways. Qualitative experiments demonstrate InstantCharacters advanced capabilities in generating high-fidelity, text-controllable, and character-consistent images, setting new benchmark for character-driven image generation. Our source code is available at https://github.com/Tencent/InstantCharacter."
        },
        {
            "title": "Introduction",
            "content": "Character-driven image generation aims to create images that incorporates the user-defined character image and text prompts, playing crucial role in various creative endeavors such as storytelling illustration, comic creation, game character design, and more. These capabilities enable wide range of applications in entertainment, film production, e-commerce advertising, and beyond. Recent advancements in generative diffusion transformers have demonstrated unprecedented capabilities in synthesizing high-fidelity images from textual descriptions. Nevertheless, the potential of these state-of-the-art models for personalized image generation remains underexplored, especially in the context of creating character-driven visual narratives that embody human-like attributes. Current methodologies for generating consistent images of specified subjects primarily rely on tuningor adapter-based approaches. Adapter-based approaches [9, 24, 13] extract visual features through subject encoder and integrate them into the image noise space via cross-attention mechanism. While these techniques achieve certain subject consistency and text controllability on UNet-based models, they struggle to personalize open-domain characters with diverse identities, poses, and styles. Although effective for customizing open-domain characters, tuning-based approaches [18] require Preprint. Under review. Figure 1: Open-domain character personalization with InstantCharacter. fine-tuning the model to reconstruct subject images, leading to long customization time and limited text controllability. Moreover, inference-time fine-tuning becomes computationally prohibitive for modern diffusion transformers with billions of parameters. Compared to traditional UNet-based architectures [17, 15], modern Diffusion Transformers (DiTs) [2, 8] exhibit powerful generative priors and offer unparalleled flexibility and capacity. However, fully unleashing their potential is non-trivial, as it requires robust adapter network compatible with the framework to ensure alignment between character-specific features and vast generative latent space. In addition, training such an adapter necessitates adequate training data and effective training strategies. We observe that directly applying traditional adapters to large-scale DiTs often fails, as these adapters are primarily designed for UNet architectures and cannot scale effectively to models with billions of parameters, such as Flux [8] with 12 billion parameters. To achieve generalized character personalization without compromising inference-time efficiency and textual editability, we propose InstantCharacter, scalable diffusion transformer framework designed for character-driven image generation. InstantCharacter offers three key advantages: 1.Generalizability. It can flexibly personalize any character with different appearances, actions, and styles, ranging from photorealistic portraits to anime game assets. 2.Scalability. We develop scalable adapter that can effectively integrate multi-stage character features and interact with the latent space of modern DiTs. 3.Versatility. To enable efficient training, we collect versatile 10-million-level character dataset, which contains paired (multi-view character) and unpaired (textimage combinations) subsets. Accordingly, we propose an efficient three-stage training strategy to accommodate heterogeneous data samples. Specifically, we decouple character consistency (unpaired data), textual controllability (paired data), and image fidelity (high-resolution data) to prevent mutual interference between high-fidelity identity maintenance and prompt-guided character manipulations. We implement InstantCharacter based on the powerful FLUX1.0-dev model. Qualitative comparisons with previous work demonstrate InstantCharacters advanced capabilities in generating high-fidelity, text-controllable, and character-consistent images."
        },
        {
            "title": "2 Related Work",
            "content": "T2I diffusion models. Recent advances [2, 15, 17] in text-to-image generation have witnessed paradigm shift from traditional U-Net architectures [17] to more powerful diffusion transform2 ers [2] (DiTs). While early diffusion models such as stable diffusion (SD) demonstrated remarkable image synthesis capabilities, modern DiT-based systems like SD3 [2] and FLUX.1 [8] have set new benchmarks in generation quality through their transformer-based architectures and advanced techniques like rectified flows. This architectural evolution presents both opportunities and challenges for character-centric applications, while DiTs offer superior generation capacity, their adaptation for identity-preserving tasks remains largely underexplored. Our work bridges this critical gap by developing the first DiT-based framework specifically optimized for character customization. Personalized character generation. Recent advances in personalized image generation have evolved from tuning-based to adapter-based approaches. Early methods [18, 1, 3, 7, 4] relied on fine-tuning the entire diffusion model for each new subject, which was computationally expensive and suffered from poor generalization due to limited training data. To address these issues, recent works [24, 9, 13, 22, 10, 5, 21, 12] introduced adapter-based techniques that avoid test-time finetuning. For instance, IP-Adapter [24] employs clip image encoder to extract subject features and injects them into frozen diffusion model via cross-attention, enabling efficient personalization. However, these adapter-based methods are predominantly built upon UNet-based architectures with restricted capacity, causing them to struggle in effectively scaling and often produce low-fidelity outputs and limited generalization across diverse character poses and styles. In contrast, our work introduces scalable diffusion transformer framework that overcomes these limitations, achieving superior open-domain generalizability, image fidelity, and text controllability compared to UNet-based alternatives."
        },
        {
            "title": "3 Methods",
            "content": "Modern DiTs [2, 8] have demonstrated unprecedented fidelity and capacity compared to traditional UNet-based architectures, offering more robust foundation for generation and editing tasks. Building upon these advances, we present InstantCharacter, novel framework that extends DiT for generalizable and high-fidelity character-driven image generation. As illustrated in Figure 2, InstantCharacters architecture centers around two key innovations. First, scalable adapter module is developed to effectively parse character features and seamlessly interact with DiTs latent space. Second, progressive three-stage training strategy is designed to adapt to our collected versatile dataset, enabling separate training for character consistency and text editability. By synergistically combining flexible adapter design and phased learning strategy, we enhance the general character customization capability while maximizing the preservation of the generative priors of the base DiT model. In the following sections, we will detail the adapters architecture and elaborate on our progressive training strategy. Figure 2: Our framework seamlessly integrates scalable adapter with pretrained DiT model. The adapter consists of multiple stacked transformer encoders that incrementally refine character representations, enabling effective interaction with the latent space of the DiT. The training process employs three-stage progressive strategy, beginning with unpaired low-resolution pretraining and culminating in paired high-resolution fine-tuning. 3 3.1 The scalable adapter design Traditional customization adapters, such as IPAdapter [24] or ReferenceNet [19], often fail in the DiT architectures because they are specifically designed for U-Net based models and lack scalability. To better adapt to DiT models, we propose scalable full-transformer adapter that serves as crucial link between conditioning character images and the latent generative space of the base model. The full-transformer structure enables scalability by increasing layer depth and hidden feature sizes. This adapter consists of three encoder blocks, as detailed below. General vision encoders. We first leverage pre-trained large vision foundation encoders to extract general character features, benefiting from their open-domain recognition abilities. Previous methods [24, 10] typically rely on CLIP [16] for its aligned visual and textual features. However, while CLIP is capable of capturing abstract semantic information, it tends to lose detailed texture information, which is crucial for maintaining character consistency. To this end, we replace CLIP with SigLIP [25], which excels in capturing finer-grained character information. In addition, we introduce DINOv2 [14] as another image encoder to enhance the robustness of features, reducing the loss of features caused by background or other interfering factors. Finally, we integrate DINOv2 and SigLIP features via channel-wise concatenation, resulting in more comprehensive representation of open-domain characters. Intermediate encoders. Since SigLIP and DINOv2 are pre-trained and inferred at relatively low resolution of 384, the raw output of general vision encoders may lose fine-grained features when processing high-resolution character images. To mitigate this issue, we employ dual-stream feature fusion strategy to explore low-level and region-level features, respectively. First, we directly extract low-level features from the shallow layers of the general vision encoders, capturing details that are often lost in higher layers. Second, we divide the reference image into multiple non-overlapping patches and feed each patch into the vision encoder to obtain region-level features. Then these two distinct feature streams undergo hierarchical integration through dedicated intermediate transformer encoders. Specifically, each feature pathway is independently processed by separate transformer encoder to integrate with high-level semantic features. Subsequently, the refined feature embeddings from both pathways are concatenated along the token dimension, establishing comprehensive fused representation that captures multi-level complementary information. Projection head. Finally, the refined character features are projected into the denoising space via projection head and interact with the latent noise. We implement this through timestep-aware Q-former [24] that processes intermediate encoder outputs as key-value pairs while dynamically updating set of learnable queries through attention mechanisms. The transformed query features are then injected into the denoising space via learnable cross-attention layers. Finally, the adapter enables faithful identity preservation and flexible adaptation to complex text-driven modifications. 3.2 Training strategies To enable effective training of the framework, we first curate high-quality dataset of 10 million images containing diverse full-body humans/characters, including both unpaired images for learning robust character consistency and paired sets for achieving precise text-to-image alignment. Our training regimen is meticulously designed to optimize character consistency, text controllability, and visual fidelity. To achieve character consistency, we first train with unpaired data, where the character image is incorporated as reference guidance to reconstruct itself and preserve structural consistency. We discovered that using resolution of 512 is significantly more efficient than 1024. In the second phase, we continue training at low resolution (512) but switch to paired training data. By taking the character image as input, we aim to generate images of the character in different actions, poses, and styles within new scene based on given textual description. This training stage efficiently eliminates the copy-paste effect and enhances text controllability, ensuring that the generated images accurately follow the textual condition. The final phase involves high-resolution joint training using both paired and non-paired images. We found that limited number of high-resolution training iterations can substantially improve the visual quality and texture of the images. This stage leverages high-quality images to achieve high-fidelity and textually controlled character images."
        },
        {
            "title": "4 Experiments",
            "content": "Figure 3: Qualitative comparison on character personalization. Our method generally demonstrates the best image fidelity and character consistency while maintaining the desirable textual controllability. Qualitative results. We conduct qualitative comparisons against state-of-the-art FLUX-based approaches: OminiControl [20], EasyControl [26], ACE+ [11], and UNO [23]; and the large multimodality model GPT4o [6]. For evaluation, we collect set of open-domain character images not present in the training data. As shown in Fig. 3 and Fig. 4, our analysis demonstrates that while existing methods show limitations: OminiControl and EasyControl fail to preserve character identity features, and ACE++ only maintains partial features in simple scenarios while struggling with actionoriented prompts. UNO overly preserves consistency, which reduces the editability of actions and backgrounds. It is notable that our method achieves comparable results with GPT4o, which is the current SoTA method but it is not open source. In contrast, InstantCharacter consistently performs the best. Specifically, InstantCharacter achieves superior character detail preservation with high fidelity while maintaining precise text controllability, even for complex action prompts. These qualitative advantages are further supported by quantitative measurements shown in Fig. 6. Personalization with different styles. Our framework can also achieve flexible character stylization by introducing different style loras. As shown in Fig. 5, our method can switch between Ghibli and 5 Figure 4: Qualitative comparison on character personalization. Our method generally demonstrates the best image fidelity and character consistency while maintaining the desirable textual controllability. Makoto styles without compromising character consistency and textual editability. However, it is difficult for Jimeng and GPT4o to preserve the styles flexibly."
        },
        {
            "title": "5 Conclusion",
            "content": "We present InstantCharacter, an innovative diffusion transformer framework that significantly advances character-driven image generation. Our solution delivers three fundamental advantages: first, it achieves unprecedented open-domain personalization across diverse character appearances, poses, and styles while preserving high-fidelity quality; second, it develops scalable adapter architecture that effectively processes character features and interacts with diffusion transformers latent space; third, it establishes an effective three-stage training methodology combining massive 10-millionscale dataset to simultaneously optimize character consistency and textual control. Qualitative results validate InstantCharacters superior performance in generating high-fidelity, character-consistent, and text-controllable images. More broadly, our work offers insights for adapting foundation diffusion transformers to specialized generation tasks, potentially inspiring new developments in controllable visual synthesis. 6 Figure 5: Qualitative comparison on character personalization with different styles."
        },
        {
            "title": "References",
            "content": "[1] Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., Cohen-Or, D.: Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG) 42(4), 110 (2023) [2] Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.: Scaling rectified flow transformers for high-resolution image synthesis. In: Forty-first international conference on machine learning (2024) [3] Feng, H., Huang, Z., Li, L., Lv, H., Sheng, L.: Personalize anything for free with diffusion transformer. arXiv preprint arXiv:2503.12590 (2025) [4] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022) [5] Huang, M., Mao, Z., Liu, M., He, Q., Zhang, Y.: Realcustom: narrowing real text word for realtime open-domain text-to-image customization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 74767485 (2024) [6] Hurst, A., Lerer, A., Goucher, A.P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al.: Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024) [7] Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 19311941 (2023) 7 Figure 6: More qualitative results of InstantCharacter. [8] Labs, B.F.: Flux: Official inference repository for flux.1 models (2024) [9] Li, D., Li, J., Hoi, S.: Blip-diffusion: Pre-trained subject representation for controllable textto-image generation and editing. Advances in Neural Information Processing Systems 36, 3014630166 (2023) [10] Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M.M., Shan, Y.: Photomaker: Customizing realistic human photos via stacked id embedding. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 86408650 (2024) [11] Mao, C., Zhang, J., Pan, Y., Jiang, Z., Han, Z., Liu, Y., Zhou, J.: Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487 (2025) [12] Mao, Z., Huang, M., Ding, F., Liu, M., He, Q., Zhang, Y.: Realcustom++: Representing images as real-word for real-time customization. arXiv preprint arXiv:2408.09744 (2024) 8 [13] Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., Shan, Y.: T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In: Proceedings of the AAAI conference on artificial intelligence. vol. 38, pp. 42964304 (2024) [14] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023) [15] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023) [16] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 87488763. PmLR (2021) [17] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1068410695 (2022) [18] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2250022510 (2023) [19] Song, K., Zhu, Y., Liu, B., Yan, Q., Elgammal, A., Yang, X.: Moma: Multimodal llm adapter for fast personalized image generation. In: European Conference on Computer Vision. pp. 117132. Springer (2024) [20] Tan, Z., Liu, S., Yang, X., Xue, Q., Wang, X.: Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098 3 (2024) [21] Wang, Q., Li, B., Li, X., Cao, B., Ma, L., Lu, H., Jia, X.: Characterfactory: Sampling consistent characters with gans for diffusion models. arXiv preprint arXiv:2404.15677 (2024) [22] Wang, Q., Bai, X., Wang, H., Qin, Z., Chen, A., Li, H., Tang, X., Hu, Y.: Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519 (2024) [23] Wu, S., Huang, M., Wu, W., Cheng, Y., Ding, F., He, Q.: Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160 (2025) [24] Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023) [25] Zhai, X., Mustafa, B., Kolesnikov, A., Beyer, L.: Sigmoid loss for language image pre-training. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 11975 11986 (2023) [26] Zhang, Y., Yuan, Y., Song, Y., Wang, H., Liu, J.: Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027 (2025)"
        }
    ],
    "affiliations": [
        "Hunyuan, Tencent",
        "InstantX Team"
    ]
}