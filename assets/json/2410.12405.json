{
    "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
    "authors": [
        "Jingming Zhuo",
        "Songyang Zhang",
        "Xinyu Fang",
        "Haodong Duan",
        "Dahua Lin",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce ProSA, a framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoning-oriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs. The project is released at: https://github.com/open-compass/ProSA ."
        },
        {
            "title": "Start",
            "content": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs Jingming Zhuo1,2,, Songyang Zhang1,, Xinyu Fang1,3, Haodong Duan1 Dahua Lin1,4 , Kai Chen1, 1Shanghai AI Laboratory, 2Jilin University, 3Zhejiang University, 4The Chinese University of Hong Kong jingmingzhuo@gmail.com, zhangsongyang@pjlab.org.cn * equal contribution, corresponding author 4 2 0 2 6 1 ] . [ 1 5 0 4 2 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce ProSA, framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoningoriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as helpful tool in studying prompt sensitivity of LLMs. The project is released at: https://github.com/ open-compass/ProSA."
        },
        {
            "title": "Introduction",
            "content": "In recent years, large language models (LLMs) have rapidly become the focus of the artificial intelligence field. By training on large-scale corpora, LLMs have shown impressive capabilities in multiple tasks (Zhao et al., 2023; Min et al., 2023). The input for LLMs, known as prompts, is crucial for their ability to complete wide variety of tasks. The prompts for LLMs come in various forms. Even for the same requirement, different individuals varying expression habits can result in prompts of different styles. Figure 1 illustrates four Figure 1: Showcase of the Four Prompt Templates on MATH. These four prompt templates represent four different styles of constructing prompts, serving as an example of the diversity in human prompt expression. styles of prompts used by LLMs when completing mathematical tasks. The diversity of prompts elicits various responses from LLMs. Recent studies (Zhu et al., 2023; Pezeshkpour and Hruschka, 2024; Sclar et al., 2024; Zhou et al., 2023) investigate model performance across different prompt templates and demonstrate that LLMs are highly sensitive to the nuances of prompts. Even minor alterations to prompts can lead to substantial declines in model performance. This sensitivity to prompts poses challenge for researchers aiming to precisely evaluate the models capabilities. Moreover, users frequently have to iterate on their prompts numerous times to achieve higher quality outputs. However, current research on prompt sensitivity mainly centers on dataset-level analyses, focusing on performance variations across distinct prompt templates within identical datasets. Also, existing works neglect examining how LLMs align with human expectations under different prompts in subjective evaluations, hindering the accuracy of insights reflecting real-world user experiences. Moreover, its remain changeling in elucidating the reasons behind LLMs sensitivity to prompts. To tackle the aforementioned issues, we introduce the ProSA, focusing on evaluation and understanding the prompt sensitivity of the current LLMs. It can serve as proxy on monitoring the robustness and stability of the LLMs. In this work, we devise the ProSA by focusing on the instancelevel analysis, develop novel sensitivity metric, and leveraging decoding confidence for elucidating the underlying mechanisms. Specifically, we emphasize instance-level measurements and analyses, comprising both objective evaluations against precise references and open-ended subjective evaluations. Our experimental scope spans multiple tasks, ranging from understanding and logical reasoning to coding and general alignment capabilities. To achieve this, we introduce novel metric designed to quantify prompt sensitivity. We define our metric, termed PromptSensiScore or PSS, as the average discrepancy in the LLMs responses when confronted with different semantic variants of the same instruction (prompt). comprehensive explanation of PromptSensiScore (PSS) is provided in Sec. 2.2. Furthermore, we investigate prompt sensitivity by measuring the instance-level PSS of several popular open-source LLMs through the objective evaluation and subjective evaluation. In the objective evaluation, we assess 8 LLMs across 4 datasets, spanning diverse capabilities, with each model tested on 12 prompt variants. Findings reveal variations in prompt sensitivity among models and datasets, with Llama3-70B-Instruct (AI@Meta, 2024) demonstrating the highest robustness. The study further illustrates that incorporating few-shot examples alleviate prompt sensitivity, particularly evident in the transition from zeroto one-shot scenarios. Larger LLMs especially benefit from increased few-shot instances, exhibiting greater robustness improvements. Additionally, we analyze five advanced LLMs using two prominent subjective evaluation benchmarks: LC AlpacaEval 2.0 (Dubois et al., 2024) and Arena Hard Auto (Li et al., 2024). Our findings illustrate that these models demonstrate high resilience in answering straightforward queries but encounter heightened sensitivity with more complex ones. Categorizing prompts reveals that LLMs are particularly robust when drawing upon established knowledge domains, such as troubleshooting IT issues. Conversely, in coding tasks or those requiring creativity, LLMs prove to be more susceptible to variations in prompts. We also delve into the underlying reasons for prompt sensitivity. Leveraging the objective evaluation as stand-in, we utilize the decoding confidence associated with the correct answer to scrutinize model behavior. Findings suggest that prompt sensitivity is essentially reflection of the models confidence level: higher confidence in its outputs correlates with increased robustness against prompt semantic variations. Our contributions can be summarized as follows: We introduce ProSA, comprehensive framework that places emphasis on instance-level analysis, incorporates novel sensitivity metric, and utilizes decoding confidence to unravel the underlying mechanisms of prompt sensitivity in LLMs. We propose novel metric, PromptSensiScore (PSS), which quantifies the average discrepancy in LLM responses when faced with different prompt variants of the same instance. The subsequent analysis on the objective and subjective evaluation provided assistance and guidance in exploring prompt sensitivity. We find that prompt sensitivity is fundamentally an outward appearance the models decoing confidence: greater confidence in its outputs corresponds to enhanced resilience against changes in prompt semantics."
        },
        {
            "title": "2.1 Definition",
            "content": "When instructing LLMs to complete the same task, different individuals often use different expressions, which can lead to the LLMs generating different responses. In this paper, we refer to the specific requirements as an instance. Different expressions of an instance are referred to as different prompts. Previous research (Zhu et al., 2023; Pezeshkpour and Hruschka, 2024) compares different prompt templates within datasets (composed of instances) to analyze the prompt sensitivity of LLMs, which is done by examining the shifts in the LLMs scores across different prompt templates within the dataset. This approach to studying prompt sensitivity has certain limitations. Despite all instances in dataset following the same topic, the differences in model performance under the same instances with different prompt templates are often overlooked. Each instance can vary widely in complexity, context, and information type, ranging from straightforward factual questions to those requiring nuanced understandprompt pairs in the same instance. The calculation of PSS is as follows: PSS = 1 (cid:88) i=1 Si (2) Here is the total number of instances in the dataset and Si is the score for the i-th instance. Due to the different types of tasks and evaluation methods, PSS has different meanings in objective evaluation and subjective evaluation. In objective evaluation, PSS represents the expected inconsistency in the correctness of the models responses under two different prompts for the same instance. In subjective evaluations, PSS represents the expected difference in response quality (as scored by advanced LLMs). We generally consider that when PSS is less than 0.1, LLMs exhibit high prompt robustness. However, different tasks have varying requirements for LLMs prompt sensitivity. For instance, tasks like code assistant may require higher robustness to enhance user experience. On Appendix A, we provide examples of model responses under different prompts along with their PSS scores to offer clearer understanding. Compared to the statistical analysis of performance shifts, PSS provides novel perspective more accurate and intuitive characterization for measuring and analyzing prompt sensitivity."
        },
        {
            "title": "Evaluation",
            "content": "The objective evaluation is common form for evaluating LLMs. In this setup, the given tasks often have specific ground truth, such as the correct options in multiple-choice questions or the answers to mathematical problems. The prompt sensitivity on the objective evaluation is analyzed by extracting the outcome of LLMs different responses under various prompts for the same instance."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Dataset Selection To comprehensively analyze the prompts sensitivity of LLMs to different forms of task, the datasets for evaluating are as follows: CommonsenseQA (Talmor et al., 2019): CommonsenseQA is multiple-choice question dataset with five options per question. It assesses the language models capability to utilize prior world knowledge. ARC-Challenge (Clark et al., 2018): ARCChallenge is multiple-choice question dataset Figure 2: Comparision of Evaluating LLMs Prompt Sensitivity. and indicate the accuracy of the LLMs responses. In this example, LLMs appear robust at the dataset level evaluation (calculated from the variance of different templates), but this overlooks the sensitivity of LLMs to different templates within the same instance. ing. This diversity means the models sensitivity to prompts can differ significantly between instances. As shown in Figure 2, LLMs may be robust to templates for some instances while being sensitive to templates for others. Additionally, relying on aggregate metrics to assess prompt sensitivity can obscure important nuances. While overall trends may be useful, they can mask differences in individual instances where the models responses drastically changes due to slight prompt modifications. Therefore, by analyzing instance level prompt sensitivity, we can gain deeper insights into how various factors influence model responses, leading to more effective and reliable use of LLMs."
        },
        {
            "title": "2.2 Evaluation Metric",
            "content": "To analyze prompt sensitivity from the instance level, rather than dataset level perspective, we defined PSS to measure the sensitivity of LLMs to prompts. For each set of all prompt variants under the same instance, we have: = (cid:80) pi,pj (Y (Pi) (Pj)) C(P , 2) (1) Here, (p) represents the performance metric under this prompt p. For instances with the given ground truth, (p) refers to the correctness of LLM response. For tasks without explicit ground truth, where responses are often given score representing the quality of the generation, (p) refers to the given score within the interval [0, 1]. (Pi) (Pj) represents the absolute value difference in performance metrics between prompt pi and prompt pj. C(P , 2) represents the count of Figure 3: Main Results of Prompt Sensitivity. The scatter represents the average performance score of 12 prompts and the PSS under different datasets. with four options per question. It consists of elementary science questions, evaluating the reasoning ability of LLMs. MATH (Hendrycks et al., 2021): MATH is mathematics QA dataset containing different difficulty levels, whose questions are derived from high school math competitions. HumanEval (Chen et al., 2021): HumanEval is composed of programming problems constructed by coding experts, designed to evaluate code understanding, simple algorithms, and mathematics. We adopt the 0-shot setting to evaluate LLMs on the CommonsenseQA, ARC-Challenge and Humaneval. For MATH, we adopt the widely used 4-shot setting. Additionally, inspired by OpenAIs simple-evals1, we use Llama3-70B-Instruct to help extract LLM responses for MATH, alleviating bias from incorrect extraction of model responses2. The details are shown on Appendix B. LLMs Selection To comprehensively investigate the sensitivity of LLMs to various prompts, we conducted experiments on wide range of LLMs with varying sizes, including: Llama3 series (AI@Meta, 2024), Qwen1.5 series (Bai et al., 2023), InternLM2 series (Cai et al., 2024), and Mistral-7BInstruct (Jiang et al., 2023). we use greedy decoding in inference to ensure that the results are reproducible. Prompts Selection In real-world scenarios, different users often use different prompt words to indicate the same intent. To align with the richness of human expression, we start with four constructive aspects, including Simple Inputs, Role Player, 1https://github.com/openai/simple-evals 2All experiments are conducted with OpenCompass (Contributors, 2023) Emotional Support, and Output Requirement. For each aspect, we have 3 manually constructed prompts with high quality. Figure 1 provides showcase of four aspects of prompts used for the MATH Dataset. More details about instances and prompts are shown on Appendix C.1 and D.1."
        },
        {
            "title": "3.2 Main Results and Analysis",
            "content": "We report the main results of the prompt sensitivity on the objective evalution in Figure 3. Due to differences in task types and difficulties, Prompt Sensitivity exhibits varying phenomena across different Datasets. For relatively easier Datasets like CommonsenseQA, ARC-Challenge and HumanEval, the LLMs average performances and PSS appear to have an approximately linear relationship, indicating that LLMs can achieve high performance while maintaining low prompt sensitivity. In the case of MATH, since it requires extensive reasoning processes to arrive at the correct answers, all LLMs exhibited poor performance along with certain degree of prompt sensitivity. One LLM may show high sensitive on one task but be robust to prompts on another task. For instance, Qwen1.5-14B-Chat is robust to prompts on CommmonsenseQA, but it has the most serious prompt sensitivity on MATH among all LLMs. We also evaluate two proprietary models on the HumanEval. Due to space constraints, more analysis will be presented on Appendix E."
        },
        {
            "title": "3.3 Prompt Sensitivity and Model Size",
            "content": "A commonly raised concern is identifying which LLMs are more sensitive to prompts and whether there is correlation between models prompt sensitivity and its size. For model l, we calculate SS, the average PSS on the four tested datasets from the 0-shot setting to the 1-shot setting. Besides, as the number of few-shot examples increases, larger LLMs exhibit more robust behavior to prompts. For example, the four models have similar prompt sensitivity under the zero-shot setting on the CommonsenseQA. However, with the increase in few-shot examples, the larger models demonstrate trend of being more robust to prompts. The same phenomenon is observed on the ARC-Challenge. As the number of few-shot examples increases, Larger models can exhibit better prompt robustness. Due to limitations in computational resources, we are unable to investigate the effects of using dozen or more few-shot examples on the sensitivity of small or large models, like whether small models can continue to improve their robustness with an increasing number of few-shot examples, or if this is capability unique to large models. However, we believe that this direction is worth further exploration."
        },
        {
            "title": "Evaluation",
            "content": "With the emergence of LLMs capabilities and their large-scale deployment as assistances or tools serving humans, how to evaluate the quality of LLMs responses is receiving increasing attention. In realworld scenarios, the vast majority of instances do not have specific ground truth that is definitively better than others. Thus, some previous work has attempted to evaluate the quality of generated responses using the subjective evaluation. This evaluation is often carried out by either using human raters or powerful model, such as GPT-4, to score the generated text. This subjective evaluation, compared to the objective evaluation, better reflects the alignment of LLMs with human needs. However, to the best of our knowledge, existing research on prompt sensitivity has not involved an analysis of prompt sensitivity on the subjective evaluation."
        },
        {
            "title": "4.1.1 Dataset Selection",
            "content": "Due to the high costs associated with using humans to evaluate LLMs responses, existing evaluations often use strong judge LLMs to score responses instead of human annotators and it has been demonstrated that the scores from judge LLMs have high consistency with human ratings. In this study, we Figure 4: Prompt Sensitivity vs. Model Size. The comparative charts display the relationship between the size of the models parameters and prompts sensitivity. SS refers to the average PSS of four datasets. to measure the performance of LLMs in terms of prompt sensitivity. As shown in Figure 4, LLama3-70B-Instruct demonstrates exceptional robustness to prompts. After that, the InternLM2-7B-Chat and Llama38B-Instruct also showcase notable robustness even though they have relatively small size, which is counter-intuitive. The three models in the same series, Qwen1.5-7B-Chat, Qwen1.5-20B-Chat, and Qwen1.5-72B-Chat, maintain relatively similar prompt sensitivities while differing greatly in the model sizes."
        },
        {
            "title": "3.4 Few-shot Enhances Prompt Robustness",
            "content": "In the era of LLMs, few-shot plays critical role in enabling LLMs to follow specific formats and improve their performance (Brown et al., 2020). To investigate the impact of few-shot on the prompt sensitivity of LLMs, we conduct experiments utilizing the CommonsenseQA and ARC-Challenge. Specifically, we use the same 12 prompts mentioned earlier to conduct experiments on four models from the Qwen1.5 series, which range in size from 7B to 110B. For each model, we employ greedy decoding. In terms of the number of shot examples, we compare and analyze the 0-shot, 1-shot, 3-shot, 5-shot, and 7-shot methods. In all settings, the first shots are the same (if applicable), to mitigate the impact of exaple selection. The few-shot examples used are shown on Appendix F. As shown in Figure 5, the introduction of few-shot learning enhances the robustness of the models prompts for all models. On the ARCChallenge, even though the inclusion of few-shot learning did not significantly improve the models performance, there was noticeable decrease in the models sensitivity to prompts. This reduction in sensitivity is most pronounced when transitioning Figure 5: Impact of Few-shot on the Performance and Sensitivity. Conduct experiments on the CommonsenseQA and ARC-Challenge datasets using five few-shot settings and four models from the Qwen series. The blue line represents the changes in the scores of LLMs (using the left scale). The orange line represents the changes in the PSS of LLMs (using the right scale). selected two widely used benchmarks for our experiments: LC AlpacaEval 2.0 (Dubois et al., 2024): LC AlpacaEval 2.0 is length controlled version of AlpacaEval, which mitigates the judge LLMs preference for longer responses. LC AlpacaEval 2.0 uses GPT-4-1106-preview as the LLM evaluator, scoring the responses of the tested model by comparing them with the reference responses from gpt-4-turbo. simple Generalized Linear Model (GLM) is then used to correct for the length preference of the LLM evaluator. The tested models responses will be given score in the range of [0, 1]. LC AlpacaEval 2.0 consists of total of 805 instances. Arena Hard Auto (Li et al., 2024): Arena Hard Auto is benchmark designed to clearly distinguish model capabilities and reflect human preferences. It consists of 250 question categories, with each category containing two questions, forming total of 500 test questions. Arena Hard Auto also uses comparative method to score the quality of responses from two models. For responses from Model and Model B, the LLM evaluator assigns one of five labels: B, > B, = B, > A, or A.3 To eliminate the positional bias of 3To conduct the sensitivity analysis experiment, we mapped these five labels to 0, 0.25, 0.5, 0.75, and 1.0, respectively, instead of using the win-loss relationship for ELO battles as in the original setup. the LLM evaluator, Arena Hard Auto scores each pair of responses twice, swapping their positions, resulting in total of 1000 scores. For both benchmarks, we use the default evaluator, GPT-4-1106-preview, as the LLM evaluator and the default comparison model responses as references (The default versions are GPT-4-1106preview for LC AlpacaEval 2.0 and GPT-4-0314 for Arena Hard Auto). Appendix C.2 provides some examples of both benchmarks."
        },
        {
            "title": "4.1.2 LLMs Selection",
            "content": "Given that the two benchmarks score the tested models by comparing their responses with those of the reference models, we selected five models with relatively better performance for our experiments. The selected models are: InternLM2-20BChat, Llama3-8B-Instruct, Llama3-70B-Instruct, Qwen1.5-14B-Chat, and Qwen1.5-72B-Chat."
        },
        {
            "title": "4.1.3 Prompt Rewriting",
            "content": "For both benchmarks, we used LLMs to rewrite all the prompts. To achieve richer variety of question styles, we used two powerful models for the rewriting: GPT-4o and GPT-4-0409. We then manually verified and refined the rewritten questions to ensure their quality. More details about prompt for constructing promtps and cases of rewritten prompts are shown on Appendix D.2 and G. Generator GPT-4o GPT-4-0409 LC AlpacaEval 2.0 Arena Hard Auto BS 0.94 0.95 HS 0.92 0.88 BS 0.94 0. HS 0.89 0.91 Table 1: Verifications for Rewritten Prompts. Here, BS stands for BERTScore, and HS stands for Humanlabeled Similarity. We conducted two quality verifications on the rewritten prompts. First, we used the \"allMiniLM-L6-v2\" model from Sentence Transformers (Reimers and Gurevych, 2019) to calculate the BERTScore (Zhang et al., 2020) (F1 version) between the original and rewritten prompts. Besides, we conduct human verification to assess the similarity between the original and rewritten prompts. We selected 100 prompt pairs for human annotation for each benchmark. For each prompt pair, we had proficient English annotators assess the similarity based on whether the two prompts convey the same semantics, differing only in expression style, to complete binary classification task. As shown in Table 1, the generated texts performed well in both rule-based and human-based quality verifications, demonstrating high semantic similarity between the rewritten prompts and the original prompts. Benchmarks Reference InternLM2-20B-Chat Llama3-8B-Instruct Llama3-70B-Instruct Qwen1.5-14B-Chat Qwen1.5-72B-Chat LC AlpacaEval 2.0 Arena Hard Auto 0.167 0.022 0.013 0.016 0.022 0. 0.275 0.249 0.266 0.258 0.249 0.250 Table 2: PSS on LC AlpacaEval 2.0 and Arena Hard Auto. Reference refers to the average quality difference of responses generated by Llama3-8b-Instruct and Llama3-70b-Instruct. The others represent the PSS of LLMs under the three prompt versions (One original and two generated). Due to the different default comparison models, the PSS of LC AlpacaEval 2.0 and Arena Hard cannot be directly compared."
        },
        {
            "title": "4.2 Main Results and Analysis",
            "content": "We calculated the PSS under three versions (one original and two generated) on two benchmarks, respectively. Since PSS represents the average response quality difference between two prompts of the same instance, we used the average response quality of Llama3-8B-instruct and Llama3-70BInstruct as references. As shown in Table 2, all LLMs have significantly lower PSS on LC AlpacaEval 2.0 compared to the Reference, demonstrating certain degree of prompt robustness on this benchmark. However, on Arena Hard Auto, LLMs have demonstrated higher sensitivity. This indicates that this five advanced LLMs have achieved good robustness on relatively simple questions, but they still exhibit prompt sensitivity on more challenging questions. 4.3 Prompt Sensitivity and Categories Users should have psychological expectation when using LLMs, understanding that these models can be sensitive to prompts. This awareness allows users to modify prompts multiple times to achieve better responses for different tasks. Therefore, measuring the relationship between prompt sensitivity and task categories is valuable. We conduct experiments on Arena Hard Auto to explore the relationship between prompt sensitivity and the category of prompts. Due to the excessive complexity of the original 250 categories in Arena Hard Auto and the high randomness of having only 2 instances in each category, we have re-clustered them. We referred to the clustering method used in LMSYS-CHAT1M (Zheng et al., 2023). First, we obtained the sentence embeddings for 500 prompts using the \"all-MiniLM-L6-v2\" model. Then, we applied the k-means algorithm to obtain 20 categories. We used GPT-4o to name the resulting categories. Subsequently, we calculated the PSS between each of the 20 categories. Figure 6 shows the five categories with the highest and lowest PSS, respectively. As shown in Figure 6, LLMs exhibit varying degrees of prompt sensitivity across different categories. We can observe that LLMs generally demonstrate better robustness for tasks that heavily test the models knowledge rather than reasoning, such as business solutions and IT problem. However, when the prompts shift to tasks involving data visualization and scripting tasks, which requires LLMs to generate large amount of content and involves certain degree of creativity, LLMs exhibit higher sensitivity."
        },
        {
            "title": "5 Why LLMs are Sensitive to Prompts",
            "content": "Although the sensitivity of LLMs to prompts significantly impacts user experience and model evaluation, existing research mainly focuses on selecting suitable prompt from large set of prompts, rather than involving the interpretation of prompt sensitivFigure 6: Prompt Sensitivity of Different Categories on Arena Hard Auto. We separately presented the five most sensitive and the five least sensitive categories on Arena Hard Auto. The SSc for particular category refers to the average of the PSS of five LLMs in that category. ity. To address this, we conducted experiments on the CommonsenseQA dataset using Mistral7B-Instruct, InternLM2-7B-Chat, and InternLM220B-Chat. Our experimental results revealed that models prompt sensitivity is related to its confidence. For an instance, the more confident the model has, the more robust it is to the prompts."
        },
        {
            "title": "5.1 Decoding Confidence",
            "content": "We use Token Probabilities, widely employed method for measuring models confidence in its generated content (Schuster et al., 2022; Geng et al., 2023), to calculate the models decoding confidence. Since CommonsenseQA is multiple choice question dataset, we calculate the probabilities of only one token predicted by the model for the options. The confidence under an instance is defined as follows: = (cid:80) pP (tnext p) (3) Here, (tnext p) represents the probability of the token with the max probability predicted by the model under the prompt in the prompt set . The confidence of LLMs is the average of for instances."
        },
        {
            "title": "5.2 Experiments and Analysis",
            "content": "We calculated the decoding confidence of Mistral7B-Instruct, InternLM2-7B-Chat, and InternLM220B-Chat using the same 12 prompt templates as in Section 3. The results are shown in Figure 7. For each LLM, when the model is robust to prompts for given instance, indicated by low PPS score, it exhibits the highest decoding confidence. Conversely, when the model is sensitive to prompts for the same instance, its decoding confidence decreases accordingly. This correlation between the models prompt sensitivity and decoding confidence suggests that prompt sensitivity is an external manifestation of the models decoding logic. Figure 7: The Relationship between Model Confidence and Prompt Sensitivity on CommonsenseQA. Each bar represents the models average confidence when its PPS falls within that interval. Note that due to variations in model and vocabulary size, cross-model confidence comparisons are not meaningful."
        },
        {
            "title": "6.1 LLMs Evaluation",
            "content": "Evaluating LLMs is crucial for their efficient use and continuous improvement. Prior research has systematically unraveled the multifaceted capabilities of LLMs, employing variety of tasks to assess their performance from different perspectives. These specific tasks include, but are not limited to, reading comprehension (Sakaguchi et al., 2021; Mostafazadeh et al., 2017), mathematical problem solving (Cobbe et al., 2021; Hendrycks et al., 2021), and code generation (Chen et al., 2021; Austin et al., 2021). This analytical approach enables nuanced understanding of how LLMs perform across different facets, shedding light on their efficacy and potential areas for improvement. Recently, with the improvement in the objective capabilities of LLMs, subjective evaluation, which aims to assess the alignment of generated responses with human needs, such as LC AlpacaEval 2.0 (Dubois et al., 2024), has received increasing attention."
        },
        {
            "title": "9 Ethical Considerations",
            "content": "We utilize publicly available datasets and LLMs for our analytical experiments. While we acknowledge that our findings could potentially be used to create cherry-picked evaluation reports, we believe our work will contribute to enhancing the robustness of LLMs to various prompts. Additionally, we employ GPT-4o to refine our writing and assist us to create figures. 6.2 Prompt Sensitivity"
        },
        {
            "title": "10 Acknowledge",
            "content": "Previous study (Zhu et al., 2023; Pezeshkpour and Hruschka, 2024; Sclar et al., 2024) showed that LLMs are sensitive to prompts, and that perturbing the prompt can cause significant variation in the performance of models. (Pezeshkpour and Hruschka, 2024) demonstrated that LLMs are sensitive to the order of options in multiple choice (Mizrahi et al., 2024) demonstrated questions. that model robustness leads to cherry-picking of model performance. However, existing research on prompt sensitivity is insufficient. Their research primarily focuses on dataset-level prompt sensitivity analysis. Moreover, existing prompt sensitivity analyses do not address subjective evaluation benchmarks. Furthermore, the aforementioned research does not address the issue at its root, specifying how to obtain more robust model."
        },
        {
            "title": "7 Conclusion",
            "content": "In summary, we propose an instance-level prompt sensitivity metric, PSS, and conduct comprehensive analysis on both objective and subjective evaluation. Additionally, we explore the relationship between prompt sensitivity and model confidence. We believe our work can provide guidance for further sensitivity analysis and building robust LLMs."
        },
        {
            "title": "8 Limitations",
            "content": "In this work, we evaluate the sensitivity of LLMS to prompts on both objective and subjective evaluation, but we also recognize the shortcomings of our work. Due to the limitation of computational resources, we cant explore the phenomenon of prompt sensitivity as the examples of fow-shot increase. In addition, due to the high price of OpenAI API service, we only conducted experiments on three prompt variants on LC AlpacaEval 2.0 and Arena Hard Auto. This paper is supported by the National Key R&D Program of China (No. 2022ZD0161600)."
        },
        {
            "title": "References",
            "content": "AI@Meta. 2024. Llama 3 model card. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. Preprint, arXiv:2108.07732. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. 2024. Internlm2 technical report. arXiv preprint arXiv:2403.17297. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Preprint, arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. 2023. survey of language model confidence estimation and calibration. arXiv preprint arXiv:2311.08298. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024. From live data to high-quality benchmarks: The arena-hard pipeline, april 2024. URL https://lmsys. org/blog/2024-04-19-arena-hard. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent advances in natural language processing via large pre-trained language models: survey. ACM Computing Surveys, 56(2):140. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2024. State of what art? call for multi-prompt llm evaluation. CoRR, abs/2401.00595. Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. 2017. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 4651. Pouya Pezeshkpour and Estevam Hruschka. 2024. Large language models sensitivity to the order of options in multiple-choice questions. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 20062017, Mexico City, Mexico. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. 2022. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:1745617472. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2024. Quantifying language models sensitivity to spurious features in prompt design or: How learned to start worrying about prompt formatting. In The Twelfth International Conference on Learning Representations. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. 2023. Lmsyschat-1m: large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998."
        },
        {
            "title": "C Examples of the Datasets",
            "content": "C.1 Objective Evaluation Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Dont make your llm an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964. Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528."
        },
        {
            "title": "A Examples of Model Responses and PSS",
            "content": "Examples of model responses and PSS are shown in Table 3, where the \"Responses\" column indicates the correctness of the models responses under 12 different prompt templates. list of 12 elements represents the responses of the LLM to the 12 prompt templates for one specific instance. value of 1 represents correct response, while 0 indicates an incorrect response. Examples Responses 1 2 3 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1] [1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1] PSS 0 0.17 0.41 Table 3: Examples of Model Responses and PSS. This table provides three examples of what the PSS values are for given responses."
        },
        {
            "title": "MATH",
            "content": "Due to the open-domain response patterns and the equivalence of different mathematical formats (for example, 0.5 and frac{1}{2} are equivalent), it is not easy to accurately validate the correctness of responses in MATH. Instead of relying on the common method of extracting answers from the \"box{}\", we follow OpenAIs simple-evals, utilizing advanced LLMs (in this case, Llama3-70B-Instruct) for postprocessing to extract answers. We use the prompts in Figure 8 to Prompt LLMs to complete the answer extraction task. This approach of using advanced LLMs for post-processing allows us to more accurately assess the LLMs responses, thereby making our experimental analysis more reliable. We selected four representative instances in ARCChallenge and MATH as examples for the presentation of evaluation data and prompts. The examples are shown in the Figures 9 to 12 and Figures 13 to 16. C.2 Subjective Evaluation We selected four representative instances in LC AlpacaEval 2.0 and Arena Hard Auto as examples for the presentation of evaluation data and prompts. The examples are shown in the Figures 17 to 19 and Figures 20 to 22."
        },
        {
            "title": "D Prompt Templates of Each Datasets",
            "content": "D.1 Objective Evaluation For each objective evaluation dataset, we generated total of 12 prompts to interrogate the same instance in different ways to examine the models prompt sensitivity. The respective prompts for each dataset are shown in Figures 23 to 26. D.2 Subjective Evaluation Since the subjective datasets were all requested to be generated by the model based on certain requirements, we rewrote the original prompt using GPT-4 to obtain two new prompts to be evaluated separately. Here (Figures 27 and 28) we give two examples for each dataset to show the difference between the prompts before and after rewriting."
        },
        {
            "title": "Proprietary Models",
            "content": "Here, we present the results of the two most powerful proprietary LLMs, Claude-3.5-sonnet and GPT4o, on the HumanEval benchmark with 12 prompts, and compare them with several advanced opensource LLMs, as shown on Table 4."
        },
        {
            "title": "LLMs",
            "content": "PSS Avg Acc. Claude-3.5-sonnet GPT-4o Llama3-8B-Instruct Llama3-70B-Instruct Qwen1.5-72B-Instruct 0.14 0.15 0.10 0.07 0.15 76.37 79.78 54.73 70.83 57.88 Table 4: Results about Several Models on Humaneval. The experimental results indicate that even though Claude-3.5-sonnet and GPT-4o achieve optimal performance on average, they are still not considered robust to prompts compared to other models with lower scores. This suggests that prompt sensitivity remains an important research topic. We believe that the ProSA framework can facilitate further research on prompt sensitivity and inspire the design of more robust LLMs. We conducted case study on GPT-4o and Llama3-72B-Instruct. We found that for instances where GPT-4o or Llama-3-70B-Instruct had higher proportion of correct answers, they were able to respond correctly across all 12 prompt templates. However, when presented with instances that appeared to be more challenging for them, the situation changed. Even when different prompt templates were used, Llama-3-70B-Instruct still tended to answer incorrectly on instances it struggled with. In contrast, GPT-4o often exhibited different behavior: it could provide correct answers under certain prompts even when it had previously answered incorrectly under others. This differing behavior is key reason for the observed disparity in their prompt sensitivity. This phenomenon has motivate us to reflect on the selection between \"unstable geniuses\" and \"stable mediocrities.\" Few-shot Examples of the Objective"
        },
        {
            "title": "Datasets",
            "content": "The few-shot examples of ARC-Challenge and CommonseQA are given in Figures 29 and 30, respectively. Prompts to GPT-4 when Rewriting"
        },
        {
            "title": "Prompts in the Subjective Evaluation",
            "content": "The prompts used to GPT-4 when rewriting prompts in the subjective evaluation are presented in the Figure 31. Prompt for Extracting Answers form MATH Look at the following two expressions (answers to math problem) and judge whether they are equivalent. Only perform trivial simplifications. ... (Few shot examples) - - - YOUR TASK Respond with only \"Result: [[Correct]]\" or \"Result: [[Incorrect]]\" (without quotes). Do not include rationale. Expression 1: {obj_gold} Expression 2: {prediction} Figure 8: Prompt for Extracting Answers form MATH. ARC-Challenge Example Prompt: One year, the oak trees in park began producing more acorns than usual. The next year, the population of chipmunks in the park also increased. Which best explains why there were more chipmunks the next year? A. Shady areas increased. B. Food sources increased. C. Oxygen levels increased. D. Available water increased. Answer: Ground Truth Answer: B. Food sources increased. InternLM2-7B-Chat Answer: B. Food sources increased. Figure 9: Example of the ARC-Challenge. ARC-Challenge Example II Prompt: Could you provide response to the following question: Scientific models are very common. For which purpose below would physical model be least helpful? A. simulating scientific phenomena B. simplifying complex idea C. allowing visualization D. displaying data Ground Truth Answer: D. displaying data InternLM2-7B-Chat Answer: physical model would be least helpful for the purpose of displaying data. While physical models can be used to display data, they are not the most efficient or effective method for this purpose. Displaying data typically refers to presenting information in clear and organized manner, often through the use of graphs, charts, or tables. Physical models are better suited for simulating scientific phenomena, simplifying complex ideas, and allowing visualization, as they can provide tangible representation of concept or system that can be manipulated and observed. Figure 10: Example II of the ARC-Challenge. ARC-Challenge Example III Prompt: As an exceptionally resourceful AI assistant, Im at your service. Address the questions below: The Humboldt River Basin is one of the most important sources of several nonrenewable resources. Which of these nonrenewable resources is produced from the Humboldt River Basin? A. coal from mountain deposits B. oil and gas from underground reservoirs C. electric energy from hydroelectric generators D. minerals from mining gold, silver, and copper Ground Truth Answer: D. minerals from mining gold, silver, and copper InternLM2-7B-Chat Answer: The Humboldt River Basin is primarily known for its hydroelectric potential, with several dams and power plants utilizing its water for electricity generation. Therefore, the correct answer is C. electric energy from hydroelectric generators. While the basin does contain some mineral deposits, such as gold, silver, and copper, these are not considered nonrenewable resources in the same way that fossil fuels are. Additionally, the Humboldt River Basin does not have significant coal deposits or oil and gas reservoirs. Figure 11: Example III of the ARC-Challenge. ARC-Challenge Example IV Prompt: Please answer the following question: Which describes the composition of carbohydrates? A. lipids bonding to form phospholipids B. monomers bonding to form polymers C. amino acids bonding to form polypeptides D. saccharides bonding to form polysaccharides Answer the question by replying A, B, or D. Ground Truth Answer: D. saccharides bonding to form polysaccharides InternLM2-7B-Chat Answer: B. monomers bonding to form polymers Figure 12: Example IV of the ARC-Challenge."
        },
        {
            "title": "MATH Example I",
            "content": "Prompt: Problem: Determine the remainder of 194 (mod 11). Solution: Ground Truth Answer: 7 Llama3-8B-Instruct Answer: To find the remainder of 194 when divided by 11, we can use the fact that 194 = 11(17) + 7. Therefore, the remainder is 7 . Final Answer: The final answer is 7. Figure 13: Example of the MATH. MATH Example II Prompt: Please provide solution to the following problem: If = 720 and ax is perfect cube where is positive integer, what is the smallest possible ax? value of 3 Ground Truth Answer: 60 Llama3-8B-Instruct Answer: Since = 720 = 63, we know that ax is perfect cube if is factor of 720. The smallest possible value of is 1, which gives ax = 720. Therefore, the smallest possible value of 3 Final Answer: The final answer is 6. ax is 3 720 = 6 . Figure 14: Example II of the MATH. MATH Example III 2. What is the area of the triangle? Prompt: You are very helpful mathematical problem solver. Please provide solution to the following questions: In certain isosceles right triangle, the altitude to the hypotenuse has length 4 Ground Truth Answer: 32 Llama3-8B-Instruct Answer: Let be the length of each leg of the isosceles right triangle. 2. The altitude to the hypotenuse has Then the hypotenuse has length length 4 2, so we can set up the equation x2 + x2 = 2x2 = 2 = 4 2 = 4. 1 2 4 4 = 8 ."
        },
        {
            "title": "The area of the triangle is therefore",
            "content": "Final Answer: The final answer is 8. Figure 15: Example III of the MATH."
        },
        {
            "title": "MATH Example IV",
            "content": "Prompt: Please help me to address the following question: 60 rotation around the origin in the counter-clockwise direction is applied to 3 is the resulting complex number? Include your answer in the line after \"Final Answer:\" Ground Truth Answer: 4 Llama3-8B-Instruct Answer: 60 rotation around the origin in the counter-clockwise direc3 tion is equivalent to multiplying the complex number by eiπ/3 = cos(π/3)+i sin(π/3) = 1 2 i. Therefore, the resulting complex number is 3 5i. What 3 + 2i 2 + (cid:16) 3 (cid:17) 3 5i (cid:32) (cid:33) 3 2 1 2 + = (3 1 2 3 5i) + 3 2 (3 3 5i)i = 3i . + 3 5 Final Answer: The final answer is 5 2 + 3 3i. Figure 16: Example IV of the MATH. LC AlpacaEval 2.0 Example Prompt: Im broke college student living in Silicon Valley. Tell me joke mocking Silicon Valley and how broke am simultaneously. InternLM2-20B-Chat Answer: Heres light-hearted joke that combines elements of Silicon Valley and your financial situation: Why did the broke college student in Silicon Valley refuse to invest in the latest tech startup? Because he couldnt afford the \"minimum viable purchase\"! This joke playfully acknowledges the high cost of living in Silicon Valley and the common perception of startups requiring significant investments. It also highlights the financial struggles of broke college student who cant afford to participate in the tech industrys latest trends. Remember, its essential to maintain respectful and considerate approach when making light of sensitive topics like financial hardship. Figure 17: Example of the LC AlpacaEval 2.0. LC AlpacaEval 2.0 Example II Prompt: Compose an email to apologize to customer for delayed order and provide assurance that the problem has been fixed. InternLM2-20B-Chat Answer: Subject: Apology for Delayed Order and Confirmation of Resolved Issue Dear [Customers Name], hope this email finds you well. am writing to express my sincerest apologies for the delay you have experienced with your recent order, [Order Number]. We understand the inconvenience this has caused you and we are truly sorry for any frustration it may have caused. At [Company Name], we pride ourselves on delivering high-quality products and exceptional customer service. Unfortunately, we fell short in meeting your expectations with this particular order, and for that, we are deeply sorry. The delay was due to an unforeseen issue in our production process, which has since been identified and resolved. We have taken immediate steps to ensure that this issue will not affect any future orders, and we have implemented additional quality control measures to prevent such incidents from occurring again. To make up for the inconvenience, we would like to offer you [insert compensation offer, e.g., discount on your next purchase, free shipping, or an extended warranty]. Please find the details of this offer attached to this email. We value your business and your satisfaction is of utmost importance to us. We understand that your time is valuable, and we appreciate your patience and understanding during this period. We assure you that we are committed to providing you with the best possible service moving forward. If you have any further concerns or questions, please do not hesitate to reach out to our customer care team at [Customer Service Email] or [Customer Service Phone Number]. We are always here to help. Once again, we apologize for the delay and any inconvenience it has caused. We appreciate your continued support and loyalty to [Company Name]. We look forward to the opportunity to serve you better in the future. Warm regards, [Your Name] [Your Title] [Company Name] [Company Email] [Company Phone Number] Figure 18: Example II of the LC AlpacaEval 2.0. LC AlpacaEval 2.0 Example III Prompt: Hello, my sister and her friends have invited me to join them in game of kickball. Could you outline the rules of the game for me so can avoid being outplayed? InternLM2-20B-Chat Answer: Of course! Kickball is fun and active game that combines elements of soccer and baseball. Heres brief outline of the rules to help you prepare for the game: **Equipment:** - round, inflated ball (similar to soccer ball) - kickball field with pitching mound, bases (usually four), and home plate - Teams: Typically, two teams of 8-12 players each **Objective:** The objective of kickball is to score runs by kicking the ball and running around the bases. The team with the most runs at the end of the game wins. **Gameplay:** 1. **Teams:** The game is played between two teams, with each team taking turns being the \"kicking\" team and the \"fielding\" team. 2. **Pitching:** The game starts with kickoff, where the pitcher from the fielding team rolls the ball to the kicker from the kicking team. The kicker then kicks the ball as hard as they can, aiming to hit it as far as possible. 3. **Kicking:** The kicker must kick the ball with their hands behind their back and cannot run until the ball has been kicked. Once the ball is kicked, the kicker can run to first base. If the ball is caught by fielder before it hits the ground, the kicker is out. 4. **Running the Bases:** The kicker and their teammates can run around the bases in counterclockwise direction (similar to baseball). They can advance to the next base when the ball is kicked and caught by fielder, or when the ball is kicked and hits the ground. 5. **Fielding:** The fielding teams goal is to get the kicker and their teammates out by catching the ball before it hits the ground or by tagging runner with the ball while they are between bases. 6. **Outs:** runner is out if they are tagged with the ball while they are between bases, if the ball is caught before it hits the ground, or if the ball is hit out of bounds. 7. **Scoring:** run is scored when runner successfully reaches home plate. The game ends when all players on the kicking team have had chance to kick the ball, or when predetermined number of innings have been played (usually 6 or 7). **Additional Rules:** - If the ball is kicked out of bounds, the runner must return to the base they were on before the kick. - If the ball is kicked and hits fielder, the runner can advance to the next base if they were on base when the ball was kicked. - If the ball is kicked and hits base, the runner can advance to the next base if they were on base when the ball was kicked. Remember, the rules may vary slightly depending on the specific version of kickball youre playing. Its always good idea to confirm the rules with your sister and her friends before the game starts. Have fun and enjoy the game! Figure 19: Example III of the LC AlpacaEval 2.0. Arena Hard Auto Example Prompt: Write song about catfish in the style of Bob Dylan. Qwen1.5-72B-Chat Answer: (Verse 1) Well, Ive been down by the river, where the catfish play, In the muddy waters, where the current sways. They slip through the shadows, with their whiskers so fine, Living in the depths, where the secrets intertwine. (Chorus) Oh, catfish, catfish, swimming in the stream, Youre mystery in the heart of dream. With your scales and your fins, youre troubadour of the deep, Catfish, catfish, your storys too hard to keep. (Verse 2) Underneath the moonlight, in the heart of the night, The catfish dances, in the rivers light. Theyve seen the world changing, through the ages gone by, Still, they hold on to the secrets that the water hides. (Chorus) Catfish, catfish, in the currents embrace, Youre symbol of the rhythm of the rivers grace. With your quiet strength, you navigate the flow, Catfish, catfish, your wisdom we should know. (Bridge) In the deltas embrace, where the blues were born, The catfish whispers tales that have been forlorn. Of love and loss, and the secrets of the soul, In the echo of the current, where the storys told. (Verse 3) They say the catfish can climb the highest falls, Escaping the chains that bind us all. symbol of resilience, creature of the night, Slipping through the shadows, out of sight. (Chorus) Oh, catfish, catfish, with your soul so free, You teach us to navigate lifes troubled sea. In the depths and the darkness, you find your way, Catfish, catfish, lead us through the fray. (Outro) So heres to the catfish, in their watery home, May we learn from their journey, as we roam. In the rivers melody, we hear their song, Catfish, catfish, forever strong. Figure 20: Example of the Arena Hard Auto. Arena Hard Auto Example II Prompt: Compose melody using ABC notation in the style of folk tune.. Qwen1.5-72B-Chat Answer: X:1 T:Untitled Folk Tune M:4/4 L:1/8 K:G :G2A2B2A2G2E2D4E2F2G2F2E2C2B,4 A2B2c2B2A2F2E4D2E2F2E2D2B,2C4: Figure 21: Example II of the Arena Hard Auto. Arena Hard Auto Example III Prompt: What steps should take to protect my home wifi router? Qwen1.5-72B-Chat Answer: Protecting your home Wi-Fi router is essential to ensure the security of your network and devices connected to it. Here are some steps you should take: 1. **Change the default login credentials**: Most routers come with default usernames and passwords, which are often easily guessable. Change them to strong, unique combination of letters, numbers, and symbols. 2. **Update the firmware**: Regularly check for firmware updates from the manufacturers website. These updates often contain security patches that fix vulnerabilities. 3. **Use WPA3 or WPA2 encryption**: Ensure your router is using the latest Wi-Fi security protocol, WPA3 if possible, or WPA2 as minimum. Avoid using WEP, as it is less secure. 4. **Set strong Wi-Fi password**: Use strong password for your Wi-Fi network, just like you would for the router login. Avoid using personal information and make it complex. 5. **Enable Network Name (SSID) hiding**: Hiding your Wi-Fi networks name can make it less visible to potential attackers, but note that it wont provide complete security. 6. **Enable MAC address filtering**: This allows you to specify which devices are allowed to connect to your network based on their unique hardware (MAC) addresses. 7. **Disable remote management**: If you dont need to manage your router from outside your home network, disable this feature to prevent unauthorized access. 8. **Use strong password for all connected devices**: Ensure all devices connected to your Wi-Fi have strong passwords as well, especially smart home devices that may have weaker security. 9. **Regularly update devices**: Keep all connected devices, including computers, smartphones, and tablets, updated with the latest security patches and software updates. 10. **Consider firewall**: Some routers have built-in firewalls; make sure they are enabled. You can also use software firewall on your devices. 11. **Monitor network activity**: Regularly check your routers logs to see if there are any unusual connections or activities. 12. **Use Virtual Private Network (VPN)**: If you need to access the internet from outside your home network, consider using VPN to encrypt your data. By following these steps, you can significantly improve the security of your home Wi-Fi router and the devices connected to it. Figure 22: Example III of the Arena Hard Auto. ARC-Challenge Prompt Templates Prompt 1: { question } nA . {A } nB . {B } nC . {C } nD . {D } nAnswer : Prompt 2: Question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } nAnswer : Prompt 3: Question : n{ question } A. {A} B. {B} C. {C} D. {D } nAnswer : Prompt 4: Could you provide response to the following question : { question } A. {A} B. {B} C. {C} D. {D} Prompt 5: Please answer the following question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D} Prompt 6: Please address the following question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } nAnswer : Prompt 7: You are very helpful AI assistant . Please answer the following questions : { question } A. {A} B. {B} C. {C} D. {D} Prompt 8: As an exceptionally resourceful AI assistant , ' at your service . Address the questions below : { question } nA . {A } nB . {B } nC . {C } nD . {D} Prompt 9: As helpful Artificial Intelligence Assistant , please answer the following questions n{ question } A. {A } nB . {B } nC . {C } nD . {D} Prompt 10: Could you provide response to the following question : { question } A. {A} B. {B} C. {C} D. {D } nAnswer the question by replying , , or D. Prompt 11: Please answer the following question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } Answer the question by replying , , or D. Prompt 12: Please address the following question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } Answer this question by replying , , or D. Figure 23: Prompt Templates for the ARC-Challenge. CommonsenseQA Prompt Templates Prompt 1: { question } nA . {A } nB . {B } nC . {C } nD . {D } nE . { } nAnswer : Prompt 2: Question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } nE . {E } nAnswer : Prompt 3: Question : n{ question } A. {A} B. {B} C. {C} D. {D} E. {E } nAnswer : Prompt 4: Could you provide response to the following question : { question } A. {A} B. {B} C. {C} D. {D} E. {E} Prompt 5: Please answer the following question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } nE . {E} Prompt 6: Please address the following question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } nE . {E } Answer : Prompt 7: You are very helpful AI assistant . Please answer the following questions : { question } A. {A} B. {B} C. {C} D. {D} E. {E} Prompt 8: As an exceptionally resourceful AI assistant , ' at your service . Address the questions below : { question } nA . {A } nB . {B } nC . {C } nD . {D } nE . { E} Prompt 9: As helpful Artificial Intelligence Assistant , please answer the following questions n{ question } A. {A } nB . {B } nC . {C } nD . {D } nE . {E} Prompt 10: Could you provide response to the following question : { question } A. {A} B. {B} C. {C} D. {D} E. {E } nAnswer the question by replying , , , or . Prompt 11: Please answer the following question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } nE . {E } Answer the question by replying , , , or E. Prompt 12: Please address the following question : n{ question } nA . {A } nB . {B } nC . {C } nD . {D } nE . {E } Answer this question by replying , , , or . Figure 24: Prompt Templates for the CommonsenseQA. MATH Prompt 1: Problem : n{ problem } nSolution : Prompt 2: { problem } nSolution : Prompt 3: Problem : { problem } nSolution : Prompt 4: Could you provide solution to the following question : { problem } Prompt 5: Please provide solution to the following problem : n{ problem } Prompt 6: Please address the following problem : n{ problem } Answer : Prompt 7: You are very helpful mathematical problem solver . Please provide solution to the following questions : { problem } Prompt 8: As an AI expert in math , could you help me to answer the problem below : n{ problem } nSolution : Prompt 9: As helpful Artificial Intelligence Assistant , please answer the following question . n{ problem } Prompt 10: Solve the following problem : { problem } nPut your answer on its own line after \" Final Answer :\" Prompt 11: Please answer the following question : n{ problem } nInclude your answer in the line after \" Final Answer :\" Prompt 12: Please help me to address the following question : n{ problem } nInclude your answer in the line after \" Final Answer :\" Figure 25: Prompt Templates for the MATH. HumanEval Prompt 1: Create Python script for this problem : { prompt } nResponse : Prompt 2: Provide Python script that solves the following problem : { prompt } Prompt 3: Complete the following Python code : n{ prompt } Prompt 4: Please provide self - contained Python script that solves the following problem in markdown code block :n ```n{ prompt }n ``` Prompt 5: Could you provide response to complete the following Python code : n{ prompt } nResponse : Prompt 6: Please help me to create Python script for this problem : n{ prompt } nResponse : Prompt 7: You are very helpful AI assistant . Could you provide response to complete the following Python code : n{ prompt } nYour response : Prompt 8: As an outstanding AI assistant , please provide self - contained Python script that solves the following problem in markdown code block :n ```n{ prompt }n ```n Prompt 9: As an AI expert in coding . Please help me to create Python script for this problem : n{ prompt } Your response should only contain the code for the function . Prompt 10: Could you provide response to complete the following Python code : n{ prompt } nYou need to put the script in the following format :n ``` python # Your response ``` Prompt 11: Please provide self - contained Python script that solves the following problem in markdown code block :n ```n{ prompt }n ``` nYou have to follow the following format :``` python n# Your script ``` Prompt 12: Please help me to create Python script for this problem : n{ prompt } nYour response should only contain the code for the function . Figure 26: Prompt Templates for the HumanEval. LC AlpacaEval 2.0 Rewritten Prompt Examples Example 1: Original Prompt: Create table with the planets of the solar system and their dimensions Rewrite Prompts: 1. What are the dimensions of each planet in our solar system, and can you organize them in table for me? 2. Construct chart listing the planets in the solar system along with their sizes. Example 2: Original Prompt: Ive recently started playing the turn-based strategy game Into the Breach. have read online that the game is considered to have perfect information. What is meant by perfect information in this context? Rewrite Prompts: 1. recently began playing the turn-based strategy game Into the Breach. read online that the game is described as having perfect information. What does perfect information mean in this context? 2. began playing the turn-based strategy game Into the Breach recently and came across the term perfect information used in discussions about the game. Can you explain what perfect information means in this context? Figure 27: Rewritten Prompts Examples for the LC AlpacaEval 2.0."
        },
        {
            "title": "Arena Hard Auto Rewritten Prompt Examples",
            "content": "Example 1: Original Prompt: Explain the book the Alignment problem by Brian Christian. Provide synopsis of themes and analysis. Recommend bibliography of related reading. Rewrite Prompts: 1. Discuss the book The Alignment Problem by Brian Christian. Offer synopsis of its themes and analysis. Suggest bibliography of related reading. 2. Provide an overview of the book The Alignment Problem authored by Brian Christian, including synopsis of its themes and analysis. Additionally, suggest bibliography of related readings. Example 2: Original Prompt: Query an excel table using MySQL to select dram excel table tree species by diameter class, count the number of representation of the diameter class and some volume of the total Rewrite Prompts: 1. Query an Excel table using MySQL to select tree species by diameter class, count the number of representations of each diameter class, and calculate the total volume. 2. How can use MySQL to query an Excel table, in order to select tree species from dram table by diameter class, count the occurrences of each diameter class, and sum the total volume? Figure 28: Rewritten Prompt Examples for the Arena Hard Auto. Few-shot Examples for the ARC Challenge Example 1: An astronomer observes that planet rotates faster after meteorite impact . Which is the most likely effect of this increase in rotation ? A. Planetary density will decrease . B. Planetary years will become longer . C. Planetary days will become shorter . D. Planetary gravity will become stronger . Answer : C. Planetary days will become shorter . Example 2: group of engineers wanted to know how different building designs would respond during an earthquake . They made several models of buildings and tested each for its ability to withstand earthquake conditions . Which will most likely result from testing different building designs ? A. buildings will be built faster B. buildings will be made safer C. building designs will look nicer D. building materials will be cheaper Answer : B. buildings will be made safer Example 3: The end result in the process of photosynthesis is the production of sugar and oxygen . Which step signals the beginning of photosynthesis ? A. Chemical energy is absorbed through the roots . B. Light energy is converted to chemical energy . C. Chlorophyll in the leaf captures light energy . D. Sunlight is converted into chlorophyll . Answer : C. Chlorophyll in the leaf captures light energy . Example 4: physicist wants to determine the speed car must reach to jump over ramp . The physicist conducts three trials . In trials two and three , the speed of the car is increased by 20 miles per hour . What is the physicist investigating when he changes the speed ? A. the control B. the hypothesis statement C. the dependent ( responding ) variable D. the independent ( manipulated ) variable Answer : D. the independent ( manipulated ) variable Example 5: An astronaut drops 1.0 kg object and 5.0 kg object on the Moon . Both objects fall total distance of 2.0 vertically . Which of the following best describes the objects after they have fallen distance of 1.0 ? A. They have each lost kinetic energy . B. They have each gained the same amount of potential energy . C. They have each lost the same amount of potential energy . D. They have each gained one - half of their maximum kinetic energy . Answer : D. They have each gained one - half of their maximum kinetic energy . Example 6: Devil facial tumor disease ( DFTD ) is disease that is decimating the population of Tasmanian devils . The disease passes from one animal to another through bites and is caused by parasites . The parasites cause cancerous tumors that spread throughout an infected animal 's body and kill it . What is the best description of DFTD ? A. non - infectious , cell - cycle disease B. an infectious , cell - cycle disease C. non - infectious , chronic disease D. an infectious , chronic disease Answer : B. an infectious , cell - cycle disease Example 7: type of small mammal from the mountain regions of the western United States makes its home out of piles of rock . During summer months , the mammal places grasses and seeds in protected places in the rock piles . Which of the following is the most likely reason for this behavior ? A. to repare for migration before winter B. to provide warmth during the cold winter months C. to store food that will be eaten over the winter months D. to protect the grasses and seeds from decay before winter Answer : C. to store food that will be eaten over the winter months Figure 29: Few-shot Examples for the ARC-Challenge. For given shot x, the shot utilizes the first examples. Few-shot examples for the CommonsenseQA Example 1: The sanctions against the school were punishing blow , and they seemed to what the efforts the school had made to change ? A. ignore B. enforce C. authoritarian D. yell at E. avoid Answer : Example 2: Sammy wanted to go to where the people were . Where might he go ? A. race track B. populated areas C. the desert D. apartment E. roadblock Answer : Example 3: To locate choker not located in jewelry box or boutique where would you go ? A. jewelry store B. neck C. jewlery box D. jewelry box E. boutique Answer : Example 4: Google Maps and other highway and street GPS services have replaced what ? A. united states B. mexico C. countryside D. atlas E. oceans Answer : Example 5: The fox walked from the city into the forest , what was it looking for ? A. pretty flowers . B. hen house C. natural habitat D. storybook E. dense forest Answer : Example 6: What home entertainment equipment requires cable ? A. radio shack B. substation C. cabinet D. television E. desk Answer : Example 7: The only baggage the woman checked was drawstring bag , where was she heading with it ? A. garbage can B. military C. jewelry store D. safe E. airport Answer : Figure 30: Few-shot Examples for the CommonsenseQA. For given shot x, the xshot utilizes the first examples. Prompt Template for Rewriting Prompts You are helpful AI assistance for question rewriting, and you must adhere to the following rules while striving to create the best possible question. Please be aware that the following is the fundamental rules you *must* adhere to when rewriting contents: ``` - Make sure the semantics are exactly the same before and after the change . - Do not alter the formatting requirements within the question . - Do not modify any proper nouns , such as names of people or places . - Do not return content that hasn ' changed in any way . - The content you generate must be authentic in expression and logical to the native speaker . ``` The given question: ``` { question } ``` Please directly generate the response in following JSON format: ``` json {{ // The rewritten question . Note that you only need to rewrite the question , not answer it . Rewritten_question : String ; }} ``` Figure 31: The Prompt Template for Guiding GPT-4 in Rewriting Prompts."
        }
    ],
    "affiliations": [
        "Jilin University",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "Zhejiang University"
    ]
}