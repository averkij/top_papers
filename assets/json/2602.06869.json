{
    "paper_title": "Uncovering Cross-Objective Interference in Multi-Objective Alignment",
    "authors": [
        "Yining Lu",
        "Meng Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence. To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 9 6 8 6 0 . 2 0 6 2 : r Uncovering Cross-Objective Interference in Multi-Objective Alignment Yining Lu 1 Meng Jiang"
        },
        {
            "title": "Abstract",
            "content": "We study persistent failure mode in multiobjective alignment for large language models (LLMs): training improves performance on only subset of objectives while causing others to degrade. We formalize this phenomenon as crossobjective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence. To explain this phenomenon, we derive local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with global convergence analysis under the Polyak Łojasiewicz condition, establishing when nonconvex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties. 1. Introduction Existing approaches to multi-objective LLM alignment predominantly build on reinforcement fine-tuning (RFT) methods (Ouyang et al., 2022; Bai et al., 2022; Lambert et al., 2025; Shen et al., 2025). These methods commonly reduce the multi-objective problem to optimizing single scalar objective through scalarization, applying either static weights (Kimi et al., 2025) or dynamic weights (Lu et al., 2025) at 1Department of Computer Science and Engineering, University of Notre Dame, USA. Correspondence to: Yining Lu <ylu33@nd.edu>, Meng Jiang <mjiang2@nd.edu>. Preprint. February 9, 2026. 1 the reward- (Guo et al., 2024) or gradient-level (Li et al., 2025). Despite its simplicity and popularity, we observe persistent and underexplored failure mode: scalarized training frequently fails to improve all objectives simultaneously. Instead, the model continues making progress on subset of easy objectives while others degrade, pattern we formalize as cross-objective interference. To investigate whether this phenomenon is an artifact of naive implementations or fundamental limitation, we turn to the rich literature of Multi-Task Learning (MTL) and Multi-Objective Optimization (MOO). We evaluated broad set of well-established algorithms with known convergence properties, spanning both rewardand gradient-level scalarization algorithms. Reward-level scalarization includes linear weighting (Barrett & Narayanan, 2008), Lagrangian primal-dual formulation (Mahdavi et al., 2013), Tchebycheff scalarization (Bowman, 1976), PAMA (He & Maghsudi, 2026), and more recent dynamic weighting (Lu et al., 2025). Gradient-level approaches aggregate per-objective gradients to unified update direction via MGDA (Desideri, 2009) or GradNorm (Chen et al., 2018). To our knowledge, this is the first systematic evaluation of classic scalarization algorithms for multi-objective LLM alignment.1 Our results reveal that all evaluated methods suffer from the cross-objective interference issue when applied to certain models (e.g., Figure 1a and Figure 1b). Critically, this occurs even when objectives are not fundamentally conflicting under traditional gradient-based definitions from MTL and MOO (Evgeniou & Pontil, 2004; Liu et al., 2021; SHI et al., 2023; Kim et al., 2025).2 This finding suggests the failure mode is model-dependent and runs deeper than existing MOO theories on linear scalarization (Lu et al., 2023), convexity (Wei & Niethammer, 2021), gradient conflict (Sener & Koltun, 2018), and generalization tradeoffs (Chen et al., 2023), as these theories are developed for simplified settings rather than LLM alignment. To address this gap, we develop theoretical framework analyzing multi-objective alignment through first-order improvement conditions. Beginning with the classic policy gradient algorithm (Sutton et al., 1999; Hu et al., 2025), we derive reward-level local covariance law that precisely 1We provide pseudocode of each algorithm in Appendix A. 2Analysis is presented in Figure 4 in Appendix B.1. Uncovering Cross-Objective Interference in Multi-Objective Alignment (a) Qwen2.5-1.5B-Base (b) Qwen2.5-1.5B-IFT (c) Qwen3-1.7B-Base Figure 1. Multi-objective alignment under different scalarization algorithms. We report moving-averaged test performance along training for three objectives: accuracy, conciseness, and clarity (left to right). We aim to train models with strong problem-solving ability (higher accuracy), computational efficiency (fewer response tokens), and clear reasoning processes (higher clarity). Results are shown for three models trained on the Math500 dataset with different scalarization algorithms adapted from MTL and MOO. Our method, CTWA, effectively mitigates cross-objective interference compared to others. Competing methods either quickly sacrifice accuracy to achieve superficially high conciseness and clarity (e.g., GradNorm in 1a, Linear and Dynamic weighting in 1b), or trying to maintain high accuracy while overlooking the improvment of others (e.g., Lagrangian in 1a and PAMA in 1b). In contrast, CTWA achieves strong, balanced performance across all three objectives. For instance, in 1c, CTWA maintains the highest accuracy without any degradation while achieving competitive conciseness and clarity. Even when CTWAs accuracy is slightly lower than Lagrangians (e.g., at training step 500 in 1a and 1b), it still surpasses all other methods and excels on both conciseness and clarity. characterizes when an objective improves under scalarized alignment: when its true reward exhibits positive covariance with the scalarized score. This explains why cross-objective interference happens: objectives that are easy to optimize can dominate the training, inducing negative covariance for harder objectives and causing them to degrade even as the overall scalarized return increases. We then extend this analysis to clipped surrogate objectives used in modern RFT, such as GRPO (Shao et al., 2024), demonstrating that under mild conditions, the first-order covariance law remains valid despite clipping. This theoretical analysis directly motivates our method, Covariance Targeted Weight Adaptation (CTWA), which monitors covariance between each objectives true reward and the scalarizationinduced (clipped) advantage weight, and adjusts weights to maintain positive covariance for all objectives. While local covariance analysis provides conditions for objective improvement, it cannot explain why some models consistently exhibit cross-objective interference while others can optimize all objectives under identical training procedures (e.g., Qwen3-1.7B-Base in Figure 1c). To address this fundamental problem, we study the global geometry of scalarized RFT using the PolyakŁojasiewicz (PL) inequality, which accommodates non-convex objectives. We derive sufficient conditions under which the scalarized RFT objective satisfies µ-PL inequality, yielding concrete, model-aware mechanism for when cross-objective interference arises: (i) the policy assigns insufficient probability mass to the optimal trajectory, (ii) the scalarization yields weak reward margins between optimal and suboptimal trajectories, or (iii) token-level gradient contributions cancel due to the ill-conditioned Jacobian mapping parameters to logits. Together, these perspectives explain why crossobjective interference is both algorithmic (covariance misalignment) and architectural (unfavorable geometry), provid2 Uncovering Cross-Objective Interference in Multi-Objective Alignment ing actionable insights for robust multi-objective alignment for LLMs. In summary, our contributions are threefold: conflict-averse updates (Kim et al., 2025), aim to improve all objectives while respecting KL or safety constraints. Systematic empirical study: We provide the first systematic evaluation of classic MOO and MTL scalarization algorithms for LLM alignment, revealing common crossobjective interference issue that varies across different models in multi-objective alignment. Local improvement theory and method: We derive reward-level local covariance law characterizing firstorder conditions for objective improvement (4.1), extend it to clipped surrogate objectives (4.3), and propose CTWA to mitigate cross-objective interference (5). Global convergence analysis: We analyze scalarized RFT under the PL condition, establishing sufficient conditions for global convergence and explaining crossobjective interference via model geometric properties, thus laying theoretical foundations for future work (6). 2. Related Work 2.1. Multi-Task Learning: Gradient Conflicts and Solutions MTL addresses joint training across multiple losses, where negative transfer often arises from conflicting gradients and imbalanced loss scales. Common solutions include adaptive weighting like GradNorm (Chen et al., 2018), directly modifying gradients such as PCGrad (Yu et al., 2020), CAGrad (Kim et al., 2025), Recon (SHI et al., 2023)), and Gradient Vaccine (Wang et al., 2021). While these MTL methods provide valuable insights for gradient-level control, they are developed primarily for supervised learning settings with convex or well-behaved loss structures. 2.2. Multi-Objective Optimization: Scalarization and Pareto Optimality MOO seeks Pareto-optimal solutions by balancing multiple objectives. Classic approaches rely on scalarization, either through linear or nonlinear schemes, to reduce MOO to single-objective optimization. foundational gradientbased approach is MGDA (Desideri, 2009), which computes common descent direction by solving minimum-norm problem over the convex hull of objective gradients. Recent extensions include PMGDA (Zhang et al., 2024), which incorporates user preferences, and PAMA (He & Maghsudi, 2026), which adapts the minimum-norm optimization to LLM alignment. Recent work has refined Pareto stationarity concepts (Hu & Yu, 2025) and explored regimes where multiple objectives can facilitate optimization (Efroni et al., 2025; Dann et al., 2023). In multi-objective RL, constrained approaches, such as CA-NPG (Gu et al., 2025) and related However, classical MOO theory typically assumes convex objectives or Pareto sets, which fail in LLM alignment where autoregressive models yield non-convex policy spaces. While Lu et al. (2023) analyze when linear scalarization can fully recover Pareto fronts in principle, they require strong non-determinism and numerical stability conditions for algorithmic success. We bridge this gap by developing theoretical analysis tailored to RFT from both local improvement and global convergence perspectives. 2.3. Multi-Objective LLM Alignment Most multi-objective alignment studies reduce multiple rewards to scalar objective through reward-level scalarization, including static linear scalarization (Wu et al., 2023; Zhang & Zuo, 2025; Yao et al., 2025), dynamic weighting (Lu et al., 2025), Lagrangian relaxation (Moskovitz et al., 2024), or Tchebycheff scalarization and its variants (Steuer & Choo, 1983; Lin et al., 2024). These methods can be extended to produce steerable policies that adapt to user preferences (Basaklar et al., 2023; Wang et al., 2024; Xie et al., 2025). Alternatively, gradient-level scalarization constructs update directions directly in parameter space, such as GAPO (Li et al., 2025), though such approaches remain less explored due to their high computational cost. Despite this rich set of approaches, to our knowledge, no prior work has formalized the cross-objective interference issue or explained why it occurs even when objectives are not fundamentally antagonistic. 2.4. Optimization Challenges in Reinforcement Fine-tuning LLM RFT faces several optimization challenges beyond multi-objective settings. Lagrangian dynamics can become unstable when convexity assumptions fail (Feijer & Paganini, 2010). LLM-specific challenges include vanishing gradients (Razin et al., 2024), sensitivity to importance weighting and normalization (Zheng et al., 2025; Liu et al., 2026), and exploration difficulties (Jiang et al., 2025). Recent RL scaling laws further suggest that optimization performance varies with model size (Khatri et al., 2025). While prior work studies single-objective RL from these different perspectives, we explore new optimization challenge in the multi-objective setting and answer the question of how to improve all objectives simultaneously. 3. Preliminaries In this section, we establish notations used throughout and review the multi-objective RL for LLM alignment. Uncovering Cross-Objective Interference in Multi-Objective Alignment Notation. Following the notations from Razin et al. (2024), let be the dataset and be finite token vocabulary. We then define Lin as the space of input prompts of length Lin, and Lout the space of output sequences of length Lout. We study objectives and for given input prompt = (x1, x2, , xLin ) Lin and generated completion = (y1, . . . , yLout ) Lout , the reward function is : Lin Lout RM . RFT as contextual bandit. We model RFT of language models as horizon-one (bandit) environment, where each input is state and each output is an action that the model can take. An autoregressive language model with parameters θ Rn induces probability distribution pθ( x) over completions of length Lout via pθ(y x) = = Lout(cid:89) l=1 Lout(cid:89) l= (cid:0)yl x, yl1 (cid:1) pθ softmax(cid:0)f (x, yl1; θ)(cid:1) , yl where yl1 := (y1, y2, , yl1) is partial completion, (x, yl1; θ) RX is the logits for the distribution of the next token at position l. For each objective {1, . . . , }, define the expected objective reward rm(pθ) := ExD Eypθ(x) (cid:2)rm(x, y)(cid:3). Scalarization. We convert vector reward in RM to scalar score via scalarization map Ψ : RM and define the per-sample scalar score s(x, y) := Ψ(r(x, y)). The induced value function for the input thus is (x; θ) := Eypθ(x) (cid:2) s(x, y) (cid:3), (1) and the overall RFT objective is to maximize (θ) := ExD (cid:2)V (x; θ)(cid:3). If = 1 and is the identity function, the above objective reduces precisely to the single-objective RFT. 4. Local Covariance Laws for Multi-Objective"
        },
        {
            "title": "Policy Improvement",
            "content": "In this section, we establish sufficient conditions under which optimizing scalarized score s(x, y) guarantees firstorder improvement in objectives rm(x, y). We begin by analyzing KL-regularized improvement step in distribution space (4.1) coupled with toy example (4.2), and extend the analysis to clipped surrogate objectives used in modern RFT (4.3). We defer all proof to Appendix C. 4.1. KL-Regularized Policy Improvement in Distribution Space For fixed x, write pθ;x() := pθ( x). For each prompt x, define the KL-regularized improvement step in distribution space: p+ θ;x := arg max q(X Lout ) (cid:110) Eyq (cid:2)s(x, y)(cid:3) KL(cid:0)qpθ;x (cid:1)(cid:111) , 1 η (2) where η > 0 is stepsize and (X Lout ) is the simplex over completions. Lemma 4.1. The optimizer of Equation (2) is pθ;x(y) exp(cid:0)η s(x, y)(cid:1) exp(cid:0)η s(x, y)(cid:1)(cid:105) . (cid:104) Eypθ;x p+ θ;x(y) = Theorem 4.2 (First-order local covariance law). Assume rm(x, y) is bounded and there exists η0 > 0 such that, for (cid:2) exp(η s(x, y))(cid:3) < . all Lin and η η0, Eypθ;x Then for each objective {1, . . . , }, θ ) rm(pθ) = rm(p+ ηExD (cid:104) Covypθ(x) (cid:0)rm(x, y), s(x, y)(cid:1)(cid:105) + O(η2). (3) Consequently, if (cid:104) ExD Covypθ(x) (cid:0)rm(x, y), s(x, y)(cid:1)(cid:105) > 0, θ ) > rm(pθ) for sufficiently small η > 0. then rm(p+ Remark 4.3. Equation (3) tells that optimizing the scalar score improves objective at first order when completions with higher s(x, y) also tend to have higher rm(x, y), leading to positive covariance averaged across prompts. Conversely, negative covariance brings local tradeoff where increasing necessarily decreases rm at first order. Because covariance is computed on-policy, its sign can flip over training as pθ moves. Therefore, the scalarized update may improve objective early in training but degrade it later (e.g., accuracy in Figure 1a), even when objectives are not inherently conflicting on the global Pareto front. For linear scalarization sλ = (cid:80) λjrj, the condition becomes Cov(rm, sλ) = (cid:80) λjCov(rm, rj), making crossobjective interference issue more concrete: emphasizing an easy objective can flip the sign for harder one when their rewards are weakly or negatively correlated on-policy. 4.2. Two-Mode Toy Example: When Scalarization Hurts an Objective We analyze minimal setting where the completion distribution places most of its mass on two modes (e.g., two distinct styles or solutions that the model frequently samples). Formally, for fixed prompt x, we idealize the completion 4 Uncovering Cross-Objective Interference in Multi-Objective Alignment space by two canonical outputs Y(x) = {ygood, ybad}, where good and bad are defined with respect to particular objective m, not the scalar score. Let pt := pθt;x(ybad) so that 1 pt = pθt;x(ygood) and define sgood := s(x, ygood), rgood := rm(x, ygood), sbad := s(x, ybad), rbad := rm(x, ybad). in modern RFT. The same corollaries and proofs apply to PPO-style clipped surrogate objectives, with the only change being the advantage estimation. Fix prompt and sample group of completions y(1), . . . , y(K) pθold( x). Let sj := s(x, y(j)) and define the group normalization Ai(x) := sis(x) . The ˆσ(x) GRPO surrogate objective is Optimizing concentrates probability on what favors. By Lemma 4.1, after one KL-regularized improvement step, the probability of ybad mode becomes pt+1 = pt eηsbad pt eηsbad + (1 pt) eηsgood , and the log-odds update as log pt+1 1 pt+1 = log pt 1 pt + η(cid:0)sbad sgood (cid:1). (4) (cid:34) (cid:88) J(θ) := Lout(cid:88) min k=1 l=1 (cid:110) ρk,l(θ)Ak(x), ρk,l(θ)Ak(x) (cid:35) (cid:111) β E(cid:2)KL(pθpref )(cid:3) + λ E(cid:2)H(pθ)(cid:3), where β, λ 0 control KL and entropy regularization. ρk,l(θ) is the clipped result of importance ratio ρk,l(θ), ρk,l(θ) = clip(cid:0)ρk,l(θ), 1 ε, 1 + ε(cid:1). We know that the unclipped indicator selecting the active branch of the minimum is Thus, if the proxy score ranks the bad mode higher, sbad > sgood, then the log-odds increase linearly in t, and pt 1 (i.e., the update drives mass toward ybad). 1k,l(θ) = 1, Ak(x) 0 and ρk,l(θ) 1 + ε, 1, Ak(x) < 0 and ρk,l(θ) 1 ε, 0, otherwise. (6) Biased objective can decrease monotonically over steps. Assume the objective prefers ygood mode, i.e., rgood > rbad. Then the expected objective for input is Recall (x, yl1; θ) RX is the next-token logit map and pθ( x, yl1) = softmax(f (x, yl1; θ)). Define the tokenwise logit-gradient feature Eypθt (x) (cid:2)rm(x, y)(cid:3) = (1 pt) rgood + pt rbad = rgood pt (rgood rbad), ϕl(x, y; θ) := θ log pθ (cid:0)yl x, yl (cid:1), and the clipped advantage weight which is strictly decreasing in pt. Combining with Equation (4) yields simple cross-objective interference: if the scalarized score favors the mode that is worse for objective (sbad > sgood) while objective prefers the other mode (rgood > rbad), then training increases pt at each step and E[rm(x, y)] decreases monotonically toward rbad. The covariance law flags the failure immediately. In this two-mode example, the conditional covariance has closed form: (cid:0)rm(x, y), s(x, y)(cid:1) Covypθt (x) = pt(1 pt) (rgood rbad) (sgood sbad). (5) Under the interference configuration sbad > sgood and rgood > rbad, Equation (5) is negative when pt (0, 1). Therefore, our local covariance law predicts that, for sufficiently small η, the KL-regularized improvement step that increases must decrease the true objective rm at first order. Wk,l(θ) := Ak(x) ρk,l(θ) 1k,l(θ). (7) Lemma 4.4. Let rm(θ) := rm(pθ) and assume rm has an Lm-Lipschitz gradient around θ. For any direction d(θ) = J(θ) and update θ+ = θ + ηd(θ), we have rm(θ+)rm(θ) η rm(θ), d(θ) Lm η2d(θ)2, m. In particular, if rm(θ), d(θ) 0 and η > 0 is sufficiently small, then rm(θ+) rm(θ). Theorem 4.5 (Fisher-covariance sufficient condition for natural gradient updates). For completions y(1), . . . , y(K) pθold( x), write ϕk,l(θ) := ϕl(x, y(k); θ). Define the aggregated Fisher matrix as (cid:34) (cid:16) (cid:88) (θ) := Lout(cid:88) (cid:17)(cid:16) (cid:88) ϕk,l(θ) Lout(cid:88) (cid:17)(cid:35) , ϕk,l(θ) k=1 l=1 k= l=1 and the weighted feature mean and regularizer gradient 4.3. Clipped Surrogate Objectives: Gradient Structure and Corollaries In this section, we use GRPO as running example to derive sufficient conditions for per-objective improvement (cid:34) (cid:88) G(θ) := Lout(cid:88) k=1 l=1 Wk,l(θ) ϕk,l(θ) (cid:35) , R(θ) := β θE(cid:2)KL(pθpref )(cid:3) λ θE(cid:2)H(pθ)(cid:3), 5 Uncovering Cross-Objective Interference in Multi-Objective Alignment where Wk,l(θ) is defined in Equation (7). Assume (θ) is invertible and consider the natural gradient direction For each objective m, define the unclipped first-order margin dnat(θ) := (θ)1(cid:0)G(θ) R(θ)(cid:1), θ+ = θ + η dnat(θ). γunclip (θ) := rm(θ)F (θ)1(cid:0)Gunclip(θ) R(θ)(cid:1). If for every m, rm(θ)F (θ)1G(θ) rm(θ)F (θ)1R(θ), (8) and moreover either dnat(θ) = 0 or the inequality holds with positive margin Then the clipped first-order margin satisfies rm(θ)F (θ)1(cid:0)Gclip(θ) R(θ)(cid:1) γunclip (cid:13) (cid:13) (cid:13)F (θ)1/2rm(θ)(cid:13) (cid:13) (cid:13)F (θ)1/2(cid:0)Gunclip(θ) Gclip(θ)(cid:1)(cid:13) (cid:13). (θ) Consequently, if γunclip (θ) κm > 0 and γm(θ) := rm(θ)F (θ)1(cid:0)G(θ) R(θ)(cid:1) > 0, m, (cid:13)F (θ)1/2(cid:0)Gunclip(θ)Gclip(θ)(cid:1)(cid:13) (cid:13) (cid:13) κm (cid:13)F (θ)1/2rm(θ)(cid:13) (cid:13) (cid:13) , then for all sufficiently small η > 0 we have rm(θ+) rm(θ) for all and strictly if minm γm(θ) > 0. Corollary 4.6 (Categorical bandit case). Fix prompt and suppose the policy over completions Lout is categorical distribution parameterized by logits θ RX Lout pθ(y x) = exp(θy) exp(θy) . (cid:80) Let w(x, y) be an arbitrary scalar weight assigned to each completion (i.e., per-sample quantity whose expected value Eypθ(x)[w(x, y)] we seek to maximize via natural gradient ascent). Define the categorical Fisher matrix (θ) := Eypθ(x) (cid:2)θ log pθ(y x)θ log pθ(y x)(cid:3). Let dnat(θ) be any natural gradient direction and take the update θ+ = θ + ηdnat(θ) with learning rate η > 0. Then for each objective m, we have Eypθ+ (x) (cid:2)rm(x, y)(cid:3) Eypθ(x) (cid:2)rm(x, y)(cid:3) = ηCovypθ(x) (cid:0)rm(x, y), w(x, y)(cid:1) + O(η2). Specifically, Cov(cid:0)rm(x, y), w(x, y)(cid:1) 0 for all is sufficient first-order condition to ensure that no objective degrades. Corollary 4.7 (Clipping robustness). Define the unclipped and clipped weights for each token wunclip k,l (θ) := Ak(x)ρk,l(θ), wclip k,l (θ) := wunclip k,l (θ)1k,l(θ), where 1k,l(θ) {0, 1} is defined in Equation (6) and 1k,l(θ) = 0 exactly when the term is clipped away and contributes zero gradient. Let Gunclip(θ) := (cid:34) (cid:88) k=1 (cid:34) (cid:88) Gclip(θ) := Lout(cid:88) l=1 Lout(cid:88) (cid:35) wunclip k,l (θ)ϕk,l(θ) , (cid:35) k,l (θ)ϕk,l(θ) . wclip k= l=1 6 then rm(θ)F (θ)1(cid:0)Gclip(θ) R(θ)(cid:1) 0, so the sufficient condition for rm(θ+) rm(θ) in Theorem 4.5 still holds for objective under clipping. k,l Remark 4.8. Corollary 4.7 shows that clipping can only affect first-order improvement guarantee by removing subset of weighted logit-gradient terms wunclip (θ) ϕk,l(θ) from the update. The Fisher-distortion (θ)1/2(Gunclip(θ) Gclip(θ)) therefore measures, in natural gradient geometry, how much gradient mass is deleted by clipping. This distortion becomes large when many importance ratios fall outside the clipping window, especially on samples with large magnitude advantages. Importantly, this can be carefully controlled in practice through techniques such as learning rate scheduling or reward normalization, ensuring that clipping distortion remains small and the covariance law continues to hold. 5. Covariance Targeted Weight Adaptation Motivated by the discussions in Section 4, we propose Covariance Targeted Weight Adaptation (CTWA), plug-andplay controller that adapts scalarization weights to maintain sufficiently large covariance between each objective reward rm(x, y) and the clipped advantage weight w(x, y) induced by the underlying PPO-style update. Algorithm 1 outlines the full procedure with GRPO as the example. 5.1. Implementation of CTWA CTWA is compatible with any differentiable scalarization function. For concreteness, following Lu et al. (2025), we use the weighted sum and update weights λm online, sλ(x, y) := (cid:88) m= λm rm(x, y), λm > 0. Let wclip k,l (θ) denote the tokenwise clipped advantage weight (defined in Corollary 4.7). We compute it under the updated policy θ on the sampled completions. We aggregate wclip k,l (θ) over tokens to obtain completion-level weight: Uncovering Cross-Objective Interference in Multi-Objective Alignment (cid:16) x, y(k); θ (cid:17) :="
        },
        {
            "title": "1\nLout",
            "content": "Lout(cid:88) l=1 wclip k,l (θ). For each prompt and objective m, CTWA first computes the within-prompt empirical covariance (cid:16) (cid:100)Covm(x) := Covk=1K rm(x, y(k)), w(x, y(k); θ) (cid:17) , then averages across prompts in the batch cm := Ex in batch (cid:2) (cid:100)Covm(x)(cid:3). Figure 2. Scalarization weights in log space (um) during training of Qwen3-1.7B-Base. In practice, CTWA treats the covariance as diagnostic of whether the induced update direction benefits objective m, and uses it to adjust λm for the next policy update. To enforce covariance safety margin, it runs an exponential moving average (EMA) of this signal and increases the scalarization weight when the smoothed covariance falls below predefined threshold. Specifically, we maintain an EMA of the batch covariance and define nonnegative deficit δm cm (1 τ )cm + τ cm, δm := (cid:2)c cm (cid:3) +. To ensure λm > 0 and obtain stable multiplicative updates, we parameterize λm = exp(um) and update in log-space: um um + ηλ δm, λm exp(um). 5.2. Experiments We evaluate our proposed method against existing baselines on the Math500 dataset (Lightman et al., 2024) using different pretrained models including Qwen2.5-1.5B-Base and its instruction-finetuned version Qwen 2.5-1.5B-IFT (Qwen et al., 2025), and Qwen3-1.7B-Base (Yang et al., 2025). We optimize three objectives: accuracy, conciseness, and clarity. We assume all objectives are equally important and initialize their weights λm to [0.333, 0.333, 0.334] respectively. We set the predefined covariance targets for the three objectives to = [0.15, 0.08, 0.08]. Each objective is evaluated using heuristic rules that produce verifiable rewards (0 or 1). For easier analysis, we report accuracy and clarity as reward scores, and conciseness as response length. The main results trained on the REINFORCE algorithm without clipping are shown in Figure 1. Additional results for GRPO with clipping are reported in Figure 5 (Appendix B.1), which yield similar findings and thus empirically validate Corollary 4.7. Below, we provide additional experimental results.3 We analyze the effectiveness of CTWA by tracking the weight evolution for each objective in Figure 2 and the 3We provide training parameters for all algorithms in Appendix B.2 and open-source code to reproduce our results: github.com/yining610/ctwa. Figure 3. Covariance cm between reward and clipped advantage weight for each objective during training of Qwen3-1.7B-Base. controlled covariance in Figure 3. As shown in Figure 2, the weight for accuracy grow exponentially faster than those for conciseness and clarity, suggesting two key insights: (1) intuitively, accuracy is more challenging objective to optimize, requiring greater attention from the scalarization mechanism, and (2) from the covariance perspective, maintaining larger covariance margin for accuracy (0.15) leads to more aggressive weight updates to match the target. Figure 3 confirms that all three objectives maintain positive covariance with the clipped advantage weights, with accuracy exhibiting larger gap than conciseness and clarity, consistent with our predefined covariance targets. Note that CTWA is also more computationally efficient than strong baselines such as dynamic weighting or MGDA, as it avoids computing per-objective gradients or performing projected gradient descent at each step. The required covariance components can be computed alongside the standard RFT process with negligible additional overhead. 6. Global Convergence of Multi-Objective Alignment via µ-PL Condition While Section 4 establishes first-order improvement conditions that characterize when individual objectives improve locally, it does not explain the model dependence we observe in practice. To study this, we move from local improvement to the global optimization geometry of the scalarizationUncovering Cross-Objective Interference in Multi-Objective Alignment induced value function (x; θ) in Equation (1). Because (x; θ) is highly non-convex in θ and the autoregressive parameterization restricts the feasible policy set {θ} to non-convex subset of the simplex, classical convex optimization analysis is inapplicable here. We therefore seek conditions under which the scalarized RFT objective exhibits benign non-convexity, geometric structure that still yields meaningful convergence guarantees despite nonconvexity. The PolyakŁojasiewicz (PL) condition provides exactly such framework. Definition 6.1. Let : Rn be differentiable objective function that we aim to maximize. Define the set of global maximizers Θ := arg maxθRn (θ), and assume Θ = . We say that satisfies the µ-PL condition with parameter µ > 0 on set Ω Rn if 1 2 (cid:13)θV (θ)(cid:13) (cid:13) 2 (cid:13) µ(cid:0)V (θ) (θ)(cid:1), θ Ω, θ Θ (9) In words, the PL condition ties gradient magnitude to global suboptimality, such that any point with small gradient norm must have an objective value close to the global maximum, even though needs not be convex. We now introduce interpretable conditions on the scalar score and the LM parameterization that together yield µ-PL inequality for the scalarized value. This enables us to analyze second mechanism behind cross-objective interference: even when the scalarized value provides welldefined ascent direction, optimization can stall near suboptimal solutions that favor easily optimized objectives if the model geometry is unfavorable (small µ). The full proof is provided in the Appendix. Assumption 6.2 (Bounded score and unique optimal completion). There exists > 0 such that s(x, y) for all Lout. Moreover, there exists unique maximizer = arg maxy s(x, y) and margin > 0 such that s(x, y) s(x, y) for all = y. Assumption 6.3 (Non-saturation for suboptimal policy). There exists ϵ (0, 1) such that for every suboptimal parameter θ (i.e., (x; θ) < (x; θ)), token probabilities are bounded away from 1: (cid:0)yl x, yl1 (cid:1) 1 ϵ, {1, . . . , Lout}, yl . pθ Theorem 6.5. (µ-PL condition of multi-objective alignment) For parameters θ Rn and input Lin , define the scalarized multiobjective value function (x, θ) as in Equation (1). If the scalarization function and policy θ meet the Assumptions 6.2-6.4, then it holds that: 1 θV (x; θ)2 µ(V (x; θ) (x; θ)), with µ = 1 2B (cid:16) pθ(y x) 1 pθ(y x) s(x, y)γ 2Bσmax (cid:17) . (cid:113) cLoutσ2 minϵ2 Lout Lout 1 . σmax and σmin are the γ = largest and smallest singular value of Jf (x,yl1;θ). Remark 6.6. The constant µ quantifies how sharp the scalarized landscape (x; θ) is around its maximizer θ. Its closed form shows that µ is large when (i) the policy places large probability mass on the optimal completion pθ(yx) 1pθ(yx) is large, (ii) the optimal scalarized score so that s(x, y) is large, and (iii) the logit map is well-conditioned, captured by large γ which summarizes the Jacobian and non-saturation constants. Intuitively, µ pθ(yx) 1pθ(yx) s(x, y)γ 2Bσmax represents the net effect of the aligned ascent signal from the optimal completion minus the worst-case destructive contribution from non-optimal completions. Therefore, µ becomes nonpositive when the aligned signal is weak (e.g., small pθ(y x) or small γ) or when the logit map Jf is highly skewed (large σmax/σmin such that the gradient step is inefficient). Importantly, favorable µ ensures convergence of and does not by itself prevent cross-objective interference. can increase while some objective rm decreases when the covariance condition from Section 4 fails for that objective. Taken together, Section 4 and Section 6 disentangle these two complementary mechanisms and suggest principled recipe for robust multi-objective alignment: ensure convergence of the scalarized objective (i.e., make µ positive and sufficiently large) while maintaining per-objective covariance alignment along training, so that increasing translates into simultaneous improvement across objectives rather than cross-objective interference. 7. Conclusion Assumption 6.4 (Aligned token gradients). There exists (0, 1] such that for any and any positions l, {1, . . . , Lout} with vl(x, y; θ) = 0 and vk(x, y; θ) = 0, cos (cid:0)vl(x, y; θ), vk(x, y; θ)(cid:1) c, where vl(x, y; θ) := (x,yl1;θ) (cid:16) eyl pθ( x, yl1) (cid:17) Rn is the token-level logit-gradient contribution with the logit map and Jf its Jacobian. In this paper, we conducted the first systematic study of scalarization in multi-objective LLM alignment and formalized common failure mode, cross-objective interference. Through rigorous analysis, we identified the conditions under which each objective can be improved at first order and when the scalarized optimization satisfies the PL inequality, jointly uncovering the fundamental challenges in multi-objective alignment. Guided by these theoretical insights, we proposed CTWA which effectively mitigates cross-objective interference compared to existing baselines. 8 Uncovering Cross-Objective Interference in Multi-Objective Alignment"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lukosuite, K., Lovitt, L., Sellitto, M., Elhage, N., Schiefer, N., Mercado, N., DasSarma, N., Lasenby, R., Larson, R., Ringer, S., Johnston, S., Kravec, S., Showk, S. E., Fort, S., Lanham, T., Telleen-Lawton, T., Conerly, T., Henighan, T., Hume, T., Bowman, S. R., Hatfield-Dodds, Z., Mann, B., Amodei, D., Joseph, N., McCandlish, S., Brown, T., and Kaplan, J. Constitutional ai: Harmlessness from ai feedback, 2022. URL https://arxiv.org/abs/2212.08073. Barrett, L. and Narayanan, S. Learning all optimal policies with multiple criteria. In Proceedings of the 25th International Conference on Machine Learning, ICML 08, pp. 4147, New York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781605582054. doi: 10.1145/1390156.1390162. URL https://doi. org/10.1145/1390156.1390162. Basaklar, T., Gumussoy, S., and Ogras, U. PD-MORL: Preference-driven multi-objective reinforcement learning In The Eleventh International Conference algorithm. on Learning Representations, 2023. URL https:// openreview.net/forum?id=zS9sRyaPFlJ. Bowman, V. J. On the relationship of the tchebycheff norm and the efficient frontier of multiple-criteria objectives. In Thiriez, H. and Zionts, S. (eds.), Multiple Criteria Decision Making, pp. 7686, Berlin, Heidelberg, 1976. Springer Berlin Heidelberg. ISBN 978-3-642-87563-2. URL https://link.springer.com/chapter/ 10.1007/978-3-642-87563-2_5. Chen, L., Fernando, H., Ying, Y., and Chen, T. Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7004570093. Curran Associates, Inc., 2023. URL https://arxiv.org/abs/2305. 20057. balancing in deep multitask networks. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 794803. PMLR, 1015 Jul 2018. URL https://proceedings.mlr. press/v80/chen18a.html. Dann, C., Mansour, Y., and Mohri, M. Reinforcement learning can be more efficient with multiple reIn Krause, A., Brunskill, E., Cho, K., Enwards. gelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 69486967. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/dann23a.html. Desideri, J.-A. Multiple-Gradient Descent Algorithm (MGDA). Research Report RR-6953, INRIA, June URL https://inria.hal.science/ 2009. inria-00389811. Efroni, Y., Kretzu, B., Jiang, D. R., Bhandari, J., Zhu, Z., and Ullrich, K. Aligned multi objective optimization. In Singh, A., Fazel, M., Hsu, D., Lacoste-Julien, S., Berkenkamp, F., Maharaj, T., Wagstaff, K., and Zhu, J. (eds.), Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pp. 1498915017. PMLR, 1319 Jul 2025. URL https://proceedings.mlr. press/v267/efroni25a.html. Evgeniou, T. and Pontil, M. Regularized multitask learning. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 04, pp. 109117, New York, NY, USA, 2004. Association for Computing Machinery. ISBN 1581138881. doi: 10.1145/1014052.1014067. URL https://doi. org/10.1145/1014052.1014067. Feijer, D. and Paganini, F. Stability of primaldual gradient dynamics and applications to network optimization. Automatica, 46(12):19741981, 2010. ISSN 00051098. doi: https://doi.org/10.1016/j.automatica.2010.08. 011. URL https://www.sciencedirect.com/ science/article/pii/S0005109810003596. Gu, S., Sel, B., Ding, Y., Wang, L., Lin, Q., Knoll, A., and Jin, M. Safe and balanced: framework learnfor constrained multi-objective reinforcement IEEE Trans. Pattern Anal. Mach. Intell., 47(5): ing. 33223331, May 2025. ISSN 0162-8828. doi: 10.1109/ TPAMI.2025.3528944. URL https://doi.org/10. 1109/TPAMI.2025.3528944. Chen, Z., Badrinarayanan, V., Lee, C.-Y., and Rabinovich, A. GradNorm: Gradient normalization for adaptive loss Guo, Y., Cui, G., Yuan, L., Ding, N., Sun, Z., Sun, B., Chen, H., Xie, R., Zhou, J., Lin, Y., Liu, Z., and Sun, M. Con9 Uncovering Cross-Objective Interference in Multi-Objective Alignment trollable preference optimization: Toward controllable multi-objective alignment. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 14371454, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.85. URL https:// aclanthology.org/2024.emnlp-main.85/. He, Q. and Maghsudi, S. Pareto multi-objective alignment for language models. In Ribeiro, R. P., Pfahringer, B., Japkowicz, N., Larranaga, P., Jorge, A. M., Soares, C., Abreu, P. H., and Gama, J. (eds.), Machine Learning and Knowledge Discovery in Databases. Research Track, pp. 257272, Cham, 2026. Springer Nature Switzerland. ISBN 978-3-032-06078-5. Hu, J., Liu, J. K., Xu, H., and Shen, W. Reinforce++: Stabilizing critic-free policy optimization with global advantage normalization, 2025. URL https://arxiv. org/abs/2501.03262. Hu, Z. and Yu, Y. Leveraging variable sparsity to refine In pareto stationarity in multi-objective optimization. The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=Bl3e8HV9xW. Jiang, Y., Li, Y., Chen, G., Liu, D., Cheng, Y., and Shao, J. Rethinking entropy regularization in large reasoning models, 2025. URL https://arxiv.org/abs/2509. 25133. Khatri, D., Madaan, L., Tiwari, R., Bansal, R., Duvvuri, S. S., Zaheer, M., Dhillon, I. S., Brandfonbrener, D., and Agarwal, R. The art of scaling reinforcement learning compute for llms, 2025. URL https://arxiv.org/ abs/2510.13786. Kim, D., Hong, M., Park, J., and Oh, S. Conflictaverse gradient aggregation for constrained multiIn The Thirteenth objective reinforcement learning. International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=ogXkmugNZw. Kimi, Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu, H., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L., Dong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W., Huang, W., Xu, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., 10 Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen, Y., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang, Z., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., Yang, Z., and Lin, Z. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. Li, C., Zhang, H., Xu, Y., Xue, H., Ao, X., and He, Q. Gradient-adaptive policy optimization: Towards multi-objective alignment of large language models. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1121411232, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long. 549. URL https://aclanthology.org/2025. acl-long.549/. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=v8L0pN6EOi. Lin, X., Zhang, X., Yang, Z., Liu, F., Wang, Z., and Zhang, Q. Smooth tchebycheff scalarization for multi-objective optimization. In International Conference on Machine Learning, 2024. URL https://arxiv.org/abs/ 2402.19078. Liu, B., Liu, X., Jin, X., Stone, P., and Liu, Q. Conflictaverse gradient descent for multi-task learning. In Ranzato, M., Beygelzimer, A., Dauphin, Y., (eds.), Advances Liang, P., and Vaughan, in Neural volume 34, pp. 1887818890. Curran Associates, Inc., 2021. URL https://proceedings.neurips. cc/paper_files/paper/2021/file/ 9d27fdf2477ffbff837d73ef7ae23db9-Paper. pdf. Information Processing Systems, J. W. Liu, S.-Y., Dong, X., Lu, X., Diao, S., Belcak, P., Liu, M., Chen, M.-H., Yin, H., Wang, Y.-C. F., Cheng, K.-T., Choi, Y., Kautz, J., and Molchanov, P. Gdpo: Group reward-decoupled normalization policy optimization for Uncovering Cross-Objective Interference in Multi-Objective Alignment multi-reward rl optimization, 2026. URL https:// arxiv.org/abs/2601.05242. Lu, H., Herman, D., and Yu, Y. Multi-objective reinforcement learning: Convexity, stationarity and pareto In The Eleventh International Conference optimality. on Learning Representations, 2023. URL https:// openreview.net/forum?id=TjEzIsyEsQ6. Lu, Y., Wang, Z., Li, S., Liu, X., Yu, C., Yin, Q., Shi, Z., Zhang, Z., and Jiang, M. Learning to optimize multiobjective alignment through dynamic reward weighting, 2025. URL https://arxiv.org/abs/2509. 11452. Mahdavi, M., Yang, T., and Jin, R. Stochastic convex optimization with multiple objectives. In Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips. cc/paper_files/paper/2013/file/ 3493894fa4ea036cfc6433c3e2ee63b0-Paper. pdf. Moskovitz, T., Singh, A. K., Strouse, D., Sandholm, T., Salakhutdinov, R., Dragan, A., and McAleer, S. M. Confronting reward model overoptimization with conIn The Twelfth International Conferstrained RLHF. ence on Learning Representations, 2024. URL https: //openreview.net/forum?id=gkfUvn0fLU. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Razin, N., Zhou, H., Nakkilan, P., Susskind, J., Saremi, O., Bradley, A., Thilak, V., and Littwin, E. Vanishing gradients in reinforcement finetuning of language models. In ICLR, 2024. URL https://arxiv.org/abs/ 2310.20703. Sener, O. and Koltun, V. Multi-task learning as In Neural Information multi-objective optimization. URL https://api. Processing Systems, 2018. semanticscholar.org/CorpusID:52957972. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Shen, Y., Xia, Y., Chang, J., and Ammanabrolu, P. Simultaneous multi-objective alignment across verifiable and non-verifiable rewards, 2025. URL https://arxiv. org/abs/2510.01167. SHI, G., Li, Q., Zhang, W., Chen, J., and Wu, X.-M. Recon: Reducing conflicting gradients from the root for multitask learning. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=ivwZO-HnzG_. Steuer, R. E. and Choo, E.-U. An interactive weighted tchebycheff procedure for multiple objective programming. Mathematical programming, 26(3):326344, 1983. URL https://link.springer.com/article/ 10.1007/BF02591870. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Solla, S., Leen, T., and Muller, K. (eds.), Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999. URL https://proceedings.neurips. cc/paper_files/paper/1999/file/ 464d828b85b0bed98e80ade0a5c43b0f-Paper. pdf. Wang, H., Lin, Y., Xiong, W., Yang, R., Diao, S., Qiu, S., Zhao, H., and Zhang, T. Arithmetic control of LLMs for diverse user preferences: Directional preference alignment with multi-objective rewards. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 86428655, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 468. URL https://aclanthology.org/2024. acl-long.468/. Wang, Z., Tsvetkov, Y., Firat, O., and Cao, Y. Gradient vaccine: Investigating and improving multi-task In Inoptimization in massively multilingual models. ternational Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=F1vEjWK-lH_. 11 Uncovering Cross-Objective Interference in Multi-Objective Alignment cal reasoning in language models, 2025. URL https: //arxiv.org/abs/2504.09696. Zhang, X., Lin, X., and Zhang, Q. Pmgda: preferencebased multiple gradient descent algorithm, 2024. URL https://arxiv.org/abs/2402.09492. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J., and Lin, J. Group sequence policy optimization, 2025. URL https://arxiv.org/abs/2507.18071. Wei, S. and Niethammer, M. The fairness-accuracy pareto front, 2021. URL https://arxiv.org/ abs/2008.10797. Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N. A., Ostendorf, M., and Hajishirzi, H. Finegrained human feedback gives better rewards for language model training. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. Xie, G., Zhang, X., Yao, T., and Shi, Y. Bone soups: seek-and-soup model merging approach for controllable In Che, W., Nabende, J., multi-objective generation. Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2723727263, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1322. URL https:// aclanthology.org/2025.acl-long.1322/. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yao, F., Wang, Z., Liu, L., Cui, J., Zhong, L., Fu, X., Mai, H., Krishnan, V., Gao, J., and Shang, J. Training language models to generate quality code with program analysis feedback. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https: //openreview.net/forum?id=3nza35A6I4. Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. Gradient surgery for multi-task learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volInc., ume 33, pp. 58245836. Curran Associates, 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 3fe78a8acf5fda99de95303940a2420c-Paper. pdf. Zhang, J. and Zuo, C. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematiUncovering Cross-Objective Interference in Multi-Objective Alignment A. Algorithms We provide pseudocode for algorithms that, to our knowledge, have not been previously applied to LLM alignment. For dynamic weighting (Lu et al., 2025), PAMA (He & Maghsudi, 2026), and common linear scalarization, we follow their original implementations. A.1. Covariance-Targed Weight Adapation Algorithm 1 CTWA on top of GRPO 1: Inputs: Objectives {rm}M m=1, scalarization weights {λm}, CTWA targets {c m}, EMA rate τ (0, 1], weight learning rate ηλ. 2: Initialize um log λm. 3: for each training iteration = 1, 2, . . . do 4: 5: 6: Set reference policy θref θ. Sample batch of prompts {xi}B for each prompt xi do i=1. 7: 8: 9: 10: 11: Sample completions {y(k) Evaluate objective rewards rm(xi, y(k) Compute group-normalized GRPO advantages {Ab k=1 pθref ( xi). }K ) and scalar scores sλ(xi, y(k) i,k}K k=1 from {sλ(xi, y(k) ). )}K k=1. end for Inner update (GRPO). Update θ by optimizing the standard clipped GRPO surrogate using θref and {Ab CTWA statistics. Using the same batch, compute completion-level clipped advantage weights w(xi, y(k) Compute per-objective covariance {cm}M Update EMA: cm (1 τ )cm + τ cm for all m. Compute deficits δm [ 12: 13: 14: 15: 16: Outer update (CTWA). Update log-weights um um + ηλ δm and set λm exp(um) for all m. 17: end for cm ]+. m=1 by aggregating within each prompt group and averaging across batch. i,k}. ; θ). A.2. GradNorm Algorithm 2 GradNorm on top of GRPO 1: Inputs: Objectives {rm}M 2: Initialize weights wm 1 for all m; initialize reference losses L(0) 3: for each training iteration = 1, 2, . . . do 4: m=1, GradNorm exponent α, weight learning rate ηw. unset. 5: Same as CTWA. Sample prompts and completions; compute rewards and GRPO quantities needed to form clipped surrogate. Per-objective gradients. Compute each objectives GRPO surrogate loss value Lm and its policy gradient gm (using the same KL/entropy regularization as the base run). if L(0) 6: 7: 8: 9: GradNorm targets. Compute gradient norms Gm gm2 and each objectives relative training rate from the loss is unset then Set L(0) Lm for all m. end if ratio Lm/L(0) ; convert it to target scaled norm using exponent α. 10: Weight update. Update wm with step size ηw to match the scaled norms wmGm to their targets; renormalize weights. 11: 12: end for Policy update. Apply one optimization step using the weighted gradient (cid:80)M m=1 wmgm. 13 Uncovering Cross-Objective Interference in Multi-Objective Alignment A.3. MGDA Algorithm 3 MGDA on top of GRPO 1: Inputs: Objectives {rm}M 2: for each training iteration = 1, 2, . . . do 3: m=1. Same as CTWA. Sample prompts and completions; compute rewards and GRPO quantities needed to form clipped surrogate. Per-objective gradients. Compute each objectives GRPO surrogate policy gradient gm (using the same KL/entropy regularization as the base run). 4: 5: MGDA weights. Solve for simplex weights that minimize the norm of the combined gradient, i.e., find the minimum-norm convex combination of {gm}M Policy update. Apply one optimization step using the weighted gradient (cid:80)M m=1. m=1 wmgm. 6: 7: end for A.4. Tchebycheff Scalarization Algorithm 4 Weighted Tchebycheff Scalarization on top of GRPO 1: Inputs: Objectives {rm}M 2: Initialize running reference point RM using the first batch. 3: for each training iteration = 1, 2, . . . do 4: m=1, scalarization weights {wm}. Same as CTWA. Sample prompts and completions; compute per-objective token-level rewards rm(xi, y(k) GRPO quantities needed to form clipped surrogate. Update reference point. Update zm max{zm, maxbatch,tokens rm} for all m. Scalar score. For each token, compute the weighted Tchebycheff scalar reward s() maxm wm Policy update. Compute GRPO advantages from the scalar score and apply one optimization step. (cid:0)zm rm()(cid:1). ) and 5: 6: 7: 8: end for A.5. Lagrangian Primal-Dual Method Algorithm 5 Lagrangian Primal-Dual Method on top of GRPO 1: Inputs: Primary objective r0, constraint objectives {rk}K 2: Initialize Lagrange multipliers λk 0 for all k. 3: for each training iteration = 1, 2, . . . do 4: k=1, target constraint rewards {ck}, dual learning rate ηλ. 5: 6: Same as CTWA. Sample prompts and completions; compute per-objective token-level rewards {r0, r1, . . . , rK} and GRPO quantities needed to form clipped surrogate. Per-objective advantages. Comput advantage for each objective separately, yielding A0 (primary) and {Ak}K (constraints). Dual update. Estimate each constraint objectives average reward on the current batch, compute the reward gaps ck E[rk], and update multipliers by dual ascent: λk max{0, λk + ηλ (ck E[rk])} for all k. Primal scalarization. Form Lagrangian-weighted advantage: A0 + (cid:80)K Policy update. Apply one optimization step using the combined advantage A. k=1 λkAk. k=1 7: 8: 9: end for 14 Uncovering Cross-Objective Interference in Multi-Objective Alignment B. Experiments B.1. Results (a) Qwen2.5-1.5B-Base (b) Qwen2.5-1.5B-IFT (c) Qwen3-1.7B-Base Figure 4. Gradient alignment across objectives during multi-objective alignment. We measure pairwise cosine similarity between per-objective gradients throughout training. Negative values indicate conflicting updates, which is standard proxy for identifying conflicting objectives in MTL (Yu et al., 2020). Across all three models, cosine similarities remain mostly non-negative and converge toward 0 as training progresses, suggesting that objectives are weakly coupled, neither strongly synergistic nor persistently antagonistic, with no conflicting behavior observed. B.2. Hyperparameters Table 1. Hyperparameter summary for different scalarization algorithms. Unless noted, runs share the same policy update and compute settings as shown in Table 1f. (a) Key hyperparameters for CTWA. (b) Key hyperparameters for GradNorm. (c) Key hyperparameters for MGDA. Hyperparameter Value Hyperparameter Initial weights λ(0) Weight learning rate ηλ Covariance targets EMA rate τ [0.333, 0.333, 0.334] 0.05 [0.15, 0.08, 0.08] 0.1 Exponent α Weight learning rate ηw Value 1.5 0.025 (d) Key hyperparameters for Tchebycheff Scalarization. (e) Key hyperparameters for Lagrangian Primal-Dual Method. Hyperparameter Value Hyperparameter Value Scalarization weights [0.333, 0.333, 0.334] Primary objective Constraint objectives Constraint targets Dual learning rate ηλ accuracy conciseness, clarity [0.9, 0.9] 0.01 Policy update KL coefficient 0 Hyperparameter Value Settings Same as Table 1f Policy update KL coefficient 0 (f) Shared hyperparameters. Setting Policy update Learning rate η LR scheduler Batch size Rollouts Max prompt length Max response length Entropy coefficient KL coefficient Clipping coefficient ϵ (for GRPO) Epochs Optimization backend Compute GPUs Inference engine Value 1e6 constant 32 16 1024 1024 0 0.001 0.2 90 FSDP 4 Nvidia L40 (48 GB) vLLM 15 Uncovering Cross-Objective Interference in Multi-Objective Alignment (a) Qwen2.5-1.5B-Base (b) Qwen2.5-1.5B-IFT (c) Qwen3-1.7B-Base Figure 5. Multi-objective alignment using GRPO with different scalarization algorithms. We report moving-averaged test performance along training for three objectives: accuracy, conciseness, and clarity (left to right). Similar to observations from Figure 1, CTWA achieves the most balanced performance across objectives without excessively sacrificing one for another. While Lagrangian, PAMA and Tchebycheff maintain higher accuracy in 5a and 5b, each of them has significant drawbacks. Lagrangian exhibits remarkably worse conciseness and clarity, PAMA fails to improve these two objectives at all, and Tchebycheff collapses entirely under REINFORCE demonstrating poor generalization across RL algorithms. Instead, CTWA effectively mitigates cross-objective interference, achieving competitive performance on all objectives across different models and RL algorithms. C. Proof C.1. Proof of Lemma 4.1 Proof. Fix x. We optimize over distributions on Lout satisfying q(y) 0 and (cid:80) gives q(y) = 1. Expanding the KL term KL(qpθ;x) = (cid:88) q(y) log q(y) pθ;x(y) . Thus the objective can be written as max q(X Lout ) (cid:40) (cid:88) q(y)s(x, y) 1 η (cid:88) q(y) log (cid:41) . q(y) pθ;x(y) Introduce Lagrange multiplier λ for the normalization constraint (cid:80) q(y) = 1, and consider L(q, λ) = (cid:88) q(y) s(x, y) 1 η (cid:88) q(y) log q(y) pθ;x(y) (cid:16) (cid:88) + λ q(y) (cid:17) . For any with q(y) > 0, the stationarity condition is q(y) L(q, λ) = s(x, y) (cid:18) log 1 η q(y) pθ;x(y) (cid:19) + + λ = 0. 16 Uncovering Cross-Objective Interference in Multi-Objective Alignment"
        },
        {
            "title": "Exponentiating both sides gives us",
            "content": "log q(y) pθ;x(y) = η s(x, y) + ηλ 1. q(y) = pθ;x(y) exp(cid:0)η s(x, y)(cid:1) exp(ηλ 1). Let := exp(ηλ 1), which does not depend on y. Enforcing normalization (cid:80) q(y) = 1 implies 1 = (cid:88) q(y) = (cid:88) pθ;x(y) exp(cid:0)η s(x, y)(cid:1), so = 1 (cid:80) pθ;x(y) exp(cid:0)η s(x, y)(cid:1) = 1 (cid:2) exp(η s(x, y))(cid:3) . Eypθ;x Substituting back, the maximizer is Identifying = p+ θ;x yields the claim. q(y) = pθ;x(y) exp(cid:0)η s(x, y)(cid:1) (cid:2) exp(η s(x, y))(cid:3) . Eypθ;x Remark C.1. The update Equation (2) trades off increasing Eyq[s(x, y)] with staying close to pθ;x in KL(qpθ;x). Lemma 4.1 shows the solution is an exponential tilting, p+ θ;x(y) pθ;x(y)eηs(x,y), so it can only reweight completions already supported by pθ;x. In particular, if pθ;x(y) = 0 then p+ θ;x(y) = 0, since otherwise KL(qpθ;x) = +. C.2. Proof of Theorem 4.2 Proof. Fix x. By Lemma 4.1, the optimizer of Equation (2) satisfies p+ θ;x(y) = pθ;x(y) exp (cid:0)η s(x, y)(cid:1) exp (cid:0)η s(x, y)(cid:1)(cid:105) Eypθ;x (cid:104) Let s(x) := Eypθ;x (cid:2)s(x, y)(cid:3). Using the Taylor expansion exp(ηs) = 1 + ηs + O(η2), we have Zη(x) = Eypθ;x (cid:2)1 + ηs(x, y) + O(η2)(cid:3) = 1 + ηs(x) + O(η2). For small η, (1 + u)1 = 1 + O(u2) implies 1 Zη(x) = 1 η s(x) + O(η2). Substituting back yields the pointwise expansion p+ θ;x(y) = pθ;x(y) (cid:16) 1 + η(cid:0)s(x, y) s(x)(cid:1)(cid:17) + O(η2)pθ;x(y). Therefore, yp+ θ;x (cid:2)rm(x, y)(cid:3) = = (cid:88) (cid:88) p+ θ;x(y)rm(x, y) pθ;x(y)rm(x, y) + η (cid:88) pθ;x(y)rm(x, y)(cid:0)s(x, y) s(x)(cid:1) + O(η2) = Eypθ;x = Eypθ;x (cid:16) Eypθ;x (cid:2)rm(x, y)(cid:3) + η (cid:2)rm(x, y)(cid:3) + ηCovypθ(x) (cid:2)rm(x, y)s(x, y)(cid:3) Eypθ;x (cid:2)rm(x, y)(cid:3)Eypθ;x (cid:2)s(x, y)(cid:3)(cid:17) + O(η2) (cid:0)rm(x, y), s(x, y)(cid:1) + O(η2). (cid:88) pθ;x(y)rm(x, y) The third equality follows since rm(x, y) is bounded, so the remainder term is O(η2). Finally, taking expectation over gives Equation (3). 17 Uncovering Cross-Objective Interference in Multi-Objective Alignment C.3. Proof of Lemma 4.4 Proof. Define the deterministic one-dimensional function g(t) := rm (cid:0)θ + td(θ)(cid:1), [0, η]. By the fundamental theorem of calculus, rm(θ+) rm(θ) = g(η) g(0) = (cid:90) η 0 g(t)dt (cid:10)d(θ), rm (cid:0)θ + td(θ)(cid:1)(cid:11)dt = = (cid:90) η 0 (cid:90) η 0 (cid:10)d(θ), rm(θ)(cid:11)dt + (cid:90) η (cid:90) η 0 (cid:10)d(θ), rm (cid:0)θ + td(θ)(cid:1) rm(θ)(cid:11)dt = η(cid:10)rm(θ), d(θ)(cid:11) + (cid:10)rm (cid:0)θ + td(θ)(cid:1) rm(θ), d(θ)(cid:11)dt. For the second term, the Cauchy-Schwarz inequality implies 0 (cid:10)rm (cid:0)θ + td(θ)(cid:1) rm(θ), d(θ)(cid:11) rm (cid:0)θ + td(θ)(cid:1) rm(θ)d(θ). By the Lm-Lipschitzness of rm, rm (cid:0)θ + td(θ)(cid:1) rm(θ) Lmθ + td(θ) θ = Lmtd(θ). Therefore, (cid:90) η 0 (cid:10)rm (cid:0)θ + td(θ)(cid:1) rm(θ), d(θ)(cid:11)dt (cid:90) η 0 Lmtd(θ)2dt = Lm 2 η2d(θ)2. Combining the bounds yields rm(θ+) rm(θ) η(cid:10)rm(θ), d(θ)(cid:11) Lm 2 η2d(θ)2, which proves the lemma. C.4. Proof of Theorem 4. Proof. Write the GRPO surrogate objective as (cid:34) (cid:88) J(θ) = Lout(cid:88) min (cid:110) ρk,l(θ)Ak(x), ρk,l(θ)Ak(x) (cid:35) (cid:111) βE(cid:2)KL(pθpref )(cid:3) + λE(cid:2)H(pθ)(cid:3). k=1 l= Fix (x, y(1:K)) and token index (k, l). Define sk,l(θ) := min (cid:110) ρk,l(θ)Ak(x), ρk,l(θ)Ak(x) (cid:111) . By the definition of tokenwise clipping, sk,l(θ) equals ρk,l(θ)Ak(x) on the unclipped region and equals ρk,l(θ)Ak(x) on the clipped region. On the clipped region, ρk,l(θ) is constant in θ (equal to 1 + ε or 1 ε), so its gradient is zero. On the unclipped region, sk,l(θ) = Ak(x)ρk,l(θ) = Ak(x)ρk,l(θ) log pθ (cid:0)y(k) x, y(k) l1 (cid:1) = Wk,l(θ)ϕk,l(θ), where Wk,l(θ) = Ak(x)ρk,l(θ)1k,l(θ) encodes exactly the unclipped region. Therefore, (cid:104) (cid:88) θE (cid:105) sk,l(θ) (cid:104) (cid:88) = (cid:105) sk,l(θ) (cid:104) (cid:88) = (cid:105) Wk,l(θ)ϕk,l(θ) = G(θ). k,l k,l k,l 18 Uncovering Cross-Objective Interference in Multi-Objective Alignment For the regularizers, (cid:16) (cid:17) βE[KL(pθpref )] + λE[H(pθ)] θ (cid:16) = (cid:17) βθE[KL(pθpref )] λθE[H(pθ)] = R(θ). Combining these two parts gives J(θ) = G(θ) R(θ), hence dnat(θ) = (θ)1J(θ) = (θ)1(cid:0)G(θ) R(θ)(cid:1). By Lemma 4.4, for each objective and any direction d(θ), rm(θ + ηd(θ)) rm(θ) ηrm(θ)d(θ) Lm 2 η2d(θ)2. Setting d(θ) = dnat(θ) and using the definition of dnat(θ) yields rm(θ)dnat(θ) = rm(θ)F (θ)1(cid:0)G(θ) R(θ)(cid:1) = rm(θ)F (θ)1G(θ) rm(θ)F (θ)1R(θ). Thus the condition Equation (8) implies rm(θ)dnat(θ) 0 for all m. If dnat(θ) = 0, then θ+ = θ and rm(θ+) = rm(θ) trivially. Otherwise, if γm(θ) = rm(θ)dnat(θ) > 0 for all m, then Choosing rm(θ+) rm(θ) ηγm(θ) Lm η2dnat(θ)2. 0 < η min 2γm(θ) Lmdnat(θ)2 ensures the right-hand side is nonnegative for every m, hence rm(θ+) rm(θ) for all m. If minm γm(θ) > 0, then taking η strictly smaller than the bound gives strict improvement for all m. C.5. Proof of Corollary 4.6 Proof. Fix x. For categorical softmax logits, for any y, Lout , the Jacobian satisfies pθ(y x) θz = pθ(y x)(cid:0)1{y = z} pθ(z x)(cid:1). For each z, differentiating the expectation yields θz Eypθ(x) (cid:2)w(x, y)(cid:3) = = (cid:88) (cid:88) w(x, y) pθ(y x) θz w(x, y)pθ(y x)(cid:0)1{y = z} pθ(z x)(cid:1) = pθ(z x)w(x, z) pθ(z x) (cid:88) pθ(y x)w(x, y) = pθ(z x) (cid:16) w(x, z) Eypθ(x) (cid:2)w(x, y)(cid:3)(cid:17) . Therefore, we obtain its vector form θEypθ(x) (cid:2)w(x, y)(cid:3) = pθ( x) (cid:16) w(x, ) Eypθ(x) (cid:17) (cid:2)w(x, y)(cid:3)1 , (10) where denotes elementwise product. For the categorical softmax family, the Fisher matrix is (θ) = diag(cid:0)pθ( x)(cid:1) pθ( x)pθ( x). 19 Uncovering Cross-Objective Interference in Multi-Objective Alignment For any vector indexed by y, the y-th coordinate of (θ)v is = (cid:2)diag(pθ( x))v(cid:3) = pθ(y x)vy pθ(y x) (cid:2)F (θ)v(cid:3) (cid:2)pθ( x)pθ( x)v(cid:3) (cid:88) pθ(z x)vz"
        },
        {
            "title": "Then",
            "content": "= pθ(y x) (cid:16) vy Ezpθ(x)[vz] (cid:17) . = w(x, ) Eypθ(x) (cid:2)w(x, y)(cid:3)1. Ezpθ(x)[vz] = Ezpθ(x) (cid:2)w(x, z)(cid:3) Eypθ(x) (cid:2)w(x, y)(cid:3) Ezpθ(x)[1] so the general coordinate formula reduces to = 0, (cid:2)F (θ)v(cid:3) = pθ(y x)vy. Equivalently, (θ)v = pθ( x) = pθ( x) (cid:16) w(x, ) Eypθ(x) (cid:2)w(x, y)(cid:3)1 (cid:17) . Comparing with the expression for θE[w] from Equation (10) shows that this particular satisfies By definition, any vector dnat(θ) satisfying (θ)dnat(θ) = θEypθ(x) (cid:2)w(x, y)(cid:3) (θ)v = θEypθ(x) (cid:2)w(x, y)(cid:3). is valid natural gradient direction (the solution is not unique because (θ)1 = 0). Therefore, one convenient and valid choice in the space 1dnat(θ) = 0 is dnat(θ) = w(x, ) Eypθ(x) (cid:2)w(x, y)(cid:3)1. (11) Take θ+ = θ + ηdnat(θ) with learning rate η > 0. first-order Taylor expansion gives pθ+(y x) pθ(y x) = η (cid:88) pθ(y x) θz dnat(θ)z + O(η2). Substituting the Jacobian formula and simplifying, pθ+(y x) pθ(y x) = ηpθ(y x) (cid:16) dnat(θ)y (cid:88) pθ(z x)dnat(θ)z (cid:17) + O(η2) = ηpθ(y x) (cid:16) dnat(θ)y Ezpθ(x) (cid:2)dnat(θ)z (cid:3)(cid:17) + O(η2). Ezpθ(x) (cid:2)dnat(θ)z (cid:3) = Ezpθ(x) (cid:2)w(x, z)(cid:3) Eypθ(x) (cid:2)w(x, y)(cid:3) = 0, pθ+(y x) pθ(y x) = ηpθ(y x) (cid:16) w(x, y) Eypθ(x) (cid:2)w(x, y)(cid:3)(cid:17) + O(η2). By Equation (11), and hence Therefore, Eypθ+ (x) (cid:88) = (cid:2)rm(x, y)(cid:3) Eypθ(x) (cid:16) pθ+(y x) pθ(y x) (cid:2)rm(x, y)(cid:3) (cid:17) rm(x, y) (cid:88) = η (cid:16) pθ(y x) w(x, y) Eypθ(x) (cid:2)w(x, y)(cid:3)(cid:17) rm(x, y) + O(η2) (cid:16) Eypθ(x) = η = ηCovypθ(x) (cid:2)rm(x, y)w(x, y)(cid:3) Eypθ(x) (cid:0)rm(x, y), w(x, y)(cid:1) + O(η2), (cid:2)rm(x, y)(cid:3)Eypθ(x) (cid:2)w(x, y)(cid:3)(cid:17) + O(η2) Uncovering Cross-Objective Interference in Multi-Objective Alignment which proves the claim. C.6. Proof of Corollary 4.7 Proof. Fix an objective and write := (θ), := R(θ), Gunclip := Gunclip(θ), and Gclip := Gclip(θ). By definition, γunclip (θ) = rm(θ)F 1(cid:0)Gunclip R(cid:1). Hence we can decompose the clipped first-order margin as rm(θ)F 1(cid:0)Gclip R(cid:1) = rm(θ)F 1(cid:0)Gunclip R(cid:1) rm(θ)F 1(cid:0)Gunclip Gclip(cid:1) = γunclip (θ) rm(θ)F 1(cid:0)Gunclip Gclip(cid:1). Now insert 1/2 and apply Cauchy-Schwarz inequality: (cid:0)F 1/2rm(θ)(cid:1)(cid:0)F 1/2(Gunclip Gclip)(cid:1)(cid:12) (cid:12) (cid:12)rm(θ)F 1(cid:0)Gunclip Gclip(cid:1)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) (cid:13)F 1/2(cid:0)Gunclip Gclip(cid:1)(cid:13) (cid:13)F 1/2rm(θ)(cid:13) (cid:13) (cid:13). (cid:13) (cid:13) (cid:12) (cid:12) Combining with the decomposition yields the margin bound (θ) (cid:13) rm(θ)F 1(cid:0)Gclip R(cid:1) γunclip (cid:13)F 1/2rm(θ)(cid:13) (cid:13) (cid:13) (cid:13)F 1/2(cid:0)Gunclip Gclip(cid:1)(cid:13) (cid:13). Therefore, if γunclip (θ) κm > 0 and (cid:13)F 1/2(cid:0)Gunclip Gclip(cid:1)(cid:13) (cid:13) (cid:13) κm (cid:13)F 1/2rm(θ)(cid:13) (cid:13) (cid:13) , then rm(θ)F 1(Gclip R) 0, which proves the claim. C.7. Proof of Theorem 6.5 Proof. We first differentiate (x; θ) with respect to θ, using the log-derivative trick gives: (cid:105) (cid:104) s(x, y)θ ln pθ(y x) θV (x; θ) = Eypθ(x) = Eypθ(x) (cid:104) s(x, y) = Eypθ(x) (cid:104) s(x, y) Lout(cid:88) l= Lout(cid:88) l=1 (cid:105) θ ln pθ(yl x, yl1) θ ln softmax(f (x, yl1; θ))yl (cid:105) (cid:88) = yX Lout pθ(y x)s(x, y) Lout(cid:88) l=1 (x,yl1;θ)(eyl pθ( x, yl1)). where Jf (x,yl1;θ) is the Jacobian of (x, yl1; θ) with respsect to θ. eyl denotes the ylth standard basis vector whose ylth entry is 1 and all other entries are 0. pθ( x, yl1) = softmax(f (x, yl1; θ)) RX returns the next-token (at position l) probability distribution. Let ϕ(x, y; θ) := (cid:80)Lout l=1 (x,yl1;θ)(eyl pθ( x, yl1)) and σmax be the largest singular value of Jf (x,yl1;θ). By definition, we know σmax = maxl{1,2, ,Lout} Jf (x,yl1;θ). Obtaining the upper bound of ϕ(x, y; θ) is nontrivial: ϕ(x, y; θ) (x,yl1;θ)2 (eyl pθ( x, yl1)) 2σmax because (eyl pθ( x, yl1)) (eyl pθ( x, yl1))1 2 and (x,yl1;θ)2 = σmax by defnition. However, we cannot lower bound ϕ(x, y; θ) without extra structure. So we first assume token probabilities are bounded away from 1 for all suboptimal θ in Assumption 6.3, consistent with practical RFT setups where KL or entropy regularization prevents probabilities from overfitting to 1 during training. Assumption 6.4 requires all nonzero per-token contributions vl are positively aligned in parameter space and their cosine similarity is uniformly bounded away from zero by c. With these assumptions about the structure of policy function, we can derive the following lemmas that will be used in the proof later. 21 Uncovering Cross-Objective Interference in Multi-Objective Alignment Lemma C.2. Under Assumption 6.3 and 6.4, we have (cid:13) (cid:13) (cid:13) Lout(cid:88) l=1 (x,yl1;θ)(eyl pθ( x, yl1)) (cid:13) 2 (cid:13) (cid:13) cLoutσ2 minϵ2 Lout Lout 1 (12) Remark C.3. The left-hand side of Equation (12) quantifies the squared norm of the summed token-level policy gradient contributions along an output sequence of length Lout, and thus measures the overall strength of the gradient signal induced by that sequence. The lower bound shows that this signal scales with the alignment constant c, the smallest singular value σmin of the logit Jacobian, the sequence length Lout, and the probability gap ϵ. Here, ϵ lower bounds 1 pθ(yl x, yl1) and captures the policys uncertainty in predicting the next token. Smaller ϵ corresponds to higher confidence and more likely token sampling, but also leads to weaker gradient signals. Proof of Lemma C.2. We lower bound the L2-norm of the vector eyl pθ( x, yl1): eyl pθ( x, yl1)2 = (cid:0)1 pθ(yl x, yl1)(cid:1)2 + (cid:88) θ(y x, yl1) p2 (cid:0)1 pθ(yl x, yl1)(cid:1)2 + = (cid:0)1 pθ(yl x, yl1)(cid:1)2 + y=yl,yX (cid:0) (cid:80) y=yl,yX pθ(y x, yl1)(cid:1)2 Lout 1 (cid:0)1 pθ(yl x, yl1)(cid:1)2 Lout 1 = Lout Lout 1 Lout Lout 1 The first inequality is derived from the CauchySchwarz inequality in Euclidean space ((cid:80) pθ)2 ((cid:80) p2 last inequality is derived from the Assumption 6.3. Then we have: (cid:0)1 pθ(yl x, yl1)(cid:1)2 ϵ2 . (13) θ)((cid:80) 12), and the (cid:13) (cid:13) (cid:13) Lout(cid:88) l=1 (x,yl1;θ)(eyl pθ( x, yl1)) (cid:13) 2 (cid:13) (cid:13) = Lout(cid:88) l=1 Lout(cid:88) l=1 (cid:13) (cid:13) (cid:13)vl (cid:13) 2 (cid:13) (cid:13) + 2 (cid:88) l<k vl, vk (cid:13) (cid:13) (cid:13)vl (cid:13) 2 (cid:13) (cid:13) + 2c (cid:88) lk vlvk (cid:16) Lout(cid:88) (cid:17)2 vl l=1 cσ2 min (cid:16) Lout(cid:88) l=1 eyl pθ( x, yl1) (cid:17)2 cLoutσ2 minϵ2 Lout Lout . Here, σmin denotes the smallest singular value of the Jacobian Jf (x,yl1;θ). The second inequality follows from the Assumption 6.4 and factors out c. The third inequality applies the singular value bound (x,yl1;θ)u σminu, u. The final inequality uses the result from Equation (13). Lemma C.4. Let Assumption 6.2 hold and is the unique maximizer of the scalarized reward, arg maxy s(x, y). Assume the policy class is rich enough to realize deterministic policy on that always outputs y, i.e., there exists parameter vector θ such that pθ (y x) = (cid:40)1, if = y, 0, otherwise. 22 Uncovering Cross-Objective Interference in Multi-Objective Alignment Then θ is an optimal parameter for the scalarized value at x, and the optimal value becomes (x; θ) = (cid:80) x)s(x, y) = s(x, y). For any θ, we have: yX Lout pθ (y (x; θ) (x; θ) 2B(cid:0)1 pθ(y x)(cid:1). Remark C.5. The Lemma C.4 shows that the value gap is Lipschitz in the probability gap on y. Every unit of probability that fails to go to the optimal sequence can hurt the value by at most 2B. Equivalently, driving the model towards neardeterministic predictions on y, pθ(y x) 1, is both necessary and sufficient (up to the factor 2B) to make the scalarized value nearly optimal. This connects convergence in value directly to how sharply the policy concentrates on the optimal sequence. Proof of Lemma C.4. We can write (x; θ) (x; θ) as follows: (x; θ) (x; θ) = s(x, y) (cid:88) pθ(y x)s(x, y) yX Lout = s(x, y) (cid:0)pθ(y x)s(x, y) + pθ(y x)s(x, y)(cid:1) (cid:88) y=y = (cid:0)1 pθ(y x)(cid:1)s(x, y) (cid:88) y=y pθ(y x)s(x, y) (cid:0)1 pθ(y x)(cid:1)s(x, y) min y=y = (cid:0)1 pθ(y x)(cid:1)s(x, y) min = (cid:0)s(x, y) min y=y 2B(cid:0)1 pθ(y x)(cid:1). y=y s(x, y)(cid:1)(cid:0)1 pθ(y x)(cid:1) s(x, y) (cid:88) y=y pθ(y x) s(x, y)(cid:0)1 pθ(y x)(cid:1) The last inequality follows from the Assumption 6.2 that s(x, y) and s(x, y) miny=y s(x, y) s(x, y) miny=y s(x, y)1 2B. Defining direction vector: u(x, y; θ) := ϕ(x, y; θ) ϕ(x, y; θ) := (cid:80)Lout l=1 l=1 (cid:13) (cid:13) (cid:80)Lout (x,yl1;θ) (x,yl1;θ) (cid:0)eyl pθ( x, yl1)(cid:1) (cid:0)eyl pθ( x, yl1)(cid:1)(cid:13) (cid:13) , which is the normalized log-probability gradient of the optimal trajectory. We first discuss its inner product with ϕ(x, y; θ) which will be used for the proof later. ϕ(x, y; θ), u(x, y; θ) = ϕ(x, y; θ) (cid:113) cLoutσ minϵ2 Lout Lout 1 , Then the directional derivative of (x; θ) along u(x, y; θ) is: ϕ(x, y; θ) 2σmax, if = y, y. (14) θV (x; θ), u(x, y; θ) = Eypθ(x) (cid:104) s(x, y)ϕ(x, y; θ), u(x, y; θ) (cid:105) . Extracting out yields: θV (x; θ), u(x, y; θ) = pθ(y x)s(x, y)ϕ(x, y; θ), u(x, y; θ) (cid:125) (cid:124) (cid:123)(cid:122) (I) (cid:88) + y=y (cid:124) pθ(y x)s(x, y)ϕ(x, y; θ), u(x, y; θ) . (cid:123)(cid:122) (II) (cid:125) (15) 23 Uncovering Cross-Objective Interference in Multi-Objective Alignment We lower bound (I) and (II) seperately. Starting with (I), by Equation (14), we know that: (I) pθ(y x)s(x, y) cLoutσ minϵ2 (cid:115) Lout Lout 1 . Recalling that ϕ(x, y) from Assumption 6.2 and the property ϕ(x, y; θ), u(x, y; θ) 2σmax, we get: (II) 2Bσmax (cid:88) y=y pθ(y x) = 2Bσmax(1 pθ(y x)). For (II), the worst case happens when, for every = y, the terms contribution to the directional derivative along is as adverse as allowed by the assumed bounds. Plugging the above two inequalities into Equation (15), rearranging the equality, it follows that: θV (x; θ), u(x, y; θ) s(x, y)γ (cid:0)s(x, y)γ + 2Bσmax (cid:1)(cid:0)1 pθ(y x)(cid:1), (cid:115) with γ = cLoutσ2 minϵ2 Lout Lout 1 . (16) Intuitively, this inequality shows that when the policy is close to the optimal θ and is the (unique) optimal trajectory maximizing the scalarized reward, the directional derivative along admits large lower bound and thus yields strong, well-aligned gradient update that pushes additional probability mass toward y. Recalling that we have lower bounded value function gap with the sequence probability gap in Lemma C.4, where we assume pθ (y x) = 1. We are close to the result and want final inequality that looks like: θV (x; θ) θV (x; θ), u(x, y; θ) µ(cid:0)pθ (y x) pθ(y x)(cid:1) µ(cid:0)V (x; θ) (x; θ)(cid:1). (17) The first inequality follows from the CauchySchwarz inequality applied to the unit vector and the last inequality comes from Lemma C.4. So the only task left here is to get an µ that satisfies the second inequality for all θ. Combining with Equation (16), we have: µ pθ(y x) 1 pθ(y x) s(x, y)γ 2Bσmax. Because the function 1p is increasing on [0, 1), sufficient and tight choice for µ is: µ := pθ(y x) 1 pθ(y x) s(x, y)γ 2Bσmax Therefore, the final µ-PL condition meets from Equation (17) and combing the bound from Lemma C.4 leads to: µ = 1 2B (cid:16) pθ(y x) 1 pθ(y x) s(x, y)γ 2Bσmax (cid:17) (18)"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, University of Notre Dame, USA"
    ]
}