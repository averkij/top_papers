{
    "paper_title": "PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks",
    "authors": [
        "Junxian Li",
        "Kai Liu",
        "Leyang Chen",
        "Weida Wang",
        "Zhixin Wang",
        "Jiaqi Xu",
        "Fan Li",
        "Renjing Pei",
        "Linghe Kong",
        "Yulun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic."
        },
        {
            "title": "Start",
            "content": "PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks Junxian Li 1 Kai Liu 1 Leyang Chen 1 Weida Wang 2 Zhixin Wang 3 Jiaqi Xu 3 Fan Li 3 Renjing Pei 3 Linghe Kong 1 Yulun Zhang"
        },
        {
            "title": "Abstract",
            "content": "Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new subtasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating humanannotated questions and reference images, and quality control process. For challenges of comprehensive and exact evaluation, task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic. 6 2 0 2 6 ] . [ 1 3 6 6 6 0 . 2 0 6 2 : r 1. Introduction Unified multimodal models (UMMs) (Deng et al., 2025; Chen et al., 2025b; OpenAI, 2025a; Wu et al., 2025b), have seen remarkable developments in recent times. They integrate the understanding capabilities of multimodal large language models (MLLMs) and the capabilities of diffusion models to generate images. Such integration helps the UMM Equal contribution 1Shanghai Jiao Tong University 2Fudan University 3Huawei Technologies Ltd. Correspondence to: Yulun Zhang <yulun100@gmail.com>. Preprint. February 9, 2026. 1 Figure 1. Examples of generation (left) and editing (right). The queries are generating flowchart on how to apply VISA and show what happens if setting Chinese(Simplified) to the display language. Both UMMs make mistakes: Bagel doesnt provide complete workflow and texts are meaningless; GPT-Image-1 provides garbled characters and fails to keep the total layout. understand diverse tasks better and find correct directions of generation more easily. As result, advanced UMMs can overcome more complex tasks like creative drawing, multi-image fusion, and logical editing (Pan et al., 2025; Chow et al., 2025; Liang et al., 2025b), which prior models only for image generation or editing (Rombach et al., 2022; Croitoru et al., 2023) struggle to deal with. Despite the huge success of UMMs in generating natural images, there exists largely underexplored dimension for them: image generation and editing for computer-use tasks. Unlike natural scenes, computer-related visual content, such as graphical user interfaces (GUIs), structured workflows, and slide layouts, is foundational to modern professional and personal daily lives. Therefore, the capabilities of UMMs to accurately synthesize and manipulate such functional visuals for users directly reveal their utility as real-world assistants. Furthermore, these tasks demand higher degree of structural precision and semantic consistency than typical image generation. Under these requirements, UMMs have to understand and precisely plan the tasks for generating high-quality images. Examples in Figure 1 support this perspective. Conversely, these requirements remain underexplored by current evaluation frameworks. Therefore, it becomes crucial to evaluate the performance of UMMs in the context of such computer-use scenarios. To fulfill this gap, we propose PlanViz, the first benchmark specially designed for image generation and editing PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks Figure 2. The overview of PlanViz. Our evaluation includes image generation and editing, with three proposed subtasks: route planning, work diagramming, and web&UI dsisplaying. Compared with existing benchmarks, we introduce new domain, computer-use tasks for the application of UMMs, and explore the planning capabilities of them with huge human effort. in computer-use tasks with planning. Note that several challenges exist on constructing the benchmark: (i) specifying the target tasks for evaluation, (ii) collecting high-quality data, as existing datasets and benchmarks for UMMs mainly include natural images and instructions based on them, (iii) defining comprehensive and exact judgment method since tasks may change lot across images and instructions. To solve challenge (i), we consider two core aspects mentioned above across numerous computer-use tasks: frequently involved in daily-life and including planning steps and choose three kinds of sub-tasks: route planning, workflow diagramming, and web&UI displaying, as shown in Figure 2. To solve challenge (ii), we manually collect and annotate the whole dataset with quality control. Additionally, we get inspired from MLLM-as-judge and propose new task-adaptive metric, PlanScore, for challenge (iii). PlanScore consists of three aspects: Cor, Vis and Ef. These aspects measure the correctness and visual quality of generated images, together with whether they contain unnecessary or irrelevant content. Regarding real-life experience that sometimes tasks include exact destinations or goals and sometimes not, we classify all questions into two categories: open-ended generation and closed-ended generation. In conclusion, our proposed benchmark provides comprehensive and detailed view of the capabilities of UMMs to finish image generation and editing in computer-use tasks. Extensive experiments are conducted on recent SOTA (stateof-the-art) UMMs and models only for image generation and editing. Results illustrate that inconsistency on performance exists across almost all models and tasks. This is likely due to the mismatch between the models mostly reactive generation behavior and the explicit planning requirements imposed by different tasks. Additionally, we discover an obvious gap between open-source and closed-source proprietary UMMs. Among all models and sub-tasks, GPTImage-1 shows the best performance, while it still struggles with editing tasks. Models also tend to perform better on close-ended questions than open-ended ones, suggesting that detailed, fine-grained guidance is important in our tasks. These findings highlight direction for future research. In summary, our contributions are threefold: We propose PlanViz, comprehensive and real-worldrelevant benchmark. To the best of our knowledge, this is the first benchmark evaluating image generation and editing in computer-use tasks with planning process. We propose PlanScore, task-adaptive judgment score including three aspects: correctness of generated images, visual quality, and measurements of unwanted parts. By utilizing MLLM-as-judge, this provides comprehensive, detailed and automated evaluation process. We conduct comprehensive evaluation and analysis on 13 open-source or propriety UMMs, together with 9 models only for image generation and editing. Results provide insightful findings about capabilities of UMMs in this new domain, and highlight areas for future improvement. 2 PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks 2. Related Work Computer-use tasks. Computer-use tasks include numerous sub-tasks like GUI interactions, workflow designing, map navigation and so on. These tasks are frequently involved in our daily life, and developing real-world AI assistants to solve these tasks has been growing trend (Cheng et al., 2024; Sun et al., 2025b;a; Xu et al., 2026). Therefore, we tend to evaluate whether image generation or editing models, especially UMMs, can finish these tasks well. UMMs. UMMs refer to models integrating understanding capabilities of MLLMs (Dubey et al., 2024; Bai et al., 2025; Team et al., 2023; Zhu et al., 2025; Liang et al., 2025a; Zang et al., 2025; Li et al., 2025b) and generation capabilities of diffusion or flow models (Croitoru et al., 2023; Rombach et al., 2022; Betker et al., 2023; Labs, 2024). Recent works on this topic (Achiam et al., 2023; Wu et al., 2024; Xie et al., 2024; Chen et al., 2025b; Deng et al., 2025; Li et al., 2025d) try to develop end-to-end UMMs based on this idea, providing strong multimodal reasoning capabilities benefiting generation. The latest state-of-the-art models, like GPTImage-1 (OpenAI, 2025a) and Gemini3-Pro-Image (Google, 2025) see huge improvement in understanding very detailed and complex instructions and generating images. Previous benchmarks & evaluation metrics for UMMs. Previous benchmarks typically contain only generating natural images. Some recent benchmarks include generating logically meaningful images or images requiring domain knowledge, like math or physics. UniEval (Li et al., 2025c), ROVER (Liang et al., 2025b), WiseEdit (Pan et al., 2025) and GenExam (Wang et al., 2025b) are some of them. However, they rarely explore whether UMMs can handle and plan various computer-use tasks and visualize them correctly. Similar to benchmarking image-generation models, evaluation metrics about image quality like LPIPS (Zhang et al., 2018), FID (Heusel et al., 2017) or object-level metrics (Ghosh et al., 2023) are frequently used. To evaluate whether generated images satisfy certain specific textual requirements, recent work (Liang et al., 2025b; Wang et al., 2025b) has also considered using MLLM-as-judge (Chen et al., 2024). Inspired by this, we design special evaluation metrics better fitting for our task. 3. PlanViz 3.1. Motivation: Task-planning-based Evaluation Table 1 shows the comparison of our benchmark to existing ones. Different from benchmarks built from natural or science images, our goal is to build comprehensive view of the capabilities of UMMs on both generating and editing images for everyday computer-use tasks. To achieve this goal, we introduce three sub-tasks frequently involved in daily life, and also include planning processes: route planning, workflow diagramming, and web&UI displaying. Then, we introduce the sub-tasks sequentially. Table 1. Comparison with other benchmarks. Benchmarks Domain Type Planning Keypoints ROVER (Liang et al., 2025b) Natural/science image GenExam (Wang et al., 2025b) Science(exam) image Mostly natural image WiseEdit (Pan et al., 2025) Computer-use image Gen&Edit PlanViz (ours) Edit Gen Edit Route planning. See Figure 2 left-top, route planning focuses on planning or designing route with specific needs. For image generation, the goal is to design and generate travel or hiking route that connects various landmarks, such as food destinations or historical sites, within renowned city or tourist area. For image editing, the task involves planning route on given map (like museum or theme park), according to predefined objectives. The route planning process aims to provide the well-optimized generated or edited routes by taking into account various factors, such as the order of destinations, distance, and user preferences. Workflow diagramming. See Figure 2 left-mid. Workflow diagramming focuses on designing flowcharts for common everyday works. For image generation, the evaluation covers wide range of workflows, including meeting preparation, scientific research, and so on, where the model is required to generate corresponding flowcharts or diagrams. For image editing, the evaluation not only includes modifying existing flowcharts according to requirements, but introduces more challenging tasks: based on webpage or app interface, flowchart must be drawn to show the process of completing specific task on that page. These tasks require UMMs to parse and transform knowledge about works or visual layouts into structured diagrams. Web&UI displaying. See Figure 2 left-bottom, this subtask focuses on generating or editing website or UI interfaces to meet user requirements. For image generation, the evaluation covers generating UI interfaces with different icons, numbers of labels, and relationships between their positions. For image editing, the task goes beyond simple modifications such as changing colors or shapes. It involves more challenging editing process. Based on the current state of website or UI and provided user interactions, models are queried to generate the potential state of the page after those actions. The evaluation of image editing under this sub-task tests the models abilities like implicit mathematical operations, multi-step actions and so on. Open-ended & closed-ended. Besides proposed sub-tasks, we notice that computer-use tasks in real-life sometimes include clear, detailed goals like buying specified thing on specified website, while sometimes only include rough directions like booking room. Therefore, we categorize all questions as open-ended and closed-ended. Open-ended questions have no fixed answers, whereas closed-ended ones usually have more well-defined answers and allow fine-grained scoring. There are 231 open and 129 closed questions in total. Examples are in the supplementary. 3 PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks Figure 3. Pipeline of data construction. It consists of four stages: high-quality data collecting and cleaning, human annotation, quality check, and prompt style transformation. These stages are displayed from the top-left to the bottom right. 3.2. Data Construction See Figure 3, the purpose of this part is to introduce the data construction process of PlanViz in detail. Step 1. High-quality data collection. For all editing subtasks, we manually1 collect the source images from websites or apps. This step includes manually taking screenshots (details in the supplementary material) from various websites and apps. We obey the three key rules to collect high-quality images: (i) images should be clear, with the roads (route planning), patterns and layouts clearly visible, (ii) the texts, if they exist, should also be easy to recognize, (iii) chosen images are better to be taken from real-world maps, GUI screens, etc. Totally, nearly 500 images are collected. We double-check all of them and leave 60 high-quality ones suitable for each sub-task of editing. Notably, the state change is also recorded for web&UI displaying tasks, described in Section 3.3. Then, unique questions are manually designed for each image. For generation sub-tasks, we also manually provide 60 questions each to have balanced evaluation. Along with the process, whether questions are open-ended or closed-ended is also recorded. Overall, the data distribution and topic-level word cloud are shown in Figure 4. Note that the number of questions is similar to prior works (Wang et al., 2025b; Zhao et al., 2025). Step 2. Human annotation. We note that, under our image editing settings, the solutions are potentially non-unique. However, there often exist relatively better answers that can be identified through human annotation. Therefore, we 1manually here means that well-trained Ph.D. candidate students in Computer Science, who have led the construction of at least 2 other benchmarks, do this work manually. The same meaning is implied in the following context. conduct human annotation process for them and provide reference images serving as reference answers, shown in the supplementary material. Notably, for web&UI displaying, we take the action mentioned in our question on the real screens, and record the screenshot of the changed screens. These reference images are provided for the MLLM when judging scores. We also manually annotate the key points of each question for scoring. Details are in Section 3.3. Step 3. Quality check. To ensure the quality of human annotation, we use blind-test strategy. In detail, another human annotators are recruited who do not know the ones build the dataset, to check two things2: (i) the difficulty of each question is proper or not (ii) the annotated reference images and keypoints (Section 3.3) are correct or not. If both are satisfied, this question is given 1 point; if not, 0 points are given. The total annotation score is 0.96, suggesting strong agreement on the annotation. For the controversial cases, all annotators are called together to modify them. Step 4. Prompt style transformation. We also hope that the language styles of questions can be diverse, due to the fact that various language styles can be used by users in daily life. For this purpose, we utilize GPT-4o (Achiam et al., 2023) to transform the language style of each query to ensure the diversity of them, to the greatest extent possible. 3.3. Score Judgement Pipeline To tackle the problems in Section 1, we propose special task-adaptive score, PlanScore, to provide comprehensive and accurate view of UMMs abilities in this era. 2They do this by discussing together and asking GPT-5.1 (OpenAI, 2025b) 4 PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks Table 2. Correlation between human and model judgments."
        },
        {
            "title": "Models",
            "content": "OmniGen2 GPT-Image-1 & Metrics"
        },
        {
            "title": "Ef Cor Vis",
            "content": "Ef"
        },
        {
            "title": "MLLM Mean\nHuman Mean",
            "content": "0.20 0.71 0.52 0.65 0.84 0.86 0.18 0.76 0.48 0.68 0.78 0.81 Human-MLLM 0.92 0.83 0.85 0.88 0.79 0.83 Human-MLLM MAE 0.02 0.05 0.04 0.03 0.08 0.05 Detailed statistics and prompts are in the supplementary material. In sum, our method of judging scores can provide an exact and comprehensive view of generated images. 3.4. Human Evaluation We include human study to test whether our judgment method is consistent with human intuition. Regarding (Wang et al., 2025b; Zhao et al., 2025), 10 human annotators are recruited here. We randomly select 100 samples individually from the generation results of OmniGen2 and GPT-Image-1 of both generating and editing tasks, and for each image generated by them, we employ our annotators to judge the PlanScore. we report the mean, Pearson correlation coefficient (r) and Mean Absolute Error (MAE) in Table 2. During evaluation, we notice that MLLMs sometimes assess higher Vis and Ef scores in our tasks, but not much. Study results suggest that strong agreement exists between human judgments and MLLM judgments with r>0.8 mostly, MAEs in an acceptable scope. 4. Experiment The purpose of this section is to benchmark the performance of recent UMMs, together with prior image generation and editing models with our proposed tasks. Main results and findings are in Section 4.2, including score distribution and influence of prompt styles. Wed like to show example cases in Section 4.3. Note that in our experiments, Cor is the most important score to measure whether models can plan tasks and generate with correct and useful images. 4.1. Implementation Models. We apply recent state-of-the-art open-source or closed-source SOTA UMMs. Some prior models are also utilized. Model list is in the supplementary material. Evaluation settings. We evaluate all closed-source models through their official API. Evaluation details are in the supplementary. Notably, we randomly select subset of 50 images from all generations and utilize the Python API of Qwen3-VL-235B-A22B-Instruct, mentioned in Section 3.3 to judge them 10 times. We discover that very similar scores are given, and after calculating the Coefficient of Variation (Abdi, 2010) ( standard deviation 100%), we gain 3.4% score, suggesting the stability of our evaluation. mean Figure 4. The distribution (left) and the word cloud (right) of our benchmark. The green part represents route planning, while the orange part represents workflow diagramming, and the blue part represents web&UI displaying. The word cloud shows the hot topics in all questions of our benchmark. Sample cases of generation results are checked to make sure the dimensions of our score. Details are in the supplementary. Based on these observations, PlanScore consists of three dimensions. (i) Correctness (Cor), which requires the models to generate correct images successfully solving the task with obeying certain rules. For instance, the planned route should follow existing roads, paths, etc. (ii) Visual Quality (Vis), which measures whether models generate visually and semantically coherent images. For editing tasks in route planning and web&UI displaying, the generated images are also evaluated to determine whether they keep the unchanged layout, details, objects, texts, and so on in the original images. Notably, editing tasks in workflow diagramming require workflows as results, and these workflows do not need to be similar to the original websites. (iii) Efficiency (Ef ), which measures whether the models generate images without including items not required. To quantify these dimensions, we adopt task-adaptive MLLM-as-judge (Liu et al., 2025) as an automated evaluation pipeline. recently published MLLM with strong capabilities of fine-grained visual understanding and making judgments, Qwen3-VL-235B-A22B-Instruct (Team, 2025), is leveraged to make the judgment. Details of this model are in the supplementary material. Moreover, its crucial to provide MLLMs with detailed rating levels and scoring criteria to get exact scores. For Cor, we introduce key-point-based measurement. unique set of key score points is annotated to each question, including points from general (like complete diagram should be provided) to specific (like the planned route should visit two museums in the image). After that, we let the MLLM judge the set of satisfied key points Ps. So we have Cor= Ps , where . is the number of elements in set. As mentioned in Section 3.2, unique human-annotated reference images are also given MLLM in judgment of this score, to help it better understand the tasks and what correct answers should be. For the other two dimensions, the full score is set to 5. The MLLM is requested to give scores Sv (Visual Quality) and Se (Efficiency) between 0 and 5 through special prompts. We then do Vis=Sv/5 and Ef =Se/5 to have the same scale of three dimensions. Finally, we have Cor, is, Ef [0, 1]. 5 PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks Table 3. Main results on image editing tasks. Avg means the average of scores. All scores are between 0 and 1. We highlight the best performance of correctness score (Cor), for it is the most important score. We also add scores from our annotated reference images."
        },
        {
            "title": "Workflow Diagraming",
            "content": "Web&UI Displaying"
        },
        {
            "title": "Overall",
            "content": "Cor Vis Ef Avg Cor Vis Ef Avg Cor Vis Ef Avg Avg AnyEdit (Yu et al., 2025) OmniGen (Xiao et al., 2024) UltraEdit (Zhao et al., 2024) SD-3.5-large (Rombach et al., 2022) Step1X-Edit-v1p2 (Liu et al., 2025) Step1X-Edit-v1p2 (Thinking) (Liu et al., 2025) FLUX.1-Kontext-dev (Labs, 2024) Hidream-E1-Full (Cai et al., 2025) Qwen-Image-Edit (Wu et al., 2025a) Onecat (Li et al., 2025a) Ovis-U1 (Wang et al., 2025a) Janus-4o (Chen et al., 2025a) OmniGen2 (Wu et al., 2025b) UniPic2-Metaquery (Wei et al., 2025) UniPic2-Metaquery-GRPO (Wei et al., 2025) NextStep-1-Large-Edit (Team et al., 2025) Bagel (Deng et al., 2025) Bagel (Thinking) (Deng et al., 2025) Wan-2.5-i2i-preview (Wan et al., 2025) Seedream-4.5 (Seedream et al., 2025) GPT-Image-1 (OpenAI, 2025a) Gemini3-Pro-Image (Google, 2025) 1B 0.00 4B 0.00 8B 0.02 8B 0.02 12B 0.02 12B 0.04 12B 0.11 17B 0.00 20B 0.15 3B 0.00 3B 0.17 7B 0.20 7B 0.27 9B 0.26 9B 0.00 14B 0.00 14B 0.04 14B 0. 0.12 0.39 0.42 0.11 0.82 0.70 0.77 0.62 0.44 0.29 0.41 0.29 0.75 0.64 0.91 0.63 0.57 0.42 0.33 0.25 0.73 0.57 0.16 0.33 0.32 0.19 0.37 0.29 0.84 0.78 0.72 0.52 0.79 0.54 0.34 0.21 0.03 0.73 0.37 0.23 0.70 0.56 0.85 0.69 0.59 0.63 0.06 0.53 0.51 0.46 0.25 0.24 0.47 0.53 0.37 0.19 0.48 0.16 0.23 0.29 0.63 0.50 0.44 0.18 0.27 0. 0.46 0.64 0.55 0.23 0.00 0.00 0.05 0.01 0.03 0.05 0.02 0.11 0.15 0.00 0.08 0.00 0.00 0.06 0.12 0.03 0.05 0.10 0.18 0.51 0.42 0.35 0.50 0.31 0.58 0.16 0.54 0.17 0.30 0.17 0.49 0.29 0.48 0.31 0.36 0.20 0.19 0.15 0.36 0.29 0.10 0.12 0.31 0.13 0.36 0.11 0.69 0.44 0.46 0.13 0.44 0.24 0.26 0.14 0.33 0.05 0.57 0. 0.57 0.41 0.87 0.83 0.93 0.92 0.01 0.72 0.27 0.25 0.25 0.16 0.27 0.28 0.19 0.15 0.27 0.07 0.17 0.16 0.38 0.22 0.27 0.14 0.14 0.27 0.39 0.74 0.76 0.36 0.01 0.01 0.03 0.02 0.01 0.05 0.00 0.02 0.07 0.02 0.05 0.00 0.05 0.04 0.14 0.00 0.07 0. 0.18 0.30 0.30 0.12 0.56 0.52 0.34 0.31 0.71 0.58 0.14 0.22 0.76 0.59 0.77 0.70 0.57 0.40 0.26 0.20 0.60 0.61 0.09 0.19 0.41 0.30 0.30 0.19 0.69 0.56 0.37 0.40 0.62 0.56 0.16 0.00 0.02 0.06 0.04 0.10 0.82 0.78 0.79 0.78 0.57 0.75 0.02 0.30 0.36 0.22 0.44 0.13 0.45 0.51 0.32 0.16 0.43 0.10 0.25 0.16 0.43 0.27 0.44 0.05 0.05 0. 0.59 0.62 0.53 0."
        },
        {
            "title": "Human Reference",
            "content": "1.00 0.96 0.98 0.98 1.00 0.97 1.00 0. 1.00 0.98 0.97 0.98 0.36 0.31 0.31 0.18 0.40 0.44 0.29 0.17 0.39 0.11 0.22 0.20 0.48 0.33 0.38 0.13 0.15 0.18 0.48 0.67 0.61 0. 0.98 4.2. Main Results Table 3 (editing) and Table 4 (generation) show the performance of recruited models on proposed tasks. The experimental results highlight five key findings. ① Challenges and strengths of PlanViz. Experimental results underscore capability gap: whereas image editing remains universally challenging with even SOTA models (0.61-0.67) trailing human-annotated ground truth by wide margin. Moreover, image generation exhibits sharp performance divide between closed-source proprietary models (e.g., Seedream 4.5, 0.88) and open-source counterparts (typically <0.55). As for Cor, the trend seems clearer. The Cors of SOTA models in image editing tasks are from 0.3 to 0.5, falling far behind the human references. While in image generation, when proprietary models gain Cor near 0.8-0.9, those from almost all open-source models are below 0.5, and open-source UMMs do not show very obvious advantages compared with image-generation-only models. We also discover that wide range of models perform poorly on Vis and Ef, suggesting that visual incoherence and inconsistency, and unwanted parts in images occur frequently. These observations across sub-tasks and model tiers confirm that our benchmark is challenging for models. In sum, PlanViz effectively avoids the problem of evaluation saturation, and also highlighting challenges and research directions among current models, especially open-source UMMs. ② Performance inconsistency between generation and editing tasks. cross-comparison of Table 3 and Table 4 reveals pronounced performance inconsistency in models, where image editing lags behind image generation. Empirical evidence shows significant degradation in both Cor and overall average scores across nearly all tasks. For instance, Qwen-Image models achieve Cor of 0.46, 0.77, and 0.77 in the generation tasks, but plummet to no more than 0.15 in the editing tasks, while Seedream-4.5 experiences decline from 0.88 to 0.67 in overall score. This performance gap suggests that while current models, including UMMs, are adept at mapping high-level semantics to visual pixels for generation tasks, they struggle to handle the dual-constraint of task in editing tasks. Unlike generation, which allows for higher stochastic variance, editing necessitates fine-grained spatial reasoning and complex planning process, representing higher complexity for models. ③ Performance inconsistency between sub-tasks. Additionally, we also discover that in image generation tasks, route planning is much more difficult than the other two tasks, as the SOTA of Cor is only 0.81, compared with about 0.90 of workflow design and web&UI displaying. While in image editing, all three tasks are big challenges for models, as the SOTA scores are much lower. The performances of other models are also sensitive to the categories of sub-tasks. These are likely due to the mismatch between the models mostly reactive generation behavior and the explicit planning requirements imposed by different tasks. Our findings suggest that modern UMMs still have long way to go on visualizing complex planning processes, like detailed routes or workflows from websites. PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks Table 4. Main results on image generation tasks. Avg means the average of scores. All scores are between 0 and 1. We highlight the best performance of correctness score (Cor), for it is the most important score on these tasks. Models Params Route Planning Workflow Diagramming Web & UI Desplaying Overall Cor Vis Ef Avg Cor Vis Ef Avg Cor Vis Ef Avg Avg OmniGen (Xiao et al., 2024) SD-3.5-large (Rombach et al., 2022) FLUX.1 dev (Labs, 2024) Hidream-I1-Full (Cai et al., 2025) Qwen-Image (Wu et al., 2025a) Onecat (Li et al., 2025a) Ovis-U1 (Wang et al., 2025a) Janus-4o (Chen et al., 2025a) OmniGen2 (Wu et al., 2025b) UniPic2-Metaquery (Wei et al., 2025) UniPic2-Metaquery-GRPO (Wei et al., 2025) NextStep-1-Large (Team et al., 2025) Bagel (Deng et al., 2025) Bagel-Thinking (Deng et al., 2025) Wan-2.5-t2i-preview (Wan et al., 2025) Seedream-4.5 (Seedream et al., 2025) GPT-Image-1 (OpenAI, 2025a) Gemini3-Pro-Image (Google, 2025) 4B 8B 12B 17B 20B 3B 3B 7B 7B 9B 9B 14B 14B 14B 0.01 0.24 0.10 0.03 0.46 0.04 0.11 0.09 0.13 0.02 0.14 0.00 0.07 0.42 0.73 0.81 0.67 0. 0.85 0.81 0.95 0.85 0.74 0.44 0.79 0.93 0.88 0.87 0.79 0.59 0.94 0.85 0.74 0.78 0.96 0.89 0.75 0.60 0.51 0.27 0.66 0.50 0.66 0.87 0.65 0.45 0.54 0.02 0.73 0.70 0.90 0.89 0.98 0. 0.54 0.55 0.52 0.38 0.62 0.33 0.52 0.63 0.55 0.45 0.49 0.20 0.58 0.66 0.79 0.83 0.87 0.82 0.01 0.26 0.05 0.00 0.77 0.01 0.15 0.25 0.23 0.00 0.13 0.00 0.09 0.22 0.91 0.88 0.89 0. 0.43 0.45 0.46 0.56 0.71 0.22 0.43 0.47 0.43 0.55 0.44 0.58 0.39 0.56 0.86 0.94 0.97 0.82 0.20 0.31 0.18 0.11 0.84 0.27 0.19 0.29 0.23 0.05 0.24 0.00 0.13 0.39 0.90 0.92 0.92 0. 0.21 0.34 0.23 0.22 0.77 0.17 0.24 0.31 0.22 0.20 0.27 0.19 0.20 0.39 0.89 0.91 0.93 0.86 0.21 0.67 0.70 0.06 0.77 0.19 0.27 0.43 0.60 0.50 0.54 0.00 0.35 0.55 0.80 0.86 0.92 0. 0.75 0.58 0.76 0.54 0.84 0.30 0.48 0.57 0.71 0.60 0.62 0.96 0.62 0.69 0.79 0.92 0.92 0.90 0.24 0.45 0.60 0.12 0.78 0.33 0.35 0.62 0.58 0.56 0.56 0.00 0.44 0.69 0.85 0.88 0.90 0. 0.40 0.57 0.69 0.24 0.80 0.27 0.37 0.54 0.63 0.55 0.57 0.32 0.47 0.64 0.81 0.89 0.91 0.87 0.38 0.49 0.48 0.28 0.73 0.26 0.38 0.49 0.47 0.40 0.44 0.24 0.42 0.56 0.83 0.88 0.80 0. Table 5. Performance comparison between open-ended questions and closed-ended questions. Each score is the average of three sub-tasks. Best correctness is highlighted. Models Open (Gen) Closed (Gen) Open (Edit) Closed (Edit) Cor Vis Ef Cor Vis Ef Cor Vis Ef Cor Vis Ef 0.20 0.71 0.39 0.67 0.79 0.63 0.05 0.55 0.39 0.05 0.47 0.31 FLUX.1 dev 0.14 0.58 0.40 0.23 0.50 0.93 0.05 0.34 0.30 0.14 0.35 0.19 Ovis-U1 0.23 0.66 0.47 0.78 0.76 0.60 0.14 0.79 0.58 0.08 0.75 0.60 OmniGen2 0.11 0.66 0.43 0.47 0.61 0.45 0.08 0.08 0.14 0.09 0.17 0.07 Bagel GPT-Image-1 0.83 0.94 0.94 0.96 1.00 0.93 0.39 0.55 0.75 0.44 0.84 0. ④ Task-specific sensitivity of thinking-based generation. The integration of Thinking\" mechanisms, including GRPO (Guo et al., 2025), within UMMs exhibits nonuniform effectiveness across different task categories in PlanViz. As evidenced in Table 3, the Thinking variant of Step1X-Edit-v1p2 shows marginal improvement in overall average scores (+0.04). For correctness scores, the performance improvement of Step1X-Edit-v1p2 only increases by 0.02-0.04. Models like Bagel and UniPic2-Metaquery see similar situation, and even in some cases, the thinking mode leads to worse performance, like web&UI displaying for Bagel and route planning for UniPic2-Metaquery. However, results in Table 4 reveal different trend. In image generation tasks, thinking mode improves the Cor of bagel on all sub-tasks by up to 0.35, and UniPic2-Metaquery by up to 0.13, much more than image editing tasks. This phenomenon suggests that specialized reasoning processes do not interact well with the generation process in our benchmark. Consequently, the bottleneck for UMMs on our proposed tasks appears to reside in generating precisely, following the guidance of reasoning steps. ⑤ Decoupling of Cor and other scores. Evidence from Tables 3 and 4 shows that many models maintain high Vis scores (>0.70) despite near-zero Cor, indicating they frequently generate \"visually plausible but semantically misFigure 5. Score distribution across different models. We choose Cor (top) and Vis (bottom) of route planning. match\" content. Conversely, several cases that achieve much higher Cor often suffer regression in Vis and Ef, like Janus4o in editing tasks of route planning. This trade-off suggests that while these models attempt to adhere to logical constraints, they lack the representational robustness or domainspecific knowledge to maintain visual fluency under strict instruction, resulting in correct but degraded outputs that hit isolated scoring points but fail in overall synthesis. Consequently, bridging this gap remains the primary challenge for modern UMMs. 4.2.1. OPEN-ENDED V.S. CLOSED-ENDED We include this study to explore how open-ended and closedended questions influence the generation results. One imageonly model, three open-source UMMs, and one closedsource UMM are chosen. Results are in Table 5. Its patently obvious that GPT-Image-1 performs better than all opensource models, supporting the findings in Section 4.2. More importantly, it seems that the Cors of almost all closedended cases are better than those of open-ended cases. This may suggest that proper constraints and details can help UMM plan and generate better in this domain. 7 PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks Figure 6. Case studies of different models on different sub-tasks. We choose both open-source and closed-source models. 4.2.2. DISTRIBUTION OF SCORES 4.3. Case Study We visualize the score distribution of several models in Figure 5. Cor of generation (left) and editing (right) of route planning are reported here. More figures of distributions are in the supplementary material. In generation tasks, the score densities of open-source models are heavily concentrated near 0.0-0.2, while those of GPT-Image-1 are concentrated near 0.8-1.0. On the contrary, in the editing tasks, all models see similar trend, with score densities concentrated at the bottom. GPT-Image-1 shows slightly better performance than the best open-source model, OmniGen2, with the score densities at higher levels wider in this task. These observations are consistent with findings about model performance in Section 4.2. For Vis, the score densities concentrate at the top in the generation tasks, while in the editing tasks the score distribution of most models obviously sees an upward shift compared to Cor. This also supports the visually plausible but semantically mismatch findings in Section 4.2. 4.2.3. INFLUENCE OF PROMPT STYLES. To study whether changes in prompt styles but not actual query meanings influence generation results, we randomly sample 10 questions and use GPT-5.1 (OpenAI, 2025b) to create 10 language styles of each. Then they are provided with several UMMs as inputs for results. Scores and more cases can be found in the supplementary material. According to that, we discover that (i) open-source UMMs are influenced more than GPT-Image-1, as the standard deviations compared to mean scores are higher (ii) performance of open-source UMMs varies, however, similar score can be seen on the same question across these models in many cases (like Cor of question 5, is of question 3, Ef of question 7, etc.). This suggests that open-source UMMs still cannot remain robust to all kinds of input styles under our task, providing future research direction for UMMs. Figure 6 displays some cases from different models. We place the results of both generation and editing tasks here. See generation sub-tasks, some open-source models (like Bagel, FLUX.1-dev) can generate the basic structure corresponding to the query (like curved line on place with foods on the ground for route planning, or some boxes with texts for workflow diagramming), but they fail to generate queried details (like real food name, place, real steps of work, etc.) Other ones generate things more irrelevant, like UniPic2-Metaquery and Janus-4o here, suggesting fundamental challenge for open-source UMMs. In contrast, closed-source models make fewer mistakes. Gemini3-ProImage and GPT-Image-1 can generate generally correct images. Seeing editing sub-tasks, all models perform worse: mistakes like no editing (Step1X-Edit), image style totally changed (Gemini3-Pro-Image), and the route not being completed often happen. This study supports the findings in Section 4.2, and underscores the need for improvements on layout understanding, spatial reasoning and so on for UMMs. More cases are in the supplementary material. 5. Conclusion We present PlanViz, the first benchmark for evaluating planning-oriented image generation and editing in computeruse scenarios. By introducing diverse planning sub-tasks with high-quality human annotations and task-adaptive evaluation metric, our benchmark reveals inconsistency on performance of current UMMs and exposes clear limitations of them, especially in planning-intensive image editing. Our results also indicate that planning for computer-use-based generation and editing remains major challenge beyond surface-level image quality. We hope that PlanViz will facilitate future research on bridging reasoning, planning, and visual generation for practical multimodal systems. 8 PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks"
        },
        {
            "title": "References",
            "content": "Abdi, H. Coefficient of variation. Encyclopedia of research design, 2010. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv, 2023. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv, 2025. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2023. Cai, Q., Chen, J., Chen, Y., Li, Y., Long, F., Pan, Y., Qiu, Z., Zhang, Y., Gao, F., Xu, P., et al. Hidream-i1: highefficient image generative foundation model with sparse diffusion transformer. arXiv, 2025. Chen, D., Chen, R., Zhang, S., Wang, Y., Liu, Y., Zhou, H., Zhang, Q., Wan, Y., Zhou, P., and Sun, L. Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In ICML, 2024. Chen, J., Cai, Z., Chen, P., Chen, S., Ji, K., Wang, X., Yang, Y., and Wang, B. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv, 2025a. Chen, X., Wu, Z., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., and Ruan, C. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv, 2025b. Cheng, K., Sun, Q., Chu, Y., Xu, F., YanTao, L., Zhang, J., and Wu, Z. Seeclick: Harnessing gui grounding for advanced visual gui agents. In ACL, 2024. Chow, W., Pan, J., Liang, Y., Zhou, M., Song, X., Jia, L., Zhang, S., Tang, S., Li, J., Zhang, F., et al. Weave: Unleashing and benchmarking the in-context interleaved comprehension and generation. arXiv preprint arXiv:2511.11434, 2025. Croitoru, F.-A., Hondru, V., Ionescu, R. T., and Shah, M. Diffusion models in vision: survey. TPAMI, 2023. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. Google. Gemini 3 pro image (nano banana pro). https://aistudio.google.com/models/ gemini-3-pro-image, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv, 2025. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. Labs, B. F. Flux. black-forest-labs/flux, 2024. https://github.com/ Li, H., Peng, X., Wang, Y., Peng, Z., Chen, X., Weng, R., Wang, J., Cai, X., Dai, W., and Xiong, H. Onecat: Decoder-only auto-regressive model for unified understanding and generation. arXiv, 2025a. Li, J., Zhang, D., Wang, X., Hao, Z., Lei, J., Tan, Q., Zhou, C., Liu, W., Yang, Y., Xiong, X., et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. In AAAI, 2025b. Li, Y., Wang, H., Zhang, Q., Xiao, B., Hu, C., Wang, H., and Li, X. Unieval: Unified holistic evaluation for unified multimodal understanding and generation. arXiv, 2025c. Li, Z., Li, H., Shi, Y., Farimani, A. B., Kluger, Y., Yang, L., and Wang, P. Dual diffusion for unified image generation and understanding. In CVPR, 2025d. Liang, W., Sun, G., He, Y., Dong, J., Dai, S., Laptev, I., Khan, S., and Cong, Y. Pixelvla: Advancing pixel-level understanding in vision-language-action model. arXiv, 2025a. Liang, Y., Chow, W., Li, F., Ma, Z., Wang, X., Mao, J., Chen, J., Gu, J., Wang, Y., and Huang, F. Rover: Benchmarking reciprocal cross-modal reasoning for omnimodal generation. arXiv, 2025b. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., Shi, G., and Fan, H. Emerging properties in unified multimodal pretraining. arXiv, 2025. Liu, S., Han, Y., Xing, P., Yin, F., Wang, R., Cheng, W., Liao, J., Wang, Y., Fu, H., Han, C., et al. Step1x-edit: practical framework for general image editing. arXiv, 2025. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv, 2024. 9 OpenAI."
        },
        {
            "title": "Introducing our",
            "content": "image generation model in the api. https://openai.com/index/ image-generation-api/, 2025a. latest PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks OpenAI. Introducing gpt-5. https://openai.com/ index/introducing-gpt-5/, 2025b. Pan, K., Chen, W., Qiu, H., Yu, Q., Bu, W., Wang, Z., Zhu, Y., Li, J., and Tang, S. Wiseedit: Benchmarking cognition-and creativity-informed image editing. arXiv, 2025. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Seedream, T., Chen, Y., Gao, Y., Gong, L., Guo, M., Guo, Q., Guo, Z., Hou, X., Huang, W., Huang, Y., et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv, 2025. Sun, Y., Zhao, S., Yu, T., Wen, H., Va, S., Xu, M., Li, Y., and Zhang, C. Gui-xplore: Empowering generalizable gui agents with one exploration. In CVPR, 2025a. Sun, Z., Liu, Z., Zang, Y., Cao, Y., Dong, X., Wu, T., Lin, D., and Wang, J. Seagent: Self-evolving computer use agent with autonomous learning from experience. arXiv, 2025b. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv, 2023. Team, N., Han, C., Li, G., Wu, J., Sun, Q., Cai, Y., Peng, Y., Ge, Z., Zhou, D., Tang, H., Zhou, H., Liu, K., Huang, A., Wang, B., Miao, C., Sun, D., Yu, E., Yin, F., Yu, G., Nie, H., Lv, H., Hu, H., Wang, J., Zhou, J., Sun, J., Tan, K., An, K., Lin, K., Zhao, L., Chen, M., Xing, P., Wang, R., Liu, S., Xia, S., You, T., Ji, W., Zeng, X., Han, X., Zhang, X., Wei, Y., Xu, Y., Jiang, Y., Wang, Y., Zhou, Y., Han, Y., Meng, Z., Jiao, B., Jiang, D., Zhang, X., and Zhu, Y. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. arXiv, 2025. Team, Q. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv, 2025. Wang, G.-H., Zhao, S., Zhang, X., Cao, L., Zhan, P., Duan, L., Lu, S., Fu, M., Zhao, J., Li, Y., and Chen, Q.-G. Ovisu1 technical report. arXiv preprint arXiv:2506.23044, 2025a. Wang, Z., Yin, P., Zhao, X., Tian, C., Qiao, Y., Wang, W., Dai, J., and Luo, G. Genexam: multidisciplinary textto-image exam. arXiv, 2025b. Wei, H., Xu, B., Liu, H., Wu, C., Liu, J., Peng, Y., Wang, P., Liu, Z., He, J., Xietian, Y., et al. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model. arXiv, 2025. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., Yin, S.-m., Bai, S., Xu, X., Chen, Y., et al. Qwen-image technical report. arXiv, 2025a. Wu, C., Zheng, P., Yan, R., Xiao, S., Luo, X., Wang, Y., Li, W., Jiang, X., Liu, Y., Zhou, J., et al. Omnigen2: Exploration to advanced multimodal generation. arXiv, 2025b. Wu, Y., Zhang, Z., Chen, J., Tang, H., Li, D., Fang, Y., Zhu, L., Xie, E., Yin, H., Yi, L., et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv, 2024. Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R., Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image generation. arXiv, 2024. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv, 2024. Xu, Z., Chen, D., Wang, S., Li, J., Wang, C., Han, M., and Wang, Y. Adamarp: An adaptive multi-agent interaction framework for general immersive role-playing. arXiv, 2026. Yu, Q., Chow, W., Yue, Z., Pan, K., Wu, Y., Wan, X., Li, J., Tang, S., Zhang, H., and Zhuang, Y. Anyedit: Mastering unified high-quality image editing for any idea. In CVPR, 2025. Zang, J., Wei, Y., Bai, R., Jiang, S., Mo, N., Li, B., Sun, Q., and Liu, H. Reward auditor: Inference on reward modeling suitability in real-world perturbed scenarios. arXiv, 2025. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Zhao, H., Ma, X. S., Chen, L., Si, S., Wu, R., An, K., Yu, P., Zhang, M., Li, Q., and Chang, B. Ultraedit: Instructionbased fine-grained image editing at scale. In NeurIPS, 2024. Zhao, X., Zhang, P., Tang, K., Zhu, X., Li, H., Chai, W., Zhang, Z., Xia, R., Zhai, G., Yan, J., et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv, 2025. PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks Zhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Tian, H., Duan, Y., Su, W., Shao, J., et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv, 2025."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Shanghai Jiao Tong University"
    ]
}