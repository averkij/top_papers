{
    "paper_title": "SynFinTabs: A Dataset of Synthetic Financial Tables for Information and Table Extraction",
    "authors": [
        "Ethan Bradley",
        "Muhammad Roman",
        "Karen Rafferty",
        "Barry Devereux"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Table extraction from document images is a challenging AI problem, and labelled data for many content domains is difficult to come by. Existing table extraction datasets often focus on scientific tables due to the vast amount of academic articles that are readily available, along with their source code. However, there are significant layout and typographical differences between tables found across scientific, financial, and other domains. Current datasets often lack the words, and their positions, contained within the tables, instead relying on unreliable OCR to extract these features for training modern machine learning models on natural language processing tasks. Therefore, there is a need for a more general method of obtaining labelled data. We present SynFinTabs, a large-scale, labelled dataset of synthetic financial tables. Our hope is that our method of generating these synthetic tables is transferable to other domains. To demonstrate the effectiveness of our dataset in training models to extract information from table images, we create FinTabQA, a layout large language model trained on an extractive question-answering task. We test our model using real-world financial tables and compare it to a state-of-the-art generative model and discuss the results. We make the dataset, model, and dataset generation code publicly available."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 2 6 2 4 0 . 2 1 4 2 : r SynFinTabs: Dataset of Synthetic Financial Tables for Information and Table Extraction Ethan Bradley Muhammad Roman Karen Rafferty Barry Devereux School of Electronics, Electrical Engineering and Computer Science Queens University Belfast Belfast, UK {ebradley24,m.roman,k.rafferty,b.devereux}@qub.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Table extraction from document images is challenging AI problem, and labelled data for many content domains is difficult to come by. Existing table extraction datasets often focus on scientific tables due to the vast amount of academic articles that are readily available, along with their source code. However, there are significant layout and typographical differences between tables found across scientific, financial, and other domains. Current datasets often lack the words, and their positions, contained within the tables, instead relying on unreliable OCR to extract these features for training modern machine learning models on natural language processing tasks. Therefore, there is need for more general method of obtaining labelled data. We present SynFinTabs, large-scale, labelled dataset of synthetic financial tables. Our hope is that our method of generating these synthetic tables is transferable to other domains. To demonstrate the effectiveness of our dataset in training models to extract information from table images, we create FinTabQA, layout large language model trained on an extractive question-answering task. We test our model using real-world financial tables and compare it to state-of-the-art generative model and discuss the results. We make the dataset, model, and dataset generation code publicly available1."
        },
        {
            "title": "Introduction",
            "content": "In the digital era, the vast amount of financial information contained within documents (for example, accounting reports) necessitates efficient extraction methods to unlock the valuable information. Among the various elements embedded within textual content, tables stand as pivotal repositories of structured financial information. However, the extraction of these tables from unstructured document images presents multifaceted challenge that 1https://ethanbradley.co.uk/research/ synfintabs intersects computer vision, machine learning, and information retrieval. Document images, whether scanned, cameracaptured, or converted from other document formats, encompass myriad of complexities. Tables within these documents vary widely in format, style, and layout, often presenting irregularities such as merged cells, diverse fonts, and intricate borders. Extracting these tables accurately requires sophisticated algorithms capable of discerning patterns amidst this inherent variability. Within Document AI (Cui et al., 2021), the reliance on robust and diverse datasets is fundamental for implementing practical solutions. Nevertheless, the limitations associated with obtaining large-scale, fully annotated datasets have persisted, hindering progress in the area of table extraction from document images. Furthermore, privacy concerns reduce the appetite of businesses to use private datasets containing sensitive information to train machine learning models, particularly those hosted by third-party service providers. When training Document AI foundation models, such as the LayoutLM family of models (Xu et al., 2020, 2021; Huang et al., 2022), incorporating layout information enriches the understanding of documents by introducing the crucial dimension of spatial context. This spatial awareness enables deeper comprehension of semantics and contextual nuances. Understanding the placement of word or number in table, whether in header or data cell, drastically alters its significance. Many existing table extraction datasets rely on optical character recognition (OCR) software to extract the words and the 2D positions of the words from images of the documents. However, as we will demonstrate, OCR is not always perfect and, in some cases, struggles significantly with the recognition of text presented in tabular format. Therefore, it is important that any dataset, used to train these machine learning models on table extraction tasks, contains accurate ground truth 2D position information for the text within the tables. To accelerate the development of artificial intelligence systems for table extraction from document images in the financial domain, we propose new, large-scale, labelled dataset, SynFinTabs (Synthetic Financial Tables). The dataset aims to capture the structural and presentation qualities of tables found in financial statements filed with Companies House2, financial spreadsheets, and typeset company reports, such as annual reports. Using SynFinTabs, we have fine-tuned LayoutLM (Xu et al., 2020), on table visual questionanswering task regarding the contents of the tables, to create our model, FinTabQA. We will discuss the experiments we have carried out with FinTabQA and the impact of OCR on the end-to-end performance."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we discuss existing datasets for table extraction tasks, previous uses of synthetic data for Document AI tasks, and current approaches to table question-answering."
        },
        {
            "title": "2.1 Table Datasets",
            "content": "The ICDAR 2013 Table Competition dataset (Göbel et al., 2013) was designed to evaluate commercial and academic table detection and table structure recognition methods. The dataset is small 117 practice tables from the authors previous work (Göbel et al., 2012) and further 156 tables for the competition. The tables are born-digital PDFs taken from two governmental sources. Ground truths, for at least the practice tables, were independently generated and validated by three experts. The ICDAR 2019 Competition on Table Detection and Recognition (cTDaR) (Gao et al., 2019) was designed to benchmark the state-of-the-art in table detection and table structure recognition. It consists of two datasets: modern documents and historical documents. The modern documents are born-digital, while the historical documents contain handwritten text and hand-drawn tables. The combined datasets consist of at least 1,600 table images across the two document types. SciTSR (Chi et al., 2019) contains 15,000 tables in PDF format, annotated for table structure recognition. The annotations were created by parsing the LATEX source code of files from arXiv3. LATEX table definitions tend to use standard syntactical properties, such as & to delimit columns and to delimit rows. The authors use the regularity of this syntax to generate cell and row structure annotations for the tables contained in these documents. TableBank (Li et al., 2020a) is dataset of 417k diverse document images from different domains, including scientific papers, business documents, and more. The original documents are made up of LATEX source code files from arXiv, and Microsoft Word (.docx) files crawled from the internet. The dataset is annotated for table detection and table structure recognition, using weak supervision, by manipulating the documents source code and adding distinguishable coloured border to each of the tables within the documents. The positions of the coloured borders are calculated from the document images to give the tables bounding boxes. However, the dataset does not contain the ground truth positions of the words within the table images. DocBank (Li et al., 2020b) is dataset of 500k documents, with token-level annotations, generated using PDFs and LATEX source code files from arXiv. The dataset is made up of documents from many areas, including mathematics, physics, and computer science. DocBank is an extension of TableBank. It uses similar annotation methods but labels more document regions, such as the abstract, figures, references, etc. PubTabNet (Zhong et al., 2020) contains 568k images of tables extracted from scientific articles in the PubMed Central Open Access Subset (PMCOA) (PMC Open Access Subset). The images have been automatically labelled with table structure information and the textual contents of cells by matching the PDF format and the XML format of the articles. The dataset is designed to aid the training of table structure recognition models and also contains corresponding HTML representations of the tables. FinTabNet (Zheng et al., 2021) is an enhanced version of PubTabNet, further annotated with cell labels to create dataset of complex scientific and financial tables for table structure recognition. The authors generated the labels by designing method of matching the PDF documents to the HTML documents. The cell position annotations only cover the pixel region is which the text is located, not the full structure of the cell. Furthermore, FinTabNet 2https://www.gov.uk/government/organisations/ companies-house 3https://arxiv.org does not contain row position annotations or position information of empty cells. Therefore, there is no explicit information about the relationship between cells, such as which cells belong to the same row or which headers belong to given cell. Containing almost one million tables from documents in PMCOA, PubTables-1M (Smock et al., 2022) is dataset targeting the table detection, table structure recognition, and functional analysis subtasks of table extraction. The tables are annotated with bounding boxes for projected row headers, rows, columns, cells (including blank cells), and words. However, as with many existing table datasets, the tables originate from scientific journal articles and preprints. Yang et al. (2017) introduced synthetic, largescale, pixel-wise annotated dataset of documents to train multimodal, fully convolutional neural networks for the task of semantic structure extraction, which can be used for table detection. The dataset does not contain word-level annotations which are required for training on other Document AI tasks such as information extraction from table images. Qasim et al. (2019) introduced dataset of 500k synthetic generic table images, annotated for the task of table structure recognition. The dataset does not contain the ground truth word bounding boxes, instead relying on OCR to extract this information from the table images. FinQA (Chen et al., 2021) and ConvFinQA (Chen et al., 2022) are datasets designed to advance the numerical reasoning capabilities of large language models (LLMs). However, they use tables and texts from FinTabNet, which are not originally in image format and therefore lack the 2D positions of words necessary for training modern layout LLMs, such as LayoutLM. Current real-world table datasets mainly focus on the scientific domain, due to large repositories of scientific articles being available online. However, scientific tables differ from financial tables in their style, appearance, and layout. Scientific tables often occupy smaller footprint, contain less cell padding, and have more borders than financial tables. Many of these datasets do not contain image versions of their documents. While some these datasets may contain the textual contents of the documents, when document page is converted to an image, there is no knowledge of where in the image the text is located. For document regions, such as tables, where layout conveys much of the meaning, OCR has to be relied on to extract the words locations. Therefore, there is need for table image dataset where the locations of the words in the images are known. 2.2 Table Question-Answering Much of the table question-answering literature focuses on the extraction of information from structured data, such as relational database tables, or semi-structured data, such as HTML tables. Pasupat and Liang (2015) used semantic parsing framework in which table is converted to knowledge graph. Questions are converted to set of logical forms which are graph queries that can be executed on the graph. Zhong et al. (2017) proposed deep neural network trained to convert natural language questions to SQL queries that can be executed against table. TaPas (Herzig et al., 2020) embeds flattened tables words and extends the BERT architecture (Devlin et al., 2019) to include an embedding to represent which column and row cell belongs to. MATE (Eisenschlos et al., 2021) introduces new architecture to deal with the complexity of selfattention for large tables and uses the same column and row embeddings as TaPas. Unstructured document images are still common format. Some old document formats use images for pages, and physical documents are still being scanned or photographed. Furthermore, images are common and universal format to which most other document formats can be converted. Therefore, there is need for method of answering questions about the contents of tables within document images."
        },
        {
            "title": "3 SynFinTabs",
            "content": "To help advance machine learning research for table extraction from images in the financial domain, we present SynFinTabs, dataset of 100,000 synthetic financial tables. The table images come with HTML, JSON, and CSV representations. Due to our method of generating the table images, we know the ground truth structure and contents of each table image, at the time of creation. Therefore, we are able to annotate each word, cell, and row with its corresponding bounding box in the image. Unlike other datasets, such as FinTabNet, our cell bounding boxes accurately represent the full spatially meaningful cell, as opposed to the minimum pixel region in which the cell text is located. In Figure 1, SynFinTabs position annotations can (a) SynFinTabs annotations searched online for financial spreadsheet images and based our CSS templates on some of the returned images. Within the themes, many of the table properties vary, including the typeface, font size, bold or regular headers, numbering of table sections, date format, number of columns, use of note column, etc. The themes create more diversity and variation within the dataset by changing the visual properties of the tables, which will help when training vision models on various information and table extraction tasks. (b) FinTabNet annotations 3.2 Generation Process Figure 1: SynFinTabs annotations compared with those of FinTabNet. SynFinTabs annotations include row and empty-cell position information. The colours of the annotations are: table , row , cell , and word . be compared with those of FinTabNet. In addition to the position annotations, each cell in SynFinTabs is labelled with type to denote its semantic role in the table. The types are section title, currency unit, row header, column header, and data."
        },
        {
            "title": "3.1 Characteristics",
            "content": "The 100,000 tables of SynFinTabs are split across six themes. The first theme makes up 40% of the dataset and aims to represent tables found in financial statements filed with Companies House. The remaining 60% of the dataset is evenly split across five themes that aim to represent financial tables that may be found in spreadsheets or stylised company reports. An example table from each of the six dataset themes can be seen in Figure 2 The dataset is split into train/validation/test splits, 80%/10%/10%, respectively, with each theme represented proportionally in each split. To design the Companies Housestyle theme, we downloaded thousands of financial statements filed with Companies House in March 2023 and extracted tables from them using an off-the-shelf table detection model, Table Transformer (Smock et al., 2022). With random sample of these tables, we made observations regarding the structure and style of the tables. Using this information, we developed CSS template to mimic these observed properties. The company reportstyle theme is designed to mimic tables found in stylised company reports as might be distributed to share holders. We looked at one companys annual report and created CSS template to style some of our tables to look similar. To design the remaining four themes, we To create synthetic financial table, we first generate table specification which is blueprint of the table to be created, such as how many sections and the number of columns there will be; what theme the table will have; the typeface and font size to be used; the format to be used for the date headers; and the stylistic properties it will have. With this table specification, we create table object which contains the rows of the table, each row contains cells, and each cell contains words or number. For each section title, we randomly select title from list of commonly seen section titles in realworld financial tables. Textual cells are populated with number of random words from vocabulary of 10,000 English words. Numerical cells are populated with random number. We then create an HTML document that contains an HTML table element to represent the table object. During the conversion of the table object to HTML, the HTML element for each row, cell, and word is given unique ID, corresponding to its location in the table. The HTML document is then opened in headless browser with window size equivalent to the size of an A4 page. Then, each row, cell, and word element is located using its ID, and the bounding box of each of these elements is retrieved and saved to an annotations file. We also save every word and number that appears in the table, along with the bounding box of the full table. screenshot of the browser window gives us the final document image. question-answer pair is generated for each non-empty cell in the table. We build natural language question using the cells row header and column header. Along with this pair, we store the headers individually and the start and end positions of the target answer span in the flattened list of table words. One of the tables question-answer pairs is randomly selected as the competition pair which can be used (a) Theme 0: Companies Housestyle tables (b) Theme 1: Spreadsheet-style tables (c) Theme 2: Spreadsheet-style tables (d) Theme 3: Spreadsheet-style tables (e) Theme 4: Spreadsheet-style tables (f) Theme 5: Company reportstyle tables Figure 2: Example tables from the six themes of SynFinTabs. Figure 3: high-level overview of the SynFinTabs generation process. for training, validation, or testing, depending on the dataset split. In Appendix A, dataset example can be seen along with its question-answer pair. The generation process is repeated until dataset of the desired size has been created. high-level overview of the generation process can be seen in Figure 3."
        },
        {
            "title": "3.3 Applications",
            "content": "Our level of labelling lends the dataset well to training machine learning models on range of table extraction tasks. The structure annotations enable the dataset to be used to train models on table structure recognition tasks. The word-level annotations allow for training on natural language processing tasks, such as table visual question-answering. Additionally, SynFinTabs could be used to create synthetic financial documents where the positions of tables within the documents are accurately known. With such dataset, table detection models could be trained to detect financial tables within financial documents. As we will demonstrate in Section 4.3, extracting features from table images with OCR is crucial step in any training, testing, and inference, where the words and their positions are not already known. Some OCR solutions do not perform well when the text in an image is presented in tabular format, impacting downstream steps that make use of the OCR output. Our annotations of SynFinTabs tables include all words, and their positions, giving the dataset the potential to be used to train OCR systems on extracting text from tables."
        },
        {
            "title": "4 Experiments",
            "content": "We approach the problem of information extraction from table images using layout LLM and table visual question-answering task, due to the fact that we can take row header and column header and formulate natural language question about value in the table. For example, for row header (an account on balance sheet) and column header (a financial year), we can ask the model, What is the value of for the year c?, or similar, and the target answer is the value of the cell where these headers intersect. Each table in SynFinTabs comes with question-answer pairs for all non-empty cells, with one of these being the competition pair for that table. We use the competition pair for fine-tuning or testing, depending on the dataset split. 4.1 Fine-Tuning We have fine-tuned LayoutLM, using SynFinTabs, for the task of extractive question-answering. The model is given tables words and word bounding boxes as context, and natural language question about value in the table. It is then tasked with extracting the answer to the question, from the context, by predicting both the start and end position of the answer span in the context. During finetuning, the words and word bounding boxes from the ground truth annotations are used. Because we have the ground truth start and end positions of the target answer span in the context, we use these to compute the loss during training. For other datasets where OCR is used to extract the words from table or document image, and the ground truth answer is text, the start and end positions must be calculated by searching for the answer text in the context. Usually, the start and end positions of the first occurrence of the answer are used as the ground truths. However, in some financial tables, multiple cells contain the same numeric value. In cases where the answer text occurs more than once in the table and the target answer is not the first occurrence of that text, the model would be considered incorrect even if it correctly identified the true start and end positions. During testing, the words and word bounding boxes from an OCR engine, EasyOCR4, are used. We search for the answer text in the words extracted by OCR and use the start and end positions of the first occurrence of the answer text. We use the strictest form of the exact match metric, checking that both the start and end position are correct for the prediction to be considered correct, as opposed to comparing the predicted text to the target text. If we compared the predicted text to the target text, there may be some cases where the model would be considered correct even if the target text had been extracted from the incorrect cell. In other words, the predictions of the span start and end positions would be incorrect even though the extracted text may match the target text. On other question-answering tasks, the F1 score is usually reported (Rajpurkar et al., 2016, 2018). However, in the case of extracting information from financial table, given the row and column headers, we are only interested in knowing if all of the cell con4https://www.jaided.ai/easyocr/ Model FinTabQA FinTabQA-A4 Image size Table boundary A4 page Accuracy 95.87% 94.97% Table 1: Test results on the test split of SynFinTabs. tent is extracted, or nothing at all. Therefore, we only calculate and report the exact match accuracy of the models. We trained second model, FinTabQA-A4, with A4 pagesize images with each table in the top left corner of the page. We experimented with the two types of image input: the table cropped to the table boundary and an A4 pagesize image. The results for this testing can be seen in Table 1. The idea behind training the model with A4 pagesize images is that it would better normalise the size of the text within the image. Given that documents come in many different sizes, it is necessary to scale the actual coordinates of the word bounding boxes to virtual coordinates (Xu et al., 2020), having values in the range 01000, before being given to the model. For very small or very large cropped table images, the effects of this scaling are greater. However, for small tables within an A4 pagesize image, much of both the actual and virtual coordinate space is unused white space. The test results show that there is less than one point difference in accuracy between the two image input types."
        },
        {
            "title": "4.2 Real-World Evaluation",
            "content": "To test that our model is effective at information extraction from tables found in real-world financial documents, we created small test set of real-world table images, an example of which can be seen in Figure 4. We randomly selected 50 table images from financial statements filed with Companies House in March 2023 and manually defined two question-answer pairs for each table to create dataset of 100 questions about the images. The realworld images are cropped to the table boundaries, not A4 pagesize images. Therefore, when testing FinTabQA-A4, we paste the table image onto white A4 pagesize canvas in the top left corner, resizing it to fit on the canvas if necessary. We use the same strict exact match metric discussed in Section 4.1 for evaluating the performance of the models. The results of the evaluation on real-world table images can be seen in Table 2. Where testing was completed with GPT-4V six of them to be incorrect. Therefore, the additional instruction lead to an increase in accuracy of 18 points. Five of the six errors were due to the model being unable to assist with the request to extract information from the given table image. The last error was due to the number being extracted without the required parentheses or any mention of the fact the number was negative. Three of the correct responses used the word negative to indicate the value in the table was negative. For the GPT-4V test with A4 pagesize images, questions, and additional instruction, we manually evaluated the responses and determined 11 of them to be incorrect. Nine of the 11 errors were due to the model being unable to assist with the request to extract information from the given table image. One error was due to the cell above the target answer being predicted and the final error was due to positive value being predicted as negative. The placement of the currency symbol was ignored for the purposes of evaluation. In other words, missing currency symbol, or the symbol inside or outside brackets, did not cause the response to be deemed incorrect."
        },
        {
            "title": "4.3 Error Analysis",
            "content": "The results reported in Table 1 were achieved after performing parameter search for the best EasyOCR parameters. Using the default EasyOCR parameters, the accuracies of the models, FinTabQA and FinTabQA-A4, were 75.27% and 75.33%, respectively. Therefore, the parameter search lead to 20.60 and 19.64 point increases in accuracy, respectively. These parameters were used for the OCR step on the real-world tables. To determine which errors were due to OCR and which errors were due to the models, we tested the aforementioned models with the ground truth words and word bounding boxes. With these input features, the accuracies achieved were 99.98% and 99.99%, respectively, meaning the models made only one or two mistakes. Given the difference in accuracy between testing with OCR outputs and testing with the ground truths, we can conclude that many of the errors, when OCR words and word bounding boxes were used, were due to the imperfect nature of the OCR step. These errors fell into two categories: the row and/or column headers were not recognised correctly, meaning they were not included in the context given to the model, or the answer text was not recognised correctly, preventing the answer from being found in Figure 4: real-world financial table from document filed with Companies House. (OpenAI, 2023), generative AI model, we manually looked at the response to each question to determine whether or not it was correct. To be considered correct, the response had to contain the target answer text or have the exact same meaning. Initially, we prompted GPT-4V with only the question text. In further tests, we prompted the model with the question text and an additional instruction before the question. The additional instruction and question were part of the same prompt message. The additional instruction was, The image contains tabular financial data. Report the answer fully, including any parentheses and negation signs. For the GPT-4V test with table boundarysize images and questions only, we manually evaluated the responses and determined 24 of them to be incorrect. Of the 24 errors, 22 were due to the model failing to extract the parentheses around the number. For example, if the target answer was (1,839) and the model response only included 1,839, the response was considered to be incorrect due to the lack, and importance, of the parentheses (indicating the number is negative). In the remaining two incorrect cases, the model could not find the answer in the image and the model tried to calculate the answer using other information in the table but failed to do the calculation correctly. In one of the responses considered to be correct, the response indicated the number was negative by using the minus sign (-), even though the target answer text used parentheses. In another correct case, the response included the word negative to indicate the answer was negative value. For the GPT-4V test with table boundarysize images, questions, and additional instruction, we manually evaluated the responses and determined Model FinTabQA FinTabQA-A4 GPT-4V GPT-4V GPT-4V Training data SynFinTabs SynFinTabs Image size Table boundary A4 page Proprietary, undisclosed Table boundary Proprietary, undisclosed Table boundary Proprietary, undisclosed A4 page Prompt Question Question Question Instruction and question Instruction and question Accuracy 89% 79% 76% 94% 89% Table 2: Test results on real-world dataset of table images from documents filed with Companies House. the context, resulting in both the ground truth start and end position being calculated as zero. datasets, advancing table extraction in the financial domain and beyond. Another common error that we observed with both of our models during testing was the end position prediction being less than or equal to the start position prediction. The answer start and end positions are calculated by taking the argmax of the start logits and the argmax of end logits, respectively. To eliminate this error, we calculated the end position to be the argmax of the end logits after the start position. This lead to 1.93 and 2.13 point increases in accuracy to give the results in Table 1, respectively. We implemented this solution when evaluating these models on the real-world dataset. We visualise these errors in Appendix C."
        },
        {
            "title": "5 Conclusion",
            "content": "To overcome the challenge of lack of high-quality, unambiguously labelled training data for table extraction in the financial domain, we introduced SynFinTabs, dataset of 100,000 synthetic financial tables, annotated for range of information and table extraction tasks. We have documented the process of creating the dataset, the characteristics of the data, and we have highlighted potential applications. Using our model, FinTabQA, fine-tuned on SynFinTabs, we have experimented with information extraction from tables using table visual question-answering task and reported the results. To evaluate our model on real-world data, we created small test dataset of real-world tables and tested our model, showing that our dataset is effective in training machine learning models to perform the task of information extraction from financial tables. We have also compared our model to GPT-4V. Our error analysis shows the impact of OCR on an end-to-end system and highlights the importance of using high-quality ground truths for training, as the output of OCR is not totally reliable. Furthermore, our contribution of the dataset generation code will allow for future work to modify the code, and generate larger, more diverse synthetic financial table"
        },
        {
            "title": "Limitations",
            "content": "The textual and numerical contents of our tables are generated randomly, as described in Section 3.2. Layout language models, such as LayoutLM, cannot therefore interpret meaning from the words and numbers within the tables. The syntax of the tables is valid however, insofar as the tables have tabular structure similar to that seen in real-world financial tables. All of the questions generated for the questionanswer pairs follow the same grammatical form, meaning model trained on question-answering task may struggle to generalise to other natural language questions with the same meaning. We explicitly store the keys (row and column headers) for each answer which would allow for future work to generate wider variety of question formats. Experiments carried out with GPT-4V were only possible using OpenAIs paid API. Due to cost constraints, further experimentation, such as prompt engineering, was not feasible. It is possible that with better prompt engineering, the performance of GPT-4V may be improved. This may be considered under future work."
        },
        {
            "title": "Ethics Statement",
            "content": "Beyond the general potential ethical considerations of using LLMs to automatically process text (including issues of bias, fairness, transparency, and accountability), there are no specific ethical or social impact issues raised by the particular methodologies or data presented in this research."
        },
        {
            "title": "Acknowledgements",
            "content": "This research is supported by the Advanced Research and Engineering Centre (ARC) in Northern Ireland, funded by PwC and Invest NI. The views expressed are those of the authors and do not necessarily represent those of ARC or the funding organisations. We are grateful for use of the computing resources from the Northern Ireland High Performance Computing (NI-HPC) service, funded by EPSRC (EP/T022175)."
        },
        {
            "title": "References",
            "content": "Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. FinQA: dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 36973711, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA: Exploring the chain of numerical reasoning in conversational finance question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6279 6292, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanxuan Yin, and Xian-Ling Mao. 2019. ComPreprint, plicated table structure recognition. arXiv:1908.04729. Lei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. 2021. Document AI: Benchmarks, models and applications. Preprint, arXiv:2111.08609. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Julian Eisenschlos, Maharshi Gor, Thomas Müller, and William Cohen. 2021. MATE: Multi-view attention for table transformer efficiency. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 76067619, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Liangcai Gao, Yilun Huang, Hervé Déjean, Jean-Luc Meunier, Qinqin Yan, Yu Fang, Florian Kleber, and Eva Lang. 2019. ICDAR 2019 competition on table detection and recognition (cTDaR). In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15101515. Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. 2012. methodology for evaluating algorithms for table understanding in PDF documents. In Proceedings of the 2012 ACM Symposium on Document Engineering, DocEng 12, pages 4548, New York, NY, USA. Association for Computing Machinery. Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. 2013. ICDAR 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 14491453. Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 43204333, Online. Association for Computational Linguistics. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for document AI with unified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia, MM 22, pages 40834091, New York, NY, USA. Association for Computing Machinery. Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. 2020a. TableBank: Table benchmark for image-based table detection and recognition. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1918 1925, Marseille, France. European Language Resources Association. Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. 2020b. DocBank: benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics, pages 949960, Barcelona, Spain (Online). International Committee on Computational Linguistics. OpenAI. 2023. GPT-4V(ision) system card. Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1470 1480, Beijing, China. Association for Computational Linguistics. PMC Open Access Subset. 2003. PMC open access subset. Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait. 2019. Rethinking table recognition using graph neural networks. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 142147. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for SQuAD. Preprint, arXiv:1806.03822. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Brandon Smock, Rohith Pesala, and Robin Abraham. 2022. PubTables-1M: Towards comprehensive table In 2022 extraction from unstructured documents. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 46244632. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. 2021. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 25792591, Online. Association for Computational Linguistics. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. LayoutLM: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 20, pages 11921200, New York, NY, USA. Association for Computing Machinery. Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, and C. Lee Giles. 2017. Learning to extract semantic structure from documents using multimodal fully convolutional neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 43424351. Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. 2021. Global table extractor (GTE): framework for joint table identification and cell structure recognition using visual context. In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 697706. Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating structured queries from natural language using reinforcement learning. Preprint, arXiv:1709.00103. Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. 2020. Image-based table recognition: Question: What is the value of Idle ver learning satisfied for 30.11.74? Answer: 52,160 Row key: Idle ver learning satisfied Column key: 30.11.74 Start position: 41 End position: 42 Figure 5: SynFinTabs example table with its predefined competition question-answer pair. The answer span start and end positions refer to the respective position in the flattened list of table words. Data, model, and evaluation. In Computer Vision ECCV 2020, pages 564580, Cham. Springer International Publishing."
        },
        {
            "title": "A SynFinTabs Example",
            "content": "In Figure 5, an example table from SynFinTabs can be seen. In addition to the predefined competition question-answer pair, other metadata about the table is also recorded. Other metadata fields include the tables ID and theme; HTML, JSON, and CSV representations of the table; the train/validation/test split that the table belongs to; for tables in the test split, words and their bounding boxes, as extracted by EasyOCR; and the tables structure as JSON object, including list of rows, cells, words, and their 2D positions within the table image."
        },
        {
            "title": "B Training Details",
            "content": "For each of our fine-tuned models, FinTabQA and FinTabQA-A4, we used the base version of LayoutLM, with 113M parameters, as the pre-trained model. When fine-tuning each model, batch size of two was used and the optimal learning rate was found using the PyTorch Lightning5 Tuner module, with an initial maximum learning rate of 0.00003. All experiments were carried out using single NVIDIA GRID M60-8Q GPU. FinTabQA was finetuned for 12 epochs and required 63 GPU hours. FinTabQA-A4 was fine-tuned for 11 epochs, requiring 57 GPU hours. The best checkpoint, for each model, was chosen using the lowest validation loss at the end of each epoch."
        },
        {
            "title": "C Error Visualisations",
            "content": "To visualise some of the FinTabQA errors on the test split of SynFinTabs, we plot the target span start and end positions against the predicted span start and end positions. Ideally, if all predictions were correct, all points would lie on the line = x. These plots can be seen in Figure 6, for testing with the ground truth words and bounding boxes; Figure 7, for testing with EasyOCR words and bounding boxes; and Figure 8, for testing with the words and bounding boxes from second OCR engine, Tesseract6. We include the plots for Tesseract because it is commonly used in the literature and is the OCR engine used by the LayoutLM creators (Xu et al., 2020) and the popular Hugging Face Transformers library (Wolf et al., 2020). For the OCR plots, the set of points on the line = 0 represents cases where OCR failed to extract the answer text correctly. The answers could therefore not be found in the contexts and were assigned start and end positions equal to zero. This set of points does not exist when the ground truths are used because we use the known ground truth start and end positions, which are never zero because the answer always exists in the context. We can see, particularly in Figures 6 and 7, sets of points, close to horizontal, on the end position plots. These predictions are significantly lower than their target values and their corresponding start positions, and are mostly corrected when we limit the end position to come after the start position. Given that all of these predictions are made by the same model, our best as in Table 1, this highlights just how important the quality of the training data is. The output of Tesseract, when applied to tabular data, is notably suboptimal. Had this been relied upon to extract the words and word bounding boxes from our table images during training, the models would not have been able to learn as well. 5https://lightning.ai/pytorch-lightning 6https://tesseract-ocr.github.io/ (a) Start positions (b) End positions (c) End positions (limited) Figure 6: Target span start and end positions against predicted span start and end positions when ground truth words and word bounding boxes are used. (a) Start positions (a) Start positions (b) End positions (b) End positions (c) End positions (limited) (c) End positions (limited) Figure 7: Target span start and end positions against predicted span start and end positions when EasyOCR words and word bounding boxes are used. Figure 8: Target span start and end positions against predicted span start and end positions when Tesseract words and word bounding boxes are used."
        }
    ],
    "affiliations": [
        "School of Electronics, Electrical Engineering and Computer Science, Queens University Belfast, Belfast, UK"
    ]
}