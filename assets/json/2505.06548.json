{
    "paper_title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback",
    "authors": [
        "Aniruddha Roy",
        "Pretam Ray",
        "Abhilash Nandy",
        "Somak Aditya",
        "Pawan Goyal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches."
        },
        {
            "title": "Start",
            "content": "1 REFINE-AF: Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback Aniruddha Roy, Pretam Ray, Abhilash Nandy, Somak Aditya, Pawan Goyal Indian Institute of Technology, Kharagpur. Email: {aniruddha,pretam,abhilash}@iitkgp.ac.in, {saditya,pawan}@cse.iitkgp.ac.in AbstractInstruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating humanannotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in semiautomated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve substantial improvements in 6366% of the tasks compared to previous approaches. Index TermsReinforcement Learning, Large Language Models, Alignment, Instruction Generation I. INTRODUCTION Significant progress has been achieved in recent NLP research towards empowering large language models to comprehend instructions provided in natural language However, the process of gathering and annotating such instruction data is labor-intensive and costly, and often limited in quantity, diversity, and creativity. Several automated or semi-automated methods [1][6] have been developed for instruction generation. [1] introduces Automatic Prompt Engineer (APE), drawing inspiration from program synthesis and human prompt engineering for automatic instruction generation and selection. [2] proposes flipped learning for language models, training the model to generate task instructions given input instances and labels. [3] introduces interpretable auto-prompting to generate naturallanguage explanations for data patterns. All these methods share the common goal of generating instructions for given task from few examples and may suffer from lack of task diversity. To address the challenge of generating instructions corresponding to diverse tasks, recent study by [5] proposed semi-automated framework for task-agnostic instruction generation. Their framework begins with initial instructions and employs bootstrapping methods for expansion. Following no-training paradigm, their framework encompasses various stages and relies on the large-parameter language model GPT3.5 [7] for inference. Notably, most of these methods rely on non-open source commercial product, large in size, expensive, and subject to rate limits. In this paper, we also focus on generating instructions along with the corresponding input-output pairs in taskagnostic manner with negligible human intervention, effort, and cost. We mainly investigate few unexplored questions. Specifically, we ask 1) How does the capability of lowparameter size language models like LLaMA 2 (7B, 13B) [8] and Mistral 7B [9] compare to GPT-3.5 in generating instructions in task-agnostic manner? 2) What is the performance analysis of the above models across varying numbers of instructions? 3) How effective is training algorithm, such as Reinforcement Learning from Automated Feedback (RLAF), in the instruction-generation pipeline utilizing small parameter models like LLaMA 2-7B, LLaMa 2-13B, and Mistral 7B? To investigate the aforementioned questions, we introduce, semi-automated framework designed to generate high-quality triplets for new tasks using Reininstruction-input-output forcement Learning From Automated Feedback. Initially, our framework utilizes small seed set of manually written tasks to generate instructions for defining new tasks (similar to Subsequently, reinforcement learning with automated feedback is employed to improve the quality of input-output pairs generated by passing instructions to the LLM. In the final stage, the model trained in the previous stage is utilized to construct the Instruction Fine Tuning dataset, serving as the basis for refining the base model through supervised finetuning. To assess the effectiveness of our framework , we utilize LLaMa 2-7B, LLaMa 2-13B and Mistral-7B models to generate 15,000 instructions each, complete with input and outputs. Analysis of the resulting data reveals diverse array of new tasks. Subsequently, we fine-tune the LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B models using the generated instructions and evaluate their performance on the SUPER-NI dataset [10]. Furthermore, we compare our framework with the self-instruct 5 2 0 2 0 1 ] . [ 1 8 4 5 6 0 . 5 0 5 2 : r framework [5], which utilizes LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B models. Our findings demonstrate significant task improvement of 64.39%, 66.39%, and 63.51% across wide range of tasks using LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B as base models, respectively. Human evaluation of user-oriented tasks also further highlights the increment in performance. We also experiment with various dataset sizes proving that the methods effectiveness increases with the number of instructions. In summary, our contributions are: We empirically analyze the effectiveness of LLaMA 27B, LLaMA 2-13B, and Mistral 7B for task-agnostic instruction generation utilizing the existing framework. We introduce REFINE-AF, method for generating instructions with minimal human-labeled data, and LLaMa 2-7B, LLaMA 2-13B, Mistral 7B. We demonstrate its effectiveness via extensive instruction-tuning experiments. We release large synthetic dataset of 45K instructions generated by REFINE-AF using different LLM backbones. II. METHODOLODY The task of annotating extensive instructional data poses challenges for human annotators, as it demands considerable time and effort, inventive thinking to devise fresh tasks and specialized knowledge to craft solutions for each task. We propose as solution to this problem - REFINE-AF, which generates synthetic instructional data from small seed of human-written instructions by bootstrapping instructions using LLM inference, followed by training the LLM to align it to automated preferences in generated (input, output) pairs. Here, we elaborate our pipeline for our proposed REFINE-AF in 3 stages (see Figure 1) - (1) Instruction Generation (2) Using RL from Automated Feedback to generate input-output pairs (3) Instance Generation. The final output of REFINE-AF is dataset of (instruction, input, output) triplets, which could later be used for updating an LLM via Instruction Fine-Tuning (IFT). A. Notation ϕ Let us define the notation utilized in this paper. π0 represents the foundational LLM responsible for generating instructions. πRL denotes the LLM undergoing training via the Reinforcement Learning (RL) pipeline, where ϕ signifies the parameter set within the LLM. stands for the input provided to the LLM, comprising the prompt and the instruction. Correspondingly, represents the response generated by the LLM, encompassing the input-output instances. Furthermore, r(x, y) is employed to signify the reward obtained from the reward model utilized for guiding the LLM. Various terms utilized in the reward function are elaborated in Table I."
        },
        {
            "title": "Rew",
            "content": "Unieval-naturalness"
        },
        {
            "title": "N at",
            "content": "Unieval-coherence"
        },
        {
            "title": "Coh",
            "content": "Unieval-understandability"
        },
        {
            "title": "U nd",
            "content": "The oasst-rm-pythia-1.4b reward model inference score [11] Whether response is natural, provided by the UniEval [12] dialogue model. Whether this response is as valid continuation of the previous conversation, provided by the UniEval [12] dialogue model. Whether the response is understandable, provided by the UniEval [12] dialogue model. TABLE I: Summary of indicators for instruction quality. Each data sample is viewed as pair of instruction and response (i.e., input and output) of LLM. seed instructions like [5]. At every step, we randomly select 8 instructions from the pool to serve as in-context examples. 6 of the 8 instructions are written by humans, while the remaining 2 are generated by the LLM in the preceding steps to ensure diversity. To promote diversity, new instruction is introduced into the instruction pool only if its ROUGEL similarity score with any existing instruction is below 0.7 maintaining the approach followed by [5]. Additionally, we eliminate instructions containing certain keywords (such as image, picture, graph) that are typically not processable by LLMs. The prompts used for the same are given in Appendix IX-C. C. Stage 2:Using RL from Automated Feedback to generate input-output pairs RL from Human Feedback (RLHF) [13], [14] leverages human preferences as form of reward signal for refining LMs and aligning them to human judgements. RLHF essentially comprises reward model that receives text sequence as input and outputs single scalar reward, intended to quantitatively reflect human preference. This scalar reward serves as feedback for the LLM, facilitating its parameter updates using policy-gradient RL algorithm such as Proximal Policy Optimization (PPO) [15]. In REFINE-AF, in order to keep human effort to minimum, we replace human feedback with automated feedback. The quality of the instruction data could be viewed as its ability to efficiently steer language models in learning to generate responses in particular manner. This can be estimated by various indicators as described in Table I. Adapting from an earlier work by [16], the reward score for any instruction, input, output triplet is calculated as: r(x, y) = 0.0078 Rew(x, y) 0.4421 nd(x, y) + 0.3212 at(x, y) + 0.1520 Coh(x, y) 0.0274 (1) B. Stage 1:Instruction Generation REFINE-AF generates additional instructions by iteratively building upon limited set of initial human-written instructions. The initial pool of instructions is initiated using 175 Table elaborates each term used in Equation 1. This score acts as scalar notion of preferability for particular instruction-input-output triplet. It is directly proportional to the oasst-rm-pythia-1.4b model reward [11], the naturalness and the coherence of the triplet and inversely proportional 3 Fig. 1: Schematic diagram of the stages in REFINE-AF pipeline. to the understandability which represents the complexity in the sentence. These metrics are obtained from the UniEval [12] dialogue model. In the training phase, the LLM is engaged through specifically designed prompt containing examples of InstructionInput-Output Instances and instructions to generate such instances given an instruction. The prompt is provided in the Appendix IX-D. The model output is then passed to the reward model described above and an automatic score is generated which serves as the feedback to the LLM. We maximise the following objective function in RL training using the PPO algorithm considering the Kullback-Leibler (KL) divergence factor: R(x, y) = r(x, y) β log (cid:0)πRL ϕ (y x)/π0(y x)(cid:1) (2) where: The second term is the KL divergence between the current policy and the reference model as denoted by [17], β is scaling factor (β > 0). D. Stage 3:Instance Generation After training the LLM using RL with automated feedback in the previous stage, we use the same prompt used while training followed by 1 instruction from the instruction pool at time. This generates (input, output) pair corresponding to each input instruction. At the end of this stage, we get an Instruction Fine Tuning (IFT) dataset of (instruction, input, output) triplets, as desired in semi-automated fashion. Instruction Finetuning: Following the instance generation phase, the generated IFT dataset serves as the foundation for refining the model through Supervised Finetuning (SFT), technique prevalently adopted for instruction finetuning. III. EXPERIMENTS In this section, we conduct experiments to measure and compare the performance of REFINE-AF with other instruction dataset generation pipelines with various base models. Fig. 2: Moving average of the model rewards over the training steps for LLaMa 13B, LLaMa 13B and Mistral 7B. A. Experimental Setup Seed Data: We use the same set of 175 seed examples utilized by Self-Instruct [5] as human-annotated seed data for bootstrapping in the initial stage. Base Models and Hyper-parameter setting. We use the following pre-trained models of different sizes - LLaMa 2 model [8] with 7B, 13B parameters, and Mistral [9] with 7B parameters as the base models for generating the instruction dataset. We use the PPO Trainer from the Transformer Reinforcement Learning (TRL) library1. The model is loaded in 4-bit mode and trained with LoRA (Low-Rank Adaptation) [18] for 200 steps with batch size of 4, learning rate 2e-5, and 4 gradient accumulation steps. For the supervised finetuning step (using the generated instruction dataset), we train the model for 3 epochs, with learning rate of 2e-5, cosine scheduler with warmup ratio of 0.3, batch size of 2 with 4 gradient accumulation steps using the HuggingFace Trainer API2. All models were trained using single A100 GPU. The initial phase of our process requires approximately 20 days to generate 15K instructions, and none of the subsequent phases require more than 120 hours. We filled out the consent form to use the LLaMa 2 model. Baselines. To evaluate the effectiveness of REFINE-AF, we compare REFINE-AF with SELF INSTRUCT framework applied on previously mentioned LLM backbones (LLaMa 2 7B, LLaMa 2 13B [8], Mistral 7B [9]) of different sizes as baselines. RL Training: Training Language Model Models (LLMs) with Reinforcement Learning (RL) presents its challenges, as noted by Beeching et al. (2023) [19], often accompanied by instability. To gauge the efficacy of the training process, we monitor the models mean reward across training steps. To ensure clarity, we employ moving average with span of 30 steps. The Spearman rank correlation between reward and steps, illustrated in Figure 2, stands at 0.553, 0.649, and 0.558 for LLaMa 7B, LLaMa 13B, and Mistral 7B, respectively. notably positive Spearman rank correlation suggests progression where the system gains proficiency over time, with robust inclination for reward enhancement as training advances. IV. DATA QUALITY OF GENERATED INSTRUCTION DATASET In this section, we discuss the generated instruction dataset using REFINE-AF. The statistical details of the generated data are outlined in Appendix IX-A. A. Analysis of the generated instances In this section, well explore the quality and diversity of the generated instances. Diversity: To evaluate the diversity of the generated instructions, we follow the approach in [5] and utilize the Berkeley Neural Parser to analyze the instructions and subsequently identify the verb along with its immediate noun object. The 1https://huggingface.co/docs/trl/en/index 2https://huggingface.co/docs/transformers/main/en/trainer 4 instructions generated by REFINE-AF using LLama 2-7B, LLama 2-13B, and Mistral 7B comprise total of 828, 790 and 467 unique noun-verb pairs, respectively. Figures 8a, 8b, and 8c in Appendix display the 20 most frequent root verbs and their top 4 associated noun objects for the models LLama 2-7B, LLama 2-13B, and Mistral 7B, respectively. We further investigated the disparity between the generated and seed instructions utilized to initiate the generation process. Specifically, for each generated instruction, we calculated its maximum ROUGE-L overlap with the pool of 175 seed instructions. We depict the distribution of these ROUGE-L scores in Figure 4. The findings reveal notable proportion of newly generated instructions by REFINE-AF using LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B having minimal overlap with the seed instructions. Fig. 3: Length distributions of the instructions, inputs, and outputs generated by REFINE-AF. Fig. 4: Distribution of Rouge-L scores between generated instructions and their most similar seed instruction. This highlights the difference in generated instructions from seed tasks."
        },
        {
            "title": "Model",
            "content": "LLaMa 2 7B LLaMa 2 13B Mistral 7B Instruction Count 5,000 10,000 15,000 5,000 10,000 15,000 5,000 10,000 15,000 SELF INSTRUCT 5.8012 5.9841 6.0414 6.5349 6.5269 6.4446 5.7615 5.8454 5.9986 REFINE-AF % Task Better 5.9613 6.0398 6.1636 6.4488 6.5381 6.6133 5.8632 5.9712 6. 66.66% 53.79% 64.39% 44.54% 52.94% 66.39% 64.29% 60.34% 63.51% 5 TABLE II: Comparative results (Average ROUGE-L Scores) of REFINE-AF with Self Instruct method for different base models on 119 tasks in SUPER-NI."
        },
        {
            "title": "Task Category",
            "content": "title generation coreference resolution textual entailment question rewriting cause effect classification dialogue act recognition answerability classification keyword tagging data to text word analogy overlap extraction grammar error correction LLaMa 2 7B LLaMa 2 13B Mistral 7B SELF INSTRUCT 8.0902 2.1905 1.7029 20.2548 5.1514 2.0877 1.5365 3.0131 15.4563 1.2353 5.4254 31.5197 REFINE-AF 8.017 2.3077 1.6419 20.6361 5.3748 2.1171 1.6594 3.1371 15.6081 1.2812 5.6473 31. SELF INSTRUCT 8.4642 2.3639 2.0271 20.5859 5.4321 2.4235 1.9846 3.8338 15.2005 1.3527 14.3674 36.8425 REFINE-AF 8.7679 2.1891 2.0019 21.6007 5.3932 2.4896 2.0515 3.8830 15.5802 1.3294 15.2493 37.2230 SELF INSTRUCT 7.8912 2.2134 1.7543 20.0412 5.0768 2.0421 1.4571 3.0021 15.1112 1.1872 6.1231 30.1172 REFINE-AF 7.7654 2.1543 1.6312 20.3401 5.2312 2.1045 1.5762 3.1127 15.3456 1.1239 7.2367 31.2387 TABLE III: Task Category Wise Comparison of the average ROUGE-L Scores between the pipelines. REFINE AF outperforms SELF INSTRUCT in the majority of the categories of SUPER-NI Dataset for all the base models. The underlined values denote significantly larger improvements (p-value 0.05) using statistical significance test Fig. 5: Similarity of instructions generated using REFINE-AF wrt the GPT 3.5 instructions for different base models. This similarity has been calculated using Rouge-L scores. Figure 3 describes the distribution of the length of the instructions, inputs, and outputs in the generated instances3. This further highlights the diversity of the generated instruction dataset. Most of these observations match the ones generated by instructions using the Self-instruct method on GPT-3.5 [5], showing that even smaller language models are capable of generating good-quality instructions. We also analyzed instructions generated by LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B with GPT-3.5. Specifically, we examined 15,000 instructions generated by REFINE-AFfor each of the models LLama 2-7B, LLama 2-13B, and Mistral 7B. For GPT-3.5, we utilized the dataset introduced by [5]. Utilizing the Rouge-L score, we determined the most similar instruction for each model-generated instruction. The distribution of similarity scores is depicted in Figure 5. Notably, many instructions closely resembled those generated by the GPT-3.5 model, with mean score of approximately 0.62. This observation underscores the capability of even smaller 3from here onwards, an instance refers to an instruction-input-output triplet LLMs to produce high-quality instructions. Quality: In this section, we evaluate the quality of our generated instructions. We randomly select 100 instructions from each set of model-generated instructions for this analysis, sampling one instance per instruction. Following the methodology outlined in [5], we ask an expert annotator (including one of the authors of this work) to evaluate whether each instance is correct in terms of the instruction, the instance input, and the instance output. The data quality assessment presented in Table IV within the Appendix indicates that the majority of the generated instructions are accurate. While there are some instances of noise in both the input and output associated with these instructions, the overall findings are promising. Despite these minor discrepancies, all the instructions remain valid and hold potential utility for model training purposes. V. EXPERIMENTAL RESULTS A. Zero-Shot Generalization on the SUPER-NI benchmark Initially, we assess the models proficiency in adhering to instructions across conventional NLP tasks in zero-shot manner. This evaluation is conducted on the evaluation set of the SUPER-NI dataset [10], using the methodology adopted by [5]. The SUPER-NI dataset comprises 119 tasks across 12 task categories, each comprising 100 instances. The models are not provided with any in-context examples to aid comprehension; they are solely prompted with task definitions. Results: The experimental results are presented in Table II. Our proposed method consistently surpasses the baseline SELF-INSTRUCT approach across various instruction set sizes. Evaluation in the SUPERNI dataset employs both exact match and Rouge-L metrics. Since [5] solely reported RougeL scores, we adhere to presenting Rouge-L metrics. While for classification tasks, similarity scores with labels are typically utilized, this practice was not adopted in the SUPERNI dataset, hence we rely on the Rouge-L metrics exclusively. Notably, it exhibits superior performance across different tasks, surpassing the baseline in the majority of them. Further insights into the performance based on task categories are presented in Table III. Remarkably, our pipeline demonstrates enhanced performance compared to the baseline in 10 out of 12 task categories, underscoring the methods robustness across diverse task domains. Moreover, the performance consistency across different models suggests the scalability of our approach, indicating its potential applicability to larger models to enhance the quality of instruction generation. B. Generalisation of User-oriented Instructions on Novel Tasks The SUPER-NI covers an extensive set of existing NLP tasks, but these were proposed for research purposes and are skewed towards classification. To better evaluate the instruction following capabilities of the models we also experiment on user-oriented instructions created by [5]. These contain 252 instructions with 1 instance per instructions which cover diverse and unfamiliar instructions. Human Evaluation: The instructions utilized in this experiment are designed to be more generic and user-oriented. Given the diverse nature of tasks, different expertise is required, rendering them unsuitable for evaluation via automatic metrics or standard crowd workers. Consequently, the authors of this paper conducted manual evaluations, referencing the golden outputs provided with these instructions. Evaluators rated the outputs into one of several categories based on how effectively the model achieved the task. Following the framework introduced by [5], four-level rating system was adopted: RATING-A: The response is valid and satisfactory. RATING-B: The response is acceptable but contains minor errors or imperfections. RATING-C: The response is relevant and addresses the instruction, but significant errors are present. RATING-D: The response is irrelevant or completely invalid. Results: Figure 6 illustrates the outcomes of the human evaluation. Annotators evaluated the responses from each model separately, without knowledge of the method employed, to mitigate bias. We can observe that REFINE-AF performs better than SELF-INSTRUCT as it can get valid and satisfactory answers to more instructions. Also, the count of irrelevant responses is smaller. Thus the RL feedback guides the model to generate better responses to instructions. C. Effect of Data Size and Quality REFINE-AF provides way to generate large number of good-quality instructions. But does increasing the number of instructions improve the performance? We evaluate the following in this section. We can observe in Table II that Fig. 6: Human Evaluation results using LLaMA 7B, LLaMA 2-13B, and Mistral 7B models trained on 15k instructions utilizing Self Instruct and our pipeline. Fig. 7: Effect of data size on the performance of the model. Shown using the scores on SUPER-NI benchmark increasing the number of instructions gradually increases the score on the SUPER-NI benchmark. more descriptive plot of the same can be found in Figure 7. Thus increasing the number of instructions proves to be beneficial to the performance. VI. RELATED WORK Instruction tuning for LLMs: Early investigations [20] [22] into instruction fine-tuning primarily focused on natural language processing (NLP) tasks. They revealed that finetuning with NLP datasets structured as instruction-output pairs enhances cross-task generalization. Additionally, few studies [10], [23] have demonstrated direct correlation between the extent and diversity of the instructional data and the models capacity to generalize to unfamiliar tasks. Works such as [17] and [5] aim to construct general-purpose Language Models capable of handling broader array of task instructions, aligning with our objectives. However, the creation of diverse datasets and the utilization of large-parameter-based, nonopen-source models pose significant challenges. Instruction Generation: significant hurdle in enabling Large Language Models (LLMs) to follow instructions is the 7 IX. APPENDIX A. Statistics of Instructions Generated Table presents the fundamental statistics about the generated data. 15,000 instructions were generated in Stage 1, resulting in an instruction dataset of around 15,000 instances corresponding to each instruction post-filtering. B. Diversity of generated instructions As discussed in the paper, we have observed the root verbs with their main nouns to evaluate the diversity of the generated instructions. Figure 8 displays the plot highlighting the top 20 most common verbs in the inner circle followed by the top 4 most common direct nouns in the outer circle for the generated instructions for different models. C. Prompt Template for Instruction Generation We have modified the prompt used by Self Instruct to be more comprehensive and give detailed instructions regarding instruction generation. This prompt has been used to train Stanfords Alpaca model [31] which was released after the Self Instruct paper and was based on the Self Instruct data pipeline. For fair comparison of our pipeline and Self Instruct, we use the same template for instruction generation in both pipelines. The updated prompt is shown in Table VI. D. Prompt Template for Instance Generation For instance generation, we use the same prompt as used by Self Instruct. It consists of set of in-context demonstrations to demonstrate how input-output examples should look like. It is shown in Table VII. E. Human Evaluation Details We followed the method used by [5] for the human evaluation of 252 user-oriented instructions. Table VIII presents examples of human annotation. Each entry includes the instruction, input, and target response, as well as the responses generated by both SELF-INSTRUCT and REFINE-AF, accompanied by the ratings assigned by evaluators. Notably, our method enhances the understandability and accuracy of model outputs compared to SELF-INSTRUCT. However, instances of model hallucination. its worth noting occasional collection of demonstration samples for fine-tuning. Current high-accuracy LLMs heavily rely on human annotators, and most instruction datasets are not open source. Moreover, these human annotation approaches are time-consuming, costly, and require expertise across various domains. To mitigate these challenges, several methods have been proposed for automated instruction generation. For instance, [24] utilize human-written text as natural response and generate corresponding instructions based on the response. [6] employ semi-automated method using back-translation approach to generate instructions from human-written text data. Several studies [1][4] have investigated the utilization of Large Language Models (LLMs) for instruction generation. Employing unnatural instruction prompts, GPT-3 generates additional instructions when provided with few initial seed instructions within contextual setting. [5] proposed semiautomated framework for task-agnostic instruction generation. Their framework begins with initial instructions and employs bootstrapping methods for expansion. Following no-training paradigm, their framework encompasses various stages and relies on the large-parameter language model GPT-3.5 [7] for inference. In contrast, our method utilizes small-parametersized open-source LLMs and employs reinforcement learning algorithm through human feedback to generate instructions directly from the model itself. Model Self-training: conventional self-training framework, as demonstrated by [25][30], entails using trained models to label unlabeled data and then utilizing this newly labeled data to enhance the models performance. VII. CONCLUSIONS AND FUTURE WORK In this study, we have conducted an empirical analysis to assess the effectiveness of three open-source, low-parameter modelsLLaMA 2-7B, LLaMA 2-13B, and Mistral 7Bin comparison to the larger model GPT-3.5 for task-agnostic instruction generation within an established framework, SELFINSTRUCT. Additionally, we have introduced REFINE-AF, novel approach for generating instructions in task-agnostic leveraging the manner with minimal human-labeled data, capabilities of LLaMa 2-7B, LLaMA 2-13B, and Mistral 7B. Through extensive experimentation in instruction-tuning, we have demonstrated the effectiveness of our approach. Furthermore, we have made significant contribution by releasing large synthetic dataset comprising 45K instructions generated by REFINE-AF using different LLM backbones, thus facilitating further research in this domain. VIII. LIMITATIONS The primary limitation of our instruction generation framework, which comprises three parts, lies in the initial stage responsible for instruction generation. This phase tends to consume significant time when generating instructions directly from the model. In future iterations, we aim to integrate more efficient methods to create instructions within shorter timeframes. Our model has been evaluated on various NLP tasks, and this work can extend to include multimodal scenarios."
        },
        {
            "title": "Quality Review Question",
            "content": "LlaMa2 7B LlaMa2 13B Mistral 7B Does the instruction describe valid task? Is the input appropriate for the instruction? Is the output correct and acceptable response to the instruction and input? 90% 81% 58% 94% 83% 65% 95% 76% 64% TABLE IV: Evaluation of the data quality for the instruction, input, and output of the generated data by REFINE-AF with different LLM backbones"
        },
        {
            "title": "Statistics",
            "content": "# of instructions # of instances # of instances with empty input average instruction length average non-empty input length average output length LlaMa2 7B 15,000 14,998 8,564 17.77 10.34 22.97 LlaMa2 13B Mistral 7B 15,000 17,524 7,504 15.63 10.50 18.43 15,000 14,953 8,749 16.82 9.64 24.56 TABLE V: Statistics of the instructions generated by REFINE-AF with different LLM backbones You are asked to come up with set of diverse task instructions. These task instructions will be given to LLM and we will evaluate the LLM for completing the instructions. Here are the requirements: 1. Try not to repeat the verb for each instruction to maximize diversity. 2. The language used for the instruction also should be diverse. For example, you should combine questions with imperative instructions. 3. The type of instructions should be diverse. 4. The list should include diverse types of tasks like open-ended generation, classification, editing, etc. 5. language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5 pm or set reminder because it cannot perform any action. 6. The instructions should be in English. 7. The instructions should be 1 to 2 sentences long. Either an imperative sentence or question is permitted. Task 1: {instruction for existing task 1} Task 2: {instruction for existing task 2} Task 3: {instruction for existing task 3} Task 4: {instruction for existing task 4} Task 5: {instruction for existing task 5} Task 6: {instruction for existing task 6} Task 7: {instruction for existing task 7} Task 8: {instruction for existing task 8} Task 9: TABLE VI: Prompt used for generating new instructions. 8 existing instructions are randomly sampled from the task pool for in-context demonstration. The model can generate instructions for new tasks until it stops its generation, reaches its length limit or generates up to 8 more tasks. 9 (a) Llama 2 7B (b) Llama 2 13B Fig. 8: The top 20 most common root verbs (inner circle) and their top 4 most common direct nouns (outside circle) in the generated instructions that contain such verb-noun structure for different models. (c) Mistral 7B 10 Come up with examples for the following tasks. Try to generate multiple examples when possible. If the task doesnt require additional input, you can generate the output directly. Task: Which exercises are best for reducing belly fat at home? Output: - Lying Leg Raises - Leg In And Out - Plank - Side Plank - Sit-ups Task: Extract all the country names in the paragraph, list them separated by commas. Example 1 Paragraph: Dr. No is the sixth novel by the English author Ian Fleming to feature his British Secret Service agent James Bond. Written at Flemings Goldeneye estate in Jamaica, it was first published in the United Kingdom by Jonathan Cape in 1958. In the novel Bond looks into the disappearance in Jamaica of two fellow MI6 operatives who had been investigating Doctor No. Bond travels to Nos Caribbean island and meets Honeychile Rider, who is there to collect shells. They are captured and taken to luxurious facility carved into mountain. The character of Doctor No, the son of German missionary and Chinese woman, was influenced by Sax Rohmers Fu Manchu stories. Dr. No was the first of Flemings novels to face widespread negative reviews in Britain, but it was received more favourably in the United States. Output: English, British, Jamaica, the United Kingdom, German, Chinese, Britain, the United States. Task: Converting 85 to Celsius. Output: 85F = 29.44C Task: Sort the given list ascendingly. Example 1 List: [10, 92, 2, 5, -4, 92, 5, 101] Output: [-4, 2, 5, 5, 10, 92, 92, 101] Example 2 Input 2 - List: [9.99, 10, -5, -1000, 5e6, 999] Output: [-1000, -5, 9.99, 10, 999, 5e6] Task: Suggest better and more professional rephrasing of the following sentence. Example 1 Sentence: This house is surprisingly not constructed very well, and you probably need more money to fix it after you buy it. If you ask me, would suggest you to consider other candidates. Output: This house does not seem to be constructed well, so you may need to spend more money to fix it after you purchase it. would suggest that you look at other properties. Example 2 Sentence: Just so you know, we did an experiment last week and found really surprising results - language model can improve itself! Output: Our experiments last week demonstrated surprising results, proving that the language model can improve itself. ... Task: {instruction for target task} TABLE VII: Prompt used for generating instances given an instruction. It consists of constant set of in-context examples. The model has to generate the instances for the given instruction following the format and idea of the given demonstrations."
        },
        {
            "title": "T\nC\nU\nR\nT\nS\nN",
            "content": "I"
        },
        {
            "title": "F\nL\nE\nS",
            "content": "e p t a : t m C : t v s m a m v - , B o , i d i , ) N ( i i r r e a p , u s n j fi a s t p v y i n e i e o c . t h e d s o )"
        },
        {
            "title": "R\nS\nC",
            "content": "( : t s r o a d t p v a e e e s s e a p . o t o e s i e e : t . . . . , a t e n s r s c f i s m e f s d n a d t p v e U t h s t n o . e l o o n y i c r i ] n k r [ : e L . ] o [ l i t j a g r ] r i u [ : t p n h t a f k m ] k e r Y [ : b F , s t r y . j s h e ] m n r Y [ : h ] m o [ n u a . ] s i [ c c t o f e s u g I ] e b r [ : s . ] p t i [ e 0 0 5 2 , 0 0 5 , 5 , a 0 0 6 , 0 5 1 , 4 , p 0 0 0 3 , 0 0 5 , 2 , B : t 0 0 5 2 , 0 0 , a 0 0 0 3 , 0 5 1 , p 0 0 0 5 4 , 0 0 0 , ] N Y [ : t , a : t : t , N , B o , i d n t , s n j , e g r a w , a s t p v r e a p g r d n i s m e d s d r r a l p S h n a a l i r i . l s s l o a r o . n , d , v : r t e g o f e a n o , r t t s : n e o o t t t s C : t t n s t f s n e n n e a r e i r i w e e y s c p c s + t t . i i a y n I"
        },
        {
            "title": "A\ne\ng\na\nu\ng\nn\na\nl",
            "content": "e e o h w e . s . N t e i p t o e o s - t q w o t s s P : t t I . e l o h r a p i t a d f r i s m e l v n r s s e a p . o t o e s i d p a e t e l D . i a t : n n t f e c l p s t . . . . ? d s e l d c c t o ] g ] s i ] n Y [ Y [ o [ ] I"
        },
        {
            "title": "D\nC\nR\nO",
            "content": "r [ : I"
        },
        {
            "title": "D\nC\nR\nO",
            "content": "] s r [ : s e p e a s a a p : t t e i e m f e s u g : n . p c n e v h f t u , r , , I 0 0 5 , 0 0 5 , 5 , a 0 0 6 , 0 5 1 , , p 0 0 0 3 , 0 0 5 1 , 2 , s p s o t e e e : t t t e t c . a i l e i d t e a i u v h i u l t , r , , I : n . a t m h ? , 0 0 , 5 , a ? , 0 5 1 , , p ? , 0 0 5 1 , 2 , t m h m l i l a e r : t t . a l o a t h i r r fi p t r d , g , t o , a h e 1 1 0 2 e e s a : n b h s l r S , G , i o , a , o . n R , 0 3 : 7 1 , 1 1 0 2 , l S . s a , 0 3 : 7 1 , 1 1 0 2 0 1 l S , 3 e , g . n R , 0 3 : 7 1 , 1 1 0 0 1 o , 4 2 v . . . . n R , 0 3 : 7 1 , 1 1 0 2 4 1 g , 3 2 v B : t 0 2 l S l S . , 3 2 v : G v : c : t . . . ) 2 - 9 ( 9 4 4 2 . . ) 4 - 6 ( g 0 . . . . . . ) 6 - 1 ( n a 0 3 ) 3 - 3 ( a 6 1 . . . ) 2 - 2 ( J 2 N N t c , 2 1 - 1 1 0 2 , 2 1 - 1 1 0 , 2 1 - 1 1 0 2 , 2 1 - 1 1 0 2 , 2 1 - 1 1 0 2 . . ) 1 - 0 ( l S 1 1 S , 2 1 - 1 1"
        },
        {
            "title": "M\nP",
            "content": "0 0 : 2 1 : t o . . ) 3 1 - 0 ( o 1 1 , 2 1 - 1 1 0 2 1 1 0 2 : a . . ) 1 1 - 4 ( o 4 2 , 2 1 - 1 1 0 . . . ) 7 - 1 1 ( x 5 1 , 2 1 - 1 1 0 2 c m m a o s 11 . 7 2 a"
        },
        {
            "title": "M\na\nL\nL",
            "content": "r g a e t n n g a"
        },
        {
            "title": "F\nA",
            "content": "- N"
        },
        {
            "title": "I\nF\nE\nR",
            "content": "d a"
        },
        {
            "title": "T\nC\nU\nR\nT\nS\nN",
            "content": "I"
        },
        {
            "title": "F\nL\nE\nS",
            "content": "r e p . i n a v m o l x : I"
        },
        {
            "title": "V\nE\nL\nB\nA\nT",
            "content": "12 [19] E. Beeching, Y. Belkada, K. Rasul, L. Tunstall, L. von Werra, N. Rajani, and N. Lambert, Stackllama: An rl fine-tuned llama model for stack exchange question and answering, 2023. [Online]. Available: https://huggingface.co/blog/stackllama [20] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, Finetuned language models are zero-shot learners, 2022. [21] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, Cross-task language crowdsourcing instructions, in generalization via natural Proceedings of the Association for Computational Linguistics (Volume 1: Long Papers), S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 34703487. [Online]. Available: https://aclanthology.org/2022.acl-long.244 the 60th Annual Meeting of [22] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. Bers, S. Biderman, L. Gao, T. Wolf, and A. M. Rush, Multitask prompted training enables zero-shot task generalization, 2022. [23] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei, Scaling instruction-finetuned language models, 2022. [24] A. Koksal, T. Schick, A. Korhonen, and H. Schutze, Longform: Effective instruction tuning with reverse instructions, 2024. [25] J. He, J. Gu, J. Shen, and M. Ranzato, Revisiting self-training for neural sequence generation, 2020. [26] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, Self-training with noisy student improves imagenet classification, 2020. [27] J. Du, E. Grave, B. Gunel, V. Chaudhary, O. Celebi, M. Auli, V. Stoyanov, and A. Conneau, Self-training improves pre-training the 2021 language understanding, in Proceedings of for natural Conference of the Association the North American Chapter of for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021, pp. 5408 5418. [Online]. Available: https://aclanthology.org/2021.naacl-main.426 [28] M.-R. Amini, V. Feofanov, L. Pauletto, L. Hadjadj, E. Devijver, and Y. Maximov, Self-training: survey, 2023. [29] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han, Large language models can self-improve, 2022. [30] C. Zhou, J. He, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, Prompt consistency for zero-shot task generalization, 2022. [31] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, Stanford alpaca: An instruction-following llama model, https://github.com/tatsu-lab/stanford alpaca, 2023."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, Large language models are human-level prompt engineers, 2023. [2] S. Ye, D. Kim, J. Jang, J. Shin, and M. Seo, Guess the instruction! flipped learning makes language models stronger zero-shot learners, 2023. [3] C. Singh, J. X. Morris, J. Aneja, A. M. Rush, and J. Gao, Explaining patterns in data with language models via interpretable autoprompting, 2023. [4] O. Honovich, U. Shaham, S. R. Bowman, and O. Levy, Instruction induction: From few examples to natural language task descriptions, 2022. [5] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, Self-instruct: Aligning language models with selfgenerated instructions, 2023. [6] X. Li, P. Yu, C. Zhou, T. Schick, O. Levy, L. Zettlemoyer, J. Weston, and M. Lewis, Self-alignment with instruction backtranslation, 2024. [7] OpenAI. (2021) Gpt-3.5 turbo documentation. [Online]. Available: https://platform.openai.com/docs/models/gpt-3- [8] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, Llama 2: Open foundation and fine-tuned chat models, 2023. [9] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, Mistral 7b, 2023. [10] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra, S. Reddy A, S. Patro, T. Dixit, and X. Shen, Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks, in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 50855109. [Online]. Available: https://aclanthology.org/2022.emnlp-main.340 [11] A. Kopf, Y. Kilcher, D. von Rutte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, S. ES, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire, C. Schuhmann, H. Nguyen, and A. Mattick, Openassistant conversations democratizing large language model alignment, 2023. [12] M. Zhong, Y. Liu, D. Yin, Y. Mao, Y. Jiao, P. Liu, C. Zhu, H. Ji, and J. Han, Towards unified multi-dimensional evaluator for text generation, 2022. [13] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, Deep reinforcement learning from human preferences, in Advances in Neural Information Processing Systems, 2017, pp. 42994307. [14] N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. Christiano, Learning to summarize from human feedback, arXiv preprint arXiv:2009.01325, 2020. [15] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [16] Y. Cao, Y. Kang, C. Wang, and L. Sun, Instruction mining: When data mining meets large language model finetuning, 2023. [17] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, Training language models to follow instructions with human feedback, 2022. [18] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, LoRA: Low-rank adaptation of large language models, in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=nZeVKeeFYf"
        }
    ],
    "affiliations": [
        "Indian Institute of Technology, Kharagpur"
    ]
}