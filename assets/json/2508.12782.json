{
    "paper_title": "HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds",
    "authors": [
        "Petr Anokhin",
        "Roman Khalikov",
        "Stefan Rebrikov",
        "Viktor Volkov",
        "Artyom Sorokin",
        "Vincent Bissonnette"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable capabilities in isolated step-by-step reasoning tasks such as mathematics and programming, but their proficiency in long-horizon planning, where solutions require extended, structured sequences of interdependent actions, remains underexplored. Existing benchmarks typically assess LLMs through abstract or low-dimensional algorithmic tasks, failing to capture the complexity of realistic planning environments. We introduce HeroBench, a novel benchmark designed specifically to evaluate long-horizon planning and structured reasoning within complex RPG-inspired virtual worlds. HeroBench provides a rigorously constructed dataset of tasks covering a wide range of difficulties, a simulated environment to execute and validate agent plans, and detailed analytical tools for evaluating model performance. Tasks challenge models to formulate strategic plans, efficiently gather resources, master necessary skills, craft equipment, and defeat adversaries, reflecting practical scenarios' layered dependencies and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning both open-source and proprietary models, including the GPT-5 family, reveals substantial performance disparities rarely observed in conventional reasoning benchmarks. Detailed error analysis further uncovers specific weaknesses in current models' abilities to generate robust high-level plans and reliably execute structured actions. HeroBench thus not only significantly advances the evaluation of LLM reasoning but also provides a flexible, scalable foundation for future research into advanced, autonomous planning in virtual environments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 2 8 7 2 1 . 8 0 5 2 : r HeroBench: Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds Petr Anokhin1,2, Roman Khalikov3, Stefan Rebrikov5,6, Viktor Volkov7, Artyom Sorokin1,4, Vincent Bissonnette7 1AIRI 2Lomonosov Moscow State University 3JSC Rotec Digital Solutions 4Skoltech 5Higher School of Economics 6Kurchatov Institute 7Independent Researcher anokhin@airi.net, rom.khalikov@gmail.com, robstef85@gmail.com, viktor.volkov.mailbox@gmail.com Abstract Large language models (LLMs) have shown remarkable capabilities in isolated step-by-step reasoning tasks such as mathematics and programming, but their proficiency in longhorizon planning, where solutions require extended, structured sequences of interdependent actions, remains underexplored. Existing benchmarks typically assess LLMs through abstract or low-dimensional algorithmic tasks, failing to capture the complexity of realistic planning environments. We introduce HeroBench, novel benchmark designed specifically to evaluate long-horizon planning and structured reasoning within complex RPG-inspired virtual worlds. HeroBench provides rigorously constructed dataset of tasks covering wide range of difficulties, simulated environment to execute and validate agent plans, and detailed analytical tools for evaluating model performance. Tasks challenge models to formulate strategic plans, efficiently gather resources, master necessary skills, craft equipment, and defeat adversaries, reflecting practical scenarios layered dependencies and constraints. Our extensive evaluation of 25 stateof-the-art LLMs, spanning both open-source and proprietary models, including the GPT-5 family, reveals substantial performance disparities rarely observed in conventional reasoning benchmarks. Detailed error analysis further uncovers specific weaknesses in current models abilities to generate robust high-level plans and reliably execute structured actions. HeroBench thus not only significantly advances the evaluation of LLM reasoning but also provides flexible, scalable foundation for future research into advanced, autonomous planning in virtual environments. Introduction The rapid advancement of large language models (LLMs) has considerably expanded their applicability beyond traditional natural language processing tasks, establishing them as core components of autonomous agent systems across diverse domains. Today, LLM-driven agents are being developed to perform increasingly complex roles in automation and strategic decision-making. Central to these applications is the ability to perform robust long-term planning and reasoning, especially in scenarios that require the execution of action sequences over extended time horizons. Despite this progress, growing body of research suggests that LLMs are not inherently capable of planning and often require external mechanisms to validate or supervise their plans (Kambhampati et al. 2024; Valmeekam et al. 2023b). Recent reinforcement learning (RL)-based training approaches have enhanced the reasoning capabilities of LLMs, giving rise to models such as OpenAI o1 (OpenAI et al. 2024) and Deepseek R1 (DeepSeek-AI et al. 2025). These models are capable of producing extended chains of thought and can engage in partial self-verification of their outputs. This has led to notable performance gains, particularly in domains such as mathematics and programming-areas that are well-suited to formal verification and commonly included in model training. However, their performance remains suboptimal in tasks that require long-term planning (Valmeekam et al. 2025; Shojaee et al. 2025). Current claims regarding LLMs planning abilities are often based on evaluations using standard algorithmic benchmarks, with tasks like Blocksworld being the primal example (Valmeekam et al. 2023a). Although such environments are easily scalable, they lack the complexity and variability characteristic of real-world scenarios. Consequently, they fail to serve as adequate proxies for evaluating the true planning capacities of LLMs. Realistic tasks typically require the integration of heterogeneous information sources, the formulation of high-level plans, the decomposition of these plans into actionable subtasks, and the selection of appropriate strategies for their execution. Simulated virtual environments and video games, such as Minecraft or Nethack, have long served as classical evaluation platforms for AI systems. However, most of these environments require the development of specialized interfaces for AI interactions, involve the execution of numerous lowlevel actions, and often include extensive information about their mechanics in the pretraining corpora of LLMs. These factors complicate efforts to isolate and rigorously evaluate the planning capabilities of LLMs in controlled and unbiased manner. To bridge this gap we introduce HeroBench, benchmark designed to evaluate the long-term planning capabilities of Figure 1: section of the HeroBench virtual environment. grid-based RPG-inspired world where agents must navigate, gather resources, craft equipment, and defeat enemies. Each location encodes specific environmental elements, forming the foundation for generating complex, structured tasks that challenge long-horizon reasoning and planning abilities of language models. LLMs and ability of agentic systems to solve tasks in structured, but complex virtual environment. Built upon the ArtifactsMMO game (V. Bissonnette 2024), originally developed to assess programming skills via script-based gameplay, HeroBench features classic RPG setting in which agents gather resources, craft equipment, and defeat enemies. The benchmark consist of predefined challenges of varying difficulty, with clear evaluation scores, enabling finegrained assessment of LLM and Large Reasoning Models (LRM) planning capabilities. We evaluated 25 state-of-the-art LLMs and 2 agentic architectures, revealing substantial differences in performance that are typically not observed on popular math and reasoning benchmarks. Code and data are available at https: //github.com/stefanrer/HeroBench HeroBench The environment displayed in Fig. 1 is structured RPGstyle game with discrete action space. The world is organized as grid of 70 locations, each containing specific elements such as resource nodes, workshops, or monster spawns. The environment includes 25 distinct monsters, 17 resource types for crafting, and 208 unique items, including gear and crafting components. All environment data is defined through JSON files. The dataset consists of tasks of varying difficulty levels, each requiring the player to defeat specific monster in the Figure 2: Example of task in the environment: the agents ultimate goal is to defeat the target monster. To achieve this, agent must calculate the optimal gear by considering both its own and the monsters stats, and acquire all the necessary ingredients. game or craft an item. An example prompt is provided in the appendix. The player starts with character of appropriate level and specific set of equipment. The tasks are divided into two categories: purely crafting tasks, which do not require any combat, and tasks that involve defeating enemies. In the latter case, defeating monster typically requires the character to craft one or more additional items beforehand. The difficulty of task is determined by the number of required items and the number of steps involved in crafting them. For example, crafting simple bronze sword may require only mining and smelting ore, whereas crafting highlevel item can involve many steps, including obtaining drops from defeated monsters and gathering and refining multiple types of resources  (Fig. 2)  . In combat-oriented tasks, the agent must explicitly calculate the optimal gear configuration before beginning any resource acquisition or crafting. This involves reasoning over multiple interacting statistics: four distinct elemental damage types (fire, earth, water, air) and their corresponding resistances, percentage-based damage amplifications, hit points, and raw attack values. The agent needs to simulate the effects of these values in turn-based combat, balancing survivability and damage output to determine the smallest viable gear set that guarantees victory against the target monster. This requirement results in more realistic setup for longhorizon planning: the agent must seamlessly combine strict strategic planning with numerical reasoning, shifting between different types of subtasksmathematical calculations, resource-gathering strategies, and crafting plan executionwhile maintaining coherence and avoiding derailment in its chain of thought. All planning experiments in this work are conducted using the deterministic version of the environment, with all item drop rates set to 1 after defeating monsters or gathering resources. However, the environment also supports stochastic mode with configurable drop rates. Benchmark Task Generation Pipeline We present systematic pipeline for constructing benchmark tasks for HeroBench. The process is both rigorous and transparent, and consists of the following steps: Monster Initialization Let = {x1, x2, . . . , xN } be the set of all monsters in the game. To generate new task, we first select target monster Xt PX (dtarget), conditioned on the desired task difficulty dtarget. Each monster Xt is associated with vector of monster statistics mt (e.g., health, damage, resistances), and difficulty level Lt, defined as the minimal character level required to defeat Xt under standard conditions. Combat Simulation To create tasks based on the target monster we simulate combat between player character and the monster Xt. Combat in HeroBench is turn-based and depends on the monster and player statistics (such as health points, attack type, resistances, and damage amplification), and the players equipped items. Combat is simulated by the function simulate: simulate(cL, mt, G) = (cid:26)1, 0, if the player wins; otherwise. , where cL is the players stats vector at level Lt , mt is the monsters stats vector, and is the set of equipped items. Minimal Winning Gear Search For selected target monster Xt, we define IL as set all equipment items with level requirement less or equal to the monsters difficulty Lt. gear set ILt is called minimal winning gear set if the following conditions hold: The player character equipped with defeats Xt in simulation: simulate(cL, mt, G) = 1. For any G, the player character equipped with loses to Xt: simulate(cL, mt, G) = 0. That is, is the smallest subset of items such that the character wins against Xt with G, but removing any single item from leads to loss. For each minimal winning gear set G, items are partitioned into: Equipped items (Ieq): items pre-equipped at episode start, Missing items (Imiss): items to be acquired or crafted during the episode. This partition is final step to create new task. The base task difficulty is defined as = Imiss, and is refined according to acquisition and crafting requirements. Figure 3: Two agent architectures were evaluated in the benchmark. A: A-1, operates in two phases: in the first phase, it generates high-level plan; in the second, it decomposes this plan into executable actions. B: Agentic system A-2, is modification of A-1 based on the idea of assigning each agent single, one-bite task. Crafting and Environment Analysis For each item Imiss the directed acyclic graph of crafting and resource dependencies is traversed to extract all required materials, intermediate monsters, and locations. The total task difficulty is given by: Dtotal = Imiss + (cid:88) cost(I) IImiss , where cost(I) is cost function for crafting the item I. For details about the cost function see App.A. Auxiliary Item Validation To enforce robust solution paths, we compute an auxiliary set of valid items Iaux such that: The character equipped with Ieq Iaux can defeat all non-target monsters Xj present in the scenario, But cannot defeat the target monster Xt without acquiring all items in Imiss. In other words For any monster Xj with corresponding stat vector mj present in the scenario, the following always holds: simulate(cL, mj, Ieq Iaux) = I[Xj = Xt], where I[] denotes the indicator function. The generation of crafting-only tasks follows the same pipeline, omitting the combat simulation and focusing solely on traversal of the crafting dependency graph for the specified goal items. Figure 4: Success rate of LLMs across nine base-task difficulty levels. Solid lines correspond to reasoning-enabled (thinking) models, while dashed lines represent standard (non-thinking) variants. Leveling mechanics In the base set of tasks, the characters crafting and gathering professions were automatically set to match the highest required level for any item in Imiss. In the extended version, we introduce skill progression: the agent starts at level 1 in all relevant professions (e.g., mining, woodcutting). To support skill growth, the agent is provided with information about all resources available for leveling up, including accessible resource nodes and corresponding experience rewards. Thus, to craft high-level item, the agent must first identify and execute sequence of actions to incrementally advance their profession skills, acquiring lower-tier resources and recipes as necessary. Noise Item Injection To increase task complexity and test agent robustness, noise items can be added to each task instance. These are plausible, high-level gear items (often of equal or higher power than true solution items) that appear valid but are in fact impossible to craft: one or more of their required components or crafting resources are omitted from the tasks environment description. Formally, for each task, set of noise items Inoise is constructed such that: Each Inoise passes all basic filtering (e.g., level, type), and is indistinguishable from valid items based on stats, requires at least one ingredient/resource not required for crafting missing items Imiss, Crafting recipes for are provided, but the agent cannot actually construct due to missing prerequisite(s). Task Representation Each task is serialized as structured JSON object, specifying: target monster or craft item name; equipped and missing items; full character state; environment information (dependencies, required monsters, locations, etc.). Prompts for language models are generated from these objects to ensure reproducibility. The final dataset contains 844 tasks with difficulty levels ranging from 2 to 97. Input prompt lengths vary from 1k to 11k tokens. For our experiments, we selected subset of 180 tasks, divided into 9 difficulty brackets. Leveling and noise mechanics can be incorporated on top of the base tasks. Evaluation The LLM or an agentic system is prompted to generate Python code that solves the given task. LLM must return the exact sequence of actions, using only for loops for continuous resource acquisition (i.e., repeated gathering or combat). The generated code is then parsed and executed in the environment, with the resulting simulation logs recorded for analysis. Two evaluation metrics are used: Success, indicating whether the final goal (crafting the target item or defeating the target monster) is achieved; and Progress score, which reflects partial completion based on valid intermediate actions such as gathering, recycling, defeating required monsters, crafting, and equipping gear. This dual-metric evaluation enables both binary assessment of task completion and fine-grained measurement of the agents progress and problem-solving efficiency. We also provide an evaluation pipeline that offers comprehensive statistics on the types of errors made by the agents. These include mistakes in hight level plan decomposition and optimal gear calculation, failures in determining the required amount of resources or appropriate level for item crafting, incorrect usage of provided information such as location coordinates, and improper code formatting in the response. This allows for more precise assessment of the models weaknesses. Multi-Agent systems architecture A-1 represents the first version of the multi-agent system developed for HeroBench. The A-1 architecture employs pair of agents: decomposer/action agent and critic agent. This agent pair is used to perform two-level task decomposition. The architecture of A-1 is shown in Fig. 3A. At the first level, the task is divided into high-level subtasks, which are used to form high-level plan. Once the plan is verified by the critic agent, it is passed to the second stage of decomposition. In this stage, each item in the highlevel plan is further broken down into basic subtasks, resulting in low-level steps (actions). The collection of all actions constitutes the final executable plan, which is carried out in the environment through calls to the corresponding environment API functions. To prevent infinite loops, the system imposes limits on the number of allowed decomposition attempts. For decomposition, the agent relies on the current environment state and automatically generated descriptions of the tasks or subtasks target objects. A-2 is more complex multi-agent system designed to evaluate deeper, linear hierarchy of cooperating agents. The main idea is to allow agents to solve simple, one-bite tasks individually while assisting one another. This system builds upon the TaskGen agentic framework (Tan et al. 2024), extending the initial A-1 architecture (Fig. 3B). Several new agents were introduced in A-2. First, curriculum agent formulates high-level plans based on the global task and the current state of the characters. An optional fight analytic agent estimates the outcomes of combat encounters, taking into account character statuses, equipment, and potential crafted items. Additional subagents were also integrated into the decomposer agent: map expert and craft expert, both capable of acquiring world knowledge and answering the decomposers queries. Finally, an action agent was added to generate the executable Python code to complete the task. Both A-1 and A-2 are also capable of solving open-ended tasks by incorporating feedback from the environment."
        },
        {
            "title": "Results",
            "content": "We evaluated wide range of state-of-the-art LLMs on our benchmark, encompassing both standard and reasoningoriented models, as well as agentic architectures. Experiments were conducted using local FP16 Qwen-8B and Qwen-32B models, the OpenAI API for the o3 and o4-mini, and the OpenRouter API for the remaining models. All hyperparameters were set to default values. The reasoning budget was set to high for OpenAI models and capped at 40,000 reasoning tokens for the other models, which was not exceeded in our experiments. All models were tested on the base set of tasks, while the top-performing reasoning models were further evaluated on harder difficulty levels incorporating additional mechanics. Model Success % Score Tokens Qwen3 8b Qwen3 32b GigaChat 2 Max Qwen3 8b (t) Deepseek-v3 Kimi-K2 GPT-oss-120b Magistral-medium Qwen3 32b (t) DeepSeek-R1-70B Qwen3-235b GPT-4.1-mini Claude-Sonnet-4 Qwen3-235b-2507 Deepseek-R1-0528 Gemini-2.5-flash GPT-4.1 o4-mini GPT-5-mini Claude-Sonnet-4 (t) o3 Gemini-2.5-pro GPT-5 Grok-4 0.0 1.7 2.8 3.9 7.2 8.3 8.9 9.4 10.0 11.2 13.3 16.1 17.2 24.4 21.7 26.1 31.7 35.0 35.0 44.4 60.6 62.9 83.9 91. 11.5 6.8 2883 1965 21.9 12.8 2074 1222 21.3 15.4 1190 228 28.8 15.5 9680 1224 1586 430 32.7 17.9 29.6 16.4 1309 237 9372 2959 27.0 8.7 25.0 18.8 10885 1667 44.8 17.1 9107 1458 27.5 21.2 7448 1029 34.9 20.5 12006 1746 53.9 17.6 4555 1398 50.6 21.0 1578 306 49.4 18.6 11387 2702 48.7 22.5 10711 2088 64.8 13.7 11028 4010 73.7 10.3 3518 1202 56.1 23.5 21993 8181 59.8 22.5 14126 4169 73.8 16.9 16397 4313 84.6 8.5 13897 5250 86.6 10.4 12935 4295 95.0 3.3 17851 7149 95.3 3.3 15470 5838 Table 1: Mean performance of all evaluated models across nine base task difficulty levels in HeroBench. Columns show success rate (%), score (mean SD), and tokens (mean SD). SD is computed across the nine difficulty-level averages for each model. Thinking-enabled variants are denoted by (t)."
        },
        {
            "title": "Base tasks",
            "content": "The success rate over different difficulties is shown in Fig. 4 and mean metrics over all difficulties are presented in table 1. For the additional results see appendix. Reasoning models consistently outperform standard models across all levels of task difficulty. However, the accuracy of most LRMs declines as complexity increases. In contrast to conventional mathematical and coding benchmarks, our evaluation reveals substantial variability in model performance. Notably, Grok 4 achieved the highest scores and exhibited the least performance degradation as task difficulty increased, clearly outperforming other models at higher difficulty levels. Among non-reasoning models, GPT-4.1 demonstrated the best performance, outperforming several open-source reasoning models and achieving success rate close to that of o4-mini, while even surpassing it in score. Its success rate was nearly double that of Claude Sonnet-4 (non-thinking), although it used more than twice the number of tokens to solve the tasks. Overall, GPT-4.1 and Claude Sonnet-4 (nonthinking) exhibited the best performance in terms of success per tokens spent. We tested the Qwen3-235b-a22 and Qwen3-235b-a222507 models following switch from the widely used GRPO (Shao et al. 2024) algorithm to GSPO (Zheng et al. 2025), which is better suited for RL training of MoE architectures. This change resulted in significant improvement in the Model Errors (mean SD) Failure Types (% of all tasks) Hight-level Execution Failed Only Gear Gear+Exec Only Exec Invalid Code Qwen3 8b Qwen3 32b GigaChat 2 Max Qwen3 8b (think) Deepseek-v3 Kimi-K2 GPT-oss-120b Magistral-medium-2506 (think) Qwen3 32b (think) DeepSeek-R1-70B (think) Qwen3-235b-a22 (think) GPT-4.1-mini Claude-Sonnet-4 Qwen3-235b-a22-2507 (think) Deepseek-R1-0528 (think) Gemini-2.5-flash (think) GPT-4.1 o4-mini (think) GPT-5-mini (think) Claude-sonnet-4 (think) o3 (think) Gemini-2.5-pro (think) GPT-5 (think) Grok-4 (think) 3.32 2.23 3.62 2.33 3.89 2.49 3.52 2.31 3.69 2.38 3.78 2.52 3.30 2.22 3.10 2.48 3.21 2.18 3.25 2.64 2.79 2.40 2.77 2.42 3.41 2.59 2.76 2.41 2.73 2.46 1.29 1.27 1.50 1.39 2.12 2.11 1.86 2.07 1.58 1.58 0.82 0.98 0.83 1.09 0.19 0.28 0.11 0. 2.59 0.49 6.44 5.86 1.93 1.11 2.69 0.88 1.39 0.63 1.34 0.71 4.90 5.95 2.14 2.10 4.89 4.09 1.86 0.88 2.79 1.91 1.91 1.79 4.93 5.01 1.89 1.15 1.38 1.39 1.56 1.66 0.78 0.89 0.44 0.30 0.37 0.32 0.34 0.21 0.18 0.17 0.10 0.12 0.08 0.07 0.02 0.04 100.0 98.3 97.2 96.1 92.8 91.7 91.9 90.6 90.0 86.7 87.8 83.9 82.8 77.8 78.3 73.9 68.3 65.0 65.0 55.6 39.4 37.8 16.1 8.3 8.9 5.0 17.2 5.6 16.7 17.8 12.2 16.1 9.4 11.1 9.4 7.2 0.6 13.9 19.4 17.8 26.1 22.8 29.4 24.4 21.7 22.2 8.3 2.8 67.8 77.2 61.1 78.9 68.3 53.9 34.4 32.8 70.6 41.7 45.0 55.0 60.6 36.7 39.4 30.6 32.2 22.2 15.6 21.7 7.8 7.8 1.1 1.7 10.6 7.2 5.6 7.8 5.0 1.1 3.9 2.8 5.6 6.7 2.8 6.7 3.3 2.2 4.4 7.8 3.3 3.9 2.8 3.9 7.8 1.1 5.6 0.6 12.8 7.2 11.7 2.8 1.7 18.3 40.6 36.7 3.3 26.7 27.2 11.1 16.1 22.2 12.2 10.6 2.2 14.4 15.0 3.3 1.1 1.7 0.6 2. Table 2: Breakdown of failure types across models. First two columns show the mean SD number of errors per task for tasks with valid generated code. The first column reports the average number of items missing in the high-level plan, while the second reports the average number of low-level execution mistakes. The remaining columns show the percentage of tasks that failed due to: gear selection only, gear selection plus execution errors, execution-only errors, or invalid code. models planning capabilities, though still not sufficient to match the performance of proprietary models. Results analysis To understand the weaknesses and failure modes of various models, we provide script for comprehensive analytics of models performance on the tasks. It scores how many tasks failed due to errors in the highlevel plan for selecting optimal gear, how many items were incorrectly chosen for the optimal outcome, how many mistakes the model made in executing the plan (e.g., incorrect amounts of resources, misusing environmental information, redundant steps, etc.), and how many plans failed due to incorrectly formatted Python code. The results presented in Table 2 show substantial differences in the ability of models to follow instructions to produce reliable code, and use provided information to execute both highand low-level plans. Proprietary reasoning models tend to make fewer mistakes in the execution of low-level actions; most of their failures stem from determining highlevel optimal gear plan. In contrast, weaker models typically make mistakes in both areas. GPT-5 had the lowest error rate in code execution, with only 0.6% failures, demonstrating excellent instruction following. In contrast, GPT-OSS-120B had the highest code format error rate at 40.6%. Leveling + distractor noise mechanics Table 3 presents the results for level 9 difficulty tasks, including the effects of leveling mechanics and distractor noise items, evaluated on top-performing models. Grok-4 demonstrates significant lead over all other models, with only Gemini 2.5 Pro managing to solve subset of the hardest tasks. Notably, Grok-4s performance remains consistently high across all difficulty levels, showing drop in success rate with addition of leveling and score with addition of noise items. The performance of GPT-5 decreases with the addition of leveling mechanics but remains unaffected by the inclusion of additional noise items. Notably, GPT-5 and Grok-4 are the only two models that substantially increase their reasoning length as task difficulty rises. The results of Grok-4 also show that the tasks are solvable within the 20-35k output tokens. The details of Grok-4s architecture and training remain undisclosed, though its impressive performance may be attributed to the reported large-scale reinforcement learning applied during post-training. Pass@k metric While recent findings suggest that reinforcement learning with verifiable rewards (RLVR) may not consistently improve over the base models pass@k performance when is sufficiently large (Yue et al. 2025), our results indicate that this conclusion may be task-dependent. In the context of our planning benchmark, particularly at difficulty levels 1 and 2, we observed that, even after 200 attempts, the base models Qwen3-8B and Qwen3-32B were unable to match the performance of their reasoning-enabled counterparts, which Model Base Succ (%) Score Tokens Succ (%) Leveling Score Tokens Succ (%) Score Tokens Leveling+Noise o3 Claude-Sonnet-4 Gemini-2.5-pro GPT-5 Grok-4 5 10 25 55 80 66.2 32.1 20688 2791 42.6 36.3 21366 6036 66.1 26.6 18636 3835 90.6 16.5 28052 3776 95.5 14.2 22850 4587 0 0 10 15 26.6 28.4 22606 2788 25.6 19.0 24588 6651 32.7 26.4 20047 3141 62.3 32.6 31704 3656 92.9 16.5 28361 5953 0 0 5 20 65 15.9 12.0 23562 3996 21.9 14.2 25404 5697 36.0 28.5 21741 3127 59.9 34.2 36052 4196 78.8 31.8 33305 6672 Table 3: Evaluation of five leading reasoning models under increased task complexity. Results are shown for three conditions: Base (standard level 9 tasks), Leveling (requires skill progression before crafting), and Leveling+Noise (adds adversarial distractor items). Metrics include success rate, progress score (mean SD), and token usage (mean SD). Model Qwen3-8B Qwen3-8B (t) Qwen3-32B Qwen3-32B (t) Diff 1 1 2 200 10 200 10 pass@k Mean Win 45.0% 65.0% 30.0% 75.0% 11.8% 30.5% 0.6% 20.0% Table 4: Performance of thinking and non-thinking Qwen3 Models using pass@k metric. Results suggest that the RLVR approach noticeably improves the results and may be taskdependent. Agent/Model Difficulty 2 Difficulty 3 65% A-1 35% A-2 45% GPT-4.1-mini 60% 10% 15% Table 5: A-1, A-2 and GPT-4.1-mini success rate comparison. The results indicate that the simple decomposer-critic loop for the small models outperformed modified decomposition architecture with one-bite tasks. achieved higher pass rates with just 10 attempts Table 4. Since these lower-difficulty tasks do not demand long reasoning chains, as evidenced by the success of other nonreasoning models on them,this gap suggests that RLVR can provide tangible benefits in planning scenarios where structured reasoning is essential. Multi-Agent systems performance To evaluate the multi-agent systems, tasks with difficulty levels 2 and 3 were selected. GPT-4.1-mini was chosen as the baseline model due to its accessibility, high speed, and reasonably strong performance. The experimental results  (Table 5)  show that the A-1 multi-agent system achieved higher success rate than the baseline model. However, the performance of A-2 was lower than that of the baseline, presumably due to its more complex architecture and prompt overengineering. Smaller models were unable to effectively process the context provided by the subagents and exhibited hallucinations during plan and subtask generation. These findings indirectly suggest that while multi-agent systems can better maintain problem-solving capabilities at higher task complexities, they require very careful design - particularly with regard to task complexity and prompt size. Related work LLMs have demonstrated impressive emergent capabilities in multi-step reasoning, long-horizon planning, and decision-making. Techniques such as Chain-of-Thought prompting (Wei et al. 2023), ReAct (Yao et al. 2023b), Reflexion (Shinn et al. 2023), and Tree-of-Thoughts (Yao et al. 2023a) augment LLMs with memory, feedback, or branching inference to support deeper planning and self-correction. These approaches have been applied in complex settings such as arithmetic reasoning, games, and embodied tasks. Recent work extends these capabilities into interactive and multi-agent contexts. For example, Generative Agents (Park et al. 2023) simulate believable social behaviors in sandbox simulations. In Minecraft environments, systems like VOYAGER (Wang et al. 2023) and Plan4MC (Yuan et al. 2023) integrate planning, code execution, and tool use for long-horizon exploration, survival, and construction. These efforts showcase how LLMs can engage with evolving environments and incomplete information through iterative decision-making and external tool integration. number of benchmarks have been developed to evaluate different aspects of LLMs, LRMs, and agentic systems. For example, AgentBench (Liu et al. 2023) focuses on coding, gaming, and web browsing tasks to evaluate decisionmaking and reasoning in open-ended, multi-turn interactions. more recent MultiAgentBench (Zhu et al. 2025) targets multi-agent coordination, featuring tasks such as cooperative puzzle-solving and bargaining. SmartPlay (Wu et al. 2024) introduces six games to evaluate agents long-horizon planning and probabilistic reasoning. CraftText (Volovikova et al. 2025) assesses instruction-following capabilities in stochastic gamified environment. The Natural Plan benchmark (Zheng et al. 2024) evaluates realistic planning through three core tasks - trip planning, meeting scheduling, and calendar coordination - each with varying constraints and difficulty levels. TravelPlanner (Xie et al. 2024) extends the idea with more complex scenarios and constraints. Plancraft (Dagan, Keller, and Lascarides 2025) introduces multi-modal benchmark built on Minecrafts crafting system to evaluate LLM and VLM agents on hierarchical planning, resource reasoning, and feasibility recognition. Finally, the PPNL benchmark (Aghzal, Plaku, and Yao 2025) proposes path planning in grid environment using natural language instructions."
        },
        {
            "title": "Conclusions",
            "content": "We present HeroBench as novel and comprehensive benchmark specifically designed to evaluate long-horizon planning and structured reasoning capabilities of LLMs and agentic systems. Unlike prior planning benchmarks, which often rely on scalable, but oversimplified algorithmic puzzles or game-based environments lacking strict evaluation metrics and relying heavily on dynamic feedback, HeroBench provides controlled yet richly structured setting that captures the combinatorial complexity and interdependent subtasks typical of real-world scenarios. Our benchmark introduces several features: (1) automatic difficulty scaling through grounded resource and skill dependencies; (2) fine-grained scoring metrics that capture both success and partial progress; (3) failure mode analytics, enabling precise diagnosis of reasoning, planning, and code generation errors; (4) support for difficulty increasing through extended mechanics such as leveling and adversarial noise injection; (5) easy benchmark deployment, environment monitoring and parallelism suitability; (6) environment scalability in terms of content, game mechanics and task types. Through systematic evaluation of 20 open-source and proprietary models, as well as agentic systems, our results reveal large performance disparities across architectures and difficulty levels. Reasoning-enhanced models consistently outperformed both non-reasoning baselines and agentic systems, particularly under increased task complexity. Among all evaluated systems, Grok-4 achieved the best overall performance by wide margin, showing strong resilience even under extended mechanics. However, no model achieved perfect scores, highlighting the remaining challenges in robust, long-horizon autonomous planning."
        },
        {
            "title": "Future work",
            "content": "Although the current set of tasks is designed to evaluate existing systems without overwhelming them with excessive complexity, the environment natively supports multi-agent dynamics, seamless transition to open-ended play, and integration of visual modalities. Future extensions may include multi-agent manipulation, collaboration and competition dynamics, additional in-game mechanics, stochasticity in the tasks, extended RL frameworks support and the requirement to generate complete game-solving scripts - challenging objective even for the latest generation of agentic systems such as AlphaEvolve (Novikov et al. 2025)."
        },
        {
            "title": "References",
            "content": "Aghzal, M.; Plaku, E.; and Yao, Z. 2025. Can Large Language Models be Good Path Planners? Benchmark and Investigation on Spatial-temporal Reasoning. arXiv:2310.03249. Dagan, G.; Keller, F.; and Lascarides, A. 2025. Plancraft: an evaluation dataset for planning with LLM agents. arXiv:2412.21033. DeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; Zhang, X.; Yu, X.; Wu, Y.; Wu, Z. F.; Gou, Z.; Shao, Z.; Li, Z.; Gao, Z.; Liu, A.; Xue, B.; Wang, B.; Wu, B.; Feng, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Chen, D.; Ji, D.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Chen, G.; Li, G.; Zhang, H.; Bao, H.; Xu, H.; Wang, H.; Ding, H.; Xin, H.; Gao, H.; Qu, H.; Li, H.; Guo, J.; Li, J.; Wang, J.; Chen, J.; Yuan, J.; Qiu, J.; Li, J.; Cai, J. L.; Ni, J.; Liang, J.; Chen, J.; Dong, K.; Hu, K.; Gao, K.; Guan, K.; Huang, K.; Yu, K.; Wang, L.; Zhang, L.; Zhao, L.; Wang, L.; Zhang, L.; Xu, L.; Xia, L.; Zhang, M.; Zhang, M.; Tang, M.; Li, M.; Wang, M.; Li, M.; Tian, N.; Huang, P.; Zhang, P.; Wang, Q.; Chen, Q.; Du, Q.; Ge, R.; Zhang, R.; Pan, R.; Wang, R.; Chen, R. J.; Jin, R. L.; Chen, R.; Lu, S.; Zhou, S.; Chen, S.; Ye, S.; Wang, S.; Yu, S.; Zhou, S.; Pan, S.; Li, S. S.; Zhou, S.; Wu, S.; Ye, S.; Yun, T.; Pei, T.; Sun, T.; Wang, T.; Zeng, W.; Zhao, W.; Liu, W.; Liang, W.; Gao, W.; Yu, W.; Zhang, W.; Xiao, W. L.; An, W.; Liu, X.; Wang, X.; Chen, X.; Nie, X.; Cheng, X.; Liu, X.; Xie, X.; Liu, X.; Yang, X.; Li, X.; Su, X.; Lin, X.; Li, X. Q.; Jin, X.; Shen, X.; Chen, X.; Sun, X.; Wang, X.; Song, X.; Zhou, X.; Wang, X.; Shan, X.; Li, Y. K.; Wang, Y. Q.; Wei, Y. X.; Zhang, Y.; Xu, Y.; Li, Y.; Zhao, Y.; Sun, Y.; Wang, Y.; Yu, Y.; Zhang, Y.; Shi, Y.; Xiong, Y.; He, Y.; Piao, Y.; Wang, Y.; Tan, Y.; Ma, Y.; Liu, Y.; Guo, Y.; Ou, Y.; Wang, Y.; Gong, Y.; Zou, Y.; He, Y.; Xiong, Y.; Luo, Y.; You, Y.; Liu, Y.; Zhou, Y.; Zhu, Y. X.; Xu, Y.; Huang, Y.; Li, Y.; Zheng, Y.; Zhu, Y.; Ma, Y.; Tang, Y.; Zha, Y.; Yan, Y.; Ren, Z. Z.; Ren, Z.; Sha, Z.; Fu, Z.; Xu, Z.; Xie, Z.; Zhang, Z.; Hao, Z.; Ma, Z.; Yan, Z.; Wu, Z.; Gu, Z.; Zhu, Z.; Liu, Z.; Li, Z.; Xie, Z.; Song, Z.; Pan, Z.; Huang, Z.; Xu, Z.; Zhang, Z.; and Zhang, Z. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948. Kambhampati, S.; Valmeekam, K.; Guan, L.; Verma, M.; Stechly, K.; Bhambri, S.; Saldyt, L. P.; and Murthy, A. B. 2024. Position: LLMs Cant Plan, But Can Help Planning In Forty-first International in LLM-Modulo Frameworks. Conference on Machine Learning. Liu, X.; Yu, H.; Zhang, H.; Xu, Y.; Lei, X.; Lai, H.; Gu, Y.; Ding, H.; Men, K.; Yang, K.; Zhang, S.; Deng, X.; Zeng, A.; Du, Z.; Zhang, C.; Shen, S.; Zhang, T.; Su, Y.; Sun, H.; Huang, M.; Dong, Y.; and Tang, J. 2023. AgentBench: Evaluating LLMs as Agents. arXiv:2308.03688. Novikov, A.; Vu, N.; Eisenberger, M.; Dupont, E.; Huang, P.-S.; Wagner, A. Z.; Shirobokov, S.; Kozlovskii, B.; Ruiz, F. J. R.; Mehrabian, A.; Kumar, M. P.; See, A.; Chaudhuri, S.; Holland, G.; Davies, A.; Nowozin, S.; Kohli, P.; and Balog, M. 2025. AlphaEvolve: coding agent for scientific and algorithmic discovery. arXiv:2506.13131. OpenAI; :; Jaech, A.; Kalai, A.; Lerer, A.; Richardson, A.; El-Kishky, A.; Low, A.; Helyar, A.; Madry, A.; Beutel, A.; Carney, A.; Iftimie, A.; Karpenko, A.; Passos, A. T.; Neitz, A.; Prokofiev, A.; Wei, A.; Tam, A.; Bennett, A.; Kumar, A.; Saraiva, A.; Vallone, A.; Duberstein, A.; Kondrich, A.; Mishchenko, A.; Applebaum, A.; Jiang, A.; Nair, A.; Zoph, B.; Ghorbani, B.; Rossen, B.; Sokolowsky, B.; Barak, B.; McGrew, B.; Minaiev, B.; Hao, B.; Baker, B.; Houghton, B.; McKinzie, B.; Eastman, B.; Lugaresi, C.; Bassin, C.; Hudson, C.; Li, C. M.; de Bourcy, C.; Voss, C.; Shen, C.; Zhang, C.; Koch, C.; Orsinger, C.; Hesse, C.; Fischer, C.; Chan, C.; Roberts, D.; Kappler, D.; Levy, D.; Selsam, D.; Dohan, D.; Farhi, D.; Mely, D.; Robinson, D.; Tsipras, D.; Li, D.; Oprica, D.; Freeman, E.; Zhang, E.; Wong, E.; Proehl, E.; Cheung, E.; Mitchell, E.; Wallace, E.; Ritter, E.; Mays, E.; Wang, F.; Such, F. P.; Raso, F.; Leoni, F.; Tsimpourlas, F.; Song, F.; von Lohmann, F.; Sulit, F.; Salmon, G.; Parascandolo, G.; Chabot, G.; Zhao, G.; Brockman, G.; Leclerc, G.; Salman, H.; Bao, H.; Sheng, H.; Andrin, H.; Bagherinezhad, H.; Ren, H.; Lightman, H.; Chung, H. W.; Kivlichan, I.; OConnell, I.; Osband, I.; Gilaberte, I. C.; Akkaya, I.; Kostrikov, I.; Sutskever, I.; Kofman, I.; Pachocki, J.; Lennon, J.; Wei, J.; Harb, J.; Twore, J.; Feng, J.; Yu, J.; Weng, J.; Tang, J.; Yu, J.; Candela, J. Q.; Palermo, J.; Parish, J.; Heidecke, J.; Hallman, J.; Rizzo, J.; Gordon, J.; Uesato, J.; Ward, J.; Huizinga, J.; Wang, J.; Chen, K.; Xiao, K.; Singhal, K.; Nguyen, K.; Cobbe, K.; Shi, K.; Wood, K.; Rimbach, K.; Gu-Lemberg, K.; Liu, K.; Lu, K.; Stone, K.; Yu, K.; Ahmad, L.; Yang, L.; Liu, L.; Maksin, L.; Ho, L.; Fedus, L.; Weng, L.; Li, L.; McCallum, L.; Held, L.; Kuhn, L.; Kondraciuk, L.; Kaiser, L.; Metz, L.; Boyd, M.; Trebacz, M.; Joglekar, M.; Chen, M.; Tintor, M.; Meyer, M.; Jones, M.; Kaufer, M.; Schwarzer, M.; Shah, M.; Yatbaz, M.; Guan, M. Y.; Xu, M.; Yan, M.; Glaese, M.; Chen, M.; Lampe, M.; Malek, M.; Wang, M.; Fradin, M.; McClay, M.; Pavlov, M.; Wang, M.; Wang, M.; Murati, M.; Bavarian, M.; Rohaninejad, M.; McAleese, N.; Chowdhury, N.; Chowdhury, N.; Ryder, N.; Tezak, N.; Brown, N.; Nachum, O.; Boiko, O.; Murk, O.; Watkins, O.; Chao, P.; Ashbourne, P.; Izmailov, P.; Zhokhov, P.; Dias, R.; Arora, R.; Lin, R.; Lopes, R. G.; Gaon, R.; Miyara, R.; Leike, R.; Hwang, R.; Garg, R.; Brown, R.; James, R.; Shu, R.; Cheu, R.; Greene, R.; Jain, S.; Altman, S.; Toizer, S.; Toyer, S.; Miserendino, S.; Agarwal, S.; Hernandez, S.; Baker, S.; McKinney, S.; Yan, S.; Zhao, S.; Hu, S.; Santurkar, S.; Chaudhuri, S. R.; Zhang, S.; Fu, S.; Papay, S.; Lin, S.; Balaji, S.; Sanjeev, S.; Sidor, S.; Broda, T.; Clark, A.; Wang, T.; Gordon, T.; Sanders, T.; Patwardhan, T.; Sottiaux, T.; Degry, T.; Dimson, T.; Zheng, T.; Garipov, T.; Stasi, T.; Bansal, T.; Creech, T.; Peterson, T.; Eloundou, T.; Qi, V.; Kosaraju, V.; Monaco, V.; Pong, V.; Fomenko, V.; Zheng, W.; Zhou, W.; McCabe, W.; Zaremba, W.; Dubois, Y.; Lu, Y.; Chen, Y.; Cha, Y.; Bai, Y.; He, Y.; Zhang, Y.; Wang, Y.; Shao, Z.; and Li, Z. 2024. OpenAI o1 System Card. arXiv:2412.16720. Park, J. S.; OBrien, J. C.; Cai, C. J.; Morris, M. R.; Liang, P.; and Bernstein, M. S. 2023. Generative Agents: Interactive Simulacra of Human Behavior. arXiv:2304.03442. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y. K.; Wu, Y.; and Guo, D. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300. Shinn, N.; Cassano, F.; Berman, E.; Gopinath, A.; Reflexion: LanNarasimhan, K.; and Yao, S. 2023. https://www. ArtifactsMMO. guage Agents with Verbal Reinforcement Learning. arXiv:2303.11366. Shojaee, P.; Mirzadeh, I.; Alizadeh, K.; Horton, M.; Bengio, S.; and Farajtabar, M. 2025. The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity. arXiv:2506.06941. Tan, J. C. M.; Saroj, P.; Runwal, B.; Maheshwari, H.; Sheng, B. L. Y.; Cottrill, R.; Chona, A.; Kumar, A.; and Motani, M. 2024. TaskGen: Task-Based, Memory-Infused Agentic Framework using StrictJSON. arXiv:2407.15734. V. Bissonnette. 2024. artifactsmmo.com/. Valmeekam, K.; Marquez, M.; Olmo, A.; Sreedharan, S.; and Kambhampati, S. 2023a. PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems, volume 36, 3897538987. Curran Associates, Inc. Valmeekam, K.; Marquez, M.; Sreedharan, S.; and Kambhampati, S. 2023b. On the Planning Abilities of Large Language Models - Critical Investigation. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems, volume 36, 7599376005. Curran Associates, Inc. Valmeekam, K.; Stechly, K.; Gundawar, A.; and Kambhampati, S. 2025. Systematic Evaluation of the Planning and Scheduling Abilities of the Reasoning Model o1. Transactions on Machine Learning Research. Volovikova, Z.; Gorbov, G.; Kuderov, P.; Panov, A. I.; and Skrynnik, A. 2025. CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World. arXiv:2505.11962. Wang, G.; Xie, Y.; Jiang, Y.; Mandlekar, A.; Xiao, C.; Zhu, Y.; Fan, L.; and Anandkumar, A. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. arXiv:2305.16291. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023. Chain-ofThought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903. Wu, Y.; Tang, X.; Mitchell, T. M.; and Li, Y. 2024. SmartPlay: Benchmark for LLMs as Intelligent Agents. arXiv:2310.01557. Xie, J.; Zhang, K.; Chen, J.; Zhu, T.; Lou, R.; Tian, Y.; Xiao, Y.; and Su, Y. 2024. TravelPlanner: Benchmark for RealWorld Planning with Language Agents. arXiv:2402.01622. Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao, Y.; and Narasimhan, K. 2023a. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv:2305.10601. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023b. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629. Yuan, H.; Zhang, C.; Wang, H.; Xie, F.; Cai, P.; Dong, H.; and Lu, Z. 2023. Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks. arXiv:2303.16563. Yue, Y.; Chen, Z.; Lu, R.; Zhao, A.; Wang, Z.; Yue, Y.; Song, S.; and Huang, G. 2025. Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? arXiv:2504.13837. Zheng, C.; Liu, S.; Li, M.; Chen, X.-H.; Yu, B.; Gao, C.; Dang, K.; Liu, Y.; Men, R.; Yang, A.; Zhou, J.; and Lin, J. 2025. Group Sequence Policy Optimization. arXiv:2507.18071. Zheng, H. S.; Mishra, S.; Zhang, H.; Chen, X.; Chen, M.; Nova, A.; Hou, L.; Cheng, H.-T.; Le, Q. V.; Chi, E. H.; and Zhou, D. 2024. NATURAL PLAN: Benchmarking LLMs on Natural Language Planning. arXiv:2406.04520. Zhu, K.; Du, H.; Hong, Z.; Yang, X.; Guo, S.; Wang, Z.; Wang, Z.; Qian, C.; Tang, X.; Ji, H.; and You, J. 2025. MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents. arXiv:2503.01935."
        },
        {
            "title": "Appendix",
            "content": "Model Inference Configuration All experiments using Openrouter API were conducted using default hyperparameters: temperature = 1.0, top = 1.0, top = 0, frequency penalty = 0.0, presence penalty = 0.0, repetition penalty = 1.0, min = 0.0, top = 0.0. The Qwen 3 models were run with the same hyperparameters on system equipped with 2 A100 GPUs (80GB VRAM each)."
        },
        {
            "title": "Additional Charts",
            "content": "Figure 5: Score of LLMs across nine base-task difficulty levels. Solid lines correspond to reasoning-enabled (thinking) models, while dashed lines represent standard (non-thinking) variants. Figure 6: Token use of LLMs across nine base-task difficulty levels. Solid lines correspond to reasoning-enabled (thinking) models, while dashed lines represent standard (non-thinking) variants. Figure 7: Mean score of LLMs over nine base-task difficulty levels. Solid bars correspond to reasoning-enabled (thinking) models, while hatched bars represent standard (non-thinking) variants. Figure 8: Mean success rate of LLMs over nine base-task difficulty levels. Solid bars correspond to reasoning-enabled (thinking) models, while hatched bars represent standard (non-thinking) variants. Figure 9: Mean score per token of LLMs over nine base-task difficulty levels. Solid bars correspond to reasoning-enabled (thinking) models, while hatched bars represent standard (non-thinking) variants. Figure 10: Mean success rate per token of LLMs over nine base-task difficulty levels. Solid bars correspond to reasoningenabled (thinking) models, while hatched bars represent standard (non-thinking) variants."
        }
    ],
    "affiliations": [
        "AIRI",
        "Higher School of Economics",
        "Independent Researcher",
        "JSC Rotec Digital Solutions",
        "Kurchatov Institute",
        "Lomonosov Moscow State University",
        "Skoltech"
    ]
}