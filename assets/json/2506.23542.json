{
    "paper_title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention",
    "authors": [
        "Weida Wang",
        "Changyong He",
        "Jin Zeng",
        "Di Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at \\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}."
        },
        {
            "title": "Start",
            "content": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention Weida Wang1 Changyong He1* Jin Zeng1 Di Qiu2 1School of Computer Science and Technology, Tongji University 2Google 5 2 0 J 0 3 ] . [ 1 2 4 5 3 2 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate maximum posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing highperformance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves stateof-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at https://github.com/davidweidawang/GIGA-ToF. 1. Introduction Continuous-wave Time-of-Flight (ToF) sensing [4] has emerged as the mainstream 3D imaging scheme due to its real-time response speed and low power consumption, empowering various applications such as robotics [22], 3D reconstruction [18], augmented reality [10], etc. For brevity, we hereinafter refer to continuous-wave ToF sensors as ToF sensors. However, depth images captured by ToF sensors are subject to noise at distant, low-reflectance, glossy areas *indicates equal contribution. Corresponding author: Jin Zeng (zengjin@tongji.edu.cn). Figure 1. Illustration of (a) GT depth, noisy ToF depth in (b) current frame and (c) previous frame, (d) single-frame GLRUN [17] where noise remains, (e) multi-frame MTDNet [9] fusing depth features, (f) proposed GIGA-ToF fusing graph structures. Due to depth shifts at corresponding pixels in red rectangles, (e) loses details, while (f) removes noise while preserving sharpness because the neighborhood correlation graphs are motion-invariant. [5] as shown in Fig. 1(b), which significantly impedes their performance in advanced applications. To enhance the quality of ToF depth images, researchers have proposed variety of denoising methods. Early work primarily focused on statistical model-based filtering techniques, such as bilinear [36] and non-local means [11] filtering. Leveraging progress in graph signal processing (GSP), ToF depth denoising is formulated as maximum posteriori (MAP) problem using graph-based image priors to promote depth image properties such as sparsity [16] or smoothness [32, 41]. With the advent of deep learning, methods based on deep neural networks (DNNs) achieve the state-of-the-art (SOTA) performance [6, 15, 30, 34]. However, most existing DNN schemes focus on single-frame processing and ignore cross-frame correlations, resulting in undesirable temporal inconsistency. Fig. 1(d) exemplifies the single-frame scheme GLRUN [17], where the result contains noticeable noise due to limited intra-frame information for denoising, further leading to temporal jittering. This motivates recent multi-frame processing methods exploiting temporal correlation inherent in ToF depth video. These methods typically estimate scene flow [20, 35] or inter-frame correlation [9] to establish correspondence between pixels in different frames, based on which the features of corresponding or correlated pixels are fused to reconstruct the final depth output. However, depth values of the same object are changing in different frames due to camera motions as shown in Fig. 1 by comparing (b) and (c), so features extracted from depth are usually inconsistent across frames. Due to the temporal variation of depth, direct fusion of depth features is likely to result in spatial ambiguity where features with shifts are aggregated. Fig. 1(e) exemplifies the multi-frame processing MTDNet [9] fusing depth features, resulting in loss of details. On the contrary, we fuse motion-invariant graph structures, simultaneously enhancing temporal stability and spatial sharpness. As illustrated in Fig. 1(b) and (c), despite the depth value shifts, the graph structures reflecting correlations among neighboring pixels are similar in current and previous frames, i.e., representing the shape of the teapot. This motivates us to construct intra-frame graphs to encode pixel correlations within depth images, then establish cross-frame geometric attention to fuse graphs in current and reference frames. In this way, the temporal correlation is efficiently utilized to generate smooth results with spatial sharpness as shown in Fig. 1(f). Apart from the spatial ambiguity issue, existing DNN schemes are usually trained on synthetic data due to the difficulty in acquiring ground truth [14, 34], resulting in poor generalization to real data. Although existing schemes adopt domain adaptation to enhance the network robustness to real noise [1, 2], the performance still fails at high noise levels. In contrast, we incorporate the image smoothness prior defined on the fused graph into the network architecture, restricting its solution space [24] and enhancing generalization to real data. Specifically, leveraging the fused graph to impose image smoothness prior and incorporating the data fidelity term based on the ToF depth noise distribution, we formulate the MAP problem for denoising ToF raw data. The solution is unrolled into iterative filters whose weights are dynamically learned from the geometric attention informed by cross-frame graph fusion. The resulting network combines high performance with graph spectral interpretability, facilitated by the graph-informed geometric attention (GIGA) module and is referred to as GIGA-ToF network. Our contributions are summarized as follows. We utilize cross-frame correlation by fusing motioninvariant graph structures, which simultaneously enhances temporal consistency and spatial sharpness; We formulate the MAP problem for ToF denoising by leveraging the fuse graph to impose image smoothness prior; the network is designed by unrolling the solution into iterative filtering to enable adaptive filter weight learning from the graph-informed geometric attention; We demonstrate the enhanced accuracy and consistency of GIGA-ToF on the synthetic DVToF dataset, outperforming competing schemes by at least 37.9% in MAE and 13.2% in TEPE. In addition, we show strong generalization ability of GIGA-ToF to real unseen Kinectv2 data. 2. Related Works 2.1. ToF Depth Denoising ToF depth denoising methods can be categorized into model-based and DNN-based approaches. Model-based methods rely on mathematical models derived from signal priors [13, 36]. Recently, leveraging progress in GSP [7, 25], ToF depth denoising is formulated as MAP problem using graph-based image priors [16, 32, 41]. However, rule-based modeling can be suboptimal in practice due to the complicated nature of real noise. Recent works focus on DNN-based methods for ToF denoising, which leverage large datasets and deep learning architectures to improve noise removal. While many approaches directly denoise generated depth images [21, 37], errors accumulate during depth construction from raw ToF data, resulting in distinctive ToF depth noise distributions and posing difficulty on denoising [34]. This motivates various methods to instead process raw ToF data and build end-to-end networks to produce denoised depth images [1, 6, 14, 33, 34]. For example, ToFNet [34] generated restored depth from raw ToF data with multi-scale network, significantly improving imaging quality. Despite the advancements in both model-based and DNN-based approaches, most existing ToF denoising methods operate in frame-by-frame manner, neglecting cross-frame correlation. This results in temporal inconsistency and hinder application of ToF depth in downstream tasks, where temporal stability is essential for robust performance. 2.2. Temporal ToF Depth Denoising In practical applications, depth restoration is typically performed on video streams rather than individual frames. Nevertheless, there is relatively little work focusing on utilizing temporal correlation and maintaining temporal stability for ToF depth denoising. In model-based methods, temporal correlation is utilized in signal modeling, such as motion vector smoothness prior in the graph domain [38] and patch similarity prior based on optical flow [23], but the optimization is usually computationally heavy and is infeasible for real-time processing. In DNN-based methods, while ConvLSTM [29] fused concatenated frames without alignment, DVSR [35] and CODD [20] estimated scene flow for multi-frame alignment, based on which the features of corresponding pixels were fused. MTDNet [9] leveraged both intraand interframe correlations for multi-frame ToF denoising, guided by confidence map to prioritize regions with strong ToF noise. Nevertheless, since depth at corresponding pixels varies across frames [20], directly fusing cross-frame depth features for depth reconstruction results in loss of details as shown in Fig. 1(e). Moreover, existing temporal ToF denoising networks are purely data-driven and ignorant of ToF sensing mechanism, resulting in poor generalization to real data due to difficulty in acquiring ground truth. In contrast, we fuse graph structures across frames which are motion-invariant and exhibit temporal self-similarity, which resolves spatial ambiguity while promoting temporal consistency. Moreover, we incorporate the image smoothness prior based on the fused graph, together with the data fidelity term based on ToF depth noise distribution in the network design, enhancing generalization to real data. 2.3. Generalizable ToF Depth Denoising Although model-based methods [23, 38] without the notion of training are robust to unseen real noise, but the optimization is computationally costly. On the other hand, DNNbased schemes achieve SOTA performance on synthetic data but are limited in generalization to real noise. UDA [1] adopted domain adaptation to enhance network generalization ability but failed at high noise levels. GLRUN [17] utilized algorithm unrolling of graph Laplacian regularization [27, 40], resulting in robust and efficient network. Nevertheless, these schemes focus on single-frame processing where temporal correlations are not utilized, while we develop an interpretable network based on temporal selfsimilarity of graph structures inherent in ToF data, enhancing accuracy and robustness to real unseen noise. 3. ToF Imaging Mechanism Overview To measure the depth xd of an object, the laser of the ToF sensor emits periodic signal se(t), which is typically modulated by sinusoidal function with frequency fm. The reflected signal sr(t), captured by the sensor, exhibits phase shift ϕ relative to se(t) after the signal travels distance of 2xd [39]. ϕ is then measured by computing the correlation between sr(t) and phase shifted version of se(t) with phase offset θ, resulting in raw measurements:"
        },
        {
            "title": "1\nT",
            "content": "(cid:90) 2 2 sr(t)se(t + θ 2πfm )dt cθ = lim α 2 = cos(ϕ + θ) + β, (1) (2) where is the exposure time, α is the signal amplitude, β is the ambient light intensity. By measuring cθ for multiple phase offsets θ, the raw ToF pair, i.e., in-phase xi and quadrature xq components of ϕ, are computed as [33], xi = (cid:88) θ cos(θ)cθ, xq = (cid:88) θ sin(θ)cθ, (3) so that ϕ is given as ϕ = arctan(xq/xi). Then depth xd and amplitude xa are reconstructed from xi and xq as xd = cϕ 4πfm = arctan(xq/xi) 4πfm , xa = (cid:113) x2 + x2 q, (4) where is the light speed. 4. Problem Formulation i, xt i, yt Given continuous frames of noisy ToF raw data yt RN in vectorized form, where [1, ] is the frame index, is total number of pixels, we aim to recover frames of RN which is then converted to depth map clean xt RN using (4). In this section, we first define intraxt frame graph modeling for each frame of ToF raw data in Sec. 4.1, then propose the cross-frame graph fusion strategy to exploit graph correlation in the reference frame for current frame denoising in Sec. 4.2. Finally, leveraging the fused graph to impose image smoothness prior and incorporating the data fidelity term based on the ToF depth noise distribution, we formulate the MAP optimization problem for denoising ToF raw data in Sec. 4.3. The solution to this problem further guides the subsequent network design. 4.1. Intra-frame Graph Modeling i, xt , Gt Since the raw data lie on image grids which naturally defines sparse graph structures [16], we construct 8-connected for xt undirected graphs Gt in each frame, where each pixel is connected to its 8 neighbors as shown in Fig. 2 for frame t. The corresponding non-negative symmetric RN represent the graph adjacency matrices Wt pair-wise correlation between connected pixels. For example, the (m, n)-th element of Wt i(m, n) 0, indicates the similarity between pixel and in xt i. i, i.e., wt i, Wt"
        },
        {
            "title": "We refer to Gt",
            "content": "q as intra-frame graphs which are constructed independently from other frames. Nevertheless, the noise corruption in captured ToF raw data may lead to suboptimal graph construction, which motivates us to exploit graph structures in neighboring frames as auxiliary features to refine graph construction in the current frame. , Gt 4.2. Cross-frame Graph Fusion i, Wt , Wt1 Due to camera motion and object movement/deformation in the dynamic scene, the graph correlations Wt1 in the reference frame need to be mapped to the corresponding pixel pairs before fused with Wt in the current frame. To do so, we implement the cross-frame graph mapping as the composition of intra-frame graph in frame 1 and inter-frame graph between frame and t1, resulting in the mapped graph of frame 1. Similar to [9], we take only the previous frame instead of all the frames for reference so that multi-frame information is propagated in forwardIn the following, we illustrate the mapped only manner. inal intra-frame graph Wt, resulting in the fused graph (cid:102)Wt: (cid:102)Wt(xt, xt1) = Φt,t1 ˆWt1 + Wt, (6) where (cid:102)Wt depends on frames xt and xt1, and the nonnegative diagonal matrix Φt,t1 RN represents the mapping confidence, so as to avoid the effect of inaccurate graph mapping, e.g., in case of occlusion where the mapping is invalid. Note that the graph edge weights in (cid:102)Wt are end-to-end trained as described in Sec. 5. 4.3. MAP Formulation via Graph Fusion i, xt i, yt with the captured noisy yt To denoise xt q, we formulate MAP problem using ToF depth noise distribution to compute likelihood term and image smoothness on fused graph for prior term. Depth Noise Distribution Induced Likelihood First, we compute the distribution of depth noise nt resulting from noise in yt q. As commonly assumed, xi and xq are corrupted by additive white Gaussian noise (AWGN) [12, 13], and the pixels in yt are independent and identically distributed with multivariate Gaussian distribution. Based on the depth noise distribution derived in [13, 17], we derive the log of likelihood of nt given as function of xt i, xt q: i, yt i, yt ln (nt d) 1 2σ2 (Xt a)1(xt yt xt yt q)2 2, (7) = diag(xt where Xt a) is the amplitude, is Hadamard product. Detailed proof of (7) is provided in Sec. 8 in the supplementary material. Graph Smoothness Prior Due to the ill-posedness of the problem, extra prior knowledge describing the characteristics of xt is required to facilitate the reconstruction. Here, we adopt the widely used graph Laplacian regularization (GLR) prior [26] to impose image smoothness on the cross-frame fused graph given as: i, xt (xt i, xt q) = exp( (xt i) (cid:101)Lt )xt ) i, xt1 i(xt σ2 q) (cid:101)Lt (xt q(xt σ2 exp( q, xt1 )xt ), (8) Figure 2. Illustration of cross-frame graph fusion, where the intraframe graph in reference frame 1 is mapped to current frame via inter-frame graph with 2-hop or 3-hop paths connecting pixel pairs in frame t. The graph weights are learned in graph-informed geometric attention (GIGA) mechanism, which updates the adjacency matrix in current frame using that in reference frame. graph construction ˆWt1 for xt and hereinafter eliminate the notations and since the same procedure applies to the two components. Inter-frame Graph For each edge (m, n) in frame t, the graph mapping aims to utilize Wt1 for recomputing the edge weight ˆwt(m, n), which is given as the sum of weights of all the possible paths between pixel and in the mapped graph. We first construct inter-frame graph Wt,t1 where each pixel in frame is connected to pixels frame 1 within the neighborhood t1 as spatial neighborhood centered at the same coordinate m, which is highlighted in green in frame 1 in Fig. 2. Mapped Graph We construct the mapped graph as the composition of Wt,t1 and Wt1. To connect and n, there are two types of paths, one is the 2-hop path marked with green dotted lines in Fig. 2, where and t1 are connected via the same pixel t1 , and the corresponding graph weights are computed as Wt,t1(Wt,t1). The other type is the 3-hop path marked with blue dotted lines in Fig. 2, where and are connected via the connected pixel pair (k, l) where k, t1 , and the corresponding graph weights are computed as Wt,t1Wt1(Wt,t1). . We set t1 t1 n In sum, the mapped graph weights are given as: ˆWt1 = Wt,t1(Wt,t1) + Wt,t1Wt1(Wt,t1) = Wt,t1(Wt1 + I)(Wt,t1), (5) where RN is the identity matrix. Note that Wt,t1 is shared for components and q. Cross-frame Fused Graph Then the cross-frame graph fusion is weighted average of mapped graph ˆWt1 and origwhere σL adjusts the sensitivity to variations on graphs, and the fused graph Laplacian matrix (cid:101)Lt ) is given as: i, xt1 i(xt (cid:101)Lt (cid:101)Dt i(xt i(xt i, xt1 i, xt1 i, xt1 ) = (cid:101)Dt i(xt i(xt ) = diag( (cid:102)Wt ) (cid:102)Wt i, xt1 i(xt )1), i, xt1 ), (9) (10) where 1 RN is an all-one vector. (cid:101)Lt puted with the same procedure. q(xt q, xt1 ) is comFigure 3. Framework of GIGA-ToF network which is composed of (1) the feature extraction network in blue to extract geometric features from ToF raw data and estimate initial prior weights and intra-graph adjacency matrices, (2) Graph-Induced Geometric Attention (GIGA) module in yellow to learn graph edges from the geometric features informed by graph structures as shown at the right side, and (3) Unrolled GLR module in green to denoise ToF data. Output dimensions are shown on top of each layer. MAP Formulation The MAP problem is formulated based on (7) and (8) and is given as: a)1(xt yt xt yt q)2 min xt i,xt + 1 2σ2 (Xt i(xt σ2 (xt i) (cid:101)Lt i, xt1 )xt + (xt q) (cid:101)Lt q, xt1 q(xt σ2 )xt , (11) ((Xt,r1 )1xt,r1 )2 (xt xt,r1 ) i, xt1 i(xt + 2λ(cid:101)Lt )xt = 0. (13) 5.1. Algorithm Unrolling and Graph Learning By differentiating (12) with respect to xt and setting the result equal to 0, we get the solution by solving the following linear system, so that ToF raw data in frame is denoised towards temporal consistency between frame and 1 by utilizing crossframe graph fusion. (11) is then approximately solved with alternating optimization. In each iteration, we fix xt and optimize xt q, and repeat until convergence. For example, in iteration r, we set xt = xt,r1 , then fix xt and optimize xt and optimize xt i, then fix xt = xt,r1 = yt as i(xt i, xt1 Unrolled GLR For accurate estimation of parameters λ and (cid:101)Lt ), we follow [17] and unroll the solution of (13) into iterative filtering based on gradient descent, so that the parameters are fully trainable with DNN. Specifically, starting with xt,r,0 , the solution is given by running the following solution procedure, = xt,r1 xt,r,p+1 = xt,r1 + Λt,r1 + Λt,r1 = 2λ (cid:0)diag(xt,r1 (cid:102)Wt (cid:101)Dt )1Xt,r1 i(xt, xt1)xt,r,p i(xt, xt1) (cid:1)2 , i , (14) (15) min xt (Xt,r1 )1xt,r1 (xt xt,r1 )2 2 Λt,r1 + 2λ(xt i) (cid:101)Lt i(xt i, xt1 )xt i, (12) where λ = (σ/σL)2. xt,0 i, yt , xt,0 in the first iteration. The remaining questions are 1) how to efficiently solve (12) and 2) how to learn fused graph from data, which are addressed as follows. are initialized with yt 5. Network Architecture The graph-based solution to (12) is unrolled into iterative convolutional filtering with kernels learned from graphinformed geometric attention in Sec. 5.1, which induces the interpretable network design in Sec. 5.2, enhancing network robustness to cross-dataset generalization. i i(xt, xt1), followed by fusion with input xt,r1 where Λt,r1 is diagonal matrix and can be considered as the pixel-wise weighting factor for the GLR prior. In (14), the (p + 1)-th iteration output is computed via convolutional transform of p-th iteration result xt,r,p with kernel (cid:102)Wt with weight Λt,r1 . By recurrently repeating the above procedure, we obtain the solution to (14), which is summarized in Algorithm 1 in Sec. 10 in the supplementary material. Graph-Informed Geometric Attention Next, we discuss graph learning to compute edge weights in (cid:102)Wt q. In the following, we illustrate the estimation of (cid:102)Wt and hereinafter eliminate the notations and since the same procedure applies to the two components. and (cid:102)Wt for xt First, for intra-frame graph learning, we use the geometric features from ToF raw data in frames and 1, i.e., Ft and Ft1 to estimate Wt and Wt1 with single convolution layer. Then, to compute inter-frame graph Wt,t1, we adopt variant of the basic self-attention operation for graph computation following [8], where attention weight is computed as, aij = softmax(eij), eij = (QFt(i))(KFt1(j)), (16) where Q, RCC are the query and key matrices, respectively, and is the feature dimension. Then the mapped and fused graphs are computed using (5) and (6). The above procedure for graph learning is named Graph-Induced Geometric Attention (GIGA) module to learn graph edges from the geometric features informed by graph structures as illustrated at the right side of Fig. 3. 5.2. GIGA-ToF Network Architecture Leveraging the GIGA module in Sec. 5.1, we propose the GIGA-ToF network for ToF depth denoising which is composed of three parts as shown in Fig. 3. The first part is the feature extraction network that adopts an encoder-decoder structure with skip-connections [31] to estimate multi-scale features of scales {1/8, 1/4, 1/2}, where the feature dimensions are shown in Fig. 3. Ft at scale 1/2 is used to estimate initial prior weights Λt,0 via 1-layer convolution. We apply sigmoid function on Λt,0 , Λt,0 to get positive weights, then scale by 10 to ensure sufficient denoising strength. All the convolutional layers adopt 3 3 kernel size with LeakyReLU activation. , Λt, The second part is the GIGA module. The features Ft1, Ft at scale 1/8 are fed into the GIGA module to compute Wt1, Wt,t1 and Φt,t1 for computational efficiency, which generates ˆWt1 at 1/8 scale. The neighborhood size for inter-frame graph is set as = 7. For detail refinement, Ft at scale 1/2 is used for computing Wt, which is fused with bilinear upsampled ˆWt1. The third part is the unrolled GLR module adopted from [17]. The final output xt, and xt, are converted to depth xt, via the raw2d module based on (4). In the case of multifrequency inputs, raw data of different fm are denoised separately with shared network parameters. Depth maps with different fm are merged via phase unwrapping [34] to generate the final depth. Graph Spectral Filtering Interpretability Since the graph Laplacian matrix in (12) is symmetric and positive semidefinite (PSD) with positive edge weight, its solution is low-pass graph spectral filtering. Therefore, together with the graph spectral filtering interpretability and the incorporation of ToF imaging mechanism in the network design, the proposed GIGA-ToF is fully interpretable which effectively enhances its robustness to cross-dataset generalization as validated in Sec. 6.3. 5.3. Loss Function We train our network with l1 loss function supervised by the ground truth xt,gt as follows: , xt,gt = 1 (cid:88) (cid:88) vV θ{i,q} (cid:12) (cid:12)xt, (cid:12) θ (v) xt,gt θ (v) (cid:12) (cid:12) (cid:12) , (17) where v, and denote the pixel index, set of valid pixels in GT, and the number of valid pixels, respectively. 6. Experimental Results We first generate syntheic DVToF data with temporal ToF data and depth, which is used for training. Then we evaluate the network performance with DVToF testing data, and further show generalization to real Kinectv2 depth images. 6.1. Experimental Settings Datasets We adopted the dataset generation protocol in [34] while the camera paths were randomly generated to augment the cross-frame flows. We have 5 static scenes, each with 10 paths of 250-frame length, generating 12.5k measurements of raw ToF correlation-depth pairs in total with resolution 320 240. The resulting dataset is named DVToF which stands for depth video of ToF data. In addition, we generated random noise using Kinectv2 noise statistics provided in [14]. We used 9375 pairs for training and 3125 pairs with unseen scene-path configurations for testing. More importantly, to evaluate with real data, we captured real ToF data with Kinectv2 camera and applied the pre-trained model on DVToF dataset to evaluate the cross-dataset generalization ability. Training details We used Adam optimizer with initial learning rate 1e3 and decay at epoch [15, 30, 45] with decay rate 0.7. The model was trained from scratch for 60 epochs. We employed the PyTorch framework [28] on single GeForce RTX 3090 GPU. We set = 3 and = 2 for the Unrolled GLR module. Metrics Following [35], we used per-frame mean absolute error (MAE), Absolute Relative Error (AbsRel), and δ1 accuracy to evaluate per-frame depth estimation accuracy; and temporal end-point error (TEPE) to measure temporal consistency. For complexity comparison, we tested the average runtime and GPU memory cost with DVToF dataset using single 3090 GPU and Intel i9-14900K CPU. We did not report memory costs for methods running only on CPU. 6.2. Comparison with Existing Schemes We compared with the following competing schemes. Model-based methods: single-frame libfreenect2 [36] and multi-frame weighted mode filter (WMF) [23]; Table 1. Comparison of denoising accuracy on synthetic DVToF testing dataset and augmented dataset"
        },
        {
            "title": "DVToF Dataset with augmented noise",
            "content": "(s) (MB) MAE(m) AbsRel δ1 TEPE(m) MAE(m) AbsRel δ1 TEPE(m) Single-frame libfreenect2 [36] DeepToF [21] ToFNet [34] UDA [2] RADU [33] GLRUN [17] Multi-frame WMF [23] ConvLSTM [29] DVSR [35] MTDNet [9] GIGA-ToF (Ours) 0.003 0.006 0.008 0.006 83.7 0.016 24.3 0.019 0.632 0.584 0.027 - 738 1468 900 11115 - 1362 1308 317 824 0.1044 0.2172 0.1290 0.0564 0.1350 0.0357 0.0311 0.1314 0.0718 0.0566 0.0193 0.0283 0.1071 0.0652 0.0152 0.0697 0.0107 0.0116 0.0337 0.0844 0.0642 0.0060 0.9746 0.8951 0.9586 0.9880 0.9497 0. 0.9955 0.9624 0.9777 0.9816 0.9974 0.1023 0.2003 0.1221 0.0884 0.1290 0.0734 0.0751 0.1143 0.1176 0.1046 0.0637 0.1230 0.2830 0.1334 0.1153 0.1264 0.0550 0.0495 0.1257 0.0791 0.0625 0.0487 0.0386 0.1409 0.0677 0.0570 0.0610 0. 0.0209 0.0406 0.0425 0.0316 0.0205 0.9645 0.8534 0.9564 0.9451 0.9623 0.9896 0.9898 0.9736 0.9736 0.9778 0.9903 0.1234 0.2705 0.1275 0.1274 0.1202 0.1221 0.0950 0.1411 0.1271 0.1129 0.1102 Figure 4. Depth results and error maps of ToF depth denoised on DvToF dataset: (a) GT, results of (b) WMF [23], (c) DVSR [35], (d) MTDNet [9] and (e) proposed GIGA-ToF. Corresponding error maps are in the second row. Single-frame based DNNs: DeepToF [21], ToFNet [34], UDA [1], RADU [33], GLRUN [17]; Multi-frame DNNs: ConvLSTM [29], DVSR [35], MTDNet [9]. To ensure fair comparison, all competing methods were retrained and tested on the DVToF dataset. In addition, following [3], we augmented the DVToF dataset with simulated edge noise. Note that the same model was used for testing in both noise settings to test generalization ability to unseen noise. As shown in Table. 1, GIGA-ToF achieves the best accuracy performance in both noise settings, outperforming other methods by at least 37.9% in MAE and 13.2% in TEPE in normal noise setting. Also, the complexity of GIGA-ToF is moderate among SOTA methods, while the competing WMF is computationally costly and hinders its application in real-time usage. For visual evaluation, we present qualitative comparison of multi-frame methods in Fig. 4. GIGA-ToF generates smooth results while preserving fine details, as highlighted in the zoomed-in region, further confirming GIGAToFs ability to maintain spatial sharpness while effectively removing noise due to motion-invariant graph structure fusion. Note that WMF generates better TEPE in the barron noise setting, showing competing temporal consistency, but suffers from quantization error as shown in Fig. 4(b). In addition, DNN-based DVSR and MTDNet show blurry details due to the fusion of temporally varying depth features. To visualize temporal consistency, we plot the x-t slices of the estimated depth images of multi-frame methods in Fig. 6. While MTDNet and WMF remain noisy with noticeable temporal jittering, GIGA-ToF exhibits clean x-t slices, demonstrating its high temporal consistency. Please refer to the supplementary video for better temporal visualizations. 6.3. Generalization to Real Data To assess generalization ability of GIGA-ToF on realworld data, we capture ToF data with Kinect v2 camera [19] and conduct qualitative comparison shown in Fig. 7. While DNN-based MTDNet fails to generalize to real data, model-based WMF shows stable but blurry results. While GLRUN shows robustness to real data, multi-frame processing GIGA-ToF further enhances the detail preservation, which validates the necessity of utilizing temporal correlation. In sum, GIGA-ToF, despite being trained on synthetic Figure 5. Comparison of denoising results under different GIGA-ToF variants. (a) GT, results (b) without and (c) with unrolled GLR in single-frame processing, feature fusion (d) without and (e) with attention; graph structure fusion (f) without and (g) with attention. Table 2. Comparison of quantitative evaluation on DvToF testing dataset with GIGA-ToF variants Modules Fusion - - Feature Feature Graph Graph GLR - Unroll Unroll Unroll Unroll Unroll Attn - - - - MAE (m) 0.0409 0.0357 0.0238 0.0214 0.0219 0.0193 AbsRel 0.0174 0.0107 0.0078 0.0069 0.0078 0. δ1 0.9909 0.9929 0.9965 0.9969 0.9970 0.9974 TEPE (m) 0.0793 0.0734 0.0718 0.0713 0.0702 0.0637 results become much more blurry, validating the effect of graph structure in detail preservation. Fusion Mechanism For multi-frame processing setting, we investigate the two fusion mechanisms, i.e., depth features and graph structures. For depth feature fusion, the current and reference frame features at scale 1/8 are fused in the feature extraction network, where the graph construction is based on the fused features. Graph-based fusion outperforms those based on depth feature fusion and exhibits sharper details, validating the effect of motion-invariant graph fusion in resolving spatial ambiguity. Inter-frame Attention In addition, we investigate the effect of inter-frame attention in fusing cross-frame features. Without using attention for fusion, the features in reference are fused into current frame indifferently, resulting in noticeable noise in the results due to inaccurate fusion correspondence between frames. This validates the effect of attention in mapping geometric features with accurate correspondence. 6.5. Limitation and Future Work In the current setting, we only consider the previous frame for reference, while the features in more previous frames are not fully utilized. Although involving two frames for multi-frame processing already boosts the depth accuracy and produces temporally consistent results, extending to more frames has not yet been explored. Therefore, for future study, the investigation will be devoted to more general processing pipeline for varying input sequence length with recurrent network design. 7. Conclusion In this paper, we propose GIGA-ToF network for ToF depth denoising, simultaneously enhancing temporal consistency Figure 6. x-t slices (along red line in Fig. 4(a)) for temporal stability visualization, where GIGA-ToF exhibits clear details and less noise than competing multi-frame schemes. Figure 7. Visual results of ToF depth denoising on real data captured by Kinect v2 sensor: (a) RGB and (b) noisy depth captured by Kinectv2 camera, and results of (c) GLRUN, (d) WMF, (e) MTDNet and (f) GIGA-ToF, where GIGA-ToF shows robustness to real noise and recovers accurate details. data, shows strong generalization to real-world data. 6.4. Ablation Study To investigate the effectiveness of each component in GIGA-ToF, we test on DVToF dataset with different variants of GIGA-ToF. Quantitative results in Table 2 and qualitative results in Fig. 5 validate the effectiveness of each component for denoising accuracy and stability. Unrolled GLR In single-frame processing settings, we compare variants with and without Unrolled GLR module. First, single-frame variants are much more noisy than multiframe variants, validating the necessity of temporal processing. In addition, by removing Unrolled GLR module, the and spatial sharpness utilizing the motion-invariant graph structures. Based on the cross-frame graph fusion, we impose image smoothness as prior in the MAP formulation, which is efficiently optimized via algorithm unrolling to produce high-performance yet interpretable network designs. The resulting network shows enhanced denoising accuracy on synthetic DVToF dataset and higher robustness to real noise over competing schemes due to the graph spectral filter interpretation."
        },
        {
            "title": "References",
            "content": "[1] Gianluca Agresti, Henrik Schaefer, Piergiorgio Sartor, and Pietro Zanuttigh. Unsupervised domain adaptation for tof data denoising with adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55845593, 2019. 2, 3, 7 [2] Gianluca Agresti, Henrik Schafer, Piergiorgio Sartor, Yalcin Incesu, and Pietro Zanuttigh. Unsupervised domain adaptation of deep networks for tof depth refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44 (12):91959208, 2022. 2, 7 [3] Jonathan Barron and Ben Poole. The fast bilateral solver. In European conference on computer vision, pages 617632. Springer, 2016. 7 [4] Ayush Bhandari and Ramesh Raskar. Signal processing for time-of-flight imaging sensors: An introduction to inverse problems in computational 3-d imaging. IEEE Signal Processing Magazine, 33(5):4558, 2016. 1 [5] Faquan Chen, Rendong Ying, Jianwei Xue, Fei Wen, and Peilin Liu. configurable and real-time multi-frequency 3d image signal processor for indirect time-of-flight sensors. IEEE Sensors Journal, 22(8):78347845, 2022. 1 [6] Yan Chen, Jimmy Ren, Xuanye Cheng, Keyuan Qian, Luyang Wang, and Jinwei Gu. Very power efficient neuIn Proceedings of the IEEE/CVF Winral time-of-flight. ter Conference on Applications of Computer Vision, pages 22572266, 2020. 1, [7] Gene Cheung, Enrico Magli, Yuichi Tanaka, and Michael Ng. Graph spectral image processing. Proceedings of the IEEE, 106(5):907930, 2018. 2 [8] Tam Thuc Do, Parham Eftekhar, Seyed Alireza Hosseini, Gene Cheung, and Philip Chou. Interpretable lightweight transformer via unrolling of learned graph smoothness priors. Advances in Neural Information Processing Systems, 37:63936416, 2025. 6 [9] Guanting Dong, Yueyi Zhang, Xiaoyan Sun, and Zhiwei Xiong. Exploiting dual-correlation for multi-frame time-offlight denoising. In European Conference on Computer Vision, pages 473489. Springer, 2024. 1, 2, 3, 7 [10] Ruofei Du, Eric Turner, Maksym Dzitsiuk, Luca Prasso, Ivo Duarte, Jason Dourgarian, Joao Afonso, Jose Pascoal, Josh Gladstone, Nuno Cruces, et al. Depthlab: Real-time 3d interaction with depth maps for mobile augmented reality. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, pages 829843, 2020. 1 [11] Mario Frank, Matthias Plaue, and Fred Hamprecht. Denoising of continuous-wave time-of-flight depth images usOptical Engineering, 48(7): ing confidence measures. 077003077003, 2009. 1 [12] Mario Frank, Matthias Plaue, Holger Rapp, Ullrich Kothe, Bernd Jahne, and Fred Hamprecht. Theoretical and experimental error analysis of continuous-wave time-of-flight range cameras. Optical Engineering, 48(1):013602013602, 2009. 4, [13] Mihail Georgiev, Robert Bregovic, and Atanas Gotchev. Time-of-flight range measurement in low-sensing environment: Noise analysis and complex-domain non-local denoising. IEEE Transactions on Image Processing, 27(6):2911 2926, 2018. 2, 4, 1 [14] Qi Guo, Iuri Frosio, Orazio Gallo, Todd Zickler, and Jan Kautz. Tackling 3d tof artifacts through learning and the flat dataset. In Proceedings of the European Conference on Computer Vision (ECCV), pages 368383, 2018. 2, 6 [15] Felipe Gutierrez-Barragan, Huaijin Chen, Mohit Gupta, Andreas Velten, and Jinwei Gu. itof2dtof: robust and flexible representation for data-driven time-of-flight imaging. IEEE Transactions on Computational Imaging, 7:1205 1214, 2021. 1 [16] Wei Hu, Xin Li, Gene Cheung, and Oscar Au. Depth map denoising using graph-based transform and group sparsity. In 2013 IEEE 15th international workshop on multimedia signal processing (MMSP), pages 001006. IEEE, 2013. 1, 2, 3 [17] Jingwei Jia, Changyong He, Jianhui Wang, Gene Cheung, and Jin Zeng. Deep unrolled graph laplacian regularization for robust time-of-flight depth denoising. IEEE Signal Processing Letters, 32:821825, 2025. 1, 3, 4, 5, 6, 7 [18] Jiwoo Kang, Seongmin Lee, Mingyu Jang, and Sanghoon Lee. Gradient flow evolution for 3d fusion from single IEEE Transactions on Circuits and Systems depth sensor. for Video Technology, 32(4):22112225, 2021. 1 [19] Gregorij Kurillo, Evan Hemingway, Mu-Lin Cheng, and Louis Cheng. Evaluating the accuracy of the Azure Kinect and Kinect v2. Sensors, 22(7):2469, 2022. [20] Zhaoshuo Li, Wei Ye, Dilin Wang, Francis Creighton, Russell Taylor, Ganesh Venkatesh, and Mathias Unberath. Temporally consistent online depth estimation in dynamic scenes. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 30183027, 2023. 1, 2 [21] Julio Marco, Quercus Hernandez, Adolfo Munoz, Yue Dong, Adrian Jarabo, Min Kim, Xin Tong, and Diego Gutierrez. Deeptof: off-the-shelf real-time correction of multipath interference in time-of-flight imaging. ACM Transactions on Graphics (ToG), 36(6):112, 2017. 2, 7 [22] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. Science Robotics, 7(62):eabk2822, 2022. 1 [23] Dongbo Min, Jiangbo Lu, and Minh Do. Depth video enhancement based on weighted mode filtering. IEEE Transactions on Image Processing, 21(3):11761190, 2011. 2, 3, 6, 7 and refinement for consumer depth cameras using cascaded cnns. In Proceedings of the European conference on computer vision (ECCV), pages 151167, 2018. 2 [38] Cheng Yang, Yu Mao, Gene Cheung, Vladimir Stankovic, and Kevin Chan. Graph-based depth video denoising and In 2014 IEEE 16th event detection for sleep monitoring. international workshop on multimedia signal processing (MMSP), pages 16. IEEE, 2014. 2, [39] Pietro Zanuttigh, Giulio Marin, Carlo Dal Mutto, Fabio Dominio, Ludovico Minto, Guido Maria Cortelazzo, et al. Time-of-flight and structured light depth cameras. Technology and Applications, 978(3), 2016. 3 [40] Jin Zeng, Jiahao Pang, Wenxiu Sun, and Gene Cheung. Deep graph laplacian regularization for robust denoising of real images. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition workshops, pages 00, 2019. 3 [41] Xue Zhang, Gene Cheung, Jiahao Pang, Yash Sanghvi, Abhiram Gnanasambandam, and Stanley Chan. Graph-based depth denoising & dequantization for point cloud enhanceIEEE Transactions on Image Processing, 31:6863 ment. 6878, 2022. 1, 2 [24] Vishal Monga, Yuelong Li, and Yonina Eldar. Algorithm Interpretable, efficient deep learning for signal IEEE Signal Processing Magazine, unrolling: and image processing. 38(2):1844, 2021. 2 [25] Antonio Ortega, Pascal Frossard, Jelena Kovavcevic, Jose MF Moura, and Pierre Vandergheynst. Graph signal processing: Overview, challenges, and applications. Proceedings of the IEEE, 106(5):808828, 2018. 2 [26] Jiahao Pang and Gene Cheung. Graph laplacian regularization for image denoising: Analysis in the continuous domain. IEEE Transactions on Image Processing, 26(4):17701785, 2017. 4 [27] Jiahao Pang and Jin Zeng. Graph spectral image restoration. Graph Spectral Image Processing, 133, 2021. 3 [28] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 6 [29] Vaishakh Patil, Wouter Van Gansbeke, Dengxin Dai, and Luc Van Gool. Dont forget the past: Recurrent depth estimation from monocular video. IEEE Robotics and Automation Letters, 5(4):68136820, 2020. 2, [30] Xin Qiao, Chenyang Ge, Pengchao Deng, Hao Wei, Matteo Poggi, and Stefano Mattoccia. Depth restoration in underdisplay time-of-flight imaging. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):56685683, 2022. 1 [31] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. UNet: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234241. Springer, 2015. 6 [32] Mattia Rossi, Mireille El Gheche, Andreas Kuhn, and Pascal Frossard. Joint graph-based depth refinement and normal estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12154 12163, 2020. 1, 2 [33] Michael Schelling, Pedro Hermosilla, and Timo Ropinski. Radu: Ray-aligned depth update convolutions for tof data denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 671680, 2022. 2, 3, 7 [34] Shuochen Su, Felix Heide, Gordon Wetzstein, and Wolfgang Heidrich. Deep end-to-end time-of-flight imaging. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 63836392, 2018. 1, 2, 6, 7 [35] Zhanghao Sun, Wei Ye, Jinhui Xiong, Gyeongmin Choe, Jialiang Wang, Shuochen Su, and Rakesh Ranjan. Consistent In Prodirect time-of-flight video depth super-resolution. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50755085, 2023. 1, 2, 6, 7, 3 [36] Lingzhu Xiang, Florian Echtler, Christian Kerl, Thiemo libfreenect2: ReWiedemeyer, Gordon, and Facioni. lease 0.2, 2016. 1, 2, 6, 7 [37] Shi Yan, Chenglei Wu, Lizhen Wang, Feng Xu, Liang An, Kaiwen Guo, and Yebin Liu. Ddrnet: Depth map denoising Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide the derivation of the data fidelity term in MAP formulation based on ToF depth noise distribution in Sec. 8. Then we evaluate the sensitivity to frame time step in Sec. 9. Next, we summarize unrolling of cross-frame graph fusion based ToF depth denoising algorithm in Sec. 10. More visualization results are provided in Sec. 11, with video demonstrating the estimation accuracy and temporal consistency. 8. Data Fidelity Term in MAP Problem In this section, we derive the data fidelity term based on ToF depth noise distribution. As assumed in Sec. 4.3, xi and xq are corrupted by additive white Gaussian noise (AWGN) [12, 13], and the pixels in yt i, yt are independent and identically distributed with multivariate Gaussian distribution. The joint probability density function of yt is given as: i, yt (yt i, yt qxt i, xt q) = 1 (2πσ2)N (cid:32) (nt exp i)nt + (nt 2σ2 q)nt (cid:33) , (18) (19) xt = yt xt q, i, nt = yt nt where σ is the noise variance. Since the final target is to reconstruct depth, we further investigate depth noise distribution based on (18). Based on (4) and (18), the distribution of depth noise nt is derived in [12, 13] as, (nt (cid:89) [ 1 + erf( cos(4πfmnt d(m)/c) γt(m) 2 ) ] d) = cos(4πfmnt d(m)/c) 2γt(m) 2π sin2(4πfmnt 2γt(m)2 m=1 exp( ) + 1 2π exp( 1 2γt(m)2 ), (20) d(m)/c) a(m), yt where γt(m) = σ/yt a(m) is noisy amplitude, erf is the Gaussian error function. Under normal noise level, i.e., γt(m) 1, we have erf output equal to 1 and last term equal to 0 in (20), then (20) is approximated with (nt d) (cid:89) ( m=1 cos(4πfmnt d(m)/c) γt(m) 2π exp( sin2(4πfmnt d(m)/c) 2(γt(m))2 )). (21) i, xt q: 1 2σ2 (Xt Based on (21), the log of likelihood (yt dxt d) is ln (yt dxt d) (cid:88) m=1 (ln(cos(4πfmnt d(m)/c)) sin2(4πfmnt d(m)/c)/(2γt(m)2)), (22) where the irrelevant term ln(γt(m) 2π) is removed. Both terms in (22) minimize nd, and with γ 1, the second term dominates. Thus, we remove the first term and compute the likelihood as function of xt as follows: i, xt ln (yt dxt d) (cid:88) m=1 sin2(cid:0)ϕ(m) ϕ(m)(cid:1) 2γt(m)2 = (cid:88) m=1 (cid:0)sin ϕ(m) cos ϕ(m) cos ϕ(m) sin ϕ(m)(cid:1)2 2γt(m)2 , (23) where ϕ = 4πfmyt Based on (23) and (4), the log of likelihood of nt d/c is the noisy phase. is given as function of xt ln (nt d) a)1(xt yt xt yt q) 2, (24) where Xt product. = diag(xt a) is the amplitude, is Hadamard 9. Analysis of Frame Time Step Following [9], to investigate the effect of time step between reference and current frames, we test GIGA-ToF on DVToF dataset with different time steps. For small time steps = 1, 2, the performances are similar. When time steps become larger, = 4, 8 reduces due to the limited similarity of graph structures in the neighboring pixels in the reference frame. Nevertheless, the performance still surpasses that of single-frame processing, validating the necessity of multiframe processing and the temporal self-similarity of graph structures despite large frame gaps. Table 3. Comparison of quantitative evaluation on DvToF testing dataset with different frame rate Time step MAE(m) AbsRel 0.0060 0.0190 0.0060 0.0192 0.0062 0.0194 0.0071 0.0210 1 2 4 8 δ1 0.9974 0.9973 0.9972 0.9970 TEPE(m) 0.0634 0.0608 0.0647 0.0725 10. Algorithm Summary Based on the algorithm unrolling of graph Laplacian regularization, we obtain the solution to (14), which is summarized in Algorithm 1. Algorithm 1 Unrolling of Cross-frame Graph Fusion based ToF Depth Denoising Algorithm Require: Noisy ToF raw data yt i, yt q, intra-frame graph adjaq, Wt1 cency matrices Wt , Wt1 , inter-frame graph adjacency matrix Wt,t1 and fusion weight Φt,t1, GLR prior weight Λt q, iteration number R, i, Wt i, Λt Ensure: Denoised output xt 1: Map reference frame graphs Wt1 to obtain mapped graphs ˆWt i, xt , Wt1 using (5) , ˆWt1 2: Fuse the mapped graphs with current frame graphs to obtain to current frame fused graphs (cid:102)Wt i, (cid:102)Wt 3: Obtain corresponding (cid:101)Dt 4: Initialize xt,0 i, xt,0 = yt 5: for = 0 : 1 do 6: 7: 8: 9: Update Λt,r1 for = 0 : 1 do Transform xt,r,p Fuse with xt,r1 (14) to update xt,r,p+ using (6) i, (cid:101)Dt from (cid:102)Wt = yt i, (cid:102)Wt using (10) with Xt,r1 , fix xt,r and optimize xt,r with convolutional kernel (cid:102)Wt with weight Λt,r1 as specified in Figure 8. Visual results of ToF depth denoising on real data captured by Kinect v2 sensor: (a) RGB and (b) noisy depth captured by Kinectv2 camera, and results of (c) GLRUN, (d) WMF, (e) MTDNet and (f) GIGA-ToF, where GIGA-ToF shows accurate and smooth estimation. end for Fix xt,r 10: 11: 12: end for 13: Output and repeat steps 7-10 to optimize xt,r Figure 9. Visual results of ToF depth denoising on real data captured by Kinect v2 sensor: (a) RGB and (b) noisy depth captured by Kinectv2 camera, and results of (c) GLRUN, (d) WMF, (e) MTDNet and (f) GIGA-ToF, where GIGA-ToF shows robustness to real noise and recovers accurate details. 11. More Visualization We provide more results for the qualitative comparison of ToF depth denoising methods. In particular, we demonstrate results on synthetic DVToF dataset in Fig. 11 and Fig. 12, and DVToF dataset with noise augmentation in Fig. 13. To further demonstrate the generalization ability to real Kinectv2 data, we shown results in Figs. 8, 9, 10. Note that we directly apply the model trained on original DVToF dataset to the noise-augmented DVToF dataset and Kinectv2 dataset without fine-tuning, which validates its generalization ability. Please kindly refer to the supplementary video for better temporal visualizations. Figure 10. Visual results of ToF depth denoising on real data captured by Kinect v2 sensor: (a) RGB and (b) noisy depth captured by Kinectv2 camera, and results of (c) GLRUN, (d) WMF, (e) MTDNet and (f) GIGA-ToF, where GIGA-ToF exhibits better spatial sharpness than other competing schemes. Figure 11. Depth results and error maps of ToF depth denoised on DvToF dataset: (a) GT, results of (b) WMF [23], (c) DVSR [35], (d) MTDNet [9] and (e) proposed GIGA-ToF. Corresponding error maps are in the second row. GIGA-ToF shows more accurate depth estimation, maintaining global smoothness with edge preservation, e.g., the teapot handle highlighted in the zoom-in block. Figure 12. Depth results and error maps of ToF depth denoised on DvToF dataset: (a) GT, results of (b) WMF [23], (c) DVSR [35], (d) MTDNet [9] and (e) proposed GIGA-ToF. Corresponding error maps are in the second row. While MTDNet shows competing results, the details are blurred as highlighted in the zoom-in block, while GIGA-ToF generates sharp edges due to utilization of motion-invariant graph structure fusion. Figure 13. Depth results and error maps of ToF depth denoised on DvToF dataset with augmented edge noise: (a) GT, results of (b) WMF [23], (c) DVSR [35], (d) MTDNet [9] and (e) proposed GIGA-ToF. Corresponding error maps are in the second row. GIGA-ToF shows strong generalization to unseen edge noise and generates accurate details, e.g., in the bookshelf in the zoom-in block."
        }
    ],
    "affiliations": [
        "Google",
        "School of Computer Science and Technology, Tongji University"
    ]
}