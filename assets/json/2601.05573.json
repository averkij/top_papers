{
    "paper_title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
    "authors": [
        "Zehan Wang",
        "Ziang Zhang",
        "Jiayang Xu",
        "Jialei Wang",
        "Tianyu Pang",
        "Chao Du",
        "HengShuang Zhao",
        "Zhou Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 3 7 5 5 0 . 1 0 6 2 : r Orient Anything V2: Unifying Orientation and Rotation Understanding Zehan Wang1,2, Ziang Zhang1, Jiayang Xu1, Jialei Wang1, Tianyu Pang3, Chao Du3, Hengshuang Zhao4, Zhou Zhao1,2 1Zhejiang University; 2Shanghai AI Lab; 3Sea AI Lab; 4The University of Hong Kong https://orient-anythingv2.github.io/ Fine Labels Strong Robustness Rotational Symmetry Absoluate Orientation Relative Rotation Joint Encoder Encoder share Encoder Pseudo ensemble 3D Assets Diverse Pose Realistic Texture Generative model Class Names Full Coverage Perfect Balance Data: Scalable Data Engine Model: Orient Anything V2 New Features: Symmetry & Rotation Figure 1: Overview of Orient Anything V2. We upgrade the foundation orientation estimation model from both Data and Model perspectives. It uniﬁes the understanding of object orientation and rotation, achieving better estimation accuracy and gaining the New Features to handle rotational symmetry and relative rotation. Zoom in for the best view."
        },
        {
            "title": "Abstract",
            "content": "This work presents Orient Anything V2, an enhanced foundation model for uniﬁed understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which deﬁnes orientation via single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efﬁcient, model-in-the-loop annotation system that robustly identiﬁes 0 to valid front faces for each object; 3) symmetry-aware, periodic distribution ﬁtting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, signiﬁcantly broadening the applicability of orientation estimation in diverse downstream tasks. Equal Contribution. Project Leader. Corresponding Author. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Estimating object orientation from images is fundamental task of computer vision. 3D object orientation information plays crucial roles in robot manipulation [49, 37, 22], autonomous driving [25], AR/VR [11, 30, 4, 57], and spatial-aware image understanding [21, 28, 60] and generation [56, 50, 35]. Orient Anything V1 [48] is foundation model for estimating the object orientation aligned with an objects unique front face. While it exhibits strong robustness and accuracy in absolute orientation estimation, it lacks an understanding of rotation (despite its intrinsic link to orientation). This deﬁciency results in difﬁculties handling numerous rotationally symmetric objects (simply classifying them as having no front face) and understanding object rotation relative to speciﬁed reference frame. These limitations around rotation understanding restrict its utility in many downstream tasks. In this work, we aim to develop an enhanced orientation estimation model, Orient Anything V2, with stronger generalization and deeper understanding of both object orientation and rotation. Our contributions include scalable data engine and more elegant model framework. From the data perspective, Orient Anything V1 uses advanced VLM [12, 31] to annotate real 3D assets from Objaverse [8, 7]. Building on this data-driven motivation, we leverage advanced 3D generation models [51, 61] to further speed up data scaling-up and improve the data coverage and balance. Additionally, we assemble pseudo labels predicted by the V1 model across multi-view renderings and reﬁne them through model-in-the-loop calibration. The proposed data engine enables highly cost-effective and ﬂexible data scaling up, delivers robust annotation performance, and shows strong understanding of rotationally symmetric objects. Our ﬁnal dataset includes 600K assets, 12 larger than the existing orientation dataset, with signiﬁcantly higher annotation quality, accurately identifying 0 to valid front faces. From the model perspective, we ﬁrst propose symmetry-aware orientation distribution, explicitly teaching the model to capture and predict rotational symmetry. Moreover, our model supports multiframe input to directly predict relative rotations between frames. This design effectively bridges the knowledge transfer between absolute orientation and relative rotation, showing strong potential in reference-known scenarios. Our experiments demonstrate the enhanced and novel capabilities of our model. It achieves superior performance on zero-shot orientation estimation and sets new records on zero-shot rotation estimation (i.e., 6DoF pose estimation [49, 26]), while also accurately handling and predicting different rotational symmetries. To summarize, we propose Orient Anything V2, which improves Orient Anything V1 as follows: We propose data engine that cost-efﬁciently scales up 3D asset collection and robustly annotates the 0 to valid front faces to capture different object rotational symmetries. We introduce symmetry-aware distribution ﬁtting as learning objective, allowing the model to directly predict all plausible object orientations. We extend the model architecture to support multi-frame input, enabling it to directly estimate relative object rotations over the reference frame. Our model demonstrates strong zero-shot generalization across absolute orientation estimation, relative rotation estimation, and object symmetry recognition."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Object Rotational Symmetry Rotational symmetry [36, 34] indicates that an object may retain its original shape after being rotated by certain angles. This property is commonly found across various objects. Understanding an objects rotational symmetry is critical for 3D object recognition and generation [24, 59], pose estimation [17, 6], and robotic manipulation [39]. While some existing works [40] attempt to detect 3D rotational symmetry from single-view 2D images, they are constrained by limited training data and lack zero-shot generalization to open-world scenarios. 2 低质量纹理 低质量纹理 T-pose T-pose T-pose (a) Low-quality Texture (b) Limited Pose&Shape Figure 2: Real assets from Objavese suffer from (a) low-quality texture and (b) limited realism. Our focus is on object orientation relative to semantic \"front\" face. The number of possible valid front-facing orientations an object possesses is determined by its rotational symmetry around its vertical axis. For example, 180-degree symmetry means there are two distinct valid front faces. Objects with continuous rotational symmetry (symmetric at any angle), like balls, are considered to have no meaningful direction. In this work, we broaden the applicability of orientation estimation models by enabling the prediction of an objects azimuthal symmetry from single 2D image. Our model demonstrates impressive zero-shot rotational symmetry recognition performance. 2.2 Relative Rotation Estimation Predicting an objects rotation in the query frame relative to the reference frame is fundamental capability in 6DoF pose estimation [26, 14, 55] and is crucial for robotics applications. Early methods [23, 18, 9] focused on speciﬁc instances or object categories. More recent approaches like OnePose [44] and OnePose++ [15] estimate object rotation by solving 2D-3D correspondences across views. POPE [10] follows similar idea and achieves zero-shot rotation estimation with single reference frame with the help of SAM [19] and DINOv2 [33]. However, the reliance on pixel matching makes these methods prone to failure under large viewpoint changes. In contrast, we propose purely implicit learning approach. Leveraging the inherent coupling between rotation and orientation, we extend the orientation estimation model to support multi-frame inputs, enabling direct zero-shot relative rotation prediction between arbitrary views. 2.3 Single-view Orientation Estimation Estimating an objects 3D front-facing orientation (interpreted as its rotation relative to the canonical front view) from single view, requires the model to have an inherent understanding of different objects standard poses and front-facing appearances. Earlier works [53, 46, 42] are mainly limited to small number of categories or speciﬁc domains. More recently, ImageNet3D [29] introduce large-scale dataset with manually annotated 3D orientations. Orient Anything [48] achieves robust orientation estimation for any objects in any scenes by leveraging an advanced automated annotation pipeline, improved learning objectives, and real-world knowledge from the pre-trained vision model. In this work, we further address several limitations of Orient Anything and upgrade the orientation estimation model from both data-driven (novel and scalable data engine) and model-driven (direct symmetry and rotation prediction) perspectives, resulting in Orient Anything V2."
        },
        {
            "title": "3 Revisiting Orient Anything V1",
            "content": "Orient Anything V1 pioneers zero-shot object orientation estimation from single images. It introduces VLM-based pipeline to annotate front faces of Objaverse 3D assets [8, 7], learns orientation estimation from their renderings via distribution ﬁtting. It also provides conﬁdence score to indicate whether an object has unique front face. To further advance the orientation prediction foundation model, we ﬁrst dig into the potential limitations in its training data and framework. Disadvantages of Real 3D Assets 1) Imbalanced Category Distribution: Stemming from the human biases in asset creation, real 3D datasets, such as Objaverse [8, 7], suffer from signiﬁcant class imbalance. Common categories like buildings and characters make up large proportion, while 3 Class Tag LLM Rewrite Caption Image Generation Image 3D Mesh Generation 3D Mesh Cage Computer Heater \"matte plastic, ethereal cage\" \"muted tones, dreamlike computer\" \"rusted, cinematic heater\" Farmer Fence Fox Worker Fighter . . . .. .. \"cartoon, dramatic farmer\" \"granite, symmetrical, fence\" \"gold made, ambient glow fox\" \"dusty, realistic, sidelight, worker\" . . . \"polished wood, pastel fighter\" .. .. . . . .. .. . . . .. .. Figure 3: Overview of 3D Asset Synthesis Pipeline. We begin with class tags and use series of advanced generative models to progressively generate high-quality 3D assets. others, like uncommon animals, are severely underrepresented. 2) Inconsistent Data Quality: Current large-scale 3D datasets often lack high-quality assets with complete geometry and rich surface details. (Fig. 2 a) Moreover, many human-created meshes exhibit ﬁxed poses, leading to substantial domain gap from real-world object variations (Fig. 2 b). Limitations of Object Rotation Understanding 1) Ignored Rotational Symmetries: Orient Anything V1 deﬁnes orientation based on the single, unique front face, overlooking the different rotational symmetries (i.e., multiple valid \"front\" faces). For the many symmetric objects in real world, the model cannot effectively distinguish or identify their potential orientations. 2) Unsupported Relative Rotations: The relative rotation between two views and the front-facing orientation (essentially the rotation relative to the front view) are inherently coupled. However, estimating relative rotation through independent absolute orientation predictions suffers from signiﬁcant error accumulation, causing Orient Anything V1 to often fail in relative rotation estimation."
        },
        {
            "title": "4 Scalable Data Engine",
            "content": "4.1 3D Asset Synthesis Motivated by the recent remarkable progress in generative models and the successful application of synthetic data in downstream tasks [45, 56], we explore whether synthetic 3D assets can serve as scalable, high-quality data sources for orientation learning. To fully harness modern generative models, we construct our asset synthesis pipeline as structured process: Class Tag Caption Image 3D Mesh, as detailed below: Step 1: Class Tag Caption. To ensure broad category coverage and diversity, we follow SynCLRs approach [45], starting from ImageNet-21K [38] category tags, and use Qwen-2.5 [54] to generate rich captions that describe detailed object attributes and diverse poses. Step 2: Caption Image. We use the state-of-the-art text-to-image model, FLUX.1-Dev [20], to generate images following the captions. Besides, we enhance captions with positional descriptors to promote explicit 3D structure and upright pose. Step 3: Image 3D mesh. We employ the leading open-source image-to-3D model, Hunyuan-3D-2.0 [61], to produce high-quality 3D meshes from the synthesized images. Finally, we generate 600k 3D assets in total, with approximately 30 items for each class tag in ImageNet-21K. These assets feature complete geometry, detailed textures, and balanced category coverage. In terms of scale, the new synthetic dataset is 12 larger than the ﬁltered real dataset used in Orient Anything V1. 4.2 Robust Annotation Orient Anything V1 employs VLM to annotate the unique canonical front view of 3D assets. However, this approach is limited by the VLMs underdeveloped spatial perception ability and struggles to handle diverse rotational symmetries. To address these challenges, we introduce more effective and robust system for annotating 3D asset orientations. 4 Multi-view Rendering V1 model Annotating Pseudo Label Voting & Standardize Automatic Label Manual Calibration Final Label Figure 4: Overview of Robust Annotation Pipeline. \"Pseudo Label\" visualizes the azimuth direction of pseudo labels and objects in the horizontal plane. By ﬁtting the pseudo labels to standard periodic distribution, we can robustly derive the orientation and symmetry label. Human calibration is only required for categories with symmetry inconsistencies. Intra-asset Ensemble Annotation We ﬁrst train an improved orientation estimation model as our automatic annotator, based on the Orient Anything V1 paradigm and incorporating the additional real-world orientation dataset, ImageNet3D. Next, for each 3D asset, we employ this model to produce pseudo-labels for various renderings. Finally, we project these pseudo-labels, obtained from different viewpoints, back into canonical 3D world coordinate system. As shown in Fig. 4, the overall distribution of pseudo-labels on the horizon plane clearly indicates the objects possible orientations. To capture the main direction and rotational symmetries, we ﬁrst arrange the discrete predicted azimuth angles over [0, 360) into probability distribution Ppseudo R360. This distribution is then ﬁtted to periodic Gaussian distribution using the least squares method: ( φ, α, σ) = arg min φ,α,σ 359 i=0 Ppesudo(i) exp (cid:16) cos(α(iφ)) σ2 1 σ2 2πI0 (cid:0) (cid:1) (cid:17) (1) where σ is the ﬁtted variance. The phase φ [0, 360) represents the main azimuth direction. The periodicity α {1, 2, . . . , } signiﬁes 360/α-degree rotational symmetry, possessing α valid front faces, while α = 0 indicates no dominant orientation. Ensembling multiple pseudo labels in the 3D world effectively suppresses outlier errors from singleview predictions, resulting in signiﬁcantly more reliable annotations. Inter-assets Consistency Calibration Building on the rotational symmetry and orientation annotations for individual assets, we further perform human-in-the-loop consistency calibration across assets. Speciﬁcally, since our 3D assets are generated based on object category tags, they are naturally grouped by category. We assume that objects of the same category should share the same type of rotational symmetry. Based on this assumption, we analyze the annotated rotational symmetries within each category. If all assets within the same category demonstrate the same symmetries, we directly consider the annotations to be correct. If inconsistencies are found, we manually review all assets in that category to re-annotate or ﬁlter out incorrect annotations. As each asset is annotated independently, the cross-asset consistency check and manual calibration offer an orthogonal perspective that efﬁciently and effectively enhances annotation reliability. Statistically, across 21k source category tags, we observe only minor inconsistencies in around 15% of categories, each involving small number of assets. The ﬁnding further validates the accuracy and robustness of our ensemble annotation strategy. 5 Orientation Token"
        },
        {
            "title": "Encoder",
            "content": "Rotation Token"
        },
        {
            "title": "Joint\nEncoder",
            "content": "Figure 5: Framework of Orient Anything V2. One or two input frames are tokenized by DINOv2 and then jointly encoded using transformer blocks. We ﬁnally employ MLP heads to predict the orientation or rotation distributions from the encoded learnable tokens of each frame."
        },
        {
            "title": "5 Framework",
            "content": "5.1 Symmetry-aware Distribution Orient Anything V1 proposes an orientation distribution ﬁtting task that guides the model to learn circular Gaussian distributions over azimuth, polar, and in-plane rotation angles, that preserve the similarity between neighboring angles. Each angle is modeled with unimodal target distribution centered on unique front-facing orientation. For symmetric objects with multiple or no semantic front faces, the model additionally predicts low orientation conﬁdence to ﬁlter them out. To recognize different types of rotational symmetry and enable general orientation prediction for objects with multiple front faces, we further introduce the symmetry-aware periodic distribution as the training target. As discussed in Sec. 4.2, our ensemble annotation and consistency calibration approach enables accurate and robust labeling of 0 to valid front-facing directions over the horizontal plane. To incorporate these annotations into prediction, we directly model 0 to valid front faces within the azimuth angle distribution. This design naturally replaces V1s extra orientation conﬁdence design. Instead, different kinds of rotational symmetries are captured directly from the predicted probability distribution. This more elegant framework enables the model to inherently share knowledge across all object categories. For training, the target Pazi R360 for the azimuth angle, originally represented as circular Gaussian distribution, is adapted to be periodic: Pazi (i φ, α, σ) = exp (cid:16) cos( α(i φ)) σ2 1 σ 2πI0 (cid:0) (cid:1) (cid:17) (2) where φ and α are the phase (azimuth angle) and periodicity (rotation symmetry) ﬁtted from Sec. 4.2, σ is the variance hyper-parameter, and = 0, . . . , 359 is the angle index. Target probability distributions for the polar angle Ppol R180 and in-plane rotation angle Prot R360 are constructed using similar method, but without the periodicity parameter. During inference, the predicted angle distributions are ﬁtted to standard distribution model using the least squares method, similar to Eq. 1. The resulting parameters (azimuth periodicity ˆα, azimuth angle ˆφ, polar angle ˆσ and rotation angle ˆδ), directly indicate the objects ˆα valid front faces (i.e., symmetric with 360/α degree rotation) and their corresponding front-facing directions in 3D space. 5.2 Relative Rotation Estimation To establish connection between absolute orientation and relative rotation, enabling knowledge sharing and transferring, we modify the network architecture to support dynamic inputs from one or multiple images. As shown in Fig. 5, we mainly follow VGGT [47], ﬁrst using visual encoder, DINOv2 [33], to encode each input image into tokens, augmented with learnable tokens. The combined set of 6 Model SUN-RGBD ARKitScenes Pascal3D+ Objectron ImageNet3D Ori_COCO Med Acc30 Med Acc30 Med Acc30 Med Acc30 Med Acc30 Acc 48.5 55.4 77.58 36.48 OriAny.V1 33.94 OriAny.V2 26.00 Table 1: Zero-shot Absolute Orientation Estimation. : ImageNet3D is used for training Orient Anything V2. To ensure fair comparison, the compared V1 model is ﬁne-tuned on ImageNet3D. Best results are highlighted in bold. 22.90 15.02 30.67 22.62 13.34 15. 71.3 65.2 49.6 56.4 55.0 72.7 35.8 43.2 72.4 86.4 tokens from all frames is then passed into uniﬁed transformer block. The ﬁnal learnable token corresponding to each frame is used for prediction. Speciﬁcally, the learnable token for the ﬁrst frame is initialized differently and is used to predict the absolute orientation using the symmetry-aware distribution described in Sec. 5.1. Tokens from subsequent frames predict the object rotation relative to the ﬁrst frame through similar probability ﬁtting task, but without considering symmetry. 5.3 Training Setting Our model is initialized from VGGT, large feed-forward transformer with 1.2 billion parameters pre-trained on 3D geometry tasks. We repurpose its original \"camera\" token, designed to predict camera extrinsics, to predict object orientation and rotation. This leverages the inherent correlation between camera pose and object rotation. We train the model to ﬁt target orientation (or rotation) distributions using Binary Cross-Entropy (BCE) loss for 20k iterations. cosine learning rate scheduler is used with an initial rate of 1e-3. Input frames are resized to 518, and random patch masking is used for data augmentation to simulate real-world occlusion. The effective batch size is set to 48, where 1-2 frames are randomly sampled for each training sample. The training dataset comprises the ImageNet3D training set and newly collected 600k synthetic assets. Furthermore, we observe that most objects exhibit only four types of rotational symmetry: {0, 1, 2, 4}. Therefore, we restrict our training to consider only these four cases. Any ﬁtted periodicity α > 4 is mapped to 0."
        },
        {
            "title": "6 Experiment",
            "content": "6.1 Zero-shot Orientation Estimation Benchmark & Baselines Predicting the 3D orientation of objects from single image is our core focus. We mainly compare with Orient Anything V1 [48] on ImageNet3D [29] test set and unseen test datasets, SUN-RGBD [41], ARKitScenes [3], Pascal3D+ [52], Objectron [1] and the Ori_COCO [48]. Since current testing datasets often provide only one ground truth orientation, even for symmetric objects, when Orient Anything V2 predicts multiple orientations, we simply select the one closest to facing the camera as the prediction. The main evaluation metrics are the median 3D angle error (Med) and accuracy within 30 degrees (Acc30). For Ori_COCO, where 20 samples are collected for each class and annotated within 8 horizontal orientations, recognition accuracy (Acc) is used. Main Results In Tab. 1, we present the comparative results on single view-based orientation estimation. Overall, Orient Anything V2 signiﬁcantly improves upon V1, beneﬁting from diverse synthetic data and robust ensemble annotation. On the representative Ori_COCO benchmark, our method achieves 86.4% accuracy and performs well on categories where V1 struggled, such as bicycles. Achieving state-of-the-art results on numerous real-world image datasets highlights our methods generalization ability. 6.2 Zero-shot Rotation Estimation Benchmark & Baselines We benchmark zero-shot 6DoF object pose estimation performance under single reference view. Evaluation is conducted on four widely used datasets: LINEMOD [16], YCB-Video [5], OnePose++ [15], and OnePose [44]. Objects are prepared using the cropping and matching, following [10]. Comparisons are made against three state-of-the-art zero-shot 6DoF object pose estimation methods: Gen6D [27], LoFTR [43], and POPE [10]. Standard metrics for relative object pose estimation are used: median error (Med) and accuracy within 15 and 30 (Acc15 and Acc30), computed for each sample pair. Model LINEMOD YCB-Video OnePose++ OnePose Med Acc30 Acc15 Med Acc30 Acc15 Med Acc30 Acc15 Med Acc30 Acc15 POPEs Sampling (Average rotation angle: 14.85) Gen6D 44.86 LoFTR 33.04 15.73 POPE OriAny.V2 7.82 36.4 56.2 77.0 98.07 9.6 32.4 48.3 89.7 54.48 19.54 13.94 6.07 23.2 68.6 80.1 91. 7.7 47.8 54.4 86.4 35.43 9.01 6.27 6.18 41.1 89.1 89.6 99.7 15.8 17.78 4.35 70.3 2.16 72.8 96.6 6.76 89.3 96.3 96.2 99.7 38.9 91.8 91.1 95. Random Sample (Average rotation angle: 78.22) POPE 98.03 OriAny.V2 28.83 10.3 51.6 4.3 28.3 41.88 15. 40.9 61.2 27.2 48.7 88.21 12.83 25.6 85.5 19.8 45.73 58.8 11.72 45.1 86. 37.3 63.4 Table 2: Zero-shot Relative Rotation Estimation (i.e., pose estimation with one reference view). We evaluate two strategies for sampling query-reference view pairs: (1) query-reference pairs provided by POPE [10], and (2) randomly sampled pairs. The average rotation angles between views for each sampling strategy are 14.85 and 78.22, respectively. Main Results Tab. 2 includes zero-shot two-view relative rotation estimation results compared with state-of-the-art pose estimation methods. With small relative rotations (using POPEs sampling), our model achieves the overall best performance across the four datasets. More importantly, our methods advantage is signiﬁcantly larger when the relative rotation between the query and reference frame is larger (using random sampling). The signiﬁcant performance drop of the previous method stems from the reliance on explicit feature matching, which becomes unreliable with large rotations due to less view overlap and scarcer reliable matching points. In contrast, our approach understands images from different viewpoints by considering overall meaning rather than just detailed matching. This makes it more robust to challenging large rotations. 6.3 Zero-shot Symmetry Recognition Benchmark & Baselines We assess our methods zero-shot performance in predicting object rotational symmetry in the horizontal plane. This evaluation uses the recent, large-scale 3D object datasets with rotational symmetry annotations: Omni6DPose [58], which contain 149 distinct object classes. To ensure their orientation deﬁnition aligns with our front-facing direction, we manually select subset of 3-5 assets per category and render 2 views per 3D asset for testing. This resulted in 838 testing sample. During inference, models receive single rendering and predict the four kinds of rotational symmetry predictions. As there are currently no dedicated zero-shot models for predicting object rotational symmetry from single view, we employ advanced VLMs (Qwen2.5VL-72B [2], GPT-4o [31], GPT-o3 [32], and Gemini-2.5-pro [13]) as baselines. We evaluate their ability to predict horizontal plane rotational symmetry using multiple-choice format, with recognition accuracy as the metric. Main Results We present comparison of our method against various advanced general VLMs for identifying object horizontal rotational symmetry in Tab. 3. Our results indicate that recognizing object rotational symmetry is challenging problem even for the strongest VLMs, thereby limiting their ability to fully understand the 3D spatial state from 2D images. In contrast, beneﬁting from high-quality annotations and uniﬁed learning objective, our model achieves 65% accuracy in distinguishing object rotational symmetry. Combining this strong symmetry recognition ability alongside the robust and accurate absolute orientation estimation performance demonstrated in Sec. 6.1, our model can accurately infer multiple potential orientations from single image in real applications. Omni6DPose Random Qwen2.5VL-72B Gemini-2.5-pro GPT-4o GPT-o3 OriAny. V2 Acc 25.0 55.8 44.4 62.5 53.7 65. Table 3: Zero-shot horizontal rotational symmetry recognition. 8 0 Front Face 1 Front Faces 2 Front Faces 4 Front Faces Figure 6: Visualization of synthetic 3D assets and robust annotation. Row Assets Type Assets Number Initialized Weights Orientation Estimation Rotation Estimation Objectron Ori_COCO LINEMOD YCB-Video Med Acc15 Acc Med Acc15 Med Acc15 1 2 3 4 5 6 7 Real Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic 40K 40K 200K 400K 600K 600K 600K VGGT VGGT VGGT VGGT VGGT DINOv2 None 25.05 24.44 23.82 25.09 22.62 26.70 62.08 54.3 54.6 55.0 53.9 54. 52.6 13.5 74.8 74.6 75.2 75.4 86.4 79.0 25.6 10.70 10.16 10.22 9.78 7. 15.11 16.54 69.8 74.1 74.5 76.3 89.7 49.1 45.3 15.49 7.28 7.49 6.48 6. 13.78 13.93 72.5 76.2 78.6 80.5 86.4 52.6 52.2 Table 4: Ablation study. For the rotation estimation, we employ POPEs sampling pairs. 6.4 Ablation Study Quality of Synthetic 3D Assets Fig. 6 visualizes our synthetic dataset and the labelled orientation, qualitatively demonstrating the high quality of both the synthetic data and its annotations. Quantitatively, Rows 1 and 2 of Tab. 4 show the comparison of training with an equal amount of annotated real or synthetic 3D assets. We observe that both data sources yield comparable results for absolute orientation estimation. However, for rotation estimation (on LINEMOD and YCB-Video), training with synthetic assets provides signiﬁcant advantage. This may be because synthetic assets possess richer, more realistic textures, which are more crucial for understanding rotation. Effect of Scaling Data In Rows 2, 3, 4, and 5 of Tab. 4, we explore the impact of data scale on the performance of ﬁnal orientation and rotation estimation. Overall, with the same training step, encountering more diverse data and 3D assets during training leads to better overall performance. Speciﬁcally, we ﬁnd that rotation estimation is more sensitive to data scale than orientation estimation. This may be because orientation relies on overall semantics and structure, while rotation estimation requires understanding diverse textures and ﬁne-grain details to capture cross-view relationships. Effect of Geometry Pre-training Tab. 4 (Rows 5-7) presents our experiments of different model initialization strategies. Training without any pre-trained initialization yields the worst results. Initializing the separated visual encoder with DINOv2 introduces valuable high-quality semantic and object structure information, leading to substantial performance gains. We observe further improvements in rotation estimation by using VGGT, pre-trained speciﬁcally on 3D geometric tasks, which boosts the models comprehension of object geometry."
        },
        {
            "title": "7 Conclusion",
            "content": "We present Orient Anything V2, an advanced model for uniﬁed object orientation and rotation understanding. Through introducing the scalable data engine, symmetry-aware distribution learning target, and multi-frame framework, our model enables: 1) Stronger single-view absolute orientation estimation. 2) Advanced two-frame object relative pose rotation estimation. 3) Powerful object horizontal rotational symmetry recognition. In practice, the model can simultaneously and accurately predict multiple valid front faces of objects, making it well-suited for diverse objects and real-world application scenarios. 9 Limitation While our models exhibit strong generalization to diverse in-the-wild objects in real images, we ﬁnd that the inherent ambiguity of monocular images leads to less accurate predictions in views with very low information or severe occlusion. Furthermore, the current framework supports maximum of two input frames. Extending the model to handle more frames will be an important direction for supporting video understanding applications."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by National Key R&D Program of China (No. 2022ZD0162000) and National Natural Science Foundation of China (No. 62222211, U24A20326, 624B2128, 62422606 and 62201484)"
        },
        {
            "title": "References",
            "content": "[1] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. Objectron: large scale dataset of object-centric videos in the wild with pose annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 78227831, 2021. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. [4] Lonni Besançon, Anders Ynnerman, Daniel Keefe, Lingyun Yu, and Tobias Isenberg. The state of the art of spatial interfaces for 3d visualization. In Computer Graphics Forum, volume 40, pages 293326. Wiley Online Library, 2021. [5] Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR), pages 510517. IEEE, 2015. [6] Enric Corona, Kaustav Kundu, and Sanja Fidler. Pose estimation for objects with rotational symmetry. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 72157222. IEEE, 2018. [7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. [8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. [9] Ufuk Efe, Kutalmis Gokalp Ince, and Aydin Alatan. Dfm: performance baseline for deep feature matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42844293, 2021. [10] Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, and Zhangyang Wang. Pope: 6-dof promptable pose estimation of any object in any scene with one reference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77717781, 2024. [11] Aaron Gardony, Shaina Martis, Holly Taylor, and Tad Brunyé. Interaction strategies for effective augmented reality geo-visualization: Insights from spatial cognition. HumanComputer Interaction, 36(2):107149, 2021. [12] Google. Gemini-2.0-ﬂash, 2025. https://aistudio.google.com/prompts/new_chat?model= gemini-2.0-flash-exp. [13] Google. Gemini-2.5-pro, 2025. https://deepmind.google/technologies/gemini/pro/. [14] Qi Guan, Zihao Sheng, and Shibei Xue. Hrpose: Real-time high-resolution 6d pose estimation network using knowledge distillation. Chinese Journal of Electronics, 32(1):189198, 2023. [15] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypointfree one-shot object pose estimation without cad models. Advances in Neural Information Processing Systems, 35:3510335115, 2022. [16] Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary Bradski, Kurt Konolige, and Nassir Navab. Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. In Asian conference on computer vision, pages 548562. Springer, 2012. [17] Tomas Hodan, Daniel Barath, and Jiri Matas. Epos: Estimating 6d pose of objects with symmetries. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1170311712, 2020. [18] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. Cotr: Correspondence transformer for matching across images. In Proceedings of the IEEE/CVF international conference on computer vision, pages 62076217, 2021. [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [20] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [21] Phillip Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, and Minhyuk Sung. Perspective-aware reasoning in vision-language models via mental imagery simulation. arXiv preprint arXiv:2504.17207, 2025. [22] Dingzhe Li, Yixiang Jin, Yuhao Sun, Hongze Yu, Jun Shi, Xiaoshuai Hao, Peng Hao, Huaping Liu, Fuchun Sun, Jianwei Zhang, et al. What foundation models can bring for robot learning in manipulation: survey. arXiv preprint arXiv:2404.18201, 2024. [23] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded recurrent network with adaptive correlation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1626316272, 2022. [24] Xiang Li, Zixuan Huang, Anh Thai, and James Rehg. Symmetry strikes back: From single-image symmetry detection to 3d generation. arXiv preprint arXiv:2411.17763, 2024. [25] Chuang Lin, Bingbing Zhuang, Shanlin Sun, Ziyu Jiang, Jianfei Cai, and Manmohan Chandraker. Drive-1to-3: Enriching diffusion priors for novel view synthesis of real vehicles. arXiv preprint arXiv:2412.14494, 2024. [26] Jian Liu, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu, Hossein Rahmani, Nicu Sebe, and Ajmal Mian. Deep learning-based object pose estimation: comprehensive survey. arXiv preprint arXiv:2405.07801, 2024. [27] Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang. Gen6d: Generalizable model-free 6-dof object pose estimation from rgb images. In European Conference on Computer Vision, pages 298315. Springer, 2022. [28] Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [29] Wufei Ma, Guofeng Zhang, Qihao Liu, Guanning Zeng, Adam Kortylewski, Yaoyao Liu, and Alan Yuille. Imagenet3d: Towards general-purpose object-level 3d understanding. Advances in Neural Information Processing Systems, 37:9612796149, 2024. [30] Pedro Monteiro, Guilherme Gonçalves, Hugo Coelho, Miguel Melo, and Maximino Bessa. Hands-free interaction in immersive virtual reality: systematic review. IEEE Transactions on Visualization and Computer Graphics, 27(5):27022713, 2021. [31] OpenAI. Gpt-4o, 2025. https://openai.com/index/introducing-4o-image-generation/. [32] OpenAI. Gpt-o3, 2025. https://openai.com/index/introducing-o3-and-o4-mini/. [33] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 11 [34] Jonathan Palacios and Eugene Zhang. Rotational symmetry ﬁeld design on surfaces. ACM Transactions on Graphics (TOG), 26(3):55es, 2007. [35] Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy Mitra. Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 76957704, 2024. [36] Shiv Naga Prasad and Larry Davis. Detecting rotational symmetries. In Tenth IEEE International Conference on Computer Vision (ICCV05) Volume 1, volume 2, pages 954961. IEEE, 2005. [37] Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, et al. Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. arXiv preprint arXiv:2502.13143, 2025. [38] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021. [39] Yifei Shi, Zixin Tang, Xiangting Cai, Hongjia Zhang, Dewen Hu, and Xin Xu. Symmetrygrasp: Symmetryaware antipodal grasp detection from single-view rgb-d images. IEEE Robotics and Automation Letters, 7(4):1223512242, 2022. [40] Yifei Shi, Xin Xu, Junhua Xi, Xiaochang Hu, Dewen Hu, and Kai Xu. Learning to detect 3d symmetry from single-view rgb-d images with weak supervision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):48824896, 2022. [41] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567576, 2015. [42] Hao Su, Charles Qi, Yangyan Li, and Leonidas Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. In Proceedings of the IEEE international conference on computer vision, pages 26862694, 2015. [43] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 89228931, 2021. [44] Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou. Onepose: One-shot object pose estimation without cad models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68256834, 2022. [45] Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola. Learning vision from models rivals learning vision from data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1588715898, 2024. [46] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26422651, 2019. [47] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. arXiv preprint arXiv:2503.11651, 2025. [48] Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient anything: Learning robust object orientation estimation from rendering 3d models. ICML, 2025. [49] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchﬁeld. Foundationpose: Uniﬁed 6d pose estimation and tracking of novel objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1786817879, 2024. [50] Ziyi Wu, Yulia Rubanova, Rishabh Kabra, Drew Hudson, Igor Gilitschenski, Yusuf Aytar, Sjoerd van Steenkiste, Kelsey Allen, and Thomas Kipf. Neural assets: 3d-aware multi-object scene synthesis with image diffusion models. Advances in Neural Information Processing Systems, 37:7628976318, 2024. [51] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. [52] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond pascal: benchmark for 3d object detection in the wild. In IEEE winter conference on applications of computer vision, pages 7582. IEEE, 2014. 12 [53] Yang Xiao, Vincent Lepetit, and Renaud Marlet. Few-shot object detection and viewpoint estimation for objects in the wild. IEEE transactions on pattern analysis and machine intelligence, 45(3):30903106, 2022. [54] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [55] Honghong Yang, Hongxi Liu, Yumei Zhang, and Xiaojun Wu. Fmr-gnet: Forward mix-hop spatial-temporal residual graph network for 3d pose estimation. Chinese Journal of Electronics, 33(6):13461359, 2024. [56] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-ﬁdelity 3d geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 3, 2025. [57] Difeng Yu, Xueshi Lu, Rongkai Shi, Hai-Ning Liang, Tilman Dingler, Eduardo Velloso, and Jorge Goncalves. Gaze-supported 3d object manipulation in virtual reality. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 113, 2021. [58] Jiyao Zhang, Weiyao Huang, Bo Peng, Mingdong Wu, Fei Hu, Zijian Chen, Bo Zhao, and Hao Dong. Omni6dpose: benchmark and model for universal 6d object pose estimation and tracking. In European Conference on Computer Vision, pages 199216. Springer, 2024. [59] Zhaoxuan Zhang, Bo Dong, Tong Li, Felix Heide, Pieter Peers, Baocai Yin, and Xin Yang. Single depth-image 3d reﬂection symmetry and shape prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 88968906, 2023. [60] Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, and Ziqiao Ma. Do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities. arXiv preprint arXiv:2410.17385, 2024. [61] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025."
        },
        {
            "title": "A More Visualizations of Images in The Wild",
            "content": "In 7 8, 9, 10, 11, 12, 13, we present more visualizations of images from various domains containing different objects. In these images, our model shows strong abilities in single-view absolute orientation estimation, powerful object horizontal rotational symmetry recognition and two-frame object relative pose rotation estimation, further highlighting the impressive zero-shot capability of Orient Anything V2. Figure 7: Relative pose rotation estimation for images in the wild. 14 Figure 8: Orientation estimation and Rotational symmetry recognition results on objects has no front direction. Figure 9: Orientation estimation and Rotational symmetry recognition results on objects has one front direction. Part 1. Figure 10: Orientation estimation and Rotational symmetry recognition results on objects has one front direction. Part 2. Figure 11: Orientation estimation and Rotational symmetry recognition results on objects has two front direction. Part 1. 16 Figure 12: Orientation estimation and Rotational symmetry recognition results on objects has two front direction. Part 2. Figure 13: Orientation estimation and Rotational symmetry recognition results on objects has four front direction. 1 2 3 4"
        },
        {
            "title": "A More Visualizations of Images in The Wild",
            "content": "In 1 2, 3, 4, 5, 6, 7, we present more visualizations of images from various domains containing different objects. In these images, our model shows strong abilities in single-view absolute orientation estimation, powerful object horizontal rotational symmetry recognition and two-frame object relative pose rotation estimation, further highlighting the impressive zero-shot capability of Orient Anything V2. Figure 1: Relative pose rotation estimation for images in the wild. 1 Figure 2: Orientation estimation and Rotational symmetry recognition results on objects has no front direction. Figure 3: Orientation estimation and Rotational symmetry recognition results on objects has one front direction. Part 1. Figure 4: Orientation estimation and Rotational symmetry recognition results on objects has one front direction. Part 2. Figure 5: Orientation estimation and Rotational symmetry recognition results on objects has two front direction. Part 1. 3 Figure 6: Orientation estimation and Rotational symmetry recognition results on objects has two front direction. Part 2. Figure 7: Orientation estimation and Rotational symmetry recognition results on objects has four front direction."
        }
    ],
    "affiliations": [
        "Sea AI Lab",
        "Shanghai AI Lab",
        "The University of Hong Kong",
        "Zhejiang University"
    ]
}