{
    "paper_title": "UniMuMo: Unified Text, Music and Motion Generation",
    "authors": [
        "Han Yang",
        "Kun Su",
        "Yutong Zhang",
        "Jiaben Chen",
        "Kaizhi Qian",
        "Gaowen Liu",
        "Chuang Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the \\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 4 3 5 4 0 . 0 1 4 2 : r UniMuMo: Unified Text, Music and Motion Generation Han Yang1 Kun Su2 Yutong Zhang3 Jiaben Chen4 Kaizhi Qian5 Gaowen Liu Chuang Gan4,5 1The Chinese University of Hong Kong 2University of Washington 3The University of British Columbia 4UMass Amherst 5MIT-IBM Watson AI Lab 6Cisco Research {email, addresses}@inst.edu Figure 1. UniMuMo is able to perform generation tasks on any combination of music, motion, and text. The tasks shown in the figure include text-to-aligned-music-motion, music-to-motion, motion-to-music, music-captioning, and motion-captioning."
        },
        {
            "title": "Abstract",
            "content": "https://hanyangclarence.github.io/unimumo demo/. We introduce UniMuMo, unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through unified encoder-decoder transformer architecture. To support multiple generation tasks within single framework, we introduce several architectural improvements. We propose encoding motion with music codebook, mapping motion into the same feature space as music. We introduce music-motion parallel generation scheme that unifies all music and motion generation tasks into single transformer decoder architecture with single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in 1. Introduction Music and body movements are synchronized and inseparable. The beat and metrical structures in rhythm encourage the spontaneous coordination of body motion with music [29], activating the motor-related areas of human brains [26]. Dance particularly exemplifies this connection through choreography that aligns with the musics rhythm, melody and emotion. Meanwhile, even though most people are not professional musicians or dancers, they often interpret music and dance using simple, natural language. This descriptive text serves as vital bridge between understandable ideas and abstract concepts in music and motion. The synergy between music, motion, and text provides natural motivation to create model capable of understanding and creating contents across all these modalities. Moreover, building framework that can flexibly generate music, motion, and text in arbitrary combinations is crucial for real-world applications, even though existing models already achieve impressive results in unidirectional generation tasks such as text-to-music [7], music-to-motion [50], motion-to-music [55] and motion-to-text [24]. In the real world, there is demand for diverse generative abilities, and more complex generation tasks may be necessary, such as creating dance sequences based on both music and textual descriptions. Training individual models for each unique combination, although potentially yielding better output quality, would significantly increase training costs, deployment efforts and storage requirements. Thus, unified model that supports all combinations of conditioning and generation tasks, rather than collection of separate models or training adapters to incorporate individual models, offers more costeffective solution. To this end, we introduce novel task of dynamically generating music, motion, and text in multitude of combinations unifiedly. As demonstrated in Fig. 1, this task is designed to handle diverse generative scenarios, ranging from text-to-music, text-to-motion, to more complex combinations like text-to-music-plus-motion or music-plustext-to-motion. However, the task could be challenging, especially in two aspects: i) the lack of comprehensive datasets that include all three modalities - music, motion, and text - limits the development of general and unified model. While there are individual datasets for music-only [44], motion-only [37], music to motion [32] and text to motion [20], holistic and large-scale dataset that encompasses all three modalities still remains absent; ii) designing unified architecture that supports both the conditioning and generation of all three modalities is challenging, mainly due to the significant differences between the neural representations for the three modalities and the multiplicity of desired generation tasks. To address the first challenge of lacking paired data, we propose to align unpaired music and motion sequences based on their rhythmic patterns. Specifically, we extract both music beats and motion visual beats, then employ dynamic time warping to find the alignment and warp the motion sequence to adjust the motion visual beats to match the music beats. We found that such augmentation is accurate and efficient. With the augmented synchronized music-motion data, we can utilize existing music and motion datasets to train our unified generative model. Additionally, we construct text descriptions from music and motion metadata using mixture of template filling, large language model generation and music-based language model generation, striking balance between diversity, language fluency and description accuracy. To overcome the second challenge, we propose novel framework, UniMuMo, to unify the generation of different modalities. Our pipeline consists of three main stages: music-motion joint tokenizer that encodes music and motion sequences into discrete representations within the same space, music-motion transformer-decoder model trained on the task of music-motion joint generation, and musicmotion captioner that generates text descriptions from music and motion features. In the first stage, we bridge the modality gap between music and motion by mapping motion into the music feature space. Specifically, instead of using separate Vector-Quantized Variational Autoencoders (VQ-VAE) to quantize music and motion sequences, we encode motion with the codebook of pre-trained music VQ-VAE, namely Encodec [10]. This design facilitates the unification of music and motion within the same generative framework in the subsequent stage. In the second stage, we train unified music and motion generative model with novel task of music-motion joint generation from text conditions. To enable the mutual conditioning of music and motion, and unlock the music-to-motion and motion-to-music generation capabilities, we introduce novel music-motion parallel generation scheme, where we perform two mutually conditioned streams of autoregressive generation of aligned music and motion simultaneously. With the reuse of Encodec and joint encoding of motion in the previous stage, the current stage can be effectively achieved by fine-tuning the pre-trained text-to-music model associated with Encodec, namely MusicGen [7], equipping it with additional motion conditioning and generation capabilities while maintaining its music generation capabilities. In the third stage, we fine-tune T5 decoder for music and motion captioning tasks, using the features extracted by the music-motion decoder trained in stage 2. To transform the decoder into an effective feature extractor, we replace its causal self-attention layers with trainable full self-attention layers, and fine-tune them together with the T5 decoder on music and motion captioning tasks. Extensive experiments demonstrate that UniMuMo achieves competitive performance across all unidirectional generation tasks in music, motion, and text when compared with existing state-of-the-art models, demonstrating the effectiveness and versatility of our approach."
        },
        {
            "title": "Our work offers significant advancements in multimodal",
            "content": "generative research, summarized as follows: To the best of our knowledge, this is the first unified framework capable of arbitrarily generating content across music, motion, and text. To address the shortage of paired multimodal data, we augment and enrich existing large-scale datasets with musicmotion data alignment and text augmentations. We propose novel joint codebook for encoding music and motion sequences, along with music-motion parallel generation scheme, facilitating multiple generation tasks within single architecture. Our framework achieves results comparable to SOTAs across all generation tasks in music, motion, and text. 2. Related Work Text to Music. Text-conditioned music generation has been widely studied in recent years. There are two main branches: diffusion-based and transformer-based. For diffusion-based models, Riffusion [15] uses latent text-to-image diffusion model to generate spectrograms, which are then con2 verted into audio clips; Mousai [45] proposes training diffusion model in the latent space of diffusion autoencoder; Noise2Music [22] introduces cascade of diffusion models that first generates the audio in coarse form and then progressively refine it. AudioLDM [34] proposes to train latent diffusion model using CLAP [51] embeddings, language-audio joint representation, for text conditioning. For transformer-based models, MusicLM [2] proposes to encode music into high-level semantic tokens and low-level acoustic tokens, and use cascade of transformer decoders to generate the two levels stage by stage. MusicGen [7] leverages single-stage transformer decoder to model the hierarchical music tokens directly. Music to Text. Several models have been proposed for audio captioning. WAC [25] proposes to transfer pre-trained speech-to-text Whisper model to the music captioning task. LTU [18] takes the concatenated music embeddings and text embeddings as input to large language model and directly trains caption generation using language modeling objectives. LP-MusicCaps [12] uses transformer encoderdecoder structure, where the music spectrogram is first encoded by the encoder and then cross-attended by the decoder for text generation. MU-LLaMA [35] leverages frozen LLaMA [49] and fine-tunes Music Understanding Adapter to fuse music features into the LLaMA model. Music to Motion. Most of the works on music-conditioned dance generation are based on transformers. Several approaches [14, 31, 41] adopt similar structures that first use music transformer encoder and motion transformer encoder to encode music and initial motion into representations separately, and then employ transformer decoder for crossmodal fusion and motion generation. Bailando [46] proposes to train transformer on motion features encoded by choreographic memory module, which is the codebook of motion VQ-VAE. Besides autoregressive transformers, EDGE [50] adopts transformer-based diffusion model capable of both dance generation and editing. Motion to Music. Most of the relevant works focus on generating corresponding music from video input. Foley Music [16] focuses on generating music for videos of people playing instruments, and uses Musical Instrument Digital Interface (MIDI) to bridge the gap between body key points and the final music. Similarly, RhythmicNet [47] extends the scenarios to arbitrary motion videos by first estimating visual rhythm and conditionally generating drum and piano music. Dance2Music [1] encodes dance similarity matrix with CNN and predicts the next note with an LSTM autoregressively. CDCD [56] proposes single-stage method that uses discrete latent diffusion model to generate music spectrograms conditioned on video features. D2M-GAN [55] proposes GAN-based model to generate the music tokens based on video and pose features. Text to Motion. Text-to-motion approaches can be mainly categorized into transformer-based and diffusion-based. Transformer-based models mainly work with motion tokens generated by motion VQ-VAE. TM2T [21] regards textto-motion and motion-to-text as machine translation tasks and trains transformer encoder-decoder to perform bidirectional translation uniformly. T2M-GPT [53] adopts GPTbased model to directly generate motion tokens from text conditions. MotionGPT [24] proposes incorporating motion tokens into natural language tokens and performing language modeling on both text and motion. For the diffusion-based model, MotionDiffuse [54] and MDM [48] directly adopt diffusion model while MLD [5] adopt latent diffusion on motion generation. Motion to Text. Motion-to-text generation is usually regarded as sequence-to-sequence translation task. Various methods adopts different sequence-to-sequence models, such as RNN [40], recurrent autoencoder [52], sequence generative adversarial nets [19] and transformer [21, 24]. 3. Text-Music-Motion Aligned Data Generation To model arbitrary generation across music, motion, and text, we propose to expand existing music and motion datasets by aligning motion with music and synthesizing textual descriptions. The data generation pipeline includes four major steps: 1) music beat detection, 2) visual beat detection, 3) music-motion alignment, and 4) text description synthesis. Music Beat Detection. We estimate music beats from music waveform RTw , where Tw represents the number of samples, using Bidirectional-LSTM-based model from [6]. This model performs beat tracking on extracted drum features and non-drum features separately, then aggregates the results with learnable fuser. We manually evaluate the accuracy of this beat tracking model and find that it performs well in most test cases, outperforming the beat tracking methods in the Librosa API [38]. The resulting music beats are represented as binary sequence Bm RTw , where each frame is marked as beat or non-beat. Visual Beats Detection. Given 3D motion sequence RTmJ3 where Tm represents the number of frames, the number of joints, and the last dimension indicates x, y, coordinates, we obtain visual beats in three steps. In the first stage, we calculate the motion directogram [8], 2D matrix that factors motion into different motion angles, similar to how an audio spectrogram factors sound amplitude into different frequencies. Specifically, we first compute the firstorder difference of the motion sequence Mt = MtMt1. Based on its motion angle, we assign the motion magnitude of every joint into one of the bins in 2π/Nbins. The motion directogram Md(t, θ) is obtained by summing the motion magnitudes of each bin: Md(t, θ) = (cid:80) Mt(j)1θ(Mt(j)), where 1θ(ϕ) = 1 if θ ϕ 2π/Nbins else 0. In the second stage, we convert the motion directogram to the kinematic offset Mk, which represents the motion changes, similar to Figure 2. Overview: The training of UniMuMo consists of three stages: In stage 1, we train motion RVQ-VAE using the frozen codebook from pre-trained music RVQ-VAE to encode motion into the same space as music. In stage 2, we fine-tune pre-trained music transformer decoder model on the text-to-music-motion task using the music-motion parallel generation scheme. In stage 3, we fine-tune T5 decoder for music-motion captioning using the previous music-motion decoder as feature extractor. the onset envelope in an audio spectrogram. We first obtain motion flux Mf , which represents the deceleration in various directions, by computing the negative first-order difference of the directogram Md. We then average each frame of Mf and filter the top 1% peaks to obtain kinematic offset Mk. In the last stage, we use dynamic programming to compute the visual beats by designing an objective function that selects strong visual changes from kinematic offsets and encourages equal-spacing beats. More details can be found in Appendix A. The final visual beats are also represented as binary sequence Bv RTm, where each frame is marked as beat or non-beat. Music-Motion Alignment. We apply dynamic time warping to determine the optimal matching between music beats Bm and visual beats Bv, finding the alignment even though the duration of these two binary sequences could be different. Finally, we warp motion sequences by interpolating according to the warping curve to obtain aligned music-motion pairs. The reason for warping motion to match music, rather than the reverse, is that music beats tend to be steady, so warping music could result in perceptually unacceptable changes. More details can be found in Appendix B. Text Description Synthesis. To compensate for the absence of text descriptions in our used datasets, we employ two methods for captions synthesis: (1) using Music Understanding Language Model to generate caption directly from audio; and (2) using Large Language Model to synthesize captions from metadata (genre, tempo, etc.), striking balance between musical accuracy and diversity. Examples and more details are shown in Appendix C. 4. UniMuMo Framework UniMuMo consists of three training stages to enable arbitrary generation between music, motion, and text. In stage 1, we encode aligned music and motion data into discrete tokens. To efficiently bridge the gap between the two modalities, we propose to use frozen pre-trained audio tokenizer Encodec [10] and train motion tokenizer that reuses the same residual codebooks of the audio tokenizer. In stage 2, we fine-tune state-of-the-art text-to-music transformer decoder [7] by conducting the task of generating music and motion tokens simultaneously with music and motion text descriptions. At the inference stage, we can perform parallel generation to unlock applications of music and motion generation. In stage 3, we treat the pre-trained music-motion decoder model in stage two as feature extractor and finetune T5 decoder on language modeling task for music and motion captioning. An overview of the UniMuMo framework is shown in Figure 2. 4.1. Stage 1. Music and Motion Joint Tokenization While existing tokenization approaches can faithfully reconstruct the music or motion individually, the correlations between the two modalities become intricate in distinct spaces. Therefore, directly applying them in the unified generation framework poses challenges. Besides, music tokenizer usually requires more training resources and time to achieve high-quality reconstruction than motion tokenizer. Inspired by these facts, we introduce an efficient and effective way to encode music and motion into joint latent space. We propose using pre-trained audio tokenizer, Encodec [10], and training new motion encoder-decoder. The motion encoder encodes the motion into the same embedding space as the 4 music and reuses the frozen music Residual Vector Quantizers (RVQ) to discretize the motion into tokens. From these tokens, the motion decoder can decode to reconstruct the motion. Given the higher complexity and richer information in music compared to motion, the learned music codebook is theoretically capable of encoding motion. Specifically, given waveform RT fw with the audio duration and fw the sample rate, Encodec first encodes it into continuous tensor of Xmusic RdT fr , where fr fw is the frame rate of the residual codebook and is the dimension of codebook entries. Xmusic is then quantized by the RVQ into music tokens Qmusic {1, . . . , }KT fr , where is the number of RVQ and is the number of codebook entries. For an aligned motion sequence of the same duration RdmT fm with frame rate fm and feature dimension dm, our motion encoder encodes it into Xmotion RdT fr , the same shape as Xmusic, which is then tokenized by the same RVQ into motion tokens Qmotion {1, . . . , }KT fr . The motion decoder decodes the motion feature after RVQ, resulting in ˆM . The motion encoder-decoder is trained by minimizing the motion reconstruction loss together with commitment loss Lcommit from the codebook: Ltotal = 1 (cid:88) (M ˆM 2 + λLcommit) (1) where is the motion dataset and λ controls the strength of the commitment loss. Empirically, λ is set to 0.02. With this design, the music-motion joint tokenization can effectively learn multimodal correlations by mapping motion features into the same space as music, without the need to train another computationally heavy music autoencoder. Moreover, it enables direct use the text-to-music model associated with Encodec as an initialization for the following music-motion decoder model, significantly reducing training costs and enhancing the performance. Experimentally, such feature alignment is crucial to learning the joint generation of music and motion within single transformer model. 4.2. Stage 2. Music and Motion Generation from"
        },
        {
            "title": "Text",
            "content": "In this stage, we modify and fine-tune an existing state-of-theart text-to-music model with the music and motion tokens extracted from Stage 1, enabling it to handle all tasks related to music and motion generation, such as text-to-musicmotion and motion-to-music. In particular, we employ MusicGen [7], an open-source, single-stage transformer decoder model that can generate multi-level music tokens with specific codebook interleaving pattern. Following their practice, we apply the delay pattern for both music and motion tokens, utilize T5 encoder for encoding text descriptions, and adopt cross-attention to incorporate text conditioning features into the transformer decoder. To enable the autoregressive generation of music and motion within unified framework, we propose training on the task of music-motion joint generation, together with novel parallel generation scheme, where two streams (i.e., music and motion) of predict-next-token generation are conducted simultaneously, with each stream conditioned on each other. Specifically, given the music tokens Qmusic and motion tokens Qmotion with the same shape where = fr is the sequence length, we first transform them with delay pattern [7] into motion respectively, resulting shape S, where = + 1. We then concatenate them in time dimension into Qinput of the shape 2S as the input to the transformer decoder. The models output is transformed back to the normal pattern for loss calculation. Training on music-motion joint generation, we adopt the predict-next-token objectives for both music and motion tokens in each forward pass: music and = 1 (cid:88) QD (cid:40) µ (cid:88) t=1 (cid:104) log Qmusic Qmusic <t , Qmotion <t (cid:105) +(1 µ) (cid:104) log (cid:88) t=1 Qmotion Qmusic <t , Qmotion <t (2) (cid:41) (cid:105) where µ balances between music loss and motion loss, and denotes predict-next-token probability of the model. Empirically, µ is set to 0.85. To enable the parallel autoregressive generation, we apply cross-modal causal attention mask, as shown in Stage 2 of Figure 2. The causal attention mask is of shape 2S 2S, each quarter of which is an S lower triangular matrix, allowing music and motion tokens to have both cross-modal and uni-modal causal attention. further illustration of the strategy can be found in Appendix D. With the above construction, the model can perform parallel sampling during inference, enabling the prediction of the next token for both music and motion concurrently: ˆQmusic = argmax iM P[Qmusic t,i ˆQmusic <t , ˆQmotion <t ] ˆQmotion = argmax iM P[Qmotion t,i ˆQmusic <t , ˆQmotion <t ] (3) (4) where is the codebook size. With this sampling strategy, we can conduct the joint generation of music and motion under text conditions. Additionally, it facilitates zero-shot music-to-motion and motion-to-music generation. For example, given music sequence Qmusic 1:S , an aligned motion sequence can be autoregressively sampled by ˆQmotion = argmax iM P[Qmotion t,i Qmusic <t , ˆQmotion <t ] (5) An illustration of the sampling process can also be found in Appendix D. Considering the inherent differences between music and motion, we further introduce the following changes to the pre-trained MusicGen to alleviate the mutual interference 5 between the two modalities. First, we add another trainable embedder for motion tokens, with which the model can learn to differentiate the two modalities. Second, to ensure the temporal parallelism, we add positional encodings {E1, E2, . . . , ES} to music and motion separately, instead of using holistic positional encoding of length 2S. Third, inspired by the idea of Mixture of Experts (MoE), we introduce an additional feed-forward network (FFN) for motion in each transformer layer. As shown in Fig. 2, in each forward pass, the first half of the feature (i.e., music features) is processed by the music FFN, and the second half (i.e., motion features) by the motion FFN. Fourth, we add new motion classification head at the end of the network to distinguish motion code prediction from music code prediction. Note that for the new modules introduced above, we initialize the motion embedder and FFNs with the corresponding components from the pre-trained MusicGen. With joint motion VQ-VAE trained in Stage 1, such initialization ensures that music features are not confused by uninitialized motion features at the beginning of training, allowing the music generation capability to be better preserved. Following MusicGen, text conditioning is added with cross-attention. In the framework of music-motion joint generation, we add the text condition of two modalities independently. We first encode music descriptions and motion descriptions separately into features and apply classifierfree guidance dropout independently. Then, during crossattention on text conditions, we specialize the attention mask to allow music features to attend only to music conditions and motion features to attend only to motion conditions. By fine-tuning the model on the music-motion dataset with the above settings, we find that the model learns to generate motion in parallel with music quickly while still keeping its music-generation ability. With single training task of music-motion joint generation, various applications could be achieved in zero-shot fashion, including text-tomusic, text-to-motion, music-to-motion, motion-to-music, motion-text-to-music, etc. 4.3. Stage 3. Music and Motion Captioning The final stage is for caption generation, where we treat the fine-tuned music-motion decoder in the previous stage as feature encoder for music and motion, and fine-tune another T5 decoder to generate captions for music and motion. However, using the music-motion decoder directly as feature extractor brings challenges. Firstly, the self-attention in the decoder is done causally, which is inadequate for capturing rich music and motion features. Secondly, since the input of the model is the concatenation of music and motion, we are limited to input music-motion pairs for captioning, which is inflexible. To address these issues, we introduce trainable full selfattention module, initialized with the trained cross-modal causal attention module, as shown in Fig. 2, Stage 3. Inspired by BLIP [30], which claims that the major difference between transformer encoders and decoders lies in the self-attention layers, with embedding layers and FFNs functioning similarly, we therefore fine-tune only the newly introduced full self-attention modules together with the T5 decoder on caption generation task, keeping the rest of the music-motion decoder unchanged. Considering that captions of music and motion are independent, we remove the crossattention areas on the attention mask. In practice, we first randomly mask the entire music or motion tokens as empty, and concatenate them together as input Qinput. This allows us to conduct music or motion captioning independently. Next, we forward it through the music-motion decoder with null condition, where full selfattention is applied. We then take the output of the last hidden layer of the model as the feature, which is cross-attended by the T5 text decoder. We fine-tune the model with the language modeling task, and the generation target is either music caption or motion caption, depending on the input masking. 5. Experiment 5.1. Effectiveness of Music-Motion Alignment To quantitatively evaluate the beat alignment results, we calculate the mean L1 distance between each music beat and its nearest visual beat before and after the alignment. Specifically, we randomly sample 300 music clips, each 10 seconds long, pair each with random motion sequence, and then calculate the score before and after aligning them using the algorithms we introduced. The mean L1 distance decreases from 6.34 to 1.78, demonstrating the overall effectiveness of our alignment algorithms. We further conduct user study, the results of which are in Appendix E. 5.2. Evaluations We conduct extensive evaluations of our model across various tasks and metrics. More implementation details about hyperparameter choices, dataset, metrics and training/evaluation setups are in Appendix F."
        },
        {
            "title": "Models",
            "content": "FADVGG KL CLAP Riffusion [15] Mubert [39] Mousai [45] MusicLM (860M) [2] MusicGen (300M) [7] AudioLDM 2-Full (346M) [34] Ours (300M) MusicGen (fine-tuned on our data) Ours (trained on data with vocals) 14.8 9.6 7.5 4.0 4.9 3.13 5.93 5.81 4.11 2.06 1.58 1.59 1.31 1.42 1.17 1.99 1.97 1.95 0.19 - 0.23 - 0.27 0.38 0.27 0.28 0.29 Table 1. Comparison of text-to-music generation on MusicCaps. Bold and underlined results are the best and second-best results. Methods R-Precision Top1 Top3 MMDist Real 0.506 0.800 MotionGPT [24] 0.534 0.803 0.525 0.814 TM2T [21] 0.520 0.806 Ours 2.986 2.978 2.995 2.958 Bleu @1 @ - - 42.61 6.04 61.76 21.98 9.27 52.84 ROUGE-L Cider BertScore - 34.47 47.40 40.11 - 7.92 71.12 6.22 - 31.57 37.27 40.90 Table 5. Comparison of motion captioning on HumanML3D dataset. Models Beats Coverage Beats Hit Dance2Music [1] Foley Music [16] CMT [11] D2M-GAN [55] CDCD [56] Ours 83.5 74.1 85.5 88.2 93.9 93.0 82.4 69.4 83.5 84.7 90.7 88.4 Table 2. Comparison of motion-conditioned music generation on AIST++. Models Distk Distg Beat Align. Real Bailando [46] FACT [31] EDGE [50] Ours (music conditioned) Ours (text conditioned) 10.61 7.92 10.85 10.58 10.68 9.14 7.48 7.72 6.14 7.62 10.35 9.37 0.24 0.23 0.22 0.27 0.24 0. Table 3. Comparison of music-conditioned and text-conditioned dance generation. Models Bleu Meteor Rouge BertScore LTU [18] LP-MusicCaps [12] MU-LLaMA [35] Ours 0.238 0.165 0.238 0.261 0.250 0.202 0.354 0. 0.332 0.281 0.475 0.369 0.876 0.879 0.913 0.892 Table 4. Comparison of music captioning on MusicQA dataset. Text-to-Music. In Table 1, we compare our UniMuMo with Riffusion [15], Mubert [39], Mousai [45], MusicLM [2], MusicGen [7] and AudioLDM 2 [34]. We evaluate the performance on MusicCaps, with results of SOTAs directly sourced from their respective papers. We employ three metrics: Frechet Audio Distance (FADVGG) [27], Kullback-Leibler Divergence (KL) [28] and CLAP similarity (CLAP) [23, 51]. The first two metrics measure the audio quality, while the last one measures the correspondence between generated audio and text descriptions. Note that the audio quality of our model does not match with SOTA models. We argue that this might be due to the poor audio quality of our training data. Following MusicGen, we also use vocal-free training data. To achieve this, we use Demucs [9, 43] to remove the vocal part of the music in Music4All dataset. Nonetheless, we observe that many of the processed audio are of bad quality. This is testified by the experiment of fine-tuning MusicGen on our dataset for the same number of epochs while keeping all other settings the same (e.g., sequence length, batch size). As shown in Table 1, the audio quality of the tuned model also degrades. We also tried training the model on the original dataset with vocals, resulting in improved quantitative scores. However, the generated music is not perceptually good, often filled with weird and meaningless vocals. This phenomenon, where training on music with vocals yields better quantitative scores, is also reported in MusicGen. Dance-to-Music. In Table 2, we compare UniMuMo with Dance2Music [1], Foley Music [16], CMT [11], D2MGAN [55] and CDCD [56] on dance-conditioned music generation. For evaluation, we adopt Beats Coverage and Beats Hit [55], both of which measure the alignment of generated music with motion. Music/Text-to-Dance. In Table 3, we compare UniMuMos dance-generation capabilities with Bailando [46], FACT [31] and EDGE [50] on AIST++ dataset. We evaluate UniMuMo on both music-conditioned and text-conditioned dance generation tasks. Although there is currently no established benchmark for the text-to-dance task, we can also apply the same evaluation metrics to measure and compare the quality of generated dance. For evaluation metrics, we adopt kinetic distribution spread (Distk) and geometric distribution spread (Distg) to measure the diversity. Additionally, we employ the beat alignment score to measure the alignment between conditioning audio and generated dance. Following EDGE, we evaluate the motion sequences on 5-second clip. For text-to-dance, we directly evaluate the dance that is jointly generated with music, conditioned on both music and motion captions, and we calculate the beat alignment score between the generated dance and music. The quantitative scores show that UniMuMo achieves competitive results on music-conditioned dance generation, even though it hasnt been fine-tuned on AIST++ music. For text-conditioned generation, it achieves inferior dance quality since there is no ground truth music for reference, but also gains higher beat alignment score due to the joint generation. Music-to-Text. In Table 4, we compare UniMuMo against SOTA music captioning models including LTU [18], LPMusicCaps [12] and MU-LLaMA [35]. The evaluation is conducted on the MusicQA dataset released by [35], which is music-related question-answering dataset. We take the answers to the question Describe the audio together with the corresponding music as evaluation data, totaling 552 music-caption pairs. Following MU-LLaMa, the metrics we use includes Bleu, Meteor, RougeL and BertScore, which are all common evaluation metrics in natural language processing. Motion-to-Text. In Table 5, we compare UniMuMo with TM2T [21] and MotionGPT [24] for motion captioning using the HumanML3D test set. Following MotionGPT, we adopt the motion-retrieval precision (R-Precision) to measure the accuracy of motion-text matching using top-1 and top-3 retrieval accuracy, multi-modal distance (MM Dist) to measure the distance between motion and text, and other popular natural language processing metrics, including Blue, Rouge, Cider and BertScore, to assess the linguistic quality. Since we source only 50% of our training motion data from HumanML3D, and the motion is augmented to align with music beats, UniMuMo still lags behind the best SOTA in certain metrics for HumanML3D motion captioning task. Based on the quantitative results presented above, UniMuMo achieves competitive performance compared to the SOTA benchmarks across various single-modal generation tasks. Specifically, in the motion-to-music, music-to-motion, music captioning and motion captioning tasks, UniMuMo generally ranks second among the SOTAs. However, in the text-to-music task, UniMuMos performance is not as competitive, which we argue may be attributed to the limitations in our training data. 5.3. Ablation Studies FADVGG KL CLAP Distk Distg Beat Align. Full Ablation 1 Ablation 2 Ablation 3 Ablation 4 Ablation 5 5.93 6.75 6.79 6.29 6.41 7. 1.99 2.13 2.06 2.11 2.06 2.22 0.27 0.26 0.26 0.24 0.27 0.23 10.54 6.03 6.73 6.03 9.22 9.61 8.15 8.28 7.35 8.28 6.58 9.01 0.24 0.22 0.23 0.22 0.22 0.23 Table 6. Comparisons of our full model with different ablation studies on MusicCaps for music generation and our Music4All for dance generation. Ablation 1-2 show the results of using an independent motion VQVAE for encoding motion sequences. Ablation 3 shows the results of model without the key structures of separate embedder and MoE. Ablation 4 shows the results of using mixture of training tasks during training. Ablation 5 shows the result of training our model from scratch. In the ablation study, we first evaluate the effectiveness of the proposed joint codebook encoding by training our model with motion encoded by an independent motion VQ-VAE (ablation 1 and 2). We then assess the impact of the additional separate embedder and FFN introduced to MusicGen by training the model without them (ablation 3). We also investigate training the model with multiple tasks, rather than the single proposed music-motion joint generation (ablation 4). Finally, we evaluate the effectiveness of using pre-trained model for initialization by training the model from scratch (ablation 5). All ablations are compared with our benchmarks on MusicCaps for music generation and Music4All for motion generation, as shown in Table 6. More details and analysis are in Appendix G. 6. Conclusion In this paper, we introduce UniMuMo, the first unified framework for arbitrary generation across music, motion, and text. To address the limitations of paired multimodal data, we expand existing datasets with rhythm-based music-motion alignment and text augmentation, thus creating comprehensive new dataset. To build unified model, we propose novel architectural designs, including music-motion joint tokenizer for bridging modality gaps and music-motion parallel generation scheme for synchronized music and motion generation. Extensive experiments show that UniMuMo achieves competitive performance in all unidirectional generative tasks. We believe our framework will not only open up new avenues for multimodal generation but also inspire future advancements in this rapidly evolving field."
        },
        {
            "title": "References",
            "content": "[1] Gunjan Aggarwal and Devi Parikh. Dance2music: AuarXiv preprint tomatic dance-driven music generation. arXiv:2107.06252, 2021. 3, 7 [2] Andrea Agostinelli, Timo I. Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi, Neil Zeghidour, and C. Frank. Musiclm: Generating music from text. ArXiv, abs/2301.11325, 2023. 3, 6, 7, 13 [3] Sebastian Bock and Gerhard Widmer. Maximum filter vibrato suppression for onset detection. In Proc. of the 16th Int. Conf. on Digital Audio Effects (DAFx). Maynooth, Ireland (Sept 2013), page 4, 2013. 11 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 11 [5] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. [6] Ching-Yu Chiu, Alvin Wen-Yu Su, and Yi-Hsuan Yang. Drumaware ensemble architecture for improved joint musical beat and downbeat tracking. IEEE Signal Processing Letters, 28: 11001104, 2021. 3 [7] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. SimarXiv preprint ple and controllable music generation. arXiv:2306.05284, 2023. 1, 2, 3, 4, 5, 6, 7, 13 [8] Abe Davis and Maneesh Agrawala. Visual rhythm and beat. ACM Transactions on Graphics (TOG), 37(4):111, 2018. 3, 11 [9] Alexandre Defossez. Hybrid spectrogram and waveform source separation. In Proceedings of the ISMIR 2021 Workshop on Music Source Separation, 2021. 7, 14 [10] Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. 2, 4 [11] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, and Shuicheng Yan. Video background music generation with controllable music transformer. In Proceedings of the 29th ACM International Conference on Multimedia, pages 20372045, 2021. 8 [12] SeungHeon Doh, Keunwoo Choi, Jongpil Lee, and Juhan Nam. Lp-musiccaps: Llm-based pseudo music captioning. arXiv preprint arXiv:2307.16372, 2023. 3, 7 [13] Daniel PW Ellis. Beat tracking by dynamic programming. Journal of New Music Research, 36(1):5160, 2007. 11 [14] Di Fan, Lili Wan, Wanru Xu, and Shenghui Wang. bidirectional attention guided cross-modal network for music based dance generation. Computers and Electrical Engineering, 103:108310, 2022. 3 [15] Seth* Forsgren and Hayk* Martiros. Riffusion - Stable diffusion for real-time music generation. 2022. 2, 6, 7 [16] Chuang Gan, Deng Huang, Peihao Chen, Joshua Tenenbaum, and Antonio Torralba. Foley music: Learning to generate music from videos. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pages 758775. Springer, 2020. 3, 7 [17] Toni Giorgino. Computing and visualizing dynamic time warping alignments in r: the dtw package. Journal of statistical Software, 31:124, 2009. [18] Yuan Gong, Hongyin Luo, Alexander Liu, Leonid Karlinsky, and James Glass. Listen, think, and understand. arXiv preprint arXiv:2305.10790, 2023. 3, 7 [19] Yusuke Goutsu and Tetsunari Inamura. Linguistic descriptions of human motion with generative adversarial seq2seq learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 42814287. IEEE, 2021. 3 [20] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 51525161, 2022. 2, 14 [21] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision, pages 580597. Springer, 2022. 3, 7, 15 [22] Qingqing Huang, Daniel Park, Tao Wang, Timo Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: Textconditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917, 2023. 3 [23] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. arXiv preprint arXiv:2301.12661, 2023. 7 [24] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. arXiv preprint arXiv:2306.14795, 2023. 1, 3, 7, [25] Marek Kadlˇcık, Adam Hajek, Jurgen Kieslich, and Radosław Winiecki. whisper transformer for audio captioning trained with synthetic captions and transfer learning. arXiv preprint arXiv:2305.09690, 2023. 3 [26] Peter Keller and Martina Rieger. Musical movement and synchronization. Music Perception, 26(5):397400, 2009. 1 [27] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018. 7 [28] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352, 2022. 7 [29] Edward Large. On synchronizing movements to music. Human movement science, 19(4):527566, 2000. 1 [30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In International Conference on Machine Learning, pages 1288812900. PMLR, 2022. [31] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1340113412, 2021. 3, 7 [32] Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. Learn to dance with aist++: Music conditioned 3d dance generation, 2021. 2, 12, 14 [33] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 15 [34] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. AudioLDM 2: Learning holistic audio generation with self-supervised pretraining. arXiv preprint arXiv:2308.05734, 2023. 3, 6, 7 [35] Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, and Ying Shan. Music understanding llama: Advancing text-tomusic generation with question answering and captioning. arXiv preprint arXiv:2308.11276, 2023. 3, 7, 11, 13, 15 [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [37] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In International Conference on Computer Vision, pages 54425451, 2019. 2, 12, 14 [38] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python. In Proceedings of the 14th python in science conference, 2015. 3, 15 https://mubert. com/, https://github.com/mubertai/ mubert-text-to-music. 2022. 6, 7 [39] Mubert-Inc. Mubert. [40] Matthias Plappert, Christian Mandery, and Tamim Asfour. Learning bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks. Robotics and Autonomous Systems, 109:1326, 2018. 3 [41] Junfu Pu and Ying Shan. Music-driven dance regeneration with controllable key pose constraints. arXiv preprint arXiv:2207.03682, 2022. [42] Lawrence Rabiner and Biing-Hwang Juang. Fundamentals of speech recognition. Tsinghua University Press, 1999. 11 [43] Simon Rouard, Francisco Massa, and Alexandre Defossez. Hybrid transformers for music source separation. In ICASSP 23, 2023. 7, 14 9 music and image generation. In The Eleventh International Conference on Learning Representations, 2022. 3, 7, 15 [44] Igor Andre Pegoraro Santana, Fabio Pinhelli, Juliano Donini, Leonardo Catharin, Rafael Biazus Mangolin, Valeria Delisandra Feltrim, Marcos Aurelio Domingues, et al. Music4all: new music database and its applications. In 2020 International Conference on Systems, Signals and Image Processing (IWSSIP), pages 399404. IEEE, 2020. 2, 11, 13 [45] Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf. Moˆ usai: Text-to-music generation with long-context latent diffusion. arXiv preprint arXiv:2301.11757, 2023. 3, 6, 7 [46] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1105011059, 2022. 3, 7, [47] Kun Su, Xiulong Liu, and Eli Shlizerman. How does it sound? Advances in Neural Information Processing Systems, 34:2925829273, 2021. 3 [48] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. 3 [49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. corr, abs/2302.13971, 2023. doi: 10.48550. arXiv preprint arXiv.2302.13971. 3 [50] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448458, 2023. 1, 3, 7, 15 [51] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and In ICASSP 2023-2023 keyword-to-caption augmentation. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 3, 7 [52] Tatsuro Yamada, Hiroyuki Matsunaga, and Tetsuya Ogata. Paired recurrent autoencoders for bidirectional translation IEEE between robot actions and linguistic descriptions. Robotics and Automation Letters, 3(4):34413448, 2018. 3 [53] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations. arXiv preprint arXiv:2301.06052, 2023. 3 [54] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Textdriven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. [55] Ye Zhu, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei Chai, Yan Yan, and Sergey Tulyakov. Quantized gan for complex music generation from dance videos. In European Conference on Computer Vision, pages 182199. Springer, 2022. 1, 3, 7, 15 [56] Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan. Discrete contrastive diffusion for cross-modal"
        },
        {
            "title": "Appendix",
            "content": "A. Visual Beats Detection Details As discussed in the main paper, there are three steps to obtain visual beats from 3D motion sequence RTmJ3 where Tm is the number of frames, is the number of joints, and the third dimension represents x, y, coordinates. Inspired by the idea used to find visual impacts in raw videos [8], we adapt this approach to find local saliency in motion sequences. The main idea is to use the sudden visible deceleration of the motion sequence as the basis of the heuristic to find visual beats. The three steps have very similar physical meanings to traditional music beat detection techniques [3, 13]. In the first step, we compute the motion directogram Md, 2D matrix that factors motion into different motion angles, as described in the main paper. The motion directogram is similar to the audio spectrogram, which could offer spectral flux to measure the change in amplitude of different frequencies over time. Therefore, in the second step, we compute the pre-direction deceleration of Md as an analogue for spectral flux to obtain the motion flux Mf . In audio, the onset envelope could be inferred from the spectral flux. In motion, we obtain the visual impact envelope, called kinematic offsets Mk, by averaging each frame of Mf and filtering the top 1% peaks. With the onset envelope, usually, an onset detection followed by dynamic programming-based beat tracking algorithms [3, 13] are used to find the most likely periodic music beats. In motion sequence, we also use dynamic programming to compute the visual beats by designing an objective function that selects strong visual changes from kinematic offsets and encourages equal-spacing beats. We optimize the objective function: (m) = (cid:88) j= VT (mj, mj+1) = n1 (cid:88) u(mj) + α VT (mj, mj+1) j=1 [bin(mj+1 mj)] Tmax 1. where is the kinematic offset value of the candidate beat to encourage strong visual impacts, α is the weight to balance the two terms, is the autocorrelation mean over the local time window, and {mj}n j=1 is subset of candidate beats. The VT (mj, mj+1) regularizes the estimated tempos within local window to encourage equal-spacing beats. We measure the deviation by computing the time-dependent autocorrelation function on kinematic offsets. B. Music Motion Alignment Details We apply dynamic time warping (DTW) to compute the optimal alignment between music beats Bm and visual beats Bv. The optimal alignment minimizes the sum of distances 11 between aligned elements, even though the lengths of Bm and Bv may differ. The local distance between elements of Bm and Bv is computed by Euclidean distance. Regarding the transitions allowed while searching for the minimumdistance path, we use the Rabiner-Juang step pattern [42]. We use python-dtw package [17] to find the alignment. Finally, we warp motion sequences according to the warping curve. C. Text Description Construction Details C.1. Music Caption Generation with Music Language Model We employed MU-LLaMa [35], specialized large language model for music-related Q&A tasks, to generate music descriptions using genre metadata from the Music4All dataset. Our experiments show that MU-LLaMa generally understands music effectively, accurately assesses tags, and integrates them into cohesive descriptions. However, we also notice that the generated descriptions often lack diversity both in sentence structure and content. Repeated results are also observed. C.2. Music Caption Generation with ChatGPT and"
        },
        {
            "title": "Template Filling",
            "content": "We adopt energy, tempo, genres, and tags from the metadata of Music4All [44]. Energy, indicating musical intensity and activity, ranges from 0.0 to 1.0. Tempo, measured in beats per minute (BPM), reflects the musics speed. The dataset categorizes music into 853 unique genres and includes 19,541 tags. First, we convert energy and tempo into descriptive tags with the criteria shown in Table 8 and 9. Adverbs and adjectives are randomly selected and paired, and the thresholds are manually determined by listening to samples and comparing values. Then, we construct phrases from tags using random templates. We construct tempo phrase from tempo description, an energy phrase from energy description and tag phrase from genres and tags. The choices of templates are shown in Table 10. Next, we randomly shuffle, dropout and concatenate the phrases to construct raw music text descriptions. Finally, we refine the descriptions with ChatGPT [4]. We set the ChatGPT content as You are an expert in music, skilled in writing music comments and descriptions. and use the prompt Here are {n} music descriptions, please polish them separately into fluent and meaningful sentences with details. Please return the polished results in the format of 1: content... 2: content... ... to polish descriptions on each request. This approach ensures that our synthesized descriptions are more natural than direct tag concatenation and more diverse than using full descriptive templates. Moreover, it has more control on the synthesized results than directly asking ChatGPT to write descriptions from tags. However, since the"
        },
        {
            "title": "ChatGPT Generated",
            "content": "MU-LLaMa Generated"
        },
        {
            "title": "Motion Desctiption",
            "content": "Blending elements of underground hip-hop, ekip, and rap, this music exudes gentle yet quick energy that immerses the listener in its unique category. This high-energy song combines the raw power of indie rock with the smooth and expressive elements of jazz, resulting in unique and captivating musical experience. This high-tempo track seamlessly combines German pop and alternative styles, boasting spirited and lively atmosphere that will keep listeners engaged from start to finish. The music is blend of neofolk, martial industrial, and dark ambient. The audio is progressive rock/metal song with fast tempo, steady drumming, and bass guitar rhythm. The music is bossa nova/jazz/MPB/soul fusion with touch of Brazilian rhythms. The style of the dance is ballet jazz. This is break style dance. The genre of the dance is LA style hip-hop. Table 7. Examples of three kinds of synthesized text descriptions."
        },
        {
            "title": "Adjective",
            "content": "tempo < 60 extremely, very 60 tempo < 75 75 tempo < 110 110 tempo < 150 tempo >= extremely, very, highly slow, languid, lethargic, relaxed, leisure, chilled slow, languid, lethargic, relaxed, leisure, chilled moderate, easy-going, laid-back,medium, balanced, neutral fast, upbeat, high, brisk, quick, rapid, swift fast, upbeat, high, brisk, quick, rapid, swift Table 8. Choices of descriptive tags for tempo. Adverbs and adjectives are randomly chosen and paired. generated captions source the musical information only from genre, tag, intensity, and tempo, they often cannot provide specific details such as the instrument composition or the emotional tone. Additionally, the descriptive tags of energy and tempo can sometimes be imprecise due to the inaccuracies of metadata. Therefore, during training we leverage mixture of the above two methods for music captioning to strike balance between musical accuracy and diversity. C.3. Motion Caption Generation The only metadata that is available for AIST++ [32] and DancedDB [37] is genre tag. Therefore, we directly construct the motion descriptions using the following templates: The genre of the dance is <>, The style of the dance is <>., The is <> style dance., where <> is filled with the genre tag. energy < 0.1 extremely, very 0.1 energy < 0.4 0.4 energy < 0.7 0.7 energy < 0.9 energy >0. extremely, very, highly soft, calm, peaceful, serene, gentle, light, tranquil, mild, mellow soft, calm, peaceful, serene, gentle, light, tranquil, mild, mellow moderate, comfortable, balanced, relaxing intense, powerful, strong, vigorous, fierce, potent, energetic intense, powerful, strong, vigorous, fierce, potent, energetic Table 9. Choices of descriptive tags for energy."
        },
        {
            "title": "Tag Phrase",
            "content": "with <> tempo, whose speed is <>, <> music, set in <> pace which is <>, with <> intensity, <> music, whose energy is <> This is atrack which is <>, This song has the style of <>, The music is <>, The genre of the music is <> Table 10. Choices of phrase template for tempo, energy and tags. Inside <>, we fill in tempo tag, energy tag or list of genres and tags. Finally, as mentioned in the paper, we apply text conditioning separately to music and motion, adding classifier-free guidance dropout independently. Examples of the several types of descriptions mentioned above are shown in Table 7. 12 D. Illustration of Music Motion Parallel Generation During the training of UniMuMo, we introduce music motion parallel generation scheme, which not only allows for the joint generation of music and motion, but also enables zero-shot music-to-motion and motion-to-music generation. In this section, we provide illustrations to further explain the training and inference processes. During training, as illustrated in Figure 3a, music and motion are trained on language modeling task separately. In each forward pass, the system is trained to predict the next music token for music and the next motion token for motion. The predict-next-token losses are calculated separately for each modality and then summed up with different weights. The customized cross-modal causal-attention mask, each quarter of which is lower triangular matrix, ensures that each modality can causally attend to itself while also allowing both modalities can causally attend to each other. Note that in our implementation, music and motion are encoded using four-layer residual codebook, resulting four tokens at each timestep. As mentioned in the paper, we follow the approach of MusicGen [7], adopting delay pattern to transform the input tokens of shape S, where is the number of codebooks, chosen to be 4 in our implementation, and is the length of the sequence of tokens. Figure 3b illustrates this pattern, with more details available in their original paper. During implementation, we first transform the music tokens and motion tokens separately according to this pattern, and then concatenate them to form the model input. During inference, as shown in Figure 4a and 4b, one music token and one motion token are sampled on each forward pass. For joint music-motion generation, the sampled next music token and next motion token are placed in the corresponding positions of the input for the next timestep. When generating music conditioned on motion, the conditioning motion token at the same timestep is used instead of the predicted next motion token. (a) An illustration of one forward pass during UniMuMos training. Music tokens (including the music start token) are denoted in blue blocks and motion tokens (including the motion start token) are denoted in orange blocks. The numbers in the block denote the timestep of each token. (b) An illustration of the delay pattern in MusicGen. Each color represents different layer of the residual codebook, and the numbers on the blocks indicate the timestep. After applying the delay pattern, the tokens denoted in grey are padded with special empty token. Figure 3. Illustrations on the technical details in our training process. E. User Study on Music-Motion Alignment F. Implementation Details F.1. Training Since no ground truth for measuring the accuracy of the music-motion alignment, we further conduct user study, where users are required to rate the audio-visual alignment from 1 (not aligned) to 5 (aligned) based on their perception. According to the responses of 8 users, each presented with 20 videos (half aligned, half randomly paired), the average score is 3.95 for aligned results and 3.26 for random pairs, demonstrating the algorithms effectiveness. screenshot of the survey form is in Figure 5. Datasets. For the music training dataset, we mainly use Music4All [44], which consists of 109K 30-second soundtracks with corresponding metadata, such as genre, energy, and tempo. For evaluation and comparison with prior work on music generation, we use the MusicCaps benchmark [2], which is composed of 5.5K ten-second samples with expertprepared text descriptions. For evaluation on music captioning, we leverage the MusicQA Dataset [35], which contains 560 music tracks with textual descriptions. 13 (a) An illustration of how UniMuMo conduct music motion parallel generation. In each timestep T, one forward pass is performed. Figure 5. screen shot of the user study form for evaluating our music-motion alignment algorithm. AIST++ dataset. Data Preparations. We remove the vocal part of all the training audios using Demucs [9, 43] and keep only the instrumental part. We randomly pair each soundtrack with 5 motion sequences, with approximately 50% drawn from 3D dance datasets AIST++ and DanceDB, and the remainder from HumanML3D. The pairing and tokenization of music and motion are done in advance to save training time. Stage 1: Music and Motion Tokenization Model. We adopt the default Encodec from the MusicGen model, which compresses 32K Hz waveform into 50 Hz. The RVQ has 4 codebooks, each with 2048 entries of dimension 128. The motion encoder encodes 60 Hz, 263-dimensional motion features into 50 Hz, 128-dimensional features suitable for quantization by the RVQ. The motion encoder-decoder, together with the frozen RVQ, is trained on 2-second motion data, with batch size of 336 and learning rate of 2e-4. Stage 2 Music-Motion Decoder Model. In stage 2, the model we fine-tune is MusicGen-small, 300M transformer decoder model together with 120M T5 text encoder. We train the model on 10-second aligned music-motion pairs with batch size 144. We train the model for 15K steps with AdamW optimizer [36], β1 = 0.9, β2 = 0.95, and learning (b) An illustration of UniMuMos motion-to-music generation. The music-to-motion generation is similar to this process. Figure 4. Illustrations on the technical details in the inference process. For motion dataset, we use mixture of 3D dance dataset AIST++ [32], DancedDB [37] and 3D motion dataset HumanML3D [20]. AIST++ contains 311 minutes of dance across 30 subjects and 10 genres, while DanceDB contains 203.38 minutes of dance across 20 subjects. Text descriptions for both datasets are synthesized by filling templates with metadata. HumanML3D, much larger dataset of general motion, contains 14616 motions with 44970 text descriptions, in total 28.59 hours long. We upsample its motion data from 20 Hz to 60 Hz to align with the frame rate of the 14 rate of 5e-5. The training takes around 6 hours on 48 Tesla V-100 32G GPUs. Stage 3: Music-Motion Captioner Model. In stage 3, we choose the T5-base model as the text decoder. We train the model on 10-second unpaired music and motion sequences. Motion sequences shorter than this duration are zero-padded to 10 seconds. The other settings remain the same as in stage 2. results. Motion-to-Text. We adopt the evaluation script from the open-sourced code of MotionGPT 6 [24]. Since they announced that they used different package for NLP-related metrics calculation due to package conflict, we re-evaluated their model and TM2T [21] on our HumanML3D test set. This accounts for the differences between our reported results and theirs. F.2. Evaluation Text-to-Music. The evaluation scripts for FAD and KL are directly from the evaluation repository of AudioLDM1 [33]. We use the default pre-trained model in the official webpage of CLAP2 to calculate the CLAP score. All the scores for SOTAs are directly borrowed from the corresponding papers, except for MusicGen. As explained on their Hugging Face pages, their publicly released models are trained on another set of vocal-free music, resulting in slightly lower quantitative scores. Since we directly fine-tune their released model, we report the scores as presented on the Hugging Face page instead. Motion-to-Music. The evaluation scripts for Beats Coverage and Beats Hit are directly sourced from D2M-GAN 3 [55]. The test music-motion pairs are 2 seconds long, and the segmentation is also from D2M-GAN [55]. All scores for other models are directly borrowed from CDCD [56]. Music/Text-to-Dance. We directly use the script in Bailando 4 [46] for evaluating feature distributions (Distk and Distg). For the beat alignment score, we modify the script from [46] to use the Librosa [38] API for music beat detection. All scores for other models are sourced from [50]. Regarding the choice of evaluation metrics, we did not include the Physical Foot Contact score (PFC) proposed in [50], as their provided script does not yield correct scores on our data, even when evaluating the ground truth. The discrepancy may be due to the slight differences in motion representation (e.g., fps, number of joint). According to [50], the Frechet Inception Distances on kinetic features and geometric features are also not employed, as they are not considered reliable for measuring dance quality. For testing data, we use the original test set split from the AIST++ dataset, and slice the dance-music pairs into 5-second segments using the script from [50]. Music-to-Text. We use the evaluation script from MULLaMa 5 [35] for the testing metrics. The testing dataset, MusicQA, is also publicly available in [35]. As we only take the subset of the dataset related to music captioning as our test set, we re-evaluate all previous SOTAs and report their 1https://github.com/haoheliu/audioldm eval 2https://github.com/LAION-AI/CLAP 3https://github.com/L-YeZhu/D2M-GAN 4https://github.com/lisiyao21/Bailando 5https://github.com/shansongliu/MU-LLaMA G. More Analysis on Ablation Study Ablation 1-2: Effect of joint codebook encoding. In this analysis, we examine the effectiveness of using shared codebook that maps motion into the same feature space as music. Specifically, we train the model on motion codes extracted by an independent motion VQ-VAE. In ablation 1, we dont initialize the motion embedder with corresponding pre-trained weights in MusicGen, while in ablation 2, we perform such initialization, which is also the practice in our full model. The results of both ablation studies yield inferior scores, demonstrate the effectiveness of our method. Importantly, ablation 2 proves that it is not merely the initialization of the motion embedders that facilitates training, but it is only when the initialized motion embedders are input with meaningful motion tokens that training is facilitated. Ablation 3: Effect of additional architectures. We also examine the effectiveness of the additional components we introduce to the pre-trained MusicGen, specifically the separate motion embedder and Mixture of Expert (MoE) structure. In ablation 3, we train the model without MoE and the separate motion embedder, allowing music and motion to share most of the MusicGen model, except the linear classifier at the end. Experimentally, this results in degraded performance, especially in motion generation. Ablation 4: An alternative multi-task training scheme. In stage two, we employ single training task of music-motion joint generation. However, within this parallel transformer structure, it is also intuitive to adopt mixture of three training tasks: music-motion generation, music-to-motion and motion-to-music. We explore this idea in ablation 4, where we randomly select among the three tasks during training and apply the corresponding cross-modal self-attention masks (e.g., in music-to-motion, we allow music to only causally attend to itself, and motion to causally attend to itself and fully attend to music). However, the results are not satisfactory. We hypothesize that the reasons could be 1) mixture training tasks might result in gradient conflicts; 2) the difficulty level of the three tasks varies, so simply randomly selecting the task might not be sufficient; and 3) the rhythms of music and dance have regular patters, so they might not necessarily need to be fully attended to when performing cross attention. 6https://github.com/OpenMotionLab/MotionGPT Ablation 5: Effect of using pre-trained model. In ablation 5, we train the same model from scratch, keeping other settings unchanged. The best results shown in the table are achieved after around 30K iterations of training, compared with only 15K iterations of training if the pre-trained model is loaded."
        }
    ],
    "affiliations": [
        "Cisco Research",
        "MIT-IBM Watson AI Lab",
        "The Chinese University of Hong Kong",
        "The University of British Columbia",
        "UMass Amherst",
        "University of Washington"
    ]
}