{
    "paper_title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
    "authors": [
        "Shumin Wang",
        "Yuexiang Xie",
        "Wenhao Zhang",
        "Yuchang Sun",
        "Yanxi Chen",
        "Yaliang Li",
        "Yanyong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning."
        },
        {
            "title": "Start",
            "content": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models Shumin Wang 1 Yuexiang Xie 2 Wenhao Zhang 2 Yuchang Sun 2 Yanxi Chen 2 Yaliang Li 2 Yanyong Zhang"
        },
        {
            "title": "Abstract",
            "content": "Entropy serves as critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement finetuning (RFT), principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with discriminant expression that quantifies entropy change under single logit update. This foundation enables the derivation of firstorder expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropydiscriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning. 6 2 0 2 3 ] . [ 1 2 9 3 3 0 . 2 0 6 2 : r 1. Introduction Reinforcement fine-tuning (RFT) (OpenAI, 2025) has recently attracted growing attention as post-training paradigm for enhancing the capabilities of large language Work done as an intern at Tongyi Lab, Alibaba Group. 1University of Science and Technology of China 2Tongyi Lab, Alibaba Group. Correspondence to: Yanyong Zhang <yanyongz@ustc.edu.cn>, Yaliang Li <yaliang.li@alibaba-inc.com>. Preprint. February 4, 2026. 1 models (LLMs) (Guo et al., 2025; Yang et al., 2025a; Agarwal et al., 2025). It has shown substantial improvements across range of downstream tasks, such as mathematical reasoning (Shao et al., 2024; Chen et al., 2025), programming (Wei et al., 2025; Zeng et al., 2025), and tool usage (Zhang et al., 2025; Feng et al., 2025). Drawing from reinforcement learning (RL) (Sutton et al., 1998), RFT transforms the fine-tuning process into policy optimization problem where LLMs are incentivized to produce high-reward responses. The exploration-exploitation trade-off presents crucial challenge for RFT, potentially leading to unstable performance and stagnation in local optima (Arulkumaran et al., 2017; Ahmed et al., 2019). In this context, the entropy of responses emerges as key diagnostic metric, offering insights into the output diversity of LLMs, and is actively leveraged by recent studies (Yu et al., 2025; Cui et al., 2025; Hu et al., 2025; Su et al., 2025) to monitor training dynamics and regulate policy behavior. However, existing methods (Wang et al., 2025; Liao et al., 2025; Yu et al., 2025; He et al., 2025) often rely on heuristic designs that treat entropy in isolation and oversimplify its adjustment. Moreover, the divergence in whether these approaches encourage or suppress entropy highlights fundamental lack of in-depth understanding of entropy dynamics (Hu et al., 2025; Luo et al., 2025; An et al., 2025). Such an unprincipled basis can lead to labor-intensive hyperparameter tuning without clear guidance, thus hindering the effective optimization of RFT. As result, theoretically grounded framework is increasingly necessary to characterize entropy dynamics in RFT. To fill this gap, we establish theoretical framework that provides principled understanding of entropy dynamics in RFT. Inspired by (Ren & Sutherland, 2025), we model the update of single tokens logit during optimization, and characterize how it propagates through the models output probability distribution, ultimately influencing the policys entropy. Our derivation reveals that the entropy change direction is determined by the interplay between the update direction (whether the token is rewarded or penalized) and the sign of the proposed discriminator score S, which captures the relationship between token probability and policy entropy. This analysis explains the widely On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models observed phenomenon of rapid entropy collapse (Yu et al., 2025) when models are consistently rewarded for generating high-probability and safe responses (Su et al., 2025). Building upon such single-logit analysis, we extend our framework to analyze the entropy change resulting from an optimization step under Group Relative Policy Optimization (GRPO) (Shao et al., 2024). We derive an expression that practically computes the entropy change trend leveraging the discriminant and its policy-weighted expectation. Our analysis provides insights for the development of entropybased methods, inspires practical clipping strategies and sheds light on the mechanisms of existing approaches. Our contributions can be summarized as follows: We propose theoretical framework that characterizes the token-level entropy change during policy optimization. We further extend it to practical GRPO optimization step and derive first-order analytical expression, indicating that the direction of entropy change is closely related to the direction of token updates and discriminator score S. Our theoretical analysis provides new insights for the design of entropy control methods. Building upon this, we explain existing entropy control methods from the perspective of entropy dynamics, offering unified and principled theoretical framework for understanding their effects and underlying mechanics. We conduct experiments to provide empirical evidence for our theoretical analysis, showing that can be reliable discriminator for the entropy dynamics. The experimental results also demonstrate the effectiveness of the derived clipping methods in stabilizing the entropy in RFT to promote model exploration. 2. Preliminaries Policy Optimization (GRPO) Group Relative GRPO (Shao et al., 2024) is prominent RFT algorithm that has proven highly effective and efficient across various tasks (Guo et al., 2025; Zhang et al., 2025). In GRPO, for each query q, behavior policy πθsample is employed to sample group of responses {oi}G i=1, where each response oi = (ai,1, . . . , ai,Ti) with Ti tokens is subsequently assigned scalar reward Ri, and each token ai,t in response is generated under state si,t = (q, oi,<t). The policy is updated by maximizing the GRPO objective function, defined as: j=1) Rimean({Rj }G std({Rj }G j=1) Following (Yu et al., 2025; An et al., 2025; Wang et al., 2025), we omit the KL divergence penalty. Here, the advantage Ai is computed by standardizing rewards within the group, and the importance ratio ri,t(θ) is the token-level probability ratio between the target and behavior policies, i.e., Ai = πθsample (ai,tsi,t) . The parameter ε defines the clipping range for PPOstyle (Schulman et al., 2017) clipped objective. In our strict on-policy training (Chen et al., 2025) setup, where the behavior policy is the optimized policy (πθsample = πθ), the importance ratio satisfies ri,t = 1 and the clipping mechanism remains inactive. In this case, the update of GRPO encourages increasing the probability of sampled tokens if Ai > 0 and decreasing it if Ai < 0. and ri,t(θ) = πθ(ai,tsi,t) Entropy Dynamics Entropy provides principled measure of uncertainty and is used to quantify the diversity of model outputs. For an LLM, the next-token distribution is given by pt() = πθ( st) = softmax(zt), where zt are the models logits at position in response. The tokenlevel entropy is then defined as Ht = (cid:80) log pt i[V ] pt i, where denotes the size of vocabulary and pt is the probability of token at being the i-th vocabulary item. The field of learning dynamics (Ren & Sutherland, 2025) studies how parameter updates affect model predictions. In this work, we introduce the concept of Entropy Dynamics, focusing specifically on how token entropy evolves during RFT. Specifically, we investigate how parameter update, triggered by single sampled token at, alters the entropy of the output token distribution at that step. We formalize this by investigating the relationship between the entropy change before and after update, Ht, and the distribution of the policy in position t, πθ(at st). By analyzing this relationship, we aim to uncover the principles that determine whether the updates in RFT encourage diverse responses or lead to repetitive, similar outputs. 3. Analysis of the Entropy Dynamics in RFT To establish principled understanding of entropy dynamics in RFT, we propose theoretical framework that characterizes the token-level entropy change during policy optimization. Specifically, we quantify how the update of single token affects the policys entropy, providing microscopic view of entropy dynamics. Upon this, we derive the firstorder expression for the entropy change resulting from policy update step when applying GRPO. Jgroup(θ) = 1 (cid:80)G j=1 Tj (cid:88) Ti(cid:88) i=1 t=1 (cid:16) min ri,t(θ)Ai, clip(cid:0)ri,t(θ), 1 εl, 1 + εh (cid:1)Ai (cid:17) . (1) 3.1. From Single Logit Update to the Entropy Change We consider single decoding step where the policy π produces distribution over vocabulary of size . Let RV be the models output logits. These logits are 2 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models transformed into probability distribution via the softmax function, where pi = , [V ]. The diversity of this probability distribution is measured by the token-level Shannon entropy (Shannon, 1948), which can be formally given as H(p) = (cid:80)V exp(zi) j=1 exp(zj ) i=1 pi log pi. (cid:80)V Throughout our analysis, we make the following standard assumptions for deriving first-order dynamics: (i) All probabilities {pi} are non-zero, as guaranteed by the softmax function; (ii) Auxiliary regularization terms, including KLdivergence penalties, and explicit entropy bonuses, are considered inactive within RFT unless explicitly specified; and (iii) We ignore tokens that trigger logit clipping as their gradients are set to zero and contribute no change to entropy. The analysis begins with fundamental operation in model update, i.e., updating the logit of single token. We model this as perturbation as δz = εek, where ek is the standard basis vector for the k-th token, and ε is the change caused by the optimization process. The sign of ε, i.e., sign(ε), represents the direction of the update: sign(ε) = +1 corresponds to rewarding the token (increasing the logits), while sign(ε) = 1 corresponds to penalizing it (decreasing the logits). The following lemma quantifies how this logit perturbation propagates to the probability distribution. Lemma 3.1. Given logit perturbation δz = ε ek on k-th token ak in the vocabulary, the resulting first-order change in the probability distribution is given by: δpk = εpk(1pk) and δpi = εpipk, [V ], = k. (2) Proof. The Jacobian of the softmax function is pi = zj pi (1{i = j} pj), where 1{} is the indicator function. The first-order change δpi is given by the Taylor expansion δpi = (cid:80)V δzj + O(ε2). Since δzj = ε 1{j = j=1 k}, [V ], we have δpi = pi ε = ε pi(1{i = k} pk), zk which yields the results in (2). pi zj An immediate consequence of Lemma 3.1 is that the relative change in probability is uniform for all unperturbed tokens. Based on (2), the changes in probabilities are given by: δpk pk = ε(1 pk) and δpi pi = εpk, [V ], = k. (3) The analysis shows that, when the probability of token ak is adjusted, its probability mass is redistributed proportionally from (or to) all other tokens. This aligns with the observation in previous works (Ren & Sutherland, 2025). Building upon this insight, we can now derive closed-form expression for the first-order change in entropy. We first define key quantity that would determine the direction of this change. Let the entropy change discriminator for token on position be defined as St i), where i(H + log pt pt the subscript is omitted when not causing confusion. In particular, assuming token ak is chosen at this position, the corresponding discriminator is denoted as St Theorem 3.2. The first-order change in entropy, denoted by H, under the perturbation δz = εek is given by: St k. = εS + O(ε2). (4) Proof. The first-order Taylor expansion of entropy around can be given by: = H(p+δp)H(p) = (cid:88) i=1 pi δpi +O(δp2). (5) Since pi implies (cid:80) = (1 + log pi) and conservation of probability δpi = 0, we have: = = (cid:88)V i=1 (cid:88)V i=1 (1 + log pi)δpi + O(ε2) log piδpi + O(ε2). (6) (7) Substituting the expressions for δpi from Lemma 3.1, can be simplified as: H=εpk (cid:0)(1 pk) log pk (cid:0)H + log pk (cid:1) + O(ε2), = ε pk (cid:88) i=k pi log pi (cid:1) + O(ε2) which completes the proof. Implications Theorem 3.2 provides simple yet effective criterion for determining how single-token update affects policy entropy. The direction of entropy change is dictated by the sign of two factors: the update direction ε and the discriminator S. The sign of the discriminator depends on the relationship between the tokens probability pk and the overall entropy H(p): sign(S) = sign (H(p) + log pk) = sign(pk eH(p)). Consequently, rewarding token (sign(ε) = +1) increases entropy if its probability pk < eH(p) (a relatively lowprobability token) and decreases entropy if pk > eH(p) (a relatively high-probability token). The relationship is reversed when token is penalized (sign(ε) = 1). This microscopic analysis is the foundational building block for understanding entropy dynamics in RFT. Given that most existing RFT algorithms (Shao et al., 2024; Yu et al., 2025; Zheng et al., 2025) apply an update signal of the same direction to all tokens within single response, our analysis explains the common empirical observation of rapid entropy collapse when models are consistently rewarded for generating high-probability and safe responses (He et al., 2025), which can lead to gradual loss of the models exploratory capabilities. 3 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models 3.2. Extension to GRPO Optimization Step Beyond the above single-logit analysis, we extend our framework to model the entropy change resulting from GRPO optimization step introduced in Section 2. Recall the training objective function of GRPO in equation 1, for chosen token ak with token id k, its contribution to the whole training target can be given as: pk , where pk denotes the current model distribution in the sampled position, is the probability of the sampled token under the sampling model distribution and represents its advantage. Therefore, its contribution to the training loss is given by surrogate loss: L(z) = log pk(z), (8) where = πθ(ak) πθsample (ak) is the importance sampling ratio. single gradient update step with learning rate η results in first-order change to the logits z: δz = η zL = α log pk , (9) where we define α = ηrA as the effective step size. Recall the Jacobian of the softmax function in Lemma 3.1, zpk = pk(ek p). Therefore, we have: δz = α log pk = α 1 pk zpk = α(ek p). (10) Theorem 3.3. Let Si be the entropy discriminant for token i, and let its expectation over the policy distribution be Eip[Si] = (cid:80)V i=1 piSi. The first-order change in entropy of token H(p) satisfies: = α (S Eip[Si]) + O(α2). (11) Proof. Recall the token-wise objective defined in (8) and single update step defined in (9). Since = softmax(z), its Jacobian matrix is = z = diag(p) pp, yielding the following equations: δp = δz = (cid:0)diag(p) pp(cid:1) α(ek p) = α (cid:2)p (ek p) (pk p2 2)(cid:3) , and δpi = α (cid:2)pi(1{i = k} pi) pi(pk p2 2)(cid:3) . As the first-order entropy change is given in (5), we substitute δpi and apply (cid:80) pi log pi = to (5), which yields: H= α(cid:2)(cid:88) (H +log pi)pk(H +log pk)(cid:3) + O(α2) p2 = α [S Eip[Si]] + O(α2). The proof is completed by applying the definition of Si and Eip[Si]. Implications Theorem 3.3 reveals crucial distinction from the single-logit case. With GRPO optimization step, the entropy change is no longer governed by the absolute value of the entropy discriminant score S, but by its deviation from the policy-weighted expectation Eip[Si], which acts as dynamic baseline. Entropy decreases if we reward (positive α) token ak whose score is above the baseline, and it increases when is below the baseline. The relationship can be reversed when we penalize (negative α) token. Since α = ηrA, with η on the order of 106, clipped near 1, and typically O(1) due to within-group standardization, we have α 1. Therefore, the O(α2) terms are negligible and the first-order approximation in Theorem 3.3 is accurate in practice. Moving step forward, we provide two corollaries derived from Theorem 3.3. Corollary 3.4. To first-order approximation, with onpolicy sampling, the expected entropy change factor Sk Eip[Si] of token within GRPO optimization is zero, i.e., Ekp (cid:2)S Eip[Si](cid:3) = 0. (12) Corollary 3.5. For on-policy GRPO training with batch, Eipt[St the expected value of entropy change factor St ] over the batch of tokens TB is zero: EtTB (cid:2)St Eipt[St ](cid:3) = 0. (13) We provide the proofs for these two corollaries in Appendix A. These corollaries demonstrate that, from both the vocabulary and batch perspectives, the discriminant score possesses favorable decentralization property under on-policy sampling. Therefore, imposing constraints on tokens based on the value of relative to its expectation offers simple and direct approach to regulating entropy dynamics. Based on such analysis, we propose two methods for constraining entropy in Section 4.1. Considering the potential distribution of the advantage term under model sampling or the statistical distribution across multiple tokens in training batch, these two corollaries can be further extended to incorporate the complete expectation, including the advantage term. Detailed discussions are provided in Appendix C, which help further understand the causes of entropy collapse in RFT. 4. Bridging Entropy Dynamics to Entropy"
        },
        {
            "title": "Control Methods",
            "content": "4.1. Entropy Discriminator Guided Clipping The theoretical analysis provides novel insights into the relationships between the discriminator score St and the entropy dynamics in RFT. Upon this, we can effectively 4 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models identify tokens within training batch that exert disproportionate impact on entropy changes, enabling us to selectively mitigate the influence of such outlier tokens for achieving fine-grained and flexible control over the entropy regularization throughout the training process. Inspired by Theorem 3.2, we propose simple yet effective batch-level clipping method. (ClipB: Algorithm 4.1. Discriminator Clipping): Batch-Normalized EntropyLet TB denote the set of all tokens in the responses of given batch B. We first compute the batch-wise mean of the discriminator scores, = EtTB [St ], and the corresponding standard deviation, σ = (cid:112)VartTB [St ]. During the RFT process, we only preserve the gradients associated with those tokens that satisfy specific condition by applying the following mask mt to each token t: mt = 1 (cid:8)µσ St µ+σ(cid:9) . (14) Here µ+ and µ are used to control the clipping threshold based on the degree of outlierness. This algorithm identifies the effects of each token on entropy change and filters out tokens that contribute to severe fluctuations in H. This is accomplished by simply examining the logits of response tokens, combined with the proposed batch-level normalization. This operation requires minimal computation on scalar values rather than high-dimensional tensors, thus introducing negligible additional computational cost and allowing for easy integration into existing training frameworks. Moreover, Theorem 3.3 provides more precise characterization of the entropy change, particularly in the context of GRPO. This analysis motivates us to derive the vocabularynormalized entropy-discriminator clipping method. Algorithm 4.2. (ClipV : Vocabulary-Normalized EntropyDiscriminator Clipping): = St Eipt[Si For each token in batch TB, we first define its vocabularycentered score as St t], where pt is the policys predictive distribution over the vocabulary at step t. We then compute the standard deviation of these centered scores in batch: σ = (cid:112)VartTB [St c]. As established in Corollary 3.5, the batch-mean of these centered scores, EtTB [St c], approximates zero. This simplifies the clipping condition. The mask for token is thus defined as: calculations. This allows us to evaluate the expectation at relatively low additional cost. In Section 5.3, we provide empirical studies on the effectiveness of ClipB and ClipV . 4.2. Interpreting Existing Methods through Entropy Dynamics Recent works have proposed various entropy-based methods to enhance training stability and effectiveness. These methods, however, are often developed from heuristic principles and can necessitate labor-intensive hyperparameter tuning without clear theoretical guidance. To provide better understanding of their underlying mechanisms, we re-examine these methods through the lens of our entropy dynamics analysis (refer to Section 3). We categorize the related studies into three groups: (i) Clipping Mechanisms, which stabilize the optimization by constraining the updates of token probability. Representative works include the clipping operation in GRPO (Guo et al., 2025), the clip-higher method in DAPO (Yu et al., 2025) and the separate clipping mechanism in CE-GPPO (Su et al., 2025). (ii) Entropy Regularization, which regularizes updates to tokens with high entropy, as proposed by Wang et al. (2025). (iii) Probability Weighted Updating, which constrains the updates based on token probabilities, exemplified by methods from (He et al., 2025; Yang et al., 2025b). Before we conduct the investigation and interpretation, lets first recall Theorem 3.3 and examine, from statistical perspective, the relationship it reveals between two factors often considered by the methods above, i.e., probability and entropy. The first term in Theorem 3.3, = pk(H + log pk), directly associates these two factors. For tokens sampled with high probability, tends to be larger; similarly, tokens sampled in positions with high entropy also have larger S. Tokens sampled with larger are more likely to obtain positive value when calculating the deviation from the expectation Eip[Si], as the expectation represents an average value under the current models sampling distribution. As result, when considering entropy change, for positive samples, higher probability and lower token entropy are often associated with decrease in entropy. In contrast, lower probability and higher entropy are often linked to an increase in entropy. For negative samples, the trend reverses. mt = 1 (cid:8)µσ St Eipt[St ] µ+σ(cid:9) . (15) Clipping Mechanisms Clipping in GRPO can be formulated as gradient mask for the t-th token in response i: In ClipV , the computation of Eipt[St ] introduces some computational overhead. Fortunately, the quantities required to compute this term, such as the policys logits over the full vocabulary, are often available as intermediate results from the forward pass used for entropy and log-probability Mi,t = 1{Ai > 0, ri,t(θ) 1 + ϵhigh} + 1{Ai < 0, ri,t(θ) 1 ϵlow}, (16) where ri,t(θ) = πθ(oi,tq,oi,<t) πsample(oi,tq,oi,<t) is the importance ratio. The clipping mechanism prevents an excessive increase in 5 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models probability for tokens in positive samples and an excessive decrease for tokens in negative samples. Due to the nature of the importance ratio, this mechanism predominantly affects tokens with low probabilities under the sampling policy. Empirical statistics from (Yu et al., 2025) show that clipped tokens typically have maximum probability of around 0.15. Across training trajectory where overall token entropy declines. As we analyzed above, these low-probability tokens are associated with the condition E[Si] < 0. For these tokens, the sign of the entropy change is given by sign(H) = sign(ε) sign(S Eip[Si]) = sign(ε). Consequently, updates on positive samples (sign(ε) = +1) tend to increase token entropy, while updates on negative samples (sign(ε) = 1) tend to decrease it. The overall entropy dynamics is superposition of these two effects: in most cases, it manifests as rapid decline in entropy, while in others, it exhibits complex fluctuations (Liu et al., 2025). The clip-higher method in DAPO, which sets larger ϵhigh for positive samples, corresponds to this insight. By relaxing the clipping constraint for positive samples, it preserves their entropy-increasing updates. This targeted intervention counteracts the tendency of entropy decrease during RFT, thereby promoting more exploration and better performance. Consistent with our theoretical framework, Su et al. (2025) empirically demonstrates that high-probability positive tokens and low-probability negative tokens tend to suppress exploration, whereas low-probability positive tokens and high-probability negative tokens encourage exploration. Entropy Regularization Entropy regularization refers to methods that compute gradients only for certain proportion of tokens with high entropy. For example, Wang et al. (2025) demonstrates improved performance by applying updates to only the top 20% of tokens with the highest entropy. As we analyzed above, high token entropy corresponds to condition where our theoretical quantity Eip[Si] is likely to be positive. According to Theorem 3.3, for these tokens, the updates on positive samples would decrease entropy, while updates on negative samples would increase it. The net effect on entropy is therefore determined by the balance between these two opposing forces. The empirical results in (Wang et al., 2025), which show that as the proportion of selected high-entropy tokens is varied, the overall entropy first increases and then decreases relative to baseline, provide strong evidence for this trade-off. Probability Weighted Updating Similar to entropy regularization, Probability Weighted Updating methods constrain or scale token updates based on their probabilities. For example, He et al. (2025) proposes to assign higher weights to positive samples with low probability. In the context of our analysis, low-probability tokens are associated with Eip[Si] < 0. When these tokens are part of positive sample, the expected change in entropy is positive. By amplifying the updates for this specific subset of tokens, the method explicitly promotes gradients that increase token entropy, alleviating the entropy collapse issue. The provided experimental results support this conclusion. In summary, our analysis offers unified view for understanding the mechanics of existing methods, which function by amplifying the effects of tokens contributing to entropy increase or suppressing those leading to entropy decrease, thereby preventing entropy collapse in RFT. 5. Experiments 5.1. Settings We select the Qwen2.5-7B-Instruct and Qwen2.5-14BInstruct (Yang et al., 2024) as our base models for RFT, utilizing the DAPO-Math-17k dataset (Yu et al., 2025) as our training set. Following previous studies (Lightman et al., 2023), we exclude 500 questions from the training set to form the validation set (denoted by DAPO500). We filter out samples from the training set with excessively high ( 15/16) or low ( 1/16) pass rates, as evaluated by Qwen2.5-7B-Instruct. For evaluation, we adopt two challenging mathematical datasets, i.e., AIME24 and AIME25, to form our test set. We adopt the Avg@32/Pass@32 evaluation metrics for AIME24 and AIME25, and Avg@8/Pass@8 for DAPO500. Here Avg@K denotes the average accuracy across responses for each question, while Pass@K represents the probability that at least one of responses is correct. 5.2. Empirical Observations of the Entropy Dynamics We first provide empirical evidence supporting Theorem 3.2, which posits close relationship between the discriminator score and the direction of change in token entropy, i.e., sign(H). Specifically, during the training process, we selectively update the loss associated with tokens exhibiting > 0 or < 0. The standard training process serves as our baseline for comparison. For clear observations, we apply these selective updates to positive (rewarding) and negative (punishing) samples separately, presenting the results in Figures 1(a) and 1(b), respectively. These results align with our analysis. For example, in Figure 1(a), when we only retain updates for tokens with > 0 in positive samples, we observe decrease in entropy consistent with sign(H) = sign(ε) sign(S) < 0. Conversely, retaining updates for tokens with < 0 induces an increment in entropy. Such phenomenon is precisely reversed in Figure 1(b) as we apply these operations to negative samples. To further probe this relationship, we investigate practical scenario where we mask the gradients of tokens that satisfy 6 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models Figure 1. We retain or mask the gradients of tokens satisfying > 0 or < 0, respectively. The resulting entropy changes are shown in (a,c) for positive samples, and (b,d) for negative samples. Figure 2. The effects of ClipB and ClipV with different µ in controlling clip fraction and entropy. specific conditions during the training process. Similarly, as shown in Figure 1(c), when the gradients associated with tokens in positive samples that satisfy > 0 are masked (these are believed to contribute to entropy decrease), the entropy increases uncontrollably. Conversely, masking the gradients of tokens that satisfy < 0 leads to continuous decrease in entropy. Figure 1(d) illustrates that performing the same masking operations on negative samples results in the opposite behavior. These experimental results further confirm our analysis, which suggests that the sign of the discriminator score is reliable predictor of tokens influence on entropy dynamics within RFT. In Figure 3, we illustrate the distribution of within training batch and its deviation from its sampling expectation, as involved in Theorem 3.3. The value of EtTB [St Eip[St ]] is three orders of magnitude smaller than that of EtTB [St ], and approaches zero, effectively validating Corollary 3.5. 5.3. Effects of Entropy Discriminator Clipping Methods In this subsection, we validate the effectiveness of the clipping methods proposed in Section 4.1, including ClipB and ClipV . Considering that entropy empirically exhibits clear decreasing trend within RFT, we choose negative samples as the primary focus and apply our clipping methods to mask the losses of specific tokens. As shown in Figures 2(a) and 2(b), the hyper-parameter µ in the ClipB and ClipV methods provides effective control over the number of clipped tokens (a larger µ indicates smaller clip proportion), thereby supporting flexible adjustment of the intervention intensity. In Figures 2(c) and 2(d), we illustrate the effects of the clipping methods in controlling entropy with different values of µ. We observe that both ClipB and ClipV successfully mitigate the entropy decay to excessively low levels, as in the baseline (the standard RFT training). Existing RFT studies (Yu et al., 2025; Liao et al., 2025) suggest that maintaining certain level of entropy can retain the models exploration capabilities, leading to better model performance. Therefore, we validate the performance of models trained using ClipB and ClipV , as summarized in Table 1. These results demonstrate that both ClipB and ClipV achieve outperformance compared to standard GRPO across various datasets, confirming their effect in preserving model exploration by controlling entropy. Besides, we also extend the evaluation across two dimensions: the training algorithm (by integrating with PPO) and model diversity (including Qwen3-4B-Base, DeepSeekDistilled-Llama3-8B, and InternLM3-8B). As shown in Appendix E, the consistent performance gains across these settings confirm that our methods are effective at preserving model exploration and enhancing model performance. 5.4. Analysis of Exploration versus Exploitation In Table 1, we compare model performance using Pass@K and Avg@K. significant gain in Pass@K indicates that the model can generate diverse responses for solving problems (exploration), whereas improvements in Avg@K primarily reflect exploitation of similar high-reward patterns. The experimental results demonstrate that our methods not only achieve significant improvements in both Pass@K and Avg@K across all datasets. These results confirm that sta7 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models Table 1. Comparison of vanilla GRPO and our methods on Avg@K and Pass@K (K=32 for AIME24/25 and K=8 for DAPO500)."
        },
        {
            "title": "Method",
            "content": "AIME24 AIME25 DAPO500 Avg@K Pass@K Avg@K Pass@K Avg@K Pass@K Qwen2.5-7B-Inst 11.35 36. 6.67 33.33 31.55 70.2 GRPO GRPO+ClipB GRPO+ClipV Qwen2.5-14B-Inst GRPO GRPO+ClipB GRPO+ClipV 16.88 19.69 (+2.81) 18.12 (+1.24) 50.00 56.67 (+6.67) 53.33 (+3.33) 15.42 16.35 (+0.93) 15.94 (+0.52) 50.00 53.33 (+3.33) 56.67 (+6.67) 48.03 49.68 (+1.65) 49.65 (+1.62) 76.8 80.2 (+3.4) 79.0 (+2.2) 12.14 41.67 11.72 38.33 40.22 74. 22.50 23.33 (+0.83) 23.44 (+0.94) 66.33 66.67 (+0.34) 66.67 (+0.34) 17.60 20.62 (+3.02) 21.35 (+3.75) 50.00 56.67 (+6.67) 56.67 (+6.67) 52.95 60.35 (+7.40) 61.92 (+8.97) 84.0 85.6 (+1.6) 86.6 (+2.6) Figure 3. The batch-averaged value of and Eip[Si]. Figure 4. Comparison between ClipB and vanilla GRPO on the distribution of problem pass rates. bilizing entropy with the proposed clipping method fosters greater solution diversity and encourages the model to discover correct reasoning paths for wider array of problems. Moreover, we further conduct study on the distribution of pass rates among multiple rollouts for individual problems. Taking the Qwen-2.5-7B-Instruct model and the ClipB method as an example, we illustrate the results in Figure 4. For the standard GRPO, the proportion of problems that are completely solved or completely failed is significantly higher than that of ClipB. This indicates that GRPO excessively prioritizes exploitation while neglecting the importance of exploration. Conversely, ClipB focuses more on encouraging exploration, resulting in pass rate distribution that is more concentrated around the middle range. This suggests that the performance gains achieved by our method stem from encouraging the model to explore solutions for broader range of problems, rather than simply memorizing easier problems that could be solved with higher certainty. 6. Related Works Reinforcement fine-tuning (RFT) has been widely adopted in tuning LLMs, including representative methods such as GRPO (Guo et al., 2025), DAPO (Yu et al., 2025), and GSPO (Zheng et al., 2025). To enhance LLM performance, many strategies have been proposed from diverse aspects (Liu et al., 2025; Hu, 2025; Yu et al., 2025). Among these works, some tricks are developed based on the influence of entropy on model behavior, such as explicitly entropy regularization (Hu et al., 2025), entropy-based token selection (Wang et al., 2025), flexible clipping schemes (Yu et al., 2025; Su et al., 2025), and others (Liao et al., 2025). Recent work (Cui et al., 2025) linked entropy changes to model sampling distributions and modeled the performanceentropy relationship, highlighting the importance of studying entropy dynamics in RFT. While establishing solid theoretical foundation, it relies on the advantage of unsampled tokens, which is difficult to estimate in most RFT algorithms and presents challenges for practical application. 7. Conclusions In this study, we focus on theoretical framework to provide principled understanding of entropy dynamics in RFT. We quantify the entropy change and further extend this analysis to practical GRPO optimization step, revealing that entropy fluctuations arise from the combined effect of tokens update direction, its probability, and the policy entropy. These insights offer explanations for the commonly observed entropy collapse phenomenon, guide the development of entropy controlling strategies, and unify the interpretation of existing entropy-based methods. We hope that the theoretical framework can foster clear understanding of the underlying mechanisms of entropy dynamics in RFT, thereby accelerating progress in the field. 8 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models"
        },
        {
            "title": "References",
            "content": "Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Ahmed, Z., Le Roux, N., Norouzi, M., and Schuurmans, D. Understanding the impact of entropy on policy optimization. In International conference on machine learning, pp. 151160. PMLR, 2019. An, C., Xie, Z., Li, X., Li, L., Zhang, J., Gong, S., Zhong, M., Xu, J., Qiu, X., Wang, M., and Kong, L. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https: //hkunlp.github.io/blog/2025/Polaris. Arulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath, A. A. Deep reinforcement learning: brief survey. IEEE signal processing magazine, 34(6):2638, 2017. Chen, Y., Yang, Z., Liu, Z., Lee, C., Xu, P., Shoeybi, M., Catanzaro, B., and Ping, W. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Feng, J., Huang, S., Qu, X., Zhang, G., Qin, Y., Zhong, B., Jiang, C., Chi, J., and Zhong, W. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, A., Fried, D., and Welleck, S. Rewarding the unlikely: Lifting grpo beyond distribution sharpening. arXiv preprint arXiv:2506.02355, 2025. Hu, J. Reinforce++: simple and efficient approach arXiv preprint for aligning large language models. arXiv:2501.03262, 2025. Hu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H.-Y. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Liao, M., Xi, X., Chen, R., Leng, J., Hu, Y., Zeng, K., Liu, S., and Wan, H. Enhancing efficiency and exploration in reinforcement learning for llms. arXiv preprint arXiv:2505.18573, 2025. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2023. Liu, Z., Liu, J., He, Y., Wang, W., Liu, J., Pan, L., Hu, X., Xiong, S., Huang, J., Hu, J., et al. Part i: Tricks or traps? deep dive into rl for llm reasoning. arXiv preprint arXiv:2508.08221, 2025. Luo, M., Tan, S., Huang, R., Patel, A., Ariyak, A., Wu, Q., Shi, X., Xin, R., Cai, C., Weber, M., Zhang, C., Li, L. E., Popa, R. A., and Stoica, I. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. Notion Blog. OpenAI. Reinforcement https://platform.openai.com/docs/ guides/reinforcement-fine-tuning, 2025. Accessed: 2025-09-10. fine-tuning guide. Pan, X., Chen, Y., Chen, Y., Sun, Y., Chen, D., Zhang, W., Xie, Y., Huang, Y., Zhang, Y., Gao, D., et al. Trinity-rft: general-purpose and unified framework for reinforcement fine-tuning of large language models. arXiv preprint arXiv:2505.17826, 2025. Ren, Y. and Sutherland, D. J. Learning dynamics of llm finetuning. In The Thirteenth International Conference on Learning Representations, 2025. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shannon, C. E. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Su, Z., Pan, L., Lv, M., Li, Y., Hu, W., Zhang, F., Gai, K., and Zhou, G. Ce-gppo: Controlling entropy via gradientpreserving clipping policy optimization in reinforcement learning, 2025. URL https://arxiv.org/abs/ 2509.20712. Sutton, R. S., Barto, A. G., et al. Reinforcement learning: An introduction. MIT press Cambridge, 1998. Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X., Yang, J., Zhang, Z., et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models Wei, Y., Duchenne, O., Copet, J., Carbonneaux, Q., Zhang, L., Fried, D., Synnaeve, G., Singh, R., and Wang, S. I. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, Z., Luo, X., Wang, Z., Han, D., He, Z., Li, D., and Xu, Y. Do not let low-probability tokens over-dominate in rl for llms. arXiv preprint arXiv:2505.12929, 2025b. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. DAPO: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Zeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J., et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Zhang, S., Dong, Y., Zhang, J., Kautz, J., Catanzaro, B., Tao, A., Wu, Q., Yu, Z., and Liu, G. Nemotron-researchtool-n1: Exploring tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 10 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models A. Proof of Corollaries Corollary 3.4. To first-order approximation, with on-policy sampling, the expected entropy change of token within GRPO optimization is zero, i.e. Proof. We derive the results as follows: Ekp (cid:2)S Eip[Si](cid:3) = 0. Ekp (cid:2)S Eip[Si](cid:3) = Ekp (cid:34) pk (cid:0)H + log pk (cid:1) (cid:35) (cid:0)H + log pi (cid:1) p2 (cid:88) i=1 p2 (cid:0)H + log pk (cid:1) (cid:88) i=1 (cid:0)H + log pi (cid:1) p2 = (cid:88) k=1 = 0. Corollary 3.5. For on-policy GRPO training with batch, the expected value of entropy change factor St over the batch of tokens TB is zero: Eipt[St ] EtTB (cid:2)St Eipt[St ](cid:3) = 0. (17) Proof. For each token TB, let pt = (p1 := pi Si (cid:0)Ht + log pi (cid:1). Draw the action index on-policy: Kt Cat(cid:0)pt , . . . , pV (cid:1). Then, conditioning on pt, ) be the on-policy token distribution, Ht = (cid:80)V i=1 pi log pi t, and (cid:104) SKt (cid:105) (cid:12) (cid:12) (cid:12) pt = (cid:88) i=1 Si pi = (cid:88) (pi t)2(cid:0)Ht + log pi (cid:1) = Eipt[Si t]. i= (cid:104) Hence SKt Eipt[Si t] (cid:105) (cid:12) (cid:12) (cid:12) pt = 0 for each token t. Averaging over the batch and using linearity of expectation, (cid:34) 1 TB (cid:88) (cid:16) tTB SKt Eipt[Si t] (cid:17) (cid:35) {pt}tTB = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 TB (cid:88) tTB 0 = 0. Finally, applying the tower property removes the conditioning and yields the stated result. B. Detailed Experiment Setup All experiments are conducted on NVIDIA A100 and H20 GPUs. We implement the experiment with the Trinity-RFT (Pan et al., 2025) framework. For the training process, we adopt the Adam optimizer with hyperparameters (0.9, 0.999). We set the training batch size to 64, the number of rollouts to 16 for Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct and 8 for other models, and employ learning rate of 4 107. The temperature is set to 1.0 for sampling rollouts and 0.7 for evaluation. Reward design The reward for each response is determined by its answer correctness, i.e., (1) ri = 1 if the answer and format are correct; (2) ri = 0, otherwise. C. Extension to Advantage-Aware Analysis In this section, we extend the findings of Corollary 3.4 and Corollary 3.5 to incorporate advantage estimation. We analyze the expectations of the full entropy change expression presented in Theorem 3.3 from two complementary perspectives: model sampling and batch averaging. 11 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models C.1. Extension of Corollary 3.4 to Model Sampling We assume that at each position t, every token ID in the vocabulary has latent advantage value, i.e., = A(i) for pt. Building upon Theorem 3.3, we derive the following corollary regarding the expected entropy change. Corollary C.1. For on-policy GRPO training, the first-order expectation of the token-wise entropy change is given by: Ekp[H] = η Covkp(A, Eip[Si]). (18) Proof. We begin by applying the result from Theorem 3.3. Recall that α = ηrA. Considering the constant η and = 1 in the on-policy setting, the first-order expectation of token entropy change under on-policy sampling can be given by: Ekp[H] = ηEkp (cid:2)A(S Eip[Si])(cid:3). (19) We decompose the covariance in this equation, which gives: Ekp[H] = η(cid:8)Ekp[A]Ekp[S] + Covkp(A, S) Ekp[A]Ekp[Eip[Si]] Covkp(A, Eip[Si])(cid:9). By Corollary 3.4, we have Ekp[S] Ekp[Eip[Si]] = 0. Substituting this into the equation above, we have: Ekp[H] = η Covkp(A, Eip[Si]). Implications. Corollary C.1, based on the on-policy policy gradient formula, provides clean expression for entropy change. It decouples the advantage from the core entropy change term Eip[Si], and establishes their relationship through covariance. It is worth noting that, In GRPO, the advantage for tokens not actually sampled is undefined and non-computable; therefore, Corollary C.1 cannot be directly applied in algorithmic implementation. Nevertheless, it offers theoretical potential for understanding entropy collapse in the GRPO training process. In GRPO, the way advantages are obtained is coupled with the policy model distribution, which promotes entropy collapse. We will verify this hypothesis from batch-level perspective in the next subsection. C.2. Extension of Corollary 3. The batch-level entropy is defined by the arithmetic average of token entropies within batch: HTB = 1 TB (cid:88) tTB Ht Therefore, batch-level entropy change is also the arithmetic average of token entropy change: HTB = 1 TB (cid:88) tTB Ht (20) Based on the above definition, we derive the following corollary: Corollary C.2. For on-policy GRPO training with batch, the first-order batch-wise entropy change of tokens TB is given by: Proof. Applying the results in Theorem 3.3 into equation 20 gives: HTB = η CovB(A, Eip[Si]). HTB = 1 TB (cid:88) tTB α(S Eipt[St ])] = EB[α(S Eip[Si])], (21) (22) On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models where EB refers to statistical expectation (i.e., an arithmetic average over batch B). Recall the definition of α = ηrA: the learning rate η is constant within batch; is constantly 1 in the on-policy setting. The advantage estimated in the GRPO algorithm is NOT independent of the chosen token id in batch TB, i.e., HTB = η EB[A(S Eip[Si])]. We further apply covariance decomposition to HTB within training batch: HTB /η = (cid:8)EB[A]EB[S] + CovB(A, S) EB[A]EB[Eip[Si]] CovB(A, Eip[Si])(cid:9). According to Corollary 3.5, we haveEB[S] EB[Eip[Si]] = 0, which gives: Finally, we multiply both sides of the above equation by η to complete the proof. HTB /η = CovB(A, Eip[Si]). (23) Implications. Corollary C.2 provides computable form analogous to Corollary C.1 from the batch perspective. We conduct experiment to monitored the quantity CovB(A, Eip[Si]) during training. As shown in Figure 5, its value has larger magnitude in the negative portion compared with the positive ones. This observation further validates the hypothesis in Appendix C.1. The model tends to obtain correct answers (i.e., > 0) by producing safe responses, those with relatively high probability, for which E[Si] tends to be positive, whereas exploratory behaviors are more likely to yield incorrect answers. This dynamic continually suppresses the models propensity to explore diverse answers. Algorithm 2 directly computes the factor Eip[Si], and masks those who contribute extremely significantly to the covariance expression. For example, for negative samples where < 0, Theorem 3.3 masks those tokens with large negative Eip[Si], who contributes large negative factor to equation 20, stabling the change of entropy. Algorithm 1 estimates this factor in batch perspective, achieving better computational efficiency. Discussions about Parameter Sharing. In Corollary C.2, the parameter updates induced by different tokens are linearly superimposed. While it is worth noting that in practical LLM training, parameters are shared across tokens and the global update dynamics involve complex coupling effects, establishing rigorous theoretical model for such high-dimensional parameter interference remains an open challenge in the field of machine learning theory. Following the context of previous research (Yu et al., 2025; He et al., 2025; Su et al., 2025; Ren & Sutherland, 2025; Cui et al., 2025; Liao et al., 2025; Wang et al., 2025), our framework focuses on the microscopic atomic unit of this process, the single-token update, which serves as the fundamental building block of the global dynamics. In standard first-order optimization (e.g., SGD or Adam), the total gradient is the accumulation of individual token gradients. Under the regime of small learning rates characteristic of fine-tuning, the superposition of these single-token effects constitutes the dominant factor driving the entropy dynamics, while higher-order inter-token coupling effects are implicitly handled by the optimizer. Our empirical observations in Figure 1 and Figure 5 strongly corroborate this: the entropy shifts trend predicted by our decoupled single-token analysis (S) accurately match the actual batch-wise training dynamics, and the overall trend of entropy change in standard RFT are correctly predicted, suggesting that the first-order approximation effectively captures the primary mechanism of entropy evolution despite the underlying parameter sharing. D. Extension to off-policy scenarios The derivation of Theorem 3.3 is based on the general GRPO formulation and is not restricted to the on-policy setting. In this section, we extend Corollaries 3.4, 3.5, C.1 and C.2 to the off-policy scenario. When off-policy sampling is used, similar expressions can be obtained by utilizing the importance ratio = πθ/πθsample. Corollary 3.4.1. To first-order approximation, the expected entropy change factor r(S Eip[Si]) of token within GRPO optimization is zero, i.e., Ekp [r(S Eip[Si])] = 0, where and denote the sampling policys and the current policy models output distributions at token t, respectively. 13 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models Figure 5. The value of CovB(A, Eip[Si]). Proof. We derive the results as follows: Ekp [r(S Eip[Si])] = Ekp (cid:40) (cid:104) pk(H + log pk) (cid:105) p2 (H + log pi) (cid:41) (cid:88) i= = (cid:88) k=1 pk p kpk(H + log pk) (cid:88) i=1 (cid:0)H + log pi (cid:1) p2 (cid:88) k= pk p (cid:88) (cid:88) pk) k= i=1 (cid:0)H + log pi (cid:1) p2 = (1 = 0. Corollary 3.5.1. For on-policy GRPO training with batch, the expected value of entropy change factor r(St over the batch of tokens TB is zero: Eipt[St ]) EtTB [r(St Eipt[St ])] = 0. (24) Proof. For each token in Batch TB, define pt as the distribution of the current policy, and sampling policy. Considering one time step t, the selected token Kt follows the distribution importance ratio of token Kt can be expressed as = pt(Kt) distribution as the distribution of the t, i.e., Kt t. The t(Kt) . The conditional expectation of each term under the sampling is then given by: Eipt[St We expand this expression according to the definition of expectation: EKtp (cid:2)r (St ]) pt, (cid:3) . EKtp (cid:2)r (St Eipt[St ]) pt, (cid:88) (cid:3) = kV (cid:88) = t(k) pt(k) t(k) pt(k) (cid:0)St ](cid:1) Eipt[St (cid:0)St ](cid:1) Eipt[St kV = Ekpt (cid:2)St Eipt[St ](cid:3) . According to Corollary 3.4, under the current policy distribution pt, the expectation of the difference between the discriminator score and its expectation is 0: Ekpt[St k] Ekpt[Eipt[St ]] = Eipt [St ] Eipt[St ] = 0. Therefore, for any token Kt in the Batch, the expected conditioned value of the entropy change factor is 0: EKtp (cid:2)r (St Eipt[St ]) pt, (cid:3) = 0 (25) 14 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models Finally, taking the mean over Batch TB utilizing the linearity of expectation and tower property: EtTB [r(St Eipt[St ])] = EKtp (cid:104) 1 (cid:88) TB tTB r(SKt Eipt[Si t] (cid:12) (cid:105) (cid:12) (cid:12) {pt}tTB ) = 1 TB (cid:88) tTB 0 = 0. To extend the off-policy version of Corollaries C.1 and C.2, we leverage similar methods in proving Corollaries 1.1 2.1, i.e., replacing the results in Corollaries 3.4 and 3.5 with their off-policy counterparts. The following corollaries show the off-policy extensions of Corollaries C.1 and C.2. Corollary C.1.1. During GRPO, the first-order expectation of token entropy change is given by: where and denote the sampling policys and the current policy models output distributions at token t, respectively. Ekp[H] = η Covkp(A, r(S Eip[Si])), (26) Corollary C.2.1. Within an GRPO training batch, the first-order expectation of entropy change is given by: HTB = η CovB(A, r(S Eip[Si])). (27) E. Supplemental results of the experiment E.1. Detailed training Curves The training dynamic of average@K accuracy and Entropy in Table 1 are provided in Figure 6. E.2. Experiments with PPO We provide simple demonstration with PPO on Qwen2.5-7B-Instruct model. We directly apply the GAE Advantage from PPO as the criterion for determining the token optimization direction in our algorithms, i.e. δt = rt + γVt+1 Vt , At = δt + (γλ)At+1 , where denotes the state value assigned by critic model, γ denotes the discount factor and λ represents smoothing parameter. As simple and direct application to PPO, our methods achieve significant improvements, as shown in table 2. Table 2. Experiment results of ClipB/ClipV with PPO training algorithm. Method AIME24 AIME25 DAPO500 Vanilla PPO ClipB ClipV 16.15 16.56 17.60 13.75 15.31 15.21 40.98 46.12 44. We believe this result convincingly demonstrates the potential of our work to be applied across various policy gradient methods and highlights that developing entropy control methods tailored for different RFT algorithms based on the entropy dynamics is promising direction for future work. E.3. Experiments with More Models We conduct additional experiments on Qwen3-4B-Base (hereafter mentioned as Qwen3), DeepSeek R1-Distill-llama-8BInstruct (hereafter mentioned as Distilled-Llama), and InternLM3-8B-Instruct (hereafter mentioned as InternLM). The average@K performance of the models is listed in Table 3, and the training dynamics are provided in Figure 7. 15 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models Figure 6. Full curves of performance and entropy for different models. As listed in Table 3, our methods outperform baselines in most scenarios, demonstrating that their effectiveness in encouraging exploration and improving model performance can generalize across different models. The training dynamics exhibited in Figure 7 vary across different models. For Qwen3, the training dynamics are similar to those of the Qwen2.5 series models. Our methods effectively alleviate the entropy collapse phenomenon. For DistilledLlama, although the models training dynamics differ significantly from those of Qwen, our method still demonstrates strong entropy-stabilizing properties and achieves competitive model performance. In the training of InternLM, our method demonstrates useful benefits in stabilizing training. Despite employing additional data filtering and hyperparameter tuning, InternLM consistently suffers from training collapse when using Vanilla GRPO. In contrast, our method enables stable and sustained training. The corresponding training dynamics are shown in Figure 7d: Vanilla GRPO exhibits significant gradient fluctuations in the later stages of training, whereas our method remains relatively stable. This suggests that our filtering of outlier tokens also contributes to training stability. 16 On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models (a) (b) (c) (d) Figure 7. Dynamics of entropy during RFT of Qwen3(a), Distilled-llama(b), Internlm(c) and gradient norm of Internlm(d). Table 3. Avg@K accuracy of models trained from more base models. Method Qwen3-4B-Base GRPO GRPO+ClipB GRPO+ClipV R1-Distill-llama-8B-Instruct GRPO GRPO+ClipB GRPO+ClipV InternLM3-8B-Instruct GRPO GRPO+ClipB GRPO+ClipV AIME24 AIME25 DAPO500 7.71 18.96 19.48 20. 23.85 24.37 24.69 3.43 6.35 7.29 6.77 27.82 50.38 49.70 50.95 60.12 60.42 59. 19.30 29.50 30.88 30.23 9.06 20.73 20.83 21.56 31.87 32.19 32.40 4. 9.17 9.27 8."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group",
        "University of Science and Technology of China"
    ]
}