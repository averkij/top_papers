{
    "paper_title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering",
    "authors": [
        "William Jurayj",
        "Jeffrey Cheng",
        "Benjamin Van Durme"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings."
        },
        {
            "title": "Start",
            "content": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering"
        },
        {
            "title": "Benjamin Van Durme",
            "content": "Johns Hopkins University {wjurayj1,jcheng71,vandurme}@jhu.edu 5 2 0 2 9 1 ] . [ 1 2 6 9 3 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that reasoning system should always give an answer to any question provided. This overlooks concerns about whether model is confident in its answer, and whether it is appropriate to always provide response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest recipe for reporting evaluations under these settings."
        },
        {
            "title": "Introduction",
            "content": "Scaling up language model inference-time compute using lengthy chains of thought has delivered impressive results on mathematical reasoning benchmarks that resisted training compute scaling (DeepSeek-AI et al., 2025; Muennighoff et al., 2025). These results, however, are reported in the zero-risk response setting: with no penalties for incorrect answers, the system always guesses even when it is not confident in its answer. In practice, this behavior is not always desirable. Many question answering settings associate incorrect answers with measurable costs, ranging from low-risk responses found in game shows (Ferrucci et al., 2010) to high-stakes responses that can alter peoples lives (Northpointe, 2017). Selective question answering addresses these challenges by allowing model to refrain from answering questions which it might answer incorrectly (Kamath et al., 2020). This requires selection function, 1 Figure 1: DeepSeek R1-32Bs accuracy is function of compute budget and confidence threshold. Increased confidence thresholds generally yield increased accuracy at the cost of response rate, while increased compute budgets sometimes decrease accuracy while increasing response rate. The vertical axis measures the accuracy of answered questions at compute budget and confidence threshold. Color indicates the proportion of questions that are answered; in redder regions, the model is more likely to answer, whereas in bluer regions the model is less likely to answer. We treat the case where the model never answers as accuracy 0. which considers risk tolerance, coverage goals, and candidate answer confidence to decide whether prediction should be given (Geifman and El-Yaniv, 2017). Knowing when not to answer is critical for systems to collaborate effectively with humans (Verma et al., 2023), especially for test-time scaling systems that must constantly decide between refusing to answer and expending further compute to search for possible solution. To help address this issue, we evaluate test-time scaling models using simple class of selection functions that reject questions if model is not confident in its answer after expending its compute Figure 2: Confidence thresholds on test-time scaling. (left) When the logit threshold is 0, the model answers 100% of questions. This is the only performance curve that is reported by test-time scaling research. (center) At moderate threshold, more frequent absentions allow higher response accuracy. (right) At high threshold, small amounts of test-time compute deliver very high accuracy, while test-time scaling provides more answers at the cost of answer accuracy. We treat the decision to never answer as yielding accuracy 0. budget. We evaluate these systems at different compute budgets, showing new aspect of model performance that answer accuracy alone struggles to measure. We suggest class of utility functions that represent various levels of error risk to empirically measure the performance of these systems in settings where incorrect answers are penalized. Evaluation in these settings shows how compute scaling affects confidence in existing systems. Based on these insights, we propose standard method for measuring model performance in settings with nonzero response risk. In summary we: Conduct the first evaluation of LLM test-time compute scaling on selective question answering, finding that increasing inference compute can help models distinguish between their correct and incorrect answers. (Section 3) Introduce evaluation settings that penalize incorrect answers and allow abstentions to help holistically models capable of scaling testtime compute. (Section 4) Invite the community to report test-time scaling performance on selective question answering under Jeopardy Odds, which incentivize confidence calibration by penalizing incorrect answers while rewarding correct answers."
        },
        {
            "title": "2 Methods",
            "content": "We explore how increasing compute budgets affects models performance on QA tasks at different confidence thresholds. The choice of budget and threshold is test-time decision. We describe methods to quantify the two factors below: Compute Budget refers to the amount of compute expended by the model at inference time. In all cases, we quantify models budget by counting the number of tokens in its reasoning trace. We use methods proposed by Muennighoff et al. (2025) to strictly enforce compute budgets. Specifically, we ignore any predicted end-of-thinking delimiters and instead append the token Wait if model attempts to end its reasoning trace before reaching the budget, and we force decode the end-ofthinking delimiter once the budget is reached. Confidence Threshold refers to the uncertainty of the model in its decoded answer. We quantify models confidence as the sum of the logprobabilities corresponding to the answer tokens.1 For confidence threshold, our selection function (Geifman and El-Yaniv, 2017) only accepts answers that the model delivers with confidence greater than its threshold, abstaining otherwise."
        },
        {
            "title": "3 Experiments",
            "content": "Experimental Setup We evaluate Deepseek-R132B (DeepSeek-AI et al., 2025) and s1 (Muennighoff et al., 2025) due to their exhibited test-time scaling capabilities and open-weight checkpoints, and choose AIME24 as our evaluation dataset. The dataset contains 30 hard math problems on which performance substantially benefits from larger compute budgets, making it popular benchmark for evaluating test-time scaling. We test the set of confidence thresholds {0.0, 0.5, 0.95} across compute 1Every answer in our dataset is 3-digit number between 000 and 999, so consists of the same number of tokens. 2 budgets within the range [500, 8000], incrementing by 100 tokens. For given budget and threshold, we report the accuracy of answered questions. As the number of answered questions differs across confidence thresholds, we note that accuracies are not directly comparable between models and compute budgets. Figure 3: Test-time scaling improves confidence in correct answers. Each dot represents R1 32Bs confidence in an answer after spending fixed amount of compute. Indigo series are correct answers, while orange series are incorrect. Note that individual dots may turn from orange to indigo if the model changes its prediction after thinking longer. See Figure 7 in Appendix for s1-32B. Results Figure 2 compares the accuracy of answers provided by R1-32B and s1-32B at different test-time compute budgets. When the confidence threshold is 0, models answer every question, so accuracy increases consistently with compute budget. We observe that these subplots are slices of surface parameterized by compute budget and confidence threshold, shown in Figure 1. While higher confidence thresholds prevent the model from answering at low budgets, scaling compute at high thresholds delivers larger volume of accurate answers. At higher confidence thresholds, increased compute budget can actually decrease answer accuracy. This decrease in accuracy of yielded answers does not necessarily reflect decreased performance at higher budgets, but instead the increase in the total number of questions answered. To investigate whether excessive thinking harms accuracy drops by pushing models to abandon correct answers, we plot how models confidence in individual answers moves over time. Figure 3 shows the answer confidences given by R1-32B at varying compute budgets, colored according to their correctness, with curve fit to the distribution. We note that as compute budget increases, the average confidence of its correct answers increases 3 even as additional correct answers are discovered."
        },
        {
            "title": "4 Utility",
            "content": "Motivation When refusal to answer is an option, accuracy can be trivially optimized by system that answers extremely infrequently. Thus, useful metric must capture both the accuracy of answers provided and the systems propensity to provide answers. Many real world scenarios reward correct answers, but incur measurable costs for incorrect answers. We show our results involving confidence thresholds can be adapted to these settings: Given model and an instance of task t, we define utility function to be (M, x) = 1c + 01a + rt1i where (1c, 1a, 1i) are mutually exclusive indicators for the predicted answer being (correct, abstained, incorrect) and rt is some task-specific cost of incorrect answers. We can assume the reward for correct answers is 1 without loss of generality due to scaling. While there exist scenarios where refusing to answer also incurs cost, this paper will only discuss the consequences when no extra cost is incurred; the conclusions we draw can be extended to these cases. Problem Scenarios We discuss three settings with varying risk levels: Exam Odds (rt = 0): There are no costs incurred by incorrect answers. These are tasks where guessing isnt punished and the model should always try to provide solution. Jeopardy Odds2 (rt = 1): The cost of an incorrect answer is equal to the reward for correct answer. In these scenarios, no answer at all is preferable to an incorrect answer. High-Stakes Odds (rt = 20): The cost of an incorrect answer far outweighs the reward for correct answer. In this case, the model should answer only if absolutely certain. Results We keep the same experimental setup as described in Section 3. Rather than reporting accuracy, we instead report the utility in the three scenarios above, shown in Figure 4. We focus on the Jeopardy setting because it highlights why system might choose not to answer; results in the other settings are in Appendix B. The Exam settings utility function does not distinguish refusal from incorrectness, so optimal performance is achieved trivially at confidence threshold to 0 so that every question gets the models best 2Inspired by the wagers made in the Final Jeopardy stage monly reported Exam Odds, R1-32B and s1-32B scale comparably at threshold 0. In Jeopardy Odds, selective question answering at threshold 0.95 dramatically improves performance for both models. Additionally, although the two models scale comparably at Exam Odds, R1-32B substantially outperforms s1-32B at larger budgets in this new evaluation setting. Previous work overlooks this comparison. We call on future test-time compute scaling research to report optimal utility at Jeopardy Odds in addition to Exam Odds, to help readers understand performance across confidence demands."
        },
        {
            "title": "5 Related Work",
            "content": "As scaling training compute has become prohibitively expensive (Hoffmann et al., 2022), models that scale performance with test-time compute have become new frontier (Snell et al., 2024; Wu et al., 2024). These methods have delivered state-of-the-art results on hard reasoning tasks using lengthy chains of thought (DeepSeek-AI et al., 2025; Muennighoff et al., 2025). Current work in this space optimizes for question answering tasks which do not penalize incorrectness, ignoring settings that favor refusal over wrong answers (Ferrucci et al., 2010; Rajpurkar et al., 2018; Kamath et al., 2020). We draw motivation from methods for cost-sensitive learning (Mienye and Sun, 2021) and selective classification (Geifman and El-Yaniv, 2017), which navigate penalties for failure. These setting reward confidence calibration, which can be critical for effective collaboration with human experts (Verma et al., 2023). We are the first to investigate how serialized test-time compute helps models identify when they should not answer. We include additional materials in Appendix for further reading on these areas."
        },
        {
            "title": "6 Conclusion",
            "content": "We highlight region of performance that is currently unexplored by test-time scaling research. We encourage the test-time scaling community to adopt these insights by reporting model scaling performance on benchmarks at both Exam Odds and Jeopardy Odds, to highlight their systems ability to scale confidence with test-time compute. Future work should focus on efficiently allocating testtime compute to meet confidence demands, and could investigate how test-time confidence scaling models should decide between extending reasoning and deferring to human experts. Figure 4: Utility Surface of DeepSeek R1-32B for Jeopardy. The vertical axis indicates performance in the Jeopardy setting at different compute budgets and confidence thresholds. The color indicates the proportion of questions that are answered, as in Figure 1. The horizontal plane divides positive and negative utility regions of the operating curve. The checkered lines show the confidence slices that we compare to s1 in Figure 5. guess. In the Jeopardy setting, however, this is nontrivial. We illustrate the complete function mapping compute budget and confidence threshold to Jeopardy performance in Figure 4: the checkered lines on this surface indicate the two slices that compose R1-32Bs portion of Figure 5. We do not suggest that our choice of 0.95 is the optimal threshold for this task, or even that threshold is the right approach to confidence calibration. Rather, we apply this naive method to show how test-time scaling for selective classification can benefit practical question-answering setting. We see on the left of Figure 2 that in the comFigure 5: Jeopardy utility scales differently across models and thresholds. Performance of s1-32B and R1-32B in the Jeopardy odds setting under different confidence thresholds. While s1 is competitive in the case when threshold is 0, higher threshold shows R1s superior scaling performance."
        },
        {
            "title": "Limitations",
            "content": "The selection function we implement is based entirely on the likelihood that large language model assigns series of tokens after thinking, which is not necessarily the optimal method for model confidence estimation. We choose the AIME dataset to demonstrate the value of our proof because it displays the most consistent performance improvements from test-time scaling (Muennighoff et al., 2025); We evaluate fixed set of three odds levels, but recognize that this does not cover the full space of real-world utility functions. Furthermore, we do not consider how compute costs might be incorporated in the models utility function, which could encourage increased energy consumption. Finally, we recognize that by evaluating only on English mathematics questions and answers, we may miss model capabilities or weaknesses in lower-resource languages."
        },
        {
            "title": "References",
            "content": "Ekin Akyürek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. 2024. The Surprising Effectiveness of Test-Time Training for Abstract Reasoning. arXiv preprint. ArXiv:2411.07279 [cs]. Daman Arora and Andrea Zanette. 2025. Training Language Models to Reason Efficiently. arXiv preprint. ArXiv:2502.04463 [cs]. Jordan Boyd-Graber and Benjamin Börschinger. 2020. What question answering can learn from trivia nerds. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7422 7435, Online. Association for Computational Linguistics. Lang Cao. 2024. Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 36283646, Miami, Florida, USA. Association for Computational Linguistics. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint. ArXiv:2501.12948 [cs]. D. A. Ferrucci. 2012. Introduction to this is watson. IBM Journal of Research and Development, 56(3.4):1:11:15. David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building watson: An overview of the deepqa project. AI Magazine, 31(3):5979. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. framework for few-shot language model evaluation. Yonatan Geifman and Ran El-Yaniv. 2017. Selective classification for deep neural networks. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 48854894, Red Hook, NY, USA. Curran Associates Inc. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, and 3 others. 2022. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Zhengping Jiang, Anqi Liu, and Benjamnin Van Durme. 2024. Addressing the Binning Problem in Calibration Assessment through Scalar Annotations. Transactions of the Association for Computational Linguistics, 12:120136. Amita Kamath, Robin Jia, and Percy Liang. 2020. Selective question answering under domain shift. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5684 5696, Online. Association for Computational Linguistics. Salman H. Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A. Sohel, and Roberto Togneri. 2018. Cost-sensitive learning of deep feature repIEEE Transresentations from imbalanced data. actions on Neural Networks and Learning Systems, 29(8):35733587. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao 5 Tang, Wei-Long Zheng, Yewen Pu, and Kevin Ellis. 2025. Combining induction and transduction for abstract reasoning. In The Thirteenth International Conference on Learning Representations. Ibomoiye Domor Mienye and Yanxia Sun. 2021. Performance analysis of cost-sensitive learning methods with application to imbalanced medical data. Informatics in Medicine Unlocked, 25:100690. Hussein Mozannar and David Sontag. 2020. Consistent estimators for learning to defer to an expert. In Proceedings of the 37th International Conference on Machine Learning, ICML20. JMLR.org. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint. ArXiv:2501.19393 [cs]. Northpointe. 2017. Practitioners guide to compas core. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. 2024. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. In The Twelfth International Conference on Learning Representations. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. 2025. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8). Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia. Association for Computational Linguistics. Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter Liu. 2023. Out-of-distribution detection and selective generation for conditional language models. In The Eleventh International Conference on Learning Representations. Pedro Rodriguez, Shi Feng, Mohit Iyyer, He He, and Jordan Boyd-Graber. 2021. Quizbowl: The Case for Incremental Question Answering. arXiv preprint. ArXiv:1904.04792 [cs]. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. Preprint, arXiv:2408.03314. Rajeev Verma, Daniel Barrejón, and Eric Nalisnick. 2023. Learning to Defer to Multiple Experts: Consistent Surrogate Losses, Confidence Calibration, and Conformal Ensembles. arXiv preprint. ArXiv:2210.16955 [stat]. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. 2024. Hypothesis search: Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models. arXiv preprint. ArXiv:2408.00724 [cs]."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Supplementary Background Test-time Scaling Many methods for scaling testtime comptue have been explored. These include searching over possible generations (Wang et al., 2024; Snell et al., 2024), tool use (Qin et al., 2024; Qu et al., 2025), making gradient updates at inference time (Akyürek et al., 2024; Li et al., 2025), and simply fine-tuning on longer chains of thought (DeepSeek-AI et al., 2025; Muennighoff et al., 2025). Our work considers models fine-tuned on extremely long reasoning chains, and augments them with the ability to refuse to answer questions where they lack confidence. Concurrent work finetunes similar models to consider the difficulty of question at when allocating compute test-time, but do not report the effect their method has on their models confidence (Arora and Zanette, 2025). In contrast, we show how model confidence scales with test-time compute, and demonstrate its value for optimizing performance in settings that allow refusal to answer. Answer Refusal Refusing to answer is an important choice in many prior works on question answering. SQuAD 2.0 included this feature by asking questions which have no answer, although they treat abstaining from answering as correct answer in these unanswerable cases (Rajpurkar et al., 2018). Game-show based research efforts use an approach more closely aligned with ours, which penalizes systems for answering incorrectly to encour6 age abstentions when system cannot develop sufficiently high confidence (Ferrucci et al., 2010; Ferrucci, 2012; Boyd-Graber and Börschinger, 2020; Rodriguez et al., 2021). Related to these quiz game based setting is research into selective classification, which evaluates models performance across the coverage-accuracy curve, rather than at single point (Geifman and El-Yaniv, 2017). These approaches can be useful for avoiding costly errors in high-pressure domains (Khan et al., 2018) or under distribution shift (Ren et al., 2023), or when designing systems that defer to expert humans when it lacks confidence that its input will be helpful (Mozannar and Sontag, 2020). Related research in language modeling has focused on calibrating prediction confidence to align with human judgments (Jiang et al., 2024). Most recently, research has investigated training language models to refuse to answer (Cao, 2024). However, this does not ask how this behavior should be measured effectively in sequential test-time compute models, which might find confident answer given higher compute budgets. Figure 7: Test-time scaling improves confidence in correct answers. Each dot represents s1-32Bs confidence in an answer after spending fixed amount of compute. Indigo series indicate correct answers, while orange series are incorrect. Note that individual dots may switch colors if the model changes its prediction after thinking longer. s1-32B does not separate its correct answers from its incorrect answers as effectively as R1-32B. See Figure 3 for R1-32B."
        },
        {
            "title": "B Supplemental Figures",
            "content": "A."
        },
        {
            "title": "Implementation Details",
            "content": "We use widely available open-source libraries to run our experiments, including HuggingFace Transformers (Wolf et al., 2020) and vLLM (Kwon et al., 2023) for language model inference, and the Language Model Evaluation Harness (Gao et al., 2024) to sample reasoning chains for the AIME problems at temperature 0. In particular, we use the variant of this library released by Muennighoff et al. (2025), and run subset of the experiments that they run. We run experiments on node with 4 H100 GPUs for approximately 4 hours. 7 Figure 6: s1-32Bs answer accuracy is function of compute budget and confidence threshold. Increased confidence thresholds generally yield increased accuracy at the cost of response rate, while increased compute budgets sometimes decrease accuracy while increasing response rate. The vertical axis indicates the accuracy for answered questions at compute budget and logit threshold. The color indicates the proportion of questions that are answered; in redder regions, the model is more likely to answer, whereas in bluer regions the model is less likely to answer. We treat the decision to never answer as accuracy 0. Figure 9: Additional Model Comparisons. We additionally compare performance of s1-32B and R1-32B in the Exam Odds (above) and High-Stakes Odds (below) settings under different confidence thresholds. Like Jeopardy odds depicted in Figure 5, High-Stakes Odds illustrates performance distinction at high confidence thresholds that is not evident from conventional Exam odds. Figure 8: Utility Surface of s1-32B for Jeopardy. The vertical axis indicates performance in the Jeopardy setting at different compute budgets and confidence thresholds. The color indicates the proportion of questions that are answered; in redder regions, the model is more likely answer, whereas in bluer regions the model is less likely to answer. The horizontal plane divides positive and negative utility regions of the operating curve. The checkered lines indicate the threshold slices that we compare against R1-32B in Figure 5. Note the relatively lower volume above the break-even point of 0 corresponds to s1-32Bs broadly inferior performance at these odds."
        }
    ],
    "affiliations": [
        "Johns Hopkins University"
    ]
}