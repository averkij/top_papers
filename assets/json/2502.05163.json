{
    "paper_title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails",
    "authors": [
        "Yihe Deng",
        "Yu Yang",
        "Junkai Zhang",
        "Wei Wang",
        "Bo Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \\ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard."
        },
        {
            "title": "Start",
            "content": "DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails This paper contains model outputs that may be offensive in nature. Yihe Deng * 1 Yu Yang * 1 2 Junkai Zhang * 1 Wei Wang 1 Bo Li 2 3 5 2 0 2 7 ] . [ 1 3 6 1 5 0 . 2 0 5 2 : r Abstract The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose novel two-player Reinforcement Learning (RL) framework, where generator and guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as two-player game, proving convergence to Nash equilibrium. Empirical evaluations show that our model DuoGuard outperforms state-of-theart models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5 faster at inference with significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lowerresource languages in collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https: //github.com/yihedeng9/DuoGuard."
        },
        {
            "title": "1 Introduction\nWhile LLMs have become increasingly effective at assisting\nwith human queries, their outputs can pose risks of harm to",
            "content": "*Equal contribution 1University of California, Los Angeles 2VirtueAI 3University of Illinois at Urbana-Champaign. Correspondence to: Yihe Deng <yihedeng@cs.ucla.edu>. Preprint. Under review. 1 Figure 1. Illustration of the use-case of guardrail model for LLMs, which functions as moderation between the user-LLM conversation. users if not properly safeguarded (Zou et al., 2023; Qi et al., 2023; Wei et al., 2024; Shen et al., 2024b). Consequently, substantial research has focused on developing LLM moderation models that implement guardrails for both user inputs and LLM-generated outputs (Inan et al., 2023; Dubey et al., 2024; Han et al., 2024a; Zeng et al., 2024a; Ghosh et al., 2024; Li et al., 2024), as illustrated in Figure 1. Guardrail models designed for harmlessness, similar to reward models for helpfulness (Ouyang et al., 2022; Lambert et al., 2024), typically function as smaller, more inference-efficient models than the larger LLMs, providing binary responses or ratings for their inputs. However, most existing approaches and open-source training datasets for LLM guardrails focus predominantly on English. Recent research has highlighted that safety-aligned models in English exhibit performance declines when applied to other languages (de Wynter et al., 2024; Jain et al., 2024; Yang et al., 2024; Shen et al., 2024a). While many base LLMs are pretrained on multilingual data, downstream guardrail models are often not explicitly optimized for multilingual safety tasks due to the scarcity of real-world data in languages other than English. The scarcity of data is not unique to multilingual model training, and synthetic data has played crucial role in addressing this issue (Aryabumi et al., 2024). Ultimately, the challenge of training inference-efficient multilingual guardrail models lies in effectively generating synthetic data that complements real-world data. Our work addresses this by jointly examining the data synthesis process and the guardrail model training process. Specifically, we ask: can we develop self-improving system in which the guardrail DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Figure 2. Overview of our main results. In the left figure, we demonstrate consistently superior performance of average f1 score across 6 benchmarks in the four languages. In the right figure, we show that our model maintains the lowest inference cost while achieving superior average performance across languages. We note that, although we focus on the four languages to demonstrate the two-player data synthesis framework, DuoGuard retains its base model Qwen-2.5s capacity to support all 29 languages. model actively guides the synthetic data generation process to enhance its own training? In response, we propose an iterative two-player RL framework involving data generator and guardrail classifier, enabling continuous improvement of both synthetic data generation and classifier training. We formulate and analyze the two-player game in theoretical setting, demonstrating that it constitutes minimax game with Nash equilibrium, and prove that our algorithm converges linearly to the equilibrium. Building on this theoretical foundation, we implement practical techniques, such as data filtering and self-judgment, to ensure stability and robustness within the framework. Additionally, we carefully curate the seed dataset to provide strong foundation for the iterative process. Our model, DuoGuard, is evaluated across six multilingual safety benchmarks, including four originally in English that were translated into the languages under consideration. The results show that DuoGuard consistently outperforms baselines of similar scale by more than 20% on average. Even when compared to larger-scale guardrail baselines, DuoGuard achieves an average improvement of approximately 10% across languages. Our contributions are listed as follows, We propose two-player RL framework for multilingual guardrail model training, grounded in theoretical analysis of convergence to Nash equilibrium. Addressing the lack of open-source multilingual safety data, our framework enables the generation of synthetic data in any language supported by the generator. Through extensive empirical evaluation, we demonstrate that our 0.5B classifier significantly outperforms state-ofthe-art guardrails of similar scale across diverse datasets and consistently surpasses larger models. We show that synthetic data generated under the guidance of the 0.5B classifier generalizes effectively to train both larger classifiers (1.5B) and different architectures (Llama3.2-1B), resulting in superior performance."
        },
        {
            "title": "2 Related Work\nGuardrail Models for LLM Safety. The rapid advance-\nment of LLM capabilities (Touvron et al., 2023a;b; OpenAI,\n2023) has underscored the need for robust safeguards to\nensure responsible use (Yao et al., 2024; Dong et al., 2024b).\nWhile safety mechanisms remain less developed than LLMs\nthemselves, early efforts introduced models such as Llama-\nGuard (Inan et al., 2023), followed by LlamaGuard2, based\non Llama3 (Dubey et al., 2024), and LlamaGuard3, built on\nLlama3.1 (Dubey et al., 2024). More recent advancements\ninclude WildGuard (Han et al., 2024a), Aegis (Ghosh et al.,\n2024), MD-Judge (Li et al., 2024), and ShieldGemma (Zeng\net al., 2024a). While the F1 score is a key metric for\nguardrail performance, the practical deployment also de-\nmands models that are small in scale and inference-efficient.\nIn this regard, state-of-the-art small-scale models include\nLlamaGuard3 (1B), built on Llama-3.2 (1B), and Shield-\nGemma (2B), based on Gemma 2 (2B).\nBenchmarks for Multilingual Safety. Extending safety\nmechanisms to multilingual settings remains challenging\ndue to the scarcity of open-source datasets in low-resource\nlanguages (Deng et al., 2024). While many base LLMs are\npretrained on multilingual corpus, most guardrail models\nare not explicitly fine-tuned for multilingual data, limiting\ntheir effectiveness (de Wynter et al., 2024). To examine this\ngap, early works introduced multilingual toxicity detection\nbenchmarks by translating English datasets (Wang et al.,\n2023) or sourcing from Reddit (Ye et al., 2023). Recently,\nde Wynter et al. (2024) proposed RTP-LX, focusing on eval-\nuating guardrails in low-resource languages. Other notable\ncontributions include PolyglotToxicityPrompts (PTP) (Jain\net al., 2024), which examines toxic degeneration in multilin-\ngual outputs, and a test suite by Yang et al. (2024) to assess\nguardrails on toxicity detection and resistance to adversarial\nprompts across resource levels.\nMultilingual Synthetic Data Generation. In recent years,\nsynthetic data generated by LLMs has emerged as a valuable",
            "content": "2 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails tool for augmenting training datasets, particularly in scenarios where real-world data is scarce or sensitive. Among the most widely used techniques is translation, which creates synthetic parallel datasets by translating monolingual text from the target language back into the source language (Bi et al., 2021; Caswell et al., 2019; Liao et al., 2021; Marie et al., 2020; Pham et al., 2021; Sennrich et al., 2016; Xu et al., 2022). This method has shown significant success in neural machine translation tasks, with strategies such as beam search and constrained sampling further improving data quality and diversity (Sennrich et al., 2016; Edunov et al., 2018; Xu et al., 2022). Fine-tuning LLMs via Two-player RL. Recent research on improving LLM reasoning has been exploring various two-player RL frameworks. Zhou et al. (2024) and Ma et al. (2024) employ online RL to fine-tune two LLM agents for collaborative task-solving. Unlike these approaches, our method, while also leveraging two-player RL framework, focuses on data synthesis and model training rather than real-time collaboration between LLM agents during inference. More relevantly, recent work has adopted adversarial approaches where two players pursue opposing objectives. Among these, Cheng et al. (2024); Chen et al. (2024); Wu et al. (2024); Munos et al. (2023); Swamy et al. (2024) employ self-play framework, where LLMs iteratively optimize themselves to outperform previous versions on generation tasks such as math reasoning or instruction following. We defer the detailed discussion of more classical adversarial training schemes to Appendix B. 3 Problem Setting and Preliminaries An LLM is represented by the probability distribution pθ, parameterized by the model weight θ. Given sequence = [x1, . . . , xn] as the prompt, the model generates response = [y1, . . . , ym], where xi and yj denote individual tokens. The response is treated as sample from the conditional probability distribution pθ(x). The conditional probability pθ(yx) can be factorized as pθ(yx) = (cid:81)m Preference Optimization. To improve LLM alignment with human preferences, reinforcement learning with human feedback (RLHF) is commonly applied. This approach optimizes the LLM using human preference data modeled under the Bradley-Terry framework (Dong et al., 2024a; Shao et al., 2024; Ahmadian et al., 2024): j=1 pθ(yjx, y1, . . . , yj1). P(yw ylx) = σ(cid:0)r(x, yw) r(x, yl)(cid:1), where yw is the preferred response, yl is the dispreferred response, and σ(t) = 1/(1 + exp(t)) is the sigmoid function. The reward function r(x, y) is designed to assign higher values to preferred responses. However, training reward model can be computationally expensive and operationally challenging. To address this, Direct Preference Optimization (DPO) (Rafailov et al., 2023) offers simplified alternative by leveraging an implicit reward function defined by the LLM itself. Specifically, the DPO objective is formulated as: LDPO(θ, θref ) = 1 Spref (cid:88) (x,yw,yl)Spref (cid:18) (cid:20) ℓ β log pθ(ywx) pθref (ywx) β log pθ(ylx) pθref (ylx) (cid:19)(cid:21) , where θref is the reference model that the policy model should not deviate too much from. Guardrail Models. guardrail model acts as function : {1, 1} that evaluates an input text sequence, which may be either user input or an LLM-generated response, and determines whether the content is harmful. In practice, guardrail models are typically built upon pretrained LLMs, parameterized by θ, and generate discrete outputs such as safe or unsafe. Some models further provide explanations for their classifications, improving performance at the cost of increased inference time. In our setting, we prioritize inference efficiency in model architecture by modifying the final layer of pre-trained LLM and converting it to binary classification model. 4 Method We propose an iterative two-player framework involving generator and guardrail classifier to synthesize multilingual training data and enhance the classifiers ability to distinguish harmful content from benign content. The process begins with seed dataset containing labeled safe and unsafe examples collected from open-source datasets. The generator proposes new samples in target language, and both the generator and classifier are iteratively updated. This framework establishes dynamic interaction: Generators Objective: Generate samples in the target language that challenge the classifier, reinforcing on the misclassified samples. Classifiers Objective: Improve robustness by minimizing errors on previously misclassified samples proposed by the generator. Figure 3 provides an overview of our approach. 4.1 The Two-Player Game: Theoretical Convergence We formalize the interaction between the adversarial generator and the defensive classifier as two-player game. The process begins with seed dataset = {(xi, yi)}iI of labeled real data, where xi is an input text sequence and yi {1, 1} is its toxicity label. Let Gϕ denote the adversarial generator parameterized by ϕ. The generator takes sample from the seed dataset and specified language ℓ as input and outputs sample text sequence (cid:101)xi in that language that preserves the toxicity label yi of xi. Formally, Gϕ : (x, y, ℓ) (cid:101)x, (cid:101)x Xℓ. DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Figure 3. Overview of the two-player training pipeline. The generator produces synthetic data from seed data. The classifier makes predictions and we measure these examples as being predicted correctly or incorrectly based on their seed data label. We train the generator with DPO to create increasingly challenging examples, which in turn improve the classifier through iterative training. In the following narrative, we fix target language and deprecate ℓ for simplicity. Let Cθ : denote the defensive classifier parameterized by θ, which takes the generated query as input and outputs the probability of toxicity. Classifier Update. At iteration t, for given input (x, y) S, the generator Gϕt samples new sequence (cid:101)x from its conditional probability distribution pϕt((cid:101)xx, y). The classifier is then updated by minimizing the negative log-likelihood of the true labels over the generators distribution pϕt((cid:101)xx, y): θt+1 = argmax θ Lt C(θ), C(θ) = Lt (cid:101)xpϕt ((cid:101)xx,y) (cid:2) log pθ(y(cid:101)x)(cid:3), (4.1) where pθ(y(cid:101)x) is the conditional distribution of the classifier. Generator Update. Simultaneously, the generator Gϕ is aimed to produce samples that cause the classifier to make incorrect predictions. Therefore, we define the reward signal with the negative log-likelihood: rt (4.2) (cid:0)(x, y), (cid:101)x(cid:1) = log pθt(y(cid:101)x). Equation (4.2) computes the negative log-likelihood of the correct label for generated samples under the current classifier, where higher value indicates greater vulnerability of the classifier to these adversarial samples. Many RL algorithms can be used to maximize the reward. For training stability and computational efficiency, we choose the offline RL algorithm DPO over the online RL algorithm PPO (Schulman et al., 2017). We thus model the preference between two generated samples, (cid:101)xw and (cid:101)xl, given input (x, y), using the Bradley-Terry framework: Pt((cid:101)xw (cid:101)xlx, y) = σ (cid:16) rt (cid:0)(x, y), (cid:101)xw (cid:1) rt (cid:0)(x, y), (cid:101)xl (cid:1)(cid:17) . Based on these preferences, the generator Gϕ is updated by minimizing the DPO objective: ϕt+1 = argmax ϕ Lt G(ϕ, ϕref) 4 LG(ϕ, ϕref) = (cid:18) (cid:20) ℓ β log (cid:101)xw,(cid:101)xlpϕt ((cid:101)xx,y)P((cid:101)xw (cid:101)xlx, y), (cid:19)(cid:21) pϕ((cid:101)xwx, y) pϕref ((cid:101)xwx, y) β log pϕ((cid:101)xlx, y) pϕref((cid:101)xlx, y) , (4.3) where ϕref is the reference generator model and β is regularization parameter controlling the deviation from the reference generator model. Min-max Game Equilibrium Analysis. The DPO objective shares the same minimizer as the corresponding PPO training objective, which is defined as: PPO(ϕ, ϕref) = Lt (cid:124) βDKL(pϕpref) (cid:123)(cid:122) (cid:125) II (cid:101)xpϕ[rt((x, y), (cid:101)x)] (cid:123)(cid:122) (cid:125) (cid:124) . Here, term in Lt PPO is indeed the same as the training objective of the classifier Lt C(θ), while the regularization term II is independent of the classifier. This connection demonstrates that our algorithm optimizes minimax game with the following objective: min pθ max pϕ (cid:101)xpϕ (cid:2) log pθ(y(cid:101)x)(cid:3) βDKL(pϕpref). (4.4) In this game, the iterative update rules for each player, as defined in Equations (4.1) and (4.3), represent their best response to the current opponent policy. As result, the generator and classifier are guaranteed to converge to Nash equilibrium. Theorem 4.1. The minimax game defined in Equation (4.4) admits Nash equilibrium. In addition, with an appropriately chosen regularization parameter β, the iterative updates in (4.1) and (4.3) converge linearly to the Nash equilibrium. The detailed proof is provided in Appendix A. 4.2 The Two-Player Game: Practical Algorithm While our method is conceptually framed as the minimax game in (4.4), additional implementation details are introduced to ensure feasibility, efficiency, and performance. DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails }k First, the generator produces new queries {(cid:101)x(i) j=1 for given input query x(i). To preserve the original label of the seed data, we use two distinct prompts cy:y=1 for generating samples, based on whether the input is safe or unsafe: (cid:101)x(i) pϕt1 ((cid:101)xx(i), cy(i) ). We detail the prompts used for the generator in Appendix D. The training data (t) at iteration is augmented exclusively with misclassified synthetic samples, defined as: (t) = (t1) (cid:101)Smis, where (cid:101)Smis = {(cid:101)x(i) = y(i)} and (0) = S. To further enhance performance, we adopt fine-grained multi-label classification setup similar to Dubey et al. (2024), where harmful inputs can have multiple labels (e.g., hate, violence), and safe content is labeled with all zeros. The classifiers objective is modified to multi-label classification loss using binary cross-entropy loss (equivalent to the negative log-likelihood minimization) for each of the 12 defined harmful classes (detailed in Appendix C): : (cid:98)y(i) L(t) (θ) = 1 (t) (cid:88) 12 (cid:88) (cid:20) yc log pθ(yc(cid:101)x) ((cid:101)x,{yc})S (t) c= + (1 yc) log(1 pθ(yc(cid:101)x)) (cid:21) . (4.5) To maintain stability, we retrain the classifier from scratch at each iteration using the evolving dataset, similar to iterative approaches in mathematical reasoning (Hosseini et al., 2024). For the generator, the DPO training objective increases the likelihood of preferred data, which are samples that cause incorrect prediction of the classifier. Therefore, we consider the correctly classified ones as the dispreferred generation samples in preference learning. The correctly classified samples are defined as (cid:101)Scor = {(cid:101)x(i) = y(i)}. The generators loss is then given by: : (cid:98)y(i) L(t) (ϕ, ϕref) = 1 (cid:88) xS (t),(cid:101)xw (cid:101)Smis,(cid:101)xl (cid:101)Scor (cid:18) (cid:20) ℓ β log pϕ((cid:101)xwx) pϕref((cid:101)xwx) β log pϕ((cid:101)xlx) pϕref((cid:101)xlx) (cid:19)(cid:21) , (4.6) where < (t) is the number of preference pairs that we were able to construct. We summarize the practical algorithm in Algorithm 1. 4.3 Data Curation Data Filtering. filtering process was applied during synthetic data generation to retain only high-quality, relevant proposals from the generator. First, the base model (without further fine-tuning) of the generator was used to assign each proposal harmfulness score on scale of 1 to 5, with the prompt detailed in Appendix D. Proposals were retained only if their scores roughly matched the seed label (e.g., scores 2 for safe seeds and 3 for harmful seeds). To 5 Algorithm 1 Two-Player Training Require: Initial generator Gϕ0 and classifier Cθ0 ; maximum iteration . Input: Seed training dataset = {(x(i), y(i))}N cy=1 and cy=1. Output: Final generator GϕT and classifier CθT . 1: for = 1, . . . , do 2: 3: Sample Queries: for (x(i), y(i)) (t1) do i=1. Prompt 4: 5: 6: 7: 8: }k j=1 pϕt1 ((cid:101)xcy(i), x(i)). Sample {(cid:101)x(i) Assign (cid:98)y(i) Partition into: = Cθt1 ((cid:101)x(i) ). mis = {(cid:101)x(i) (cid:101)S (i) : (cid:98)y(i) = y(i)}, cor = {(cid:101)x(i) (cid:101)S (i) : (cid:98)y(i) = y(i)}. end for Update Training Dataset: (t) = (t1) (cid:32) (cid:91) (cid:33) (cid:101)S (i) mis . 9: Update Classifier According to (4.5): θt argmin θ L(t) (θ). 10: Update Generator According to (4.6): ϕt argmin ϕ L(t) (ϕ, ϕref). 11: end for 12: return CθT maintain alignment with the original seeds context, length constraint was enforced: proposals differing by more than 200 characters from the seed were discarded. Furthermore, outputs that contain refusal phrases, such as apologize or cannot comply in any language, were excluded, as the generator fails to produce meaningful samples due to internal censorship. Finally, all retained proposals were evaluated with the current guardrail classifier. Proposals that led to misclassifications were selected for training the classifier. Preference Data Construction. To enhance the generator within the two-player game, we construct preference data for DPO. For each seed instance, the generated proposals are categorized into one of four levels based on two key criteria: whether the proposal causes the classifier to misclassify and whether its harm rating matches the seed label. Level 1 (Best, Preferred): The proposal causes the classifier to misclassify. The proposals generator-assigned rating matches the seed label (e.g., rating 2 for safe, DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails 3 for harmful). Level 2 (Dispreferred): The proposal does not cause the classifier to misclassify. The rating matches the seed label. Level 3 (Dispreferred): The proposal causes the classifier to misclassify. The rating does not match the seed label. Level 4 (Unsure): The proposal does not cause the classifier to misclassify. The rating does not match the label. Preference pairs are derived by comparing proposals across these categories. For each seed instance, Level 1 data are prioritized as the preferred option, with Level 2 serving as the dispreferred reference. If no Level 1 examples are available, the instance is excluded from preference pairing. Alternatively, if no Level 2 examples exist, Level 3 may be used to form weaker preference signal, since it improves the generator towards better instruction-following ability. 5 Experiments Setup. In our experiments, we use Qwen2.5-0.5B and Qwen2.5-1.5B (Qwen Team, 2024) as the base models for the classifier, since the guardrail model is typically small-scale model and Qwen2.5-0.5B and Qwen2.5-1.5B models are among the most effective small-scale multilingual models available. In addition, we use dolphin-2.9.4llama3.1-8b1 as the base model for the generator, which is an uncensored multilingual model that meets our requirements for generating harmful queries in multiple languages. We follow the optimization process outlined in Section 4.2 and Algorithm 1 to train both models, applying full finetuning to the classifier and generator. For baselines, we compare against specialized guardrail models, including LlamaGuard3 (Inan et al., 2023) (1B) and ShieldGemma (Zeng et al., 2024a) (2B), which are SOTA models of similar scale to DuoGuard. Additionally, we include larger-scale versions of LlamaGuard2 (8B) and LlamaGuard3 (8B) for more comprehensive comparison. Experiments were conducted on NVIDIA H100 80GB GPU clusters and we detail the hyperparameters in Appendix D. Data. To construct the seed dataset, we gather and combine training data from existing open-source data related to safety and toxicity, with detailed source information provided in Appendix C. We note that, instruction-following and QA data in sensitive domains (e.g., medical, legal, political) were also selected as benign examples containing potentially sensitive keywords. To prevent the classifier from relying on superficial keyword cues, we downsampled harmful examples dominated by specific terms. Harmful examples were further categorized into 12 groups, with an LLM assisting in labeling when category boundaries were ambiguous. Duplicate entries were removed to avoid overrepresentation, and the corpus was decontaminated to ensure no overlap with test data. The final linguistic composition of our gathered 1https://huggingface.co/cognitivecomputations/dolphin-2.9.4llama3.1-8b open-source dataset reveals pronounced linguistic imbalance, where English data takes 81.4% (1,679,516 instances), substantially predominating over French as 8.9% (183,919), Spanish as 5.2% (107,052), and German as 4.5% (92,793). For generating the synthetic data, we set temperature of 0.7 to encourage more diverse and creative generations and consider = 8. Evaluation. We evaluate our method in four languages: English, French, German, and Spanish. We note that, while we considered the four languages to show the effectiveness of our data generation framework, DuoGuard supports the 29 languages as its base model Qwen-2.5 does. For benchmarking guardrail models, we use six safety datasets: XSTest (Rottger et al., 2023), ToxicChat (Lin et al., 2023), OpenAI Moderation (Markov et al., 2023), Beavertails (Ji et al., 2024b), RTP-LX (de Wynter et al., 2024), and XSafety (Wang et al., 2023). Among these, RTP-LX and XSafety are dedicated multilingual safety benchmarks, while the remaining four (XSTest, ToxicChat, OpenAI Moderation, and Beavertails) are commonly used English safety benchmarks. To enable multilingual evaluation, we translate these four datasets into languages that we considered. 5.1 Main Results We present our main results in Figure 2 and detail the performance on each dataset for each language in Table 1. DuoGuard, demonstrates significant advantages over existing guardrail models in both performance and efficiency. As shown in Figure 2, DuoGuard achieves the highest average F1 score across English, French, Spanish, and German, outperforming all baselines, including the larger-scale LlamaGuard3 (8B) model, by over 10%. Compared to models of similar scale, such as LlamaGuard3 (1B) and ShieldGemma (2B), DuoGuard surpasses their performance by more than 30% on average. Additionally, DuoGuard exhibits the lowest inference cost (16.47 ms/input), achieving over 4.5 speedup compared to LlamaGuard3 (8B) (58.88 ms/input) and ShieldGemma (2B) (57.83 ms/input). This highlights the efficiency of our approach, as it not only surpasses larger models in multilingual safety performance but also maintains significantly lower computational overhead, making it more practical for real-world deployment. In Figure 4, we present the average performance of each model across the three non-English languages relative to the English performance of our model DuoGuard. Here, DuoGuard achieves the lowest performance decline across all languages as compared to the English performance. 5.2 Weak-to-Strong Generalization Weak-to-strong generalization refers to the ability of weaker model to generalize in supervising the training of stronger models. In Table 2, we leverage the training data generated by our two-player framework to train Llama-3.2 (1B), the base model for LlamaGuard3 (1B), and Qwen-2.5 (1.5B), larger-scale model used to evaluate the weak-to6 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Table 1. Detailed F-1 scores on the classification benchmarks. The bold numbers indicate the best results among the methods evaluated and the underscored numbers represent the second-best results. Model Size XSTest OpenAI ToxicC. BeaverT. RTP-LX XSafety Average XSTest OpenAI ToxicC. BeaverT. RTP-LX XSafety Average English German LlamaGuard3 ShieldGemma LlamaGuard2 LlamaGuard 1B 2B 8B 8B DuoGuard 0.5B 43.4 69.4 88.8 88.4 82.3 36. 44.8 75.9 79.0 70.8 22.3 36. 46.3 54.0 70.1 Model Size 54. 26.0 39.5 48.5 91.7 51.6 51. 72.3 70.1 86.1 French 62.3 30. 35.2 40.5 48.5 45.2 43.1 59. 63.4 74.9 43.0 59.6 79.8 82. 75.8 37.4 38.7 74.4 78.5 65. 20.9 27.5 40.5 48.0 61.4 55. 19.5 38.7 50.2 87.3 50.2 51. 68.5 70.4 80.8 Spanish 61.4 24. 30.6 37.8 60.4 44.7 36.8 55. 61.3 71.9 XSTest OpenAI ToxicC. BeaverT. RTP-LX XSafety Average XSTest OpenAI ToxicC. BeaverT. RTP-LX XSafety Average LlamaGuard ShieldGemma LlamaGuard2 LlamaGuard3 1B 2B 8B 8B DuoGuard 0.5B 43.0 63.3 81. 84.4 79.2 37.8 36.8 74.5 78. 67.1 19.5 28.7 39.7 50.1 62. 50.9 50.1 68.6 69.5 81.3 54. 21.5 40.0 48.8 91.0 61.3 23. 35.4 40.3 54.7 44.6 37.4 56. 61.9 72.7 46.9 62.4 84.0 86. 81.4 37.9 37.7 74.8 77.7 66. 20.4 29.1 39.2 48.4 64.9 50. 50.8 67.5 69.5 81.4 52.1 17. 39.4 48.4 88.0 62.1 24.0 33. 39.0 61.0 45.0 37.0 56.5 61. 73.9 our main evaluation adopts binary classification for consistency, DuoGuard can provide detailed reasons for flagging content. Additionally, adjusting the final threshold (or applying individual thresholds) allows for customizable caution levels. 6 Ablation Study 6.1 Seed Data Benefit of Incorporating Multilingual Data. We evaluate three training configurations using only the seed dataset: training on English data alone, training on English and French data, and training on all four languages. Figure 5 presents the F1 scores on the OpenAI moderation test set for models trained under these conditions, all based on the Qwen2.5-0.5B model. Interestingly, training exclusively on English provides relatively strong foundation for performance on French but is weaker on Spanish and German. Incorporating French data significantly improves performance on the French-translated OpenAI test set (from 51.3 to 65.2) while also enhancing performance on the Spanishand German-translated test sets by 7.4 and 12.9 points, respectively. Additionally, English and French data appear to be mutually beneficial. The inclusion of Spanish and German data further improves performance on their respective test sets. However, as their addition reduces the proportion of English and French data, it leads to slight performance decline overall. Performance Differences Due to Disproportionate Data. Figure 6 illustrates the relationship between training data volume per language and model performance (average F1 scores) across six benchmarks. The model is trained on the entire seed dataset, without synthetic data augmentation. The horizontal axis represents languages (English, French, Spanish, and German), while the left and right vertical axes indicate F1 scores and training data volume in the seed data, respectively. clear trend emerges: languages with larger training datasets (e.g., English) achieve higher F1 scores, while those with less data (e.g., Spanish, German) perform Figure 4. Relative performance decline (average F1 across six benchmarks and three languages) of various models compared to the English performance of DuoGuard. strong generalization capabilities of our method. We draw the following observations: (1) While the final fine-tuning results vary across base models, the data generated by our framework generalizes effectively across architectures, consistently outperforming baselines trained on the same base model by more than 20%. (2) The two-player framework demonstrates weak-to-strong generalization, as data generated with the 0.5B classifier significantly improves the performance of the 1.5B classifier. Table 2. Average F-1 scores across languages of different models trained with the dataset developed by our two-player scheme. The data can easily generalize to different base models (Llama-3.2) and different scales (1.5B). Model Base Size En Fr LlamaGuard3 Llama-3.2 DuoGuard Llama-3. 1B 1B 45.2 75.7 DuoGuard DuoGuard Qwen-2.5 0.5B 74.9 Qwen-2.5 1.5B 76.2 44.6 74. 71.9 75.0 Es 45.0 71.7 72. 73.7 De 44.7 71.3 73.9 74. Note. DuoGuard moderates content across 12 distinct subcategories as outlined in Appendix C. Each forward pass produces 12-dimensional logits vectorone dimension per risk area. Applying sigmoid function yields multi-label probability distribution, enabling fine-grained detection of potentially unsafe content. For binary moderation, we compare the maximum subcategory probability to threshold (e.g., 0.5). If it exceeds the threshold, the content is labeled unsafe; otherwise, safe. Although 7 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails ularly observe large gains in Spanish and German, highlighting the effectiveness of the iterative process in bridging performance gaps for lower-resource languages. By iteration 2, the performance for all languages converges, with Spanish and German achieving scores comparable to French, and all non-English languages narrowing the gap with English. In Figure 7(b), we further show the data proportion across languages for iteration 0 (seed data) and synthetic data generated at iteration 1. In iteration 0, English dominates with 81% of the data, while other languages (French, German, and Spanish) collectively account for less than 20%. At iteration 1, the distribution for synthetic balances with the seed data, with English decreasing to 13%, and significant increases in French (27%), German (35%), and Spanish (24%). (a) (b) Figure 7. (a) Iterative performance improvements of DuoGuard. (b) Shift in data distribution across languages over iterations. Table 3. Models average F1 with different training data at Iter1. Data Seed (Iter0) Unfiltered Filtered (Iter1) 63.3 64.1 70.8 Quality over Quantity. Since we generate eight candidate samples per input seed, one possible approach is to include all synthetic data in training. Table 3 compares classifier performance (average F1 across 6 benchmarks) at iteration 1 when trained on filtered misclassified data versus all generated synthetic data. While our frameworks selected synthetic data leads to significant improvements, incorporating all generated dataincluding correctly classified samplesyields negligible gains, highlighting the importance of targeted data selection. Furthermore, we present an example comparing selected synthetic data to the full set of filtered data. The selected samples preserve ungrammatical and jumbled syntax, better reflecting realworld use cases and enhancing the models robustness in handling noisy inputs. Seed data Generator (selected) Translation Generator (filtered) Translation twats know fuck anything. Stinkkafer kennen fiesen Scheiß von nichts Stink bugs dont know anything about nasty shits Arrogante Schwachkopfe behaupten, alles zu kennen, obschon sie eindeutig nichts Ahnung haben. Arrogant idiots claim to know everything when they clearly have no idea. Figure 5. The F1 score on OpenAI benchmark of models trained with data containing different languages in our seed data. The inclusion of French in addition to English improves model performance on Spanish (36.9% to 62.8%) and German (31.9 to 59.6). worse. Although the performance gap varies across test sets, F1 scores consistently decline with reduced dataset size. This underscores the importance of synthetic data in mitigating performance disparities for low-resource languages. While the base LLM (Qwen-2.5 in our case) may have inherent limitations on low-resource languages, our method and the results of DuoGuard demonstrate that incorporating synthetic multilingual data during post-training can significantly reduce this gap for the downstream task we consider. Figure 6. Performance by languages of the model trained on seed data. With larger data proportion in seed data, the models average performance on English is markedly higher than on other languages. Takeaways. Incorporating multilingual data improves model generalization across languages. More available real data (e.g., English) yields better performance, underscoring the need for synthetic data in low-resource languages. 6.2 Synthetic Data Iterative Improvement. In Figure 7(a), we demonstrate the iterative improvement of the guardrail classifier in average F1 scores across English (En), French (Fr), Spanish (Es), and German (De) on the 6 benchmarks. Starting from iteration 0, which represents the baseline performance of training on seed data, substantial improvements are observed for all non-English languages after the first iteration. We partic8 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Takeaways. Iterative synthetic data generation reduced the imbalance and gap across languages in real data. Filtering synthetic data boosts classifier performance by removing lower-quality samples."
        },
        {
            "title": "7 Conclusion and Discussion\nIn summary, our work addresses the data scarcity challenge\nof multilingual LLM safety through a self-improving frame-\nwork that combines synthetic data generation with guardrail\ntraining. Our two-player reinforcement learning approach,\ntheoretically grounded as a min-max game with proven con-\nvergence properties, enables joint optimization of data qual-\nity and classifier performance. Empirical evaluation across\nsix languages shows our model outperforming similarly-\nsized baselines by over 20% and larger models by 10%,\nwith a 0.5B model size and 4.5× speedup comparing to\nexisting guardrails.\nA key limitation of synthetic data generation is reliance\non an LLM: stronger models naturally yield better out-\ncomes. In multilingual settings, the generator’s pre-training\ndata dictates which languages it can effectively produce.\nHowever, pre-training data is generally easier to obtain\nthan high-quality post-training data for specific downstream\ntasks. Many modern LLMs, such as Qwen-2.5, already\nsupport over 29 languages. The challenge lies in leverag-\ning these models to generate high-quality post-training data.\nOur work thus focuses on the contribution toward better\npost-training synthetic data generation. Lastly, although\nDuoGuard focuses on English, French, German, and Span-\nish to demonstrate the two-player data synthesis framework,\nit retains Qwen-2.5’s capacity to support all 29 languages.\nAcknowledgment\nWe thank Yi Zeng for providing early constructive sugges-\ntions on candidate models for the generator and one of the\nsource dataset SCOPE (Zeng et al., 2024b).\nImpact Statement\nThis work enhances moderation capabilities across lan-\nguages while addressing the scarcity of multilingual safety\ndata. Theoretical guarantees on convergence and empirical\ngains across six multilingual safety benchmarks demonstrate\nthe effectiveness and robustness of our approach.\nFrom an ethical standpoint, our method inherits common\nrisks associated with LLM moderation, such as potential\nbiases in training data and potential overreliance on certain\nshortcuts. Ensuring responsible synthetic data curation and\nevaluation is crucial for minimizing unintended harms.\nFurthermore, while our approach improves multilingual\nsafety alignment, it does not address all possible risks re-\nlated to adversarial attacks or nuanced cultural contexts in\nsafety assessments. Future research should explore tech-",
            "content": "niques for refining synthetic data generation, incorporating human oversight, and ensuring that moderation models remain robust across diverse linguistic and sociocultural settings. Our work underscores the importance of scalable, multilingual safety solutions and provides foundation for further advancements in responsible LLM alignment. References Aakanksha, Ahmadian, A., Ermis, B., Goldfarb-Tarrant, S., Kreutzer, J., Fadaee, M., and Hooker, S. The multilingual alignment prism: Aligning global and local preferences to reduce harm, 2024. URL https://arxiv.org/ abs/2406.18682. Ahmadian, A., Cremer, C., Galle, M., Fadaee, M., Kreutzer, Ustun, A., and Hooker, S. Back J., Pietquin, O., to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Alonso, I., Oronoz, M., and Agerri, R. Medexpqa: Multilingual benchmarking of large language models for medical question answering. Artificial Intelligence in Medicine, pp. 102938, 2024. ISSN 0933-3657. https://doi.org/10.1016/j.artmed.2024.102938. doi: https://www.sciencedirect.com/ URL science/article/pii/S0933365724001805. Aryabumi, V., Dang, J., Talupuru, D., Dash, S., Cairuz, D., Lin, H., Venkitesh, B., Smith, M., Campos, J. A., Tan, Y. C., et al. Aya 23: Open weight releases to further multilingual progress. arXiv preprint arXiv:2405.15032, 2024. Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Bai, T., Luo, J., Zhao, J., Wen, B., and Wang, Q. Recent advances in adversarial training for adversarial robustness. arXiv preprint arXiv:2102.01356, 2021. Bhardwaj, R., Anh, D. D., and Poria, S. Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic. arXiv preprint arXiv:2402.11746, 2024. Bi, W., Li, H., and Huang, J. Data augmentation for text generation without any augmented data. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 22232237, Online, 9 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.173. URL https: //aclanthology.org/2021.acl-long.173/. Caswell, I., Chelba, C., and Grangier, D. Tagged backtranslation. In Bojar, O., Chatterjee, R., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Huck, M., Yepes, A. J., Koehn, P., Martins, A., Monz, C., Negri, M., Neveol, A., Neves, M., Post, M., Turchi, M., and Verspoor, K. (eds.), Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pp. 5363, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5206. URL https://aclanthology.org/W19-5206/. Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Cheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., and Du, N. Self-playing adversarial language game enhances llm reasoning. arXiv preprint arXiv:2404.10642, 2024. Corrˆea, N. K. Dynamic normativity: Necessary and sufficient conditions for value alignment. arXiv preprint arXiv:2406.11039, 2024. de Wynter, A., Watts, I., Altıntoprak, N. E., Wongsangaroonsri, T., Zhang, M., Farra, N., Baur, L., Claudet, S., Gajdusek, P., Goren, C., et al. Rtp-lx: Can llms evaluate toxicity in multilingual scenarios? arXiv preprint arXiv:2404.14397, 2024. Dementieva, D., Moskovskiy, D., Babakov, N., Ayele, A. A., Rizwan, N., Schneider, F., Wang, X., Yimam, S. M., Ustalov, D., Stakovskii, E., Smirnova, A., Elnagar, A., Mukherjee, A., and Panchenko, A. Overview of the multilingual text detoxification task at pan 2024. In Faggioli, G., Ferro, N., Galuˇsˇcakova, P., and de Herrera, A. G. S. (eds.), Working Notes of CLEF 2024 - Conference and Labs of the Evaluation Forum. CEUR-WS.org, 2024. Dritsoula, L., Loiseau, P., and Musacchio, J. gameIEEE theoretic analysis of adversarial classification. Transactions on Information Forensics and Security, 12 (12):30943109, 2017. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Edunov, S., Ott, M., Auli, M., and Grangier, D. Understanding back-translation at scale. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J. (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 489500, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1045. URL https://aclanthology.org/D18-1045/. Ghosh, S., Varshney, P., Galinkin, E., and Parisien, C. Aegis: Online adaptive ai content safety moderation with ensemble of llm experts. arXiv preprint arXiv:2404.05993, 2024. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Han, S., Rao, K., Ettinger, A., Jiang, L., Lin, B. Y., Lambert, N., Choi, Y., and Dziri, N. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. arXiv preprint arXiv:2406.18495, 2024a. Han, S., Rao, K., Ettinger, A., Jiang, L., Lin, B. Y., Lambert, N., Choi, Y., and Dziri, N. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms, 2024b. URL https://arxiv.org/abs/ 2406.18495. Hosseini, A., Yuan, X., Malkin, N., Courville, A., Sordoni, A., and Agarwal, R. V-star: Training verifiers for selftaught reasoners. arXiv preprint arXiv:2402.06457, 2024. Deng, Y., Zhang, W., Pan, S. J., and Bing, L. Multilingual jailbreak challenges in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=vESNKdEMGp. Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., Tontchev, M., Hu, Q., Fuller, B., Testuggine, D., et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024a. Jain, D., Kumar, P., Gehman, S., Zhou, X., Hartvigsen, T., and Sap, M. Polyglotoxicityprompts: Multilingual evaluation of neural toxic degeneration in large language models. arXiv preprint arXiv:2405.09373, 2024. Dong, Y., Mu, R., Jin, G., Qi, Y., Hu, J., Zhao, X., Meng, J., Ruan, W., and Huang, X. Building guardrails for large language models. arXiv preprint arXiv:2402.01822, 2024b. Ji, J., Hong, D., Zhang, B., Chen, B., Dai, J., Zheng, B., Qiu, T., Li, B., and Yang, Y. Pku-saferlhf: safety alignment preference dataset for llama family models. arXiv e-prints, pp. arXiv2406, 2024a. 10 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Ji, J., Liu, M., Dai, J., Pan, X., Zhang, C., Bian, C., Chen, B., Sun, R., Wang, Y., and Yang, Y. Beavertails: Towards improved safety alignment of llm via human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024b. Jiang, L., Rao, K., Han, S., Ettinger, A., Brahman, F., Kumar, S., Mireshghallah, N., Lu, X., Sap, M., Choi, Y., and Dziri, N. Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models, 2024. URL https://arxiv.org/abs/2406.18510. Lambert, N., Pyatkin, V., Morrison, J., Miranda, L., Lin, B. Y., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Li, B. and Vorobeychik, Y. Feature cross-substitution in adversarial classification. Advances in neural information processing systems, 27, 2014. Li, L., Dong, B., Wang, R., Hu, X., Zuo, W., Lin, D., Qiao, Y., and Shao, J. Salad-bench: hierarchical and comprehensive safety benchmark for large language models. arXiv preprint arXiv:2402.05044, 2024. Lian, W., Goodson, B., Pentland, E., Cook, A., Vong, C., and Teknium. Openorca: An open dataset of gpt augmented flan reasoning traces. https://https: //huggingface.co/Open-Orca/OpenOrca, 2023. Liao, B., Khadivi, S., and Hewavitharana, S. Backtranslation for large-scale multilingual machine translation. In Barrault, L., Bojar, O., Bougares, F., Chatterjee, R., Costa-jussa, M. R., Federmann, C., Fishel, M., Fraser, A., Freitag, M., Graham, Y., Grundkiewicz, R., Guzman, P., Haddow, B., Huck, M., Yepes, A. J., Koehn, P., Kocmi, T., Martins, A., Morishita, M., and Monz, C. (eds.), Proceedings of the Sixth Conference on Machine Translation, pp. 418424, Online, November 2021. Association for Computational Linguistics. URL https: //aclanthology.org/2021.wmt-1.50/. Lin, Z., Wang, Z., Tong, Y., Wang, Y., Guo, Y., Wang, Y., and Shang, J. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation. arXiv preprint arXiv:2310.17389, 2023. Logacheva, V., Dementieva, D., Ustyantsev, S., Moskovskiy, D., Dale, D., Krotova, I., Semenov, N., and Panchenko, A. Paradetox: Detoxification with parallel data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 68046818, 2022. Ma, H., Hu, T., Pu, Z., Liu, B., Ai, X., Liang, Y., and Chen, M. Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2410.06101, 2024. Machado, G. R., Silva, E., and Goldschmidt, R. R. Adversarial machine learning in image classification: survey toward the defenders perspective. ACM Computing Surveys (CSUR), 55(1):138, 2021. Madry, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. Marie, B., Rubino, R., and Fujita, A. Tagged backtranslation revisited: Why does it really work? In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 59905997, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.532. URL https: //aclanthology.org/2020.acl-main.532/. Markov, T., Zhang, C., Agarwal, S., Nekoul, F. E., Lee, T., Adler, S., Jiang, A., and Weng, L. holistic approach to undesired content detection in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1500915018, 2023. Munos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang, Y., Geist, M., Mesnard, T., Michi, A., et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023. OpenAI. Gpt-4 technical report, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Pham, H., Wang, X., Yang, Y., and Neubig, G. Meta backIn International Conference on Learning translation. Representations, 2021. URL https://openreview. net/forum?id=3jjmdp7Hha. Qi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github. io/blog/qwen2.5/. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 11 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290, 2023. Rottger, P., Seelawi, H., Nozza, D., Talat, Z., and Vidgen, B. Multilingual hatecheck: Functional tests for multilingual hate speech detection models. arXiv preprint arXiv:2206.09917, 2022. Rottger, P., Kirk, H. R., Vidgen, B., Attanasio, G., Bianchi, F., and Hovy, D. Xstest: test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Sennrich, R., Haddow, B., and Birch, A. Improving neural machine translation models with monolingual data. In Erk, K. and Smith, N. A. (eds.), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8696, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL https: //aclanthology.org/P16-1009/. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shen, L., Tan, W., Chen, S., Chen, Y., Zhang, J., Xu, H., Zheng, B., Koehn, P., and Khashabi, D. The language barrier: Dissecting safety challenges of llms in multilingual contexts. arXiv preprint arXiv:2401.13136, 2024a. Shen, X., Chen, Z., Backes, M., Shen, Y., and Zhang, Y. do anything now: Characterizing and evaluating inthe-wild jailbreak prompts on large language models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, pp. 16711685, 2024b. Swamy, G., Dann, C., Kidambi, R., Wu, Z. S., and Agarwal, A. minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024. Tonneau, M., Liu, D., Fraiberger, S., Schroeder, R., Hale, S., and Rottger, P. From languages to geographies: Towards evaluating cultural bias in hate speech datasets. In Chung, Y.-L., Talat, Z., Nozza, D., Plaza-del Arco, F. M., Rottger, P., Mostafazadeh Davani, A., and Calabrese, A. (eds.), Proceedings of the 8th Workshop on Online Abuse and Harms (WOAH 2024), pp. 283311, Mexico City, Mexico, June 2024. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.woah-1.23. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Trillos, N. G. and Murray, R. Adversarial classification: Necessary conditions and geometric flows. Journal of Machine Learning Research, 23(187):138, 2022. Wang, W., Tu, Z., Chen, C., Yuan, Y., Huang, J.-t., Jiao, W., and Lyu, M. R. All languages matter: On the multilingual safety of large language models. arXiv preprint arXiv:2310.00905, 2023. Wei, A., Haghtalab, N., and Steinhardt, J. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36, 2024. Wu, Y., Sun, Z., Yuan, H., Ji, K., Yang, Y., and Gu, Q. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. Xie, T., Qi, X., Zeng, Y., Huang, Y., Sehwag, U. M., Huang, K., He, L., Wei, B., Li, D., Sheng, Y., Jia, R., Li, B., Li, K., Chen, D., Henderson, P., and Mittal, P. Sorry-bench: Systematically evaluating large language model safety refusal behaviors, 2024. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Xu, J., Ruan, Y., Bi, W., Huang, G., Shi, S., Chen, L., and Liu, L. On synthetic data for back translation. In Carpuat, M., de Marneffe, M.-C., and Meza Ruiz, I. V. (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 419430, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 32. URL https://aclanthology.org/2022. naacl-main.32/. Yang, Y., Dan, S., Roth, D., and Lee, I. Benchmarking llm guardrails in handling multilingual toxicity. arXiv preprint arXiv:2410.22153, 2024. DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Yao, Y., Duan, J., Xu, K., Cai, Y., Sun, Z., and Zhang, Y. survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, pp. 100211, 2024. Ye, M., Sikka, K., Atwell, K., Hassan, S., Divakaran, A., and Alikhani, M. Multilingual content moderation: case study on reddit. arXiv preprint arXiv:2302.09618, 2023. Zeng, W., Liu, Y., Mullins, R., Peran, L., Fernandez, J., Harkous, H., Narasimhan, K., Proud, D., Kumar, P., Radharapu, B., et al. Shieldgemma: Generative ai content moderation based on gemma. arXiv preprint arXiv:2407.21772, 2024a. Zeng, Y., Nguyen, A., Li, B., and Jia, R. Scope: Scalable and adaptive evaluation of misguided safety refusal in llms. https://openreview.net/forum?id= 72H3w4LHXM, 2024b. Zheng, L., Chiang, W.-L., Sheng, Y., Li, T., Zhuang, S., Wu, Z., Zhuang, Y., Li, Z., Lin, Z., Xing, E. P., et al. Lmsys-chat-1m: large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998, 2023. Zhou, R., Du, S. S., and Li, B. Reflect-rl: Two-player online rl fine-tuning for lms. arXiv preprint arXiv:2402.12621, 2024. Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. 13 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Theoretical Analysis In this section, we provide detailed theoretical analysis about our two-player minimax game framework. A.1 Minimizer of Loss First of all, we derive the solution of the optimization objectives defined in Equations (4.1) and (4.3). A.1.1 GENERATOR Recall that the corresponding PPO training objective of DPO objective (4.3) is: PPO(ϕ, ϕref) = E(x,y)ρ(x,y) Lt (cid:2)E (cid:101)xpϕ((cid:101)xx,y)[rt(x, (cid:101)x)] βDKL(pϕpref)(cid:3), (A.1) where ρ(x, y) is the data distribution and rt((x, y), (cid:101)x) = log pθt(y(cid:101)x) is the reward function defined in (4.2). We will show that the DPO objective (4.3) and the PPO objective A.1 shares the same minimizer. Azar et al. (2024) provided the following connection between the PPO and DPO objectives. Proposition A.1 (Proposition 4 in Azar et al. (2024)). Let the DPO training objective be LDPO(ϕ, ϕref) = ExρEyw,ylµ(x) (cid:20) P(yw ylx)ℓ (cid:18) β log pϕ(ywx) pϕref (ywx) β log pϕ(ylx) pϕref(ylx) (cid:19)(cid:21) , and the RLHF training objective be LPPO(ϕ, ϕref) = Exρ(x)Eypϕ(x)[r(y, x)] βDKL(pϕpref). Consider preference model such that there exists minimizer to the Bradley-Terry loss arg min ExρEyw,ylµ(x) [p(yw ylx) log σ(r(x, yw) r(x, yl))] . Then, the optimal policy for the DPO objective and for the RLHF objective with the reward model given as the minimizer to the Bradley-Terry loss above are identical, regardless of whether or not corresponds to Bradley-Terry preference model. Therefore, we only need to show that the reward function is the minimizer of the Bradley-Terry loss. Lemma A.2. Let σ be the sigmoid function and p((cid:101)xw (cid:101)xlx, y) = σ(cid:0)r((x, y), (cid:101)xw) r((x, y), (cid:101)xl)(cid:1). Then, we have p((cid:101)xw (cid:101)xlx, y) log σ(cid:0)r((x, y)(cid:101)xw) r((x, y), (cid:101)xl)(cid:1) = r((x, y), (cid:101)x) + c(x, y). (x,y)ρ(x,y) (cid:21) (cid:20) argmin (cid:101)xw,(cid:101)xlpϕn (x,y) Proof of Lemma A.2. The objective can be viewed as cross-entropy between the distribution p((cid:101)xw (cid:101)xl x, y) and σ(cid:0)r((x, y), (cid:101)xw) r((x, y), (cid:101)xl)(cid:1). In particular, the objective depends only on the difference r((x, y), (cid:101)xw) r((x, y), (cid:101)xl). Hence the value of the objective doesnt change if we replace by (cid:101)r((x, y), (cid:101)x) = r((x, y), (cid:101)x) + c(x, y). The function p((cid:101)xw (cid:101)xlx, y) is given by the sigmoid p((cid:101)xw (cid:101)xlx, y) = σ(cid:0)r((x, y), (cid:101)xw) r((x, y), (cid:101)xl))(cid:1). Minimizing the cross-entropy is achieved exactly when σ(cid:0)r((x, y)(cid:101)xw) r((x, y)(cid:101)xl)(cid:1) = σ(cid:0)r((x, y), (cid:101)xw) r((x, y), (cid:101)xl)(cid:1) for all x, (cid:101)xw, (cid:101)xl, y. Since the sigmoid is strictly increasing, we have r((x, y), (cid:101)xw) r((x, y), (cid:101)xl) = r((x, y), (cid:101)xw) r((x, y), (cid:101)xl). The solution is r((x, y), (cid:101)x) = r((x, y), (cid:101)x) + c(x, y). Then, by Proposition A.1 and Lemma A.2, the DPO objective (4.3) shares the same minimizer with its corresponding PPO training objective (A.1). In addition, according to Rafailov et al. (2023), the minimizer is pϕn+1 ((cid:101)xx, y) = 1 Z(x, y) pref((cid:101)xx, y) exp(β1[ log pθn (y(cid:101)x)]) pref((cid:101)xx, y) exp(β1[ log pθn (y(cid:101)x)]), where Z(x, y) = (cid:101)xpref((cid:101)xx,y) exp(β1[ log pθn(y(cid:101)x)]) is the normalization term. DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails A.1.2 CLASSIFIER Next, we will derive the solution to the objective (4.1). We first prove tool lemma. Lemma A.3. Let p(y, (cid:101)x) be joint distribution over (y, (cid:101)x). Then max E(y,(cid:101)x)p(y,(cid:101)x) (cid:2) log q(y(cid:101)x)(cid:3) = H[p(y(cid:101)x)], and the maximizer is q(y(cid:101)x) = p(y(cid:101)x). Here, is the entropy. Proof of Lemma A.3. (cid:2) log q(y(cid:101)x)(cid:3) L(q) = E(y,(cid:101)x)p(y,(cid:101)x) = Ep((cid:101)x) = Ep((cid:101)x) Ep((cid:101)x) (cid:2)E(y(cid:101)x)p(y,(cid:101)x) (cid:2)E(y(cid:101)x)p(y,(cid:101)x) (cid:2)E(y(cid:101)x)p(y,(cid:101)x) (cid:2) log q(y(cid:101)x)(cid:3)(cid:3) (cid:2) log p(y(cid:101)x)(cid:3) DKL(p(y(cid:101)x)q(y(cid:101)x))(cid:3) (cid:2) log p(y(cid:101)x)(cid:3)(cid:3), and the last equity holds if and only if p(y(cid:101)x) = q(y(cid:101)x). Then, we can calculate the minimizer of (4.1). Lemma A.4. (cid:82) ρ(x, y)pϕn ((cid:101)xx, y)dx (cid:46)(cid:82) ρ(x, y)pϕn ((cid:101)xx, y)dxdy is the minimizer to the following optimization problem: argmin (x,y)ρ(x,y) (cid:101)xpϕn ((cid:101)xx,y) (cid:2) log q(y(cid:101)x)(cid:3). Proof of Lemma A.4. The joint distribution of (y, (cid:101)x) is (cid:90) p(y, (cid:101)x) = ρ(x, y)pϕn ((cid:101)xx, y)dx, and the marginal distribution of (cid:101)x is (cid:90) p((cid:101)x) = ρ(x, y)pϕn ((cid:101)xx, y)dxdy. We can restate the optimization problem as argmax E(y,(cid:101)x)p(y,(cid:101)x) (cid:2) log q(y(cid:101)x)(cid:3). By Lemma A.3, the solution is q(y(cid:101)x) = p(y(cid:101)x) = p(y, (cid:101)x) p((cid:101)x) = (cid:82) ρ(x, y)pϕn ((cid:101)xx, y)dx (cid:82) ρ(x, y)pϕn ((cid:101)xx, y)dxdy . Therefore, for the classifier, by Lemma A.4, we have pθn+1(y(cid:101)x) = argmin (x,y)ρ(x,y) (cid:101)xpϕn ((cid:101)xx,y) [ log q(y(cid:101)x)] = (cid:82) ρ(x, y)pϕn((cid:101)xx, y)dx (cid:82) ρ(x, y)pϕn ((cid:101)xx, y)dxdy . In two player game perspective, pθn+1 can be viewed as the best response to pϕn , and pϕn+1 can be viewed as the best response to pθn. For simplicity, we denote that pθn+1 = Tθ(pϕn ) and pϕn+1 = Tϕ(pθn ). 15 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails A.2 Nash Equilibrium A.2.1 SETUP In our two-player game framework, we indeed optimize the following minimax two player game: (cid:2)E (cid:101)xpϕ((cid:101)xx,y)[ log pθt(y(cid:101)x)] βDKL(pϕ(x, y)pref(x, y))(cid:3), (pϕ, pθ) := E(x,y)ρ(x,y) min θ max ϕ We observe that (pϕ, pθ) is concave on pϕ since the first term is linear in pϕ and DKL(pϕpref) is convex in pϕ. In addition, (pϕ, pθ) is convex in pθ since log is concave function. Theorem A.5 (Von Neumanns Minimax Theorem). Let Rn and Rm be compact convex sets. If : is continuous function that is concave-convex, i.e. Then, we have that (, y) : is concave for every fixed Y, and (x, ) : is convex for every fixed X. max xX min yY (x, y) = min yY max xX (x, y). By Von Neumanns Minimax Theorem, we have min pθ max pϕ (pϕ, pθ) = max pϕ min pθ (pϕ, pθ). We further enforce the following regularity conditions: Both and (cid:101)X are finite discrete sets of tokens, with = < and (cid:101)X = (cid:101)X < . We constrain pθ within half-space of the Euclidean space, ensuring pθ(yx) γ > 0. The normalization term of the generator distribution is strictly positive: (cid:88) pref((cid:101)xx, y) exp (cid:0)β1[ log pθ(y(cid:101)x)](cid:1) δ > 0. The distribution pϕ is non-degenerate, i.e., (cid:80) y=1 (cid:80) xX ρ(x, y)pϕ((cid:101)xx, y) α > 0. (cid:101)x (cid:101)X A.2.2 EXISTENCE OF NASH EQUILIBRIUM We will first show that Nash equilibrium exists in the two-player game. Nash equilibrium in this game is state where no player can improve their payoff by unilaterally changing their strategy, assuming that the other player keeps their strategy fixed. Since our update rules correspond to the best response to the opponents policy, the existence of Nash equilibrium is equivalent to our updating rule having fixed point. Let (S) mean the set of probability distribution over the set S. Therefore, the pθ amounts to choosing element from space Θ = (cid:81) (x,y)X {1} ( (cid:101)X ), which is compact and convex set in R2X (cid:101)X . Hence, the joint parameter space is Ψ = Θ Φ, which is compact, convex subset of the R2 (cid:101)X+2X (cid:101)X . For simplicity, we also write ψ = (ϕ, θ). We define mapping to represent our update rule: (cid:101)x (cid:101)X ({1}), which is compact and convex set in R2 (cid:101)X . Similarly, pϕ is element in Φ = (cid:81) : Ψ Ψ (pθ, pϕ) = (Tθ(pϕ), Tϕ(pθ)). Thus, is continuous map from the compact convex set Ψ into itself. Theorem A.6 (Brouwers Fixed Point Theorem). Every continuous function from nonempty convex compact subset of Euclidean space to itself has fixed point. By the Brouwers Fixed Point Theorem, there is (p θ, (p ϕ) Ψ such that θ, ϕ) = (p θ, ϕ). By definition of T, this means that θ = Tθ(p ϕ), since that Tθ and Tϕ are both best response to the opponents policy, (p ϕ) is indeed the Nash equilibrium. ϕ = Tϕ(p θ), θ, DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails A.2.3 CONVERGENCE TO NASH EQUILIBRIUM In this section, we first show that both Tθ and Tϕ are Lipschitz, and then we prove that our algorithm converges to the fixed point. Lipschitz Mapping Tϕ. Recall that Tϕ(pθ)((cid:101)xx, y) = (cid:80) pref((cid:101)xx, y) exp(β1[ log pθn (y(cid:101)x)]) (cid:101)x pref((cid:101)xx, y) exp(β1[ log pθn (y(cid:101)x)]) . Let gθ((cid:101)x, y) = exp (cid:0)β1[ log pθ(y(cid:101)x)](cid:1), by the regularity conditions, we have (cid:12) (cid:12)β1(cid:0)pθ(yx)(cid:1)1 (cid:12) exp (cid:0)β1[ log pθ(y(cid:101)x)](cid:1)(cid:12) = (cid:12) (cid:12) (cid:12) (cid:12) gθ((cid:101)x, y) pθ(y(cid:101)x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) β1γ1β1 (cid:12) ."
        },
        {
            "title": "We rewrite",
            "content": "gθ((cid:101)x, y) gθ((cid:101)x, y) β1γ1β1 pθ(y(cid:101)x) pθ(y(cid:101)x). Tϕ(pθ)((cid:101)xx, y) = Nθ((cid:101)x, x, y) Dθ(x, y) , where Nθ((cid:101)x, x, y) = pref((cid:101)xx, y)gθ((cid:101)x, y), Dθ(x, y) = Nθ((cid:101)x, x, y). (cid:88) (cid:101)x (cid:101)X Then, we have (cid:88) Tϕ(pθ) Tϕ(pθ)((cid:101)xx, y) (cid:101)x (cid:101)X (cid:88) = (cid:101)x (cid:101)X (cid:88) (cid:101)x (cid:101)X (cid:88) = = = And we have (cid:101)x (cid:101)X 1 Dθ(x, y) 1 Dθ(x, y) (cid:101)x (cid:101)X (cid:16) (cid:88) (cid:101)x (cid:101)X (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Nθ((cid:101)x, x, y) Dθ(x, y) Nθ((cid:101)x, x, y) Dθ(x, y) Nθ((cid:101)x, x, y) Dθ(x, y) (cid:12) (cid:12) (cid:12) (cid:12) Nθ((cid:101)x, x, y) Dθ(x, y) + Nθ((cid:101)x, x, y) Dθ(x, y) Nθ((cid:101)x, x, y) Dθ(x, y) (cid:12) (cid:12) (cid:12) (cid:12) Nθ((cid:101)x, x, y) Nθ((cid:101)x, x, y) Dθ(x, y) (cid:88) + (cid:101)x (cid:101)X Nθ((cid:101)x, x, y) (cid:12) (cid:12) (cid:12) (cid:12) 1 Dθ(x, y) 1 Dθ(x, y) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) Nθ((cid:101)x, x, y) Nθ((cid:101)x, x, y) + Dθ(x, y) Dθ(x, y)Dθ(x, y) (cid:12) (cid:12) (cid:12) (cid:12) Dθ(x, y) Dθ(x, y) (cid:12) (cid:12) (cid:12) (cid:12) Nθ((cid:101)x, x, y) Nθ((cid:101)x, x, y) + (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Dθ(x, y) Dθ(x, y) (cid:12) (cid:17) . Nθ((cid:101)x, x, y) Nθ((cid:101)x, x, y) pref((cid:101)xx, y)gθ((cid:101)x, y) gθ((cid:101)x, y) gθ((cid:101)x, y) gθ((cid:101)x, y), gθ((cid:101)x, y) gθ((cid:101)x, y). pref((cid:101)xx, y)gθ((cid:101)x, y) gθ((cid:101)x, y) Dθ(x, y) Dθ(x, y) (cid:88) (cid:88) In addition, by the regularity conditions, we have that (cid:101)x (cid:101)X (cid:101)x (cid:101)X (cid:88) (cid:88) (cid:88) Tϕ(pθ) Tϕ(pθ)((cid:101)xx, y) xX y= (cid:101)x (cid:101)X (cid:88) (cid:88) xX y=1 (cid:88) (cid:88) xX y=1 2 δ (cid:88) (cid:101)x (cid:101)X 2 Dθ(x, y) (cid:88) gθ((cid:101)x, y) gθ((cid:101)x, y) (cid:101)x (cid:101)X β1γ1β pθ(y(cid:101)x) pθ(y(cid:101)x) 17 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails = 2δ1β1γ1β1 (cid:88) (cid:88) yY (cid:101)x (cid:101)X pθ(y(cid:101)x) pθ(y(cid:101)x)."
        },
        {
            "title": "This means that",
            "content": "Tϕ(pθ) Tϕ(pθ)1 2δ1β1γ1β1 pθ pθ1. Lipschitz Mapping Tθ. Recall that Tθ(pϕ)(y(cid:101)x) = (cid:80) xX (cid:80) xX ρ(x, y)pϕ((cid:101)xx, y) (cid:80) y=1 ρ(x, y)pϕ((cid:101)xx, y) ."
        },
        {
            "title": "Denote that",
            "content": "Then, In addition, Therefore, Tθ(pϕ)(y(cid:101)x) = Nϕ(y, (cid:101)x) Dϕ((cid:101)x) , where Nϕ(y, (cid:101)x) = (cid:88) xX ρ(x, y)pϕ((cid:101)xx, y), Dϕ((cid:101)x) = (cid:88) y=1 Nϕ(y, (cid:101)x). Tθ(pϕ) Tθ(pϕ)(y(cid:101)x) = (cid:88) (cid:88) yY Nϕ(y, (cid:101)x) Dϕ((cid:101)x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Nϕ(y, (cid:101)x) Dϕ((cid:101)x) (cid:18) (cid:88) y=1 y=1 1 Dϕ((cid:101)x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + (cid:12)Nϕ(y, (cid:101)x) Nϕ(y, (cid:101)x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Dϕ((cid:101)x) Dϕ((cid:101)x) (cid:12) (cid:19) . (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Nϕ(y, (cid:101)x) Nϕ(y, (cid:101)x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Dϕ((cid:101)x) Dϕ((cid:101)x) (cid:88) xX (cid:88) ρ(x, y)pϕ((cid:101)xx, y) pϕ((cid:101)xx, y) (cid:88) ρ(x, y)pϕ((cid:101)xx, y) pϕ((cid:101)xx, y). y=1 xX (cid:88) (cid:88) y=1 (cid:101)x (cid:101)X Tθ(pϕ) Tθ(pϕ)(y(cid:101)x) 2 Dϕ((cid:101)x) (cid:88) (cid:101)x (cid:101)X (cid:88) (cid:88) y=1 xX ρ(x, y)pϕ((cid:101)xx, y) pϕ((cid:101)xx, y). Let the marginal of ρ(x) be uniform distribution on the space . Then we have ρ(x, y) 1/X . By the regularity conditions, we have Tθ(pϕ) Tθ(pϕ)1 2α1X 1pϕ pϕ1. Proof of Convergence. With proper choice of β, we have Tϕ is α1-Lipchitz and Tθ α2-Lipchitz, with α1α2 = 4δ1β1γβ11α1 < 1 (this can be ensured if β is large enough). That is, Tϕ(pθ) Tϕ(pθ) α1pθ pθ, Tθ(pϕ) Tθ(pϕ) α2pϕ pϕ. Then, we have 2(pψ) 2(pψ) = Tϕ(Tθ(pϕ)) Tϕ(Tθ(pϕ)) + Tθ(Tψ(pθ)) Tθ(Tψ(pθ)) α1α2pϕ pϕ + α1α2pθ pθ = α1α2pψ pψ. Hence, 2 is contraction map on the compact space Ψ. 18 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Theorem A.7 (Banach Fixed Point Theorem). Let (X, d) be complete metric space and let : be contraction mapping, meaning that there exists constant 0 < 1 such that for all x, X, d(T (x), (y)) d(x, y). Then has unique fixed point X, meaning that (x) = x. Moreover, for any x0 X, the sequence defined by xn+1 = (xn) converges to x. By Banach Fixed Point Theorem, 2 converges to its unique fixed point. Therefore, the two subsequences {T 2k(pψ0 )} and {T 2k+1(pψ0)} converge to the same fixed point. Therefore, {T k(ψ0)} have k=0 k=0 both converge on the compact space Ψ. Since 2 has unique fixed point, these two subsequences k=0, we k=0 converges. In addition, for the subsequence {T 2k(pψ0)} 2n+2(pψ0 ) pψ 2n(pψ0) pψ α1α2 < 1. k=0. Therefore, both subsequences converge linearly to the fixed point Similarly, similar inequality holds for {T 2k+1(pψ0 )} pψ . Therefore, for any ϵ, we can get an ϵ-equilibrium policy pψ, i.e., pψ pψ ϵ, within O(log(1/ϵ)) iterations. Additional Related Work Adversarial training in classification (Bai et al., 2021; Machado et al., 2021) has been approached through robust optimization, game theory, and algorithmic defenses aimed at training models resilient to adversarial attacks. One theoretical study derived necessary conditions for an optimal robust classifier under bounded input perturbations and described how decision boundaries evolve via mean-curvature flow as the adversarys budget increases (Trillos & Murray, 2022). complementary game-theoretic approach models classification as two-player game: the attacker generates malicious inputs while the defender optimizes randomized classifier strategy, yielding Nash equilibrium rather than worst-case fixed solution (Dritsoula et al., 2017). Others have focused on specific attack vectors; for example, substituting features like synonyms in text to evade detection, with proposed defenses including simple feature-level heuristics and mixed-integer programming to jointly optimize feature selection under adversarial evasion constraints (Li & Vorobeychik, 2014). Classical adversarial training methods (e.g., FGSM (Goodfellow et al., 2014) or PGD-based training (Madry, 2017)) generally augment data by applying small perturbations to existing inputs (Trillos & Murray, 2022), thereby improving robustness on in-distribution variations but not introducing fundamentally new examples or languages. As result, these perturbation-focused techniques remain limited in multilingual settings, since they cannot generate adversarial data in languages beyond the original training distribution. In contrast, two-player self-improving framework for LLMs can leverage generative adversary to produce novel challenging examples (across different languages) and defender model that learns from them, expanding the training distribution beyond mere perturbed replicas and enhancing cross-lingual robustness. Seed Data Details Figure 8. Data proportion by language in our collected seed data from open sources. In Figure 8, we show the overall proportion of data by language in our collected and processed seed data. Below, we list the sources of our seed training data gathered from HuggingFace. We also note the additional processing measure we took to ensure data quality for each source. At the last step of seed data curation, we conduct deduplication and decontamination from the test benchmarks. 19 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails BeaverTail (Ji et al., 2024b) training set, containing both safe and unsafe data. Upon manual inspection, we make the following notes: In BeaverTail, safety is labeled based on the instruction-response pair. Same instruction with different responses may have different labels. Moreover, same QA pair has 3 labels from different label workers, resulting in 3 data examples in the dataset. We consider prompts as safe if all labels are safe, and unsafe if any one label is unsafe. We only considere responses as unsafe if all labels are unsafe, and disregard the rest data. ToxicChat (Lin et al., 2023) training set, containing both safe and unsafe data. Aegis AI Content Safety Dataset 1.0 (Ghosh et al., 2024), containing both safe and unsafe data. WildJailbreak (Jiang et al., 2024) training set, containing both safe and unsafe data. WildGuardMix (Han et al., 2024b) training set, containing both safe and unsafe data. SaladBench (Li et al., 2024), containing both safe and unsafe data. SORRY-Bench (2024/06) (Xie et al., 2024), containing both safe and unsafe data. PKU-SafeRLHF-QA (Ji et al., 2024a), containing both safe and unsafe data. Kaggle Toxic Comment Classification challenge2, containing both safe and unsafe data. Upon manual inspection, we make the following notes: Safe data: data labeled as non-toxic further filtered by Llama-3.1 (8B), retaining 82,254 safe samples that agrees with the judge of Llama-3.1. Reddit Suicide Detection3, containing only unsafe data. Upon manual inspection, we make the following notes: Data are originally either labeled as suicidal or non-suicidal. However, we cannot consider the non-suicidal examples as safe. Therefore, we disregard all data labeled as non-suicidal. We consider the data labeled as suicidal as unsafe training data. We split the data by keyword detection, and downsample the set of data that contains the keywords kill and suicide to avoid over-reliance on just the keywords during model training. LMSYS-Chat-1M (Zheng et al., 2023), containing only safe data. We randomly sample 150k subset from the data to represent safe user inputs in daily LLM interactions. AI Medical Chatbot Dataset4, containing only safe data. We maintain only the description in our data, and remove the format (Q: ) in the original data. Medical QA5, containing only safe data. We maintain only the input in our data. Law-StackExchange6, containing only safe data. We maintain only the question title in our data. ParaDetox (Logacheva et al., 2022)7, containing both safe and unsafe data. SCOPE (Zeng et al., 2024b), containing safe data that are more likely to be classified as unsafe by models due to shortcut learning (over-cautiousness). Jailbreak Classification8, containing both safe and unsafe data, with jailbreak prompts source from (Shen et al., 2024b) and benign prompts source from (Lian et al., 2023). Prompt Injections9, containing both safe and unsafe data. Toxic-comments (Teeny-Tiny Castle)10, containing both safe and unsafe data. ForbiddenQuestions11, containing only unsafe data sourced from (Shen et al., 2024b). Toxic-Aira (Corrˆea, 2024)12, containing only unsafe instructions. CatHarmfulQA (Bhardwaj et al., 2024)13, containing only unsafe instructions. Multilingual safety data is much more scarce, and we included the following in our seed data: Aya Red-teaming (Aakanksha et al., 2024), containing both safe and unsafe data in English, French, and Spanish. 2https://huggingface.co/datasets/OxAISH-AL-LLM/wiki toxic, https://huggingface.co/datasets/Arsive/toxicity classification jigsaw 3https://huggingface.co/datasets/Lucidest/reddit-suicidal-classify-kaggle 4https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot 5https://huggingface.co/datasets/lavita/medical-qa-datasets 6https://huggingface.co/datasets/ymoslem/Law-StackExchange 7https://huggingface.co/datasets/s-nlp/en paradetox toxicity 8https://huggingface.co/datasets/jackhhao/jailbreak-classification 9https://huggingface.co/datasets/deepset/prompt-injections 10https://huggingface.co/datasets/AiresPucrs/toxic-comments 11https://huggingface.co/datasets/walledai/ForbiddenQuestions 12https://huggingface.co/datasets/nicholasKluge/toxic-aira-dataset 13https://huggingface.co/datasets/walledai/CatHarmfulQA 20 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Multilingual Toxicity Dataset (Dementieva et al., 2024)14, containing both safe and unsafe data in English, German, and Spanish. Multilingual HateCheck (Rottger et al., 2022), containing both safe and unsafe data in English, French, German, and Spanish. French Hate Speech Superset (Tonneau et al., 2024)15, containing both safe and unsafe data in French. German Hate Speech Superset (Tonneau et al., 2024)16, containing both safe and unsafe data in German. Spanish Hate Speech Superset (Tonneau et al., 2024)17, containing both safe and unsafe data in Spanish. MexExpQA (Alonso et al., 2024)18 containing only safe data in English, French and Spanish. PornHub Titles19, containing only unsafe data. We use language detection model to filter out the languages that we need (English, French, Spanish and German). French Instruct Sharegpt20, containing only safe French data. We only maintain the instructions in the original data. Fr Instructs21, containing only safe french-only instructions deduplicated from various sources. MedicalNER Fr22, containing only safe data in French. We maintain the text column of this dataset. Belgian-Law-QAFrench23, containing only safe data in French. We extract and maintain the user instructions. Databricks-Dolly-15k-Curated-Multilingual24, containing only safe data in French, German and Spanish. We maintain the instructions. Lambada OpenAI (Radford et al., 2019)25, containing only safe data. We only leverage the German and Spanish part as additional sources for safe data to mitigate the imbalance in language. For the collected unsafe data, we further assign fine-grained labels of the following 12 subcategories: Violent crimes Non-violent crimes Sex-related crimes Child sexual exploitation Specialized advice Privacy Intellectual property Indiscriminate weapons Hate Suicide and self-harm Sexual content Jailbreak prompts Each data may receive one or multiple labels. The mapping is done based on the datas original label with manual inspection. If the original label is not enough, we further apply Llama-3.1 to do the labeling with self-consistency over three queries. Experiment Details In Table 4 and 5, we detail the hyperparameters that we used for training the classifier and the generator. We further lay out the prompts we used for the generator as well as for judging the outputs safety. Our prompts draw inspiration from EvolInstruct (Xu et al., 2023). 14https://huggingface.co/datasets/textdetox/multilingual toxicity dataset 15https://huggingface.co/datasets/manueltonneau/french-hate-speech-superset 16https://huggingface.co/datasets/manueltonneau/german-hate-speech-superset 17https://huggingface.co/datasets/manueltonneau/spanish-hate-speech-superset 18https://huggingface.co/datasets/HiTZ/MedExpQA 19https://huggingface.co/datasets/Nikity/Pornhub?not-for-all-audiences=true 20https://huggingface.co/datasets/MaziyarPanahi/french instruct sharegpt 21https://huggingface.co/datasets/Enno-Ai/fr-instructs 22https://huggingface.co/datasets/TypicaAI/MedicalNER Fr 23https://huggingface.co/datasets/naimsassine/belgian-law-qafrench-dataset 24https://huggingface.co/datasets/argilla/databricks-dolly-15k-curated-multilingual 25https://huggingface.co/datasets/EleutherAI/lambada openai 21 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Table 4. Fine-tuning hyperparameters for the generator."
        },
        {
            "title": "Data type\nLearning rate\nOptimizer\nGlobal batch size\nGradient accumulation steps\nScheduler\nWarmup ratio\nNum train epochs\nGroup by length\nMax grad norm",
            "content": "bf16 5e-5 AdamW 640 4 Cosine 0.1 2 True 1.0 Table 5. Fine-tuning hyperparameters for the generator."
        },
        {
            "title": "Data type\nLearning rate\nOptimizer\nGlobal batch size\nGradient accumulation steps\nScheduler\nWarmup ratio\nBeta\nRPO alpha\nMax length\nNum train epochs",
            "content": "bf16 5e-7 AdamW 64 8 Cosine 0.1 0.01 0.4 1024 1 Prompt for Unsafe Data Generation <System> You are English-{language} Toxic Prompt Creator. Your goal is to draw inspiration from the #Given English Prompt# to create brand new toxic prompt in {language}. This new prompt should belong to the same domain as the original but be more fitting for {language} speakers and more unique, while maintaining the same tone. Ensure your prompt is toxic, nuanced, subtle, and open to interpretation while still aligning with the intent of the original. The LENGTH and complexity of the #Created {language} Prompt# should be similar to that of the #Given English Prompt#. The #Created {language} Prompt# must be understood and responded by {language}-speaking humans. Do not include #Created {language} Prompt# in your response. You are not allowed to respond to the #Given English Prompt#. You must create new prompt in {language}. <User> #Given English Prompt#: {Input} Please create ONE new prompt in {language} that is inspired by the #Given English Prompt#. Do not respond to the #Given English Prompt#. Use any explicit language when needed. #Created {language} Prompt#: 22 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails"
        },
        {
            "title": "Prompt for Safe Data Generation",
            "content": "<System> You are English-{language} Safe Prompt Creator. Your goal is to draw inspiration from the #Given English Prompt# to create brand new safe prompt in {language}. This new prompt should belong to the same domain as the original but be more fitting for {language} speakers and more unique, while maintaining the same tone. Ensure your prompt is nuanced, subtle, and open to interpretation while still aligning with the intent of the original. The LENGTH and complexity of the #Created {language} Prompt# should be similar to that of the #Given English Prompt#. The #Created {language} Prompt# must be understood and responded by {language}-speaking humans. Do not include #Created {language} Prompt# in your response. You are not allowed to respond to the #Given English Prompt#. You must create new prompt in {language}. <User> #Given English Prompt#: {Input} Please create ONE new prompt in {language} that is inspired by the #Given English Prompt#. Do not respond to the #Given English Prompt#. You must maintain the same tone. #Created {language} Prompt#:"
        },
        {
            "title": "Prompt for Safety Evaluation",
            "content": "<System> You are {language} Prompt Evaluator. Your goal is to analyze the following #{language} Prompt# in the specific {language} context for any inappropriate, unsafe, or offensive content. Your audience is English-speaking humans. You must only answer with English. <User> #{language} Prompt#: {Input} Evaluate the given #{language} Prompt# in the {language} context for the following aspect: **Language-Specific Unsafety**. Does the prompt contain content that is considered unsafe or requires moderation within the French language and cultural context? Assign rating from 1 to 5 for the given criterion (1 = Absolutely safe, does not meet the criterion at all, 5 = Extremely unsafe, fully meets the criterion). Justify your rating in 1-2 sentences. Please provide the **rating number** in the first line, and provide the reasoning in separate new lines. Additional Results Model confidence in incorrect predictions. critical question in improving model performance is understanding the nature of its errors. Do these errors primarily stem from unseen data, where the model exhibits uncertainty, or from spurious correlations, where the model demonstrates high confidence and relies on shortcuts learned from imbalanced real-world data? Figure 9 illustrates the output probability distribution of false positives and false negatives across the classifier trained on seed data. The distribution reveals notable skew: false negatives tend to cluster around probability of 0, while false positives concentrate near probability of 1. This indicates that the classifier often exhibits overconfidence in its incorrect predictions, raising concerns about its reliability when faced with challenging examples or distribution shifts. 23 DuoGuard: Two-Player RL-Driven Framework for Multilingual LLM Guardrails Figure 9. Output Probability Distribution of False Positives and False Negatives in the Classifier Trained on Seed Data. skewed distribution toward 0 for false negatives and toward 1 for false positives indicates higher classifier confidence in its incorrect predictions. Analysis across the four French datasets reveals that the classifier exhibits significant confidence in its false predictions."
        }
    ],
    "affiliations": [
        "University of California, Los Angeles",
        "University of Illinois at Urbana-Champaign",
        "VirtueAI"
    ]
}