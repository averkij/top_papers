{
    "paper_title": "Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG",
    "authors": [
        "Siavash Shams",
        "Richard Antonello",
        "Gavin Mischler",
        "Stephan Bickel",
        "Ashesh Mehta",
        "Nima Mesgarani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data, outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies."
        },
        {
            "title": "Start",
            "content": "Neuro2Semantic: Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG Siavash Shams1, Richard Antonello1, Gavin Mischler1, Stephan Bickel2, Ashesh Mehta2, Nima Mesgarani1 1Department of Electrical Engineering, Columbia University, USA 2The Feinstein Institutes for Medical Research, USA ss6928@columbia.edu 5 2 0 2 1 3 ] . [ 1 1 8 3 0 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Decoding continuous language from neural signals remains significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data, outperforming recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies. Index Terms: brain decoding, semantic reconstruction, transfer learning, natural language processing, large language models 1. Introduction Recent advances at the intersection of artificial intelligence (AI) and neuroscience have enabled powerful new modeling capabilities, particularly in the development of neural decoding models. These models aim to reconstruct stimuli or intentions based on measured neural activity [1]. Decoding models have been explored across various neuroimaging modalities, including intracranial EEG (iEEG) [2, 3, 4], functional magnetic resonance imaging (fMRI) [5, 6], magnetoencephalography (MEG) [7, 8], and electroencephalography (EEG) [9, 10]. These models have been applied to diverse settings such as imagined and perceived language [9, 7, 6], speech reconstruction [3, 11, 12], motor control [13, 14], and vision [15, 16, 17]. Of particular note are recent efforts showcasing the ability of these models to decode motor intention for speech at near real-time speeds with high accuracy [18, 19]. Such models have the potential to revolutionize speech therapies for those who suffer from maladies that make it difficult to produce speech. However, these approaches primarily focus on decoding motor intentions, which may not capture the full richness of linguistic semantic content. An alternative to decoding motor intention of speech is decoding the semantics of intended speech from elsewhere in cortex [20, 21]. While semantic decoding has been investigated using fMRI and MEG [6, 22], there is less research leveraging the higher temporal resolution and signal quality of iEEG for this purpose [23]. Despite the potential advantages of using iEEG for semantic decoding, existing methods face significant challenges when adapting to this domain, particularly due to data scarcity. To address these limitations, we propose Neuro2Semantic, novel framework that employs transfer learning to efficiently decode the semantics of perceived speech from iEEG recordings with limited data availability. Our approach has two main parts. First, we train an LSTM [24] adapter to align neural data with pre-trained text embedding space [25, 26] using contrastive loss function. Second, after aligning the neural embeddings, we fine-tune pre-trained text reconstruction model [27] to extract coherent text from the neural-aligned embeddings. This step allows for unconstrained text generation, moving beyond classification-based methods that are restricted to predefined vocabularies or limited sets of candidates. We demonstrate that our Neuro2Semantic framework can successfully perform few-shot reconstruction of the meaning of perceived speech in in-domain settings with as little as 30 minutes of neural data. Moreover, it achieves strong performance in zero-shot reconstruction in out-of-domain contexts, showcasing its ability to generalize to entirely new semantic content. Our results highlight the effectiveness of transfer learning as potential method for effective neural decoding. This advancement opens new possibilities for developing more flexible and data-efficient neural decoding models, with potential applications in augmentative and alternative communication technologies. While our dataset includes only three subjects, prior iEEG studies have shown meaningful results with similar sample sizes [23, 18]. We thus present this as an exploratory study. 2. Methods 2.1. Neuro2Semantic The proposed Neuro2Semantic framework, illustrated in Figure 1, is designed to map neural signals to their corresponding semantic content through two-phase training process. In the first phase, an LSTM adapter processes the neural data and aligns it with text embeddings obtained from pre-trained text embedding model (text-embedding-ada-002 [26]). In the second phase, we utilize the methodology outlined in the Vec2text framework [27], which translates these text embeddings back into natural language. During this phase, the LSTM adapter is frozen, and we fine-tune the Vec2text inversion model to reconstruct the text from the aligned neural embeddings. We release our code and trained models.1 2.1.1. LSTM adapter and alignment Neuro2Semantic employs an LSTM adapter to encode iEEG signals into fixed-dimensional embeddings , thereby aligning them with the semantic space of pre-trained text embeddings. To appear in Interspeech 2025 1Available at github.com/SiavashShams/neuro2semantic the model minimizes the distance between the current hypothesis embedding ˆe(x(t)) and the target embedding e, progressively enhancing the coherence and accuracy of the generated text. Mathematically, the goal is to solve the following optimization problem: ˆx = arg max cos(ˆe(x), e) (2) Here, cos(ˆe(x), e) represents the cosine similarity between the embedding of the generated text and the target embedding. The optimization seeks to find the text sequence that maximizes this similarity. The iterative refinement process can be expressed as: x(t+1) = arg max p(xe, x(t), ˆe(x(t))) (3) where p(xe, x(t), ˆe(x(t))) is the probability distribution over possible next texts conditioned on the target embedding e, the current hypothesis x(t), and its corresponding embedding ˆe(x(t)). The model updates the text hypothesis by comparing the embedding of the current hypothesis ˆe(x(t)) with the target embedding e, and generating new text hypothesis that is more aligned with e. The Vec2Text model employs an encoder-decoder transformer architecture conditioned on the previous text hypothesis x(t) and the target embedding e. This iterative process continues until the cosine similarity cos(ˆe(x), e) converges, resulting in text that accurately reflects the original semantic and syntactic structure of the text. During fine-tuning, the LSTM adapter is kept frozen to preserve the semantic alignment established in the first phase. Only the parameters of the Vec2Text corrector module are updated. The process begins by passing the preprocessed iEEG segments through the LSTM adapter to generate fixed-dimensional neural embeddings en. These embeddings, now aligned with the text embedding space, serve as input to the Vec2Text corrector, which aims to reconstruct the original text sequences = (x1, x2, . . . , xT ), using standard NLL loss objective. 2.2. Intracranial Recordings and Data Processing Three subjects undergoing surgical evaluation for drug-resistant epilepsy participated. Stereotactic EEG electrodes were implanted intracranially (iEEG) for epileptogenic localization, only responsive electrodes were used (same selection criteria as [29]). Prior to electrode implantation, all subjects provided written informed consent for research participation. The subjects listened to naturalistic recordings of people engaging in podcast-like conversations between several speakers. There were six different conversations used. In total, the task included about 30 minutes of speech. The envelope of the high-gamma band (70 150 Hz) of the neural recordings during listening was computed using the Hilbert transform [30] and downsampled to 100 Hz. total of 864 electrodes were used across the three subjects after filtering and significance selection. The research protocol was approved by the institutional review board at North Shore University Hospital. 2.3. Baseline Model For baseline comparison, we use the Bayesian decoding method proposed by Tang et al. In brief, the method employs beam search to generate candidate continuations, which are then evaluated and ranked using encoding models following Mischler et al. [29] by modeling the [6] to generate decoded stimuli. Figure 1: Neuro2Semantic architecture and training methodology. Training is split into 2 phases. In Phase 1, an adapter module is trained to output neural embedding that is aligned with fixed sentence embedding. In Phase 2, corrector module is trained to read out the neural embedding as continuous language. To achieve effective alignment between the neural embeddings generated by the LSTM adapter and the corresponding semantic embeddings, we employ an alignment loss that combines contrastive objective with batch-level similarity optimization [28]. Formally, the adapter is trained using an alignment loss that is weighted combination of contrastive loss objective and triplet margin loss objective: Lalignment = αLclip + (1 α)Ltriplet (1) This ensures that the neural embeddings are both closely aligned with their corresponding text embeddings and sufficiently distinct from non-corresponding pairs. 2.1.2. Vec2Text Corrector Module The second phase of the Neuro2Semantic framework focuses on transforming the aligned neural embeddings into coherent text sequences. This is accomplished by fine-tuning the Vec2Text corrector module [27], which is designed to invert text embeddings back into their original textual form. Although the Vec2Text model is pre-trained on large-scale text corpora, finetuning it with neural embeddings allows the model to adapt to the specific characteristics of neural embeddings, enhancing its ability to accurately reconstruct the original text from these embeddings. This inversion task is framed as controlled generation problem, which aims to generate text whose embedding ˆe(x) closely approximates the target embedding e. The model operates iteratively, starting with an initial hypothesis x(0) and refining it over multiple steps t. At each step, Figure 2: Performance comparison between Neuro2Semantic, baseline, and random control. The BLEU and BERTScore gains correspond to tangible boost in semantic accuracy (A) Boxplots of BERTScore (left) and BLEU Score (right) comparing the performance of Neuro2Semantic, the baseline model [6], and random control. Significance is indicated with star based on paired t-test (p < 0.05). (B) Out-of-domain performance for each method is shown. (C) Example sentence reconstructions from Neuro2Semantic (left), original text (middle), and baseline model (right). Samples represent moderately above-average performance rather than extreme cases. (D) The performance of the method is plotted across different percentages of training data. (E) The performance of the method is plotted across different percentages of electrode coverage. All error bars show one standard deviation. likelihood p(RS) of observing brain responses given stimulus as multivariate Gaussian with mean µ = ˆR(S) and covariance Σ estimated from encoding residuals. We modified this approach for iEEG by using encoding models based on the high-gamma band and by applying fewer and shorter finite impulse response delays to account for the absence of delayed hemodynamic response. This method was chosen as it represents state-of-the-art results in fMRI decoding and aligns closely with our objective of reconstructing perceived speech semantics through continuous generative decoding. 3. Experiments and Results 3.1. Experimental Setup Training Procedure. We trained the model using leave-oneout cross-validation approach, where the last trial of each story was left out for testing. Each trial was split into sentences, with the corresponding neural data segment from when the sentence was spoken used for training. This setup prevented any anticausal leakage of information when fine-tuning the language models while allowing the model to train on the semantic content of past sentences within the same conversation. This process was repeated for each of the six stories, with model performance evaluated after each epoch using cross-validation. The held-out trial from each story served as the test set for that split. During Phase 1, the LSTM adapter was trained for 100 epochs with batch size of 8, using the Adam optimizer [31] with learning rate of 1.3e-3. Once the adapter training was complete, its parameters were frozen for Phase 2, where the pre-trained corrector was fine-tuned for 2 epochs. In this phase, the corrector used only one step for the refinement process. The CLIP-based contrastive loss was employed with temperature parameter of τ = 0.1, and the α = 0.25 term was used to control the contribution between the contrastive loss and triplet margin loss. The chosen parameters were optimized by coordinate descent. To evaluate the quality of the reconstructed text, we used two metrics commonly applied in neural decoding analysis, specifically BLEU [32], and BERTScore [33]. These metrics are used to measure both the surface-level (BLEU) and semantic accuracy (BERTScore) of the generated text compared Table 1: Ablation study demonstrating the impact of the two training phases of Neuro2Semantic. Metrics are reported as mean standard deviation. Significant improvements over random are marked with * (p < 0.05) based on paired t-test. Model Random BERT BLEU -0.245 0.132 0.002 0.003 Baseline - (Tang et al.) 0.032 0.127 * 0.064 0.053 * Neuro2Semantic (Full Model) 0.195 0.128 * 0.079 0.062 * Neuro2Semantic - Adapter Only (Phase 1) 0.056 0.086 * 0.068 0.038 * Neuro2Semantic - Corrector Only (Phase 2) 0.100 0.099 * 0.035 0.045 * to the ground truth. 3.2. Performance Comparison: Neuro2Semantic, Baseline, and Phases of Training We evaluate Neuro2Semantic against baseline model from previous work [6] and random control to rigorously assess our approachs effectiveness. Results are k-fold cross validated using each of the six stories as test set per fold. Figure 2A presents boxplots illustrating the distribution of performance metrics across all sentence pairs, providing comprehensive view of each models variability and consistency. Additionally, Figure 2C demonstrates sample reconstructed sentences from each model alongside the original ground truth transcriptions, highlighting the qualitative improvements achieved by Neuro2Semantic. Our Neuro2Semantic model significantly outperforms the baseline, particularly in BERTScore, indicating its suitability for low-data settings. Ablation results comparing the impact of each stage of the training process to the full model and the baseline are shown in Table 1. 3.3. Zero-Shot Out-of-Domain Performance While the previous evaluations assessed Neuro2Semantics performance in settings where the model encountered familiar semantic contexts, it is also important to evaluate how well the model generalizes to completely unseen semantics. Here, we explore the zero-shot out-of-domain performance by holding out entire stories that the model has not been exposed to during training. This provides more challenging test of the models robustness and its ability to reconstruct coherent semantic content when faced with new contexts. Figure 2B presents bar plot showing Neuro2Semantics BERT and BLEU scores which consistently outperform the baseline model. In particular, the BERTScore shows substantial improvement, suggesting that the model can maintain semantic coherence even when exposed to entirely new stories. This result further indicates that Neuro2Semantic captures broader semantic patterns instead of just memorizing training examples. 3.4. Impact of Data and Electrode Scaling on Model Performance We evaluate how scaling both the training data and the number of electrodes affects the performance of the Neuro2Semantic model. First, we assess the impact of scaling the training data by training the model on random subsets of 20%, 40%, 60%, 80%, and 100% of the available data. For each subset percentage, five independent runs were conducted, with the standard deviation across runs displayed as error bars in Figure 2D. As the training data increases, we observe significant performance improvements that appear linear across BERT and BLEU scores. This demonstrates that larger datasets enhance the models ability to generalize, leading to more accurate text reconstruction. This emphasizes the scaling potential of our method when exposed to larger datasets. Similarly, we investigated the effect of varying electrode usage by training the model on random subset of 20%, 40%, 60%, 80%, and 100% of the available electrodes. We ran the experiment five times with different selected subset for each percentage. The results are presented in Figure 2E. We observe similar linear scaling with electrode count, which suggests that Neuro2Semantic could benefit substantially from denser cortical coverage. However, the relatively large error bars imply that some electrodes are substantially more valuable than others for decoding. This suggests that in decoding applications, there are optimal coverage patterns to extract maximally useful information with fixed quantity of electrodes. 4. Discussion Neuro2Semantic demonstrates significant advances in neural language decoding through its novel two-phase architecture and efficient data utilization. Unlike classification-based approaches [23] or retrieval-oriented frameworks [7], our model directly aligns iEEG signals with semantic embeddings, enabling unconstrained text generation without predefined vocabularies. When compared to replicated current state-of-the-art continuous decoding method [6], our approach achieves substantially higher semantic accuracy while requiring only 30 minutes of training data, fraction of the 16+ hours typically needed by existing approaches [6, 7, 9]. Our ablation studies confirm that the initial alignment phase is crucial for performance, rather than merely relying on language model capabilities. This methodology enables zero-shot generalization to unseen semantic content without domain-specific fine-tuning, distinguishing it from previous methods that are constrained by their training vocabularies. Furthermore, our scaling experiments demonstrate consistent performance improvements with increased data and electrode coverage, suggesting significant headroom for enhancement as more data becomes available. The small sample size and clinical population limit immediate generalizability. Our current goal is to validate feasibility, not yet draw population-level conclusions. Additionally, as we gather more data, we aim to investigate transformer-based architectures for the alignment phase, which typically require larger datasets but could offer enhanced modeling capacity. These developments would further strengthen Neuro2Semantics capabilities across different subjects and linguistic contexts. 5. Conclusion We introduce Neuro2Semantic, transfer learning framework that decodes continuous language from neural signals using pretrained text embeddings. With just 30 minutes of data, it outperforms existing methods and demonstrates strong zero-shot generalization and unconstrained text generation. The approach scales with larger datasets and limited brain coverage, highlighting its promise for real-world brain-computer interface applications and assistive communication technologies. 6. References [1] N. Kriegeskorte and P. K. Douglas, Interpreting encoding and decoding models, Current opinion in neurobiology, vol. 55, pp. 167179, 2019. [2] S. Chakrabarti, H. M. Sandberg, J. S. Brumberg, and D. J. Krusienski, Progress in speech decoding from the electrocorticogram, Biomedical Engineering Letters, vol. 5, pp. 1021, 2015. [3] H. Akbari, B. Khalighinejad, J. L. Herrero, A. D. Mehta, and N. Mesgarani, Towards reconstructing intelligible speech from the human auditory cortex, Scientific reports, vol. 9, no. 1, p. 874, 2019. [4] C. Wang, V. Subramaniam, A. U. Yaari, G. Kreiman, B. Katz, I. Cases, and A. Barbu, Brainbert: Self-supervised representation learning for intracranial recordings, arXiv preprint arXiv:2302.14367, 2023. [5] T. Naselaris, K. N. Kay, S. Nishimoto, and J. L. Gallant, Encoding and decoding in fmri, Neuroimage, vol. 56, no. 2, pp. 400410, 2011. [6] J. Tang, A. LeBel, S. Jain, and A. G. Huth, Semantic reconstruction of continuous language from non-invasive brain recordings, Nature Neuroscience, pp. 19, 2023. [7] A. Defossez, C. Caucheteux, J. Rapin, O. Kabeli, and J.-R. King, Decoding speech from non-invasive brain recordings, arXiv preprint arXiv:2208.12266, 2022. [8] B. Wang, X. Xu, L. Zhang, B. Xiao, X. Wu, and J. Chen, Semantic reconstruction of continuous language from meg signals, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 21902194. [9] Z. Wang and H. Ji, Open vocabulary electroencephalography-totext decoding and zero-shot sentiment classification, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 5, 2022, pp. 53505358. [10] H. Liu, D. Hajialigol, B. Antony, A. Han, and X. Wang, Eeg2text: Open vocabulary eeg-to-text decoding with eeg arXiv preprint pre-training and multi-view transformer, arXiv:2405.02165, 2024. [11] J. Li, C. Guo, L. Fu, L. Fan, E. F. Chang, and Y. Li, Neural2speech: transfer learning framework for neural-driven speech reconstruction, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 22002204. [12] J. Lee, A. Kommineni, T. Feng, K. Avramidis, X. Shi, S. R. Kadiri, and S. Narayanan, Toward fully-end-to-end listened speech decoding from eeg signals, in Interspeech 2024, 2024, pp. 15001504. [13] N. Robinson and A. Vinod, Noninvasive brain-computer interface: decoding arm movement kinematics and motor control, IEEE Systems, Man, and Cybernetics Magazine, vol. 2, no. 4, pp. 416, 2016. [14] C. Pandarinath, P. Nuyujukian, C. H. Blabe, B. L. Sorice, J. Saab, F. R. Willett, L. R. Hochberg, K. V. Shenoy, and J. M. Henderson, High performance communication by people with paralysis using an intracortical brain-computer interface, elife, vol. 6, p. e18554, 2017. [15] X. Zou, Z.-Y. Dou, J. Yang, Z. Gan, L. Li, C. Li, X. Dai, H. Behl, J. Wang, L. Yuan et al., Generalized decoding for pixel, image, and language, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 15 116 15 127. [16] W. Xia, R. de Charette, C. Oztireli, and J.-H. Xue, Dream: Visual decoding from reversing human visual system, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 82268235. [17] Y. Benchetrit, H. Banville, and J.-R. King, Brain decoding: toward real-time reconstruction of visual perception, in The Twelfth International Conference on Learning Representations, 2024. [18] F. R. Willett, E. M. Kunz, C. Fan, D. T. Avansino, G. H. Wilson, E. Y. Choi, F. Kamdar, M. F. Glasser, L. R. Hochberg, S. Druckmann et al., high-performance speech neuroprosthesis, Nature, vol. 620, no. 7976, pp. 10311036, 2023. [19] S. L. Metzger, K. T. Littlejohn, A. B. Silva, D. A. Moses, M. P. Seaton, R. Wang, M. E. Dougherty, J. R. Liu, P. Wu, M. A. Berger et al., high-performance neuroprosthesis for speech decoding and avatar control, Nature, vol. 620, no. 7976, pp. 10371046, 2023. [20] A. G. Huth, W. A. De Heer, T. L. Griffiths, F. E. Theunissen, and J. L. Gallant, Natural speech reveals the semantic maps that tile human cerebral cortex, Nature, vol. 532, no. 7600, pp. 453458, 2016. [21] M. Rybaˇr and I. Daly, Neural decoding of semantic concepts: systematic literature review, Journal of Neural Engineering, vol. 19, no. 2, p. 021002, 2022. [22] D. Dash, P. Ferrari, and J. Wang, Decoding imagined and spoken phrases from non-invasive neural (meg) signals, Frontiers in neuroscience, vol. 14, p. 290, 2020. [23] J. G. Makin, D. A. Moses, and E. F. Chang, Machine translation of cortical activity to text with an encoderdecoder framework, Nature neuroscience, vol. 23, no. 4, pp. 575582, 2020. [24] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural Comput., vol. 9, no. 8, p. 17351780, Nov. 1997. [25] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer, Journal of Machine Learning Research, vol. 21, no. 140, pp. 167, 2020. [26] OpenAI, Openai api: Embeddings, https://platform.openai. com/docs/guides/embeddings, 2022. [27] J. Morris, V. Kuleshov, V. Shmatikov, and A. Rush, Text Embeddings Reveal (Almost) As Much As Text, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 12 44812 460. [28] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning Transferable Visual Models From Natural Language Supervision, in Proceedings of the 38th International Conference on Machine Learning. PMLR, Jul. 2021, pp. 8748 8763, iSSN: 2640-3498. [29] G. Mischler, Y. A. Li, S. Bickel, A. D. Mehta, and N. Mesgarani, Contextual feature extraction hierarchies converge in large language models and the brain, Nature Machine Intelligence, pp. 111, 2024. [30] E. Edwards, M. Soltani, W. Kim, S. S. Dalal, S. S. Nagarajan, M. S. Berger, and R. T. Knight, Comparison of timefrequency responses and the event-related potential to auditory speech stimuli in human cortex, Journal of neurophysiology, vol. 102, no. 1, pp. 377386, 2009. [31] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [32] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, Bleu: method for automatic evaluation of machine translation, in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311318. [33] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, BERTScore: Evaluating Text Generation with BERT, Apr. 2020."
        }
    ],
    "affiliations": [
        "Department of Electrical Engineering, Columbia University, USA",
        "The Feinstein Institutes for Medical Research, USA"
    ]
}