{
    "paper_title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test",
    "authors": [
        "Yuhui Li",
        "Fangyun Wei",
        "Chao Zhang",
        "Hongyang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE."
        },
        {
            "title": "Start",
            "content": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test Yuhui Li1,3, Fangyun Wei2, Chao Zhang1, Hongyang Zhang3,4 1Peking University, 2Microsoft Research, 3University of Waterloo, 4Vector Institute. yuhui.li@stu.pku.edu.cn, fawe@microsoft.com c.zhang@pku.edu.cn, hongyang.zhang@uwaterloo.ca 5 2 0 2 ] . [ 1 0 4 8 1 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing toplayer features from the target model to achieve better results than vanilla speculative sampling. growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLEs feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE."
        },
        {
            "title": "Introduction",
            "content": "Modern Large Language Models (LLMs) are being applied to more domains, with their improved capabilities driven by scaling model parameterssome LLMs now exceed hundreds of billions of parameters. In autoregressive generation, each token requires accessing all model parameters, making LLM inference slow and costly. Recently, test-time scaling up has gained significant attention. Models like ChatGPT o1 and DeepSeek-R1 (Guo et al., 2025) engage in deliberate reasoning before responding, pushing the Figure 1: Scaling law evaluated on the MT-bench using LLaMA-Instruct 3.1 8B as the target model, with the x-axis representing the data scale relative to ShareGPT. The new architectural designs in EAGLE-3 enable an increasing scaling curve, which was never observed in the previous works. boundaries of LLM capabilities at the cost of longer inference time. However, these models often require lengthy reasoning processes, making them extremely costly, while the increased response time severely impacts user satisfaction. These reasoning models significantly increase the proportion of inference costs in the overall LLM pipeline, driving researchers to explore cheaper and faster inference optimization methods. Speculative sampling methods can reduce LLM latency by partially parallelizing the generation process. These methods rapidly generate draft tokens and then verify them in parallel. This allows multiple tokens to be produced in single forward pass, significantly reducing inference latency. In vanilla speculative sampling, the draft model is separate, smaller LLM, typically lower-parameter version from the same series as the target model. This draft model operates independently of the target model. Unlike the vanilla speculative sampling, Figure 2: Speedup ratios of different methods at temperature=0. For the standard speculative sampling, Vicuna-13B uses Vicuna-68M as the draft model. In Table 1, we present comparisons with additional methods, but this figure only showcases subset. Chat models evaluation dataset is MT-bench, and the reasoning models evaluation dataset is GSM8K. DeepSeek R1 LLaMA 8B refers to DeepSeek-R1-Distill-LLaMA 8B. EAGLE (Li et al., 2024c) reuses the top-layer features of the target model (the features before the LM head). It trains the draft model to autoregressively predict the next feature and then uses the target models LM head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better acceleration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on larger training datasets to achieve better performance. For example, LLaMA series models with sizes of 7B (8B) have used 1T, 2T, and 15T tokens of training data for LLaMA 1 (Touvron et al., 2023a), LLaMA 2 (Touvron et al., 2023b), and LLaMA 3 (Dubey et al., 2024), respectively, resulting in significant improvements across various metrics while keeping the model architecture and inference cost largely unchanged. Similarly, we aim to improve the acceptance rate and acceleration ratio of EAGLE by increasing its training data. Unfortunately, we observe that the gains from additional training data for EAGLE are limited. We analyze the reasons behind this phenomenon. As shown in the upper part of Figure 3, EAGLE performs autoregressive prediction at the feature level, predicting the next feature and then feeding the feature into the LM head of the target model to obtain the token distribution. EAGLEs loss function consists of two components: the feature prediction loss lfea and the token prediction loss ltoken. Thanks to the feature prediction loss, the draft model trained only at Step 1 can adapt to Step 2 and acquire multi-step prediction capabilities. However, with token prediction as the ultimate goal, feature prediction can be seen as an additional constraint, which limits the expressiveness of the draft model and makes it difficult to benefit from increased data. After removing the feature constraint and expanding the training data (the middle part of Figure 3), as shown in Figure 4, the acceptance rate 0-α of the first draft token improves significantly. However, the output of the draft model in Step 1, denoted as ˆat+1, is far away from the ground-truth ft+1, causing the the input sequence f1, f2, , ft, ˆat+1 in Step 2 to deviate significantly from the training distribution, resulting in very low acceptance rate 1-α for the second draft token, as shown in Figure 4. We can address this issue by incorporating Step 1 into the training process (the bottom of Figure 3). Using this method, the benefits of increasing training data become more pronounced. We name this technique as training-time test. EAGLE and speculative sampling methods such as Medusa (Cai et al., 2024) reuse the top-layer features of the target model, specifically the features immediately before the LM head. For an LM head with full-rank weight matrix, the top-layer features corresponding to the logits of the next token are unique, ensuring that the information contained in these features aligns directly with the logits of the next token. However, predicting the next-next token based solely on top-layer featureswhich are inherently limited to the next tokenposes significant challenge. Fortunately, the training-time test technique described above enables the use of features from intermediate layers instead of relying solely on the top layer, as the feature prediction loss lfea has been removed during training. To summarize, this paper introduces EAGLE3, an enhanced version of EAGLE that achieves Figure 4: Comparison of acceptance rates across different methods, with the x-axis representing the data scale relative to ShareGPT. Discovery of scaling law for inference acceleration in large language models: With the new architecture, we observe that increasing the amount of training data for the draft model leads to proportional increase in the speedup ratio of EAGLE-3. This scaling behavior was not observed in the original EAGLE architecture, as shown in Figure 1 Improved inference acceleration: EAGLE3, trained with approximately 8x more data than EAGLE, achieves 1.4x latency speedup over EAGLE-2 at batch size 1. For large-batch inference, EAGLE-3 also surpasses the implementation of vLLM (Kwon et al., 2023) in throughput, even at batch sizes up to 56. We expect larger data size would lead to further improved speedup ratio."
        },
        {
            "title": "2.1 Speculative Sampling",
            "content": "Speculative sampling (Leviathan et al., 2023; Chen et al., 2023; Sun et al., 2024c,b) is lossless LLM acceleration technique that alternates between drafting and verification, where drafting is performed at low cost and verification is parallelized, corresponding to the generation of drafts and the verification process, respectively. We use ti to denote the i-th token and Ta:b to represent the token sequence ta, ta+1, , tb. When T1:j is used as the prefix, the two stages of speculative sampling are as follows. In the drafting stage, speculative sampling utilizes draft model (a smaller version from the same series as the target model) to autoregressively generate tokens to form the draft. ˆTj+1:j+k, while also recording the probability ˆp for each token. In the verification stage, speculative sampling invokes the target model to evaluate the draft ˆTj+1:j+k and records its probability p. Speculative sampling then determines the acceptance of Figure 3: Illustration of training-time test (the bottom part) and its comparison with other draft methods (the upper and middle parts). denotes the feature, denotes the token, and represents the unconstrained vectors. We use the hat to denote the predictions from models. All the methods shown in the figure use the token sequence from the previous time step, but for simplicity, this is not depicted in the figure. The input to EAGLE-3 is not actually , but it is not shown in this figure. We will provide detailed explanation in the following section. significant speedup. EAGLE-3 is parallelized and fully compatible with the drafting tree technique from EAGLE-2 (Li et al., 2024b). Our key contributions include: novel training-time test architecture for the draft model: We remove the feature prediction constraint and directly predict tokens while simulating multi-step generation during training. This direct token prediction provides complete flexibility in the draft models input. Instead of reusing only the top-layer features, we integrate and leverage low-, mid-, and highlevel features from the target model, capturing rich semantic information from different layers. draft tokens sequentially, from front to back. For token ˆtj+i, the probability of acceptance is given by min(1, pj+i(ˆtj+i)/ˆpj+i(ˆtj+i)). If the token is accepted, the process moves to the next token. Otherwise, token is sampled from the distribution norm(max(0, pj+i ˆpj+i)) to replace ˆtj+i, and the remaining tokens in the draft are discarded. Appendix A.1 of (Leviathan et al., 2023) proves that speculative sampling is consistent with the distribution of vanilla autoregressive decoding."
        },
        {
            "title": "2.2 EAGLE and EAGLE-2",
            "content": "The draft model with limited capacity struggles to precisely approximate the large-scale target model. EAGLE leverages the top-layer features of the target model as additional information and performs autoregression at the feature level, simplifying the drafting process. EAGLE performs autoregression at the feature level and then uses the LM head of the target model to obtain the draft token. Due to the sampling results at the token layer being hidden, feature-level autoregression introduces uncertainty. EAGLE addresses this issue by feeding the token sequence from the previous time step, i.e., the sampling results, into the draft model. Unlike the chain-like drafts of Vanilla speculative sampling, EAGLE generates multiple draft tokens at the same position, resulting in tree-like draft. In the verification stage, EAGLE uses tree attention to parallelize the verification of the draft tree. Interestingly, EAGLE inspired the multi-token prediction technique used in the pre-training of DeepSeekv3 (Liu et al., 2024a), which in turn inspired new architectural designs in EAGLE-3. EAGLE (Li et al., 2024c) and Medusa (Cai et al., 2024), among others, use tree-shaped drafts, where the structure of the draft tree is predefined, static, and context-independent. The difficulty of drafting is closely related to the context, and static draft tree can lead to resource wastage. EAGLE2 (Li et al., 2024b) approximates the acceptance rate using the confidence of the draft model and dynamically generates the draft tree based on this, performing pruning of the draft tree at the end of the drafting stage. EAGLE-3 also adopts the contextaware dynamic draft tree proposed in EAGLE-2."
        },
        {
            "title": "3 EAGLE-3",
            "content": "In this section, we provide detailed description of the implementation of EAGLE-3. Figure 5: Diagram of the EAGLE-3 inference pipeline, illustrating the three steps of the draft model. l, m, and represent the low, middle, and high-level features of the target model, respectively. denotes the embedding. 3."
        },
        {
            "title": "Inference Pipeline",
            "content": "Consistent with other speculative sampling methods, EAGLE-3 alternates between the drafting and verification stages. The difference between EAGLE-3 and EAGLE lies in the drafting stage, which we introduce with an example, as shown in Figure 5. Consider the prefix How can. During the prefill phase or the previous verification stage, the target model performs forward pass to generate the next token, I. We record the low, middle, and high-level feature sequences from the target models forward pass, denoted as l, m, and h, respectively. We concatenate the k-dimensional vectors l, m, and to form 3k-dimensional vector, then pass it through fully connected (FC) layer to reduce it to k-dimensions, obtaining feature that integrates information from different layers. Here, refers to the hidden size of the target model. Our goal is to generate draft token sequence with the prefix How can I. By inputting only ghow and gcan, the draft model cannot access the random sampling process. Therefore, similar to EAGLE (Li et al., 2024c), we introduce the embedding eI of the sampled token I. The concatenated vector is then passed through an FC layer to reduce its dimensionality to k, and subsequently inputted into single layer decoder, producing the output a. Finally, we input aI into the LM head and sample to obtain the draft token do. In Step 1, with the prefix How can, we reuse ghow and gcan from the target model. In Step 2, the prefix becomes How can I. Ideally, we would reuse ghow, gcan, and gI from the target model. However, this is not possible because the token has not yet been checked by the target model, and we cannot obtain gI. Instead, we use the output aI from the draft model in the previous step to replace gI, and concatenate aI with the embedding edo of the sampled result do as the input to the draft model in Step 1. In Step 3, we similarly cannot obtain gdo, so we use ado as replacement, concatenating ado with eit as the input to the draft model. The same approach is followed for subsequent steps."
        },
        {
            "title": "3.2 Draft Model Training",
            "content": "The input to the draft model in EAGLE is either, or at least approximately, the top-layer features f1, f2, , ft of the target model. In contrast, the input to the draft model in EAGLE-3 may include the features g1, g2, , gt from the target model, or it may include the output at+1, at+2 , at+j from the draft model. Therefore, we need to train the draft model to adapt to different inputs. During training, we perform test steps, where we generate and feed it back into the draft model for further training. The core of the draft model in EAGLE-3 is Transformer decoder layer. Aside from the selfattention operation, no other components interact with the context, so no further modifications are required during training or testing. The only component that requires slight modification is the selfattention, which we will describe in detail below. Although the actual input consists of features, for clarity, we describe the process using tokens as input. As shown in Figure 6, the original training data is sequence of length 3, How can I, with normal sequential dependency in the context. Therefore, the attention mask is standard lower triangular matrix. The outputs at the three positions are are, we, and do, which have tree-like contextual relationship with how, can, and I. As result, when the input are, we, and do is fed into Step 2, the attention mask needs to be adjusted accordingly, as shown in the top-right corner of Figure 6. All attention masks are diagonal, except when the original training data is used as the key. Using matrix multiplication in this case would result in significant computational waste, so we can use vector dot products to calculate the attention score only for the corresponding positions. HASS (Zhang et al., 2024) and EAGLE-3 both make similar modifications to the attention mechanism to simulate the testing process during training, but this is not the main focus of EAGLE-3. The motivations, methods, and outcomes of the two approaches are distinctly different. The motivation behind HASS is to mitigate the error accumulation caused by inaccurate feature predictions in EAGLE. HASS still performs feature prediction, includes feature prediction loss lfea, and the input to the draft model must be the top-layer features. In contrast, the motivation behind EAGLE-3 is to remove unnecessary constraints to enhance the models expressive power. EAGLE-3 no longer requires the draft models output to fit the top-layer features of the target model, thus avoiding error accumulation. After removing feature prediction, the input to EAGLE-3 is completely free, and it is replaced by fusion of features from different layers of semantic information. The removal of the feature prediction loss also enables us to discover new scaling law for inference acceleration which was never found before. Figure 2 also shows the speedup of EAGLE-3 and HASS, with EAGLE-3 demonstrating significantly better performance."
        },
        {
            "title": "4 Experiments",
            "content": "Models. We conduct experiments with state-ofthe-art open-source chat and reasoning models, including Vicuna 13B (Chiang et al., 2023), LLaMAInstruct 3.1 8B, LLaMA-Instruct 3.3 70B (Dubey et al., 2024), and DeepSeek-R1-Distill-LLaMA 8B (DeepSeek-AI et al., 2025). Due to the GPU constraint, we are unable to test EAGLE-3 on the 405B and 671B models. Tasks. Following EAGLE (Li et al., 2024c) and Spec-Bench (Xia et al., 2024), we evaluate on five common tasks, using the same weights for all tasks without fine-tuning on the respective tasks. For multi-turn conversation, code generation, mathematical reasoning, instruction following, and summarization we chose the MT-bench (Zheng et al., 2023), HumanEval (Chen et al., 2021), GSM8K (Cobbe et al., 2021), Alpaca (Taori Figure 6: Diagram of the attention causal masks during training-time test. It sequentially shows native training step (the first step) and two simulated training steps (the second and third steps). The arrows between tokens represent contextual relationships. The gray tokens represent the training data while the blue and yellow tokens represent the firstand second-round predictions by the draft model, respectively. et al., 2023), and CNN/Daily Mail (Nallapati et al., 2016) datasets, respectively. Metrics. EAGLE-3 does not modify the target models weights and uses strict speculative sampling acceptance conditions, ensuring no loss in performance. Therefore, we do not evaluate generation quality. Instead, we use the following metrics to assess the acceleration performance: Speedup Ratio: The actual test speedup ratio relative to vanilla autoregressive decoding. Average Acceptance Length τ : The average number of tokens generated per draftingverification cycle, which corresponds to the number of tokens accepted from the draft. Acceptance Rate n-α: The proportion of draft tokens accepted, which directly reflects the draft models approximation to the target model. Following EAGLEs setup, we use chain-like draft rather than tree-like draft when testing acceptance rates. EAGLE suffers from error accumulation, meaning that the input to the draft model may be its own estimates rather than the exact values from the target model. Therefore, EAGLE uses n-α to represent the acceptance rate when the input contains estimated features, under the condition that the previous estimated tokens are all accepted by the target model. In other words, the acceptance rate for inputs f1, f2, , fi, ˆfi+1, , ˆfi+n, where is the exact value and ˆf is the draft models estimate. Similarly, we use n-α to represent the acceptance rate in EAGLE-3 when the input contains self-predicted values a, i.e., the acceptance rate for inputs g1, g2, , gi, ai+1, , ai+n, where is the fused feature from the target model. Implementation. We use the AdamW optimizer, with beta values (β1, β2) set to (0.9, 0.95) and implemented gradient clipping of 0.5. The learning rate is set to 5e-5. We use ShareGPT and UltraChat200K (Ding et al., 2023) as training data, containing approximately 68K and 464K data entries, respectively. We call the target model to generate responses rather than using fixed dataset. For the reasoning model DeepSeek-R1-Distill-LLaMA 8B, we also used the OpenThoughts-114k-math dataset for training. Comparison. We use vanilla autoregressive decoding as the baseline, which serves as the benchmark for speedup ratios (1.00x). We compare EAGLE-3 with recent lossless speculative sampling methods, including standard speculative sampling (Leviathan et al., 2023; Chen et al., 2023; Gante, 2023), PLD (Saxena, 2023), Medusa (Cai et al., 2024), Lookahead (Fu et al., 2024), Hydra (Ankner et al., 2024), HASS (Zhang et al., 2024), EAGLE (Li et al., 2024c), and EAGLE-2 (Li et al., 2024b). Table 1: Speedup ratios and average acceptance lengths τ of different methods. represents Vicuna, L31 represents LLaMA-Instruct 3.1, L33 represents LLaMA-Instruct 3.3, and DSL represents DeepSeek-R1-Distill-LLaMA. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare EAGLE-3 with these methods when temperature=1. Model Method Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ MT-bench HumanEval GSM8K Alpaca CNN/DM Mean SpS PLD Medusa Lookahead Hydra EAGLE EAGLE-2 EAGLE-3 EAGLE-2 EAGLE-3 EAGLE-2 EAGLE-3 EAGLE-2 EAGLESpS EAGLE EAGLE-2 EAGLE-3 EAGLE-2 EAGLE-3 EAGLE-2 EAGLE-3 EAGLE-2 EAGLE-3 13B L31 8B L33 70B DSL 8B 13B L31 8B L33 70B DSL 8B 1.93x 1.58x 2.07x 1.65x 2.88x 3.07x 4.26x 5.58x 3.16x 4.40x 2.83x 4.11x 2.92x 4.05x 1.62x 2.32x 3.80x 4.57x 2.44x 3.07x 2.73x 3.96x 2.69x 3.20x 2.27 1.63 2.59 1.69 3.65 3.98 4.83 6.65 4.05 6.13 3.67 5.63 3.80 5. 1.84 3.20 4.40 5.42 3.16 4.24 3.51 5.45 3.41 4.49 2.23x 1.85x 2.50x 1.71x 3.28x 3.58x 4.96x 6.47x 3.66x 4.85x 3.12x 4.79x 3.42x 4.59x 1.72x 2.65x 4.22x 5.15x 3.39x 4.13x 2.89x 4.36x 3.01x 3.77x Temperature=0 2.57 1.93 2.78 1.75 3.87 4.39 5.41 7.54 4.71 6.74 4.09 6.52 4.29 6.38 1.77x 1.68x 2.23x 1.81x 2.93x 3.08x 4.22x 5.32x 3.39x 4.48x 2.83x 4.34x 3.40x 5.01x 2.01 1.73 2.64 1.90 3.66 3.97 4.79 6.29 4.24 6.23 3.69 6. 4.40 6.93 Temperature=1 1.97 3.63 4.89 6.22 4.39 5.82 3.81 6.16 3.82 5. 1.46x 2.57x 3.77x 4.71x 2.86x 3.32x 2.52x 4.17x 3.16x 4.38x 1.73 3.60 4.41 5.58 3.74 4. 3.36 5.95 4.05 6.10 1.76x 1.16x 2.08x 1.46x 2.86x 3.03x 4.25x 5.16x 3.28x 4.82x 3.03x 4.30x 3.01x 3.65x 1.52x 2.45x 3.78x 4.49x 2.83x 3.90x 2.77x 4.14x 2.64x 3.16x 2.03 1.19 2.45 1.51 3.53 3.95 4.89 6.17 4.12 6. 3.92 6.09 3.80 5.37 1.78 3.57 4.37 5.39 3.65 5.56 3.73 5.87 3.29 4. 1.93x 2.42x 1.71x 1.46x 2.05x 2.49x 3.40x 5.01x 2.65x 3.65x 2.44x 3.27x 3.53x 3.52x 1.66x 2.23x 3.25x 4.33x 2.44x 2.99x 2.32x 3.11x 2.35x 3.08x 2.33 2.50 2.09 1.50 2.81 3.52 4.21 6.47 3.45 5.34 3.55 5.02 3.33 4. 1.89 3.26 3.97 5.72 3.14 4.39 3.27 4.88 3.13 4.27 1.92x 1.74x 2.12x 1.62x 2.80x 3.05x 4.22x 5.51x 3.23x 4.44x 2.85x 4.12x 3.26x 4.16x 1.60x 2.44x 3.76x 4.65x 2.80x 3.45x 2.65x 3.95x 2.77x 3.52x 2.24 1.80 2.51 1.67 3.50 3.96 4.83 6.62 4.11 6.23 3.78 5.88 3.92 5.84 1.84 3.45 4.41 5.67 3.62 4. 3.54 5.66 3.54 4."
        },
        {
            "title": "4.1 Effectiveness",
            "content": "Figure 1 and Table 1 demonstrate the acceleration performance of EAGLE-3. On all tasks and target models, EAGLE-3 achieves the highest speedup ratio and average acceptance length. EAGLE-3 provides speedup of approximately 3.0x-6.5x compared to vanilla autoregressive generation, with 20%-40% improvement over EAGLE-2. Different tasks affect the draft models acceptance rate, so both the average acceptance length and speedup ratio are task-dependent. Due to the presence of many fixed templates in code generation tasks, generating drafts is the easiest, which is why EAGLE-3 performs best on HumanEval, achieving speedup ratio of up to 6.5x and an average acceptance length of up to 7.5. DeepSeek-R1-Distill-LLaMA 8B is an exception, with the highest speedup ratio on the mathematical reasoning dataset GSM8K. This may be because we trained the draft model of DeepSeekR1-Distill-LLaMA 8B using the OpenThoughts114k-math dataset. Figure 7 shows the acceptance rates of EAGLE and EAGLE-3 on MT-bench with LLaMA-Instruct 3.1 8B as the target model. The acceptance rate of EAGLE-3 is significantly higher than that of EAGLE. As the input from the draft model itself increases, the acceptance rate of EAGLE drops significantly, whereas EAGLE-3s acceptance rate remains almost unchanged, demonstrating the effectiveness of the Training-time test."
        },
        {
            "title": "4.2 Ablation Study",
            "content": "The improvements of EAGLE-3 mainly come from two aspects: first, the removal of the feature regression constraint, and second, the improvement from reusing only the top-layer features to reusing mix of low, middle, and high-level features. We conducted an ablation study on MT-bench with LLaMA-Instruct 3.1 8B as the target model. The Table 3: Throughput improvement under different batch sizes, with vLLM without speculative sampling as the baseline (1.00x). Batch size 2 4 16 24 32 48 56 EAGLE EAGLE1.30x 1.75x 1.25x 1.68x 1.21x 1.58x 1.10x 1.49x 1.03x 1.42x 0.93x 1.36x 0.82x 1.21x 0.71x 1.01x"
        },
        {
            "title": "5 Related Work",
            "content": "Many methods have been used to accelerate inference in LLMs, such as quantization (Hubara et al., 2018; Shen et al., 2020; Kim et al., 2021; Zadeh et al., 2020; Zafrir et al., 2019) and distillation (Hinton et al., 2015). These methods generally have trade-offs, where there is need to balance model performance with acceleration benefits. Speculative sampling uses the target model for verification to ensure lossless acceleration. Early speculative decoding methods (Stern et al., 2018; Sun et al., 2021) accelerated generation in greedy settings, while Leviathan et al. (2023); Chen et al. (2023) introduced speculative sampling to extend the draft verification framework to nongreedy generation. Many subsequent works have improved upon speculative sampling. EAGLE (Li et al., 2024c), EAGLE-2 (Li et al., 2024b), Medusa (Cai et al., 2024), and Hydra (Ankner et al., 2024) reused the features of the target model. HASS (Zhang et al., 2024) simulates multi-step draft process during training to mitigate the issues of training-inference inconsistency and error accumulation in EAGLE. GLIDE and CAPE (Du et al., 2024) reuse the target models KV cache, while methods (Hooper et al., 2023; Yang et al., 2023; Monea et al., 2023; Li et al., 2024a; Yi et al., 2024; Liu et al., 2024b; Sun et al., 2024a; Elhoushi et al., 2024; Svirschevski et al., 2024) like Draft & Verify (Zhang et al., 2023) use layer skipping or early exits to reuse parts of the target models parameters."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce EAGLE-3. Building upon EAGLE, EAGLE-3 incorporates two key improvements. First, it removes the feature prediction constraint, instead directly predicting draft tokens through Training-time test. Second, it replaces the use of the target models top-layer features with fusion of the target models lower, middle, and upper-layer features to obtain richer information. With these improvements, EAGLE-3 continues to benefit from the augmentation of training data, achieving maximum speedup of 6.5x. Figure 7: Acceptance rate of EAGLE and EAGLE-3 on MT-bench, with the target model being LLaMAInstruct 3.1 8B. Hereby, n-α refers to the acceptance rate when the input contains estimated features, under the condition that the previous estimated tokens are all accepted by the target model. results, shown in Table 2, indicate that both improvements in EAGLE-3 significantly enhance the acceptance length and speedup ratio, demonstrating the rationality of the EAGLE-3 design. Table 2: Ablation study results with LLaMA-Instruct 3.1 8B as the target model. Remove fea con refers to the first improvement of EAGLE-3, which removes the feature prediction constraint. Fused features refers to the second improvement of EAGLE-3, where low, middle, and high-level feature fusion replaces the use of top-layer features. MT-bench GSM8K"
        },
        {
            "title": "Speedup",
            "content": "τ"
        },
        {
            "title": "Speedup",
            "content": "τ EAGLE-2 + remove fea con + fused features (ours) 3.16x 3.82x 4.40x 4.05 5.37 6.13 3.39x 3.77x 4.48x 4.24 5.22 6."
        },
        {
            "title": "4.3 Throughput",
            "content": "Speculative sampling algorithms like EAGLE-3 reduce memory accesses and lower latency during memory-bound decoding by leveraging redundant computational power. As batch sizes increase, this redundancy decreases, reducing the effectiveness of speculative sampling. We conducted study on the impact of EAGLE-3 on throughput for large batch sizes based on vLLM, widely used production-grade framework, and the results are shown in Table 3. EAGLE shows the maximum throughput improvement at batch size of 24, while EAGLE-3 shows this at 56. This part of the experiment did not use the tree structure, and the maximum chain length was set to 2. The framework used is also different from the other sections of this paper and from the typical settings in the speculative sampling domain, so comparisons with other experimental results are not meaningful."
        },
        {
            "title": "References",
            "content": "Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan RaganKelley, and William Brandon. 2024. Hydra: Sequentially-dependent draft heads for Medusa decoding. arXiv preprint arXiv:2402.05109. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. arXiv preprint arXiv: 2401.10774. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Daya Guo DeepSeek-AI, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233. Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, et al. 2024. Glide with cape: low-hassle method to accelerate speculative decoding. arXiv preprint arXiv:2402.02082. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et al. 2024. Layer skip: Enabling early exit inference and self-speculative decoding. arXiv preprint arXiv:2404.16710. Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. 2024. Break the sequential dependency of LLM inference using lookahead decoding. arXiv preprint arXiv:2402.02057. Joao Gante. 2023. new direction toward low-latency text generation. https://huggingface. co/blog/assisted-generation. Assisted generation: Xiangxiang Gao, Weisheng Xie, Yiwei Xiang, and Feng Ji. 2024. Falcon: Faster and parallel inference of large language models through enhanced semi-autoregressive drafting and custom-designed decoding tree. arXiv preprint arXiv:2412.12639. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Sophia Shao. 2023. Speed: Speculative pipelined execution for efficient decoding. arXiv preprint arXiv:2310.12072. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2018. Quantized neural networks: Training neural networks with low precision weights and activations. journal of machine learning research, 18(187):130. Sehoon Kim, Amir Gholami, Zhewei Yao, Michael Mahoney, and Kurt Keutzer. 2021. I-bert: Integeronly bert quantization. In International conference on machine learning, pages 55065518. PMLR. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Symposium on Operating Systems Principles, pages 611626. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via specIn International Conference on ulative decoding. Machine Learning, pages 1927419286. PMLR. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783. Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024a. Nearest neighbor speculative decoding for llm generation and attribution. arXiv preprint arXiv:2405.19325. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024b. EAGLE-2: Faster inference of language models with dynamic draft trees. In Conference on Empirical Methods in Natural Language Processing. Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, and Max Ryabinin. 2024. Specexec: Massively parallel speculative decoding for interactive llm inference on consumer devices. arXiv preprint arXiv:2406.02532. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024c. EAGLE: Speculative sampling reIn Internaquires rethinking feature uncertainty. tional Conference on Machine Learning. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024a. arXiv preprint DeepSeek-v3 technical report. arXiv:2412.19437. Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, and Yunhe Wang. 2024b. Kangaroo: Lossless self-speculative decoding via double early exiting. arXiv preprint arXiv:2404.18911. Giovanni Monea, Armand Joulin, and Edouard Grave. 2023. Pass: Parallel speculative sampling. arXiv preprint arXiv:2311.13581. Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023. Apoorv Saxena. 2023. Prompt lookup decoding. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low In Proceedings of precision quantization of bert. the AAAI Conference on Artificial Intelligence, volume 34, pages 88158821. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31. Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. 2024a. Triforce: Lossless acceleration of long sequence generation with hiarXiv preprint erarchical speculative decoding. arXiv:2404.11912. Xin Sun, Tao Ge, Furu Wei, and Houfeng Wang. 2021. Instantaneous grammatical error correction with shallow aggressive decoding. arXiv preprint arXiv:2106.04970. Ziteng Sun, Jae Hun Ro, Ahmad Beirami, and Ananda Theertha Suresh. 2024b. Optimal blocklevel draft verification for accelerating speculative decoding. arXiv preprint arXiv:2403.10444. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking efficiency in large language model inference: comprehensive surarXiv preprint vey of speculative decoding. arXiv:2401.07851. Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris Papailiopoulos, and Kangwook Lee. 2023. Predictive pipelined decoding: compute-latency trade-off for exact llm decoding. arXiv preprint arXiv:2307.05908. Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, and Rong Xiao. 2024. Generation meets verification: Accelerating large language model inference with smart parallel auto-correct decoding. arXiv preprint arXiv:2402.11809. Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020. Gobo: Quantizing attention-based nlp models for low latency and In 2020 53rd Annual energy efficient inference. IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 811824. IEEE. Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 3639. IEEE. Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. 2024c. SpecTr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36. Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. 2023. Draft & verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint arXiv:2309.08168. Lefan Zhang, Xiaodan Wang, Yanhua Huang, and Ruiwen Xu. 2024. Learning harmonized representations for speculative sampling. arXiv preprint arXiv:2408.15766. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench and chatbot Arena. arXiv preprint arXiv:2306.05685."
        },
        {
            "title": "A Implementation Details",
            "content": "Vanilla: We use models from the Huggingface.transformers library with the PyTorch backend and pre-allocated KV cache. Other methods also use these models as their base. (Standard) Speculative Sampling: We use the assisted generation feature from the HuggingFace Transformers library. PLD, Lookahead, Medusa, and Hydra: We use the default settings and the officially released weights. EAGLE: Vicuna and LLaMA2-Chat draft models use the officially released weights, while LLaMA3-Instruct is trained using the ShareGPT dataset (consistent with Medusa and Hydra). EAGLE-2: For the 7B (8B), 13B, and 70B original LLMs, we set the total number of draft tokens to 60, 50, and 48, respectively, with draft tree depth of 6, and select 10 nodes during the expansion phase."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Peking University",
        "University of Waterloo",
        "Vector Institute"
    ]
}