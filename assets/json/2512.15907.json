{
    "paper_title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "authors": [
        "Tejas Anvekar",
        "Juhna Park",
        "Aparna Garimella",
        "Vivek Gupta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems."
        },
        {
            "title": "Start",
            "content": "TABREX : Tabular Referenceless eXplainable Evaluation"
        },
        {
            "title": "Adobe Research",
            "content": "(cid:128) Project-Page Code {tanvekar,jpark284,vgupta140}@asu.edu garimell@adobe.com 5 2 0 2 7 1 ] . [ 1 7 0 9 5 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TABREX , reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TABREX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable tradeoffs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TABREX-BENCH , large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TABREX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables finegrained model-vs-prompt analysis establishing new paradigm for trustworthy, explainable evaluation of structured generation systems."
        },
        {
            "title": "Introduction",
            "content": "Structured data underpins critical workflows across domains such as finance, healthcare, scientific reporting, and logistics. Beyond spreadsheets and relational tables, modern ecosystems rely on JSON records, knowledge graphs, and visual dashboards. These formats enable consistent reasoning and aggregation, yet even single misplaced column, unit mismatch, or corrupted cell can propagate costly downstream errors. As large language models (LLMs) increasingly generate or transform structured outputs e.g., converting reports into financial tables, synthesizing 1 Figure 1: Metric Movements Across Difficulty Levels. Arrows show each metrics shift from easy (blue) to hard (red) perturbations. Axes plot specificity (y) vs. sensitivity (x), with the green region denoting the balanced ideal zone. The dashed diagonal marks the optimal trade-off. TABREX stay near this zone, maintaining right direction even for hard examples. patient dashboards, or reformatting analytical data the need for reliable automatic evaluation has become major bottleneck. Unlike free-form text, structured generation demands assessment of not just semantic fidelity but also schema alignment, syntactic consistency, and cell-level correctness. Most existing metrics, however, flatten tables into plain text. N-gram scores like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) ignore row-column structure and unit semantics, while embedding-based metrics such as BERTSCORE (Zhang* et al., 2020) and BLEURT (Sellam et al., 2020) capture semantics but miss structural perturbations. Token-level methods like Exact Match or PARENT (Dhingra et al., 2019) cannot distinguish harmless reformatting from genuine factual errors. Reference-less QA metrics such as DATAQUESTEVAL (Rebuffel et al., 2021) ground evaluation in source evidence but over-penalize layout changes, and recent TABEVAL (Ramu et al., 2024) and TABXEVAL (Pancholi et al., 2025) improve explainability yet remain limited by small, single-pass benchmarks and one-shot perturbation schemes. We argue that next-generation evaluation must be both property-driven and personalizable. Effective metrics should obey key propertiespermutation and format invariance, schemaand unit-consistent alignment, monotonic improvement as errors are fixed, and robustness to outliers while allowing tunable trade-offs between sensitivity (coverage) and specificity (hallucination control). Real-world domains differ in their error tolerance (e.g., precision in finance vs. recall in clinical data), requiring metrics that are domain-agnostic by design yet easily adaptable through interpretable property weights. To meet these needs, we propose TABREX , graph-based, explainable evaluation framework. TABREX converts both reference text and generated tables into structured graphs via hybrid pipeline: rule-based Table2Graph converter and an LLM-assisted Text2Graph extractor-followed by an LLM-guided Graph Alignment that identifies factual correspondences and discrepancies. From these alignments, property-driven scoring function computes interpretable, rubric-aware penalties capturing both structure and content quality, yielding an explainable, reference-less score. To stress-test metric reliability, we introduce TABREX-BENCH , large-scale benchmark covering six domains (finance, healthcare, hierarchical tables, and narratives) and twelve planner-driven perturbation types across three difficulty levels. Unlike prior one-shot datasets, TABREX-BENCH systematically combines factual and structural edits ranging from benign reformatting to severe semantic corruption enabling robust sensitivity-specificity analysis under realistic perturbation regimes. In summary, our contributions are: TABREX : reference-less, property-driven evaluation framework that aligns tabletext graphs and computes interpretable, rubric-aware scores. Empirical results showing that TABREX achieves strong human correlation and robustness under harder perturbations. Rubric-wise analyses demonstrating that TABREX provides explainable diagnostics at both table and cell levels for modelprompt alignment."
        },
        {
            "title": "2 TABREX",
            "content": "Illustration of propsed TABREX . Both Figure 2: source text and generated tables are converted into knowledge graphs via Text2Graph and Table2Graph, aligned through an LLM-guided Graph Alignment, finally scored by Property-Driven Scoring function that aggregates alignment statistics into interpretable, controllable tableand cell-level penalties. We propose TABREX , unified evaluation framework for tabular generation that converts both candidate table and reference / source text into knowledge graphs and scores them through small set of property-driven signals. This design yields metric that is reference-less, effective in detecting true discrepancies, and explainable by construction, best illustrated in Figure"
        },
        {
            "title": "2.1 Pipeline Overview",
            "content": "Stage 1: Text2Graph and Table2Graph. To enable uniform comparison, TABREX represents both textual summaries and tables as knowledgegraph triplets [s, p, o]. For text, we use an LLM guided by strict entity-centric grammar (Prompt C) to extract minimal atomic facts, where the subject is an entity or time slice, the predicate normalized property, and the object canonical value. This design enforces consistent granularity, normalized predicates, and unit-aware values across free-form text: GS = {(si, pi, oi) = 1, . . . , n}. TABREX-BENCH : large, systematically perturbed dataset enabling reproducible metric evaluation across domains and difficulty levels. For tables, we apply lightweight rule-based unrolling. Headers define predicates; each row specifies subject; every non-empty cell yields triplet (srow, pheader, ocell). To support diverse table formats, we implemented both RuleHTMLConverter and RuleMDConverter, and in this work, we use the latter. This deterministic approach is fast, schema-aware, and requires no training. By converting both modalities into common, interpretable triplet space, TABREX ensures structural clarity and prepares them for downstream alignment and scoring. Stage 2: Graph Alignment. In our referenceless setup, we align the graph extracted from the generated table, GT , with that from the source text, GS, so the table can be judged directly against the textual evidence. Both graphs consist of factual triplets (s, p, o). The alignment, guided by an LLM prompt (Prompt D), maps triplets in GT to their counterparts in GS. We adopt two-step procedure: (i) deterministic pass aligns triplets with identical or schema-normalized subjectpredicate pairs; (ii) an LLM-assisted refinement aligns the remainder, resolving paraphrases, abbreviations, and compound attributes (e.g., GDP growth (YoY)\" growth_rate_2021\"). Each matched pair is annotated with difference vector recording unit-aware numeric gaps, categorical mismatches, and whether fact is missing in the table or extra relative to the source. The resulting aligned set exposes, at the row/column/cell level, the precise correspondences and discrepancies required for property-driven scoring. Stage 3: Property-Driven Scoring. The aligned set provides structured evidence of matches, omissions, and numeric deviations between the table graph GT and the source text graph GS. From these alignments, TABREX derives interpretable statistics counts of missing (MI), extra (EI), and partially matched triplets aggregated over rows, columns, and cells. These alignment-derived quantities directly drive two complementary components capturing structural and factual quality. TablePenalty = βMI (cid:16)"
        },
        {
            "title": "MIr\nNr",
            "content": "+ αc αr (cid:16) + βEI αr"
        },
        {
            "title": "EIr\nNr",
            "content": "+ αc (cid:17)"
        },
        {
            "title": "MIc\nNc\nEIc\nNc",
            "content": "(cid:17) , where Nr and Nc denote the total numbers of rows and columns in GS, and MI / EI count missing and extra entities, respectively. The cell-level penalty captures factual fidelity: 3 CellPenalty = βMIαcell MIcell Ncell + βpartialαcell"
        },
        {
            "title": "EIcell\nNcell",
            "content": "+ βEIαcell Γ , Ncell where Γ is the sum of normalized numeric deviations over partially aligned cells. The final score combines both components: STABREX = TablePenalty + CellPenalty. The weighting parameters (α, β) provide intuitive control over the metrics behavior: increasing βMI favors sensitivity (rewarding comprehensive coverage), while increasing βEI favors specificity (penalizing hallucinated entries). Because all quantities are derived directly from A, the score remains reference-less, and fully explainable. All the weight configurations and walk through example is illustrated in Appendix C."
        },
        {
            "title": "2.2 TABREX-BENCH",
            "content": "Figure 3: Perturbation landscape across difficulty and type. The radial stacked donut visualizes the distribution of perturbation types segmented by difficulty: Easy (green), Medium (blue), and Hard (red). The top and bottom semicircles correspond to data-altering and data-preserving transformations, respectively. TABREX-BENCH is comprehensive benchmark for evaluating tabular metrics under both datapreserving and data-altering perturbations. Unlike prior resources such as TABXBENCH (Pancholi et al., 2025), which includes only 50 reference tables with 5 perturbations each, TABREX-BENCH spans six heterogeneous datasets FinQA (Chen Dataset FinQA HiTabQA ToTTo OpenML med MIMIC-IV RotoWire Total # of Tables 150 150 150 10 100 150 710 # Perturb / Table 12 12 12 12 12 Tables 1950 1950 1950 120 1200 1950 9120 Avg Row 05.55 20.08 24.97 04.20 10.58 10.18 Avg Col 02.47 05.60 05.49 11.58 03.94 05.86 Avg Cell 13.22 115.1 142.2 47.94 40.84 59.50 Avg Tokens 119.5 434.8 361.3 210.9 153.5 146. Avg Num 33.55 102.7 69.63 23.80 26.29 14.33 Table 1: Statistics of TABREX-BENCH : Datasets, perturbation counts, and average table and summary characteristics. et al., 2021), HiTabQA (Cheng et al., 2022), ToTTo (Parikh et al., 2020), OpenML-med (Smith et al., 1988; Centers for Medicare & Medicaid Services, 2019), MIMIC-IV (Johnson et al., 2024), and RotoWire (Wiseman et al., 2017) covering finance, healthcare, hierarchical tables, and narrative-totable tasks. As summarized in Table 1, the benchmark comprises 710 source tables, each expanded with 12 perturbations, yielding 9,120 perturbed instances spanning compact clinical sheets to large multi-column tables. Figure 3 illustrates the perturbation composition. We define two complementary perturbation groups: Data-Preserving (Group 0) alters layout or presentation e.g., row or header reordering, unit conversion, or paraphrasing without changing factual content; Data-Altering (Group 1) introduces semantic modifications such as adding or deleting rows/columns, swapping numeric values, or injecting noise and misspellings. Each group is further stratified into three difficulty tiers (Easy, Medium, Hard), supporting controlled analyses of metric robustness as perturbation severity increases. key innovation over prior work is our planner-driven perturbation generation. Rather than issuing separate LLM calls for each edit, TABREX-BENCH employs an LLM-based planner (Prompt B) that generates executable code to produce all 12 perturbations across both groups and difficulty levels in single pass, yielding more diverse and reproducible variants. Each perturbed table is also paired with concise, fact aligned table-level summary (Prompt A) and stats for the avg # token and Numerical data present are given in Table 1, enabling the evaluation of referenceless metrics assessing factual consistency between tables and summaries an aspect not present in TABXBENCH. All perturbations and summaries were initially generated through this planner-driven pipeline and validated on 20% of the data, achieving interannotator agreement of 87% for summaries and 4 91% for perturbations, ensuring correctness and diversity. By combining broad domain coverage, structured perturbation design, paired summaries, and tiered difficulty, TABREX-BENCH enables rigorous evaluation of metric robustness, sensitivity, and human alignment across both reference-based and reference-less settings."
        },
        {
            "title": "3 Experiments",
            "content": "To assess the efficacy of TABREX , we conduct experiments using our synthetic benchmark TABREX-BENCH . All results are reported with GPT-5-nano (Team, 2025b), evaluating both components of TABREX : Text2Graph and Graph Alignment using proposed TABREX-BENCH dataset. Baselines. We compare TABREX against diverse set of automatic evaluation metrics grouped by methodological design. Deterministic metrics: Exact Match (EM), CHRF, and ROUGE-L: compute tokenor character-level overlaps, offering reproducible yet surface-biased comparisons. Algorithmic metrics such as H-SCORE perform structured alignment and rule-based matching without relying on neural embeddings, offering deterministic, training-free evaluation. Neural metrics such as BERTSCORE and BLUERT leverage contextual embeddings to capture semantic similarity but may exhibit variability across runs. Among recent LLM-based approaches, we include P-SCORE (an LLM-judged quality metric producing 010 scores) and TABEVAL, which flattens tables via an LLM and measures entailment using RoBERTa-MNLI. We also evaluate the state-of-the-art TABXEVAL, two-phase rubricbased framework that first aligns tables structurally (TabAlign) and then performs semantic and syntactic comparison (TabCompare) for interpretable, human-aligned evaluation. Finally, we benchmark the reference-less QUESTEVAL, which generates questionanswer pairs from both the source and the generated text or table, performs crossvalidation using two LLM calls, and computes F1 scores to measure factual and semantic consistency. conduct GPT-5-nano, LLMs. We experiments using (4B/27BInstruct) (Team, 2025a), and InternVL3.5 (8BInstruct/Thinking) (Wang et al., 2025). Unless all Gemmastated otherwise, we employ uniform decoding settings across models, using their default temperature, top-k, and top-p parameters. All gpu-intensive conducted on NVIDIA-2H100s. The full prompts for Text2Graph (Prompt C) and Graph Alignment (Prompt D) are provided in Appendix A. experiments were from human perception. Sentence-level embedding metrics (BLEURT, BERTSCORE) capture partial semantic similarity but exhibit modest RBO (0.39) and high footrule distances (ζF 4553), reflecting poor rank stability. Their near-zero tie ratios (πt < 2%) further suggest coarse differentiation, failing to separate semantically close variants."
        },
        {
            "title": "Metric",
            "content": "ρS τK τw RBO ζF πt Non-LLM Based (w/ Ref) EM CHRF ROUGE-L BLUERT BERTSCORE H-SCORE 45.88 41.76 31.18 44.66 36.21 56.87 39.38 34.55 26.69 37.64 30.66 47.97 39.51 31.61 22.56 36.09 27.96 51.73 LLM-Based (w/ Ref) P-SCORE TABEVAL TABXEVAL 49.24 49.01 80.27 40.00 39.22 72.37 37.43 34.21 66.87 (w/o Ref) 43.33 39.39 37.65 39.57 38.11 41.11 40.73 41.11 47. 47.49 49.26 55.94 48.09 53.25 40.02 43.93 43.06 20.94 58.40 01.64 01.97 00.77 00.92 00.99 07.39 00.63 45."
        },
        {
            "title": "QUESTEVAL\nTABREX",
            "content": "62.93 74.51 52.29 64.24 51.71 62.28 42.70 44.85 35.04 27.01 03.03 13. Table 2: Correlation of automatic evaluation metrics with human rankings across synthetic perturbation sets. Higher values of Spearmans rank correlation (ρS), Kendalls tau (τK), weighted Kendalls tau (τw), and Rank-Biased Overlap (RBO) indicate stronger monotonic and positional agreement with human orderings (), while lower values of Spearmans footrule distance (ζF ) and tie ratio (πt) denote better rank stability and finer discriminative resolution (). The proposed TABREX achieves the best overall consistency with human judgment. Table 2 reports the correlation between automatic evaluation metrics and human judgments over the synthetic perturbation benchmark. Each ground-truth (GT) table was paired with twelve systematically perturbed variants six preserving factual content (labels 0: 1-easy, 1-medium, 1hard) and six introducing data alterations (labels 1: 1-easy, 1-medium, 1-hard). Human annotators ranked these variants by perceived semantic and factual fidelity to the GT, providing gold human order for correlation analysis. Metrics are grouped by family Non-LLM, LLM-based, and referenceless to examine their consistency and robustness under controlled perturbations. (a) Non-LLM metrics. such as EM, CHRF, and ROUGE-L show limited alignment with human judgment. Their Spearmans (ρS) and Kendalls (τK) values remain low (ρS < 0.45, τK < 0.35), indicating that rank orderings diverge substantially (b) LLM-based metrics. such as P-SCORE, TABEVAL, and TABXEVAL show notably higher agreement with human preferences (ρS 0.49 0.80, τK 0.390.72). Among them, TABXEVAL achieves the strongest overall correlation (ρS = 0.80, τK = 0.72), confirming that instruction-tuned evaluators capture perturbation sensitivity effectively. However, its elevated tie ratio (πt = 45.3%) and moderate rank dispersion (ζF = 20.9) indicate frequent scoring saturation, where distinct variants receive identical judgments reducing discriminative precision even when global trends align. (c) Reference-less metrics. Without access to reference tables, QUESTEVAL maintains moderate alignment (ρS = 0.63, τK = 0.52) by generating QA pairs from both the source and system outputs, yet exhibits instability under data-altering In contrast, our metric achieves perturbations. the most balanced performance across all dimensions Spearmans ρS = 0.75, Kendalls τK = 0.64, and weighted τw = 0.62 while also maintaining competitive RBO (44.9) and low rank dispersion (ζF = 27.0). Its moderate tie ratio (πt = 13.6%) indicates finer discriminative granularity, avoiding overconfidence and reflecting human-perceived difficulty progression. Together, these findings highlight that our method preserves ordinal consistency across perturbation severity while generalizing robustly in the absence of reference data."
        },
        {
            "title": "Metric",
            "content": "ρS τK τw RBO ζF πt"
        },
        {
            "title": "Ensemble Baselines",
            "content": "Lex-Emb (M ) Lex-Emb (H) LLM (M ) LLM (H) Hybrid (M ) Hybrid (H) TABREX 38.43 29.80 48.49 56.00 32.04 54.03 74.51 32.65 24.00 39.21 46.93 24.94 42.71 64.24 30.17 19.68 36.94 50.64 20.29 32.61 62.28 38.52 37.65 40.56 40.95 37.03 42.31 44.85 52.15 55.04 44.38 40.63 51.51 40.11 27. 00.49 00.63 00.42 00.42 01.13 01.13 13.59 Table 3: Comparison of ensemble baselines with the proposed TABREX . Ensembles combine metric families: Lex-Emb (lexical + embedding), LLM (LLM-based), and Hybrid (reference + reference-less) using either simple Mean (M) or Harmonic (H) aggregation. All ensemble variants fall short of TABREX , which achieves the highest correlation with human rankings and better rank stability. 5 (d) Ensemble of Scores. We further benchmarked ensemble baselines that aggregate complementary metrics using either simple averaging (Mean) or harmonic averaging (Harmonic). LexThese ensembles span three families: Emb (EM, ROUGE-L, BERTSCORE, BLEURT, CHRF), LLM (P-SCORE, H-SCORE), and Hybrid (TABXEVAL, QUESTEVAL). While the bestperforming ensemble, LLM (Harmonic), achieves ρS = 0.56 and τK = 0.47, it still lags behind our TABREX , which attains ρS = 0.75 and τK = 0.64 with lower rank dispersion. This highlights that naive aggregation of diverse metrics cannot match the targeted, reference-less reasoning of TABREX , which better aligns with human judgment across perturbation severities."
        },
        {
            "title": "3.2 Can TABREX Generalize Across",
            "content": "Perturbation Regimes? robust evaluation metric must remain reliable not only in standard (easy) settings but also under hard perturbations tables with subtle misalignments, semantic shifts, or fine-grained numeric errors. Using our proposed TABREX-BENCH , we sample both easy and hard cases across data-preserving and data-changing perturbations to compute truepositive and true-negative rates (sensitivity and specificity). Figure 1 plots each metrics trajectory on the specificitysensitivity plane as difficulty increases, revealing whether it remains stable or degrades under stress. Embedding-Driven Metrics. Many popular metrics (e.g., BERTSCORE, BLUERT, TABEVAL) rely on neural embeddings rather than surface-level string matching. For example, TABEVAL first unrolls tables into natural-language atomic statements using an LLM, then applies RoBERTa-MNLI (Liu et al., 2019) to score entailment between candidate and reference statements. Such embedding-based approaches capture deeper semantics, yet as Figure 1 shows, they still exhibit large drops in sensitivity or specificity under harder perturbations. Stability vs. Fragility. Metrics with only short arrow movements from easy to hard cases (e.g., TABXEVAL, TABREX ) demonstrate stable tradeoffs and thus robust generalization. Interestingly, even though TABXEVAL sits in the ideal zone, its trajectory drifts slightly away from the optimal direction as difficulty rises. By contrast, metrics such as EM, H-SCORE, and even the LLM-based P-SCORE experience sharp drops in sensitivity, revealing an over-reliance on surface-level cuesshowing that an LLM backbone alone does not guarantee proper alignment. Reference-less Metrics. Both QUESTEVAL and our proposed TABREX evaluate tables without explicit references, instead judging how well candidate table supports automatically generated questions. QUESTEVAL employs an LLM for question generation and QA module to assess semantic fidelity, but its reliance on generic QA signals often penalizes harmless re-orderings or formatting changes. In contrast, TABREX tailors question generation to tabular structure and integrates explicit reasoning over extracted facts, enabling it to better separate meaningful discrepancies from superficial variations. As shown in Figure 1, this specialization helps TABREX stay closer to the ideal zone even under tougher perturbations, reflecting stronger alignment with human judgment. Towards Trustworthy Evaluation. These rethe importance of balanced, sults highlight difficulty-robust metrics for downstream evaluation. As generative table models encounter noisier, real-world data, reliable metrics must reward genuine comprehension rather than superficial matches. The ability of TABREX to remain in the green ideal zone across difficulty levels-despite being reference-less underscores its suitability for highstakes domains such as scientific reporting and financial auditing, where both false alarms and missed discrepancies can be costly."
        },
        {
            "title": "3.3 Evaluation on Text-to-Table Task",
            "content": "To assess TABREX robustness in realistic reference-less settings, we evaluate its performance on text-to-table generation across diverse domains including finance, healthcare, and sports. Generated tables are produced by strong open and proprietary LLMs (Gemma-3-(4/27B), and InternVL-3.5-thinking (on/off)). Humans ranking generated tables across models and prompting strategies (zero-shot, CoT, Map&Make). Expert annotators ranked the model outputs along three axes structural correctness, factual fidelity, and semantic coverage. We then measured how well automatic metrics correlate with these human rankings (detailed in Appendix B) using Spearmans ρS, Kendalls τK, and rank-biased overlap (RBO). 6 Figure 4: Rubric-wise alignment across models and prompting strategies. Top row: cell-level agreement within model across prompts. Bottom row: table-level agreement. Model size and reasoning style influence local precision more than structural coherence, while prompt strategy (like Map&Make (Ahuja et al., 2025)) drives balanced alignment across rubric dimensions. Table 4: Correlation of automatic metrics with human rankings on real-world text-to-table generation. TABREX achieves the highest alignment across all correlation metrics. Metric EM ROUGE-L BERTScore BLEURT CHRF QuestEval (ref-less) TabEval (ref-based) TabXEval (ref-based) TABREX (ref-less) ρS 0.01 0.33 0.26 0.29 0.25 0.28 0.25 0.24 0.39 τK 0.01 0.25 0.19 0.20 0.19 0.20 0.19 0.17 0.30 RBO 0.33 0.29 0.38 0.39 0.36 0.39 0.36 0.37 0.41 Observations. Surfaceand embedding-based metrics (e.g., ROUGE-L, BERTScore, BLEURT) exhibit weak correlation with human preferences, primarily due to their sensitivity to lexical and formatting variation. QuestEval performs better but remains brittle to domain-specific structure shifts such as nested headers or missing subtables. In contrast, TABREX achieves the strongest correlations across all measures Spearmans ρ = 0.39, Kendalls τb = 0.30, and RBO=0.41 demonstrating superior alignment with expert judgments. Its graph-based reasoning captures factual and structural consistency more effectively, validating its reliability as reference-less evaluator for real-world table generation systems."
        },
        {
            "title": "3.4 Rubric-wise Model–Prompt Alignment",
            "content": "TABREX rubric-aware scoring enables coarse to fine-grained comparison across models (e.g., Gemma 8B vs. 27B, InternVL-Thinking On vs. Off) and prompting strategies (Zero-Shot, Chain-of-Thought, Map&Make (Ahuja et al., 2025)), measured at both cell-level and table-level granularity (Figure 4). Cell-level alignment (top row). Larger models (e.g., Gemma 27B) show clear gains in local fidelity especially for numeric and structural rubrics but only modest improvement in semantic consistency. Reasoning-oriented (Thinking) variants improve precision on numeric and structural dimensions yet often underperform on partial or contextual agreement, suggesting over-cautious reasoning can reduce semantic coverage. Chain-of-Thought prompting enhances numeric correctness but sometimes amplifies inconsistency, while Map&Make maintains more balanced yet slightly conservative performance. Table-level alignment (bottom row). At global scale, model size yields diminishing returns: Gemma 27Bs advantage narrows, and Thinking variants do not consistently outperform standard modes. Zero-shot improves row-column coherence but increases rubric variance. Map&Make achieves steadier rubric alignment, indicating stronger inte7 gration of local reasoning into structural organization. Insights. Overall, three trends emerge: (1) larger models enhance fine-grained (cell-level) fidelity but not global coherence; (2) Thinking reasoning improves precision but limits coverage, favoring accuracy over breadth; and (3) prompt design particularly Map&Make contributes as much as model scale to balanced rubric alignment. These results illustrate how referenceless, explainable evaluation metric can reveal the strengths and weaknesses of models and prompting strategies across hierarchical levels. Such rubric-aware scorers enable targeted analysis and can support verifiable reward modeling (Shao et al., 2024) for improved alignment."
        },
        {
            "title": "4 Comparison with Related Work",
            "content": "From Text-to-Table to Structural Benchmarks. Early text-to-table datasets such as ROTOWIRE for basketball summaries (Wiseman et al., 2017), E2E for restaurant descriptions (Novikova et al., 2017), WIKIBIO for infobox biographies (Lebret et al., 2016), and WIKITABLETEXT (Pasupat and Liang, 2015) provided important initial testbeds but offered limited schema diversity and often encouraged hallucinated or under-structured outputs. Recent resources, including STRUCTBENCH (Gu et al., 2025) and TANQ (Akhtar et al., 2025), introduced challenging phenomena such as header permutations, schema reshuffling, and multi-hop reasoning. These benchmarks exposed fundamental weaknesses in both generation models and evaluation metrics, motivating the need for metrics that go beyond surface overlap and can reason about structural and semantic fidelity. Metric Families: From Overlap to Explainability. Conventional reference-based metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005), chrF (Popovic, 2015), and even embedding-based BERTSCORE (Zhang* et al., 2020) treat tables as flat text, often ignoring header alignment, units, or cell hierarchy. PARENT (Dhingra et al., 2019) partly grounds evaluation in the input source but still struggles with schema-level changes. Algorithmic and LLM-assisted metrics such as HSCORE and P-SCORE (Tang et al., 2024) move toward structural sensitivity but differ in design: the former computes heuristic, rule-based structural and content similarity, while the latter leverages LLM judgments; both offer limited interpretability. TABEVAL (Ramu et al., 2024) improves semantic coverage by decomposing tables into atomic statements and applying textual entailment, yet incurs NLI overheads and often over-penalizes harmless layout differences. The recent TABXEVAL (Panits choli et al., 2025) represents step-change: two-phase design TabAlign for structural alignment and TabCompare for semantic/syntactic checks delivers interpretable cell-level diagnostics and consistently balances sensitivity and specificity, achieving strong human correlation and placing it in the Goldilocks zone for robust evaluation. Reference-less Evaluation and Remaining Gaps. Metrics such as QUESTEVAL and DataQUESTEVAL (Rebuffel et al., 2021) demonstrate that reference-less evaluation is viable by generating and answering questions over the source data, showing strong alignment with humans in datato-text tasks. However, their reliance on generic QA signals often misses table-specific structural errors, unit inconsistencies, or localized discrepancies. Despite advances from overlap-based to LLM-driven and rubric-based methods, most existing approaches still emphasize either semantics or structure and condense diverse errors into single opaque score, limiting error traceability and robustness under realistic perturbations."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "We introduced TABREX , property-driven, reference-less framework for evaluating tabular generation through graph-based reasoning and interpretable, rubric-aware scoring. By unifying structured alignment, factual comparison, and sensitivityspecificity control within single pipeline, TABREX delivers consistent, human-aligned judgments that remain robust under domain shifts and perturbation difficulty. Our accompanying benchmark, TABREX-BENCH , establishes new standard for systematic stress testing of table metrics across six diverse domains and twelve controlled perturbation types. Experiments demonstrate that TABREX not only correlates most strongly with human evaluations but also provides fine-grained, explainable diagnostics at both cell and table levels enabling actionable analysis of model and prompt behaviors. Beyond outperforming reference-based and LLMjudge baselines, it shows that reliable table eval8 uation is possible without explicit references by reasoning over grounded factual graphs. Future work will focus on extending TABREX to richer structural formats such as hierarchical or multi-modal tables, and on distilling its LLM components into lightweight, domain-adaptive evaluators for scalable deployment. We envision TABREX as foundation for trustworthy, interpretable evaluation in structured generation supporting better model selection, alignment, and reward learning across real-world applications."
        },
        {
            "title": "6 Limitations",
            "content": "While TABREX achieves robust and interpretable evaluation, it has few limitations. It relies on large language models for fact extraction and alignment, which adds computational cost and mild variability due to model stochasticity. The current implementation supports only structured digital tables (e.g., HTML, Markdown) and cannot yet handle tables embedded in images or PDFs requiring OCR or visual parsing. Finally, although TABREX-BENCH spans six diverse domains, it remains limited to English and synthetic perturbations, leaving realworld noise, multilingual data, and complex layouts for future exploration."
        },
        {
            "title": "7 Ethics Statement",
            "content": "The authors affirm that this work adheres to the highest ethical standards in research and publication. Ethical considerations have been meticulously addressed to ensure responsible conduct and the fair application of computational linguistics methodologies. Our findings are aligned with experimental data, and while some degree of stochasticity is inherent in black-box Large Language Models (LLMs), we mitigate this variability by maintaining fixed parameters such as temperature, topp, and topk. Furthermore, our use of LLMs, including GPT-5-nano, Gemma, and InternVL, complies with their respective usage policies. To refine the clarity and grammatical accuracy of the text, AI based tools such as Grammarly and ChatGPT were employed. Additionally, human annotators who are also among the authors actively contributed to data labeling and verification, ensuring high-quality annotations. To the best of our knowledge, this study introduces no additional ethical risks."
        },
        {
            "title": "References",
            "content": "Naman Ahuja, Fenil Bardoliya, Chitta Baral, and Vivek Gupta. 2025. Map&make: Schema guided text to In Proceedings of the 63rd Antable generation. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 30249 30262, Vienna, Austria. Association for Computational Linguistics. Mubashara Akhtar, Chenxi Pang, Andreea Marzoca, Yasemin Altun, and Julian Martin Eisenschlos. 2025. TANQ: An open domain dataset of table answered questions. Preprint, arXiv:2405.07765. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Centers for Medicare & Medicaid Services. 2019. Medical charges. Retrieved from OpenML (ID: 43928). Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. FinQA: Dataset of Numerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 36973711, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and Dongmei Zhang. 2022. HiTab: hierarchical table dataset for question answering and natural language generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10941110, Dublin, Ireland. Association for Computational Linguistics. Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, MingWei Chang, Dipanjan Das, and William Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 48844895, Florence, Italy. Association for Computational Linguistics. Zhouhong Gu, Haoning Ye, Xingzhou Chen, Zeyang Zhou, Hongwei Feng, and Yanghua Xiao. 2025. StrucText-eval: Evaluating large language models reasoning ability in structure-rich text. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 223244, Vienna, Austria. Association for Computational Linguistics. Alistair Johnson, Luca Bulgarelli, Tom Pollard, Brian Gow, Benjamin Moody, Steven Horng, Leo Anthony Celi, and Roger Mark. 2024. MIMIC-IV (version 3.1). RRID:SCR_007345. 9 Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 12031213, Austin, Texas. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Jekaterina Novikova, Ondˇrej Dušek, and Verena Rieser. 2017. The E2E Dataset: New Challenges For Endto-End Generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201206, Saarbrücken, Germany. Association for Computational Linguistics. Vihang Pancholi, Jainit Sushil Bafna, Tejas Anvekar, Manish Shrivastava, and Vivek Gupta. 2025. TabXEval: Why this is bad table? an eXhaustive rubric for table evaluation. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2291322934, Vienna, Austria. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: controlled table-to-text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 11731186, Online. Association for Computational Linguistics. Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1470 1480, Beijing, China. Association for Computational Linguistics. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Pritika Ramu, Aparna Garimella, and Sambaran Bandyopadhyay. 2024. Is this bad table? closer look at the evaluation of table generation from text. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 22206 22216, Miami, Florida, USA. Association for Computational Linguistics. Clement Rebuffel, Thomas Scialom, Laure Soulier, Benjamin Piwowarski, Sylvain Lamprier, Jacopo Staiano, Geoffrey Scoutheeten, and Patrick Gallinari. 2021. Data-QuestEval: Referenceless Metric for Datato-Text Semantic Evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 80298036, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. Bleurt: Learning robust metrics for text generation. Preprint, arXiv:2004.04696. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. J.W. Smith, J.E. Everhart, W.C. Dickson, W.C. Knowler, and R.S. Johannes. 1988. Using the adap learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care, pages 261265. IEEE Computer Society Press. Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. 2024. Struc-bench: Are large language models good at generating complex structured tabular data? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 1234, Mexico City, Mexico. Association for Computational Linguistics. Gemma Team. 2025a. Gemma 3. OpenAI Team. 2025b. GPT-5 System Card. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, and 1 others. 2025. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265. Sam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in Data-to-Document Generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 22532263, Copenhagen, Denmark. Association for Computational Linguistics. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations."
        },
        {
            "title": "A Prompt Templates",
            "content": "This rubric ensured consistent and interpretable human rankings aligned with the metrics propertydriven principles. Prompt A: Table Summary Generation Walk-Through Example of TABREX # System Prompt You are neutral data narrator for arbitrary domains. (cid:44) Write cohesive, flowing paragraph (4-7 (cid:44) sentences) describing the information in (cid:44) markdown table. Do not ask for additional data or (cid:44) refuse; never mention the table itself or (cid:44) formatting. Avoid lists, bullets, colons, or name (cid:44) :value patterns; use full sentences and connect (cid:44) ideas. Summarize salient figures, ranges, (cid:44) extremes, comparisons, and notable trends. Light (cid:44) interpretation is allowed if consistent with the (cid:44) numbers.Do not mention the table or its structure (cid:44) . Plain text only. # User Prompt f\"Markdown Table: {markdown_table}\""
        },
        {
            "title": "B Human Evaluation Protocol",
            "content": "Human annotators were instructed to evaluate the similarity of generated tables to the gold (groundtruth) tables whenever available or against source text following consistent rubric. Each annotation batch contained one gold table and five generated candidates. Annotators ranked candidates from 1 (best) to 5/12 (depending on task) (worst) based on their structural and contextual fidelity to the gold table. Structural Factors. Annotators prioritized structural integrity in the following order: (1) Column Missing - tables omitting columns were penalized most heavily; (2) Column Extra - extra columns ranked lower in case of ties; (3) Row Missing and (4) Row Extra - missing or spurious rows reduced rank; (5) Cell Missing and (6) Cell Extra - missing or redundant cells influenced ranking proportionally; (7) Partial Mismatching Severity - deviations in value accuracy or format were also considered. Contextual Factors. Within equal structural quality, contextual accuracy guided ranking: (1) string-value mismatches, (2) numeric, boolean, or date-time inaccuracies, (3) inconsistencies in listtype entries, and (4) deviations in other less common data types. Tie-Breaking. In case of ties, rankings were determined by the number of affected cells within rows and columns. Column headers with semantically incorrect or mismatched meanings were treated as wrong columns and penalized equivalently to missing columns. 11 For full details of the formalism, please refer to the main paper. Here we provide only the default hyperparameters and worked example to show how the score is computed in the reference-less setting."
        },
        {
            "title": "Symbol Meaning",
            "content": "Weight for Missing Information (MI) Weight for Extra Information (EI) βMI βEI βpartial Weight for partially correct cell values αr Row-level (subject) structural weight αc Column-level (predicate) structural weight αcell Cell-level (object) structural weight Scaling factor for partial deviation γ ωp"
        },
        {
            "title": "Value",
            "content": "1.0 0.9 0.8 0.9 1.0 0.8 0.9 Table 5: Default TABREX hyperparameters. Hyperparameters. Setup. Let GS be the source-text evidence graph and GT the generated-table graph. All counts below are measured relative to GS. Assume Nr = 5, Nc = 4, Ncell = 20, with discrepancies: MIr = 1, EIc = 1, MIcell = 2, EIcell = 1, and two partially aligned cells with normalized deviations 0.2 and 0.5. Step 1: Table-level penalty. TablePenalty = βMIαr"
        },
        {
            "title": "MIr\nNr",
            "content": "+ βMIαc MIc Nc EIc EIr + βEIαc + βEIαr Nc Nr 5 ) + 0.9(1.0 1 = 1.0(0.9 1 4 ) = 0.18 + 0.225 = 0.405. Step 2: Cell-level penalty. Partial-match deviations: γ1 = ωp 0.2 = 0.18, γ2 = ωp 0.5 = 0.45, (cid:88) γi = 0.63. CellPenalty = βMIαcell"
        },
        {
            "title": "EIcell\nNcell",
            "content": "+ βEIαcell (cid:88) MIcell Ncell + βpartialαcell = 1.00.8 2 + 0.80.8 0.63 20 = 0.08 + 0.036 + 0.0202 = 0.1362. 20 + 0.90.8 1 γi 20 Step 3: Final score. STABREX = TablePenalty + CellPenalty = 0.405 + 0.1362 = 0.5412. Interpretation. The example shows that both structural discrepancies (missing rows, extra columns) and factual deviations (partially mismatched cell values) jointly contribute to the final reference-less TABREX score. 12 Prompt B: Perturbation Planning # Allowed Types by Group (overview mapping) ```python group_to_types = { \"0\": [\"header_shuffle\", \"reorder_columns\", \"reorder_rows\", \"columns_to_rename\", \"rows_to_rename\", \"data_type_change\", \" (cid:44) unit_conversion\", \"paraphrased_cell_values\",], \"1\": [\"columns_to_delete\", \"rows_to_delete\", \"add_columns\", \"add_rows\", \"column_disintegration\", \"columns_merge\", \" (cid:44) structure_change\", \"slight_data_differences\", \"precision_change\", \"misspellings\", \"data_swap\", \"add_symbols\", \" (cid:44) remove_symbols\"], } ``` # System Prompt You are an expert Python programmer. Generate two outputs - Python function and JSON array - separated by marker. Core (cid:44) goal is to produce perturbed tables under two philosophies: **Group 0 (\"Semantically Identical\")** - alter presentation without changing facts. Allowed: `reorder_rows`, `header_shuffle`, `paraphrased_cell_values`, `data_type_change`, `unit_conversion`. Do not add/ (cid:44) delete rows or columns. Difficulty levels: [easy --> one simple change, medium --> two combined changes, hard --> three or more complex changes] **Unit Conversion Rules** - Convert only when the unit is in cell text (e.g., ``12 km --> 7.456 mi\"). - Update headers and recompute totals if affected. - Ensure - f^{-1}(v') <= max(1e-6, 0.001*v). **Paraphrase & Format Rules** - Preserve meaning, entities, and tokens. - Format/rounding variation <= 0.1%. - Totals/percentages must stay numerically identical. **Group 1 (\"Semantically Different\")** - break meaning and falsify facts. Combine weak perturbations (`misspellings`, ` (cid:44) precision_change`) only with strong ones (`data_swap`, `delete_rows`, etc.). **Quantified Impact** - `slight_data_differences`: +5-10% (easy), +20-50% (hard). - `data_swap`: swap entire columns. - `add_rows` / `delete_rows`: modify >= 20% of rows. - `delete_columns`: remove key or total column. ## Output Specification Your output must follow this structure exactly: 1. Python block defining `apply_perturbations()` (closed by ```). 2. Separator line: `---JSON---` 3. Raw JSON array (no markdown fences). ### Python Section Define: ```python def apply_perturbations(): ... ``` It must return list of dicts with: `{'perturbed_table': <markdown>, 'metadata': <object>}` **Metadata fields:** `slot_id`, `group`, `difficulty`, `selected_types`, `applied_order`. Use helpers only: `create_markdown_table`, `safe_float`, `add_noise`, `safe_round`, `parse_markdown_table`. Always use `parse_markdown_table(markdown_text)` - never manual ``\" splitting. Preserve empty headers and columns. Check for (cid:44) `None` after `safe_float` before math. **Operation order:** 1. Structural --> 2. Layout --> 3. Naming --> 4. Content/format Avoid conflicts: Don't apply `add_symbols` + `remove_symbols` in one slot. Don't rename then delete the same column. Perform (cid:44) merges before renames. **Result Example** ```python results.append({ \"perturbed_table\": create_markdown_table(headers, data_rows), \"metadata\": {\"slot_id\": slot_id, \"group\": group, \"difficulty\": difficulty, \"selected_types\": selected_types, \"applied_order (cid:44) \": applied_order} }) ``` # User Prompt (generation-time wrapper) Here is the markdown table and the `{len(plan_slots)}` plan slots to implement. Return list of dicts, each with ` (cid:44) perturbed_table` and `metadata`. ```markdown {markdown_table} ``` 13 Prompt C: Text2Graph # System Prompt You are precise data structuring agent. Convert information from any source (text or table) into standardized knowledge (cid:44) graph of [subject, property, value] triplets. --- THE GOLDEN RULE: STANDARDIZED GRAMMAR --- Follow this strict grammar for every triplet. 1. The subject is the PRIMARY ENTITY: - Choose the main entity the fact is about (e.g., \"aboriginal population\", \"non-aboriginal population\"). - ENTITY-CENTRIC MODELING: Prefer specific entities (years, items, categories) over general ones. - GOOD: \"2020\", \"product_a\", \"category_x\" - AVOID: \"company_data\", \"financial_info\" - For time-based data: use the time period as the subject (e.g., \"2020\", \"q1_2021\", \"january\"). - For categorical data: use the category as the subject (e.g., \"electronics\", \"clothing\", \"services\"). 2. The predicate is NORMALIZED PROPERTY KEY: - Combine the core concept and its condition using underscores: concept_condition. - Use lowercase throughout. - Maintain consistent patterns for similar concepts (e.g., \"revenue_2020\", \"revenue_2021\"). - Do not add prefixes like \"total_\", \"combined_\", \"gross_\". - Examples: - Core: \"participation rate\", Condition: \"married or common-law\" --> `participation_rate_married_or_common_law` - Core: \"employment rate difference\", Condition: \"single or previously married\" --> ` (cid:44) employment_rate_difference_single_or_previously_married` 3. The object is the CLEAN VALUE: - Use the most atomic data point (e.g., \"81.9%\", \"7.0 percentage points\"). - Preserve units and formatting. - Use \"-\" if missing. --- EXAMPLE OF APPLYING THE GRAMMAR --- Source: \"For the non-aboriginal population, the unemployment rate for those who are single or previously married was 8.2%.\" Step 1: Subject --> \"non-aboriginal population\" Step 2: Predicate --> `unemployment_rate_single_or_previously_married` Step 3: Object --> \"8.2%\" Final Triplet--> [\"non-aboriginal population\", \"unemployment_rate_single_or_previously_married\", \"8.2%\"] --- ENTITY-CENTRIC MODELING EXAMPLES --- GOOD (time-based): [\"2020\", \"debt_amount\", \"100000\"] [\"2021\", \"debt_amount\", \"120000\"] BAD (concept-centric): [\"debt_data\", \"amount_2020\", \"100000\"] GOOD (category-based): [\"electronics\", \"sales_volume\", \"50000\"] [\"clothing\", \"sales_volume\", \"30000\"] BAD (concept-centric): [\"product_sales\", \"electronics_volume\", \"50000\"] [\"product_sales\", \"clothing_volume\", \"30000\"] --- STRICT FORMAT CHECKLIST --- - Output ONLY JSON arrays of [subject, predicate, object]. - Each triplet must have exactly 3 elements. - Do NOT paraphrase, re-case, or stem subjects/predicates - copy labels verbatim from the source. - Preserve punctuation, spaces, and capitalization. - Use \"-\" for unknown values. - Do NOT invent or omit data. - Examples: - GOOD: [\"2014\", \"copenhagen_shipment_volume\", \"448.6 million\"] - BAD: [\"2014\", \"copenhagen_shipment_volume\"] - BAD: {\"subject\": \"2014\", \"predicate\": \"...\", \"object\": \"...\"} (wrong format) # User Prompt Follow the standardized grammar from the System Prompt to convert the given input into triplets. CRITICAL REMINDERS: 1. Use ENTITY-CENTRIC modeling make specific years, categories, or items the subjects. 2. For time-based data: use years or periods (e.g., \"2020\", \"q1_2021\"). 3. For categorical data: use categories (e.g., \"electronics\", \"clothing\"). 4. Avoid using one central subject for all facts. 5. Use consistent, minimal predicates (e.g., \"amount\", \"value\", \"count\"). 6. Use \"-\" for missing data. Task Input: Input Summary: {summary} 14 Prompt D: Graph Alignment # System Prompt You are structured reasoning engine comparing two knowledge graphs: **T1 (summary_graph)** and **T2 (table_graph)**. Your goal: align their triplets `[subject, predicate, object]` semantically and output structured JSON comparison. ### ALIGNMENT PRINCIPLES Content-first alignment - never by position. Example: match ``Product Alpha\" <--> ``Product Alpha\" even if order differs. - Match facts based on meaning (subject/predicate semantics). - Report unmatched ones as Missing (MI) or Extra (EI). ### OUTPUT STRUCTURE Each aligned pair becomes: ```json { \"aligned_triplet\": [\"subject1/subject2\", \"predicate1/predicate2\", \"object1/object2\"], \"object_metadata\": {\"datatype\": \"...\", \"entitytype\": \"...\", \"unit\": \"...\", \"difference\": \"...\", \"missing_extra_info\": \"...\"} (cid:44) } ``` Rules: - Copy strings verbatim from source (no rephrasing, re-casing, or normalization). - Always include `/` in each component (use `-` for missing). - Allow cross-component semantic matches (e.g., ``2014\" <--> ``shipment_volume_2014\"). - Each source triplet is used once. ### OBJECT METADATA Compute difference and add context: - If numeric with scale units (thousand/million/billion) --> convert and return absolute numeric difference. - `\"448.6 million\"` vs `\"449 million\"` --> `\"difference\": \"400000\"` - If non-numeric or missing --> `\"difference\": \"-\"` - Mark `\"missing_extra_info\"` as `\"MI\"` or `\"EI\"` when one side absent. Range Handling: Single vs range --> min distance. Overlapping ranges --> `\"difference\": \"0\"`. ### OUTPUT RULES - Output ONLY JSON matching `FinalComparisonResult`. - Every `aligned_triplet` has exactly three entries (each with one `/`). - `difference` is numeric-only (no text, commas, or units). - Use `\"-\"` for truly unmatched components. ### Example Input: ```json T1: [[\"2014\", \"copenhagen_shipment_volume\", \"448.6 million\"]] T2: [[\"copenhagen\", \"shipment_volume_2014\", \"449 million\"], [\"copenhagen\", \"average_growth_rate\", \"6.96%\"]] ``` Output: ```json { \"aligned_facts\": [ { \"aligned_triplet\": [\"2014/copenhagen\", \"copenhagen_shipment_volume/shipment_volume_2014\", \"448.6 million/449 million\"], (cid:44) \"object_metadata\": {\"difference\": \"400000\", \"missing_extra_info\": \"None\"} }, { \"aligned_triplet\": [\"copenhagen/copenhagen\", \"-/average_growth_rate\", \"-/6.96%\"], \"object_metadata\": {\"difference\": \"-\", (cid:44) \"missing_extra_info\": \"EI\"} } ] } ``` # User Prompt Apply the alignment process above to the following graphs: Input Graph T1 (summary_graph): ```json {json.dumps(summary_graph, indent=2)} ``` Input Graph T2 (table_graph): ```json {json.dumps(table_graph, indent=2)} ``` Unit Hints (optional): ```json {_unit_hints_json} ``` Use hints only for internal numeric scaling (e.g., \"thousand\" --> *1,000). Do not modify string outputs."
        }
    ],
    "affiliations": [
        "Adobe",
        "Arizona State University"
    ]
}