{
    "paper_title": "Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos",
    "authors": [
        "Gengshan Yang",
        "Andrea Bajcsy",
        "Shunsuke Saito",
        "Angjoo Kanazawa"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over a long time-span (e.g., a month) in a single environment. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos captured by a smartphone."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 9 5 2 6 1 . 0 1 4 2 : r AGENT-TO-SIM: LEARNING INTERACTIVE BEHAVIOR MODELS FROM CASUAL LONGITUDINAL VIDEOS Gengshan Yang1 Andrea Bajcsy2 Shunsuke Saito1 Angjoo Kanazawa 3 1Codec Avatar Labs, Meta 2Carnegie Mellon University 3UC Berkeley"
        },
        {
            "title": "ABSTRACT",
            "content": "We present Agent-to-Sim (ATS), framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over long time-span (e.g. month) in single environment. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over long time period. To obtain such data, we develop coarse-to-fine registration method that tracks the agent and the camera over time through canonical 3D space, resulting in complete and persistent spacetime 4D representation. We then train generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos captured by smartphone. Project page: gengshan-y.github.io/agent2sim-www/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Consider an image on the right: where will the cat go and how will it move? Having seen cats interacting with the environment and people many times, we know that cats often go to the couch and follow humans around, but run away if people come too close. Our goal is to learn such behavior model of physical agents from visual observations, just like humans can. This is fundamental problem with practical application in content generation for VR/AR, robot planning in safetycritical scenarios, and behavior imitation from the real world (Park et al., 2023; Ettinger et al., 2021; Puig et al., 2023; Srivastava et al., 2022; Li et al., 2024). In step towards building faithful models of agent behaviors, we present ATS (Agent-to-Sim), framework for learning interactive behavior models of 3D agents observed over long span of time in single environment, as shown in Fig. 1. The benefits of such setup is multitude: 1) It is accessible, unlike approaches that capture motion data in controlled studio with multiple cameras (Mahmood et al., 2019; Joo et al., 2017; Hassan et al., 2021; Kim et al., 2024), our approach only requires single smartphone; 2) It is natural since the capture happens in the agents everyday environment, it enables observing the full spectrum of natural behavior non-invasively; 3) Furthermore, it allows for longitudinal behavior capture, e.g., one that happens over span of month, which helps capturing wider variety of behaviors; 4) In addition, this setup enables modeling the interactions between the agents and the observer, i.e. the person taking the video. While learning from casual longitudinal video observations has benefits, it also brings new challenges. Videos captured over time needs to be registered and reconstructed in consistent manner. Earlier methods that reconstruct each video independently (Song et al., 2023; Gao et al., 2022; Park et al., 2021) is not enough, as they do not solve correspondence across the videos. In this work, we tackle more challenging scenario: building complete and persistent 4D representation from orders of magnitude more data, e.g., 20k frames of videos, and use them to learn behavior models of an agent. The last two authors equally mentored this project by both having babies. 1 Figure 1: Learning agent behavior from longitudinal casual video recordings. We answer the following question: can we simulate the behavior of an agent, by learning from casually-captured videos of the same agent recorded across long period of time (e.g., month)? A) We first reconstruct videos in 4D (3D & time), which includes the scene, the trajectory of the agent, and the trajectory of the observer (i.e., camera held by the observer). Such individual 4D reconstructions are registered across time, resulting in complete and persistent 4D representation. B) Then we learn model of the agent for interactive behavior generation. The behavior model explicitly reasons about goals, paths, and full body movements conditioned on the agents ego-perception and past trajectory. Such an agent representation allows generation of novel scenarios through conditioning. For example, conditioned on different observer trajectories, the cat agent chooses to walk to the carpet, stays still while quivering his tail, or hide under the tray stand. Please see videos results in the supplement. To this end, we introduce novel coarse-to-fine registration approach that re-purposes large image models, such as DiNO-v2 (Oquab et al., 2023), as neural localizers, which register the cameras with respect to canonical spaces of both the agent and the scene. While TotalRecon (Song et al., 2023) explored reconstructing both the agent and the scene from single video, our approach enables reconstructing multiple videos into complete and persistent 4D representation containing the agent, the scene, and the observer. Then, an interactive behavior model can be learned by querying paired ego-perception and motion data from such 4D representation. The resulting framework, ATS, can simulate interactive behaviors like those described at the start: agents like pets that leap onto furniture, dart quickly across the room, timidly approach nearby users, and run away if approached too quickly. Our contributions are summarized as follows: 1. 4D from Video Collections. We build persistent and complete 4D representations from collection of casual videos, accounting for deformations of the agent, the observer, and changes of the scene across time, enabled by coarse-to-fine registration method. 2. Interactive Behavior Generation. ATS learns behavior that is interactive to both the observer and 3D scene. We show results of generating plausible animal and human behaviors reactive to the observers motion, and aware of the 3D scene. 3. Agent-to-Sim (ATS) Framework. We introduce real-to-sim framework to learn simulators of interactive agent behavior from casually-captured videos. ATS learns natural agent behavior, and is scalable to diverse scenarios, such as animal behavior and casual events."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "4D Reconstruction from Monocular Videos. Reconstructing time-varying 3D structures from monocular videos is challenging due to its under-constrained nature. Given monocular video, there are multiple different interpretations of the underlying 3D geometry, motion, appearance, and lighting (Szeliski & Kang, 1997). As such, previous methods often rely on category-specific 3D prior (e.g., 3D humans) (Goel et al., 2023; Loper et al., 2015; Kocabas et al., 2020) to deal with the ambiguities. Along this line of work, there are methods to align reconstructed 3D humans to the world coordinate with the help of SLAM and visual odometry (Ye et al., 2023; Yuan et al., 2022; Kocabas et al., 2023). Sitcoms3D (Pavlakos et al., 2022) reconstructs both the scene and human parameters, while relying on shot changes to determine the scale of the scene. However, the use of parametric body models limits the degrees of freedom they can capture, and makes it difficult to reconstruct agents from arbitrary categories which do not have pre-built body model, for example, animals. Another line of work (Yang et al., 2022; Wu et al., 2021) avoids using category-specific 3D priors and optimizes the shape and deformation parameters of the agent given pixel priors (e.g., optical flow and object segmentation), which works well for broad range of categories including human, animals, and vehicles. TotalRecon (Song et al., 2023) further takes into account the background scene, such that the motion of the agent can be decoupled from the camera and aligned to the world space. However, most of the method operates on few hundreds of frames, and none of them can reconstruct complete 4D scene while obtaining persistent 3D tracks over orders of magnitude more data (e.g., 20k frames of videos). We develop coarse-to-fine registration method to register the agent and the environment into canonical 3D space, which allows us to leverage large-scale video collection to build agent behavior models. Behavior Prediction and Generation. Behavior prediction has long history, starting from simple physics-based models such as social forces (Helbing & Molnar, 1995; Alahi et al., 2016) to more sophisticated planning-based models that cast prediction as reward optimization, where the reward is learned via inverse reinforcement learning(Kitani et al., 2012; Ziebart et al., 2009; Ma et al., 2017; Ziebart et al., 2008). With the advent of large-scale motion data, generative models have been used to express behavior multi-modality (Mangalam et al., 2021; Salzmann et al., 2020; Choi et al., 2021; Seff et al., 2023; Rhinehart et al., 2019). Specifically, diffusion models are used for behavior modeling for being easily controlled via additional signals such as cost functions (Jiang et al., 2023) or logical formulae (Zhong et al., 2023). However, to capture plausible behavior of agents, they require diverse data collected in-the-wild with associated scene context, e.g., 3D map of the scene (Ettinger et al., 2021). Such data are often manually annotated at bounding box level (Girase et al., 2021; Ettinger et al., 2021), which limits the scale and the level of detail they can capture. 3D Agent Motion Generation. Beyond autonomous driving setup, existing works for human and animal motion generation (Tevet et al., 2022; Rempe et al., 2023; Xie et al., 2023; Shafir et al., 2023; Karunratanakul et al., 2023; Pi et al., 2023; Zhang et al., 2018; Starke et al., 2022; Ling et al., 2020; Fussell et al., 2021) have been primarily using simulated data (Cao et al., 2020; Van Den Berg et al., 2011) or motion capture data collected with multiple synchronized cameras (Kim et al., 2024; Mahmood et al., 2019; Hassan et al., 2021; Luo et al., 2022). Such data provide high-quality body motion, but the interactions of the agents with the environment are either restricted to flat ground, or set of pre-defined furniture or objects (Hassan et al., 2023; Zhao et al., 2023; Lee & Joo, 2023; Zhang et al., 2023a; Menapace et al., 2024). Furthermore, the use of simulated data and motion capture data inherently limits the naturalness of the learned behavior, since agents often behave differently when being recorded in capture studio compared to natural environment. To bridge the gap, we develop 4D reconstruction methods to obtain high-quality trajectories of agents interacting with natural environment, with simple setup that can be achieved with smartphone."
        },
        {
            "title": "3 APPROACH",
            "content": "ATS learns behavior models of an agent in 3D environment given RGBD videos. Sec. 3.1 describes our spacetime 4D representation that contains the agent, the scene, and the observer. We fit such 4D representation to collection of videos in coarse-to-fine manner, where the camera poses are initialized from data-driven methods and refined through differentiable rendering optimization (Sec. 3.2). Given the 4D reconstruction, Sec. 3.3 trains an behavior model of the agent that is interactive to the scene and the observer. We provide table of notations and modules in Tab. 6-7. 3 3.1 4D REPRESENTATION: AGENT, SCENE, AND OBSERVER Given many monocular videos, our goal is to build complete and persistent spacetime 4D reconstruction of the underlying world, including deformable agent, rigid scene, and moving observer. We factorizes the 4D reconstruction into canonical structure and time-varying structure. Canonical Structure = {σ, c, ψ}. The canonical structure contains an agent neural field and scene neural field, which are time-independent. They represent densities σ, colors c, and semantic features ψ implicitly with MLPs. To query the value at any 3D location X, we have (σs, cs, ψs) = MLPscene(X, βi), (σa, ca, ψa) = MLPagent(X). (1) (2) The scene field takes in learnable code βi (Niemeyer & Geiger, 2021) per-video, which can represent scenes of slightly different appearance and layout (across videos) with shared backbone. Time-varying Structure = {ξ, G, W}. The time-varying structure contains an observer and an agent. The observer is represented by the camera pose ξt SE(3), defined as canonical-to-camera transformations. The agent is represented by root pose G0 SE(3), defined as canonical-tocamera transformations, and set of 3D Gaussians, {Gb t}{b=1,...,25}, referred to as bones (Yang et al., 2022). Bones have time-varying centers and orientations but constant scales. Through blendskinning (Magnenat et al., 1988) with learned forward and backward skinning weights (Saito et al., 2021), any 3D location in the canonical space can be mapped to the time space and vice versa, Xt = GaX = (cid:33) WbGb X, (cid:32) (cid:88) b= (3) which computes the motion of point by blending the bone transformations (we do so in the dual quaternion space (Kavan et al., 2007) to ensure Ga is valid rigid transformation). The skinning weights are defined as the probability of point assigned to each bone. Rendering. To render images from the 4D representation, we use differentiable volume rendering (Mildenhall et al., 2020) to sample rays in the camera space, map them separately to the canonical space of the scene and the agent with D, and query values (e.g., density, color, feature) from the canonical fields of the scene and the agent. The values are then composed for ray integration (Niemeyer & Geiger, 2021). To optimize the world representation {T, D}, we minimize the difference between the rendered pixel values and the observations, as described later in Sec. 3.2. 3.2 OPTIMIZATION: COARSE-TO-FINE MULTI-VIDEO REGISTRATION Given images from videos represented by color and feature descriptors (Oquab et al., 2023), {Ii, ψi}i={1,...,M }, our goal is to find spacetime 4D representation where pixels with the same semantics can be mapped to same canonical 3D locations. Variations of appearance, lighting, and camera viewpoint across videos make it challenging to buil such persistent 4D representation. We design coarse-to-fine registration approach that globally aligns the agent and the observer poses to their canonical space, and then jointly optimizes the 4D representation while adjusting the poses locally. Such coarse-to-fine registration avoids bad local optima in the optimization. Initialization: Neural Localization. Due to the evolving nature of scenes across long period of time (Sun et al., 2023), there exist both global layout changes (e.g., furniture get rearranged) and appearance changes (e.g., table cloth gets replaced), making it challenging to find accurate geometric correspondences (Brachmann & Rother, 2019; Brachmann et al., 2023; Sarlin et al., 2019). With the observation that large image models have good 3D and viewpoint awareness (El Banani et al., 2024), we adapt them for camera localization. We learn scene-specific neural localizer that directly regresses the camera pose of an image with respect to canonical structure, where fθ is ResNet-18 (He et al., 2016) and ψ is the DINOv2 (Oquab et al., 2023) feature of the input image. We find it to be more robust than geometric correspondence, while being more ξ = fθ(ψ), (4) 4 computationally efficient than pairwise matches (Wang et al., 2023). To learn the neural localizer, we first capture walk-through video and build 3D map of the scene. Then we use it to train the neural localizer by randomly sampling camera poses = (R, t) and rendering images on the fly, arg min θ (cid:88) (cid:0) log(RT 0 (θ)R) + t0(θ) t2 2 (cid:1) , (5) where we use geodesic distance (Huynh, 2009) for camera rotation and L2 error for camera translation. Similarly, we train camera pose estimator of the agent. First, we fit dynamic 3DGS (Luiten et al., 2024; Yang et al., 2023a) to long video of the agent with complete viewpoint coverage. Then we use the dynamic 3DGS as the synthetic data generator, and train pose regressor to predict root poses G0. During training, we randomly sample camera poses, time instances, and apply image space augmentations, including color jittering, cropping and masking. Objective: Feature-metric Loss. To refine the camera registration as well as learn the deformable agent model, we fit the 4D representation {T, D} to the data {Ii, ψi}i={1,...,M } using differentiable rendering. Compared to fitting raw rgb values, feature descriptors from large pixel models (Oquab et al., 2023) are found more robust to appearance and viewpoint changes. Therefore, we model 3D feature fields (Kobayashi et al., 2022) besides colors in our canonical NeRFs (Eq. 1-2), render them, and apply both photometric and featuremetric losses, min T,D (cid:88) (cid:0)It RI (t; T, D)2 2 + ψt Rψ(t; T, D)2 2 (cid:1) + Lreg(T, D), (6) where R() is the renderer described in Sec 3.1. The observer (scene camera) and the agents root pose are initialized from the coarse registration. Using featuremetric errors makes the optimization robust to change of lighting, appearance, and minor layout changes, which helps find accurate alignment across videos. We also apply regularization term that includes eikonal loss, silhouette loss, flow loss and depth loss similar to Song et al. (2023). Scene Annealing. To reconstruct complete 3D scene when some videos are partial capture (e.g. half of the room), we encourage the reconstructed scenes across videos to be similar. To do so, we randomly swap the code β of two videos during optimization, and gradually decrease the probability of applyig swaps from = 1.0 0.05 over the course of optimization. This regularizes the model to share structures across all videos, but keeps video-specific details  (Fig. 3)  . 3.3 INTERACTIVE BEHAVIOR GENERATION Given the 4D representation, we extract 3D feature volume of the scene Ψ and world-space trajectories of the observer ξw = ξ1 as well as the agent G0,w = ξwG0, Gb,w = G0,w{Gb}{b=1,...,25}, as shown in Fig. 5. Next, we learn an agent behavior model interactive with the world. Behavior Representation. We represent the behavior of an agent by its body pose in the scene space R6BT over time horizon = 5.6s. We design hierarchical model as shown in Fig. 2, where the body motion is conditioned on path R3T , which is further conditioned on the goal R3. Such decomposition makes it easier to learn individual components compared to learning joint model, as shown in Tab. 4 (a). Goal Generation. We represent multi-modal distribution of goals R3 by its score function s(Z, σ) R3 (Ho et al., 2020; Song et al., 2020). The score function is implemented as an MLP, s(Z; σ) = MLPθZ(Z, σ), trained by predicting the amount of noise ϵ added to the clean goal, given the corrupted goal + ϵ: (7) arg min θZ EZEσq(σ)EϵN (0,σ2I) MLPθZ (Z + ϵ; σ) ϵ2 2 . (8) Trajectory Generation. To generate path conditioned on goals, we represent its score function as s(P; σ) = ControlUNetθP(P, Z, σ), where the Control UNet contains two standard UNets with the same architecture (Zhang et al., 2023b; Xie et al., 2023), one taking (P, σ) as input to perform unconditional generation, another taking (9) 5 Figure 2: Pipeline for behavior generation. We encode egocentric information into perception code ω, conditioned on which we generate fully body motion in hierarchical fashion. We start by generating goals Z, then paths and finally body poses G. Each node is represented by the gradient of its log distribution, trained with denoising objectives (Eq. 8). Given G, the full body motion of an agent can be computed via blend skinning (Eq. 3). (Z, σ) as inputs to inject goal conditions densely into the neural network blocks of the first one. We apply the same architecture to generate body poses conditioned on paths, s(G; σ) = ControlUNetθG(G, P, σ). (10) Compared to concatenating the goal condition to the noise latent, this encourages close alignment between the input goal and the path (Xie et al., 2023). Ego-Perception of the World. To generate plausible interactive behaviors, we encode the world egocentrically perceived by the agent, and use it to condition the behavior generation. The egoperception code ω contains scene code ωs, an observer code ωo, and past code ωp, as detailed later. The ego-perception code is concatenated to the noise value σ and passed to the denoising networks. Transforming the world to the egocentric coordinates avoids over-fitting to specific locations of the scene (Tab. 4-(b)). We find that specific behavior can be learned and generalized to novel situations even when seen once. Although theres only one data point where the cat jumps off the dining table, our method can generate diverse motion of cat jumping off the table while landing at different locations (to the left, middle, and right of the table). Please see Fig. 11 for the corresponding visual. Scene, Observer, and Past Encoding. To encode the scene, we extract latent representation from local feature volume around the agent, where the volume is queried from the 3D feature volume by transforming the sampled ego-coordinates Xa using the agent-to-world transformation at time t, ωs = ResNet3Dθψ (Ψs(Xw)), Xw = (G0,w )Xa. (11) where ResNet3Dθϕ is 3D ConvNet with residual connections, and ωs R64. To encode the observers motion in the past = 0.8s seconds, we transform observers trajectories to the ego-coordinate, ξa = (G0,w ωo = MLPθo(ξa), (12) where ωo R64 represents the observer perceived by the agent. Accounting for the external factors from the world enables interactive behavior generation, where the motion of an agent follows the environment constraints and is influenced by the trajectory of the observer, as shown in Fig. 4. We additionally encode the root and body motion of the agent in the past seconds, ωp = MLPθp (G{0,...,B},a), G{0,...,B},a = (G0,w )1G{0,...,B},w. )1ξw, (13) By conditioning on the past motion, we can generate long sequences by chaining individual ones."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Dataset. We collect dataset that emphasizes interactions of an agent with the environment and the observer. As shown in Tab. 2, it contains RGBD iPhone video collections of 4 agents in 3 different scenes, where human and cat share the same scene. The dataset is curated to contain diverse motion of agents, including walking, lying down, eating, as well as diverse interaction patterns with the environment, including following the camera, sitting on coach, etc. 6 Figure 3: Comparison on multi-video scene reconstruction. We show birds-eye-view rendering of the reconstructed scene using the bunny dataset. Compared to TotalRecon that does not register multiple videos, ATS produces higher-quality scene reconstruction. Neural localizer (NL) and featuremetric losses (FBA) are shown important for camera registration. Scene annealing is important for reconstructing complete scene from partial video captures. Table 1: Evaluation of Camera Registration. Table 2: Dataset used in ATS. Method Rotation Error () Translation Error (m) Videos Length Unique Days / Span Ours w/o Neural Localizer w/o Featuremetric BA Multi-video TotalRecon 6.35 37.59 22.47 59. 0.41 0.83 1.30 0.68 Cat Human Dog Bunny 23 5 3 2 25m 39s 9m 27s 7m 13s 1m 48s 9 / 37 days 2 / 4 days 1 / 1 day 1 / 1 day 4. 4D RECONSTRUCTION OF AGENT & ENVIRONMENT Implementation Details. We take video collection of the same agent as input, and build 4D reconstruction of the agent, the scene, and the observer. We extract frames from the videos at 10 FPS, and use off-the-shelf models to produce augmented image measurements, including object segmentation (Yang et al., 2023b), optical flow (Yang & Ramanan, 2019), DINOv2 features (Oquab et al., 2023). We use AdamW to first optimize the environment with feature-metric loss for 30k iterations, and then jointly optimize the environment and agent for another 30k iterations with all losses in Eq. 6. Optimization takes roughly 24 hours. 8 A100 GPUs are used to optimize 23 videos of the cat data, and 1 A100 GPU is used in 2-3 video setup (for dog, bunny, and human). Results of Camera Registration. We evaluate camera registration using GT cameras estimated from annotated 2D correspondences. visual of the annotated correspondence and 3D alignment can be found in Fig. 12. We report camera translation and rotation errors in Tab. 1. We observe that removing neural localization (Eq. 4) produces significantly larger localization error (e.g., Rotation error: 6.35 vs 37.56). Removing feature-metric bundle adjustment (Eq. 5) also increases the error (e.g., Rotation error: 6.35 vs 22.47). Our method outperforms multi-video TotalRecon by large margin due to the above innovations. visual comparison on scene registration is shown in Fig. 3. Without the ability to register multiple videos, TotalRecon produces protruded and misaligned structures (as pointed by the red arrow). In contrast, our method reconstructs single coherent scene. With featuremetric alignment (FBA) alone but without good camera initialization from neural localization (NL), our method produces inaccurate reconstruction due to inaccurate global alignment in cameras poses. Removing FBA while keeping NL, the method fails to accurately localize the cameras and produces noisy scene structures. Finally, removing scene annealing procures lower quality reconstruction due to the partial capture. Results of 4D Reconstruction. We evaluate the accuracy of 4D reconstruction using synchronized videos captured with two moving iPhone cameras looking from opposite views. The results can be found in Tab. 3. We compute the GT relative camera pose between the two cameras from 2D correspondence annotations. One of the synchronized videos is used for 4D reconstruction, and the other one is used as held-out test data. For evaluation, we render novel views from the held-out cameras and compute novel view depth accuracy DepthAcc (depth accuracy thresholded at 0.1m) for all pixels, agent, and scene, following TotalRecon. Our method outperforms both the multi-video and single-video versions of TotalRecon by large margin in terms of depth accuracy and LPIPS, due to the ability of leveraging multiple videos. Please see Fig. 7 for the corresponding visual. Table 3: Evaluation of 4D Reconstruction. SV: Single-video. MV: Multi-video. Method DepthAcc (all) DepthAcc (fg) DepthAcc (bg) LPIPS (all) LPIPS (fg) LPIPS (bg) Ours SV TotalRecon MV TotalRecon 0.708 0.533 0.099 0.695 0.685 0.647 0.703 0.518 0.053 0.613 0.641 0.634 0.609 0.619 0. 0.613 0.641 0.633 Table 4: End-to-end Evaluation of Interactive Behavior Prediction. We report results of predicting goal, path, orientation, and joint angles, using = 16 samples across = 12 trials. The metrics are minimum average displacement error (minADE) with standard deviations (σ). The lower the better and the best results are in bold. Method Goal (m) Path (m) Orientation (rad) Joint Angles (rad) Location prior (Ziebart et al., 2009) Gaussian (Kendall & Gal, 2017) ATS (Ours) (a) hier1-stage (Tevet et al., 2022) (b) egoworld (Rhinehart & Kitani, 2016) (c) w/o observer ωo (d) w/o scene ωs 0.6630.307 0.9420.081 0.4480.146 1.3220.071 1.1640.043 0.6470.148 0.7840.126 N.A. 0.440 0.002 0.234 0. 0.575 0.026 0.577 0.022 0.327 0.076 0.340 0.051 N.A. 1.099 0.003 0.550 0.112 0.879 0.041 0.873 0.027 0.620 0.092 0.678 0.081 N.A. 0.295 0.001 0.237 0.006 0.263 0.007 0.295 0.006 0.240 0.006 0.243 0.007 Table 5: Evaluation of Spatial Control. We evaluate goal-conditioned path generation and pathconditoned full body motion generation respectively. Method Path (m) Orientation (rad) Joint Angles (rad) Gaussian (Kendall & Gal, 2017) ATS (Ours) (a) egoworld (Rhinehart & Kitani, 2016) (b) control-unetcode 0.2060.002 0.1150.006 0.2090.002 0.146 0.005 0.3700.003 0.3310.004 0.4290.006 0.351 0.004 0.2320.001 0.2130.001 0.2500.002 0.220 0. Qualitative results of 4D reconstruction can be found in Fig. 5 and the supplementary webpage. visual comparison with TotalRecon (Single Video) is shown in Fig. 6, where we show that multiple videos helps improving the reconstruction quality on both the agent and the scene. 4.2 INTERACTIVE AGENT BEHAVIOR PREDICTION Dataset. We train agent-specific behavior models for cat, dog, bunny, and human using 4D reconstruction from their corresponding video collections. We use the cat dataset for quantitative evaluation, where the data are split into training set of 22 videos and test set of 1 video. Implementation Details. Our model consists of three diffusion models, for goal, path, and full body motion respectively. To train the behavior model, we slice the reconstructed trajectory in the training set into overlapping window of 6.4s, resulting in 12k data samples. We use AdamW to optimize the parameters of the scores functions {θZ, θP, θG} and the ego-perception encoders {θψ, θo, θp} for 120k steps with batch size 1024. Training takes 10 hours on single A100 GPU. Each diffusion model is trained with random dropout of the conditioning (Ho & Salimans, 2022). Metrics. The behavior of an agent can be evaluated along multiple axes, and we focus on goal, path, and body motion prediction. For goal prediction, we use minimum displacement error (minDE) (Chai et al., 2019). The evaluation asks the model to produce = 16 hypotheses, and minDE finds the one closest to the ground-truth to compute the distance. For path and body motion prediction, we use minimum average displacement error (minADE), which are similar to goal prediction, but additionally averages the distance over path and joint angles before taking the min. When evaluating path prediction and body motion prediction, the output is conditioned on the ground-truth goal and path respectively. Comparisons and Ablations. We compare to related methods in our setup and the quantitative results are shown in Tab. 4. To predict the goal of an agent, classic methods build statistical models 8 Figure 4: Analysis of conditioning signals. We show results of removing one conditioning signal at time. Removing observer conditioning and past trajectory conditioning makes the sampled goals more spread out (e.g., regions both in front of the agent and behind the agent); removing the environment conditioning introduces infeasible goals that penetrate the ground and the walls. of how likely an agent visits spatial location of the scene, referred to as location prior (Ziebart et al., 2009; Kitani et al., 2012). Given the extracted 3D trajectories of an agent in the egocentric coordinate, we build 3D preference map over 3D locations as histogram, which can be turned into probabilities and used to sample goals. Since it does not take into account of the scene and the observer, it fails to accurately predict the goal. We implement Gaussian baseline that represents the goal, path, and full body motion as Gaussians, by predicting both the mean and variance of Gaussian distributions (Kendall & Gal, 2017). It is trained on the same data and takes the same input as ATS. As result, the Gaussian baseline performs worse than ATS since Gaussian cannot represent multi-modal distributions of agent behaviors, resulting in mode averaging. We implement 1-stage model similar to MDM (Tevet et al., 2022) that directly denoises body motion without predicting goals and paths (Tab. 4 (a)). Our hierarchical model out-performs 1-stage by large margin. We posit hierarchical model makes it easier to learn individual modules. Finally, learning behavior in the world coordinates (Tab. 4 (b)), akin to ActionMap (Rhinehart & Kitani, 2016), performs worse for all metrics due to the over-fits to specific locations of the scene. Analysing Interactions. We analyse the agents interactions with the environment and the observer by removing the conditioning signals and study their influence on behavior prediction. In Fig. 4, we show that by gradually removing conditional signals, the generated goal samples become more spread out. In Tab. 4, we drop one of the conditioning signals at time, and find that dropping either the observer conditioning or the environment conditioning increases behavior prediction errors. Spatial Control. Besides generating behaviors conditioned on agents perception, we could also condition on user-provided spatial signals (e.g., goal and path) to steer the generated behavior. The results are reported in Tab. 5. We found that ATS performs better than Gaussians for behavior control due to its ability to represent complex distributions. Furthermore, egocentric representation produces better behavior generation results. Finally, replacing control-unet architecture by concatenating spatial control with perception codes produces worse alignment (e.g., Path error: 0.115 vs 0.146)."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We have presented method for learning interactive behavior of agents grounded in 3D environments. Given multiple casually-captured video recordings, we build persistent 4D reconstructions including the agent, the environment, and the observer. Such data collected over long time period allows us to learn behavior model of the agent that is reactive to the observer and respects the environment constraints. We validate our design choices on casual video collections, and show better results than prior work for 4D reconstruction and interactive behavior prediction."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This project was funded in part by NSF:CNS-2235013 and IARPA DOI/IBC No. 140D0423C0035 Figure 5: Results of 4D reconstruction. Top: reference images and renderings. Background color represents correspondence. Colored blobs on the cat represent = 25 bones (e.g., head is represented by the yellow blob). The magenta colored lines represents reconstructed trajectories of each blob in the world space. Bottom: Birds eye view of the reconstructed scene and agent trajectories, registered to the same scene coordinate. Each colored line represents unique video sequence where boxes and spheres indicate the starting and the end location."
        },
        {
            "title": "REFERENCES",
            "content": "Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In CVPR, pp. 961971, 2016. 3 Eric Brachmann and Carsten Rother. NeuralGuided RANSAC: Learning where to sample model hypotheses. In ICCV, 2019. 4 Eric Brachmann, Tommaso Cavallari, and Victor Adrian Prisacariu. Accelerated coordinate encoding: Learning to relocalize in minutes using rgb and poses. In CVPR, 2023. 4 Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, and Jitendra Malik. Long-term human motion prediction with scene context. In ECCV, pp. 387404. Springer, 2020. 3 Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. arXiv preprint arXiv:1910.05449, 2019. 8 Chiho Choi, Srikanth Malla, Abhishek Patil, and Joon Hee Choi. Drogon: trajectory prediction model based on intention-conditioned behavior reasoning. In CoRL, pp. 4963. PMLR, 2021. 3 Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In CVPR, pp. 2179521806, 2024. 4 Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In ICCV, pp. 97109719, 2021. 1, 3 Levi Fussell, Kevin Bergamin, and Daniel Holden. Supertrack: Motion tracking for physically simulated characters using supervised learning. ACM Transactions on Graphics (TOG), 40(6): 113, 2021. Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. NeurIPS, 35:3376833780, 2022. 1 Harshayu Girase, Haiming Gang, Srikanth Malla, Jiachen Li, Akira Kanehara, Karttikeya Mangalam, and Chiho Choi. Loki: Long term and key intentions for trajectory prediction. In ICCV, pp. 98039812, 2021. 3 Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa*, and Jitendra Malik*. Humans in 4D: Reconstructing and tracking humans with transformers. In ICCV, 2023. 3 Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael Black. Stochastic scene-aware motion prediction. In ICCV, pp. 1137411384, 2021. 1, 3 Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. Synthesizing physical character-scene interactions. In SIGGRAPH 2023 Conference Proceedings, pp. 19, 2023. 3 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770778, 2016. 4 Dirk Helbing and Peter Molnar. Social force model for pedestrian dynamics. Physical review E, 51 (5):4282, 1995. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 8, 16 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33: 68406851, 2020. 5 Du Huynh. Metrics for 3d rotations: Comparison and analysis. Journal of Mathematical Imaging and Vision, 35:155164, 2009. 11 Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir Anguelov, et al. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In CVPR, pp. 9644 9653, 2023. 3 Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe, Iain Matthews, et al. Panoptic studio: massively multiview system for social interaction capture. TPAMI, 41(1):190204, 2017. 1 Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Guided motion diffusion for controllable human motion synthesis. In ICCV, pp. 21512162, 2023. 3, 16 Ladislav Kavan, Steven Collins, Jiˇrí Žára, and Carol OSullivan. Skinning with dual quaternions. In Proceedings of the 2007 symposium on Interactive 3D graphics and games, pp. 3946, 2007. 4 Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In NIPS, 2017. 8, 9 Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):114, 2023. 16 Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, and Hanbyul Joo. Parahome: Parameterizing everyday home activities towards 3d generative modeling of human-object interactions. arXiv preprint arXiv:2401.10232, 2024. 1, 3 Kris Kitani, Brian Ziebart, James Andrew Bagnell, and Martial Hebert. Activity forecasting. In ECCV, pp. 201214. Springer, 2012. 3, 9 Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. Advances in Neural Information Processing Systems, 35:2331123330, 2022. Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black. Vibe: Video inference for human body pose and shape estimation. In CVPR, June 2020. 3 Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal. Pace: Human and camera motion estimation from in-the-wild videos. arXiv preprint arXiv:2310.13768, 2023. 3 Jiye Lee and Hanbyul Joo. Locomotion-action-manipulation: Synthesizing human-scene interactions in complex 3d environments. In ICCV, 2023. 3 Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto MartínMartín, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, et al. Behavior-1k: human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. arXiv preprint arXiv:2403.09227, 2024. Hung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. Character controllers using motion vaes. ACM Transactions on Graphics (TOG), 39(4):401, 2020. 3 Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multi-person linear model. SIGGRAPH Asia, 2015. 3 Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis. 3DV, 2024. Haimin Luo, Teng Xu, Yuheng Jiang, Chenglin Zhou, Qiwei Qiu, Yingliang Zhang, Wei Yang, Lan Xu, and Jingyi Yu. Artemis: articulated neural pets with appearance and motion synthesis. ACM Transactions on Graphics (TOG), 41(4):119, 2022. 3 Wei-Chiu Ma, De-An Huang, Namhoon Lee, and Kris Kitani. Forecasting interactive dynamics of pedestrians with fictitious play. In CVPR, pp. 774782, 2017. 3 Thalmann Magnenat, Richard Laperrière, and Daniel Thalmann. Joint-dependent local deformations for hand animation and object grasping. In Proceedings of Graphics Interface88, pp. 2633. Canadian Inf. Process. Soc, 1988. 4 12 Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In ICCV, pp. 54425451, 2019. 1, 3 Karttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Malik. From goals, waypoints & paths to long term human trajectory forecasting. In ICCV, pp. 1523315242, 2021. 3 Willi Menapace, Aliaksandr Siarohin, Stéphane Lathuilière, Panos Achlioptas, Vladislav Golyanik, Sergey Tulyakov, and Elisa Ricci. Promptable game models: Text-guided game simulation via masked diffusion models. ACM Transactions on Graphics, 43(2):116, 2024. 3 Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 4 Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In CVPR, pp. 1145311464, 2021. 4 Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 2, 4, 5, 7 Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pp. 122, 2023. 1 Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021. 1 Georgios Pavlakos, Ethan Weber, Matthew Tancik, and Angjoo Kanazawa. The one where they reconstructed 3d humans and environments in tv shows. In ECCV, pp. 732749. Springer, 2022. 3 Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In ICCV, pp. 1506115073, 2023. Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander Clegg, Michal Hlavac, So Yeon Min, et al. Habitat 3.0: co-habitat for humans, avatars, and robots. In ICLR, 2023. 1 Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, and Or Litany. Trace and pace: Controllable pedestrian animation via guided trajectory diffusion. In CVPR, pp. 1375613766, 2023. 3 Nicholas Rhinehart and Kris Kitani. Learning action maps of large environments via first-person vision. In CVPR, pp. 580588, 2016. 8, 9 Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction conditioned on goals in visual multi-agent settings. In ICCV, pp. 28212830, 2019. Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael Black. Scanimate: Weakly supervised learning of skinned clothed avatar networks. In CVPR, pp. 28862897, 2021. 4 Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamicallyfeasible trajectory forecasting with heterogeneous data. In ECCV, pp. 683700. Springer, 2020. 3 Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, pp. 1271612725, 2019. 4 Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled Refaat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In ICCV, pp. 85798590, 2023. 13 Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit Bermano. Human motion diffusion as generative prior. arXiv preprint arXiv:2303.01418, 2023. 3 Chonghyuk Song, Gengshan Yang, Kangle Deng, Jun-Yan Zhu, and Deva Ramanan. Total-recon: Deformable scene reconstruction for embodied view synthesis. In ICCV, 2023. 1, 2, 3, 5, 16, 18 Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 5 Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In CoRL, pp. 477490, 2022. Sebastian Starke, Ian Mason, and Taku Komura. Deepphase: Periodic autoencoders for learning motion phase manifolds. ACM Transactions on Graphics (TOG), 41(4):113, 2022. 3 Tao Sun, Yan Hao, Shengyu Huang, Silvio Savarese, Konrad Schindler, Marc Pollefeys, and Iro Armeni. Nothing stands still: spatiotemporal benchmark on 3d point cloud registration under large geometric and temporal change. arXiv preprint arXiv:2311.09346, 2023. 4 Richard Szeliski and Sing Bing Kang. Shape ambiguities in structure from motion. TPAMI, 19(5): 506512, 1997. 3 Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. 3, 8, 9, 16 Jur Van Den Berg, Stephen Guy, Ming Lin, and Dinesh Manocha. Reciprocal n-body collision avoidance. In Robotics Research: The 14th International Symposium ISRR, pp. 319. Springer, 2011. 3 Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. arXiv preprint arXiv:2312.14132, 2023. 5 Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. arXiv preprint arXiv:2312.02981, 2023. 20 Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Dove: Learning deformable 3d objects by watching videos. arXiv preprint arXiv:2107.10844, 2021. 3 Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. arXiv preprint arXiv:2310.08580, 2023. 3, 5, 6 Gengshan Yang and Deva Ramanan. Volumetric correspondence networks for optical flow. In NeurIPS, 2019. 7 Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building animatable 3d neural models from many casual videos. In CVPR, 2022. 3, 4 Gengshan Yang, Jeff Tan, Alex Lyons, Neehar Peri, and Deva Ramanan. Lab4d - framework for in-the-wild 4D reconstruction from monocular videos, June 2023a. URL https://github. com/lab4d-org/lab4d. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos, 2023b. 7 Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In CVPR, pp. 2122221232, 2023. 3 Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusion-aware human mesh recovery with dynamic cameras. In CVPR, pp. 1103811049, 2022. 14 Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In ICCV, pp. 1601016021, 2023. 20 Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, and Kayvon Fatahalian. Learning physically simulated tennis skills from broadcast videos. ACM Transactions on Graphics (TOG), 42(4):114, 2023a. 3 He Zhang, Sebastian Starke, Taku Komura, and Jun Saito. Mode-adaptive neural networks for quadruped motion control. ACM Transactions on Graphics (TOG), 37(4):111, 2018. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023b. 5 Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and Siyu Tang. Synthesizing diverse human motions in 3d indoor scenes. arXiv preprint arXiv:2305.12411, 2023. 3 Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. In ICRA, pp. 35603566. IEEE, 2023. 3 Brian Ziebart, Andrew Maas, Andrew Bagnell, Anind Dey, et al. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pp. 14331438. Chicago, IL, USA, 2008. 3 Brian Ziebart, Nathan Ratliff, Garratt Gallagher, Christoph Mertz, Kevin Peterson, Andrew Bagnell, Martial Hebert, Anind Dey, and Siddhartha Srinivasa. Planning-based prediction for pedestrians. In IROS, pp. 39313936. IEEE, 2009. 3, 8,"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 REFINEMENT WITH 3D GAUSSIANS On the project webpage, we show results of rendering the generated interactions with 3D Gaussians (Kerbl et al., 2023). We explain how this is achieved below. Representation. Neural implicit representations are suitable for coarse optimization, but can be difficult to converge and slow to render. Therefore, we introduce refinement procedure that replaces the canonical shape model with 3D Gaussians while keeping the motion model as is. We use 20k Gaussians for the agent and 200k Gaussians for the scene, each parameterized by 14 values, including its opacity, RGB color, center location, orientation, and axis-aligned scales. To render an image, we warp the agent Gaussians forward from canonical space to time (Eq. 3), compose them with the scene Gaussians, and call the differentiable Gaussian rasterizer. Optimization. Gaussians are initialized with points on mesh extracted from Eq. (1-2) and assigned isotropic scales. To initialize the color of each Gaussian, we query the canonical color MLP at its center. We update both the canonical 3D Gaussian parameters and the motion fields by minimizing min T,D (cid:88) It RI (t; T, D)2 2 + Lreg(T, D), (14) where the regularization term includes flow loss, depth loss and silhouette loss on the agent. A.2 DETAILS ON MODEL AND DATA Table of Notation. table of notation used in the paper can be found in Tab. 6. Summary of I/O. summary of inputs and outputs of the method is shown in Tab. 7 Data Collection. We collect RGBD videos using an iPhone, similar to TotalRecon (Song et al., 2023). To train the neural localizer, we use Polycam to take the walkthrough video and extract textured mesh. For behavior capture, we use Record3D App to record videos and extract color images and depth images. Diffusion Model Architecture. The score function of the goal is implemented as 6-layer MLP with hidden size 128. The the score functions of the paths and body motions are implemented as 1D UNets taken from GMD (Karunratanakul et al., 2023). The sampling frequency is set to be 0.1s, resulting sequence length of 56. The environment encoder is implemented as 6-layer 3D ConvNet with kernel size 3 and channel dimension 128. The observer encoder and history encoder are implemented as 3-layer MLP with hidden size 128. Diffusion Model Training and Testing. We use linear noise schedule at training time and 50 denoising steps. We train all the diffusion models (goal, path and pose) with classifier-free guidance (Ho & Salimans, 2022; Tevet et al., 2022) that randomly sets conditioning signals to zeros = randomly. This allows us to control the trade-off between interactive behavior and unconditional behavior generation, as shown in Fig. 10. At test time, each goal denoising step takes 2ms and each path/body denoising step takes 9ms on an A100 GPU. A.3 ADDITIONAL RESULTS Comparison to TotalRecon. In the main paper, we compare to TotalRecon on scene reconstruction by providing it multiple videos. Here, we include additional comparison in their the original single video setup. We find that TotalRecon fails to build good agent model, or complete scene model given limited observations, while our method can leverage multiple videos as inputs to build better agent and scene model. The results are shown in Fig. 6. Visual Ablation on Scene Awareness. We show final camera and agent registration to the canonical scene in Fig. 9. The registered 3D trajectories provides statistics of agents and users preference over the environment."
        },
        {
            "title": "Description",
            "content": "Table 6: Table of Notation. Ni Ii ψi Ti R3 R3T R6BT ωs R64 ωo R64 ωp R64 βi R128 ξt SE(3) G0 SE(3) Gb SE(3) RB fθ Global Notations The number of bones of an agent. By defatult = 25. The number of videos. The number of image frames extracted from video i. The sequence of color images {I1, . . . , INi} extracted from video i. The sequence of DINOv2 feature images {ψ1, . . . , ψNi } extracted from video i. The length of video i. The time horizon of behavior diffusion. By default = 5.6s. The time horizon of past conditioning. By default = 0.8s Goal of the agent, defined as the location at the end of . Path of the agent, defined as the root body trajectory over . Pose of the agent, defined as the 6DoF rigid motion of bones over . Scene code, representing the scene perceived by the agent. Observer code, representing the observer perceived by the agent. Past code, representing the history of events happened to the agent. Learnable Parameters of 4D Reconstruction Canonical NeRFs, including scene MLP and an agent MLP. Per-video code that allows NeRFs to represent variations across videos. Time-varying parameters, including {ξ, G, W}. The camera pose that transforms the scene to the camera coordinates at t. The camera pose that transforms the canonical agent to the camera coordinates at t. The transformation that moves bone from its rest state to time state. Skinning weights of point, defined as the probability of belonging to bones. PoseNet that takes DINOv2 feature image as input and produces camera pose. MLPθZ ControlUNetθP ControlUNetθG ResNet3Dθψ MLPθo MLPθp Learnable Parameters of Behavior Generation Goal MLP that represent the score function of goal distributions. Path UNet that represents the score function of path distributions. Pose UNet that represents the score function of pose distributions. Scene perception network that produces ωs from 3D feature grids ψ. Observer MLP that produces ωo from observers past trajectory in . Past MLP that produces ωp from agents past trajectory in . Table 7: Summary of inputs and outputs at different stages of the method. Stage Overall Description Input: walk-through video of the scene and videos with agent interactions. Output: An interactive behavior generator of the agent. Localizer Training Neural Localization Input: 3D reconstruction of the environment and the agent. Output: Neural localizer fθ. Input: Neural localizer fθ and the agent interaction videos. Output: Camera poses for each video frame. 4D Reconstruction Input: collection of videos and their corresponding camera poses. Output: Scene feature volume Ψ, motion of the agent and observer ξ. Behavior Learning Input: Scene feature volume Ψ, motion of the agent and observer ξ. Output: An interactive behavior generator of the agent. 17 Figure 6: Qualitative comparison with TotalRecon (Song et al., 2023) on 4D reconstruction. Top: reconstruction of the agent at at specific frame. Total-recon produces shapes with missing limbs and bone transformations that are misaligned with the shape, while our method produces complete shapes and good alignment. Bottom: reconstruction of the environment. TotalRecon produces distorted and incomplete geometry (due to lack of observations from single video), while our method produces an accurate and complete environment reconstruction. Figure 7: Qualitative comparison on 4D reconstruction (Tab. 3). We compare with TotalRecon on 4D reconstruction quality. We show novel views rendered with held-out camera that looks from the opposite side. ATS is able to leverage multiple videos captured at different times to reconstruct the wall (blue box) and the tripod stand (red box) even they are not visible in the input views. Multi-video TotalRecon produces blurry RGB and depth due to bad camera registration. The original TotalRecon takes single video as input and therefore fails to reconstruct the regions (the tripod and the wall) that are not visible in the input video. Histogram of Agent / Observer Visitation. We show final camera and agent registration to the canonical scene in Fig. 8. The registered 3D trajectories provides statistics of agents and users preference over the environment. 18 Figure 8: Visual ablation on scene awareness. We demonstrate the effect of the scene code ωs through goal-conditioned path generation (birds-eye-view, blue spheregoal; gradient colorgenerated path; gray blockslocations that have been visited in the training data). Conditioned on scene, the generated path abide by the scene geometry, while removing the scene code, the generated paths go through the wall in between two empty spaces. Figure 9: Given the 3D trajectories of the agent and the user accumulated over time (top), one could compute their preference represented by 3D heatmaps (bottom). Note the high agent preference over table and sofa. A.4 LIMITATIONS AND FUTURE WORKS Environment Reconstruction. To build complete reconstruction of the environment, we register multiple videos to shared canonical space. However, the transient structures (e.g., cushion that can be moved over time) may not be reconstructed well due to lack of observations. We notice displacement of chairs and appearance of new furniture in our capture data. Our method is robust to these in terms of camera localization (Tab. 1 and Fig. 13). However, 3D reconstruction of these transient components is challenging. As shown in Fig 13, our method fails to reconstruct notable layout changes when they are only observed in few views, e.g., the cushion and the large boxes Figure 10: Interactivity of the agent. By changing the classifier-free guidance scale s, we can find trade-off between interactive behavior and unconditional behavior. We demonstrate the control over interactivity by goal-conditioned path generation (birds-eye-view, blue spheregoal; gradient colorgenerated path). With higher classifier-free guidance scale s, the model is controlled more by the conditional generator, and therefore exhibits higher interactivity. = 0 corresponds to fully unconditional generation. Figure 11: Generalization ability of the behavior model. Thanks to the ego-centric encoding design (Eq. 12), specific behavior can be learned and generalized to novel situations even it was seen once. Although theres only one data point where the cat jumps off the dining table, our method can generate diverse motion of cat jumping off the table while landing at different locations (to the left, middle, and right of the table) as shown in the visual. (left) and the box (right). We leave this as future work. Leveraging generative image prior to in-paint the missing regions is promising direction to tackle this problem (Wu et al., 2023). Scaling-up. We demonstrate our approach on four types of agents with different morphology living in different environments. For the cat, we use 23 video clips over span of month. This isnt large-scale but we believe this is an important step to go beyond single video. In terms of robustness, we showed meaningful step towards scaling up 4D reconstruction by neural initialization (Eq. 6). The major difficulty towards large-scale deployment is the cost and robustness of 4D reconstruction using test-time optimization. Multi-agent Interactions. ATS only handles interactions between the agent and the observer. Interactions with other agents in the scene are out of scope, as it requires data containing more than one agent. Solving re-identification and multi-object tracking in 4D reconstruction will enable introducing multiple agents. We leave learning multi-agent behavior from videos as future work. Complex Scene Interactions. Our approach treat the background as rigid component without accounting for movable and articulated scene structures, such as doors and drawers. To reconstruct complex interactions with the environment, one approach is to extend the scene representation to be hierarchical (with kinematic tree), such that it consists of articulated models of interactable objects. To generate plausible interactions between the agent and the scene (e.g., opening door), one could extend the agent representation to include both the agent and the articulated objects (e.g., door). Physical Interactions. Our method reconstructs and generates the kinematics of an agent, which may produce physically-implausible results (e.g., penetration with the ground and foot sliding). One promising way to deal with this problem is to add physics constraints to the reconstruction and motion generation (Yuan et al., 2023). 20 Long-term Behavior. The current ATS model is trained with time-horizon of = 6.4 seconds. We observe that the model only learns mid-level behaviors of an agent (e.g., trying to move to destination; staying at location; walking around). We hope incorporating memory module and training with longer time horizon will enable learning higher-level behaviors of an agent. Figure 12: GT correspondence and 3D alignment. Left: Annotated 2D correspondence between the canonical scene (top) and the input image (bottom). Right: we visualize the GT camera registration by transforming the input frame 3D points (blue, back-projected from depth) to the canonical frame (red). The points align visually. to layout Figure 13: Robustness changes. We find our camera localization to be robust to layout changes, e.g., the cushion and the large boxes (left) and the box (right). However, it fails to reconstruct layout changes, especially when they are only observed in few views. A.5 SOCIAL IMPACT Our method is able to learn interactive behavior from videos, which could help build simulators for autonomous driving, gaming, and movie applications. It is also capable of building personalized behavior models from casually collected video data, which can benefit users who do not have access to motion capture studio. On the negative side, the behavior generation model could be used as deepfake and poses threats to users privacy and social security."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Codec Avatar Labs, Meta",
        "UC Berkeley"
    ]
}