{
    "paper_title": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models",
    "authors": [
        "Jiwoo Chung",
        "Sangeek Hyun",
        "MinKyu Lee",
        "Byeongju Han",
        "Geonho Cha",
        "Dongyoon Wee",
        "Youngjun Hong",
        "Jae-Pil Heo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs."
        },
        {
            "title": "Start",
            "content": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models Jiwoo Chung1, Sangeek Hyun1 MinKyu Lee1 Byeongju Han2 Jae-Pil Heo1,* Geonho Cha2 Dongyoon Wee2 Youngjun Hong2,* 1Sungkyunkwan University 2NAVER Cloud 6 2 0 2 2 2 ] . [ 1 3 9 9 8 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models are strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feaThis ture differences that entangle content and noise. design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), training-free cache schedule that bases reuse decisions on spectrally aligned representation. Through theoretical and empirical analysis, we derive Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-ofthe-art latency-quality trade-offs. Codes are available at github.com/jiwoogit/SeaCache. 1. Introduction Recent diffusion [14, 48, 5658] and rectified-flow (RF) [16, 38] models produce high-quality images and videos through iterative denoising. Despite this progress, sampling still requires tens to hundreds of steps, which turns user-facing applications into latency bound. common remedy is to reduce the step count or the per-step cost through distillation [15, 26, 45, 5052], quantization [8, 54, 70, 76], or efficient attention [67, 68, 71, 74, 75]. These approaches are effective but introduce added training overhead and dependence on task or data-specific tuning. This work was done during an internship at NAVER Cloud. * Co-corresponding authors. Figure 1. Conceptual illustration and motivation of the proposed caching scheme (SeaCache) compared with previous caching schemes. The lower panel shows denoising trajectory of cat image where coarse low-frequency structure appears at early steps and fine high-frequency details emerge at later steps, illustrating the spectral evolution of iterative generative models. SeaCache applies Spectral-Evolution-Aware (SEA) Filter to raw diffusion features so that the distance measure better captures timestepaware spectral residuals between timesteps. complementary direction exploits redundancy between consecutive steps via caching. Caching reduces the number of forward passes by reusing intermediate features from previous timesteps. Early work adopts static schedules [31, 36, 79] that cache features at fixed intervals along the trajectory, which yields predictable speedups. More recent methods introduce dynamic schedules [1, 6, 33] that decide when to reuse based on the distance between current and cached features, thereby reducing the error introduced by caching. These approaches focus on where to cache, for example which layers or blocks, while the error itself is still measured in the raw feature space. However, these approaches measure errors directly in the raw feature space and overlook spectral evolution, key prior underlying the denoising process. Independent of caching, prior studies [17, 22, 30, 73] have provided clear evidence that diffusion models exhibit spectral evolution, where ujkearly timesteps establish low-frequency structure and later timesteps refine high-frequency detail, as also illustrated in the lower panel of Fig. 1. From this viewpoint, spectral evolution at given timestep can be interpreted as 1 change in the signal-to-noise ratio. We use the term signal for the content-carrying component that is aligned with the clean sample and mainly lies in low frequencies, and noise for the residual component that is concentrated in high frequencies and reflects stochastic variation. In this paper, we incorporate this spectral evolution, or equivalently the evolution of the signal-to-noise ratio, into cache scheduling. Rather than treating all spectral components equally, we design cache metric that focuses on the signal component while downweighting the noise component. By grounding reuse decisions on discrepancies in the synthesized content, the resulting metric becomes less sensitive to high-frequency noise and encourages cache gating to respond to meaningful signal alignment rather than stochastic variation. To validate this idea, we conduct an oracle experiment that compares cache schedules derived from raw feature distances with those derived from distances in signalemphasized space. In standard caching schemes, the decision to skip or compute is based on the distance between input features at consecutive timesteps. In our oracle analysis, we instead compare consecutive output features, thereby removing input-to-output approximation error and isolating the effect of spectral filtering. Specifically, we compare two criteria: one th at measures distances after applying the SEA (Spectral-Evolution-Aware) filter, which downweights the noise component (Sec. 4.1), and another that uses unfiltered raw outputs, as shown in Fig. 2. The filtered criterion yields cache decisions that more closely track the full-compute trajectory, as evidenced by consistently higher PSNR. This suggests that spectrum-aware scheduling better preserves the behavior of the original model. To this end, we propose Spectral-Evolution-Aware Cache (SeaCache), simple yet effective caching scheme that encodes the spectral prior of iterative denoising models through Spectral-Evolution-Aware (SEA) filter, as illustrated in Fig. 1. The SEA filter provides practical scheduling policy by allowing cache decisions to be driven by the signal component. Before measuring feature distances, SeaCache passes intermediate features through theoretically motivated, timestep-dependent filter that modulates the frequency response along the sampling trajectory. This operation acts as lightweight reweighting that amplifies the content-relevant signal while downweighting noisedominated components. SeaCache is plug-and-play: it requires no architectural modification or retraining, and can be attached to existing caching policies by inserting single filtering step before distance computation. The method is both network-agnostic and sampler-agnostic, enabling integration across diverse diffusion and rectified-flow models. In practice, SeaCache substantially reduces the number of forward passes while preserving the perceptual fidelity of the original outputs, (a) Latency-quality trade-off on FLUX. (b) Latency-quality trade-off on Wan2.1 1.3B. Figure 2. Latency-quality trade-off in oracle experiments. We compare cache decisions based on raw output differences and SEA-filtered output differences (Sec. 4.1) on FLUX [28, 29] and Wan2.1 1.3B [63]. The refresh ratio is the fraction of timesteps that run full denoiser evaluation instead of reusing cached features. For each criterion, PSNR is computed between the cached sample and the corresponding full timestep (no-cache) sample, averaged over each prompt set [23, 49]. At matched refresh ratios, the filtered criterion consistently achieves higher PSNR with respect to the full-compute trajectory, validating the effectiveness of spectrum-aware distance for cache scheduling. and it consistently improves the latency-quality trade-off over prior caching schemes across experiments. Our main contributions are threefold. We propose SeaCache, simple yet effective caching policy that bases reuse decisions on timestep-aligned spectral representation of the generative trajectory. We revisit prior caching strategies and show that raw feature metrics ignore spectral evolution, while our formulation bases cache decisions on content rather than noise. Extensive experiments on multiple visual generative models show that our method achieves better latencyquality trade-offs than prior caching baselines. 2. Related Work 2.1. Generative Model Acceleration Recent generative models [10, 14, 16, 38, 48, 5658] have advanced visual synthesis, but their multi-step denoising procedures make inference latency and computation primary bottleneck. Step reduction methods compress the sampling trajectory using improved solvers [39, 57, 78] and distillation-based samplers [40, 50, 59]. These approaches are effective but require additional training and often modify the original model. Another line of work reduces the cost of each step through quantization [20, 32, 54, 55], efficient attention [3, 12, 13, 47, 67, 68, 71, 74, 75], and token reduction [25, 55, 75]. These techniques lower FLOPs while preserving the sequential dependency of the sampler, but they typically demand extra resources and engineering effort. This limitation motivates caching-based acceleration, which exploits redundancy across successive timesteps to reuse intermediate features without additional training. 2.2. Caching-based Acceleration Caching-based acceleration reuses intermediate computations across adjacent timesteps without retraining. Early methods [31, 42, 66] achieve speedups by reusing features but are designed for U-Net architectures, which limits their applicability to transformer-based models. To address this limitation, later work [9, 34, 53] adapts caching to DiT architectures [31, 53] for image synthesis. For video, PAB [79] selects different timestep intervals for each attention block and achieves speedups. These methods rely on static schedules and cannot adapt to input diversity, so recent work adopts dynamic policies that respond to the generated signal [2, 24, 33, 37, 41, 43]. For example, AdaCache [24] accounts for motion complexity for accelerating video generation. TeaCache [33] and DiCache [6] estimate output changes from distances measured near the input features and assume that these distances provide reliable redundancy signal beIn our work, we measure retween adjacent-timesteps. dundancy in timestep-aligned spectral space that emphasizes content-carrying components. Unlike prior dynamic caching, SeaCache explicitly models spectral evolution through timestep-conditioned SEA filter motivated by linear-denoiser view, and applies gain normalization to enable stable distance measurements across timesteps. As result, SeaCache is the first caching policy that injects an explicit frequency prior into the reuse decision. Recent studies [35, 37, 81] explore reusing features differently across frequency bands. In contrast, we focus on when to reuse rather than how to utilize cached features. Leveraging the spectral evolution prior where lowfrequency structure emerges early while high-frequency details are refined later, we propose simple cache policy that plugs easily into existing caching baselines. 3. Preliminary 3.1. Denoising Generative Models Diffusion probabilistic models (DPMs) [21] and rectified flow (RF) models [38] generate samples by iteratively removing noise. Let denote clean image or video, and let an encoder map to latent x0. For images, we denote x0 RHW C, and for videos x0 RHW C, where H, W, F, and denote the height, width, number of frames, and channels of the latent representation, respectively."
        },
        {
            "title": "We adopt the standard forward noising model at discrete",
            "content": "solver steps {0, . . . , }: xt = atx0 + btε, ε (0, I), (1) where is the total number of steps and (at, bt) are deterαt mined by the noise schedule. For DPMs [21], at = and bt = 1 αt with αt [0, 1] given by the schedule. For RFs [38], the same linear mixture provides useful approximation with at = 1 αt and bt = αt, where αt = . Under this noise mixture model, DPMs are trained to predict the noise ε from the noised latent xt at timestep t. The corresponding training objective is LDPM = Ex0, t, ε, (cid:13)ε ϵθ(xt, t, y)(cid:13) (cid:2)(cid:13) 2 (cid:13) 2 (cid:3), (2) where is conditioning signal and ϵθ is denoising network that estimates the noise added to x0. Sampling proceeds in reverse, starting from xT ε and iteratively reconstructing x0. This iterative denoising process induces strong redundancy between outputs at adjacent timesteps, and cache-based acceleration exploits this redundancy by reusing intermediate predictions. 3.2. Timestep-Aware Dynamic Caching recent approach, TeaCache [33], quantifies change at step using the timestep-modulated input It = ϕ(xt, t), where ϕ injects timestep embedding into the input xt. This proxy is strongly correlated with the denoiser output Ot while remaining inexpensive to compute, and for brevity we refer to It as the input feature. The relative ℓ1 distance is then defined as = L1rel(It, It+1) = It It+11 It+11 + ξ , (3) with small constant ξ for numerical stability [6, 33, 42]. After computing the model output at step ta, the same output is reused for steps [ta, tb 1] until the accumulated change exceeds threshold δ. Let tb > ta be the smallest index that satisfies tb1 (cid:88) s=ta δ < tb(cid:88) s=ta s, (4) at which point refresh is triggered at tb and the accumulator is reset. Smaller δ leads to more frequent refreshes and higher fidelity, while larger δ increases speed at the risk of artifacts. We follow the accumulated-distance rule and keep the same refresh logic on the timestep-modulated feature at the pre-attention input of the first transformer block to maximize skipped computation, as in TeaCache. Accordingly, SeaCache injects spectral priors into by measuring change in frequency-aware filtered representation. 3 and Gnorm Figure 3. Overview of SeaCache. Given input features It and It+1, SeaCache first applies FFT, multiplies by the timestep-dependent SEA filters Gnorm t+1 , It+1) (Sec. 4.1). spectrum-aware dynamic caching module (Sec. 4.2) measures the relative distance (cid:101)t between consecutive filtered features, accumulates it over timesteps, and either reuses the cached output or refreshes the denoiser when the threshold δ is exceeded. The underlying diffusion model remains unchanged, so SeaCache acts as plug-and-play cache policy that replaces only the distance metric. t+1 , and then applies iFFT to obtain spectral-evolution-aware features P(Gnorm , It) and P(Gnorm 4. Method: SeaCache Prior analyses [3, 22, 30] and the lower panel of Fig. 1 reveal form of spectral evolution in diffusion models, where early steps build low-frequency structure and later steps refine high-frequency detail. Motivated by this behavior, we design spectrum-aware reuse metric that guides cache scheduling across timesteps. Our approach proceeds in three stages  (Fig. 3)  . First, in Sec. 4.1, we formalize the denoiser frequency response and derive timestepdependent filter that captures this evolution. Second, in Sec. 4.2, we introduce an input proxy whose filtered distance is closely related to the filtered output distance, which enables training-free, plug-and-play schedule. Finally, we replace the original metric with its spectrum-aware counterpart (cid:101)t while preserving the standard accumulated distance based refresh rule. 4.1. Spectral-Evolution-Aware Filter To design filter that reflects spectral evolution, we formalize how the effective frequency band changes across timesteps. Motivated by Spectral Diffusion [72], we adopt the timestep-dependent frequency response derived under the optimal linear denoiser. For notational simplicity, we describe single-channel 2D filter. In implementation, the filter is applied per-channel over the spatial (2D) axes for images and over the spatiotemporal (3D) axes for videos. We consider the linear minimum mean squared error (MMSE) estimator (cid:98)x0 = ht xt obtained by minimizing Jt(ht) = ht xt x02 2, where ht is linear denoising filter and denotes convolution (which corresponds to pointwise multiplication in the frequency domain). We denote by the optimal linear filter that minimizes Jt. Let Gt(f ) denote the frequency response of the optimal linear denoising filter at frequency , and let Sx(f ) denote the power spectrum of x0 at frequency . Under the linear mixture in Eq. 1, the optimal frequency response of takes (a) Optimal linear denoising filter Gt(f ) for different t. (b) Normalized SEA filter Gnorm (f ) for different t. Figure 4. Visualization of timestep-dependent denoising filters. (a) Optimal linear denoising responses Gt(f ) across timesteps, where early steps primarily pass low-frequencies and later steps gradually include higher frequencies, reflecting spectral evolution. (b) Corresponding normalized filters Gnorm (f ) with unit mean gain, which stabilize filtered feature energy across timesteps and are used as SEA filters for cache scheduling. Wiener-like form [65]: Gt(f ) = at Sx(f ) Sx(f ) + b2 a2 , (5) and although the linearity assumption on ht is restrictive, it still provides useful insight into spectral evolution. Assuming natural power law spectrum [7, 18, 61, 62] for Sx(f ), representative DPM responses Gt(f ) are shown in Fig. 4a. In the reverse diffusion process, decreases from to 0 while at increases from 0 to 1, gradually recovering high frequency detail in way that is consistent with spectral evolution. The resulting filters for DPMs and RF models exhibit nearly identical behavior, and for brevity we present the analysis in terms of the DPM in the main text. Full derivations are provided in the supplementary material. In this view, we refer to the low-frequency contentcarrying component aligned with the clean sample as the signal and to the high-frequency residual that primarily reflects stochastic variation as noise. We formulate the optimal response Gt(f ) and confirm spectral evolution under this model. We then use Gt(f ) to filter features in the frequency domain for constructing spectrum-aware representation that emphasizes the signal component while suppressing noise. Specifically, we define feature-level mapping Pt by applying the fast Fourier transform (FFT [46]), multiplying by the timestepdependent spectrum-aware filter Gt(f ), and returning to the original space via the inverse FFT (iFFT): P(Gt, It) = iFFT(cid:0)Gt(f ) FFT(It)(cid:1), (6) where indexes radial frequencies on the discrete Fourier grid and denotes element-wise multiplication with broadcasting across channels and spatial or spatiotemporal dimensions. This operator Pt(Gt, ) induces timestepdependent passband and defines the filtered feature space in which spectrum-aware cache distances are computed. Before filtering the features, the raw response Gt exhibits timestep-dependent gain, as in the varying radial averages in Fig. 4a. To ensure that distances are comparable across timesteps, we normalize this gain by enforcing constant mean over radial frequencies, yielding density-normalized response Gnorm (Fig. 4b). Specifically, let the discrete radial frequencies on the FFT grid be = {fℓ}L1 ℓ=0 , where is the number of radial bins, induced by the spatial resolution for images. We define νt = (cid:16) 1 (cid:88) fℓF (cid:17) Gt(fℓ) , Gnorm (f ) = νt Gt(f ), (7) where νt is average energy over radial bins such that Gnorm has constant mean gain over F. Using this density normalized filter, we empirically observe that distances computed after filtering better reflect denoising redundancy than their raw counterparts, and we use Gnorm for our SEA filter in the subsequent caching schedule. 4.2. Spectrum-Aware Dynamic Caching Prior caching methods [6, 34, 36, 42, 66] typically assume that differences between consecutive model outputs reflect redundancy relative to full-compute trajectory. Building on this assumption, they construct dynamic schedules by approximating output differences from input-side features such as intermediate layers or blocks. However, the oracle study in Sec. 1 and Fig. 2 show that this raw feature formulation is suboptimal. Cache decisions based on SEA-filtered outputs stay closer to the full compute trajectory than those based on raw outputs at the same refresh ratio. (a) Relative ℓ1 across the generation process on FLUX. (b) Relative ℓ1 across the generation process on Wan2.1 1.3B. Figure 5. Relative ℓ1 across the generation process. Stepwise relative ℓ1 distances between consecutive timesteps for different feature choices, averaged over ten samples for each model. Input denotes distances on the timestep-modulated input features It. Output is the last block outputs Ot. SEA(Input), SEA(Output) applies the SEA filter to the input and output features, respectively. Poly(Input) corresponds to the polynomial-fitted input distance which is designed to approximate output differences from input features. SEA-filtered inputs closely track SEA-filtered outputs across timesteps, whereas other inputs show weaker alignment. Directly using SEA-filtered outputs in the cache metric is not practical, since the output Ot is only available after full denoiser run and thus offers no speedup. We therefore seek an input-side proxy that matches the SEA-filtered output distance as closely as possible. Building on the input features It, introduced in Sec. 3.2, we compare several candidates: raw input It, raw output Ot, the polynomial fitted input used in TeaCache [33], and their SEA-filtered counterparts obtained by applying Pt(Gt, ) from Sec. 4.1. Fig. 5 reports the relative ℓ1 distance between consecutive timesteps for these feature choices, averaged over ten samples on FLUX and Wan2.1 1.3B. The SEA-filtered input distances Pt(Gt, It) closely follow the SEA-filtered output distances Pt(Gt, Ot) along the entire trajectory, while raw input and polynomial fitted input show weaker alignment, especially at early timesteps. Moreover, the SEA-filtered input distances are larger at early timesteps, which is consistent with the common practice of always recomputing early steps in many prior caching schemes [6, 36, 42, 43]. This empirical finding is desirable because the SEA filter suppresses stochastic noise while preserving content-carrying components, which makes adjacent-timestep features more stable and faithful proxies for output change. These results support SEA-filtered inputs as reliable, training-free proxy for adapting spectrum-aware redundancy. 5 Table 1. Quantitative comparison in FLUX.1-dev [28, 29]. Table 3. Quantitative comparison in HunyuanVideo [27]. Method Latency (s) TFLOPs PSNR LPIPS SSIM Method Latency (s) TFLOPs PSNR LPIPS SSIM Original (50 steps) Vanilla 25 steps Vanilla 15 steps TeaCache (δ=0.3) TaylorSeer (S=3) SeaCache (δ=0.3) -Dit ToCa TeaCache (δ=0.6) TaylorSeer (S=5) SeaCache (δ=0.6) 20.9 10.5 6. 11.4 9.8 9.4 15.5 15.9 7.1 7.5 6.4 2976 1487 892 1547 1191 1098 1984 1263 892 834 773 15.553 17. 20.762 22.783 26.285 17.403 18.398 17.214 19.972 21.332 0.409 0.305 0.211 0.163 0.106 0.336 0.324 0.348 0.236 0.226 0.668 0. 0.810 0.828 0.893 0.710 0.700 0.714 0.762 0.798 Table 2. Comparison of average rank on CycleReward [4]. Method (50%) Rank Method (30%) Rank TeaCache (δ=0.3) [33] TaylorSeer (S=3) [36] SeaCache (δ=0.3) 2.01 2.08 1.91 TeaCache (δ=0.6) [33] TaylorSeer (S=5) [36] SeaCache (δ=0.6) 2.07 1.98 1.96 In SeaCache, distance is therefore measured after density normalized filtering, and the per-step cache metric is defined as (cid:101)t = L1rel (cid:0)P(Gnorm , It), P(Gnorm t+1 , It+1)(cid:1). (8) The accumulated distance rule in Eq. (4) is kept unchanged. After refresh at ta, the cached output is reused for [ta, tb 1], and the next refresh occurs at the smallest tb whose accumulated distance exceeds the threshold δ. This yields spectral-evolution-aware, timestep-dependent gate that is training-free and architecture-agnostic, and depends only on the shared sampler schedule coefficients (at, bt). 5. Experiments 5.1. Experimental Settings Model configurations. We evaluate on three state-of-theart visual generative models. FLUX.1-dev [28, 29] is textto-image model. HunyuanVideo [27] and Wan2.1 [63] are text-to-video models. For Wan2.1, we use the 1.3B pretrained checkpoint. All models are sampled for 50 steps under their default configurations. For example, when using TaylorSeer [36], we follow its default settings and set the expansion order to 1 for FLUX.1-dev and to 2 for HunyuanVideo and Wan2.1. FLUX experiments run on NVIDIA Blackwell Pro 6000 GPUs, and HunyuanVideo and Wan2.1 are evaluated on NVIDIA A100 GPUs. Baseline configurations. TeaCache [33] is applied using the official implementation with default settings, and we adjust the distance threshold δ to control the cache ratio. TaylorSeer [36] is also used with the official code. For fair comparison, we explicitly refresh the first five timesteps for images and the first three timesteps for videos, and we adjust the stride to control the cache ratio. ToCa [80] and Original (50 steps) Vanilla 25 steps Vanilla 15 steps TeaCache (δ=0.12) TaylorSeer (S=2) SeaCache (δ=0.19) TeaCache (δ=0.2) TaylorSeer (S=3) SeaCache (δ=0.35) 182.6 93.7 56.8 14038 7019 4211 98.5 96.9 90.8 64.4 68.8 58.1 6994 7299 6747 4794 5053 19.97 17.49 23.40 24.14 32.39 20.42 20.42 26.46 0.263 0.371 0.133 0.152 0.047 0.172 0.242 0. 0.731 0.662 0.805 0.820 0.932 0.734 0.733 0.857 DiCache [6] are employed through their official implementations under default settings. For -DiT [9], we follow the reference implementation provided with TaylorSeer. Evaluation protocol. For all experiments, generated images and videos are stored as PNG and MP4 files, respectively. For text-to-image generation, we evaluate 200 DrawBench prompts [49] and generate 1024 1024 images. For text-to-video generation, we use 944 prompts from VBench [23] and generate 480p video with 65 frames per prompt. For each configuration, the full timestep output of the original model serves as the reference, and PSNR (computed on RGB values), LPIPS [77], and SSIM [64] are computed between each cached sample and its reference and then averaged over all samples. TFLOPs are measured with Calflops [69] and reported in tera operations. We further assess perceptual quality using CycleReward [4], state-ofthe-art image reward benchmark. The initial random seed is shared across our method and all baselines, and we consider two cache budgets, approximately 50% and 30%. 5.2. Quantitative Comparison Text-to-image generation. We compare SeaCache with existing caching methods on FLUX.1-dev [28, 29] in Tab. 1. At moderate budget (roughly 50% refresh ratio), TeaCache [33] and TaylorSeer [36] stay close to the 25 step baseline, while SeaCache further reduces latency and FLOPs and at the same time improves PSNR, LPIPS, and SSIM. This trend persists under stronger acceleration (roughly 30% refresh ratio). Baselines exhibit clear drops in reconstruction quality, whereas SeaCache achieves the fastest setting among caching methods and still attains the best metrics, yielding stronger latency-quality trade-off. We also assess perceptual quality using CycleReward [4] in Tab. 2. At both budgets ( 50% and 30%), SeaCache achieves the lowest average reward rank among TeaCache and TaylorSeer, showing better latency-quality trade-off for both reconstruction fidelity and preference. Text-to-video generation. in Tab. 3, SeaCache consistently achieves stronger latencyquality trade-off than the baselines. In the higher cache budget setting (upper block), SeaCache reduces latency and For HunyuanVideo [27], 6 Figure 6. Qualitative comparison of SeaCache and baselines on FLUX at refresh ratios of approximately 30% and 50%. Table 4. Quantitative comparison in Wan2.1 1.3B [63]. Method Latency (s) TFLOPs PSNR LPIPS SSIM Original (50 steps) 176.3 TeaCache (δ=0.09) TaylorSeer (S=2) SeaCache (δ=0.2) TeaCache (δ=0.15) TaylorSeer (S=3) SeaCache (δ=0.35) 86.6 93.1 83. 63.6 67.1 56.6 8214 4107 4189 3942 2957 2956 2793 20.84 16.15 26.60 18.88 14.18 21.78 0.171 0.336 0.075 0.245 0.455 0.170 0.721 0.543 0. 0.645 0.453 0.740 TFLOPs while improving all metrics. PSNR increases by roughly 8 dB over the strongest baseline, with lower LPIPS and higher SSIM. In the more aggressive setting (lower block), this trend remains. SeaCache runs faster than the baselines and still delivers clearly better overall metrics, whereas other methods show noticeable degradation. For Wan2.1 1.3B [63], similar pattern appears in Tab. 4. At the higher cache budget (upper block), SeaCache again reduces latency and TFLOPs while providing substantially higher PSNR and SSIM and lower LPIPS than TeaCache and TaylorSeer. Under the aggressive setting (lower block), SeaCache also shows the fastest latency and preserves reconstruction quality effectively, with consistently better metrics. Overall, these results indicate that the spectrumaware schedule of SeaCache transfers well to video models and validate superior performance across architectures. 5.3. Qualitative Comparison Text-to-image generation. In Fig. 6, we compare SeaCache with TeaCache [33] and TaylorSeer [36] at two cache budgets with refresh ratios of approximately 30% and 50%. SeaCache preserves both the semantic content and overall perceptual quality of the original images, whereas the baselines frequently lose text or fine details. At 30% refresh ratio, the baselines fail to reproduce the word quantum specified in the prompt, while SeaCache faithfully preserves the text present in the fully computed image. At 50% refresh ratio (second row), SeaCache generates two orange plates consistent with both the prompt and the original imFigure 7. Qualitative comparison of text-to-video generative models at refresh ratios of approximately 30% and 50%. age, whereas the baselines either alter the plate color by filling the plates with food or remove one of the plates. Text-to-video generation. We further conduct qualitative comparisons on text-to-video models HunyuanVideo and Wan2.1 1.3B, as shown in Fig. 7. For each prompt, we display the same frame index for the original model. Our cache scheme preserves the content of the original implementation while using comparable computation budget. In the second row of HunyuanVideo, SeaCache maintains sharp and legible STOP sign that closely matches the original, whereas the baselines fail to synthesize the letters. In the third row on Wan2.1 1.3B, SeaCache produces clear pandas and coffee cups with fewer artifacts, while competing methods blur the foreground and introduce the distortions. 5.4. Additional Analysis Ablation study. We quantitatively evaluate the effect of each design choice in SeaCache. Fig. 8 compares four variants of the cache distance: the SEA filter, its complementary form 1SEA, version without normalization, and simple cutoff low-pass filter that only keeps the component of lowest 30% frequency. Across both FLUX and Hunyuan7 (a) PSNR-refresh ratio trade-off on FLUX [28, 29]. (b) PSNR-refresh ratio trade-off on HunyuanVideo [27]. Figure 8. Ablation on spectrum-aware filtering. Trade-offs for different cache metrics on FLUX and HunyuanVideo. Results are averaged over 200 prompts for FLUX and 20 randomly selected from VBench for HunyuanVideo, with the other settings fixed. Figure 9. Plug-and-play adaptation to DiCache. PSNR-refresh ratio trade-off on FLUX when applying the SEA-based cache metric to DiCache [6]. DiCache+Ours denotes DiCache combined with our SEA filter, while DiCache uses the original metric. Video, the SEA filter shows the best PSNR-refresh ratio trade-off, while 1SEA produces similar but consistently lower curve, indicating that tracking the spectral evolution of the noise component is somewhat informative but less aligned with content redundancy than our signal-focused design. Removing normalization leads to drop in PSNR, since the filtered feature magnitude drifts across timesteps and the cache metric becomes biased. The static low-pass baseline (LPF 30%) also performs noticeably worse than ours with SEA filter, showing that simply emphasizing low frequencies is insufficient and that the timestep-dependent spectral evolution captured by the SEA filter is crucial for effective cache scheduling. Adaptation to other cache methods. To examine the plugand-play nature of SeaCache, we integrate the SEA filter 8 Figure 10. Refresh pattern across timesteps on FLUX. Pertimestep refresh ratio at 30% budget. (a) SeaCache automatically concentrates refreshes on early timesteps, whereas (b) TeaCache spreads refreshes more uniformly over the trajectory. into DiCache [6], recent dynamic caching method that bases its metric on intermediate blocks. Instead of modifying the policy or network, we apply our SEA filtering to DiCaches block-level features and reuse its original accumulation rule. On FLUX, the adapted variant (DiCache+Ours) achieves consistently higher PSNR for the same refresh ratio than the original DiCache, as shown in Fig. 9, while keeping latency and FLOPs comparable. This result indicates that the proposed spectrum-aware distance is not limited to input-side features and can also enhance cache metrics defined on intermediate representations, suggesting broad compatibility with future caching schemes. Cache ratio visualization. For each timestep in FLUX with the 30% refresh ratio setting, Fig. 10 shows the fraction of samples that trigger refresh among 200 DrawBench prompts. Bright cells indicate steps that are frequently computed, while dark cells correspond to steps that are almost always skipped. Many prior open-source methods [6, 36, 42] fix several early steps to always compute in order to improve quality, introducing an extra hyperparameter that should be tuned by hand. SeaCache instead concentrates most refreshes on early timesteps, aligned with the spectral-evolution prior, whereas TeaCache [33] distributes refreshes in more gridlike pattern that does not adapt to timestep importance in Fig. 10. This adaptive schedule removes the need to manually choose how many early steps to compute and uses the cache budget more effectively. 6. Conclusion We study cache-based acceleration for diffusion models via spectral evolution, showing that raw feature distances used in prior cache-based approaches fail to separate the signal and the noise. We introduce SeaCache, trainingfree policy that bases reuse decisions on spectrally aligned space. From this analysis, we derived the SpectralEvolution-Aware (SEA) filter, whose distances follow the full-compute trajectory more faithfully than unfiltered metrics. By applying the SEA filter to input features, we obtain the schedules that adapt to content while respecting the spectral priors of the underlying diffusion model. We expect that incorporating spectral evolution into cache design can be combined with future acceleration methods."
        },
        {
            "title": "References",
            "content": "[1] Muhammad Adnan, Nithesh Kurella, Akhil Arunkumar, and Prashant Nair. Foresight: Adaptive layer reuse for accelerated and high-quality text-to-video generation. arXiv preprint arXiv:2506.00329, 2025. 1 [2] Anirud Aggarwal, Abhinav Shrivastava, and Matthew Gwilliam. Evolutionary caching to accelerate your off-thearXiv preprint arXiv:2506.15682, shelf diffusion model. 2025. 3 [3] Sotiris Anagnostidis, Gregor Bachmann, Yeongmin Kim, Jonas Kohler, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Albert Pumarola, Ali Thabet, and Edgar Schonfeld. Flexidit: Your diffusion transformer can easily In Progenerate high-quality samples with less compute. ceedings of the Computer Vision and Pattern Recognition Conference, pages 2831628326, 2025. 2, 4 [4] Hyojin Bahng, Caroline Chan, Fredo Durand, and Phillip Isola. Cycle consistency as reward: Learning image-text alignment without human preferences. 2025. 6 [5] David Brandwood. complex gradient operator and its application in adaptive array theory. In IEE Proceedings (Communications, Radar and Signal Processing), pages 11 16. IET, 1983. 1 [6] Jiazi Bu, Pengyang Ling, Yujie Zhou, Yibin Wang, Yuhang Zang, Dahua Lin, and Jiaqi Wang. Dicache: Let difarXiv preprint fusion model determine its own cache. arXiv:2508.17356, 2025. 1, 3, 5, 6, [7] Geoffrey Burton and Ian Moorhead. Color and spatial structure in natural scenes. Applied optics, 26(1):157170, 1987. 4, 2 [8] Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, and Wenwu Zhu. Q-dit: Accurate post-training quantization for diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2830628315, 2025. 1 [9] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, -DiT: training-free acceleration and Tao Chen. method tailored for diffusion transformers. arXiv preprint arXiv:2406.01125, 2024. 3, 6 [10] Jiwoo Chung, Sangeek Hyun, Hyunjun Kim, Eunseo Koh, MinKyu Lee, and Jae-Pil Heo. Fine-tuning visual autoregressive models for subject-driven generation. arXiv preprint arXiv:2504.02612, 2025. 2 [11] LightX2V Contributors. Lightx2v: Light video generation inference framework. https://github.com/ ModelTC/lightx2v, 2025. 2, 4 [12] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [13] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient In Advances in Neural exact attention with IO-awareness. Information Processing Systems (NeurIPS), 2022. 2 [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1, 2 [15] Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang, Zhijie Deng, Ion Stoica, and Hao Zhang. Efficient-vdit: Efficient video diffusion transformers with attention tile. arXiv preprint arXiv:2502.06155, 2025. 1 [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 2 [17] Fabian Falck, Teodora Pandeva, Kiarash Zahirnia, Rachel Lawrence, Richard Turner, Edward Meeds, Javier Zazo, and Sushrut Karmalkar. fourier space perspective on diffusion models. arXiv preprint arXiv:2505.11278, 2025. 1 [18] David Field. Relations between the statistics of natural images and the response properties of cortical cells. Journal of the Optical Society of America A, 4(12):23792394, 1987. 4, [19] Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, and Yu Wu. Forecasting when to forecast: Accelerating diffusion models with confidence-gated taylor. Knowledge-Based Systems, page 114635, 2025. 2 [20] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36:1323713249, 2023. 2 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [22] Xingchang Huang, Corentin Salaun, Cristina Vasconcelos, Christian Theobalt, Cengiz Oztireli, and Gurprit Singh. Blue noise for diffusion models. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. 1, 4 [23] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 2, 6, 4 [24] Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael Ryoo, and Tian Xie. Adaptive caching for faster video generation with diffusion transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1524015252, 2025. [25] Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, and Hongxia Jin. Token fusion: Bridging the gap between In Proceedings of the token pruning and token merging. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 13831392, 2024. 3 [26] Yeongmin Kim, Sotiris Anagnostidis, Yuming Du, Edgar Schonfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, and Artsiom Sanakoyeu. AutoregresIn Proceedings sive distillation of diffusion transformers. of the Computer Vision and Pattern Recognition Conference, pages 1574515756, 2025. 1 9 [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 6, 8, 2 [28] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 6, [29] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 2, 6, 8 [30] Haeil Lee, Hansang Lee, Seoyeon Gye, and Junmo Kim. Beta sampling is all you need: Efficient image generation strategy for diffusion models using stepwise spectral analysis. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 42154224. IEEE, 2025. 1, 4 [31] Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models. CoRR, 2023. 1, 3 [32] Yanjing Li, Sheng Xu, Xianbin Cao, Xiao Sun, and Baochang Zhang. Q-dm: An efficient low-bit quantized diffusion model. Advances in neural information processing systems, 36:7668076691, 2023. 2 [33] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 73537363, 2025. 1, 3, 5, 6, 7, 8, 2 [34] Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, Juan Manuel Perez-Rua, and Jurgen Schmidhuber. Faster diffusion via temporal attention decomposition. 2025. 3, [35] Jiacheng Liu, Peiliang Cai, Qinming Zhou, Yuqi Lin, Deyang Kong, Benhao Huang, Yupei Pan, Haowen Xu, Chang Zou, Junshu Tang, et al. Freqca: Accelerating diffusion models via frequency-aware caching. arXiv preprint arXiv:2510.08669, 2025. 3 [36] Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang. From reusing to forecasting: Accelerating diffusion models with taylorseers. arXiv preprint arXiv:2503.06923, 2025. 1, 5, 6, 7, 8 [37] Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Fei Ren, Shaobo Wang, Kaixin Li, and Linfeng Zhang. Speca: Accelerating diffusion transformers with speculative feature caching. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 1002410033, 2025. 3 [38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 1, 2, 3 [39] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:5775 5787, 2022. [40] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2 [41] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee Wong. Fastercache: Training-free video diffusion model acceleration with high In The Thirteenth International Conference on quality. Learning Representations. 3 [42] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: In Proceedings of Accelerating diffusion models for free. the IEEE/CVF conference on computer vision and pattern recognition, pages 1576215772, 2024. 3, 5, 8 [43] Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, and Qi Tian. Magcache: Fast video generation with magnitudeaware cache. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 3, 5, 4 [44] Stephane Mallat. wavelet tour of signal processing. Academic press, 1999. [45] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1429714306, 2023. 1 [46] Henri Nussbaumer. The fast fourier transform. In Fast Fourier transform and convolution algorithms, pages 80 111. Springer, 1981. 5 [47] Yifan Pu, Zhuofan Xia, Jiayi Guo, Dongchen Han, Qixiu Li, Duo Li, Yuhui Yuan, Ji Li, Yizeng Han, Shiji Song, et al. Efficient diffusion transformer with step-wise dynamic attenIn European Conference on Computer Vition mediators. sion, pages 424441. Springer, 2024. 2 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2 [49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 2, [50] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations. 1, 2 [51] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion In SIGGRAPH Asia 2024 Conference Papers, distillation. pages 111, 2024. [52] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. 10 Conference on Computer Vision, pages 87103. Springer, 2024. [53] Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. 3 [54] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19721981, 2023. 1, 2 [55] Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, and Eunhyeok Park. Temporal dynamic quantization for diffusion models. Advances in neural information processing systems, 36:4868648698, 2023. 2, 3 [56] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. pmlr, 2015. 1, 2 [57] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations. 2 [58] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 1, [59] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya In Proceedings of the Sutskever. Consistency models. 40th International Conference on Machine Learning, pages 3221132252, 2023. 2 [60] Wei Sun, Tao Wang, Xiongkuo Min, Fuwang Yi, and Guangtao Zhai. Deep learning based full-reference and no-reference quality assessment models for compressed ugc videos. In 2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pages 16. IEEE, 2021. 2, 4 [61] David Tolhurst, Yoav Tadmor, and Tang Chao. Amplitude spectra of natural images. Ophthalmic and Physiological Optics, 12(2):229232, 1992. 4, 2 [62] van Van der Schaaf and JH van van Hateren. Modelling the power spectra of natural images: statistics and information. Vision research, 36(17):27592770, 1996. 4, 2 [63] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 6, 7 [64] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [65] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series. The MIT press, 1964. 4, 1 [66] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62116220, 2024. 3, 5 [67] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse video-gen: Accelerating video diffusion transformers with spatial-temporal sparsity. In Fortysecond International Conference on Machine Learning. 1, 2 [68] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. 1, 2 [69] xiaoju ye. calflops: flops and params calculate tool for neural networks in pytorch framework, 2023. 6 [70] Chenglin Yang, Celong Liu, Xueqing Deng, Dongwon Kim, Xing Mei, Xiaohui Shen, and Liang-Chieh Chen. 1.58-bit flux. arXiv preprint arXiv:2412.18653, 2024. [71] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv preprint arXiv:2505.18875, 2025. 1, 2 [72] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 2255222562, 2023. 4, 1 [73] Cuihong Yu, Cheng Han, and Chao Zhang. Dmfft: improving the generation quality of diffusion models using fast fourier transform. Scientific Reports, 15(1):10200, 2025. 1 [74] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. Advances in Neural Information Processing Systems, 37:11961219, 2024. 1, 2 [75] Evelyn Zhang, Jiayi Tang, Xuefei Ning, and Linfeng Zhang. Training-free and hardware-friendly acceleration for diffusion models via similarity-based token pruning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 98789886, 2025. 1, 2, 3, [76] Jintao Zhang, Pengle Zhang, Jun Zhu, Jianfei Chen, et al. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In The Thirteenth International Conference on Learning Representations. 1 [77] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 11 [78] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:4984249869, 2023. 2 [79] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Realtime video generation with pyramid attention broadcast. In The Thirteenth International Conference on Learning Representations. 1, 3 [80] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with tokenwise feature caching. In The Thirteenth International Conference on Learning Representations, . 6 [81] Zhen Zou, Jie Huang, Hu Yu, and Feng Zhao. Feb-cache: Frequency-guided exposure bias reduction for enhancing diffusion transformer caching. Available at SSRN 5584552, . 12 SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "We first present the derivation analysis of the linear diffusion process. We then provide additional experiments that further validate the effectiveness of the proposed SeaCache. 7. Derivation of Optimal Linear Response To design filter that reflects spectral evolution, we formalize how the effective frequency band changes across timesteps. Motivated by Spectral Diffusion [72] and Wiener Filtering [65], we adopt the timestep-dependent frequency response derived from the optimal linear denoiser . 1 Setup and assumptions. We consider the linear mixture of iterative denoising generative models (DPMs and RFs) at timestep t, xt = at x0 + bt ϵ, ϵ (0, I), (9) where x0 is the clean signal, assumed to be wide-sense stationary, and ϵ is zero-mean white Gaussian noise with flat power spectral density Sε(f ) = 1. We also assume that x0 and ϵ are independent. The Fourier-domain version of Eq. (9) is Xt(f ) = at X0(f ) + bt E(f ), (10) where X0(f ), Xt(f ), and E(f ) are the Fourier transforms of x0, xt and ϵ at frequency , respectively. The filter ht estimates x0 from xt as (cid:98)x0 = ht xt (cid:98)X0(f ) = Ht(f ) Xt(f ), (11) where ht is linear reconstruction estimator, Ht(f ) is the frequency response of ht, and (cid:98)x0, (cid:98)X0(f ) are the estimated signal and its Fourier counterpart, respectively. We define the signal reconstruction MSE objective, which is equivalent to the denoising objective of diffusion models, Jt = (cid:13) (cid:13) ht xt x0 (cid:13) 2 2, (cid:13) = arg min ht E[ Jt ] , (12) where the expectation is taken over (x0, ϵ). Frequency-domain MSE expansion. By Parsevals theorem [44], the reconstruction MSE (Eq. (12)) decomposes 1Throughout this section, denotes convolution in the frequency domain, the superscript denotes the optimal solution, and () denotes complex conjugation of Fourier coefficients. as an integral over frequencies. Since Ht(f ) acts independently at each frequency, minimizing the total MSE is equivalent to minimizing Jt(f ) for every , (cid:104) (cid:12) (cid:12) Ht(f ) Xt(f ) X0(f )(cid:12) (cid:12) Jt(f ) = (13) 2 (cid:105) . We now expand Jt(f ) using standard complex-valued Jt(f ) = quadratic expansion: 2 (cid:105) (cid:104) (cid:12) (cid:12) Ht(f ) Xt(f ) X0(f )(cid:12) (cid:12) (cid:104) (cid:0)Ht(f ) Xt(f ) X0(f )(cid:1)(cid:0)Ht(f ) Xt(f ) X0(f )(cid:1) (cid:105) = = Ht(f )2 E(cid:2) Xt(f )2 (cid:3) Ht(f ) (cid:104) Xt(f ) X0(f ) (cid:105) Ht(f ) (cid:104) X0(f ) Xt(f ) (cid:105) + E(cid:2) X0(f )2 (cid:3) , (14) next simplify where all quantities are evaluated at frequency . terms expectation the We E[X0(f ) Xt(f )] and E[Xt(f )2], which will be used in the subsequent derivation. Let Sx(f ) denote the power spectrum of x0. The first term can be written as X0(f ) (cid:0)at X0(f ) + bt E(f )(cid:1)(cid:105) (cid:104) (cid:105) X0(f ) Xt(f ) = two (cid:104) = at E(cid:2)X0(f )2(cid:3) + bt = at E(cid:2)X0(f )2(cid:3) = at Sx(f ), (cid:104) X0(f ) E(f ) (cid:105) (15) since we assume that x0 is wide-sense stationary and independent of the noise ϵ, so E[X0(f ) E(f )] = 0 in Eq. (15). Next, we expand the second expectation term: E(cid:2) Xt(f )2 (cid:3) = = a2 (cid:104)(cid:0)atX0(f ) + btE(f )(cid:1)(cid:0)at X0(f ) + bt E(f )(cid:1)(cid:105) E(cid:2)E(f )2(cid:3) E(cid:2)X0(f )2(cid:3) + b2 (cid:104) (cid:105) + atbt + atbt X0(f ) E(f ) E(cid:2)E(f )2(cid:3) E(cid:2)X0(f )2(cid:3) + b2 Sx(f ) + b2 Sε(f ) Sx(f ) + b2 , X0(f ) E(f ) (cid:104) (cid:105) (16) = a2 = a2 = a2 since in Eq. (16), the cross terms vanish because of independence, E[X0(f )E(f )] = E[X0(f )E(f )] = 0, and whiteness of the noise ϵ implies E[E(f )2] = Sε(f ) = 1. Optimality by differentiation. Differentiating Eq. (14) with respect to Ht(f ) using Wirtinger derivative [5] and setting the result to zero to find the optimal linear filter under the linear MMSE criterion, we obtain = Ht(f ) E[ Xt(f )2 ] E[X0(f ) Xt(f )] = 0. (17) Jt(f ) Ht(f ) 1 Table 5. Runtime overhead of SEA filtering per sample, averaged over 10 runs. Model FLUX (2D FFT) HunyuanVideo (3D FFT) SEA Filter (s) Latency (s) Overhead (%) 0.6 0.4 0.058 0.362 9.4 90.8 Table 6. Runtime overhead of SEA filtering per sample on Wan2.1-14B-T2V at different output resolutions. Resolution SEA Filter (s) Total Latency (s) Overhead (%) 0.41 480p 0.27 720p 0.668 1.539 161.5 561.1 Using Eqs. (15) and (16), the unique minimizer is (f ) = E[X0(f ) Xt(f )] E[ Xt(f )2 ] = at Sx(f ) Sx(f ) + b2 a2 , (18) where optimal frequency response (f ) is the Fourier transform of t . We define the Gt(f ) (f ). (19) Power-law prior. We adopt an empirical natural-image power-law assumption for the power spectrum [7, 18, 61, 62], Sx(f ) β, (20) where > 0 is an amplitude scaling factor and β is frequency exponent. In our experiments, we set = 1 and β = 2 for images and β = 3 for videos. Substituting this prior into the optimal response in Eq. (18) gives Gt(f ) = at β β + b2 a2 , (21) which shows that the effective passband widens as at increases (spectral evolution). Note that the SEA filter used in our method Gnorm (f ) is normalized variant of Gt(f ). Its form is provided in the main manuscript. 8. Runtime Overhead of SEA Filtering At every sampling step, SeaCache inserts an additional FFT frequency-domain filtering iFFT pass to construct SEA-filtered features. Thus, we measure how much of the end-to-end sampling time this pass occupies under 50% caching ratio, keeping all other settings identical to the main experiments. For FLUX [28, 29] with SeaCache, the SEA filtering pass takes on average 0.058 per sample out of total latency of 9.4 s, corresponding to only about 0.6% of the overall generation time. For HunyuanVideo [27] with SeaCache, the 3D FFT-based SEA filtering costs 0.362 per sample while the total latency is 90.8 s, roughly 0.4% of the end-to-end runtime. As summarized in Table 5, the SEA 2 filtering introduces negligible runtime overhead while enabling substantially better preservation of the original outputs compared to prior caching schemes. To further quantify the SEA filter and FFT/iFFT overhead on large text-to-video (T2V) diffusion model, we additionally profile LightX2V [11] on Wan2.1-14B-T2V [63] under 50% caching ratio, using single Blackwell Pro 6000 GPU, while keeping all other settings identical. Since sampling is performed in compressed latent space, the SEA filtering pass occupies only tiny fraction of the end-to-end runtime. As shown in Tab. 6, the overhead stays below 1% in practice and remains small even when increasing the output resolution (0.41% at 480p and 0.27% at 720p). 9. Compatibility with Fast Inference Works We additionally validate SeaCache on Wan2.1-T2V under two orthogonal acceleration settings: (i) distilled sampler (LightX2V [11], 16-step) and (ii) an efficient-attention variant (Jenga [75], 50-step), using each methods default configuration. All results are evaluated on VBench [23] at 480p with 41 frames, using videos generated from 50 randomly sampled VBench prompts. Under comparable refresh ratio budgets, SeaCache consistently improves quality over TeaCache and vanilla step reduction, as shown in Tab. 12. 10. Additional Evaluation 10.1. Quantitative Comparison in T2V Generation VBench on HunyuanVideo. We evaluate SeaCache against TeaCache [33] and TaylorSeer [19] on all VBench [23] dimensions (Tab. 7), where the upper rows correspond to the 50% refresh-ratio budget and the lower rows to the 30% budget. All detailed settings follow the main manuscript. Aggregating by average rank across dimensions (Tab. 9), SeaCache ranks first under both budgets, scoring 1.91 vs. 2.03/2.06 at 50%, and 1.75 vs. 2.16/2.09 at 30%. This indicates the strongest overall performance across VBench dimensions on HunyuanVideo. VBench on Wan2.1 1.3B. We repeat the evaluation on all VBench dimensions for Wan2.1 [63] (Tab. 8) with the same two budgets and the same experimental details as in the main manuscript. In aggregate (Tab. 10), SeaCache delivers stable performance across dimensions, ranking second under both budgets, 1.97 at 50% (vs. the best 1.91) and 2.13 at 30% (vs. the best 1.53). Although our cache configurations are designed to closely track the original full-refresh sampling trajectory, the VBench results on Wan2.1 still show that SeaCache provides robust performance across dimensions and refresh-ratio budgets. CompressedVQA on T2V. To further quantify how caching affects video quality, we report scores from CompressedVQA [60], full-reference video quality assessment Table 7. VBench metrics in HunyuanVideo. Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality 95.59% 95.75% 95.77% 95.57% 95.67% 95.78% Multiple Objects 64.71% 58.38% 63.64% 63.34% 60.06% 58.38% 95.99% 96.20% 96.28% 96.04% 96.18% 96.35% Human Action 96.00% 95.00% 94.00% 92.00% 92.00% 94.00% 99.14% 99.09% 99.15% 99.18% 99.07% 99.20% Color 89.61% 90.87% 90.26% 89.81% 89.26% 92.24% 98.77% 98.83% 98.88% 98.76% 98.86% 98.92% 62.50% 60.92% 62.07% 63.89% 60.93% 62.73% 62.01% 62.50% 60.55% 60.28% 62.50% 60.28% 63.89% 60.64% 63.25% 61.02% 61.11% 60.00% 61.84% 60.80% 62.96% 59.65% 57.78% 60.63% 42.81% 24.39% 40.48% 24.44% 40.92% 24.66% 44.48% 24.26% 41.72% 24.35% 42.88% 24.34% 19.85% 19.89% 19.83% 19.93% 20.02% 20.10% Spatial Relationship Scene Temporal Style Appearance Style Overall Consistency Table 8. VBench metrics in Wan2.1 1.3B. Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Dynamic Degree Aesthetic Quality 95.89% 95.78% 95.96% 96.04% 95.32% 96.03% Multiple Objects 52.67% 53.73% 53.89% 51.91% 45.05% 53.20% 97.09% 96.90% 97.05% 97.02% 96.54% 97.00% Human Action 72.00% 70.00% 70.00% 72.00% 69.00% 68.00% 98.30% 98.37% 98.20% 98.21% 98.21% 98.12% Color 92.95% 91.22% 93.01% 90.56% 87.83% 89.67% 81.94% 62.48% 97.37% 97.47% 88.89% 62.14% 84.72% 62.31% 97.41% 83.33% 62.25% 97.35% 97.48% 84.72% 60.85% 81.94% 61.71% 97.39% Imaging Quality 67.88% 68.08% 68.01% 67.47% 67.83% 67.66% Spatial Relationship Scene Temporal Style Appearance Style Overall Consistency 71.46% 23.91% 23.07% 75.48% 30.09% 22.75% 22.89% 23.32% 69.50% 67.67% 60.79% 69.57% 24.27% 22.98% 20.20% 22.37% 23.62% 22.96% 20.06% 20.13% 20.04% 20.09% 20.64% 20.06% 23.42% 23.41% 23.51% 23.58% 23.17% 23.18% Object Class 86.31% 83.47% 85.28% 86.47% 82.20% 82.59% 26.91% 26.60% 26.63% 26.68% 26.57% 26.33% Object Class 80.46% 82.75% 81.17% 80.22% 78.32% 79.75% Models TeaCache (δ=0.12) TaylorSeer (S=2) SeaCache (δ=0.19) TeaCache (δ=0.2) TaylorSeer (S=3) SeaCache (δ=0.35) Models TeaCache (δ=0.12) TaylorSeer (S=2) SeaCache (δ=0.19) TeaCache (δ=0.2) TaylorSeer (S=3) SeaCache (δ=0.35) Models TeaCache (δ=0.09) TaylorSeer (S=2) SeaCache (δ=0.2) TeaCache (δ=0.15) TaylorSeer (S=3) SeaCache (δ=0.35) Models TeaCache (δ=0.09) TaylorSeer (S=2) SeaCache (δ=0.2) TeaCache (δ=0.15) TaylorSeer (S=3) SeaCache (δ=0.35) Table 9. Comparison of avg. rank on VBench in HunyuanVideo. Table 10. Comparison of avg. rank on VBench in Wan2.1 1.3B. Method ( 50%) Rank Method ( 30%) Rank Method ( 50%) Rank Method ( 30%) Rank TeaCache (δ=0.12) TaylorSeer (S=2) SeaCache (δ=0.19) 2.03 2.06 1.91 TeaCache (δ=0.20) TaylorSeer (S=3) SeaCache (δ=0.35) 2.16 2.09 1.75 TeaCache (δ=0.09) TaylorSeer (S=2) SeaCache (δ=0.30) 2.13 1.91 1.97 TeaCache (δ=0.15) TaylorSeer (S=3) SeaCache (δ=0.35) 1.53 2.34 2.13 Table 11. CompressedVQA [60] scores on HunyuanVideo and Wan2.1 1.3B under single-scale and multi-scale settings. HunyuanVideo Method ( 50%) TeaCache (δ=0.12) TaylorSeer (S=2) SeaCache (δ=0.19) Method ( 30%) TeaCache (δ=0.20) TaylorSeer (S=3) SeaCache (δ=0.35) 2.72 2.92 3.98 Single-scale score Multi-scale score Method ( 50%) TeaCache (δ=0.09) 2.76 TaylorSeer (S=2) 2.95 SeaCache (δ=0.30) 3.99 Single-scale score Multi-scale score Method ( 30%) TeaCache (δ=0.15) 2.16 TaylorSeer (S=3) 2.26 SeaCache (δ=0.35) 3.17 2.11 2.22 3. Wan2.1 1.3B Single-scale score Multi-scale score 2.97 1.90 3.93 3.03 1.95 3.95 Single-scale score Multi-scale score 2.44 1.38 3. 2.49 1.42 3.11 Table 12. Compatibility with fast inference works on Wan2.1T2V: LightX2V [11] (14B, 16-step) and Jenga [75] (1.3B, 50step), evaluated under comparable refresh ratio budgets. Method Refresh Ratio PSNR LPIPS SSIM Distillation (LightX2V [11]) Vanilla TeaCache SeaCache 25% (4 steps) 11.444 25% (4 steps) 11.762 25% (4 steps) 11.926 Efficient Attn. (Jenga [75]) Vanilla 50% (25 steps) 15.154 TeaCache 50% (25 steps) 19.463 SeaCache 48% (24 steps) 24.453 Vanilla 32% (16 steps) 13.440 TeaCache 32% (16 steps) 17.692 SeaCache 32% (16 steps) 20.259 0.475 0.405 0.480 0.420 0.465 0. 0.357 0.604 0.191 0.744 0.097 0.852 0.455 0.534 0.259 0.681 0.194 0.748 (VQA) metric. For each video, we treat the uncached trajectory as the reference and compute single-scale and multiscale scores between the cached outputs. Tab. 11 summarizes the results on HunyuanVideo and Wan2.1 1.3B at two cache budgets with refresh ratios of approximately 50% and 30%. Across both models and budgets, SeaCache consistently achieves the highest single-scale and multi-scale scores among all caching baselines, indicating that it best preserves the visual quality of the original trajectory while still enjoying substantial reductions in the refresh ratio. 10.2. Comparison with MagCache We further compare SeaCache with MagCache [43] under matched refresh ratios by tuning the cache threshold δ. We use the default MagCache configuration and report results. For FLUX.1-dev, we follow the manuscript protocol (DrawBench, 200 prompts). For Wan2.1 1.3B T2V, we evaluate at 480p with 41 frames using 50 randomly sampled prompts from VBench [23]. As shown in Tab. 13, SeaCache consistently improves quality at the same refresh ratio. We attribute this to SeaCaches input-adaptive redundancy estimation, whereas MagCache relies on fixed magnitude threshold, which is less responsive to contentand timestepdependent variations. 10.3. Qualitative Comparison in T2I Generation In Fig. 12, we provide additional qualitative comparisons on FLUX at refresh ratios of approximately 50% (top panel) and 30% (middle panel), along with an additional set of examples at both cache budgets in the bottom panel. At 50% refresh ratio in the top-left of Fig 12, SeaCache preserves clean water surface without the blocky artifacts or texture distortions that appear in the baselines. In the top-right example, the baselines either generate blurry lemon or fail to capture the fluid dynamics inside the bottle, whereas SeaCache correctly synthesizes both the glass bottle and the orange liquid, closely matching the full-compute original reference. At more aggressive 30% refresh ratio in the middle panel, SeaCache again stays closest to the full-compute reference. In the middle-left example, only SeaCache reconstructs seven well-formed stars consistent with the original, while competing methods either miss or severely deform several stars. In the middle-right example, SeaCache produces five chopsticks with consistent length and color, whereas the baselines generate chopsticks with mismatched geometry and appearance. In the bottom panel of Fig. 12, we further compare the same text prompts across different cache budgets using the same seed. In the top row of the panel, for the prompt requesting exactly the word CUBE, the baselines repeatedly hallucinate cube-like patterns in the background, whereas SeaCache is the only method that successfully renders the intended text. In the last row of the panel, all methods generate six wooden ice creams, but the baselines produce slightly different designs or colors compared to the fullcompute reference, while SeaCache most closely matches the original design. These additional cases further support that SeaCache best preserves the original content and layout while operating under the same cache budgets. Table 13. Comparison with MagCache at matched refresh ratios (R.R.) by tuning the cache threshold δ. We report full-reference quality against the uncached outputs on FLUX.1-dev and Wan2.1 1.3B-T2V. SeaCache shows higher PSNR and lower LPIPS than MagCache [43] at the same refresh ratio."
        },
        {
            "title": "Method",
            "content": "R.R. PSNR LPIPS Method R.R. PSNR LPIPS FLUX.1-dev (50-step) MagCache (δ=0.04) [43] SeaCache (δ=0.215) MagCache (δ=0.07) [43] SeaCache (δ=0.27) 52% 52% 44% 44% 29.96 30.37 27.89 28."
        },
        {
            "title": "0.079 MagCache (δ=0.35) [43] 28%\n0.072 SeaCache (δ=0.55)\n28%",
            "content": "24.73 24.97 22.51 23.01 0.126 0.123 0.179 0.172 Wan2.1 1.3B T2V (50-step) MagCache (δ=0.055) [43] 50% SeaCache (δ=0.19) 49% 25.55 29."
        },
        {
            "title": "0.079 MagCache (δ=0.15) [43] 32%\n0.047 SeaCache (δ=0.4)\n32%",
            "content": "19.32 21.98 0.226 0.156 Figure 11. Qualitative comparison between MagCache and SeaCache at matched refresh ratios on FLUX.1-dev and Wan2.1 1.3B T2V. At the same refresh ratio, SeaCache better preserves the uncached trajectory while reducing refresh operations. 10.4. Qualitative Comparison in T2V Generation Fig. 13 presents further qualitative comparisons on HunyuanVideo and Wan2.1 1.3B, respectively. For each prompt, we horizontally concatenate the same intermediate frame index from the full-compute reference and all caching variants to isolate per-frame differences. On HunyuanVideo at 30% refresh ratio, the baselines exhibit severe artifacts around the hands during the Taichi motion, while SeaCache preserves plausible pose with smooth limb contours. At 50% refresh, the baselines render skateboard that appears to float above the surfboard, whereas SeaCache correctly places the skateboard in contact with the surfboard, matching the original video, as shown in the right side of Fig. 13. On Wan2.1 1.3B at 30% refresh ratio the baselines introduce noticeable distortions near the truck wheels and bicycles, but these artifacts do not appear in the SeaCache outputs, as visualized in Fig. 13. At 50% refresh, competing methods either cause food items on the table to disappear or introduce artifacts on the panda, while SeaCache closely follows the full-compute trajectory without these failures. Overall, these qualitative results indicate that SeaCache better tracks the original dynamics and adheres more faithfully to the text prompts while avoiding objectionable artifacts. 11. Limitation To derive the optimal linear filter, we adopt several simplifying assumptions that make the spectral response analytically tractable, even though they need not hold exactly in practice. We model the signal spectrum with power law under radial view, whereas generated samples, particu5 larly at later timesteps or in highly synthetic backgrounds with no salient objects, can deviate from this behavior. We also assume wide-sense stationarity and independence between signal and noise. When these conditions are violated, the closed-form linear filter is no longer strictly optimal and can introduce bias. In addition, our analysis is formulated in the image or video domain, while most modern generative models operate in learned latent space. The encoder can reshape the spectrum, so the latent distribution may differ from the assumed pixel-domain power-law model, and our filter then only approximates the optimal latent-space response. promising extension is to relax these assumptions by estimating per-timestep spectra, designing content-aware filters directly in the latent space, and augmenting them with lightweight nonlinear corrections, while preserving the plug-and-play nature of our cache policy. These extensions would reduce the gap between the assumed and actual signal models and further improve fidelity under real-world deviations from our assumptions. 6 Figure 12. Additional qualitative comparison of SeaCache and baselines on FLUX at refresh ratios of approximately 30% and 50%. 7 Figure 13. Additional T2V qualitative comparison of SeaCache and baselines at refresh ratios of approximately 30% and 50%."
        }
    ],
    "affiliations": [
        "NAVER Cloud",
        "Sungkyunkwan University"
    ]
}