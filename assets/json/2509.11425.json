{
    "paper_title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs",
    "authors": [
        "Md Mubtasim Ahasan",
        "Rafat Hasan Khan",
        "Tasnim Mohiuddin",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Amin Ahsan Ali",
        "Md Mofijul Islam",
        "A K M Mahbubur Rahman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodology's applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks. Code and pretrained models are available at https://github.com/mubtasimahasan/FuseCodec."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 5 2 4 1 1 . 9 0 5 2 : r FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs Md Mubtasim Ahasan1*, Rafat Hasan Khan1, Tasnim Mohiuddin3, Aman Chadha2, Tariq Iqbal4, Ashraful Amin1, Amin Ahsan Ali1, Md Mofijul Islam2,4, Mahbubur Rahman1 1 Center for Computational & Data Sciences, Independent University, Bangladesh 2 Amazon GenAI 3 Qatar Computing Research Institute 4 University of Virginia"
        },
        {
            "title": "Abstract",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within local window for fine-grained token-level supervision. We further introduce FuseCodecTTS, our methodologys applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-theart performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks. Code and pretrained models are available at https:// github.com/mubtasimahasan/FuseCodec. demonstrating *Corresponding author: mubtasimahasan@gmail.com Work does not relate to position at Amazon. Equal Supervision. Tokenization has become foundational in natural language processing (NLP), enabling language models to learn discrete representations, while facilitating efficient autoregressive modeling and scalable downstream applications (Schmidt et al., 2024). Inspired by this paradigm, the speech domain has increasingly adopted neural codecs, popularized by Encodec (Défossez et al., 2022) and SoundStream (Zeghidour et al., 2022). Neural codecs tokenize speech using an encoder, residual vector quantizer, and decoder architecture, enabling modeling discrete representations suitable for modular extension to downstream tasks such as speech synthesis (Wang et al., 2023). However, the continuous and multidimensional nature of human speech makes learning discrete representations inherently challenging (Ju et al., 2024). While neural codecs learn acoustic representations (waveform and low-level signal characteristics), they struggle to capture high-level semantics requiring downstream models to adopt additional self-supervised masked language objectives to derive semantic representations (phonetic content and linguistic meaning) (Borsos et al., 2023). To address this drawback, recent neural codec architectures incorporated semantic distillation from pretrained self-supervised speech models (Zhang et al., 2024; Défossez et al., 2024), improving the quality of speech reconstruction and the semantic aspect of learned representations. In addition, another fundamental aspect of human speech remains missing in above mentioned works: speech is inherently grounded in context and surrounding cues (Brown et al., 2022). Discrete speech representations, lacking grounding in context, fall short of capturing this essential attribute (Hallap et al., 2023). While language models have demonstrated strong capabilities in learning such contextual dependencies from text corpora (Devlin 1 et al., 2019a; Peters et al., 2018), speech tokenizers have yet to fully leverage these capabilities. Although recent neural codec (Ahasan et al., 2024) explored matching discrete speech representations with contextual representation from pre-trained language model, it falls short in effective crossmodal alignment, constraining the models ability to fully unify semantic and contextual information. Therefore, despite recent advances, several challenges remain unaddressed. Firstly, current approaches fail to unify all three aspects of discrete speech representation: acoustic (learned by neural codecs), semantic (from self-supervised speech models), and contextual (from language models). Most work incorporates only semantic information (Zhang et al., 2024; Défossez et al., 2024; Ye et al., 2024), neglecting contextual grounding. Secondly, while recent effort (Ahasan et al., 2024) attempts to integrate contextual representations, it lacks effective mechanisms for aligning text and speech modalities. Thirdly, existing methods rely on similarity-based objectives for representation matching without directly incorporating information into the latent space, limiting coherence and downstream performance. We address these challenges through our proposed methodologies, while preserving the core architecture and utilizing frozen representations with zero inference overhead. To address these challenges, we propose speech tokenization framework with three different strategies/variations that enrich discrete speech representations with unified and aligned semantic and contextual information. Our first strategy involves (i) Latent Representation Fusion, which integrates semantic and contextual embeddings into the encoders latent space through cross-modal attention and additive fusion, resulting in more robust and coherent representations. Building on this, we present (ii) Global Semantic-Contextual Supervision, where globally pooled and broadcasted modality vectors supervise each quantized token across time, facilitating temporally consistent and globally informed representation learning. To enforce explicit alignment, we introduce another strategy: (iii) Temporally Aligned Contextual Supervision, which dynamically matches contextual and speech tokens prior to timestep-level similarity supervision, enabling fine-grained cross-modal alignment and enhancing representation quality. Then, we instantiate our framework through three model variants: FuseCodec-Fusion with Latent Representation Fusion, FuseCodec-Distill with Global Semantic-Contextual Supervision, and FuseCodec-ContextAlign with Temporally Aligned Contextual Supervision. FuseCodec establishes new state-of-the-art performance on the LibriSpeech test set (Panayotov et al., 2015) by integrating contextual and semantic guidance into the learning of discrete speech tokens. Specifically, FuseCodec-Fusion achieves the best scores in transcription accuracy (WER 3.99, WIL 6.45), intelligibility (STOI 0.95), and perceptual quality (ViSQOL 3.47, PESQ 3.13), outperforming EnCodec (Défossez et al., 2022), SpeechTokenizer (Zhang et al., 2024), and DM-Codec (Ahasan et al., 2024). FuseCodec-Distill further achieves the highest UTMOS (3.65) and speaker similarity (0.996), highlighting its strength in perceptual naturalness and speaker fidelity. Meanwhile, FuseCodec-ContextAlign provides strong tradeoff between interpretability and performance, with particularly competitive scores in UTMOS (3.65) and similarity (0.995). These results underscore the effectiveness of incorporating contextual and semantic signals into the tokenization process for high-quality speech reconstruction. Therefore, our key contributions are: We introduce three novel neural codecs based on our method: Latent Representation Fusion (FuseCodec-Fusion), Global SemanticContextual Supervision (FuseCodec-Distill), and Temporally Aligned Contextual Supervision (FuseCodec-ContextAlign). Our framework tackles different limitations of neural codecs by integrating semantic and contextual information through distinct methods, improving cross-modal alignment and enhancing discrete representation learning. We demonstrate the utility of our approach in downstream TTS model and validate each component with extensive ablation studies. FuseCodec achieves state-of-the-art performance on LibriSpeech reducing transcription error and improving speech naturalness."
        },
        {
            "title": "2 Related Work",
            "content": "Recent progress in speech and audio generation has been largely driven by advances in discrete representation learning, neural audio codecs, and language model-based synthesis. VQ-VAE (van den Oord et al., 2018) introduced vector quantization 2 in latent spaces to support symbolic modeling of audio, while HuBERT (Hsu et al., 2021) applied masked prediction over cluster-derived labels to learn speech features in self-supervised manner. SoundStream (Zeghidour et al., 2022) proposed causal adversarially trained codec with residual vector quantization (RVQ) and demonstrated scalable compression at low bitrates. HiFi-Codec (Yang et al., 2023) further improved efficiency by introducing group residual quantization, reducing the number of required codebooks while preserving audio fidelity. On the generative side, AudioLM (Borsos et al., 2023) modeled long-range dependencies in semantic and acoustic tokens using transformerbased language modeling. This approach was extended by VALL-E (Wang et al., 2023), which enabled zero-shot text-to-speech synthesis by conditioning on short acoustic prompts and leveraging codec token generation. To improve the suitability of tokenization for language modeling tasks, X-Codec (Ye et al., 2024) integrated speech embeddings from pretrained models into the quantization pipeline, while LAST (Turetzky and Adi, 2024) learned tokenizer supervised by language model to improve downstream ASR and speech generation performance. HiFi-GAN (Kong et al., 2020) introduced multi-period and multi-scale discriminators, enabling high-fidelity waveform synthesis with real-time efficiency. In parallel, codec designs have evolved to improve training stability and perceptual quality. EnCodec (Défossez et al., 2022) introduced GANbased codec architecture with multi-loss balancing and spectrogram-based discrimination, setting new benchmark for real-time low-bitrate synthesis. BigCodec (Xin et al., 2024) scaled the VQVAE framework and showed that single large codebook could achieve near-human perceptual quality at 1 kbps. DAC (Kumar et al., 2023) proposed refinements to residual quantization, such as factorized and normalized codebooks, and introduced advanced discriminators to improve quality under bitrate constraints. More recent work has focused on improving token expressiveness for downstream tasks. SpeechTokenizer (Zhang et al., 2024) demonstrated that hierarchical quantization improves reconstruction and zero-shot TTS, while DM-Codec (Ahasan et al., 2024) matched quantization layer representations with pre-trained speech and text models to reduce WER and enhance contextual fidelity. Finally, NaturalSpeech 3 (Ju et al., 2024) introduced factorized codec to disentangle prosodic and acoustic attributes in speech, and Moshi (Défossez et al., 2024) unified ASR and TTS in streaming, full-duplex transformer model operating on jointly learned speech tokens."
        },
        {
            "title": "3 Proposed Method",
            "content": "As shown in Figure 1, we first introduce the speech discretization pipeline (3.1) and describe the extraction of semantic and contextual representations from pre-trained models (3.2). We then present three strategies for integrating multimodal guidance into speech tokenization: (i) Latent Representation Fusion (3.3.1), (ii) Global SemanticContextual Supervision (3.3.2), and (iii) Temporally Aligned Contextual Supervision (3.3.3). Finally, we outline the training objective (3.4) and the extension to text-to-speech task (3.5). 3.1 Discrete Speech Representation Discrete tokens serve as the foundation of neural codec-based speech-language models. Following established approaches (Défossez et al., 2022; Zhang et al., 2024; Ahasan et al., 2024), we discretize audio using an encoder-quantizer setup. Given an input speech waveform x, an encoder compresses into sequence of latent representations = {zi}T i=1, where is the number of encoded frames. The encoder output is then passed through Residual Vector Quantization module (RVQ), consisting of quantization layers. Each layer produces sequence of token indices {q(k) , we retrieve its corresponding embedding from the k-th codebook, resulting in sequence of quantized vectors Q(k) = {q(k) RD, with denoting the embedding dimensionality. We use the embeddings from the first quantization layer Q(1) as the discrete representation of speech, guided with multimodal representations. i=1. For each token index q(k) i=1, where q(k) }T }T 3.2 Multimodal Representation Extraction Concurrently, we extract representations from pretrained models. Specifically, we obtain contextual representations from pre-trained language model, which are dynamic, token-level embeddings that adapt to surrounding text (Devlin et al., 2019b; Peters et al., 2018). In parallel, we derive semantic representations from pre-trained self-supervised speech model, which capture the high-level structure and meaning (Borsos et al., 2023). Contextual Representation. The input speech 3 Figure 1: Overview of the FuseCodec speech tokenization framework. Input speech is encoded into latent features Z, then quantized into discrete tokens Q(1:K) via residual vector quantization (RVQ). To enrich these tokens, we incorporate semantic (Si, ˆS) and contextual (Ci, ˆC, C) representations from frozen pre-trained models. Global vectors ˆS and ˆC are formed via mean pooling and [CLS] selection, respectively. We propose three strategies: (i) Latent Representation Fusion, injecting global vectors ˆS, ˆC with to yield fused latent Z; (ii) Global SemanticContextual Supervision, supervising Q(1) with global vectors; and (iii) Temporally Aligned Contextual Supervision, aligning full contextual embeddings {Ci} to RVQ outputs via windowed matching algorithm to form C. waveform is transcribed into text using pre-trained Automatic Speech Recognition (ASR) model A, such that = A(x). The ASR model functions purely as speech-to-text converter and remains detached during training. The transcribed text is processed by pre-trained language model B, which produces token sequence {ci}n i=1. For each token ci, we extract hidden states from all layers, represented as {h(l) l=1. These are averaged to produce contextual embeddings: Ci = 1 , and deL notes the hidden dimension of the language model. Semantic Representation. The input speech waveform is passed through pre-trained selfsupervised speech model H, which outputs sequence of frame-level tokens {si}m i=1,. For each frame si, we extract hidden states from all layers: {h(l) l=1. These are averaged to obtain semantic l=1 h(l) , where Si RD embeddings: Si = 1 , and denotes the hidden dimension. , where Ci RD l=1 h(l) }L }L (cid:80)L (cid:80)L 3.3 Semantic-Contextual Guidance Our goal is to enrich discrete speech representations by integrating contextual and semantic information, enabling tighter alignment between acoustic structure and linguistic meaning. Prior work has explored similar directions: Zhang et al. (2024); Défossez et al. (2024) aligned HuBERT4 based semantic features with the first RVQ layer using cosine similarity, while Ahasan et al. (2024) matched BERT-based embeddings to RVQ outputs via padded sequences and similarity loss. However, these methods either rely on single modality (semantic in Zhang et al. (2024); Défossez et al. (2024)) or lack robust cross-modal alignment (misaligned context in Ahasan et al. (2024)). In contrast, we unify semantic and contextual representations while ensuring robust alignment. For this, we propose three strategies: (i) Latent Representation Fusion (3.3.1), (ii) Global SemanticContextual Supervision (3.3.2), and (iii) Temporally Aligned Contextual Supervision (3.3.3) 3.3.1 Latent Representation Fusion We first propose fusing semantic and contextual representations with the encoders latent output. The enhanced latents are then passed to the residual vector quantization (RVQ) module, enabling the learning of discrete codes enriched with semantic and contextual information. We begin by obtaining global semantic and contextual representations. Specifically, we take the average of semantic embeddings {Si}m i=1 to com- (cid:80)m pute the global semantic vector ˆS = 1 i=1 Si. For the textual modality, we select the [CLS] token embedding from the contextual representations {Ci}n i=1, yielding ˆC = C[CLS]. t=1, and = { ˆC}T We then broadcast each global vector across the discrete token sequence length , forming: = {ˆS}T t=1. Broadcasting allows each token to inherit the full semantic or contextual knowledge of the sequence, ensuring every position is enriched with the most informative signal for cross-modal fusion or distillation. Next, we apply multi-head cross-attention to enable cross-modal interaction, followed by an MLP projection to match the encoder dimension D: (1) = CrossAttention(S, C, C)WS, = CrossAttention( C, S, S)WC, where WS, WC RDD are learned projection matrices and CrossAttention() denotes multihead cross-attention. Finally, we fuse the modality signals with the latent representation RT via additive fusion and modality dropout: = + (S DS) + (C DC), (2) where DS, DC {0, 1}T are stochastic dropout masks applied during training. Dropout promotes robustness by preventing the quantized representations from over-relying on the fused modalities (Hussen Abdelaziz et al., 2020), and allows inference using only the encoder signal. The resulting fused representation is then passed to the RVQ module for discrete speech quantization. 3.3.2 Semantic-Contextual Supervision In addition to latent fusion, we explore an alternative representation supervision strategy, motivated by its effectiveness of similarity matching in prior speech tokenization work (Zhang et al., 2024; Défossez et al., 2024; Ahasan et al., 2024). Unlike previous methods that supervise over feature dimensions or require local frame-level alignment, we introduce global-to-local time-axis distillation scheme. Specifically, we use global semantic ˆS and contextual ˆC vectors to supervise the RVQ output across time. This provides temporally consistent guidance and encourages the quantized space to capture modality-aware temporal dynamics. We adapt the combined distillation loss from Ahasan et al. (2024), proposing it to operate along the temporal axis rather than the feature axis. This modification enables more effective alignment of discrete latent representations with temporally distributed semantic and contextual signals, enhancing cross-modal coherence over time. Given the broadcasted global signals (see 3.3.1) , we apply linear projection to S, RT the first-layer RVQ output Q(1) RT to align dimensionality: Q(1) = Q(1)W, where RDD. Finally, we apply semantic-contextual supervision using temporally-aware distillation loss. Ldistill = 1 (cid:16) (cid:88) log σ t=1 Q(1) , Ct (cid:18) 1 2 (cid:17)(cid:105)(cid:17) (cid:16) (cid:104) cos Q(1) , St (cid:17) , + cos (3) where σ() is the sigmoid function and cos(, ) denotes cosine similarity. This formulation provides fine-grained temporal supervision using global modality signals, enhancing the representational quality of the learned discrete tokens. 3.3.3 Aligned Contextual Supervision Building on our use of the global contextual vector ˆC for supervision, we propose finer-grained approach that leverages the full sequence of contextual embeddings {Ci}n i=1 to supervise the RVQ token sequence {q(1) }T t=1, enabling richer, timesteplevel guidance. key challenge, however, is the mismatch in sequence lengths between the contextual embeddings (n) and the RVQ output (T ). To address this, we propose dynamic windowbased alignment strategy that assigns each contextual embedding Ci RD to the most similar RVQ token Q(1) embedding within localized search window using cosine similarity. dynamic window-shifting mechanism prevents alignment overlap and ensures sequence-wide consistency. If multiple RVQ tokens within the window share the highest similarity, Ci is assigned to all corresponding positions in the aligned output C, accounting for cases where single linguistic token spans multiple speech frames. The resulting sequence RT enables timestep-level supervision. The full procedure is detailed in Algorithm 1. Finally, we apply temporally aligned contextual supervision using timestep-level distillation loss: Ldistill = 1 (cid:88) t=1 log σ (cid:16) (cid:16) cos Q(1) , (cid:17)(cid:17) , (4) where Q(1) = Q(1)W RT is the linearly projected RVQ output, and σ() denotes the sigmoid function. This loss enforces temporally precise alignment between acoustic tokens and their corresponding contextual representations, encouraging modality-aware token learning. Algorithm 1 Window-Based Token Alignment Require: Contextual embeddings {Ci}n i=1, RVQ tokens {Q(1) }T t=1, optional window size 1: if not provided then /n 2: 3: end if 4: Initialize aligned output RT 0 5: Initialize ℓ 0 {last matched index} 6: for = 1 to do 7: if dynamic window then ℓ + 1 if > 1, else 0 {start index} min(s + w, ) {end index} else (i 1) w, min(s + w, ) ) for [s, e) end if Compute cosine similarity αt = cos(Ci, Q(1) Let τ maxt αt {maximum similarity} Ti {t αt τ } for each Ti do Ci 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for ℓ max(Ti) 19: 20: end for 21: return 3.4 Architecture and Training Objective We build on widely adopted neural codec architectures and training objectives, following (Défossez et al., 2022; Zhang et al., 2024; Ahasan et al., 2024), to establish strong and reliable foundation. We contribute to enhancing the learned representations through semantic and contextual supervision and fusion without altering the model architecture. Architecture. We use wav2vec 2.0 (base-960h) as the ASR model (Baevski et al., 2020), BERT (bert-base-uncased) as the language model (Devlin et al., 2019a), and HuBERT (base-ls960) as the self-supervised speech model (Hsu et al., 2021). All pre-trained models are frozen during training. The speech tokenizer consists of an encoder E, an RVQ module with 8 quantization layers (codebooks) of size 1024, decoder D, and three discriminators (multi-period, multi-scale, and multi-scale STFT). Architectural details are provided in Sec. 4.1. Quantization operates on 50 Hz frame rates. The encoder and RVQ use an embedding dimension of = 1024, while the pre-trained langauge and speech model have = 768. Cross-Attentions are implemented using 8-heads. The dropout masks DS and DC are applied at rate of 10%. Training Objective. We also adopt multiobjective training setup grounded in established neural codec practices. This includes time-domain reconstruction loss Ltime, frequency-domain reconstruction loss Lfreq, adversarial loss Lgen, feature matching loss Lfeat, and RVQ commitment loss Lcommit (see Sec. 4.2 for details). To further enhance representation quality, we introduce two auxiliary supervision objectives: global distillation loss as Ldistill (Sec. 3.3.2) and temporally aligned contextual loss as Ldistill (Sec. 3.3.3). Ldistill is set to 0, when applying the Latent Representation Fusion (Sec. 3.3.1). The final training objective is weighted sum: Ltotal = λtimeLtime + λfreqLfreq + λgenLgen + λfeatLfeat + λcommitLcommit + (λdistillLdistill or 0) (5) 3.5 Downstream Extension to TTS Model We extend the learned discrete token representations to downstream text-to-speech (TTS) task, following the neural codec language modeling framework and objective used in prior work (Wang et al., 2023; Zhang et al., 2024; Ahasan et al., 2024). In this paradigm, speech synthesis is performed by predicting quantized acoustic tokens produced by the RVQ and decoded by neural codec. For this, we propose FuseCodec-TTS, an extension of FuseCodec-Fusion trained with either Latent Representation Fusion (see 3.3.1) or FuseCodec-Distill using Global SemanticContextual Supervision (see 3.3.2). This allows the TTS model to operate on discrete speech tokens enriched with semantic and contextual information. Given phoneme sequence and an acoustic prompt Rτ extracted from reference utterance using FuseCodec, the goal is to predict sequence of discrete token indices q(1), . . . , q(K), corresponding to the RVQ layers. To model coarse content and prosodic structure, we autoregressively predict the token indices q(1) from the first quantizer using decoder-only Transformer conditioned on the phoneme sequence p. The autoregressive (AR) training objective is: LAR = log (cid:81)T i=1 p(cid:0)q(1) q(1) <i , p; θAR (cid:1) (6) To capture fine-grained acoustic details, we use non-autoregressive model to predict q(k) for each = 2, . . . , K, conditioned on the previously predicted layers q(<k), the phoneme sequence p, and the acoustic prompt A. The non-autoregressive (NAR) training objective is: LNAR = log (cid:81)K k=2 p(cid:0)q(k) q(<k), p, A; θNAR (cid:1) (7) Both AR and NAR token generators are implemented using 12-layer Transformers with 16 attention heads, 1024-dimensional embeddings, 4096dimensional feed-forward layers, and dropout rate of 0.1. The predicted token indices are mapped to their corresponding quantized embeddings Q(k), which are then passed to FuseCodecs decoder to reconstruct the synthesized speech waveform."
        },
        {
            "title": "4 Tokenizer Design and Loss Functions",
            "content": "In this section, we provide additional details on our tokenizer backbone (4.1) and the training objectives for the backbone neural codec (4.2). 4.1 Model Details To implement strong speech tokenizer baseline, we adopt standard neural codec architecture and discriminator setup commonly used in prior work (Défossez et al., 2022; Zeghidour et al., 2022). Encoder and Decoder. The Encoder consists of an initial 1D convolutional layer with 32 channels and kernel size of 7, followed by 4 stacked residual blocks. Each block includes two dilated convolutions with (3, 1) kernel and no dilation expansion (dilation = 1), residual connection, and strided convolutional layer for temporal downsampling. Stride values across the blocks are set to 2, 4, 5, and 8, with kernel sizes for the downsampling layers set to twice the corresponding stride. Channel dimensions double at each downsampling stage. The encoder then includes two-layer BiLSTM, and concludes with 1D convolution (kernel size 7) to project to the target embedding dimension. ELU (Clevert et al., 2016) is used as the activation function, and layer normalization or weight normalization is applied depending on the layer. The Decoder mirrors the encoder architecture, with the only difference being the use of transposed convolutions in place of strided convolutions to reverse the downsampling steps, and the inclusion of LSTM layers to restore temporal resolution. Residual Vector Quantizer. The Residual Vector Quantizer (RVQ) module discretizes the encoders continuous latent representations into sequence of codebook indices. Specifically, we quantize the encoder latent tensor of shape [B, D, ] using 8 residual codebooks, each with 1024 codebook entries. Each subsequent codebook quantizes the residual error of the previous one. Codebook entries are updated using an exponential moving average with decay factor of 0.99. To prevent codebook collapse, unused entries are randomly resampled using vectors from the current batch. The RVQ output is discrete tensor of shape [B, Nq, ], where Nq is the number of active quantizers. The indices are mapped back to the original latent space by summing the corresponding codebook embeddings and are then fed into the decoder to reconstruct the input. straight-through estimator (Bengio et al., 2013) is used to propagate gradients through the quantizer. Discriminators. We utilize discriminators to guide the generators (Encoder, RVQ, and Decoder) to reconstruct speech more closely to the original. We make use of three distinct discriminators: Multi-Scale STFT (MS-STFT) discriminator, Multi-Scale Discriminator (MSD), and MultiPeriod Discriminator (MPD). The MS-STFT discriminator, proposed by (Défossez et al., 2022), works on multiple resolutions of the complexvalued short-time Fourier transform (STFT). It treats the real and imaginary parts as concatenated and applies sequence of 2D convolutional layers. The initial layer uses kernel size of 3 8 with 32 channels. This is followed by convolutions with increasing temporal dilation rates (1, 2, and 4) and stride of 2 along the frequency axis. final 3 3 convolution with stride 1 outputs the discriminator prediction. The MSD processes the raw waveform at various temporal scales using progressively downsampled versions of the input. We adopt the configuration from (Zeghidour et al., 2022), which was originally based on (Kumar et al., 2019). Similarly, the MPD, introduced by (Kong et al., 2020), models periodic structure in the waveform by reshaping it into 2D input with unique periodic patterns. For consistency, we standardize the number of channels in both the MSD and MPD to match those in the MS-STFT discriminator. 4.2 Training Objective To ensure that FuseCodec learns discrete speech representations, we ground our training objective 7 on proven techniques, following (Défossez et al., 2022; Zhang et al., 2024; Ahasan et al., 2024). Reconstruction loss. Let and ˆx denote the original and reconstructed speech waveforms, respectively. For spectral comparisons, we define 64-bin Mel-spectrograms Mi() using STFTs with window size 2i and hop size 2i/4, where = {5, . . . , 11} indexes different resolution scales. We compute the time-domain Ltime and frequencydomain Lfreq reconstruction losses as: Ltime = ˆx (8) Lfreq = (cid:88) iE (cid:18) Mi(x) Mi(ˆx) +Mi(x) Mi(ˆx)2 (cid:19) (9) Adversarial loss. To reduce the discriminability of reconstructed speech, we adopt GANbased training objective with set of discriminators {D(i)}d i=1, including multi-period (MPD), multiscale (MSD), and multi-scale STFT (MS-STFT) variants (see Sec. 4 for details). The generator Lgen and discriminator Ldisc losses are computed as: Lgen = Ldisc = 1 (cid:88) i=1 1 (cid:88) i= (cid:16) max 0, 1 D(i)(ˆx) (cid:17) (10) (cid:2) max(0, 1 D(i)(x)) + max(0, 1 + D(i)(ˆx))(cid:3) (11) Let D(i) () denote the output of the j-th layer of D(i), with ℓ total layers. We include feature Lfeat matching loss to stabilize training and align intermediate features as: Lfeat = 1 dℓ (cid:88) ℓ (cid:88) i=1 j=1 D(i) (x) D(i) D(i) (cid:16) (ˆx)1 (cid:17) (x)1 mean (12) Commitment Loss. To ensure encoder outputs align closely with their quantized representations, we apply commitment penalty during residual vector quantization (RVQ). Let rj denote the residual vector at step {1, . . . , q}, and cj be its corresponding nearest codebook entry, we calculate commitment loss Lcommit as: 8 Lcommit = (cid:88) j=1 rj cj2 2 (13)"
        },
        {
            "title": "5 Experimental Setup",
            "content": "Dataset. Following prior work in speech tokenization (Zhang et al., 2024; Ahasan et al., 2024), we train FuseCodec on the LibriSpeech (Panayotov et al., 2015) train-clean-100 subset, which contains 100 hours of English speech from 251 speakers, sampled at 16 kHz. During training, we randomly crop 3-second audio segments and reserve 100 samples for validation. For FuseCodec-TTS, we combine the train and dev subsets of LibriTTS (Zen et al., 2019), comprising 570 hours of speech. For evaluating FuseCodec, we use the LibriSpeech test-clean subset, which comprises 2,620 utterances held out entirely from training. This setup follows prior baselines (Zhang et al., 2024; Ahasan et al., 2024), though we evaluate on the full set rather than sampled subset. For FuseCodecTTS, we adopt two established benchmark protocols. In the LibriSpeech evaluation, following Wang et al. (2023), we select utterances between 4 and 10 seconds, yielding 2.2-hour subset. For each synthesis, 3-second enrollment segment is randomly cropped from different utterance by the same speaker. In the VCTK evaluation, following Zhang et al. (2024), 3-second prompt is selected or cropped from one utterance, and the transcript of separate utterance from the same speaker serves as the synthesis target. Training. FuseCodec is trained for 100 epochs on two A40 GPUs with batch size of 6, using the Adam optimizer with learning rate of 1 104 and exponential decay factor 0.98. FuseCodec-TTS is trained on A100 and L40S GPUs. The AR model is trained for 200 epochs, and the NAR model for 150 epochs. Training employs dynamic batching, with each batch containing up to 550 seconds of audio for AR and 100200 seconds for NAR. We use the ScaledAdam optimizer with learning rate of 5 102 and 200 warm-up steps. Reproducibility. We provide fully reproducible setup, including Dockerized environment, source code, model checkpoints, and configuration files (see Abstract for GitHub link). Baselines. We compare FuseCodec against both established and recent strong baseline speech tokenizers, including EnCodec (24 kHz) (Défossez et al., 2022) and SpeechTokenizer (Zhang et al., Table 1: Speech reconstruction results across content preservation and naturalness metrics. Orange and light orange cells indicate the best and second-best scores, respectively. Results show that FuseCodec variants outperform baselines by unifying contextual and semantic signals in the discrete speech representations. Model BigCodec DAC DM-Codec EnCodec FACodec Mimi SpeechTokenizer FuseCodec (Baseline) FuseCodec-ContextAlign FuseCodec-Distill FuseCodec-Fusion Content Preservation Speech Naturalness WER WIL STOI ViSQOL PESQ UTMOS Similarity 4.58 4.09 4.09 4.04 4.11 11.61 4.16 4.62 4.15 4.09 3.99 7.45 6.54 6.75 6.58 6.58 18.05 6.71 7.44 6.70 6.60 6.45 0.93 0.94 0.93 0.92 0.95 0.85 0. 0.93 0.93 0.94 0.95 3.02 3.36 3.20 3.06 3.11 2.49 3.08 2.95 3.18 3.43 3.47 2.68 2.72 2.77 2.31 2.89 1.69 2.60 2.54 2.85 3.06 3.13 3.44 3.33 3.45 2.41 3.45 2.28 3. 3.18 3.65 3.65 3.63 0.996 0.996 0.994 0.980 0.996 0.934 0.996 0.990 0.995 0.996 0.995 2024), as well as BigCodec (Xin et al., 2024), DAC (16 kHz) (Kumar et al., 2023), DM-Codec (LM+SM) (Ahasan et al., 2024) FACodec (NaturalSpeech 3) (Ju et al., 2024), and Moshi (Défossez et al., 2024). All baseline results are obtained using official released checkpoints. For FuseCodecTTS, we compare with neural codec language models that incorporate external representation guidance. Specifically, we compare against USLM (from SpeechTokenizer) (Zhang et al., 2024) and DM-Codec-TTS (Ahasan et al., 2024), using their official released LibriTTS trained checkpoints. Metrics. We evaluate FuseCodec using two complementary categories of metrics: Content Preservation and Speech Naturalness. To assess Content Preservation, we transcribe generated speech using Whisper (medium) (Radford et al., 2023) and compare it to ground-truth text. We report Word Error Rate (WER), defined as WER = S+D+I , where S, D, and denote the number of substitutions, deletions, and insertions, and is the number of words in the reference. We also report Word Information Lost (WIL), given by WIL = 1 , where is the number of correct words, is the number of words in the reference, and is the number of words in the prediction. Additionally, we include Short-Time Objective Intelligibility (STOI), reference-based metric estimating intelligibility via short-time spectral similarity. For Speech Naturalness, we evaluate perceptual and acoustic fidelity using both reference-based and learned metrics. ViSQOL and PESQ assess perceptual quality by modeling auditory similarity and signal distortion, respectively. We also report UTMOS for estimatN + ing human-judged naturalness, which is neural MOS predictor trained on large-scale human ratings. Lastly, we compute Similarity as the cosine similarity between L2-normalized speaker embeddings extracted using WavLM-TDNN (Chen et al., 2022), reflecting speaker or content consistency. For FuseCodec-TTS, we omit metrics requiring reference audio (e.g., STOI, ViSQOL, PESQ), as exact references are unavailable in synthesis."
        },
        {
            "title": "6 Experimental Results and Discussion",
            "content": "In this section, we evaluate our proposed methods on speech reconstruction quality (6.1) and their extension to speech synthesis (6.2). We further validate the contribution of each component through ablation studies in the next section (7). 6.1 Speech Reconstruction Evaluation We evaluate our three proposed methods: (i) Latent Representation Fusion: FuseCodec-Fusion (Sec. 3.3.1) (ii) Global Semantic-Contextual Supervision: FuseCodec-Distill (Sec. 3.3.2), and (iii) Temporally Aligned Contextual Supervision: FuseCodec-ContextAlign (Sec. 3.3.3). Results. The results in Table 1 show that FuseCodec improves performance across all metrics related to content preservation and speech naturalness. FuseCodec-Fusion performs best overall, achieving the lowest WER (3.99) and WIL (6.45), along with the highest STOI (0.95), reducing transcription error and improving intelligibility. It also achieves the highest scores in ViSQOL (3.47) and PESQ (3.13), reflecting superior perceptual quality. FuseCodec-Distill attains top scores in UTMOS (3.65) and Similarity (0.996), while also rank9 Table 2: Zero-shot TTS evaluation on LibriSpeech and VCTK. FuseCodec-TTS variants are compared to official neural codec-based TTS checkpoints trained on LibriTTS. Bold and underline indicate best and second-best scores. FuseCodec-TTS improves intelligibility, similarity, and naturalness via semantic-contextual aware tokenization. Model WER WIL Similarity UTMOS LibriSpeech VCTK LibriSpeech VCTK LibriSpeech VCTK LibriSpeech VCTK USLM DM-Codec-TTS FuseCodec-Distill-TTS FuseCodec-Fusion-TTS 16.72 10.26 8.55 9.67 14.79 5.02 3.66 4.07 25.65 13.79 12.07 13.23 23.24 8.21 6.02 7.18 0.80 0.82 0.82 0.83 0.78 0.79 0.78 0. 2.93 3.70 3.55 3.63 3.01 3.86 3.75 3.82 ing second in STOI (0.94), ViSQOL (3.43), and PESQ (3.06), demonstrating strong naturalness and speaker consistency. FuseCodec-ContextAlign also performs competitively, particularly in UTMOS (3.65) and Similarity (0.995), while showing consistent improvements over FuseCodec (Baseline). Discussion. FuseCodec-Fusion achieves the best overall performance. Compared to EnCodec, which focuses purely on acoustic representations, and FACodec, which separates attribute learning without unifying representations, FuseCodecFusion incorporates both semantic and contextual signals directly into the encoders latent space. This enables the quantizer to learn unified representation aligned with both linguistic meaning and acoustic structure. It also outperforms models like DAC and BigCodec, which prioritize compression but lack representational alignment. FuseCodecDistill improves upon SpeechTokenizer and Mimi, which distill only semantic representations from speech models and underperform on intelligibility and quality. In contrast, FuseCodec-Distill supervises the quantized space with global contextual and semantic signals, promoting alignment with high-level linguistic and acoustic content. introduces finegrained supervision by aligning discrete tokens with temporally matched contextual tokens, encouraging each token to reflect local linguistic context. Although its constrained alignment limits global contextual guidance, leading to slightly lower performance than FuseCodec-Fusion and FuseCodec-Distill, it still outperforms DM-Codec, improving intelligibility and speaker similarity. Overall, contextual guidance is most effective for content preservation, while semantic supervision enhances speech naturalness. FuseCodec-Fusion delivers the best balance, FuseCodec-Distill excels in speaker fidelity, and FuseCodec-ContextAlign offers interpretable gains. These results underscore the benefit of unifying multimodal representations. FuseCodec-ContextAlign 6.2 Speech Synthesis Evaluation We extend our methods to the zero-shot text-tospeech task and evaluate against neural codec TTS models that incorporate representational supervision for comparison. We deicde to adapt FuseCodec-Fusion to FuseCodec-FusionTTS and FuseCodec-Distill to FuseCodec-DistillTTS, based on their superior performance. Results. Table 2 shows that both FuseCodecTTS variants outperform prior methods across most metrics on LibriSpeech and VCTK. FuseCodecDistill-TTS achieves the best content preservation, with the lowest WER (8.55 / 3.66) and WIL (12.07 / 6.02), surpassing both DM-Codec-TTS and USLM. FuseCodec-Fusion-TTS delivers the highest perceptual quality, achieving the top speaker similarity (0.83 / 0.79), while also maintaining strong intelligibility with the second-best UTMOS (3.63 / 3.82), WER (9.67 / 4.07), and WIL (13.23 / 7.18). Discussion. FuseCodec-Fusion-TTS leads in perceptual quality and speaker similarity. Unlike DM-Codec-TTS, which lacks precise alignment, and USLM, which incorporates only semantic features, FuseCodec-Fusion-TTS integrates both semantic and contextual signals directly into the encoders latent space. This allows the quantizer to capture expressive prosody and speaker identity, resulting in more natural and coherent speech. FuseCodec-Distill-TTS achieves the highest intelligibility and transcription accuracy. In contrast to USLMs lack of contextual grounding and DMCodec-TTSs limited supervision, it distills global semantic-contextual representations into the quantized token space, enhancing alignment with semantic and contextual info. While FuseCodecFusion-TTS excels in naturalness and speaker fidelity, FuseCodec-Distill-TTS offers stronger linguistic precision. This trade-off reflects the complementary strengths of each variant and underscores the importance of integrating semantic-contextual fusion or supervision into speech tokenization. 10 Table 3: Ablation of attention-projection configurations in multimodal latent fusion. Cross variants incorporate cross-modal attention between semantic and contextual signals, while Self variants apply self-attention. Before applies attention prior to projection into the encoders latent space, whereas After applies attention post-projection. None uses direct projection without attention. Applying cross-modal attention before projection consistently improves content preservation and speech naturalness by enabling richer multimodal interactions in the original dimension. Model Variant Attn-Proj Type Content Preservation Speech Naturalness WER WIL STOI ViSQOL PESQ UTMOS Similarity FuseCodec-Fusion None Self-After FuseCodec-Fusion FuseCodec-Fusion Self-Before FuseCodec-Fusion Cross-After FuseCodec-Fusion Cross-Before 4.10 4.07 3.92 4.17 3.99 6.60 6.61 6.36 6.70 6.45 0.93 0.93 0.94 0.93 0.95 3.26 3.26 3.43 3.28 3.47 2.92 2.95 3.05 2.90 3. 3.65 3.63 3.59 3.61 3.63 0.995 0.995 0.995 0.995 0."
        },
        {
            "title": "7 Ablation Studies",
            "content": "We ablate and investigate each design choice and the necessity of components in our proposed methodology for FuseCodec. All model hyperparameters, training procedures, and configurations are kept fixed, except for the specific changes introduced in each ablation setup. 7.1 Ablation: Attention-Projection Configuration in Representation Fusion Setup. We investigate the impact of changing the attention-projection configuration in FuseCodecFusion (Section 3.3.1). The selected method, Cross-Before, applies multi-head cross-attention prior to projection: = CrossAttention(S, C, C)WS, = CrossAttention( C, S, S)WC, (14) where S, RT are broadcasted global semantic and contextual vectors. We compare this with the following ablated variants: None, which skips attention and directly applies projection: = SWS, = CWC Self-Before, which applies self-attention before projection: (15) = SelfAttention(S, S, S)WS, = SelfAttention( C, C, C)WC (16) Self-After, which projects first and then applies self-attention: = SelfAttention(SWS), = SelfAttention( CWC) (17) 11 Cross-After, which applies projection before crossattention: = CrossAttention(SWS, CWC, CWC), = CrossAttention( CWC, SWS, SWS) (18) Results. Table 3 shows the results of five variants. The selected Cross-Before setup achieves the highest performance on intelligibility STOI (0.95), and all naturalness metrics: ViSQOL (3.47), PESQ (3.13), and second-best UTMOS (3.63). SelfBefore yields the best WER (3.92) and WIL (6.36), and second-best ViSQOL (3.43), PESQ (3.05), and STOI (0.94). The None and Cross-After configurations perform comparatively worse across intelligibility and naturalness. Discussion. These results demonstrate that the configuration of attention relative to projection significantly impacts the effectiveness of representation fusion. The best-performing method, CrossBefore, applies cross-modal attention in the original lower-dimensional space. This enables richer semantic-contextual interactions to be captured before transformation into the higher-dimensional encoder space, leading to improved intelligibility and perceptual quality. Self-Before performs competitively by achieving the best WER and WIL, suggesting that intramodal structuring of global feature representations also benefits the fusion approach. However, the absence of explicit cross-modal exchange limits its effectiveness on naturalness metrics such as UTMOS and PESQ. By contrast, Cross-After performs poorly, indicating that applying cross-attention after projection diminishes its effectiveness. Suggesting that once Table 4: Ablation of attention and guidance strategies in semantic-contextual distillation. Cross variants apply cross-attention between contextual embeddings and discrete tokens, while None applies supervision directly. Semantic-Contextual combines both global semantic and contextual signals. Direct supervision using both signals achieves the best intelligibility and perceptual quality by preserving global structure. Model Variant Attention Guidance Content Preservation Speech Naturalness WER WIL STOI ViSQOL PESQ UTMOS Similarity FuseCodec-Distill None FuseCodec-Distill Cross FuseCodec-Distill None FuseCodec-Distill Cross Contextual Contextual Semantic-Contextual Semantic-Contextual 4.20 4.18 4.09 4.21 6.77 6.75 6.60 6.82 0.93 0.93 0.94 0.93 3.13 3.21 3.43 3. 2.74 2.83 3.06 2.84 3.60 3.60 3.65 3.62 0.995 0.995 0.996 0.994 projected into the higher-dimensional space, the global vectors lose semantic coherence, resulting in less expressive fusion and lower audio quality. Finally, removing attention (None) results in the weakest performance on intelligibility and perceptual scores, despite yielding the highest UTMOS. This indicates that even unstructured modality signals can enhance naturalness, but without alignment through attention mechanisms, they fail to deliver consistent semantic-contextual grounding. Overall, these results confirm that performing attention prior to projection, especially cross-modal attention, is essential for extracting the most benefit from semantic-contextual signals during fusion. 7.2 Ablation: Attention-Guidance Configuration in Semantic-Contextual Guidance Setup. We study the impact of attention configuration and guidance modality used in the distillation objective. Our method, FuseCodec-Distill, introduces timestep-aligned supervision using global contextual and semantic signals (Section 3.3.2). The selected configuration, None + SemanticContextual, projects the first-layer RVQ tokens Q(1) and computes cosine similarity with both semantic and contextual guidance vectors: Ldistill = 1 (cid:16) (cid:16) (cid:104) cos Q(1) , St (cid:17) (cid:88) log σ t=1 Q(1) , Ct (cid:18) 1 2 (cid:17)(cid:105)(cid:17) + cos (19) We compare this against three ablated variants: None + Contextual, which excludes both attention and semantic guidance: Ldistill = 1 (cid:88) t=1 log σ (cid:16) (cid:16) cos Q(1) , Ct (cid:17)(cid:17) (20) Cross + Contextual, which introduces crossattention between contextual vectors and projected RVQ tokens: = CrossAttention( C, Q(1), Q(1)) (21) Cross + Semantic-Contextual, which includes cross-attention but retains both guidance signals. Results. Table 4 reports the performance across four configurations. The best-performing variant is None + Semantic-Contextual, achieving the lowest WER (4.09) and WIL (6.60), and highest scores on STOI (0.940), ViSQOL (3.43), PESQ (3.06), UTMOS (3.65), and Similarity (0.996). The second-best results are obtained by Cross + Contextual, but excluding semantic guidance or using attention degrades performance across all metrics. Discussion. These results show that including both semantic and contextual supervision is essential for improving the quantization quality of the discrete tokens. The None + Semantic-Contextual configuration outperforms all others, highlighting that cosine-based alignment with both modalities provides the most stable and effective guidance during quantized representation learning. Introducing cross-attention (Cross) reduces performance, suggesting that attention distorts the global nature of the guidance signals and makes supervision less consistent across time. The Cross + Semantic-Contextual variant also underperforms, despite having access to both guidance sources, indicating that attention interferes with their inherent structure and alignment function. The Contextual-only variants perform comparatively worse, confirming that semantic signals play an important role in guiding the learned representations toward higher-level content fidelity and improved intelligibility. Overall, these findings support using both guidance signals in their original global forms and applying them directly, without attention, to ensure stable, timestep-aligned distillation. Table 5: Ablation of windowing and guidance strategies in temporally aligned contextual supervision. Dynamic variants adapt the alignment window per token based on content similarity, while Fixed variants use uniform window. Semantic-Contextual combines semantic and contextual signals for supervision. Dynamic windowing consistently improves intelligibility and clarity by enabling finer temporal alignment of contextual embeddings. Model Variant Window Guidance Content Preservation Speech Naturalness WER WIL STOI ViSQOL PESQ UTMOS Similarity FuseCodec-ContextAlign FuseCodec-ContextAlign Dynamic FuseCodec-ContextAlign FuseCodec-ContextAlign Dynamic Fixed Fixed Contextual Contextual Semantic-Contextual Semantic-Contextual 4.26 4.15 4.30 4.21 6.88 6.70 6.88 6.78 0.92 0.93 0.92 0.93 3.19 3.18 3.10 3.12 2.71 2.85 2.62 2.72 3.58 3.65 3.74 3. 0.994 0.995 0.995 0.995 7.3 Ablation: Fixed vs. Dynamic Window Configuration in Temporal Alignment Setup. We investigate the effect of fixed versus dynamic windowing in the token alignment algorithm (Algorithm 1). Our full method, FuseCodecContextAlign, aligns each contextual embedding Ci RD to localized region of RVQ tokens {Q(1) }T t=1 based on cosine similarity. The selected configuration, Dynamic-window Contextual (see Section 3.3.3), dynamically adjusts the alignment window for each Ci, using the index of the previous match to guide the next search range. This contentaware strategy produces temporally aligned sequence RT , which is used to compute timestep-level distillation loss: Lalign = 1 (cid:88) t=1 log σ (cid:16) (cid:16) cos Q(1) , (cid:17)(cid:17) (22) We compare this setup against the following ablated variants: Fixed-window Contextual, which uses fixed alignment window of size = /n, where is the RVQ sequence length and is the number of contextual embeddings. Each Ci is aligned to the most similar token Q(1) within its predefined window. Fixed-window Semantic-Contextual, which adds semantic supervision using semantic representations {Si}m i=1, in addition to contextual representations aligned via fixed-window token alignment. Since both semantic and RVQ tokens are extracted at the same frame rate, they are inherently timealigned, requiring no additional alignment. The combined loss is: Lalign = + cos 1 (cid:16) (cid:88) log σ t=1 Q(1) , St (cid:18) 1 2 (cid:17)(cid:105)(cid:17) (cid:16) (cid:104) cos Q(1) , (cid:17) (23) 13 Dynamic-window Semantic-Contextual, which replaces the fixed window with dynamic alignment strategy, while also incorporating direct supervision from semantic embeddings {St}. Results. As shown in Table 5, the Dynamicwindow Contextual configuration achieves the best performance across content preservation metrics, achieving the lowest WER (4.15), WIL (6.70), and highest STOI (0.93). It also performs strongly in terms of speech naturalness, with the best PESQ (2.85), high ViSQOL (3.18), and top Similarity (0.995). The Dynamic Semantic-Contextual variant achieves the best UTMOS (3.75), second-best WER (4.21) and WIL (6.78), and matches the top Similarity. By contrast, both Fixed-window configurations obtains lower scores across most metrics, particularly the Fixed Semantic-Contextual configuration, which scores the lowest ViSQOL (3.10) and PESQ (2.62), despite relatively high UTMOS (3.74). Discussion. These results highlight the importance of the temporal alignment strategy in influencing speech reconstruction quality. The superior performance of the Dynamic-window Contextual variant demonstrates that token alignment using dynamic window, where contextual embeddings are adaptively aligned based on token similarity, achieves better semantic grounding and contextual precision. In contrast, the Fixed-window variants suffer from rigid alignment constraints. They fail to capture fine-grained temporal dependencies by enforcing fixed windowing strategy, which resuls in degraded speech clarity (lower ViSQOL and PESQ). This limitation is especially noticeable in the Fixed Semantic-Contextual setup, where the addition of semantic supervision is insufficient to compensate for the strictly aligned contextual embeddings as the fixed window does not account for local content Table 6: Ablation of modality dropout probability during latent representation fusion in FuseCodec. Dropout indicates the stochastic masking rate applied independently to semantic and contextual representations during training. Moderate dropout prevents over-reliance on single modality, while higher rates degrade multimodal integration. 10% dropout rate achieves the best trade-off, maximizing intelligibility and perceptual quality. Model Variant Dropout Content Preservation Speech Naturalness WER WIL STOI ViSQOL PESQ UTMOS Similarity FuseCodec-Fusion FuseCodec-Fusion FuseCodec-Fusion FuseCodec-Fusion FuseCodec-Fusion 10% 30% 50% 70% 90% 3.99 4.10 4.09 4.08 4.15 6.45 6.63 6.58 6.64 6.67 0.95 0.94 0.94 0.93 0. 3.47 3.29 3.33 3.26 3.26 3.13 2.96 2.97 2.91 2.86 3.63 3.65 3.66 3.63 3.61 0.995 0.995 0.996 0.995 0.995 variations. Both Semantic-Contextual variants improve UTMOS, indicating that semantic supervision contributes positively to speech naturalness. However, this comes with trade-off when not paired with dynamically aligned contextual guidance, as the semantic-only supervision fails to improve content accuracy. Overall, these findings underscore that dynamic alignment is essential for effective contextual representation guidance. They also highlight that while semantic supervision enhances fluency and naturalness, it must be combined with flexible alignment mechanisms to avoid compromising content preservation. 7.4 Ablation: Dropout Mask Configuration in Representation Fusion Setup. We investigate the effect of modality dropout rate on the quality of latent representation fusion. As described in Section 3.3.1, we apply stochastic dropout masks DS, DC {0, 1}T element-wise to the projected semantic (S) and contextual (C) vectors during training: = + (S DS) + (C DC) (24) This stochastic masking prevents FuseCodec from over-reliance on any single modality and encourages the model to learn robust representations. The selected configuration uses 10% dropout ratei.e., each element in DS and DC has 10% chance of being masked to zero during training. We compare this against higher dropout rates: 30%, 50%, 70%, and 90%. and PESQ (3.13). Increasing the dropout rate to 3090% leads to the worsening of the most content preservation and speech naturalness metrics. While UTMOS and Similarity remain relatively stable, 50% dropout achieves minor gains in UTMOS (3.66) and Similarity (0.996). Discussion. These results confirm the importance of carefully balancing modality dropout during latent fusion and underscore the value of semantic-contextual representation integration. Preserving sufficient portion of the auxiliary representations by using small 10% dropout rate achieves the most effective use of semantic and contextual information. As the dropout rate increases, the model receives increasingly less additional modality information, reducing its ability to align latent tokens with multimodal supervision. This negatively affects intelligibility (WER, WIL) and perceptual quality (ViSQOL, PESQ). Interestingly, metrics such as UTMOS and Similarity remain relatively stable or improve at moderate dropout rates (50%), suggesting that prosodic and speaker characteristics are preserved within the base latent representations. However, the loss of some semantic-contextual information comes at the cost of worse content preservation. Overall, the findings suggest that light dropout (10%) provides the best trade-off, ensuring robust yet expressive multimodal grounding during latent token fusion. 7.5 Ablation: Quntizer Layer Configuration in Semantic-Contextual Guidance Results. The best overall performance is achieved with the 10% dropout rate configuration, which achieves the lowest WER (3.99) and WIL (6.45) and the highest STOI (0.95), ViSQOL (3.47), Setup. We study the impact of RVQ layer supervision depth in the distillation objective. Our method, FuseCodec-Distill, uses first-layer supervision, projecting the first-layer RVQ tokens Q(1) Table 7: Ablation of RVQ supervision depth under global (Distill) and temporally aligned (ContexAlign) guidance. First Layer indicates supervision is applied only to the first-layer RVQ tokens, while All Layers averages representations from all eight RVQ layers before supervision. Supervising the first-layer RVQ tokens leads to stronger semantic-contextual grounding and improved intelligibility compared to all-layer supervision. Model Variant RVQ Layer Content Preservation Speech Naturalness WER WIL STOI ViSQOL PESQ UTMOS Similarity FuseCodec-ContextAlign First Layer FuseCodec-ContextAlign All Layers FuseCodec-Distill FuseCodec-Distill First Layer All Layers 4.15 4.34 4.09 4.23 6.70 7. 6.60 6.86 0.93 0.93 0.94 0.93 3.18 3.17 3.43 3.26 2.85 2. 3.06 2.84 3.65 3.65 3.65 3.61 0.995 0.993 0.996 0.994 and computing cosine similarity (see Sections 3.3.2 and 3.3.3). We compare this against an ablated variant, alllayer supervision, which averages the outputs from all eight RVQ layers. We define the averaged RVQ output as: Q(1:8) = 1 8 8 (cid:88) i=1 Q(i) RT D, (25) Q(1:8) = Q(1:8)W In the Global Semantic-Contextual Supervision setting, we apply the all-layer supervision to the distillation loss as: Ldistill = 1 (cid:88) t=1 log σ (cid:18) 1 2 (cid:16) (cid:104) cos Q(1:8) , St (cid:17) + cos (cid:16) Q(1:8) , Ct (cid:17)(cid:105)(cid:17) (26) Similarly, for the Temporally Aligned Contextual Supervision setting, we apply the all-layer supervision to the distillation loss as: Lalign = 1 (cid:88) t=1 log σ (cid:16) (cid:16) cos Q(1:8) , (cid:17)(cid:17) (27) Results. Table 7 shows the effect of RVQ supervision depth across both distillation configurations. For FuseCodec (Distill), which uses Global Semantic-Contextual Supervision, first-layer supervision achieves the strongest performance across all content preservation and naturalness metrics, with the lowest WER (4.09), WIL (6.60), and highest STOI (0.94), ViSQOL (3.43), PESQ (3.06), UTMOS (3.65), and Similarity (0.996). Similarly, FuseCodec (ContexAlign), which uses Temporally Aligned Contextual Supervision, First-layer supervision again achieves stronger results in WER (4.15), WIL (6.70), ViSQOL (3.18), PESQ (2.85), and Similarity (0.995). In contrast, using all-layer supervision leads to consistent degradation across most metrics in both settings. Discussion. The results highlight that the layer at which RVQ tokens are supervised significantly impacts the quality of semantic and contextual guidance during distillation. Supervising the first RVQ layer yields stronger performance, as these tokens encode high-level, abstract representations more aligned with semantic intent and global context. This leads to better linguistic grounding and intelligibility, reflected in improved WER, STOI, and ViSQOL scores. In contrast, deeper RVQ layers capture lowerlevel acoustic and residual details, which are less suitable for semantic or contextual alignment. Averaging supervision across all layers matches these fine-grained signals with global ones, impacting the alignment objective. This results in performance drop across content preservation and speech naturalness metrics. Some naturalness metrics, such as UTMOS and Similarity, remain relatively stable with all-layer supervision, suggesting that speaker identity and prosodic features are distributed throughout the RVQ layers. However, these are insufficient for guiding semantic alignment during distillation. Overall, applying supervision at the first RVQ layer provides clearer, more semantically grounded signal, leading to better alignment and overall performance in speech reconstruction."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduced FuseCodec, unified speech tokenization framework that integrates acoustic, semantic, and contextual signals via multimodal representation fusion and supervision. Our methods 15 enable fine-grained alignment and achieve state-ofthe-art results on speech reconstruction, improving intelligibility, quality, and speaker similarity. These findings highlight the value of semantic and contextual grounding in discrete speech modeling."
        },
        {
            "title": "References",
            "content": "Md Mubtasim Ahasan, Md Fahim, Tasnim Mohiuddin, Mahbubur Rahman, Aman Chadha, Tariq Iqbal, Ashraful Amin, Md Mofijul Islam, and Amin Ahsan Ali. 2024. Dm-codec: Distilling multimodal representations for speech tokenization. Preprint, arXiv:2410.15017. Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: framework for self-supervised learning of speech representations. Preprint, arXiv:2006.11477. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. Preprint, arXiv:1308.3432. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. 2023. Audiolm: language modeling approach to audio generation. IEEE/ACM Trans. Audio, Speech and Lang. Proc., 31:25232533. Annemarie C. Brown, Eva Childers, Elijah F. W. Bowen, Gabriel A. Zuckerberg, and Richard Granger. 2022. Phonemes in continuous speech are better recognized in context than in isolation. Frontiers in Communication, Volume 7 - 2022. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, and Furu Wei. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518. Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. 2016. Fast and accurate deep network learning by exponential linear units (elus). Preprint, arXiv:1511.07289. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019a. Bert: Pre-training of deep bidirectional transformers for language understanding. Preprint, arXiv:1810.04805. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019b. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022. High fidelity neural audio compression. Preprint, arXiv:2210.13438. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: speech-text foundation model for real-time dialogue. Preprint, arXiv:2410.00037. Mark Hallap, Emmanuel Dupoux, and Ewan Dunbar. 2023. Evaluating context-invariance in unsupervised speech representations. Preprint, arXiv:2210.15775. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460. Ahmed Hussen Abdelaziz, Barry-John Theobald, Paul Dixon, Reinhard Knothe, Nicholas Apostoloff, and Sachin Kajareker. 2020. Modality dropout for imIn Proproved performance-driven talking faces. ceedings of the 2020 International Conference on Multimodal Interaction, ICMI 20, page 378386, New York, NY, USA. Association for Computing Machinery. Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, and Sheng Zhao. 2024. Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. Preprint, arXiv:2403.03100. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. Hifi-gan: generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA. Curran Associates Inc. Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brebisson, Yoshua Bengio, and Aaron Courville. 2019. Melgan: Generative adversarial networks Preprint, for conditional waveform synthesis. arXiv:1910.06711. Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. 2023. Highfidelity audio compression with improved rvqgan. Preprint, arXiv:2306.06546. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE 16 Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. 2019. Libritts: corpus derived from librispeech for textto-speech. Preprint, arXiv:1904.02882. Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. 2024. Speechtokenizer: Unified speech tokenizer for speech language models. In The Twelfth International Conference on Learning Representations. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 22272237, New Orleans, Louisiana. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. PMLR. Craig Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, and Chris Tanner. 2024. Tokenization is more than compresIn Proceedings of the 2024 Conference on sion. Empirical Methods in Natural Language Processing, pages 678702, Miami, Florida, USA. Association for Computational Linguistics. Arnon Turetzky and Yossi Adi. 2024. Last: Language model aware speech tokenization. Preprint, arXiv:2409.03701. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2018. Neural discrete representation learning. Preprint, arXiv:1711.00937. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023. Neural codec language models are zero-shot text to speech synthesizers. Preprint, arXiv:2301.02111. Detai Xin, Xu Tan, Shinnosuke Takamichi, and Hiroshi Saruwatari. 2024. Bigcodec: Pushing the limits of low-bitrate neural speech codec. Preprint, arXiv:2409.05377. Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou. 2023. Hifi-codec: Group-residual vector quantization for high fidelity audio codec. Preprint, arXiv:2305.02765. Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, Yike Guo, and Wei Xue. 2024. Codec does matter: Exploring the semantic shortcoming of codec for audio language model. Preprint, arXiv:2408.17175. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2022. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507."
        }
    ],
    "affiliations": [
        "Amazon GenAI",
        "Center for Computational & Data Sciences, Independent University, Bangladesh",
        "Qatar Computing Research Institute",
        "University of Virginia"
    ]
}