{
    "paper_title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
    "authors": [
        "Mengchen Zhang",
        "Qi Chen",
        "Tong Wu",
        "Zihan Liu",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project."
        },
        {
            "title": "Start",
            "content": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation Mengchen Zhang1,2, Qi Chen3,4, Tong Wu5(cid:66), Zihan Liu6,2, Dahua Lin2,7(cid:66) 1Zhejiang University, 2Shanghai Artificial Intelligence Laboratory, 3Shanghai Jiao Tong University, 4Shanghai Innovation Institute, 5Stanford University, 6Beihang University, 7The Chinese University of Hong Kong zhangmengchen@zju.edu.cn, cq1073554383@sjtu.edu.cn, wutong16@stanford.edu, liuzihan@buaa.edu.cn, dhlin@ie.cuhk.edu.hk, Figure 1. Overview. Left: BiAudio dataset converts 360 videos and FOA audio into perspective video and binaural audio pairs, employing diverse camera rotations to enhance spatial cues. Middle: Our end-to-end pipeline employs conditional flow matching with dual-branch generation architecture, integrated with conditional spacetime module to generate spatially immersive binaural audio from multimodal inputs. Right: Example results generated by ViSAudio. As shown above, our model faithfully generates the visible sound of waves crashing, highlighted with red boxes in both the video frames and the audio waveform, with the left channel louder since the sound event occurs on the left. It also captures subtle environmental sounds like ocean noise, highlighted with blue boxes, demonstrating its ability to reproduce fine-grained background acoustics. As shown below, as the camera rotates right, the marimba sound moves left, increasing left-channel amplitude while decreasing the right, demonstrating dynamic adaptation to viewpoint changes. 5 2 0 2 ] . [ 1 6 3 0 3 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-toend binaural spatial audio generation directly from silent the BiAudio video. this task, we present To support dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through semi-automated pipeline. Furthermore, we propose ViSAudio, an endto-end framework that employs conditional flow matching with dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatiotemporal alignment between audio and the input video. 1 Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project. 1. Introduction With the rapid growth of virtual and augmented reality [1, 18], the demand for realistic, immersive audio-visual experiences has surged. To achieve immersion, sound must not only be synchronized with visual content but also convey spatial awareness. Binaural spatial audio creates highly realistic spatial listening experience by simulating twodimensional soundscape. However, traditional binaural audio production requires specialized equipment and expertise [61]. Therefore, automatically generating spatial binaural audio from silent video offers substantial practical value. Driven by advancements in generative AI, recent research [5, 7, 31, 47, 50, 52, 55, 56] has made significant strides in generating mono audio from silent video in an end-to-end manner. However, binaural audio generation remains constrained by two-stage process: mono audio is first generated using pretrained video-to-monoaudio generator and then transformed into binaural spatial audio through separate spatialization process. Some methods [8, 49, 59] localize and track sound sources in input video to synthesize plausible spatial audio, while others [12, 13, 34, 53, 60] employ UNet-like [41] architectures to predict binaural channels directly from mono audio. However, these approaches rely heavily on presynthesized mono audio, making their performance inherently constrained by the quality of the first-stage output. Meanwhile, the subsequent spatialization often considers only visible sound sources, ignoring off-screen sounds and environmental noise. This two-stage paradigm is therefore prone to error accumulation, leading to misalignment with the input video and inconsistencies in spatial. Recent methods [22, 28] have established an end-to-end paradigm for spatial audio generation, enabling direct synthesis of FirstOrder Ambisonics (FOA) from video. However, these approaches depend on 360 video inputs or rely on extra parameters such as camera orientation. In contrast, end-to-end binaural audio generation aims to extract spatial cues directly from perspective video, making it broadly applicable. Nevertheless, this direction remains largely unexplored. In this work, we propose to generate binaural spatial audio from silent video in an end-to-end manner. This is challenging task due to data sparsity: Existing realworld video-binaural datasets [11, 45, 57, 58] are small in scale, lack diversity, and often focus on narrow environments like street scenes or music. Others rely on synthetic audio and video [14], limiting their real-world applicability. Moreover, previous datasets are often constrained by fixed camera perspectives, with very few containing camera motion. To address these limitations, we introduce BiAudio, large-scale, open-domain dataset featuring diverse real-world sound environments. It consists of approximately 97,000 pairs of binaural spatial audio and video clips, each lasting 8 seconds and accompanied by descriptive captions, totaling 215 hours. We developed semiautomated pipeline for dataset construction, during which we diversified camera rotation trajectories to ensure that the generated audio is not restricted by fixed viewpoints. Furthermore, we propose ViSAudio, the first end-toend framework designed to generate binaural spatial audio based on silent video and optional text conditions. ViSAudio employs conditional flow-matching to jointly model the left and right audio channels through DualBranch Audio Generation design, ensuring channel consistency while maintaining distinct spatial characteristics. To further improve spatio-temporal fidelity, we introduce Conditional Spacetime Module that extracts synchronization and spatial features from the video and injects them into the dual-channel generation process. The architecture enables the generation of high-quality, immersive audio that remains spatio-temporally aligned with the input video, outperforming existing state-of-the-art approaches. Our contributions can be summarized as follows: We curate BiAudio, large-scale, open-domain dataset of videobinaural audio pairs with diverse camera motions, along with semi-automated construction pipeline. We propose ViSAudio, novel framework that integrates conditional spacetime module into dual-branch audio generation, achieving channel coherence and spatial distinctiveness with precise audiovisual spatial consistency. We achieve end-to-end binaural audio generation from silent video, bypassing the limitations of conventional two-stage approaches. Extensive experiments demonstrate that ViSAudio outperforms state-of-the-art methods on both objective metrics and subjective evaluations, generating high-quality binaural audio with strong spatial immersion and adapting effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. 2. Related Work Video-to-Audio Generation. Early research [19, 33, 36, 42, 46] primarily focuses on auto-regressive models. SpecVQGAN [19] pioneers open-domain video-to-audio generation using VQGAN-based [9] Mel-spectrogram codebook. Diffusion models [40] significantly advance V2A generation [5, 7, 31, 47, 50, 52, 55, 56]. DiffFoley [31] combines contrastive audiovisual pretraining Table 1. BiAudio Dataset. Comparison between BiAudio and existing binaural audio-video datasets. FoV and 360 denote Field-ofView and panoramic videos, respectively, while FOA stands for First-order Ambisonics. BiAudio is currently the largest video-binaural audio dataset, featuring open-domain sounds from diverse real-world environments and varied camera rotation trajectories, enabling audio generation beyond fixed viewpoints. Rich captions allow modeling of subtle environmental sounds, producing more realistic audio. Dataset Statistic Data Type #Clips Duration Video Type Open Domain Real-world Visible Caption Camera Invisible Viewpoint OAP [45] Fair-Play [11] SimBinaural [14] MUSIC-21 [57, 58] YouTube-Binaural [14] BiAudio (Ours) 64k 1.9k 22k 1.7k 0.4k 97k 26h 5.2h 116.1h 81h 27h 215h 360 FoV FoV FoV FoV FoV Street Music Music Music Fixed Fixed Fixed Fixed Fixed Moving with latent diffusion to model spectrogram representations in latent space. Recent works [6, 29, 38, 48] adopt flowmatching-based [27, 44] generative models. Frieren [48] applies rectified flow matching [30] with reflow and onestep distillation for efficiency. MMAudio [6] employs flow-matching framework conditioned on multimodal inputs. Emerging multimodal frameworks [2, 5, 6, 29, 38, 42, 47, 55, 56] further advance the field by jointly modeling audio, visual, and textual modalities. Our method, ViSAudio, goes further by leveraging spatial cues to generate binaural audio, employing flow-matching framework conditioned on multimodal inputs. Visual-based Spatial Audio Generation. Producing spatial audio typically requires specialized equipment (e.g., microphone arrays), motivating research into generating it automatically from visual information. Early approaches [4, 8, 12, 13, 34, 49, 53, 59, 60] typically follow two-stage pipeline for visual-based spatial audio generation: mono audio is first generated and then transformed into spatial audio using visual cues. Some methods [8, 49, 59] localize and track sound sources from input videos to synthesize plausible spatial audio, while others [4, 12, 13, 34, 53, 60] adopt UNetlike [41] architectures to predict binaural channels from mono audio directly. However, these methods rely heavily on pre-existing mono audio inputs, introducing additional challenges such as misalignment with visuals and limited spatial consistency. Recently, end-to-end methods [22, 28] have emerged, directly generating spatial audio by leveraging both spatial and semantic cues from videos. OmniAudio [28] generates first-order ambisonics (FOA) audio from 360 panoramic videos, while ViSAGe [22] generates FOA audio from field-of-view (FoV) videos conditioned on corresponding camera directions. However, endto-end binaural audio generation remains unexplored. We propose the first end-to-end framework that directly infers spatially consistent binaural audio from visual content. 3. BiAudio Dataset We introduce BiAudio, large-scale dataset for video-tospatial audio generation. It consists of about 97,000 pairs of binaural spatial audio and perspective video clips, each lasting 8 seconds and accompanied by descriptive captions, for total of 215 hours. To ensure perceptually distinct spatial cues, we generate diverse camera rotation trajectories and discard samples with minimal left-right channel differences. The data construction process is detailed in Sec. 3.1, and the dataset statistics are presented in Sec. 3.2. 3.1. Dataset Construction Data Creation. We construct our dataset by sampling paired first-order ambisonics (FOA) audio and 360 video from the Sphere360 dataset [28], which covers diverse range of real-world, open-domain acoustic environments. To better align with human perception, the 360 videos are projected into 90 perspective views along pre-defined camera rotation trajectories, while the corresponding FOA audio is rendered into binaural signals via Head-Related Impulse Response (HRIR) convolution. However, in real-world perception, humans can hear off-screen sounds even when observing limited perspective view. Accordingly, real-world binaural audio contains abundant off-screen sounds as well as environmental noises that cannot be localized. As illustrated on the left of Fig. 1, humans can localize the sound of music and conversation, which is visible in the video and marked with yellow speaker icons, whereas off-screen sounds such as distant traffic and environmental noises like wind cannot be precisely localized. Using such raw audio-visual data indiscriminately may introduce semantic noise and weaken audio-visual alignment. To address this, we design twostage pipeline to finely annotate visible and invisible sound sources. Specifically, Qwen2.5-Omni [51] produces detailed textual descriptions that capture both visible sounds and background sounds, including off-screen sources and environmental noise. These descriptions are then distilled by Qwen3-Instruct-2507 [54] into concise captions following the format: Visible sound: <audible sound>, Invisible sound: <background sound>. sample is shown in Fig. 1. For details, please refer to Appendix Sec. A.2 Spatial Cue Enhancement. To strengthen perceptually 3 distinct spatial cues and overcome the constraints of fixed viewpoints, we diversify camera rotation trajectories. Each trajectory starts from random pitch, yaw, and roll orientation, followed by random drifts to mimic natural movements. Additionally, we ensure that the main visual cues corresponding to the audio stay within the field of view for certain duration. Specifically, we compute the direction of maximum audio energy [22, 35] for each clip using spherical harmonics decomposition and adjust the cameras initial orientation so that the trajectory is roughly centered around the primary sound source, as shown in Fig. 1. After obtaining the binaural audio, we discard samples with normalized leftright channel difference below 0.01, ensuring that the retained samples exhibit distinct spatial auditory cues. 3.2. Dataset Statistics BiAudio stands out as the largest FoV video dataset with binaural audio format. As shown in Tab. 1, previous realworld binaural datasets are much smaller in scale. While SimBinaural [14] offers over 100 hours of content, its synthetic video and audio limit its reliability for real-world applications. Moreover, these datasets typically focus on narrow sound environments: OAP [45] on street scenes, and Fair-Play [11], SimBinaural [14], and MUSIC-21 [57, 58] on music. In contrast, BiAudio is large-scale and opendomain, offering diverse real-world sound environments. Spatial Cue Enhancement (Sec. 3.1) further improves spatial perception and dynamics, making it ideal for video-tobinaural spatial audio generation. 4. ViSAudio Method The overall architecture of ViSAudio is depicted in Figure 2. Our approach builds upon parts of the MMAudio [6] architecture, fine-tuning the pretrained MMAudio to inherit its robust audio generation capabilities, ensuring strong generalization across open-domain acoustic environments. To generate binaural audio that ensures both channel coherence and spatial distinctiveness, we adopt DualBranch Audio Generation (Sec. 4.2), where two dedicated branches independently predict left and right flows. Additionally, we introduce Conditional Spacetime Module (Sec. 4.3), which extracts spatio-temporal features from the video and integrates them into the generation process, further enhancing the audios spatial realism. 4.1. Preliminaries Conditional Flow Matching. We employ the CFM objective [27, 44] for audio generation, guiding the model to progressively transform noise x0 sampled from the standard normal distribution into audio latents x1, conditioned on video and text. CFM defines probability density path pt(x) for [0, 1]. common and theoretically grounded choice is to construct this path using Optimal Transport (OT) displacement interpolation [32], yielding: xt = tx1 + (1 t)x0, and the corresponding velocity field at xt is: u(xt x0, x1) = x1 x0. (1) (2) The goal of CFM is to learn time-dependent conditional velocity field vθ(t, C, x) : [0, 1] RC Rd Rd, parameterized by neural network with parameters θ, where is the timestep, denotes the conditioning features, and denotes point in the vector field. During training, we optimize θ using the conditional flow matching objective: Et,q(x0),q(x1,C)vθ(t, C, xt) u(xt x0, x1)2, (3) where is uniformly sampled, q(x0) is the standard normal distribution, and q(x1, C) is sampled from the training set. Binaural Spatial Audio Generation. Given perspective video = {In}N n=1, where denotes the number of frames and each In R3HW is an RGB image, optionally accompanied by text , our objective is to generate high-quality binaural spatial audio = {Al, Ar}, with and indicating the left and right channels respectively. The generated audio should be spatially immersive and maintain spatio-temporal consistency with the video. Fig. 1 illustrates our end-to-end pipeline. Building on conditional flow matching, we extend the mono flow vθ into dual learning targets vl θ, corresponding to the left and right audio channels. During inference, ViSAudio estimates flows vl θ, vr θ for the current latent states xl , conditioned on the video and text inputs = {I[, ]}, along with timestep t. The flows are learned during training by optimizing the following objective: θ and vr t, xr (cid:88) a{l,r} Et,q(xa 0 ),q(xa 1 ,C) va θ (t, C, xa ) (xa 1 xa 0)2 . (4) The network then numerically integrates the noises xl 0 and 0 over the interval [0, 1], guided by flows vl xr θ and vr θ. The resulting latents xl and xr are separately decoded by the VAE [23] into mel spectrograms, which are in turn vocoded [26] into binaural audio waveform. 4.2. Dual-Branch Audio Generation This module centers on the joint generation of the two audio channels. Although some previous methods [10, 29, 43] utilize stereo audio VAEs directly, these VAEs discard significant amount of spatial information, merely generating audio in the form of stereo channels without preserving the spatial cues necessary for an immersive auditory experience. Instead, our method learns two correlated latent representations, xl and xr. These latents remain temporally synchronized and semantically consistent, while each encodes spatial cues unique to its corresponding channel. Figure 2. Our ViSAudio Network Architecture. Left: We adopt Dual-Branch Audio Generation (Sec. 4.2), where two dedicated branches independently predict the left and right audio flows. Right: Conditional Spacetime Module (Sec. 4.3) extracts spatiotemporal cues from the video and injects them into the generation process, improving spatio-temporal alignment between audio and video. and xr Features. Multimodal features are extracted during training. At each timestep t, the audio latents xl are derived from the CFM latent space. We extract multimodal features to capture information at multiple levels from the video. The text feature Ftext and visual feature Fvis are obtained using CLIP [39], which captures coarse-grained semantic content of the video. The synchronization feature Fsync is obtained via Synchformer [20], which captures the timing and dynamics of sound events from the video frames. However, these features do not explicitly encode the spatial positions of sound sources. To address this, we introduce the spatially tuned Perception Encoder [3], which leverages separate learnable positional embeddings for the left and right channels to capture fine-grained spatial and semantic cues from distinct perceptual perspectives, producing the spatial feature {l,r} that enables more accurate sound source localization. pe All features are then projected into shared hidden dimension h. After upsampling, {xl } Rmh and {Ftext, Fvis} Rh, where denotes the latent sequence length. The features are then sequentially passed through N1 Multimodal Joint Transformer Blocks and N2 Single-Modal Branch Transformer Blocks for cross-modal alignment and channel-specific refinement. , Fsync, {l,r} t, xr pe Multimodal Joint Transformer Block. To achieve crossmodal alignment, we build upon the multimodal transformer block design from MMAudio [6] and introduce sequential update mechanism. Within each block, the audio latents xl are updated sequentially by the shared block to ensure the two channels remain synchronized and semantically consistent, conditioned on Ftext, Fvis, and and xr Fsync, as follows: xl , , xr vis, vis, text = Bjoint text = Bjoint (cid:0)xl (cid:0)xr (cid:1), t, Fvis, Ftext, Fsync (cid:1). , text, Fsync vis, (5) (6) Single-modal Branch Transformer Block. To further capture channel-specific spatio-temporal details, we incorporate audio-only transformer blocks following FLUX [25]. Each channel is processed independently by its own branch, refining the latents based on channel-specific conditioning: xl = Bl = Br xr single single (cid:0)xl (cid:0)xr t, sp , sp (cid:1), (cid:1), (7) (8) single and Br where Bl single denote the single-modal transformer blocks for the left and right branches respectively, while Fsp represents the global spacetime feature extracted from our Conditional Spacetime Module (Sec. 4.3). 4.3. Conditional Spacetime Module This module is designed to generate global spacetime features Fsp by incorporating the synchronization feature Fsync and the spatial feature Fpe with global contextual information. This integration improves the spatio-temporal consistency between the generated audio and the video. Pe Spatial Feature. As shown in the left part of Fig. 2, the spatial feature Fpe Rmpe1616hpe is extracted by the spatially tuned Perception Encoder [3], where mpe represents the number of video frames (8 fps), hpe is the feature dimension, and 16 is the patch size. We use PE Spatial because it retains semantic information while producing highquality spatial features. Fpe is then downsampled through 2x2 average pooling to reduce spatial dimensions. Additionally, two learnable spatial position embeddings, El and Table 2. Objective Evaluation Results. We compare ViSAudio with baseline methods on the BiAudio, MUSIC, and FAIR-Play test sets. Notably, ThinkSound [29] and AudioX [43] produce 2-channel stereo audio rather than authentic binaural spatial audio. Comprehensive details regarding the baseline methods and evaluation metrics are provided in Sec. 5.1. The best results are highlighted in bold, and the second-best results are underlined. Our model consistently outperforms all baselines across all metrics and test sets. Dataset Method In-distribution BiAudio MUSIC ThinkSound [29] AudioX [43] ViSAGe [22] See2Sound [8] ViSAudio (Ours) ThinkSound [29] AudioX [43] ViSAGe [22] See2Sound [8] ViSAudio (Ours) Out-of-distribution FAIR-Play ThinkSound [29] AudioX [43] ViSAGe [22] See2Sound [8] ViSAudio (Ours) Audio Distribution Matching Video-Audio Alignment FDmix VGG FDavg VGG FDmix PANN FDavg PANN KLmix PANN KLavg PANN DeSync 5.125 4.224 14.212 9.573 2.516 16.451 24.299 48.260 45.038 4.649 6.648 11.421 15.010 25.611 4. 5.949 3.811 14.432 9.904 2.479 16.922 24.173 50.691 45.912 3.225 6.232 11.457 14.660 24.079 4.179 23.801 22.240 62.587 42.853 13.917 14.948 21.602 104.082 87.253 8.853 37.207 58.358 86.568 87.577 27. 23.928 20.941 60.289 41.797 12.684 14.349 20.992 100.693 81.747 6.288 35.811 57.436 83.241 82.750 23.330 2.462 2.167 3.546 3.410 1.355 0.481 0.689 3.374 3.247 0.332 1.916 2.977 2.514 3.672 1. 2.480 2.165 3.483 3.427 1.360 0.477 0.695 3.346 3.111 0.267 1.951 3.088 2.468 3.474 1.538 0.903 1.157 1.159 1.244 0.788 0.331 1.322 1.266 1.269 0.178 0.764 1.111 1.103 1.140 0. IB-Score 0.191 0.235 0.123 0.088 0.299 0.347 0.350 0.063 0.067 0.429 0.118 0.133 0.067 -0.004 0.194 Er, are introduced for the left and right channels, respectively, to capture the spatial characteristics specific to each channel. After projections and upsampling, we obtain the frame-aligned spatial features pe Rmh, {l, r}: pe = Upsample(MLP(Flatten(Conv(Fpe + Ea)))). (9) Global Spacetime Feature. The global spacetime features Fsp serve to inject spatio-temporal information into singlemodal branch transformer blocks via adaptive layer normalization (adaLN) [37]. These features are constructed by integrating the frame-aligned spatial features {l,r} with the synchronization feature Fsync and global conditioning cg: pe sp = Lineara (cid:0)(cid:2)cg + Fsync; a pe (cid:3)(cid:1) , {l, r}, (10) where Linear{l,r} denotes channel-specific linear projections that map the concatenated features from dimension 2h to h. This design enables the joint representation of spatial layouts and temporal dynamics while preserving distinctive channel-wise characteristics for binaural perception. 5. Experiments 5.1. Experimental Setup Datasets. The binaural audiovideo datasets we utilize include our BiAudio, MUSIC-21 [57, 58] and FAIRPlay [11]. Each video is segmented into 8 clips, with audio sampled at 44.1 kHz. For all datasets, we discard samples with minimal left-right channel differences, as described in Sec. 3. Subsequently, we split BiAudio by video ID, resulting in 94,845 training clips and 2,695 test clips, and split MUSIC-21 into 17,825 training clips and 1,857 test clips. The FAIR-Play dataset is employed as an out-ofdistribution test set, comprising 1,871 clips, to evaluate the models generalization ability beyond the training domain. Baselines. The following methods serve as the baselines: ThinkSound [29] and AudioX [43]: multimodal V2A approaches that leverage stereo VAEs to produce stereo audio, yet ignore spatial information from the video. ViSAGe [22]: It generates first-order ambisonics (FOA) based on visual content and camera direction. We fix the camera direction at (44.5, 89.5). Since ViSAGe only handles 5 video clips, we compress the original 8 videos to 5 s, generate the corresponding FOA, render binaural audio, and then restore the output to 8 s. See2Sound [8]: It generates mono-audio for each visual region of interest and synthesizes 5.1 surround audio. We mix it down to the left and right channels. Evaluation Metrics. We evaluate ViSAudio using both objective and subjective metrics to comprehensively assess its performance. The objective metrics are as follows: FD and KL: We employ Frechet Distance (FD) with VGGish [15] (FDVGG) and PANN [24] (FDPANN) embeddings, as well as KullbackLeibler divergence (KL) with PANN 6 Table 3. User Study. Subjective evaluation using Mean Opinion Score (MOS) with 95% confidence intervals to assess spatial impression, alignment with input video across spatial, temporal, and semantic dimensions, and overall realism of the generated audio. Method Spatial Impression Spatial Consistency Temporal Align. Semantic Align. Audio Realism ThinkSound [29] AudioX [43] ViSAGe [22] See2Sound [8] ViSAudio (Ours) 3.250 0.247 3.050 0.272 1.658 0.228 2.033 0.298 4.133 0.294 2.875 0.294 2.792 0.305 1.517 0.209 1.517 0.205 4.100 0. 3.483 0.243 2.867 0.301 1.700 0.308 1.742 0.333 4.275 0.272 3.400 0.265 3.367 0.261 1.725 0.310 1.725 0.281 4.292 0.235 3.067 0.272 3.258 0.315 1.492 0.205 1.650 0.333 4.158 0.282 (KLPANN) as classifiers, to evaluate the distributional similarity between the generated and ground-truth binaural audio. To holistically reflect the quality of the generated audio, we report FDmix and KLmix computed on the mixed mono audio. To assess the fidelity of the individual channels, we report FDavg and KLavg, which are obtained by averaging the scores from each channel. We further introduce DeSync to evaluate audio-visual synchrony, which measures the temporal misalignment between the generated audio and video, predicted by Synchformer [20], following MMAudio [6]. Moreover, we assess semantic alignment using IB-Score, the average cosine similarity between visual features extracted via ImageBind [16] and audio features. However, objective metrics fall short in intuitively assessing the spatial impression and audio-visual consistency of generated audio. Therefore, we conducted subjective evaluation using Mean Opinion Score (MOS) across five perceptual aspects to incorporate human-aligned perspective: Spatial Impression: Whether audio conveys clear sense of spatiality, including left-right and depth cues. Spatial Consistency: how well the audio aligns with the visual spatial cues, ensuring that audio and visual elements correspond correctly in 3D space. Temporal Alignment: Synchronization between audio and the input video, matching the timing of visual events. Semantic Alignment: Correspondence between the generated binaural spatial audio and the input modalities, including video and optional text. Audio Realism: Naturalness, clarity, and realism of the generated audio, independent of the video content. Implementation Details. All training experiments are conducted on 8 NVIDIA A800-SXM4-80GB GPUs with 2 AMD EPYC 7H12 64-Core CPUs. We fine-tune the MMAudio [6] flow prediction network (large, 44.1kHz, v2) on our training set using batch size of 64, learning rate of 1e-4, and weight decay of 1e-6. Training typically converges after 50,000 iterations, within two days on 8 A800 GPUs. Approximately 4TB of high-speed NVMe storage is required. Details are provided in Appendix Sec. C.1. 5.2. Quantitative Results Objective Evaluation. We report the superior performance of ViSAudio across multiple objective metrics on both in-distribution and out-of-distribution datasets, as shown in Tab. 2. It is compared with several state-of-the-art video-to-spatial-audio generation methods, with details of all compared methods provided in Sec. 5.1. For the BiAudio and MUSIC test sets, we use both video and caption inputs, while for FAIR-Play, which lacks captions, only video is provided. Models without text-processing capability are evaluated using video inputs only. The results in Tab. 2 show that ViSAudio outperforms other binaural-audio generation methods in overall quality, spatial channel distribution matching, audio-visual synchrony, and semantic alignment. This highlights the models ability to generate highfidelity binaural audio while maintaining audio-visual coherence under varying acoustic environments. Notably, its strong performance on the out-of-distribution FAIR-Play dataset, which was not seen during training, demonstrates robust generalization across diverse sound environments. User Study. We performed comprehensive subjective evaluation in Tab. 3 to establish human-aligned evaluation metrics. We sampled 10 videos, along with their corresponding captions when available, from three test sets, covering diverse domains, acoustic environments, and events, with both stationary and moving sound sources. For each video, binaural audio was generated using our method and the baseline approaches. We collected responses from 12 experts. Model performance was evaluated using the Mean Opinion Score (MOS) and its 95% confidence interval, where participants assigned scores from 1 to 5 to each of the five competing models per task, with higher scores indicating better performance. Each participant rated 50 audiovisual samples according to the five criteria outlined in Sec. 5.1. As shown in Tab. 3, our approach outperforms all baselines across every metric, closely matching earlier quantitative results and validating both its perceptual quality and technical reliability. Overall, our method achieves state-of-the-art performance in spatial impression, audiovisual consistency, and audio realism. 5.3. Qualitative Results We present comparative analysis of video-conditioned binaural spatial audio generation in Fig. 3. The example shows person playing the sitar, with the camera moving from left to right, causing the sound source to shift from right to left in the video. We visualize the spectrograms of the generated binaural audio from our method ViSAudio, Table 4. Ablation Study on Key Model Components. We conduct an ablation study to evaluate the contributions of key model components: Dual, Dual-Branch Audio Generation (Sec. 4.2) and Spt, Conditional Spacetime Module (Sec. 4.3). Pretrained refers to our pretrained MMAudio [6] model, spatialized by duplicating the mono output into both channels for evaluation. Model Pretrained w/ Dual only Dual+Spt FDavg VGG DeSync IB-S U-SI U-SC 2.817 0.285 3.658 0.289 4.233 0.299 2.775 4.017 4. 0.793 0.766 0.788 4.482 2.803 2.479 the ground truth, and other baselines. Our method produces audio that is most consistent with the ground truth and effectively reflects spatial changes in the sound source. The boxed regions indicate that when the sound source is located on the left, the left-channel spectrogram exhibits higher energy. This reflects the increased loudness perceived by the left ear and matches the ground-truth spatial audio, demonstrating our models ability to adapt to viewpoint changes. In contrast, other methods generate incorrect rhythms and fail to reflect any left-right channel differences. 5.4. Ablation Study We conduct an ablation study to evaluate the contributions of key model components. We assess three objective metrics to quantify improvements in distribution matching (FDavg VGG), temporal alignment (DeSync), and semantic alignment (IB-Score) on our BiAudio test set. To further capture the gains in spatial perception, we perform user study following the setup in Sec. 5.2, evaluating Spatial Impression (U-SI) and Spatial Consistency (U-SC) using the Mean Opinion Score (MOS). We compare several variants of our model to analyze the impact of different model components: (1) the pretrained MMAudio [6] model, spatialized by duplicating the mono output into two channels for evaluation; (2) adding the Dual-Branch Audio Generation module to train on our binaural data; and (3) adding the Conditional Spacetime Module to integrate spatio-temporal information into the model. From the results in Tab. 4, we observe that our Dual-Branch Audio Generation module effectively learns the distribution of our binaural training dataset, substantially enhancing spatial generation capabilities. Meanwhile, it retains the pretrained models strong audio generation quality and generalization, maintaining superior performance in temporal alignment and semantic alignment. With the introduction of Conditional Spacetime Module, the joint spatio-temporal learning slightly decreases temporal alignment, but further improves the spatial perception of the generated binaural audio. The improvement in U-SC, with gain of 0.575, demonstrates that the module effectively injects precise spatial information into our architecture, enabling the model to accurately localize sound sources and deliver more reliable and distinguishable Figure 3. Qualitative Comparison. The example shows person playing the sitar while the camera moves from left to right, causing the perceived sound source to shift from right to left. ViSAudio generates binaural audio that best matches the ground truth and accurately captures the spatial movement of the sound source. spatial cues for binaural audio. 6. Conclusion We propose ViSAudio, an end-to-end framework that integrates dual-branch audio generation with conditional spacetime module to produce spatially immersive binaural audio. To support this task, we curate BiAudio, largescale, open-domain dataset of video-binaural pairs featuring diverse camera motions. Our approach achieves state-ofthe-art performance, generating high-quality binaural audio with compelling spatial immersion that adapts effectively to viewpoint changes. This work paves the way for more immersive, end-to-end visual-to-spatial-audio generation. Limitations and future work. Our model currently handles short video clips, which limits its ability to capture long-range temporal dependencies and complex acoustic In addition, while BiAudio is an open-domain events. dataset of real-world recordings, background noise in the audio may introduce artifacts in the generated outputs. Our future work will focus on extending the model to handle longer sequences. Looking forward, we plan to extend our framework to support multi-channel audio generation, such as directly producing FOA audio."
        },
        {
            "title": "Appendix",
            "content": "The appendix presents supplementary material related to the BiAudio dataset and the ViSAudio method. Sec. begins with copyright disclaimer and an overview of the dataset construction pipeline, followed by comprehensive statistics to deepen understanding of its construction and content. Sec. offers detailed explanation of the ViSAudio framework, including the calculation of frameIn Sec. C, we outline the experaligned spatial features. imental details, covering model configurations, user study specifics, and additional ablation studies. Finally, we present cases from the main text and provide further examples through the accompanying video demo. Sec. elaborates on the video, showcasing the models performance across various acoustic scenarios and highlighting its potential for real-world applications. A. BiAudio Dataset A.1. Disclaimer on Copyright and Data Usage The construction of the BiAudio dataset is based on the extension of the existing dataset [28], and we strictly adhere to the terms of data usage. The video data utilized in this study were sourced from the YouTube platform. All content is copyrighted by their respective creators and owners. The videos included in this research adhere to YouTubes terms of service and, where applicable, to Creative Commons licenses. Specifically, videos under the Creative Commons license have been appropriately attributed to the original authors in accordance with the license terms (CC BY 4.0). For videos not governed by Creative Commons license, we acknowledge that they are protected by copyright and are used exclusively for academic research purposes. No commercial use of these videos is intended. The use of these videos falls under the fair use doctrine for educational and research purposes, as permitted by copyright law. A.2. Dataset Construction Details We present semi-automated pipeline that converts raw 360 panoramic video and first-order ambisonics (FOA) audio into field-of-view (FOV) video with dynamic perspectives and corresponding binaural audio. The pipeline consists of six key modules: Sound Source Localization. To enhance perceptually distinct spatial cues, we ensure that primary visual elements corresponding to dominant audio sources remain within the field of view for sustained periods. Specifically, we first conduct directional analysis [22, 35] of FOA audio through spherical harmonic decomposition to determine the primary sound source direction. In first-order ambisonics (ACN/SN3D), the signal A(t) = [W (t), (t), Z(t), X(t)] consists of four components: , X, , and Z. represents the omnidirectional component, capturing the overall sound pressure without any directional bias, essentially the total sound intensity from all directions. The directional components, X, , and Z, correspond to different spatial dimensions of the sound field: captures the front-back direction, represents the left-right direction, and corresponds to the up-down direction. These components are derived from spherical harmonics, with being the zero-order component and X, , and being the first-order components, which together describe the full 3D sound field. We compute the energy distribution of the sound field through spherical harmonics decomposition: E(ϕ, θ) ="
        },
        {
            "title": "1\nT",
            "content": "(cid:90) 0 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:88) (cid:88) l= m=l alm(t)Y (ϕ, θ) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) dt, (11) where (ϕ, θ) denotes the real spherical harmonic functions of order and degree m, ϕ [π, π] represents the azimuth angle, and θ [π/2, π/2] represents the elevation angle. The spherical harmonic functions for first-order ambisonics are defined as: (ϕ, θ) = sin ϕ cos θ, 0 0 (ϕ, θ) = 1, 1 1 0 1 (ϕ, θ) = sin θ, 1 1 (ϕ, θ) = cos ϕ cos θ. (12) (13) (14) (15) The coefficients amn(t) are derived from the ambisonics components through the relationship: a00(t) = (t), a1,1(t) = (t), a1,0(t) = Z(t), a1,1(t) = X(t). (16) (17) (18) (19) The energy distribution map E(ϕ, θ) is computed over discrete spherical grid with angular resolution of 2. The primary sound source direction (ϕmax, θmax) is determined by locating the position of maximum energy: (ϕmax, θmax) = argmax ϕ,θ E(ϕ, θ). (20) Dynamic Viewpoint Trajectory Generation. To overcome the limitations of fixed viewing perspectives, we introduce dynamic camera rotation trajectories. After localizing the primary sound source direction (ϕmax, θmax), the cameras initial orientation is adjusted to positions near this direction, followed by gradual drift to simulate natural, fluid movements. This approach ensures that the viewing trajectory evolves around the primary sound source region, keeping the main visual elements corresponding to the audio within the field of view for extended periods. 9 of being zero, to simulate diverse motion patterns: αϕ (cid:0)αmax ϕ , αmax ϕ (cid:1) I(ξϕ > αθ (αmax θ , αmax θ ) I(ξθ > αψ (cid:0)αmax ψ , αmax ψ (cid:1) I(ξψ > 1 3 1 3 1 3 ), ), ), (28) (29) (30) ψ = π 18 for yaw, αmax where U(a, b) denotes uniform distribution in the range [a, b], and I(ξ > 1 3 ) is an indicator function that randomly zeroes out the drift coefficient with probability of 1 3 . The values ξϕ, ξθ, and ξψ are independent random variables uniformly distributed between 0 and 1. The maximum drift ϕ = π rates are set as follows: αmax 90 for pitch, and αmax 180 for roll. Video Rendering. Based on the generated viewpoint trajectory, we convert the 360 equirectangular projection video into field-of-view (FOV) perspective sequences using the method described in [21]. For each frame the corresponding rotation matrix R(t) = at Rx(ψ(t))Ry(θ(t))Rz(ϕ(t)) is applied to transform the viewpoint. The composite rotation matrix is constructed from elementary rotations about the cardinal axes: θ = π time t, Rz(ϕ) = Ry(θ) = , cos ϕ sin ϕ 0 0 sin ϕ 1 0 cos ϕ 0 cos θ 0 sin θ 0 1 0 sin θ 0 cos θ (31) , (32) Rx(ψ) = 1 0 0 0 cos ψ sin ψ cos ψ sin ψ . (33) The rendering process employs fixed field of view FOV = 90 and outputs video at resolution of 512 512 pixels, ensuring the creation of natural perspective sequences that dynamically follow the camera trajectory. Audio Spatialization. The FOA audio stream undergoes frame-synchronized spatial transformation aligned with the viewing trajectory, followed by binaural rendering through multi-stage processing pipeline. The audio is divided into temporal segments corresponding to the trajectory points, with each segment containing N/M samples, where denotes the total number of audio samples in the stream. For the i-th segment spanning frames ti to ti+1, we construct the rotation matrix Ri = Rx(ψi)Ry(θi)Rz(ϕi) using the corresponding Euler angles (ϕi, θi, ψi) from the viewpoint trajectory. The directional components within each segment are rearranged from the ACN channel order [Wi, Yi, Zi, Xi] and spatially transformed according to: rot rot rot = Xi Yi Zi , (34) Figure R1. Caption Annotation Pipeline. We design two-stage annotation pipeline to label visible and non-visible sound sources. First, Qwen2.5-Omni [51] generates comprehensive textual descriptions that capture both visible sounds and background audio elements, including off-screen sources and environmental noise. These detailed descriptions are subsequently refined by Qwen3Instruct-2507 [54] into structured captions. The camera rotation trajectory is parameterized by three Euler angles: yaw ϕ(t), pitch θ(t), and roll ψ(t). The trajectory generation follows piecewise linear model: ϕ(t) = ϕ0 + αϕ t, θ(t) = θ0 + αθ t, ψ(t) = ψ0 + αψ t, (21) (22) (23) where [0, ] denotes the temporal coordinate, and (ϕ0, θ0, ψ0) represent the initial Euler angles. The initial yaw angle ϕ0 is determined by: ϕ0 = ϕmax + ϕ, 4 , 0(cid:1) (cid:1) 6 , π (cid:0) π (cid:0)0, π (cid:0) π 6 4 ϕ (cid:1) if αϕ > π 6 , if αϕ < π 6 , otherwise. The initial pitch angle θ0 is computed as: θ0 = clip (cid:16) θ, π , θ (cid:40) U(0, θmax) U(θmax, 0) , (cid:17) π 6 if θmax > 0, otherwise. (24) (25) (26) (27) The initial roll angle is fixed to ψ0 = 0. The drift coefficients (αϕ, αθ, αψ) are randomly sampled from uniform distribution with certain probability 10 Figure R2. Vocabulary in BiAudio captions. Left: Bar charts displaying the top 50 nouns for visible (blue) and invisible (red) sound sources. Right: Word clouds illustrating the distribution of the top 200 nouns in the vocabulary. where the omnidirectional component Wi remains unchanged throughout the rotation process. This segmentwise transformation ensures continuous spatial alignment between the audio field and the dynamically evolving camera perspective. The final output maintains the ambisonics signal in the channel order [W rot, rot, rot, rot], consistent with the original ACN layout. The spatially rotated ambisonics signal is rendered to binaural audio through convolution with head-related impulse responses (HRIRs) from the Omnitone library [17]. The ambisonics channels are grouped into two pairs and processed through partitioned convolution as follows: wyconv = conv zxconv = conv (cid:18)(cid:20)W (cid:18)(cid:20)Z (cid:21) (cid:19) , hrirwy , (cid:21) (cid:19) , hrirzx , (35) (36) where conv() denotes the full linear convolution operation, and hrirwy, hrirzx are the first-order FOA HRIRs decoded from Omnitone [17]. The binaural output signals are then synthesized by combining the convolved components: = wyconv[0] + wyconv[1] + zxconv[0] + zxconv[1], (37) = wyconv[0] wyconv[1] + zxconv[0] + zxconv[1]. (38) Caption Generation. To incorporate semantic guidance into our framework, we generate descriptive captions that go beyond basic sound tagging by accounting for the complexities of real-world auditory perception. Human hearing extends beyond the immediate field of view, encompassing both localized on-screen sounds and non-localizable audio 11 elements such as off-screen sources and ambient environmental noise. As illustrated on the left of Fig. R1, viewers can accurately spatialize visible sound sources like water splashing, but cannot precisely localize off-screen sounds such as birds chirping. The direct use of such unprocessed audio-visual data may introduce semantic interference and compromise audio-visual alignment. To address this issue, we design two-stage captioning pipeline that precisely distinguishes visible and nonvisible sound sources, as illustrated in Fig. R1. In the first stage, given the perspective video and its corresponding audio as input, Qwen2.5-Omni [51] produces comprehensive descriptions that capture both visible audio events and background sound elements, including off-screen sources and ambient noise. In the second stage, these detailed descriptions are refined by Qwen3-Instruct-2507 [54] into structured captions. Dataset Filtering. To ensure reliable spatial supervision, we apply filtering procedure to the collected binaural audio data. Given binaural waveform, we first normalize each channel by removing its DC component and scaling it to the range [1, 1]: xa xa µa max(xa µa) + ϵ , {l, r}, (39) where µa denotes the mean of channel a, and ϵ = 109 avoids numerical instability. We then evaluate the spatial distinctiveness of each audio clip by measuring the mean absolute difference between the two normalized channels: = 1 (cid:88) t=1 xl(t) xr(t) . (40) (a) Heatmap of initial camera viewpoints. (b) 3D visualization of camera rotation trajectories Figure R3. Camera Trajectory Statistics in BiAudio. (a) Heatmap of initial camera viewpoints, showing the distribution of starting yawpitch angles across all clips. (b) 3D visualization of camera rotation trajectories on unit sphere. Each curve represents temporally evolving viewing direction, with color changing from light to dark to indicate progression along the trajectory. Only samples satisfying > τ with τ = 0.01 are retained. This filtering criterion ensures that the curated dataset contains audio clips with sufficiently strong inter-channel disparities, which are essential for learning robust and meaningful spatial auditory representations. A.3. Dataset Statistics Details In this section, we provide detailed statistics of our dataset, covering the vocabulary, initial positions of viewpoint, and viewpoint trajectory visibility. Vocabulary: The dataset includes 2,360 unique sound categories or descriptive terms for visible sounds and 1,265 for invisible sounds. The distribution of these categories is illustrated in Fig. R2, with the left panel showing the top 50 nouns in bar charts and the right panel displaying the top 200 nouns in the vocabulary as word clouds. These statistics indicate that our dataset is open-domain and highly diverse, covering wide range of acoustic environments. Initial Camera Viewpoint: We analyze the spatial distribution of the camera viewpoint at the start of each clip. The heatmap in Fig. R3a shows the distribution of yaw and pitch angles, highlighting which regions of the spherical field of view are most commonly observed. Camera Rotation Trajectory: We report statistics on how the camera rotates along the viewpoint trajectories. Fig. R3b visualizes 500 sampled 3D rotation trajectories on unit sphere, where each curve represents temporally evolving viewing direction. The color along each curve transitions from light to dark, indicating progression through time, which allows us to observe how the camera moves across the field of view during the clip. B. GenDoP Method B.1. Frame-aligned Spatial Features In this section, we provide detailed explanation of how the frame-aligned spatial features mentioned in Section 4.3 (Conditional Spacetime Module, PE Spatial Feature) are pe Rmh, for obtained. The frame-aligned features {l, r}, are computed as: (cid:16) pe = Upsample MLP(cid:0)Flatten(Conv(Fpe+Ea))(cid:1)(cid:17) . (41) The detailed processing steps are presented in Algorithm 1. The spatial feature Fpe Rmpe1616hpe is extracted using the spatially tuned Perception Encoder [3], where mpe denotes the number of video frames (8 fps), hpe is the feature dimension, and 16 corresponds to the patch size. Two learnable spatial position embeddings, El and Er, are introduced for the left and right audio channels, respectively, to capture channel-specific spatial characteristics. Subsequently, series of convolutional and linear projections are applied to transform these features into frame-aligned feape Rmh, {l, r}, where corresponds to tures the target audio sequence length and is the hidden feature dimension. These frame-aligned features can then be merged with the synchronization feature Fsync to construct the global spacetime features Fsp, which serve to inject spatio-temporal information into the subsequent audio generation modules. 12 Algorithm 1 Frame-Aligned PE Feature Projection Input: PE features Fpe RbmpeHW hpe , batch size b, frames mpe, height H, width , channels hpe Output: Frame-aligned features pe Rbmh, {l, r} target audio length m, hidden dimension 1: Add learnable stereo position embeddings: pe Fpe + Ea pos 2: Channel-wise 1D convolution: pe SiLU(Conv1d(F pe)) 3: Spatial downsampling via strided convolution: pe SiLU(Conv2d(F pe, stride = 2)) // H/2 W/2 pe SiLU(Conv2d(F 4: Flatten spatial dimensions: pe, stride = 2)) // H/4 W/4 pe reshape(F pe, (b mpe, (H/4) (W/4), h)) 5: Gated convolution (ConvMLP): pe Conv1d2(SiLU(Conv1d1(F pe)) Conv1d3(F pe)) 6: Two-layer linear projection: pe Linear(SiLU(Linear(F a pe))) 7: Restore batch and temporal dimensions: pe reshape(F pe, (b, mpe, h)) 8: Temporal upsampling to audio length: pe Interpolate(F a pe, size = m, mode = linear) 9: return pe Rbmh, {l, r} C. Experiments C.1. Experimental Settings Model Configuration. All experiments are conducted by fine-tuning the large, 44.1kHz, v2 variant of MMAudio [6]. The model uses latent dimension of 40, with feature dimensions of 1024 for clip features, 768 for synchronization features, 1024 for PE spatial features, and 1024 for text embeddings. Input sequence lengths are 345 tokens for audio latent representations, 64 frames for video clip sequences, 192 for synchronization sequences, 64 for PE spatial feature sequences, and 77 tokens for text captions. Video clip and text sequences are averaged over the temporal dimension to yield single conditional feature vector, while other features are temporally aligned prior to fusion. patch size of 16 is applied during PE spatial feature extraction. The transformer backbone comprises 7 layers of multimodal joint transformer blocks and 13 the model In all experiments, 14 layers (28 blocks) of single-modal branch transformer blocks, with hidden dimension of 896 with 14 attention heads. Using the notation introduced in the preceding section: mpe = 64, hpe = 1024, = 345, = 896. Training Details. is trained on combined dataset comprising BiAudio and MUSIC [57, 58]. Specifically, the BiAudio training set is loaded along with the MUSIC training set, which is oversampled three times to balance the dataset. For textual captions are automatically the MUSIC dataset, generated by parsing instrument names from the corresponding audio file names. For instance, the video file duet acoustic guitar violin WeeRb3LMb8E.mp4 yields the caption acoustic guitar, violin. Inference Details. For the BiAudio dataset, during inference, we remove the Visible and Invisible labels used during training, retaining only the descriptions of the sounds to ensure fair comparison across methods. C.2. User Study Details The user study uses web-based questionnaire. Participants wear headphones on both ears to ensure proper spatial audio perception. Fig. R4 shows the subjective evaluation criteria presented to participants. It includes the five perceptual dimensions, Spatial Impression, Spatial Consistency, Temporal Alignment, Semantic Alignment, and Audio Realism, along with their specific scoring guidelines. Fig. R5 depicts the rating interface where participants evaluate five spatial audio samples within each videoaudio group. For each sample, participants provide ratings from 1 to 5 for all five perceptual dimensions. C.3. Additional Ablation Studies In this section, we present additional ablation studies to further investigate the contributions of different model components in our ViSAudio model and the BiAudio dataset. Tab. S1 presents the results of an ablation study on key model components. The table provides the complete objective metric results, serving as supplement to Tab. 4 in the main text. The findings highlight that the combination of both modules significantly improves performance. Additionally, we conduct an ablation study to evaluate the impact of our BiAudio dataset. We compare several variants of our model: (1) the model trained without BiAudio, using only the MUSIC dataset [57, 58]; (2) the model trained on both BiAudio and MUSIC. As shown in Tab. S2, the model trained with BiAudio performs significantly better on open-domain data, underscoring its importance in generating high-quality binaural spatial audio. D. Additional Cases supplementary video is provided, showcasing cases from the main text along with additional binaural spatial audio Figure R4. Subjective Evaluation Criteria. Screenshot of the questionnaire webpage shown to participants, presenting the five perceptual dimensions and their specific scoring guidelines used for the subjective evaluation of spatial audio generation. Figure R5. User Study Rating Interface. Screenshot of the questionnaire webpage used to collect participants MOS ratings across the five perceptual dimensions for each videoaudio group. 14 Table S1. Ablation Study on Key Model Components. We conduct an ablation study to evaluate the contributions of key model components: Dual, Dual-Branch Audio Generation (Sec. 4.2) and Spt, Conditional Spacetime Module (Sec. 4.3). Pretrained refers to our pretrained MMAudio [6] model, spatialized by duplicating the mono output into both channels for evaluation. Model Pretrained w/ Dual only Dual+Spt FDmix VGG FDavg 4.482 2.803 2.479 5.675 2.908 2.516 Audio Distribution Matching PANN FDavg VGG FDmix PANN KLavg PANN KLmix 1.837 1.845 1.572 1.646 1.638 1. 16.711 13.008 12.684 PANN DeSync IB-Score 0.285 0.289 0.299 0.793 0.766 0.788 18.341 15.151 13. Video-Audio Alignment Table S2. Ablation Study on Training Dataset. We conduct an ablation study to evaluate the contributions of our dataset. We train the model both without and with using our BiAudio dataset, and evaluate it on our test set. The results show that without our dataset, the model fails to generate high-quality binaural audio on open-domain data. Model FDmix VGG FDavg Audio Distribution Matching PANN FDavg VGG FDmix w/o BiAudio w/ BiAudio 10.966 2.516 12.677 2.479 43.711 13.917 Video-Audio Alignment PANN KLavg PANN KLmix 4.280 4.203 1.573 1.638 45.175 12.684 PANN DeSync IB-Score 0.102 0.299 0.916 0. cases generated in diverse acoustic environments from input video and optional text prompts. The audiovisual presentation allows the audience to better evaluate the models ability to produce realistic and spatially precise sound. The video is organized into the following sections for detailed demonstration: exclusively on visual frames to infer audio. When text captions are provided, they deliver explicit semantic guidance, enabling the generation of off-screen sounds. We present examples where sounds originate from off-screen sources, demonstrating the models capacity to infer and spatialize unseen audio events based on contextual cues. D.1. Dynamic Sound Sources D.4. Diverse Acoustic Environments We evaluate the model under diverse motion configurations to assess its spatiotemporal modeling capabilities. With fixed viewpoint, the model accurately localizes static objects and reliably tracks moving sound sources. Under dynamic camera motion, it maintains stable spatial perception of stationary sound sources. These experiments collectively demonstrate the models robustness in handling complex auditory scenarios involving both object and viewpoint dynamics, generating high-quality binaural audio with spatial immersion that adapts seamlessly to viewpoint changes and sound-source motion. D.2. Multiple Sound Sources We showcase the models capacity to handle multiple simultaneous sounds across three scenarios: identical sources at different positions, demonstrating precise spatial discrimination; two interacting sources, showing effective separation and localization; and complex ensembles with overlapping sources. These examples collectively validate the models ability to generate coherent auditory scenes with accurate spatial perception in complex sound environments. D.3. Invisible Sound Sources Our method supports both video-only and video-text multimodal conditioning. In video-only mode, the model relies The examples illustrate the models strong generalization ability across range of acoustic environments, including outdoor, underwater, and indoor scenes. It consistently produces high-quality binaural audio with convincing spatial immersion, validating its robustness in adapting to diverse acoustic environments. D.5. Cases in the Main Text The video also includes demonstrations of the results contained in the main text, namely the two cases from Fig. 1 and the case from Fig. 3."
        },
        {
            "title": "References",
            "content": "[1] Juan Isaac Engel Alonso-Martınez. Improving binaural audio techniques for augmented reality. 2021. 2 [2] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. See, CoRR, hear, and read: Deep aligned representations. abs/1706.00932, 2017. 3 [3] Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollar, and Christoph Feichtenhofer. Perception encoder: The best visual embeddings are not at the output of the network. CoRR, abs/2504.13181, 2025. 5, 12 [4] Yuanhong Chen, Kazuki Shimada, Christian Simon, Yukara Ikemiya, Takashi Shibuya, and Yuki Mitsufuji. Ccstereo: Audio-visual contextual and contrastive learning for binaural audio generation. arXiv preprint arXiv:2501.02786, 2025. 3 [5] Ziyang Chen, Prem Seetharaman, Bryan C. Russell, Oriol Nieto, David Bourgin, Andrew Owens, and Justin Salamon. Video-guided foley sound generation with multimodal controls. In CVPR, pages 1877018781. Computer Vision Foundation / IEEE, 2025. 2, 3 [6] Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander G. Schwing, and Yuki Mitsufuji. Mmaujoint dio: Taming multimodal training for high-quality In CVPR, pages 2890128911. video-to-audio synthesis. Computer Vision Foundation / IEEE, 2025. 3, 4, 5, 7, 8, 13, 15 [7] Xin Cheng, Xihua Wang, Yihan Wu, Yuyue Wang, and Ruihua Song. Lova: Long-form video-to-audio generation. In ICASSP, pages 15. IEEE, 2025. 2 [8] Rishit Dagli, Shivesh Prakash, Robert Wu, and Houman Khosravani. See-2-sound: Zero-shot spatial environment-tospatial sound. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters, pages 12, 2025. 2, 3, 6, 7 [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883. Computer Vision Foundation / IEEE, 2021. 2 [10] Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. In Forty-first International Conference on Machine Learning, 2024. [11] Ruohan Gao and Kristen Grauman. 2.5d visual sound. In CVPR, 2019. 2, 3, 4, 6 [12] Ruohan Gao and Kristen Grauman. 2.5 visual sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 324333, 2019. 2, 3 [13] Rishabh Garg, Ruohan Gao, and Kristen Grauman. Geometry-aware multi-task learning for binaural audio generation from video. arXiv preprint arXiv:2111.10882, 2021. 2, 3 [14] Rishabh Garg, Ruohan Gao, and Kristen Grauman. Visuallyguided audio spatialization in video with geometry-aware multi-task learning. Int. J. Comput. Vis., 131(10):27232737, 2023. 2, 3, [15] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and humanlabeled dataset for audio events. In ICASSP, pages 776780. IEEE, 2017. 6 [16] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind one embedding space to bind them all. In CVPR, pages 1518015190. IEEE, 2023. 7 [17] GoogleChrome. Omnitone: Spatial audio rendering on the web, 2016. 11 [18] Emil Hoeg, Lynda Gerry, Lui Thomsen, Niels Nilsson, and Stefania Serafin. Binaural sound reduces reaction time in virtual reality search task. In 2017 IEEE 3rd VR workshop on sonic interactions for virtual environments (SIVE), pages 14. IEEE, 2017. [19] Vladimir Iashin and Esa Rahtu. Taming visually guided In BMVC, page 2. BMVA Press, 2021. sound generation. 2 [20] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP, pages 53255329. IEEE, 2024. 5, 7 [21] Haruya Ishikawa. Pyequilib: Processing equirectangular images with python, 2021. 10 [22] Jaeyeon Kim, Heeseung Yun, and Gunhee Kim. Visage: Video-to-spatial audio generation. In ICLR. OpenReview.net, 2025. 2, 3, 4, 6, 7, 9 [23] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 4 [24] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D. Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:28802894, 2019. 6 [25] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 5 [26] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: universal neural vocoder with large-scale training. In ICLR. OpenReview.net, 2023. 4 [27] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR. OpenReview.net, 2023. 3, 4 [28] Huadai Liu, Tianyi Luo, Qikai Jiang, Kaicheng Luo, Peiwen Sun, Jialei Wang, Rongjie Huang, Qian Chen, Wen Wang, Xiangtai Li, Shiliang Zhang, Zhijie Yan, Zhou Zhao, and Wei Xue. Omniaudio: Generating spatial audio from 360degree video. CoRR, abs/2504.14906, 2025. 2, 3, 9 [29] Huadai Liu, Jialei Wang, Kaicheng Luo, Wen Wang, Qian Chen, Zhou Zhao, and Wei Xue. Thinksound: Chain-ofthought reasoning in multimodal large language models for audio generation and editing. CoRR, abs/2506.21448, 2025. 3, 4, 6, [30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR. OpenReview.net, 2023. 3 [31] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. In NeurIPS, 2023. 2 [32] Robert McCann. convexity principle for interacting gases. Advances in mathematics, 128(1):153179, 1997. 4 [33] Xinhao Mei, Varun Nagaraja, Gael Le Lan, Zhaoheng Ni, Ernie Chang, Yangyang Shi, and Vikas Chandra. Foleygen: Visually-guided audio generation. In MLSP, pages 16. IEEE, 2024. 16 [34] Pedro Morgado, Nuno Nvasconcelos, Timothy Langlois, and Oliver Wang. Self-supervised generation of spatial audio for 360 video. Advances in neural information processing systems, 31, 2018. 2, 3 [35] Pedro Morgado, Nuno Vasconcelos, Timothy R. Langlois, and Oliver Wang. Self-supervised generation of spatial audio for 360 video. In NeurIPS, pages 360370, 2018. 4, 9 [36] Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, and Joan Serr`a. Masked generative video-to-audio transformers with In ECCV (87), pages 247264. enhanced synchronicity. Springer, 2024. 2 [37] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with general conditioning layer. In AAAI, 2018. 6 [38] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam S. Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali K. Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dmitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. CoRR, abs/2410.13720, 2024. 3 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 5 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10674 10685. IEEE, 2022. [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 2, 3 [42] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. Codi-2: In-context, interleaved, and interactive any-to-any generation. In CVPR, pages 2741527424. IEEE, 2024. 2, 3 [43] Zeyue Tian, Yizhu Jin, Zhaoyang Liu, Ruibin Yuan, Xu Tan, Qifeng Chen, Wei Xue, and Yike Guo. Audiox: Diffusion transformer for anything-to-audio generation. arXiv preprint arXiv:2503.10522, 2025. 4, 6, 7 [44] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Trans. Mach. Learn. Res., 2024, 2024. 3, 4 [45] Arun Balajee Vasudevan, Dengxin Dai, and Luc Van Gool. Semantic object prediction and spatial sound superresolution with binaural sounds. In ECCV (4), pages 638 655. Springer, 2020. 2, 3, 4 [46] Ilpo Viertola, Vladimir Iashin, and Esa Rahtu. Temporally In ICASSP, aligned audio for video with autoregression. pages 15. IEEE, 2025. 2 [47] Jun Wang, Xijuan Zeng, Chunyu Qiang, Ruilong Chen, Shiyao Wang, Le Wang, Wangjing Zhou, Pengfei Cai, Jiahui Zhao, Nan Li, Zihan Li, Yuzhe Liang, Xiaopeng Wang, Haorui Zheng, Ming Wen, Kang Yin, Yiran Wang, Nan Li, Feng Deng, Liang Dong, Chen Zhang, Di Zhang, and Kun Gai. Kling-foley: Multimodal diffusion transformer for high-quality video-to-audio generation. CoRR, abs/2506.19774, 2025. 2, 3 [48] Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. Frieren: Efficient video-to-audio generation network with rectified flow matching. In NeurIPS, 2024. 3 [49] Siyi Xie, Hanxin Zhu, Tianyu He, Xin Li, and Zhibo Chen. Sonic4d: Spatial audio generation for immersive 4d scene exploration. arXiv preprint arXiv:2506.15759, 2025. 2, 3 [50] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visualIn CVPR, audio generation with diffusion latent aligners. pages 71517161. IEEE, 2024. 2 [51] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. 3, 10, [52] Manjie Xu, Chenxing Li, Yong Ren, Rilin Chen, Yu Gu, Wei Liang, and Dong Yu. Video-to-audio generation with hidden alignment. CoRR, abs/2407.07464, 2024. 2 [53] Xudong Xu, Hang Zhou, Ziwei Liu, Bo Dai, Xiaogang Wang, and Dahua Lin. Visually informed binaural auIn Proceedings of dio generation without binaural audios. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1548515494, 2021. 2, 3 [54] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang 17 Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 3, 10, 11 [55] Yuhuan You, Xihong Wu, and Tianshu Qu. TA-V2A: textually assisted video-to-audio generation. In ICASSP, pages 15. IEEE, 2025. 2, [56] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. CoRR, abs/2407.01494, 2024. 2, 3 [57] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In The European Conference on Computer Vision (ECCV), 2018. 2, 3, 4, 6, 13 [58] Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba. The sound of motions. In Proceedings of the IEEE International Conference on Computer Vision, pages 1735 1744, 2019. 2, 3, 4, 6, 13 [59] Lei Zhao, Rujin Chen, Chi Zhang, Xiao-Lei Zhang, and Xuelong Li. Foleyspace: Vision-aligned binaural spatial audio generation. arXiv preprint arXiv:2508.12918, 2025. 2, 3 [60] Hang Zhou, Xudong Xu, Dahua Lin, Xiaogang Wang, and Ziwei Liu. Sep-stereo: Visually guided stereophonic audio In European generation by associating source separation. Conference on Computer Vision, pages 5269. Springer, 2020. 2, 3 [61] Franz Zotter and Matthias Frank. Ambisonics: practical 3D audio theory for recording, studio production, sound reinforcement, and virtual reality. Springer Nature, 2019."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "Stanford University",
        "The Chinese University of Hong Kong",
        "Zhejiang University"
    ]
}