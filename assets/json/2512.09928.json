{
    "paper_title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
    "authors": [
        "Minghui Lin",
        "Pengxiang Ding",
        "Shu Wang",
        "Zifeng Zhuang",
        "Yang Liu",
        "Xinyang Tong",
        "Wenxuan Song",
        "Shangke Lyu",
        "Siteng Huang",
        "Donglin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 8 2 9 9 0 . 2 1 5 2 : r HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models Minghui Lin1, Pengxiang Ding1,2, Shu Wang1, Zifeng Zhuang1,2, Yang Liu1,2, Xinyang Tong1, Wenxuan Song3, Shangke Lyu4, Siteng Huang2, Donglin Wang1,5 1Westlake University 2Zhejiang University 3HKUST(GZ) 4Nanjing University 5Westlake Robotics linminghui@westlake.edu.cn, siteng.huang@gmail.com (cid:128) https://hifvla.github.io https://github.com/OpenHelix-Team/HiF-VLA"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through hindsight-modulated joint expert to enable think-whileacting paradigm for long-horizon manipulation. As result, HiF-VLA surpasses strong baselines on LIBEROLong and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiFVLA achieves substantial improvements in real-world longhorizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings. 1. Introduction Vision-Language-Action models (VLAs) [4, 15, 16, 19, 50, 53] have emerged as promising framework for robotic manipulation, leveraging powerful Vision-Language Models (VLMs) [1, 2, 10, 18, 34] to map visual and language representations to the action space. Despite progress, most VLAs implicitly assume Markov property, predicting actions solely from the current observation without explicitly modeling temporal dependencies. This simplification leads Corresponding author. to form of temporal myopia. However, long-horizon manipulation requires reasoning that extends beyond the present, maintaining temporal continuity across visual, language, and motor modalities. Without such temporal reasoning, dependencies between consecutive actions deteriorate, resulting in fragmented trajectories and diminished task-level coherence. Recent efforts [26, 35, 51] have sought to alleviate this temporal myopia by incorporating historical context, most commonly by stacking multiple past observation frames. However, this approach is fundamentally limited. Stacking raw frames is not only computationally prohibitive and increases inference latency, hindering real-time control (see Tab. 3), but it also introduces substantial pixel-level redundancy. This deluge of static information often obscures the salient, task-relevant dynamics, making it difficult for the model to distinguish meaningful changes from background noise. We argue that more precise and efficient representation of history is not the raw visual content of the past, but the motion that transpired between states. Motion serves as direct and compact proxy for memory, faithfully capturing the dynamics of interactions (such as an object being moved or drawer being closed), while discarding redundant static information. This makes motion an ideal primitive for representing history in way that is both expressive and computationally efficient. Building on the principle of motion as compact representation of history, we argue that robust decision-making requires bidirectional temporal reasoning, connecting the past to the future. An intelligent agent must possess hindsight, the ability to interpret recent dynamics that led to its current state, grounding its decisions in verified past outcomes. At the same time, it must exhibit foresight, the capability to anticipate plausible future dynamics, enabling proactive, goal-directed behavior rather than purely reactive responses. We identify motion as the natural bridge unifying these two temporal dimensions. Consequently, we 1 Figure 1. (a-b) Comparison with existing methods: VLAs rely on instantaneous observations [19, 20] (a-top), stack multiple past frames [26, 36] (a-second), or generate pixel-level subgoals [46, 49] (a-third), suffering from redundancy, high inference cost, and weak structure. In contrast, HiF-VLA (a-bottom) jointly models Hindsight, Insight, and Foresight, expanding the temporal receptive field bidirectionally for compact, structured, and efficient reasoning. (c) HiF-VLA reduces inference latency and achieves state-of-the-art performance on LIBERO-Long and CALVIN ABC-D, significantly outperforming the baseline in real-world experiments. propose HiF-VLA (Hindsight, Insight, and Foresight for VLA Models), unified framework that leverages motion to structure this reasoning process. As detailed in Fig. 2, HiF-VLA comprises three components: 1) Hindsight prior acquisition: encodes historical frames into structured, lowdimensional motion vectors [17] as hindsight, preserving dynamics without redundant pixels. 2) Foresight reasoning with insight: interprets task instructions and current observations to anticipate plausible foresight motions and latent action tokens. 3) Hindsight-modulated joint expert: applies hindsight as top-down constraint on foresight and action streams, which interact within the expert decoder to generate temporally coherent actions. As shown in Fig. 1, compared to methods that rely solely on frame stacking or subgoal prediction, HiF-VLA more effectively bridges perception, dynamics, and control within unified representational space. This design enables an embodied think-while-acting paradigm, enhancing robustness, temporal continuity, and causal consistency in long-horizon manipulation tasks. Our main contributions are summarized as follows: We propose HiF-VLA, VLA framework endowed with bidirectional spatio-temporal completion. By incorporating motion vectors as structured, low-dimensional temporal primitives, HiF-VLA explicitly expands the temporal receptive field, enabling temporally consistent and efficient action prediction while reducing redundancy. We propose hindsight-modulated joint expert that unifies temporal and action representations within unified space, enabling think-while-acting paradigm for causally consistent and temporally coherent long-horizon motion generation. Extensive experiments demonstrate that our approach achieves substantial performance gains on widely adopted long-horizon benchmarks, while exhibiting strong temporal scalability and high inference efficiency. 2. Related Work Vision-Language-Action Models. VLA models learn endto-end mappings from language instructions and visual observations to low-level actions. Prior works can be broadly grouped by their action policies. Diffusion-based methods such as RDT-1B [27], CogACT [23], and DexVLA [39] employ iterative denoising to synthesize continuous control trajectories; autoregressive approaches like RT-2 [53] and OpenVLA [19] discretize continuous actions into tokens and predict them sequentially using VLM backbones; regression-oriented systems such as OpenVLA-OFT [20], VLA-Adapter [37] and SF [22] adopt bidirectional attention to directly learn continuous actions via L1 regression. However, most VLA models do not explicitly model temporal dependencies. They often neglect the importance of historical information and reasoning capabilities through VLMs, both of which have been shown to substantially improve 2 performance in long-horizon video tasks [9, 12, 32]. Temporal Modeling and Inference in Robotics. Temporal modeling and foresight reasoning have been extensively studied in long video understanding and generation, but remain relatively underexplored in robotic manipulation. Existing approaches largely focus on one-sided reasoning. One line of work incorporates historical frames as visual prompts within the VLM input context, as in TraceVLA [51], Octo [35], GR-2 [8], RoboVLMs [26]. Another emphasizes foresight by predicting future visual subgoals to guide action generation, such as CoT-VLA [49], Seer [36], UniVLA [38], and UP-VLA [46], which typically condition an inverse dynamics model (IDM) on predicted future subgoals to infer actions. However, frame-level historical encoding and pixel-level future prediction introduce substantial computational overhead and redundancy, while failing to capture the importance of bidirectional temporal information. To address these, we propose HiF-VLA, which efficiently unifies hindsight, insight, and foresight within single framework, enabling temporally coordinated decision-making for action generation. 3. Method 3.1. Preliminary We begin by defining the setting and notation for VLA reasoning. Robot expert demonstrations are denoted as Dr = (l, a1...T , o1...T ). At each time step t, the model receives an observation ot and textual instruction l, and the objective is to predict an action chunk at:t+n over horizon of length n. vanilla VLA Pθ [20] builds on visionlanguage models (VLMs), learning from diverse manipulation demonstrations to transfer vision-language understanding and generation capabilities to embodied scenarios. Formally: at:t+n Pθ (cid:0)at:t+n ot, l(cid:1). (1) Our key insight is to expand the temporal receptive field available prior to action execution by compensating sparse visual observations. To this end, we propose HiF-VLA, as illustrated in Fig. 2, unified framework built upon vanilla VLA architecture, which incorporates compact historical information prior mhis th:t of length and additionally predicts future motion mt:t+n conditioned on the current insight. Formally, the inference process can be expressed as: 3.2. Hindsight Prior Acquisition Relying solely on instantaneous visual observations for action decision-making often fails to provide stable perception, especially under challenging conditions such as occlusion or repeated executions. Hence, capturing consistent dynamic patterns of the manipulator from historical states is critical. Existing approaches [8, 35, 36] typically maintain sliding window of past observations and concatenate them with the current observation to form global representation. However, this frame-stacking strategy introduces significant redundancy: the high similarity between adjacent frames makes it difficult for the model to focus on taskrelevant dynamics, potentially dispersing its attention. To address this, we introduce compressed historical prior: Motion Vectors (MVs) [17]. In video codec standards like H.264 [41] and MPEG-4 [21], MVs predict the displacements of macroblocks between adjacent frames to avoid pixel-wise redundancy. Importantly, MVs are not coarse approximation: combined with keyframes, they enable near-lossless video reconstruction and maintain high compression. This property provides natural, efficient, and faithful solution for capturing historical dynamics. Formally, let (x, y) denote the position of macroblock in an image (size 3). The motion vector is defined as: Vt1:t(x, y) = (xt xt1, yt yt1), (3) where (xt, yt) and (xt1, yt1) denote the positions of the macroblock in consecutive frames ot and ot1. We adopt MPEG-4 to extract keyframes and motion information, regarding the current observation ot at time step as the keyframe, while maintaining historical window of length m, forming GOP (Group of Pictures) units, GOP = [M Vtm:tm+1, ..., Vt2:t1, Vt1:t, ot]. Here, MVs follow the MPEG-4 1616 macroblock layout and are represented as tensor of size (H//16) (W//16) 2, compactly encoding the historical motion trajectories of entities in the scene. Compared to raw frames, this representation significantly reduces redundancy while retaining taskrelevant dynamics, thereby providing structured spatiotemporal priors to the decision layer. Then, we adopt lightweight ViT-based [11] hindsight encoder, combined with shallow 3D convolutions, to encode hindsight motions into compact hindsight tokens Mh RKhd. (at:t+n, mt:t+n)P θ(at:t+n, mt:t+not, l, mhis th:t). (2) 3.3. Foresight Reasoning with Insight During training, we augment the input with historical information and jointly predict both the future n-step motion and the corresponding actions. During inference, motion decoding is optional and can be omitted depending on downstream task requirements. Beyond historical cues, foresight is equally critical for robot tasks. It allows the agent to evaluate the consequences of its actions before execution, promoting more reliable action decision-making. Existing approaches, such as CoTVLA [49] and UP-VLA [46], achieve visual CoT reasoning by generating future visual subgoals. However, these 3 Figure 2. HiF-VLA Pipeline. (a) In Hindsight Prior Acquisition (see Sec. 3.2), HiF-VLA encodes dense historical frame sequences into compact Motion Vector (MV) streams, forming structured hindsight primitives that capture temporal dynamics without pixel redundancy. (b) In Foresight Reasoning with Insight (see Sec. 3.3), the VLM interprets the task instruction and current observation to infer plausible foresight motions and corresponding latent action tokens. (c) Finally, the Hindsight-Modulated Joint Expert (see Sec. 3.4) fuses hindsight, foresight, and action representations within unified latent space, producing temporally consistent and causally coherent action predictions. approaches depend on pixel-level predictions prone to local distortions and semantic drift. The resulting dense and redundant subgoals obscure task-relevant dynamics, while the reliance on discrete frame prediction (without continuous temporal modeling) further undermines temporal consistency in complex environments. To address these limitations, we propose an efficient foresight reasoning mechanism guided by current insight, as shown in Fig. 2(b). Instead of predicting raw future pixels, we employ more general and structured MVs as spatiotemporal targets for future action execution. MVs compactly encode the trajectory evolution of the manipulator within the scene, effectively reducing pixel redundancy while providing structured spatial prior. 1 , qf 1 , qa 2 , ..., qf Kf 2 , ..., qa Ka Specifically, we introduce Kf learnable foresight query tokens {qf }, together with Ka empty action tokens {qa }, into the VLM embedding space. These tokens are concatenated with the original inputs (including the task instruction l, the current observation ot), and then fed into VLM Fθ, enabling parallel reasoning over continuous visual dynamics and action generation, resulting in foresight motion tokens Mf RKf and action latent tokens Af RKad. Formally, the reasoning process can be expressed as: (Mf , Af ) = Fθ(ot, l). We aim for the model to reason about visual foresight and action execution in parallel, and subsequently integrate and refine information from these distinct reasoning streams. This design enriches the diversity of the VLMs internal thought process and unlocks the potential of parallel reasoning. 3.4. Hindsight-Modulated Joint Expert Conventionally, the action expert receives high-level representations from the VLM and translates them into low-level control signals. However, predicting actions in isolation lacks reasoning about the future dynamics. Motion, as the physical manifestation of actions in the visual space, provides complementary information: jointly predicting both motion and actions enables VLA models to better align semantic understanding with underlying dynamics. To achieve such synergistic reasoning, we propose the Hindsight-Modulated Joint Expert, which jointly models action and motion as two complementary streams within shared temporal latent space and where historical information is introduced as conditional prior to guide future inference and suppress erroneous replay of historical trajectories. Importantly, we integrate historical motion priors as adaptive temporal conditions, rather than embedding them directly into the VLM input. Injecting extra history motion into the VLM risks disrupting the established alignment between visual and language modalities [48] (see Fig. 4). Instead, historical motion tokens Mh are encoded into compact conditional representation and modulate the joint reasoning process via Adaptive Layer Normalization (AdaLN) conditioning, where layered modulation and regularization jointly constrain future motion-action patterns. Formally, we define three types of sequence represen-"
        },
        {
            "title": "Method",
            "content": "Avg. SR Put soup and box in basket Put box and butter in basket Turn on stove and put pot Put bowl in drawer and close Put mugs on left and right plates Pick book and place it in back Put mug on plate, pudding right Put soup and sauce in basket Put both pots on stove Put mug in microwave and close Policy inputs: third-view image, language instruction OpenVLA [19] UniVLA* [24] MemoryVLA [31] OpenVLA-OFT* [20] HiF-VLA(Ours) 54.0 63.0 93.4 91.0 94.4 35.0 64.0 92.0 82.0 94.0 95.0 82.0 96.0 96.0 98.0 65.0 76.0 96.0 96.0 100 45.0 96.0 100 94.0 Policy inputs: third-person image, wrist-view image, language instruction Seer (scratch) [36] Seer [36] UniVLA* [24] OpenVLA-OFT* [20] HiF-VLA(Ours) 78.7 87.7 90.0 94.0 96.4 80.0 91.7 100 90.0 88.0 90.0 90.0 92.0 98.0 98.0 91.7 98.3 94.0 98.0 81.7 100 98.0 98.0 100 40.0 58.0 100 90.0 94.0 85.0 91.7 86.0 96.0 100 80.0 98.0 100 96.0 100 65.0 93.3 100 100 100 60.0 24.0 96.0 92.0 90. 86.7 85.0 80.0 92.0 96.0 45.0 74.0 96.0 100 98.0 88.3 88.3 100 100 100 20.0 32.0 62.0 70.0 76.0 51.7 61.7 70.0 72.0 82.0 55.0 26.0 96.0 94.0 94. 66.7 71.7 82.0 96.0 100 Table 1. Performance comparison on the LIBERO-Long benchmark. We report the average success rate (%) across 10 tasks with Avg. SR. Results marked with were reproduced using the official open-source code. Bold indicates the best performance. tations: hindsight motion tokens Mh RKhd (used only as conditional input), foresight motion latent tokens Mf RKf d, and action tokens Af RKad. As shown in Fig. 2(c), the foresight motion tokens and action tokens form two parallel streams, which interact via cross-stream joint attention while retaining separate FFNs to ensure complementary yet disentangled representations. The hindsight tokens Mh are projected through linear layer to obtain the conditioning vectors hc, which is then injected into each joint expert module via AdaLN to modulate both foresight and action representations {Mf , Af }: Mf , Af = JointExpert(Mf , Af hc), AdaLN(z; hc) = γ(hc) µ(z) σ(z) + β(hc), (4) (5) where σ(z) and µ(z) are the mean and standard deviation of z, γ(hc) and β(hc) are modulation parameters from hc. The Joint Attention employs non-causal self-attention over sequence formed by concatenating Mf and Af , from which Q, K, and are jointly projected. Finally, the fused motion and action representations are projected through their respective heads to generate the future motions mt:t+n and the actions at:t+n. In general, the hindsight-modulated joint expert not only explicitly models the complementary relationship between action and motion, but also employs historical conditioning regularization to guide temporally consistent and physically plausible behaviors, thereby enhancing the stability and causal consistency of action. 3.5. Unified HiF-VLA Model Input Representation. Following OpenVLA-OFT [20], we feed the current observation into both the DINOv2 [29] and SigLIP [45] visual encoders to obtain hybrid visual embedding. At the same time, we initialize set of learnable foresight motion queries and empty action tokens. These tokens are concatenated with the instruction embedding and the current observation embedding to form the multi-modal input sequence. In parallel, the historical motion sequence is compressed via spatial-temporal convolution, aggregated by ViT encoder, and projected into the latent space of the joint expert as the hindsight prior. Parallel Decoding and Joint Expert. The VLM adopts non-causal attention mask, enabling joint prediction of future motion latents and action latents. The three streams of tokens (hindsight, foresight, predicted action) are then fused via the Hindsight-Modulated Joint Expert. Training Objective. We now describe in detail how we train the model to predict both actions and motion. To ensure both action and motion predictions remain wellcalibrated, we define two L1-loss objectives: LM = 1 (cid:88) j= mt+j mt+j, LA = 1 (cid:88) j=1 at+j at+j. The overall loss combines the two objectives: Lall = LA + λ LM , (6) (7) where λ is balancing factor between action accuracy and motion reconstruction quality, set to 0.01. 4. Experiments In this section, we design experiments to address the following research questions (RQs): RQ1: How does HiF-VLA perform compared to SOTA methods on challenging long-horizon benchmarks?"
        },
        {
            "title": "Method",
            "content": "1 2 3 4 5 Avg. Len. Third View Multi-View SuSIE [5] OpenVLA [19] CLOVER [6] VPP [13] π0 [4] UniVLA [7] HiF-VLA(Ours) GR-1 [43] Vidman [40] π0 [4] UP-VLA [46] OpenVLA-OFT [20] RoboVLMs [26] Seer [36] VPP [13] HiF-VLA(Ours) 87.0 91.3 96.0 90.9 93.7 95.5 93.5 85.4 91.5 93.8 92.8 96.3 98.0 96.3 96.5 98. 69.0 77.8 83.5 81.5 83.2 85.8 87.4 71.2 76.4 85.0 86.5 89.1 93.6 91.6 90.9 94.1 49.0 62.0 70.8 71.3 74.0 74.8 81.4 59.6 68.2 76.7 81.5 82.4 85.4 86.1 86.6 88.1 38.0 52.1 57.5 62.0 62.9 66.9 75.9 49.7 59.2 68.1 76.9 75.8 77.8 80.3 82.0 81. 26.0 43.5 45.4 51.8 51.0 56.5 69.4 40.1 46.7 59.9 69.9 66.5 70.4 74.0 76.9 73.1 2.69 3.27 3.53 3.58 3.65 3.80 4.08 3.06 3.42 3.92 4.08 4.10 4.25 4.28 4.33 4.35 Table 2. Performance comparison on the CALVIN ABC-D benchmarks. We report the average number of successfully completed tasks across five consecutive instructions. Bold indicates the best performance. RQ2: Does HiF-VLA effectively mitigate the redundancy and inefficiency issues in conventional approaches? RQ3: How well does HiF-VLA maintain inference scalability as the temporal horizon increases? RQ4: Through ablation studies, how do different components contribute to HiF-VLAs overall performance? RQ5: Can HiF-VLA successfully handle long-horizon tasks on real-world robotic platforms? 4.1. Overall Performance Experimental Setups. We evaluate HiF-VLA on two long-horizon benchmarks. LIBERO-Long [25] comprises ten multi-subgoal manipulation tasks across diverse scenes. CALVIN ABC-D [28] includes four indoor environments (A-D); policies are trained on A-C and evaluated on the unseen to assess generalization on consecutive tasks. All experiments are conducted under two settings following [20]: third-view setup using the primary camera, or multiview setup using both the primary and wrist cameras. Implementation Details. We adopt Prismatic-7B [18] as the VLM backbone, and initialize it with weights from OpenVLA [19], which were pretrained on OXE [30]. All other modules are randomly initialized. Training is performed on 8 NVIDIA A100 GPUs with global batch size of 64. fixed temporal chunk of n=8 is adopted for both action and foresight modeling, while the hindsight window is variable (default 8). Fine-tuning is conducted for 150k steps on LIBERO and 80k on CALVIN. The main baseline is OpenVLA-OFT [20], which shares the same pretrained initialization. Additional comparisons include Seer [36], VPP [13], and π0 [4], etc. Result Analysis. 1) LIBERO-Long: As shown in Tab. 1, we present the detailed performance of HiF-VLA across 10 tasks in the LIBERO-Long benchmark. We evaluate both third-view and multi-view inputs over 500 trials. Compared to the baseline third-view method, our approach achieves 94.4% success rate, representing 3.4% absolute improvement. Notably, our third-view variant performs on par with multi-view baselines, underscoring HiF-VLAs robust temporal reasoning capability. Moreover, our method achieves 96.4% success rate under the multi-view setting, consistently outperforming other state-of-the-art VLA models. 2) CALVIN-ABC-D: We further evaluate the generalization capability of HiF-VLA, which was trained on the ABC dataset and evaluated in the environment. As shown in Tab. 2, our method surpasses the baseline by 0.25 in terms of the average task length metric and achieves superior performance under both third-view and multi-view settings. These results clearly answer RQ1. This improvement can be attributed to the bidirectional temporal perception and reasoning architecture incorporated in our model, which enables more effective understanding of long-term action dependencies and consequently leads to enhanced adaptability and robustness in complex long-horizon tasks. 4.2. Efficiency and Redundancy Analysis Experimental Setup. We conduct experiments exclusively on LIBERO-Long to evaluate the efficiency and redundancy aspects, and the results are presented in Tab. 3. For the baseline (1), we adopt the method proposed by [20]. (2) + Subgoal and (4) + History Frames correspond to variants that incorporate RGB-based prediction and historical infor- (a) Historical Frame Example (b) Inference Efficiency (c) Effect of Hindsight Figure 3. Effect of hindsight length on performance and efficiency. (a) Example of historical frames. (b) HiF-VLA maintains low inference latency as hindsight length increases. (c) Performance of hindsight of different lengths in third-view and multi-view perspectives. Methods (1) Baseline (2) (3) + Subgoal + Foresight (Ours) + History frames + Hindsight (Ours) (4) (5) (6) ciency, effectively addressing RQ2. Peak GPU Memory(GB) Latency (ms) 30.8 (1.00) 38.2 (1.24) 31.8 (1.03) 63.6 (2.06) 31.4 (1.02) 72.9 (1.00) 115.9 (1.59) 82.7 (1.13) 229.5 (3.15) 117.7 (1.61) Avg. SR 91.0 91.8 92.2 90.4 92.2 + Hindsight + Foresight (Ours) 32.2 (1.05) 121.6 (1.67) 93. Table 3. Performance comparison between multi-frame baselines variants and HiF-VLA variants. Peak GPU memory (GB) during training and inference latency (ms) are reported. The history and hindsight lengths are fixed to 4 for fair and computationally feasible comparison across all methods. mation following [8, 49]. (3) + Foresight (Ours) and (5) + Hindsight (Ours) denote variants that introduce Motionbased foresight and historical information, respectively. (6) + Hindsight + Foresight (Ours) integrates both types of Motion-based information simultaneously. Note that we used third-person input, hindsight length of 4, and batch size of 4 in all experiments. Result Analysis. As shown in Tab. 3(2) and (3), incorporating subgoal or foresight prediction improves task success rates; however, subgoal-based methods incur substantial latency overhead (1.59 slower than the baseline). In contrast, our foresight head introduces negligible additional cost (0.13 latency and 0.03 GPU Memory). Moreover, as illustrated in Tab. 3(4), dense multi-frame inputs significantly slow down inference (229.5 ms, 3.15 slower than the baseline) and even degrade performance, suggesting that redundant pixel information dilutes task-relevant temporal cues and may cause overfitting to visually irrelevant details. To address this, HiF-VLA replaces dense RGB inputs with compact motion representations, allowing the model to focus on dynamic and task-relevant cues, thereby improving both efficiency and accuracy. Furthermore, unified integration of hindsight and foresight further enhances overall performance. These results highlight HiF-VLAs clear advantages in reducing redundancy and improving inference effi- (a) Injecting VLM (b) Conditioning Decoder (c) Results Figure 4. Performance comparison on different hindsight embedding locations. (a) represents direct injection into the VLM, and (b) represents conditional embedding as an expert decoder. (c) shows the performance of both on LIBERO-Long. Mh denotes the hindsight tokens, Mf represents the foresight tokens and Af is the action tokens. 4.3. Inference Scalability Experimental Setup. We evaluate the inference efficiency of HiF-VLA against two multi-frame baselines [20]: (i) multi-frame extension with stacked past observations, and (ii) multi-frame history combined with single-frame subgoal prediction. For each model variant, we conduct 100 inference runs on an NVIDIA A100 GPU and report the average latency (defined as the time required to generate one action chunk) in the LIBERO-Long simulation. Results Analysis. As shown in Fig. 3b, the latency of multi-frame baselines increases almost linearly with the length of history, reflecting their high computational sensitivity to frame stacking. For instance, at history length of 8, the baseline incurs over 4.5 higher latency compared to the vanilla VLA. In contrast, HiF-VLA maintains consistently low computational overhead across all context lengths, with latency increasing only marginally as history grows, demonstrating superior scalability with respect to temporal context length, directly addresses RQ3. 7 Figure 5. Real-world long-horizon tasks. (a) We deploy our system on the AgileX Piper robotic arm equipped with an external scene camera (Intel RealSense D435) and wrist-mounted camera. (b) We design three long-horizon tasks covering diverse primitives such as pick, put, cover, stack, and press, emphasizing temporal consistency in action generation. 4.4. Ablation Studies Experimental Setup. To address RQ4 regarding the optimal integration of historical information, the effects of hindsight length and historical embedding positions are investigated on the LIBERO-Long. Hindsight Length. We examine the impact of hindsight length in both third-view and multi-view settings. As illustrated in Fig. 3c, the model achieves peak performance 94.4% and 96.4% when the hindsight length is set to 8. We believe, in the LIBERO-Long benchmark, most manipulation sequences exhibit moderate temporal dependencies, where 8 hindsight lengths are sufficient to capture meaningful causal cues without introducing redundant information. Hindsight Embedding Position. We study how the embedding strategy of hindsight information affects model performance. Specifically, we compare two variants: (i) naıvely concatenating hindsight with language and vision as direct inputs to the VLM, and (ii) conditioning hindsight within the expert module, which is adopted in our implementation. As shown in Fig. 4, the expert-conditioned embedding consistently achieves higher success rates than VLM-based embeddings. We attribute this to the fact that motion-based hindsight may interfere with the pretrained alignment between visual and language representations. In contrast, injecting hindsight at the decoding stage provides more direct, residual-like path for motion information. This allows the low-level dynamics encoded in the hindsight to guide future action prediction without passing through, and potentially being corrupted by the VLMs semantic fusion layers. 4.5. Real-world Experiments Experiment setups. To evaluate the effectiveness of our approach in real-world applications, we conduct real-world experiments using the AgileX Piper robot. As shown in Fig. 5(a), RealSense D435 camera captures the scene from third-view, while an additional USB camera is mounted on the robots wrist for egocentric observations. We collect three long-horizon tasks, each with 100 demonstrations involving diverse manipulation primitives, including pick, place, stack, cover, and press. Real-world evaluation. For real-world environments, we train each model separately for every task and evaluate performance by averaging success rates over 20 trials per task. These long-horizon tasks require the model to maintain action consistency across stages, correctly associate time states. As shown in Fig. 5(b), the baseline model (OpenVLA-OFT) performs poorly across these longhorizon tasks for example, achieving only 17.4% success on Press-Buttons-Order, often failing to complete required button presses. This is likely due to the minimal visual difference between pressed and unpressed states, making it difficult for the baseline to detect successful actuation. In contrast, HiF-VLA benefits from its broad temporal receptive field, enabling reliable detection of subtle state transitions and robust execution of long-horizon tasks, resulting in superior performance across real-world tasks. 5. Conclusion This work introduces HiF-VLA, an efficient and unified framework for temporal perception and reasoning built upon low-dimensional, structured motion vectors. By integrating hindsight, insight, and foresight cues, HiFVLA establishes bidirectional temporal expansion over sparse visual receptive field, enabling the robot to capture task-critical dynamics at minimal computational cost and thereby improving temporal consistency and causal coherence in long-horizon tasks. Across both simulated and realworld long-horizon manipulation benchmarks, HiF-VLA demonstrates strong performance. Limitations. The current motion representation remains dependent on estimation accuracy and may be sensitive to noise in highly dynamic scenes; We leave the exploration of large-scale pre8 training on internet videos to enhance motion understanding and generation capabilities for future work."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1 [2] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. PaliGemma: versatile 3B VLM for transfer. arXiv preprint arXiv:2407.07726, 2024. 1 [3] Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open arXiv foundation model for generalist humanoid robots. preprint arXiv:2503.14734, 2025. 2 [4] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1, 6, 2 [5] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Rich Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pre-trained In Proceedings of the image-editing diffusion models. International Conference on Learning Representations, 2024. 6 [6] Qingwen Bu, Jia Zeng, Li Chen, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, and Hongyang Li. Closed-loop visuomotor control with generative expectation for robotic manipulation. In Proceedings of the Advances in Neural Information Processing Systems, pages 139002139029, 2024. [7] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. UniVLA: Learning to act anywhere with task-centric latent actions. arXiv preprint arXiv:2505.06111, 2025. 6, 2 [8] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. GR-2: generative video-languageaction model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. 3, 7, 1 [9] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion Forcing: Next-token prediction meets full-sequence diffusion. Proceedings of the Advances in Neural Information Processing Systems, 37:2408124125, 2024. 3 [10] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. On scaling up multilingual vision and language model. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1443214444, 2023. 1 [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations, 2021. 3, 1 [12] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long arXiv preprint context arXiv:2503.10589, 2025. tuning for video generation. [13] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. In Proceedings of the International Conference on Machine Learning, 2025. 6, 1 [14] Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, YuChiang Frank Wang, and Fu-En Yang. ThinkAct: Visionlanguage-action reasoning via reinforced visual latent planning. In Proceedings of the Advances in Neural Information Processing Systems, 2025. 2 [15] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. 1 [16] Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, Fan Wang, Deli Zhao, and Xin Li. RynnVLA-001: Using human demonstrations to improve robot manipulation. arXiv preprint arXiv:2509.15212, 2025. 1 [17] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. Video-LaVIT: Unified video-language pretraining with decoupled visual-motional tokenization. In Proceedings of the International Conference on Machine Learning, 2024. 2, [18] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic VLMs: Investigating the design space of visuallyconditioned language models. In Proceedings of the International Conference on Machine Learning, 2024. 1, 6 [19] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. OpenVLA: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1, 2, 5, 6 [20] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. 2, 3, 5, 6, 7 [21] Didier Le Gall. MPEG: video compression standard for multimedia applications. Communications of the ACM, 34 (4):4658, 1991. 3 [22] Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, and Haoang Li. Spatial Forcing: Implicit spatial representation alignarXiv preprint ment for vision-language-action model. arXiv:2510.12276, 2025. 2 [23] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. CogACT: foundational visionlanguage-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. 2 [24] Shuang Li, Yihuai Gao, Dorsa Sadigh, Song. arXiv:2503.00200, 2025. 5 Unified video action model. and Shuran arXiv preprint [25] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. LIBERO: Benchmarking knowlIn Proceedings edge transfer for lifelong robot learning. of the Advances in Neural Information Processing Systems, pages 4477644791, 2023. 6, [26] Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, and Hanbo Zhang. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2025. 1, 2, 3, 6 [27] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. RDT-1B: diffusion foundation model for bimanual manipulation. In Proceedings of the International Conference on Learning Representations, 2025. 2 [28] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. CALVIN: benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3): 73277334, 2022. 6 [29] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 5, 1 [30] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open X-Embodiment: Robotic learning datasets and RT-X models : Open X-Embodiment collaboration. In 2024 IEEE International Conference on Robotics and Automation, pages 6892 6903. IEEE, 2024. 6 [31] Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, and Gao Huang. MemoryVLA: Perceptual-cognitive memory in vision-language-action models for robotic manipulation. arXiv preprint arXiv:2508.19236, 2025. 5, [32] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. 3 [33] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 1 [34] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: 10 family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [35] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: arXiv preprint An open-source generalist robot policy. arXiv:2405.12213, 2024. 1, 3, 2 [36] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In Proceedings of the International Conference on Learning Representations, 2025. 2, 3, 5, [37] Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, et al. VLA-Adapter: An effective paradigm for tiny-scale vision-language-action model. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025. 2 [38] Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang. Unified vision-language-action model. arXiv preprint arXiv:2506.19850, 2025. 3 [39] Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. DexVLA: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025. 2 [40] Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, and Xiaodan Liang. VidMan: Exploiting implicit dynamics from video diffusion model for effective robot manipulation. Proceedings of the Advances in Neural Information Processing Systems, 37:4105141075, 2024. 6, 1 [41] Thomas Wiegand, Gary Sullivan, Gisle Bjontegaard, and Ajay Luthra. Overview of the H. 264/AVC video coding standard. IEEE Transactions On Circuits and Systems For Video Technology, 13(7):560576, 2003. 3 [42] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative prearXiv preprint training for visual robot manipulation. arXiv:2312.13139, 2023. 1 [43] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In Proceedings of the International Conference on Learning Representations, 2024. 6 [44] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. Advances in neural information processing systems, 32, 2019. 1 [45] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 5, 1 [46] Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. UP-VLA: unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867, 2025. 2, 3, [47] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng Zhang, et al. DreamVLA: visionlanguage-action model dreamed with comprehensive world knowledge. arXiv preprint arXiv:2507.04447, 2025. 2 [48] Zongzheng Zhang, Haobo Xu, Zhuo Yang, Chenghao Yue, Zehao Lin, Huan-ang Gao, Ziwei Wang, and Hao Zhao. Tavla: Elucidating the design space of torque-aware visionlanguage-action models. arXiv preprint arXiv:2509.07962, 2025. 4 [49] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Tsung-Yi Lin, Gordon Wetzstein, Ming-Yu Liu, and Donglai Xiang. CoT-VLA: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17021713, 2025. 2, 3, 7 [50] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3DVLA: 3D vision-language-action generative world model. In Proceedings of the International Conference on Machine Learning, 2024. 1 [51] Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. In Proceedings of the International Conference on Learning Representations, 2025. 1, 3, 2 [52] Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, and Haoang Li. FlowVLA: Thinking in motion with visual chain of thought. arXiv preprint arXiv:2508.18269, 2025. [53] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. RT-2: Vision-language-action models transfer web knowledge to robotic control. In Proceedings of the Conference on Robot Learning, pages 21652183, 2023. 1, 2 11 HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models"
        },
        {
            "title": "Supplementary Material",
            "content": "6. More Implementation Details Beyond the SigLIP [45] and DINOv2 [29] image encoders and the Prismatic VLM [18] backbone described in the main text, we provide further additional implementation details for the two core modules used in HiF-VLA: the Hindsight Encoder and the Hindsight-Modulated Joint Expert. These details complement the high-level description given in the main text. 6.1. Hindsight Encoder We employ 4-layer Vision Transformer (ViT) [11] as the hindsight motion encoder. The hindsight motion sequence of length is first partitioned into (h//2) (H//2) (W//2) spatiotemporal blocks using 3D convolution, which simultaneously reduces temporal redundancy and preserves local motion continuity. These blocks, together with an additional [CLS] token that aggregates global temporal context, are fed into the Transformer. The resulting historical features are subsequently projected via linear layer into the joint expert embedding space, ensuring dimensional compatibility with the foresight and action pathways. This design allows the hindsight encoder to provide structured historical priors that can effectively regularize downstream decision-making. 6.2. Hindsight-Modulated Joint Expert The hindsight-modulated joint expert serves as the fusion module that integrates hindsight, foresight, and action representations. All tokens are projected into shared embedding dimension of 1024, including: hindsight motion tokens from the hindsight encoder, foresight motion tokens and latent action tokens produced by the VLM backbone. Within this space, the hindsight tokens serve as adaptive conditioning signals that modulate both foresight and action streams via AdaLN [44] scaling and shifting. This conditioning mechanism allows the model to dynamically adjust its predictions based on temporally grounded historical cues, enabling causal alignment between past observations and future behavior. Positional information is provided through Rotary Positional Embedding (RoPE) [33], allowing efficient encoding of both spatial and temporal ordering. The joint expert consists of 6 Transformer layers, each capable of cross-stream attention, enabling the model to capture complementary relationships between predicted dynamics and actions. Figure 6. Architecture of the hindsight-modulated joint expert. 7. Comparison with Video-Generation VLAs Compared to VLA approaches that rely on video generation [8, 13, 40, 42], our method differs fundamentally in how it models temporal dynamics. large body of recent work [8, 13, 40, 42] employs general-purpose video generative models to predict future frames, using these predictions either for inverse dynamics computation or as conditional representations to assist action policy generation. While these approaches have made substantial progress, they face some limitations when contrasted with HiF-VLA, which leverages sparse motion representations. Specifically, video-generation-based methods typically require multi-frame historical input and high-resolution future video synthesis, resulting in substantial computational overhead. Moreover, pixel-level prediction is prone to local artifacts and distortions, introducing uncontrollable noise. In contrast, HiF-VLA replaces video-level temporal modeling with low-dimensional, structured motion representations, enabling more efficient and stable way to encode historical visual states and predict future dynamics. In addition, HiF-VLA, by jointly predicting foresight motion and action trajectories as two complementary information streams, allows the model to anticipate how the physical world may evolve while generating the corresponding actionseffectively enabling thinking while acting."
        },
        {
            "title": "Methods",
            "content": "LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Average TraceVLA [51] Octo [35] CoT-VLA [49] SpatialVLA [22] ThinkAct [14] Seer [36] FlowVLA [52] DreamVLA [47] CogACT [23] π0 [4] GR00T N1 [3] UniVLA [7] MemoryVLA [31] OpenVLA-OFT [20] HiF-VLA(ours) 84.6 78.9 81.1 88.2 88.3 - 93.2 97.5 97.2 96.8 94.4 96.5 98.4 97.6 98.8 85.2 85.7 87.5 89.9 91.4 - 95.0 94.0 08.0 98.8 97.6 96.8 98.4 98.4 99.4 75.1 84.6 91.6 78.6 87.1 - 91.6 89.5 90.2 95.8 93.0 95.6 96.4 97.9 97.4 54.1 51.1 87.6 55.5 70.9 87.7 72.6 89.5 88.8 85.2 90.6 92.0 93.4 94.5 96. 74.8 75.1 69.0 78.1 84.4 87.7 88.1 92.6 93.2 94.2 93.9 95.2 96.5 97.1 98.0 Table 4. Performance on the LIBERO benchmark. The table compares our method against wide range of state-of-the-art approaches, with the best performance highlighted in bold. λ 0.1 0.05 0. 0.001 SR (%) 94.4 95.2 96.4 95. from the motion-prediction branch is essential for achieving balanced and robust model behavior. Table 5. Analysis of Weight factor λ for foresight motion loss. 8. More Experimental Results 8.1. Comprehensive Evaluation on the LIBERO"
        },
        {
            "title": "Benchmark",
            "content": "We report detailed evaluation results on all four suites of the LIBERO benchmark [25] and compare our method against broad set of baseline models, as summarized in Tab. 4. While achieving its greatest margin of superiority under the most challenging LIBERO-Long suite, HiF-VLA also delivers competitive or superior performance across the remaining three suites when compared to prior state-of-the-art approaches. As result, HiF-VLA attains the best average performance over all four suites, demonstrating its robustness and general applicability across diverse task scenarios. 8.2. The Impact of Weight Factor λ The hyperparameter λ balances the contributions between foresight motion prediction and action prediction. To systematically investigate this trade-off, we evaluate range of λ values. Our analysis reveals that an appropriate weighting allows the motion prediction to effectively support the models reasoning, thereby enhancing planning capabilities; however, an unsuitable value destabilizes the VLA architecture and impedes policy generation. Our results in Tab. 5 identify an optimal performance point at λ = 0.01. This key finding indicates that modest, well-calibrated contribution Figure 7. Convergence of the foresight-motion L1 loss during training. The curve w/o action prediction corresponds to variant where the action-prediction branch is removed and only the foresight-motion pathway is trained. The curve w/ action prediction represents the full HiF-VLA architecture, where both streams in the joint expert (foresight motion and action) are retained. 8.3. Analysis of Think-while-Acting Paradigm Foresight Motion Prediction. To more comprehensively evaluate the interaction between foresight motion and action prediction, we additionally removed the actionprediction branch and trained the model using only the motion prediction pathway. We then compared the evolution of the motion L1 loss during training, as shown in Fig. 7. The 2 models ability to distinguish visually similar states, maintain correct action ordering, and perform time-sensitive decision making. Because pre-press and post-press observations can look visually similar, the task naturally introduces visual ambiguity and demands strong historical information integration and temporal reasoning. 9.3. Real-World Execution Visualization Fig. 9 provides visualizations of HiF-VLAs rollout across the real-world tasks. The figure presents the models generated actions at key time steps. As shown, HiF-VLA demonstrates stable behavior planning capabilities in real operation: in the block-grasping task, the robot consistently maintains reliable object recognition and performs precise grasp-and-place actions; in the multi-step bowl covering and stacking task, the system robustly executes cross-object sequential operations and preserves coherent action structure throughout stages with clear hierarchical dependencies; in the button-pressing task, the model correctly distinguishes between visually similar states and adheres to the required action order. These visualizations illustrate that HiF-VLA exhibits robust visual understanding, coherent action organization, and strong execution of long-horizon task structures in real scenarios, thereby validating its reliability in performing complex manipulation tasks. 9.4. Failure Cases Despite HiF-VLA outperforming in both simulation benchmarks and real-world experiments, several failure cases still occur, as shown in Fig. 10. In the first task, the model prematurely opened the gripper based on an incorrect spatial judgment, resulting in placement failure. In the second task, the stacking failed because the robotic arm did not lift the bowl to an appropriate height. In the third task, an erroneous depth estimation caused the gripper to descend insufficiently, leading to an incomplete button press. These failure modes highlight the critical role of spatial geometry and 3D perception. They suggest that future work may benefit from integrating richer 3D representations into our framework to further enhance robustness in real-world manipulation. results indicate that incorporating action prediction leads to faster convergence and noticeably more stable training curve for motion prediction. This indicates that the action branch provides effective complementary information to the motion branch, enabling synergistic interaction between the two. Such coupling supports the models ability to reason about future dynamics while simultaneously making action decisionsthereby achieving genuine thinkwhile-acting capability. Foresight and Action Prediction Visualization. We present qualitative visualizations of the foresight motion predictions and action predictions across several tasks in the LIBERO-Long dataset, as shown in Fig. 8. The visualizations demonstrate how our models anticipated motion sequences closely align with its generated action plans. Across diverse long-horizon tasks, this close alignment ensures that the agents actions are consistently grounded in coherent forecast of future states. This visually corroborates that HiF-VLA maintains temporal coherence and successfully executes the proposed think-while-acting paradigm by dynamically adapting its policy based on foresighted reasoning. 9. Real-World Experiments 9.1. Real-World Experimental Setup We evaluate our method on series of long-horizon realworld tasks using an AgileX Piper robot, which is equipped with 6-DoF manipulator and 1-DoF gripper. single Intel RealSense D435 camera provides third-person observations, while an additional USB wrist-mounted camera provides egocentric input. Data are collected at 20 Hz. All models are trained under the standard HiF-VLA configuration and deployed on single NVIDIA RTX 4090 GPU. 9.2. Task Descriptions Place blocks on the plates. The robot must place the white block onto the white tray and the pink block onto the pink tray. This task primarily evaluates the models basic visual recognition ability and precise object placement. Its clear goal structure and low action complexity make it suitable for assessing whether the model can reliably establish accurate observationaction mappings in simple environments. Cover block and stack bowls. The robot first covers white block with green bowl and then stacks pink bowl on top of the green one. This task stresses the models ability in multi-step sequential reasoning, spatial relation understanding, and long-horizon dependency modeling. The explicit hierarchical dependency structure (cover stack) makes it an effective test for the models capacity to handle layered object interactions and long-term constraints. Press buttons in order. The robot must press three colored buttons in specified order. This task assesses the 3 Figure 8. Example rollouts of three tasks in LIBERO-Long, illustrating the close alignment between the predicted foresight motion and the observed action execution over short inference window. (a) Place blocks on the plates. (b) Cover block and stack bowls. (c) Press buttons in order. Figure 9. Example rollouts of real-world tasks. (a) Place blocks on the plates. (b) Cover block and stack bowls. (c) Press buttons in order. Figure 10. Failure cases of real-world tasks."
        }
    ],
    "affiliations": [
        "HKUST(GZ)",
        "Nanjing University",
        "Westlake Robotics",
        "Westlake University",
        "Zhejiang University"
    ]
}