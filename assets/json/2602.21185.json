{
    "paper_title": "The Diffusion Duality, Chapter II: $Ψ$-Samplers and Efficient Curriculum",
    "authors": [
        "Justin Deschenaux",
        "Caglar Gulcehre",
        "Subham Sekhar Sahoo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 5 8 1 1 2 . 2 0 6 2 : r Published as conference paper at ICLR THE DIFFUSION DUALITY, CHAPTER II: Ψ-SAMPLERS AND EFFICIENT CURRICULUM Justin Deschenaux1 Caglar Gulcehre1,2 Subham Sekhar Sahoo3 1EPFL, Lausanne, Switzerland 2Microsoft AI 3Cornell Tech, NY"
        },
        {
            "title": "ABSTRACT",
            "content": "Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and video-tutorial on: https://s-sahoo.com/duo-ch2 Figure 1: Performance on Language Modeling and Image Modeling. Ψ-samplers generalize ReMDM (Wang et al., 2025) to arbitrary noise distributions. (Left): Generative perplexity (Gen. PPL; ) as function of NFEs, with nucleus sampling = 0.9. Ψ-samplers consistently improve with more steps, unlike ancestral sampling which plateaus. Curves are annotated with the average unigram entropy per sequence as proxy for diversity. (Right): On CIFAR-10, Ψ-samplers achieve better FID () than MDLM (with ReMDM)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models are powerful generative algorithms that have achieved remarkable success in modeling continuous data domains, including images (Ho et al., 2020a; Rombach et al., 2022), audio (Kong et al., 2021; Liu et al., 2023b; Huang et al., 2023), and videos (Ho et al., 2022; Esser et al., 2023; Blattmann et al., 2023; Polyak et al., 2025). Recent advances have extended diffusion Correspondence to justin.deschenaux@epfl.ch and ssahoo@cs.cornell.edu 1 Published as conference paper at ICLR 2026 models to categorical data, demonstrating their potential for language modeling (Austin et al., 2023; Lou et al., 2024; Sahoo et al., 2024; Shi et al., 2025; Ou et al., 2025; Sahoo et al., 2025a;b), graphs (Liu et al., 2023a), molecules (Lee et al., 2025), and audio (Ku et al., 2025). Unlike autoregressive models that generate tokens sequentially from left to right, diffusion language models can decode tokens in parallel and in any order while leveraging bidirectional contextual information. This capability enables the design of language models that can be significantly faster than their autoregressive counterparts while maintaining strong downstream performance (Song et al., 2025; Labs et al., 2025). Discrete diffusion models primarily employ one of two noise distributions: uniform prior or masked prior that concentrates all probability mass on special [MASK] token. Unlike Masked Diffusion Models (MDMs), which update each token exactly once, Uniform-State Diffusion Models (USDMs) allow tokens to be revised multiple times during generation, enabling self-correction. This makes USDMs particularly effective for few-step (Sahoo et al., 2025a) and guided generation (Schiff et al., 2025). However, the generation quality of USDMs has not yet matched that of MDMs in high-sampling-step regimes, and USDMs modeling capacity, as measured by likelihood, remains inferior to MDMs. Although Sahoo et al. (2025a) proposed curriculum learning strategy (Bengio et al., 2009) that narrows the likelihood gap, this curriculum is computationally expensive. To address MDMs inability to remask tokens, ReMDM (Wang et al., 2025) introduced PredictorCorrector (PC) samplers that generalize and outperform earlier PC methods (Campbell et al., 2022; Gat et al., 2024). These samplers substantially improve the inference time scaling behavior of MDMs. However, PC methods for uniform-state diffusion remain underexplored. Campbell et al. (2022) proposed PC methods for samplers that take advantage of the rate change matrices of the continuoustime Markov chain (CTMC) formulation of discrete diffusion processes, but such samplers are known to perform worse than ancestral samplers (Lou et al., 2024; Schiff et al., 2025). We propose Duo++ to address these challenges, which expands the design space of USDMs using non-Markovian superposition posteriors (or as we refer to them in this paper, Ψ-posteriors). These posteriors align with the intermediate marginals of discrete diffusion processes and give rise to Ψ-samplers with predictor-corrector capabilities that are crucial for improving sample quality. In addition, Duo++ introduces an efficient curriculum learning strategy that advances the approach of Sahoo et al. (2025a) by accelerating training and reducing memory usage. In summary, our contributions are threefold: (1) we propose family of non-Markovian posteriors (Ψ-posteriors) for discrete diffusion with arbitrary noise priors that share the same marginals as the Markovian discrete diffusion process (Sec. 3). (2) We demonstrate that the induced Ψ-samplers improve text and image generation quality and scale better than standard ancestral samplers in high NFE regimes, closing the performance gap with respect to MDMs coupled with remasking samplers in high NFE regimes for text generation (Sec. 5.1) and surpassing them on image generation tasks (Sec. 5.1.2). (3) We reformulate the curriculum learning strategy proposed in Sahoo et al. (2025a), achieving 2 speedup while reducing peak memory usage by 33% and end-to-end training time by 25%, while maintaining similar perplexity (Fig. 1, right, Table 5) and downstream task accuracy  (Table 1)  ."
        },
        {
            "title": "2.1 DISCRETE DIFFUSION MODELS",
            "content": "Consider the clean data sequence of length drawn from the data distribution qdata. Discrete diffusion models (Sohl-Dickstein et al., 2015; Austin et al., 2023) define sequence of increasingly noisy distributions (qt)t[0,1], interpolating from qdata to factorized prior distribution, which is product of independent Cat(.; π) distributions, using Markovian transitions defined independently 2 Published as conference paper at ICLR 2026 across input dimensions (Campbell et al., 2022; Sahoo et al., 2024; Shi et al., 2025; Ou et al., 2025; Schiff et al., 2025; Sahoo et al., 2025a). Let zt (cid:81)L ℓ=1 qt(.xℓ) denote the intermediate latents (sequence) at time step t. This work focuses on factorized, interpolating noise processes (Sahoo et al., 2024), whose conditional marginal distribution takes the form: qt(.xℓ; αt) = Cat(.; αtxℓ + (1 αt)π), zℓ where αt [0, 1] is monotonically decreasing with t, and is known as the noise schedule. (1) defines the forward process, which progressively corrupts the data. The goal is to learn generative process pθ, parameterized by neural network with parameters θ, that reverses this forward process to map from the noise prior back to qdata. The model is typically trained by minimizing the Negative Evidence Lower Bound (NELBO). The choice of token prior π gives rise to two popular variants: Masked Diffusion Models (MDMs) and Uniform-state Diffusion Models (USDMs), which we discuss in the following. (1)"
        },
        {
            "title": "2.1.1 MASKED DIFFUSION PROCESSES",
            "content": "MDMs (Sahoo et al., 2024; Shi et al., 2025; Ou et al., 2025) use masked prior, where π = is the one-hot representation of special [MASK] token (Devlin et al., 2019). During the forward process (1), tokens either remain unchanged or transition to the masked state m, after which they stay masked. This behavior carries over to the reverse process. The posterior of the reverse process qMDM st for 0 < < 1 can be derived using Bayes Rule, and is given by: (cid:40) qMDM st (.zℓ t, xℓ) = .; αsαt 1αt (cid:16) Cat Cat(.; xℓ) st = (cid:81) xℓ + 1αs 1αt zℓ (cid:17) if zℓ = m, otherwise. (2) The approximate reverse posterior is pθ , t)) where xθ : [0, 1] is the denoising model. key limitation is that once unmasked, tokens cannot be remasked (2). This can create compounding errors during inference, as the denoising model xθ imperfectly models the clean data. t, xℓ = xℓ ℓ qMDM st θ(z1:L (.zℓ Predictor-Corrector Methods Wang et al. (2025) propose posteriors, and associated samplers (ReMDM) that maintain the same marginals as (2) during the generation process, while allowing remasking and generalizing previous training-free predictor-corrector methods such as Campbell et al. (2022); Gat et al. (2024)."
        },
        {
            "title": "2.1.2 UNIFORM-STATE DIFFUSION PROCESSES",
            "content": "Alternatively, discrete diffusion models can use uniform prior π = 1/K (Schiff et al., 2025; Sahoo et al., 2025a). This choice allows tokens to change values multiple times throughout the generative process, in contrast to Masked diffusion. This property allows USDMs to excel in few-step generation (Sahoo et al., 2025a) and guidance applications (Schiff et al., 2025). USDMs admit the following posterior distribution qUSDM qUSDM st st ): (for brevity, we simply write qst for qst(. zℓ t, xℓ) = Cat .; (cid:32) Kαtzℓ xℓ + (αts αt)zℓ + (αs αt)xℓ + (1 αts)(1 αs)1/K Kαtzℓ t, xℓ + 1 αt This posterior induces the following NELBO (Sahoo et al., 2025a): NELBO (q, pθ; x) = tU [0,1], qt(zℓ txℓ;αt) (cid:88) ℓ[L] where f(zℓ t, xℓ θ(zℓ t, t), αt; xℓ), (cid:33) . (3) (4) f(zℓ t, xℓ θ(zℓ t, t),αt; xℓ) = (cid:34) αt Kαt xℓ (xℓ θ)r (cid:16) ζt1 =xℓ + zℓ zℓ =xℓ (cid:17) (cid:88) log (xℓ (xℓ θ)r θ)j αt 1 αt log (xℓ (xℓ θ)r θ)i =xℓ zℓ (cid:18) (K 1)ζt1 =xℓ zℓ 3 1 ζt (cid:19) (cid:35) . log ζt 1 zℓ =xℓ (5) Published as conference paper at ICLR 2026 Figure 2: Ψ-samplers combine predictor and corrector steps. The predictor transitions from zt to zs via qst, but fails to remask tokens in MDMs. The corrector steps inject noise via qs, to revise earlier predictions. For κt < 1, noise injection enables error correction while preserving the forward process marginals. Our framework extends prior PC methods (Campbell et al., 2022; Gat et al., 2024; Wang et al., 2025) to arbitrary priors π. Here, xℓ = Kαtxℓ + (1 αt)1, xℓ αt, = arg maxj[K](zℓ corresponding to 1, that is, xi = 1. t)j is the nonzero entry of zt, ζt = 1αt θ(zt, t) + (1 αt)1, α Kαt+1αt θ = Kαtxℓ denotes the time derivative of , and denotes the index in The Diffusion Duality Sahoo et al. (2025a) show that USDMs emerge from an underlying Gaussian diffusion process (Sohl-Dickstein et al., 2015; Ho et al., 2020b; Song et al., 2021; Kingma et al., 2023) on the one-hot representation xℓ V. The Gaussian diffusion begins with xℓ and progressively RK qt(.xℓ) for [0, 1] with adds Gaussian noise leading to sequence of noisy latents wℓ the marginals: qt(.xℓ; αt) = (.; αtxℓ, (1 α2 )IK), where (αt)t[0,1] is monotonically decreasing noise schedule. Let arg max : RK map continuous vector RK to the one-hot vector corresponding to the index of its largest entry in v, that is, arg max(v) = arg maxzV zv. When applied to sequence of Gaussian latents w, arg max qt(.xℓ; αt := (αt)), transforms them to the discrete latents zt whose marginals take the form: zℓ where the function : [0, 1] [0, 1] is the Diffusion Transformation Operator: (αt) = 1 (cid:34)(cid:90) (cid:32) ϕ αt (cid:112)1 α2 (cid:33) ΦK1(z)dz (cid:35) ,"
        },
        {
            "title": "1\nK",
            "content": "(6) where ϕ(z) = exp(z2/2)/ respectively. More formally, this relationship is expressed as: 2π and Φ(z) = (cid:82) ϕ(t)dt are the standard Normal PDF and CDF, qt(zℓ txℓ; (αt)) = [arg max] qt(wℓ txℓ; αt) (7) where the operator denotes the pushforward of the K-dimensional Gaussian density under the arg max map, yielding categorical distribution with classes. Note that while the marginal distribution qt(ztx; (αt)) matches the discrete-space marginal in (1), this does not imply that the full trajectory {zt := arg max(wt)}t[0,1] follows (Markovian) discrete diffusion process (Sahoo et al., 2025a). An interesting outcome of (7) is that the discrete NELBO (4) can be written in terms of Gaussian latents in the following manner, where the second arg max is applied to each token independently: NELBO (q, pθ; x) = Ex,tU [0,1],qt (cid:88) (cid:16) ℓ[L] := arg max(wℓ zℓ t), xℓ θ(arg max(wt), t), αt := (αt); xℓ(cid:17) . (8) Curriculum Learning Curriculum learning (Bengio et al., 2009) trains models by gradually increasing task difficulty. Building on this idea, Sahoo et al. (2025a) accelerate early training by using biased but low-variance NELBO estimator for (8). Concretely, during the first 50% of training steps, the hard arg max used to convert Gaussian latents into discrete tokens in the transformers input is replaced by low-temperature softmax relaxation. This relaxation interfaces naturally with the transformer input layer: if the latent at position ℓ is probability vector yℓ K, then the token representation is computed as matrix product Vyℓ, where RKm is the vocabulary embedding matrix. For one-hot yℓ (as produced by arg max), this reduces to standard embedding 4 Published as conference paper at ICLR 2026 lookup; for softmax-relaxed latents, it becomes linear combination of vocabulary embeddings. As result, the model is no longer asked to denoise from fully corrupted discrete token embedding, but instead receives an embedding that is superposition of clean and noisy token embedding. This partially clean input provides direct signal about the underlying token, making denoising easier than relying solely on the surrounding context. Fig. 3 (top) illustrates this curriculum. More formally, during the curriculum phase Sahoo et al. (2025a) optimize the following loss, where the softmax is applied independently at each position: Ltrain = Ex,tU [β,γ],qt (cid:88) ℓ[L] (cid:16) := arg max(wℓ zℓ t), xℓ θ(softmax(wt/τ ), t), αt := ( αt); xℓ(cid:17) . (9) Notice that Ltrain in (9) reduces to the NELBO (8) in the limit limτ 0, for β = 0 and γ = 1, since limτ 0 softmax(v/τ ) = arg max(v), as shown by Jang et al. (2017); Maddison et al. (2017). However, explicitly materializing the high-dimensional latents wt is memory-intensive, an issue we address in Sec. 4."
        },
        {
            "title": "2.2 DIFFUSION GUIDANCE",
            "content": "For continuous data, diffusion models have achieved state-of-the-art controllable generation through both classifier-based guidance (Sohl-Dickstein et al., 2015; Dhariwal & Nichol, 2021) and ClassifierFree Guidance (CFG; Nichol & Dhariwal (2021); Ho & Salimans (2022)). These approaches have since been extended to discrete data (Gruver et al., 2023). Let {1, . . . , C} denote one of (γ), which modulates the strength of the guidance possible classes. For CFG, the sampling posterior pθ term via the temperature parameter γ, is defined as (Nisonoff et al., 2024; Schiff et al., 2025): log pθ (γ)(zℓ y, zt) = γ log pθ(zℓ y, zt) + (1 γ) log pθ(zℓ , zt); ℓ [L], (10) where denotes no class conditioning, and pθ is the generative posterior (Sec. 2.1)."
        },
        {
            "title": "3 THE Ψ-POSTERIORS",
            "content": "Multiple joint distributions can give rise to the same marginals as the discrete diffusion process defined in (1). In this work, we introduce family of posteriors, denoted Ψ, that share the same marginals as in (1); see Suppl. A.2 for details. These alternative generative processes are nonMarkovian and apply both to the Masked diffusion processes and to the Uniform-state diffusion processes. Specifically, we define the posteriors for the generative process as: t) = κtqst(.zℓ (11) where κt [0, 1] and Ψ1(.xℓ) = Cat(.π), with π = for MDMs and π = 1/K for USDMs. (11) is thus linear combination of the forward process (1) and the reverse posteriors (2, 3) of standard discrete diffusion models. We therefore refer to these as superposition posteriors, or simply Ψ-posteriors. t, xℓ) + (1 κt)qs(.xℓ); ℓ [L] Ψst(.xℓ, zℓ Ψ-Forward Processes Consider the interpolating diffusion process in (1) discretized into steps. Let zt(i) denote the latent variables at times t(i) = i/T for 0 . The distribution of trajectory z0:1 factorizes independently over tokens as: Ψ(z0:1x) = (cid:81) 0:1xℓ) where t(i), xℓ). In what follows, we use s, as shorthand for s(i)zℓ Ψ(zℓ s, xℓ) = s(i), t(i), respectively. The forward process can be derived from Bayes rule: Ψ(zℓ Ψ(zℓ sxℓ). Unlike the Markovian interpolating process in (1), this forward process is generally not Markovian, since each zℓ 0:1xℓ) = Ψ1(zℓ may depend on both zℓ i=1 Ψst(zℓ txℓ)/Ψ(zℓ 1xℓ) (cid:81)T t, xℓ)Ψ(zℓ and xℓ. ℓ Ψ(zℓ szℓ tzℓ [Ψθ t, xℓ Ψ-Reverse Processes st(.zt)]ℓ = κtqst(.zℓ In Suppl. A.1, we show that the approximate reverse posterior takes the form: θ(zt, t)) + (1 κt) (cid:2)αsq0t(.zℓ (12) where xθ denotes the denoising model. We dub (12) as Ψ-sampler. For (κt = 1)t[0,1], we recover the standard ancestral sampler defined in (2) for MDMs and (3) for USDMs. Notice that for κt < 1, Ψst corresponds to noisier version of the ancestral sampler marginal qst. This is analogous to Predictor-Corrector methods in Gaussian diffusion (Song et al., 2021), where the corrector introduces additional Gaussian noise. In our case, qt plays the role of the corrector, while qst acts as the predictor. The Ψ-posteriors also admit principled NELBO formulation (see Suppl. A.3), though this is not directly relevant for sampling. θ(zt, t)) + (1 αs)π(cid:3) . t, xℓ 5 Published as conference paper at ICLR Figure 3: Efficient Curriculum for USDMs. Duo (Sahoo et al., 2025a) replaces discrete lookups with linear combinations of all embeddings: (1) Gaussian diffusion on one-hot representations, (2) Low-temperature softmax, (3) weighted sum. Duo++ exploits the sparsity of the tempered softmax (most weights are effectively zero), and simulate the largest entries (out of K) using ordered statistics. The approximate normalizer admits closed form expression (14). Duo++ has 33% lower memory and 25% faster training than Duo. Corollary For π = m, different choices of {κt}t[0,1] recover previous Predictor-Corrector formulations in the literature (Campbell et al., 2022; Gat et al., 2024; Wang et al., 2025) (see Suppl. A.4 for the proof). The Ψ framework thus subsumes these samplers as special cases, extending these predictor-corrector methods for discrete diffusion with any prior π. Takeaway 1: Ψ-samplers generalize prior Predictor-Corrector methods to arbitrary noise priors, subsuming ReMDM (Wang et al., 2025) and prior work (Campbell et al., 2022; Gat et al., 2024) as special cases. Intuitive Explanation In practice, the denoiser xθ imperfectly models the clean data x. The key to the effectiveness of Ψ-sampler is the offset term (1 κt)(1 αs)π in (12), which enables error correction during generation. For MDMs (π = m), this offset allows previously denoised tokens to return to the masked state, unlike the ancestral sampler, which prevents remasking (see Sec. 2.1.1). Incorrect tokens can thus be replaced with better ones. For USDMs (π = 1/K), the offset ensures every token has non-zero sampling probability. Even if the denoiser assigns near-zero probability to the correct token, the Ψ-sampler gives it chance to appear, whereas ancestral sampling would not. While this offset may occasionally introduce incorrect tokens, the marginals of the Ψ-samplers (11) match those of the Markovian forward process (1), hence we converge to the correct distribution given sufficient samples."
        },
        {
            "title": "4 SCALABLE CURRICULUM FOR FASTER TRAINING",
            "content": "Recall from Sec. 2.1.2 that the curriculum of Sahoo et al. (2025a) accelerates training by replacing discrete tokens as inputs to the transformer with softmaxed Gaussian latents. Naively, however, this requires materializing K-dimensional weight vector for every token at every training step, which is infeasible for modern LLM vocabularies with > 100,000 (Touvron et al., 2023; OpenAI, 2024). Our key observation is that Sahoo et al. (2025a) use very low temperature τ = 103 in (8). At such temperatures, the softmax concentrates almost all its mass on only few entries, making most of the weights negligible. We exploit this induced sparsity by approximating the full linear combination using only embeddings (Fig. 3, bottom). We illustrate the process hereby: Step 1: Generating top-k entries in wℓ w/o materializing it Let [K] be the nonzero coordinate of the one-hot vector xℓ, i.e. (xℓ)o = 1. Recall that wℓ and ϵ (0, IK). Therefore, (wℓ ) and (wℓ ). All coordinates in [K] are exchangeable (i.i.d. with mean 0), while the coordinate is the only one with shifted mean. As = αtxℓ + σtϵ, with σt = (cid:112)1 α2 t)i=o (0, σ2 t)o (αt, σ2 Published as conference paper at ICLR 2026 result, the top-k set falls into one of two cases: Case 1: is not among the top-k , so all winners lie in [K] o. Case 2: is among the top-k , so the winners are plus 1 indices from [K] o. Next, we describe how to sample the top-k values (and later their indices) without ever forming the full K-dimensional vector. Let = K1 and consider i.i.d. samples w1, . . . , wm (0, σ2 ). Rather than drawing all values, we exploit the fact that order statistics of i.i.d. uniform random variables can be sampled recursively: the maximum of i.i.d. U[0, 1] variables has CDF um, so it can be sampled directly. Conditioned on the maximum, the remaining values are also i.i.d. uniforms, so the next-largest can be sampled the same way, and so on. Applying the inverse normal CDF Φ1() σt converts the top-k uniform order statistics into the top-k Gaussian values (see Suppl. B.1.4 for details). We denote the result = (K1 Kk) , where Kj is the j-th largest among the K1 zero-mean coordinates. (cid:12){j [k] : Kj > w}(cid:12) ), which matches the distribution of (wℓ We draw independently the special entry (αt, σ2 t)o. Now compare to the current k-th largest value among the zero-mean coordinates: If Kk > w, then cannot enter the top-k. We are in Case 1, and already contains the correct top-k values. If > Kk, then must be in the top-k. We are in Case 2. Let = (cid:12) (cid:12) be the number of values in that are larger than w. We insert at rank + 1 and drop the previous smallest value: K1:r wKr+1:k1 , where denotes concatenation. (cid:1) denote the set of all possible tuples with distinct elements in the set S. We For clarity, let (cid:0) generate the index tuple corresponding to the top-k values as follows: Case 1: all top-k indices lie in [K] o; hence, (cid:0)[K]{o} (cid:1). Case 2: the index appears at position + 1 in K. Sample the indices for the remaining 1 values from [K] o, split into those above and below o: (cid:1). This produces both the top-k values and = L(o)R , where (cid:0)[K]{o} their matching indices while sampling only O(k) random variables, without constructing the full K-dimensional vector. (cid:1), (cid:0)[K]{o}L kr1 Step 2: Approximating the softmax Given the top-k values and indices (K, I) from Step 1, we approximate the softmax-weighted embedding vector by retaining only the selected rows of the embeddings RKd (d denotes the embedding size): softmax(wℓ t)embeddings (cid:88) i=1 exp(Ki/τ ) embeddings[Ii], (13) where embeddings[j] denotes the j-th row. The normalizer includes both sampled (top-k) and unsampled terms. While each unsampled term is small, their sum may be non-negligible, hence we approximate it as (Suppl. B.2): (cid:88) i=1 (cid:124) exp (cid:19) (cid:18) Ki τ (cid:123)(cid:122) top-k terms (cid:125) + δ exp (cid:19) (cid:18) τ + (K δ) exp (cid:124) (cid:123)(cid:122) clean token (cid:125) (cid:124) (cid:18) σ (cid:19) (cid:18) Kk σt 2τ 2 log Φ (cid:123)(cid:122) unsampled zero-mean terms + log Φ (cid:18) Kk σ /τ σt (cid:19)(cid:19) , (cid:125) (14) where δ = 1 if ( case 2 ) and 0 otherwise. We provide the full derivation in Suppl. B.2 and pseudocode in Algo. 3. Lastly, the curriculum objective in (9) requires evaluating the diffusion transformation operator (). Directly computing via (6) is prohibitively expensive during training; Sahoo et al. (2025a) therefore precompute and cache many (αt, (αt)) pairs, which is cumbersome. Instead, we compute () on the fly using its Taylor expansion; see Suppl. B.3.1. 5 EXPERIMENTS We evaluate Duo++ with Ψ-samplers on language modeling (Sec. 5.1.1) and image generation (Sec. 5.1.2), showing that Ψ-samplers markedly improve text and image quality, making USDMs 7 Published as conference paper at ICLR 2026 Table 1: Accuracy on multiple-choice question answering datasets. Abbreviations: Arc-e (ARC-Easy), Arc-c (ARC-Challenge), HSwag (HellaSwag), WinoG (Winogrande), PIQA (Physical Intelligence Question Answering), OQA (OpenBookQA). Results from Deschenaux et al. (2025). Duo++ (k = 2) achieves slightly higher accuracy than Duo on 4 out of 6 tasks. Overall, Duo++ matches Duos performance while using 25% fewer flops. The highest accuracy among USDMs is bolded. The absolute best per column is underlined. Arc-e Arc-c HSwag WinoG PIQA MathQA OQA AR Transformer MDLM Duo Duo++ (k = 2) Duo++ (k = 3) Duo++ (k = 5) 44.95 34.26 28.11 27.32 28.28 28.03 23.04 24. 25.43 26.11 25.00 25.77 30.55 31.54 26.46 26.26 25.89 26.90 52.80 51.93 47.20 49.64 47.36 50.12 63.71 57. 51.14 52.12 50.65 51.25 22.24 20.70 20.00 20.40 21.01 20.20 19.00 28.60 23.40 27.80 23.00 25.40 outperform MDMs in sample quality. In Sec. 5.2, we show that Duo++ matches Duo (Sahoo et al., 2025a) while using 33% less memory and training 25% faster, enabled by our efficient curriculum strategy (Sec. 4). 5.1 Ψ-SAMPLERS"
        },
        {
            "title": "5.1.1 LANGUAGE MODELING",
            "content": "Takeaway 2: Ψ-samplers substantially improve Generative Perplexity for USDMs, with gains especially pronounced when NFEs exceed the sequence length. Takeaway 3: Unlike ancestral sampling, which plateaus, Ψ-samplers continue to improve with more sampling steps, closing the gap with Masked diffusion models. Experimental Settings We compare MDLM (Sahoo et al., 2024) and ReMDM (Wang et al., 2025) with Duo++ and Ψ-samplers. We use the original checkpoints of Sahoo et al. (2024), trained for 1M steps with batch size of 512 on OpenWebText (OWT; Gokaslan & Cohen (2019)) and context length = 1024. Duo++ is trained with the same context length, batch size and number of steps, but with the efficient curriculum. Refer to the original works for more details. We measure the sample quality using the Gen. PPL () computed with GPT-2 Large (Radford et al., 2019) and the diversity using the unigram entropy () (Dieleman et al., 2022; Sahoo et al., 2024; 2025a). We cast logits to 64-bit precision for sampling (Zheng et al., 2025). See Suppl. C.1 for more details. Figure 4: Illustration of the possible evolution of and the associated κt. In practice, we use κt close to 1 during the PC phase. Results and Ablation Fig. 1 (left) shows the Gen. PPL and entropy as function of the NFE. Duo++ with Ψ-samplers outperforms MDLM with ReMDM and ancestral sampling across the entire range of NFEs. As the number of NFEs increases beyond the sequence length, ReMDM and Ψ-samplers further improve sample quality while ancestral sampling plateaus. We ablate on the κt schedule type (cap, rescale, loop, see Suppl. A.5), the step-size parameter η, the nucleus sampling threshold {0.9, 0.95, 1.0} (Suppl. D.1). The rescale schedule with η = 0.05 yields the best Gen. PPL while preserving the unigram entropy. Nucleus sampling (p=0.9) consistently improves Gen. PPL for both MDLM and Duo, as observed in Wang et al. (2025). How to Pick κt? We recommend the ReMDM-equivalent rescale schedule with η = 0.05 and nucleus sampling (p = 0.9), using the log-linear noise schedule with linearly decreasing t, which outperforms the loop strategy (Suppl. A.5). Published as conference paper at ICLR 2026 5.1."
        },
        {
            "title": "IMAGE MODELING",
            "content": "Takeaway 4: On CIFAR-10, Duo++ with Ψ-samplers achieves better FID and IS than MDLM with both ancestral sampling and ReMDM. Experimental Setup We train the same 35M-parameter U-Net as Austin et al. (2023) on CIFAR-10 for 1.5M steps. Following Schiff et al. (2025), the U-Net is made class conditional and we sample with Discrete Classifier-free Guidance (CFG; Ho & Salimans (2022); Schiff et al. (2025)). See Suppl. C.1 for full training details. We report the Fréchet Inception Distance (FID; Heusel et al. (2018)) and Inception Score (IS; Salimans et al. (2016)) between the training set and generated samples. Results and Ablation Fig. 1 (right) and Fig. 6 show that Ψ-samplers and ReMDM substantially improve FID and IS compared to ancestral sampling, with Duo++ reaching the best scores overall. We ablate on the sampling noise schedule (cosine vs. log-linear), κt, the activation range [toff, ton], nucleus sampling, and the κt schedules (including the ReMDM variants; see Suppl. A.5). Full results are in Suppl. D.1. Using κt close to 1 (light noise injection) with cosine schedule achieves the best FID and IS, and Duo++ tolerates stronger noise injection than MDLM. With ancestral sampling, nucleus sampling improves the FID in the low NFE regime for MDLM, and always helps for Duo. Since it is detrimental to MDLM at high NFE, we do not use nucleus sampling when using the Ψ-samplers, for both MDLM and Duo. How to Pick κt? We recommend cosine sampling schedule with κt = 0.95, ton {0.5, 0.6}, toff = 0.1 for Duo++, and κt = 0.99, ton = 1.0, toff = 0.1 for MDLM. We suggest using piecewise constant κt with linearly decreasing t, rather than the ReMDM loop schedule (Suppl. A.5), setting κt < 1 when [toff, ton], since it outperforms the ReMDM schedules."
        },
        {
            "title": "5.2 FAST CURRICULUM",
            "content": "Takeaway 5: The efficient curriculum reduces peak memory by 33% and training time by 25%, while matching the performance of Duo on likelihood benchmarks and downstream tasks. Experimental Settings We train Duo++ with the scalable curriculum (Sec. 4) on OpenWebText (OWT; Gokaslan & Cohen (2019)) and LM1B (Chelba et al., 2014). We train all models for 1M steps, using batch size of 512. For LM1B, we use the bert-base-uncased tokenizer with context length of 128, padding shorter sequences. This setup follows previous work (Sahoo et al., 2024; Lou et al., 2024; He et al., 2022). For OWT, we use the GPT-2 tokenizer (Radford et al., 2019), and reserve the last 100k documents for validation, following Sahoo et al. (2025a; 2024). We follow Lou et al. (2024) and use modified diffusion transformer (DiT) (Peebles & Xie, 2023) with rotary positional encoding (Su et al., 2023). We evaluate the impact of = {2, 3, 5} during the efficient curriculum. All models are trained on 16 H100 GPUs with bfloat16 precision. Training uses the loss in (9), with τ = 0.001 and (β, γ) = (0.03, 0.15) for the first 500K steps (Sahoo et al., 2025a). Likelihood Results Table 2 shows that on both LM1B and OWT, our efficient curriculum Duo++ matches the performance of Duo with its expensive curriculum. The lowest validation perplexity is achieved with = 2, although {2, 3, 5} performs similarly. We also compare Table 2: Test perplexity (PPL) on LM1B and OWT. Lower is better. Results from Sahoo et al. (2025a). Best Uniform-state diffusion numbers are bolded. Duo and Duo++ achieve comparable performance across both datasets while requiring 25% fewer GPUhours  (Table 4)  , demonstrating the effectiveness of our memory-efficient curriculum. Autoregressive Transformer Masked Diffusion SEDD Absorb MDLM Uniform-state Diffusion SEDD Uniform UDLM Duo Duo++ (Ours), = 2 Duo++ (Ours), = 3 Duo++ (Ours), = 5 LM1B OWT 22.3 17.5 32.7 27.0 40.3 31.3 29.9 30.0 30.1 30. 24.1 23.2 29.7 27.4 25.2 25.2 25.3 25.4 9 Published as conference paper at ICLR 2026 the models trained on OWT in Zero-Shot perplexity, and find that Duo++ achieves performance comparable to Duo. That is, we evaluate on the validation splits of the Penn Treebank (Marcus et al., 1993), WikiText (Merity et al., 2016), LM1B (Chelba et al., 2014), LAMBADA (Paperno et al., 2016), AG News (Zhang et al., 2016) and scientific articles from ArXiv and PubMed (Cohan et al., 2018). Table 5 shows that Duo++ reaches zero-shot probability similar to that of Duo while requiring 25% less training GPU-hours. Likelihood-based Downstream Tasks In Table 1, we compare the multiple-choice question (MCQ) accuracy of Duo, Duo++, MDLM (Sahoo et al., 2024), and an autoregressive transformer (1M training steps with batch size of 512 on OWT, same hyperparameters as MDLM) using the lm-eval-harness suite (Gao et al., 2024) ; details in Suppl. C.3). We find that Duo++ achieves an accuracy similar to that of Duo, despite requiring 25% less training GPU-hours. However, it trails MDLM on most tasks, consistent with its higher perplexity. Throughput and Peak Memory Usage Table 4 reports the throughput and peak memory usage for Duo and Duo++. Duo++ reduces the peak memory usage by about 33% and doubles the speed of the Curriculum Learning phase. When applying Curriculum Learning for half of the training steps, Duo++ trains 25% faster than Duo on the 138M-parameter scale. Notably, both peak memory usage and throughput remain stable over the full training run when {2, 3, 5}."
        },
        {
            "title": "6 RELATED WORK AND DISCUSSION",
            "content": "Compatibility with General Discrete Diffusion Processes This work focuses on discrete diffusion with uniform or masked noise. However, our approach extends to more general discrete diffusion processes (Shaul et al., 2024; von Rütte et al., 2025; Holderrieth et al., 2025) featuring combination of masked and uniform prior, since we provide general predictorcorrector algorithm for discrete diffusion with arbitrary noise. Predictor-Corrector Samplers In the context of Masked diffusion, ReMDM (Wang et al., 2025) generalizes previous predictor-corrector methods (Campbell et al., 2022; 2024; Gat et al., 2024) that were based on Continuous Time Markov Chain formulation of discrete diffusion processes. Our approach further generalizes ReMDM to support arbitrary diffusion processes. Unlike Lezama et al. (2023); Zhao et al. (2025); Liu et al. (2025); Kim et al. (2025), who train an additional corrector module, our method does not introduce additional learned components. Comparison to Other Discrete Diffusion Samplers Park et al. (2024) uses noise-adaptive step sizes; while we use uniform steps, Ψ-samplers support any step-size schedule. Ren et al. (2025) develops higher-order samplers; we use only first-order information, though the posterior in (11) could be approximated with higher-order methods. Thus, Ψ-samplers complement both lines of work."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced unified and practical framework for predictor-corrector sampling in discrete diffusion language models through Ψ-posteriors. By linearly superposing the forward and reverse diffusion processes (11), the Ψ-posteriors preserve the marginals of standard diffusion models. Importantly, the Ψ-posteriors and associated Ψ-samplers subsume prior masked-diffusion PC samplers (Campbell et al., 2022; Gat et al., 2024; Wang et al., 2025) as special cases, and naturally extend to discrete diffusion models with uniform prior. Empirically, Duo++ with Ψ-samplers matches the performance of MDMs on natural language generation and achieves stronger FID and IS scores on CIFAR-10. Moreover, they exhibit superior scaling: performance continues to improve with NFEs, unlike ancestral samplers, which plateau. Finally, we propose scalable training curriculum (Sahoo et al., 2025a) that reduces the peak memory usage by 33% and shortens the training time by 25%. Concurrently, Sahoo et al. (2026) show that Duo surpasses an autoregressive model at the 1.7B scale on the math and reasoning benchmark (GSM8K). Taken together, these results challenge the view that Masked diffusion is categorically the future of diffusion language modeling. 10 Published as conference paper at ICLR"
        },
        {
            "title": "8 ACKNOWLEDGEMENTS",
            "content": "This work has received funding from the Swiss State Secretariat for Education, Research and Innovation (SERI). We are grateful to Ricky T. Q. Chen and Zhihan Yang for insightful discussions and suggestions. We acknowledge the SCITAS team at EPFL for providing access to their cluster, and the Swiss National Supercomputing Centre for the Alps platform. We are grateful to Karin Gétaz for her administrative assistance."
        },
        {
            "title": "REFERENCES",
            "content": "Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces, 2023. URL https://arxiv.org/abs/ 2107.03006. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In International Conference on Machine Learning, 2009. URL https://api.semanticscholar. org/CorpusID:873046. Jon Louis Bentley. Programming Pearls. Addison-Wesley Professional, Boston, MA, 2nd edition, 1999. ISBN 978-0201657883. Roger Berger and George Casella. Statistical Inference. Duxbury Press, Florence, AL, 2 edition, June 2001. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023. Kim C. Border. Lecture 15: Order statistics; conditional expectation. https://healy. econ.ohio-state.edu/kcb/Ma103/Notes/Lecture15.pdf, 2021. Course notes for MA103. Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models, 2022. URL https://arxiv.org/abs/2205.14987. Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design, 2024. URL https://arxiv.org/abs/2402.04997. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling, 2014. URL https://arxiv.org/abs/1312.3005. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. discourse-aware attention model for abstractive summarization of long In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 documents. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 615621, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2097. URL https: //aclanthology.org/N18-2097/. Justin Deschenaux and Caglar Gulcehre. Beyond autoregression: Fast llms via self-distillation through time, 2025. URL https://arxiv.org/abs/2410.21035. Justin Deschenaux, Lan Tran, and Caglar Gulcehre. Partition generative modeling: Masked modeling without masks, 2025. URL https://arxiv.org/abs/2505.18883. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL https://arxiv.org/ abs/1810.04805. Published as conference paper at ICLR 2026 Luc Devroye. Non-Uniform Random Variate Generation. Springer-Verlag, 1986. URL https: //www.springer.com/gp/book/9780387963051. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, Rémi Leblond, Will Grathwohl, and Jonas Adler. Continuous diffusion for categorical data, 2022. URL https://arxiv.org/abs/2211.15089. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. In Proceedings of the Structure and content-guided video synthesis with diffusion models. IEEE/CVF International Conference on Computer Vision, pp. 73467356, 2023. Gerald Folland. Real analysis. Pure and Applied Mathematics: Wiley Series of Texts, Monographs and Tracts. John Wiley & Sons, Nashville, TN, 2 edition, March 1999. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching, 2024. URL https://arxiv.org/abs/2407. 15595. Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Nate Gruver, Samuel Stanton, Nathan Frey, Tim GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Wilson. Protein design with guided discrete diffusion. Advances in neural information processing systems, 36:1248912517, 2023. Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models, 2022. URL https: //arxiv.org/abs/2211.15029. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. URL https://arxiv.org/abs/1706.08500. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv. org/abs/2207.12598. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020a. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020b. URL https://arxiv.org/abs/2006.11239. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. Peter Holderrieth, Marton Havasi, Jason Yim, Neta Shaul, Itai Gat, Tommi Jaakkola, Brian Karrer, Ricky T. Q. Chen, and Yaron Lipman. Generator matching: Generative modeling with arbitrary markov processes, 2025. URL https://arxiv.org/abs/2410.20587. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration, 2020. URL https://arxiv.org/abs/1904.09751. 12 Published as conference paper at ICLR Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, Jesse Engel, Quoc V. Le, William Chan, Zhifeng Chen, and Wei Han. Noise2music: Text-conditioned music generation with diffusion models, 2023. URL https://arxiv.org/abs/2302.03917. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax, 2017. URL https://arxiv.org/abs/1611.01144. Jaeyeon Kim, Seunggeun Kim, Taekyun Lee, David Pan, Hyeji Kim, Sham Kakade, and Sitan Chen. Fine-tuning masked diffusion for provable self-correction. arXiv preprint arXiv:2510.01384, 2025. Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models, 2023. URL https://arxiv.org/abs/2107.00630. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=a-xFK8Ymz5J. Pin-Jui Ku, He Huang, Jean-Marie Lemercier, Subham Sekhar Sahoo, Zhehuai Chen, and Ante Jukic. Discrete diffusion for generative modeling of text-aligned speech tokens. arXiv preprint arXiv:2509.20060, 2025. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, and Arash Vahdat. Genmol: drug discovery generalist with discrete diffusion. arXiv preprint arXiv:2501.06158, 2025. Jose Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Discrete predictor-corrector diffusion models for image synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= VM8batVBWvg. Chengyi Liu, Wenqi Fan, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, and Qing Li. Generative diffusion models on graphs: Methods and applications. arXiv preprint arXiv:2302.02591, 2023a. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: text-to-audio generation with latent diffusion models. In Proceedings of the 40th International Conference on Machine Learning, pp. 2145021474, 2023b. Sulin Liu, Juno Nam, Andrew Campbell, Hannes Stärk, Yilun Xu, Tommi Jaakkola, and Rafael Gómez-Bombarelli. Think while you generate: Discrete diffusion with planned denoising, 2025. URL https://arxiv.org/abs/2410.06264. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https: //arxiv.org/abs/1711.05101. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution, 2024. URL https://arxiv.org/abs/2310.16834. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: continuous relaxation of discrete random variables, 2017. URL https://arxiv.org/abs/1611.00712. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313330, 1993. URL https://aclanthology.org/J93-2004/. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. URL https://arxiv.org/abs/1609.07843. 13 Published as conference paper at ICLR Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021. URL https://arxiv.org/abs/2102.09672. Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance for discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572, 2024. OpenAI. Gpt-oss: open-weight language models by openai. https://github.com/openai/ gpt-oss, 2024. GitHub repository. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data, 2025. URL https://arxiv.org/abs/2406.03736. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context, 2016. URL https://arxiv.org/abs/ 1606.06031. Yong-Hyun Park, Chieh-Hsin Lai, Satoshi Hayakawa, Yuhta Takida, and Yuki Mitsufuji. Jump Your Steps: Optimizing sampling schedule of discrete diffusion models, 2024. URL https://arxiv.org/abs/2410.07761. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https: //arxiv.org/abs/2212.09748. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, WeiNing Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2025. URL https://arxiv.org/abs/2410.13720. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. Yinuo Ren, Haoxuan Chen, Yuchen Zhu, Wei Guo, Yongxin Chen, Grant M. Rotskoff, Molei Tao, and Lexing Ying. Fast solvers for discrete diffusion models: Theory and applications of high-order algorithms, 2025. URL https://arxiv.org/abs/2502.00234. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/ abs/2112.10752. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. URL https://arxiv.org/abs/1505.04597. Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models, 2024. URL https://arxiv.org/abs/2406.07524. Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, and Volodymyr Kuleshov. The diffusion duality. In Forty-second International Conference on Machine Learning, 2025a. URL https://openreview.net/forum?id=9P9Y8FOSOk. 14 Published as conference paper at ICLR 2026 Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing, John Thickstun, and Arash Vahdat. Esoteric language models. arXiv preprint arXiv:2506.01928, 2025b. Subham Sekhar Sahoo, Jean-Marie Lemercier, Zhihan Yang, Justin Deschenaux, Jingyu Liu, John Thickstun, and Ante Jukic. Scaling beyond masked diffusion language models. arXiv preprint arXiv:2602.15014, 2026. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans, 2016. URL https://arxiv.org/abs/1606.03498. Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre, Bernardo P. de Almeida, Alexander Rush, Thomas Pierrot, and Volodymyr Kuleshov. Simple guidance mechanisms for discrete diffusion models, 2025. URL https://arxiv.org/abs/ 2412.10193. Neta Shaul, Itai Gat, Marton Havasi, Daniel Severo, Anuroop Sriram, Peter Holderrieth, Brian Karrer, Yaron Lipman, and Ricky T. Q. Chen. Flow matching with general discrete paths: kinetic-optimal perspective, 2024. URL https://arxiv.org/abs/2412.03487. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K. Titsias. Simplified and generalized masked diffusion for discrete data, 2025. URL https://arxiv.org/abs/ 2406.04329. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/abs/ 1503.03585. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.org/abs/2010.02502. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=PxTIG12RRHS. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104. 09864. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Dimitri von Rütte, Janis Fluri, Yuhui Ding, Antonio Orvieto, Bernhard Schölkopf, and Thomas Hofmann. Generalized interpolating discrete diffusion, 2025. URL https://arxiv.org/ abs/2503.04482. 15 Published as conference paper at ICLR 2026 Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking discrete diffusion models with inference-time scaling, 2025. URL https://arxiv.org/abs/2503. 00307. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification, 2016. URL https://arxiv.org/abs/1509.01626. Yixiu Zhao, Jiaxin Shi, Feng Chen, Shaul Druckmann, Lester Mackey, and Scott Linderman. Informed correctors for discrete diffusion models, 2025. URL https://arxiv.org/abs/ 2407.21243. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling, 2025. URL https://arxiv.org/abs/2409.02908. 16 Published as conference paper at ICLR"
        },
        {
            "title": "2.1 Discrete Diffusion Models",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.2 Diffusion Guidance .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Ψ-Samplers . . ."
        },
        {
            "title": "5.2 Fast Curriculum .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "8 Acknowledgements",
            "content": "A Ψ-Posteriors A.1 Approximate Reverse Marginals . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Proof that the Ψ-posteriors have the correct marginals . . . . . . . . . . . . . . . . A.3 Negative Evidence Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Recovering Predictor-Corrector Methods for Masked Diffusion . . . . . . . . . . . A.5 ReMDM Sampling Schedules . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Fast Curriculum",
            "content": "B.1 Sampling the top-k values in wℓ Without Materializing The Full Vector . . . . . . B.2 Approximating the Weighted Sum of the Embeddings . . . . . . . . . . . . . . . . B.3 Efficient computation of During Training . . . . . . . . . . . . . . . . . . . . . B.4 Implementation of the Fast Curriculum . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Experimental Details",
            "content": "C.1 Ψ-samplers . . . . . . C.2 Improved Curriculum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Downstream Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Zero-Shot Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Additional Experimental results",
            "content": "D.1 Tuning κt for the Ψ-samplers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 1 2 5 5 6 7 8 10 10 11 18 18 19 19 20 20 20 25 28 29 29 30 31 31 31 Published as conference paper at ICLR 2026 D.2 Distribution of the top entries of the softmax . . . . . . . . . . . . . . . . . . . . D.3 Training Efficiency of Our Fast Curriculum . . . . . . . . . . . . . . . . . . . . . 32 32 Ψ-POSTERIORS A.1 APPROXIMATE REVERSE MARGINALS We parameterize the (generative) Ψ-reverse marginals to have similar form as the true posterior (11). Therefore, the generative reverse marginals also factorize over the sequence length. Because x1:L is not available during sampling, there are two terms in (11) that are intractable. First, we choose to replace the posterior qst(.zℓ t, xℓ = xℓ θ). Additionally, as we cannot sample from qs(.xℓ) without xℓ, we replace xℓ by q0t(.zt, xℓ = xℓ θ), ℓ [L]. Replacing these two intractable terms yields our generative reverse marginals: t, xℓ) by qst(.zℓ st(.zt) = κtqst(.zt, = xθ(zt, t)) + (1 κt) (cid:2)αsq0t(.zt, = xθ(zt, t)) + (1 αs)π(cid:3) . Ψθ (15) Note that for the masked posterior (2), q0t(.zt, = xθ(zt, t)) = xθ(zt, t). A.2 PROOF THAT THE Ψ-POSTERIORS HAVE THE CORRECT MARGINALS Let Ψst(.xℓ, zℓ t) denote the Ψ-posteriors defined in (11). Let denote s(k) = t(k 1) and denote t(k). To prove that the Ψ-posteriors have the correct marginals, we proceed by (downwards) sxℓ) can be written as marginalization induction, similar to Song et al. (2022). First, note that Ψs(zℓ over zℓ t, for < t: Ψs(zℓ sxℓ) = (cid:88) zℓ Ψt(zℓ txℓ)Ψst(zℓ szℓ t, x) (16) Base Case Let Ψ1(zℓ Cat(.π). Therefore, the Ψ-posteriors have the correct marginal for = 1. 1xℓ) denote the marginal at time = 1. By definition in (11), Ψ1(zℓ 1xℓ) = Induction hypothesis Suppose that the Ψ-posteriors have the correct marginal for certain 1, that is, Ψt(.xℓ) = qt(.xℓ). Inductive step Based on the induction hypothesis, we now show that Ψs(.xℓ) = qs(.xℓ), for s(k) = t(k 1). Indeed Ψs(.xℓ) (cid:88) (1) = (2) = (3) = zℓ (cid:88) zℓ (cid:88) zt Ψt(zℓ txℓ)Ψst(zℓ szℓ t, xℓ) qt(zℓ txℓ)Ψst(zℓ szℓ t, xℓ) qt(zℓ txℓ) (cid:2)κtqst(zℓ sxℓ, zℓ t) + (1 κt)qs(zℓ sxℓ)(cid:3) (4) = κt (cid:88) zℓ qt(zℓ txℓ)qst(zℓ sxℓ, zℓ t) + (1 κt)qs(zℓ sxℓ) qt(zℓ txℓ) (cid:88) zℓ (5) = κtqs(zℓ sxℓ) + (1 κt)qs(zℓ sxℓ) = qs(zℓ sxℓ). Specifically, (1) hold by (16), (2) by the induction hypothesis, (3) by definition of the Ψ-posteriors, (4) by distributing qt(zℓ txℓ), (5) by definition of marginal probability (first term), and by observing that (cid:80) qt(zℓ txℓ) = 1 since qt is normalized. This concludes the inductive step, and shows that the Ψ-posteriors have the correct marginal. zℓ Published as conference paper at ICLR 2026 A.3 NEGATIVE EVIDENCE LOWER BOUND Let zℓ tion of (xℓ, zℓ 0:1 denote reverse trajectory with time indices {0, 1 0:1) under the generative model factorizes as (cid:89) pθ(xℓ, zℓ 0:1) = p(xℓ zℓ 0)Ψ1(zℓ 1) , 2 , . . . , 1} for token ℓ. The joint distribuΨθ st(zℓ s(i) zℓ t(i)), (17) i=1 where each pair (s(i), t(i)) denotes one reverse transition with s(i) < t(i). The marginal likelihood is pθ(xℓ) = (cid:88) pθ(xℓ, zℓ 0:1). 0:1 zℓ 0:1 xℓ) = Ψ1(zℓ (18) 1 xℓ) (cid:81)T i=1 Ψst(zℓ s(i) zℓ t(i), xℓ), Introducing the variational distribution Ψ(zℓ Jensens inequality results in: log pθ(xℓ) Ψ(zℓ 0:1xℓ) (cid:2) log p(xℓ zℓ 0)(cid:3) + KL(cid:0)Ψ1( xℓ)(cid:13) (cid:13)Ψ1 (cid:1) + (cid:88) i=1 Ψ(zℓ t(i)xℓ) (cid:104)"
        },
        {
            "title": "DKL",
            "content": "(cid:16) Ψst( zℓ t(i), xℓ) (cid:13) (cid:13) Ψθ st( zℓ t(i)) (cid:17)(cid:105) (19) (20) . This expression is similar to the standard diffusion NELBO, with reconstruction term, prior term at t=1, and sum of KL divergences. As , p(xℓ zℓ 0) concentrates around xℓ, hence log p(xℓ zℓ 0) 0. Furthermore, the prior term is zero by definition of the Ψ-posteriors in (11). A.4 RECOVERING PREDICTOR-CORRECTOR METHODS FOR MASKED DIFFUSION Wang et al. (2025) introduce the ReMDM posterior, which generalizes the MDM posterior (2) by allowing previously decoded tokens to be remasked. For given position ℓ, the ReMDM posterior is qσ(zℓ zℓ t, xℓ) = Cat(cid:0).; (1 σt) xℓ + σt m(cid:1), .; αs(1σt)αt 1αt"
        },
        {
            "title": "Cat",
            "content": "(cid:16) xℓ + 1αsσtαt 1αt zℓ = m, (cid:17) , zℓ = m, (21) where σt [0, σmax ] is free parameter that controls the remasking probability. The upper bound σmax := min{1, (1 αs)/αt} ensures that (21) defines valid distribution. When σt = 0, the ReMDM posterior reduces to the standard MDM posterior. Below, we show that the Ψ-posteriors recover the ReMDM posterior with the substitution κt = 1 σt/(1 αs). Suppose that we work with Masked diffusion, hence π = m. The Ψ-posteriors can be expanded as Ψst(.zℓ t) = κtqst(.zℓ t, xℓ) + (1 κt) (cid:2)αsq0t(.zℓ (22) + (1 κt) (cid:2)αsxℓ + (1 αs)m(cid:3) = κt Cat(.; zℓ (cid:18)"
        },
        {
            "title": "Cat",
            "content": ".; t), (1 αs)m + (αs αt)xℓ 1 αt t, xℓ) + (1 αs)π(cid:3) zℓ = m, zℓ = (cid:19) , (1) = κt Cat(.; xℓ), (cid:18)"
        },
        {
            "title": "Cat",
            "content": ".; (1 αs)m + (αs αt)xℓ 1 αt zℓ = m, zℓ = (cid:19) , + (1 κt) (cid:2)αsxℓ + (1 αs)m(cid:3) (23) (cid:40) Cat(.; κtxℓ + (1 κt)[αsxℓ + (1 αs)m]), (cid:16)"
        },
        {
            "title": "Cat",
            "content": ".; κt (1αs)m+(αsαt)xℓ 1αt (cid:17) + (1 κt)[αsxℓ + (1 αs)m] zℓ = , zℓ = (24) (25) (cid:40) Cat(.; [κt + (1 κt)αs]xℓ + (1 κt)(1 αs)m), + (1 κt)αs (cid:104) κt xℓ +"
        },
        {
            "title": "Cat",
            "content": "κt (cid:16) .; (cid:105) (cid:104) αsαt 1αt 1αs 1αt + (1 κt)(1 αs) (cid:105) (cid:17) zℓ = , zℓ = , (26) = = where (1) holds since zℓ either clean token or the masked token. = implies that zℓ = xℓ, since in Masked diffusion, the latents zℓ are 19 Published as conference paper at ICLR 2026 To conclude, if we pick κt = 1 σt , where σt is the free parameter in the ReMDM sampler, 1αs then the equation reduces to the ReMDM posterior. Therefore, the Ψ-posteriors generalize ReMDM, which itself generalized the FB (Campbell et al., 2022) and DFM (Gat et al., 2024) posteriors. Additionally, the Ψ-posteriors are not limited to Masked diffusion, as we showed in this work. In Table 11, we sample from the ReMDM-equivalent Ψ-samplers, comparing the official ReMDM implementation and our version, and find obtain similar performance. A.5 REMDM SAMPLING SCHEDULES Wang et al. (2025) introduce three schedules for the remasking parameter σt, which controls how aggressively previously generated tokens are remasked. As shown in Suppl. A.4, the Ψ-samplers recover ReMDM when κt = 1 σt/(1 αs), and (σt)1 t=0 denotes the ReMDM noise injection schedule. Therefore, each ReMDM σt schedule has direct Ψ-samplers equivalent. Below, we present the three σt schedules studied in Wang et al. (2025). The schedules must satisfy 0 σt σmax := min{1, (1 αs)/αt} to ensure that the reverse posterior remains valid distribution, and are generally defined in terms of σmax and parameter η [0, 1]. Cap Schedule With the Cap schedule, the remasking probability is capped at constant η [0, 1], as long as it remains in the valid bounds: σt = min(cid:8)η, σmax (cid:9). (27) Rescale Schedule With the Rescale schedule, σt is obtained by scaling the upper bound σmax constant η [0, 1]: by σt = η σmax . (28) Loop Schedule Unlike the cap and rescale schedules, which only modulate σt, the Loop schedule also changes the evolution of during sampling (see Fig. 4 for an illustration). It is controlled by the time boundaries ton > toff. The noise schedule αt is reparametrized to be piecewise linear. First, αt increases linearly from 0 to αton when > ton. Secondly, when [toff, ton], it is held constant to αton , and finally increases from αton to 1 when < toff. When / [toff, ton], ReMDM samples from the MDM posterior (2). When [toff, ton], since αt = αs = αton, the model samples from the ReMDM posterior (21) with constant σt = η. In practice, ton is chosen so that αton is close to 1, where most tokens have already been decoded."
        },
        {
            "title": "B FAST CURRICULUM",
            "content": "In this section, we expand on the implementation of the efficient curriculum. In Sec. B.4, we present the overall design, pseudocode, and the three main implementation challenges. Our approach relies on several mathematical results, which we present in order of dependency. We first recall inverse transform sampling (Sec. B.1.1), then derive the distributions of the largest (Sec. B.1.2) and second largest (Sec. B.1.3) uniform order statistics. These results enable generating the largest Gaussian random variables out of without materializing the full vector (Sec. B.1.4). We also derive closed-form expression for the conditional mean of the exponential of Gaussian random variable (Sec. B.2), used to estimate the softmax normalizer. Furthermore, although the efficient curriculum could be implemented using the original definition of the Diffusion Transformation Operator , we show that admits convenient series expansion in Sec. B.3.1. This avoids the need to precompute 100k function values, and simplifies the implementation. Finally, in Sec. B.3.2, we show that can be well approximated by degree-9 polynomial, which removes the need to store large number of coefficients during training. B.1 SAMPLING THE TOP-k VALUES IN wℓ WITHOUT MATERIALIZING THE FULL VECTOR RK, but explicitly materializing wℓ Computing our curriculum step requires access to the largest entries of the diffused weight vector wℓ (and then running full top-k) is prohibitively expensive when is large. In this subsection, we show how to obtain the top-k values and their associated indices while using only O(k) memory and without simulating all random variables. The key 20 Published as conference paper at ICLR 2026 idea is to decouple values from locations: we first sample the top-k Gaussian order statistics directly via inverse transform sampling (Sec. B.1.1), leveraging closed-form expressions for uniform order statistics (Sec. B.1.2, Sec. B.1.3) and numerically stable log-space implementation (Algo. 1). We then assign these top-k to their corresponding indices (Sec. B.1.5, Algo. 2). This yields an efficient routine whose cost scales with rather than K, enabling top-k truncation of wℓ without ever materializing it. B.1."
        },
        {
            "title": "INVERSE TRANSFORM SAMPLING",
            "content": "The Inverse Transform Sampling method (Devroye, 1986) is an algorithm for generating continuous random variables with known Cumulative Distribution Function (CDF) FX . Implementing Inverse Transform Sampling requires access to the inverse CDF 1 , and source of i.i.d uniform random variables. If = 1 (U ), where U[0, 1], then FX . Indeed, for R, (U ) x) = P(U FX (x)) = FX (x), P(X x) = P(F 1 (29) since for [0, 1], P(U a) = a. This shows that has the correct distribution. B.1.2 DISTRIBUTION OF THE LARGEST UNIFORM RANDOM VARIABLE OUT OF The distribution of the largest uniform random variable out of admits simple closed-form expression: Proposition B.1 (Distribution of the largest uniform random variable out of K). (1) (2) ... (K) denote an order statistic over i.i.d uniform random variables U([0, θ]) with Cumulative Density Function (CDF) FU . Suppose that [0, 1], then FU (u) = θ . Then, the CDF FU (1) and probability density function (PDF) fU (1) of the largest random variable (1) are as follows: FU (1)(u) = fU (1)(u) = KF K1 (u) = uKθK (u)fU (u) = KuK1θK Proof. FU (1)(u) = P(U (1) u) = P(Ui u)i[K] = (U u)K = (u). The PDF is obtained by differentiation: fU (1)(u) = du FU (1)(u) = KF U (u)fU (u), (30) (31) (32) B.1.3 DISTRIBUTION OF THE LARGEST UNIFORM RANDOM VARIABLE OUT OF In this part, we show how to sample the largest uniform random variables out of K. Importantly, we do not need to draw all values and sort them, which would be impractical for large K. Let (1) (K) denote the order statistics of i.i.d. U[0, 1] random variables. We argue that for 1 < K, conditioned on (i) = u(i), (i+1) is distributed as the largest out of uniform random variables on [0, u(i)]. This enables an iterative scheme to generate the largest variables in decreasing order. The argument relies on two standard results: the conditional density formula (Prop. B.2) and the joint density of pair of order statistics (Prop. B.3). The conditional distribution of (i+1) (i) = u(i) is given in Prop. B.4. Proposition B.2 (Conditional Density (Berger & Casella, 2001)). Let X, be two random variables with joint density fX,Y and marginals fX , fY . Then, the conditional density of given = is fXY =y(xy) = fX,Y (x, y) fY (y) . 21 (33) Published as conference paper at ICLR 2026 Proposition B.3 (Joint Density of Order Statistics (Berger & Casella (2001); proof in Border (2021))). Let (1) (K) denote the order statistics of random variables with CDF and PDF , arranged in descending order. Then, the joint density of (n) and (m), where < (so that (n) (m)), is given by fX (n),X (m)(u, v) = K! (n 1)!(m 1)!(K m)! (1 (u))n1 (F (u) (v))mn1 (v)Km (u) (v). (34) Proposition B.4 (Conditional Distribution of (i+1) given (i)). Let (1) (K) denote the order statistics of independent and uniformly distributed random variables on [0, 1], arranged in descending order. For any 1 < K, conditioned on (i) = u(i), (i+1) is distributed as the largest of Ki i.i.d. uniform random variables on [0, u(i)]. Proof. From Prop. B.3 with = and = + 1, the joint density of (U (i), (i+1)) for U[0, 1] variables (where (u) = and (u) = 1) is fU (i),U (i+1)(u(i), u(i+1)) = K! (i 1)! (K 1)! (1 u(i))i1 (u(i+1))Ki1, (35) since (F (u(i)) (u(i+1)))lk1 = 1 as = 1. To apply Prop. B.2, we need the marginal density of (i). The event {u(i) (i) u(i) + du} requires that, among the i.i.d. draws, exactly 1 fall in (u(i) + du, 1], exactly one falls in [u(i), u(i) + du), and the remaining fall in [0, u(i)). The number of such assign- (i1)! 1! (Ki)! , and the probability of each assignment is ments is the multinomial coefficient (cid:0)1 u(i) du(cid:1)i1 . Multiplying these terms gives du (cid:0)u(i)(cid:1)Ki K! (u(i) (i) u(i) + du) = K! (i 1)! (K i)! (1 u(i) du)i1 du (u(i))Ki. (36) By definition, fU (i)(u(i)) = limdu0 as du 0, we obtain (u(i)U (i)u(i)+du) du . Since (1u(i) du)i1 (1u(i))i1 fU (i)(u(i)) = K! (i 1)! (K i)! (1 u(i))i1 (u(i))Ki. (37) Applying Prop. B.2: fU (i+1)U (i)(u(i+1) u(i)) = = fU (i),U (i+1)(u(i), u(i+1)) fU (i)(u(i)) K! (i1)! (Ki1)! (1 u(i))i1 (u(i+1))Ki1 (i1)! (Ki)! (1 u(i))i1 (u(i))Ki K! (38) = (K i) (u(i+1))Ki1 (u(i))Ki , since the factors K!/(i 1)! and (1 u(i))i1 cancel. This is precisely the density of the largest of Ki i.i.d. U[0, u(i)] random variables. B.1.4 GENERATING THE LARGEST GAUSSIAN RANDOM VARIABLES OUT OF We now show that it is possible to generate the largest Gaussian random variables out of via inverse transform sampling (Sec. B.1.1) as follows. Given single uniform random variable U[0, 1], one can obtain standard Gaussian random variable = Φ1(U ), where Φ is the Gaussian CDF, via inverse transform sampling. Now Published as conference paper at ICLR 2026 assume we have sorted list of uniform random variables U1 U2 ... UK. Since Φ is monotonically increasing function, the largest uniform random variable, U1, is mapped to the largest Gaussian random variable, i.e. Φ1(U1) is distributed as the largest Gaussian random variable out of K. Sampling The Largest As shown in Prop. B.1, the CDF of the largest uniform random variable out of has an analytical solution. For [0, 1], (U1 u) = uK, hence it can be generated via inverse transform sampling. Sampling The Second Largest Furthermore, the distribution of the second largest, conditioned on U1 = u1, also admits closed-form solution (Sec. B.1.3): for u2 [0, u1], it is given by (U2 u2U1 = u1) = uK1 , i.e. it is distributed as the largest uniform variable out of 2 1, supported on [0, u1]. u(K1) 1 Sampling The kth-largest More generally, the same argument shows that conditioned on Ui = ui, the random variable Ui+1 is distributed as the largest uniform variable on [0, ui] out of + 1. This shows that we can sample U1, ..., Uk in decreasing order and without simulating all the variables. Finally, the largest Ui can be transformed into the largest standard Gaussians out of as {Φ1(Ui)}k i=1. In practice, naive implementation of inverse transform sampling is numerically unstable when is large. For stability, operations should be implemented in log-space. Algo. 1 shows the pseudocode of the log-space implementation. Algorithm 1 Reverse Sampling from Order Statistics of Gaussian Random Variables. Here corresponds to K1 (the number of zero-mean entries) and σ to σt = (cid:112)1 α2 . Input Number of variables , standard deviation σ, number of top values Sample Uℓ U(0, 1), for ℓ + 1 Compute the random variables: Rℓ = log Uℓ Compute the cumulative sums: Pℓ = (cid:80)N Let Vℓ = exp(Pℓ), the ℓ-th sample from the (uniform) order statistic. Apply inverse normal CDF: (ℓ) = Φ1(Vℓ) σ return {X (ℓ)}N k+1 ℓ m=ℓ Rm ℓ=N B.1.5 SAMPLING INTEGERS WITHOUT REPETITIONS AND WITHOUT SHUFFLING Suppose that denotes the one-hot vector of category o. By symmetry, after applying Gaussian diffusion to x, all entries (xj)j=o such that follow the exact same distribution. Therefore, they have the same probability of being one of the top largest random variable. To implement the curriculum, we must not only approximate the weights of the embedding combination but also select which embeddings to include. As described in Sec. 4, we sample random indices without repetition excluding i. If the random variable at position o, corresponding to the clean token, belongs to the top-k, we replace one of the sampled indices with o. Otherwise, we use the sampled indices directly. simple way to sample random indices without repetition is to shuffle list of integers and take the first k. However, this defeats the purpose of our efficient curriculum, as it requires materializing large tensors. Instead, Floyds algorithm (Bentley, 1999), given in Algo. 2, samples without repetition while avoiding shuffling. Although sequential with iterations, it is much faster than shuffling when K. 23 Published as conference paper at ICLR 2026 Algorithm 2 Floyds Algorithm for Sampling Without Repetition Input Number of possible values , number of samples k. Initialize array of size to store samples for = 0 to 1 do Sample Randint(0, + t) if > 0 and appears in S[0 : t] then S[t] + {Use largest remaining value} else S[t] end if end for return B.2 APPROXIMATING THE WEIGHTED SUM OF THE EMBEDDINGS After extracting the top-k values and indices (K, I) from wℓ t, we approximate the softmax-weighted embedding by retaining only the selected rows of the embeddings RKd (d denotes the embedding size): softmax(wℓ t)embeddings (cid:88) i=1 exp(Ki/τ ) embeddings[Ii], (39) where embeddings[j] denotes the j-th row. The normalizer includes both sampled (top-k) and unsampled terms. To account for this contribution, let µ = E[exp(X/τ ) < Kk] (40) denote the expectation that r.v. with pmf (0, σ2 We approximate via two cases. Let be the index corresponding to the clean-token category in xℓ, and let (αt, σ2 Case 1. If is not among the top-k indices (i.e., / I, and consequently / K), then includes: (i) the terms in K, (ii) the explicit contribution from index o, and (iii) the remaining 1 unsampled terms, each approximated by µ. Thus, ) is less than Kk. ). (cid:88) exp (cid:19) (cid:18) Ki τ + exp (cid:19) (cid:18) τ i=1 (cid:124) (cid:123)(cid:122) top-k terms (cid:125) (cid:124) (cid:123)(cid:122) index (cid:125) + (K 1)µ (cid:123)(cid:122) (cid:125) unsampled terms (cid:124) . (41) Case 2. If is among the top-k indices (i.e., I, and hence, K), its contribution is already included in the top-k sum, leaving unsampled terms. Hence, (cid:88) exp (cid:19) (cid:18) Ki τ i=1 (cid:124) (cid:123)(cid:122) top-k terms (cid:125) + (K k)µ (cid:124) (cid:125) (cid:123)(cid:122) unsampled terms . (42) Next, we derive closed-form expression for µ in (40). µ = log E[exp(X) < Kk] = σ2 2τ 2 log Φ (cid:19) (cid:18) Kk σt + log Φ (cid:18) Kk σ2 /τ (cid:19) σt . (43) 24 Published as conference paper at ICLR 2026 Proof. µ = log (cid:20) exp (cid:19) (cid:18) τ (cid:21) < Kk Applying change of variables = X/τ ; we get, = log E[exp( X) < Kk/τ ] (x) P( < Kk/τ ) exp(x) (cid:90) Kk/τ = log dx Substituting σ := σt/τ , we get: 1 Φ(Kk/σt) (cid:90) Kk/τ exp(x) 1 2πσ2 (cid:18) exp (cid:35) dx (cid:19) x2 2σ2 (cid:34) (cid:34) (cid:34) (cid:34) = log = log = log = log = log = σ2 2 1 Φ(Kk/σt) 1 2πσ2 1 Φ(Kk/σt) 1 2πσ2 (cid:90) Kk/τ (cid:90) Kk/τ Φ (cid:90) Kk/τ 1 2πσ2 (cid:18) Kk/τ σ2 σ exp(σ2/2) Φ(Kk/σt) (cid:20) exp(σ2/2) Φ(Kk/σt) (cid:18) Kk σt log Φ (cid:19) + log Φ (cid:18) Kk/τ σ2 σ (cid:19) (cid:18) exp (cid:19) x2 2σ2 + (cid:35) dx (cid:35) dx (cid:18) exp (cid:18) (cid:19) 1 2σ2 (x2 2σ2x + σ4 σ4) (cid:19) (cid:35) 1 2σ2 (x σ2)2 dx exp (cid:19)(cid:21) Substituting back σ := σt/τ , we get: = σ2 2τ 2 log Φ (cid:19) (cid:18) Kk σt + log Φ (cid:18) Kk σ2 /τ (cid:19) σt This concludes our proof. Finally, substituting the value of µ from (44) into (41) and (42), we obtain: (cid:88) i=1 (cid:124) exp (cid:19) (cid:18) Ki τ (cid:123)(cid:122) top-k terms (cid:125) + δ exp (cid:19) (cid:18) τ + (K δ) exp (cid:124) (cid:123)(cid:122) clean token (cid:125) (cid:124) (cid:18) σ2 (cid:19) (cid:18) Kk σt 2τ 2 log Φ (cid:123)(cid:122) unsampled zero-mean terms + log Φ (cid:18) Kk σ2 /τ σt (44) (cid:19)(cid:19) , (cid:125) (45) B.3 EFFICIENT COMPUTATION OF DURING TRAINING The curriculum objective in (9) requires evaluating the diffusion transformation operator (). Directly computing via (6) is prohibitively expensive during training; Sahoo et al. (2025a) therefore precompute and cache many (αt, (αt)) pairs, which is cumbersome. Instead, we compute () on the fly using its Taylor expansion, detailed below. This derivation relies on two propositions that justify swapping the order of (i) summation and integration (Prop. B.5) and (ii) differentiation and integration (Prop. B.6). Proposition B.5 (First Corollary of the Dominated Convergence Theorem (Folland (1999), Theorem 2.25)). If the sum (cid:80) n=0 fn(x) exists for all and there exists an integrable function g(x) such that for all k, then (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) n=0 (cid:12) (cid:12) (cid:12) fn(x) (cid:12) (cid:12) g(x) (cid:90) (cid:88) n=0 fn(x)dx = (cid:88) (cid:90) n=0 fn(x)dx. (46) (47) Published as conference paper at ICLR 2026 Proposition B.6 (Second Corollary of the Dominated Convergence Theorem (Folland (1999), Theorem 2.27)). Let (x, t) be differentiable in and suppose there exists function g(x, t) such that: (x,t) (cid:12) (cid:12) (cid:12) g(x, t0) for all and in some neighborhood t0 δ 1. (cid:12) (cid:12) (cid:12) 2. (cid:82) g(x, t)dx < for all t"
        },
        {
            "title": "Then",
            "content": "d dt (cid:90) (x, t)dx = (cid:90) (x, t) dx (48) B.3.1 SERIES REPRESENTATION OF AND tT Evaluating the series expansion on the fly is faster than precomputing and caching for many (αt, (αt)) pairs as done by Sahoo et al. (2025a). We can further speed the evaluation up by fitting low-degree polynomial to the series, which we use in practice (see Suppl. B.3.2 for the approximation error analysis). Below, we derive the series expansion for (Prop. B.7) and its time-derivative tT (Prop. B.8): Proposition B.7 (Series Expansion of the Diffusion Transformation Operator). The diffusion transformation operator can be expressed as: (αt) = (cid:34) eν2 /2 1 (cid:88) n=0 νn n! Mn (cid:35)"
        },
        {
            "title": "1\nK",
            "content": "νt = αt 1 α2 and Mn = (cid:82) znϕ(z)ΦK1(z)dz. Proof. Recall that the standard Gaussian PDF is given by ϕ(x) = 1 2π ex2/2. For notational convenience, let νt = αt 1 α2 . We can rewrite ϕ(x νt) in terms of ϕ(x): ϕ(x νt) = 1 2π e(xνt)2/2 = 1 2π e(x22νtx+ν2 )/2 = ϕ(x)eνtxeν2 /2. Using the definition of the infinite series of ex, we can expand eνtx: ϕ(x νt) = ϕ(x)eν2 /2 (cid:88) n=0 νn xn n! . (49) (50) (51) (52) Substituting this into our original integral: (cid:90) ϕ (z νt) ΦK1(z)dz = (cid:90) ϕ(z)eν /2 (cid:88) n=0 νn zn n! ΦK1(z)dz (53) Since Prop. B.5 is satisfied, as the sum is the Taylor series of the exponential function, we can exchange the order of integration and summation. This leads to our final result: (cid:90) ϕ (z νt) ΦK1(z)dz = eν2 /2 = eν /2 26 znϕ(z)ΦK1(z)dz (54) (cid:88) n=0 (cid:88) n=0 νn n! νn n! (cid:90) Mn. Published as conference paper at ICLR 2026 Advantages At this point, one might ask what is gained by expressing as series expansion. There are two key advantages. First, since is intractable, Sahoo et al. (2024) resort to precomputing 100k evaluations, which can take up to two hours with the GPT-2 tokenizer. Second, they approximate the time derivative using finite differences. Crucially, observe that Mn and In in Prop. B.7 and B.8 are the only intractable components of the series expansion, and they are independent of the input αt. We find that the terms of the series decay to zero after roughly 150 terms (with slower decay as 1). Thus, instead of pre-computing 100k evaluations of , it suffices to cache Mn and In for < 150. In practice, this takes only few seconds and can be performed at the start of training. Proposition B.8 (Time-Derivative of the Diffusion Transformation Operator). The time-derivative of the diffusion transformation operator can be expressed as: dt (αt) = /2 eν2 α (1 α2 )3/2 (cid:88) n=0 νn n! [In νtMn] (55) where νt and Mn are defined as in Prop. B.7. Finally, In = (cid:82) denotes the time-derivative of the Gaussian noise schedule αt. zn+1ϕ(z)ΦK1(z)dz, and α Proof. We want to compute dνt (αt) = (cid:90) 1 dνt ϕ(z νt)ΦK1(z)dz. (56) To justify passing the derivative under the integral, we verify the conditions of Prop. B.6. Define (cid:32) (z, t) = ϕ (cid:33) αt (cid:112)1 α2 ΦK1(z) = ϕ (z νt) ΦK1(z), which has time derivative (z, t) = (z νt)ϕ(z νt) (1 α )3/2 ΦK1(z). (57) (58) We need to find suitable dominating function g. Let 1 > δ0 > 0 and choose t0 = 1δ0 . When 2 t0 δ0, we have [t0 δ0, t0 + δ0]. Since t0 δ0 < t0 < 1 and t0 + δ0 = 1δ0 2 + δ0 < 1, we are guaranteed that < 1. This ensures that νt is finite. Because αt [0, 1) when < 1, there exists constant C, such that := max tt0δ0 1 (1 α2 )3/2 < . (59) For and t0 δ0, we can bound the absolute value of the time derivative of as follows: (cid:12) (cid:12) (cid:12) (cid:12) (z, t) (cid:12) (cid:12) (cid:12) (cid:12) = νt (1 α )3/2 ϕ(z νt) ΦK1(z) Cz νtϕ(z νt) = g(z, t). Finally, for all [0, 1): (cid:90) g(z, t)dz = = (cid:90) (cid:90) νtϕ(z νt)dz = (cid:90) zϕ(z)dz zϕ(z)dz = 2C (cid:90) zϕ(z)dz (cid:90) = 2C 0 (cid:90) 0 ez2/2dz 1 2π zez2/2dz = = 2C 2π 2C 2π 0 1 = (cid:114) 2 π 27 < , (60) Published as conference paper at ICLR 2026 Figure 5: Polynomial approximation and approximation error, compared to the series approximation, truncated at 150 terms. The degree-9 polynomial (left) achieves orders of magnitude lower error than the degree-5 polynomial (center) and sigmoid (right) approximations. where we used the substitution = z2/2 in the integral (cid:82) The conditions of Prop. B.6 are satisfied, so we can pass the derivative under the integral. Applying the derivative under the integral sign and using the identity ϕ(z νt) = ϕ(z)eνtzeν2 we have: 0 zez2/2dz to obtain (cid:82) 0 eudu = 1. /2, dνt ϕ(z νt) = ϕ(z) [eνtzν /2] dνt = ϕ(z)eνtzν2 = (z νt)ϕ(z νt) /2(z νt) Therefore: dνt (αt) = 1 (cid:90) (z νt)ϕ(z νt)ΦK1(z)dz Now using the Taylor series of ϕ(z νt), found earlier, and inverting the sum and integral as before, we find dνt (z νt)ϕ(z)eνtzeν2 /2ΦK1(z)dz (αt) = (cid:90) 1 eν2 1 / /2 eν2 1 (cid:88) n=0 (cid:88) n=0 = = (cid:20)(cid:90) zn+1ϕ(z)ΦK1(z)dz νt znϕ(z)ΦK1(z)dz (cid:21) (cid:90) [In νtMn] . νn n! νn n! (61) (62) (63) where In = (cid:82) zn+1ϕ(z)ΦK1(z)dz and Mn = (cid:82) znϕ(z)ΦK1(z)dz. B.3.2 POLYNOMIAL APPROXIMATION OF Because the Diffusion Transformation Operator has sigmoid-like shape, we approximate it with S-shaped functions that require only handful of coefficients. This allows us to store fewer parameters during training, instead of the 100k values required by the original curriculum or the 300 coefficients from the series approximation. Concretely, we test several functional forms with fewer than 10 parameters and fit them using non-linear least squares, via scipy.optimize.curve_fit. As shown in Fig. 5, approximations tend to be less accurate at the boundaries, when 0 or 1. We find that the degree-9 polynomial works better than sigmoid function of the form aσ(bt + c) + d, especially at the boundaries. B."
        },
        {
            "title": "IMPLEMENTATION OF THE FAST CURRICULUM",
            "content": "In Sec. 4, we described our efficient curriculum. Here we provide the pseudocode (Algo. 3) and elaborate on the three main implementation challenges: First, we need to sample the largest zero-mean Gaussian random variables out of K, to emulate the Gaussian Diffusion over the one-hot data samples (Sec. B.1.4). 28 Published as conference paper at ICLR 2026 t)o (αt, σ Algorithm 3 Scalable Top-k Curriculum Weights (Sec. 4). Recall that wℓ gory o. Hence, (wℓ to approximate softmax(wℓ = αtxℓ + σtϵ, ϵ (0, IK), where xℓ is the one-hot vector representation of catet ). The goal is ) and the remaining K1 entries are i.i.d. (0, σ2 t/τ ) using only the largest entries (k K). Input: Clean token index [K], vocabulary size K, top-k count k, temperature τ , Gaussian schedule (αt, σt) Output: Approximate softmax weights λ [0, 1]k and corresponding token indices [K]k Step 1: Sample the largest values of wℓ K1 Kk top-k order statistics of (K1) i.i.d. (0, σ2 (αt, σ2 ) without materializing all entries ) draws Algo. 1 t)o Step 2: Build the top-k set and approximate the softmax normalizer Clean-token value: sample of (wℓ if > Kk then µ E[exp(X/τ ) < Kk1], (0, σ2 ) (Suppl. B.2) (cid:12) (cid:12){j [k] : Kj > w}(cid:12) (cid:12) (K1:r, w, Kr+1:k1) Sample k1 indices uniformly w/o replacement from [K] {o} (J1:r, o, Jr+1:k1) (cid:80)k i=1 exp(Ki/τ ) + (K k) µ else Case 2 : clean token belongs to the top-k Mean contribution unsimulated entry Rank of in Insert w, drop the smallest Kk Algo. 2 Kk unsimulated entries, each contributing µ µ E[exp(X/τ ) < Kk], (0, σ2 Sample indices uniformly w/o replacement from [K] {o} (cid:80)k i=1 exp(Ki/τ ) + (Kk1) µ + exp( w/τ ) Case 1 : clean token is not in the top-k ) Mean contribution unsimulated entry (Suppl. B.2) Algo. 2 counted exactly, Kk1 via µ end if λi exp(Ki/τ ) / Z, return λ, Step 3: Normalized softmax weights over the selected entries = 1, . . . , Secondly, we must estimate the normalization constant of the softmax, without actually sampling the random variables (Sec. B.2). Third, we require an efficient method to sample distinct integers from without replacement (Sec. B.1.5). Algo. 3 shows the pseudocode of the complete algorithm."
        },
        {
            "title": "C EXPERIMENTAL DETAILS",
            "content": "C.1 Ψ-SAMPLERS C.1.1 OPENWEBTEXT To evaluate the samplers, we use the pre-trained MDLM (Sahoo et al., 2024) and Duo (Sahoo et al., 2025a) checkpoints, as well as their distilled variants (using SDTT (Deschenaux & Gulcehre, 2025) and discrete consistency distillation, respectively, after 5 rounds of 10k steps). We re-state the training hyperparameters of both models in Suppl. C.2.1. For ReMDM, we use both the official implementation of Wang et al. (2025) and our re-implementation, which matches the original results while supporting additional sampling schedules beyond the log-linear one. See Suppl. D.1 for details on selecting κt. 29 Published as conference paper at ICLR 2026 C.1.2 CIFAR10 (D3PM-LIKE ARCHITECTURE) We train U-Net backbone (Ronneberger et al., 2015) for 1.5M steps with batch size of 128, using class conditioning with class-dropout rate of 0.1 (as in Schiff et al. (2025)), and the default hyperparameters of Austin et al. (2023)  (Table 3)  . For both MDLM and Duo, we experiment with time-conditional and unconditional variants, and train models using either cosine or log-linear noise schedules. See Table 6 for the ancestral-sampling evaluation of all variants after pre-training. See Suppl. D.1 for details on selecting κt. C."
        },
        {
            "title": "IMPROVED CURRICULUM",
            "content": "C.2.1 LANGUAGE MODELING Table 3: Model architecture on CIFAR"
        },
        {
            "title": "Value",
            "content": "256 2 128 (1,2,2,2) 16 128 35.8M We adopt the same setup as prior work on discrete diffusion (Lou et al., 2024; Sahoo et al., 2024; 2025a), and restate it for completeness. LM1B We detokenize the One Billion Words (Chelba et al., 2014) as in Lou et al. (2024); Sahoo et al. (2024)1, and tokenize it using the bert-base-uncased tokenizer (Devlin et al., 2019), as He et al. (2022). We use context length of 128 and pad shorter documents. OpenWebText We tokenize OpenWebText (Gokaslan & Cohen, 2019) with the GPT-2 tokenizer, concatenate sequences to length of 1024, and insert an eos token between documents. Since the dataset lacks an official validation split, we reserve the last 100k documents for validation. Backbone We parameterize all models using the modified diffusion transformer architecture of Peebles & Xie (2023), following Lou et al. (2024); Sahoo et al. (2024). Our models use 12 layers, hidden dimension of 768, 12 attention heads, and timestep embedding of size 128 for the uniform-state diffusion variants. Word embeddings are not tied between input and output. Curriculum Lookup For the Duo baseline, we train models using the original code. To implement the efficient curriculum, we replace the full linear combination of embeddings by sparse lookup, implemented using torch.nn.functional.embedding_bag to avoid materializing intermediate tensors. The curriculum phase lasts for the first 500k steps, after which we perform regular embedding table lookups, just like Sahoo et al. (2025a). Optimization We train all models with the AdamW optimizer (Loshchilov & Hutter, 2019) using batch size of 512. The learning rate is linearly warmed up from 0 to 3 104 over 2,500 steps, then kept constant for the remainder of training. We apply dropout rate of 0.1 throughout. C.3 DOWNSTREAM EVALUATION PROTOCOL We evaluate downstream performance using the lm-eval-harness library (Gao et al., 2024), following the protocol of Deschenaux et al. (2025). We focus on multiple choice tasks, where the log-likelihood of each candidate answer, given prompt, is computed and the answer with the highest score is selected. For diffusion language models, which optimize variational bound on the log-likelihood of the full sequence, we adapt the evaluation by using Bayes rule: log p(yix) = log p(x, yi) log p(x) log p(x, yi), (64) Since log p(x) does not depend on the candidate yi, we simply select the answer that maximizes log p(x, yi). In practice, we use the log-likelihood ELBO (4), estimated via Monte Carlo with 1024 samples, and choose the continuation yi with the highest estimated likelihood. 1https://github.com/louaaron/Score-Entropy-Discrete-Diffusion/blob/ main/data.py 30 Published as conference paper at ICLR 2026 C.4 ZERO-SHOT LIKELIHOOD Our setting is the same as used by Sahoo et al. (2025a). Specifically, we measure the likelihood of the models trained on OpenWebText using the validation splits of seven diverse datasets: Penn Tree Bank (PTB; Marcus et al. (1993)), Wikitext (Merity et al., 2016), One Billion Words (LM1B; Chelba et al. (2014)), Lambada (Paperno et al., 2016), AG News (Zhang et al., 2016), and Scientific Papers (Pubmed and Arxiv subsets; Cohan et al. (2018)). The datasets are detokenized following the protocol of Lou et al. (2024); Sahoo et al. (2025a). We wrap all sequences to maximum length of 1024 tokens and do not insert eos tokens between them. Table 5 shows that we reach similar performance as Duo."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "In Suppl. D.1, we elaborate on the impact of κt on the performance of the Ψ-samplers. In Suppl. D.2, we show that our efficient curriculum produces weights with the same marginal distributions as Sahoo et al. (2025a). D.1 TUNING κt FOR THE Ψ-SAMPLERS As discussed in Sec. 5.1, the choice of κt is critical for strong performance. With poor choice of κt, Ψ-samplers can underperform ancestral sampling. Below, we report all of our hyperparameter sweeps across datasets. We perform image modeling on CIFAR-10 using the U-Net architecture of Austin et al. (2023); Schiff et al. (2025), and use horizontal flipping as the sole data augmentation. We evaluate Ψ-samplers on OpenWebText (Gokaslan & Cohen, 2019) using the original checkpoint of MDLM (Sahoo et al., 2024) and Duo (Sahoo et al., 2025a). D.1.1 CIFARFigure 6: Ψ-samplers, which generalize ReMDM, significantly improve the Inception Score on CIFAR-10, compared to ancestral sampling. We report FID (Heusel et al., 2018), computed between 50k generated samples and the training set. Before evaluating Ψ-samplers, we ablate on the training hyperparameters. Specifically, we train models with cosine and log-linear noise schedule, optionally with time-conditioning. We sample with both cosine and log-linear schedules. Finally, we check whether nucleus sampling (Holtzman et al., 2020) and greedy decoding on the final step can help, compared to vanilly ancestral sampling. Since nucleus sampling helps Duo but not MDLM, we compare the two models without nucleus sampling. Table 6 shows the validation perplexity and FID for few number of sampling steps. Table 7 reports FID for ancestral sampling using step counts that are powers of two, from 32 up to 4096. Table 8 shows the results with ReMDM. Table 9 reports FID scores for Ψ-samplers using stepwise-constant κ schedule. Table 11 shows the performance of Ψ-samplers using the κ schedule equivalent to ReMDM. We obtain similar results, which supports our theoretical claims. MDLM (Ancestral). Training with cosine noise schedule and time conditioning yields the best validation perplexity and FID. MDLM (ReMDM). We find that ReMDM improves the best FID over ancestral sampling, from 24.73 to 23.71 using 4096 sampling steps. Nucleus sampling can help at very low step counts, but the best performance is obtained with ancestral sampling. As the number of steps increases, nucleus sampling worsens the FID. Duo (Ancestral). Cosine training without time conditioning yields the lowest perplexity, while log-linear training without time conditioning gives the best FID. We use the latter in downstream experiments. Nucleus sampling improves FID, and greedy decoding slightly worsens it. 31 Published as conference paper at ICLR 2026 Table 4: Training efficiency comparison between Duo and Duo++ on 138M parameter models. All measurements are conducted on training job on 8 NVIDIA GH200-120GB GPU with batch size 32. We report the average throughput in sequence per second. The row Duo (after CL) denotes the resources consumption of Duo after the Curriculum phase. The impact of is minimal when {2, 3, 5}, and Duo++ uses similar resources."
        },
        {
            "title": "Method",
            "content": "Throughput (samples/s) Peak Memory (GiB) Duo Duo (after the CL) Duo++ (k {2, 3, 5}) 81.8 122.4 121.9 94.3 63.3 63.4 Duo (Ψ-samplers). Ψ-samplers further improve performance beyond ReMDM. With the log-linear sampling schedule (as used by ReMDM), Ψ-samplers reduce the FID from 23.71 to 20.71. Using cosine sampling schedule further improves the FID. Overall, Duo improves from an FID of 25.63 (ancestral) to 15.05 with Ψ-samplers, and MDLM improves from 24.73 (ancestral) to 17.86 with Ψ-samplers. D.1.2 OPENWEBTEXT We report the generative perplexity using GPT-2 Large, following standard practice (Sahoo et al., 2024; 2025a). Because language models can artificially lower the generative perplexity by producing repetitive text, we also report unigram entropy (Dieleman et al., 2022), as proxy. Some Ψ-samplers schedules reduce the unigram entropy more than others. Therefore, for figures, we select the κ schedule whose unigram entropy matches (or is closest to) the entropy of samples generated with ancestral sampling. If multiple schedules achieve the same entropy, we choose the one with the lowest generative perplexity. We indicate which schedule is used for plots by highlighting the corresponding row in blue in the tables. Overall, the Ψ-samplers can reduce the Gen. PPL of all models while retaining the unigram entropy. Best results are achieved using the rescale schedule with η {0.01, 0.02}, for both MDLM and Duo. Table 12 shows the generative perplexity of MDLM and Duo after pre-training and after distillation with SDTT (Deschenaux & Gulcehre, 2025) or DCD (Sahoo et al., 2025a) respectively, with and without nucleus sampling, using ancestral sampling. Table 13 shows the results when sampling with Ψ-samplers that are equivalent to ReMDM (Wang et al., 2025), with the non-distilled models, while Table 14 shows the result for the distilled models. D.2 DISTRIBUTION OF THE TOP ENTRIES OF THE SOFTMAX To verify that our sparse implementation accurately approximates the curriculum weights of Sahoo et al. (2025a), we compare the empirical distributions of the top-k largest entries between the original and our efficient implementation. While matching marginal distributions does not guarantee matching joint distributions, matching marginals are necessary for matching joints, and are easier to visualize. Recall that experimentally, our efficient implementation is sufficient to achieve strong performance (Sec. 5.2). Specifically, we show histograms using tokenizer with 100k tokens in Figures 7, 8, 9, 10, and with the GPT-2 tokenizer in Figures 11, 12, 13, 14, with varying temperature and log signal-to-noise ratios. In all cases, the top variables have matching distributions. D.3 TRAINING EFFICIENCY OF OUR FAST CURRICULUM As shown in Table 4, our sparse curriculum achieves 33% reduction in peak memory usage and reaches an average throughput 25% higher than Duo, at context length of 1024. 32 Published as conference paper at ICLR 2026 Table 5: Zero-shot perplexity (PPL) on seven datasets. Lower is better. Results taken from Sahoo et al. (2025a). Duo++ (k = 2) achieves slightly lower zero-shot perplexity than Duo on 6 of 7 datasets."
        },
        {
            "title": "PTB Wiki",
            "content": "LM1B LBD AG News"
        },
        {
            "title": "PubMed ArXiv",
            "content": "82.05 25.75 51.25 51.28 52.09 49. 41.73 Autoregressive Transformer Diffusion (138M) SEDD Uniform UDLM Duo Duo++ (k = 2) Duo++ (k = 3) Duo++ (k = 5) 105.51 112.82 89.35 94.96 91.94 94.46 41.10 39.42 33.57 34.05 34.65 34. 82.62 77.59 73.86 73.80 74.16 74.91 57.29 53.57 49.78 48.67 49.89 50.93 82.64 80.96 67.81 67.14 66.89 68.72 55.89 50.98 44.48 43.98 44.87 46.79 50.86 44.08 40.39 38.93 40.42 41.04 Published as conference paper at ICLR 2026 Figure 7: Marginal distributions of the top-5 entries using tokenizer with 100k tokens, inverse temperature 100, and log signal-to-noise ratio 2. The histograms of the efficient and naive implementation match closely. Figure 8: Marginal distributions of the top-5 entries using tokenizer with 100k tokens, inverse temperature 1000, and log signal-to-noise ratio 1. The histograms of the efficient and naive implementation match closely. Figure 9: Marginal distributions of the top-5 entries using tokenizer with 100k tokens, inverse temperature 1000, and log signal-to-noise ratio 2. The histograms of the efficient and naive implementation match closely. Figure 10: Marginal distributions of the top-5 entries using tokenizer with 100k tokens, inverse temperature 1000, and log signal-to-noise ratio 4. The histograms of the efficient and naive implementation match closely. Figure 11: Marginal distributions of the top-5 entries using the GPT-2 tokenizer, inverse temperature 100, and log signal-to-noise ratio 2. The histograms of the efficient and naive implementation match closely. 34 Published as conference paper at ICLR 2026 Figure 12: Marginal distributions of the top-5 entries using the GPT-2 tokenizer, inverse temperature 1000, and log signal-to-noise ratio 1. The histograms of the efficient and naive implementation match closely. Figure 13: Marginal distributions of the top-5 entries using the GPT-2 tokenizer, inverse temperature 1000, and log signal-to-noise ratio 2. The histograms of the efficient and naive implementation match closely. Figure 14: Marginal distributions of the top-5 entries using the GPT-2 tokenizer, inverse temperature 1000, and log signal-to-noise ratio 4. The histograms of the efficient and naive implementation match closely. Published as conference paper at ICLR 2026 Table 6: FID on CIFAR-10 with ancestral sampling. We train and sample with the log-linear and cosine scheduler. MDLM performs best with time-conditioning while Duo does not. We sample with discrete classifier-free guidance (Schiff et al., 2025) with strength 1, and greedy predictions on the last step."
        },
        {
            "title": "Time",
            "content": "PPL FID (Cosine) FID (Log-linear) 64 256 2048 64 256"
        },
        {
            "title": "MDLM",
            "content": "Cosine Cosine Log-linear Log-linear 8.86 8.72 8.76 8.75 42.60 41.89 43.95 49.36 27.71 27.03 29.01 32.10 24.90 24.67 26.11 28. 24.56 24.24 25.67 28.21 107.62 114.56 111.77 122.70 40.81 40.60 42.15 41.79 27.65 27.08 28.85 27.89 25.73 25.50 26.89 26.02 MDLM (nucleus p=0.9)"
        },
        {
            "title": "Cosine",
            "content": "MDLM (no greedy)"
        },
        {
            "title": "Duo",
            "content": "Cosine Cosine Log-linear Log-linear Duo (nucleus p=0.9) Log-linear Duo (no greedy) Log-linear 8.72 34.81 44.04 47.84 48.37 41. 33.33 43.12 45.98 8.72 42.14 27. 24.47 24.46 114.55 40.92 27.13 25. 10.27 10.32 10.49 10.45 32.37 33.74 31.78 34.05 27.28 27.98 27.03 27.74 26.38 26.81 26.00 26.58 26.02 26.96 25.75 26.37 33.93 36.23 33.44 36. 27.93 28.77 27.46 28.49 26.51 27.08 26.08 26.60 26.03 26.79 25.87 26.22 10.49 23.13 22. 22.58 22.49 24.24 22.41 22.35 22. 10.49 33.03 27.43 26.16 25.96 34. 27.76 26.30 26.06 Table 7: FID on CIFAR-10 with ancestral sampling and finer grid. We pick the variant with the best FID from Table 6."
        },
        {
            "title": "Sample",
            "content": "p"
        },
        {
            "title": "Duo\nDuo",
            "content": "log-lin log-lin log-lin log-lin"
        },
        {
            "title": "MDLM cos\nMDLM cos",
            "content": "log-lin log-lin cos cos log-lin log-lin cos cos 1.0 0.9 1.0 0. 1.0 0.9 1.0 0.9 32 42.71 28.53 39.65 25.96 33.44 24.24 31.78 23.13 212.95 84.85 114.56 41.73 73.82 58.31 41.89 34. 128 29.18 22.89 28.55 22.68 62.86 31.28 36.21 37.91 FID 256 27.46 22.41 27.03 22.21 40.60 33.33 27.03 44. 512 26.62 22.56 26.03 22.26 31.05 38.49 25.63 45.32 26.08 22.35 25.89 22.58 27.08 43.12 24.67 47.84 2048 25.87 22. 25.75 22.49 25.50 45.98 24.24 48.37 4096 25.79 22.41 25.63 22. 24.73 55.37 23.93 49.23 Published as conference paper at ICLR 2026 Table 8: FID on CIFAR-10 with ReMDM (best checkpoints, as shown in Table 7). We sample with/without nucleus sampling, and with the 3 schedules of Wang et al. (2025) (cap, loop, rescale). For the loop schedule, we use ton = 0.55, toff = 0.05, αon = 0.9, following ReMDM. Sampling experiments are executed in the original codebase of Wang et al. (2025). 32 128 256 512 1024"
        },
        {
            "title": "Number of steps",
            "content": "215.67 218.41 224.20 242.25 307.56 307.81 308.24 308.88 216.92 221.21 229.72 234.35 88.08 87.68 85.95 81.91 116.24 118.25 122.61 143.21 234.55 237.28 242.70 248. 116.73 119.79 127.94 133.08 40.02 39.55 38.46 35.56 209.24 208.36 206.51 204.83 100.01 99.29 98.18 97.24 87.31 85.94 83.47 82.26 39.51 38.45 36.44 35. 63.37 64.50 66.95 84.41 40.82 41.77 44.54 64.10 31.40 32.40 36.26 73.89 138.56 142.21 152.28 165.79 80.50 83.68 94.63 114.92 55.86 59.96 76.93 113. 63.56 65.08 70.89 75.02 27.31 27.35 27.80 29.39 47.27 47.12 46.87 46.72 27.25 27.45 28.29 28.99 40.65 42.02 46.98 50.92 29.43 31.24 35.01 46. 29.44 29.38 29.28 29.19 30.74 34.13 41.76 44.69 30.86 32.29 38.74 45.01 36.50 41.22 50.50 70.24 27.55 27.74 28.09 28.30 40.22 49.00 63.40 68. 27.28 28.68 35.39 132.13 47.05 53.88 88.53 157.92 26.03 28.11 41.23 57.03 45.10 54.55 69.60 95.24 30.50 31.17 32.12 32.77 53.30 67.89 87.03 94. 24.97 27.91 46.01 210.60 45.44 60.50 135.05 223.70 23.77 28.66 67.44 107.13 57.08 71.65 91.49 125.60 34.21 35.42 37.19 38.47 24.78 33.68 92.48 203. 50.44 87.54 196.58 237.16 23.71 39.39 130.05 164.44 73.40 93.06 118.87 163.32 37.56 39.52 42.45 44.64 70.24 90.61 115.60 125.42 91.79 118.10 153.03 165. 215.67 81.91 116.24 81.91 63.37 27.25 40.65 29.19 30.86 27.55 26.03 30. 23.77 34.21 23.71 37.56 ReMDM cap (p=1.0) η = 0.005 η = 0.010 η = 0.020 η = 0.050 ReMDM loop (p=1.0) η = 0.01 η = 0.02 η = 0.04 η = 0. ReMDM rescale (p=1.0) η = 0.01 η = 0.02 η = 0.04 η = 0.05 ReMDM cap (p=0.9) η = 0.005 η = 0.010 η = 0.020 η = 0.050 ReMDM loop (p=0.9) η = 0.01 η = 0.02 η = 0.04 η = 0. ReMDM rescale (p=0.9) η = 0.01 η = 0.02 η = 0.04 η = 0."
        },
        {
            "title": "ReMDM",
            "content": "Best (p = 1.0) Best (p = 0.9)"
        },
        {
            "title": "MDLM",
            "content": "Ancestral (p=1.0) Ancestral (p=0.9) 212.95 84.85 114.56 41.73 62.86 31.28 40.60 33.33 31.05 38. 27.08 43.12 25.50 45.98 24.73 55.37 37 Published as conference paper at ICLR 2026 Table 9: FID on CIFAR-10 with Ψ-samplers, where Ψ-samplers are activated for steps with [toff, ton], when κt is kept constant (according to the κ column, 1 otherwise). We use the same checkpoints as in Table 7. Using cosine sampling schedule and light noise injection (κ close to 1) generally perform best. The CIFAR-10 curves in Fig. 1 show the best FID per number of steps. Algo κ Train Sample ton toff 32 64 Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo 0.02 0.02 0.02 0.5 0.5 0.5 0.95 0.95 0.95 0.95 0.98 0.98 0.99 0.99 0.02 0.02 0. 0.5 0.5 0.5 0.95 0.95 0.95 0.95 0.98 0.98 0.99 0.99 MDLM 0.02 MDLM 0.02 MDLM 0.02 MDLM MDLM MDLM 0.5 0.5 0.5 MDLM 0.95 MDLM 0.95 MDLM 0.95 MDLM 0.95 MDLM 0.98 MDLM 0.98 MDLM 0.99 MDLM 0.99 MDLM 0.02 MDLM 0.02 MDLM 0."
        },
        {
            "title": "MDLM\nMDLM\nMDLM",
            "content": "0.5 0.5 0.5 MDLM 0.95 MDLM 0.95 MDLM 0.95 MDLM 0.95 MDLM 0.98 MDLM 0.98 MDLM 0.99 MDLM 0.99 log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin cos cos cos cos cos cos cos cos cos cos cos cos cos cos log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin 0.2 0.5 0.8 0.2 0.6 0.9 0.5 0.6 0.9 0.9 1.0 1. 1.0 1.0 0.2 0.5 0.8 0.2 0.6 0.9 0.5 0.6 0.9 0.9 1.0 1.0 1.0 1. 0.2 0.5 0.8 0.2 0.6 0.9 0.5 0.6 0.9 0.9 1.0 1.0 1.0 1.0 0.2 0.5 0. 0.2 0.6 0.9 0.5 0.6 0.9 0.9 1.0 1.0 1.0 1.0 0.15 0.45 0.7 0.1 0.4 0. 0.1 0.1 0.3 0.4 0.05 0.1 0.05 0.1 0.15 0.45 0.7 0.1 0.4 0.65 0.1 0.1 0.3 0. 0.05 0.1 0.05 0.1 0.15 0.45 0.7 0.1 0.4 0.65 0.1 0.1 0.3 0.4 0.05 0. 0.05 0.1 0.15 0.45 0.7 0.1 0.4 0.65 0.1 0.1 0.3 0.4 0.05 0.1 0.05 0. 33.06 33.67 37.41 32.14 29.40 34.68 30.58 30.15 29.88 30.29 30.97 30.99 31.56 31.57 33.71 36.29 33. 33.40 33.68 31.04 32.97 32.47 30.55 30.58 31.96 31.98 32.63 32.63 49.18 89.53 197.61 46.10 114.88 131. 44.15 45.00 47.02 46.36 43.85 43.85 42.87 42.87 118.08 157.61 294.71 116.99 195.76 268.98 117.05 118.21 124.03 123. 118.08 118.09 116.02 116.03 40.64 41.81 43.99 39.95 39.54 43.00 39.30 39.19 39.04 39.21 39.31 39. 39.34 39.35 42.25 43.86 43.95 42.10 42.44 42.87 41.74 41.46 41.10 41.18 41.80 41.81 41.99 41. 75.63 117.57 172.24 73.13 134.11 151.90 73.03 74.57 79.25 78.18 74.05 74.05 72.39 72.38 217.56 247.31 298. 216.08 266.16 296.08 216.90 218.58 225.19 223.84 218.15 218.14 215.41 215.40 38 FID 256 29.85 26.55 38.88 27.18 20.77 34.73 23.46 21.54 20.90 21.57 23.13 23.14 24.73 24. 27.95 33.24 27.78 27.14 25.93 31.86 26.05 24.97 20.50 20.59 23.17 23.17 24.67 24.67 54.67 200.49 262. 39.71 217.74 177.75 30.50 30.32 27.84 26.69 26.69 26.65 26.65 26.64 51.76 162.92 312.49 45.72 212.68 278. 43.50 44.32 44.06 43.29 40.97 40.96 40.42 40.41 512 31.31 24.83 46.76 26.57 23.72 45. 20.93 18.64 19.20 19.92 20.56 20.58 22.35 22.37 27.64 34.74 37.12 26.22 24.16 61.36 24.62 22.94 17.97 18. 20.10 20.12 22.13 22.13 83.47 283.55 269.22 48.49 268.03 198.33 29.93 29.16 24.24 22.67 23.22 23. 23.72 23.69 55.02 256.01 317.03 41.32 273.48 281.68 36.06 37.14 35.20 33.85 30.67 30.65 30.43 30. 128 30.36 29.50 35.68 28.86 23.46 31.85 26.15 25.14 24.72 25.26 26.39 26.40 27.46 27. 29.84 33.35 28.32 29.19 29.15 26.37 28.57 27.74 24.54 24.71 26.83 26.85 27.74 27.75 45.02 111.75 232. 38.47 144.25 147.67 33.68 34.07 33.97 33.06 32.32 32.31 31.79 31.78 68.02 124.97 298.95 65.73 171.73 265. 64.76 65.33 67.82 67.19 63.97 63.96 63.30 63.27 1024 34.36 25.12 59.68 26.46 38.42 64. 18.48 16.70 21.09 21.50 18.80 18.83 20.07 20.09 27.56 36.97 69.66 25.52 22.44 121.64 23.13 20.83 18.04 18. 18.12 18.15 19.72 19.75 181.18 310.51 267.86 75.27 274.83 201.97 31.50 31.03 23.43 20.91 20.81 20. 21.07 21.04 78.21 298.74 312.60 45.95 281.96 275.20 34.84 36.09 33.97 32.00 25.69 25.64 25.37 25. 2048 39.06 31.63 75.46 27.29 72.97 88.07 16.38 16.30 30.00 30.03 19.46 19.48 18.50 18. 29.35 36.77 113.05 25.10 21.00 155.77 21.81 18.87 22.14 22.07 18.38 18.40 17.93 17.95 280.42 314.98 264. 173.09 270.53 193.77 35.72 37.46 26.96 21.90 19.41 19.26 19.24 19.19 171.72 305.05 308.42 68.60 272.45 265. 37.06 39.42 42.48 37.23 23.64 23.57 22.45 22.42 4096 38.38 51.83 91.73 28.35 105.75 107. 15.05 18.99 51.43 50.88 25.83 25.82 19.39 19.41 31.02 37.30 132.86 24.71 27.97 151.48 20.16 16.82 35.43 35. 22.89 22.94 18.25 18.28 297.52 313.93 259.88 266.36 256.03 184.76 51.53 64.74 42.58 28.82 20.20 19. 17.94 17.86 275.25 310.28 302.37 152.77 260.26 247.21 44.92 55.34 80.34 63.89 25.40 25.29 20.77 20. Published as conference paper at ICLR 2026 Table 10: Inception Score on CIFAR-10 with Ψ-samplers, where Ψ-samplers are activated for steps with [toff, ton], when κt is kept constant (according to the κ column, 1 otherwise). We use the same checkpoints as in Table 7. The CIFAR-10 curves in Fig. 6 show the best Inception Score per number of steps. Algo κ Train Sample ton toff Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo Duo 0.02 0.02 0.02 0.5 0.5 0.5 0.95 0.95 0.95 0.95 0.98 0.98 0.99 0.99 0.02 0.02 0. 0.5 0.5 0.5 0.95 0.95 0.95 0.95 0.98 0.98 0.99 0.99 MDLM 0.02 MDLM 0.02 MDLM 0.02 MDLM MDLM MDLM 0.5 0.5 0.5 MDLM 0.95 MDLM 0.95 MDLM 0.95 MDLM 0.95 MDLM 0.98 MDLM 0.98 MDLM 0.99 MDLM 0.99 MDLM 0.02 MDLM 0.02 MDLM 0."
        },
        {
            "title": "MDLM\nMDLM\nMDLM",
            "content": "0.5 0.5 0.5 MDLM 0.95 MDLM 0.95 MDLM 0.95 MDLM 0.95 MDLM 0.98 MDLM 0.98 MDLM 0.99 MDLM 0.99 log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos cos log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin cos cos cos cos cos cos cos cos cos cos cos cos cos cos log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin log-lin 0.2 0.5 0.8 0.2 0.6 0.9 0.5 0.6 0.9 0.9 1.0 1. 1.0 1.0 0.2 0.5 0.8 0.2 0.6 0.9 0.5 0.6 0.9 0.9 1.0 1.0 1.0 1. 0.2 0.5 0.8 0.2 0.6 0.9 0.5 0.6 0.9 0.9 1.0 1.0 1.0 1.0 0.2 0.5 0. 0.2 0.6 0.9 0.5 0.6 0.9 0.9 1.0 1.0 1.0 1.0 0.15 0.45 0.7 0.1 0.4 0. 0.1 0.1 0.3 0.4 0.05 0.1 0.05 0.1 0.15 0.45 0.7 0.1 0.4 0.65 0.1 0.1 0.3 0. 0.05 0.1 0.05 0.1 0.15 0.45 0.7 0.1 0.4 0.65 0.1 0.1 0.3 0.4 0.05 0. 0.05 0.1 0.15 0.45 0.7 0.1 0.4 0.65 0.1 0.1 0.3 0.4 0.05 0.1 0.05 0. 64 7.25 7.44 6.99 7.21 7.73 7.10 7.26 7.31 7.37 7.31 7.25 7.25 7.22 7. 7.09 7.28 7.54 7.04 7.45 7.61 7.10 7.12 7.27 7.25 7.19 7.20 7.13 7.14 6.61 5.11 2. 6.63 4.32 4.18 6.68 6.70 6.68 6.66 6.71 6.72 6.73 6.73 4.59 3.56 1.63 4.60 2.78 1. 4.60 4.55 4.45 4.46 4.56 4.56 4.64 4.64 32 7.02 7.09 6.84 6.96 7.31 6. 6.98 7.00 7.08 7.04 7.00 6.99 6.98 6.98 6.82 6.95 7.00 6.81 7.04 7.05 6.80 6.85 6.89 6. 6.85 6.85 6.81 6.81 5.56 4.22 3.12 5.63 3.83 3.62 5.66 5.59 5.43 5.45 5.57 5. 5.60 5.60 2.63 2.21 1.65 2.66 1.97 1.69 2.65 2.62 2.51 2.54 2.62 2.62 2.68 2. 39 Inception Score 128 256 512 2048 4096 7.35 7.64 7.00 7.28 8.14 7.22 7.48 8.04 6.91 7.39 8.51 7. 7.52 8.32 6.64 7.45 8.46 6.72 7.45 7.45 7.54 7.50 7.40 7.40 7.37 7.37 7.22 7.45 8. 7.20 7.73 7.97 7.25 7.28 7.58 7.58 7.36 7.38 7.32 7.32 6.90 4.36 2.41 7.00 3.55 3. 7.53 7.70 7.84 7.78 7.55 7.55 7.45 7.46 7.30 7.64 8.18 7.26 7.93 7.74 7.31 7.40 7.78 7. 7.49 7.49 7.45 7.45 6.75 2.44 2.03 7.09 2.35 3.47 7.67 7.91 8.01 7.92 7.73 7. 7.58 7.58 7.36 7.67 7.89 7.29 8.20 6.45 7.35 7.46 8.10 8.05 7.72 7.72 7.61 7. 5.52 1.61 1.96 6.90 1.85 3.18 7.47 8.59 6.16 7.48 7.91 5.97 7.89 8.17 8.07 8.08 7.97 7. 7.77 7.77 7.44 7.70 6.46 7.36 8.51 4.46 7.43 7.66 8.22 8.25 7.96 7.96 7.71 7. 2.68 1.41 1.97 5.68 2.14 3.15 7.13 7.21 7.25 7.25 7.29 7.41 7.63 7.64 7.44 7.52 7.90 7.93 7.41 7.57 8.15 8. 7.22 7.22 7.18 7.19 5.86 4.08 1.55 5.91 2.99 1.90 5.95 5.94 5.93 5.94 6.04 6. 6.02 6.02 7.45 7.46 7.39 7.39 6.46 3.06 1.43 6.58 2.27 1.79 6.64 6.66 6.84 6. 6.85 6.85 6.84 6.84 7.71 7.72 7.53 7.53 6.45 1.78 1.42 6.81 1.62 1. 6.93 6.98 7.35 7.40 7.31 7.32 7.21 7.21 7.93 7.93 7.81 7.81 5.59 1.43 1. 6.76 1.58 2.35 7.03 7.16 7.61 7.69 7.66 7.67 7.47 7.47 7.38 8.57 5.67 7.56 6.40 5. 8.06 8.34 7.72 7.78 8.10 8.09 7.96 7.96 7.46 8.06 5.03 7.63 7.94 5.19 7.73 5.39 4. 8.29 8.46 6.84 6.89 7.91 7.91 8.08 8.10 7.43 8.68 4.55 7.47 9.00 3.77 7.50 9.50 4. 7.55 7.81 8.20 8.26 8.05 8.04 7.98 7.99 1.57 1.45 2.02 2.78 2.51 3.37 7.44 7.45 8.18 8. 8.14 8.15 7.97 7.97 2.78 1.38 1.80 5.77 1.92 2.87 7.07 7.10 7.31 7.59 7.79 7. 7.70 7.70 7.63 7.97 7.67 7.69 8.03 8.02 8.12 8.13 1.56 1.56 2.09 1.73 2.99 3. 6.77 6.33 7.58 8.18 8.30 8.31 8.12 8.14 1.67 1.36 1.96 3.15 2.33 3.29 6.78 6.47 5.83 6. 8.01 8.03 7.93 7.93 Published as conference paper at ICLR 2026 Table 11: FID on CIFAR-10 using Ψ-samplers whose κt schedulers are equivalent to ReMDM. We use no nucleus sampling, no temperature scaling, and cfg = 1. As expected, with the log-linear scheduler, we reach similar FID as when using the ReMDM codebase  (Table 8)  . However, note that by using log-linear scheduler, using constant κt = 0.99, we reach better FID than with the original ReMDM scheduler. Algo Train Sample FID 32 64 128 512 1024 2048 4096 Duo with the ReMDM rescale schedule Duo Duo cos log-lin log-lin log-lin 39.64 42.27 32.03 33.58 28.49 29.49 26.95 27.36 26.16 26. 25.71 25.86 25.25 25.07 25.02 25.21 ReMDM Rescale (η = 0.01) MDLM cos cos log-lin MDLM cos ReMDM Cap (η = 0.005) MDLM cos log-lin 70.64 213.22 41.94 114.24 31.60 62.51 27.31 40.51 25.27 30.28 24.61 26. 23.41 23.61 23.25 23.40 215.75 115.77 63.20 41. 31.60 27.30 25.16 24.79 ReMDM Loop (ton = 0.55, toff = 0.05, αon = 0.9, η = 0.01) 120.58 MDLM cos 305. 224.84 log-lin 66.39 45.70 39.06 41. 52.71 Table 12: Generative Perplexity (Gen. PPL) and Unigram Entropy on OpenWebText (Gokaslan & Cohen, 2019) with ancestral sampling (no nucleus, no temperature scaling). We train using the log-linear noise scheduler, and sampling with the cosine scheduler is slightly better. We stick to the log-linear schedule for sampling in further experiments, to follow prior work, and since the cosine schedule only marginally reduces the Gen. PPL. Algo Dist. Sched. Gen. PPL () Duo Duo Duo Duo Duo Duo Duo Duo MDLM MDLM MDLM MDLM MDLM MDLM MDLM MDLM 1.0 1. 0.9 0.9 1.0 1.0 0.9 0.9 1.0 1.0 0.9 0.9 1.0 1. 0.9 0.9 cos log-lin cos log-lin cos log-lin cos log-lin cos log-lin cos log-lin cos log-lin cos log-lin 32 64 256 512 1024 2048 4096 87.23 (5.54) 96.76 (5.57) 42.42 (5.36) 44.24 (5.40) 67.04 (5.47) 68.35 (5.54) 34.20 (5.31) 35.92 (5.41) 79.94 (5.55) 86.01 (5.56) 39.26 (5.37) 40.08 (5.40) 61.09 (5.45) 62.92 (5.54) 31.79 (5.29) 32.98 (5.40) 75.87 (5.53) 79.97 (5.55) 37.62 (5.35) 37.93 (5.39) 59.65 (5.42) 59.82 (5.50) 31.09 (5.25) 31.49 (5.36) 73.95 (5.54) 78.46 (5.53) 36.52 (5.35) 36.66 (5.37) 57.76 (5.42) 58.77 (5.46) 30.05 (5.25) 30.32 (5.31) 72.13 (5.54) 76.93 (5.54) 35.21 (5.34) 35.77 (5.37) 57.90 (5.42) 58.32 (5.46) 29.82 (5.26) 30.06 (5.29) 71.41 (5.53) 75.02 (5.53) 35.37 (5.34) 34.79 (5.35) 56.81 (5.43) 57.82 (5.45) 29.68 (5.27) 30.00 (5.28) 72.29 (5.53) 75.65 (5.52) 35.39 (5.34) 34.93 (5.35) 56.39 (5.41) 55.39 (5.43) 29.52 (5.24) 28.90 (5.25) 70.77 (5.52) 75.39 (5.52) 34.91 (5.33) 34.75 (5.35) 57.32 (5.42) 55.89 (5.42) 29.73 (5.23) 29.19 (5.25) 168.66 (5.68) 194.09 (5.74) 131.55 (5.66) 141.67 (5.69) 115.74 (5.64) 120.95 (5.67) 111.72 (5.63) 111.85 (5.65) 106.63 (5.63) 107.89 (5.64) 104.56 (5.62) 105.64 (5.64) 103.12 (5.62) 105.40 (5.63) 104.73 (5.62) 105.03 (5.62) 58.33 (5.39) 70.34 (5.49) 63.04 (5.45) 68.61 (5.48) 31.47 (5.21) 34.85 (5.26) 46.71 (5.36) 51.14 (5.43) 52.72 (5.43) 55.26 (5.45) 26.52 (5.19) 28.21 (5.23) 40.66 (5.32) 43.60 (5.39) 47.83 (5.41) 49.51 (5.44) 24.14 (5.18) 25.27 (5.21) 39.43 (5.33) 40.01 (5.37) 45.94 (5.42) 46.13 (5.42) 23.49 (5.17) 24.01 (5.19) 37.64 (5.32) 39.02 (5.35) 44.67 (5.41) 45.61 (5.42) 22.93 (5.17) 23.25 (5.18) 37.39 (5.33) 37.91 (5.34) 44.60 (5.41) 44.87 (5.42) 22.64 (5.17) 22.75 (5.17) 36.98 (5.31) 37.59 (5.32) 44.50 (5.41) 44.53 (5.41) 22.38 (5.16) 22.73 (5.17) 36.87 (5.31) 36.76 (5.31) 44.42 (5.41) 44.38 (5.42) 22.49 (5.17) 22.46 (5.16) 40 Published as conference paper at ICLR 2026 Table 13: Generative Perplexity (Gen. PPL) and Unigram Entropy on OpenWebText (Gokaslan & Cohen, 2019) with Ψ-samplers using κt schedules matching ReMDM (log-linear step size) and non-distilled models (as in Table 12). We experiment with nucleus sampling, following Wang et al. (2025). The rescale schedule is most effective to improve the Gen. PPL while retaining the unigram entropy. The lightblue rows are the ones plotted in Fig. 1 (left). Algo Eta Nucleus Gen. PPL () 32 128 256 512 1024 2048 Ancestral Sampling Duo Duo Duo N.A N.A N.A MDLM MDLM MDLM N.A N.A N.A Cap Schedule Duo Duo Duo Duo Duo Duo 0.005 0.01 0.005 0.01 0.005 0. MDLM MDLM MDLM MDLM MDLM MDLM 0.005 0.01 0.005 0.01 0.005 0.01 Rescale Schedule 0.01 Duo 0.02 Duo 0.01 Duo 0.02 Duo 0.01 Duo 0.02 Duo 0.03 Duo 0.04 Duo 0.05 Duo MDLM MDLM MDLM MDLM MDLM MDLM MDLM MDLM MDLM 0.01 0.02 0.01 0.02 0.01 0.02 0.03 0.04 0.05 Loop Schedule Duo Duo Duo Duo Duo Duo 0.01 0.02 0.01 0.02 0.01 0.02 MDLM MDLM MDLM MDLM MDLM MDLM 0.01 0.02 0.01 0.02 0.01 0.02 1.0 0.95 0.9 1.0 0.95 0.9 1.0 1.0 0.95 0.95 0.9 0. 1.0 1.0 0.95 0.95 0.9 0.9 1.0 1.0 0.95 0.95 0.9 0.9 0.9 0.9 0.9 1.0 1.0 0.95 0.95 0.9 0.9 0.9 0.9 0.9 1.0 1.0 0.95 0.95 0.9 0.9 1.0 1.0 0.95 0.95 0.9 0.9 96.76 (5.57) 56.65 (5.49) 44.24 (5.40) 86.01 (5.56) 50.78 (5.48) 40.08 (5.40) 79.97 (5.55) 48.68 (5.48) 37.93 (5.39) 78.46 (5.53) 47.26 (5.46) 36.66 (5.37) 76.93 (5.54) 45.42 (5.45) 35.77 (5.37) 75.02 (5.53) 45.11 (5.44) 34.79 (5.35) 75.65 (5.52) 45.12 (5.44) 34.93 (5.35) 75.39 (5.52) 44.84 (5.44) 34.75 (5.35) 194.09 (5.74) 106.28 (5.61) 70.34 (5.49) 141.67 (5.69) 77.06 (5.55) 51.14 (5.43) 120.95 (5.67) 68.34 (5.53) 43.60 (5.39) 111.85 (5.65) 63.19 (5.51) 40.01 (5.37) 107.89 (5.64) 58.80 (5.49) 39.02 (5.35) 105.64 (5.64) 56.94 (5.48) 37.91 (5.34) 105.40 (5.63) 57.54 (5.47) 37.59 (5.32) 105.03 (5.62) 56.44 (5.46) 36.76 (5.31) 88.78 (5.58) 86.89 (5.58) 55.56 (5.49) 54.07 (5.48) 44.06 (5.41) 43.05 (5.40) 195.83 (5.74) 198.02 (5.75) 106.40 (5.61) 105.45 (5.61) 69.20 (5.49) 68.57 (5.48) 89.63 (5.58) 89.55 (5.58) 56.68 (5.49) 56.68 (5.49) 45.03 (5.41) 45.04 (5.41) 44.87 (5.41) 44.43 (5.41) 44.52 (5.41) 194.29 (5.74) 194.54 (5.74) 106.43 (5.61) 105.92 (5.60) 70.45 (5.49) 70.31 (5.49) 69.89 (5.49) 69.54 (5.49) 69.44 (5.48) 108.15 (5.58) 103.48 (5.58) 65.29 (5.49) 61.61 (5.48) 52.12 (5.40) 49.08 (5.40) 340.32 (5.81) 338.82 (5.82) 182.65 (5.67) 177.31 (5.67) 117.28 (5.55) 112.21 (5.55) 77.12 (5.57) 75.23 (5.56) 48.74 (5.47) 46.27 (5.46) 38.38 (5.39) 36.75 (5.38) 142.25 (5.70) 144.89 (5.70) 74.97 (5.54) 73.92 (5.54) 49.59 (5.42) 48.30 (5.42) 79.80 (5.57) 79.44 (5.57) 50.80 (5.48) 50.66 (5.48) 40.02 (5.40) 40.00 (5.40) 40.05 (5.40) 39.67 (5.39) 39.49 (5.40) 141.40 (5.69) 140.81 (5.69) 76.89 (5.55) 76.23 (5.55) 51.33 (5.43) 51.06 (5.43) 50.76 (5.42) 50.30 (5.42) 50.15 (5.42) 83.10 (5.58) 79.75 (5.58) 51.36 (5.48) 47.46 (5.47) 40.27 (5.39) 37.00 (5.38) 192.48 (5.74) 193.71 (5.75) 101.56 (5.61) 97.61 (5.61) 65.24 (5.48) 61.93 (5.48) 72.05 (5.56) 68.98 (5.55) 44.93 (5.46) 41.93 (5.45) 34.84 (5.37) 32.27 (5.35) 121.99 (5.68) 125.25 (5.68) 63.15 (5.52) 61.41 (5.51) 41.08 (5.38) 38.80 (5.37) 76.11 (5.56) 75.98 (5.56) 48.38 (5.47) 48.09 (5.47) 38.17 (5.39) 38.05 (5.39) 37.61 (5.39) 37.21 (5.38) 36.41 (5.38) 121.04 (5.67) 120.86 (5.67) 65.42 (5.52) 65.43 (5.52) 43.59 (5.39) 43.51 (5.39) 43.23 (5.39) 42.84 (5.39) 42.39 (5.38) 71.16 (5.56) 67.99 (5.56) 43.27 (5.46) 38.78 (5.44) 33.71 (5.37) 30.08 (5.34) 140.70 (5.70) 144.92 (5.72) 71.76 (5.56) 68.49 (5.55) 46.91 (5.43) 43.89 (5.42) 66.44 (5.54) 63.66 (5.54) 40.53 (5.43) 36.60 (5.41) 30.95 (5.33) 27.83 (5.30) 113.94 (5.67) 117.84 (5.68) 55.82 (5.49) 52.81 (5.48) 35.19 (5.34) 32.38 (5.32) 73.43 (5.55) 72.99 (5.54) 46.91 (5.46) 46.19 (5.46) 36.60 (5.36) 36.15 (5.36) 35.26 (5.36) 34.75 (5.35) 33.68 (5.35) 112.95 (5.65) 112.64 (5.65) 61.07 (5.50) 60.80 (5.50) 40.14 (5.36) 39.61 (5.36) 38.86 (5.36) 38.02 (5.35) 37.27 (5.35) 66.15 (5.55) 63.05 (5.55) 37.64 (5.43) 32.69 (5.40) 28.73 (5.33) 24.88 (5.29) 127.32 (5.70) 140.73 (5.72) 58.43 (5.52) 55.21 (5.51) 37.62 (5.38) 34.69 (5.37) 61.63 (5.53) 57.34 (5.52) 36.26 (5.41) 30.98 (5.37) 27.37 (5.30) 23.38 (5.26) 110.75 (5.66) 116.62 (5.68) 50.31 (5.47) 46.03 (5.45) 31.49 (5.31) 27.66 (5.28) 70.66 (5.54) 69.85 (5.54) 45.24 (5.45) 44.17 (5.44) 35.25 (5.35) 34.74 (5.35) 33.35 (5.34) 32.47 (5.34) 31.06 (5.34) 107.80 (5.64) 108.26 (5.64) 58.77 (5.49) 57.32 (5.49) 38.68 (5.35) 37.88 (5.35) 36.77 (5.34) 35.73 (5.33) 34.10 (5.33) 60.49 (5.55) 56.92 (5.54) 32.04 (5.40) 27.26 (5.36) 24.47 (5.29) 20.59 (5.24) 119.34 (5.69) 136.30 (5.71) 51.33 (5.50) 47.71 (5.49) 31.93 (5.34) 28.99 (5.33) 57.14 (5.51) 52.06 (5.50) 30.85 (5.37) 25.53 (5.31) 22.78 (5.24) 18.74 (5.17) 112.78 (5.67) 126.32 (5.71) 43.78 (5.44) 38.85 (5.42) 26.33 (5.26) 21.57 (5.18) 70.46 (5.53) 68.39 (5.53) 44.64 (5.44) 42.71 (5.43) 34.35 (5.34) 33.13 (5.33) 31.17 (5.32) 29.30 (5.31) 26.94 (5.28) 105.58 (5.64) 105.65 (5.64) 56.34 (5.47) 54.94 (5.47) 37.64 (5.34) 36.28 (5.33) 34.62 (5.32) 32.44 (5.31) 30.29 (5.30) 56.35 (5.53) 52.69 (5.51) 26.97 (5.35) 22.35 (5.29) 20.32 (5.23) 16.69 (5.16) 127.63 (5.70) 162.47 (5.75) 45.27 (5.47) 41.64 (5.45) 27.80 (5.31) 24.58 (5.29) 52.49 (5.51) 46.04 (5.46) 25.66 (5.32) 20.10 (5.23) 18.66 (5.16) 14.40 (5.06) 119.61 (5.69) 143.96 (5.73) 37.04 (5.40) 31.30 (5.34) 21.16 (5.18) 16.26 (5.05) 69.20 (5.54) 66.60 (5.53) 44.11 (5.44) 41.47 (5.43) 34.27 (5.35) 31.79 (5.32) 28.90 (5.31) 26.15 (5.28) 23.61 (5.25) 105.69 (5.63) 104.47 (5.63) 56.29 (5.47) 53.92 (5.46) 36.48 (5.32) 34.53 (5.31) 31.44 (5.29) 28.55 (5.27) 26.03 (5.25) 53.06 (5.51) 48.63 (5.47) 22.94 (5.30) 18.43 (5.22) 17.01 (5.16) 13.61 (5.06) 149.13 (5.73) 246.89 (5.81) 39.08 (5.43) 34.91 (5.40) 23.38 (5.25) 20.09 (5.20) 45.64 (5.45) 39.48 (5.39) 20.22 (5.22) 15.19 (5.07) 14.33 (5.03) 10.88 (4.87) 131.85 (5.71) 186.72 (5.76) 30.46 (5.34) 24.31 (5.23) 15.87 (5.04) 11.67 (4.79) 68.25 (5.53) 63.70 (5.52) 43.49 (5.43) 38.06 (5.40) 33.07 (5.33) 29.08 (5.30) 24.93 (5.26) 22.05 (5.22) 19.21 (5.17) 105.64 (5.63) 105.61 (5.64) 54.42 (5.45) 50.57 (5.45) 35.10 (5.31) 31.62 (5.29) 27.19 (5.25) 23.72 (5.21) 20.85 (5.16) 48.93 (5.48) 43.28 (5.37) 18.40 (5.20) 14.31 (5.06) 13.61 (5.05) 10.77 (4.92) 198.48 (5.77) 354.65 (5.78) 33.48 (5.38) 29.63 (5.33) 19.78 (5.20) 16.68 (5.13) 41 Published as conference paper at ICLR 2026 Table 14: Generative Perplexity (Gen. PPL) and Unigram Entropy on OpenWebText (Gokaslan & Cohen, 2019) with Ψ-samplers using κt schedules matching ReMDM (log-linear step size) and distilled models (as in Table 12). We experiment with nucleus sampling, following Wang et al. (2025). Algo Eta Nucleus Gen. PPLL () 32 128 256 512 1024 2048 Ancestral Sampling Duo Duo Duo N.A N.A N.A MDLM MDLM MDLM N.A N.A N.A Cap Schedule Duo Duo Duo Duo Duo Duo 0.005 0.01 0.005 0.01 0.005 0. MDLM MDLM MDLM MDLM MDLM MDLM 0.005 0.01 0.005 0.01 0.005 0.01 Rescale Schedule 0.01 Duo 0.02 Duo 0.01 Duo 0.02 Duo 0.01 Duo 0.02 Duo MDLM MDLM MDLM MDLM MDLM MDLM 0.01 0.02 0.01 0.02 0.01 0.02 Loop Schedule Duo Duo Duo Duo Duo Duo 0.01 0.02 0.01 0.02 0.01 0.02 MDLM MDLM MDLM MDLM MDLM MDLM 0.01 0.02 0.01 0.02 0.01 0.02 1.0 0.95 0.9 1.0 0.95 0.9 1.0 1.0 0.95 0.95 0.9 0. 1.0 1.0 0.95 0.95 0.9 0.9 1.0 1.0 0.95 0.95 0.9 0.9 1.0 1.0 0.95 0.95 0.9 0.9 1.0 1.0 0.95 0.95 0.9 0.9 1.0 1.0 0.95 0.95 0.9 0.9 68.35 (5.54) 44.94 (5.47) 35.92 (5.41) 68.61 (5.48) 46.07 (5.37) 34.85 (5.26) 66.13 (5.54) 64.22 (5.53) 43.68 (5.47) 42.34 (5.46) 34.80 (5.40) 33.91 (5.40) 67.27 (5.48) 65.29 (5.47) 44.71 (5.36) 43.20 (5.36) 33.81 (5.26) 32.94 (5.25) 68.33 (5.54) 68.18 (5.54) 45.04 (5.47) 44.89 (5.47) 35.91 (5.41) 35.81 (5.41) 68.66 (5.48) 68.73 (5.48) 46.01 (5.37) 45.92 (5.37) 34.83 (5.26) 34.83 (5.26) 80.39 (5.55) 75.97 (5.55) 51.76 (5.48) 48.78 (5.48) 41.15 (5.42) 38.73 (5.42) 99.76 (5.51) 93.99 (5.51) 65.09 (5.40) 61.24 (5.40) 48.86 (5.29) 46.12 (5.29) 62.92 (5.54) 41.78 (5.46) 32.98 (5.40) 55.26 (5.45) 36.55 (5.33) 28.21 (5.23) 58.49 (5.52) 55.84 (5.51) 38.77 (5.45) 37.14 (5.44) 30.95 (5.38) 29.27 (5.37) 52.34 (5.45) 49.78 (5.44) 34.56 (5.32) 32.84 (5.32) 26.71 (5.22) 25.51 (5.22) 62.77 (5.53) 62.24 (5.53) 41.74 (5.46) 41.33 (5.46) 33.05 (5.40) 32.77 (5.40) 55.16 (5.45) 54.85 (5.45) 36.58 (5.33) 36.45 (5.33) 28.15 (5.23) 28.17 (5.23) 61.64 (5.54) 57.36 (5.53) 40.91 (5.47) 37.61 (5.46) 32.51 (5.40) 30.04 (5.40) 62.76 (5.48) 58.00 (5.48) 41.85 (5.37) 38.68 (5.36) 32.03 (5.26) 29.77 (5.27) 59.82 (5.50) 40.32 (5.43) 31.49 (5.36) 49.51 (5.44) 32.91 (5.31) 25.27 (5.21) 53.61 (5.48) 49.90 (5.48) 35.55 (5.40) 32.39 (5.38) 28.15 (5.34) 25.28 (5.31) 44.38 (5.42) 41.29 (5.40) 29.42 (5.30) 26.90 (5.29) 22.81 (5.19) 20.89 (5.18) 59.65 (5.50) 59.07 (5.50) 39.99 (5.43) 39.81 (5.43) 31.55 (5.36) 31.17 (5.36) 49.71 (5.43) 48.12 (5.43) 32.80 (5.31) 32.49 (5.31) 25.24 (5.21) 24.97 (5.21) 52.51 (5.52) 47.47 (5.52) 34.68 (5.44) 30.95 (5.43) 27.96 (5.38) 24.99 (5.37) 47.50 (5.45) 43.00 (5.45) 31.76 (5.33) 28.92 (5.33) 24.51 (5.23) 22.52 (5.22) 58.77 (5.46) 38.93 (5.39) 30.32 (5.31) 46.13 (5.42) 30.96 (5.30) 24.31 (5.19) 47.85 (5.42) 40.95 (5.39) 31.36 (5.33) 27.25 (5.30) 24.47 (5.26) 21.40 (5.21) 38.14 (5.40) 33.39 (5.38) 25.28 (5.27) 22.19 (5.24) 19.65 (5.16) 17.19 (5.13) 57.89 (5.46) 56.96 (5.46) 38.80 (5.38) 38.09 (5.38) 30.39 (5.31) 29.70 (5.30) 45.88 (5.42) 45.35 (5.42) 30.65 (5.30) 30.25 (5.29) 23.73 (5.19) 23.34 (5.19) 47.30 (5.48) 41.33 (5.47) 30.83 (5.39) 26.55 (5.36) 24.49 (5.32) 21.24 (5.29) 39.07 (5.43) 33.84 (5.42) 26.11 (5.30) 23.21 (5.29) 20.56 (5.20) 18.46 (5.19) 58.32 (5.46) 38.69 (5.37) 30.06 (5.29) 45.61 (5.42) 30.26 (5.29) 23.25 (5.18) 41.59 (5.39) 33.90 (5.34) 26.74 (5.28) 21.84 (5.22) 21.25 (5.18) 17.36 (5.13) 32.35 (5.37) 27.16 (5.34) 21.55 (5.23) 17.80 (5.19) 16.67 (5.11) 13.91 (5.05) 57.43 (5.45) 55.73 (5.44) 38.10 (5.37) 36.79 (5.36) 29.94 (5.29) 28.70 (5.28) 45.11 (5.42) 44.10 (5.42) 29.92 (5.29) 29.01 (5.28) 23.03 (5.18) 22.34 (5.17) 40.27 (5.44) 34.18 (5.41) 25.86 (5.34) 21.60 (5.30) 20.52 (5.25) 17.40 (5.21) 32.85 (5.41) 28.60 (5.39) 22.21 (5.28) 19.45 (5.26) 17.79 (5.18) 15.86 (5.16) 57.82 (5.45) 38.45 (5.36) 30.00 (5.28) 44.87 (5.42) 29.73 (5.29) 22.75 (5.17) 34.05 (5.34) 26.29 (5.24) 21.84 (5.22) 16.74 (5.10) 17.02 (5.12) 13.22 (5.00) 26.37 (5.34) 21.04 (5.28) 17.39 (5.18) 13.93 (5.11) 13.79 (5.06) 10.91 (4.95) 56.18 (5.44) 53.31 (5.43) 37.51 (5.36) 35.47 (5.35) 29.70 (5.28) 27.70 (5.26) 43.79 (5.41) 41.48 (5.41) 29.18 (5.28) 27.68 (5.28) 22.36 (5.17) 21.33 (5.17) 34.27 (5.40) 28.72 (5.36) 21.31 (5.27) 17.64 (5.22) 17.17 (5.19) 14.25 (5.13) 28.01 (5.38) 24.13 (5.36) 19.19 (5.24) 16.60 (5.21) 15.42 (5.14) 13.57 (5.11) 55.39 (5.43) 36.92 (5.33) 28.90 (5.25) 44.53 (5.41) 29.54 (5.28) 22.73 (5.17) 25.67 (5.22) 19.34 (5.11) 16.22 (5.08) 11.70 (4.92) 12.86 (4.99) 9.55 (4.82) 20.64 (5.27) 16.13 (5.19) 13.63 (5.09) 10.61 (4.98) 10.74 (4.94) 8.15 (4.78) 53.13 (5.42) 48.20 (5.40) 35.43 (5.33) 31.97 (5.31) 27.73 (5.25) 25.31 (5.22) 42.55 (5.40) 38.76 (5.39) 28.34 (5.28) 25.75 (5.26) 21.75 (5.17) 19.88 (5.15) 27.28 (5.32) 22.16 (5.26) 17.15 (5.18) 13.84 (5.11) 13.90 (5.10) 11.51 (5.02) 23.18 (5.34) 19.81 (5.30) 16.12 (5.20) 13.84 (5.16) 13.19 (5.09) 11.54 (5.05) 55.89 (5.42) 37.26 (5.33) 29.19 (5.25) 44.38 (5.42) 29.53 (5.28) 22.46 (5.16) 19.25 (5.11) 14.31 (4.96) 12.00 (4.94) 8.68 (4.72) 9.48 (4.81) 6.92 (4.56) 15.80 (5.19) 12.16 (5.08) 10.47 (4.98) 7.68 (4.76) 8.10 (4.78) 5.93 (4.54) 51.93 (5.41) 44.51 (5.38) 34.71 (5.32) 29.25 (5.28) 27.43 (5.24) 22.83 (5.19) 40.90 (5.40) 34.66 (5.38) 27.38 (5.27) 22.95 (5.24) 20.93 (5.15) 17.75 (5.12) 21.97 (5.26) 17.67 (5.18) 13.69 (5.10) 11.15 (5.02) 11.44 (5.02) 9.51 (4.94) 19.32 (5.29) 16.32 (5.24) 13.59 (5.15) 11.73 (5.09) 11.29 (5.04) 9.85 (4.98)"
        }
    ],
    "affiliations": [
        "Cornell Tech, NY",
        "EPFL, Lausanne, Switzerland",
        "Microsoft AI"
    ]
}