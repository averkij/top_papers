{
    "paper_title": "VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement",
    "authors": [
        "Daeun Lee",
        "Jaehong Yoon",
        "Jaemin Cho",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent text-to-video (T2V) diffusion models have demonstrated impressive generation capabilities across various domains. However, these models often generate videos that have misalignments with text prompts, especially when the prompts describe complex scenes with multiple objects and attributes. To address this, we introduce VideoRepair, a novel model-agnostic, training-free video refinement framework that automatically identifies fine-grained text-video misalignments and generates explicit spatial and textual feedback, enabling a T2V diffusion model to perform targeted, localized refinements. VideoRepair consists of four stages: In (1) video evaluation, we detect misalignments by generating fine-grained evaluation questions and answering those questions with MLLM. In (2) refinement planning, we identify accurately generated objects and then create localized prompts to refine other areas in the video. Next, in (3) region decomposition, we segment the correctly generated area using a combined grounding module. We regenerate the video by adjusting the misaligned regions while preserving the correct regions in (4) localized refinement. On two popular video generation benchmarks (EvalCrafter and T2V-CompBench), VideoRepair substantially outperforms recent baselines across various text-video alignment metrics. We provide a comprehensive analysis of VideoRepair components and qualitative examples."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 5 1 1 5 1 . 1 1 4 2 : r VIDEOREPAIR: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement"
        },
        {
            "title": "Mohit Bansal",
            "content": "UNC Chapel Hill {daeun,jhyoon,jmincho,mbansal}@cs.unc.edu https://video-repair.github.io Figure 1. VIDEOREPAIR is model-agnostic, training-free, automatic refinement framework for improving alignments in text-to-video generation. Given an initial video from text-to-video generation model, VIDEOREPAIR refines video in four stages: (1) video evaluation, (2) refinement planning, (3) region decomposition, and (4) localized refinement. The black-white mask in the bottom left of each example indicates the localized refinement plan (black: regions to preserve / white: regions to refine)."
        },
        {
            "title": "Abstract",
            "content": "Recent text-to-video (T2V) diffusion models have demonstrated impressive generation capabilities across various domains. However, these models often generate videos that have misalignments with text prompts, especially when the prompts describe complex scenes with multiple objects and attributes. To address this, we introduce VIDEOREPAIR, novel model-agnostic, training-free video refinement framework that automatically identifies fine-grained text-video misalignments and generates explicit spatial and textual feedback, enabling T2V diffusion model to perform targeted, localized refinements. VIDEOREPAIR consists of four stages: In (1) video evaluation, we detect misalignments by generating fine-grained evaluation questions and answering those questions with MLLM. In (2) refinement planning, we identify accurately generated objects and then create localized prompts to refine other areas in the video. Next, in (3) region decomposition, we segment the correctly generated area using combined grounding module. We regenerate the video by adjusting the misaligned regions while preserving the correct regions in (4) localized refinement. On two popular video generation benchmarks (EvalCrafter and T2V-CompBench), VIDEOREPAIR substantially outperforms recent baselines across various text-video alignment metrics. We provide comprehensive analysis of VIDEOREPAIR components and qualitative examples. 1 Figure 2. Comparison of different refinement methods for alignment. (a) Prompt optimization (e.g., OPT2I [29]) by LLM-based rewriting without visual/fine-grained feedback, making the search expensive (e.g., 30 iterations). (b) Recent work on localized feedback (e.g., SLD [52]) provides visual feedback but requires the use of an externally trained layout-guided generation module and result in unnatural refined output. Our (c) VIDEOREPAIR is training-free, model-agnostic refinement framework for T2V alignment that provides fine-grained localized visual guidance and uses the original T2V model. 1. Introduction Recent text-to-video (T2V) diffusion models [3, 8, 11, 14, 41, 47, 54] have shown impressive photorealism and versatility across diverse domains. However, these models often struggle to generate videos that accurately follow text prompts, particularly when the prompt includes multiple objects and attributes, such as incorrect number of objects or attribute bindings (see Fig. 3). Such misalignment problems largely discourage practical applications. Several recent work has studied enhancing text alignments of diffusion models via training-free iterative refinements [29, 52]. Prompt optimization [29] iteratively searches for better prompts, by creating multiple variations of prompts with LLM and choosing videos with the highest score (e.g., DSG [6]), as described in Fig. 2 (a). However, as there is no explicit feedback for misalignment, such method is sensitive to initial noise [1, 30, 37, 45] and thus requires many iterations (e.g., 30) to reach prompt that improves alignment. In different approach, SLD [52] proposes refinement framework with more explicit guidance, as described in Fig. 2 (b). SLD first generates bounding-box level plan with an LLM, then runs set of refinement operations (e.g., object addition, deletion, reposition) following the plan. However, the object addition requires an external layout-guided object 2 Figure 3. Example misalignments in T2V generation. Left: object misalignment (e.g., man is missing / 5 children instead of 4). Right: attribute misalignment (e.g., dogs color is leaked from dolphins color / bear is brown instead of blue). Videos are generated with T2V-turbo [18]. generator (e.g., GLIGEN [19]), and the added object often poorly harmonizes with the original image (see Fig. 5). To address these limitations, we introduce VIDEOREPAIR, new framework that automatically detects finegrained text misalignment in generated videos and performs localized refinements (Fig. 2 (c)). VIDEOREPAIR performs four-stage process: (1) video evaluation, (2) refinement planning, (3) region decomposition, and (4) localized refinement, as illustrated in Fig. 4. In (1) video evaluation, we first generate list of fine-grained, object-centric evaluation questions from the initial prompt with an LLM (e.g., GPT-4o) and establish corresponding answer, including evaluation score (i.e., 0 or 1), using an MLLM (e.g., GPT-4o [31]) to identify errors in the generated videos. In (2) refinement planning, we identify accurately generated objects using obtained question-answer pairs. Here, we also create localized prompt for the refinement region, specifying the objects, attributes, and concepts to be refined and how they should be adjusted. This prompt will guide the refinement process in the last stage. In (3) region decomposition, we adopt Molmo [7] and Semantic-SAM [15] to segment the regions to keep in the video during refinement. Finally, in (4) localized refinement, we generate new video based on MultiDiffusion [2], by maintaining the correctly-generated regions with the original noises/prompts and updating the remaining regions with refinement prompt+resampled noise. We demonstrate the effectiveness of VIDEOREPAIR in two popular text-to-video generation benchmarks: EvalCrafter [25] and T2V-CompBench [44], where VIDEOREPAIR significantly outperforms recent refinement methods across various types of compositional prompts with different object number, attributes, and locations. We provide comprehensive analysis on VIDEOREPAIR components. Lastly, we provide qualitative examples where VIDEOREPAIR is more effective in improving text-video alignments than baselines. We hope our study encourages future advancements in automatic refinement frameworks in visual generation tasks. 2. Related Works Text-to-video generation with diffusion models. Text-tovideo (T2V) diffusion models [3, 8, 11, 12, 14, 26, 41, 47, 49, 51, 54, 56] aim to produce videos describing given text prompts. These methods train denoising model that can gradually generate clear videos from noisy videos, where the noises are added via diffusion process [10]. The denoising is commonly performed in compact latent space of an autoencoder [39] for computational efficiency. VideoCrafter2 [4] synthesizes low-quality videos with high-quality images through joint training design of spatial and temporal modules, obtaining high-quality videos. T2V-turbo [18] presents distilled video consistency model [42, 48] for improved and rapid video generation. line of recent work also studies LLM-guided planning frameworks, where an LLM first generates an overall plan (e.g., list of bounding boxes) then video diffusion models render the scene following the plan [21, 22, 27]. However, even the recent T2V diffusion models suffer from misalignment problems. In the following, we discuss the research direction of refining the image/video diffusion models, including VIDEOREPAIR. Automatic refinement for image/video diffusion models. Recent works propose refinement frameworks that automatically improve diffusion models text alignment [17, 29, 43, 52]. line of work studies training-based refinement, where they detect errors of diffusion model, generate training data, and then finetune the model to improve alignment [17, 43]. However, training-based methods are expensive and can often make the model overfit to specific domains of generated training data. Another line of work proposes training-free refinement [29, 52]. OPT2I [29] presents iterative prompt optimization, where an LLM provides various variations of text prompts, T2I diffusion models generate images from the prompts, and the images are ranked with T2I alignment score (e.g., DSG [6]) to provide the final image. Since no explicit feedback is given to the backbone generation model, it usually takes long iterations (e.g., 30 LLM calls) to find prompt that provides improved alignment, making the framework expensive to use in practice. SLD [52] proposes refinement framework with more explicit guidance, where an LLM provides bounding-box level plan, followed by set of operations (e.g., object addition, deletion, reposition). However, SLD requires an external layout-guided object generator (e.g., GLIGEN [19]) to add objects, and the added object often poorly harmonizes with the original image. VIDEOREPAIR is training-free refinement framework that gives fine-grained localized feedback, works with any T2V diffusion model, and does not require additional generator. 3. VIDEOREPAIR: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement We propose VIDEOREPAIR, novel automatic refinement framework for T2V generation. VIDEOREPAIR is designed with three key questions in mind: (i) which prompt elements are misaligned in the video?; (ii) which objects are valuable to be preserved?; (iii) which video areas should be modified?; (iv) how can we update the video? In VIDEOREPAIR, we explicitly detect, localize, and address the fine-grained errors of T2V generation models in four-stage process: (1) video evaluation (Sec. 3.1), (2) refinement planning (Sec. 3.2), (3) region decomposition (Sec. 3.3), and (4) localized refinement (Sec. 3.4). Below, we describe the problem definition and details of each stage. 3.1. Video Evaluation with Misalignment Detection Generating evaluation questions. Our primary goal is to generate video that achieves improved alignment with text prompts, using pre-trained T2V diffusion model without requiring additional fine-tuning. Given an input text prompt and initial noise ϵ0 (0, I), we first generate an initial video V0 = (p, ϵ0). To identify which elements of the prompt are misaligned with the generated video, we create list of evaluation questions designed to be answered with yes or no. Specifically, we generate object-centric questions by providing manually written in-context examples to the LLM. Following DSG [6], we first define semantic categories consisting of entities, attributes, and relationships. Each element is represented as semantic tuple : attributes are expressed in 2-tuples (entity, its attribute, (e.g., {bed, blue}), and relationships are in 3-tuples (subject entity, object entity, and their relationship, (e.g., {people, pizza, make}). Based on , which covers all scene-relevant information, we generate questions using the LLM (e.g., GPT-4). Note that although DSG includes count questions, they are only generated when there is more than one object class; i.e., there is no penalty about object counts when there is single object in the prompt. For example, given prompt there is bear, DSG only generates an evaluation question 3 Figure 4. Illustration of VIDEOREPAIR. VIDEOREPAIR refines the generated video in four stages: (1) video evaluation (Sec. 3.1), (2) refinement planning (Sec. 3.2), (3) region decomposition (Sec. 3.3), and (4) localized refinement (Sec. 3.4). From the initial prompt p, we first generate fine-grained evaluation question set Qo and ask MLLM to answer it. Next, we identify by MLLM and plan how to refine the other region by LLM. Using O, we decompose the region by Molmo and Semantic SAM to get the region to keep. In the end, we conduct localized refinement by the original frozen T2V model. is there bear?, which only checks the bears existence, but does not penalize when more than one bear is generated. To address this, we edit in-context examples of DSG so that it can generate count-related questions for single object (e.g., Is there one bear?) as well, as visualized in Fig. 4 step 1. We denote our modified DSG as DSGObj, Qc as count-related questions, and attribute/relationship-related questions as Qa. Our ablation study (see Table 3) shows that evaluation questions of DSGObj are more effective than the original DSG questions in guiding video refinements. See appendix for more details. Answering to identify video errors. We now evaluate the generated video V0 to identify elements that are accurately generated and those that require refinement. We first define the set of key object names in from entity tuples and group questions in corresponding to each object. Let Qo = {Qo c} be the set of questions related to object (e.g., bear), including both attribute-related (Qo c), each requiring distinct answer format. For example, Are the people making pizza? corresponds to Qo (attribute-related question), and a) and count-related questions (Qo a, Qo = {bo Is there one bear? corresponds to Qo (count-related question). Specifically, for attribute-related questions Qo a, we prompt GPT-4o to analyze the initial video V0 and provide binary answer (yes=1 and no=0), denoted as Ao a}, to evaluate the alignment between the objects in V0 and those specified in the prompt p. For count-related questions Qo c, we generate the exact number of objects and binary answer and no = {bo in the form of triplet Ao are the numbers of the object in and V0. That is, bo = 1 if no v, otherwise bo = 0. For example, in Fig. 4, for the question Is there one bear? bbear = 1 and nbear = 2 (when the video V0 shows two bears and the prompt describes only one bear, it is marked incorrect). = 0 when nbear v}, where no = no p, no c, no In this way, we can quantitatively assess both the presence and count of essential objects, enabling us to detect any discrepancies between the objects specified in V0 and p. Here, we use the first frame of V0 as input for GPT-4o. 3.2. Refinement Planning: what to keep and refine Identifying visual content to be retained. As mentioned earlier, the initially generated video may suffer from insufficient text-video alignment due to incorrect or distorted 4 generation of objects and their attributes. However, this does not mean that all components are mis-generated; some key concepts or objects may have been generated precisely in certain areas. VIDEOREPAIR aims to retain these accurately generated portions while focusing on correcting only the mis-generated regions to ensure improved text-video alignment. To this end, we first identify the key object and determine the number of its instances to be preserved. First, to select which object (i.e., object name) should be retained, we prompt GPT-4o with question-answer pairs and the initial video as input, allowing it to identify the most suitable object to preserve. Next, to determine the number of instances of to retain, we define the count of as based on the previous triplet Ao , no } as follows: = {bo , no = (cid:40) no no no if no otherwise, (1) where no < no indicates the need to remove excess instances of the object, and no > no suggests that additional instances are required. For example, in step 2 of Fig. 4, if represents bear with no = 1 and no = 2, we set = 1, preserving only one bear because there is an extra bear in < no the generated content (no ), requiring the deletion of an extra instance. This approach effectively removes the additional instance, ensuring accurate object representation. Prompt regeneration for regions requiring refinement. We additionally generate local prompt for refinement to enable distinct control over different regions during generation. To this end, we prompt an LLM to produce refinementoriented prompt, pr, based on but excluding any questions related to O. As illustrated in Fig. 4, steps 2 and 4, this regenerated local prompt will be used to guide the denoising process for specific areas to be refined during video generation in later stage (will be discussed in Sec. 3.4). 3.3. Region Decomposition Given the errors identified in earlier stages, we localize the video regions corresponding to the errors to create concrete guidance in the following refinement stage. Similarly, SLD [52] uses an open-vocabulary object detector to detect localization errors through bounding boxes. However, unlike T2I generation, T2V generation often introduces complex distortions (e.g., attribute mixing) across scenes, making it difficult to capture all cases of distortion using only modelbased detector. Additionally, segmenting each object area with separate bounding boxes complicates interactions between objects in the scene. To address these challenges, we propose an alternative approach: identifying and retaining correctly generated objects while regenerating misaligned objects in the remaining areas. 5 To localize the correctly generated area, we generate local mask for the area to be refined using Molmo [7], VLM that can localize specific objects by referring 2d points in an image given text-pointing prompt, and SemanticSAM [15]. As illustrated in step 3 of Fig. 4, we first construct pointing prompt pp using predefined and O, Point {N } {O} (e.g., Point 1 bear) and obtain the referring 2D coordinates for with Molmo. Then, using this point as input, we employ Semantic-SAM to segment the specified area and finally represent it as binary segmentation mask RHW , where and denotes the height and width of the video frame, respectively. 3.4. Localized Refinement Localized noise re-initialization. At this stage, we refine the video to achieve more accurately aligned output video. Inspired by region-based text-to-image generation framework [2, 19], we adopt mask-based segmentation approach to control specific regions within the video generation process. Given the region to refine (obtained in Sec. 3.3), we introduce selective noise re-sampling process to enable controlled regeneration of specific regions. Specifically, different from MultiDiffusion [2], we preserve the partial region of the initial noise map ϵ0 detected by and re-initialize the rest areas with newly sampled noise ϵ 0 (0, I) to expect different generation tendency from initial video. This aims to keep the masked areas in the video consistent while allowing the unmasked areas to be refined based on the updated prompt. To process the pixel-level mask in the latent space, we transform from pixel space to latent space through block averaging (i.e., pooling). Specifically, we take the mean of each H/c W/c submatrix within M, where denotes the downsampling scale factor, effectively reducing the spatial resolution of the mask. We apply this transformed mask consistently across the entire temporal domain. The combined noise map ϵ 0 is then computed as follows: 0 = (ϵ0 pool (M, c)) + (ϵ ϵ 0 (1 pool (M, c))) , (2) where pool(, c) : RHW RH/cW/c denotes the block averaging (i.e., pooling) operation with scale factor c, and represents element-wise multiplication, which preserves the initial noise map structure in the masked regions. Using this hybrid noise map ϵ 0 as input to frozen video diffusion model with corresponding localized prompts, we achieve targeted refinement within designated regions, resulting in high-fidelity video with controlled updates that align with the intended modifications. Localized text guidance. We apply distinct text prompts to regions based on their noise re-initialization status, using 1 for re-initialized areas and for preserved regions. For the re-initialized regions, we guide generation in the latent space using regenerated prompts pr (See Sec. 3.2) tailored to those areas. Meanwhile, motivated by the presence of noise bias [1, 37, 45] (or referred to as trigger patches [30]) tied to specific text prompts in diffusion models, we reuse the initial prompt to ensure that features associated with are preserved in the regions designated for retention (Fig. 4, step 4). Our proposed regionalized decomposition of the initial prompt allows for the creation of new or additional objects in the unmasked areas while maintaining the designated content within the masked regions. In the end, VIDEOREPAIR enables the existing video diffusion model to generate enhanced video content that is both consistent and more closely aligned with the input prompt. By leveraging advanced local prompts and mask configurations, VIDEOREPAIR effectively resolves discrepancies between the query and the generated video, all without requiring additional training. Video ranking. To further ensure the quality of refined videos, we implement simple video ranking strategy. Similar to generating multiple candidate prompts in [29], we produce refined videos using different random seeds and select the best one based on their DSGobj scores, as obtained in Sec. 3.1, thus avoiding additional computations or resource burdens. If multiple videos receive tied DSGobj (since DSGobj provides discrete scores based on the number of questions), we select the video with the highest BLIPBLEU score [25] among them. Iterative refinement. Users can iteratively perform the four-step VIDEOREPAIR cycle for further improvement in text-video alignment. By default, we use single iteration as we find single iterations could already provide meaningful improvement (see Fig. 7). 4. Experiments We compare VIDEOREPAIR and recent refinement methods on different text-to-video generation benchmarks. Below we provide the experiment setups (Sec. 4.1), quantitative evaluation with baselines (Sec. 4.2), qualitative examples (Sec. 4.3), and additional analysis of VIDEOREPAIR components (Sec. 4.4). 4.1. Experiment Setups Benchmarks and evaluation metrics. We adopt two textto-video generation benchmarks: EvalCrafter [25] and T2VCompBench [44], which extensively evaluate text-to-video alignment with various types of prompts. For EvalCrafter, we split prompts by attributes according to their official metadata.1 In our experiments, we use count, color, and ac1https : / / github . com / evalcrafter / EvalCrafter / blob/master/metadata.json tion prompts among their attributes, each containing specific descriptions: object counts (e.g., 2 Dog and whale, ocean adventure), object colors (e.g., green umbrella with yellow bird perched on top), and actions (e.g., Hawaiian woman playing ukulele). General prompts without specific attributes were also included in the others section (e.g., goldfish in glass). For evaluation metrics in EvalCrafter, we mainly adopt the overall Text-Video Alignment and video quality scores. Here, the Text-Video Alignment score is defined as an average of CLIP-Score [38], SD-Score [36], BLIP-BLEU [16, 33], Detection-Score, Count-Score, and Color-Score [5]. The video quality score indicates the average score of the Video Quality Assessment score [50] and the Inception Score [40]. full table with metric-wise results is provided in the appendix. For T2V-CompBench, we evaluate generation capabilities from the composition-centric scene prompts, including spatial relationships, generative numeracy, and consistent attribute binding, where each of these categories includes 100 prompts. We evaluate each category using ImageGridLLaVA [23] for consistent attribute binding and GroundingDINO [24] for the other two dimensions. See appendix for more details. Implementation Details. We implement VIDEOREPAIR on two recent T2V models (T2V-turbo [18] and VideoCrafter2 [4]). In the video evaluation stage (Sec. 3.1), we follow the DSG [6] framework, where we employ GPT-40125 [32] to generate evaluation questions and GPT-4o [31] to provide corresponding answers. In the region decomposition stage (Sec. 3.3), we employ MolmoE-1B and SemanticSAM (L). We use = 5 and 1 iteration for experiments. Once the DSG score reaches 1.0 (the maximum score), we do early-stop and skip further refinements. We use two NVIDIA RTX A6000 GPUs (40GB) for experiments. See appendix for more details. Baselines. We compare VIDEOREPAIR with recent refinement methods, OPT2I [29]) and SLD [52], on the same T2V models (T2V-turbo [18] and VideoCrafter2 [4]). We unified random seeds in all experiments, ensuring that all methods refine the same initial video. Note that both OPT2I and SLD are originally introduced for text-to-image refinement. Following Manas et al. [29], we also include LLM paraphrasing as baseline - using GPT-4 to generate diverse paraphrases of the initial prompt. For OPT2I, we score the videos by the original DSG [6] (unmodified version) on the first frame, using GPT-3.5 for question generation and GPT-4o for VQA. We set OPT2I to iteratively generate five prompt candidates and perform 10 and 5 iteration steps for T2V-turbo and VideoCrafter2, considering the slower inference time of VideoCrafter2. For SLD, since its refine6 Table 1. Evaluation results on EvalCrafter [25]. We report the results with four prompt splits, including Count, Color, Action, and Others. Average represents the average score of all splits. The others section contains general prompts without specific attributes. The best numbers within each block are bolded. Method Text-Video Alignment Visual Quality Count Color Action Others Average VideoCrafter2 + LLM paraphrasing + SLD [52] + OPT2I [29] + VIDEOREPAIR (Ours) T2V-turbo + LLM paraphrasing + SLD [52] + OPT2I [29] + VIDEOREPAIR (Ours) 47.52 45.87 44.47 47.69 48.67 46.14 49.49 47.39 47.44 52.51 46.28 47.81 46.45 47.67 48.80 45.05 43.16 43.99 45.00 51. 44.07 44.41 39.89 45.04 45.47 41.42 41.32 42.13 44.64 45.79 46.20 45.16 44.06 44.65 46.35 43.16 44.75 43.28 45.54 46.11 46.02 45.81 43.72 46.26 47.32 43.94 44.68 44.20 45.66 48. 61.89 62.53 52.53 62.13 62.15 63.54 62.98 56.67 63.35 63.28 Figure 5. Videos generated with T2V-turbo and refinement frameworks (OPT2I / SLD / VIDEOREPAIR) on T2V-turbo. VIDEOREPAIR successfully addresses object and attribute misalignment issues (e.g., numeracy, spatial relationship, attribute blending) compared to T2Vturbo and other refinement methods. More visualization examples with T2V-turbo and VideoCrafter2 are provided in the appendix. ment model is based on LMD+ [20], we apply SLD to each frame of initial videos from T2V models. See appendix for more details. In addition, we also compare VIDEOREPAIR with state-of-the-art (SoTA) T2V models: ModelScope [47], ZeroScope [13], Latte [28], Show-1 [55], Open-Sora-Plan v1.1.0 [35], VideoTetris [46], and Vico [53]. 4.2. Quantitative Results EvalCrafter: VIDEOREPAIR improves T2V alignments, outperforming existing refinement methods. Tab. 1 shows the evaluation results on EvalCrafter, measured with text-video alignment and visual quality. We observe that SLD and OPT2I only improve the baseline diffusion backbones minimally or even hurt the performance. Specifically, SLD SLD significantly deteriorates the alignment of action and count categories when applied to VideoCrafter2. We expect that this is because SLD is designed to merge denoised latent features of each object for each frame, resulting in challenges in maintaining consistent object counts and locations for video generation tasks. OPT2I improves the alignment on action category by extensively searching for optimized prompts through sequential processes of prompt candidate generation, ranking via DSG, and selection. However, its overall improvements are limited as it cannot provide finegrained localized guidance beyond the text space. Alternatively, VIDEOREPAIR surpasses all baselines in the text-video alignment metric (evaluated using CLIP, BLIP2, and SAM-Track) across all four splits by significant margin, achieving relative improvements of +2.87% and +11.09% over VideoCrafter2 and T2V-turbo on initial 7 Table 2. Evaluation results on T2V-Compbench [44]. We report the results with three prompt splits (Consist-Attr/Spatial/Numeracy) The best numbers within each block are bolded. Table 3. Ablations of different VIDEOREPAIR components, evaluated on three splits (Count/Color/Action) of EvalCrafter. Our default setup is highlighted with blue background. Method Consist-Attr Spatial Numeracy Avg. (Other T2V models) ModelScope [47] ZeroScope [13] Latte [28] Show-1 [55] Open-Sora-Plan [35] VideoTetris [46] VideoCrafter2 + Vico [53] + VIDEOREPAIR (Ours) T2V-turbo + LLM paraphrasing + SLD [52] + OPT2I [29] + VIDEOREPAIR (Ours) 0.5483 0.4495 0.5325 0.6388 0.7413 0.7125 0.6868 0.6470 0.7275 0.7162 0.6582 0.6850 0.7452 0.7475 0.4220 0.4073 0.4476 0.4649 0.5587 0.5148 0.5174 0.5425 0.5440 0.5585 0.5380 0.6163 0.5972 0. 0.2066 0.2378 0.2187 0.1644 0.2928 0.2609 0.2962 0.2762 0.3603 0.2821 0.2512 0.2455 0.2909 0.2931 0.3923 0.3648 0.3996 0.4227 0.5309 0.4961 0.5001 0.4886 0.5439 0.5189 0.4825 0.5156 0.5444 0. Figure 6. The iterative refinement of VIDEOREPAIR. Videos in each column represent the outputs of successive refinement iterations, where the output from the previous step serves as the input for the current step. The text at the bottom of each video row indicates the corresponding text prompt. More visualization examples are provided in the appendix. video generations. Note that VIDEOREPAIR performs refinement process that preserves the visual quality of the backbone models. This shows the strong effectiveness of VIDEOREPAIR in improving T2V alignment without sacrificing visual quality. T2V-Compbench: VIDEOREPAIR improves T2V alignments, also outperforming strong T2V baselines. Tab. 2 shows the evaluation results on T2V-Compbenchs three dimensions: consistent attribute binding, spatial relationship, and generative numeracy. We observe that VIDEOREPAIR improves initial videos from both T2V models (VideoCrafter2 and T2V-turbo) in all three splits, achieving relative improvements of +8.76% and +5.40%, respectively. While SLD with T2V-turbo demonstrates strong per8 Eval. Question (Sec. 3.1) Object Selection (Sec. 3.2/Sec. 3.3) Ranking Metric (Sec. 3.4) Text-Video Alignment DSG DSGObj DSGObj DSGObj DSGObj random random GPT-4o GPT-4o GPT-4o DSGObj DSGObj DSGObj CLIPScore BLIP-BLEU 46.73 47.11 49.82 46.30 49. formance in the spatial relationship dimension, it struggles with consistent attribute binding, which requires managing at least two dynamic objects with distinct attributes, resulting in -0.64% decrease in average performance from the original T2V-turbo. Vico [53] is built upon the VideoCrafter2. It improves spatial relationships of objects in videos. However, it notably degrades other aspects of compositionality in video generation, such as numeracy and attribute binding. 4.3. Qualitative Results Fig. 5 visualizes videos generated by the original T2V-turbo and refinement frameworks (OPT2I, SLD, and VIDEOREPAIR) on T2V-turbo. These examples clearly illustrate how effectively VIDEOREPAIR addresses object and attribute misalignment issues (as discussed in Fig. 3) compared to T2V-turbo and other refinement methods. In the leftmost example, VIDEOREPAIR precisely generates the specified color attribute (blue apple), while other methods incorrectly produce pink apples blended with the pink tree. In the middle example, VIDEOREPAIR improves on T2V-turbo by accurately generating three distinct three dogs, whereas other baselines either fail to do (OPT2I) so or introduce artificial distortions (SLD). In the rightmost example, VIDEOREPAIR successfully captures spatial relationships among different objects (sandcastle on the left of beach umbrella) without compromising multi-object generation. In addition, we validate the potential of VIDEOREPAIR for iterative refinement. While single refinement step of VIDEOREPAIR may not fully achieve precise alignment with the initial prompt, we explore an iterative refinement process to progressively enhance alignment and address any residual discrepancies. As shown in Fig. 6, the initial refinement process partially resolves misalignments between the video and the prompt (generating scene depicting night of camping under the stars) but misses the family. but omits the presence of the family. Through subsequent refinement iterations, VIDEOREPAIR successfully achieves precise alignment with the text prompt. Similarly, the example at the bottom of Fig. 6 demonstrates the generation of seven cute puppies after iterative refinements. Please also see the appendix for additional qualitative Table 4. Ablations of # Videos candidates (K) evaluated on EvalCrafter Count, Color, and Action prompts using T2V-turbo. Avg. represents the average performance of these three categories. Our default setup is highlighted with blue background. # Video candidates (K) 1 5 Text-Video Alignment Count Color Action Avg. 46.88 52.51 46.36 51. 44.79 45.79 46.01 49.82 examples where we compare VIDEOREPAIR with baselines, iterative refinements, and step-by-step illustrations of VIDEOREPAIR. 4.4. Additional Analysis VIDEOREPAIR components. We compare different components of VIDEOREPAIR using T2V-turbo backbone, on three splits (Count+Color+Action) of EvalCrafter. For evaluation questions (Sec. 3.1), we compare the original DSG question and our modified DSGObj. For the selection of the most prominent object (Sec. 3.2/Sec. 3.3), we compare selecting via GPT-4o to randomly selecting from all objects from the DSGObj semantic tuples. For scoring methods for video ranking (Sec. 3.4), we compare DSGObj with CLIPScore [9] and BLIP-BLEU [16, 34], which are parts of metrics used in EvalCrafter. Tab. 3 shows that the combination of DSGObj for evaluation question, GPT-4o for object selection, and DSGObj for video ranking achieves the best performance overall. We use these components for the default setting of VIDEOREPAIR. Ablations on # Videos candidates. We quantitatively analyze the impact of the video ranking in VIDEOREPAIR using subset of EvalCrafter (Count, Color, and Action categories). As shown in Tab. 4, VIDEOREPAIR already obtain superior performance without video ranking (i.e., = 1), compared to strong baselines (LLM paraphrasing: 44.7, SLD: 44.5, OPT2I: 45.7 in average), highlighting the effectiveness of VIDEOREPAIR refinement process. Furthermore, we enhance text-video alignment by incorporating video ranking based on DSGObj. Impact of Iterative Refinement. We experiment with iteratively performing VIDEOREPAIR to further improve the text-video alignments. We monitor the DSGObj score and terminate the iterative refinement when the DSGObj reaches 1.0 (max score), and use video ranking with K=5 candidates. As illustrated in Fig. 7, iterative refinement benefits all three prompt splits (count / color / action) of EvalCrafter. Additional iterative refinement examples are provided in Fig. 17. Figure 7. Impact of iterative refinement. Iterative refinement gradually improves DSGObj and text-video alignment score on all three prompt categories (count/color/action) of EvalCrafter. The initial video refers to video from T2V-turbo. We use video ranking with K=5 candidates. 5. Conclusion We introduce VIDEOREPAIR, novel model-agnostic, training-free video refinement framework that automatically identifies fine-grained text-video misalignments and generates explicit spatial and textual feedback, enabling T2V diffusion model to perform targeted, localized refinements. VIDEOREPAIR consists of four stages: In (1) video evaluation, we detect misalignments by generating evaluation questions and answering the questions with MLLM. In (2) refinement planning, we identify accurately generated objects and create local prompt for refinement. In (3) region decomposition, we segment areas in video to preserve and refine using combined grounding module. Finally, in (4) localized refinement, we regenerate the video by adjusting the misaligned regions while preserving the correct regions. On two popular video generation benchmarks (EvalCrafter and T2V-CompBench), VIDEOREPAIR substantially outperforms recent baselines across various text-video alignment metrics. We provide comprehensive analysis on VIDEOREPAIR components. We hope that our work encourages future advancements in automatic refinement frameworks in visual generation tasks."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, Accelerate Foundation Models Research program, and Bloomberg Data Science PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "[1] Yuanhao Ban, Ruochen Wang, Tianyi Zhou, Boqing Gong, Cho-Jui Hsieh, and Minhao Cheng. The crystal ball hypothesis in diffusion models: Anticipating object positions from initial noise. arXiv preprint arXiv:2406.01970, 2024. 2, 6 9 [2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In Proceedings of the International Conference on Machine Learning (ICML), 2023. 2, 5 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3 [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. 3, 6 [5] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023. 6 [6] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. 2, 3, 6, [7] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 2, 5 [8] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. 2, 3 [9] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. 9 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [11] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 2, 3 [12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [13] huggingface. Zeroscope, 2023. 7, 8 [14] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. 2, 3 [15] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv preprint arXiv:2307.04767, 2023. 2, 5 [16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 6, 9 [17] Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, and Mohit Bansal. Selma: Learning and merging skill-specific textto-image experts with auto-generated data. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [18] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 2, 3, 6, 14 [19] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, 5, 13 [20] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llmgrounded diffusion: Enhancing prompt understanding of textto-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023. 7 [21] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and Boyi Li. Llm-grounded video diffusion models. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. [22] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. arXiv preprint arXiv:2309.15091, 2023. 3 [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 6 [24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 6 [25] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evalIn Proceedings of uating large video generation models. the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3, 6, 7, 14 [26] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for highquality video generation. arXiv preprint arXiv:2303.08320, 2023. 3 [27] Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, and Shifeng Chen. Gpt4motion: Scripting physical motions in text-to-video generation via blender-oriented gpt planning. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [28] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 7, 8 [29] Oscar Manas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving text-toimage consistency via automatic prompt optimization. arXiv preprint arXiv:2403.17804, 2024. 2, 3, 6, 7, 8, 13, 14, 15, 22 [30] Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa. Guided image synthesis via initial image editing in diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, pages 53215329, 2023. 2, 6 [31] OpenAI. Hello gpt-4o, 2024. 2, 6 [32] OpenAI. GPT-4 technical report, 2024. 6 [33] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 6 [34] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 9 [35] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2023. 7, [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 6 [37] Zipeng Qi, Lichen Bai, Haoyi Xiong, et al. Not all noises are created equally: Diffusion noise selection and optimization. arXiv preprint arXiv:2407.14041, 2024. 2, 6 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [40] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6, 14 [41] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2, 3 [42] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. [43] Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning textto-image generation with image understanding feedback. In Synthetic Data for Computer Vision Workshop@ CVPR 2024, 2023. 3 [44] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. 3, 6, 8, 14 [45] Wenqiang Sun, Teng Li, Zehong Lin, and Jun Zhang. Spatialaware latent initialization for controllable image generation. arXiv preprint arXiv:2401.16157, 2024. 2, 6 [46] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, et al. Videotetris: Towards compositional text-to-video generation. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 7, 8 [47] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3, 7, 8 [48] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model, 2023. [49] Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and Nong Sang. recipe for scaling up text-to-video generation with text-free videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6572 6582, 2024. 3 [50] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. 6, 14 [51] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. 3 [52] Tsung-Han Wu, Long Lian, Joseph Gonzalez, Boyi Li, and Trevor Darrell. Self-correcting llm-controlled diffusion models. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3, 5, 6, 7, 8, 13, 14, 15 [53] Xingyi Yang and Xinchao Wang. Compositional video generation as flow equalization. arXiv preprint arXiv:2407.06182, 2024. 7, 8, 15 [54] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video 11 diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3 [55] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024. 7, 8 [56] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Trainingfree controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023."
        },
        {
            "title": "Appendix",
            "content": "A. VIDEOREPAIR Implementation Details . . . . . A.1. Question Generation . . . A.2. Visual Question Answering . A.3. Key Object Extraction . . . A.4. Refinement Prompt Generation . . . . . . . . . B. Additional Baseline Details C. Additional Evaluation Details D. Additional Quantitative Analysis . . D.1. Inference Time . D.2. Increasing # of Video Candidates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Additional Qualitative Examples . . . . E.1 . Comparison with Baselines . E.2 . Iterative Refinement . . . . . E.3 . Step-by-step Illustration of VIDEOREPAIR . . . . . . . . . . . . . 13 13 13 13 13 13 14 14 14 15 15 15 15 A. VIDEOREPAIR Implementation Details A.1. Question Generation For generating DSGObj, we follow DSG [6] mainly but revise in-context examples. Given the limitation of DSG as described in Fig. 8, we change all entity-whole tuples of DSG to count attribute tuples to capture the exact number of objects. Figure 8. Comparison of DSG and DSGObj. Compared to DSGObj (ours), DSG does not penalize the video even if more than one object (e.g., 1 bear in this case) is generated when the target object count = 1 in the text prompt. A.2. Visual Question Answering c) and attribute-related (Qo To evaluate the generated videos, we utilize GPT-4o to answer both count-related (Qo a) questions, as illustrated in Fig. 20. For Qo prompts, we guide GPT-4o through four steps: reasoning, answering, counting the predicted number of objects (no p), and verifying the true count (no v). These steps yield an answer triplet Ao c, no v}. To ensure valid responses, we account for dependencies among questions, following the methodology of DSG [6]. Each question is posed to GPT-4o sequentially, and DSG score is calculated after processing all VQA tasks. This DSG score determines whether the VIDEOREPAIR process should continue. If the DSG score reaches = {bo p, no 1.0 (indicating perfect score), the VIDEOREPAIR process is terminated. A.3. Key Object Extraction To extract the key concept from initial videos V0, we provide the first frame of V0 and the list of question-answer pairs for each object to GPT4o as shown in Fig. 21. Here, we prioritize selecting objects with higher number of 1.0 scores. Moreover, we force GPT4o to select object instead of background elements to improve the accuracy of region decomposition by pointing. A.4. Refinement Prompt Generation To produce refinement prompt pr, we use GPT4 with instruction as shown in Fig. 22. After getting O, we can decompose the whole question set as Qo and others depending on whether the keyword is included in the question. To generate pr from specific question sets, we utilize five manually crafted in-context examples to ensure the accuracy of the generation process. If the DSG score is 0.0 (indicating complete failure from VQA) and the key object cannot be identified, we consider the T2V model to have failed in generating any object correctly. In such cases, we paraphrase directly into pr using large language model (LLM). B. Additional Baseline Details LLM Paraphrasing. Following [29], we compare VIDEOREPAIR with paraphrasing prompts from LLM. Here, we ask GPT4 to generate diverse paraphrases of each prompt, without any context about the consistency of the images generated from it. The prompt used to obtain paraphrases is provided in Fig. 23. OPT2I. Since OPT2I [29] aims to improve text-image consistency for T2I models, we reimplement OPT2I for T2V setup. Specifically, we replace the original T2I model part with T2V models (T2V-Turbo and VideoCrafter2) to generate outputs. Using GPT-4o, we then pose DSG questions to these outputs. For prompts, we directly adopt the ones provided in the original OPT2I paper. For LLM, we use GPT4 as VIDEOREPAIR. Finally, we perform iterative refinement, running 10 iterations for T2V-Turbo and 5 iterations for VideoCrafter2, with five video candidates per iteration. SLD. To adapt SLD [52] to the T2V setup, we apply their official code to individual video frames and maintain their default setup. Note that SLD is GLIGEN [19]-based T2I model, which poses challenges for direct extension to video generation. Since SLD operates using DDIM inversion, we use the initial videos generated by T2V-Turbo and VideoCrafter2 as inputs, enabling the implementation of their noise composition method. Here, we use one iteration for SLD and GPT4 for LLM. Table 5. Inference time and text-video alignment of VIDEOREPAIR and baselines. Measured with single NVIDIA A100 80GB GPU, on EvalCrafter count split with 50 prompts. C. Additional Evaluation Details EvalCrafter. To evaluate the effectiveness of VIDEOREPAIR across different prompt dimensions, we decompose EvalCrafter [25] using the official metadata.json. Specifically, we utilize the attributes key for each prompt and categorize the dataset into count, color, action, text, face, and amp (camera motion). Prompts without explicit attributes are grouped into an others category. Among these dimensions, we focus on count, color, action, and others, excluding text, face, and amp. This decision is based on our observation that video errors related to text prompts (e.g., the words KEEP OFF THE GRASS), face prompts (e.g., Kanye West eating spaghetti), and amp prompts (e.g., Vietnam map, large motion) cannot be reliably detected through GPT-4o question-answering, therefore hard to proceed VIDEOREPAIR. For evaluation metrics, we mainly adopt the average text-video alignment score they proposed. Among their all text-video alignment scores (CLIP-Score, SDScore, BLIP-BLEU, Detection-Score, Count-Score, Color-Score, Celebrity ID Score, and OCR-Score) and OCR-Score exclude Celebrity ID Score we since they are related to face and text categories. Therefore, we calculate text-video alignment score as Avg(CLIP-Score, SD-Score, BLIP-BLEU, DetectionScore, CountScore, ColorScore). For overall video quality, we directly adopt their metrics including Inception Score [40] and Video Quality Assessment (V QAA, QAT ) [50]. T2V-Compbench. Since VIDEOREPAIR has strength in compositional generation, we adopt T2V-Compbench [44] and evaluate three dimensions: spatial relationships, generative numeracy, and consistent attribute binding. Spatial relationships requires the model to generate at least two objects while maintaining accurate spatial relationships (e.g. to the left of, to the right of, above, below, in front of) throughout the dynamic video. Generative numeracy specifies one or two object types, with quantities ranging from one to eight. Consistent attribute binding contains color, shape, and texture attributes among two objects. Following [44], we adopt Video LLM-based metrics for consistent attribute binding and detection-based metrics for spatial Relationships and numeracy. D. Additional Quantitative Analysis In this section, we present additional quantitative results to provide deeper understanding. Specifically, we demonstrate that VIDEOREPAIR achieves superior efficiency in Time per video () Total time () T2V-turbo [18] OPT2I [29] (k=5, iter=5) SLD [52] (iter=1) Ours (k=5, iter=1) 3.55s 185.86s 365.30s 59.61s 3m 12s 2h 34m 53s 5h 4m 25s 49m 40s () Text-Video Alignment 46.14 47.44 47.39 52.51 inference time compared to other baselines, while also highlighting the impact of iterative refinement and the effect of varying the number of video candidates. D.1. Inference Time To validate the efficiency of VIDEOREPAIR, we compare its inference time against other baselines. The evaluation is conducted using single NVIDIA A100 80GB GPU on the count section of EvalCrafter. We report the average inference time per video, the total inference time for 50 videos, and the text-video alignment score. As shown in Table 5, VIDEOREPAIR demonstrates the highest efficiency among all baselines while also achieving superior text-video alignment scores. Notably, even with just one iteration, VIDEOREPAIR can refine single video in only 59 seconds. Figure 9. Impact of the number of video candidates. We vary the number of video candidates as 1, 5, 10, and 20 for ranking. D.2. Increasing # of Video Candidates To evaluate the impact of video ranking, we vary the number of video candidates as = 1, 5, 10, and 20 during the ranking process. The variation among video candidates arises from different random seeds used to initialize ϵ 0. For example, video ranking is not applied when = 1, and only one refinement is produced using single random seed noise ϵ 0. For ranking metrics, we rely on DSGObj across all ablation studies. As depicted in Fig. 9, higher values (5, 10, and 20) consistently yield higher scores across all categories than = 1. This trend is particularly prominent in 14 the count category, where increasing leads to noticeable performance improvements, highlighting the importance of considering multiple candidates for ranking. E. Additional Qualitative Examples E.1. Comparison with Baselines We present additional qualitative comparisons with baseline methods (OPT2I [29], SLD [52], and Vico [53]) in Figs. 10 to 16. These examples address variety of failure cases commonly observed in T2V models, including inaccuracies in object count and attribute depiction, as highlighted in our main paper. Figs. 10 to 13 correspond to results from T2V-Turbo, while Figs. 14 to 16 showcase examples from VideoCrafter2. Additionally, we provide binary segmentation masks that identify preserved areas (in black) and updated areas (in white). Across these examples, VIDEOREPAIR effectively preserves the areas while refining the remaining regions using pr. For instance, in Fig. 10, the camel from the original T2V-Turbo video is preserved, and snowman is successfully added. In contrast, while SLD also leverages DDIM inversion to preserve objects, it often fails to integrate new objects seamlessly. E.2. Iterative Refinement We also demonstrate the results of iterative refinement in Fig. 17, showing the initial video alongside the first and second refinements generated from T2V-Turbo. Overall, VIDEOREPAIR progressively enhances text-video alignment with each refinement step. For numeracy-related cases (e.g., six dancers and five cows), VIDEOREPAIR iteratively adds or removes specific objects, ensuring alignment with the given prompts. In cases of missing objects (e.g., biologists and ducks), VIDEOREPAIR successfully generates additional biologists and multiple ducks while preserving the context of the initial video. Additionally, for attribute-related prompts (e.g., yellow umbrella and blue cup), VIDEOREPAIR effectively refines object attributes, such as adding wooden handle to the umbrella and enhancing the cups blue color. These results demonstrate the ability of VIDEOREPAIR to iteratively improve both object count and attribute alignment with high fidelity. E.3. Step-by-step Illustration of VIDEOREPAIR In Figs. 18 and 19, we provide detailed illustrations of all four VIDEOREPAIR steps. 15 Figure 10. Qualitative examples from T2V-turbo. Figure 11. Qualitative examples from T2V-turbo. 16 Figure 12. Qualitative examples from T2V-turbo. Figure 13. Qualitative examples from T2V-turbo. 17 Figure 14. Qualitative examples from VideoCrafter2. Figure 15. Qualitative examples from VideoCrafter2. 18 Figure 16. Qualitative examples from VideoCrafter2. Figure 17. Videos generated using iterative refinement with VIDEOREPAIR. We depict iterative refinement results generated from T2V-Turbo. Overall, VIDEOREPAIR progressively enhances text-video alignment with each refinement step. 19 Figure 18. Output from each step of VIDEOREPAIR. We illustrate whole outputs from each step of VIDEOREPAIR. Figure 19. Output from each step of VIDEOREPAIR. We illustrate whole outputs from each step of VIDEOREPAIR. 20 Figure 20. Prompts to perform visual question answering in video evaluation steps. Top: The prompt for Qo Bottom: prompt for Qo in each question. (count-related question), (attribute-related question). cur question means each DSGObj question and key objects means entity word Figure 21. Prompt to choose which object(s) to preserve. We ask GPT4o to select objects to preserve in the scene. 21 Figure 22. Prompt to plan how to refine the other regions. We use five in-context examples to create the refinement prompt from the question related to other objects. Figure 23. Prompt for LLM paraphrasing. Following OPT2I [29], we ask GPT4 to generate diverse paraphrases of each prompt for LLM paraphrasing baseline experiments."
        }
    ],
    "affiliations": [
        "UNC Chapel Hill"
    ]
}