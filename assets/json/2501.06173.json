{
    "paper_title": "VideoAuteur: Towards Long Narrative Video Generation",
    "authors": [
        "Junfei Xiao",
        "Feng Cheng",
        "Lu Qi",
        "Liangke Gui",
        "Jiepeng Cen",
        "Zhibei Ma",
        "Alan Yuille",
        "Lu Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent video generation models have shown promising results in producing high-quality video clips lasting several seconds. However, these models face challenges in generating long sequences that convey clear and informative events, limiting their ability to support coherent narrations. In this paper, we present a large-scale cooking video dataset designed to advance long-form narrative generation in the cooking domain. We validate the quality of our proposed dataset in terms of visual fidelity and textual caption accuracy using state-of-the-art Vision-Language Models (VLMs) and video generation models, respectively. We further introduce a Long Narrative Video Director to enhance both visual and semantic coherence in generated videos and emphasize the role of aligning visual embeddings to achieve improved overall video quality. Our method demonstrates substantial improvements in generating visually detailed and semantically aligned keyframes, supported by finetuning techniques that integrate text and image embeddings within the video generation process. Project page: https://videoauteur.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 7 1 6 0 . 1 0 5 2 : r VideoAuteur: Towards Long Narrative Video Generation Junfei Xiao1, Feng Cheng2, Lu Qi2, Liangke Gui2, Jiepeng Cen3, Zhibei Ma2, Alan Yuille1, Lu Jiang2 1Johns Hopkins University 2ByteDance Seed 3ByteDance Project Page: https://videoauteur.github.io Figure 1. Long Narrative Video Generation. We curate large-scale cooking video dataset to develop an interleaved auto-regressive model VideoAuteur, which acts as narrative director, sequentially generating actions, captions, and keyframes (two generated examples here). These elements condition video generation model to create long narrative videos."
        },
        {
            "title": "Abstract",
            "content": "within the video generation process. Recent video generation models have shown promising results in producing high-quality video clips lasting several seconds. However, these models face challenges in generating long sequences that convey clear and informative events, limiting their ability to support coherent narrations. In this paper, we present large-scale cooking video dataset designed to advance long-form narrative generation in the cooking domain. We validate the quality of our proposed dataset in terms of visual fidelity and textual caption accuracy using state-of-the-art Vision-Language Models (VLMs) and video generation models, respectively. We further introduce Long Narrative Video Director to enhance both visual and semantic coherence in generated videos and emphasize the role of aligning visual embeddings to achieve improved overall video quality. Our method demonstrates substantial improvements in generating visually detailed and semantically aligned keyframes, supported by finetuning techniques that integrate text and image embeddings 1. Introduction Video generation [5, 6, 17, 18, 39, 49, 60] has recently witnessed remarkable advancements with diffusion [2, 19, 32, 55] and auto-regressive models [22, 41, 42, 51]. primary objective is to generate coherent video clips from text prompts. In addition, text-to-video model is able to serve as foundation that can be adapted for various downstream applications, such as image animation [8, 52], video editing [4, 11], video stylization [20], etc. With the maturity of generating high-fidelity short video clips, researchers begin setting their sights on the next north-star: creating videos capable of conveying complete narrative which captures an account of events unfolding over time. The literature has extensively highlighted narratives. The importance of narratives has been extensively documented in the literature, emphasizing their fundamental role for mankind. For example, Bruner argues that narratives are essential tools for organizing experiences and memories [3]. Similarly, Harari indicates that the ability to share narrativesstories and abstract ideashas been pivotal in human development, setting humans apart from other animals and leading to the creation of nations and economic systems in our society [15]. It is not unexpected that all the key challenges associated with short-video will resurface in the context of long narrative video generation (NVG), along with new, unprecedented challenges across multi-clips, including constructing semantic consistency within complex event sequences, preserving object/character identity across multiple scenes, and more. notble challenge researchers face is the lack of highquality video data for learning narratives in video generation. While our community has developed many video datasets, most are unsuitable for NVG due to several reasons. First, most videos are tagged with descriptions that are partially or irrelevant to NVG. Second, even for the relevant descriptions, the quality may be insufficient, because these descriptions may be either too coarse or lack detailed actions needed for NVG. Finally, the type of video itself is factor; not all videos contain meaningful narratives suitable for learning. Learning from videos that have meaningful, complete, and unambiguous narratives is essential not only for training but for evaluating NVG progress and benchmarking different approaches. Partly due to the data challenge, progress in narrative video generation has been sluggish, especially when compared to story generation through series of images, such as comics, cartoon or illustrations [14, 21, 30, 54]. Some of these images, when treated as keyframes, may be further processed into video clips [16, 28, 59, 62, 63]. drawback of the keyframe-based approach is that the narrative video creation process is scattered across different modules and fine-tuned in multiple steps, making it difficult to optimize the overall process. Very recently, Yang et al. [54] proposed story generation method using vision-language model (VLM) to generate both images and text. Despite the promising results, there has yet to be comprehensive study on using VLM models for narrative video generation. This paper contributes to advancing research in narrative video generation in two ways. First, we curate and annotate large-scale video dataset focused on the cooking domain. The samples in our dataset are structured with clear narrative flows, each composed of sequential actions and visual states. Our dataset consists of approximately 200,000 video clips, with an average duration of 9.5 seconds per clip. We chose cooking videos due to their complete and unambiguous narratives, which are more objective to annotate and evaluate consistently. To address video copyright concerns, we source videos from existing video datasets, YouCook2 [61] and HowTo100M [31]. Beyond the video pre-processing such as quality filtering and captioning, we conduct caption-action matching mechanism to extract narrative clips, following the strict, step-by-step process inherent in cooking tasks. Furthermore, we present general auto-regressive pipeline for long narrative video generation, comprising two main components: long narrative director and visualconditioned video generation model. The long narrative director produces coherent narrative flow by generating sequence of visual embeddings or keyframes that represent the storys logical progression, as illustrated in Figure 1. Extensive experiments on our dataset demonstrate the effectiveness of the proposed pipeline for long narrative video generation. To sum up, our contributions are as follows: We build CookGen, large, structured dataset and comprehensive data pipeline designed to benchmark longform narrative video generation. We will open-source the data along with the necessary functionalities to support future long video generation research. We propose VideoAuteur, an effective data-driven pipeline for automatic long video generation. We emperically explore the design and training of an interleaved image-text auto-regressive model for generating visual states and visual-conditioned video generation model. 2. Related Works Text-to-Image/Video Generation Text-to-image [7, 23, 33, 34, 36, 48, 56] and video generation [5, 6, 17, 18, 39, 49, 60] have made remarkable progress to generate highfidelity video clip of 5-10 seconds. For example, latent design [36] has become mainstream, balancing effectiveness with efficiency. Building upon this design, diffusion-based models like DiT [32], Sora [2], and CogVideo [19, 55] leveraged larger datasets and explored refined architectures and loss functions to enhance performance. In contrast, auto-regressive models such as VideoPoet [22] and Emu series [41, 42, 51] sequentially predict image or video tokens. Instead, our work focuses on the models ability to generate long narrative videos beyond 5 seconds. Interleaved Image-Text Modeling Interleaved image-text generation [1, 9, 13, 13, 43, 53] has garnered attention as compelling research area that merges visual and textual modalities to produce rich outputs. Earlier approaches [26, 35, 35, 40] primarily relied on large-scale image-text paired datasets [12, 37] but were often confined to single-modality tasks, such as captioning or text-to-image generation. With the emergence of large language models (LLMs) [45], various vision-language models (VLMs) [25, 29, 50] have ushered in new era of unified representations, leveraging wellcurated datasets for interleaved generation. However, most existing works focus on the one-time generation and do not address the coherence of generated content, which is the focus of our work. Figure 2. CookGen contains long narrative videos annotated with actions and captions. Each source video is cut into clips and matched with the labeled actions. We use refined pseudo labels from ASR for Howto100M videos and use manual annotations for Youcook2 videos. We use state-of-the-art VLMs (i.e. GPT-4o and finetuned video captioner) to provide high-quality captions for all video clips. Narrative Visual Generation The narrative visual generation lies in ensuring consistency across generated images. With this motivation, several works like SEEDStory [54], StoryDiffusion [62], MovieDreamer [59], LVD [27], VideoDirectorGPT [28], Vlogger [63], Animatea-story [16], VideoTeris [44] predominantly employ conditional generation in diffusion or auto-regressive models. Our work closely relates to the very recent work [54], which generates multimodal, long-form coherent narratives based on user-provided images and texts as story beginnings. However, we aim to generate coherent stories through videos, presenting greater identity and motion preservation challenges. 3. Long Narrative Video Data To the best of our knowledge, datasets for long narrative video generation research is extremely limited. To enable in-depth exploration and establish an experimental setting, we establish CookGen, large video dataset with detailed annotations on captions, actions, and annotations. As the data example provided in Figure 2, our dataset focuses on cooking videos. We prioritize cooking over other video categories because each dish follows pre-defined, strict sequence of action steps. These structured and unambiguous objectives in cooking videos are essential for learning and evaluating long video narrative generation. 3.1. Overview We source over 30,000 raw videos about from two existing video datasets: YouCook2 [61] and HowTo100M [31]. Each video is filtered and cropped with processing to remove obvious logos or watermarks. Table 2 provides detailed information about the dataset statistics, video and clip details, and the train/validation partitioning."
        },
        {
            "title": "Type\nComic\nComic\nComic\nComic\nReal world\nReal world",
            "content": "# Images Text Length 122k 74k 160k 258k 210K 39M 86 74 106 146 70 763.8 Table 1. Comparison with multi-modal narrative datasets. Most existing datasets focus on image-based comic story generation. In contrast, our dataset consists of long narrative videos, containing 150 the number of frames and 5 the dense text annotations compared to the previous largest dataset, StoryStream. Table 1 compares our dataset with existing datasets most relevant to multimodal narrative generation. Unlike existing datasets that primarily focus on image-based comic story generation, our real-world narrative dataset offers several advantages. First, the videos in our dataset depict procedural activities (i.e., cooking), providing unambiguous narratives that are easier to annotate and evaluate. Second, our dataset contains 150 the number of frames compared to the previous largest dataset, StoryStream. Third, we offer 5 denser textual descriptions, with an average of 763.8 words per video. These advantages make our dataset better resource for narrative video generation. 3.2. Annotation and Processing To ensure scalability and quality, we design an efficient annotation pipeline to support the annotation as below. Captions. For open-source and scalability, we train video captioner based on open-sourced VLM. Inspired by LLaVA-Hound [57], we begin by collecting caption dataset using GPT-4o, with focus on object attributes, subject-object interactions, and temporal dynamics. Subse3 Data Source # Vid. (train/val) # Clips Clip Len. # Clips / Vid. Validation Set w/. GT keyframe W/o. GT keyframe YouCook2 HowTo100M (subset) 10K 1333 / 457 30039 / 933 183K 19.6s 9.5s 7.7 5.9 # Clips 5504 FVD 116. FVD 561.1 Table 2. Long narrative dataset sources. Our dataset is built upon Youcook2 and cooking subset of Howto100M. quently, we fine-tune captioning model based on LLaVANeXT [58] to optimize captioning performance. Actions. We use HowTo100M ASR-based pseudo labels for actions in each video, further refined by LLMs to provide enhanced annotations of the actions throughout the video [38]. This refinement improves the action quality to capture events and narrative context. However, the annotations are still noisy and sometimes not informative due to the inherent errors in ASR scripts. Caption-Action Matching and Filtering. To ensure alignment between captions and actions, we implement matching process based on time intervals. Using Intersectionover-Union (IoU) as metric, we evaluate whether the overlap between the captioned clip time and action time meets threshold. An action is considered match if the following conditions are met: the difference between the clip start time and the action start time (start diff) is less than 5 seconds; the clip end time is later than the action end time; and the IoU between the clip and action time intervals is greater than 0.25, or if IoU>0.5. Here, clip time and action time represent the time intervals for the clip and action, respectively. Using this rule, we filter and match captions to actions, ensuring that each caption aligns with the relevant action. We found this step is important for creating narrative consistency throughout the video. 3.3. Evaluation: Generation and Understanding High-quality captions are essential for narrative visual generation. To assess the quality of our annotations, we evaluate them from two perspectives: inverse generation (3.3.1) and visual understanding through VLM experts (3.3.2). Table 3. Inverse video generation. Evaluation of caption quality through inverse video generation with and without keyframes. FVD scores reflect reasonable video reconstruction quality. GPT-4o Evaluation Human Evaluation Score (0-100) Qwen2-VL-72B Ours Qwen2-VL-72B Ours 82. 98.0 95.2 79.3 Table 4. Caption Quality Evaluation. We compare the caption quality between our captioner and the Qwen2-VL-72B model by both GPT-4o and human annotators. Our model achieves competitive results despite much smaller model size. aligned with the original videos, as shown by the low FVD score (116.3). Without keyframes, the captions alone still provide reasonable alignment. Examples of reconstructed videos are included in the supplementary materials."
        },
        {
            "title": "3.3.2 Semantic Consistency across VLM Experts",
            "content": "GPT-4o & human evaluation. We evaluate the quality of our captions using both GPT-4o and six human annotators, in which we ask humans and GPT-4o to rate our dataset provided captions according to two criteria: the coverage of video elements and the absence of hallucinations in the caption. Following [57], hallucination refers to the model generating content absent or unsupported by the video, such as incorrect details about objects, actions, or counts. To demonstrate the quality, we compare our captions with those generated by state-of-the-art open-source VLM (Qwen2-VL-72B). As shown in Table 4, our datasets captions receive decent score of 95.2 out of 100, showing slightly better alignment with rigorous human evaluation than the Qwen2-VL-72B model. Results from both human evaluators and GPT-4 assessments indicate that the dataset contains high-quality captions."
        },
        {
            "title": "3.3.1 Inverse Video Generation",
            "content": "4. Method This evaluation is motivated by the understanding that high quality captions, when combined with ground truth keyframes, more effectively reconstruct the original videos. We evaluate the datasets ability to reconstruct original videos using the annotated captions, with and without conditioning with ground truth keyframes. For this evaluation, we assess the validation set (5,000 video clips) We measure reconstruction quality using FVD [47]. The results, shown in Table 3, indicate that our captions capture sufficient semantic information, enabling effective representation of the original videos. When generating with groundtruth keyframes, the video quality is very high and closely Given the text input, the task of long narrative video generation aims at generating coherent long video RHW that aligns with the progression of the text input sequentially. The H, , and are generated videos height, width, and frame numbers. To achieve this, we propose VideoAuteur, pipeline that involves two main components: long narrative video director and visualconditioned video generation. The long narrative video director is used to generate sequence of visual embeddings or keyframes that capture the narrative flow (4.1), while the visual-conditioned video generation generate video clips based on these visual conditions (4.2). 4 Figure 3. Long Narrative Video Director. The video director, Visual Language Model (VLM), takes user query (e.g., How to cook tuna sandwich?) and an initial image-text pair as input. It then generates captions, actions, and visual states step-by-step. Each video clip is created using either visual embeddings or keyframe derived from these embeddings. This study focuses on key design choices, such as the visual latent space, regression loss for visual embeddings, and the regression task. 4.1. Long Narrative Video Director As shown in Figure 3, the long narrative video director generates sequence of visual embeddings (or keyframes) that capture the narrative flow. In the following subsections, we first conduct analysis of two kinds of video directors based on interleaved generation (4.1.1) and language-centric visual generation (4.1.2)."
        },
        {
            "title": "4.1.1 Interleaved Image-Text Director",
            "content": "The interleaved image-text director creates sequence where text tokens and visual embeddings are interleaved, integrating narrative and visual content tightly. Using an auto-regressive model, it predicts the next token based on the accumulated context of both text and images, maintaining narrative coherence and aligning visuals with the text. Interleaved auto-regressive model. Our model performs next-token prediction for cross-modal generation, learning from sequences of interleaved image-text pairs with context window size . Each text token is supervised with cross-entropy loss, and the final visual embedding zT is regressed using learnable query tokens, as illustrated in Figure 3. The auto-regressive conditioning is given by: p(yt y1:t1) = p(ct c1:t1) p(zt c1:t, z1:t1), (1) where ct represents texts and zt denotes visual embeddings. Regression latent space. We utilize CLIP-Diffusion visual autoencoder with CLIP encoder Eclip and diffusion decoder Ddiff to encode raw images to visual embeddings for auto-regressive generation: = Eclip(x), ˆx = Ddiff(z) (2) This setup generates language-aligned visual embeddings and reconstructs images from them. Regression loss. To align the generated visual latents zpred with the target latents ztarget, we use combined loss: (cid:18) Lreg = α 1 zpred ztarget zpredztarget (cid:19) + β"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (ˆzi zi)2 i=1 (3) where α and β balance the contributions of cosine similarity and mean squared error to regress both scale and direction. Narrative from actions to visual states. The interleaved model generates coherent narrative sequence by progressively conditioning each step on the cumulative context from previous steps, Figure 3. At each time step t, the model generates an action at, caption ct, and visual state zt, conditioned on the cumulative history Ht1: Ht1 = {a1:t1, c1:t1, z1:t1} at Ht1 ct {Ht1, at} zt {Ht1, at, ct} (4) This layered conditioning ensures coherence across the sequence, aligning actions, language, and visuals."
        },
        {
            "title": "4.1.2 Language-Centric Keyframe Director",
            "content": "The interleaved auto-regressive model can also act as language-centric director when reduced to using only textbased guidance. In this case, keyframes are synthesized using text-conditioned diffusion model with only captions. For each caption ct, the diffusion model Dtext produces the visual state xt = Dtext(ct). This text-only approach benefits from straightforward integration with off-the-shelf text-conditioned diffusion models (e.g., FLUX-1 [24]) and hence enjoys high-fidelity image generation. However, this approach structures the narrative and maintains visual consistency without regressed embeddings. Therefore, it lacks 5 Figure 4. Visual-Conditioned video generation. Our interleaved auto-regressive director generates both text and visual conditions, enabling the video generation process to be conditioned either on keyframes (VAE embeddings) or on CLIP latents regressed by the interleaved director. We apply Gaussain noise, random masking and random shuffling as regularization during the training process to improve robustness with the imperfect visual embeddings. nuanced transitions between keyframes compared to the interleaved model, which enhances coherence by directly incorporating visual embeddings into the generation process. 4.2. Visual-Conditioned Video Generation Using the sequence of actions at, captions ct, and visual states zt generated by the narrative director, we condition video generation model to produce coherent long narrative videos. Unlike the classic Image-to-Video (I2V) pipeline that uses an image as the starting frame, our approach leverages the regressed visual latents zt as continuous conditions throughout the sequence (see 4.2.1). Furthermore, we improve the robustness and quality of the generated videos by adapting the model to handle noisy visual embeddings, since the regressed visual latents may not be perfect due to regression errors (see 4.2.2)."
        },
        {
            "title": "4.2.1 Visual Conditions Beyond Keyframes",
            "content": "Traditional visual-conditioned video generation typically uses initial keyframes to guide the model, where each frame xt is generated as xt = Dvisual(It). Our interleaved auto-regressive director extends this by generating visual states zt in semantically aligned latent space, allowing direct conditioning without biases from pretrained visual decoder, as shown in Figure 4. By using these regressed visual latents zt directly, each frame is generated as xt = Dvisual(zt), ensuring that the video accurately follows the narrative and enhancing consistency by relying on narrative-aligned embeddings rather than potentially biased keyframes (illustrated in Figure 5)."
        },
        {
            "title": "4.2.2 Learning from Noisy Visual Conditions",
            "content": "To enhance the video generation models robustness to imperfect visual embeddings zt from the auto-regressive director, we fine-tune the model using noisy embeddings Figure 5. Auto-encoded results with different latent spaces. While SEED-X and EMU-2 both use CLIP vision encoder and diffusion model (i.e. finetuned SDXL) as decoder for autoencoding visual latents, SEED-X is semantic-biased and EMU-2 keeps much more visual details. SDXL-VAE shows the best image reconstruction ability, however, the latent space is not aligned with language (i.e. without pretraining on image-text pairs like CLIP). defined by: = S(M(zt + ϵ)) (5) where ϵ (0, σ2zt) represents Gaussian noise, is masking operator that sets fraction of elements to zero, and is shuffling operator that permutes some embedding dimension. Training with improves the models ability to handle noisy visual conditions, improving generation quality and robustness with imperfect embeddings. 5. Experiments 5.1. Experimental Setup Models. Following Seed-Story [54], we use the SEEDX [13] pretrained 7B multi-modal LLM as our base model and apply LoRA finetuning on our narrative dataset. For video generation, we employ Sora-like model [2], which has been pretrained on large-scale video-text pairs and could accept both text and visual conditions. Data. We use total of 32K narrative videos for model developing and also use 1K videos for validation. All the videos are resized to 448 (short-side) and then centercropped with 448x448 resolution. Training & Evaluation. Experiments of interleaved autoregressive director model are trained with 5,000 steps by default. Training loss is combination of cosine similarity loss and MSE loss for visual tokens and CrossEntropy loss for language tokens. For visual-conditioned video generation, we use the diffusion loss following DiT [32] and Stable Diffusion 3 [10]. Narrative generation is mostly evaluated on the Youcook2 validation set because of the high-quality of action annotations and the Howto100M validation set is mostly used for data quality evaluation and I2V generation. 6 Method Autoencoder Style VL Aligned. Recon. Ability CLIP-T FID SDXL-VAE Variational U-Net CLIP-Diffusion CLIP-Diffusion High Medium Low EMU-2 SEED-X 286.6 76.7 30.1 13.2 25.4 25.1 Table 5. Visual latent spaces for visual regression. The VAE latent space is challenging for auto-regressive models to regress in single step due to its limited correlation with language. In contrast, the language-aligned latent spaces (EMU-2 and SEED-X) allow for easier and effective regression in an interleaved manner. 5.2. Interleaved Narrative Director In this section, we discuss three key aspects we explored to improve interleaved auto-regressive model for interleaved narrative visual generation: 1) different visual latent space for visual regression, 2) loss design for accurate latent regression, and 3) the cross-modality regression task. CLIP beats VAE for interleaved generation. We experiment with three different auto-encoded visual latent spaces for regression: the EMU-2 [42] CLIP-Diffusion autoencoder, the SEED-X CLIP-Diffusion autoencoder, and the KL Variational autoencoder (VAE) used by SDXL. Both SEED-X and EMU-2 use CLIP vision encoder and finetuned SDXL diffusion model as the decoder for encoding visual latent. From Figure 5, we can observe that SDXLVAE achieves the best reconstruction quality. However, in terms of visual generation quality, as shown in Table 5, the CLIP-Diffusion based autoencoders significantly outperform VAE (i.e., +12.2 CLIP-T score and 256.6 better FID). This suggests that CLIP embeddings are more suitable for interleaved visual generation compared to VAEs latent space. This is reasonable, as SDXL-VAE is not aligned with language and lacks semantic understanding. Latent scale and direction matters. To determine an effective supervision strategy for visual embeddings, we firstly test the robustness of the latents to pseudo regression errors by rescaling (multiplying by factor) and adding random Gaussian noise. Figure 6 indicates that both scale and direction are critical in latent regression. Notably, rescaling primarily affects object shape while preserving key semantic information (i.e. object type and location), whereas adding noise drastically impacts reconstruction quality. As shown in Table 6, combining MSE loss (minimizing scale error) and cosine similarity (minimizing direction error) leads to the best generation quality (i.e., +1.9 CLIP score and 3.4 better FID), which further verifies our findings. From Actions to Visual States. We also explore how different regression tasks influence the directors capability in narrative visual generation. Specifically, we compare various reasoning settings for the interleaved director, examining transitions from sequential actions to language states, and ultimately to visual embeddings. As shown in Table 7, chain of reasoning that progresses from actions to language states and then to visual states proves effective for Figure 6. Both Scale and Direction Matters. We experiment with pseudo regression errors by altering latent direction and scale using Gaussian noise and scaling factors. The reconstruction results confirm that preserving both scale and direction is essential for accurate latent regression. SEED-X Latent EMU-2 Latent Training Validation Validation Loss Type MSE Cos. L2 Dist. Cosine. CLIP FID L2 Dist. Cosine. CLIP FID 25.1 80.1 23.6 31.9 23.5 115.3 24.1 32.1 25.4 76.7 25.1 30. 0.78 0.79 0.79 0.82 0.82 0.83 0.41 1.1 0.41 1.3 2.5 1.4 Training Table 6. Regression loss with scale and direction. We track the training convergence and evaluate models with the CLIP-T and FID metrics on the validation set. Both Seed-X and EMU-2 latent space show that combination of both MSE loss and Cosine Similarity loss considering both scale and direction performs best. SEED-X and EMU-2 original regression loss setting is grayed . Regression Task Action Vis. Embed. Caption Vis. Embed. Action Caption Vis. Embed. Training Validation L2 Dist. Cosine Sim. CLIP-T FID 0.43 0.41 0. 0.82 0.82 0.83 22.7 25.7 26.1 27.9 26.1 25.3 Table 7. From Actions to Visual States. We report the L2 distance and cosine similarity scores for tracking the training convergence and evaluate the generation images with CLIP score and FID score. Models are trained and evaluated on the collected Howto100M subset. SEED-X latent is used for visual regression. long narrative visual generation. This approach enhances both training convergence, achieving lower L2 distance (0.41 vs. 0.43), and generation quality, reflected in superior FID score of 25.3 (an improvement of +0.8). Interleaved or language-centric? We compare the interleaved auto-regressive model, with language-centric approach that generates images based solely on language states (captions) using models like SD-XL and FLUX.1schnell. As illustrated in Figure 7, the language-centric method using text-to-image models generates visually appealing keyframes but lacks consistency across frames due to limited mutual information. In contrast, the interleaved generation approach, leveraging language-aligned visual latents, learns realistic visual style through training. However, it occasionally produces images with repetitive or noisy visual elements, as the auto-regressive model struggles to generate accurate embeddings in single forward. 7 Figure 7. Quality Comparison Example. Given global system promptStep-by-step guide to cooking mapo tofu:along with the action, caption, and image embeddings of the first step keyframe, our interleaved director sequentially generates actions, captions, and image embeddings to construct narrative on how to cook the dish step by step. The first two rows display the directly generated keyframes (decoded from the image latents) using the EMU-2 and SEED-X latent spaces. The generated images are realistic with strong visual consistency but are less aesthetically refined than those produced by state-of-the-art text-to-image models, i.e. SDXL and FLUX.1-s. Latent condition Gen. strategy Aesthetic Realistic Visual consist. Narrative EMU-2 Latent SEED-X Latent Interleaved Interleaved Text (SDXL) Language-centric Text (FLUX.1-s) Language-centric 0.7 2.1 4.0 4.8 1.2 4.3 2.9 3.1 2.9 4. 3.3 3.4 2.2 4.4 4.0 4.4 Table 8. Human Evaluation Interleaved vs. Languagecentric. This table compares different latent conditioning and generation strategies based on semantic alignment, aesthetic quality, visual consistency, and narrative coherence. Each aspect is scored with five tiers: 15, score higher is better. The human evaluation  (Table 8)  further validates this point. From Table 8, we can observe that interleaved methods achieve the highest scores in realism (4.3 vs. 3.1) and visual consistency (4.5 vs. 3.4), while language-centric methods achieve the best aesthetic score (4.8 vs. 2.1). We argue that visual consistency is more crucial in long narrative video generation, making interleaved methods preferable. 5.3. Visual-Conditioned Video Generation As detailed in Section 4.2, we fine-tune the model to be directly conditioned on the visual latents generated by our interleaved director, rather than relying solely on initial keyframes. Table 9 compares the keyframe-conditioned Visual Condition Keyframe Embedding YouCook2 HowTo100M CLIP-T 25.9 26. FVD 557.7 512.6 CLIP-T 26.6 27.3 FVD 541.1 520.7 Table 9. Keyframes vs. Visual Embeddings. Evaluate CLIP-T and FVD scores for video generation conditioned on keyframes versus visual embeddings generated by our interleaved director. approach with our visual embedding-conditioned strategy. Our method improves CLIP-T scores on both validation setsfrom 25.9 to 26.4 on YouCook2 and from 26.6 to 27.3 on HowTo100M. Additionally, FVD scores decrease, indicating better video quality ( 557.7 vs. 512.6 on YouCook2, 541.1 vs. 520.7 on HowTo100M). Videos conditioned on visual embeddings demonstrate higher semantic alignment and improved generation quality. Video demos are provided on the demo page and in the supplementary materials. 6. Conclusion In this paper, we tackle the challenges of generating longform narrative videos and empirically evaluate its efficacy in the cooking domain. We curate and annotate large-scale cooking video dataset, capturing clear and high-quality narratives essential for training and evaluation. Our proposed 8 two-stage auto-regressive pipeline, which includes long narrative director and visual-conditioned video generation, demonstrates promising improvements in semantic consistency and visual fidelity in generated long narrative videos. Through experiments on our dataset, we observe enhancements in spatial and temporal coherence across video sequences. We hope our work can facilitate further research in long narrative video generation."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: In NeurlPS, visual language model for few-shot learning. 2022. 2 [2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators, 2024. 1, 2, 6 [3] Jerome Bruner. The narrative construction of reality. Critical inquiry, 18(1):121, 1991. 2 [4] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In CVPR, 2023. 1 [5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. In arXiv, 2023. 1, [6] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, 2024. 1, 2 [7] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In CVPR, 2024. 2 [8] Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real image animation with text-guided motion control. In ECCV, 2025. 1 [9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. In arXiv, 2023. 2 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the 41st International Conference on Machine Learning, pages 1260612633. PMLR, 2024. 6 [11] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo. Ccedit: Creative and controllable video editing via diffusion models. In CVPR, 2024. [12] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurlPS, 2024. 2 [13] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. In arXiv, 2024. 2, 6 [14] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine this! scripts to compositions to videos. In Proceedings of the European conference on computer vision (ECCV), pages 598613, 2018. 2 [15] Yuval Noah Harari. Sapiens: brief history of humankind. Random House, 2014. 2 [16] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. 2, 3 [17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Imagen Poole, Mohammad Norouzi, David Fleet, et al. video: High definition video generation with diffusion models. In arXiv, 2022. 1, [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In NeurlPS, 2022. 1, 2 [19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In arXiv, 2022. 1, 2 [20] Nisha Huang, Yuxin Zhang, and Weiming Dong. Style-avideo: Agile diffusion for arbitrary text-based video style transfer. IEEE Signal Processing Letters, 2024. 1 [21] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies, pages 12331239, 2016. 2 [22] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In ICML, 2024. 1, 2 [23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, 2023. [24] Black Forest Labs. Flux.1 [dev]: 12 billion parameter rectified flow transformer for text-to-image generation, 2024. Accessed: 2024-11-13. 5 [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 2 9 [26] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In CVPR, 2023. 2 [27] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and In The Boyi Li. LLM-grounded video diffusion models. Twelfth International Conference on Learning Representations, 2024. 3 [28] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. In COLM, 2024. 2, [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurlPS, 2024. 2 [30] Adyasha Maharana and Mohit Bansal. Integrating visuospatial, linguistic and commonsense structure into story visualization. arXiv preprint arXiv:2110.10834, 2021. 2 [31] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, and Josef Sivic. Ivan Laptev, Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 26302640, 2019. 2, 3 [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 1, 2, [33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [34] Lu Qi, Lehan Yang, Weidong Guo, Yu Xu, Bo Du, Varun Jampani, and Ming-Hsuan Yang. Unigs: Unified representation for image generation and segmentation. In CVPR, 2024. 2 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurlPS, 2022. 2 [38] Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, and Hilde Kuehne. Howtocaption: Prompting llms to transform video annotations at scale. In European Conference on Computer Vision, pages 118. Springer, 2025. 4 [39] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In arXiv, 2022. 1, 2 [40] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. In arXiv, 2023. [41] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In ICLR, 2023. 1, 2 [42] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024. 1, 2, 7 [43] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer. In arXiv, 2024. 2 [44] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, et al. Videotetris: Towards compositional text-to-video generation. arXiv preprint arXiv:2406.04277, 2024. 3 [45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 17 [47] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv:1812.01717, 2018. [48] Chaoyang Wang, Xiangtai Li, Lu Qi, Henghui Ding, Yunhai Tong, and Ming-Hsuan Yang. Semflow: Binding semantic segmentation and image synthesis via rectified flow. In NeurlPS, 2024. 2 [49] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. In arXiv, 2023. 1, 2 [50] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an openended decoder for vision-centric tasks. In Advances in Neural Information Processing Systems, 2024. 2 [51] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. In arXiv, 2024. 1, 2 [52] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In CVPR, 2024. 1 [53] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. In arXiv, 2024. [54] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal 10 long story generation with large language model. preprint arXiv:2407.08683, 2024. 2, 3, 6 arXiv [55] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In arXiv, 2024. 1, 2 [56] Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, and Hanwang Zhang. Diffusion time-step curriculum for one image to 3d generation. In CVPR, 2024. [57] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 3, 4, 19 [58] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 4 [59] Canyu Zhao, Mingyu Liu, Wen Wang, Jianlong Yuan, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. 2, 3 [60] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. In arXiv, 2022. 1, 2 [61] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 2, 3 [62] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Storydiffusion: Consistent selfFeng, and Qibin Hou. attention for long-range image and video generation. arXiv preprint arXiv:2405.01434, 2024. 2, 3 [63] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream vlog. In CVPR, 2024. 2,"
        },
        {
            "title": "Supplementary Material",
            "content": "This appendix provides comprehensive supplementary materials to support our study. Below are brief descriptions of all the sections covered in the appendix. Please visit our project page for more visualization. Appendix A: Data Examples with Annotations Presents data examples from our CookGen dataset. Showcases annotated actions and captions that provide detailed multimodal information of cooking processes. Appendix B: Additional Data Statistics Offers distributions of video lengths, clip lengths, and textual annotations. Demonstrates the datasets richness and suitability for long narrative video generation. Appendix C: Data Evaluation Details Details our data evaluation process. Includes inverse video generation results, the prompts used for video captioning, GPT-4o evaluations, and human evaluation results. Appendix D: Implementation Details Outlines the implementation details of our models. Provides key hyperparameters and training & inference configurations. Appendix E: Action-Caption Matching Pseudo Code Includes the pseudo code for our action-caption matching algorithm. Essential for aligning video clips with their corresponding annotations. Appendix F: Generated Video Examples Showcases generated video examples. Illustrates the effectiveness of our pipeline in producing long narrative videos for cooking recipes like Fried Chicken and Shish Kabob. Appendix G: Limitations Discusses the limitations of our approach. Includes issues with noisy actions from automatic speech recognition and potential failure cases in video generation. 12 A. Data Examples with Annotations Figures 8 and 9 shows two data examples from our CookGen dataset, annotated with high-quality descriptions that provide detailed multi-modal information of cooking processes. The examples clearly show structured annotations of key actions and corresponding visual descriptions, making the dataset ideal for generating long narrative videos. (a) Action: Elise works with chicken thighs, advises to trim excess skin and fat Caption: person is preparing chicken on wooden cutting board. He uses pair of black-handled scissors to cut through the chicken pieces, which are spread out on clear cutting mat. (b) Action: She offers alternatives with chicken breast bone-in skin-on or chicken drumsticks Caption: person with light skin is preparing raw chicken pieces on wooden surface. He places several pieces of chicken on white cutting board. (c) Action: Elise heats up large skillet with two teaspoons of olive oil and teaspoon of butter Caption: person is seen in kitchen setting, holding wooden spoon. He places small piece of butter into black frying pan on gas stove. (d) Action: Turn over the chicken pieces and cook for another 4 minutes Remove the chicken from the pan but keep the browned pieces in the pan Caption: Golden-brown chicken pieces are sizzling in black frying pan on gas stove. 13 (e) Action: Use the remaining oil in the pan to brown the orzo Cook the orzo like traditional rice pilaf, using the same method as before Caption: person is cooking rice in black frying pan on gas stove. He pours the rice from glass bowl into the pan, then uses wooden spatula to spread and stir the rice. (f) Action: Add 2 cups of gordos to hot pan Caption: person wearing blue shirt is cooking rice in black frying pan on stovetop. Using wooden spatula, he stirs the rice, ensuring it is evenly cooked. (g) Action: Combine the mixture with the orzo and cook for few minutes until the sauce thickens Caption: woman is cooking on stovetop, adding pieces of breaded chicken to pan filled with chopped onions and rice. (h) Action: Stock is cooked until orzo has fully absorbed liquid and chicken is cooked through, about 10-12 minutes Dish is removed from heat and left to sit for five minutes Dish is sprinkled with unspecified seasoning Caption: delicious dish of roasted chicken pieces is presented in black skillet, surrounded by colorful mix of diced vegetables and grains. Figure 8. Data examples with annotated actions and captions. video of cooking recipe of One Pot Chicken and Orzo. 14 (a) Action: Hi everyone, this ones called rainbow broken glass jello Caption: colorful, multi-layered dessert is displayed on black surface. The dessert features vibrant red, green, blue, and purple segments, arranged in geometric pattern. (b) Action: Now normally when you make jello you use two cups of boiling water, but in this case were only using one cup because we want the jello to be extra firm Caption: The video shows the interior of refrigerator, focusing on the door shelf. The containers are filled with dark, blue, orange, and red liquids. (c) Action: find the easiest way to do this is to put the small container into larger container of hot water Caption: person with light skin is holding clear plastic container filled with yellow liquid, inspecting its contents. (d) Action: Loosen the edges of the Jello piece Slide the Jello piece out and cut it into cubes Cut the Jello cubes into half-inch pieces Caption: person is slicing block of yellow gelatin on wooden cutting board, cutting it into uniform strips. 15 (e) Action: Spread out the different colored Jello pieces in 9 by 13 inch baking dish Caption: person is arranging colorful gelatin cubes in glass baking dish, adjusting the placement of green, orange, purple, and black cubes. (f) Action: Make separate gelatin mixture by boiling two cups of water and adding two envelopes of gelatin Caption: clear glass measuring cup is placed on countertop, containing water. person pours white powder into it. (g) Action: Stir the sweetened condensed milk into the gelatin and water mixture Caption: person is vigorously whisking creamy mixture in clear glass measuring cup. (h) Action: Let it set for several hours, then cut it into squares and serve Caption: glass baking dish is filled with creamy white liquid, topped with colorful, triangular-shaped glass pieces. Figure 9. Data examples with annotated actions and captions. video of preparing Rainbow Broken Glass Jello. 16 B. Additional Data Statistics Figure 10. Statistics on the video data. We do statistics on the video lengths of the collected whole videos, the clip lengths of the scene-cut video clips, and the number of clips selected for each video. Figure 11. Statistics on the text annotations. We do statistics on the number of words and tokens (Llama [46] tokenized) of annotated actions and captions, respectively. The statistics in Figure 10 and Figure 11 demonstrate the high quality and suitability of our dataset for long narrative video generation. Figure 10 reveals that the video lengths range broadly, with most videos falling between 30 and 150 seconds. Clip lengths are primarily distributed between 5 and 30 seconds, ensuring manageable segments for modeling. Additionally, the majority of videos contain 4 to 12 clips, providing balanced structure for narrative flow. Figure 11 shows that the word counts for actions predominantly range from 10 to 25, while captions range from 40 to 70. Token distributions further highlight their richness, with actions having 20 to 60 tokens and captions extending up to 120 tokens. These detailed annotations ensure well-aligned and contextually rich representations of the video content. Overall, the datasets design ensures coherent sequences of actions and captions with reasonable clip and video lengths, making it well-suited for generating high-quality, long-form narrative videos. 17 C. Data Evaluation Details C.1. Inverse Video Generation Results As discussed in Section 3.3.1, high-quality captions, especially with ground truth keyframes, enable effective video reconstruction. We compare ground truth video frames with inversely generated frames using the GT first keyframe and annotated captions, as shown in Figures 12 to 14. The reconstruction aligns well with the narrative, accurately capturing actions, though patterns and interactions differ slightly from the original video. This shows that while the captions convey crucial information for reconstruction, they lack finer visual details, limitation for current vision-language models and human annotators. For example, in Figure 12, the ground truth shows hand pouring creamy liquid into slow cooker and stirring, while the generated frames replicate the actions with slight differences in texture and liquid mixing. Similarly, in Figure 14, the ground truth shows face drawn with cream on orange liquid, but the generated frames vary in precision and interaction details. These examples highlight the captions strength in preserving narrative flow while exposing gaps in capturing fine-grained visual detail. Figure 12. Left: Ground truth, Right: Inverse generation with GT keyframe. Caption: Chunks of meat are simmering in dark-colored slow cooker. hand pours creamy liquid into the pot, causing the liquid to mix with the meat and broth. The mixture bubbles and thickens as the liquid is added. The person stirs the contents with black spoon, ensuring the ingredients are well combined. The slow cooker continues to cook the meat, which appears tender and well-cooked. Figure 13. Left: Ground truth, Right: Inverse generation with GT keyframe. Caption: person wearing black sleeve is whisking creamy mixture in clear glass bowl. The mixture appears to be batter or dough, gradually becoming smoother and more uniform. The persons left hand holds the bowl steady on light-colored countertop. The whisking motion is consistent and thorough, ensuring the mixture is well-blended. The background is plain, focusing attention on the mixing process. Figure 14. Left: Ground truth, Right: Inverse generation with GT keyframe. Caption: red bowl filled with thick, orange liquid is placed on stovetop. womans hand, holding white spoon, appears and begins to draw on the surface of the liquid. She creates face with white cream, adding details to the eyes and mouth. The background shows granite countertop with bunch of red tomatoes and white pot. The woman continues to add finishing touches to the face. 18 C.2. Prompt for Video Captioning Below is the prompt we designed to effectively caption video clips and also for benchmarking VLMs, ensuring detailed and accurate descriptions while avoiding redundancy: You are an expert in describing videos and catching the sequential motions from video frames. For the given ten video frames, you need to generate detailed good description within five sentences / 80 words. Please do not include the word frame or frames in your answer. If the gender of person is clear, use he or she instead of they. Do not describe single motion/action twice like xxx continues doing yyy. Dont assume actions like discussion or having conversation unless it is very clear in the frames. Describe the video given the frame sequence. Describe both the appearance of people (gender, clothes, etc), objects, background in the video, and the actions they take. C.3. GPT-4o Evaluation on Captions Listing 1. Video Captioning Prompt Below is the evaluation prompt designed to objectively assess the quality of video captions generated by Large Multimodal Model (LMM), focusing on coverage and hallucination. Your role is to serve as an impartial and objective evaluator of video caption provided by Large Multimodal Model (LMM). Based on the input frames of video, assess primarily on two criteria: the coverage of video elements in the caption and the absence of hallucinations in the response. In this context, hallucination refers to the model generating content not present or implied in the video, mainly focused on incorrect details about objects, actions, counts, temporal order, or other aspects not evidenced in the video frames. To evaluate the LMMs response: Start with brief explanation of your evaluation process. Then, assign rating from the following scale: Rating 6: Very informative with good coverage, no hallucination Rating 5: Very informative, no hallucination Rating 4: Somewhat informative with some missing details, no hallucination Rating 3: Not informative, no hallucination Rating 2: Very informative, with hallucination Rating 1: Somewhat informative, with hallucination Rating 0: Not informative, with hallucination Do not provide any other output symbols, text, or explanation for the score. C.4. Human Evaluation on Captions Listing 2. GPT-4o Evaluation Prompt Matching Tier Action (Important Info.) Object (Important Info.) Score Very Match Good Match Somehow Match Not Match Good Coverage, No Hallucination Good Coverage, No Hallucination Good Coverage, Limited Hallucination Good Coverage, Limited Hallucination Fair Coverage, Some Hallucination Little Coverage or High Hallucination Fair Coverage, Some Hallucination Little Coverage or High Hallucination 100 85 70 0 Table 10. Human Evaluation Matching Rules. Captions are rated based on coverage and hallucination levels, using four matching tiers. We assess the quality of our captions through evaluations by six human annotators, who rate the captions based on two key criteria: the coverage of video elements (such as objects and actions) and the absence of hallucinations, defined as generating content unsupported or absent in the video [57]. As shown in Table 4, our captions achieve high human evaluation score of 82.0, surpassing the state-of-the-art open-source VLM (Qwen2-VL-72B) score of 79.3. These results demonstrate the superior quality of our captions, which are more aligned with human preferences and exhibit better narrative accuracy. For evaluation, annotators rate the captions across four tiersVery Match, Good Match, Somehow Match, and Not Matchbased on consistency with video content. The scoring rubric, detailed in Table 10, considers both coverage and hallucination levels. Our captioner consistently achieves high scores in the top tiers, validating its reliability and quality for narrative video generation. D. Implementation Details E. Action-Caption Matching Pseudo Code We provide the training and inference hyperparameters for the interleaved auto-regressive model and the visualconditioned video generation model in Table 11 and Table 12, respectively. The interleaved auto-regressive model is trained on images with resolution of 448 448, using batch size of 512 and bfloat16 precision. It employs AdamW as the optimizer, with peak learning rate of 2 104 and cosine decay schedule, training for 2,500 steps. Training context pairs vary between 2 and 8, while inference always uses 8 pairs for consistency. The visualconditioned video generation model processes video data at resolution of 448 448 , with batch size of 64 and bfloat16 precision. It uses AdamW with peak learning rate of 1 105 and constant decay schedule, training for 20,000 steps to handle temporal conditioning effectively. Configuration Image resolution Optimizer Optimizer hyperparameters Peak learning rate Learning rate schedule Gradient clip Total training steps Warm-up steps Batch size Numerical precision Training context pairs Inference context pairs Setting 448 448 AdamW β1 = 0.9, β2 = 0.98, ϵ = 106 2 104 Linear warm-up, cosine decay 1.0 2, 500 200 512 bfloat16 [2, 8] 8 Table 11. regressive model. Implementation details of the interleaved autoConfiguration Image/Video resolution Optimizer Optimizer hyperparameters Peak learning rate Learning rate schedule Gradient clip Total training steps Warm-up steps Batch size Numerical precision Setting 448 448 AdamW β1 = 0.9, β2 = 0.95, ϵ = 108 1 105 Linear warm-up, constant 1.0 20, 000 1, 000 64 bfloat16 Table 12. video generation model. Implementation details of the visual-conditioned The action-caption matching algorithm detailed in Algorithm 1 aligns video clips with actions based on temporal overlap and specific rules. It uses the Intersection over Union (IoU) to measure the overlap between the time intervals of video clips and actions. match is identified if either the IoU exceeds 0.5 or all of the following conditions the start time difference (start diff) is less are met: than 5 seconds, the clips end time exceeds the actions end time, and the IoU is greater than 0.2. The algorithm processes each video iteratively. For each video, it retrieves all associated actions and their time intervals. Then, for each clip in the video, it calculates the IoU with every action and evaluates the matching conditions. Valid matches, along with their metadata such as clip IDs and descriptions, are stored in list M. This systematic approach ensures that the matched actions and captions are temporally and semantically consistent, providing highquality annotations for keyframe visual states. Algorithm 1 Pseudo code for action-caption matching. 1: function IOU([s1, e1], [s2, e2]) 2: 3: 4: intersection max(0, min(e1, e2) max(s1, s2)) union max(e1, e2) min(s1, s2) if union > 0 then union else end if return intersection return 0 5: 6: 7: 8: 9: end function 10: Initialize an empty list [] 11: for all do 12: 13: 14: 15: 16: vid v.id if vid then Av A[vid] action times Av.times action descriptions Av.descriptions for all v.clips do 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: [sc, ec] c.start end for all action times do [sa, ea] start diff sc sa iou IOU([sc, ec], [sa, ea]) if (start diff < 5 ec > ea iou > 0.2) iou > 0.5 then Create match: end if end for end for end if 28: 29: end for 30: return 20 F. Generated Video Examples Figures 15 and 16 present two examples of long narrative video generation for cooking Fried Chicken and Shish Kabob, illustrated step-by-step. The generation process begins with our interleaved auto-regressive director, which generates keyframe visual embeddings and their corresponding captions. These embeddings and captions are then used as conditions for the video generation model, which produces high-quality video clips that effectively narrate the cooking process and emphasize the crucial action information. The resulting video clips demonstrate excellent performance in capturing the step-by-step cooking instructions. All video clips are also included in the supplementary materials for further review. (a) Action: Add raw chicken pieces and seasoning to bowl of flour. (b) Action: Mix yogurt or buttermilk with seasoning in bowl. (c) Action: Dip chicken pieces into the batter to coat evenly. (d) Action: Coat the battered chicken in the flour mixture. (e) Action: Fry the coated chicken in hot oil until crispy and golden. (f) Action: Sprinkle seasoning on the fried chicken and serve. Figure 15. Video generation example. Our pipeline effectively accomplishes long narrative video generation by producing six essential steps (i.e., video clips) for cooking Fried Chicken. It delivers clear, structured, and instructional step-by-step narrative, showcasing the models capability to generate coherent and comprehensive videos. 21 (a) Action: Mix chopped vegetables in glass bowl. (b) Action: Add seasoning to the mixture of chopped vegetables. (c) Action: Thoroughly mix the seasoned vegetable mixture. (d) Action: Add chicken pieces to vegetable and chicken mixture. (e) Action: Brush oil onto the skewered chicken and vegetable kebabs. (f) Action: Place the prepared chicken and vegetable kebabs onto grill. (g) Action: Drizzle olive oil over the chicken and vegetable kebabs. (h) Action: Check on the grilling skewered chicken and vegetable kebabs. Figure 16. Video generation example. Our pipeline successfully generates eight crucial steps (i.e., video clips) to prepare the dish Shish Kabob. This showcases clear, structured, and instructional step-by-step narrative, demonstrating the models capability to produce coherent and comprehensive video content. 22 G. Limitations G.1. Noisy Actions\" from ASR While our CookGen dataset provides high-quality visual and contextual annotations, the action annotations derived from automatic speech recognition (ASR) have notable limitations. ASR-generated text often contains noise, resulting in action descriptions that are incomplete, ambiguous, or not directly informative for capturing the crucial steps in cooking processes. For instance, in Figure 9(a), the action annotation Hi everyone, this ones called rainbow broken glass jello offers little value for understanding the cooking process, while another annotation in Figure 9(b) Now normally when you make jello you use two cups of boiling water provides vague guidance without specific details about the method. Such noisy annotations fail to align with the detailed and instructive nature of cooking instructions, which require precision and clarity to guide long narrative video generation effectively. This limitation underscores the importance of refining action annotations to improve their informativeness and utility for modeling cooking tasks. G.2. Failure Cases While our method generates high-quality long narrative videos, there are instances where the model fails to produce meaningful cooking steps, and the rendered video clips contain unrealistic or irrelevant content due to hallucination. Auto-regressive Director: Repeated Steps. Figure 17 illustrates failure case where the auto-regressive director repeatedly generates similar visual embeddings, resulting in redundant and uninformative cooking video clips. For example, in the provided frames, the generated steps involve repeatedly cutting the salmon fillet, which adds little value to the narrative and fails to progress meaningfully. This issue is known limitation of auto-regressive models, often caused by lack of diversity in the embedding generation process. potential solution is to introduce penalties for repeated embeddings or add constraints to encourage greater variability in visual outputs. (a) Action: Cutting away the salmon fillet from the backbone (b) Action: Slicing the salmon fillet into even pieces Figure 17. Failure Case. Auto-regressive model could generate repeated Steps, which is not informative to viewer. Video Generation Model: Unrealistic Hallucination.Unrealistic hallucination occurs when video generation model produces content inconsistent with the intended narrative. In Figure 18(a), the action placing the fried chicken into an oven set to preheat is misrepresented as frying chicken in pan, with an unrealistic increase in the quantity of chicken, showing lack of object continuity. In Figure 18(b), the action adding drizzle of sauce to plate of grilled skewers introduces an illogical appearance of new grilled food items, deviating from the intended action and disrupting narrative coherence. (a) Action: Placing the fried chicken into oven set to preheat (b) Action: Adding drizzle of sauce to plate of grilled skewers Figure 18. Failure Case. Video generation model could make unrealisc hallucination to generate things from air."
        }
    ],
    "affiliations": [
        "ByteDance",
        "ByteDance Seed",
        "Johns Hopkins University"
    ]
}