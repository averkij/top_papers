{
    "paper_title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "authors": [
        "Wenxuan Huang",
        "Bohan Jia",
        "Zijie Zhai",
        "Shaosheng Cao",
        "Zheyu Ye",
        "Fei Zhao",
        "Yao Hu",
        "Shaohui Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 9 4 7 6 0 . 3 0 5 2 : r Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models Wenxuan Huang1* Zheyu Ye2 Bohan Jia1* Fei Zhao2 Zijie Zhai1 Yao Hu2 Shaosheng Cao2(cid:66) Shaohui Lin1(cid:66) 1East China Normal University 2Xiaohongshu Inc. osilly0616@gmail.com, shaohuilin007@gmail.com Figure 1. Left panel: Our Vision-R1 Pipeline. We first use the existing MLLM and DeepSeek-R1 to obtain high-quantity Multimodal CoT dataset, which is used as the cold-start initialization data for the base MLLM to obtain the post-cold-start Vision-R1-CI, and then we perform the RL training on Vision-R1-CI to obtain the reasoning MLLM, Vision-R1. Right panel: We observe that directly applying RL to MLLMs fails to effectively incentivize strong reasoning capability (see (C) and (D)). Vision-R1-Zero, trained via RL without prior initialization, struggles to generalize from limited data (see (E), (F), notably, Vision-R1-Zero was applied in format reward function). Vision-R1-CI faces the Overthinking Optimization Problem, favoring shorter CoT reasoning, where correct reasoning processes mostly focus on the shorter CoT reasoning sequences (see (A)). During subsequent RL training, we observe lengthening of reasoning steps but decline in performance (see (D) and (E)), making optimization particularly challenging For Vision-R1, it initially shortens CoT to refine the right thought process under RL training. PTST enables Vision-R1 to progressively acquire more complex reasoning process (see (C), (D), and (E)) to improve the performance, such that our Vision-R1 with 7B parameters achieves comparable performance to the strongest MLLMs with 70B+ parameters (see (B)). Note that Vision-R1 used various colored lines to indicate the different stages in PTST."
        },
        {
            "title": "Abstract",
            "content": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, VisionR1, to improve multimodal reasoning capability. Specifically, we first construct high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the models ability to learn correct and complex reasoning processes on 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of 6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1. 1. Introduction Enhancing the complex reasoning capability of Large Language Models (LLMs) remains one of the most challenging problems in Artificial Intelligence (AI), which is widely regarded as critical pathway toward Artificial General Intelligence (AGI) [1, 2]. Conventional inference paradigms typically rely on simple direct prediction approach to generate concise final answers without explicit, structured intermediate reasoning steps, which often exhibits suboptimal performance on complex reasoning tasks [1]. OpenAI O1 [1] was the first LLM with strong reasoning ability by using complex Chain-of-Thought (CoT) for training to achieve significant performance gains over prior LLMs. Meanwhile, various methods [314] have been explored to generate highquality complex CoT reasoning and further advance the field by optimizing reasoning pathways. In the field of Multimodal Large Language Models (MLLMs), recent works [15, 16] have also explored the application of CoT reasoning. These approaches assume that MLLMs lack structured reasoning process, achieving low performance on the tasks that require logical inference. To improve the reasoning capability of MLLMs, several methods [15, 17] try to construct the datasets manually containing step-level reasoning processes and apply supervised fine-tuning (SFT) to reformat MLLMs outputs. However, this manually designed Formatted Reasoning MLLM often results in Pseudo-CoT reasoning, which lacks essential cognitive processes commonly observed in human thoughts, such as questioning, reflection and inspecting (see Fig. 2, the data examples of Pseudo-CoT and the complex CoT). This limitation hinders their application on complex vision reasoning tasks. Thus, it is important to generate humanlike, high-quality, complex CoT reasoning data for training MLLMs, enabling them to more effectively tackle intricate multimodal reasoning tasks. Recently, DeepSeek-R1 [2] has successfully applied Reinforcement Learning (RL) to induce the self-emergence of complex cognitive reasoning ability in LLMs. This begs our rethinking: Can RL be utilized to incentivize the reasoning capability in MLLMs? To answer this question, we first follow the DeepSeek-R1-Zero paradigm [2], by directly using RL to improve the reasoning capability of MLLMs. Unfortunately, this direct RL training is challenged, as it struggles to effectively guide MLLMs generating complex CoT reasoning in absence of large-scale, high-quality multimodal data and prolonged training (see Fig. 1 (E) and (F)). To address the above issue, we propose Vision-R1, reasoning MLLM that integrates cold-start initialization with RL training. First, we construct high-quality multimodal CoT dataset without requiring manual annotations. Specifically, we leverage an existing MLLM to generate PseudoCoT reasoning text from multimodal image-text pairs. This Pseudo-CoT reasoning explicitly incorporates both vision descriptions and structured step-level reasoning process, exposing more detailed vision information in textual format. Next, we feed the enriched reasoning text back into the MLLM to obtain description including necessary vision information. This process effectively implements Modality Bridging, converting vision information to language. The resulting textual descriptions are then passed to text-only reasoning LLM, DeepSeek-R1, to extract high-quality CoT reasoning. Finally, the dataset is refined through rule-based data filtering, ultimately obtaining dataset with 200K multimodal human-like complex CoT reasoning samples, which serves as the cold-start initialization dataset for Vision-R1. Following the DeepSeek-R1 training pipeline, we need to apply Group Relative Policy Optimization (GRPO) [2, 18] on 10K multimodal math dataset to enhance the models reasoning capability. However, as shown in Fig. 1 (A) and (D), we observe an overthinking phenomenon in the coldstart initialized MLLM, i.e., the correct reasoning processes tend to be concentrated on shorter CoT reasoning sequences. This issue leads to the optimization problem in subsequent RL training. To address this challenge, we propose Progressive Thinking Suppression Training (PTST) alongside GRPO, which incorporates hard-formatting result reward function. This approach encourages Vision-R1 to compress CoT reasoning steps early in the RL process, internalizing correct reasoning methods while progressively extending its reasoning duration over time to effectively tackle more complex problems. Our main contributions can be summarized as follows: We explore how to use RL for MLLMs and introduce Vision-R1, reasoning MLLM that leverages cold-start initialization and RL training to incentivize reasoning capability. To the best of our knowledge, it is the first work that investigates the application of RL for enhancing reasoning capability in MLLMs and analyzes differences between direct RL training and the combined approach of coldstart initialization and RL training. We believe that our exploration can inspire new insights for the community. high-quality 200K multimodal CoT dataset without human annotations is constructed to serve as cold-start initialization data for MLLMs. We leverage the proposed PTST to GRPO with hard-formatting result reward function, which effectively addresses the overthinking optimization problem in RL training. PTST enables Vision-R1 Figure 2. The overall data generation pipeline incorporating our Modality Bridging method. The multimodal data is first sent to MLLMs to obtain Pseudo-CoT consisting of caption and reasoning process, which serves as the input of MLLMs along with the original image-question pair to produce detailed descriptions. Through this modality bridging approach, the textual descriptions provide DeepSeek-R1 with holistic information that facilitates the generation of high-quality CoT processes, which are post-processed and integrated with the original data to create the final Vision-R1-cold dataset. 2. Related Work 2.1. Large Language Model Reasoning As researchers have discovered that enabling LLMs to simulate human-like thought processes and perform stepwise reasoning can significantly enhance their performance on reasoning tasks [1], extensive work has been dedicated to exploring LLM reasoning methods. These approaches typically rely on human design to format LLM outputs to follow specific steps, such as Chain-of-Thought (CoT) prompting methods [3], plan-based methods like Tree-of-Thought and Graph-of-Thought [4, 5], process-based reward models [6 9], Monte Carlo Tree Search (MCTS) and Beam Search [10 12], as well as constructing complex Supervised Fine-Tuning (SFT) datasets [13, 14]. recent development, DeepSeek-R1 [2], demonstrates that applying large-scale Reinforcement Learning (RL) with formatting and result-only reward functions can guide LLMs toward self-emerging thought processes, producing humanlike complex CoT reasoning and achieving significant advantages in complex reasoning tasks. This approach has shown immense potential in the field of Large Language Model Reasoning, however, its application to MLLMs remains an open area of inquiry. 2.2. Multimodal Large Language Model Reasoning MLLMs typically map inputs from other modalities to the textual modality, which are then processed by LLMs. This approach has been proven to exhibit superior performance in range of vision understanding tasks [1924]. Inspired by advancements in LLM reasoning, many studies have also sought to enhance the reasoning capability of MLLMs. For instance, efforts have been made to employ CoT prompting [2527] and to construct SFT datasets that include stepFigure 3. Comparison between the CoT processes generated by descriptions with and without Pseudo-CoT. Simple descriptions generated without the Pseudo-CoT input lack sufficient visual information, leading to confusion and hallucination in the reasoning process of DeepSeek-R1. In contrast, detailed descriptions enhanced through our Modality Bridging with Pseudo-CoT integrate high-quality visual information into textual descriptions, which facilitates accurate reasoning and enables R1 to generate correct answers. to progressively develop more complex reasoning processes while effectively guiding MLLMs toward enhanced reasoning capability. Extensive experiments demonstrate the strong reasoning capability of Vision-R1. Notably, despite having only 7B parameters, Vision-R1 achieves performance comparable to State-of-The-Art (SoTA) MLLMs with over 70B parameters in math reasoning tasks. level reasoning [15, 16]. However, the CoT generated by these methods often lacks natural human cognitive processes, such as questioning, reflection, and inspecting, which limits their effectiveness in solving complex reasoning tasks. In contrast, Vision-R1 distinguishes itself by combining coldstart initialization with RL training to acquire high-quality, complex CoT reasoning capability. This enables it to effectively address challenging vision reasoning tasks. 3. Method 3.1. Can Only RL Incentivize Reasoning Capability in MLLMs? Inspired by DeepSeek-R1-Zero [2], we aimed to directly use RL to guide models towards self-thought and the emergence of complex reasoning capability. To this end, we collected dataset of 10K open-source math problems for RL training. Specifically, we followed the DeepSeek-R1-Zero pipeline, training base MLLM using GRPO [18], with output format constraints dictated by the following system prompt: conversation between User and ... first thinks about Assistant. the reasoning process ... provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags ... For the reward function, we utilized both formatting and result rewards: Formatting reward function: The models output must adhere to the <think> </think><answer> </answer> format. Result reward function: The models generated final result must align with the ground truth. then we set the formatting and result reward ratio as 1 : 1. The model after purely RL training is named as VisionR1-Zero. Unfortunately, as shown in Fig. 1 (D) and (E), directly applying RL to train MLLMs has proven challenging in stimulating MLLMs reasoning capability and producing lengthy, complex CoT, limiting the models reasoning ability. Moreover, we observed that as training progressed over extended periods, the model gradually learned to use longer reasoning processes to solve hard problems, but led to significant performance degradations. Thus, we claim that directly using RL to incentivize the reasoning capability of MLLMs for solving complex reasoning problems remains challenging task, especially under constraints of data quality and quantity, as well as computation resources. 3.2. Overview of Vision-R1 In our above explorations, we observed that an RL-only approach struggles to guide MLLMs in generating humanlike, complex CoT. Consequently, we explored an alternative strategy and introduced the reasoning MLLM, Vision-R1. This method begins with cold-start using multimodal CoT dataset, which initially teaches the base model to reason in human-like manner. Subsequently, we apply RL to the cold-start initialized model Vision-R1-CI to guide it towards adopting the correct reasoning process, thereby incentivizing the reasoning capability in the final model Vision-R1. In the following sections, we first describe our approach to create high-quality, human annotation-free multimodal CoT dataset in Sec. 3.3.1, as well as the Overthinking Optimization Problem faced by post-cold-start MLLMs in Sec. 3.3.2. Then we discuss the RL training method we proposed, Progressive Thinking Suppression Training (PTST) in Sec. 3.4, to address the Overthinking Optimization Problem. 3.3. Cold-start Initialization 3.3.1. Modality Bridging to obtain High-quality Multimodal CoT Data Many existing works [15, 17] have attempted to construct multimodal reasoning datasets to enhance the reasoning capability of MLLMs. Prior efforts typically gathered CoT data which often lack the natural cognitive processes of questioning, and self-reflection. These datasets are typically constructed in step-by-step form based on human heuristics. However, our goal is to collect multimodal CoT dataset that encompasses complex cognitive processes to teach Vision-R1 to reason in human-like, natural manner. Furthermore, DeepSeek-R1 has demonstrated the ability to generate CoT with natural cognitive processes and has proven to have strong reasoning capability. By using the high-quality CoT data it generates, which incorporates human-like cognitive self-reflection processes, we can train MLLMs to enhance their reasoning capability. However, being text-only LLM, DeepSeek-R1 struggles to effectively process multimodal inputs to produce high-quality CoT. To overcome these limitations, we utilize existing MLLMs alongside DeepSeek-R1 and propose method named Modality Bridging to indirectly convert multimodal information into textual information, thereby capturing the complex cognitive processes of DeepSeek-R1. As the data generation pipeline that shown in Fig. 2, firstly, we follow prior works [15, 17] by inputting an image-question-answer pair and prompt into MLLM to generate Pseudo-CoT featuring both image description and reasoning processes. Subsequently, we concatenate the image-question pair with the Pseudo-CoT and prompt, and then feed them into MLLM to obtain detailed image description. Below is the prompt template: Given image, question:{question} and thinking process:{thinking process}, provide detailed description containing all the necessary details of the image to Figure 4. GRPO with our proposed PTST strategy. We progressively loosen the context length restrictions, increasing the length of reasoning process. Specifically, we set the reasoning length to 4K, 8K and 16K tokens for each stage, with corresponding group numbers of 16, 8 and 4 respectively. The reward function for GRPO is based on hard formatting result reward function (HFRRF). The dotted line in the Stage 3 indicates that the final version of Vision-R1 did not undergo the third stage of training. answer the question correctly... This process of generating Pseudo-CoT explicitly exposes more of the necessary vision details for reasoning in textual form, compared to pure image descriptions. This assists the MLLM in producing more detailed descriptions, thereby minimizing information loss during the conversion of original multimodal information to textual information (see Fig. 3). At this stage, we cleverly bridge image information to textual information and feed it into DeepSeek-R1 to obtain high-quality CoT processes. We then retain reasoning processes from the generated multimodal CoT data that the final answer is aligned with ground truth and apply rule-based data filtering to remove logical inconsistent samples and replace some words for semantic coherence. Finally, we pair the pure text CoT data generated by DeepSeek-R1 from the above process with the corresponding images, integrating into multimodal CoT data, named Vision-R1-cold dataset. This dataset is used for the coldstart initialization of Vision-R1. By acquiring CoT data in this manner, which closely mimics human cognitive behavior, the reasoning processes exhibit natural and logical thinking. 3.3.2. Overthinking Optimization Problem After obtaining multimodal CoT dataset, we conducted SFT on pretrained MLLM (such as Qwen2.5-VL-7BInstruct [31]) as the base MLLM for cold-start initialization. The MLLM after cold start initialization is named as Vision-R1-CI. At this stage, the base MLLM had learned the complex reasoning mode from DeepSeek-R1, however, this led to the Overthinking Optimization Problem, i.e., VisionR1-CI would engage in prolonged thought processes on certain problems, whereas the correct reasoning processes were typically concentrated in shorter cognitive chains. As shown in Fig. 1 (D) and (E), this propensity for excessive incorrect reasoning significantly complicates the optimization of subsequent RL training. For instance, when the allowed thought length during RL training for VisionR1-CI was directly extended to 16K, the model tended to generate longer answers to fulfill the demands of complex reasoning. However, this incorrect reasoning did not lead to performance improvements, thus presenting challenges in incentivizing reasoning capability in MLLMs. So, it is crucial to guide the model to learn correct thinking in the early stages for the reasoning performance of MLLMs. 3.4. Progressive Thinking Suppression Training Inspired by the above phenomenon, we propose the Progressive Thinking Suppression Training (PTST) algorithm to initially suppress the length of reasoning during the early stages of RL training for Vision-R1, while guiding it towards the correct reasoning processes. As training progresses, we gradually relax these constraints, allowing Vision-R1 to autonomously learn to utilize longer CoT to address increasingly complex problems, thereby enhancing its reasoning capability. Specifically, we implement Group Relative Policy Optimization (GRPO) with hard formatting result rewards for the models self-learning. Consider the standard GRPO approach, it samples group of generated output set {o1, o2, , oG} for each question from policy model πθold. Then GRPO maximizes the following objective and optimizes the model πθ. JGRPO(θ) = qP (Q), {oi}G i=1πθold (Oq) (cid:34) 1 clip (cid:88) min (cid:16) πθ(oi q) πθold (oi q) Ai, i=1 (cid:16) πθ(oi q) πθold (oi q) β DKL (cid:0)πθ (cid:13) (cid:13) πref (cid:35) (cid:1) , , 1 ε, 1 + ε (cid:17) (cid:17) Ai (1) where ε and β are the PPO clipping hyper-parameter and the coefficient controlling the KullbackLeibler (KL) penalty [18, 37], respectively. We set ε = 0.2 and β = 1e2 during training. Ai = rimean({r1,r2,...,rG}) is the computed advantage using the group rewards {r1, r2, , rG} std({r1,r2,...,rG}) Table 1. Comprehensive comparison with SoTA MLLMs (closed-source, open-source general/math/reasoning MLLMs) across diverse multimodal math benchmarks.Avg. denotes the average performance over all benchmarks. For MathVista benchmark, we have specifically compared all models on three sub-tasks that are highly related to mathematical reasoning: geometry reasoning (GEO), algebraic reasoning (ARI), geometry problem solving (GPS) and math word problems (MWP). ALL denotes the average score on MathVista benchmark. The best results are bolded and the second best results are underlined in all following tables. Our Vision-R1 achieve superior performance in math reasoning tasks, outperforming baseline approaches by significant margin. Model Params. MathVista MathVerse MM-Math Avg. (2024/09) OpenAI O1 [1] (2024/05) GPT-4o [28] (2023/09) GPT-4V [29] (2024/06) Claude-3.5 Sonnet [30] (2025/02) Qwen2.5-VL-7B [31] (2025/02) Qwen2.5-VL-72B [31] (2024/12) InternVL2.5-78B [32] (2024/06) Math-LLaVA-13B [33] (2024/08) Math-PUMA-Qwen2-7B [34] (2024/09) Multimath-7B [35] (2025/01) URSA-8B [36] GEO ARI GPS MWP ALL Closed-Source MLLMs 73.9 63.8 49.9 67.7 Open-Source General MLLMs 7B 72B 78B 13B 7B 7B 8B 66.9 77.8 76. 68.7 77.9 76.5 66.8 78.8 77.9 Open-Source Math MLLMs 56.5 47.3 40.2 57.7 48.1 66.8 79. 76.9 74.7 75.8 56.5 61.8 75.3 Open-Source Reasoning MLLMs 68.1 73.5 71.9 46.6 47.9 50.0 59.8 54.8 63. 37.6 39.4 26.5 46.7 51.3 25.4 22.9 33.6 26.9 45.7 20.3 31.8 23.1 34.1 45.6 17. 44.4 37.5 49.6 56.8 38.4 16.5 23.7 30.5 (2024/11) LLaVA-CoT-11B [17] (2024/12) Mulberry-7B [15] 11B 7B Our Model Vision-R1-7B (Ours) 7B 80.3 79.0 83. 80.6 73.5 52.4 40.2 55.4 The result is collected from the official MathVista leaderboard (https://mathvista.github.io/#leaderboard). Table 2. Comparison of the occurrence frequency of self-reflective indicators between llava-cot, mulberry and our Vision-R1-cold dataset. The higher frequency of these reflective markers in our dataset demonstrates its distinctive self-reflection and selfcorrection characteristic."
        },
        {
            "title": "Word",
            "content": "llava-cot (100K) Mulberry (260K) Vision-R1-cold (200K) Wait Hmm Mistake Alternatively Check 2,300 1 183 251 8,332 1,122 0 8,784 68 26, 585,719 75,853 26,697 188,187 100,148 (cid:0)πθ πref and DKL KL divergence. (cid:1) = πref(oiq) πθ(oiq) log (cid:16) πref(oiq) πθ(oiq) (cid:17) 1 is the As shown in Fig. 4, in our proposed PTST, we denote the total number of training stages by S, with each stage having its own sampling count Gs and sequence length limit Ls. The output space for stage {1, 2, . . . , S} is O(s) = { : Ls}. The training objective for the s-th stage can be further reformulated based on Eq. 1 as: (s),w/PTST GRPO (θ) = qP (Q), {o (s) }Gs (cid:0) O(s)q i=1πθold q(cid:1) (cid:0)o(s) q(cid:1) (cid:0)o(s) A(s) i (cid:1) , (cid:16) πθ min (cid:34) 1 Gs Gs(cid:88) i=1 clip (cid:16) πθ (cid:0)o(s) (cid:0)o(s) πθold πθold q(cid:1) q(cid:1) , 1 ε, 1 + ε (2) (cid:17) (cid:17) A(s) β DKL (cid:0)πθ (cid:13) (cid:13) πref (cid:35) (cid:1) , where A(s) denotes the advantage estimate for the i-th sample in the training stage s, and πθold denotes the policy model with an output length constraint of O(s). We employ the hard formatting result reward function as the reward mechanism for GRPO, i.e., the model receives reward score of ri = 1 only when both the formatting requirements and the correctness of the final answer are simultaneously satisfied; otherwise, it receives score of ri = 0. Moreover, we do not impose constraints using system prompt, as Vision-R1-CI has already acquired robust formatting capability during the cold-start initialization. Table 3. Comparison with SoTA MLLMs across diverse comprehensive multimodal benchmarks. The base MLLM is Llama-3.2-11BV-Instruct [38]. The results indicate that our data significantly enhances the generalization capabilities of our model, leading to superior performance across all benchmarks. Method General Benchmark Math Benchmark MMStar ChartQA MMEsum HallBench MathVista MathVerse MM-Math (2024/09) Llama-3.2-11B-V [38] (2024/12) Mulberry-Llama-11B [15] (2024/11) LLaVA-Cot-11B [17] Vision-R1-LlamaV-CI-11B 49.8 51.3 57.6 61.4 83.4 83.5 81.9 83. 1787 2035 2137 2190 40.3 48.9 47.8 49.5 48.6 61.1 54.8 62. 8.4 20.3 27.1 4.1 18.7 16.5 26.1 Table 4. Evaluating the impact of Cold Start Initialization, GRPO, Progressive Thinking Suppression Training (PTST). Avg. Len. denotes the average output token length on the 10K multimodal math dataset for RL training. Avg. Acc. denotes the average performance over all multimodal math benchmarks (MathVista, MathVerse, MM-Math). Method Cold Start GRPO PTST Avg. Len. Avg. Acc. Vision-R1-Zero Vision-R1-CI Vision-R1-Long Vision-R1 (Ours) 1285 3566 3107 2057 50.7 44.5 47.7 55.4 By applying PTST, we compress the models thought length in the early training stages to guide correct reasoning and gradually relax these constraints in later stages. As illustrated in Fig. 1, this progressive strategy enables Vision-R1 to generate more complex CoT and significantly enhances its reasoning capability. Notably, in practice, we observe that Vision-R1 achieves competitive performance by the end of the second training stage, leading us to select it as the final stage, i.e., we set the parameters as = 2, Ls {4K, 8K, 16K}, and Gs {16, 8, 4}, respectively. Moreover, compared to directly training with fixed 16K CoT length (Vision-R1-Long), our proposed PTST approach can achieve comparable level of CoT complexity in the third stage while yielding significant improvement in reasoning performance (see Fig. 1 (D) and (E)). 4. Experiments 4.1. Experiment Settings Dataset and Benchmarks. To obtain the cold-start dataset, we use the multimodal visual question answering (VQA) datasets, LLaVA-CoT dataset (100K) [17] and Mulberry dataset (260K) [15], to conduct Vision-R1-cold (200K). During GRPO process, we mix the math datasets of WeMath [39], MathVision [40], Polymath [41], SceMQA [42], Geometry3K [43] as our RL training data. Total amount of data is around 10K. For evaluating the reasoning capability of our Vision-R1, we choose three widely used multimodal math benchmarks: MM-Math [44], MathVista [45], MathVerse [46]. These benchmarks covers various mathematical fields which can provide thorough evaluation of MLLMs math reasoning ability. Besides, we select four general multimodal benchmarks to demenstrate the general ability of our model: MMStar [47], ChartQA [48], MME [49] and HallBench [50]. Those general benchmarks are used to evaluate the data quality of our proposed Vision-R1-cold dataset. Implementation Details. For the Vision-R1-cold dataset preparation, we utilize 128 NVIDIA H800 80G GPUs to deploy the open-source MLLM Qwen-2.5-VL-72B [31] and the reasoning LLM DeepSeek-R1 [2]. We then process the VQA datasets using Qwen-2.5-VL-72B and DeepSeek-R1 over approximately 2 days. For the cold-start initialization of Vision-R1-7B, we adopt Qwen-2.5-VL-7B-Instruct [31] as the base model and train it via supervised fine-tuning (SFT) for 2 epochs on 32 NVIDIA H800 80G GPUs for 10 hours using Llama-Factory framework [51]. After cold-start initialization, we obtain the postcold-start model, Vision-R1-CI-7B, which is subsequently trained on the collected 10K math dataset using GRPO on 64 NVIDIA H800 80G GPUs for approximately 2 days in Verl training framework [52, 53], while following the twostage PTST approach (note that we do not use the third stage checkpoints), i.e., was set to 2. The all models in the paper are summarised as follows: Vision-R1-Zero: This represents the baseline approach where reinforcement learning (RL) is applied directly to the base MLLM without cold-start initialization. Vision-R1-CI: The base MLLM is cold-start initialized using the Vision-R1-cold dataset, resulting in this model. Vision-R1-Long: This variant is trained with maximum generation length of 16K tokens, where four samples per input are generated from the cold-started Vision-R1-CI, followed by 300 training steps. Vision-R1: This model follows the Progressive Thinking Suppression Training (PTST) strategy, where two-stage RL training process is applied: Stage 1: The model is trained from Vision-R1-CI for 100 steps with an 8K token generation limit, sampling 16 samples per input. Stage 2: Training continues for another 100 steps with maximum generation length of 8K tokens, sampling 8 samples per input. The final model checkpoint at the end of Stage 2 is the final Vision-R1 model, as this stage achieves an optimal balance between reasoning length and overall performance. Additionally, Vision-R1 can be further extended to third Figure 5. The output examples of Vision-R1-7B on MathVerse benchmark. Vision-R1-7B shows human-like questioning and self-reflective thought process when solving math reasoning problems, which is also called Aha moment in DeepSeek-R1s paper [2]. More examples are provided in supplementary materials. training stage, following the same parameter settings as Vision-R1-Long and continuing for an additional 100 training steps. However, as shown in Fig. 1, further training does not yield significant performance improvements but does generate more complex reasoning processes. To assess the quality of the Vision-R1-cold dataset, we apply the same training hyper-parameters of the cold-start initialization to Llama-3.2-V-Instruct [38] for SFT, resulting in another post-cold-start MLLM, Vision-R1-LlamaV-CI, which can also be used for subsequent RL training. 4.2. Main Results Math Reasoning. As shown in Tab. 1, our proposed VisionR1-7B achieves competitive results across multiple math reasoning benchmarks, even when compared to SoTA models with over 10 times the parameters of Vision-R1-7B. For instance, on the MathVista benchmark, Vision-R1-7B achieves score of 73.5%, only 0.4% lower than OpenAI O1, the most widely recognized reasoning model. Moreover, on the complex math reasoning sub-tasks of MathVista, i.e., GEO, ARI, and GPS, Vision-R1-7B achieves scores of 80.3%, 79.0%, and 83.2%, respectively, exceeding the base model Qwen-2.5-VL-7B by an average accuracy improvement of over 10%. These results highlight the strong reasoning capability Vision-R1-7B gains through human-like complex thinking processes. Furthermore, on the more challenging MathVerse and MM-Math benchmarks, Vision-R1-7B ranks Top-1 and Top-2, respectively, with the latter being second only to Qwen-2.5-VL-72B. This demonstrates Vision-R17Bs effectiveness in solving complex math problems. Cold-start Dataset Quality. We conduct quality analysis of our proposed Vision-R1-cold dataset. The primary objective of constructing Vision-R1-cold is to supplement the existing multimodal CoT datasets, which lack complex cognitive processes, and to leverage DeepSeek-R1s highquality CoT process as cold-start data. To evaluate this, we present comparative analysis in Tab. 2, where we statistically examine the presence of questioning, reflection, and inspection within the CoT processes of Mulberry, LLaVACoT, and Vision-R1-cold datasets. The results indicate that Vision-R1-cold contains significantly higher proportion of human-like cognitive processes compared to previous multimodal CoT datasets. This complex CoT structure facilitates the base MLLM in learning reasoning mechanisms, providing high-quality cold-start initialization for subsequent RL training. Additionally, we compare Vision-R1-cold with previous multimodal CoT datasets by training on Llama-3.211B-V. After SFT, our Vision-R1-LlamaV-CI-11B model achieves SoTA performance across all general and math reasoning benchmarks, outperforming both LLaVA-CoT-11B and Mulberry-Llama-11B. Notably, Vision-R1-LlamaV-CI11B achieves 7.4% accuracy improvement over MulberryLlama-11B on MM-Math benchmark, directly confirming the superior data quality of our Vision-R1-cold dataset. 4.3. Ablation Study As shown in Table 4, we compare the performance of various RL training strategies. The results indicate that Vision-R1Zero, which applies RL training directly without cold-start initialization, struggles to generate sufficiently long and complex CoT reasoning, thereby limiting its ability to handle intricate reasoning tasks. In contrast, Vision-R1-CI, after cold-start initialization, tends to generate excessively long CoTs. However, the presence of numerous incorrect reasoning steps leads to lower overall performance. Furthermore, Vision-R1-Long via applying RL training directly to Vision-R1-CI results in optimization difficulty, making it challenging to achieve significant performance improvements. In comparison, our proposed Vision-R1 demonstrates substantial advantage in reasoning performance, effectively balancing CoT complexity and accuracy. 4.4. Visualization As shown in Fig. 5, our proposed Vision-R1-7B is capable of generating complex reasoning processes and exhibits the emergence of the so-called Aha moment [2], i.e., phenomenon analogous to human cognitive processes involving questioning and reflection. This sophisticated reasoning capability significantly enhances the models inference performance, leading to substantial improvements in solving complex reasoning tasks. 5. Conclusion We explore how to use RL training to incentivize reasoning capability in MLLMs. Moreover, we proposed Vision-R1 and achieved the strong math reasoning ability, achieving comparable performance to SoTA MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 2, 3, 6 [2] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 2, 3, 4, 7, 8, 9 [3] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837, 2022. 2, 3 [4] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:11809 11822, 2023. 3 [5] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690, 2024. 3 [6] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [7] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [8] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. [9] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. 3 [10] Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. In Forty-first International Conference on Machine Learning, 2024. 3 [11] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. [12] Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024. 3 [13] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [14] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. 2, 3 [15] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. 2, 4, 6, 7 [16] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamavo1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. 2, 4 [17] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. 2, 4, 6, 7 [18] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 4, 5 [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [21] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [22] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [23] Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaoshen Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, and Shaohui Lin. Dynamic-llava: Efficient multimodal large language models via dynamic vision-language context sparsification. arXiv preprint arXiv:2412.00876, 2024. [24] Fei Zhao, Taotian Pang, Chunhui Li, Zhen Wu, Junjie Guo, Shangyu Xing, and Xinyu Dai. Aligngpt: Multi-modal large language models with adaptive alignment capability. arXiv preprint arXiv:2405.14129, 2024. 3 [25] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. 3 [26] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-thought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014431, 2024. [27] Bozhi Luan, Hao Feng, Hong Chen, Yonghui Wang, Wengang Zhou, and Houqiang Li. Textcot: Zoom in for enhanced multimodal text-rich image understanding. arXiv preprint arXiv:2404.09797, 2024. 3 [28] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6 [29] OpenAI. Gpt-4v(ision) system card. https://openai. [30] Anthropic. com/index/gpt-4v-system-card, 2023. 6 Claude 3.5 sonnet. https : / / www . anthropic . com / news / claude - 3 - 5 - sonnet, 2024. [31] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5, 6, 7 [32] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 6 [33] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. MathLLaVA: Bootstrapping mathematical reasoning for multiIn Findings of the Associmodal large language models. ation for Computational Linguistics: EMNLP 2024, pages 46634680, November 2024. 6, 1 [34] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024. 6 [35] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. 6 [36] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. 6 [37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 5 [38] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 7, [39] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 7 [40] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. 7, 1 [41] Himanshu Gupta, Shreyas Verma, Ujjwala Anantheswaran, Kevin Scaria, Mihir Parmar, Swaroop Mishra, and Chitta Baral. Polymath: challenging multi-modal mathematical reasoning benchmark. arXiv preprint arXiv:2410.14702, 2024. 7 [42] Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, and Xiangliang Zhang. Scemqa: scientific college entrance level multimodal question answering benchmark. arXiv preprint arXiv:2402.05138, 2024. 7 [43] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 7, 1 [44] Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juanzi Li. Mmmath: Advancing multimodal math evaluation with process evaluation and fine-grained classification. arXiv preprint arXiv:2404.05091, 2024. 7 [45] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [46] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly In European see the diagrams in visual math problems? Conference on Computer Vision, pages 169186. Springer, 2024. 7 [47] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large visionlanguage models? arXiv preprint arXiv:2403.20330, 2024. 7 [48] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 7, 1 [49] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. 7 [50] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [51] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. 7 [52] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 7 [53] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https:// github.com/hiyouga/EasyR1, 2025. 7 [54] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 1 [55] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476, 2015. 1 [56] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746, 2022. [57] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. 1 [58] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241, 2023. 1 [59] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. 1 [60] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, SongChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for In International semi-structured mathematical reasoning. Conference on Learning Representations (ICLR), 2023. 1 [61] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. 1 [62] Adam Dahlgren Lindstrom and Savitha Sam Abraham. Clevrmath: dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. 1 [63] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1496314973, 2023. [64] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. 1 [65] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan Kankanhalli. Dual-glance model for deciphering social relationships. In Proceedings of the IEEE international conference on computer vision, pages 26502659, 2017. 1 [66] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 1 [67] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European conference on computer vision, pages 146162. Springer, 2022. 1 [68] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 1 [69] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from In Proceedings of the IEEE conference on blind people. computer vision and pattern recognition, pages 36083617, 2018. [70] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating [83] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. 1 the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. 1 [71] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th international conference on computational linguistics, pages 15111520, 2022. 1 [72] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pages 49995007, 2017. 1 [73] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In Computer VisionECCV 2016: worth dozen images. 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. 1 [74] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [75] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. 1 [76] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. 1 [77] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. 1 [78] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 1 [79] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. 1 [80] Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 15271536, 2020. [81] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 1 [82] Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. arXiv preprint arXiv:2206.01347, 2022. 1 Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Additional Dataset Illustrations We provide some more examples of our Vision-R1-cold dataset in Fig. 6. Fig. 7 showcases multiple response instances generated by our Vision-R1 model, including comprehensive reasoning processes and corresponding final answers. 7. Data Sources Our proposed Vision-R1-cold dataset consists of wide range of multimodal data, including the following categories: Mathematical Data: GLLaVA [54], GEOS [55], UniGeo [56], GeoQA Plus [57], Geo3K [43], MathVision [40], GeoMverse [58], MathV360K [33], IconQA [59], TabMWP [60], CLEVR [61], CLEVR-Math [62], and Super-CLEVR [63]. General QA Data: ShareGPT4V [64], PISC [65] VQAAS [66], A-OKVQA [67], TextVQA [68], Vizwiz [69], and VQA2.0 [70] Science and Medical Data: From GeoQA+ [71], CLEVRMath [62], TQA [72], AI2D [73], ScienceQA [74], VQARAD [75], and PMC-VQA [76] Figure Understanding Data: FigureQA [79], InfoVQA [81], MultiHiertt From DVQA [77], PlotQA [80], [82], DocVQA [78], ChartQA [48], and LRV-Chart [83]. Figure 6. Examples of our Vision-R1-cold data. It comprises abundant information obtained through our Modality Bridging method. Figure 7. More output examples of our Vision-R1-7B on MathVerse benchmark."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Xiaohongshu Inc."
    ]
}