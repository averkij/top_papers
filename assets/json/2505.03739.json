{
    "paper_title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model",
    "authors": [
        "Zuwei Long",
        "Yunhang Shen",
        "Chaoyou Fu",
        "Heting Gao",
        "Lijiang Li",
        "Peixian Chen",
        "Mengdan Zhang",
        "Hang Shao",
        "Jian Li",
        "Jinlong Peng",
        "Haoyu Cao",
        "Ke Li",
        "Rongrong Ji",
        "Xing Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 9 3 7 3 0 . 5 0 5 2 : r VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model Zuwei Long1,, Yunhang Shen1,,, Chaoyou Fu2,, Heting Gao1, Lijiang Li2 Peixian Chen1, Mengdan Zhang1, Hang Shao1, Jian Li1, Jinlong Peng1 Haoyu Cao1, Ke Li1, Rongrong Ji3, Xing Sun1, 1Tencent Youtu Lab, 2Nanjing University, 3Xiamen University Equal Contribution Project Leader Corresponding Author https://github.com/VITA-MLLM/VITA-Audio"
        },
        {
            "title": "Abstract",
            "content": "With the growing requirement for natural human-computer interaction, speechbased systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3 5 at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks."
        },
        {
            "title": "Introduction",
            "content": "Speech interaction is becoming the primary modality for human-computer interaction due to its naturalness and its ability to convey richer informational dimensions through paralinguistic features such as prosody and intonation. Real-time speech systems have thus become crucial research focus for enabling natural dialogue. Traditional speech systems predominantly adopt modular design that decomposes real-time speech processing into three discrete components: automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) [55, 30, 69]. However, this cascaded approach suffers from cumulative latency, loss of paralinguistic information (e.g., emotional prosody, rhythm) during modality conversion, and error accumulation between modules, substantially lowering the practical Preprint. Under review. (a) The visualization of the attention maps between text tokens and audio tokens. (b) The transcription results of the generated audio into text under different attention masks. Figure 1: (a) The audio sequence generated by the speech model exhibits strong correlation with the corresponding text tokens. (b) With irrelevant text tokens being masked out, the model is still able to generate the correct audio, and the pronunciation remains contextually appropriate. However, if all text tokens are masked, the model outputs random audio. This suggests that the hidden states from the LLM include sufficient contextual information for generating the corresponding audio tokens. Consequently, the mapping from text hidden states to audio tokens can be accomplished using relatively simple modules, without the need for the extensive semantic modeling typically required by LLMs. utility of cascaded architectures in real-time interactive scenarios. To address the limitations of traditional methods, many recent studies have adopted an end-to-end approach to handle inputs and outputs of the model [59, 10, 22]. These methods directly input speech into LLMs through an audio encoder and then synthesize speech response using the discrete tokens [66] or hidden states [57] output by LLMs. While existing end-to-end speech models generate output in streaming fashion to reduce the response latency, their first token delay is still high. Specifically, current speech models cannot directly deliver the first streaming audio chunk upon completing the first LLM forward pass, i.e., decoding the first text token. In the applications requiring high real-time performance, this delay poses significant bottleneck to the deployment of LLMs for speech processing. This prompts pertinent question: How can we achieve more real-time audio generation within end-to-end speech models? To explore this issue, we visualized the hidden states of the final decoder layer of the speech model. As shown in Fig. 1a, the audio tokens generated by the speech model show increased attention to the text tokens they correspond to. As the generation of audio tokens progresses, the text tokens attended by the new audio token advance accordingly. This finding is also reported in many literature on attention-based speech systems [7, 32] In Fig. 1b, we show Chinese sentence with homograph 行 as an example. The pronunciation of this character can be /xing/ or /hang/ depending on its context. Our speech model correctly decides the characters pronunciation to be the former, given the hidden states of historical inputs. We then modify the model inference process by masking out all text hidden states, except the one corresponding to the token 一行(/yixing/) before generating its corresponding audio tokens. This modification prevents subsequently generated audio tokens from directly attending to other text tokens, although they can still attend to previously generated audio tokens. We find that the subsequently generated audio tokens accurately produce the sounds as yixing, which remains contextually appropriate. The same observation also holds for other non-homograph tokens. We therefore argue that the hidden states from the LLM include sufficient contextual information for generating its corresponding audio tokens, and attending to additional texts is unnecessary. Finally, we experiment with masking 2 Table 1: Comparison of recent speech models, VITA-Audio leverages the hidden state to enhance model performance, adopts an end-to-end architecture, and achieves zero audio token delay. Model Audio Token Delay Leveraging Hidden States End-to-End [57] Freeze-Omni [9] MinMo [59] Mini-Omni Moshi [18] GLM-4-Voice [66] [27] LUCY VITA-Audio Text Length 5 7 1 13 7 out all text tokens. This time, the generated audio fails to align with its text and sounds like random non-speech babbles even though the model has access to the previously generated audio tokens. These findings suggest that the speech model learns to primarily focus on the small span of corresponding text hidden states without heavily modeling the semantic space of the entire text and audio sequence. This discovery instills confidence that we can learn the simple mapping relationship between text hidden states and audio tokens with relatively simple modules and without relying on the extensive semantic modeling of LLMs. In this paper, we introduce VITA-Audio, lightweight framework that uses separate efficient modules, named Multiple Cross-modal Token Prediction (MCTP), to efficiently generate audio responses from text embeddings and LLM hidden states. This approach enables obtaining both text tokens and an audio chunk in single LLM forward pass, achieving zero delay in audio tokens. comparison of the delay of the first audio token is presented in Table 1 , where we define audio token delay as the number of additional LLM forward steps required to generate the first audio token after the first LLM forward pass. We distinguish this delay from audio generation delay which is the number of additional LLM forward passes to generate meaningful and consistent chunk of audio. This distinction exists because speech tokenizers can handle varied lengths of context window. For example, the CNN-based SNAC decoder [49] requires at least 3 7 tokens to generate 0.083 seconds of audio that is consistent with neighboring audio chunks at its boundaries. VITA-Audio has both zero audio token delay and zero audio generation delay. To this end, through four-stage progressive training strategy, we construct set of lightweight yet powerful MCTP modules, which predict 10 audio tokens directly from historical inputs and LLM hidden states without requiring additional LLM forward passes, thus significantly enhancing the models inference speed without sacrificing audio quality. In summary, our main contributions are as follows. We introduce VITA-Audio, the first end-to-end speech model capable of generating audio during the first forward pass. Leveraging audio generation without relying on extensive text semantic modeling capabilities, VITA-Audio designs lightweight MCTP modules to generate decodable audio token chunks with zero audio token delay, thus overcoming the real-time limitations in traditional cascaded models and existing end-to-end methods. VITA-Audio achieves remarkable end-to-end inference acceleration by generating ten audio tokens in single forward pass, resulting in 3 5 speedup when implemented on 7B LLM while preserving the ability of high-quality speech synthesis. We fully release VITA-Audio to the open-source community. Although VITA-Audio is trained on open-source data only, comprehensive evaluations reveal that VITA-Audio achieves the state-of-the-art performance on multiple benchmarks for ASR, TTS, and SQA tasks, outperforming existing models in both efficiency and accuracy, especially the opensource ones with similar parameter scale, therefore setting new standard for real-time speech-to-speech models."
        },
        {
            "title": "2 Related Work",
            "content": "Large language models have recently transformed human-computer interaction through their powerful natural language processing abilities. Extending their power to other modalities has since become the next step to explore. As one of the most natural forms of communication, speech interaction systems have thus become major research focus. Traditional systems [55, 30, 69] typically employ cascaded architecture, combination of separate ASR, LLMs, and TTS modules. However, this design presents several significant challenges, including accumulation of latency, loss of paralinguistic information, and error propagation across modules. Recent studies [23, 50] have attempted to integrate audio encoders directly into LLMs using trainable adapters, which significantly improves system integration. However, these systems still rely on an independent TTS module to generate audio. To enrich the input information of the speech decoder module, one research direction is to integrate the hidden states of LLMs into the speech decoder module. Llama-Omni [22] utilizes non-autoregressive transformer to predict discrete audio tokens from upsampled hidden states and is optimized using the Connectionist Temporal Classification loss. Freeze-omni [57] maximizes the retention of language capabilities by freezing the language model and employs combination of autoregressive and nonautoregressive speech decoders, predicting audio tokens from the language models hidden states and text embeddings. Minmo [9] integrates language model with CosyVoice2 [21], where the language model processes pure text, and the CosyVoice2 mixed speech-text sequences. End-to-end systems further integrate TTS functionality into LLMs, allowing the LLM to generate not only text responses but also speech responses. Currently, end-to-end speech models are primarily divided into two categories: parallel audio-text modeling and interleaved audio-text modeling. In the parallel modeling paradigm, the model uses different heads to process hidden states, generating both text and multiple audio tokens [10, 18]. Since the input to the LLM is altered during autoregression, maintaining the original capabilities of the LLM presents significant challenges. To perform inference without large-scale pretraining, Mini-Omni [59] and LUCY [27] rely on batch parallel decoding to preserve the inference capability of the LLM. In the interleaved modeling paradigm [66, 42], text and audio tokens are combined into sequence, with the model alternating between predicting text and audio tokens. Compared to parallel-paradigm models, interleave-paradigm models appear to better preserve language capabilities, as suggested by the performance comparison on spoken question-answering benchmarks [66]. We attribute this difference to the fact that parallel-paradigm models use an average of text and audio representations as input, which significantly diverges from the inputs used during pretraining. However, interleaveparadigm models face latency issue due to their sequential prediction of audio tokens, especially when the audio token rate is high. VITA-Audio leverages the strengths of these architectures by adopting the interleaved modeling paradigm and introducing MCTP for audio generation. The former maximally preserves the LLMs language ability, and the latter reduces inference latency by generating multiple audio tokens in single forward pass."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview As illustrated in Fig. 2, VITA-Audio consists of four major components: an audio encoder, an audio decoder, large language model backbone, and set of Cross-modal Token Prediction (MCTP) modules. Similarly to GLM-4-Voice [66], we use CosyVoice [20] as the audio encoder and decoder. The audio signal is first encoded into sequence of discrete audio tokens through the audio encoder, which are subsequently fed into the LLM for processing. During each forward pass, the LLM alternately generates text and audio tokens. The hidden states from the final layer of LLM, along with the embedding of the predicted token, are provided as input to the MCTP modules. The historical input tokens, the tokens predicted by the LLM, and by the MCTP modules are concatenated to form the inputs to the next LLM forward pass. Finally, the audio tokens generated by both the LLM and the MCTP modules are aggregated and passed to the audio decoder to generate the final audio output. 4 Figure 2: Architecture overview. (a) VITA-Audio is an end-to-end large speech model equipped with 10 light-weight Multiple Cross-modal Token Prediction (MCTP) modules that enable speech generation with extremely low latency. As shown in Fig. 1, we observe that the hidden states of certain text tokens in the LLM backbone contain sufficient semantic information for generating the corresponding audio tokens, which means that it is unnecessary to attend to additional text tokens when generating audio. Thus, we propose to utilize set of light-weight MCTP modules to model the mapping from LLM hidden states to the audio tokens. (b) The details of the MCTP modules. Our MCTP module has light-weight architecture, which enables it to finish one forward pass within 0.0024 seconds ( 11% of the LLM backbone). The MCTP module is capable of generating 10 audio tokens from the LLM hidden states and the text embedding, and the generated audio tokens can be decoded by the audio decoder directly. The utilization of MCTP modules enables VITA-Audio to generate audio responses in one LLM forward pass, which achieves extremely fast generation speed. 3.2 Multiple Cross-Modal Token Prediction (MCTP) Module As described in Section 1, the text and speech modalities exhibit monotonic alignment pattern. This cross-modal alignment allows us to avoid complex modeling of the semantic latent space and to focus on learning simple text-to-speech mapping relationship, which we propose to use lightweight modules to learn. In the preliminary experiments, we use few lightweight Transformer blocks to predict multiple audio tokens from LLM hidden states, and embed the predicted tokens into the LLMs autoregressive inference. Standard autoregressive modeling can be formulated as: pt(Yt1, ..., Y0) [YtYt1, ..., Y0], (1) where Yt denotes the predicted audio token at time step t, and pt represents the conditional probability distribution based on the historical sequence. When extended to multi-step prediction, i.e., predicting the i-th audio token at time step + i, the formulation becomes as: pt+i(Yt1, ..., Y0) (cid:101)P [Yt+iYt1, ..., Y0]. At this point, there is significant deviation in the consistency of the distribution between (cid:101)P and . As increases, the difference between the two distributions will progressively widen, resulting in growing accumulation of errors and leading to poor mapping between text and audio. (2) To address this issue, we adopt cascaded prediction architecture. Specifically, the hidden states and output sequence from the preceding modules are employed as joint input conditions for the subsequent modules: pt+i(Yt1, . . . , Y0) (cid:101)P [Yt+iYt1, . . . , Y0, ht+i1, ot+i1, . . . , ot], (3) 5 Table 2: Summary of datasets used in VITA-Audio for different stages. Task Name Total Number Stage 1 Sampling Ratio Stage 3 Stage 2 Stage 4 ASR TTS Speech QA Text QA Long Text QA [67] WenetSpeech Librispeech [43] Multilingual LibriSpeech [44] [43] Common Voice 17 [39] MMCRSC [8] GigaSpeech [26] Peoples Speech [54] VoxPopuli [6] AISHELL-1 [19] AISHELL-2 [48] AISHELL-3 [25] AISHELL-4 Wenetspeech4TTS LibriTTS GLOBE Emilia VoiceAssistant-400K AudioQA-1.0M OpenHermes-2.5 LIMA databricks-dolly-15k MetaMathQA MathInstruct Orca-Math atlas-math-sets goat camel-ai-math Long-Instruction LongForm LongAlign-10k LongCite-45k LongWriter-6k LongQLoRA LongAlpaca Long-Data-Collections [38] [65] [56] [29] [59] [27] [51] [70] [15] [62] [64] [40] [53] [52] [34] [63] [33] [3] [68] [4] [61] [11] [13] 10,000H 1,000H 71,506H 2.849H 755H 10,000H 1,000H 543H 170H 1,000H 85H 120H 12,800H 585H 535H 96,700H 400K 1M 1M 1K 15K 395K 262K 200K 17.8M 1.7M 50K 16K 23K 10K 45K 6K 39K 12K 98K 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1. 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0. 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1. 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 2.0 2.0 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0. where ht+i1 and ot+i1 represent the hidden state and output sequence of the preceding module, respectively. By introducing progressively updated contextual information, modules can achieve incremental optimization of cross-modal mapping, ensuring accurate modality synchronization at each time step. Inspired by DeepSeek V3 [17], we adopt an isomorphic Multi-Token Prediction (MTP) framework to construct our MCTP module. Unlike DeepSeekV3 and speculative decoding [36], where the former focuses on improving training and the latter requires verification to ensure that the sampling distribution matches exactly with that of the original model, we use the MCTP module for audio-text mapping, presumably simpler task than semantic modeling. As result, we require comparatively small amount of text data to train our model. Since the embedding layer and output heads are shared with the LLM, the audio tokens generated by the MCTP module are directly incorporated into the autoregressive process of the LLM. As illustrated in Fig. 2, the hidden states and output token, from the LLM or the preceding MCTP module, are concatenated with the input tokens and fed into Transformer block for next-step processing. The resulting hidden states and token are then passed to the next MCTP module. Upon completion of forward pass, the audio tokens generated by either the LLM or the MCTP modules are aggregated as the input sequence for the subsequent LLM forward iteration. 3.3 Training 3.3.1 Data Construction VITA-Audio is trained exclusively on open-source datasets, integrating multi-domain and multilanguage speech data resources. The training dataset encompasses diverse range of sources. Detailed descriptions of the datasets and the data proportions used at each stage are provided in Table 2. 6 Figure 3: Training pipeline of VITA-Audio. The first stage (Audio-Text Alignment) enhances the LLM by extending its audio modeling capability through large-scale speech pre-training. The second stage (Single MCTP module Training) connects an MCTP module with the LLM to predict one subsequent token based on the input tokens and the LLMs hidden states. The third stage (Multiple MCTP Modules Training) increases the number of MCTP modules in the model to predict more tokens in each model forward. The last stage (Supervised Fine-tuning) provides the speech-to-speech capability to the model by optimizing it on the large-scale speech QA dataset. ASR Data We aggregated approximately 100, 000 hours of open-source Automatic Speech Recognition (ASR) data, including WenetSpeech [67], Librispeech [43], Multilingual LibriSpeech [44], Common Voice 17 [43], MMCRSC [39], GigaSpeech [8], Peoples Speech [26], VoxPopuli [54], and the AISHELL series (AISHELL-1 [6] to AISHELL-4 [25]). TTS Data Concurrently, we integrate approximately 100, 000 hours of open-source Text-to-Speech (TTS) data, primarily consisting of the Wenetspeech4TTS [38], LibriTTS [65], GLOBE [56], and Emilia [29] datasets. Speech QA Data For speech question-answering (Speech-QA), we utilize VoiceAssistant400K [59] and AudioQA-1.0M [27], totaling 1.4 million speech QA data, to enhance the models speech-to-speech dialogue capabilities. Text-Only Data The pure text data is collected from OpenHermes-2.5 [51] LIMA [70], databricksdolly-15k [15], MetaMathQA [62], MathInstruct [64], Orca-Math [40], atlas-math-sets [53], goat [52], and camel-ai-math [34]. Given that discrete audio token sequences exhibit significantly longer lengths compared to their textual counterparts, we incorporate several specialized long-context text datasets following the Long-VITA [47] to enhance contextual modeling capabilities. These include Long-Instruction [63], LongForm [33], LongAlign-10k [3], LongCite-45k [68], LongWriter6k [4], LongQLoRA [61], LongAlpaca [11], and LongData-Collections [13]. All training data are uniformly packed into sequences of fixed length (8K tokens), an approach that enables effective training on samples of varying lengths [47]. We reinitialize the positional embeddings and attention masks for all packed samples to ensure that the model attends exclusively to tokens within the same original sample. This processing strategy not only eliminates potential artifacts introduced by data concatenation but also significantly enhances training stability and reduces computational overhead. 3.3.2 Training Pipline For VITA-Audio to output consistent sequence of audio tokens in single forward pass, each MCTP module needs to model different distribution. As result, training all the MCTP modules simultaneously becomes challenging task, especially when the number of modules is large, because 7 Figure 4: The four text-audio interleaved inference modes are illustrated as follows: 1) Turbo: As the fastest inference mode, it generates 1 token by the main model and 10 additional tokens via MCTP in each forward pass. To ensure that valid audio chunk is decoded after the first forward pass, the first generated 11 tokens are split into 1 text token and 10 audio tokens. Then, the Turbo mode iteratively generates 4 text tokens and 10 audio tokens in the following forward. 2) Boost: To enhance the quality of text tokens, Boost mode follows the text-audio cyclic pattern of Turbo mode, with the main model generating every text token and MCTP generating every audio token. 3) Balance: To keep balanced text-audio ratio, i.e., 1 : 2, the balance mode further changes the text-audio cyclic pattern of the Boost mode. Specifically, the balance mode sequentially generates 1 text token from the main model, 4 audio tokens (2 tag tokens mark the beginning and end of audios, and 2 common tokens denote the audio content) from MCTP, 3 text tokens from the main model, 8 text tokens (2 tag tokens mark the beginning and end of audios, and 6 common tokens denote the audio content) from MCTP, and then iteratively generates 4 text tokens from the main model and 10 audio tokens (2 tag tokens mark the beginning and end of audios, and 8 common tokens denote the audio content) from MCTP. 4) Vanilla: As the slowest inference mode, Vanilla mode follows the text-audio cyclic pattern of Balance mode, with the main model generating every token. their optimization objectives are not guaranteed to align. We propose four-stage training strategy, as illustrated in Fig. 3, to progressively equip the MCTP modules with the ability to map text to its audio, thereby reducing the difficulty of their convergence. Stage 1: Audio-Text Alignment. Building upon pretrained language models, the goal of this stage is to extend the audio modeling capabilities to the LLM through large-scale speech pretraining. We freeze the audio encoder and audio decoder, and train the LLM using ASR, TTS, and Text-only data. During this stage, the output of the LLM can be either pure text tokens or audio tokens. Stage 2: Single MCTP Module Training. After Stage 1, the model learns both text and audio distributions. The objective of Stage 2 is to train the initial MCTP module to predict one subsequent token based on the output tokens and hidden states from the LLM. This stage employs the same dataset configuration as Stage 1. We initialize the MCTP module using parameters from the final layer of the LLM, with the gradient detached from the LLM. Stage 3: Multiple MCTP Modules Training. The objective of this stage is to extend the single MCTP module to multiple MCTP modules. Specifically, each MCTP module predicts the token at its corresponding position given the output tokens and hidden states of the previous MCTP module. All subsequent MCTP modules are initialized using the weights of the MTCP module from Stage 2. This stage also incorporates gradient detachment to optimize the model training process. Stage 4: Supervised Fine-tuning. After the previous three training stages, VITA-Audio has acquired the ability to efficiently and accurately map text to audio. To enable speech-to-speech dialogue capability, we then conduct supervised fine-tuning using speech QA datasets while maintaining small amount of TTS, ASR, and text-only data to ensure training stability. To optimize training effectiveness, different learning rates are used for the MCTP module and the main LLM. For the speech-to-speech data, we employ an interleaved output format. This design enforces the model to initiate audio token generation during the first forward pass, enabling synchronized decoding of the audio tokens rather than waiting until all text tokens have been generated. This interleaved data format is exemplified as follows: 8 Table 3: Comparison of model structures between VITA-Audio and VITA-Audio-Plus. Name Base LLM Audio Encoder Audio Adapter Audio Decoder VITA-Audio VITA-Audio-Plus Qwen2.5-7B [45] Qwen2.5-7B [45] GLM-4-Voice-Tokenizer [66] SenseVoiceSmall [1] MLP GLM-4-Voice-Decoder [66] GLM-4-Voice-Decoder [66] Speech QA Interleaved Data Format { } \"messages\": [ { \"role\": \"user\", \"content\": \"<begin_of_audio> audio_sequence <end_of_audio>\" }, { } ] \"role\": \"assistant\", \"content\": \"text_sequence_1 <begin_of_audio> audio_sequence_1 <end_of_audio> text_sequence_2 <begin_of_audio> audio_sequence_2 <end_of_audio>\" 3.4 Inference As shown in Fig. 4, we have designed four different inference paradigms to address various scenarios. For the ASR and TTS tasks, we propose VITA-Audio-Turbo. In each forward pass, the LLM generates one token, followed by the generation of ten tokens by the MCTP modules. This paradigm is the most efficient among the options; however, the performance of speech dialogue tasks degrades due to the need to predict the text token. For speech dialogue tasks, we introduce VITA-Audio-Speed and VITA-Audio-Balance. Their main difference lies in audio token generation: VITA-Audio-Speed generates eight directly decodable audio tokens in the first forward pass, whereas VITA-Audio-Balance adheres to strict 1 : 2 text-to-audio token ratio for enhanced speech quality. The latter requires two forward passes to generate enough audio tokens for decoding. To optimize model performance, distinct models were trained for each of the two modes. VITA-Audio-Vanilla is designed for scenarios that require higher language performance. It generates tokens solely using the main LLM, sacrificing efficiency but offering slight performance boost."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experiment Settings We use the pretrained Qwen2.5-7B [45] as the pre-trained text LLM. The initial version of VITAAudio utilizes the speech tokenizer and speech decoder in GLM-4-Voice [66], which effectively captures semantic information at an ultra-low bitrate. In the second version, i.e., VITA-Audio-Plus further replaces the GLM-4-Voice tokenizer with SenseVoiceSmall [1] and an MLP-based adapter. The detail comparison between VITA-Audio and VITA-Audio-Plus is listed in Table 3. Throughout the training process, the global batch size is set to 256. In Stage 1, the learning rate for the LLM is 6e5. In Stages 2 and 3, the learning rate for the MCTP is 1e3. In Stage 4, the learning rate for the LLM is 5e5, while the learning rate for the MCTP is 4e5. The model is trained for 1 epoch. We use the AdamW [37] optimizer with β1 = 0.9 and β2 = 0.95. We evaluate VITA-Audio performance on ASR, TTS, and SQA tasks, respectively. 9 Table 4: Results on Spoken Question Answering (SQA) benchmarks. Sx denotes the x-th training stage of speech models. Model MinMo Moshi GLM-4-Voice LUCY (S2) [9] [18] [66] [27] VITA-Audio-Boost VITA-Audio-Vanilla VITA-Audio-Plus-Vanilla #Params Llama Question TriviaQA Web Question Mean 7B 7B 9B 7B 7B 7B 7B Proprietary Models 78.9 64.1 48.3 37. 55.0 39.9 60.7 47.2 Open-source Models 62.3 64.7 59. 68.7 71.3 75.6 21.0 50.7 51.0 60.3 66.3 68.0 22.8 39.1 23.2 30.5 31.9 45.9 7.3 26.5 18. 29.3 30.1 42.7 26.6 55.0 26.6 32.9 33.5 45.0 9.2 39.9 18.2 30.4 31.4 41.7 37.2 45.3 36. 44.0 45.6 55.5 12.5 31.0 29.1 40.0 42.6 50.8 Table 5: Results on Text to Speech (TTS) Benchmarks. Sx denotes the x-th training stage. Model Seed-TTS CosyVoice CosyVoice2 VITA-1.5 (S3) GLM-4-Voice [2] [20] [21] [24] [66] VITA-Audio-Turbo (S1) VITA-Audio-Turbo (S2) VITA-Audio-Turbo (S3) VITA-Audio-Turbo VITA-Audio-Plus-Vanilla LibriTTS test-clean CER (%) WER (%) WER (%) WER (%) Seed-TTS test-en test-hard test-zh 1.12 3.63 1.45 8.44 2.91 1.18 0.96 1.05 1.07 1.13 2.25 4.29 2.57 2.63 2.10 1.92 1.92 1.77 2.26 1.85 7.59 11.75 6.83 10.58 9.72 9.86 10.08 10. 2.89 2.47 5.64 1.96 1.98 1.99 2.08 1.89 4.2 Evaluation on Spoken Question Answering We evaluate the spoken question answering capability of VITA-Audio on three public English datasets: Web-Questions [5], Llama-Question [41], and TriviaQA [31]. Two evaluation methods are employed: ST, where the text responses generated by the model are evaluated directly, and SS, where the models speech responses are transcribed using Whisper [46] before evaluation. We compare VITA-Audio with the latest speech models that have comparable parameter sizes, and the results are shown in Table 4. Our model demonstrates superior performance in the ST task and achieves SOTA results in the SS setup. It is particularly noteworthy that the training approach of VITA-Audio ensures minimal degradation between ST and SS, with performance drop of only 9%. This indicates that VITA-Audio achieves high-quality alignment between text and speech modalities, with benefits extending beyond processing speed alone. 4.3 Evaluation on Fundamental Speech Competence TTS We evaluate the TTS performance of VITA-Audio on Seed-TTS [2] and LibriTTS [65] benchmarks. We use Whisper-Large-V3 [46] and Paraformer[28] to transcribe into text the generated English and Chinese speech, respectively. For all VITA-Audio models, we use the following prompt during inference. 10 Table 6: Results on Automatic Speech Recognition (ASR) Benchmarks. Sx denotes the x-th training stage. Compared to other methods, VITA-Audio is trained with open-source data only."
        },
        {
            "title": "Model",
            "content": "LibriSpeech [43] test-clean test-other Fleurs [14] zh en Qwen2-Audio-base Baichuan-Audio-base Freeze-Omni VITA-1.5 (S3) LUCY (S1) Step-Audio-chat Qwen2.5-Omni [12] [35] [57] [24] [27] [50] [60] VITA-Audio-Turbo VITA-Audio-Vanilla VITA-Audio-Plus-Vanilla (S1) VITA-Audio-Plus-Vanilla Qwen2-Audio-base Baichuan-Audio-base Step-Audio-chat Qwen2.5-Omni [12] [35] [50] [60] VITA-Audio-Plus-Vanilla WER (%) 1.74 4.04 3.02 6.04 3.82 9.79 8.14 18.41 3.36 8.05 3.19 10.67 2.37 4.21 6.29 12.86 2.98 8.07 1.91 4.29 2.00 4.60 3.63 5.20 4.15 8.07 4.26 8.56 2.92 4. 3.69 4.54 Prompt for TTS task. { } \"messages\": [ { \"role\": \"user\", \"content\": \"Convert the text to speech.ntext_sequence\" } ] We present the results of VITA-Audio at each stage in Table 5. The experiments demonstrate that VITA-Audio outperforms other open-source models with similar number of parameters. Additionally, it should be noted that, despite using ten MCTP modules for accelerated inference, VITAAudios TTS capabilities are largely preserved throughout the training process, further validating the effectiveness of the MCTP module in aligning text and audio. ASR We evaluate the ASR performance of the four stages of VITA-Audio on WenetSpeech [67], AIshell [6], LibriSpeech [43] and Fleurs [14], and the results are reported in Table 6 and Table 7. We follow the original data splits for each benchmark. For all VITA-Audio models, we use the following prompt during inference. Prompt for ASR task. { } \"messages\": [ { \"role\": \"user\", \"content\": \"Convert the speech to text.n <begin_of_audio> audio_sequence <end_of_audio>\" } ] 11 Table 7: Results on Automatic Speech Recognition (ASR) Benchmarks. Sx denotes the x-th training stage. Compared to other methods, VITA-Audio is trained with open-source data only."
        },
        {
            "title": "Model",
            "content": "WER (%) AISHELL-1 [6] AISHELL-2 ios [19] WenetSpeech [67] test-meeting test-net Qwen2-Audio-base Baichuan-Audio-base Freeze-Omni LUCY (S1) Step-Audio-chat Qwen2.5-Omni [12] [35] [57] [27] [50] [60] VITA-Audio-Turbo VITA-Audio-Vanilla VITA-Audio-Plus-Vanilla (S1) VITA-Audio-Plus-Vanilla Qwen2-Audio-base Baichuan-Audio-base Step-Audio-chat Qwen2.5-Omni VITA-Audio-Plus-Vanilla Qwen2-Audio-base Baichuan-Audio-base Freeze-Omni VITA-1.5 (S3) LUCY (S1) Step-Audio-chat Qwen2.5-Omni [12] [35] [50] [60] [12] [35] [57] [24] [27] [50] [60] VITA-Audio-Turbo VITA-Audio-Vanilla VITA-Audio-Plus-Vanilla (S1) VITA-Audio-Plus-Vanilla 1.52 1.93 2.48 2.40 2.14 1.13 7.70 4.46 1.51 1.94 3.08 3.87 3.89 2.56 3.29 8.40 7.64 13.28 10.13 11.80 13.46 8.14 18.41 10.42 10.42 10.83 9.47 7.71 6. 18.66 23.97 13.45 17.34 6.68 6.59 7.12 6.90 Figure 5: Token generation speed curves of four text-audio interleaved modes. The results for other works are partially reproduced from their respective original works for comparison. In these results, the output of VITA-Audios first stage (S1) consists of text tokens directly generated by the LLM, while the outputs of the second (S2), third (S3), and fourth (S4) stages are alternately generated by both the LLM and the MCTP module. Although the ASR performance of VITA-Audio does not demonstrate significant advantages, the ASR capability in S3 remains largely consistent with that of S1, indicating that VITA-Audio does not establish unidirectional text-to-audio mapping. Instead, it constructs an alignment relationship between the text and speech modalities, facilitating efficient mapping between them. 12 Table 8: Boostup Ratio under Different Inference Paradigms."
        },
        {
            "title": "Model Size",
            "content": "#GPU Total Second Token Per Second Speedup"
        },
        {
            "title": "Vanilla\nBoost\nBalance\nTurbo",
            "content": "0.5B 7B 72B 1 1 53.89 20.65 20.71 11.83 63.38 23.97 23.94 13.43 255.13 84.98 85.13 39.5 76.00 198.35 197.78 346.24 64.62 170.88 171.09 304.99 16.05 48.20 58.11 103. 1.00 2.61 2.60 4.56 1.00 2.64 2.64 4.72 1.00 3.00 3.00 6.46 4.4 Evaluation of Latency Inference Speedup Efficient mapping between text and speech is the core of VITA-Audio. To demonstrate its effectiveness, we compare the inference time across different modes of VITAAudio for various model sizes. Specifically, we evaluate the inference speed on GPUs capable of 148 TFLOPS under bfloat16 precision, with the output fixed at 4096 tokens, and record the total time as the models inference time. All models, regardless of size, are randomly initialized, and this initialization do not affect the inference time measurements. We use Transformers [58] and FlashAttention-2 [16]. As mentioned in Section 3.4, VITA-Audio-Vanilla only uses the main model for output; VITAAudio-Turbo uses both the main model and all the MCTP modules during each forward pass; and VITA-Audio-Speed and VITA-Audio-Balance progressively increase the number of MCTP modules used to ensure higher accuracy. As shown in Table 8, we present comparison of the time consumption for different inference modes and model sizes. Noted that the time consumption does not include the cost of the speech encoder and speech decoder. We observe that in VITA-Audio-Turbo, speedup of approximately 5 is achieved across models ranging from 0.5B to 72B, greatly enhancing the output token throughput. VITA-Audio-Speed also achieves around 3 acceleration across various sizes, resulting in desirable performance for real-time speech dialogue systems. For example, 72 VITA-Audio generates approximately 50 tokens per second, which, excluding generated text tokens, corresponds to roughly three seconds of audio and associated text when using 12.5Hz speech tokenizer. This performance is sufficiently fast for human-computer interaction. Latency In human-computer interaction, latency is crucial metric, as it determines whether users can interact with the model in real-time. Given that most speech models support streaming output, the key to reducing perceived latency lies in shortening the time required to generate the first chunk of audio. We visualize the timeline of model decoding phrase in Fig. 5. The green marks denote the tokens generated by the main model, and the blue marks are the tokens generated by MCTP modules. We set the number of prefiil tokens to 32. And Fig. 5 shows that VITA-Audio-Turbo completes the generation of the first audio chunk in about 50 ms, while VITA-Audio-Vanilla requires about 220 ms. VITA-Audio-Speed and VITA-Audio-Balance generate fewer audio tokens in the first forward and more text audio tokens in the following forward. Thus, they are slower than VITA-Audio-Turbo but still significantly faster than VITA-Audio-Vanilla. Thanks to the advantage of zero audio generation delay, VITA-Audio produces multiple audio tokens in the first forward pass, allowing the first audio token chunk to be generated during the initial forward pass, which can then be used for decoding. This significantly reduces the perceived delay. In the 13 Table 9: Generation time (ms) of the first audio segment under different inference modes in streaming inference. To enable more real-time speech generation, we progressively increase the number of steps in the flow matching model during streaming inference. The table shows the decoding time when the sampling step of the flow matching model is set to 1."
        },
        {
            "title": "Boost\nVanilla",
            "content": "39 39 53 236 151 151 243 426 experimental environment previously mentioned, VITA-Audio reduces the time to generate the first audio token chunk from 236 to 53 ms, as shown in Table 9."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce VITA-Audio, lightweight framework that uses separate efficient modules, named Multiple Cross-modal Token Prediction (MCTP) modules, to efficiently generate audio responses from text embeddings and LLM hidden states. MCTP learns the simple mapping relationship between text hidden states and audio tokens with relatively simple modules and without relying on the extensive semantic modeling of LLMs. Our model achieves new state-of-the-art performance on multiple benchmarks for ASR, TTS, and SQA tasks, outperforming existing models in efficiency and accuracy, especially the open-source ones with similar parameter scale. Therefore, it sets new standard for real-time speech-to-speech models."
        },
        {
            "title": "References",
            "content": "[1] Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, Shengpeng Ji, Yabin Li, Zerui Li, Heng Lu, Haoneng Luo, Xiang Lv, Bin Ma, Ziyang Ma, Chongjia Ni, Changhe Song, Jiaqi Shi, Xian Shi, Hao Wang, Wen Wang, Yuxuan Wang, Zhangyu Xiao, Zhijie Yan, Yexin Yang, Bin Zhang, Qinglin Zhang, Shiliang Zhang, Nan Zhao, and Siqi Zheng. FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs. arXiv:2407.04051, 2024. 9 [2] Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, Mingqing Gong, Peisong Huang, Qingqing Huang, Zhiying Huang, Yuanyuan Huo, Dongya Jia, Chumin Li, Feiya Li, Hui Li, Jiaxin Li, Xiaoyang Li, Xingxing Li, Lin Liu, Shouda Liu, Sichao Liu, Xudong Liu, Yuchen Liu, Zhengxi Liu, Lu Lu, Junjie Pan, Xin Wang, Yuping Wang, Yuxuan Wang, Zhen Wei, Jian Wu, Chao Yao, Yifeng Yang, Yuanhao Yi, Junteng Zhang, Qidi Zhang, Shuo Zhang, Wenjie Zhang, Yang Zhang, Zilin Zhao, Dejian Zhong, and Xiaobin Zhuang. Seed-TTS: Family of High-Quality Versatile Speech Generation Models. arXiv:2406.02430, 2024. 10 [3] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign: Recipe for Long Context Alignment of Large Language Models. 2024. 6, 7 [4] Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs. 2024. 6, [5] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013. 10 [6] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. AISHELL-1: An Open-Source Mandarin Speech Corpus and Speech Recognition Baseline. Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA), 2018. 6, 7, 11, 12 [7] William Chan, Navdeep Jaitly, Quoc Le, and Vinyals Google Brain. Listen, Attend and Spell. 2015. 2 [8] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Zhao You, and Zhiyong Yan. GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2021. 6, 7 14 [9] Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, Yabin Li, Xiang Lv, Jiaqing Liu, Haoneng Luo, Bin Ma, Chongjia Ni, Xian Shi, Jialong Tang, Hui Wang, Hao Wang, Wen Wang, Yuxuan Wang, Yunlan Xu, Fan Yu, Zhijie Yan, Yexin Yang, Baosong Yang, Xian Yang, Guanrou Yang, Tianyu Zhao, Qinglin Zhang, Shiliang Zhang, Nan Zhao, Pei Zhang, Chong Zhang, and Jinren Zhou. MinMo: Multimodal Large Language Model for Seamless Voice Interaction. arXiv:2501.06282, 2025. 3, 4, [10] Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, Xiquan Li, Ruiyang Xu, Zhikang Niu, Yanqiao Zhu, Yifan Yang, Zhanxun Liu, Kai Yu, Yuxuan Hu, Jinyu Li, Yan Lu, Shujie Liu, and Xie Chen. SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training. arXiv:2412.15649, 2024. 2, 4 [11] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. LongLoRA: Efficient Fine-Tuning of Long-Context Large Language Models. 2023. 6, 7 [12] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-Audio Technical Report. arXiv:2407.10759, 2024. 11, 12 [13] Together Computer. Long Data Collections, 2023. 6, 7 [14] Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech. 2022 IEEE Spoken Language Technology Workshop, SLT 2022 - Proceedings, 2023. 11 [15] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free Dolly: Introducing the Worlds First Truly Open Instruction-Tuned LLM, 2023. 6, 7 [16] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. The International Conference on Learning Representations (ICLR), 2023. 13 [17] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. DeepSeek-V3 Technical Report. 2024. 6 [18] Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: Speech-Text Foundation Model for Real-Time Dialogue. arXiv:2410.00037, 2024. 3, 4, 10 [19] Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. AISHELL-2: Transforming Mandarin ASR Research into Industrial Scale. arXiv:1808.10583, 2018. 6, 12 [20] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhifu Gao, and Zhijie Yan. CosyVoice: Scalable Multilingual Zero-Shot Text-To-Speech Synthesizer Based on Supervised Semantic Tokens. arXiv:2407.05407, 2024. 4, [21] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, and Jingren Zhou. CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models. arXiv:2412.10117, 2024. 4, 10 15 [22] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. LLaMA-Omni: Seamless Speech Interaction with Large Language Models. arXiv:2409.06666, 2024. 2, 4 [23] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, and Xing Sun. VITA: Towards Open-Source Interactive Omni Multimodal LLM. arXiv:2408.05211, 2024. 4 [24] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, and Ran He. VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction. arXiv:2501.01957, 2025. 10, 11, [25] Yihui Fu, Luyao Cheng, Shubo Lv, Yukai Jv, Yuxiang Kong, Zhuo Chen, Yanxin Hu, Lei Xie, Jian Wu, Hui Bu, Xin Xu, Jun Du, and Jingdong Chen. AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2021. 6, 7 [26] Daniel Galvez, Greg Diamos, Juan Ciro, Juan Felipe Cerón, Keith Achorn, Anjali Gopi, David Kanter, Maximilian Lam, Mark Mazumder, and Vijay Janapa Reddi. the Peoples Speech: Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage. 2021. 6, 7 [27] Heting Gao, Hang Shao, Xiong Wang, Chaofan Qiu, Yunhang Shen, Siqi Cai, Yuchen Shi, Zihan Xu, Zuwei Long, Yike Zhang, Shaoqi Dong, Chaoyou Fu, Ke Li, Long Ma, and Xing Sun. LUCY: Linguistic Understanding and Control Yielding Early Stage of Her. arXiv:2501.16327, 2025. 3, 4, 6, 7, 10, 11, 12 [28] Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan. Paraformer: Fast and Accurate Parallel Transformer for Non-Autoregressive End-To-End Speech Recognition. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2022. 10 [29] Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, Yuancheng Wang, Kai Chen, Pengyuan Zhang, and Zhizheng Wu. Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation. arXiv:2501.15907, 2024. 6, 7 [30] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Yuexian Zou, Zhou Zhao, and Shinji Watanabe. AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head. In AAAI Conference on Artificial Intelligence (AAAI), 2023. 1, [31] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2017. 10 [32] Suyoun Kim, Takaaki Hori, and Shinji Watanabe. Joint CTC-Attention Based End-To-End Speech Recognition Using Multi-Task Learning. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017. 2 [33] Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze. LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction. 2023. 6, 7 [34] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society. 2023. 6, 7 [35] Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, Jianhua Xu, Haoze Sun, Zenan Zhou, and Weipeng Chen. Baichuan-Audio: Unified Framework for End-To-End Speech Interaction. arXiv:2502.17239, 2025. 11, [36] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. International Conference on Machine Learning (ICML), 2024. 6 [37] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In The International Conference on Learning Representations (ICLR), 2019. 9 [38] Linhan Ma, Dake Guo, Kun Song, Yuepeng Jiang, Shuai Wang, Liumeng Xue, Weiming Xu, Huan Zhao, Binbin Zhang, and Lei Xie. WenetSpeech4TTS: 12,800-Hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2024. 6, 7 [39] Ltd. Magic Data Technology Co. MAGICDATa Mandarin Chinese Read Speech Corpus, 2019. 6, [40] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-Math: Unlocking the Potential of SLMs in Grade School Math. 2024. 6, 7 [41] Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, R. J. Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken Question Answering and Speech Continuation Using Spectrogram-Powered Llm. In The International Conference on Learning Representations (ICLR), 2024. 10 16 [42] Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta Costa-Jussa B,+, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Mary Williamson, Gabriel Synnaeve, Juan Pino, Benoît Sagot, Emmanuel Dupoux, and Meta Ai. Spirit LM: Interleaved Spoken and Written Language Model. 2024. 4 [43] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR Corpus Based on Public Domain Audio Books. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. 6, 7, [44] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: Large-Scale Multilingual Dataset for Speech Research. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2020. 6, 7 [45] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report. 2024. 9 [46] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust Speech Recognition Via Large-Scale Weak Supervision. International Conference on Machine Learning (PMLR), 2022. 10 [47] Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Rongrong Ji, and Xing Sun. Long-VITA: Scaling Large Multi-Modal Models to 1 Million Tokens with Leading Short-Context Accuracy. arXiv:2502.05177, 2025. 7 [48] Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li. AISHELL-3: Multi-Speaker Mandarin TTS Corpus and the Baselines. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2020. 6 [49] Hubert Siuzdak, Florian Grötschla, and Luca A. Lanzendörfer. SNAC: Multi-Scale Neural Audio Codec. arXiv:2410.14411, 2024. 3 [50] Step-audio Team. Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction. arXiv:2502.11946, 2024. 4, 11, 12 [51] Teknium. OpenHermes 2.5: An Open Dataset of Synthetic Data for Generalist LLM Assistants, 2023. 6, 7 [52] Liu Tiedong. Goat, 2023. 6, 7 [53] Atlas Unified. Atlas math sets, 2023. 6, [54] Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. VoxPopuli: Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. In International Joint Conference on Natural Language Processing (IJCNLP), 2021. 6, 7 [55] Peng Wang, Songshuo Lu, Yaohua Tang, Sijie Yan, Yuanjun Xiong, Wei Xia, and Mthreads Ai. Full-Duplex Speech Dialogue Scheme Based on Large Language Models. 1, 4 [56] Wenbin Wang, Yang Song, and Sanjay Jha. GLOBE: High-Quality English Corpus with Global Accents for Zero-Shot Speaker Adaptive Text-To-Speech. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2024. 6, 7 [57] Xiong Wang, Yangze Li, Chaoyou Fu, Lei Xie, Ke Li, Xing Sun, and Long Ma. Freeze-Omni: Smart and Low Latency Speech-To-Speech Dialogue Model with Frozen LLM. arXiv:2411.00774, 2024. 2, 3, 4, 11, 12 [58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. HuggingFaces Transformers: State-Of-The-Art Natural Language Processing. 2019. [59] Zhifei Xie and Changqiao Wu. Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming. arXiv:2408.16725, 2024. 2, 3, 4, 6, 7 [60] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-Omni Technical Report. arXiv:2503.20215, 2025. 11, 12 [61] Jianxin Yang. LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. 2023. 6, 7 [62] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap Your Own Mathematical Questions for Large Language Models. The International Conference on Learning Representations (ICLR), 2024. 6, 7 [63] Yijiong Yu. \"Paraphrasing the Original Text\" Makes High Accuracy Long-Context QA. 2023. 6, 7 [64] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building Math Generalist Models Through Hybrid Instruction Tuning. The International Conference on Learning Representations (ICLR), 2024. 6, 7 [65] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. Libritts: Corpus Derived from LibriSpeech for Text-To-Speech. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2019. 6, 7, 10 [66] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. GLM-4-Voice: Towards Intelligent and Human-like End-To-End Spoken Chatbot. arXiv:2412.02612, 2024. 2, 3, 4, 9, 10 [67] Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, Di Wu, and Zhendong Peng. WenetSpeech: 10000+ Hours Multi-Domain Mandarin Corpus for Speech Recognition. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. 6, 7, 11, [68] Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li. LongCite: Enabling LLMs to Generate Fine-Grained Citations in Long-Context QA. 2024. 6, 7 [69] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, and Jiaqi Wang. InternLM-XComposer2.5-OmniLive: Comprehensive Multimodal System for Long-Term Streaming Video and Audio Interactions. 2024. 1, 4 [70] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less Is More for Alignment. 2023. 6,"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Tencent Youtu Lab",
        "Xiamen University"
    ]
}