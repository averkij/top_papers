{
    "paper_title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
    "authors": [
        "Yushi Wang",
        "Changsheng Luo",
        "Penghui Chen",
        "Jianran Liu",
        "Weijian Sun",
        "Tong Guo",
        "Kechang Yang",
        "Biao Hu",
        "Yangang Zhang",
        "Mingguo Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control. We introduce an encoder-decoder architecture combined with a virtual perception system that models real-world visual characteristics, allowing the policy to recover privileged states from imperfect observations and establish active coordination between perception and action. The resulting controller demonstrates strong reactivity, consistently executing coherent and robust soccer behaviors across various scenarios, including real RoboCup matches."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 6 9 9 3 0 . 1 1 5 2 : r Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots Yushi Wang1,2,, Changsheng Luo1, Penghui Chen1,2,, Jianran Liu2, Weijian Sun2, Tong Guo1, Kechang Yang3, Biao Hu3, Yangang Zhang2, Mingguo Zhao1, 1Tsinghua University, 2ByteDance Seed, 3China Agricultural University Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Humanoid soccer poses representative challenge for embodied intelligence, requiring robots to operate within tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control. We introduce an encoder-decoder architecture combined with virtual perception system that models real-world visual characteristics, allowing the policy to recover privileged states from imperfect observations and establish active coordination between perception and action. The resulting controller demonstrates strong reactivity, consistently executing coherent and robust soccer behaviors across various scenarios, including real RoboCup matches. Date: November 7, 2025 Correspondence: Mingguo Zhao at mgzhao@tsinghua.edu.cn Project Page: https://humanoid-kick.github.io"
        },
        {
            "title": "Introduction",
            "content": "Soccer stands as globally celebrated sport that epitomizes the pinnacle of human sensorimotor intelligence. Mastery of the game demands not only physical prowess but also an intuitive perception-action coupling that enables seamless fusion of acute visual perception, rapid decision-making, and adaptive sensorimotor coordination. Human players track fast-moving ball, anticipate opponents actions, adjust their posture mid-stride, and strike with precise timing and force. These capabilities are precisely what humanoid robots require to operate autonomously in unstructured, dynamic environments. Unlike benchmarks that target either locomotion or manipulation skills, robot soccer serves as focused testbed that compresses broad spectrum of essential robotic competencies into single continuous task, offering practical arena that drives technological advances toward human-level embodied intelligence in the real world. However, reproducing even the most basic soccer skills on humanoid robots remains substantial challenge. The robot is required to generate long-horizon behaviors that satisfy complex task demands while continuously adapting its motion in real time to rapidly changing sensory feedback. These challenges are further amplified 1 by onboard visual perception that is noisy, delayed, and spatially constrained during dynamic motion, leading to distorted or incomplete observations of the environment state. Meanwhile, the controller must remain robust and generalizable when exposed to disturbances introduced by competitive physical interactions and visually cluttered environments. To overcome these obstacles, the robot requires advanced locomotion capabilities, reliable visual tracking of dynamic targets, and rapid adjustment of foot placement and body orientation, ultimately achieving successful strikes through precise synchronization of timing, angle, and force. Achieving such tightly coupled perception-action competence goes far beyond conventional locomotion control. Robot soccer has attracted substantial research attention, particularly under the auspices of RoboCup [1, 2]. Early learning-based approaches typically targeted isolated skills such as running [3], dribbling [47], shooting [8], or goalkeeping [9], while others focused on high-level strategic planning like team coordination [10, 11]. Such frameworks often separate low-level motor skills from tactical decision-making and rely on manually designed components, leading to decoupled systems that struggle to generate agile and coherent behaviors. notable shift occurred with the introduction of unified policy trained through reinforcement learning (RL) to acquire long-horizon humanoid soccer behaviors [12]. This paradigm was later extended to vision-based control by incorporating egocentric perception rendered through Neural Radiance Fields (NeRF) [13] during training [14]. While these approaches enabled compound behaviors, the resulting controllers exhibited reduced reactivity to the ball and relied heavily on specifically modeled visual environments, restricting generalization to real match conditions. Despite these efforts, vision-driven reactive controller capable of coherent and robust soccer behaviors in unconstrained environments remains an open challenge. Various methods have been explored to enhance the capabilities of humanoid robots. RL has driven remarkable advancements in robust and agile behaviors, including running [15], jumping [16], parkour [17], and fall recovery [18, 19]. Recently, learning from demonstrations has emerged as popular framework, empowering humanoid robots to replicate specific human behaviors in realistic, human-like manner, such as stylized walking, dancing, leg kicks, and acrobatic flips [2025]. This approach leverages expert demonstrations to replace part of manually engineered reward functions, reducing exploration complexity by providing structured guidance that biases policy learning toward desired behaviors [26]. The most widely adopted methods in these works involved constructing dense tracking rewards based on hand-crafted features of demonstrated motions frame-aligned with the policy, framework established by DeepMimic [27] that enabled high-fidelity replication of motion details. However, these feature-based methods relied heavily on explicit motion matching, making them less adaptable when deviations from reference trajectories were needed to achieve task-specific objectives, and rigid temporal alignment also limited their flexibility in dynamic environments. Recent advances have sought to address these limitations by using supervised distillation to remove reference dependencies [28], or training high-level policies to generate reference trajectories [29] or latent motion embeddings [4, 3032], yet the training process remained cumbersome, typically requiring at least three training stages. In contrast, GAN-based methods [3337] enabled flexible integration of task-specific objectives with imitation. They leveraged discriminator to distinguish short motion transitions from the policy versus expert demonstrations, thereby providing an implicit reward signal without the need for explicit temporal alignment. Recent advancements [38] introduced Wasserstein GAN [39] to mitigate limitations like discriminator saturation and mode collapse. Despite their promise, existing GAN-based methods primarily focused on motion imitation using proprioception in static scenarios, overlooking the utilization of exteroceptive sensors required in dynamic tasks such as robot soccer. To address these challenges in robot soccer, we develop unified controller that tightly integrates visual perception into the control loop to achieve reactive soccer skills. The robot is trained via RL to kick the ball toward the goal in simulated soccer field, while Adversarial Motion Priors (AMP) [33] guide it toward expert-like behaviors. The controller processes historical observation through an encoder to compress visual cues into control-relevant latent representation for perception-action coupling, as well as decoder to recover privileged states from partial noisy inputs. To foster robust and active perception and bridge the sim-to-real gap, virtual perception system is introduced in simulation to emulate key characteristics of onboard vision and provide compact detection cues to the policy. Our work demonstrated that GAN-based motion learning can be effectively extended beyond proprioceptive imitation to real-world dynamic environments involving visual feedback and perception-action coordination. With single-stage training process, the robot acquired 2 Figure 1 System overview. The real-world robot is equipped with an onboard camera for visual perception. Image detections are projected into the BEV space. Ball detections are provided directly to the policy, while field landmarks are processed by an odometer module to infer the goal location from long-term information. The perception pipeline is designed to efficiently extract and represent visual features for the RL policy. adaptive and coherent behaviors, including ball searching, chasing, and multi-directional kicking, without manual skill segmentation, while responding reactively to visual inputs. In addition, multi-critic framework is adopted to mitigate interference between reward objectives and stabilize training. Extensive experiments validated the controllers ability to execute agile and accurate kicking behaviors across diverse scenarios using only onboard vision, effectively bridging the gap between limited visual perception and human-like soccer performance. By dynamically adjusting its gait to the balls position, the policy achieved fast, powerful shots while maintaining stability throughout movement. The design of the encoder-decoder network and the virtual perception system equipped the policy to mitigate perceptual noise and predict ball movement, enabling highly reactive coordination behaviors. As the champion team of the Adult-size Humanoid League in RoboCup 2025 and the 2025 World Humanoid Robot Games, we further showcased the practical efficacy of this method in real-world competitive settings, where the robot operated under strict constraints yet delivered strong performance."
        },
        {
            "title": "2 Results",
            "content": "We evaluated the proposed learning framework through extensive simulations and real-world experiments. The results demonstrated that the learned policy enabled reactive soccer skills, seamlessly integrating real-time ball tracking, agile locomotion, and accurate kicking, while maintaining robust performance across diverse scenarios. We further analyzed emergent perceptual and gait behaviors to reveal how the policy coupled visual cues with motion control."
        },
        {
            "title": "2.1 System overview",
            "content": "We deployed our policy on the Booster T1, humanoid robot measuring approximately 1.2 in height and 30 kg in mass. Booster T1 has been used as humanoid platform for locomotion control [19], loco-manipulation 3 [40, 41], navigation [42], and the development of RL algorithms [43] and suites [4446]. For the present work, no specific modifications were made to the robots original hardware. The robot is equipped with an Intel RealSense Depth Camera D435i, mounted on its head with two joints to control its yaw and pitch orientation during dynamic movements, allowing for active ball and field perception. We trained the soccer policy in simulation and deployed it directly onto the real robot. As illustrated in Fig. 1, the camera continuously updates images at 25 Hz, which undergo detection processing before being projected into Birds Eye View (BEV) space on the onboard Jetson AGX Orin. The policy receives proprioception data, ball detections, and goal information from separate odometry module, generating joint position commands at frequency of 50 Hz."
        },
        {
            "title": "2.2 Soccer skills performance",
            "content": "We deployed the proposed controller across diverse range of scenarios to assess its ability to execute robust reactive soccer skills, as shown in Fig. 2. The evaluation encompassed various surface types, including grass, slabstone, soil, asphalt, and rubber, each characterized by distinct terrain properties that led to varied interaction dynamics. In addition, visual conditions such as color and illumination varied significantly across scenarios, occasionally causing detection failures. Notably, the controller was trained entirely in simulation without incorporating data from these environments and was deployed to the real world zero-shot. Despite these challenges, the controller demonstrated robust adaptation, consistently achieving reliable ball tracking and locomotion. Even in scenarios lacking field landmarks, the robot sustained robust soccer performance using proprioceptive odometry. We further validated the systems ability to sustain extended rounds. As demonstrated in the movie, the policy successfully adapted to multiple balls placed at various locations on the field, enabling continuous kicking with diverse and coherent behaviors. In interactive matches against human goalkeeper, the robot adjusted its movements to maintain consistent contact with the ball, continuously tracking and executing shots, and even occasionally overcoming the human defense. When ball tracking was temporarily lost, the robot exhibited active visual search behaviors, thereby ensuring continuity in kicking and enabling long-duration interactions. The proposed controller was adopted as module by the Tsinghua Hephaestus team in both the RoboCup 2025 Adult-size Humanoid League and the 2025 World Humanoid Robot Games, where the team won championships with 76 goals scored and only 11 conceded. In these competitions, robots were required to operate fully autonomously, relying exclusively on onboard computation and sensing, while the use of sensors exceeding human sensory capabilities was strictly prohibited, such as LiDAR or multiple camera systems. Moreover, adaptation time to the actual competition venue was highly limited. These factors collectively presented substantial challenges to the deployment of our method. Despite these constraints, our approach demonstrated its effectiveness and agility, contributing to multiple successful goals during the competitions. By enabling agile kicking motions, this approach allowed the robot to gain an advantage in ball possession against opponents. The method also exhibited robust performance in challenging scenarios, including accurate long-range shots from the touchline midpoint and reliable operation despite visual occlusion and external physical disturbances."
        },
        {
            "title": "2.3 Kicking success rates",
            "content": "We evaluated the proposed policy by measuring success rates across different regions of the soccer field, following the Adult-size specifications of the RoboCup Humanoid League. At the start of each trial, the ball was placed at predefined positions on the field, while the robot was always initialized at the center. trial was considered successful if the robot kicked the ball into the goal, and failed if the robot fell or the ball went out of bounds. If the ball remained in play, the robot was permitted to attempt additional kicks. Consecutive trials were conducted for each region, and the resulting success rates are summarized in Fig. 3A. We first evaluated the policy in simulation. To ensure consistency, external disturbances such as uneven terrain and physical perturbations to the ball or robot were removed, while the virtual perception system modeled after real-world visual characteristics was retained. This setup allowed for comprehensive evaluation of the vision-driven pipeline, from perception to motor execution, requiring the robot to handle perceptual noise, accurately localize the ball, and generate appropriate kicking motions. Results demonstrated the high reliability of the policy in executing soccer skills. In regions close to the goal, success rates were particularly 4 Figure 2 Performance of the controller in various scenarios. (A to F) Real match performance in cluttered environments with disturbances. (G to I) Reactive responses and real-time adaptation to the ball. (J to L) Robust behavior across varying terrain and visually diverse environments. high, with failures occurring only rarely. Performance declined as the distance from the goal or angular offsets relative to the goals normal direction increased. This reduction was primarily due to tighter tolerance for kicking angle accuracy, especially in backfield regions, where angular deviations exceeding 10 resulted in missed shots. In addition, success rates were lower when the ball was positioned directly along the line Figure 3 Validation and behavior analysis. (A) The background grid color represents the success rate of 8192 simulation tests, while the dots indicate the success rate of 10 consecutive hardware tests. Owing to effective alignment, our policy delivers reliable hardware performance that closely matches simulation results. (B) The robot searches for the ball in the distance when starting near the field edge, guided by the policys estimated ball position. (C) The robot turns to search for the ball behind itself when it is near the field center. (D and E) The robots foothold locations and timing reveal adaptive gait with shorter strides and faster cadence, enabling effective adjustment before kicking, as illustrated by the forward and backward kick examples. between the robot and the goal, since the robot struggled to select the appropriate kicking foot, and also when the ball was placed very close to the robot, as the limited time constrained accurate ball localization and gait adaptation. Nevertheless, the robot achieved successful kicks in most of the tested regions. 6 Figure 4 Perception-action coordination. (A) Training curves for different methods, reporting overall success rates in disturbed training environments. (B) Proportion of ball perception, average perception error, and policys ball position estimation error across 4096 kicking tests. (C) Distribution of angular distance between the ball and the camera center, evaluated over 1000 steps across 2048 environments. The shaded area indicates the cameras FOV. (D and E) The policys estimation predicts the balls movement when the ball is removed from the robots FOV, guiding the robot to re-acquire visual detection of the ball in the direction it disappeared. We further validated the policy on the physical robot. Hardware experiment achieved success rates comparable to those in simulation, with the robot maintaining high performance across all tested positions, including the challenging backfield regions. These findings confirmed that the virtual perception system effectively bridged the gap between simulation and the real world. Notably, the robot remained stable throughout all trials, with no falls recorded, underscoring the robustness of the proposed policy. These results collectively demonstrated the effectiveness of our controller in real-world settings."
        },
        {
            "title": "2.4 Perception-action coordination",
            "content": "We investigated the policys perception-action coordination, emphasizing its active adjustment of torso and head orientations to sustain robust ball tracking under partially observable perceptual conditions. As illustrated in Fig. 4C, this behavior enabled the policy to keep the ball within the cameras field of view (FOV) for the majority of the time. Remarkably, this active perceptual behavior, together with successful kick execution, was preserved even when explicit ball-tracking rewards were removed. However, incorporating such rewards encouraged the robot to maintain the ball near the center of its FOV rather than at the periphery, thereby improving robustness in real-world deployment. We further examined the active perception capability in dynamic scenarios. As shown in Fig. 4D, when the ball rolled toward the edge of the FOV, the policy 7 Figure 5 Versatile gait behaviors. Visualization of 20,000 collected joint-space trajectory frames together with the reference motion dataset reduced to 2D plane using UMAP. Five distinct clusters highlight the policys ability to integrate reference motions and generalize beyond them to generate task-specific behaviors. From each major cluster, representative trajectory is selected to illustrate the corresponding behavior. adaptively rotated its head and torso toward the direction in which the ball was leaving the view, successfully repositioning it within the cameras view to sustain tracking. The ball positions estimated by the decoder network within the policy confirmed that the robot deliberately oriented itself toward the predicted trajectory of ball motion. We further analyzed the policys ball position estimation and its alignment with the ground-truth position. As shown in Fig. 4B, compared with the raw visual perception, the policys estimates exhibited significantly reduced noise, with the minimum RMSE in the last 1 before kicking decreasing from 0.344 to 0.186 m. This demonstrated the policys effectiveness in denoising perceptual data. Considering that the length of the robots arch is only 0.23 m, such an improvement is essential for accurate ball positioning and reliable strikes. This capability arose from the encoder-decoder architecture within the policy network, where latent states retain ball information over 1 observation window to achieve accurate estimates. If the decoder was removed during policy training and trained separately afterward to estimate the ball position from latent states, the resulting predictions remained at the noise level. During the ball-search phase, the policy exhibited an adaptive strategy tailored to field position, as illustrated in Fig. 3(B and C). When the robot was near the field edges, it first reoriented toward the center, enabling wide visual sweep of the playing area. It then tended to move toward the central region, which provided positional advantage for detecting balls located in distant corners or along the perimeter. Upon reaching the center, the robot switched to rotational scanning pattern, covering all spatial zones to ensure full visual coverage. The searching strategy was strongly supported by the policys ball position estimates during the search. The predicted coordinates consistently corresponded to field regions to be explored. For example, while moving toward the center, the estimates often pointed to distant outer areas, guiding the robot to expand its search range. Similarly, when turning to scan behind, the estimates shifted to rearward regions not yet surveyed. This close coupling between search behaviors and predictive estimates highlighted the 8 Figure 6 Comparison of the learned policy versus rule-based strategy representative of the current RoboCup state-of-the-art. (A and B) Time from robot start to ball contact and the maximum angular velocity of the robot during this process, measured across various approach angles. The robot starts stationary from 1.5 distance. Shaded areas indicate the SD, calculated from 5 tests per direction. (C and D) Representative behaviors when the robot kicks the ball forward (0) and backward (180) with the rule-based strategy. (E and F) The learned policy enables the robot to seamlessly integrate approaching and kicking the ball. policys capacity to integrate spatial awareness of unexplored areas into decision-making, thereby improving ball localization efficiency."
        },
        {
            "title": "2.5 Gait behavior analysis",
            "content": "We analyzed the gait behaviors exhibited by our controller. Following the approach in [12], we applied Uniform Manifold Approximation and Projection (UMAP) [47] to reduce the dimensionality of the joint-space trajectories generated by our policy into 2D space for visualization. As shown in Fig. 5, the resulting embeddings revealed 5 distinct gait clusters, corresponding to walking, turning left, turning right, left-foot kicking, and right-foot kicking. These results demonstrated that our controller produced versatile set of behaviors within single policy and transitioned smoothly among them, enabling coherent execution of tasks such as searching, chasing, and kicking the ball. To further evaluate the effectiveness of AMP, we projected the reference motion dataset into the same space. The policy trajectories broadly covered the dataset, indicating successful integration of demonstrated motions, while also extending beyond them to synthesize task-specific behaviors. representative example was the pivot hook kick observed when the goal is behind the robot. In this case, the robot pivoted on the supporting foot and swung the kicking leg laterally to hook the ball, eliminating the need for full body and thereby reducing execution time. This underscored the policys capacity to compose novel behaviors beyond demonstrated motions, effectively adapting to task requirements. We compared the learned policy with state-of-the-art rule-based soccer strategy employed by the runner-up team in the RoboCup Humanoid League. The rule-based approach leveraged the robots built-in walking controller and relied on preprogrammed behavior trees to generate velocity commands for decision-making. In our experiment, the ball was positioned at the center of the field, and the robot was initialized on circle with 1.5 radius centered on the ball. As illustrated in Fig. 6, we quantitatively evaluated performance in terms of kicking time and agility. Kicking time was defined as the interval between the onset of movement and the moment the ball was kicked out. Using the rule-based strategy, the process required approximately 2 when the robot was oriented toward the goal, but increased to about 5 when the target direction was located behind the robot. This delay resulted from the need for extensive rotation and fine positional adjustments around the ball to achieve proper alignment. In contrast, the learned policy consistently achieved shorter times across all orientations, with only minor variation. This improvement could be attributed to its capacity for 9 Figure 7 Agile gait behaviors when chasing rolling ball. The plots show the trajectories of the robot and the ball, together with the foot contact pattern over time. (A) Agile lateral movement and kicking when the ball rolls horizontally. (B) Agile turning and kicking when the ball rolls toward the robots rear. dynamically adjusting foot placement, enabling seamless transition from approaching the ball to executing the kick. We further analyzed the robots maximum angular velocity during the kicking process to assess agility. While not complete measure of agility, this metric reflected the presence of agile behavior. The learned policy attained higher turning speeds across all tested orientations, underscoring its greater agility in aligning with the target direction. We further analyzed the gait characteristics exhibited by the policy during ball tracking, revealing adaptive patterns tailored to the balls proximity and movement state. As illustrated in Fig. 3(D and E), when the robot was at greater distance from the ball, it employed gait with slower step frequency, allowing the robot to maintain consistent visual tracking of the ball while conserving energy. In contrast, as the robot approached the ball, notable increase in step frequency was observed, manifested through shorter and more rapid strides that facilitated accurate adjustment of foot placement in the vicinity of the ball. This accelerated gait pattern enhanced the robots maneuverability, enabling it to make subtle positional corrections to ensure alignment with the optimal kicking position. 10 This gait adjustment became even more pronounced when chasing rolling ball. When the ball rolled in front of the robot, the policy triggered rapid lateral steps in an attempt to intercept the balls trajectory before it moved out of range (Fig. 7A). Furthermore, agile turning behavior was exhibited when the ball rolled from the robots side toward its rear (Fig. 7B). In such scenarios, the robot executed an extremely rapid rotational maneuver where one foot pushed off the ground, while the other foot pivoted on the surface, enabling full reorientation in as few as two steps. This swift turning motion enabled prompt response, ensuring the robot retained visual contact with the ball and maintained the opportunity to execute successful kick. Collectively, these gait adaptations reflected the policys ability to integrate sensory feedback about ball position and movement into motor control, optimizing both efficiency and precision across dynamic tracking scenarios."
        },
        {
            "title": "3 Discussion",
            "content": "In this work, we achieved agile soccer skills for humanoid robots in challenging real-world environments, enabled by an RL-based controller that integrates real-time visual perception with agile locomotion. Our approach extended the AMP framework with perceptual feedback, enabling the policy to learn versatile and adaptive behaviors from human demonstration. The design of the encoder-decoder network, in conjunction with virtual perception system that mimicked real-world visual characteristics, addressed the misalignment between noisy real-world perception and the precise motion control required for stable kicking, while also promoting the robots ability to generalize robustly across diverse environments. The practical efficacy of the proposed controller was confirmed through experiments across range of real-world conditions. The humanoid robot maintained stable and fluent soccer performance across varied terrains, adapting seamlessly to variations in interaction dynamics and visual noise while exhibiting remarkable agility and responsiveness. This adaptability manifested in dynamic movement adjustments tailored to varying ball positions and trajectories, with the robot achieving faster ball possession compared with rule-based strategies and adapting to rolling balls without pausing for them to stop. This superiority was further validated in the RoboCup Humanoid League competition, where the robot successfully scored goals under strict constraints. These results underscore the practical value of our approach in real-world competitive settings. The use of an adversarial discriminator provides implicit motion guidance for the policy to acquire soccer skills aligned with human motion patterns, obviating the need for manual segmentation into discrete behavioral stages. This enables single policy to learn versatile behaviors while retaining the flexibility required for dynamic tasks. This unified policy, which maps visual detections directly to motor commands, not only simplifies the system architecture but also fosters seamless coordination between perception and locomotion, as evidenced by the robots ability to dynamically adjust its gait, head orientation, and kicking angle in response to the real-time positions of the ball and goal. Unlike commonly used rule-based strategies that separate strategic planning from motor control, our unified policy avoids the disjointed movements in such modular systems. It also circumvents the rigidity of feature-based imitation methods, which require strict temporal alignment to reference motions and thus lack adaptability in dynamic scenarios. The policy demonstrated robust performance across environments through the deliberate design of its observation space and perception pipeline. We condition the policy on compact state space derived from ball detections rather than high-dimensional raw images, ensuring consistent and abstract representation across simulation and hardware, thereby enhancing generalization to diverse real-world conditions. This design is complemented by an odometry module that provides long-term memory of the robots location, improving robustness against temporary losses of visual landmarks. By integrating the virtual perception system and an encoder-decoder network during training, the policy learns to actively coordinate perception and action. This coupled behavior, together with the policys ability to denoise visual information and maintain accurate ball position estimates, substantially enhances robustness against motion blur, partial observability, and other perception challenges in dynamic soccer scenarios. The resulting reduction in position estimation error and emergence of purposeful ball-search behaviors highlight how the perception pipeline design enables simulated learning to translate effectively to real-world performance. Despite these successes, our work has several limitations that warrant further exploration. First, the current controller focuses primarily on individual soccer skills, lacking mechanisms to respond to opponents and coordinate with teammates in scenarios involving multiple robots. The environmental information received by the policy is currently restricted to the positions of the ball and goal, with no integration of social or adversarial context. This limitation constrains its potential to execute complex team strategies, such as collaborative passing, defensive interception, or dynamic role shifts. Future research will extend the training environment to include multiple robots and expand the policys observation space to incorporate real-time sensory data of other agents. By integrating such multi-agent contextual information, the policy can be trained to enable dynamic collaboration and adversarial adaptation, enhancing the robots performance in complex team-based soccer scenarios. Second, while the policy has demonstrated walking and adaptive kicking behaviors across varied ball and goal configurations, it still falls short of replicating the broader range of soccer skills exhibited in human matches, including dribbling, ball trapping, and strategic passing. This gap stems primarily from the policys limited capacity to identify context-specific action requirements across different scenarios, such as figuring out when to prioritize keeping possession of the ball over striking it. The current training framework, which emphasizes goal-oriented outcomes, tends to drive the policy toward converging on single movement pattern. Introducing strategic priors or task-level decision cues, analogous to the tactical planning employed by human players, could encourage the policy to develop more diverse and context-aware behaviors, bringing its performance closer to human-level capabilities. In conclusion, our research demonstrated that by incorporating perceptual characteristics into the training framework, humanoid robots can effectively learn vision-driven soccer skills that transfer robustly to the real world. As robots move toward unstructured and dynamic environments, the ability to couple perception with motion control becomes an essential requirement for effective interaction with surroundings. Beyond the specific soccer domain, this work suggests an approach for advancing real-world humanoid capabilities through integrated learning of perception and action, paving the way toward autonomous and adaptive embodied systems."
        },
        {
            "title": "4 Materials and Methods",
            "content": "Our main objective is to develop system that enables humanoid robots to kick the ball into the goal with onboard vision. An overview of our method is presented in Fig. 8."
        },
        {
            "title": "4.1 Training environment",
            "content": "We trained the policy using RL in simulation and then transferred it to the real world. The simulation environment, constructed using NVIDIA Isaac Gym, comprises robot and ball on the soccer field. The field dimensions adhere to the Adult-size specifications of the RoboCup Humanoid League, measuring 14 in length and 9 in width, with two goals (each 2.6 in width) positioned at opposite ends. With reference to the general characteristic parameters of standard Size 5 soccer balls, the physical properties of the ball are randomized within certain domain. Despite the flat terrain of the deployed real-world field, small uneven terrain is incorporated during the training process to facilitate more stable locomotion in real-world scenarios. At the start of each episode, the robot and the ball are randomly initialized within the field. An episode terminates when the robot falls, the ball goes out of bounds, goal is scored, or time limit of 60 is reached. To facilitate the robots learning of continuous kicking skills, when the ball goes out of bounds or enters the goal, only the balls position is reset, while the robots state remains unchanged. To simulate potential scenarios in real matches, the ball is randomly subjected to an additional velocity or teleported to new position on the field with certain probability, thereby mimicking interventions on the ball by opponents or referees. Similarly, external forces or extra velocities are applied to the robot to simulate physical confrontations during gameplay. These configurations contribute to the development of robust behaviors."
        },
        {
            "title": "4.2 Training procedure",
            "content": "We formulate the control problem as Partially Observable Markov Decision Process (POMDP) and adopt asymmetric actor-critic (AAC) [48] and Proximal Policy Optimization (PPO) [49] to train the policy. The actor and critic are represented by policy network and value network, parameterized by the multilayer perceptron (MLP). The value network is trained to estimate the state value, receiving the full state only 12 Figure 8 The proposed learning framework. The actor receives partial observations and reconstructs the full state from historical data using an encoder-decoder architecture. The policy is trained with PPO, using rewards from both the environment and discriminator encoding motion priors, while multiple critics provide value estimates. The policy is trained in simulation with only the modules highlighted in blue deployed on hardware. available within the simulator. The policy only has access to partial information about the complete state, which is consistent with information that can be obtained from sensors on the robot in the real world, and outputs desired joint positions as actions that are tracked using proportional-derivative (PD) controller. While the policys observations are noisy and incomplete, critical latent information can be inferred from historical observations [50, 51]. To leverage this temporal context, we provide the network with time sequence of 50 preceding observation frames (covering 1 of history) alongside the current observation. This sequence is encoded into 64-dimensional latent representation using an MLP, which is then concatenated with the current observation and fed into the actor network. Following the approach in [52], we further integrate decoder that reconstructs auxiliary information critical to the task but unavailable during real-world deployment from this latent state, such as the robots dynamic parameters and the balls true position. This design guides the policy to extract meaningful insights from historical data, facilitating the learning of more robust latent representations. By implicitly modeling the mapping between noisy observations and their underlying true states, the framework enhances the policys ability to perform accurate state estimation and adaptive behavior in real-world deployment. The agents objective is to kick the ball into the goal. To facilitate the learning of long-horizon kicking behaviors, we address the high sparsity of goal-scoring rewards by introducing two dense auxiliary rewards in addition to the terminal goal reward. One encourages the robot to approach the ball, and another promotes the balls motion toward the goal. This design provides the agent with more frequent and informative feedback during training. The proximity rewards are constructed using potential-based reward shaping [53], where the potential function is defined as the Euclidean distance between relevant entities, such as the robot and the ball, or the ball and the goal. This formulation guarantees that the learned behaviors remain directed toward successful goal scoring. Furthermore, we observe that using single critic to estimate combined rewards can cause negative interference between distinct reward components, leading to reduced learning stability and performance (Fig. 4A). To mitigate this, we adopt multi-critic framework [54], where separate critics independently estimate returns for two reward groups. The first group comprises goal-related rewards, while the second encompasses auxiliary rewards such as style and regularization terms. Each reward group is treated as dedicated subtask paired with its own critic, and weighted sum of their respective advantages is then used by PPO to update the policy. This design effectively reduces interference between reward components, enhancing the robustness of the learning process."
        },
        {
            "title": "4.3 Perception system",
            "content": "Incorporating perception into RL training amplifies the challenges of sim-to-real transfer. Raw RGB camera inputs introduce substantial gap between simulation and the real world due to variations in lighting, texture, and noise characteristics. While depth images [17, 55] and elevation maps [56, 57] have proven effective for terrain traversal by capturing geometric abstractions, they are less suited to dynamic soccer scenarios where accurate tracking of moving objects is essential. Alternative approaches that learn from synthetic RGB images [14, 58] in simulation demand extensive computational resources to cover the diverse visual conditions of real environments, which limits scalability. To overcome these challenges, we extract task-specific structured information from visual inputs, thereby narrowing the sim-to-real gap while preserving task relevance. After perceptual processing, the policy receives only the ball position, the goal center coordinates, and the goal normal direction, all expressed in the robots coordinate frame. This representation enables the execution of vision-driven behaviors while abstracting away irrelevant visual complexity. By focusing solely on environmental cues critical to soccer performance, our framework promotes the development of robust, generalizable behaviors with significantly reduced training overhead. Specifically, we employ the YOLOv8 object detection model [59], fine-tuned on self-collected dataset from the robots onboard camera, to detect the ball and field landmarks such as goalposts and line intersections from RGB images. The pixel coordinates of these detected features in 2D images are projected into the BEV space through fusion of depth-based and geometric projections, using the estimated camera height and pose. The BEV positions of field landmarks are then fused with proprioceptive odometry to obtain real-time estimates of the goal center and normal direction. For ball position estimation, conventional approaches typically filter detected ball positions and fuse them with odometry data to mitigate perceptual noise and detection failures. However, these methods often require slow calibration when the robot approaches the ball to achieve accurate estimates and struggle to cope with highly dynamic kicking motions. Instead, we enable the policy to directly process noisy or incomplete perceptual observations, allowing it to autonomously learn the optimal balance between perception and action for accurate ball positioning during kicking. Although ground-truth ball positions are available in simulation, we introduce virtual perception system within the simulation environment to expose the policy to realistic perceptual characteristics during training. This system replicates camera and perception processing behaviors observed on the real robot by modeling four key perceptual factors: detection probability, noise distribution, update frequency, and latency. This modeling allows the policy to learn robustness to perceptual uncertainty and to recover reliable, low-noise information from imperfect visual data. To construct the virtual perception model, we collect data from the real robot observing the ball under diverse conditions, including varying head orientations and relative positions. rule-based ball-tracking program, 14 executed using the robots default walking gait, was deployed while human operator moved the ball across the field to collect approximately 1 hour of data. Ground truth positions of both the robot and the ball were obtained via motion capture system, alongside logs of visually detected ball positions and the robots joint positions. Positional noise was modeled using Gaussian distribution, with variance regressed as linear function of the relative position between the robot and the ball. Additionally, we estimate the robots FOV and corresponding detection probabilities, as well as the perception update frequency and latency between image capture and detection output. These modeling efforts are essential for achieving accurate and reliable kicking behaviors on the physical robot. Notably, although the walking gait used for data collection differs from the learned policy, which introduces potential distribution discrepancies, the simplicity of our perceptual modeling ensures strong generalization across locomotion patterns."
        },
        {
            "title": "4.4 Learning from demonstration",
            "content": "To improve the robots kicking performance, we draw inspiration from human kicking behaviors, where the arch of the foot is the most frequently used part. The arch provides larger contact area with the ball compared to other parts of the foot, such as the curved toe, facilitating better ball control and higher kicking accuracy. This characteristic avoids significant deviations in kicking direction caused by minor positional errors when using less stable contact points. To encourage the policy to learn human-like kicking behaviors and this specific kicking pattern, the AMP approach [33] is employed to specify the agents behavioral style by leveraging unstructured motion clip datasets. This method enables automatic balancing between task execution and motion imitation, and can dynamically interpolate and generalize from the dataset without explicit clip selection, ensuring the robot performs the task effectively while exhibiting stylistically consistent movements. Specifically, we first curate dataset of reference motions encompassing relevant behaviors. We select 76.28 of omnidirectional walking data, covering forward, backward, turning, and side-stepping motions, from the ACCAD dataset [60], and record 30 of arch-based kicking motions using an optical motion capture system. These human movements are retargeted to the robot to align with its physical dimensions and joint configuration. This dataset is utilized to train discriminator network, which is adversarially optimized to distinguish state transitions (st, st+1) from the reference motion dataset (real samples) and those generated by the robot during training (fake samples), thereby learning general metric for similarity between the robots motions and the stylistic characteristics embedded in the dataset. The discriminators predictions are employed to compute style reward that guides the robot toward movement patterns consistent with the reference dataset. The policy is trained via RL to maximize cumulative reward that combines this style reward with other environmental rewards. This framework enables the policy to autonomously compose skills, such as transitioning from walking to leg positioning for kicking, without the need for explicit motion sequencing. To stabilize adversarial training dynamics and prevent model collapse, we adopt Wasserstein GAN framework with soft boundary constraints imposed on the discriminator [38, 39]. Additionally, we initialize the robots states by randomly sampling from reference motion clips [27], which aids in stabilizing training during the initial phase. We further introduce mirror symmetry loss [61] to promote symmetric motions, which is critical for facilitating the policys acquisition of bilateral kicking capabilities and preventing convergence to unilateral kicking strategy."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Booster Robotics for providing the robot platform, experimental environment, and technical support. This work was partly supported by STI 2030-Major Projects (No. 2021ZD0201402, No. 2021ZD0201401), Beijing Natural Science Foundation (No. L243004), and Tsinghua University Initiative Scientific Research Program (Student Academic Research Advancement Program: Zhuiguang Special Project, No. 20257020011)."
        },
        {
            "title": "References",
            "content": "[1] Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, and Eiichi Osawa. Robocup: The robot world cup initiative. In Proceedings of the first international conference on Autonomous agents, pages 340347, 1997. [2] Gabriel Fernandez, Yeting Liu, Colin Togashi, Kyle Gillespie, Alvin Zhu, Quanyou Wang, Yicheng Wang, Hyunwoo Nam, Shiqi Wang, Ruochen Hou, et al. RoboCup 2024 adult-sized humanoid champions guide for hardware, vision, and strategy. Robot World Cup. Springer, 2024. [3] Miguel Abreu, Luis Paulo Reis, and Nuno Lau. Learning to run faster in humanoid robot soccer environment through reinforcement learning. Robot World Cup. Springer, 2019. [4] Steven Bohez, Saran Tunyasuvunakool, Philemon Brakel, Fereshteh Sadeghi, Leonard Hasenclever, Yuval Tassa, Emilio Parisotto, Jan Humplik, Tuomas Haarnoja, Roland Hafner, et al. Imitate and repurpose: Learning reusable robot movement skills from human and animal behaviors. arXiv preprint arXiv:2203.17138, 2022. [5] Yandong Ji, Gabriel B. Margolis, and Pulkit Agrawal. Dribblebot: Dynamic legged manipulation in the wild. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 51555162, 2023. doi: 10.1109/ICRA48891.2023.10160325. [6] Prashanth Ravichandar, Lokesh Krishna, Nikhil Sobanbabu, and Quan Nguyen. Preferenced oracle guided multi-mode policies for dynamic bipedal loco-manipulation. arXiv preprint arXiv:2410.01030, 2024. [7] Shaofeng Yin, Yanjie Ze, Hong-Xing Yu, C. Karen Liu, and Jiajun Wu. Visualmimic: Visual humanoid locomanipulation via motion tracking and generation. arXiv preprint arXiv:2509.20322, 2025. [8] Yandong Ji, Zhongyu Li, Yinan Sun, Xue Bin Peng, Sergey Levine, Glen Berseth, and Koushil Sreenath. Hierarchical reinforcement learning for precise soccer shooting skills using quadrupedal robot. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 14791486. IEEE, 2022. [9] Xiaoyu Huang, Zhongyu Li, Yanzhen Xiang, Yiming Ni, Yufeng Chi, Yunhao Li, Lizhi Yang, Xue Bin Peng, and Koushil Sreenath. Creating dynamic quadrupedal robotic goalkeeper with reinforcement learning. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 27152722. IEEE, 2023. [10] Miguel Abreu, Lus Paulo Reis, and Nuno Lau. Designing skilled soccer team for robocup: Exploring skill-setprimitives through reinforcement learning. Neural Computing and Applications, pages 136, 2025. [11] Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, et al. Toward real-world cooperative and competitive soccer with quadrupedal robot teams. arXiv preprint arXiv:2505.13834, 2025. [12] Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy Huang, Dhruva Tirumala, Jan Humplik, Markus Wulfmeier, Saran Tunyasuvunakool, Noah Siegel, Roland Hafner, et al. Learning agile soccer skills for bipedal robot with deep reinforcement learning. Science Robotics, 9(89):eadi8022, 2024. [13] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [14] Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, and Nicolas Heess. Learning robot soccer from egocentric vision with deep reinforcement learning. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/ forum?id=fC0wWeXsVm. [15] Devin Crowley, Jeremy Dao, Helei Duan, Kevin Green, Jonathan Hurst, and Alan Fern. Optimizing bipedal locomotion for the 100m dash with comparison to human running. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1220512211, 2023. doi: 10.1109/ICRA48891.2023.10160436. [16] Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath. Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. The International Journal of Robotics Research, 44(5):840888, 2025. [17] Ziwen Zhuang, Shenzhe Yao, and Hang Zhao. Humanoid parkour learning. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/forum?id=fs7ia3FqUM. 16 [18] Tao Huang, Junli Ren, Huayi Wang, Zirui Wang, Qingwei Ben, Muning Wen, Xiao Chen, Jianan Li, and Jiangmiao Pang. Learning humanoid standing-up control across diverse postures. arXiv preprint arXiv:2502.08378, 2025. [19] Penghui Chen, Yushi Wang, Changsheng Luo, Wenhan Cai, and Mingguo Zhao. Hifar: Multi-stage curriculum learning for high-dynamics humanoid fall recovery. arXiv preprint arXiv:2502.20061, 2025. [20] Tong Zhang, Boyuan Zheng, Ruiqian Nai, Yingdong Hu, Yen-Jen Wang, Geng Chen, Fanqi Lin, Jiongye Li, Chuye Hong, Koushil Sreenath, et al. Hub: Learning extreme humanoid balance. arXiv preprint arXiv:2505.07294, 2025. [21] Weiji Xie, Jinrui Han, Jiakun Zheng, Huanyu Li, Xinzhe Liu, Jiyuan Shi, Weinan Zhang, Chenjia Bai, and Xuelong Li. Kungfubot: physics-based humanoid whole-body control for learning highly-dynamic skills. arXiv preprint arXiv:2506.12851, 2025. [22] Zixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, and Xiaolong Wang. Gmt: General motion tracking for humanoid whole-body control. arXiv preprint arXiv:2506.14770, 2025. [23] Kangning Yin, Weishuai Zeng, Ke Fan, Zirui Wang, Qiang Zhang, Zheng Tian, Jingbo Wang, Jiangmiao Pang, and Weinan Zhang. Unitracker: Learning universal whole-body motion tracker for humanoid robots. arXiv preprint arXiv:2507.07356, 2025. [24] Weishuai Zeng, Shunlin Lu, Kangning Yin, Xiaojie Niu, Minyue Dai, Jingbo Wang, and Jiangmiao Pang. Behavior foundation model for humanoid robots. arXiv preprint arXiv:2509.13780, 2025. [25] Zhikai Zhang, Jun Guo, Chao Chen, Jilong Wang, Chenghuai Lin, Yunrui Lian, Han Xue, Zhenrong Wang, Maoqi Liu, Huaping Liu, et al. Track any motions under any disturbances. arXiv preprint arXiv:2509.13833, 2025. [26] Chenhao Li, Marco Hutter, and Andreas Krause. Feature-based vs. gan-based learning from demonstrations: When and why. arXiv preprint arXiv:2507.05906, 2025. [27] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG), 37(4):114, 2018. [28] Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, and Angjoo Kanazawa. Visual imitation enables contextual humanoid control. arXiv preprint arXiv:2505.03729, 2025. [29] Takara Truong, Qiayuan Liao, Xiaoyu Huang, Guy Tevet, Karen Liu, and Koushil Sreenath. Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241, 2025. [30] Zewei Zhang, Chenhao Li, Takahiro Miki, and Marco Hutter. Motion priors reimagined: Adapting flat-terrain In 9th Annual Conference on Robot Learning, 2025. URL https: skills for complex quadruped mobility. //openreview.net/forum?id=JXBm4Xfrvj. [31] Zhikai Zhang, Chao Chen, Han Xue, Jilong Wang, Sikai Liang, Yun Liu, Zongzhang Zhang, He Wang, and Li Yi. Unleashing humanoid reaching potential via real-world-ready skill space. arXiv preprint arXiv:2505.10918, 2025. [32] Dongho Kang, Jin Cheng, Fatemeh Zargarbashi, Taerim Yoon, Sungjoon Choi, and Stelian Coros. Learning steerable imitation controllers from unstructured animal motions. arXiv preprint arXiv:2507.00677, 2025. [33] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Transactions on Graphics (ToG), 40(4):120, 2021. [34] Alejandro Escontrela, Xue Bin Peng, Wenhao Yu, Tingnan Zhang, Atil Iscen, Ken Goldberg, and Pieter Abbeel. Adversarial motion priors make good substitutes for complex reward functions. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2532. IEEE, 2022. [35] Eric Vollenweider, Marko Bjelonic, Victor Klemm, Nikita Rudin, Joonho Lee, and Marco Hutter. Advanced skills through multiple adversarial motion priors in reinforcement learning. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 51205126, 2023. doi: 10.1109/ICRA48891.2023.10160751. [36] Arturo Flores Alvarez, Fatemeh Zargarbashi, Havel Liu, Shiqi Wang, Liam Edwards, Jessica Anz, Alex Xu, Fan Shi, Stelian Coros, and Dennis Hong. Learning to walk in costume: Adversarial motion priors for aesthetically constrained humanoids. arXiv preprint arXiv:2509.05581, 2025. [37] Kehan Wen, Chenhao Li, Junzhe He, and Marco Hutter. Constrained style learning from imperfect demonstrations under task optimality. arXiv preprint arXiv:2507.09371, 2025. 17 [38] Annan Tang, Takuma Hiraoka, Naoki Hiraoka, Fan Shi, Kento Kawaharazuka, Kunio Kojima, Kei Okada, and Masayuki Inaba. Humanmimic: Learning natural locomotion and transitions for humanoid robot via wasserstein adversarial imitation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1310713114. IEEE, 2024. [39] Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214223. PMLR, 2017. [40] Yuanhang Zhang, Yifu Yuan, Prajwal Gurunath, Tairan He, Shayegan Omidshafiei, Ali-akbar Agha-mohammadi, Marcell Vazquez-Chanlatte, Liam Pedersen, and Guanya Shi. Falcon: Learning force-adaptive humanoid locomanipulation. arXiv preprint arXiv:2505.06776, 2025. [41] Jaehwi Jang, Zhuoheng Wang, Ziyi Zhou, Feiyang Wu, and Ye Zhao. Seec: Stable end-effector control with model-enhanced residual learning for humanoid loco-manipulation. arXiv preprint arXiv:2509.21231, 2025. [42] An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Xueyan Zou, Jan Kautz, Erdem Biyik, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot vision-language-action model for navigation. In Robotics: Science and Systems, 2025. [43] Younggyo Seo, Carmelo Sferrazza, Haoran Geng, Michal Nauman, Zhao-Heng Yin, and Pieter Abbeel. Fasttd3: Simple, fast, and capable reinforcement learning for humanoid control. arXiv preprint arXiv:2505.22642, 2025. [44] Yushi Wang, Penghui Chen, Xinyu Han, Feng Wu, and Mingguo Zhao. Booster gym: An end-to-end reinforcement learning framework for humanoid robot locomotion. arXiv preprint arXiv:2506.15132, 2025. [45] Kevin Zakka, Baruch Tabanpour, Qiayuan Liao, Mustafa Haiderbhai, Samuel Holt, Jing Yuan Luo, Arthur Allshire, Erik Frey, Koushil Sreenath, Lueder Kahrs, et al. Mujoco playground. arXiv preprint arXiv:2502.08844, 2025. [46] Haoran Geng, Feishi Wang, Songlin Wei, Yuyang Li, Bangjun Wang, Boshi An, Charlie Tianyue Cheng, Haozhe Lou, Peihao Li, Yen-Jen Wang, et al. Roboverse: Towards unified platform, dataset and benchmark for scalable and generalizable robot learning. arXiv preprint arXiv:2504.18904, 2025. [47] John Healy and Leland McInnes. Uniform manifold approximation and projection. Nature Reviews Methods Primers, 4(1):82, nov 2024. ISSN 2662-8449. doi: 10.1038/s43586-024-00363-x. [48] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. arXiv preprint arXiv:1710.06542, 2017. [49] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [50] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science robotics, 5(47):eabc5986, 2020. [51] Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath. Robust and versatile bipedal jumping control through reinforcement learning. In Robotics: Science and Systems, Daegu, Republic of Korea, 2023. doi: 10.15607/RSS.2023.XIX.052. [52] Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, and Jianyu Chen. Advancing humanoid locomotion: Mastering challenging terrains with denoising world model learning. In Robotics: Science and Systems, Delft, Netherlands, July 2024. doi: 10.15607/RSS.2024.XX.058. [53] Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278287. Citeseer, 1999. [54] Siddharth Mysore, George Cheng, Yunqi Zhao, Kate Saenko, and Meng Wu. Multi-critic actor learning: Teaching rl policies to act with style. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=rJvY_5OzoI. [55] Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Sren Schwertfeger, Chelsea Finn, and Hang Zhao. Robot parkour learning. In 7th Annual Conference on Robot Learning, 2023. URL https://openreview.net/ forum?id=uo937r5eTE. [56] David Hoeller, Nikita Rudin, Dhionis Sako, and Marco Hutter. Anymal parkour: Learning agile navigation for quadrupedal robots. Science Robotics, 9(88):eadi7566, 2024. 18 [57] Huayi Wang, Zirui Wang, Junli Ren, Qingwei Ben, Tao Huang, Weinan Zhang, and Jiangmiao Pang. Beamdojo: Learning agile humanoid locomotion on sparse footholds. In Robotics: Science and Systems, 2025. [58] Alan Yu, Ge Yang, Ran Choi, Yajvan Ravan, John Leonard, and Phillip Isola. Learning visual parkour from generated images. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/forum? id=cGswIOxHcN. [59] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics yolov8, 2023. URL https://github.com/ultralytics/ ultralytics. [60] Advanced Computing Center for the Arts and Design. ACCAD MoCap dataset. URL https://accad.osu.edu/ research/motion-lab/mocap-system-and-data. [61] Wenhao Yu, Greg Turk, and Karen Liu. Learning symmetric and low-energy locomotion. ACM Transactions on Graphics (TOG), 37(4):112, 2018."
        },
        {
            "title": "A Training Details",
            "content": "Our training pipeline is built upon sim-to-real framework [44], utilizing NVIDIA Isaac Gym for large-scale parallel simulation. To accelerate training, we employ distributed setup with 8 NVIDIA V100 GPUs. The policy is trained for 20,000 epochs across 16,384 parallel environments, taking approximately 1 day to complete. The training hyperparameters used in training are summarized in Table 1. Observation spaces for the actor and critic, along with the state space used for reconstruction, are detailed in Table 2. To match real-world sensing conditions, only the actors observations, which are restricted to available information on the physical robot, are corrupted with simulated sensor noise."
        },
        {
            "title": "B Reward Functions",
            "content": "The reward function is weighted sum of task rewards, style rewards, and regularization rewards that jointly guide policy learning. Reward components are given in Table 3. To improve training stability under heterogeneous reward sources, we employ multi-critic structure, in which distinct reward groups are assigned to separate value functions. The goal-related critic receives rewards associated with task progress, including goal scored, ball approach, and goal progress terms, while the auxiliary critic is trained on all remaining rewards. Each critic learns its own value estimate from their respective reward subsets. The final policy advantage combines the two advantages via weighted sum: where wgoal = 2 and waux = 1. This decomposition reduces negative interference between reward components and improves the stability of PPO updates. Atotal = wgoalAgoal + wauxAaux, (1)"
        },
        {
            "title": "C AMP Training",
            "content": "To stabilize adversarial imitation learning, we adopt Wasserstein GAN formulation with gradient penalty. The discriminator D estimates the Wasserstein distance between expert and policy-generated motion transitions, while the policy  is trained to minimize this distance as part of its optimization objective. At each iteration, the discriminator processes pairs of consecutive motion states xt = (st, st+1), sampled either from expert demonstrations xE , and outputs scalar score D(xt). The discriminator loss is defined as: or from policy rollouts x LD = (cid:0)tanh(0.4D(xE ))(cid:1) + (cid:16) tanh(0.4D(x )) (cid:17) , (2) which approximates the Wasserstein distance while suppressing large gradients during early training via the tanh() transformation. To enforce Lipschitz continuity, gradient penalty term is added: Lgrad = (cid:16) (D( xt) 1)2(cid:17) , where random interpolations xt = xE + (1 )x and  U(0, 1)."
        },
        {
            "title": "The discriminator provides an adversarial reward",
            "content": "ramp = tanh(0.4D(x )), encouraging the policy to generate motions closely aligned with expert demonstrations. 20 (3) (4)"
        },
        {
            "title": "D Symmetry Loss",
            "content": "To discourage asymmetric strategies, such as consistently kicking with single leg, we introduce symmetry loss that promotes bilateral coordination. For each sampled observation ot, we construct mirrored counterpart ot = Mo(ot) using mirroring operator Mo() that swaps leftright joint states and inverts lateral components of ball and goal observations in the robot-centric frame. The policy outputs two actions: at = (at), at = (at). Applying corresponding mirroring operator Ma() to at, we define the symmetry loss as: Lsym = (cid:16) at Ma(at)2(cid:17) . (5) (6) This regularization encourages consistent responses to symmetric states, maintaining bilateral motion capability and enabling the policy to autonomously select either leg depending on task context."
        },
        {
            "title": "E Perception Modeling",
            "content": "The modeling of our virtual perception system is illustrated in Fig. 9. Positional noise is sampled from Gaussian distribution (0, (0.124 + 0.149)2), where is the distance to the ball. Perception latency and update frequency are modeled as (116 ms, (18 ms)2) and (25.36 Hz, (1.06 Hz)2), respectively. The ball is detected with 90% probability within the robots FOV up to range of 7 m, beyond which the detection rate gradually decays."
        },
        {
            "title": "F Odometry Implementation",
            "content": "We implement hybrid odometry that fuses proprioceptive and visual information for reliable localization. The proprioceptive odometry is trained using trajectories collected from the policy executed within NVIDIA Isaac Gym. The simulator generates data in parallel across diverse environmental conditions, enabling the odometry to learn generalizable mappings between proprioceptive inputs and the resulting motion. The odometry is implemented as an MLP that processes 1 history of proprioceptive observations, including joint positions, joint velocities, base orientation, angular velocity, and the previous action. The previous odometry output is also fed back as part of the input, forming an autoregressive loop that improves robustness against accumulated drift. The odometry predicts the robots position increment for the next frame interval and is trained to minimize the mean squared error between the predicted and ground-truth motion increments obtained from simulation. To mitigate long-term drift, the proprioceptive estimation is periodically corrected through vision-based localization derived from detected field landmarks, including goalposts and T-, X-, and L-shaped intersections. particle filter matches the detected landmarks to the predefined global field map, producing an updated global position estimate. This fusion approach effectively compensates for proprioceptive drift while maintaining stable localization under temporary loss of visual information. 21 Figure 9 Virtual perception system. (A) Positional noise. (B) Distribution of perception latency. (C) Distribution of perception frequency. (D) Detection rate when the ball is in FOV. Table 1 Training hyperparameters Parameter"
        },
        {
            "title": "Critic layer sizes\nActor layer sizes\nEncoder layer sizes\nDecoder layer sizes\nNetwork activation\nNumber of learning epochs\nMini batch size\nLearning rate\nDiscount factor\nGAE lambda\nDesired KL\nOptimizer\nEntropy coefficient\nReconstruction coefficient\nDiscriminator coefficient\nGradient penalty coefficient\nSymmetry coefficient",
            "content": "Value (256, 256, 128) (256, 256, 128) (1024, 128) (128, 128) ELU 5 4 Adaptive 0.995 0.95 0.01 Adam 0.01 1 1 50 10 22 Table 2 Observation and state spaces Observation Description Actor Critic Reconstruct"
        },
        {
            "title": "Projected gravity\nAngular velocity\nJoint position\nJoint velocity\nPrevious action\nBall position\nBall mask\nGoal position\nGoal direction",
            "content": "Gravity vector in the robot frame Angular velocity of the robots base from IMU Joint position offset from default configuration Joint velocity Previous policy action output Ball position (x, y) in the robot frame Ball detection flag Goal position (x, y) in the robot frame Goal direction (cos , sin ) in the robot frame Linear velocity of the base in the robot frame Linear velocity Base height Height of the robots base above the ground Mass Randomization Randomized mass and CoM of the base link Ball velocity Ball friction Ball velocity (x, y) in the world frame Friction force (x, y) acting on the ball Reward Description Table 3 Reward components"
        },
        {
            "title": "Survival\nTermination\nStagnation\nGoal scored\nBall approach\nGoal progress\nHead pitch alignment Penalty for difference between head pitch and ball elevation\nHead yaw alignment",
            "content": "Constant reward for each timestep the robot remains operational Penalty when robot falls Penalty when robot remains nearly motionless for 1 Reward when ball enters goal area Reward for reduction in distance from robot to ball Reward for reduction in distance from ball to goal"
        },
        {
            "title": "Penalty for difference between head yaw and ball direction",
            "content": "Weight 3 1000 100 15 50 500 0.5 0."
        },
        {
            "title": "Reward for motions similar to reference via adversarial discriminator\nReward for sideways foot movement when in contact with the ball\nPenalty for forward foot movement when in contact with the ball\nPenalty when feet are too close to each other",
            "content": "0.3 20"
        },
        {
            "title": "Penalty for abrupt changes in head joint actions\nPenalty for abrupt changes in leg joint actions\nPenalty for joint positions exceeding limits\nPenalty for excessive root accelerations\nPenalty for collision on body parts except the feet",
            "content": "15 1 100 0.001"
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "China Agricultural University",
        "Tsinghua University"
    ]
}