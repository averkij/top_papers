{
    "paper_title": "CodeContests+: High-Quality Test Case Generation for Competitive Programming",
    "authors": [
        "Zihan Wang",
        "Siyao Liu",
        "Yang Sun",
        "Hongyan Li",
        "Kai Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Competitive programming, due to its high reasoning difficulty and precise correctness feedback, has become a key task for both training and evaluating the reasoning capabilities of large language models (LLMs). However, while a large amount of public problem data, such as problem statements and solutions, is available, the test cases of these problems are often difficult to obtain. Therefore, test case generation is a necessary task for building large-scale datasets, and the quality of the test cases directly determines the accuracy of the evaluation. In this paper, we introduce an LLM-based agent system that creates high-quality test cases for competitive programming problems. We apply this system to the CodeContests dataset and propose a new version with improved test cases, named CodeContests+. We evaluated the quality of test cases in CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels to examine the accuracy of these test cases in evaluation. The results indicated that CodeContests+ achieves significantly higher accuracy than CodeContests, particularly with a notably higher True Positive Rate (TPR). Subsequently, our experiments in LLM Reinforcement Learning (RL) further confirmed that improvements in test case quality yield considerable advantages for RL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 7 1 8 5 0 . 6 0 5 2 : r CodeContests+: High-Quality Test Case Generation for Competitive Programming Zihan Wang1,2, Siyao Liu1, Yang Sun1, Hongyan Li2, Kai Shen 1ByteDance Seed, 2Peking University"
        },
        {
            "title": "Abstract",
            "content": "Competitive programming, due to its high reasoning difficulty and precise correctness feedback, has become key task for both training and evaluating the reasoning capabilities of large language models (LLMs). However, while large amount of public problem data, such as problem statements and solutions, is available, the test cases of these problems are often difficult to obtain. Therefore, test case generation is necessary task for building large-scale datasets, and the quality of the test cases directly determines the accuracy of the evaluation. In this paper, we introduce an LLM-based agent system that creates high-quality test cases for competitive programming problems. We apply this system to the CodeContests dataset and propose new version with improved test cases, named CodeContests+. We evaluated the quality of test cases in CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels to examine the accuracy of these test cases in evaluation. The results indicated that CodeContests+ achieves significantly higher accuracy than CodeContests, particularly with notably higher True Positive Rate (TPR). Subsequently, our experiments in LLM Reinforcement Learning (RL) further confirmed that improvements in test case quality yield considerable advantages for RL. Date: June 9, 2025 Correspondence: Kai Shen at shen.kai@bytedance.com Project Page: https://huggingface.co/datasets/ByteDance-Seed/Code-Contests-Plus"
        },
        {
            "title": "Introduction",
            "content": "Competitive programming is widely recognized as an important benchmark for evaluating the reasoning and coding capabilities of LLMs [5]. Solving complex competitive programming problems requires strong reasoning capabilities, as well as mastery of wide range of algorithms, data structures, and mathematical knowledge. More importantly, competitive programming problems are objectively verifiable tasks; thus, they are not only widely used for benchmarks, but they can also provide accurate rewards for reinforcement learning and serve as vital data foundation for training large reasoning models [2, 6]. Existing open-source competitive programming datasets usually collect problems from competition platforms like CodeForces [14], LeetCode, and AtCoder. However, these competition platforms do not publicly release their test cases. Consequently, although large amounts of problems with statements and solutions are publicly available, the lack of test cases prevents these problems from being effectively constructed into RL training datasets. Consequently, existing datasets primarily utilize their own created test cases, rather than fully using the 1 Figure 1 Competitive programming problems typically impose constraints on the input. (a) simple example of topological sort problem, which requires the input to be Directed Acyclic Graph (DAG) and specifies limits on its size. (b) An invalid input, as the graph contains cycle, which means no topological sort exists. (c) valid input. official ones. Commonly used automated test case generation methods include mutation, as well as some LLM-based methods. However, the quality of these automatically generated test cases still falls far short of those designed by professional human problem setters. Specifically, the main gaps lie in the following two aspects: Limited coverage. Some methods, such as Mutation, can only blindly construct large amounts of random data. They struggle to generate tricky cases and corner cases that require deep algorithmic understanding to discover, making it hard to cover deep-level, complex boundary conditions or special situations within the problems logic. Additionally, some methods, like directly outputting test cases via LLMs, are often unable to generate large-scale test cases. Therefore, they cannot identify incorrect solutions that are logically correct but fail due to excessively high time or memory complexity. Limited coverage may lead to false positives, meaning that incorrect solutions might be judged as correct. Incorrect test cases. As shown in Fig. 1, typical programming problem usually imposes constraints on the test cases themselves. Existing methods struggle to ensure that the generated test cases can satisfy these constraints, and incorrect test cases can simultaneously lead to both false positives and false negatives. To the best of our knowledge, the issue of incorrect test cases has not received attention in previous research. We examined the test cases in the CodeContests dataset and found that incorrect test cases are one of the main causes of inaccurate evaluation. In this paper, we propose an LLM-based agent system for constructing test cases for programming problems that have more comprehensive coverage and better correctness. This will allow for further improvement in the quality and scale of code RL datasets. Specifically, to address the above two limitations, we propose the following solutions: An Agent for Test Case Generation. We designed Generator agent that writes generator program for each problem to specifically construct diverse test cases, including random data, corner cases, and tricky cases, thereby fully testing various possible solutions and potential error patterns, as well as examining the efficiency of the algorithm through large test cases. This generator program can be run any number of times with different random seeds, thus obtaining any number of test cases, further improving coverage. An Agent for Test Case Validation. Although this generator agent consciously attempts to satisfy the constraints in the problem, it still has noticeable probability of making mistakes. Therefore, we designed validator agent. This agent writes validator program to check whether the input of the generated test cases satisfies all the constraints in the problem. Incorrect test cases and the specific reasons for the errors will be fed back to the generator for revision until all test cases satisfy the conditions. Table 1 Comparison between CodeContests+ and other code datasets and benchmarks."
        },
        {
            "title": "Type",
            "content": "# Problems"
        },
        {
            "title": "Customized\nChecker",
            "content": "How Test Cases Are Constructed? MBPP [1] HumanEval [3] USACO [15] LiveCodeBench [8] APPS [7] CodeContests [11] TACO [10]"
        },
        {
            "title": "Benchmark\nBenchmark\nBenchmark\nBenchmark\nTrain\nTrain\nTrain",
            "content": "CodeContests+(Ours)"
        },
        {
            "title": "Train",
            "content": "974 164 307 1055 10000 13610 26433 11690 Handcrafted Handcrafted Publicly accessible Semi-automatic Crawled Mutation Output by LLM G-V agent system The contributions of this paper is summarized as follows: 1. An LLM-Based Agent System for Test Case Construction. We propose the Generator-Validator (G-V) agent system, the first LLM agent system designed for constructing high-quality test cases for competitive programming problems. 2. Code Dataset with Verified Test Cases. Using the G-V agent system, we create CodeContests+, the first competition-level code dataset with verified test cases. We verify the coverage and correctness of the test cases by evaluating the true positive rate (TPR) and true negative rate (TNR) of each problem using 1.72 million labelled solutions. CodeContests+share the same problem set with CodeContests, but replacing test cases with those generated by our G-V agent system. 3. Study. Comparing under the same TPR and TNR thresholds, CodeContests+can yield twice the number of effective problems compared to CodeContests, thus validating that the test case quality of CodeContests+is significantly better than that of CodeContests. We trained 32B reasoning model using RL separately with CodeContests+and CodeContests and observed clear advantage for CodeContests+during the training process."
        },
        {
            "title": "2 Related Work",
            "content": "Since most competitive programming platforms do not disclose their test cases, constructing test cases is one of the primary bottlenecks in building code datasets. The test case generation methods currently employed by existing code datasets can generally be categorized into three types: manual generation, mutation-based generation, and LLM-based generation. Manual. Representative works that use manually constructed test cases include MBPP [1], HumanEval [3], and LiveCodeBench [8]. There are slight differences among these three: the test cases in MBPP and HumanEval are handcrafted, resulting in smaller quantity and insufficient coverage. In contrast, for portion of the problems in LiveCodeBench, test cases are constructed by human experts specifically targeting the problem characteristics, leading to better coverage. The common drawbacks of manually constructing test cases are their high cost, lack of automation, and difficulty in scaling up. Therefore, such methods are only suitable for building small-scale evaluation sets and are too costly to use for constructing large-scale training sets. Mutation-Based. Liu et al. [12] identified the issue of high False Positive Rates (FPR) in MBPP and HumanEval due to their small number of test cases. They proposed Type-aware input mutation\" to generate new test cases by recombining few existing test cases. similar mutation approach was also employed to construct the CodeContests [11] dataset. The advantage of mutation methods lies in their complete automation, allowing for the generation of large volume of test cases. However, their limitation is that if problem involves complex constraints, mutation often fails to satisfy these constraints, thereby introducing incorrect test cases and leading to high False Negative Rate (FNR). LLM-based. Since competitive programming problems often involve complex constraints on test input, some approaches turn to LLMs to handle this. TACO [10], for instance, uses LLMs to directly output test input. However, while LLMs can access the problem description and understand the constraints to some extent, they are not guaranteed to output an input that satisfies these constraints. Furthermore, this method can only construct small test cases and is limited by the context window size, preventing it from outputting large test cases. For example, an LLM cannot directly output graph containing one million vertices."
        },
        {
            "title": "3 The Generator-Validator Agent System",
            "content": "Figure 2 Generator-Validator Agents Pipeline. (a) The Generator Agent writes generator program and generator commands to produce test cases. (b) The Validator Agent writes validator program to check if the test cases satisfy all the constraints stated in the problem. Test cases that fail validation, along with specific comments provided by the validator program, are then fed back to the Generator Agent for revision. To simultaneously address the challenges of correctness and coverage in test case construction, we propose Generator-Validator agent system. The Generator agent is an LLM-based agent that writes test input generators based on the problem description. The Validator agent is also an LLM agent that writes test input validators to supervise the Generator agent, ensuring that the test input generated by the Generator agent conforms to the problem constraints. This agent system can automatically build large-scale and rigorous dataset for LLM training by leveraging publicly available data such as problem statements and ground truth solutions. An overview of the workflow is presented in Fig. 2. Specific implementation details for the generator and validator are presented below."
        },
        {
            "title": "3.1 Generator\nGenerator Program. The generator program accepts input data conditions, such as data size and characteristics,\nvia command-line arguments. It then generates a piece of input data that conforms to these specified conditions.\nThis generated input data is then fed into the ground truth solution to produce outputs and hence form a\ncomplete set of test cases. The advantage of this design is that a single generator program, combined with\nvarying command-line arguments, can generate a diverse range of test cases. A generator demo is presented\nin Appendix D.2.",
            "content": "Agent Workflow. Initially, the Generator Agent is given problem statement. It is then instructed to read the statement carefully, and identify, and summarize the constraints of the input data. Subsequently, the Generator Agent will analyze the problem to anticipate potential mistakes contestants might make and 4 identify possible corner cases. Based on this analysis, it will design targeted adversarial test cases. Finally, the Generator Agent will synthesize all of this information to produce compliant generator program. Since it is up to the agent to determine which command-line arguments the generator program needs to receive, we also require the agent to provide approximately 20 commands. These commands should cover range of data sizes, from small to large, and all relevant special types. We will then execute these commands to finally obtain the test input. Details. The generator agent is instructed to use testlib [13], tool library developed by Codeforces for contest problem setters, to implement the generator. testlib provides some useful utility functions, such as random number generators and command-line argument parsing tools. Building upon testlib, we have developed more LLM-friendly version, reducing the difficulty of LLM usage and the likelihood of compilation errors and hallucinations. The use of testlib helps regularize the behavior of LLM-written generators. For instance, we enforce the use of testlibs random number generator, rather than using the C++ standard librarys random facilities. This is to ensure random number consistency, i.e., the same command and the same generator, even on different platforms, will produce identical test cases. Scalability. Some commands provided by the agent can generate not only single test case but also an arbitrary number of test cases by altering the random seed. The random seed for the generator program is calculated based on the hash of the command. The agent is not allowed to modify the random seed in the generator program, ensuring that the same command always produces identical test cases. We can set different random seeds by appending an irrelevant label to the end of the command. This label will not be parsed, and therefore will not affect the behavior of the generator program. In this way, we can flexibly change random seeds and ensure consistency across different platforms at the same time. The number of test cases can be adjusted according to actual needs. For example, the number could be reduced during training to minimize evaluation time overhead and improve training efficiency. When used for benchmarking, the number of test cases may be increased. Supervision. The generator agent is very likely to make mistakes while writing the generator program. Although the agent may recognize some constraints in the problem description, it can still miss specific details either partially or entirely due to limited attention or imperfect comprehension. Therefore, supervision mechanism is necessary to help the generator agent identify and correct errors. We use validator to check if the test input satisfies all the constraints specified in the problem statement. If errors are found, it provides specific error locations and causes. These error reports are then fed back to the generator agent. Subsequently, the agent reflects and corrects the issue, providing revised generator program and commands. Additionally, specific error messages for other potential errors, like compilation errors or generator timeouts, are also provided to the agent for further revision. An example of the supervision and reflection procedure is shown in Fig. 6. The implementation details of the validator will be discussed in Section 3.2."
        },
        {
            "title": "3.2 Validator\nValidator Program. Generating test cases is a very intricate task, so even professional competitive programming\nproblem setters can make mistakes sometimes due to oversight. For example, in the ACM ICPC World\nFinals 2007, Problem J was found to have an incorrect test case due to an error by the problem setter. As\nmentioned above, the validator plays an even more crucial role for the generator agent; it not only provides a\ndouble check on the correctness of test cases but also provides important supervision information to help\nthe generator agent reflect and correct its errors. A validator is a program that takes one input data as its\ninput and determines whether this input data satisfies all the constraints of the problem. If errors exist, the\nvalidator outputs exactly which constraints were violated. For errors where the location can be specifically\nidentified, the validator also provides the error location, for example, the line number in the input data where\nthe error was found. An example of a validator is presented in Appendix D.1.",
            "content": "Agent Workflow. Initially, the validator agent is provided with problem statement. Next, the agent is required to carefully read the problem statement, identify all input data constraints, including data ranges, format requirements, and structural constraints, and summarize them. Finally, the agent will write validator program to check these constraints. 5 Table 2 Comparison between CodeContests and CodeContests+"
        },
        {
            "title": "CodeContests",
            "content": "CodeContests+"
        },
        {
            "title": "Problem Count\nAverage Tests\nValidation Pass Rate\nGenerator\nValidator\nChecker",
            "content": "13610 101 67.1% 11690 25/44/62/80/98/ 100% Supervision. While the probability of validator agent making mistakes is much lower than that of generator agent, errors are still possible. Based on our observations, common errors fall into two categories: The first is where the agent correctly understands the constraints but makes mistakes while writing the validator program. The second is where the agent overlooks some of the constraints stated in the problem. Errors of the first type can lead to the program failing to compile or run correctly, or causing valid input data to fail the validation. Therefore, for the first type of error, we feed the sample inputs from the problem statement to the validator and check whether these data pass the validators checks. If not, it indicates an error in the validator. In this case, both the sample data and the validators output are fed back to the validator agent, which then reflects and makes revisions based on the feedback. Additionally, the validator receives specific error messages for common issues like compilation failures, runtime errors, and timeouts. This supervision mechanism could detect most of the errors of the agent. Unfortunately, we still lack an automatic supervision method to address the second type of error, which can still result in small number of incorrect data being generated. Detailed statistics and case studies will be presented in Section 4.4 below."
        },
        {
            "title": "4 CodeContests+: A Competitive Coding Dataset with Verified Test Cases",
            "content": "CodeContests [11] stands as one of the largest and most widely recognized competitive coding datasets. It collects large number of problems, authentic contestant submission records, and generates numerous additional test cases through mutation. However, test cases generated through mutation are often of low quality and may yield unreliable evaluation results, such as misclassifying incorrect solutions as correct and vice versa. In this Section, we present our methodology for constructing an enhanced dataset, CodeContests+, by building upon the original CodeContests. Section 4.1 outlines the data cleaning procedures we implemented. Section 4.2 details how we utilized the G-V Agent system to generate higher-quality test cases for CodeContests+. Section 4.3 describes the development of customized checkers for problems that accept multiple valid solutions. Following this, we compare the quality of CodeContests+and CodeContests across two primary dimensions. First, in Section 4.4, we verify the quality of the test cases of both datasets. More specifically, we utilized 1.72 million authentic contestant submissions, which comprise both correct and incorrect ones, to assess the performance of both datasets in discriminating between correct and incorrect solutions. Then, in Section 4.5, we employed each dataset to train 32B LLM through DAPO [18], to evaluate the impact of dataset quality on RL training efficacy."
        },
        {
            "title": "4.1 Data Cleaning",
            "content": "We examined the problems in the CodeContests dataset and identified some that were either incorrect or unsuitable for training. We then cleaned up these problems. Specifically, we removed the following types of problems: (1) problems without problem statements, (2) interactive problems, (3) problems without correct submissions, (4) problems involving file input/output (5) special problems, such as April Fools Day problems, (6) problems that require images for proper understanding, (7) problems with crawling errors, (8) other low-quality problems, e.g., problems which lack data ranges, have unclear requirements, or contain incorrect sample formats. After cleaning, the total problem count was reduced from 13,610 to 11,690. It should be noted that due to our additional filtering work, CodeContests and CodeContests+differ in the number of problems. In the experiments in Sections 4.4 and 4.5, our focus is solely on the impact of test case 6 quality. Therefore, we selected the same subset of problems for CodeContests as well, ensuring that the set of problems used for both datasets in these experiments is identical."
        },
        {
            "title": "4.2 Generated Test Cases using G-V Agent System",
            "content": "In CodeContests+, we have replaced all original test cases from CodeContests with generators and validators produced by the G-V Agent system. Furthermore, we offer two methods for utilizing these test cases: Dynamic Generation. Using the provided generators and validators to produce test cases. This approach offers greater flexibility, allowing for the generation of an arbitrary number of test cases based on specific needs and computational budgets. As detailed in Section 3.1, we have ensured random consistency, guaranteeing that using the same generator and commands will produce identical test cases across different platforms. To facilitate the execution of these generators and validators and enable automated evaluation, we open-source SandboxFusion1, which supports over 20 programming languages, including C++, Java, Python, Rust, Go, and more than 10 open-source datasets, such as MBPP, HumanEval, CodeContests. Pre-Processed Test Cases. We release five versions of pre-generated test cases, labeled CodeContests+1x through CodeContests+5x. These correspond to running each generator command 1 to 5 times, respectively, each time with different random seed. The 1x version contains an average of 25 test cases per problem, while the 2x, 3x, 4x, and 5x versions contain averages of 44, 62, 80, and 98 test cases per problem, respectively. Using these pre-generated test sets offers easier compatibility with other sandboxes and evaluation environments. The statistics of CodeContests+and CodeContests are presented in Table 2. All the pre-processed test cases have passed the validators. We used validators to check the correctness of all 1.18 million generated test cases in CodeContests. Only 0.79 million of these passed validation, which is 67.1%."
        },
        {
            "title": "4.3 Customized Checkers for Multiple-Answer Problems",
            "content": "Problems with multiple valid solutions are common in programming contests. For such problems, multiple distinct outputs can be considered correct for the same input. For instance, as illustrated by the examples in Fig. 1 and Fig. 2, DAG can have multiple valid topological sorts, and any one of these is an acceptable output. Furthermore, the number of correct solutions can be vast, potentially even infinite. For example, DAG with nodes can have up to n! distinct topological sorts, making it infeasible to enumerate and store all of them. Previous datasets, such as CodeContests, have collected numerous problems with multiple solutions but lack customized judging logic for them. We have developed Checker Agent that provides customized checker programs for all problems. checker program is designed to determine if codes output is correct for given input. Taking the topological sort problem as an example, its checker program would read the input graph and the submitted codes output, then verify if this output constitutes valid topological order. Due to space constraints, the implementation details of the Checker Agent are provided in the Appendix B."
        },
        {
            "title": "4.4 Test Case Quality Verification\nTask. Test cases serve to determine the correctness of a given code. Therefore, we treat code evaluation\nas a binary classification problem. This approach allows us to evaluate the test cases themselves as binary\nclassifiers, assessing their ability to distinguish between correct and incorrect solutions accurately. By doing\nso, we can objectively and rigorously evaluate the quality of test cases within a code dataset, including their\ncoverage and correctness. To our knowledge, CodeContests+ is the first dataset project to conduct such\nrigorous validation of test case accuracy. We believe this level of evaluation is crucial for establishing a\ndataset’s trustworthiness.",
            "content": "Data. CodeContests has collected tens of millions of submissions, each with correct/incorrect label. We sampled 100 positive samples (correct submissions) and 100 negative samples (incorrect submissions) for each problem. For problems without sufficient samples, we included as many available submissions as possible. In this way, we selected 10,166 problems that contained at least 10 positive and negative samples, as well as 1https://github.com/bytedance/SandboxFusion 7 (a) CodeContests (b) CodeContests+ 5x (c) #Problems v.s. Threshold Figure 3 The histogram of the TPR and TNR of selected problems from (a) CodeContests and (b) CodeContests+, and (c) the number of qualified problems with TPR and TNR greater than threshold in CodeContests+(blue ones, ours) and CodeContests (red). corresponding 1.72 million submissions. We used these problems and samples to evaluate the accuracy of the test cases in CodeContests+ and CodeContests. Engineering. Since each submission needs to be evaluated on approximately 200 test cases on average, the total number of program executions is more than 300 million. We implemented cloud architecture for such large-scale evaluation, running on cluster with 25,000 CPU cores and 70 TB memory, and completed the experiment on this platform. Some engineering details are presented in Appendix C. Metric. We use True Positive Rate (TPR) and True Negative Rate (TNR) to quantitatively assess accuracy. TPR measures the ability of test cases to correctly classify positive instances (correct solutions), thus reflecting the test cases correctness. This is because if test case satisfies all the problems constraints, correct solution should not be misclassified as incorrect. Therefore, low TPR primarily indicates that the test case itself is flawed. TNR measures the ability of test cases to correctly classify negative instances (incorrect solutions) as incorrect, thereby primarily reflecting the test cases coverage. Overly simplistic test cases may fail to identify errors in incorrect code, leading to false positives (i.e., incorrect solutions being deemed correct). Results. We calculated the TPR and TNR for each problem in CodeContests and CodeContests+5x and plotted histograms, which are shown in Fig 3. From the results, it is clearly observed that in CodeContests, there are over 4000 problems with TPR 0.1 and TNR 0.9, indicating that these problems incorrectly classify almost all correct submissions as incorrect, making these problems practically impossible to solve and cannot be used in training. We analyzed these problems and identified two primary reasons for this phenomenon. First, CodeContests includes large number of incorrect test cases, causing program outputs to be meaningless or causing programs to fail to run properly. Second, CodeContests does not provide custom checkers for multi-solution problems. In contrast, our proposed Agent System can better ensure the correctness of test cases and provide custom checkers for multi-solution problems, so similar phenomena are not as prominent. Furthermore, we can observe that as the number of test cases increases (from 1x to 5x), the overall evaluation accuracy improves. This is reflected in Fig. 4 by the gradual increase in the number of problems with TPR&TNR 0.9. To better demonstrate this change, we counted the number of problems with TPR&TNR greater than given threshold, and the results are shown in Fig. 3c. The results show that, at various thresholds, the number of qualified problems in CodeContests+ increases as the number of test cases rises. In particular, the number of qualified problems in CodeContests+ 5x is almost twice that of CodeContests. Even with only one-quarter of the test cases as compared to CodeContests, the 1x version of CodeContests+ has significantly higher evaluation accuracy and yields over 80% more qualified problems than CodeContests. This large-scale verification effort, as described above, allows us to further refine our selection by identifying problems with high-quality test cases and excluding those with problematic ones. Consequently, we have selected problems achieving both TPR&TNR 0.9 to form distilled subset, which we designate CodeContests+HQ. In the next Section, we will use both CodeContests+HQ and the original CodeContests 8 Table 3 Pass@1 on LiveCodeBench across difficulty levels. CodeContests+achieves consistent gains over CodeContests."
        },
        {
            "title": "Easy Medium Hard All",
            "content": "CodeContests 0.958 CodeContests+HQ 0.965 0.786 0.812 0.329 0.622 0.340 0.637 for RL training to demonstrate that higher-quality test cases yield significant benefits for the training process. Within our dataset, we provide the TPR and TNR for each problem, enabling users to set appropriate thresholds for quality-based filtering according to their specific requirements."
        },
        {
            "title": "4.5 Test Case Quality Matters in RL Training",
            "content": "To investigate the effect of unit test coverage on code generation performance, we conduct controlled ablation study using reinforcement learning (RL) based on the PPO-style training paradigm [16]. Benchmark. Our experiments are performed on the LiveCodeBench benchmark [8], which evaluates code generation models across varying difficulty levels: Easy, Medium, and Hard. The time window is Aug 2024 - Feb 2025. We use avg@15 as the performance metric, which is the average of pass@1 from 15 independent responses. Cold Start. We initialize our policy from Qwen2.5-32B [17], reasoning-optimized large language model. To enhance its zero-shot reasoning capabilities, we further perform supervised fine-tuning using curated cold-start reasoning dataset. Optimization Objective. We adopt the Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) objective [18] to enhance policy learning via group-based advantage estimation. Given an input query (Q), group of candidate outputs {oi,t}G is sampled from the policy πθold (O q) using dynamic i=1 sampling strategy. The DAPO objective is defined as: JDAPO(θ) = Eq,{oi}[ 1 oi i=1 oi t=1 min πθ(oi,t q) πθold (oi,t q) Ai, clip ( πθ(oi,t q) πθold(oi,t q) , 1 ϵlow, 1 + ϵhigh) Ai ] (1) where Ri mean({Ri}G std({Ri}G i=1) is the relative advantage computed from group-level reward signals. ϵlow and ϵhigh denote the lower and upper clipping ratios, empirically set to 0.2 and 0.28, respectively. Note that the loss is computed at the token level to better accommodate long-horizon reasoning patterns. In addition, we filtered the overlong samples to avoid reward noise in the training process. Ai = i=1) (2) Rule-based Reward. We use rule-based reward. response receives reward of +1 if it passes all unit tests and 1 otherwise. Results. Table 3 shows performance across difficulty levels. We observe consistent improvements in CodeContests+across all categories, especially on Easy and Medium tasks. This demonstrates the benefit of incorporating better test cases during training."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this paper, we propose an LLM-based Generator-Validator agent system capable of leveraging public problem data to construct high-quality test cases for competitive programming problems. This system facilitates the scaling up of high-quality code datasets. Using this agent system, we have developed CodeContests+ by enhancing the original CodeContests dataset with better test cases. Experimental results demonstrate that the test cases in CodeContests+are of significantly higher quality than those in CodeContests, and that CodeContests+also exhibits substantial advantages in RL training. 9 According to our rough estimate, there are more than 100,000 programming problems with publicly available problem statements and ground truth solutions. Therefore, our proposed agent system will enable the full utilization of these data resources, thereby laying the data foundation for further enhancing the reasoning and coding capabilities of LLMs."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Jiaze Chen, Jinxin Chi, Zhicheng Liu, Siqian Chen, Jingjing Xu, Jinhua Zhu, Rui Long, Qi Liu, Li Han, Liang Xiang, as well as other colleagues at ByteDance, for their support for the CodeContests+ project."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732. [2] Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, Yu Yue, Lin Yan, Qiying Yu, Xiaochen Zuo, Chi Zhang, Ruofei Zhu, Zhecheng An, Zhihao Bai, Yu Bao, Xingyan Bin, Jiangjie Chen, Feng Chen, Hongmin Chen, Riwei Chen, Liangqiang Chen, Zixin Chen, Jinsong Chen, Siyan Chen, Kaiyuan Chen, Zhi Chen, Jin Chen, Jiecao Chen, Jinxin Chi, Weinan Dai, Ning Dai, Jiahui Dai, Shihan Dou, Yantao Du, Zhengyin Du, Jianhui Duan, Chen Dun, Ting-Han Fan, Jiazhan Feng, Junda Feng, Ziyuan Feng, Yuwei Fu, Wenqi Fu, Hanjie Fu, Hao Ge, Hongyi Guo, Mingji Han, Li Han, Wenhao Hao, Xintong Hao, Qianyu He, Jerry He, Feng He, Wen Heng, Zehua Hong, Qi Hou, Liang Hu, Shengding Hu, Nan Hu, Kai Hua, Qi Huang, Ziyue Huang, Hongzhi Huang, Zihao Huang, Ting Huang, Wenhao Huang, Wei Jia, Bin Jia, Xiaoying Jia, Yuhua Jiang, Haobin Jiang, Ziheng Jiang, Kaihua Jiang, Chengquan Jiang, Jianpeng Jiao, Xiaoran Jin, Xing Jin, Xunhao Lai, Zheng Li, Xiang Li, Liyi Li, Hongkai Li, Zheng Li, Shengxian Wan, Ya Wang, Yunshui Li, Chenggang Li, Niuniu Li, Siyu Li, Xi Li, Xiao Li, Aoyan Li, Yuntao Li, Nianning Liang, Xinnian Liang, Haibin Lin, Weijian Lin, Ye Lin, Zhicheng Liu, Guanlin Liu, Guanlin Liu, Chenxiao Liu, Yan Liu, Gaohong Liu, Juncai Liu, Chundian Liu, Deyi Liu, Kaibo Liu, Siyao Liu, Qi Liu, Yongfei Liu, Kang Liu, Gan Liu, Boyi Liu, Rui Long, Weiqiang Lou, Chenwei Lou, Xiang Luo, Yao Luo, Caiping Lv, Heyang Lv, Bole Ma, Qianli Ma, Hongzhi Ma, Yiyuan Ma, Jin Ma, Wenchang Ma, Tingting Ma, Chen Mao, Qiyang Min, Zhe Nan, Guanghan Ning, Jinxiang Ou, Haojie Pan, Renming Pang, Yanghua Peng, Tao Peng, Lihua Qian, Lihua Qian, Mu Qiao, Meng Qu, Cheng Ren, Hongbin Ren, Yong Shan, Wei Shen, Ke Shen, Kai Shen, Guangming Sheng, Jinlong Shi, Wenlei Shi, Guang Shi, Shuai Shuai Cao, Yuxin Song, Zuquan Song, Jing Su, Yifan Sun, Tao Sun, Zewei Sun, Borui Wan, Zihan Wang, Xiaohui Wang, Xi Wang, Shuguang Wang, Jun Wang, Qinlong Wang, Chenyuan Wang, Shuai Wang, Zihan Wang, Changbao Wang, Jiaqiang Wang, Shihang Wang, Xuwu Wang, Zaiyuan Wang, Yuxuan Wang, Wenqi Wang, Taiqing Wang, Chengzhi Wei, Houmin Wei, Ziyun Wei, Shufa Wei, Zheng Wu, Yonghui Wu, Yangjun Wu, Bohong Wu, Shuang Wu, Jingqiao Wu, Ning Wu, Shuangzhi Wu, Jianmin Wu, Chenguang Xi, Fan Xia, Yuqiao Xian, Liang Xiang, Boren Xiang, Bowen Xiao, Zhen Xiao, Xia Xiao, Yongsheng Xiao, Chao Xin, Shulin Xin, Yuwen Xiong, Jingjing Xu, Ziwen Xu, Chenyin Xu, Jiayi Xu, Yifan Xu, Wei Xu, Yufei Xu, Shikun Xu, Shipeng Yan, Shen Yan, Qingping Yang, Xi Yang, Tianhao Yang, Yuehang Yang, Yuan Yang, Ximing Yang, Zeyu Yang, Guang Yang, Yifan Yang, Xuesong Yao, Bairen Yi, Fan Yin, Jianian Yin, Ziqiang Ying, Xiangyu Yu, Hongli Yu, Song Yu, Menghan Yu, Huan Yu, Siyu Yuan, Jun Yuan, Yutao Zeng, Tianyang Zhan, Zheng Zhang, Yun Zhang, Mofan Zhang, Wang Zhang, Ru Zhang, Zhi Zhang, Tianqi Zhang, Xinyi Zhang, Zhexi Zhang, Sijun Zhang, Wenqiang Zhang, Xiangxiang Zhang, Yongtao Zhang, Yuyu Zhang, Ge Zhang, He Zhang, Yue Zhang, Renjie Zheng, Ningxin Zheng, Zhuolin Zheng, Yaowei Zheng, Chen Zheng, Xiaoyun Zhi, Wanjun Zhong, Cheng Zhong, Zheng Zhong, Baoquan Zhong, Xun Zhou, Na Zhou, Huan Zhou, Hang Zhu, Defa Zhu, Wenjia Zhu, and Lei Zuo. Seed-thinking-v1.5: Advancing superb reasoning models with reinforcement learning, 2025. URL https://arxiv.org/abs/2504.13914. [3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. [4] Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng, Aoyan Li, Bo Li, Bowen Li, Linyi Li, Boyi Liu, Jerry Liu, Kaibo Liu, Qi Liu, Shukai Liu, Siyao Liu, Tianyi Liu, Tingkai Liu, Yongfei Liu, Rui Long, Jing Mai, Guanghan Ning, Z. Y. Peng, Kai Shen, Jiahao Su, Jing Su, Tao Sun, Yifan Sun, Yunzhe Tao, Guoyin Wang, Siwei Wang, Xuwu Wang, Yite Wang, Zihan Wang, Jinxiang Xia, Liang Xiang, Xia Xiao, Yongsheng Xiao, Chenguang Xi, Shulin Xin, Jingjing Xu, Shikun Xu, Hongxia Yang, Jack Yang, Yingxiang Yang, Jianbo Yuan, Jun Zhang, Yufeng Zhang, Yuyu Zhang, Shen Zheng, He Zhu, and Ming Zhu. FullStack Bench: Evaluating LLMs as full stack coders, 2024. URL https://arxiv.org/abs/2412.00535. [5] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, and Wenda Zhou. Competitive programming with large reasoning models, 2025. URL https://arxiv.org/abs/2502.06807. [6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [7] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge comIn Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural petence with APPS. Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html. [8] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. In The Twelfth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. [9] Yichuan Jiang. survey of task allocation and load balancing in distributed systems. IEEE Trans. Parallel Distributed Syst., 27(2):585599, 2016. doi: 10.1109/TPDS.2015.2407900. URL https://doi.org/10.1109/TPDS. 2015.2407900. [10] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. TACO: topics in algorithmic code generation dataset. CoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852. [11] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode. Science, 378(6624):10921097, 2022. doi: 10.1126/science.abq1158. URL https://www.science.org/doi/abs/10.1126/science.abq1158. [12] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information 12 Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html. [13] Mike Mirzayanov. Testlib, 2005. URL https://github.com/MikeMirzayanov/testlib. [14] Mike Mirzayanov, Oksana Pavlova, Pavel MAVRIN, Roman Melnikov, Andrew Plotnikov, Vladimir Parfenov, and Andrew Stankevich. Codeforces as an educational platform for learning programming in digitalization. Olympiads in Informatics, 14(133-142):14, 2020. [15] Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. Can language models solve olympiad programming?, 2024. URL https://arxiv.org/abs/2404.10952. [16] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K. Reddy. Execution-based code generation using deep reinforcement learning. Trans. Mach. Learn. Res., 2023, 2023. URL https://openreview.net/forum?id= 0XBuaxqEcG. [17] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [18] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: An open-source LLM reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. (a) CodeContests+ 1x (b) CodeContests+ 2x (c) CodeContests+ 3x (d) CodeContests+ 4x (e) CodeContests+ 5x (f) CodeContests Figure 4 The histogram of the TPR, TNR of problems in (a)-(e) CodeContests+ 1x-5x (ours) and (f) CodeContests."
        },
        {
            "title": "A Terms and Definitions",
            "content": "The terms used in this paper and their definitions are summarized as follows. Submission. In programming competitions, submission is program submitted by contestant. This submission is evaluated by judging system, resulting in verdict such as Accepted, Wrong Answer, Time Limit Exceeded, Runtime Error, or Compile Error, among others. Test case, test data. In competitive programming, test case is used to check whether the participants submission is correct. It usually consists of test input and test output. Test input, input data. The input data of test case will be fed into the contestants program. The output obtained will then be compared with the ground truth data. Test output, output data, reference answer. The output data of test case is the correct answer corresponding to the input data. For problems with multiple correct solutions, the output data typically represents one of the possible correct answers. Validator, input validator, validator program. validator is program used to check if input data is correct. Generator, input generator, generator program. Because some input data is very large or complex and cannot be handcrafted, generator is used to produce it. generator is program that creates test input. Checker, output checker, special judge. checker is program used to determine if contestants output is correct. Usually, it simply checks if the contestants output matches the test output. For problems with multiple solutions, it may contain more complex logic."
        },
        {
            "title": "B Checker",
            "content": "Checker Program. Previous research has largely employed simple character-by-character comparison to determine if programs output is correct. However, this approach does not apply to problems with multiple solutions. Many problems in programming competitions have multiple valid solutions, meaning the same input 14 Figure 5 Evaluation results during RL training process. can correspond to multiple correct outputs. For example, approximately 1/4 of the problems on Codeforces are multi-solution problems. These types of problems require special checking logic. Checker is program used to check if programs output is correct; its inputs include the input data, program output, and the reference answer, and it determines if the program output is correct. For problems without multiple solutions, checker program may still be necessary. For example, problems with floating-point outputs need to compute relative or absolute errors to judge the correctness of the answer. Even for the most general problems, checker program may need simple logic to ignore extra spaces or line breaks from the programs output. Therefore, we believe providing custom checker program for every problem is necessary. checker example is presented in Appendix D.2. Additionally, the checker can provide richer error feedback than simple correct/incorrect binary label. For example, the checker can specify the exact location of the error in the output, the differences between the correct answer and the programs output, and even the reason for the error. This information can be utilized in future research, for example, to help Code LLMs with reflection. Agent Workflow. First, the checker agent is provided with problem statement. Next, the agent is required to carefully read the problem statement and determine if the problem has multiple valid solutions. Multi-solution problems typically have explicit hints, such as \"If multiple feasible solutions exist, output any one of them.\" If the agent determines that the problem does not have multiple solutions, the agent then needs to select one of the built-in checkers based on the problems output format. These built-in checkers include an integer checker, floating-point checker, yes/no checker, and general full-text comparison checker, among others. complete list of built-in checkers is presented in Table 4. If the agent determines that the problem has multiple solutions, then it needs to implement custom checking logic based on the problem requirements and output checker program. Supervision. Based on our observations, the main errors made by the checker agent are concentrated in multi-solution problems where the checker is more difficult to implement, and more often, these errors involve incorrectly checker correct answers as incorrect. We use relatively simple supervision method where we input the problems sample input and sample output into the checker (with the sample output serving as both the programs output and the reference answer). If the checker fails, it indicates an error in the checkers implementation. In such cases, the sample input, sample output, and the checkers output are all fed back to the checker agent for reflection and correction. Even with this, there are still some problems where the agent is unable to correctly implement the checker. These case studies will be presented in the Section 4.4. To ease the burden on the agent, for problems without multiple valid solutions, we provide the agent with selection of pre-written checkers, eliminating the need for the agent to write one itself. These built-in checkers are presented in Table 4. Evaluation as Service (EaaS): Cloud Architecture for Large-Scale Code"
        },
        {
            "title": "Evaluation",
            "content": "Training Code LLMs with RL requires sampling large amount of solutions and evaluating them to obtain rewards for model training. Therefore, code evaluation has become bottleneck affecting training efficiency, with its computational overhead being comparable to the LLMs parameter updates. To address this, we 15 Table 4 Built-in checkers and their uses."
        },
        {
            "title": "Use",
            "content": "ncmp.cc Compare ordered sequences of signed 64-bit integer numbers. Compare two sequences of floating point numbers, with max absolute or relative error = 104. rcmp4.cc Compare two sequences of floating point numbers, with max absolute or relative error = 106. rcmp6.cc Compare two sequences of floating point numbers, with max absolute or relative error = 109. rcmp9.cc wcmp.cc Compare sequences of tokens. Invisible characters are regarded as separators and are not compared. hcmp.cc Compare two signed huge integers. nyesno.cc Compare multiple YES or NO (case insensitive). fcmp.cc Full-text comparison. Whitespaces, tabs, and linebreaks are also strictly compared. implemented cloud service for large-scale code evaluation. This cloud service runs on cluster of 25,000 CPU cores, where 8,000 cores comprise 2,000 4c16g judging pods, and 17,000 cores comprise 8,500 2c4g execution pods. Each judging pod is responsible for evaluating an entire code request, while each execution pod [4] is responsible for running code with single test input and returning the output. The judging pod passes the code and input files to the execution pod; after execution, the execution pod returns the output to the judging pod, which runs checker to determine the correctness of the output. Execution pods have concurrency of 1 and exclusively use two cores of the host machine, ensuring both isolation and consistent performance. Judging pods, on the other hand, have higher concurrency and do not need to run with exclusive cores. Execution pods are scheduled using load balancing [9] while judging pods are scheduled using message queue. All test cases are preprocessed and synchronized across judging pods via network file system, which eliminates the time overhead of compiling and running generators. Execution pods are equipped with runtime environments for multiple languages, including C, C++, Java, Python, Rust, Go, C#, and PHP, among others. Illustrations, Demos and Cases D.1 Demo: Validator The agent-written validator of the problem Color the Map Extreme, which is presented in Fig. 7, is as follow: 1 # include \" testlib .h\" 2 # include < bits / stdc ++. > 3 using namespace std ; 4 5 struct Point { long long , y; 6 7 }; 8 9 static long long cross ( const Point &A , const Point &B , const Point &C) { // Returns the cross product (B -A) (C -A). // Positive if ->B ->C is counter - clockwise turn , // negative if clockwise , 0 if collinear . return (B.x - A.x) * (C.y - A.y) - ( B.y - A.y) * (C.x - A.x); 10 11 12 13 14 15 } 16 17 // Check if two line segments (p1 , p2 ) and (p3 , p4 ) intersect in way 18 // that produces 2D area overlap (i.e., truly crossing ). 19 // We * allow * them to share endpoints or overlap on boundary with no interior crossing . 20 bool segmentsProperlyIntersect ( const Point &p1 , const Point &p2 , const Point &p3 , const Point & p4 ) 21 22 { 23 24 25 27 28 long long c1 = cross (p1 , p2 , p3 ); long long c2 = cross (p1 , p2 , p4 ); long long c3 = cross (p3 , p4 , p1 ); long long c4 = cross (p3 , p4 , p2 ); // If the two segments share more than point on the boundary in line overlap , 16 Figure 6 An illustration of the reflection process of Geneartor Agent. // that is still not an \" area \" intersection . We consider \" proper \" intersection // if each segment straddles the line extended by the other segment // ( typical computational geometry check for crossing ). if (( c1 == 0 && c2 == 0 && c3 == 0 && c4 == 0) ) { // Collinear or overlapping segments on same line : // We check bounding boxes for overlap . That still not 2D overlap ( area ). // So do not treat as an area intersection . return false ; } // If there genuine crossing , each cross product pair has different sign // (i.e. c1 and c2 differ in sign , c3 and c4 differ in sign ). // That means proper crossing that yields an intersection in the interior . bool s1 = ( c1 < 0 && c2 > 0) ( c1 > 0 && c2 < 0) ; bool s2 = ( c3 < 0 && c4 > 0) ( c3 > 0 && c4 < 0) ; return ( s1 && s2 ); 29 31 32 33 34 35 37 38 39 40 41 43 44 45 } 46 47 // Checks if point is strictly inside the polygon poly ( not on boundary ). 48 // Standard ray casting or winding approach . Here we ll use winding number 49 // or cross approach . We exclude boundary checks , so if it is on boundary 50 // we ll consider that \" not strictly inside .\" 51 bool pointInPolygonStrict ( const vector < Point > & poly , const Point &P) { 17 52 54 55 56 57 58 60 61 62 63 64 66 67 68 Figure 7 An example problem Color the Map Extreme. // typical approach is to check the sum of angles or the winding number . // We ll do ray - casting approach that ignores boundary if crossing hits vertex . // Because coordinates can be negative and up to 1000 in absolute value , we ll do // the standard approach carefully with integer checks . int cnt = 0; for ( int = 0; < ( int ) poly . size () ; ++) { Point = poly [i ]; Point = poly [( + 1) % poly . size () ]; // Check if the ray from horizontally to the right intersects segment AB . // We ll skip boundary checks in way that if is exactly on an edge , we return false . if (( A.y > P.y) != ( B.y > P.y)) { // Possible crossing in long double x_cross = ( long double )(B.x - A.x) * ( long double )(P.y - A.y) / ( long double )(B.y - A.y) + A.x; if ( x_cross > P.x) { cnt ++; } 18 } // Also check if is exactly on segment AB . If so , it not strictly inside . long long = cross (A , , P); if ( == 0) { // Check if is within bounding box of and if ( min (A.x , B.x) <= P.x && P.x <= max (A.x , B.x) && min (A.y , B.y) <= P.y && P.y <= max (A.y , B.y)) { // On boundary return false ; } } } // If odd , inside ; if even , outside . // Because we skip boundary as \" strict inside ,\" we only count if definitely inside . return ( cnt % 2 == 1) ; 69 70 72 73 74 75 76 78 79 80 81 82 83 84 } 85 86 // Check if polygon is simple ( no self intersections ) 87 // and has strictly positive area ( CCW is required ). 88 void checkSimplePolygon ( const vector < Point > & poly ) { 89 90 91 92 94 95 96 97 98 100 101 102 103 104 106 107 108 109 110 112 113 114 115 116 118 119 120 121 122 124 125 126 127 128 int = ( int ) poly . size () ; // 1) Check no two vertices coincide . // 2) Check edges do not intersect except at adjacent vertices . // 3) Check area > 0 and orientation is CCW . // Distinct vertices : set < pair < long long , long long >> st ; for ( auto & pt : poly ) { auto = make_pair ( pt .x , pt .y); ensuref (! st . count (p) , \" Polygon has duplicate vertices .\"); st . insert (p); } // Check area ( signed area for CCW ). // Standard shoelace formula : long long area2 = 0; // 2 * area , signed for ( int = 0; < m; ++) { int = (i + 1) % m; area2 += ( poly [i ]. * poly [j ]. - poly [j ]. * poly [i ]. y); } ensuref ( area2 > 0, \" Polygon must have positive area and CCW order ( area2 =% lld )\" , area2 ); // Check self - intersections : // We ll compare every pair of edges (i , +1) with (j , +1) , skipping adjacency . for ( int = 0; < m; ++) { Point p1 = poly [i ]; Point p2 = poly [( + 1) % ]; for ( int = + 1; < m; ++) { // edges (i ->i +1) and (j ->j +1) share endpoints if == +1 or == +1 // ( mod m). We ll skip adjacency or the same edge . if ( abs (i - j) <= 1 (i == 0 && == - 1) (j == 0 && == - 1) ) { continue ; // adjacent edges or the same edge } Point p3 = poly [j ]; Point p4 = poly [( + 1) % ]; // If proper intersection occurs , it invalid : if ( segmentsProperlyIntersect (p1 , p2 , p3 , p4 )) { ensuref ( false , \" Polygon edges intersect in their interiors .\"); } } } 129 130 } 131 132 // Check if polygons and share any area . 133 // Return true if they do share area > 0, false if they only meet at boundary or do not meet at all . 134 bool polygonsShareArea ( const vector < Point > &A , const vector < Point > &B) { 19 // We ll do : // 1) Check if any vertex of is strictly inside => area overlap // 2) Check if any vertex of is strictly inside => area overlap // 3) Check if edges properly intersect ( cross ) => area overlap // If none of these => no area overlap . // Overlapping in line or point only is fine ( no area ). for ( auto &p : A) { if ( pointInPolygonStrict (B , p)) { return true ; } } for ( auto &p : B) { if ( pointInPolygonStrict (A , p)) { return true ; } } // Check edges pairwise for crossing intersection for ( int = 0; < ( int )A. size () ; ++) { Point p1 = A[i ]; Point p2 = [( + 1) % A. size () ]; for ( int = 0; < ( int )B. size () ; ++) { Point p3 = B[j ]; Point p4 = [( + 1) % B. size () ]; if ( segmentsProperlyIntersect (p1 , p2 , p3 , p4 )) { return true ; } } } return false ; 135 137 138 139 140 141 143 144 145 146 147 149 150 151 152 153 155 156 157 158 159 161 162 163 164 } 165 166 int main ( int argc , char * argv []) { 167 169 170 171 172 173 175 176 177 178 179 181 182 183 184 185 187 188 189 190 191 193 194 195 196 197 199 200 201 registerValidation ( argc , argv ); while ( true ) { int = inf . readInt (0 , 35) ; // can be zero or up to 35 inf . readEoln () ; if ( == 0) { // End of input break ; } // Read polygons vector < vector < Point >> polygons ( n); for ( int = 0; < n; ++) { int = inf . readInt (3 , 50) ; inf . readEoln () ; vector < Point > poly (m); for ( int = 0; < m; ++) { long long = inf . readLong ( -1000 , 1000) ; inf . readSpace () ; long long = inf . readLong ( -1000 , 1000) ; inf . readEoln () ; poly [j] = {x , }; } // Check that this polygon is valid ( simple , area >0 , CCW ). checkSimplePolygon ( poly ); polygons [i] = poly ; } // Check that no two polygons share area . // ( They may only meet at boundary or be disjoint .) for ( int = 0; < n; ++) { for ( int = + 1; < n; ++) { bool hasAreaOverlap = polygonsShareArea ( polygons [i], polygons [j ]) ; ensuref (! hasAreaOverlap , \" Two polygons share area ( overlap ).\"); } } } inf . readEof () ; return 0; 202 203 205 206 207 } D.2 Demo: Generator and Checker Problem: https://codeforces.com/problemset/problem/152/E The generator of this problem is as follows: 1 # include \" testlib .h\" 2 # include < bits / stdc ++. > 3 using namespace std ; 4 5 /* This generator produces single test case for the \" Vasya Garden \" problem . It reads four arguments : , , , and type ( default = \" random \") . Constraints to satisfy : 1 <= , <= 100 n*m <= 200 1 <= <= min (n*m , 7) 1 <= a[i ][ j] <= 1000 All important squares must have different coordinates . The generator supports several \" type \" parameters to produce variety of data patterns , exposing different corner cases . NOTE : - Please DO NOT set or parse random seed parameter . - This generator will produce ONE test . - We rely on \" rnd \" from testlib for randomness . 6 7 8 9 11 12 13 14 15 17 18 19 20 21 22 23 */ 24 25 int main ( int argc , char * argv []) 26 { 27 28 29 30 32 33 34 35 36 38 39 40 41 42 44 45 46 47 48 50 51 52 53 54 registerGen ( argc , argv , 1) ; // Parsing parameters with default values where appropriate int = opt < int >( \"n\"); int = opt < int >( \"m\"); int = opt < int >( \"k\"); string type = opt < string >( \" type \" , \" random \"); // Basic validation ( not strictly required but good for sanity ) // Ensure 1 <= n*m <= 200 // Ensure 1 <= <= min (n*m , 7) // We ll assume user input doesn violate constraints , but you may check if needed . if (n <= 0 <= 0 * > 200 < 1 > min (n*m , 7) ) { cerr << \" Invalid parameters : n= \" << << \" , m=\" << << \" , k=\" << << endl ; return 1; } // Create 2D array to store the number of flowers . // We ll fill it depending on the \" type \". vector < vector < int >> garden (n , vector < int >(m , 0) ); // helper lambda to generate random in [1..1000]. auto genValue = [&]() { return rnd . next (1 , 1000) ; }; // Fill the garden according to \" type \": if ( type == \" allmin \") { 21 55 57 58 59 60 61 63 64 65 66 67 69 70 71 72 73 75 76 77 78 79 81 82 83 84 85 87 88 89 90 91 93 94 95 96 97 99 100 101 102 103 105 106 107 108 109 111 112 113 114 115 117 118 119 120 // All squares have 1 flower for ( int = 0; < n; ++) { for ( int = 0; < m; ++) { garden [i ][ j] = 1; } } } else if ( type == \" allmax \") { // All squares have 1000 flowers for ( int = 0; < n; ++) { for ( int = 0; < m; ++) { garden [i ][ j] = 1000; } } } else if ( type == \" random \") { // Fully random for ( int = 0; < n; ++) { for ( int = 0; < m; ++) { garden [i ][ j] = genValue () ; } } } else { // For \" corners \", \" line \", \" block \", or anything else , we ll do random fill // and then handle building squares in special pattern . for ( int = 0; < n; ++) { for ( int = 0; < m; ++) { garden [i ][ j] = genValue () ; } } } // We ll store the building squares here vector < pair < int , int >> buildings ; buildings . reserve (k); // According to \" type \", choose building squares . // Must ensure distinct squares . We ll do it differently for each type . // Indices are 1based for the final output , but we ll pick 0based internally . if ( type == \" corners \") { // Up to 4 corners : (0 ,0) , (0 ,m -1) , (n -1 ,0) , (n -1 ,m -1) // If <= 4, place them in corners first . If > 4, fill corners , then random for the rest . vector < pair < int , int >> corners ; corners . push_back ({0 , 0}) ; if ( > 1) corners . push_back ({0 , -1}) ; if ( > 1) corners . push_back ({n -1 , 0}) ; if ( > 1 && > 1) corners . push_back ({n -1 , -1}) ; int used = 0; for ( int = 0; < ( int ) corners . size () && used < k; ++) { buildings . push_back ( corners [c ]) ; used ++; } // If not enough , fill the remainder randomly while (( int ) buildings . size () < k) { int rr = rnd . next (0 , -1) ; int cc = rnd . next (0 , -1) ; if ( find ( buildings . begin () , buildings . end () , make_pair (rr , cc )) == buildings . end () ) { buildings . push_back ({ rr , cc }) ; } } } else if ( type == \" line \") { // Place building squares in the first row , left to right , then second row , etc . // We only do this if it doesn exceed n*m ( which it doesn t: <= n*m). 22 // If squares don fit in row 1 alone , continue row by row . // Could also do single row if =1 or =2 , etc . int count = 0; for ( int = 0; < && count < k; ++) { for ( int = 0; < && count < k; ++) { buildings . push_back ({i , }) ; count ++; } } } else if ( type == \" block \") { // Place building squares in top - left block // We ll place them in reading order ( row by row ) int count = 0; for ( int = 0; < && count < k; ++) { for ( int = 0; < && count < k; ++) { buildings . push_back ({i , }) ; count ++; } } } else { // \" random \" or any unknown type => pick distinct squares at random // ( If we got here from \" random \" or leftover type , we handle it similarly .) // We ll skip the fill since we already filled the grid above . // Just pick distinct squares randomly . set < pair < int , int >> used ; while (( int ) used . size () < k) { int rr = rnd . next (0 , -1) ; int cc = rnd . next (0 , -1) ; used . insert ({ rr , cc }) ; } for ( auto &x: used ) { buildings . push_back (x); } } // Now we have our matrix of flowers and set of building squares . // Output format : // // // line 1: next lines : each line has integers next lines : \" row col \" for building squares (1 - based ) printf (\"%d %d %dn\" , , , k); for ( int = 0; < n; ++) { for ( int = 0; < m; ++) { printf (\"%d%c\" , garden [i ][ j], (j +1 == ? : )); } } for ( int = 0; < k; ++) { // Add 1 for 1based coordinates printf (\"%d %dn\" , buildings [i ]. first + 1, buildings [i ]. second + 1) ; } 121 122 123 124 126 127 128 129 130 132 133 134 135 136 138 139 140 141 142 144 145 146 147 148 150 151 152 153 154 156 157 158 159 160 162 163 164 165 166 168 169 170 171 172 174 return 0; 175 176 } The generator commnads are: 1 # Below are approximately 20 distinct commands to run the generator . 2 # Each produces single test case on stdout . 3 4 # 1) Minimum case : 1 x1 grid 5 ./ gen -n 1 -m 1 -k 1 - type random 6 7 # 2) Small case : 1 x2 grid , =1 8 ./ gen -n 1 -m 2 -k 1 - type random 9 23 10 # 3) Square grid , small 11 ./ gen -n 2 -m 2 -k 2 - type corners 12 13 # 4) Another small grid 14 ./ gen -n 2 -m 3 -k 2 - type line 15 16 # 5) Another small grid with \" block \" 17 ./ gen -n 3 -m 3 -k 3 - type block 18 19 # 6) Mixed dimensions , near upper limit in product (10 * 20 = 200) 20 ./ gen -n 10 -m 20 -k 3 - type random 21 22 # 7) Same n*m =200 but different 23 ./ gen -n 10 -m 20 -k 7 - type corners 24 25 # 8) Another large shape , tall but narrow grid 26 ./ gen -n 25 -m 8 -k 5 - type line 27 28 # 9) Also near max area 29 ./ gen -n 20 -m 10 -k 7 - type block 30 31 # 10) All minimal values 32 ./ gen -n 5 -m 5 -k 5 - type allmin 33 34 # 11) All maximal values 35 ./ gen -n 5 -m 5 -k 3 - type allmax 36 37 # 12) Very narrow , but tall 38 ./ gen -n 100 -m 2 -k 2 - type random 39 40 # 13) Another shape with block 41 ./ gen -n 8 -m 8 -k 6 - type block 42 43 # 14) \" line \" style with bit bigger dimension 44 ./ gen -n 5 -m 10 -k 4 - type line 45 46 # 15) Corners with bigger dimension 47 ./ gen -n 4 -m 7 -k 4 - type corners 48 49 # 16) Random distribution with =7 50 ./ gen -n 10 -m 10 -k 7 - type random 51 52 # 17) Dimensions 4 x8 = 32 squares 53 ./ gen -n 4 -m 8 -k 3 - type random 54 55 # 18) Single row case 56 ./ gen -n 1 -m 10 -k 5 - type line 57 58 # 19) Single column case 59 ./ gen -n 10 -m 1 -k 3 - type block 60 61 # 20) Another random large (n*m =200) with max =7 62 ./ gen -n 4 -m 50 -k 7 - type random This is also problem with multiple valid answers. The checker of this problem is: 1 # include \" testlib .h\" 2 # include < bits / stdc ++. > 3 using namespace std ; 4 5 int , , k; 6 vector < vector < int >> a; 7 vector < pair < int , int >> important_squares ; 8 9 int main ( int argc , char * argv []) { 10 11 12 registerTestlibCmd ( argc , argv ); = inf . readInt () ; 24 13 14 15 16 18 19 20 21 22 24 25 26 27 28 30 31 32 33 34 36 37 38 39 40 42 43 44 45 46 48 49 50 51 52 54 55 56 57 58 60 61 62 63 64 66 67 68 69 70 72 73 74 75 = inf . readInt () ; = inf . readInt () ; // Read the garden flower counts a. resize (n , vector < int >( m)); for ( int = 0; < n; ++) { for ( int = 0; < m; ++) { a[i ][ j] = inf . readInt () ; } } // Read the important squares set < pair < int , int >> important_set ; for ( int = 0; < k; ++) { int = inf . readInt () ; int = inf . readInt () ; important_squares . emplace_back (x - 1, - 1) ; important_set . emplace (x - 1, - 1) ; } // Read the jury answer ( minimal total sum ) int jans = ans . readInt () ; // Read the participant total sum int pans = ouf . readInt () ; // Read the participant plan vector < string > plan (n); for ( int = 0; < n; ++) { plan [i] = ouf . readToken () ; if ( int ( plan [i ]. length () ) != m) { quitf ( _wa , \" Invalid plan : line %d should have length %d , but has length %d\" , + 1, , int ( plan [i ]. length () )); } for ( char : plan [i ]) { if (c != && != . ) { quitf ( _wa , \" Invalid character %c in plan at line %d\" , , + 1) ; } } } // Compute the actual total sum over s int actual_pans = 0; for ( int = 0; < n; ++) { for ( int = 0; < m; ++) { if ( plan [i ][ j] == ) { actual_pans += a[i ][ ]; } } } if ( actual_pans != pans ) { quitf ( _wa , \" The total sum of dead plants does not match the plan : expected %d , found %d\" , actual_pans , pans ); } if ( pans > jans ) { quitf ( _wa , \" Participant total sum (% d) is greater than minimal total sum (% d)\" , pans , jans ); } else if ( pans < jans ) { quitf ( _fail , \" Participant total sum (% d) is less than minimal total sum (% d)\" , pans , jans ); } // Check that all important squares are covered with concrete (X ) for ( auto [x , y] : important_squares ) { if ( plan [x ][ y] != ) { quitf ( _wa , \" Important square (%d , %d) is not covered with concrete \" , + 1, + 1) ; } } // Check connectivity between all important squares queue < pair < int , int >> q; vector < vector < bool >> visited (n , vector < bool >(m , false )); q. push ( important_squares [0]) ; visited [ important_squares [0]. first ][ important_squares [0]. second ] = true ; // Directions : up , down , left , right int dx [] = { -1 , 1, 0, 0}; int dy [] = {0 , 0, -1, 1}; while (! q. empty () ) { auto [x , y] = q. front () ; q. pop () ; for ( int dir = 0; dir < 4; dir ++) { int nx = + dx [ dir ]; int ny = + dy [ dir ]; if (0 <= nx && nx < && 0 <= ny && ny < m) { if (! visited [ nx ][ ny ] && plan [ nx ][ ny ] == ) { visited [ nx ][ ny ] = true ; q. emplace (nx , ny ); } } } } // Verify that all important squares are connected for ( auto [x , y] : important_squares ) { if (! visited [x ][ ]) { quitf ( _wa , \" Important square (%d , %d) is not connected to all other important squares \" , + 1, + 1) ; } } 76 77 79 80 81 82 83 85 86 87 88 89 91 92 93 94 95 97 98 99 100 101 103 104 105 106 107 109 110 111 112 quitf ( _ok , \" Correct solution with minimal total sum %d\" , pans ); 113 114 } D.3 Case Study: Problems with Stronger Test Cases than Official Test Cases Problem: https://codeforces.com/problemset/problem/392/D The following submission passes the official test cases. 1 # include < bits / stdc ++. > 2 using namespace std ; 3 const int MAXN = 6 e5 + 21 , inf = 1 e7 + 21; 4 int , sz ; 5 int A[ MAXN ], B[ MAXN ], C[ MAXN ], a[ MAXN ], b[ MAXN ], c[ MAXN ]; 6 int dp [ MAXN ], cnt [ MAXN ]; 7 int bw [ MAXN << 2] , lazy [ MAXN << 2]; 8 struct node { 9 int ans , mn , mx ; node ( int = 0, int = 0, int = 0) { mn = , mx = , ans = c; } 10 11 } seg [ MAXN << 2]; 12 inline node MRG ( node , node b) { return node ( min (a.mn , b. mn ) , max (a.mx , b. mx ) , min (a. ans , b. ans )); 13 14 } 15 inline void relax ( int , int id , int st ) { seg [ id ] = node ( lazy [ id ] = , , <= ? + st : 3 * n); 16 17 } 18 inline void shift ( int id , int st , int en ) { 20 if (!( lazy [ id ]) ) return ; int mid = ( st + en ) >> 1; 26 21 22 27 28 29 30 31 33 37 38 39 40 42 53 54 55 56 58 59 60 61 62 64 65 66 67 68 70 71 72 relax ( lazy [ id ], id << 1, st ); relax ( lazy [ id ], id << 1 1, mid ); lazy [ id ] = -1; 23 24 } 25 void build ( int id = 1, int st = 0, int en = + 1) { lazy [ id ] = -1; if ( en - st == 1) { seg [ id ] = node ( dp [ st ], dp [ st ], dp [ st ] + st ); return ; } int mid = ( st + en ) >> 1; build ( id << 1, st , mid ); build ( id << 1 1, mid , en ); seg [ id ] = MRG ( seg [ id << 1] , seg [ id << 1 1]) ; 34 35 } 36 void update ( int , int , int , int id = 1, int st = 0, int en = + 1) { if (r <= st en <= seg [ id ]. mn >= x) return ; if (l <= st && en <= && seg [ id ]. mx < x) return relax (x , id , st ); shift (id , st , en ); int mid = ( st + en ) >> 1; update (l , , , id << 1, st , mid ); update (l , , , id << 1 1, mid , en ); seg [ id ] = MRG ( seg [ id << 1] , seg [ id << 1 1]) ; 46 43 44 } 45 inline void pre () { int cur = 0; sort (bw , bw + sz ); sz = unique (bw , bw + sz ) - bw ; fill (A , + sz , + 1) ; fill (B , + sz , + 1) ; fill (C , + sz , + 1) ; for ( int = n; i; - -) { 48 49 51 50 a[i] = lower_bound (bw , bw + sz , a[i ]) - bw ; b[i] = lower_bound (bw , bw + sz , b[i ]) - bw ; c[i] = lower_bound (bw , bw + sz , c[i ]) - bw ; A[a[i ]] = i; B[b[i ]] = i; C[c[i ]] = i; } for ( int = 1; <= n; ++) { cur += ! cnt [a[i ]]++; cur += ! cnt [c[i ]]++; } for ( int = 0, = n; <= n; ++) { cur += ! cnt [b[i ]]++; while (p && cnt [c[p ]] > 1) cnt [c[p - -]] - -; if ( cur < sz ) dp [i] = 3 * n; else dp [i] = p; } build () ; return ; 73 74 } 75 int main () { 76 78 79 80 81 82 84 85 86 87 ios :: sync_with_stdio (0) , cin . tie (0) , cout . tie (0) ; cin >> n; for ( int = 1; <= n; ++) { cin >> a[i ]; bw [ sz ++] = a[i ]; } for ( int = 1; <= n; ++) { cin >> b[i ]; bw [ sz ++] = b[i ]; } for ( int = 1; <= n; ++) { cin >> c[i ]; 27 bw [ sz ++] = c[i ]; } pre () ; int ans = + seg [1]. ans ; for ( int = n; > 0; - -) { if (A[a[ ]] == i) { update (0 , B[a[i ]] , C[a[i ]]) ; } ans = min ( ans , + seg [1]. ans - 1) ; } cout << ans ; return 0; 89 90 91 92 93 95 96 97 98 99 100 } The agent system constructed the following input: 1 1 2 497025789 3 364691059 4 954413461 The correct answer is 3 but the program outputs 2. Therefore, this submission is actually incorrect."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Peking University"
    ]
}