{
    "paper_title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
    "authors": [
        "Chenlong Deng",
        "Zhisong Zhang",
        "Kelong Mao",
        "Shuaiyi Li",
        "Xinting Huang",
        "Dong Yu",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities."
        },
        {
            "title": "Start",
            "content": "A Silver Bullet or Compromise for Full Attention? Comprehensive Study of Gist Token-based Context Compression Chenlong Deng1,2, Zhisong Zhang2, Kelong Mao1, Shuaiyi Li2, Xinting Huang2, Dong Yu2, Zhicheng Dou1 1Gaoling School of Artificial Intelligence, Renmin University of China 2Tencent AI Lab {dengchenlong,dou}@ruc.edu.cn zhisonzhang@tencent.com"
        },
        {
            "title": "Abstract",
            "content": "In this work, we provide thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gistbased compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segmentwise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are increasingly recognized as key pathway toward general artificial intelligence (OpenAI, 2023; Zhao et al., 2023), with long-context processing emerging as critical research frontier (Chen et al., 2023; Peng et al., 2024). This capability is crucial for advanced applications like retrieval-augmented generation (RAG), long-term memory systems, and complex reasoning frameworks (Gao et al., 2023; Zhu et al., 2023; Zhang et al., 2024c; Wei et al., 2022; Lightman et al., 2024). Despite the proliferation of architectural innovations, Transformerbased models remain the performance standard. This work was done during internship at Tencent AI Lab. *Corresponding authors. However, these architectures face significant computational challenges when processing extended text sequences: the key-value (KV) cache memory grows linearly with sequence length, while the attention mechanisms quadratic computational scaling introduces substantial overhead. In models like Llama3-8B (Meta-Llama, 2024), 128K context KV cache can consume memory equivalent to the entire models parameters, limiting deployment on edge devices and constraining context windows. promising approach to mitigate these challenges involves reducing overhead by compressing the number of past tokens stored in the KV cache. This work focuses on specific type of compression method that condenses the context into small set of special tokens, called gist tokens (Mu et al., 2023).1 By replacing the original tokens with limited number of gist tokens, these methods effectively reduce both KV cache size and computational cost. While such techniques have been successfully applied in real-world tasks (Qian et al., 2024), two critical questions remain unresolved: Q1: To what extent can this architecture replace full attention models? Q2: Does the compression introduce potential, yet significant, failure patterns? In this work, we thoroughly investigate these two questions through extensive experiments. Specifically, we propose unified framework for categorizing existing gist-based model architectures along two dimensions: Memory Location and Gist Granularity. We provide comprehensive evaluations for them with wide range of language tasks. For Q1, our findings indicate that the finegrained KV cache architecture (referred to as Fine KV) is highly effective, achieving near-lossless compression performance on various tasks, such as RAG, long-document QA, and summarization, when compared to the full attention model. How1Previous works refer to this concept by various names. We unify these terms and refer to them as gist tokens for consistency in this paper. 4 2 0 2 3 2 ] . [ 1 3 8 4 7 1 . 2 1 4 2 : r Figure 1: Overview of gist token-based context compression architectures. Long texts are segmented for compression, enabling diverse architectures through different memory locations and gist granularity. ever, it still exhibits notable gaps in tasks like reranking and synthetic recall, suggesting that while promising, it is prone to severe compression failures in certain scenarios. Regarding Q2, we conduct probing experiment focused on context reconstruction and discover that the compression bottlenecks occur in the gist representations. We further identify three failure patterns resulting from this bottleneck: 1) lost by the boundary, where generation degrades near the start of segment; 2) lost if surprise, where unexpected details tend to be ignored if budgets are limited; and 3) lost along the way, where compressed models make errors midway for tasks requiring precise recall. Building on the above findings, we further propose two strategies to enhance the Fine KV architecture for more effective context compression. The first, fine-grained autoencoding, adds weak decoder with an autoencoding loss to reconstruct original token information from gist tokens, ensuring efficient and accurate compression. The second, segment-wise token importance estimation, adjusts loss weights based on tokens dependency on the compressed context, dynamically optimizing tokens that require more contextual understanding. Experiments show that both strategies significantly improve model performance, with joint optimization achieving the best results. The contributions of this work are: We propose unified framework for categorizing existing gist-based model architectures and conduct comprehensive experiments to evaluate their effectiveness. (2) We show that that gist-based models achieve nearlossless performance on many tasks but still face challenges in particular scenarios. (3) We identify three critical failure patterns arising from compression bottlenecks, offering valuable insights into the limitations of current gist-based compression methods. (4) We propose two strategies: fine-grained autoencoding and segment-wise token importance estimation, which effectively mitigate these bottlenecks and enhance model performance. (5)"
        },
        {
            "title": "2 Preliminaries",
            "content": "Gist token-based context compression reduces KV cache by using some special tokens, which are referred to as gists, to represent the full context. The number of special tokens is much fewer than that of the full context, leading to lower memory usage. While many pervious work studies compressing the full prompt at once (Mu et al., 2023; Ge et al., 2024b), we focus on generalized scenario that dynamically compresses and generates context on the fly, as such setting holds promise for broader general-purpose tasks. To this end, we provide unified perspective to analyze and understand existing architectures. Figure 1 illustrates an overview of gist-based context compression methods. We take segmentwise approach that splits the input sequence into segments and iteratively applies compression for each segment. Assuming an input sequence = [x1, . . . , xn], it is divided into segments of fixed length L, where the i-th segment is represented as Si = [x(i1)L+1, . . . , x(i1)L+L]. When processing the i-th segment, the model accumulates all previously compressed information and generates new compressed representations as the memory for later processing: ˆG<(i+1) LLM([ ˆG<i, Insert(Si, Gi)]) Here, Gi = [g1, . . . , gt] are new gist tokens inserted into the i-th segment, and ˆGi are compressed context representations preceding this segment. The function Insert() denotes the insertion of gist tokens into the input sequence. This procedure effectively compresses the information of tokens into tokens, achieving compression ratio of L/t. For example, with compression ratio of 4, every four raw tokens can be replaced by one gist token on average, thereby reaching 75% reduction in memory usage. Following this formula, existing architectures can be categorized along two dimensions: memory location and gist granularity. Memory Location After the forward pass of each segment, we can choose to store either the last hidden states of the gist tokens or their KV cache as memory. Opting for the last hidden states is commonly referred to as recurrent memory, which serves as input embeddings to deliver compressed context to subsequent segments. Note that this design can be viewed as segment-wise RNN, and typical representatives include RMT (Bulatov et al., 2022) and AutoCompressors (Chevalier et al., 2023). Alternatively, the KV cache of the gist tokens can be directly reused as the memory to avoid extra computations, and this shares the same design as in sparse attention. Typical representatives of the KV approach include Gist (Mu et al., 2023), Landmark (Mohtashami and Jaggi, 2023) and Activation Beacon (Zhang et al., 2024a). Gist Granularity The Insert() function in the formula can be implemented in two ways: (1) Coarse-grained: Gist tokens are appended after all raw tokens, allowing each gist token to attend to the entire segment and all preceding contexts, which is the scheme adopted in most previous works; (2) Fine-grained: Gist tokens are evenly inserted among the raw tokens, enabling each gist token to focus on specific context, which is investigated in Activation Beacon (Zhang et al., 2024a). Besides, this design can also enhance language modeling through an implicit chain-of-thought mechanism. Notably, the combination of recurrent memory and fine-grained gist tokens is practically infeasible, since it requires too many non-parallelizable forward passes within segment. Therefore, we mainly explore the remaining three combinations in this work, as illustrated in Figure 1."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Training Recipes In our main experiments, we perform continued-training on the base models using general-purpose corpus to analyze their intrinsic context compression capabilities. To avoid potential confounding effects from techniques like supervised fine-tuning, we focus exclusively on the base models rather than the SFT ones.2 Specifically, we select Llama3.1-8B (Meta-Llama, 2024) and Qwen2-7B (Qwen-Team, 2024) as our base models, given their widespread recognition and adoption in the community. We use the SlimPajama dataset and follow the processing procedure of Fu et al. (2024), by upsampling long sequences and ultimately obtaining 3B tokens for training. Further training details are provided in Appendix A. Evaluation Tasks We perform extensive experiments, covering wide range of tasks: (1) Language modeling, for which we evaluate perplexity on PG19 (Rae et al., 2020), ProofPile (Zhangir Azerbayev), and CodeParrot (CodeParrot); (2) Weak Context-dependent Tasks,3 for which we evaluate four tasks with MMLUPro (Wang et al., 2024), GSM8K (Cobbe et al., 2021), HellaSwag (Zellers et al., 2019), and BBH (Suzgun et al., 2023), to evaluate the models abilities in knowledge, mathematics, common sense, and comprehensive reasoning, respectively; (3) Long Context Tasks, which thoroughly assess the models handling of long texts and we select seven types of tasks: RAG, Rerank, LongQA, Many-shot ICL, Synthetic Recall, Summarization, and Code. The datasets selected for testing these tasks include portions from popular longtext benchmarks such as RULER (Hsieh et al., 2024) and Bench (Zhang et al., 2024b). Inspired by Yen et al. (2024)s setting, we adopt 2-shot demonstrations to ensure robust evaluation of long-context performance. Further details on the datasets and metrics are provided in Appendix B."
        },
        {
            "title": "3.2 Overall Performance Comparisons",
            "content": "We present the results of the Llama model in the main text, while the results of the Qwen model are 2Extra analysis of SFT is showed in Appendix D. 3These tasks do not inherently require long contexts. We increase their context length by adding demonstration examples, although the tasks themselves exhibit only weak dependence on this additional context. Figure 2: Comparisons of different compression methods on perplexity evaluation for language modeling. presented in Appendix C. Ratio Type MMLU-Pro BBH GSM8K HellaSwag Language Modeling As shown in Figure 2, the differences between the architectures are clear and consistent across all datasets. Full attention outperforms all methods that compress contexts. Among the compression-enhanced architectures, fine-grained compression delivers better performance than coarse-grained, and KV cache performs better than recurrent memory. Note that the absolute differences in perplexity are small; for example, with compression ratio of 4, the gap between the fine-grained KV cache and the full attention on Proof-Pile is only 0.1. Weak Context-dependent Tasks As shown in Table 1,4 among four datasets, full attention shows clear advantage only on the BBH dataset, which involves some complex reasoning tasks. In the BBH dataset, reasoning paths can usually extend over several hundred tokens. Long-form reasoning within compressed contexts frequently encounters challenges, such as generating content that spans multiple segments, which results in the accumulation of substantial inaccuracies during the process. This severely impacts the final output. However, in the other three datasets, despite the diversity of task types, the reasoning paths are typically only dozens of tokens long, which explains why compression models maintain near-lossless performance. Long Context Tasks Table 2 presents the results, where we have the following findings: (1) Higher Compression Ratio Leads to Lower Performance. While Fine-KV can achieve comparable performance to full attention in some tasks at lower compression ratios (e.g., 4), it struggle to maintain this level of performance at higher ra- (2) The extent of performance degradatios. tion in compressed models varies significantly - 8 16 32 Full Attention Coarse-Rec Coarse-KV Fine-KV Coarse-Rec Coarse-KV Fine-KV Coarse-Rec Coarse-KV Fine-KV Coarse-Rec Coarse-KV Fine-KV 34.1 34.1 35.3 33.9 34.1 35.6 34.6 34.1 35.6 34. 34.1 35.6 33.6 64.8 53.8 58.1 59.2 54.6 56.1 56.8 53.2 55.7 56.0 54.8 50.6 55. 51.2 50.3 48.7 52.2 51.9 49.0 51.9 50.0 50.1 51.7 50.8 50.5 50.6 82. 81.9 82.3 82.5 82.0 82.2 82.5 81.9 82.2 82.2 81.9 82.2 82.2 Table 1: Performance on weak context-dependent tasks. across different types of tasks. For tasks where the required information is somewhat fuzzy (e.g., Summarization), or where the query is closely related to the general topics of the context (e.g., RAG and LongQA), compression does not noticeably affect the performance. For many-shot ICL, which requires almost the full context, the fine-grained KV cache can maintain performance comparable to full attention even at low compression rates. However, in tasks that demand precise rephrasing or involve highly complex multi-hop reasoning, such as Rerank5, none of the compressed models perform on par with full attention. (3) Coarse-grained methods appear to struggle in fully utilizing the available memory budget. Despite having the same memory budget, the Fine-KVs performance decreases systematically as the compression rate increases, whereas coarse-grained methods show consistently poor performance across different ratios. The trends observed in perplexity evaluation support this finding, suggesting that coarse-grained gist placement is less effective at learning how to optimize the memory budget for compression. 4We report the performance in which contexts are compressed at least once here. Additional results in the shortcontext setting can be found in Appendix B.1 5This task needs O(n) to evaluate the relevance score for each candidate document, and then sort these documents with O(n log n) on average."
        },
        {
            "title": "Synthetic",
            "content": "Summ. Code Average - 4 8 16 Full Attention Full Attention, Finetune Coarse-grained, Recurrent Coarse-grained, KV Cache Fine-grained, KV Cache Coarse-grained, Recurrent Coarse-grained, KV Cache Fine-grained, KV Cache Coarse-grained, Recurrent Coarse-grained, KV Cache Fine-grained, KV Cache Coarse-grained, Recurrent Coarse-grained, KV Cache Fine-grained, KV Cache 61.8 61. 49.9 51.7 60.6 49.8 50.8 57.6 49.9 50.2 55.4 49.3 49.9 53.1 39.9 38.5 2.1 5.2 23. 1.3 3.8 14.5 1.4 4.4 10.0 1.2 2.6 3.1 41.6 42.3 35.2 33.9 40.3 36.0 36.5 40. 34.9 34.2 40.4 33.6 34.2 37.6 62.3 60.0 29.4 36.0 70.6 25.9 33.6 68.1 20.8 29.1 49. 21.1 25.0 36.4 93.9 91.0 11.2 14.2 40.6 11.2 13.5 26.9 11.2 13.1 13.8 11.1 12.2 11. 23.8 24.1 18.2 17.6 21.0 17.7 16.1 16.7 17.8 16.7 16.3 17.5 17.1 16.1 66.1 65. 59.3 57.8 63.0 58.6 57.2 60.7 57.5 58.1 59.2 58.2 58.2 59.2 55.6 54.7 29.3 30.9 46. 28.6 30.2 40.7 27.6 29.4 34.9 27.4 28.5 31.0 Table 2: Performance comparison among full attention and compression architectures on long context tasks. Bold indicates the best result along the same compression ratio."
        },
        {
            "title": "Compression Fails",
            "content": "Previous results show that gist token-based context compression exhibits discernible performance gap compared to full attention, particularly in tasks like synthetic recall that require exact rehearsal. This suggests the presence of compression bottleneck that prevents the language model from treating gist tokens as equivalent to uncompressed context. We conduct probing experiment to investigate the nature of this bottleneck and examine three critical failure modes arising from it."
        },
        {
            "title": "4.1 Compression Bottleneck Probing",
            "content": "Experimental Setting We adopt the concept of autoencoder to investigate the quality of compressed representations in gist tokens. For this experiment, we use the Fine-KV architecture, which is the most effective compression architecture according to previous results. We evaluate whether each gist token completely stores the contextual information of its corresponding snippet by training probing decoder to recover the corresponding token sequence. We examine two decoders: an LLAMA38B model that inherits the full pre-trained parameters and model with only single transformer block. This allows us to explore the compression quality from the perspective of decoder capacities. Results In Table 3, we report the training loss after 2K training steps for two models, along with their token-level reconstruction accuracy on the PG19 dataset. Although the full model demonstrates superior performance, it still exhibits significant shortcomings in decoding the information Decoder Type Train Loss Reconstruction Accuracy 8 16 32 Weak Strong 2.64 2.01 53.9% 19.2% 9.6% 5.1% 77.3% 39.9% 19.3% 10.0% Table 3: Reconstruction accuracies with different compression ratios (CR). within gist tokens. Under high compression ratios, the models accuracy even falls below 20%, indicating that it can only retain fuzzy content rather than remember the precise details from the original context. Ideally, copying small set of recent tokens should be an easy task, yet probing experiments reveal poor performance. This suggests that the representations of current gist token memory impose severe compression bottleneck, limiting the models capacity to extract and utilize contextual information effectively."
        },
        {
            "title": "4.2 Failure Pattern Observations",
            "content": "The compression bottleneck may evolve into specific failure patterns. We highlight three representative and interesting patterns: Lost by the boundary This discovery stems from an analysis of token-level perplexity distribution. As illustrated in Figure 3, we compute the average perplexity of the tokens at each position within individual segments, excluding the first segment since it lacks gist tokens as contextual input. The results reveal that, while token perplexity in the full attention model remains relatively uniform across positions, the compressed model exhibits clear pattern of higher perplexity at the start of the segment and lower perplexity toward the end. Furthermore, we evaluated the impact on generation tasks by truncating the context to specific Figure 3: Average Perplexity of tokens in different positions among segments. Needle Type Rel. Compression Ratio 4 8 16 32 Word Number 89.8(+0.0) 89.6(-0.2) 84.5(+0.0) 84.4(-0.1) 50.7(+0.0) 35.8(-14.9) 69.2(+0.0) 59.0(-10.2) 26.0(+0.0) 18.0(-8.0) 26.3(+0.0) 20.9(-5.7) 19.6(+0.0) 16.8(-2.8) 17.2(+0.0) 16.6(-0.6) Table 4: Performance on synthetic recall task (PopQA). the overarching theme of the context. To validate this, we construct synthetic dataset6 with different configurations based on the PopQA dataset from the RAG task, as it provides explicit question subjects, and most documents are typically related to the same subject. We randomly insert needle between sentences in the gold document, formatted as: {subj}s special {needle_type} is {needle_content}. Here, {subj} can either be the original subject or Mr. Tree, while {needle_type} can be either food or an 8-digit number. When {subj} is the original subject, we consider the needle to be relevant to the theme of most of the context; otherwise, it is surprising and unrelated. All needles are transformed into compressed gist tokens during the models decoding stage. As shown in Table 4, our experimental results reveal significant performance differences in both needle types when altering only the subject of single sentence. This indicates that the successful retrieval of compressed information is associated with its relevance to the context. An unexpected information is more likely to be lost during compression. Lost along the way We notice that compressionenhanced architectures struggle to recover exact rehearsal effectively. When dealing with relatively long needle, the compression process can scatter critical information across multiple gist tokens. Consequently, even if the model identifies the beginning of the target information, it risks losing track during subsequent steps of generation. To validate this observation, we conducted re6We provide an example for clarity in Table 13 Figure 4: Performance on different tasks while truncating context to the last tokens. When is multiple of 2048, the model will generate near the boundary. Figure 5: Performance on the 32-digit uuid recall task. We report the exact match rates of various first-k digits. length. As shown in Figure 4, with segment length set to 2K, the performance when generation starts at the beginning of segment is substantially worse compared to the case when generation starts from the middle of segment. This indicates that the segment boundary effects influence not only the accuracy of reading specific information but also the models overall language modeling capability. Lost if surprise We find that under constrained memory budgets, the model tends to prioritize retaining detailed information that closely aligns with call experiment using 32-digit UUIDs, comparing the performance of full attention models against compressed models, and analyzed their accuracy across prefixes of varying lengths. As illustrated in Figure 5, the replication accuracy of full attention models remains stable regardless of prefix length, suggesting that once the starting point is identified, copying the rest of the content is straightforward. In contrast, compressed models show significant drop in accuracy, decreasing to less than half of the original as the prefix extends from the first four digits to all 32 digits. This finding highlights the reduced copying reliability associated with compressed representations."
        },
        {
            "title": "5.1 Methodology",
            "content": "Building on these findings, we have identified critical shortcomings in the current architectures context compression. In this section, we propose two effective learning strategies to address them. Fine-grained Autoencoding (AE) The probing experiments in Section 4 indicate that the compressed representations of current gist tokens struggle to reconstruct the original content. To address this issue, we introduce an additional autoencoding loss during training to explicitly encourage the retention of the original contextual information. Different from ICAE (Ge et al., 2024b), we require each gist token to be responsible for specific snippet. Following the mainstream conclusion in autoencoding research that weak decoders help learn better representations (Lu et al., 2021), we adopt single-layer transformer as the decoder. For each gist token gkv , the objective is to reconstruct the original token sequence between the current and previous gist tokens. The input for this task is: [gkv , [ae]r, x1, . . . , xr] where [ae]r is special token to prompt model to reconstruct tokens (i.e., x1 to xr). The loss of autoencoding is similarly defined in an autoregressive way: Lae ="
        },
        {
            "title": "1\nN",
            "content": "1 (cid:88) (cid:88) i=1 j=1 log Pθ(xjgkv , [ae]r, x<j) Segment-wise Token Importance Estimation (TIE) Another approach to promote compression is to adjust the loss weights of different tokens, since each token depends on the context in different degrees. We hypothesize that the importance of token is determined by the modeling difficulty it presents during segment-wise compression. The more token relies on the compressed gist context for prediction, the more effort should be dedicated to learning it. Inspired by LongPPL (Fang et al., 2024), we estimate the reliance of each token (xi) on the gist context and allocate tailored learning weight wi accordingly: Diff(xi) = min(log Pθ(xixseg <i ) Pθ(xixfull <i ) , γ), wi = eDiff(xi) j=1 eDiff(xj ) (cid:80)N . Here, Pθ denotes the original language model, xseg <i denotes the preceding tokens only in the current segment, and xfull <i denotes the full context, including tokens in previous segments. This reliance is quantified by analyzing the difference in modeling probabilities when the token attends to the full context versus the local segment alone."
        },
        {
            "title": "5.2 Experiments",
            "content": "Boundary Effect Test Previous results show that gist-based models demonstrate strong performance on weak context-dependent tasks but are severely constrained by the lost by the boundary phenomenon. We test two improved methods under the same experimental conditions in Section 4, with the results presented in Table 6. Both methods significantly enhance performance in boundary regions, particularly on the BBH dataset, which involves tasks requiring long-form reasoning. This improvement may be attributed to their ability to reduce the accumulation of errors during the generation process. While these methods do not completely eliminate the boundary effect, they offer promising strategies for mitigating its impact. Long Context Tasks Table 5 highlights that both methods consistently enhance the models performance on long-context tasks, particularly under low compression ratios. Key observations include: (1) For tasks where the performance gap between the compression-enhanced model and full attention is relatively small (e.g., RAG and LongQA), both methods maintain excellent performance without negative impacts. For the many-shot ICL task, they even demonstrate continuous improvements. (2) For tasks where the original architectures strugRatio Compression Type - 8 16 32 Full Attention Fine-grained, KV Cache + Fine-grained AE + Segment-wise TIE + Both Strategies Fine-grained, KV Cache + Fine-grained AE + Segment-wise TIE + Both Strategies Fine-grained, KV Cache + Fine-grained AE + Segment-wise TIE + Both Strategies Fine-grained, KV Cache + Fine-grained AE + Segment-wise TIE + Both Strategies RAG 61.8 60.6(+0.0) 60.9(+0.3) 60.4(-0.2) 61.1(+0.5) 57.6(+0.0) 58.3(+0.7) 58.1(+0.4) 58.3(+0.7) 55.4(+0.0) 55.6(+0.2) 55.6(+0.2) 56.3(+0.9) 53.1(+0.0) 54.3(+1.2) 53.1(+0.0) 54.4(+1.3) Rerank LongQA 39.9 41. 23.4(+0.0) 27.4(+4.0) 27.0(+3.6) 27.4(+4.0) 14.5(+0.0) 15.6(+0.9) 17.6(+3.1) 19.7(+5.2) 10.0(+0.0) 11.3(+1.3) 10.4(+0.4) 12.7(+2.7) 3.1(+0.0) 4.6(+1.5) 4.6(+1.5) 4.9(+1.8) 40.3(+0.0) 40.8(+0.5) 41.2(+0.9) 40.3(+0.0) 40.2(+0.0) 39.8(-0.4) 40.0(-0.2) 40.4(+0.0) 40.4(+0.0) 40.4(+0.0) 40.7(+0.3) 41.7(+1.3) 37.6(+0.0) 39.3(+1.7) 40.3(+2.7) 39.8(+2.2) ICL 62.3 70.6(+0.0) 72.0(+1.4) 72.7(+2.1) 75.0(+4.4) 68.1(+0.0) 68.7(+0.6) 70.0(+1.9) 70.7(+2.6) 49.3(+0.0) 47.1(+0.3) 55.5(+8.4) 56.3(+7.0) 36.4(+0.0) 34.1(-2.3) 43.6(+7.2) 41.8(+5.4) Synthetic Summ. 93.9 23. 40.6(+0.0) 62.0(+21.4) 54.3(+13.7) 62.1(+21.5) 26.9(+0.0) 34.8(+7.9) 30.2(+3.3) 35.2(+8.9) 13.8(+0.0) 14.7(+0.9) 14.8(+1.0) 14.9(+1.1) 11.9(+0.0) 13.1(+1.2) 13.1(+1.2) 13.1(+0.9) 21.0(+0.0) 22.3(+1.3) 20.2(-0.8) 22.2(+1.2) 16.7(+0.0) 18.5(+1.8) 17.7(+1.0) 19.5(+2.8) 16.3(+0.0) 16.2(-0.1) 15.3(-1.0) 15.7(-0.6) 16.1(+0.0) 17.1(+1.0) 17.0(+0.9) 17.1(+1.0) Code 66.1 62.0(+0.0) 62.9(+0.9) 62.1(+0.1) 62.9(+0.9) 60.7(+0.0) 61.3(+0.6) 60.7(+0.0) 61.4(+0.7) 59.2(+0.0) 59.6(+0.4) 58.1(-1.1) 59.6(+0.4) 59.2(+0.0) 59.8(+0.6) 59.8(+0.6) 59.8(+0.6) Average 55.6 46.1(+0.0) 49.8(+3.7) 48.3(+2.2) 50.1(+4.0) 40.7(+0.0) 42.4(+1.7) 42.0(+1.3) 43.6(+2.9) 34.9(+0.0) 35.0(+0.1) 35.7(+0.8) 36.7(+1.8) 31.0(+0.0) 31.8(+0.8) 33.1(+2.1) 33.0(+2.0) Table 5: Performance comparisons using our methods, with the best average results bolded for clarity. 2048 Model MMLU-Pro BBH GSM8K Fine-grained KV + Fine-grained AE + Segment-wise TIE Fine-grained KV + Fine-grained AE + Segment-wise TIE 20.3(+0.0) 23.4(+3.1) 22.9(+2.6) 19.7(+0.0) 22.5(+2.8) 22.9(+3.2) 41.3(+0.0) 47.8(+6.5) 46.3(+5.0) 43.8(+0.0) 51.0(+7.2) 50.8(+7.0) 31.9(+0.0) 34.3(+2.4) 32.3(+2.0) 31.8(+0.0) 35.1(+3.3) 34.7(+2.9) Table 6: Improvements of our mitigating methods on the lost by the boundary problem. gle, such as rerank and synthetic recall, both methods deliver remarkable performance gains. For instance, under compression ratio of 4, the improvements on the synthetic recall task reach as high as 52.7% and 33.7%, respectively. These indicate that our methods can effectively enhance the model to read context information from gist tokens."
        },
        {
            "title": "6 Related Work",
            "content": "KV Cache Compression Recent work has explored KV cache optimization at the layer, head, token, and tensor levels. Layer-level methods merge caches across layers using inter-layer similarities (Brandon et al., 2024; Sun et al., 2024; Wu and Tu, 2024; Liu et al., 2024a). Head-level techniques allow multiple query heads to share keyvalue pairs (Ainslie et al., 2023; Shazeer, 2019). Tensor-level approaches, such as low-rank approximations, compress caches into compact representations (DeepSeek-AI, 2024), while quantization reduces precision for memory savings (Liu et al., 2024b). Token-level methods preserve only critical tokens, including learnable tokens (Mu et al., 2023; Ge et al., 2024b; Qin and Durme, 2023; Mohtashami and Jaggi, 2023; Chevalier et al., 2023; Zhang et al., 2024a), token eviction (Zhang et al., 2023; Liu et al., 2023; Ge et al., 2024a), external memory (Xiao et al., 2024a), and hard selection (Li et al., 2023; Jiang et al., 2024b). In this work, we focus on the direction that introduces few learnable special tokens to replace the previous full context. Sparse Attention Researchers have been exploring efficient alternatives of full attention (Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Zhou et al., 2022; Tay et al., 2020). Recently, it has been widely observed that LLMs naturally exhibit significant sparse attention patterns, especially in long-form texts (Jiang et al., 2024a). To leverage such characteristics, researchers have developed heuristic or learnable sparsification strategies that achieve significant speedup while maintaining reliable performance (Jiang et al., 2024a; Xiao et al., 2024b). The gist token-based context compression approach can be regarded as special case of sparse attention with segment-wise approach (Chevalier et al., 2023; Zhang et al., 2024a): where full attention is employed within each segment."
        },
        {
            "title": "7 Conclusion",
            "content": "Our comprehensive evaluation presents that while gist-based context compression shows promise as an alternative to full attention in many tasks, it still falls short in specific scenarios. Through carefully designed probing experiments, we identify critical compression bottlenecks and typical failure modes. Furthermore, we propose two effective strategies that significantly enhance compression performance. These findings offer new insights and directions for advancing context compression techniques in the future."
        },
        {
            "title": "Limitations",
            "content": "Model Scale and Context Length Constrained by our available computational resource, we are able to train long-text large language models with sizes up to 7/8B parameters in 16K context window. Larger models (e.g., Llama3.1-70B) typically have more layers, which enables them to offer greater memory capacity and stronger reading capabilities under the same compression ratio when using gist token-based compression. Thus, such larger models may offer advantages in reducing performance degradation, but this still needs to be verified in future studies. Scope of Compression Methods Our study concentrates on comparative analysis between gist token-based context compression and the full attention mechanism. While other techniques, such as token-dropping methods represented by StreamingLLM and H2O, are also capable of context compression, including them in our scope would go beyond the focus of this paper. Our primary aim is to investigate the effectiveness and limitations of gist token-based context compression, using full attention as the ideal performance upper bound for comparison. Incorporating additional methods would risk complicating the analysis and diluting the focus on the central research question. Therefore, we choose to maintain the scope to ensure clarity and depth in our insights and analysis."
        },
        {
            "title": "Ethical Discussion",
            "content": "This study focuses on the performance of gist tokenbased context compression techniques, without introducing explicitly designed features that could directly influence the cognition of language models. We select widely recognized and validated public training datasets. This can minimize the risk of injecting new biases or toxic data. These datasets are typically subjected to rigorous review and curation, ensuring balanced and stable data distributions. As result, they help mitigate the impact of harmful information on the models learning process and prevent significant distortions in its cognitive and decision-making patterns."
        },
        {
            "title": "References",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 48954901. Association for Computational Linguistics. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan RaganKelley. 2024. Reducing transformer key-value CoRR, cache size with cross-layer attention. abs/2405.12981. Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. 2022. Recurrent memory transformer. CoRR, abs/2207.06881. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. CoRR, abs/2306.15595. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2024. Longlora: Efficient fine-tuning of long-context large language In The Twelfth International Conference models. on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 610, 2023, pages 38293846. Association for Computational Linguistics. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. CodeParrot. https://huggingface.co/codeparrot/codeparrot. DeepSeek-AI. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434. Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. 2024. What is wrong with perplexity for long-context language modeling? CoRR, abs/2410.23771. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. 2024. Data engineering for scaling language models to 128k context. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2024. How to train long-context language models (effectively). CoRR, abs/2410.02660. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrievalaugmented generation for large language models: survey. CoRR, abs/2312.10997. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2024a. Model tells you what to discard: Adaptive KV cache compression for llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2024b. In-context autoencoder for context compression in large language model. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. RULER: whats the real context size of your long-context language models? CoRR, abs/2404.06654. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024a. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. CoRR, abs/2407.02490. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024b. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 16581677. Association for Computational Linguistics. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020. OpenReview.net. Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022. BOOKSUM: collection of datasets for long-form narrative summarization. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 65366558. Association for Computational Linguistics. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023. Compressing context to enhance inference efficiency of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 63426353. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. 2024a. Minicache: KV cache compression in depth dimension for large language models. CoRR, abs/2405.14366. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2023. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024b. KIVI: tuning-free asymmetric 2bit In Forty-first Internaquantization for KV cache. tional Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. 2021. Less is more: Pretrain strong siamese encoder for dense text retrieval using weak decoder. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 27802791. Association for Computational Linguistics. Meta-Llama. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Amirkeivan Mohtashami and Martin Jaggi. 2023. Random-access infinite context length for transformers. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Jesse Mu, Xiang Li, and Noah D. Goodman. 2023. Learning to compress prompts with gist tokens. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2024. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Hongjin Qian, Peitian Zhang, Zheng Liu, Kelong Mao, and Zhicheng Dou. 2024. Memorag: Moving towards next-gen RAG via memory-inspired knowledge discovery. CoRR, abs/2409.05591. Guanghui Qin and Benjamin Van Durme. 2023. Nugget: Neural agglomerative embeddings of text. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2833728350. PMLR. Qwen-Team. 2024. Qwen2 technical report. CoRR, abs/2407.10671. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020. Compressive transformers for long-range sequence modelling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150. Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. 2024. You only cache once: Decoderdecoder architectures for language models. CoRR, abs/2405.05254. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1300313051. Association for Computational Linguistics. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient transformers: survey. CoRR, abs/2009.06732. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574. Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Haoyi Wu and Kewei Tu. 2024. Layer-condensed KV cache for efficient inference of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1117511188. Association for Computational Linguistics. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. 2024a. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. CoRR, abs/2402.04617. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. 2024b. Duoattention: Efficient long-context LLM inference with retrieval and streaming heads. CoRR, abs/2410.10819. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. 2024. HELMET: how to evaluate longcontext language models effectively and thoroughly. CoRR, abs/2410.02694. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 47914800. Association for Computational Linguistics. Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. 2024a. Long context compression with activation beacon. arXiv preprint arXiv:2401.03462. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024b. bench: Extending long context evaluation beyond 100k tokens. CoRR, abs/2402.13718. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. 2024c. survey on the memory mechanism of large language model based agents. CoRR, abs/2404.13501. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. 2023. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Bartosz Piotrowski Zhangir Azerbayev, Edward Ayers. Proofpile: pre-training dataset of mathematical texts. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. survey of large language models. CoRR, abs/2303.18223. Yujia Zhou, Zhicheng Dou, Huaying Yuan, and Zhengyi Ma. 2022. Socialformer: Social network inspired long document modeling for document ranking. In WWW 22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, pages 339 347. ACM. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: survey. CoRR, abs/2308.07107."
        },
        {
            "title": "A Training Details",
            "content": "We train all models using 2B tokens from the upsampled SlimPajama dataset, with document boundaries marked by the eos token. Each model was augmented with 4 sink tokens to enhance modeling stability. To support dynamic compression ratio assignment, the compression ratio for each data instance is randomly sampled from {4, 8, 16, 32}. The context length of the training data is set to 16K, with fixed segment length of 2K. The learning rate is set to 1e-5, using cosine lr scheduler that reduces the learning rate to 50% of its highest value in the end. Additionally, the first 1% of training steps are allocated for learning rate warmup."
        },
        {
            "title": "B Evaluation Details",
            "content": "Perplexity The average perplexity is calculated across all data using 16K-length context window, with sliding window stride equal to the length of the context window. Weak Context-dependent Tasks To ensure that the context for each task is compressed at least once, few-shot examples are used to fill the context. The number of examples used for each task is detailed in Table 7. For all tasks except HellaSwag, which selects answers based on the likelihood of candidate answers, the Chain-of-Thought (CoT) reasoning approach is employed to generate answers. Category RAG Rerank Long-doc QA Many-shot ICL Tasks NQ TriviaQA PopQA HotpotQA MS Marco Bench QA Bench MC TREC Coarse TREC Fine NLU BANKING77 CLINIC Synthetic recall JSON KV RULER MK Needle RULER MK UUID RULER MV Metrics SubEM SubEM SubEM SumEM NDCG@10 ROUGE Recall Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy SubEM SubEM SubEM SubEM Summ. Code Bench Sum Multi-LexSum ROUGE-Sum F1 ROUGE-Sum RepoBench Edit Distance Table 8: Details of long context tasks. Type MMLU-Pro BBH GSM8K HellaSwag Full Attention Coarse, Rec Coarse, KV Fine, KV 35.1 34.8 35.1 35.0 59.0 59.2 58.5 59.5 50.9 50.4 51.6 50.1 79.8 79.3 79.2 79.5 Table 9: Performance of short context tasks. B.1 Results in the Short Context Setting We report model performance in the short context setting in Table 9, in which 2-shot demos are applied and contexts are not compressed. The results indicate that short-context capabilities are not affected by learning compression. Performance of Qwen2-7B"
        },
        {
            "title": "Dataset",
            "content": "#Few-shot demos Answer acquisition MMLU-Pro BBH GSM8K HellaSwag 12 8 16 32 Chain-of-Thought Chain-of-Thought Chain-of-Thought Logits In addition to LLAMA3.1-8B, we also conduct full set of experiments on another widely acknowledged model, QWEN2-7B. The results are shown in Table 10. Table 7: Evaluation setting of weak context-dependent tasks. Long Context Tasks The majority of our task configurations are based on Yen et al. (2024) and Gao et al. (2024), with code tasks leveraging RepoBench. We sample up to 1K samples for each dataset, and contexts are constructed under the configs of max length of 16K. Details are presented in Table 8. We apply greedy decoding to all generation tasks for stability. Results of Supervised Fine-tuning Supervised Fine-tuning (SFT) is critical factor influencing model performance on downstream tasks. Gist token-based context compression models often struggle with certain tasks (e.g., synthetic ones), which may be attributed to the low proportion of long-dependency data in the generalpurpose continue-training corpus. To investigate the effect of high-quality SFT data on the models compression ability, we fine-tune the LLAMA3.18B-INSTRUCT with the Fine-KV architecture. The training data is consisted with LongAlpaca (Chen"
        },
        {
            "title": "Synthetic",
            "content": "Summ. Code Average - 4 8"
        },
        {
            "title": "Full Attention",
            "content": "Coarse-grained, Recurrent Coarse-grained, KV Cache Fine-grained, KV Cache Coarse-grained, Recurrent Coarse-grained, KV Cache Fine-grained, KV Cache Coarse-grained, Recurrent Coarse-grained, KV Cache Fine-grained, KV Cache Coarse-grained, Recurrent Coarse-grained, KV Cache Fine-grained, KV Cache 56.2 44.1 45.4 54. 49.8 44.8 52.0 49.9 45.1 49.5 44.2 45.0 47.5 26.6 0.9 1.6 10.6 1.3 0.5 5. 1.4 0.9 3.1 2.4 1.1 1.7 44.5 35.6 36.2 43.8 36.0 39.3 44.2 34.9 38.6 42. 34.1 37.1 40.6 67.1 27.9 29.8 67.5 25.9 28.5 62.7 20.8 27.9 44.5 27.5 23.6 36. 81.8 12.1 12.4 15.5 11.2 12.3 11.6 11.2 12.2 11.7 11.5 12.2 12.1 19. 19.3 17.8 18.2 17.7 18.1 17.9 17.8 17.8 16.9 18.5 17.6 16.8 64.6 56.9 59.4 59. 58.6 59.4 61.7 57.5 58.7 59.6 57.3 57.9 59.5 51.4 28.1 29.2 38.9 28.6 28.9 36. 27.6 28.7 32.5 27.9 27.8 30.8 Table 10: Long context performance based on QWEN2-7B. Compression Type RAG ICL Synthetic Summ. Avg. Length Model CR. RAG ICL Synthetic Avg. 16K 32K Full Fine-KV Full Fine-KV - 4 - 4 61.8 60.4 60.5 59. 62.3 72.7 74.9 76.8 93.9 62.1 88.7 34.9 72.7 65.1 74.7 57. Table 12: Performance of compression models when inference length exceeds training length. learned the corresponding positional encodings during pre-training, this method holds promise for extrapolating actual inference lengths. Using LLAMA3.1-8B as the base model, we evaluate the compressed model trained with 16K contexts on tasks involving 32K contexts. As shown in Table 12, the results indicate that the compressed model continues to perform well even with context lengths multiple times longer than the training length. This suggests that the ability to read context from gist tokens is generalizable. Fine-KV + SFT 59.9 60.2 75.5 73. 54.1 66.3 21.0 21.7 52.6 55.4 Table 11: Performance of the compression model after SFT (compression ratio=4). et al., 2024), BookSum (Kryscinski et al., 2022), and synthetic data from (Zhang et al., 2024a). We then evaluate its performance on long-context tasks. the fineTable 11 presents the detailed results: tuned model shows significant gains in the previously weakest task (i.e., synthetic recall), while maintaining its performance on tasks where it already excelled. This suggests that long-range supervised signals effectively enhance the ability of gist tokens to preserve precise information in dense memory. Thus, high-quality SFT data containing long-distance dependencies is not only beneficial but potentially essential for the compression model."
        },
        {
            "title": "E Extrapolation Capabilities",
            "content": "This work explores segment-wise context compression method that can effectively reduce the maximum length that each transformer block needs to model. For example, taking LLAMA3-8B as an example, assuming fixed compression ratio of 4 and segment length of 1K, the context length after continue-training would be the same as the pre-training length, which is 8K. Even if the users input context length reaches 16K, exceeding the maximum length after continue-training, the actual maximum length that each transformer block needs to model would only be (16K-1K)/4+1K=4.75K, which still falls within the pre-trained context length of the model. Since the model has already"
        },
        {
            "title": "A Synthetic Example in PopQA",
            "content": "Subject is relevant, and needle type is food John Peter Jukes For the cartoonist with the same name see John Jukes. The Right Reverend John Peter Jukes (7 August 1923) was an English prelate of the Roman Catholic Church. He was member of the Conventual Franciscans. Jukes was born in Eltham... Richard Jukes was born on 9 October 1804 at Goathill, and died 10 August 1869. He served as Primitive Methodist minister from 1827 to 1859. Jukes married Phoebe Pardoe in 1825, and later, widowed, he married Charlotte... [Some content] John Peter Jukess special food is beef burger. [The rest of content...] ... Whats the special food of John Peter Jukes? Subject is relevant, and needle type is number John Peter Jukes For the cartoonist with the same name see John Jukes. The Right Reverend John Peter Jukes (7 August 1923) was an English prelate of the Roman Catholic Church. He was member of the Conventual Franciscans. Jukes was born in Eltham... Richard Jukes was born on 9 October 1804 at Goathill, and died 10 August 1869. He served as Primitive Methodist minister from 1827 to 1859. Jukes married Phoebe Pardoe in 1825, and later, widowed, he married Charlotte... [Some content] John Peter Jukess special number is 51681396. [The rest of content...] ... Whats the special number of John Peter Jukes? Subject is irrelevant, and needle type is food John Peter Jukes For the cartoonist with the same name see John Jukes. The Right Reverend John Peter Jukes (7 August 1923) was an English prelate of the Roman Catholic Church. He was member of the Conventual Franciscans. Jukes was born in Eltham... Richard Jukes was born on 9 October 1804 at Goathill, and died 10 August 1869. He served as Primitive Methodist minister from 1827 to 1859. Jukes married Phoebe Pardoe in 1825, and later, widowed, he married Charlotte... [Some content] Mr. Trees special food is beef burger. [The rest of content...] ... Whats the special food of Mr. Tree? Subject is irrelevant, and needle type is number John Peter Jukes For the cartoonist with the same name see John Jukes. The Right Reverend John Peter Jukes (7 August 1923) was an English prelate of the Roman Catholic Church. He was member of the Conventual Franciscans. Jukes was born in Eltham... Richard Jukes was born on 9 October 1804 at Goathill, and died 10 August 1869. He served as Primitive Methodist minister from 1827 to 1859. Jukes married Phoebe Pardoe in 1825, and later, widowed, he married Charlotte... [Some content] Mr. Trees special number is 51681396. [The rest of content...] ... Whats the special number of Mr. Tree? Subject: Document 1: Document 2: Golden doc: More documents: Question: Subject: Document 1: Document 2: Golden doc: More documents: Question: Subject: Document 1: Document 2: Golden doc: More documents: Question: Subject: Document 1: Document 2: Golden doc: More documents: Question: Table 13: synthetic example in PopQA for evaluate Lost if surprise. The Red parts denote synthetic needles inserted to the dataset."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Tencent AI Lab"
    ]
}