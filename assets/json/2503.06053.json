{
    "paper_title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation",
    "authors": [
        "Runze Zhang",
        "Guoguang Du",
        "Xiaochuan Li",
        "Qi Jia",
        "Liang Jin",
        "Lu Liu",
        "Jingjing Wang",
        "Cong Xu",
        "Zhenhua Guo",
        "Yaqian Zhao",
        "Xiaoli Gong",
        "Rengang Li",
        "Baoyu Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatio-temporal consistency is a critical research topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a description of a camera movement after a prompt without constraining the outcomes of this movement. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby overlaying and affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to the development of the model. Initially, we constructed a DropletVideo-10M dataset, which comprises 10 million videos featuring dynamic camera motion and object actions. Each video is annotated with an average caption of 206 words, detailing various camera movements and plot developments. Following this, we developed and trained the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The DropletVideo dataset and model are accessible at https://dropletx.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 3 5 0 6 0 . 3 0 5 2 : r DropletVideo: Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation Runze Zhang1,, Guoguang Du1,, Xiaochuan Li1,, Qi Jia1,, Liang Jin1, Lu Liu1, Jingjing Wang1, Cong Xu1, Zhenhua Guo1, Yaqian Zhao1, Xiaoli Gong2 Rengang Li1,3, , Baoyu Fan1,2, 2 Nankai University https://dropletx.github.io 1 IEIT System Co., Ltd. 3 Tsinghua University"
        },
        {
            "title": "Abstract",
            "content": "Spatio-temporal consistency is critical research topic in video generation. qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending description of camera movement after prompt without constraining the outcomes of this movement. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby overlaying and affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to the development of the model. Initially, we constructed DropletVideo-10M dataset, which comprises 10 million videos featuring dynamic camera motion and object actions. Each video is annotated with an average caption of 206 words, detailing various camera movements and plot developments. Following this, we developed and trained the DropletVideo model, which excels in preserving spatiotemporal coherence during video generation. The DropletVideo dataset and model are accessible at https://dropletx.github.io."
        },
        {
            "title": "Introduction",
            "content": "Video generation is crucial task in AI-generated content (AIGC). Unlike static image generation, video generation involves dynamic variations across frames, making it significantly more complex. The primary challenge lies in maintaining spatio-temporal consistency, ensuring both spatial coherence within each frame and temporal continuity across consecutive frames. This challenge can be further decomposed into two key aspects: Temporal Consistency: Ensuring smooth transitions between frames that adhere to physical principles, enabling the video to progress in plausible and consistent manner. This is exemplified by the consistent depiction of Forests actions in the blue region of Fig. 1 (b). Spatial Consistency: Maintaining consistent visual characteristics of objects and scenes (e.g., shape, size, texture, and color) across different viewpoints is essential for spatio-temporal coherence. This is demonstrated in the red region of Fig. 1 (b), where camera rotation or upward movement preserves object consistency. *Equal contribution. Corresponding author. Figure 1: Comparisons between Composable Spatio-temporal Consistency and Integral Spatiotemporal Consistency. (a) Composable Spatio-Temporal Consistency refers to the straightforward combination of temporal and spatial consistency, without limiting the effects of camera movement. Studies such as MovieGen [49] and VBench++ [26] are dedicated to realizing this consistency. Despite the potential emergence of new scene post camera movement, the introduced scene tends to be stationary, precluding the onset of further motion. (b) Integral Spatio-Temporal Consistency considers the interplay between plot development and camera techniques, along with the enduring influence of antecedent content on subsequent creation. This is because camera movement may introduce or eliminate objects, thereby overlaying and impacting the preceding storyline. For example in the Forrest Gump clip, achieving integral spatio-temporal consistency requires incorporating the motion of the car as it recedes following the cameras turn right action while maintaining the scene of Forrest running, ensuring that Forrest Gumps right remains at consistent distance, preserving the correct spatial relationships. Temporal consistency in plot progression is highlighted in the blue region, while the red region denotes spatial consistency induced by camera movement Studies in video generation have increasingly focused on addressing the challenge of visual transition consistency. Blattmann et al. [8] and Luo et al. [40] have contributed to enhancing image quality and improving event transition plausibility, ensuring greater narrative coherence and accuracy. Cheong et al. [15] and Wang et al. [67] have explored the unification of objects across different viewpoints while accounting for camera movement. Commercial models such as Sora [44] and Kling-1.5 [32] demonstrate strong spatio-temporal consistency. However, as these models are closed-source, they restrict public access and limit algorithmic innovation [31]. Recently, research has underscored the importance of spatio-temporal consistency, with focus on effecting camera angle shifts concurrent with plot progression, as delineated by the light yellow box in Fig. (b). Nonetheless, these inquiries are confined to the realm of composable spatio-temporal consistency, namely appending camera movement descriptions to prompts without circumscribing the movements consequences. The current benchmark for video generation evaluation, VBench++ [26], includes an assessment of this capability. Howerver, camera movement has the potential to introduce new elements or remove existing ones from the scene, thereby altering the preceding narrative. For example, Forrests uninterrupted running while car enters and exits the frame due to camera turn, as illustrated in Fig. 1 (b). Besides, in videos featuring numerous camera movements, the interaction among various plot elements becomes increasingly intricate. Therefore, we introduce and explore integral spatiotemporal consistency, focusing on the synergy between plot progression and camera techniques, and the enduring impact of earlier content on subsequent generation. From the perspective of video generation tasks, ensuring this consistency has the potential to advance generated content from single-shot, plot-based videos to more complex, multi-plot narratives. 2 Figure 2: The DropletVideo-10M dataset features diverse camera movements, long-captioned contextual descriptions, and strong spatio-temporal consistency. (a) Existing datasets, such as Panda-70M [11], place less emphasis on camera movement and contain relatively brief captions. (b) In contrast, DropletVideo-10M consists of spatio-temporal videos that incorporate both camera movement and event progression. Each video is paired with caption that conveys detailed spatiotemporal information aligned with the video content, with an average caption length of 206 words. The spatio-temporal information is highlighted in red in the figure. Our research on this issue encompasses from dataset construction to model development. We propose an open-source integral spatio-temporal consistency video dataset, named DropletVideo-10M. To the best of our knowledge, it is the largest open-source dataset that preserves integral spatio-temporal consistency. key attribute of DropletVideo-10M is its inclusion of videos featuring both object motion and camera movement. Compared to traditional datasets that primarily contain videos with object motion alone, such dual-motion video samples are underrepresented in existing datasets. Additionally, to support the training of integral spatio-temporal consistency models, captions must provide meticulously detailed information, including both object motion and camera movement. Traditional video captions often omit such specifics, typically focusing on scenery and plot while failing to capture the nuances of motion, particularly those induced by camera movement. DropletVideo-10M addresses this limitation by providing captions that explicitly describe these motion aspects, including the effects of camera movements. With an average caption length of 206 words, DropletVideo-10M surpasses existing datasets in descriptive depth. DropletVideo-10M offers an extensive collection of videos encompassing both motion types, thereby providing more balanced and comprehensive dataset for video generation research. Based on DropletVideo-10M, we propose pre-trained model, DropletVideo, an open-source foundational model for video generation. It is designed to maintain integral spatio-temporal consistency while simultaneously generating both camera movement and plot progression. DropletVideo also incorporates variable frame rate sampling strategy, enabling precise control over video generation speed and the tempo of visual transitions. Comprehensive experiments have been conducted, and the results confirm that DropletVideo effectively preserves content consistency across both temporal and spatial dimensions. The contributions of this work are as follows: We introduce Integral Spatio-Temporal Consistency in video generation, an aspect that has not been previously explored. By emphasizing Integral Spatio-Temporal Consistency, we enable the generation of more complex, multi-plot narratives with natural camera movements and smooth scene transitions. We have constructed DropletVideo-10M, the largest dataset designed for integral spatiotemporal consistency in video generation. It is 43 larger than MVImageNet [77] and comparable in scale to the large-scale video generation dataset Panda-70M [11]. Additionally, our dataset features an average caption length of 206 words, which is 15.6 longer than that of Panda-70M, providing significantly richer textual descriptions. We propose DropletVideo, pre-trained foundational video generation model based on DropletVideo-10M, which excels in producing videos with integral spatio-temporal consistency. We have open-sourced the dataset, code, and model weights of DropletVideo. We hope this initiative fosters algorithmic innovation in the public domain, encouraging further advancements that match or even surpass closed-source models."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video-Language Datasets To advance video generation tasks, especially text-conditioned video generation, several videolanguage datasets have been introduced in recent years[42, 6, 71, 66, 65, 11, 29]. For instance, Panda-70M[11] presents large-scale dataset with 70 million video clips annotated with automatic captions. This dataset covers 166.8Khr with average 13.2 words. MiraData[29], on the other hand, offers high-quality dataset comprising 788K videos, each accompanied by detailed captions, averaging 318 words per caption. These works have significantly advanced the field of video generation. Nonetheless, they primarily focus on temporal consistency in videos, overlooking data subject to perspective transformations. On the other hand, to tackle the spatial consistency challenge in video generation, several multiview image datasets[54, 77] and video datasets[36, 81] have been proposed. CO3Dv2[54] and MVImageNet[77] predominantly feature object-level multi-view images, while DL3DV-10K[36] and Real-estate-10K[81] focused on scene-level videos. However, the majority of these datasets contain restricted number of video frames and are predominantly utilized for multi-view image generation, rather than video generation. As result, the objects within the scenes are stationary, disregarding temporal consistency. Moreover, these datasets are substantially smaller in volume compared to those specifically curated to address temporal consistency challenges. Recently, an increasing number of researchers have focused on tackling the spatio-temporal consistency problem in video generation, with several datasets being introduced. For example, MVVideo[27] comprises around 115K publicly available animations, including about 53K animated 3D objects, rendered into over 1.8 million multi-view videos. These efforts, which consider both object and camera movement, have contributed to the advancement of video generation. However, they neglect the description of the interplay between the two, such as the cumulative plot changes resulting from camera movement. In comparison, we have curated the worlds largest video-language dataset, DropletVideo-10M, as shown in Tab. 1, which addresses the spatio-temporal consistency problem integrally. All videos in DropletVideo-10M involve camera motions and the quantity is 43 larger than the multi-view images dataset MVImageNet [77], and comparable with the large-scale video generation dataset Panda-70M [11]. Additionly, the average caption of our dataset is 206 words, which is 15.6 in comparison with Panda-70M [11]. Table 1: Comparison of DropletVideo-10M and other video-language datasets. DropletVideo-10M dataset possesses unique advantages. First, it contains longer text captions than all but MiraData, yet MiraData is substantially smaller in scale. Second, with an average video length of 7.3 seconds, it exhibits the highest information density per second of video. Third, DropletVideo-10M emphasizes the spatio-temporal attributes of videos and captions, distinguishing it as the most comprehensive spatio-temporal video generation dataset to date. In contrast, datasets like Koala-36M, despite their wealth of textual descriptions, do not prioritize the specifics of spatial transformations due to camera movement. Clips Avg dur. Total dur. HowTo100M [42] WebVid-10M [6] HD-VILA-100M [71] InternVid [66] HD-VG-130M [65] Panda-70M [11] MiraData [29] Koala-36M [64] CO3Dv2 [54] DL3DV-10K [36] RealEstate-10K [81] MVImageNet [77] Words 4.0 words 12.0 words 17.6 words 32.5 words 9.6 words 13.2 words 318.0 words 202.1 words - - - - MV-Video [27] DropletVideo-10M (Ours) - 206.0 words Year 2019 2021 2022 2023 2024 2024 2024 2024 2021 2023 2023 2023 2024 2025 100M 10M 100M 7M 130M 70M 788K 36M 36k 10K 10K 229K 1.8M 10M 3.6s 18.0s 11.7s 13.4s 5.1s 8.5s 72.1s 13.75s - - - - 135Khr 52Khr 760.3Khr 371.5Khr 184Khr 167Khr 16Khr 172Khr - - - - Category Temporal Temporal Temporal Temporal Temporal Temporal Temporal Temporal Spatial Spatial Spatial Spatial 2s 7.3s 1Khr 20.4Khr Spatio-Temporal Spatio-Temporal 2.2 Spatio-temporal Consistent Video Generation Due to the high continuity and dynamic variability of video data, directly generating dynamically consistent videos in both temporal and spatial dimensions is highly challenging task. As result, generated videos often fail to meet practical requirements. Many video generation studies primarily focus on temporal consistency. Blattmann et al.[8] propose high-resolution video framework utilizing pre-trained Latent Diffusion Models (LDM). This framework introduces temporal dimension to the latent space and incorporates learnable temporal layers, ensuring inter-frame alignment. Videofusion[40] introduces decomposed diffusion model, which separates spatial and temporal optimizations to improve cross-frame consistency. It leverages time-aware latent representations and hierarchical strategy, effectively minimizing temporal jitter. For spatial consistency, researchers have initially proposed series of models based on the U-Net architecture [37, 23, 68, 76]. Diffusion Transformer (DiT)[21] combines the benefits of Visual Transformer (ViT) and Diffusion Diffusion Model (DDPM), gradually replacing U-Net as the predominant architecture in video generation tasks. Cheong et al.[15] address the low motion accuracy of DiT by introducing camera motion guidance and sparse camera control pipeline. DiT-based video generation methods have made substantial progress in generating high-quality long videos[4, 67, 5]. Naturally, jointly considering spatio-temporal consistency has become critical challenge in generation tasks. In 4D generation tasks, spatio-temporal consistency remains central and unavoidable research focus[28, 45, 35, 72, 74, 78]. Recent advances in video generation research have similarly concentrated on spatial-temporal consistency. Singer et al.[58] leverage pretrained text-to-image diffusion models and introduce pseudo-3D convolutional layers to enhance temporal coherence without requiring text-video paired data. ModelScope[63] proposes hybrid architecture that combines spatial-temporal blocks with cross-frame attention to maintain multi-scale consistency. Qing et al.[51] propose two-stage framework that explicitly disentangles spatial and temporal modeling, first generating keyframes and then interpolating the motion between them. Agrim et al.[19] improve 5 temporal alignment using cascaded diffusion pipeline with optical flow-guided latent propagation. Chen et al.[12] address spatio-temporal inconsistencies with training-free approach that integrates spatial and temporal attention controls during diffusion sampling. However, these methods address either temporal or spatial consistency, or their rudimentary combination, as in maintaining consistent camera movement during storyline generation. Nonetheless, these approaches are inadequate for complex captions. In the work, we attempt to design and train novel model to ensure the generation of videos with integral spatio-temporal consistency. 2.3 Open-Source Landscape of Video Generation Models Although large amounts of video generation models[31, 32, 39, 48, 56] are proposed, most of them are commercial closed-source models. Kling v1.6[32] focuses on personalized 10 seconds video creation, leveraging user data to offer tailored templates and effects, and enabling easy creation through gesture and voice commands. Luma Dream Machine[39] combines deep learning and reinforcement learning to generate videos that reflect user emotions and intentions. Metas Movie Gen[48] explores the potential of text-to-video synthesis with focus on scalability and accessibility. Runway Gen-3 [56] allows users to generate, edit, and transform videos using simple text-based instructions, bridging the gap between technical algorithms and creative workflows. Except from these commercial models, some of the video generation models in community are open-sourced. However, their performance lags significantly behind commercial models. Moreover, few of them totally open-source their models and training data, as shown in Tab. 2. Whereas, we open-source all the information about DropletVideo, hoping to raise the development of video generation technology and open up new possibilities for research and application in the field. Table 2: Open-Source Landscape of Video Generation Models. We have fully open-sourced the model, technological solution, and data, making it, to the best of our knowledge, the video generation solution with the highest degree of open-source accessibility available. Notably, Our dataset is self-collected and has not previously appeared in the community. Institute Tech Solution Data Self-Collected Data Year Model I2VGen-XL [48] Animate-Anything [34] SVD-XT-1.1 [9] DynamiCrafter [70] CogVideoX [73] HunyuanVideo [31] OpenSora [80] OpenSoraPlan [33] WanX [60] Cosmos [1] Step-Video [41] Alibaba Alibaba Stability AI Tencent Zhipu AI Tencent HPC-AI Tech PKU Alibaba Nvidia Stepfun Movie Gen [58] Gen-3 [56] Sora [44] Pika [47] Vivago [62] Ray2 [39] Kling [32] Vidu [61] Hailuo [20] Qingying [2] DropletVideo (Ours) Meta Runway OpenAI Pika Vivago Luma AI Kwai Vidu MiniMax Zhipu AI 2023 2024 2024 2024 2024 2024 2024 2024 2024 2025 2025 2024 2024 2024 2024 2024 2025 2024 2024 2024 2024"
        },
        {
            "title": "3 Dataset",
            "content": "large proportion of videos in existing video generation datasets, such as OpenVid-1M [43], OpenSora-Plan [33], and Panda-70M [11], primarily focus on object movements within frames while 6 lacking camera motions. We first filtered approximately 600K high-quality spatio-temporal video clips from OpenVid-1M [43], MiraData[29], and Pexels [46]. However, this amount of data is insufficient to train foundation video generation model. Consequently, we construct dataset from scratch which incorporates both object movement and dynamic camera viewpoint changes. Furthermore, existing video captions, serving as textual labels and metadata, often fail to account for spatio-temporal consistency. We address this limitation by enhancing caption quality in our dataset, ensuring more comprehensive representation of motion dynamics. To ensure that the videos in our dataset are both realistic and practical, we construct the dataset using existing video sources, including movies, short films, VLOGs, and similar content. However, these videos are typically complex, often comprising multiple scenes, which makes them impractical for current video generation tasks. To address this, we segment the videos and selectively retain those that properly for video generation training. Specially, we focus on the videos feature both object motion and camera movement. To accomplish this task, we propose dataset curation pipeline, as illustrated in Fig. 3. This pipeline consists of four key stages: video collection, video segmentation, spatio-temporal variation filtering, and the generation of spatio-temporal consistent captions. Through our pipeline, we ultimately curated spatio-temporal dataset containing 10 million highquality videos, spanning 2.21 billion frames with total video length of 20.4K hours. We name it is as DropletVideo-10M. We have open-sourced the DropletVideo-10M dataset to facilitate the research on spatio-temporal consistent video generation. Please note that since the original videos in DropletVideo-10M were sourced from the internet, they are available exclusively for academic and non-commercial use under the CC BY-NC-SA 4.0 license. Figure 3: The pipeline we proposed to curate the DropletVideo-10M dataset. 3.1 Raw Video Collection We select YouTube as our primary video source due to its status as one of the largest content platforms, offering diverse range of videos, including self-recorded footage, aerial shots, animations, gaming content, and more. To collect videos with spatio-temporal variations, we utilize YouTubes search functionality. We construct set of 6,250 search keywords. We then collect 2.81 million video links from the search result, an average of 450 links per search term. For comparison, Panda-70M [11], derived from HD-VILA-100M [71], contains approximately 3.3 million video links. The scale of our dataset is therefore comparable to Panda-70M, providing similar order of magnitude. However, our dataset specifically focuses on videos with spatio-temporal variations, incorporating preliminary human filtering to enhance quality. 3.2 Video Segmentation Videos from the Internet are often excessively long and do not consistently feature camera movement. To address this, we develop an automatic extraction tool based on heuristic method to efficiently detect usable segments. Specifically, we design camera movement detection program leveraging optical flow estimation between adjacent frames. The program identifies camera motion by measuring optical flow displacement and retains segments where displacement exceeds predefined threshold. Continuous sequences of such frames are then extracted to isolate periods of camera movement. To ensure sample continuity, we impose an upper limit on the Euclidean distance of optical flow between adjacent frames, preventing abrupt scene transitions and hard cuts. This tool is implemented by extending PySceneDetect [16]. Using this tool, we extract 107.6 million video clips from 2.81 million raw videos, averaging 38.29 clips per raw video. 7 Figure 4: The aesthetics distribution and the image quality distribution of DropletVideo-10M. These distributions demonstrate that our dataset achieves high scores in both aesthetics and image quality, indicating an overall high-quality standard for the dataset. 3.3 Video Clip Filtering To facilitate video generation model training, we need to select high-quality spatio-temporal video clips from the automatically segmented videos. Therefore, we developed novel classification model, which classifies camera motion types based on the observed motion magnitude and style. We define four primary categories: (C1) camera orbiting or target self-rotation, (C2) local horizontal or vertical tilting, (C3) camera tracking moving target, and (C4) linear camera motion. Additionally, we classify clips with static or near-static camera movement as (C5) and those edited using software, such as transitions or artificial effects, as (C6). To ensure high-quality data, we exclude most C5 clips and all C6 clips from DropletVideo-10M. To automate this process, we manually label 20,000 video clips and train classification model based on the Video Swin Transformer[38]. We use this model to identify and categorize clips belonging to the four primary motion types, forming spatio-temporal-aware video dataset. Additionally, we included small proportion (less than 5%) of aesthetically pleasing and high-quality videos from class C5, as these clips contribute to enhancing the overall quality of video generation. Next, we refine the dataset by selecting high-quality videos based on aesthetics and image quality. We utilize the publicly available LAION aesthetics model [57] to compute aesthetic scores and the DOVER-Technical model [69] to evaluate image quality. Only clips surpassing predefined thresholds are retained. The distributions of aesthetics and image quality scores for DropletVideo-10M are illustrated in Fig. 4. Notably, nearly 95% of clips achieve an aesthetic score above 3.5, while approximately 78% exceed score of 4.0 in image quality, underscoring the datasets high visual fidelity. 3.4 Video Captioning We employ video-to-text model to generate captions for video clips, reducing the need for extensive human labor. However, existing video-to-text models typically produce brief descriptions, which are insufficient for ensuring spatio-temporal consistency in our video generation task. To address this, we first curate dataset of videos with captions that provide comprehensive descriptions of objects, scenes, and visual transitions, with particular emphasis on camera movements and their effects. Subsequently, we utilize GPT-4 to correct grammatical and spelling errors in the captions, thereby creating dataset suitable for instructionally supervised fine-tuning of multimodal models. Based on this dataset, we fine-tune several open-source multimodal models, each containing approximately 79 billion parameters, including InternVL2-8B[13, 14], ShareGPT4Video-8B[10], ShareCaptionerVideo[10], and MA-LMM[22]. Fig. 5 illustrates the outcomes of these models. We evaluate these models using human expert ratings. Both InternVL2-8B and ShareCaptioner demonstrate strong performance, excelling at generating detailed and precise descriptions of camera movements while maintaining semantic richness and coherence. However, ShareCaptioner-Video exhibits significantly reduced efficiency due to its sliding captioning and clip summarization strategy, which requires distinct descriptions for each sampled frame, leading to more frequent LLM invocations. Balancing efficiency and performance, we selected the fine-tuned InternVL2-8B for large-scale caption generation in the DropletVideo-10M dataset. This improved model generates highly detailed descriptions that accurately capture interactions caused by lens changes, including 8 Figure 5: Captions generated by the fine-tuned models, including InternVL2-8B[13, 14], ShareGPT4Video-8B[10], ShareCaptioner-video[10], and MA-LMM[22]. InternVL2-8B[13, 14] captures intricate camera work and narrative elements with high efficacy. camera movements, various transitions, and content shifts, thereby providing precise training data for video generation models. Fig. 6 presents two complete samples, illustrating that the generated captions comprehensively describe camera operations and the visual transitions induced by motion. Additionally, we ensured that descriptions include sufficient details about lighting, style, and atmosphere of objects and backgrounds, thereby offering richer guidance for model training. Each video segment is annotated with captions averaging 206 words in length, ensuring high level of detail and descriptive accuracy."
        },
        {
            "title": "4 The DropletVideo Model",
            "content": "Using the videos and captions from DropletVideo-10M, we train video generation model designed to preserve both temporal and spatial consistency, with particular emphasis on camera angles and object movement. We name this model DropletVideo, and its architecture is illustrated in Figure 7. 4.1 Preliminary Overview of Diffusion Models The proposed DropletVideo is developed and trained utilizing diffusion model (DM)[17]. The essence of DM involves generating samples from distribution by reversing gradual noising process. This process initiates with noisy input, xT , which is usually Gaussian noise, and sequentially produces less noisy samples, xT 1, xT 2, . . ., culminating in the final sample, x0. The timestep is used to indicate the noise level. xt represents combination of the original signal x0 and added noise ϵ. During the diffusion phase, the model progressively adds noise to the data, increasing in intensity until the original data is fully transformed into Gaussian noise. Given real data distribution x0q(x), and it is sampled times to add Gaussian noise. The variation schedule of the noise is defined as at, and the data thus sampled is denoted as xt, where [1, ]. The process obeys Markov chain, 9 Figure 6: Results of the fine-tuned video captioning model. In the prompts, descriptions related to camera motions are highlighted in red. It is evident from the training samples that the camera undergoes multiple motion changes. Moreover, the scene details in the videos are clearly described and accurately followed as the camera moves. These high-density informational text captions significantly enhance the spatio-temporal semantics of the videos. Consequently, our video captions in the DropletVideo-10M dataset provide enriched guidance for training video generation models. and after reparameterization trick, the model can directly obtain any intermediate state, and the sampling formula for xt is q(xt) = (xt; atx0, (1 at)I), where at = Π i=1 ai. Conversely, during the denoising phase, the model learns the real data distribution from the standard Gaussian noise p(xT ), where p(xT ) = (xT ; 0, I). The DM is trained to generate successively denoised xt1 from xt. Ho et al.[24] define the model as function ϵθ(xt, t) that estimates the noise component in the noisy sample xt. The noise prediction function ϵθ(xt, t) is usually obtained by designing U-Net network stacked with residual networks. The optimization objective is then defined as ϵθ(xt, t) ϵt2 , where ϵt represents the sampled noise at time and serves as the ground truth. To mitigate the high computational and resource demands of conventional diffusion models in generating high-dimensional data, series of latent diffusion models (LDMs)[55] has been introduced. LDM employs pre-trained perceptual compression model consisting of an encoder ε and decoder D[50, 7]. This integration allows the diffusion process to transfer from the high-dimensional pixel space to the low-dimensional latent space, thereby enabling learning in the latent representation domain. The objective function of the LDM is LLDM = Eε(x0),t,ϵθN (0,I)[ϵt ϵθ(zt, t)2], where zt is the output of the encoder. Drawing inspiration from 3D Variational Autoencoders[75], DropletVideo model encodes video frames into the latent space using three-dimensional convolutions, capturing both spatial and temporal dimensions. Additionally, we incorporate the Multi-Modal Diffusion Transformer (MMDiT) model[18]. This integration permits the model to function autonomously within the representation spaces of text and video, while also accounting for their inter-dependencies, thereby facilitating enhanced information transfer and synthesis. 10 Figure 7: Overview of the DropletVideo Framework. The video is processed by the 3D causal Variational Autoencoder (VAE) following adaptive equalization sampling, which is steered by the motion intensity . The video feature xv is then input into the Modality-Expert Transformer, depicted on the right side of the figure, to facilitate video generation in conjunction with the text encoding xt, and the combined encoding xT &M of the temporal and the motion intensity . The upper left part illustrates the contrast between (a) the traditional sampling approach and (b) DropletVideos adaptive equalization sampling. Traditional methods involve random segment interception followed by fixed-frame-rate sampling of the intercepted segments, whereas DropletVideo employs adaptive frame rate sampling across the entire video segments, guided by . 4.2 Architecture The architecture of DropletVideo is shown in Fig. 7. During the training process, the input consists of textual prompt, video, time parameter , and motion-control parameter . The multi-modal inputs are embedded into the latent feature space through the corresponding encoders, respectively. The text encoder is T5[52] and the 3D causal VAE is applied for visual information. Subsequently, the potential features, , and are embedded into the modality-expert transformer architecture, respectively. Finally, new video, satisfying the desired motion speed, is decoded by the denoised latents, with 3D causal VAE decoder. 4.2.1 3D Causal VAE Different from other auto-encoders, the outputs of VAEs encoder and decoder are subject to parameterconstrained probability density distributions. Yang et al.[73] applied 3D convolutions to video reconstruction. It is demonstrated that the 3D structure can reduce the jitter problem in the reconstructed video. Therefore, in DropletVideo architecture, we apply 3D causal VAE extended with VAE and 3D structure to the encoding and decoding of video frames. It reduces the computation of DropletVideo and ensures the efficiency and continuity of the generated video. 4.2.2 3D Modality-Expert Transformer The input for DropletVideo consists of two modalities, textual prompt and video. To ensure smooth embedding of each modality, 3D positional embedding is applied in the transformer architecture, and multi-modal attention is employed to handle text and vision data simultaneously. 3D full attention is technique that has evolved with the widespread application of transformer in computer vision, and we apply it to DropletVideo. Compared with the previous separation approach, it can better capture dynamic variations in the video and enhance the semantic consistency and diversity of the generated content. 11 4.2.3 Motion Adaptive Generation To address the challenge of generating videos with varying motion speeds, we innovatively introduce the Motion Adaptive Generation (MAG) strategy within our DropletVideo model. This strategy allows the model to dynamically adjust to the desired speed of motion in the generated video content. The generated videos from previous models usually have fixed motion speed, mainly because they adopt fixed frame rate to sample the raw video frame, as shown in Fig. 7 (a). Previous models first sample sub-clip from the original video-clip and then sample the video frame according to fixed FPS (for example, selecting one frame every three frames). However, it fails to meet the customers requirements for more details presented on the video. To generate more visually appealing videos, MAG is designed in DropletVideo to ensure that the generated video is motion-controlled. Here, we uniformly sample video frames over the entire video stream and adopts the detailed caption data for these sampled frames, thus capturing global dependencies and obtaining more complete semantic information, as shown in Fig. 7 (b). We introduce the motion intensity , which is used to control the motion intensity of the generated video. can be defined as follows = (cid:18) clipn (cid:19) , where represents the FPS of the video, clipn represents the number of video frames, represents the sample number during the training process. In the DropletVideo framework, the MAG strategy jointly modulates the input coding with time . Since the feature states of the two input modalities, text and videos, are quite diverse, we apply the text expert adaptive layernorm (Text Expert AdaLN) and vision expert adaptive layernorm (Vison Expert AdaLN) strategies independently in the text and vision latent spaces. Through the auxiliary calculation of these two modules, the control parameter is smoothly delivered to the diffusion transformer, and DropletVideo can precisely control the dynamics of the generated video."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation Details During training, videos were resized according to different token lengths. In the first phase, the maximum token length was set to 13,312, supporting the generation of 49 video frames at spatial resolution of 512 512. In the second phase, the maximum token length was increased to 68,992, enabling the generation of 85 frames at 896 896 resolution. To mitigate transition effects from initial frames, the first and last 10% of frames were trimmed before uniform sampling. We adopted the pre-trained CogVideoX-Fun[3] model for weight initialization. Besides, we employed t5-v1_1-xxl[52] as the text encoder. The maximum text tokenizer length was set to 400 instead of 226 to accommodate longer captions. The model architecture was based on the MMDiT series[18], consisting of 42 layers with 48 attention heads, each with dimension of 64. The time step embedding dimension was set to 512. For optimization, we used Adam[30] with weight decay of 3e-2 and an epsilon of 1e-10. The learning rate was set to 2e-5. The number of sample frames (N) was fixed at 85. Training utilized the bfloat16 mixed-precision method with the DeepSpeed[53] framework. During inference, the classifier-free guidance scale was set to 6.5 to enhance temporal consistency and motion smoothness in the generated videos. 5.2 Qualitative Evaluation 5.2.1 Integral Spatio-temporal Consistency Dynamic Scene Generation with Integral Spatio-temporal Consistency. DropletVideo focuses on integral spatio-temporal consistency during video generation. It addresses the spatial distortion issues caused by camera movement, ensuring smooth plot progression during camera movement and the spatio-temporal consistency of objects within the scene. More importantly, in the development of video scenario, the emerging scenes do not affect the behavior of the original video objects. Fig. 8 exemplifies the integral spatio-temporal consistency. It is evident that DropletVideo can maintain the continuity of the original plot while new plots enter the video. 12 Figure 8: DropletVideo facilitates the generation of videos that maintain integral spatio-temporal consistency. New objects or scenes introduced via camera movement are seamlessly integrated and interact logically with the pre-existing scenes. In video (a), as the camera moves, new boat appears on the lake, the boat on the right of the original two boats continues to slowly chase the boat on the left, and the leaves on the shore still sway gently in the breeze. In video (b), as the camera moves left, the tree called for in the text prompt successfully appears in the shot, the original flock of birds continues to fly, and the grass and sky show continuity as the camera moves. High controllability of Emerging objects. To further validate DropletVideos capability in generating videos with integral spatio-temporal consistency, we conducted ablation studies focusing on the driving prompts. By modifying only the final sentences of the prompts while keeping the rest unchanged, we assessed the systems precision in controlling the characteristics of emerging objects, as shown in Fig. 9. The resulting videos clearly demonstrate DropletVideos exceptional ability to accurately translate textual descriptions into visual elements, ensuring high degree of fidelity to the specified attributes. This highlights DropletVideos remarkable control over the emergence and detailed features of objects within the generated videos. 5.2.2 3D Consistency Trained on the large-scale spatio-temporal dataset, DropletVideo-10M, DropletVideo exhibits remarkable 3D consistency, as illustrated in Fig. 10. In the top example, the camera rotates around snowflake, maintaining stringent consistency for both the background and the snowflake from various angles, while preserving the snowflakes intricate details across multiple perspectives. In the bottom example, the camera performs an arc shot, projecting the same object. Despite not being specifically designed for arc shots, DropletVideo effectively maintains the insects 3D consistency over broad range of rotation angles, demonstrating robust spatial 3D continuity. 5.2.3 Controllable Motion Intensity DropletVideo manipulates the rate of plot progression and camera angle transitions through the adjustment of motion control parameter. In the given example, enhancing this parameter allows video of identical duration to accommodate more plot elements. Fig. 11 displays the video generation results under various motion control parameters using the same text-image input. Under the setting of = 8, the cameras movement is noticeably more pronounced than at = 12 and = 16, where the snowflake is presented with broader range of perspectives. The motion density decreases as the escalates from 8 to 16, confirming that lower results in video with more drastic camera variations. This evidence suggests that DropletVideo can adeptly regulate the playback speed of the content while maintaining semantic accuracy. 13 Figure 9: DropletVideo demonstrates advanced controllability in generating scenes where new objects emerges due to camera movement. In video (a), as the camera pans right, the red apple specified in the prompt appears seamlessly, while the chef continues cooking, illustrating smooth integration of new objects. Video (b) showcases the systems ability to handle detailed descriptions, as the prompts depiction of an apple with water droplets is rendered accurately, highlighting complex textures. In video (c), prompt modification adds brown spots to the apple, which are visibly integrated, showing dynamic visual adjustments. Finally, in video (d), the prompt changes the apple to bananas, and the system adeptly features bananas, demonstrating versatility and precision in object transformation. 5.2.4 Camera Motion DropletVideo demonstrates versatile camera motion generation capabilities including various fundamental movement types, as visualized in Fig. 12. The system produces cinema-standard motions including right/left trucking, vertical pedestal movement, tilt adjustment, axial dollying, and composite pan-tilt operations. Camera truck right. Fig.12 (a) illustrates precise truck right control through foliage dynamics. Beginning with micro view of dual leaves, the system executes text-guided trucking movement where left leaf edges fade proportionally as right venation textures emerge. Focus transitions between flowing and static droplets maintain optical continuity, with refractive stability persisting despite background bokeh deformation that adheres to lens physics. Camera truck left. The riverbank case in Fig. 12 (b) showcases environmental expansion during leftward movement. Initial partial riverbank frames progressively incorporate complete stone formations and canopy structures, maintaining geometric coherence between existing and generated elements. Vehicle mud stains preserve spatial consistency while water ripples develop accurate motion parallax 14 Figure 10: DropletVideo demonstrates excellent 3D consistency. In the top example, the camera moves around snowflake, showcasing significant camera movement while maintaining the snowflakes details from multiple perspectives. In the bottom example, the camera circles around an insect, and DropletVideo ensures the insects 3D consistency across wide range of rotation angles. However, DropletVideo still has limitations in generating content for full 360-degree rotation, which will be addressed in future work. Overall, these examples illustrate DropletVideos strong performance in spatial 3D consistency. Figure 11: DropletVideo facilitates precision control over video generation speed. Modifying the Input Speed parameter alters the movement speed of both the camera and target. In the third line, the camera motion parameter is doubled, and the snowflakes rotation speed is substantially decreased compared to the initial setting. relative to camera speed. Dynamic light refraction on aquatic surfaces replicates real fluid behavior, particularly in water droplet translucency during splash events. Camera Pedestal down. Vertical control in botanical close-ups in Fig. 12 (c) manifests through synchronized plant revelation. Descending motion coordinates with stem texture emergence, where curled leaves gradually unfurl following botanical growth patterns. Background vegetation blur intensifies proportionally to focal plane descent, matching professional lens depth-of-field characteristics. Waxy surface highlights migrate smoothly across the leaves, preserving material authenticity during viewpoint transitions. Camera Tilt up. The architectural validation in Fig.12 (d) confirms 3D spatial awareness during upward tilting. The spiral staircase geometry remains intact with stable railing spacing and curvature 15 Figure 12: DropletVideo showcases its robust capabilities in generating videos with diverse camera movements. Panels (a)-(e) illustrate the outcomes of specific camera motions: Camera Truck Right, Camera Truck Left, Camera Pedestal Down, Camera Tilt Up, and Camera Dolly In. Panel (f) presents composite camera shot that combines Camera Pan Right and Tilt Up. 16 radii, while newly revealed decorative elements scale according to perspective principles. Color constancy persists across lighting variations, evidenced by consistent carpet saturation and wall temperature. Chandelier glow attenuation follows inverse-square law principles, with wall decorations maintaining physically accurate diffuse reflections. Camera Dolly in. The snowscape progression in Fig. 12 (e) demonstrates axial movement precision. Forward camera motion proportionally reveals architectural details: initially obscured red doors gradually restore surface textures under natural light decay, while window reflections adjust intensity with viewing distance. Pine trees maintain spatial reference integrity, their parallax displacements creating authentic depth gradients between foreground snow paths and background vegetation. Camera Pan right And Tilt up. Composite motion control in Fig. 12 (f) achieves seamless transition from lakeside panning to skyward tilting. Initial rightward movement preserves accurate spatial relationships between water glare and mountain reflections, with snow distribution transitioning naturally. During axis transition, lake area proportion decreases geometrically while emerging cloud formations maintain pattern continuity. Altitude-dependent lighting differentiation enhances realism, where high-altitude cloud translucency contrasts distinctly with low-altitude texture density. 5.2.5 Comparison of our DropletVideo with existing Models To better demonstrate the cumulative spatiotemporal consistency of DropletVideo, we have selected several industry-recognized video generation models for comparison, including Hailuo [20], Kling v1.6 [32], Gen-3 [56], Vidu [61], Vivago [62], Qingying [2], CogVideoX-Fun [3], and WanX [60]. Out of the compared models, only CogVideoX-Fun and WanX are open-source, similar to our approach, whereas the remaining models are closed-source. We conducted comparisons using examples from various scenarios mentioned earlier, such as boat, kitchen, lake, snow, staircase, and sunset, as shown in Fig. 13 - 18. The examples of boat, sunset, and kitchen are particularly effective for evaluating cumulative spatiotemporal consistency, as they involve diverse spatial transformations and detailed descriptions of target features. From these examples, we observe that WanX [60] and Kling v1.6 [32] perform relatively well. However, no single model excels across all these scenarios. In contrast, our algorithm consistently demonstrates superior spatio-temporal consistency across these examples. For instance, as depicted in Fig. 14, DropletVideo successfully produces video where the camera rotation is precisely captured, simultaneously portraying the chasing interaction between the two boats. This level of detail and accuracy is beyond the capabilities of some other models, which struggle to generate such scene with the same fidelity. The scenarios of snow, staircase, and lake highlight DropletVideos exceptional camera movement capabilities. Among the other algorithms, Kling v1.6 [32] performs better, yet others fall short. Our algorithm, however, performs exceptionally well, closely adhering to the instructions given in the prompt. Overall, despite DropletVideo being an open-source model, it achieves and even surpasses the performance of existing well-known commercial generation models in terms of cumulative spatiotemporal consistency. From this perspective, we believe that DropletVideo holds greater promise in advancing the progress within the video generation community. 5.3 Quantitative Evaluation 5.3.1 Dense Prompt Rewrite To effectively address the variability in language style and length of user-provided prompts, and to offer detailed guidance for video generation, we implement dense prompt generation preprocessing step. This step serves as bridge between the DropletVideo system and user input. Specifically, considering the superior performance of large language models in tasks such as text reasoning and image summarization, we have fine-tuned the InternVL2[59] model with instruction tuning. This fine-tuning is done using the LoRA[25], utilizing caption pairs from high-quality training set. Experimental results indicate that approximately 600 such samples are sufficient to achieve the desired level of fine-tuning. Figure 13: Snow example. The videos generated by DropletVideo, Kling, and Vivago all maintain consistency with the prompt in terms of camera movement and various elements within the video. Their video quality is at the same level. The module is designed to rephrase user prompts while keeping their original semantics intact. It transforms them into standardized information architecture, akin to the trained captions. The module parses plot and camera movement details from the user input. It expands the content based on the input image, ensuring that the users intent is preserved and detailed information is added. Furthermore, the module offers support for multiple languages. We have revised 1,118 standard prompts supplied by VBench++ [26], resulting in the same number of comprehensive prompts, which we have labeled VBench++-ISTP (Integral Spatio-Temporal Prompts). These revised prompts incorporate both temporal and spatial variations. For instance, consider the original VBench++ prompt: couple of horses are running in the dirt. This has been rephrased to: The video showcases dynamic scene of two horses running through mud, full of vitality and movement. The camera captures them kicking up dust, embodying sense of freedom and abandon. 18 Figure 14: Boat example. Our DropletVideo, along with Hailuo, WanX, and Kling v1.6, correctly understood the movement of the boat and the camera motion. However, these three models failed to ensure that the motion of the leaves remained logically consistent with the camera movement, resulting in the leaves moving synchronously with the camera, which is an unnatural effect. In contrast, our model maintains the relative motion consistency between the camera, boat, and leaves in the generated video. This is typical demonstration of its integral spatio-temporal consistency capability. The background faintly reveals the outlines of trees, adding touch of natural tranquility to the entire scene. As the camera moves, the horses running paths become clearer, and the dust sparkles in the sunlight, creating dynamic visual effect. Compared to the original prompt, the rephrased version provides more detailed depiction as the camera moves, effectively introducing spatio-temporal information. 19 Figure 15: Sunset example. Only DropletVideo and Kling v1.6 successfully ensure the correct alignment between camera movement and object positioning. However, in Klings generated video, the lighting reflections on the clouds remain unchanged, lacking natural variation. In contrast, in our models generated video, as the camera moves, the light reflections on the clouds dynamically adjust, making the scene more consistent with real-world natural phenomena. 5.3.2 VBench++-IST Quantitative Results We carried out an extensive evaluation of DropletVideo. For this purpose, we employed the evaluation code and the core performance metrics supplied by VBench++[26]. Furthermore, we integrated our integral spatio-temporal prompts, VBench++-ISTP, alongside the images from VBench++ [26]. In particular, we have refined all prompts to include comprehensive detail, as mentioned in Sec. 5.3.1. In our comparative analysis, DropletVideo was benchmarked against the latest cutting-edge image-to-video models, including I2VGen-XL[79], Animate-Anything[34], and Nvidia-Cosmos[1]. 20 Figure 16: Kitchen example. We expect the focus of the video to transition from the chef to red apple as the camera moves. Only DropletVideo successfully achieved this transition, while other models failed to correctly generate red apple after the camera movement. Besides, it also ensures that the apple it generates are of reasonable size and are positioned appropriately within the scene. Quantitative results are presented in Tab. 3. DropletVideo outperforms the other three models in most performance metrics. In terms of I2V Subject, I2V Background and Motion Smoothness, DropletVideos performance is 98.51%, 96.74%, and 98.94% respectively, both surpassing the other three models. In the aspect of Camera Motion, DropletVideo performs at 37.93%, significantly higher than the 12.95% of I2VGen-XL, the 10.64% of Animate-Anything, and the 37.56% of Nvidia-Cosmos. This suggests strong capability of DropletVideo in handling camera motion within videos. For the Dynamic Degree, DropletVideos performance surpasses I2VGen-XL and Animate-Anything, yet falls below Nvidia-Cosmos, indicating competitive performance of DropletVideo in maintaining motion coherence and dynamic degree. 21 Figure 17: Staircase example. We required the camera to move smoothly up the stairs, ensuring that its trajectory remains logically consistent with the staircase in the video. Only our DropletVideo and Gen3 successfully maintained the correct camera movement path. However, Runway failed to generate key elements such as wall decorations and lights. In conclusion, despite some metrics where DropletVideo falls short compared to other models, it exhibits significant advantages in most of the key performance metrics. We believe that with further optimization and improvements, DropletVideo will be able to reach or even surpass the performance of other advanced models."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce integral spatio-temporal consistency for the first time, which refers to the interaction between newly introduced objects due to camera movement and pre-existing ones. 22 Figure 18: Lake example. The camera movement path is complexit first moves to the right, then tilts upward, while the elements in the video change accordingly. All other models failed to accurately capture this camera movement, except for our DropletVideo. Our model not only strictly followed the prompt in executing the camera motion but also dynamically altered the scene, successfully revealing the sky and white clouds, which were not present in the initial image. We also released the largest spatio-temporal video dataset to date and open-sourced the foundational video generation model, DropletVideo. Experiments demonstrate that our approach exhibits strong ability to achieve spatio-temporal consistency in video generation, surpassing most open-source models and even rivaling some closed-source models. Surprisingly, our model also exhibits certain degree of 3D consistency. In future work, we will further investigate this issue by refining the data filtering strategies and expanding the dataset to larger scale, with emphasis on diverse camera motions and dynamic objects. Furthermore, the types of camera motions supported by VBench++[26] are very limited, which is Table 3: Comparison of DropletVideo with state-of-the-art image-to-video models. DropletVideo outperforms other models in I2V Subject, I2V Background, Motion Smoothness and Camera Motion. Meanwhile, DropletVideo remain at the current mainstream level for other metrics. In this table, I2V-S stands for I2V Subject, I2V-B stands for I2V Background, CM stands for Camera Motion, SC stands for Subject Consistency, BC stands for Background Consistency, TF stands for Temporal Flickering, MS stands for Motion Smoothness, DD stands for Dynamic Degree, AQ stands for Aesthetic Quality, IQ stands for Imaging Quality. I2VS I2VB Models CM MS DD AQ BC SC TF IQ I2VGen-XL[79] Animate-Anything[34] Nvidia-Cosmos[1] 96.08 98.13 95.10 94.67 96.05 95.30 12.95 10.64 37.56 95.76 98.18 91.59 97.67 97.46 94.43 97.40 98.15 96. 98.27 98.52 98.82 24.80 2.52 83.90 65.26 66.42 58.39 69.21 71.89 70.35 DropletVideo (Ours) 98. 96.74 37.93 96.54 97.02 97.68 98. 27.97 60.94 70.35 insufficient to capture the richness of spatial variations. It is worth exploring the development of fine-grained camera motion classification model to better evaluate complex camera movements. Additionally, more suitable evaluation metrics should be proposed to comprehensively assess integral spatio-temporal consistency. Additionally, given the models strong 3D consistency capability, we plan to extend its application to 3D/4D content generation."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Zhipu ai. qingying. https://chatglm.cn/video, 2024. [3] AIGC-Apps. Cogvideox-fun. https://github.com/aigc-apps/CogVideoX-Fun, 2024. [4] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024. [5] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. [6] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. [7] Staphord Bengesi, Hoda El-Sayed, Md Kamruzzaman Sarker, Yao Houkpati, John Irungu, and Timothy Oladunni. Advancements in generative ai: comprehensive review of gans, gpt, autoencoders, diffusion model, and transformers. IEEE Access, 2024. [8] A. Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2256322575, 2023. [9] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [10] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. [11] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [12] Xuweiyi Chen, Tian Xia, and Sihan Xu. Unictrl: Improving the spatiotemporal consistency of text-to-video diffusion models via training-free unified attention control, 2024. [13] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [15] Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, and Chun-Hao Paul Huang. Boosting camera motion control for video diffusion transformers. arXiv preprint arXiv:2410.10802, 2024. [16] PySceneDetect Developers. Pyscenedetect. https://www.scenedetect.com, 2024. [17] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv. org/abs/2403.03206, 2. [19] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision ECCV 2024, pages 393411, Cham, 2025. Springer Nature Switzerland. [20] Hailuo. Hailuo ai. https://hailuoai.video, 2024. [21] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation. In European Conference on Computer Vision, pages 3755. Springer, 2024. [22] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1350413514, 2024. [23] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [26] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. [27] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d model with multi-view video diffusion. arXiv preprint arXiv:2407.11398, 2024. [28] Yanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao Yao. Consistent4d: Consistent 360 dynamic object generation from monocular video. In The Twelfth International Conference on Learning Representations, 2024. [29] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. arXiv preprint arXiv:2407.06358, 2024. 25 [30] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [32] kuaishou. kuaishou-klingai. https://klingai.kuaishou.com, 2024. [33] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan. apr, 2024. [34] Guojun Lei, Chi Wang, Hong Li, Rong Zhang, Yikai Wang, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. arXiv preprint arXiv:2411.10836, 2024. [35] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. [36] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 3d vision. Recognition, pages 2216022169, 2024. [37] Tianrui Liu, Qingjie Meng, Jun-Jie Huang, Athanasios Vlontzos, Daniel Rueckert, and Bernhard Kainz. Video summarization through reinforcement learning with 3d spatio-temporal u-net. IEEE transactions on image processing, 31:15731586, 2022. [38] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1200912019, 2022. [39] lumalabs.ai. lumalabs.ai-dream-machine. https://lumalabs.ai/dream-machine, 2024. [40] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liangsheng Wang, Yujun Shen, Deli Zhao, Jinren Zhou, and Tien-Ping Tan. Videofusion: Decomposed diffusion models for high-quality video generation. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1020910218, 2023. [41] Guoqing Ma, Haoyang Huang, and et al. Kun Yan. Step-video-t2v technical report: The practice, challenges, and future of video foundation model, 2025. [42] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. [43] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. [44] OpenAI. Openai-sora. https://openai.com/sora, 2024. [45] Zijie Pan, Zeyu Yang, Xiatian Zhu, and Li Zhang. Fast dynamic 3d object generation from single-view video. arXiv preprint arXiv 2401.08742, 2024. [46] Pexels. https://www.pexels.com. [47] Pika. Pika. https://pika.art, 2024. [48] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, and et al. Movie gen: cast of media foundation models. https://arxiv.org/abs/2410.13720, 2024. [49] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, 26 Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2024. [50] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward meaningful and decodable representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1061910629, 2022. [51] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-tovideo generation. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 66356645, 2023. [52] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. [53] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506, 2020. [54] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. [56] Runway. Gen-3 alpha. https://runwayml.com/research/introducing-gen-3-alpha, 2024. [57] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. [58] Uriel Singer, Adam Polyak, Thomas Hayes, Xiaoyue Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. ArXiv, abs/2209.14792, 2022. [59] OpenGVLab Team. Internvl2: Better than the bestexpanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024. [60] Tongyi. wanxiang. https://tongyi.aliyun.com/wanxiang/videoCreation, 2024. [61] vidu ai. vidu. https://www.vidu.com, 2024. [62] vivago ai. vivago. https://vivago.ai/video-generation, 2024. [63] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. ArXiv, abs/2308.06571, 2023. [64] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. arXiv preprint arXiv:2410.08260, 2024. [65] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. 2023. [66] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 27 [67] Yuelei Wang, Jian Zhang, Pengtao Jiang, Hao Zhang, Jinwei Chen, and Bo Li. Cpa: Camerapose-awareness diffusion transformer for video generation. arXiv preprint arXiv:2412.01429, 2024. [68] Zhouxia Wang, Yushi Lan, Shangchen Zhou, and Chen Change Loy. Objctrl-2.5 d: Training-free object control with camera poses. arXiv preprint arXiv:2412.07721, 2024. [69] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. [70] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. [71] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50365045, 2022. [72] Zeyu Yang, Zijie Pan, Chun Gu, and Li Zhang. Diffusion²: Dynamic 3d content generation via score composition of video and multi-view diffusion models. In International Conference on Learning Representations (ICLR), 2025. [73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [74] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei. 4dgen: Grounded 4d content generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023. [75] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [76] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [77] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91509161, 2023. [78] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. 2024. [79] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. [80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. [81] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018."
        }
    ],
    "affiliations": [
        "IEIT System Co., Ltd.",
        "Nankai University",
        "Tsinghua University"
    ]
}