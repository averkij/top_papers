{
    "paper_title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
    "authors": [
        "Grzegorz Stefanski",
        "Alberto Presta",
        "Michal Byra"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning."
        },
        {
            "title": "Start",
            "content": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Grzegorz Stefa nski 1 Alberto Presta 1 Michał Byra 1 2 6 2 0 2 9 2 ] A . [ 1 1 4 1 2 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms singleand multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, performance drop under aggressive pruning, and introduce subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning. 1. Introduction Despite the remarkable progress deep neural networks have achieved over the past decades, modern models often require billions of parameters and hundreds of gigaflops per inference, making them impractical for deployment in resourceconstrained or real-time settings. This inefficiency stands in stark contrast to biological intelligence, which achieves high performance with extreme parsimony. While specialized hardware continues to advance, the growth in model scale consistently outpaces gains in 1Samsung AI Center Warsaw, Poland 2Institute of Fundamental Technological Research, Polish Academy of SciCorrespondence to: Grzegorz Stefanski ences, Poland. <g.stefanski@samsung.com>. Preprint. January 30, 2026. 1 computational efficiency. Consequently, reducing model complexity is not only crucial for practical deployment but also for understanding the fundamental principles of generalization in deep learning. Pruning has been cornerstone of model efficiency research (Cheng et al., 2024). By removing redundant weights or structures, it aims to uncover smaller subnetworks that retain the performance of their dense counterparts. The Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019) revitalized this line of work by positing that winning tickets, i.e. sparse subnetworks within large, randomly initialized networks, can be trained in isolation to match full-model accuracy. This reframed pruning as discovery process aiming to reveal the minimal structures responsible for learning. However, nearly all LTH-inspired methods assume universal subnetwork - single sparse mask applied uniformly across all inputs, overlooking real-world data heterogeneity. Different classes, clusters, or environmental conditions often rely on distinct feature representations. one-sizefits-all mask may therefore sacrifice performance by forcing diverse patterns through shared, rigid architecture. Bridging this gap requires moving beyond global sparsity toward adaptive, data-aware pruning, shift that aligns model structure with the intrinsic organization of the data itself. In this work, we introduce Routing the Lottery (RTL), novel adaptive pruning framework that rethinks LTH by discovering multiple specialized subnetworks, dubbed adaptive tickets, each tailored to distinct data subset (e.g., class or semantic cluster). This shift enables the model to allocate representational capacity heterogeneously across the input space, aligning sparsity with data structure rather than enforcing uniform compression. Our framework achieves specialization through pruning alone, without auxiliary routing networks or additional parameters. It discovers multiple stable, sparse subnetworks, each tied to class or cluster, and selects them via simple context-based routing (e.g., label or environment), offering lightweight and interpretable alternative to Mixture-of-Experts (MoE) architectures that prioritizes structural efficiency over dynamic routing flexibility. This work serves as precursor, aiming to guide pruning from static tool into dynamic mechanism for building modular and semantically grounded models. The major Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data contributions of this paper are: RTL jointly learns multiple sparse subnetworks from shared dense initialization, with masks adapted to data subsets while preserving parameter sharing, maintaining single compact backbone and employing mask-based routing to enable context-aware inference. We show that class-specific subnetworks on CIFAR-10 outperform both single-mask and multi-model pruning baselines while using up to an order of magnitude fewer parameters than independent models, and that on CIFAR-100, RTL scales naturally to enable effective specialization across larger number of classes. We validate RTL on real-world speech enhancement task, where subnetworks specialized for acoustic environments achieve higher SI-SNRi than universal or independent baselines. 2. Related Work Pruning has rich history, beginning with sensitivity-based methods like Optimal Brain Damage (LeCun et al., 1989) and Optimal Brain Surgeon (Hassibi et al., 1993). In particular, unstructured pruning removes non-relevant weights without imposing structural constraints, i.e., without removing entire layers. (Han et al., 2015) performed pruning by learning meaningful connections, while (Yang et al., 2017; 2018) exploit energy consumption as metric for removing elements. (Sreenivasan et al., 2022; Tartaglione et al., 2022) introduced regularization terms to constrain the magnitude of non-relevant parameters during iterative training, while (Benbaki et al., 2023) considered the combined effect of pruning and updating multiple weights under sparsity constraint. Furthermore, (Tartaglione et al., 2020) compared and analyze two different pruning approaches, i.e. one-shot and gradual, highlighting that the latter allows for better generalization. (Zhang et al., 2024) analyzed fundamental aspects of pruning, identifying two key factors that determine the pruning ratio limit, i.e., weight magnitude and network sharpness, while (Liao et al., 2023; Hur & Kang, 2019; Luo & Wu, 2017; Min et al., 2018) proposed entropybased approaches to guide pruning. In general, owing to the conceptual simplicity of pruning, wide range of methods have been proposed for different scenarios, such as LLM pruning (Frantar & Alistarh, 2023; Sun et al., 2023; Lu et al., 2024; Wei et al., 2024; Tan et al., 2024), convolutional pruning (Zhao et al., 2023), and even spiking neural networks (Shi et al., 2024). However, none of these have considered whether finding single mask applicable to all types of data might be suboptimal solution. Iterative Magnitude Pruning (IMP) (Frankle & Carbin, 2019) emerged as practical algorithm for identifying winning tickets, though it remains computationally intensive due to repeated training cycles. Furthermore, (Paul et al., 2022) attempted to demystify the IMP method, investigating what kind of information the obtained mask encodes and how SGD allows the network to extract such information. Despite numerous extensions covering initialization schemes (Frankle et al., 2021), learning rate rewinding (Renda et al., 2020), theoretical analysis (Tartaglione, 2022; Burkholz et al., 2021; Sakamoto & Sato, 2022; Paul et al., 2022), and algorithmic variants (Wang et al., 2023; Lin et al., 2023), the LTH paradigm has largely focused on global masks shared across the entire dataset. Dynamic sparse training methods relax the fixed-mask assumption by evolving sparsity patterns during training. Approaches such as SET (Mocanu et al., 2017), SNFS (Dettmers & Zettlemoyer, 2020), and RigL (Evci et al., 2020) prune and regrow connections online, implicitly acknowledging that different inputs may activate different network pathways. Furthermore, (Molchanov et al., 2017) extended variational dropout in order to sparsify deep neural networks, while (Tartaglione et al., 2021) introduced sensitivity-based regularization of neurons to learn structured sparse topologies, exploiting neural sensitivity as regularizer. Nevertheless, these methods still maintain single evolving subnetwork rather than explicitly specializing distinct structures for different data regimes. Closer in spirit are conditional computation techniques like MoE (Shazeer et al., 2017; Fedus et al., 2022) and conditional convolutions (Yang et al., 2019), which activate inputdependent subnetworks to improve scalability. However, these methods typically require complex routing mechanisms, large auxiliary parameter sets, and substantial compute budgets. 3. Method We propose an adaptive pruning framework that extends the LTH to support multiple specialized subnetworks. An overview of the method is shown in Fig. 1. Rather than identifying single winning ticket, our approach discovers distinct subnetworks, each tailored to specific data cluster. Let (x; θ) denote neural network parameterized by θ Rd, be given task, and general dataset. In standard pruning, binary mask {0, 1}d selects subnetwork fm via element-wise multiplication: fm = (x; θ), (1) where denotes the Hadamard product. The LTH posits that there exists mask such that the corresponding subnetwork, when trained in isolation from random initialization, matches the performance of the dense model, i.e.: Lfm(D, ) = Lf (D, ), (2) 2 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Algorithm 1 Adaptive ticket extraction Input: Network (x; θ0), train data D, number of subsets K, target sparsity s, pruning factor p, steps , training epoch . Output: Set of mask M. Random initialization of (x; θ0). mk 1θ0 for = 1, . . . , K. DK = {d1, ..., dK } partition of into subsets. θ θ0 while density(mK ) < do for = 1 to do ) optimize for steps on dk dk k-th subset from DK (, θ(k) mk pruning with factor θ θ0. end for end while mk for each of the target subsets, initialized to ones. The dataset is then partitioned into disjoint subsets DK = {d1, ..., dK} using predefined rule, like manual labeling or automatic clustering. Importantly, RTL is fully agnostic with respect to how these subsets are defined, allowing flexibility across diverse tasks and data modalities. Each pruning iteration proceeds sequentially over all subsets. For given subset dk, the network (x; θ0) is trained for steps to obtain temporary parameters θ(k) . We then perform pruning on , removing the lowest-magnitude weights from θ(k) by fraction p. From the set of removed parameters, we yield sparse subnetwork defined by the mask mk. The remaining weights are subsequently reset to their initial values θ0. This process repeats until each mask reaches the desired sparsity level s, producing the final set of adaptive masks = {m1, . . . , mK}. Each mask defines specialized subnetwork (x; mk θ0). 3.3. Joint Retraining of adaptive tickets After obtaining the sparse masks = {m1, . . . , mK}, we perform lightweight joint retraining phase to refine subnetwork performance and reinforce specialization. This step, outlined in Alg. 2, is crucial in our proposed methodology, since it preserves the sparsity structure discovered during mask extraction (alg. 1) and fine-tunes only the active weights without altering mask topology. The process begins with balanced dataset creation. As before, the training set is partitioned into subsets DK = {d1, . . . , dK}, each corresponding to target class or cluster. Each subset dk is divided into mini-batches Bk = {Bk }. To ensure synchronized updates across subnetworks, we repeat batches from smaller subsets cyclically until all subsets contain the same number of batches = max(M1, . . . , MK). This balancing step guarantees that each subnetwork receives the same number of gradient updates per epoch, even if subsets differ in size. 1 , . . . , Bk Mk Figure 1. Adaptive pruning pipeline. First, the dataset is divided into subsets via predefined clustering. Then we extracts adaptive tickets, i.e. subnetworks, optimized to specific data cluster, and finally we performed network joint retraining. where L: denotes the models performance on task and dataset D. While prior work seeks single universal mask m, we hypothesize that different data subsets Dk benefit from distinct, pruned subnetworks. The remainder of this section is organized as follows: Sec. 3.1 formalizes the adaptive pruning objective, Sec. 3.2 describes adaptive mask extraction, and Sec. 3.3 presents the joint retraining procedure. 3.1. Formulation of the adaptive pruning algorithm Given dataset and task , we partition into subsets {D1, . . . , DK} corresponding to classes or clusters. Such labels can be obtained from supervised labeling systems, such as class annotations, or derived through unsupervised clustering, enabling flexible adaptation across settings. For each subset Dk, our method involves learning dedicated mask mk, yielding subnetwork: φmk = (x; mk θ), for Dk. (3) Given this setup, our objective is to jointly learn the set 1, . . . , of masks = {m K} that maximize predictive performance on task under sparsity constraint: LM = min mkM (cid:88) k=1 E(x,y)Dk [ℓ (y, (x; mk θ))] (4) subject to mk0 k, (5) where ℓ denotes the loss function and controls the maximum number of non-zero parameters per subnetwork. 3.2. Adaptive tickets extraction We now describe how adaptive tickets are extracted for each data subset (see Alg. 1). Starting from random initialization (x; θ0), we create an equal-size binary mask 3 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Algorithm 2 Joint retraining of the adaptive tickets Input: Network (x; θ), train data D, number of subsets K, training epochs , learning rate η, mask set M. Output: optimized adaptive tickets fk(; θk). Balanced dataset creation DK = {d1, ..., dK } partition of into subsets. for = 1 to do dk k-th subset from DK Bk {Bk 1 , ..., Bk Mk Bk Mk } {Set of batches from dk} end for max(M1, . . . , MK ). for = 1 to do Repeat batches in Bk cyclically until Bk = . end for Model training for = 1 to do {Training epochs} for = 1 to do {Batch indices} for = 1 to do {Subnetwork indices} mk M[k]. fk(; θk) (; mk θ) {k-th subnetwork} (xk m) Bk θk θk η (cid:0)θL(fk(xk m. {Input and label } m; θk), yk m) mk m, yk (cid:1). end for end for end for During joint retraining, each subnetwork (x; mk θ) is trained exclusively on its corresponding data subset dk. We interleave mini-batches from different subsets and apply gradient updates to the shared dense parameter tensor θ, masking out gradients for pruned weights. Specifically, for subnetwork k, the parameter update is given by: fk(, θk) (, mk θ) θk θη (cid:0)θL(fk(xk; θk), yk) mk (cid:1) (6) (7) where η is the learning rate, denotes the empirical loss, while (xk, yk) represents the input data with its correspondent label coming from the k-th subset. By masking the gradient as in Eq. 7, only the weights retained by mask mk are updated, preventing interference between subnetworks and avoiding both catastrophic forgetting and collapse. Maintaining limited overlap between subnetworks is crucial for ensuring optimal performance within each cluster without mutual interference or cancellation. 4. Experiments We present sequence of experiments that progressively validate our adaptive pruning framework, moving from controlled settings to real-world applications: (i) class-specific subnetworks on CIFAR-10 (Krizhevsky, 2009), (ii) clusteraware pruning on CIFAR-100, (iii) implicit neural representations (INRs) with within-image semantic specialization, (iv) speech enhancement in heterogeneous acoustic environ4 ments, and (v) subnetwork overlap and semantic alignment analysis. Across all tasks, we control for architecture, sparsity, and training budget to isolate the effects of specialization. Quantitative results and discussion are reported in Sec. 5, while full implementation details are provided in the appendix. 4.1. CIFAR-10 subnetwork specialization We begin with controlled CIFAR-10 experiment, where each of the 10 classes defines distinct data subset and is assigned its own subnetwork. This idealized setting allows us to directly test the core premise of our approach: whether subnetworks specialized to disjoint subsets outperform single universal pruning mask under identical constraints. We compare RTL against two baselines: (i) IMP (single model), which produces single shared subnetwork across all classes, and (ii) IMP (multiple models), which independently prunes one model per class without weight sharing. All methods use the same backbone architecture, sparsity targets, and pruning budget. Performance is evaluated using balanced accuracy, precision, recall, and parameter count at 25%, 50%, and 75% sparsity. This experiment establishes an upper bound on specialization benefits and serves as reference point for more challenging scenarios. Training protocols and architectural details are provided in Appendix A. 4.2. Cluster-aware pruning on CIFAR-100 To assess robustness under less ideal conditions, we evaluate RTL on CIFAR-100, where class boundaries are finegrained and semantically overlapping. Instead of assigning subnetworks to individual classes, we group the 100 classes into 8 coarse semantic clusters using an unsupervised textbased clustering procedure. The resulting clusters are semantically coherent but imperfectly aligned with visual features, introducing ambiguity that better reflects real-world data partitioning. RTL is compared against the same two IMP baselines under matched conditions, using the same metrics and sparsity levels as in the CIFAR-10 experiment. Details of the clustering pipeline are provided in Appendix B. 4.3. Implicit Neural Representations We evaluate RTL in the context of INRs, where specialization is defined over semantic regions within single image. The task consists of reconstructing an image by mapping continuous pixel coordinates to RGB values using coordinate-based neural network. Experiments are conducted on 10 images from the ADE20K dataset (Zhou et al., 2019) selected to ensure diversity in Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data scene category and luminosity/color characteristics. Semantic segmentation masks define region-level classes, and RTL learns specialized subnetworks for these regions, while standard baseline conditions single network on region identity via class embeddings. Reconstruction quality is evaluated using peak signal-tonoise ratio (PSNR), averaged over all pixels and all images. Architectural choices, positional encoding, training protocol, and per-image results are reported in Appendix C. 4.4. Speech enhancement in realistic environments We evaluate RTL on real-world speech enhancement task characterized by heterogeneous acoustic conditions. Clean speech is mixed with noise from three distinct acoustic scenes (indoor, outdoor, and transportation), each defining specialization subset. RTL learns one subnetwork per acoustic scene and is compared against (i) single IMP-pruned model shared across all environments and (ii) independently pruned IMP models without weight sharing. All methods operate under identical sparsity and computational constraints. Performance is measured using scale-invariant signal-to-noise ratio improvement (SI-SNRi). Dataset construction, model architecture, and signal processing details are provided in Appendix D. 4.5. Subnetwork collapse and semantic alignment Beyond task-level performance, we analyze relationships among learned subnetworks in the CIFAR-10 and CIFAR100 experiments. Specifically, we study how subnetwork overlap evolves with increasing sparsity and how excessive overlap relates to performance degradation, phenomenon we refer to as subnetwork collapse. We quantify pairwise mask similarity at multiple sparsity levels and examine its correlation with balanced accuracy. Finally, using CIFAR-10, we compare structural similarity with semantic distances between class labels derived from WordNet, assessing whether pruning structures encode high-level conceptual relationships. Formal definitions and analysis procedures are provided in Appendix E. 5. Results 5.1. CIFAR-10 subnetwork specialization Table 1 summarizes CIFAR-10 results under class-specific pruning. RTL consistently achieves the highest balanced accuracy and recall across all sparsity levels. At 25% sparsity, RTL attains balanced accuracy of 0.781, significantly outperforming both baselines (0.711 for single-model IMP and 0.712 for multi-model IMP). This advantage persists at 50% sparsity (0.778 vs. 0.711) and remains competitive even at 75% sparsity (0.772 vs. 0.760 for multi-model IMP), despite using an order of magnitude fewer parameters. RTL also achieves the highest recall at all sparsity levels (0.821 / 0.810 / 0.816), exceeding the single-model baseline and matching or surpassing the multi-model alternative. This demonstrates that RTL subnetworks effectively preserve class-relevant signals under aggressive pruning. The lower precision of RTL (0.257-0.282) compared to the single-model IMP baseline (0.478-0.515) reflects design trade-off: by prioritizing recall, RTL subnetworks favor sensitivity to true positives over strict discrimination, which is well-suited for class-specialized inference. In downstream applications, precision can be recovered via thresholding or ensemble methods if needed. Table 1. Results on CIFAR-10 with class-specific subnetworks and CIFAR-100 with cluster-specialized subnetworks. We report balanced accuracy, precision, recall, and the number of remaining parameters at three sparsity levels. RTL is compared against (a) IMP with single shared subnetwork and (b) IMP trained independently per class (multiple models). Best values per metric and sparsity level are bolded. Clusters are derived from semantic embeddings (see Section 4). Method Balanced accuracy Precision Recall #Params 25% 50% 75% 25% 50% 75% 25% 50% 75% 25% 50% 75% RTL (ours) 0.781 0.778 0. 0.282 0.276 0.257 0.821 0.810 0. 103K 72K 38K IMP (single model) IMP (multiple models) 0.711 0. 0.732 0.479 0.478 0.515 0.480 0. 0.518 94K 63K 31K 0.712 0. 0.760 0.233 0.222 0.262 0.701 0. 0.766 944K 629K 314K RTL (ours) 0.765 0.751 0. 0.298 0.290 0.289 0.764 0.729 0. 108K 76K 40K IMP (single model) IMP (multiple models) 0.722 0. 0.742 0.45 0.463 0.522 0.42 0. 0.463 94K 63K 31K 0.712 0. 0.744 0.276 0.271 0.286 0.660 0. 0.732 944K 629K 314K 0 1 - I 0 0 1 - I 5 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Table 2. INR results on ADE20k dataset. Methods are evaluated at 25%, 50%, and 75% sparsity using PSNR, reporting the number of trainable parameters. Table 3. Speech enhancement results on DNS Challenge 2020 and TAU Urban Acoustic Scenes 2020 datasets. Methods are evaluated at 25%, 50%, and 75% sparsity using SI-SNRi, reporting the number of trainable parameters. Method PSNR #Params 25% 50% 75% 25% 50% 75% Method SI-SNRi #Params 25% 50% 75% 25% 50% 75% RTL (ours) 18.86 17.25 14.87 48.0K 36.5K 22.0K IMP (single model) 15.94 14.72 12.69 40.5K 27.0K 13.5K Critically, these gains come with exceptional parameter efficiency. RTL uses only 103K / 72K / 38K parameters at 25% / 50% / 75% sparsity, compared to 944K / 629K / 314K for multi-model IMP (nearly 10 more) to achieve comparable accuracy. This underscores RTLs ability to deliver high performance without sacrificing compactness. 5.2. Cluster-aware pruning on CIFAR-100 Table 1 also reports CIFAR-100 results, where subnetworks are specialized to semantically derived clusters. Despite the inherent ambiguity in this partitioning, RTL again achieves the highest balanced accuracy across all sparsity levels: 0.765 (25%), 0.751 (50%), and 0.759 (75%). Notably, RTL outperforms both baselines even at 75% sparsity, where model capacity is most constrained, demonstrating robustness to imperfect data grouping. Recall follows the same trend: RTL achieves 0.764 / 0.729 / 0.754, substantially exceeding the single-model IMP baseline (0.420-0.463) and consistently outperforming the multimodel alternative. This confirms that RTL subnetworks retain cluster-discriminative features more effectively under pruning, even when cluster boundaries are noisy. As in CIFAR-10, RTL exhibits lower precision than the single-model baseline, reflecting its emphasis on sensitivity over selectivity. Given the coarse and overlapping nature of the clusters, high recall is particularly valuable for ensuring coverage of relevant visual concepts. In terms of efficiency, RTL uses only 108K / 76K / 40K parameters, comparable to the single-model model baseline and drastically fewer than the multi-model baseline (944K / 629K / 314K). This reaffirms that RTL achieves strong specialization without requiring redundant, over-parameterized subnetworks, making it especially suitable for scenarios with limited memory or compute. 5.3. Implicit Neural Representations Table 2 reports reconstruction performance for the INR experiment on ADE20K images at varying sparsity levels. Across all sparsity regimes, RTL consistently outperforms the standard IMP baseline. At 25% sparsity, RTL achieves RTL (ours) 7.248 7.178 6.992 32.0K 22.8K 12.3K IMP (single model) IMP (multiple model) 6.885 6.97 6.967 28.0K 18.6K 9.3K 5.295 5.775 5.899 84.1K 56.1K 28.0K PSNR of 18.58, exceeding the single-model IMP baseline by almost 3 dB. This performance gap remains substantial as sparsity increases, with RTL maintaining advantages of 2.53 dB and 2.18 dB at 50% and 75% sparsity, respectively. Notably, these gains are achieved despite RTL retaining larger number of parameters than the single-model IMP baseline. This behavior mirrors observations from the classification and speech enhancement experiments: enforcing single global pruning mask across heterogeneous subsets (in this case, semantically distinct image regions) leads to suboptimal allocation of model capacity. By contrast, RTL enables region-specific subnetworks that preserve functionally important parameters, resulting in higher reconstruction fidelity under identical computational budgets. As sparsity increases, performance degrades for both methods, but RTL degrades more gracefully, indicating that specialization is especially beneficial in high-sparsity regimes, where competition for shared parameters intensifies. The results demonstrate that adaptive pruning can effectively capture semantic structure even in coordinate-based representations, extending the benefits of specialization beyond dataset-level tasks to within-image semantic decomposition. Per-image PSNR values are reported in Appendix C, confirming that the observed trends are consistent across images with diverse scene content and appearance characteristics. 5.4. Speech Enhancement in realistic environments Table 3 reports speech enhancement performance across three acoustic environments (indoor, outdoor, and transportation). RTL achieves the highest SI-SNRi at all sparsity levels: 7.248, 7.178, and 6.992 at 25%, 50%, and 75% sparsity, respectively, consistently outperforming both IMP baselines. This demonstrates that environment-specific subnetworks better capture each noise types distinct spectro-temporal characteristics, yielding superior waveform reconstruction. The single-mask IMP model performs reasonably well, likely by learning general-purpose denoising strategy. However, it lags behind RTL, confirming that specialization 6 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data yields tangible gains even in non-classification task. In contrast, the multi-model IMP baseline underperforms despite larger parameter budget (84.1K vs. 32.0K at 25% sparsity), suggesting that independent pruning without shared initialization or joint retraining yields suboptimal results. Critically, RTL achieves these gains with only modest overhead in model size: 32.0K / 22.8K / 12.3K parameters, just slightly larger than the single-mask model and less than half the size of the multi-model alternative. Notably, even the most aggressively pruned RTL subnetwork (12.3K parameters) surpasses both baselines in SI-SNRi, underscoring the efficiency of our approach. These results confirm that RTL generalizes beyond controlled vision benchmarks: when subnetworks align with meaningful real-world structure (in this case, acoustic environments), they deliver significantly better performance while remaining compact and deployable. 5.5. Subnetworks collapse and semantic alignment Subnetworks collapse Fig. 2 illustrates the relationship between performance and mask similarity across sparsity levels for CIFAR-10 and CIFAR-100. For each class or cluster, we plot (i) balanced accuracy and (ii) average Jaccard similarity (IoU) to all other subnetworks. These curves reveal the onset of subnetwork collapse - failure mode in which excessive pruning forces subnetworks to converge to overlapping weight sets, eroding their specialization. In CIFAR-10 (Fig. 2A-J), where classes are perfectly separated, most subnetworks maintain high accuracy and low mask similarity up to 70-80% sparsity. Beyond this point, sharp increase in IoU coincides with precipitous drop in accuracy, confirming that performance degradation is directly linked to loss of structural distinctiveness. Crucially, the IoU spike reliably precedes or coincides with the accuracy drop, suggesting that mask similarity can serve as label-free early-warning signal for oversparsification. On CIFAR-100 (Fig. 2K-S), where classes are grouped into eight semantically derived clusters, the same collapse signature emerges: accuracy remains stable as long as mask similarity is bounded, but deteriorates sharply once pruning forces subnetworks into excessive overlap. Collapse occurs at slightly higher sparsity thresholds than in CIFAR-10, likely because clusters share more visual features, allowing for greater weight reuse before specialization is lost. Across both datasets, the evidence is consistent: subnetwork specialization is essential for RTLs performance, and mask similarity is robust, label-free predictor of collapse. Moreover, the abruptness of the accuracy drop suggests that once specialized weights are pruned beyond critical point, recovery is unlikely without full retraining, highlighting the importance of stopping pruning before collapse occurs. Semantic alignment Fig. 3 summarizes how semantic alignment relates to subnetwork structure throughout training and across depth. Panels A-D report Spearmans rank-order correlations between semantic proximity (from WordNet) and mask similarity (Jaccard index), while panel shows the corresponding WordNet and mask similarity matrices for shallow and deep layers. Panel shows pruning correlations across layer depths: shallow and middle layers increase correlation up to 2035% sparsity, then decline, indicating that moderate pruning enhances semantic organization in early layers, while excessive sparsity degrades it. Deep layers start with low correlations but rise steadily to around 55% sparsity, reflecting stronger semantic alignment as higher-level representations develop. Panel tracks training evolution: early optimization shows weak correlations decreasing with depth, but later epochs reveal consistent increases, particularly in middle and deep layers, indicating gradual emergence of semantic structure as subnetworks refine specialized connectivity. Panels and highlight four representative classes (airFigure 2. Mask collapse analysis on CIFAR-10 and CIFAR-100. Each subplot corresponds to one class and shows balanced accuracy (solid line) and mask similarity to other subnetworks (dashed line). Plots A-J corresponds to CIFAR-10 network and plots K-S to CIFAR-100. 7 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Figure 3. Semantic and structural correlation analysis. (A) Spearmans rank-order correlation between semantic similarity and mask similarity versus pruning ratio across shallow, middle, and deep layers. (B) Correlation across depth for early, middle, and late training stages. (C) Correlation versus pruning ratio for four representative classes: airplane, cat, deer, and truck. (D) Correlation across depth for the same four classes. (E) WordNet path similarity (top) and RTL mask similarity matrices for shallow (middle) and deep (bottom) layers. plane, cat, deer, truck). Subnetworks for semantically related classes (cat, deer) show stronger correlations (0.6-0.7) that grow with pruning and depth, while unrelated ones (airplane, truck) remain weaker (0.2-0.3). Thus, RTL subnetworks for related categories preserve overlapping structures, whereas distant ones evolve independently - pattern consistent with overall mask stability across dissimilar classes. Panel provides complementary heatmaps. The top matrix shows WordNet path similarities, followed by mask similarities for shallow and deep layers. Shallow masks are uniformly similar (mean 0.86), reflecting shared early filters for low-level, class-agnostic features such as edges and textures. Slight local increases for related pairs, for example cat-dog (0.89), automobile-truck (0.87), and deerhorse (0.88), suggest broad semantic grouping even at early stages. In deep layers, mask similarity drops (mean 0.31), and clear block-diagonal structure emerges that aligns with WordNet relations. Related animal classes (cat, dog, horse, deer) show higher overlap (0.33-0.36) than unrelated ones (0.27-0.31). RTL forms weaker specialization among vehicle classes (airplane, automobile, ship, truck), likely due to limited visual similarity beyond the automobile-truck pair. Overall, RTL subnetworks gradually organize according to semantic structure in the data. Early layers are shared and class-agnostic, while deeper layers increasingly reflect conceptual hierarchies. The growing correlation with both depth and training indicates that RTL pruning not only enforces sparsity but also promotes semantically meaningful specialization within shared model architecture. 6. Conclusion We introduced Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks (adaptive tickets) rather than single universal winning ticket. Across multiple settings RTL consistently achieves superior or competitive performance while maintaining compact parameter footprint. These results show that specialization naturally emerges when subnetworks are allowed to diverge in response to data heterogeneity. Our analysis reveals two critical insights. First, subnetwork distinctiveness is essential: mask similarity serves as reliable, label-free indicator of subnetwork collapse, showing that the bottleneck is not raw capacity, but the preservation of structural diversity. Second, RTL is robust to imperfect data partitions - it excels with clean class boundaries (CIFAR-10), remains effective under noisy semantic clustering (CIFAR-100), and generalizes to real-world applications where subnetworks align with meaningful environmental factors. We also observe that RTL subnetworks tend to favor recall over precision, increasing sensitivity to weak classor environment-specific signals. While beneficial for routingbased inference, this suggests that downstream calibration or selective filtering could further improve precision without compromising specialization. This work positions adaptive pruning as pathway toward more modular, efficient, and interpretable deep models, ones that dynamically allocate representational capacity in alignment with the intrinsic structure of the data they process. 8 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Benbaki, R., Chen, W., Meng, X., Hazimeh, H., Ponomareva, N., Zhao, Z., and Mazumder, R. Fast as chita: Neural network pruning with combinatorial optimization. In International Conference on Machine Learning, pp. 20312049. PMLR, 2023. Burkholz, R., Laha, N., Mukherjee, R., and Gotovos, A. On the existence of universal lottery tickets. arXiv preprint arXiv:2111.11146, 2021. Cheng, H., Zhang, M., and Shi, J. Q. survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Dettmers, T. and Zettlemoyer, L. Sparse networks from scratch: Faster training without losing performance, 2020. URL https://openreview.net/forum? id=ByeSYa4KPS. Evci, U., Elsen, E., Castro, P., and Gale, T. Rigging the lottery: Making all tickets winners, 2020. URL https: //openreview.net/forum?id=ryg7vA4tPB. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple Journal of Machine Learning and efficient sparsity. Research, 23(120):139, 2022. URL http://jmlr. org/papers/v23/21-0998.html. Frankle, J. and Carbin, M. The lottery ticket hypotheIn Insis: Finding sparse, trainable neural networks. ternational Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=rJl-b3RcF7. Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Pruning neural networks at initialization: Why are we missing In International Conference on Learning the mark? Representations, 2021. URL https://openreview. net/forum?id=Ig-VyQc-MLK. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning, pp. 1032310337. PMLR, 2023. Han, K., Wang, Y., Tian, Q., Guo, J., Xu, C., and Xu, C. Ghostnet: More features from cheap operations. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15771586, 2019. URL https://api.semanticscholar. org/CorpusID:208310058. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. Hassibi, B., Stork, D. G., and Wolff, G. J. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pp. 293299 vol.1, 1993. URL https://api.semanticscholar. org/CorpusID:61815367. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 10261034. IEEE, 2015. doi: 10.1109/ICCV.2015.123. Heittola, T., Mesaros, A., and Virtanen, T. Tau urban acoustic scenes 2020 3class, development dataset, February 2020. URL https://doi.org/10.5281/ zenodo.3670185. Hur, C. and Kang, S. Entropy-based pruning method for convolutional neural networks. Journal of Supercomputing, 75(6), 2019. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015. URL https://arxiv. org/abs/1412.6980. Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Technical Report. LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. In Touretzky, D. (ed.), Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989. URL https://proceedings.neurips. cc/paper_files/paper/1989/file/ 6c9882bbac1c7093bd25041881277658-Paper. pdf. Liao, Z., Quetu, V., Nguyen, V.-T., and Tartaglione, E. Can unstructured pruning reduce the depth in deep neural networks? In Proceedings of the IEEE/CVF international conference on computer vision, pp. 14021406, 2023. Lin, J., Luo, X., Hong, M., Qu, Y., Xie, Y., and Wu, Z. Memory-friendly scalable super-resolution via rewinding lottery ticket hypothesis. In Proceedings of the IEEE/CVF 9 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Conference on Computer Vision and Pattern Recognition, pp. 1439814407, 2023. Lu, X., Zhou, A., Xu, Y., Zhang, R., Gao, P., and Li, H. Spp: Sparsity-preserved parameter-efficient fine-tuning for large language models. arXiv preprint arXiv:2405.16057, 2024. Luo, J.-H. and Wu, J. An entropy-based pruning method for cnn compression. arXiv preprint arXiv:1706.05791, 2017. Miller, G. A. Wordnet: lexical database for english. Communications of the ACM, 38(11):3941, 1995. doi: 10.1145/219717.219748. Min, C., Wang, A., Chen, Y., Xu, W., and Chen, X. 2pfpce: Two-phase filter pruning based on conditional entropy. arXiv preprint arXiv:1809.02220, 2018. Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 9, 2017. URL https://api.semanticscholar. org/CorpusID:49310977. Molchanov, D., Ashukha, A., and Vetrov, D. Variational dropout sparsifies deep neural networks. In International conference on machine learning, pp. 24982507. PMLR, 2017. Paul, M., Chen, F., Larsen, B. W., Frankle, J., Ganguli, S., and Dziugaite, G. K. Unmasking the lottery ticket hypothesis: Whats encoded in winning tickets mask? arXiv preprint arXiv:2210.03044, 2022. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum? id=B1ckMDqlg. Shi, X., Ding, J., Hao, Z., and Yu, Z. Towards energy efficient spiking neural networks: An unstructured pruning framework. In The Twelfth International Conference on Learning Representations, 2024. Sreenivasan, K., Sohn, J.-y., Yang, L., Grinde, M., Nagle, A., Wang, H., Xing, E., Lee, K., and Papailiopoulos, D. Rare gems: Finding lottery tickets at initialization. Advances in neural information processing systems, 35: 1452914540, 2022. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. Tan, Z., Zhang, X., and Wei, Z. Wrp: Weight recover prune for structured sparsity. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 64336443, 2024. Tartaglione, E. The rise of the lottery heroes: why zero-shot pruning is hard. In 2022 IEEE International Conference on Image Processing (ICIP), pp. 23612365. IEEE, 2022. Tartaglione, E., Bragagnolo, A., and Grangetto, M. Pruning artificial neural networks: way to find wellIn Internageneralizing, high-entropy sharp minima. tional Conference on Artificial Neural Networks, pp. 67 78. Springer, 2020. Reddy, C. K., Gopal, V., Cutler, R., Beyrami, E., Cheng, R., Dubey, H., Matusevych, S., Aichner, R., Aazami, A., Braun, S., et al. The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results. In Proc. Interspeech 2020, 2020. Tartaglione, E., Bragagnolo, A., Odierna, F., Fiandrotti, A., and Grangetto, M. Serene: Sensitivity-based regularization of neurons for structured sparsity in neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(12):72377250, 2021. Renda, A., Frankle, J., and Carbin, M. Comparing rewindIn Ining and fine-tuning in neural network pruning. ternational Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=S1gSj0NKvB. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. ArXiv, abs/1505.04597, 2015. URL https://api. semanticscholar.org/CorpusID:3719281. Sakamoto, K. and Sato, I. Analyzing lottery ticket hypothesis from pac-bayesian theory perspective. Advances in Neural Information Processing Systems, 35:30937 30949, 2022. Tartaglione, E., Bragagnolo, A., Fiandrotti, A., and Grangetto, M. Loss-based sensitivity regularization: towards deep sparse neural networks. Neural Networks, 146:230237, 2022. Wang, Q., Dun, C., Liao, F., Jermaine, C., and Kyrillidis, A. Loft: Finding lottery tickets through filter-wise training. In International Conference on Artificial Intelligence and Statistics, pp. 64986526. PMLR, 2023. Wei, J., Lu, Q., Jiang, N., Li, S., Xiang, J., Chen, J., and Liu, Y. Structured optimal brain pruning for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1399114007, 2024. 10 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Yang, B., Bender, G., Le, Q. V., and Ngiam, J. CondConditionally parameterized convolutions conv: for efficient inference. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ f2201f5191c4e92cc5af043eebfd0946-Paper. pdf. Yang, T.-J., Chen, Y.-H., and Sze, V. Designing energyefficient convolutional neural networks using energyaware pruning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 56875695, 2017. Yang, T.-J., Howard, A., Chen, B., Zhang, X., Go, A., Sandler, M., Sze, V., and Adam, H. Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of the European conference on computer vision (ECCV), pp. 285300, 2018. Zhang, Q., Zhang, R., Sun, J., and Liu, Y. How sparse can we prune deep network: fundamental limit perspective. Advances in Neural Information Processing Systems, 37:9133791372, 2024. Zhao, K., Jain, A., and Zhao, M. Automatic attention pruning: Improving and automating model pruning using attentions. In International Conference on Artificial Intelligence and Statistics, pp. 1047010486. PMLR, 2023. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., and Torralba, A. Semantic understanding of scenes International Journal of through the ade20k dataset. Computer Vision, 127(3):302321, 2019. Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data A. Vision Model and Training Setup A.1. Model architecture and pruning scope For all computer vision experiments on CIFAR-10 and CIFAR-100, we use GhostNet (Han et al., 2019) as the backbone. To control model capacity and isolate the effects of adaptive pruning, we retain the convolutional stem followed by the first nine Ghost bottleneck blocks, discarding deeper stages of the original network. All convolutional layers are implemented as masked convolutions and constitute the pruning scope. Batch normalization layers, squeeze-and-excitation modules, and the final classifier remain dense. Under this configuration, the model contains approximately 126K prunable parameters. The retained GhostNet variant follows the standard Ghost bottleneck design, combining inexpensive depthwise convolutions with pointwise expansions and optional downsampling and squeeze-and-excitation. Spatial resolution is progressively reduced via strided depthwise convolutions in selected bottlenecks. detailed summary of the architecture, including channel dimensions and downsampling stages, is provided in Table 4 to ensure reproducibility. Table 4. GhostNet backbone used for CIFAR experiments. Only convolutional layers are pruned. All convolutions are followed by batch normalization and ReLU unless stated otherwise. Stage Stem 1 2 4 5 6 7 8 Head Classifier Block type Input Output Kernel / Stride Notes Conv2D GhostBottleneck GhostBottleneck GhostBottleneck GhostBottleneck GhostBottleneck GhostBottleneck GhostBottleneck 3 Conv11 Conv + Pool Linear 3 16 16 16 24 24 24 24 40 40 40 40 80 80 80 184 184 128 128 3 3 / 2 Initial downsampling 1 1, 3 3 / No shortcut 3 3 / 2 Downsampling + shortcut 1 1, 3 3 / 1 5 5 / 2 1 1, 3 3 / Shortcut SE + shortcut SE 3 3 / 2 Downsampling + shortcut 1 1, 3 3 / Repeated blocks 1 1 / 1 Channel expansion 1 1 Global avg pooling = 10 or 100 A.2. Optimization and training protocol All models are trained using the Adam optimizer (Kingma & Ba, 2015) with learning rate of 1e4 and no weight decay. Each pruning iteration consists of two phases: (i) pruning and (ii) joint retraining, both run for 10 epochs. Batch sizes are set to 320 for CIFAR-10 and 256 for CIFAR-100. To ensure fair comparison, all methods (RTL, single-model IMP, and multi-model IMP) share the same random Kaiming initialization (He et al., 2015). RTL and multi-model IMP subnetworks are trained as binary classifiers with balanced mini-batches, effectively doubling the number of sample presentations per epoch. To match total data exposure, the single-model IMP baseline is trained for 20 epochs. A.3. Pruning schedule and compute Pruning follows fixed schedule, removing 4,096 weights per epoch for each subnetwork until the target sparsity is reached. All experiments are executed on single NVIDIA H100 GPU. Full RTL and multi-model IMP runs require approximately 6 hours, while the single-model IMP baseline completes in roughly 45 minutes. B. Semantic Clustering Procedure To construct coarse semantic subsets for CIFAR-100, we apply an unsupervised clustering pipeline based on textual class descriptions rather than visual features. 12 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data First, we extract text embeddings for each of the 100 CIFAR-100 class names using the CLIP text encoder. These embeddings are then reduced in dimensionality using UMAP to facilitate clustering in lower-dimensional space. Finally, we apply HDBSCAN to the reduced embeddings to obtain cluster assignments. This procedure yields 8 semantic clusters. While the resulting clusters are semantically coherent, they are not perfectly aligned with visual similarity, intentionally introducing noise and overlap. This imperfect alignment better reflects real-world scenarios where data partitions are ambiguous or only approximately defined. The full cluster composition is reported in Table 5. Table 5. Semantic clusters used in the CIFAR-100 experiments. Each cluster defines data subset to which specialized subnetwork is assigned. Cluster CIFAR-100 Classes 1 (Aquatic animals) aquarium fish, dinosaur, dolphin, flatfish, ray, seal, shark, trout, whale 2 (People & objects) baby, bed, bicycle, bottle, bowl, boy, bridge, can, castle, chair, clock, couch, cup, girl, house, keyboard, lamp, man, plain, plate, road, rocket, skyscraper, table, telephone, television, wardrobe, woman 3 (Mammals) bear, beaver, camel, cattle, chimpanzee, elephant, fox, hamster, kangaroo, leopard, lion, mouse, otter, porcupine, possum, rabbit, raccoon, shrew, skunk, squirrel, tiger, wolf 4 (Reptiles & insects) bee, beetle, butterfly, caterpillar, cockroach, crab, crocodile, lizard, lobster, mushroom, snail, snake, spider, turtle, worm 5 (Vehicles) bus, lawn mower, motorcycle, pickup truck, streetcar, tank, tractor, train 6 (Natural scenes) cloud, forest, maple tree, mountain, oak tree, palm tree, pine tree, sea, willow tree 7 (Fruits) 8 (Flowers) apple, orange, pear, sweet pepper orchid, poppy, rose, sunflower, tulip All clustering hyperparameters are fixed across experiments and are not tuned to downstream task performance. As shown in Sec. 5, RTL remains effective under this setting, demonstrating robustness to non-ideal specialization boundaries and highlighting the benefits of adaptive pruning beyond strictly class-aligned scenarios. C. Implicit Neural Representation C.1. Task setup We consider the standard INR formulation of mapping continuous pixel coordinates (x, y) to RGB values. The full image is reconstructed through point-wise evaluation of the network over all pixel locations. Experiments are conducted on 10 images selected from the training split of the ADE20K dataset to ensure diversity in scene category and luminosity/color characteristics. Specifically, we use images with indices 0, 3733, 7304, 8399, 11708, 12963, 13783, 14236, 18813, and 23790. Each image is treated independently, and separate model is trained per image. The selected images and their corresponding preprocessed semantic segmentation masks are shown in Fig. 4. Semantic segmentation annotations are used to define classes: all masks corresponding to the same semantic object category within an image are merged into single class (e.g., multiple instances of tree are treated as one mask). The dataset contains small unassigned or noisy regions, typically at object boundaries. These regions are reassigned to neighboring semantic class, which was empirically found to stabilize training for both RTL and baseline models. C.2. Positional encoding Input coordinates (x, y) are encoded using Fourier features prior to being passed to the network, following standard INR practice. This encoded representation forms the sole input to the RTL model. 13 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Figure 4. Data samples from the ADE20K dataset used in the INR experiments, shown together with their corresponding preprocessed semantic segmentation masks. C.3. Model architectures All models are multilayer perceptrons (MLPs) with ReLU activations. For RTL, the network consists of five layers with the following inputoutput dimensions: [[128, 34], [128, 128], [128, 128], [128, 128], [3, 128]] For the standard baseline, the input is augmented with learned class embedding corresponding to the semantic mask of each pixel, increasing the first-layer dimensionality to [128, 50]. All subsequent layers are identical to those used in RTL. C.4. Training and pruning protocol All models are trained using the Adam optimizer with learning rate of 0.01. Training proceeds for 10,000 optimization steps, after which pruning is applied by removing 4,096 weights per pruning iteration. Following each pruning step, all remaining (non-pruned) weights are rewound to their initial values, consistent with the IMP-style pruning protocol used throughout the paper. This prune-and-rewind process is repeated until the target sparsity is reached. C.5. Initialization and fairness considerations Because the RTL and baseline models differ in input dimensionality, care is taken to ensure fair initialization. We initialize the larger baseline network (including the class embedding input) and manually remove the weights corresponding to the class embedding dimensions to obtain the RTL initialization. This ensures that all shared parameters are initialized identically across methods. 14 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data The same initialization seed is used across all images. C.6. Evaluation protocol Reconstruction quality is evaluated using peak signal-to-noise ratio (PSNR). In the main text, PSNR is reported as single scalar value averaged over all pixels and all images. For completeness, we additionally report per-image PSNR values in the appendix to illustrate variability across samples. C.7. Per-Image INR Results Table 6 reports per-image reconstruction performance for the INR experiment on 10 ADE20K images. Results are shown for RTL and the single-model IMP baseline at 25%, 50%, and 75% sparsity, measured using PSNR. Table 6. Per-image PSNR for the INR experiment on 10 ADE20K images. Results are reported for RTL and the single-model IMP baseline at 25%, 50%, and 75% sparsity. PSNR"
        },
        {
            "title": "RTL",
            "content": "IMP (single model) 25% 50% 75% 25% 50% 75% 1 2 3 5 6 7 8 9 13. 12.08 10.40 9.27 9.68 7.94 11. 10.14 8.60 7.95 8.39 6.83 17. 16.92 15.66 16.32 15.45 13.96 24. 22.58 20.56 21.58 20.19 17.79 18. 16.88 13.99 14.63 12.99 10.96 19. 17.23 14.50 16.63 15.44 13.24 25. 23.14 19.30 19.81 18.62 15.96 15. 14.08 11.97 12.47 11.45 9.79 19. 17.73 15.41 16.81 15.43 13.17 24.31 22.63 19.68 21.07 19.53 16. Across all samples and sparsity levels, RTL consistently achieves higher PSNR than the single-model IMP baseline. The performance gap is observed for images with diverse scene content and appearance characteristics, confirming that the aggregate improvements reported in the main text are not driven by small subset of favorable samples. The advantage of RTL is particularly pronounced at higher sparsity levels, where enforcing single global pruning mask across semantically heterogeneous regions leads to larger reconstruction errors. In contrast, RTL preserves region-specific parameters through specialized subnetworks, resulting in more stable degradation as sparsity increases. These per-image results complement the averaged PSNR values reported in the main text by demonstrating that the benefits of adaptive pruning in INR settings are consistent across images, rather than arising from outliers or dataset bias. Figure 5 shows the relationship between reconstruction quality and pruning mask similarity for each of the 10 ADE20K images (AJ) used in the INR experiment. For each image, we report PSNR (orange solid line, left axis) and average mask similarity measured by the Jaccard index (blue dashed line, right axis) as function of sparsity. Across all images, PSNR decreases gradually as sparsity increases, followed by sharper drop at high sparsity levels. similar trend is observed for mask similarity: pruning masks remain relatively stable at low to moderate sparsity, but their similarity rapidly decreases beyond critical sparsity threshold. This transition is consistent across images, despite substantial differences in scene content and appearance. Notably, the sharp decline in PSNR closely coincides with the point at which mask similarity collapses. This indicates that reconstruction quality degrades most significantly once subnetworks corresponding to different semantic regions begin to overlap or interfere, rather than as direct consequence of parameter removal alone. At very high sparsity, mask similarity approaches zero, reflecting near-disjoint or unstable subnetworks and resulting in poor reconstruction quality. Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Figure 5. Per-image relationship between reconstruction quality and pruning mask similarity in the INR experiment. For each ADE20K image (AJ), PSNR (orange, left axis) and average mask similarity measured by the Jaccard index (blue dashed, right axis) are shown as function of sparsity. These per-image trends mirror the averaged results reported in the main text and further support the interpretation that maintaining distinct, region-specific pruning masks is critical for preserving reconstruction performance in high-sparsity INR settings. C.8. INR subnetwork similarity vs. reconstruction quality We further analyze the relationship between subnetwork specialization and reconstruction quality in the INR setting by examining how per-region PSNR correlates with mask similarity. Fig. 6 reports results for single ADE20K image containing four semantic regions. For each region-specific subnetwork, we plot PSNR together with the average Jaccard similarity of its pruning mask to all other region masks across sparsity levels. PSNR remains relatively stable while mask similarity is low, but degrades sharply once similarity increases, indicating the onset of subnetwork collapse. Fig. 7 extends this analysis to second ADE20K image with fifteen semantic regions. Despite the larger number of classes, the same trend holds across all regions (A-O): reconstruction quality deteriorates rapidly once subnetworks lose structural distinctiveness under aggressive pruning. To isolate correlation effects independent of absolute scale, Fig. 8 re-plots the 15-region case with both PSNR and mask similarity mean-centered (panels A-O). Across regions, fluctuations in reconstruction quality closely track changes in mask similarity, revealing an inverse specialization effect: regions that maintain higher PSNR tend to exhibit greater mask overlap, while regions with lower PSNR show increased structural specialization. This suggests that well-reconstructed regions rely Figure 6. Per-class PSNR and subnetwork similarity for single ADE20K image with four semantic regions. PSNR (solid) and average mask similarity to all other region-specific subnetworks (dashed) are shown as functions of sparsity. Performance degradation coincides with rapid increase in mask similarity, indicating subnetwork collapse at high pruning ratios. 16 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Figure 7. Per-class PSNR and subnetwork similarity for second ADE20K image with fifteen semantic regions. Each subplot (A-O) corresponds to one semantic region. As sparsity increases, PSNR declines gradually until sharp drop aligns with increasing similarity between region-specific pruning masks, reflecting loss of structural specialization. on more shared parameters, whereas harder regions benefit from stronger subnetwork differentiation. Panel summarizes this relationship at 50% sparsity by plotting PSNR against mask similarity for all regions and fitting linear regression. We observe strong monotonic relationship, with an overall Spearman rank correlation of ρ 0.982. This confirms that subnetwork similarity is strong predictor of reconstruction quality in INRs, even within single image. Together, these results reinforce the central claim of RTL: preserving subnetwork distinctiveness is critical not only across datasets or tasks, but also for fine-grained, within-image semantic specialization. Mask similarity thus provides reliable, label-free diagnostic for identifying oversparsification and impending performance collapse in coordinate-based representations. C.9. Qualitative INR Reconstructions under High Sparsity To complement the quantitative PSNR analysis, we provide qualitative visualizations of INR reconstructions under increasing sparsity levels. Figures 9, 10, and 11 show reconstructed images at 50%, 75%, and 90% sparsity, respectively. Figure 8. Correlation between reconstruction quality and subnetwork similarity in INRs. Panels AO show mean-centered PSNR and mean-centered mask similarity for the fifteen-region image. Panel reports linear regression between the two quantities at 50% sparsity across regions, revealing strong monotonic relationship (Spearman ρ 0.982). 17 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Each figure contains reconstructions for all 10 ADE20K images used in our experiments (rows). For each image, we display three columns: the ground-truth target image, the reconstruction produced by RTL, and the reconstruction produced by single-mask IMP baseline. All models are trained under identical architectural, optimization, and sparsity constraints. At 50% sparsity  (Fig. 9)  , both methods recover the overall scene structure, but RTL consistently preserves finer details and sharper object boundaries. Differences are particularly noticeable in textured regions and along semantic edges, where IMP reconstructions exhibit mild blurring and loss of contrast. At 75% sparsity  (Fig. 10)  , the qualitative gap widens. IMP reconstructions show clear degradation in high-frequency content, with washed-out colors and incomplete reconstruction of small structures. In contrast, RTL maintains more faithful geometry and color consistency across most images, indicating that region-specialized subnetworks better preserve semantically important parameters under aggressive pruning. At the extreme 90% sparsity regime  (Fig. 11)  , the difference becomes pronounced. IMP often collapses to coarse, low-detail approximations, with significant artifacts and loss of semantic coherence. RTL reconstructions, while degraded relative to lower sparsity levels, retain recognizable object shapes, clearer region boundaries, and more stable color distributions. This qualitative evidence aligns with the PSNR trends reported in the main paper, confirming that adaptive, region-specific pruning enables more graceful degradation as sparsity increases. Overall, these visual results demonstrate that RTL not only improves average reconstruction metrics but also yields perceptually superior outputs, especially in the high-sparsity regime where competition for shared parameters is most severe. 18 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Figure 9. Qualitative INR reconstructions at 50% sparsity. Rows correspond to 10 ADE20K images. Columns show the ground-truth target, RTL reconstruction, and IMP reconstruction. RTL preserves sharper edges and finer details compared to IMP, particularly in textured and semantically complex regions. 19 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Figure 10. Qualitative INR reconstructions at 75% sparsity. RTL maintains more faithful structure and color consistency, while IMP exhibits increased blurring and loss of high-frequency details. The gap between methods becomes more visible as sparsity increases. 20 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Figure 11. Qualitative INR reconstructions at 90% sparsity. Under extreme pruning, IMP reconstructions often collapse to coarse approximations with severe artifacts. RTL degrades more gracefully, preserving recognizable object shapes and semantic boundaries across images. 21 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data D. Speech Enhancement D.1. Dataset construction We use clean speech samples from the DNS Challenge 2020 dataset (Reddy et al., 2020) and mix them with environmental noise from the TAU Urban Acoustic Scenes 2020 dataset (Heittola et al., 2020). Noise samples are grouped into three acoustic scenes: indoor, outdoor, and transportation. Each scene defines distinct subset used to train specialized subnetwork. D.2. Model architecture For the speech enhancement task, we use lightweight U-Netstyle architecture (Ronneberger et al., 2015) operating on complex STFT representations. The network takes 2-channel input (real and imaginary components) and predicts 2-channel complex ratio mask. The model consists of five encoder and five decoder stages. Each encoder stage applies masked 2D convolution followed by ELU activation and batch normalization. Downsampling is performed only along the time axis using stride of (1, 2), preserving frequency resolution. The encoder channel dimensions are [12, 12, 12, 24, 48]. The decoder mirrors the encoder using masked transposed convolutions with symmetric kernel sizes and strides. Skip connections concatenate encoder features with decoder inputs, doubling the channel dimensionality prior to decoding. The final decoder layer outputs 2-channel complex mask. All convolutions use 3 3 kernels, except for the first and last layers, which use 3 5 kernels to capture wider temporal context. This compact design intentionally limits model capacity, allowing improvements from adaptive pruning to be isolated from architectural scaling effects. detailed layer-by-layer specification of the network is provided in Table 7 for reproducibility. Table 7. Speech enhancement U-Net architecture. All layers use ELU activation and batch normalization. Stride (1, 2) downsamples only along the time axis. Stage Enc-1 Enc-2 EncEnc-4 Enc-5 Dec-1 Dec-2 Dec-3 DecDec-5 Type Channels (in out) Kernel / Stride Conv2D Conv2D Conv2D Conv2D Conv2D TConv2D TConv2D TConv2D TConv2D TConv2D 2 12 12 12 12 12 12 24 48 48 24 48 12 24 12 24 12 24 3 5 / (1, 2) 3 3 / (1, 2) 3 3 / (1, 2) 3 3 / (1, 2) 3 3 / (1, 2) 3 3 / (1, 2) 3 3 / (1, 2) 3 3 / (1, 2) 3 3 / (1, 2) 3 5 / (1, 2) D.3. Signal processing and loss Input audio consists of 10-second, 16-bit waveforms. We compute STFTs using 1024-sample window and 256-sample hop size, producing spectrograms of shape [2, 626, 513]. The model predicts mask of identical shape, which is applied to the noisy spectrogram. Training uses the weighted source-to-distortion ratio (wSDR) loss. Unlike the vision experiments, this task does not involve negative samples; consequently, all methods are trained for the same number of epochs. D.4. Optimization and runtime We use the Adam optimizer with learning rate of 1e4 and no weight decay. Both pruning and retraining phases run for 10 epochs. All models share identical initial weights. Experiments are conducted on single NVIDIA H100 GPU. Full RTL 22 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Figure 12. Qualitative speech enhancement results at 50% sparsity. Each row corresponds to different acoustic environment, and columns show the noisy input, clean target, RTL output, and IMP output. RTL more effectively suppresses noise and preserves harmonic speech structure across all environments. and multi-model IMP runs take approximately 10 hours, while the single-model IMP baseline completes in about 8 hours. D.5. Qualitative Analysis of Speech Enhancement Fig. 12 provides qualitative comparison of speech enhancement results across three acoustic environments at 50% weight sparsity. Each row corresponds to distinct environment class, while columns show the noisy input spectrogram, the clean target, and the enhanced outputs produced by RTL and the IMP baseline. Across all environments, RTL more faithfully reconstructs the time-frequency structure of clean speech. Harmonic components are clearer, transient events are better preserved, and noise-dominated regions are more effectively suppressed. In contrast, the IMP baseline exhibits residual noise, smeared harmonics, and reduced contrast between speech and background, particularly in midand high-frequency bands. The qualitative gap becomes especially apparent in challenging conditions, where environment-specific noise patterns dominate the input. RTL subnetworks, specialized to each acoustic scene, recover speech structure with higher temporal coherence and sharper spectral detail, whereas the single-mask IMP model struggles to balance denoising across heterogeneous conditions. These visual results are consistent with the quantitative SI-SNRi improvements reported in Sec. 5, and further support the claim that adaptive, environment-aligned pruning yields more effective representations than single global sparse model. E. Subnetwork Similarity Analysis E.1. Mask similarity To analyze relationships between learned subnetworks, we compute pairwise similarity between binary pruning masks at multiple sparsity levels. Given two masks Mi and Mj, similarity is measured using the Jaccard coefficient: J(Mi, Mj) = Mi Mj Mi Mj , (8) where intersection and union are computed element-wise over all prunable parameters. Similarity is evaluated both globally and on per-layer basis to study how overlap varies across network depth. 23 Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data E.2. Collapse analysis For each subnetwork, we record balanced accuracy at multiple sparsity levels and compute its average pairwise mask similarity to other subnetworks. Correlating these quantities allows us to assess how excessive overlap (i.e., subnetwork collapse) relates to performance degradation. E.3. Semantic alignment To assess whether structural similarity reflects semantic similarity, we use CIFAR-10 as case study. Pairwise semantic distances between class labels are computed using WordNet path similarity (Miller, 1995). These distances are compared against corresponding mask similarity values, yielding aligned similarity matrices that reveal whether conceptually related classes share pruning structure."
        }
    ],
    "affiliations": [
        "Institute of Fundamental Technological Research, Polish Academy of Sciences, Poland",
        "Samsung AI Center Warsaw, Poland"
    ]
}