{
    "paper_title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
    "authors": [
        "Ao Li",
        "Wei Fang",
        "Hongbo Zhao",
        "Le Lu",
        "Ge Yang",
        "Minfeng Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation."
        },
        {
            "title": "Start",
            "content": "Published as conference paper at ICLR 2025 MRS: FAST SAMPLER FOR MEAN REVERTING DIFFUSION BASED ON ODE AND SDE SOLVERS Ao Li 1,2 Wei Fang 3,4 Hongbo Zhao 1,2 Le Lu 3 Ge Yang 1,2: Minfeng Xu 3,4: 1School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China 2Institute of Automation, Chinese Academy of Sciences (CASIA) 3DAMO Academy, Alibaba Group 4Hupan Laboratory, Hangzhou, China 5 2 0 2 3 1 ] . [ 2 6 5 8 7 0 . 2 0 5 2 : r {liao2022, zhaohongbo2022, ge.yang}@ia.ac.cn lucas.fw@alibaba-inc.com minfengxu@163.com tiger.lelu@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have emerged as powerful class of generative models, demonstrating remarkable capabilities across variety of applications, including image synthesis (Dhariwal & Nichol, 2021; Ruiz et al., 2023; Rombach et al., 2022) and video generation (Ho et al., 2022a;b). In these applications, controllable generation is very important in practice, but it also poses considerable challenges. Various methods have been proposed to incorporate text or image conditions into the score function of diffusion models(Ho & Salimans, 2022; Ye et al., 2023; Zhang et al., 2023), whereas Mean Reverting (MR) Diffusion offers new avenue of control in the generation process (Luo et al., 2023b). Previous diffusion models (such as DDPM (Ho et al., 2020)) simulate diffusion process that gradually transforms data into pure Gaussian noise, followed by learning to reverse this process for sample generation (Song & Ermon, 2020; Song et al., 2021). In contrast, MR Diffusion is designed to produce final states that follow Gaussian distribution with non-zero mean, which provides simple and natural way to introduce image conditions. This characteristic makes MR Diffusion particularly suitable for solving inverse problems and potentially extensible to multi-modal conditions. However, the sampling process of MR Diffusion requires hundreds of iterative steps, which is time-consuming. These authors contributed equally to this work. :Corresponding authors. 1 Published as conference paper at ICLR 2025 Figure 1: Qualitative comparisons between MR Sampler and Posterior Sampling. All images are generated by sampling from pre-trained MR Diffusion (Luo et al., 2024a) on the RESIDE-6k (Qin et al., 2020b) dataset and the CelebA-HQ (Karras, 2017) dataset. To improve the sampling efficiency of diffusion models, various acceleration strategies have been proposed, which can be divided into two categories. The first explores methods that establish direct mappings between starting and ending points on the sampling trajectory, enabling acceleration through knowledge distillation (Salimans & Ho, 2022; Song et al., 2023; Liu et al., 2022b). However, such algorithms often come with trade-offs, such as the need for extensive training and limitations in their adaptability across different tasks and datasets. The second category involves the design of fast numerical solvers that increase step sizes while controlling truncation errors, thus allowing for faster convergence to solutions (Lu et al., 2022a; Zhang & Chen, 2022; Song et al., 2020a). Notably, fast sampling solvers mentioned above are designed for common SDEs such as VPSDE and VESDE (Song et al., 2020b). Due to the difference between these SDEs and MRSDE, existing training-free fast samplers cannot be directly applied to Mean Reverting (MR) Diffusion. In this paper, we propose novel algorithm named MRS (MR Sampler) that improves the sampling efficiency of MR Diffusion. Specifically, we solve the reverse-time stochastic differential equation (SDE) and probability flow ordinary differential equation (PF-ODE) (Song et al., 2020b) derived from MRSDE, and obtain semi-analytical solution, which consists of an analytical function and an integral parameterized by neural networks. We prove that the difference of MRSDE only leads to change in analytical part of solution, which can be calculated precisely. And the integral part can be estimated by discretization methods developed in several previous works (Lu et al., 2022a; Zhang & Chen, 2022; Zhao et al., 2024). We derive sampling formulas for two types of neural network parameterizations: noise prediction (Ho et al., 2020; Song et al., 2020b) and data prediction (Salimans & Ho, 2022). Through theoretical analysis and experimental validation, we demonstrate that data prediction exhibits superior numerical stability compared to noise prediction. Additionally, we propose transformation methods for velocity prediction networks (Salimans & Ho, 2022) so that our algorithm supports all common training objectives. Extensive experiments show that our fast sampler converges in 5 or 10 NFEs with high sampling quality. As illustrated in Figure 1, our algorithm achieves stable performance with speedup factors ranging from 10 to 20. In summary, our main contributions are as follows: We propose MR Sampler, fast sampling algorithm for MR Diffusion, based on solving the PF-ODE and SDE derived from MRSDE. Our algorithm is plug-and-play and can adapt to all common training objectives. 2 Published as conference paper at ICLR 2025 We demonstrate that posterior sampling (Luo et al., 2024b) for MR Diffusion is equivalent to Euler-Maruyama discretization, whereas MR Sampler computes semi-analytical solution, thereby eliminating part of approximation errors. Through extensive experiments on ten image restoration tasks, we demonstrate that MR Sampler can reduce the required sampling time by factor of 10 to 20 with comparable sampling quality. Moreover, we reveal that data prediction exhibits superior numerical stability compared to noise prediction."
        },
        {
            "title": "2 BACKGROUND",
            "content": "In this section, we briefly review the basic definitions and characteristics of diffusion probabilistic models and mean-reverting diffusion models."
        },
        {
            "title": "2.1 DIFFUSION PROBABILISTIC MODELS",
            "content": "According to Song et al. (2020b), Diffusion Probabilistic Models (DPMs) can be defined as the solution of the following Itˆo stochastic differential equation (SDE), which is stochastic process txtutPr0,T with ą 0, called forward process, where xt RD is D-dimensional random variable. dx px, tqdt ` gptqdw. (1) The forward process performs adding noise to the data x0, while there exists corresponding reverse process that gradually removes the noise and recovers x0. Anderson (1982) shows that the reverse of the forward process is also solution of an Itˆo SDE: dx rf px, tq gptq2x log ptpxqsdt ` gptqd w, (2) where and are the drift and diffusion coefficients respectively, is standard Wiener process running backwards in time, and time flows from to 0, which means dt ă 0. The score function log ptpxq is generally intractable and thus neural network sθpx, tq is used to estimate it by optimizing the following objective (Song et al., 2020b; Hyvarinen & Dayan, 2005): θ arg min θ Et ! λptqEx0 Extx0 }sθpxt, tq xt log ppxtx0q}2 2 (3) ı) . where λptq : r0, Ñ R` is positive weighting function, is uniformly sampled over r0, s, x0 p0pxq and xt ppxtx0q. To facilitate the computation of ppxtx0q, the drift coefficient px, tq is typically defined as linear function of x, as presented in Eq.(4). Based on the inference by Sarkka & Solin (2019) in Section 5.5, the transition probability ppxtx0q corresponding to Eq.(4) follows Gaussian distribution, as shown in Eq.(5). dx ptqxdt ` gptqdw, ˆ ż ppxtx0q xt; x0e ş 0 pτ qdτ , ş τ pξqdξg2pτ qdτ e2 . (4) (5) Song et al. (2020b) proved that Denoising Diffusion Probabilistic Models (Ho et al., 2020) and Noise Conditional Score Networks (Song & Ermon, 2019) can be regarded as discretizations of Variance Preserving SDE (VPSDE) and Variance Exploding SDE (VESDE), respectively. As shown in Table 1, the SDEs corresponding to the two most commonly used diffusion models both follow the form of Eq.(4). 0 Table 1: Two popular SDEs, Variance Preserving SDE (VPSDE) and Variance Exploding SDE (VESDE). mptq and vptq refer to mean and variance of the transition probability ppxtx0q. SDE VPSDE(Ho et al., 2020) VESDE(Song & Ermon, 2019) ptq 1 2 βptq gptq βptq drσ2ptqs dt 3 mptq ş x0e 1 2 0 βpτ qdτ x0 vptq ş 0 βpτ qdτ σ2ptq σ2p0q Ie Published as conference paper at ICLR"
        },
        {
            "title": "2.2 MEAN REVERTING DIFFUSION MODELS",
            "content": "Luo et al. (2023b) proposed special case of Itˆo SDE named Mean Reverting SDE (MRSDE), as follows: dx ptq pµ xq dt ` gptqdw, (6) where µ is parameter vector that has the same shape of variable x, and ptq, gptq are timedependent non-negative parameters that control the speed of the mean reversion and stochastic volatility, respectively. To prevent potential confusion, we have substituted the notation used in the original paper (Luo et al., 2023b). For further details, please refer to Appendix B. Under the assumption that g2ptq{f ptq 2σ2 8 for any r0, with ą 0, Eq.(6) has closed-form solution, given by ş 0 pτ qdτ ` µp1 ş 0 pτ qdτ ` σ xt x0e (7) where σ8 is positive hyper-parameter that determines the standard deviation of xt when Ñ 8 and p0, Iq. Note that xt starts from x0, and converges to µ ` σ8z as Ñ 8. According to Anderson (1982)s result, we can derive the following reverse-time SDE: 1 e2 ş 0 pτ qdτ z, ptq pµ xq g2ptqx log ptpxq dx dt ` gptqd w. (8) Similar to DPMs, the score function in Eq.(8) can also be estimated by score matching methods Song & Ermon (2019); Song et al. (2021). Once the score function is known, we can generate x0 from noisy state xT . In summary, MRSDE illustrates the conversion between two distinct types of data and has demonstrated promising results in image restoration tasks (Luo et al., 2023c). Various algorithms have been developed to accelerate sampling of VPSDE, including methods like CCDF (Chung et al., 2022), DDIM (Song et al., 2020a), PNDM (Liu et al., 2022a), DPM-Solver (Lu et al., 2022a) and UniPC (Zhao et al., 2024). Additionally, Karras et al. (2022) and Zhou et al. (2024) have introduced techniques for accelerating sampling of VESDE. However, the drift coefficient of VPSDE and VESDE is linear function of x, while the drift coefficient in MRSDE is an affine function w.r.t. x, adding an intercept µ (see Eq.(4) and Eq.(6)). Therefore, current sampling acceleration algorithms cannot be applied to MR Diffusion. To the best of our knowledge, MR Sampler has been the first sampling acceleration algorithm for MR Diffusion so far."
        },
        {
            "title": "PREDICTION",
            "content": "According to Song et al. (2020b), the states xt in the sampling procedure of diffusion models correspond to solutions of reverse-time SDE and PF-ODE. Therefore, we look for ways to accelerate sampling by studying these solutions. In this section, we solve the noise-prediction-based reversetime SDE and PF-ODE, and we numerically estimate the non-closed-form component of the solution, which serves to accelerate the sampling process of MR diffusion models. Next, we analyze the sampling method currently used by MR Diffusion and demonstrate that this method corresponds to variant of discretization for the reverse-time MRSDE. 3.1 SOLUTIONS TO MEAN REVERTING SDES WITH NOISE PREDICTION Ho et al. (2020) reported that score matching can be simplified to predicting noise, and Song et al. (2020b) revealed the connection between score function and noise prediction models, which is xt log ppxtx0q ϵθpxt, µ, tq σt , (9) ş 0 pτ qdτ is the standard deviation of the transition distribution ppxtx0q. where σt σ8 Because µ is independent of and x, we substitute ϵθpxt, µ, tq with ϵθpxt, tq for notation simplicity. According to Eq.(9), we can rewrite Eq.(8) as 1 e2 ȷ dx ptq pµ xq ` ϵθpxt, tq dt ` gptqd w. (10) g2ptq σt Using Itˆos formula (in the differential form), we can obtain the following semi-analytical solution: 4 Published as conference paper at ICLR 2025 Proposition 1. Given an initial value xs at time r0, s, the solution xt at time r0, ss of Eq.(10) is xt αt αs ˆ xs ` 1 ż µ ` αt g2pτ αt αs ϵθpxτ , τ ατ στ ż α2 α2 τ dτ ` g2pτ qdτ z, (11) where we denote αt : ş 0 pτ qdτ and p0, Iq. The proof is in Appendix A.1. However, the integral with respect to neural network output is still complicated. There have been several methods (Lu et al., 2022a; Zhang & Chen, 2022; Zhao et al., 2024) to estimate the integral numerically. We follow Lu et al. (2022b)s method and introduce the half log-SNR λt : logpαt{σtq. Since both ptq and gptq are deliberately designed to ensure that αt is monotonically decreasing over and σt is monotonically increasing over t. Thus, λt is strictly decreasing function of and there exists an inverse function tpλq. Then we can rewrite gpτ in Eq.(11) as g2pτ 2σ2 8f pτ 2f pτ qpσ2 τ ` σ2 8α2 τ 2σ τ pf pτ ` 2σ2 τ pf pτ ` 1 2σ2 τ dσ2 τ dτ 2σ2 τ dλτ dτ . pτ qσ2 8α2 τ σ2 τ (12) By substituting Eq.(12) into Eq.(11), we obtain ˆ λt ż xt xs ` 1 αt αs αt αs µ 2αt λs eλϵθpxλ, λqdλ ` σt pe2pλtλsq 1qz, (13) where xλ : xtpλτ q, ϵθpxλ, λq : ϵθpxtpλτ q, tpλτ qq. According to the methods of exponential integrators (Hochbruck & Ostermann, 2010; 2005), the pk 1q-th order Taylor expansion of ϵθpxλ, λq and integration-by-parts of the integral part in Eq.(13) yields ff eλϵθpxλ, λqdλ 2σt pxλs , λsq eh n0 m0 nÿ phqm m! ` Ophk`1q, (14) ż λt 2αt λs k1ÿ ϵpnq θ where : λt λs. We drop the discretization error term Ophk`1q and estimate the derivatives with backward difference method. We name this algorithm as MR Sampler-SDE-n-k, where means noise prediction and is the order. We present details in Algorithm 1 and 2. 3.2 SOLUTIONS TO MEAN REVERTING ODES WITH NOISE PREDICTION Song et al. (2020b) have illustrated that for any Itˆo SDE, there exists probability flow ODE, sharing the same marginal distribution ptpxq as reverse-time SDE. Therefore, the solutions of PF-ODEs are also helpful in acceleration of sampling. Specifically, the PF-ODE corresponding to Eq.(10) is dx dt ptq pµ xq ` g2ptq 2σt ϵθpxt, tq. (15) The aforementioned equation exhibits semi-linear structure with respect to x, thus permitting resolution through the method of variation of constants. We can draw the following conclusions: Proposition 2. Given an initial value xs at time r0, s, the solution xt at time r0, ss of Eq.(15) is ˆ xt xs ` 1 µ ` αt αt αs αt αs g2pτ 2ατ στ ϵθpxτ , τ qdτ, (16) ż where αt : ş 0 pτ qdτ . The proof is in Appendix A.1. 1 xs ` αt αs αt αs pxλ, λq : dnϵθpxλ,λq Then we follow the variable substitution and Eq.(12-14) in Section 3.1, and we obtain ϵpnq θ pxλs, λsq µ σt eh xt k1ÿ ff nÿ ˆ phqm m! n0 m0 ` Ophk`1q, (17) θ where ϵpnq is the n-th order total derivatives of ϵθ with respect to λ. By dropping the discretization error term Ophk`1q and estimating the derivatives of ϵθpxλs, λsq with backward difference method, we design the sampling algorithm from the perspective of ODE (see Algorithm 3 and 4). dλn 5 Published as conference paper at ICLR"
        },
        {
            "title": "3.3 POSTERIOR SAMPLING FOR MEAN REVERTING DIFFUSION MODELS",
            "content": "In order to improve the sampling process of Mean Reverting Diffusion, Luo et al. (2024b) proposed the posterior sampling algorithm. They define monotonically increasing time series ttiuT i0 and the reverse process as Markov chain: ppx1:T x0q ppxT x0q Tź i2 ppxi1 xi, x0q and xT p0, Iq, (18) where we denote xi : xti for simplicity. They obtain an optimal posterior distribution by minimizing the negative log-likelihood, which is Gaussian distribution given by ppxi1 xi, x0q pxi1 µipxi, x0q, βiIq, µipxi, x0q βi pxi µq ` p1 α2 p1 α p1 α2 i1qαi qαi1 i1qp1 α2 1 α2 α2 i1 , 1 α2 α2 1 α2 i1 αi1px0 µq ` µ, (19) ş 0 pτ qdτ and x0 pxi µ σiϵθpxi, µ, tiqq {αi ` µ. Actually, the reparamewhere αi terization of posterior distribution in Eq.(19) is equivalent to variant of the Euler-Maruyama discretization of the reverse-time SDE (see details in Appendix A.2). Specifically, the Euler-Maruyama method computes the solution in the following form: ż ȷ ż xt xs ` pτ pµ xτ ` ϵθpxτ , τ dτ ` gpτ qd wτ , (20) g2pτ στ which introduces approximation errors from both the analytical term and the non-linear component associated with neural network predictions. In contrast, our approach delivers an exact solution for the analytical part, leading to reduced approximation errors and higher order of convergence."
        },
        {
            "title": "PREDICTION",
            "content": "Unfortunately, the sampler based on noise prediction can exhibit substantial instability, particularly with small NFEs, and may perform even worse than posterior sampling. It is well recognized that the Taylor expansion has limited convergence domain, primarily influenced by the derivatives of the neural networks. In fact, higher-order derivatives often result in smaller convergence radii. During the training phase, the noise prediction neural network is designed to fit normally distributed Gaussian noise. When the standard deviation of this Gaussian noise is set to 1, the values of samples can fall outside the range of r1, 1s with probability of 34.74%. This discrepancy results in numerical instability in the output of the neural network, causing its derivatives to exhibit more pronounced fluctuations (refer to the experimental results in Section 5 for further details). Consequently, the numerical instability leads to very narrow convergence domains, or in extreme cases, no convergence at all, which ultimately yields awful sampling results. Lu et al. (2022b) have identified that the choice of parameterization for either ODEs or SDEs is critical for the boundedness of the convergent solution. In contrast to noise prediction, the data prediction model (Salimans & Ho, 2022) focuses on fitting x0, ensuring that its output remains strictly confined within the bounds of r1, 1s, thereby achieving high numerical stability. 4.1 SOLUTIONS TO MEAN REVERTING SDES WITH DATA PREDICTION According to Eq.(7), we can parameterize x0 as follows: ϵθpxt, tq xt αtxθpxt, tq p1 αtqµ σt . (21) 6 Published as conference paper at ICLR 2025 By substituting Eq.(21) into Eq.(10), we derive the following SDE that incorporates data prediction: ȷ dx ptq ` ptq p1 αtq µ g2ptq σ2 g2ptq σ2 αtxθpxt, tq ` gptqd w. (22) ˆ g2ptq σ2 This equation remains semi-linear with respect to and thus we can employ Itˆos formula (in the differential form) to obtain the solution to Eq.(22). Proposition 3. Given an initial value xs at time r0, s, the solution xt at time r0, ss of Eq.(22) is ˆ 1 αt αs xt σt σs epλtλsqxs ` µ ż λt `2αt λs e2pλtλsq αt ` αte2pλtλsq (23) e2pλtλqxθpxλ, λqdλ ` σt 1 e2pλtλsqz, where p0, Iq. The proof is in Appendix A.1. Then we apply Taylor expansion and integration-by-parts to estimate the integral part in Eq.(23) and obtain the stochastic sampling algorithm for data prediction (see details in Algorithm 5 and 6). 4.2 SOLUTIONS TO MEAN REVERTING ODES WITH DATA PREDICTION By substituting Eq.(21) into Eq.(15), we can obtain the following ODE parameterized by data prediction. ȷ ptq ` ptq p1 αtq µ g2ptq 2σ2 g2ptq 2σ2 αtxθpxt, tq. (24) ˆ dx dt g2ptq 2σ2 The incorporation of the parameter µ does not disrupt the semi-linear structure of the equation with respect to x, and µ is not coupled to the neural network. This implies that analytical part of solutions can still be derived concerning both and µ. We present the solution below (see Appendix A.1 for detailed derivation). Proposition 4. Given an initial value xs at time r0, s, the solution xt at time r0, ss of Eq.(24) is ˆ ż xt σt σs xs ` µ 1 αs αt ` σt σt σs ` σt σs λt eλxθpxλ, λqdλ. (25) λs Similarly, only the neural network component requires approximation through the exponential integrator method (Hochbruck & Ostermann, 2005; 2010). And we can obtain the deterministic sampling algorithm for data prediction (see Algorithm 7 and 8 for details). 4.3 TRANSFORMATION BETWEEN THREE KINDS OF PARAMETERIZATIONS There are three mainstream parameterization methods. Ho et al. (2020) introduced training objective based on noise prediction, while Salimans & Ho (2022) proposed parameterization strategies for data and velocity prediction to keep network outputs stable under the variation of time or log-SNR. All three methods can be regarded as score matching approaches (Song et al., 2020b; Hyvarinen & Dayan, 2005) with weighted coefficients. To ensure our proposed algorithm is compatible with these parameterization strategies, it is necessary to provide transformation formulas for each pairs among the three strategies. The transformation formula between noise prediction and data prediction can be easily derived from Eq.(7): # xθptq xtp1αtqµσtϵθptq ϵθptq xtαtxθptqp1αtqµ αt , . (26) σt For velocity prediction, we define ϕt : arctanp σt q, which is slightly different from the defσ8αt inition of Salimans & Ho (2022). Then we have αt cos ϕt, σt σ8 sin ϕt and hence xt x0 cos ϕt ` µp1 cos ϕtq ` σ8 sin pϕtqϵ. And the definition of vptq is vt dxt dϕt µ sin ϕt x0 sin ϕt ` σ8 cospϕtqϵ. (27) Published as conference paper at ICLR 2025 If we have score function model vθptq trained with velocity prediction, we can obtain xθptq and ϵθptq by (see Appendix A.3 for detailed derivations) xθptq xt cos ϕt ` µp1 cos ϕtq vθptq sin ϕt, ϵθptq pvθptq cos ϕt ` xt sin ϕt µ sin ϕtq{σ8. (28) (29)"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we conduct extensive experiments to show that MR Sampler can significantly speed up the sampling of existing MR Diffusion. To rigorously validate the effectiveness of our method, we follow the settings and checkpoints from Luo et al. (2024a) and only modify the sampling part. Our experiment is divided into three parts. Section 5.1 compares the sampling results for different NFE cases. Section 5.2 studies the effects of different parameter settings on our algorithm, including network parameterizations and solver types. In Section 5.3, we visualize the sampling trajectories to show the speedup achieved by MR Sampler and analyze why noise prediction gets obviously worse when NFE is less than 20. 5.1 MAIN RESULTS Following Luo et al. (2024a), we conduct experiments with ten different types of image degradation: blurry, hazy, JPEG-compression, low-light, noisy, raindrop, rainy, shadowed, snowy, and inpainting (see Appendix D.1 for details). We adopt LPIPS (Zhang et al., 2018) and FID (Heusel et al., 2017) as main metrics for perceptual evaluation, and also report PSNR and SSIM (Wang et al., 2004) for reference. We compare MR Sampler with other sampling methods, including posterior sampling (Luo et al., 2024b) and Euler-Maruyama discretization (Kloeden et al., 1992). We take two tasks as examples and the metrics are shown in Figure 2. Unless explicitly mentioned, we always use MR Sampler based on SDE solver, with data prediction and uniform λ. The complete experimental results can be found in Appendix D.3. The results demonstrate that MR Sampler converges in few (5 or 10) steps and produces samples with stable quality. Our algorithm significantly reduces the time cost without compromising sampling performance, which is of great practical value for MR Diffusion. 5.2 EFFECTS OF PARAMETER CHOICE In Table 2, we compare the results of two network parameterizations. The data prediction shows stable performance across different NFEs. The noise prediction performs similarly to data prediction with large NFEs, but its performance deteriorates significantly with smaller NFEs. The detailed analysis can be found in Section 5.3. In Table 3, we compare MR Sampler-ODE-d-2 and MR Sampler-SDE-d-2 on the inpainting task, which are derived from PF-ODE and reverse-time SDE respectively. SDE-based solver works better with large NFE, whereas ODE-based solver is more effective with small NFE. In general, neither solver type is inherently better. Table 2: Ablation study of network parameterizations on the Rain100H dataset. Table 3: Ablation study of solver types on the CelebA-HQ dataset. NFE Parameterization LPIPS FID PSNR SSIM NFE Solver Type LPIPS FID PSNR SSIM 50 20 10 5 Noise Prediction Data Prediction Noise Prediction Data Prediction Noise Prediction Data Prediction Noise Prediction Data Prediction 0.0606 0.0620 0.1429 0.0635 1.376 0. 1.416 0.0637 27.28 27.65 47.31 27.79 402.3 29.54 447.0 26.92 28.89 28. 27.68 28.60 6.623 28.09 5.755 28.82 0.8615 0.8602 0.7954 0.8559 0.0114 0. 0.0051 0.8685 50 20 10 5 ODE SDE ODE SDE ODE SDE ODE SDE 0.0499 0.0402 0.0475 0.0408 0.0417 0. 0.0526 0.0529 22.91 19.09 21.35 19.13 19.44 19.29 27.44 24.02 28.49 29. 28.51 28.98 28.94 28.48 31.02 28.35 0.8921 0.9046 0.8940 0.9032 0.9048 0. 0.9335 0.8930 5.3 ANALYSIS Sampling trajectory. Inspired by the design idea of NCSN (Song & Ermon, 2019), we provide new perspective of diffusion sampling process. Song & Ermon (2019) consider each data point 8 Published as conference paper at ICLR 2025 (a) FID on low-light dataset (b) LPIPS on low-light dataset (c) FID on motion-blurry dataset (d) LPIPS on motion-blurry dataset Figure 2: Perceptual evaluations on low-light and motion-blurry datasets. (a) Sampling results. (b) Trajectory. Figure 3: Sampling trajectories. In (a), we compare our method (with order 1 and order 2) and previous sampling methods (i.e., posterior sampling and Euler discretization) on motion blurry image. The numbers in parentheses indicate the NFE. In (b), we illustrate trajectories of each sampling method. Previous methods need to take many unnecessary paths to converge. With few NFEs, they fail to reach the ground truth (i.e., the location of x0). Our methods follow more direct trajectory. (e.g., an image) as point in high-dimensional space. During the diffusion process, noise is added to each point x0, causing it to spread throughout the space, while the score function (a neural network) remembers the direction towards x0. In the sampling process, we start from random point by sampling Gaussian distribution and follow the guidance of the reverse-time SDE (or PF-ODE) and the score function to locate x0. By connecting each intermediate state xt, we obtain sampling trajectory. However, this trajectory exists in high-dimensional space, making it difficult to visualize. Therefore, we use Principal Component Analysis (PCA) to reduce xt to two dimensions, obtaining the projection of the sampling trajectory in 2D space. As shown in Figure 3, we present an example. Previous sampling methods (Luo et al., 2024b) often require long path to find x0, 9 Published as conference paper at ICLR 2025 and reducing NFE can lead to cumulative errors, making it impossible to locate x0. In contrast, our algorithm produces more direct trajectories, allowing us to find x0 with fewer NFEs. (a) Sampling results. (b) Ratio of convergence. Figure 4: Convergence of noise prediction and data prediction. In (a), we choose low-light image for example. The numbers in parentheses indicate the NFE. In (b), we illustrate the ratio of components of neural network output that satisfy the Taylor expansion convergence requirement. Numerical stability of parameterizations. From Table 1, we observe poor sampling results for noise prediction in the case of few NFEs. The reason may be that the neural network parameterized by noise prediction is numerically unstable. Recall that we used Taylor expansion in Eq.(14), and the condition for the equality to hold is λ λs ă Rpsq. And the radius of convergence Rptq can be calculated by 1 Rptq lim nÑ8 ˇ ˇ ˇ ˇ cn`1ptq cnptq ˇ ˇ ˇ ˇ , (30) where cnptq is the coefficient of the n-th term in Taylor expansion. We are unable to compute this limit and can only compute the 0 case as an approximation. The output of the neural network can be viewed as vector, with each component corresponding to radius of convergence. At each time step, we count the ratio of components that satisfy Ripsq ą λ λs as criterion for judging the convergence, where denotes the i-th component. As shown in Figure 4, the neural network parameterized by data prediction meets the convergence criteria at almost every step. However, the neural network parameterized by noise prediction always has components that cannot converge, which will lead to large errors and failed sampling. Therefore, data prediction has better numerical stability and is more recommended choice."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We have developed the fast sampling algorithm of MR Diffusion. Compared with DPMs, MR Diffusion is different in SDE and thus not adaptable to existing training-free fast samplers. We propose MR Sampler for acceleration of sampling of MR Diffusion. We solve the reverse-time SDE and PF-ODE derived from MRSDE and find semi-analytical solution. We adopt the methods of exponential integrators to estimate the non-linear integral part. Abundant experiments demonstrate that our algorithm achieves small errors and fast convergence. Additionally, we visualize sampling trajectories and explain why the parameterization of noise prediction does not perform well in the case of small NFEs. Limitations and broader impact. Despite the effectiveness of MR Sampler, our method is still inferior to distillation methods (Song et al., 2023; Luo et al., 2023a) within less than 5 NFEs. Additionally, our method can only accelerate sampling, but cannot improve the upper limit of sampling quality. 10 Published as conference paper at ICLR"
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "Our codes are based on the official code of MR Diffusion (Luo et al., 2023b) and DPM-Solver (Lu et al., 2022b). And we use the checkpoints and datasets provided by MR Diffusion (Luo et al., 2023b). We will release them after the blind review."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This work was supported in part by the National Natural Science Foundation of China (grant 92354307), the National Key Research and Development Program of China (grant 2024YFF0729202), the Strategic Priority Research Program of the Chinese Academy of Sciences (grant XDA0460305), and the Fundamental Research Funds for the Central Universities (grant E3E45201X2). This work was also supported by Alibaba Group through Alibaba Research Intern Program. 11 Published as conference paper at ICLR"
        },
        {
            "title": "REFERENCES",
            "content": "Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: dataset and study. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 126135, 2017. Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313326, 1982. Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1241312422, 2022. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633 8646, 2022b. Marlis Hochbruck and Alexander Ostermann. Explicit exponential rungekutta methods for semilinear parabolic problems. SIAM Journal on Numerical Analysis, 43(3):10691090, 2005. Marlis Hochbruck and Alexander Ostermann. Exponential integrators. Acta Numerica, 19:209286, 2010. Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005. Tero Karras. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Peter Kloeden, Eckhard Platen, Peter Kloeden, and Eckhard Platen. Stochastic differential equations. Springer, 1992. Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022a. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022b. Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, and Jenq-Neng Hwang. Desnownet: Context-aware IEEE Transactions on Image Processing, 27(6):30643073, deep network for snow removal. 2018. 12 Published as conference paper at ICLR 2025 Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. In Proceedings of the Repaint: IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1146111471, 2022. Inpainting using denoising diffusion probabilistic models. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023a. Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas Schon. Image restoration with mean-reverting stochastic differential equations. arXiv preprint arXiv:2301.11699, 2023b. Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas Schon. Refusion: Enabling large-size realistic image restoration with latent-space diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16801691, 2023c. Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas Schon. Controlling vision-language models for multi-task image restoration. In The Twelfth International Conference on Learning Representations, 2024a. Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas Schon. Photo-realistic In Proceedings of the image restoration in the wild with controlled vision-language models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66416651, 2024b. David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the 18th IEEE International Conference on Computer Vision (ICCV), volume 2, pp. 416423. IEEE, 2001. Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 38833891, 2017. Rui Qian, Robby Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative adversarial network for raindrop removal from single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 24822491, 2018. Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu Jia. FFA-Net: Feature fusion attention network for single image dehazing. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1190811915, 2020a. Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu Jia. Ffa-net: Feature fusion attention network for single image dehazing. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 1190811915, 2020b. Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson WH Lau. Deshadownet: multi-context embedding deep network for shadow removal. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 40674075, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023. Published as conference paper at ICLR 2025 Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Simo Sarkka and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019. Sheikh. Live image quality assessment database release 2. http://live. ece. utexas. edu/research/quality, 2005. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:1243812448, 2020. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415 1428, 2021. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, and Lei Zhang. NTIRE 2017 In Proceedings of the IEEE challenge on single image super-resolution: methods and results. Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 114125, 2017. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018. Wenhan Yang, Robby Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep joint rain detection and removal from single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 13571366, 2017. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Zhenyu Zhou, Defang Chen, Can Wang, and Chun Chen. Fast ode-based sampling for diffusion models in around 5 steps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 77777786, 2024. 14 Published as conference paper at ICLR"
        },
        {
            "title": "APPENDIX",
            "content": "We include several appendices with derivations, additional details and results. In Appendix A, we provide derivations of propositions in Section 3 and 4, equivalence between posterior sampling and Euler-Maruyama discretization, and velocity prediction, respectively. In Appendix B, we compare the notations used in this paper and MRSDE (Luo et al., 2023b). In Appendix C, we list detailed algorithms of MR Sampler with various orders and parameterizations. In Appendix D, we present details about datasets, settings and results in experiments. In Appendix E, we provide an in-depth discussion on determining the optimal NFE."
        },
        {
            "title": "A DERIVATION DETAILS",
            "content": "A.1 PROOFS OF PROPOSITIONS Proposition 1. Given an initial value xs at time r0, s, the solution xt at time r0, ss of Eq.(10) is xt where αt : ż g2pτ αt αs xs ` p1 αt αs ş 0 pτ qdτ and p0, Iq. qµ ` αt s ϵθpxτ , τ ατ στ dτ ` ż α2 α2 τ g2pτ qdτ z, (31) Proof. For SDEs in the form of Eq.(1), Itˆos formula gives the following conclusion: dψpx, tq Bψpx, tq Bt dt ` Bψpx, tq Bx rf px, tqdt ` gptqdws ` 1 B2ψpx, tq Bx2 g2ptqdt, (32) where ψpx, tq is differentiable function. And we define ψpx, tq xe ş 0 pτ qdτ By substituting px, tq and gptq with the corresponding drift and diffusion coefficients in Eq.(10), we obtain dψpx, tq µf ptqe ş 0 pτ qdτ dt ` ş 0 pτ qdτ g2ptq σt ȷ ϵθpxt, tqdt ` gptqd . And we integrate both sides of the above equation from to t: ψpx, tqψpx, sq µpe ş 0 pτ qdτ ş 0 pτ qdτ q` τ ş 0 pξqdξg2pτ ϵθpxτ , τ στ dτ ` ż τ ş 0 pξqdξgpτ qd w. Note that is standard Wiener process running backwards in time and we have the quadratic variation pd wq2 dτ . According to the definition of ψpx, tq and αt, we have ˆ ż xt αt xs αs µ 1 α 1 α ` g2pτ ϵθpxτ , τ ατ στ dτ ` g2pτ α2 τ dτ z, ż ż which is equivalent to Eq.(31). Proposition 2. Given an initial value xs at time r0, s, the solution xt at time r0, ss of Eq.(15) is ż qµ ` αt g2pτ 2ατ στ ϵθpxτ , τ qdτ, (33) xt where αt : ş 0 pτ qdτ . αt αs xs ` p1 αt αs Proof. For ODEs which have semi-linear structure as follows: dx dt ptqx ` Qpx, tq, (34) the method of variation of constants gives the following solution: ş xptq t 0 pτ qdτ Qpx, τ qe τ ş 0 prqdrdτ ` ȷ . ż 0 15 Published as conference paper at ICLR"
        },
        {
            "title": "By simultaneously considering the following two equations",
            "content": "$ & xptq % xpsq ş 0 pτ qdτ ş 0 pτ qdτ ş 0 Qpx, τ qe ş 0 Qpx, τ qe ı τ ş 0 prqdrdτ ` ş 0 prqdrdτ ` τ , ı , and eliminating C, we obtain xptq xpsqe ş pτ qdτ ` ż ş Qpx, τ qe τ pξqdξdτ. (35) Now we compare Eq.(15) with Eq.(34) and let ptq ptq and Qpx, tq ptqµ ` g2ptq 2σt ϵθpxt, tq. Therefore, we can rewrite Eq.(35) as ş ş xt xse xse pτ qdτ ` ż ş τ pξqdξ pτ qµ ` ȷ ϵθpxτ , τ dτ pτ qdτ ` µp1 ş pτ qdτ ` τ pξqdξ g2pτ 2στ ϵθpxτ , τ qdτ, g2pτ 2στ ş ż s which is equivalent to Eq.(33). Proposition 3. Given an initial value xs at time r0, s, the solution xt at time r0, ss of Eq.(22) is xt σt σs epλtλsqxs ` µ ż λt `2αt λs ˆ 1 αt αs e2pλtλsq αt ` αte2pλtλsq (36) e2pλtλqxθpxλ, λqdλ ` σt 1 e2pλtλsqz, where p0, Iq. Proof. According to Eq.(32), we define uptq g2ptq σ2 and ψpx, tq xe ptq ş 0 upτ qdτ . We substitute px, tq and gptq in Eq.(32) with the corresponding drift and diffusion coefficients in Eq.(22), and integrate both sides of the equation from to t: ş upτ qdτ ` µ xt xse ż s ş τ upξqdξ g2pτ σ2 τ ż ş τ upξqdξ pτ ȷ ż ατ xθpxτ , τ dτ ` ȷ p1 ατ g2pτ σ2 τ ş τ upξqdξgpτ qd w. dτ t (37) We can rewrite gpτ as Eq.(12) and obtain ş upτ qdτ exp ˆ ż s 2 dλτ dτ pτ dτ αt αs e2pλtλsq σt σs epλtλsq. (38) Published as conference paper at ICLR 2025 Next, we consider each term in Eq.(37) by employing Eq.(12) and Eq.(38). Firstly, we simplify the second term: ş ż ż µ µ τ upξqdξ g2pτ σ2 τ pτ ȷ p1 ατ dτ pτ ` 2p1 ατ ȷ dτ dλτ dτ σt στ epλtλτ ż µσteλt µσteλt µσteλt ż ż eλτ στ ατ σ2 τ dατ σ2 τ rf pτ qdτ ` 2p1 ατ qdλτ rf pτ qdτ ` 2dλτ 2ατ dλτ ` 2ατ σ2 τ dλτ α2 τ σ2 τ dλτ . (39) (40)"
        },
        {
            "title": "Note that",
            "content": "αt 1 α2 Substitute Eq.(40) into Eq.(39) and we obtain dλt σ8 log ż µ ş τ upξqdξ pτ dαt αt ` αtdαt 1 α2 dαt αtp1 α2 . ȷ p1 ατ dτ g2pτ σ2 τ µσteλt µσteλt ż s ż ż µσteλt ˆ αt αs dλτ ` α2 τ σ2 τ 2 dατ 2dατ σ2 τ p1 α2 σ2 τ τ 1 ` α2 τ τ q2 dατ 2e2λτ dλτ 8p1 α2 σ2 ˆ ατ 1 2e2λτ dλ 1 α2 σ2 τ 8 µ 1 e2pλtλsq αt ` αte2pλtλsq . (41) Secondly, we rewrite the third term in Eq.(37) by employing Eq.(12) and Eq.(38). ȷ ȷ ż ş τ upξqdξ g2pτ σ2 τ ατ xθpxτ , τ dτ ż ż σt στ 2 2αt λt λs epλtλτ 2 ατ xθpxτ , τ dτ dλτ dτ σte2λτ λtxθpxτ , λτ qdλτ ż e2pλtλqxθpxλ, λqdλ. (42) Thirdly, we consider the fourth term in Eq.(37) (note that pd wq2 dτ ): ż ş τ upξqdξgpτ qd e ż ż σ2 s ż ş τ upξqdξg2pτ qdτ ˆ e2pλtλτ 2σ2 τ dλτ dτ dτ σ2 σ2 τ 2e2pλτ λtqdλτ Lastly, we substitute Eq.(38) and Eq.(41-43) into Eq.(37) and obtain the solution as presented in Eq.(36). σt 1 e2pλtλsqz. (43) 17 Published as conference paper at ICLR 2025 Proposition 4. Given an initial value xs at time r0, s, the solution xt at time r0, ss of Eq.(24) is ˆ ż xt xs ` µ 1 ` αs αt ` σt λs Proof. Note that Eq.(24) shares the same structure as Eq.(34). Let σt σs σt σs σt σs λt eλxθpxλ, λqdλ. (44) ptq g2ptq 2σ2 ptq, and Qpx, tq ptq ȷ p1 αtq µ g2ptq 2σ2 g2ptq 2σ2 αtxθpxt, tq. According to Eq.(12), we first consider ş pτ qdτ exp ż ż g2pτ 2σ2 τ ȷ pτ dτ exp ż ż dλτ ` log ατ exp log ατ log exp log στ σt σs . (45) Then, we can rewrite Eq.(35) as σt σs ż σt στ g2pτ 2σ2 τ xt xs ` µ pτ p1 ατ dτ ż σt στ g2pτ 2σ2 τ ατ xθpxτ , τ qdτ. (46) Firstly, we consider the second term in Eq.(46) ż ȷ ατ στ ȷ µ σt στ ż pτ g2pτ 2σ2 τ µσt ż µσt 1 στ 1 στ ż µσt µσt ˆ ż s µ 1 pτ ˆ pτ ˆ log στ στ 1 στ p1 ατ dτ g2pτ 2σ2 τ ` g2pτ 2σ2 τ g2pτ 2σ2 τ ż ατ dλτ στ ȷ ż dτ ` ȷ deλτ σt σs ` σt σs αs αt . ȷ ατ ż dτ ȷ g2pτ 2σ3 τ ατ dτ (refer to Eq.(12) and Eq.(45) Secondly, we rewrite the third term in Eq.(46) ż σt στ g2pτ 2σ2 τ ατ xθpxτ , τ qdτ σt ż eλτ xθpxλ, λqdλτ . (47) (48) By substituting Eq.(47) and Eq.(48) into Eq.(46), we can obtain the solution shown in Eq.(44). A.2 EQUIVALENCE BETWEEN POSTERIOR SAMPLING AND EULER-MARUYAMA DISCRETIZATION The posterior sampling (Luo et al., 2024b) algorithm utilizes the reparameterization of Gaussian distribution in Eq.(19) and computes xi1 from xi iteratively as follows: xi1 µipxi, x0q ` βizi, 1 α2 α2 1 α2 i1 αi1px0 µq ` µ, (49) µipxi, x0q βi pxi µq ` p1 α2 p1 α2 p1 α2 i1qαi qαi1 i1qp1 α2 1 α2 α2 i1 , Published as conference paper at ICLR 2025 where zi p0, Iq, αi tuting x0 into µi, we arrange the equation and obtain 0 pτ qdτ and x0 pxi µ σiϵθpxi, µ, tiqq {αi ` µ. By substiş µipxi, x0q αi1 αi αi1 αi xi ` p1 xi ` p1 αi1 αi αi1 αi qµ qµ αi1 αi αi1 αi αi αi1 1 α2 αi αi1 1 α2 σi ϵθpxi, µ, tiq σ8 ϵθpxi, µ, tiq. (50)"
        },
        {
            "title": "We note that",
            "content": "i ş i1 pτ qdτ 1 ` αi1 αi ż i1 ˆż pτ qdτ ` pτ qdτ 1 ` ptiqti, (51) i1 where the high-order error term is omitted and ti : ti ti1. By substituting Eq.(51) into Eq.(50) and Eq.(49), we obtain µipxi, x0q p1 ` ptiqtiqxi ptiqtiµ 2f ptiqtiσ8 1 α2 ϵθpxi, µ, tiq, (52) βi p1 α2 i1qp1 α2 1 α2 α2 i1 2f ptiqtip1 α i1q 1 α2 . (53) On the other hand, the reverse-time SDE has been presented in Eq.(10). Combining the assumption g2ptq{f ptq 2σ2 8 in Section 2.2 and the definition of σt in Section 3.1, the EulerMaruyama descretization of this SDE is xi1 xi ptiqpµ xiqti g2ptiq σi ϵθpxi, µ, tiqti ` gptiq tizi, 6 xi1 p1 ` ptiqtiqxi ptiqtiµ p1 ` ptiqtiqxi ptiqtiµ µipxi, x0q ` σ8 1 α2 1 α2 i1 βizi. 2σ2 8f ptiq 1 α2 σ8 2f ptiqtiσ8 1 α2 a ϵθpxi, µ, tiqti ` gptiq tizi ϵθpxi, µ, tiq ` σ8 2f ptiqtizi (54) Thus, the posterior sampling algorithm is special EulerMaruyama descretization of reverse-time SDE with different coefficient of Gaussian noise. A.3 DERIVATIONS ABOUT VELOCITY PREDICTION Following Eq.(27), We can define the velocity prediction as And we have the relationship between xθptq and ϵθptq as follows: vθptq µ sin ϕt xθptq sin ϕt ` σ8 cospϕtqϵθptq. xt xθptq cos ϕt ` µp1 cos ϕtq ` σ8 sin pϕtqϵθptq. In order to get xθ from vθ, we rewrite Eq.(55) as xθptq sin2 ϕt µ sin2 ϕt vθptq sin ϕt ` σ8ϵθptq sin ϕt cos ϕt. Then we replace ϵθptq according to Eq.(56) (55) (56) (57) xθptq sin2 ϕt µ sin2 ϕt vθptq sin ϕt ` rxt xθptq cos ϕt µp1 cos ϕtqs cos ϕt p1 cos ϕtqµ vθptq sin ϕt ` xt cos ϕt xθptq cos2 ϕt. (58) Arranging the above equation, we can obtain the transformation from vθ to xθ, as shown in Eq.(28). Similarly, we can also rewrite Eq.(55) and replace xθptq as follows: σ8 cos2pϕtqϵθptq vθptq cos ϕt µ sin ϕt cos ϕt ` xθptq sin ϕt cos ϕt vθptq cos ϕt µ sin ϕt cos ϕt ` sin ϕt rxt µp1 cos ϕtq σ8 sin pϕtqϵθptqs vθptq cos ϕt µ sin ϕt ` xθptq sin ϕt σ8 sin2 ϕtϵθptq. (59) Thus we obtain the transformation from vθ to ϵθ, as presented in Eq.(29). 19 Published as conference paper at ICLR"
        },
        {
            "title": "This paper",
            "content": "f ptq gptq MRSDE (Luo et al., 2023b) θt σt αt ş 0 θτ dτ σt σ8 1 ş 0 θτ dτ Table 4: The correspondence between the notations used in this paper (left column) and notations used by MRSDE (right column)."
        },
        {
            "title": "C DETAILED SAMPLING ALGORITHM OF MR SAMPLER",
            "content": "We list the detailed MR Sampler algorithm with different solvers, parameterizations and orders as follows. Algorithm 1 MR Sampler-SDE-n-1. Require: initial value xT µ ` σ8ϵ, Gaussian noise sequence tzizi p0, IquM ttiuM i0, data prediction model xθ. Denote hi : λti λti1 for 1, . . . , . i1, time steps 1: xt0 Ð xT . Initialize an empty buffer Q. 2: bufferÐÝÝÝ xθpxt0 , t0q 3: for Ð 1 to do 4: xti1 ` xti If ă , then bufferÐÝÝÝ xθpxti , tiq αti αti1 αti αti1 1 5: 6: end for 7: return xtM µ 2σtipehi 1qϵθpxti1 , ti1q ` σti ? e2hi 1zi Algorithm 2 MR Sampler-SDE-n-2. Require: initial value xT µ ` σ8ϵ, Gaussian noise sequence tzizi p0, IquM ttiuM i0, data prediction model xθ. Denote hi : λti λti1 for 1, . . . , . i1, time steps µ 2σt1 peh1 1qϵθpxt0 , t0q ` σt1 ? e2h1 1z1 µ 2σti pehi 1qϵθpxti1 , ti1q ` pehi 1 hiqDi ` 1: xt0 Ð xT . Initialize an empty buffer Q. 2: bufferÐÝÝÝ xθpxt0 , t0q αt1 3: xt1 αt0 4: bufferÐÝÝÝ xθpxt1, t1q 5: for Ð 2 to do 6: Di ϵθpxti1 ,ti1qϵθpxti2 ,ti2q hi1 xt0 ` αt1 αt0 1 7: 1 xti1 ` αti αti1 e2hi 1zi xti ? σti If ă , then bufferÐÝÝÝ xθpxti , tiq αti αti1 8: 9: end for 10: return xtM Published as conference paper at ICLR 2025 Algorithm 3 MR Sampler-ODE-n-1. Require: initial value xT µ ` σ8ϵ, time steps ttiuM i0, data prediction model xθ. Denote hi : λti λti1 for 1, . . . , . 1: xt0 Ð xT . Initialize an empty buffer Q. 2: bufferÐÝÝÝ xθpxt0 , t0q 3: for Ð 1 to do 4: xti xti1 ` If ă , then bufferÐÝÝÝ xθpxti , tiq αti αti αti αti1 1 5: 6: end for 7: return xtM µ σti pehi 1qϵθpxti1, ti1q Algorithm 4 MR Sampler-ODE-n-2. Require: initial value xT µ ` σ8ϵ, time steps ttiuM i0, data prediction model xθ. Denote µ σt1peh1 1qϵθpxt0, t0q hi : λti λti1 for 1, . . . , . 1: xt0 Ð xT . Initialize an empty buffer Q. 2: bufferÐÝÝÝ xθpxt0 , t0q αt1 3: xt1 αt0 4: bufferÐÝÝÝ xθpxt1, t1q 5: for Ð 2 to do 6: Di ϵθpxti1 ,ti1qϵθpxti2 ,ti2q hi1 xt0 ` αt1 αt0 1 7: αti αti1 xti xti1 ` If ă , then bufferÐÝÝÝ xθpxti , tiq 1 αti αti1 8: 9: end for 10: return xtM µ σti pehi 1qϵθpxti1 , ti1q ` pehi 1 hiqDi Algorithm 5 MR Sampler-SDE-d-1. Require: initial value xT µ ` σ8ϵ, Gaussian noise sequence tzizi p0, IquM ttiuM i0, data prediction model xθ. Denote hi : λti λti1 for 1, . . . , . i1, time steps e2hi αti ` αtie2hi ` σti ? 1 e2hizi ` 1: xt0 Ð xT . Initialize an empty buffer Q. 2: bufferÐÝÝÝ xθpxt0 , t0q 3: for Ð 1 to do 4: σti σti1 1 e2hi xti ` αti If ă , then bufferÐÝÝÝ xθpxti , tiq ehixti1 ` µ xθpxti1, ti1q 1 αti αti1 5: 6: end for 7: return xtM 21 Published as conference paper at ICLR 2025 Algorithm 6 MR Sampler-SDE-d-2. Require: initial value xT µ ` σ8ϵ, Gaussian noise sequence tzizi p0, IquM ttiuM i0, data prediction model xθ. Denote hi : λti λti1 for 1, . . . , . i1, time steps 1: xt0 Ð xT . Initialize an empty buffer Q. 2: bufferÐÝÝÝ xθpxt0 , t0q σt1 3: xt1 σt0 ? σt1 eh1xt0 ` µ 1 e2h1z1 αt1 αt0 1 e2h1 αt1 ` αt1e2h1 ` ` αt1 1 e2h xθpxt0, t0q ` 4: bufferÐÝÝÝ xθpxti, tiq 5: for Ð 2 to do 6: Di 7: xti ` σti σti1 1 e2hi 8: 9: end for 10: return xtM xθpxti1 ,ti1qxθpxti2 ,ti2q hi1 ehixti1 ` µ 1 αti αti1 e2hi αti ` αtie2hi hi 1e2hi 2 Di αti If ă , then bufferÐÝÝÝ xθpxti, tiq xθpxti1, ti1q ` αti ? ` σti 1 e2hizi ` Algorithm 7 MR Sampler-ODE-d-1. Require: initial value xT µ ` σ8ϵ, time steps ttiuM i0, data prediction model xθ. Denote hi : λti λti1 for 1, . . . , . 1: xt0 Ð xT . Initialize an empty buffer Q. 2: bufferÐÝÝÝ xθpxt0 , t0q 3: for Ð 1 to do 4: σti xti xti1 ` µ ` σti1 If ă , then bufferÐÝÝÝ xθpxti, tiq σti σti1 1 5: 6: end for 7: return xtM αti1 αti ` αti ` 1 ehi xθpxti1 , ti1q σti σti1 Algorithm 8 MR Sampler-ODE-d-2. Require: initial value xT µ ` σ8ϵ, time steps ttiuM i0, data prediction model xθ. Denote hi : λti λti1 for 1, . . . , . 1: xt0 Ð xT . Initialize an empty buffer Q. 2: bufferÐÝÝÝ xθpxt0 , t0q σt1 3: xt1 xt0 ` µ σt0 4: bufferÐÝÝÝ xθpxt1, t1q 5: for Ð 2 to do 6: σt1 σt0 σt1 σt 1 ` σti σti1 1 xti ` xti1 ` µ ` xθpxti1 ,ti1qxθpxti2 ,ti2q αti hi1 If ă , then bufferÐÝÝÝ xθpxti , tiq hi 1 ` ehi σti σti1 σti σti1 αti1 αti 7: 8: end for 9: return xtM αt0 αt1 ` ` αt1 1 eh1 xθpxt0, t0q ` ` αti 1 ehi xθpxti1 , ti1q ` 22 Published as conference paper at ICLR"
        },
        {
            "title": "D DETAILS ABOUT EXPERIMENTS",
            "content": "D.1 DETAILS ABOUT DATASETS We list details about the used datesets in 10 image restoration tasks in Table 5. Task name Blurry Hazy Dataset name GoPro RESIDE-6k Reference Nah et al. (2017) Qin et al. (2020a) JPEG-compressing DIV2K, Flickr2K and LIVE1 Agustsson & Timofte (2017),Timofte et al. (2017), Sheikh (2005) Low-light Noisy Raindrop Rainy Shadowed Snowy Inpainting LOL Wei et al. (2018) DIV2K, Flickr2K and CBSD68 Agustsson & Timofte (2017),Timofte et al. (2017),Martin et al. (2001) RainDrop Rain100H SRD Snow100K-L CelebaHQ Qian et al. (2018) (Yang et al., 2017) (Qu et al., 2017) (Liu et al., 2018) (Lugmayr et al., 2022) Number of testing images 1000 29 15 68 58 408 601 100 Table 5: Details about the used datasets in 10 image restoration tasks D.2 DETAILS ON THE NEURAL NETWORK ARCHITECTURE In this section, we describe the neural network architecture used in experiments. We follow the framework of Luo et al. (2024a), an image restoration model designed to address multiple degradation problems simultaneously without requiring prior knowledge of the degradation. The diffusion model in Luo et al. (2024a) is derived from Luo et al. (2023c), and its neural network architecture is based on NAFNet. NAFNet builds upon the U-Net architecture by replacing traditional activation functions with SimpleGate and incorporating an additional multi-layer perceptron to manage channel scaling and offset parameters for embedding temporal information into the attention and feedforward layers. For further details, please refer to Section 4.2 in Luo et al. (2023c). D.3 DETAILED METRICS ON ALL TASKS We list results on four metrics for ten image restoration tasks in Table 6-15. NFE Method LPIPS 50 20 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 0.0385 0.0426 0.0390 0.0397 0.4238 0.4449 0.0379 0.0402 0.7130 0.7257 0.0383 0. 0.8097 0.8154 0.0401 0.0437 0.8489 0.8525 0.0428 0.0529 FID 18.35 20.71 18.16 18.81 247.0 249.5 18.03 19.09 347.4 344.3 18.29 19. 374.1 381.1 18.46 19.29 385.6 384.7 20.00 24.02 PSNR SSIM NFE Method 29.49 29.34 29.45 29.30 12.85 12.68 29.68 29.15 10.19 10.16 30.05 28.98 9.802 9.786 30.61 28.48 9.599 9.587 31.03 28.35 0.9102 0.8981 0.9086 0. 0.6048 0.5800 0.9118 0.9046 0.2171 0.2073 0.9172 0.9032 0.1339 0.1305 0.9229 0.8996 0.1057 0.1042 0.9262 0.8930 100 20 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 LPIPS 0.0614 0.0683 0.0608 0.0626 0.2374 0.2730 0.0602 0.0628 0.6622 0.6861 0.0601 0. 0.8013 0.8164 0.0608 0.0698 0.8590 0.8680 0.0611 0.0628 FID 21.42 23.27 21.30 21.47 72.04 76.02 20.91 21.85 123.8 126.0 21.32 22. 138.5 140.4 22.26 23.92 145.8 145.8 23.29 21.95 PSNR SSIM 27.43 27.09 27.45 27.18 21.35 21.02 27.63 27. 16.42 16.23 28.07 26.89 14.76 14.64 28.50 26.49 13.92 13.85 28.89 27.06 0.8763 0.8577 0.8754 0.8691 0.7037 0.6676 0.8803 0.8685 0.3546 0.3431 0.8903 0. 0.2694 0.2640 0.8992 0.8573 0.2318 0.2290 0.9065 0.8718 Table 6: Image inpainting. Table 7: Snowy image restoration. 23 Published as conference paper at ICLR NFE Method LPIPS 100 50 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 0.0970 0.1129 0.0984 0.0989 0.6119 0.6985 0.0978 0.1006 1.043 1.065 0.0954 0.1014 1.122 1.133 0.0956 0.1044 1.155 1.161 0.0964 0. FID 20.14 20.30 20.03 20.69 101.8 114.2 20.90 20.74 187.2 192.9 19.79 20.93 208.4 209.7 19.87 21.85 218.9 221.4 20.16 36. PSNR SSIM NFE Method LPIPS 27.84 27.59 27.73 27. 18.29 17.80 27.92 27.20 14.73 14.56 28.31 27.17 13.67 13.57 28.67 26.99 13.12 13.06 28.90 23.73 0.8391 0.8112 0.8370 0.8329 0.4720 0.4123 0.8409 0. 0.2049 0.1955 0.8505 0.8299 0.1525 0.1484 0.8554 0.8276 0.1298 0.1278 0.8601 0.6690 100 50 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 0.0594 0.0725 0.0594 0.0616 0.4418 0.4560 0.0586 0.0620 0.6865 0.6943 0.0604 0.0635 0.7972 0.8043 0.0659 0.0678 0.8663 0.8714 0.0729 0. FID 25.58 28.80 30.53 27.33 183.1 185.2 30.73 27.65 293.6 299.0 31.19 27.79 323.0 330.8 31.66 29.54 332.4 332.5 32.06 26. PSNR SSIM 29.14 28.77 29.15 28.92 16.41 16.24 29.34 28.85 12.54 12.49 29.81 28.60 11.50 11.46 30.28 28. 10.96 10.94 30.68 28.82 0.8704 0.8473 0.8679 0.8614 0.4903 0.4729 0.8730 0.8602 0.2464 0.2402 0.8845 0.8559 0.1755 0.1724 0.8943 0.8483 0.1450 0.1435 0.9029 0. Table 8: Shadowed image restoration. Table 9: Rainy image restoration. NFE Method LPIPS 50 20 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 0.0443 0.0634 0.0437 0.0454 0.4289 0.5011 0.0428 0.0459 0.8873 0.9082 0.0439 0. 0.9884 0.9993 0.0466 0.0485 1.030 1.037 0.0497 0.0733 FID 19.70 24.19 19.94 21.35 100.1 111.6 19.14 20.50 190.8 194.1 19.31 21. 215.5 213.1 20.60 22.17 226.3 226.4 21.18 28.26 PSNR SSIM NFE Method LPIPS 30.05 29.31 29.91 29.55 23.19 22.35 30.07 29.40 17.37 17.10 30.44 29.34 15.67 15.52 30.77 29.37 14.82 14.74 31.04 28. 0.8910 0.8438 0.8852 0.8768 0.4663 0.4106 0.8914 0.8764 0.1925 0.1839 0.9025 0.8745 0.1419 0.1381 0.9114 0.8779 0.1209 0.1190 0.9175 0.8369 50 20 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 0.1694 0.2719 0.1629 0.1586 0.7713 0.8060 0.1680 0.1615 0.9941 1.006 0.1872 0. 1.057 1.063 0.2043 0.1853 1.087 1.091 0.2046 0.3178 FID 65.79 68.69 59.12 64.02 135.7 143.5 65.14 64.24 181.6 188.3 71.31 64. 202.6 207.0 79.28 70.19 213.5 218.6 80.45 73.93 PSNR SSIM 25.97 24.28 25.97 25.67 18.19 17.74 26.20 25. 14.76 14.61 26.68 25.46 13.59 13.51 27.13 25.09 13.00 12.96 27.51 24.32 0.7267 0.5686 0.7244 0.7126 0.2763 0.2615 0.7330 0.7127 0.1821 0.1781 0.7494 0. 0.1550 0.1529 0.7628 0.6984 0.1419 0.1408 0.7743 0.5485 Table 10: Raindrop image restoration. Table 11: Noisy image restoration. 24 Published as conference paper at ICLR NFE Method LPIPS 100 50 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 0.0796 0.1014 0.0774 0.0789 0.6572 0.7517 0.0784 0.0786 1.288 1.297 0.0791 0.0792 1.351 1.354 0.0831 0.0841 1.371 1.372 0.0860 0. FID 34.70 37.46 32.91 32.91 151.3 176.7 34.45 32.85 390.1 396.8 35.28 32.80 432.2 424.2 41.10 36.53 453.0 447.2 41.81 33. PSNR SSIM NFE Method LPIPS 23.84 23.27 23.80 23. 9.490 9.402 23.51 23.52 8.211 8.212 24.22 23.63 8.130 8.136 24.04 23.22 8.114 8.118 24.02 24.13 0.8496 0.8027 0.8451 0.8394 0.2746 0.2340 0.8476 0. 0.0648 0.0625 0.8586 0.8399 0.0476 0.0467 0.8619 0.8398 0.0408 0.0405 0.8676 0.8507 100 50 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 0.1702 0.2949 0.1636 0.1555 0.6494 0.7035 0.1734 0.1567 0.8252 0.8477 0.1993 0.1675 0.8723 0.8859 0.2183 0.1871 0.8941 0.9013 0.2281 0. FID 45.77 56.99 43.67 44.58 103.7 113.7 47.09 45.56 140.6 144.5 51.43 47.36 158.9 159.9 57.83 51.25 163.6 162.0 59.17 62. PSNR SSIM 25.78 23.84 25.81 25.46 19.34 18.62 26.06 25.41 16.44 16.19 26.58 25.33 15.49 15.34 26.98 25. 14.99 14.91 27.28 23.15 0.7380 0.5398 0.7362 0.7220 0.2908 0.2659 0.7470 0.7224 0.1988 0.1921 0.7649 0.7235 0.1738 0.1701 0.7771 0.7197 0.1615 0.1596 0.7853 0. Table 12: Low-light image restoration. Table 13: JPEG image restoration. NFE Method LPIPS 50 20 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 0.0211 0.0358 0.0219 0.0230 0.3994 0.4745 0.0211 0.0233 0.9911 1.012 0.0200 0. 1.116 1.128 0.0197 0.0246 1.162 1.168 0.0205 0.0228 FID 4.755 6.182 4.826 4.978 35.47 42.70 4.737 4.993 114.0 118.5 4.682 5. 144.0 147.3 4.785 5.228 159.1 161.2 4.926 5.174 PSNR SSIM NFE Method LPIPS 30.37 29.95 30.42 30.26 15.34 15.16 30.49 30.19 12.81 12.71 30.63 30.06 11.94 11.87 30.80 29.65 11.47 11.43 30.56 29. 0.9485 0.9319 0.9462 0.9431 0.5808 0.5160 0.9484 0.9427 0.1832 0.1752 0.9534 0.9409 0.1261 0.1228 0.9579 0.9372 0.1042 0.1026 0.9604 0.9416 50 20 10 5 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR SamplerPosterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 Posterior Euler MR Sampler-1 MR Sampler-2 0.1249 0.1404 0.1239 0.1248 0.5112 0.5739 0.244 0.1251 1.069 1.089 0.1287 0. 1.187 1.197 0.1356 0.1300 1.228 1.234 0.1422 0.1248 FID 14.48 16.06 14.61 14.61 45.13 61.21 14.47 14.62 117.8 120.8 14.92 14. 141.9 143.7 15.65 15.51 155.3 157.3 16.32 14.20 PSNR SSIM 27.48 27.16 27.46 27.30 24.41 23.79 27.59 27. 18.20 17.94 27.85 27.13 16.11 15.96 28.10 26.88 15.08 15.00 28.31 26.92 0.8442 0.8179 0.8419 0.8365 0.5571 0.4957 0.8461 0.8360 0.1717 0.1637 0.8544 0. 0.1136 0.1104 0.8613 0.8295 0.0922 0.0907 0.8668 0.8354 Table 14: Hazy image restoration. Table 15: Motion-blurry image restoration. D.4 DETAILS ON NUMERICAL STABILITY AT LOW NFES We have included further details regarding numerical stability at 5 NFEs to complement the experiments presented in Section 5.3. As illustrated in Figure 5, when the NFE is relatively low, the convergence rate of noise prediction at each step does not exceed 40%, which results in sampling collapse. In contrast, during the final 1 or 2 steps, the convergence rate of data prediction approaches nearly 100%. D.5 WALL CLOCK TIME We measured the wall clock time in our experiments on single NVIDIA A800 GPU. The average wall clock time per image of two representative tasks (low-light and motion-blurry image restoration) are reported in Table 16 and 17. D.6 PRESENTATION OF SAMPLING RESULTS We present the sampling results for all image restoration tasks in Figure 6 and 7. We choose one image for each task. 25 Published as conference paper at ICLR (a) Sampling results. (b) Ratio of convergence. Figure 5: Convergence of noise prediction and data prediction at 5 NFEs. In (a), we choose stained image for example. The numbers in parentheses indicate the NFE. In (b), we illustrate the ratio of components of neural network output that satisfy the Taylor expansion convergence requirement. Table 16: Wall clock time on the Low-light dataset. Table 17: Wall clock time on the Motionblurry dataset."
        },
        {
            "title": "NFE",
            "content": "100 50 20"
        },
        {
            "title": "Method",
            "content": "Time(s) Posterior Sampling MR Sampler-2 Posterior Sampling MR Sampler-2 Posterior Sampling MR Sampler-2 Posterior Sampling MR Sampler-2 17.19 17. 8.605 8.439 3.445 3.285 1.727 1.569 Posterior Sampling MR Sampler-2 0.8696 0."
        },
        {
            "title": "NFE",
            "content": "100 50 20"
        },
        {
            "title": "Method",
            "content": "Time(s) Posterior Sampling MR Sampler-2 Posterior Sampling MR Sampler-2 Posterior Sampling MR Sampler-2 Posterior Sampling MR Sampler-2 Posterior Sampling MR Sampler82.04 81.16 41.23 40.18 16.44 15.59 8.212 7.413 4.133 3."
        },
        {
            "title": "E SELECTION OF THE OPTIMAL NFE",
            "content": "In practice, sampling quality is expected to degrade as the NFE decreases, which requires us to make trade-off between sampling quality and efficiency. From the perspective of reverse-time SDE and PF-ODE, the estimation error of the solution plays critical role in determining the sampling quality. This estimation error is primarily influenced by hmax max1ďiďM tλi λi1u Op 1 q, where represents the NFE. During sampling, the noise schedule must be designed in advance, which also determines the λ schedule. Since λi is monotonically increasing, larger NFE results in smaller hmax, thereby reducing the estimation error and improving sampling quality. However, different sampling algorithms exhibit varying convergence rates. Experimental results show that the MR Sampler (our method) can achieve good score with as few as 5 NFEs, whereas posterior sampling and Euler discretization fail to converge even with 50 NFEs. Specifically, we conducted experiments on the snowy dataset with low NFE settings, and the results are presented in Table 18. The sampling quality remains largely stable for NFE values larger than 10, gradually deteriorates when the NFE is below 10, and collapses entirely when NFE is reduced to 2. Based on our experience, we recommend using 1020 NFEs, which provides reasonable trade-off between efficiency and performance. 26 Published as conference paper at ICLR 2025 Figure 6: Comparisons between posterior sampling and MR Sampler-SDE-d-2 on snowy, shadowed, rainy, raindrop, noisy and low-light datasets. Table 18: Results of MR Sampler-SDE-2 with data prediction and uniform λ on the snowy dataset. NFE 100 50 20 10 6 5 4 2 LPIPS FID PSNR SSIM 0.0626 21.47 27.18 0. 0.0628 21.85 27.08 0.8685 0.0650 22.34 26.89 0.8645 0.0698 23.92 26.49 0.8573 0.0725 24.81 26.61 0.8462 0.0744 25.60 26.40 0.8407 0.0628 21.95 27.06 0. 0.1063 30.18 25.38 0.7640 1.422 421.1 6.753 0.0311 27 Published as conference paper at ICLR 2025 Figure 7: Comparisons between posterior sampling and textitMR Sampler-SDE-d-2 on JPEGcompressed, hazy, inpainting and motion-blurry datasets."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Laboratory, Hangzhou, China",
        "Institute of Automation, Chinese Academy of Sciences (CASIA)",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China"
    ]
}