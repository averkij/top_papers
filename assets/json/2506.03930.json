{
    "paper_title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation",
    "authors": [
        "Yuansheng Ni",
        "Ping Nie",
        "Kai Zou",
        "Xiang Yue",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 3 9 3 0 . 6 0 5 2 : r VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation Yuansheng Ni1, Ping Nie4, Kai Zou3, Xiang Yue2, Wenhu Chen1, 1University of Waterloo, 2Carnegie Mellon University, 3Netmind.ai, 4Independent Researcher, {yuansheng.ni, wenhuchen}@uwaterloo.ca https://tiger-ai-lab.github.io/VisCoder"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack executiongrounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt selfdebug evaluation protocol to assess iterative repair, demonstrating the benefits of feedbackdriven learning for executable, visually accurate code generation."
        },
        {
            "title": "Introduction",
            "content": "Despite the growing capabilities of large language models (LLMs) in general-purpose code generation (Chen et al., 2021; Guo et al., 2024), they continue to struggle with one of the most common and visually essential tasks in data analysis: generating code that produces valid and semantically meaningful plot. For example, given tabular description, models may generate code that appears syntactically correct and invokes the appropriate libraries (Dibia, 2023; Xie et al., 2024). But when executed, the result is often broken: exceptions are raised, plots render blank or malformed, or the visual fails to reflect the intended semantics of the instruction (Chen et al., 2024; Yang et al., 2024; Galimzyanov et al., 2024). These failures are not incidental: they reflect structural challenges in visualization code generation. Unlike standard text-to-code tasks, visualization requires grounding across three modalities: natural language (the user instruction), data structure (the tabular or other data input), and visual output (the rendered chart). Execution correctness is not binary; script may run and still fail to convey the intended meaning. Visualization libraries such as matplotlib (Hunter, 2007), seaborn (Waskom, 2021), and plotly (Inc, 2015) further complicate the task, with API idiosyncrasies and intricate bindings between data, layout, and style. Current instruction-tuning datasets do not meet the demands of this setting. Most lack explicit visual grounding, do not enforce runtime validation, and provide little to no supervision for recovery from failure. As result, even advanced open models like Qwen-Coder (Team, 2024) struggle with executable, semantically accurate visualization code, particularly when debugging is required (Zheng et al., 2024). To address these gaps, we introduce VisCode200K, new instruction-tuning dataset for Pythonbased visualization code generation and multi-turn correction. VisCode-200K contains over 200K supervised examples derived from two complementary data sources: 1) Executable visualization code, extracted from open-source Python repositories and filtered across widely-used plotting libraries, including matplotlib, seaborn and others. All code samples are validated for runtime executability and paired with rendered plots. Natural language instructions are generated using LLMs conditioned on both the code and its output image (Galimzyanov et al., 2024). 2) Multi-turn revision dialogues, drawn from the Code-Feedback dataset (Zheng et al., 2024), which contains realistic interactions where models revise faulty Python code based on runtime errors and follow-up prompts. While not visualization-specific, these traces provide essential supervision for teaching models to debug and recover from execution failures. This dual-source construction enables training for both single-shot generation and multi-round refinement, allowing models to generate code initially and improve it iteratively through feedback. To demonstrate the effectiveness of VisCode200K, we fine-tune Qwen2.5-Coder-Instruct (Hui et al., 2024) at both 3B and 7B scales to produce VisCoder, an open-source model tuned specifically for Python visualization tasks. We evaluate VisCoder on PandasPlotBench (Galimzyanov et al., 2024), benchmark that assesses executable code generation from natural language and data previews across three plotting libraries. We also introduce self-debug evaluation mode, in which models are given multiple rounds to revise failed outputs based on execution traces, simulating realistic developer-style correction loop. Our experiments show that VisCoder substantially outperforms competitive open-source baselines. VisCoder-3B and 7B achieve average execution pass rate improvements of 19.6 and 14.5 points over Qwen2.5-Coder. Under self-debug mode, it reaches over 90% execution pass rate on Matplotlib and Seaborn. Compared to proprietary models, VisCoder-7B surpasses GPT-4o-mini on both Seaborn and Plotly under the default setting, and approaches GPT-4o performance on both libraries after self-debugging. At 3B scale, it outperforms GPT-4o-mini on Seaborn and narrows the gap in other libraries. These results demonstrate the impact of combining domain-specific instruction tuning with feedback-driven correction for grounded visualization code generation."
        },
        {
            "title": "2 Related Work",
            "content": "LLMs for Visualization Code Generation. Recent work has explored using large language models to generate visualization code from natural language prompts. Benchmarks such as MatPlotAgent and VisEval (Yang et al., 2024; Chen et al., 2024) evaluate model performance on structured NL2VIS tasks with paired chart specifications and data previews, while PandasPlotBench (Galimzyanov et al., 2024) provides curated benchmark for assessing executable visualization code generation across multiple plotting libraries. Plot2Code (Wu et al., 2024) investigates the reverse direction by generating code from rendered plots, but it relies on image-level inputs and bypasses the textual reasoning central to real-world data workflows. These studies highlight persistent challenges in semantic grounding, API correctness, and robustness across different plotting tasks. Broader evaluations have analyzed model behavior across visualization types and libraries (Vázquez, 2024; Podo et al., 2024), while specification-based approaches using VegaLite (Xie et al., 2024) offer an alternate formulation that lacks direct executability. Beyond evaluation, systems like LIDA (Dibia, 2023) and VisPath (Seo et al., 2025) incorporate summarization, code synthesis, and feedback-driven refinement into end-toend pipelines. Related efforts have also extended visual code generation to structured domains such as parametric CAD modeling (Li et al., 2025) and mathematical animation (Ku et al., 2025), where outputs reflect domain-specific constraints rather than general-purpose charting semantics. However, most prior work lacks training data grounded in execution outcomes and provides limited support for iterative refinement. These limitations hinder model reliability, especially when generating code that must be both syntactically correct and semantically faithful to the intended visualization. Execution Feedback and Code Correction. Execution feedback has been widely explored as supervisory signal for improving the reliability of code generation. Prior work investigates using runtime traces to guide post-hoc refinement (Jain et al., 2025; Chen et al., 2025; Tian et al., 2024; Zhang and Yang, 2025), or integrates such signals into training through reinforcement learning (Gehring et al., 2024; Zeng et al., 2025). Other approaches emphasize multi-turn correction, where models revise faulty code using internal or external feedback (Madaan et al., 2023; Jiang et al., 2024; Zheng et al., 2024; Ruiz et al., 2025), or simulate debugging workflows with planning and agent collaboration (Grishina et al., 2025; Li et al., 2024). In the context of visualization, VisPath (Seo et al., 2025) and MatPlotAgent (Yang et al., 2024) explore chart refinement using visual feedback from rendered outputs. Yet despite these advances, supervision grounded in execution feedback or revision traces has rarely been used to train models for visualization code generation, where runtime validity and semantic alignment remain central challenges. Figure 1: Data construction pipeline for VisCode-200K. We extract and filter visualization code blocks from opensource Python sources, validate their executability and plot rendering via Jupyter-based runtime checks, and generate structured instructions paired with rendered plots. We integrate multi-turn correction data from Code-Feedback during instruction construction to support iterative refinement."
        },
        {
            "title": "Instruction Tuning Dataset",
            "content": "In this section, we present VisCode-200K, supervised instruction tuning dataset for Python-based visualization and feedback-driven code correction. It is designed to support robust code generation across diverse plotting libraries and to enable iterative refinement through multi-turn supervision. VisCode-200K integrates two complementary sources of supervision. The first consists of executable visualization code extracted from opensource Python repositories, covering wide range of real-world chart types, layouts, and plotting libraries. All samples are filtered to ensure runtime validity and compatibility with standard Python environments, exposing models to diverse and realistic plotting practices. The second source comprises multi-turn Python dialogues from the CodeFeedback dataset (Zheng et al., 2024), which offer supervision for revising faulty code in response to execution errors. While not specific to visualization, these interactions are critical for modeling realistic correction behaviors in iterative workflows. Figure 1 provides an overview of the VisCode200K construction pipeline, which consists of code filtering, runtime validation, and structured instruction generation. The following subsections detail each component."
        },
        {
            "title": "3.1 Code Extraction from Public Repositories",
            "content": "To build large corpus of executable Python visualization code, we source data from two open the Python subset of stack-edu1 (Aldatasets: lal et al., 2025) and the chart/table partitions of CoSyn-400K2 (Yang et al., 2025; Deitke et al., 2024). From these corpora, we extract code that uses commonly adopted visualization libraries, in1hf.co/datasets/HuggingFaceTB/stack-edu 2hf.co/datasets/allenai/CoSyn-400K cluding matplotlib, seaborn and others, to ensure broad coverage of real-world plotting styles. The construction pipeline consists of four stages: library-based filtering, code block extraction, runtime validation, and instruction generation. Filtering and Code Block Extraction. For the stack-edu source, which contains large collection of Python code examples from educational contexts, we begin by applying library-based filters to identify approximately 1.7M samples that invoke common Python visualization libs. Since most examples embed visualization logic within broader program contexts, we use GPT-4o-mini (OpenAI, 2024a) to extract minimal, standalone plotting blocks. During this process, we inject mock data to replace missing inputs and ensure that each block can be executed in isolation. This structural cleaning step yields code samples that reflect realistic plotting usage while remaining compatible with our runtime pipeline. After filtering and reconstruction, we obtain roughly 1M candidate blocks. To balance library distribution, we retain all seaborn and ohter samples and randomly subsample matching number of matplotlib examples, resulting in curated subset of 300K visualization blocks. From CoSyn-400K, we extract 112K Python code snippets that include calls to one of the target visualization libraries. CoSyn provides highquality synthetic plotting code spanning wide range of styles, with well-rendered outputs and consistent structure. Unlike stack-edu, it stores code and data separately, which requires reconstruction to enable runtime execution. We synthesize runnable scripts by inserting inline annotations such as column headers and the first data row to emulate realistic pandas.read_csv loading. When necessary, we append missing plotting function calls to ensure that each script can execute fully within notebook environment. Runtime Validation. To verify executability, we run each code block in an isolated Jupyter environment using nbconvert with allow-error=False. We enforce timeout and terminate executions that hang or enter infinite loops using simulated keyboard interrupt. Only samples that run successfully and generate valid image file are retained. This step yields 105K validated plotting scripts from stack-edu and 50K from CoSyn-400K, each paired with its corresponding output image. Instruction Generation. To construct meaningful instructions for visualization code generation, we use GPT-4o (OpenAI, 2024b) to synthesize instruction components based on each validated code block and its corresponding plot. This enables the model to incorporate both structural code features and visual semantics from the rendered image. Each instruction consists of five components: (1) brief setup description specifying the programming language and visualization libraries used; (2) data description summarizing the tabular input and column semantics; (3) data block indicating the input table, either as mock data (for stack-edu) or two-row preview (for CoSyn); (4) high-level plot description outlining axes and structural layout; and (5) style description capturing colors, grid layout, and other visual properties. For stack-edu samples, mock data is extracted directly from the code block, where it was inserted during preprocessing. For CoSyn, where data is stored separately, we construct compact preview using the first two rows of the table. The five components are then assembled using fixed template to form the final instruction: [Plot Description] [Setup] [Data Description] \"The mock data shows below:\" or \"The first two rows of the data are shown below:\" [Data] [Plot Style Description] This format enforces consistent prompt structure across both data sources, providing models with clear description of the target plot as well as the data and style required to render it."
        },
        {
            "title": "Dialogues with Execution Feedback",
            "content": "To train models with self-correction capabilities, we incorporate 45K multi-turn dialogues from the Code-Feedback3 dataset (Zheng et al., 2024). 3hf.co/datasets/m-a-p/Code-Feedback These dialogues involve Python-based tasks, including user instructions, model-generated code, and follow-up turns containing execution feedback or revision prompts. We begin with 56K Python dialogues and remove those with excessive length or turn count to maintain consistency and reduce training complexity. The resulting 45K samples span diverse Python tasks with realistic correction behaviors. While not specific to visualization, these dialogues offer valuable supervision for teaching models to revise faulty code based on runtime signals and to reason over iterative interactions. We integrate them into the instruction tuning corpus alongside the single-turn samples from stack-edu and CoSyn, enabling models to learn both initial generation and multi-turn refinement strategies."
        },
        {
            "title": "4 Experiment Setup",
            "content": "Training Setup. We fine-tune Qwen2.5-CoderInstruct (Hui et al., 2024) at two parameter scales: 3B and 7B. This allows us to assess the generalizability of VisCode-200K across different model capacities. Both models are trained for 3 epochs with learning rate of 5 106, warm-up ratio of 0.05, and cosine learning rate scheduler. We perform full-parameter tuning in bfloat16 precision on 8A100 GPUs with total batch size of 128. Evaluation Setup. We evaluate models using PandasPlotBench (Galimzyanov et al., 2024), benchmark designed to assess the ability of language models to generate executable and semantically accurate visualization code from tabular data descriptions. It contains 175 tasks spanning three widely used Python plotting libraries: matplotlib, seaborn, and plotly. Each task includes natural language instruction and preview of the input DataFrame. The model is expected to generate Python code that produces valid plot when executed according to the instruction. The benchmark reports three metrics: (1) Incorrect Code Rate, the proportion of outputs that fail to produce any plot; and two GPT-4o-judged scores: (2) task-based score measuring alignment with the instruction, and (3) visual score assessing similarity to the reference plot. Among these metrics, Incorrect Code Rate provides only coarse signal of success. It indicates whether plot is rendered, but does not capture execution errors if figure is produced. As result, blank or semantically meaningless outputssuch as plots with only axesmay be misclassified as correct. To address this issue, we introduce an additional metric: Execution Pass Rate, defined as the percentage of outputs that execute without error. Self-Debug Evaluation Mode. To evaluate models ability to recover from failure, we extend the benchmark with self-debug evaluation mode. In this setting, if the initial generation fails to execute or does not produce valid plot, the model is allowed up to rounds to iteratively revise its output based on accumulated feedback. At each round, only the tasks that remain unsolved from the previous attempt are reconsidered. The model receives multi-turn prompt constructed as dialogue, including the original instruction, its failed code response, and follow-up message requesting correction based on the execution error. Conditioned on this dialogue history, the model generates revised version of the code. Tasks are considered successfully fixed if the generated code executes without error and produces valid plot. These tasks are excluded from subsequent rounds. Algorithm 1 Self-Debug Evaluation Protocol 1: Let F0 be failed tasks from initial evaluation 2: for = 1 to do 3: for each task in Fi1 not yet fixed do Fix via feedback-driven prompting Evaluate the result of the revised code if successful then Mark as fixed & record output else Record xs latest failed output end if end for 11: 12: end for 13: Evaluate all tasks with final recorded outputs 4: 5: 6: 7: 8: 9: 10: We set = 3 for all experiments. After the final round of self-debug, each task is evaluated based on its recorded final output, which is either the successfully revised version from an earlier round or the last failed attempt if no fix was found. The resulting outputs are scored using the same evaluation pipeline as in the default setting. The full procedure is summarized in Algorithm 1. This iterative process simulates developer-style debugging loop and enables systematic evaluation of the models ability to recover from failure through multi-round code correction."
        },
        {
            "title": "5 Main Results",
            "content": "We present the main experimental results on PandasPlotBench, including overall model comparisons, performance under the self-debug evaluation protocol, error type analysis, and training data ablation study."
        },
        {
            "title": "5.1 Overall Model Comparison",
            "content": "We evaluate VisCoder models against both proprietary and open-source language models to assess executable visualization performance across scales and libraries. The proprietary group includes GPT-4o (OpenAI, 2024b), the strongest model in the original PandasPlotBench benchmark, and its lightweight variant GPT-4o-mini (OpenAI, 2024a). Among open-source baselines, we compare LLaMA-3.2-3B, LLaMA-3.1-8B (Grattafiori et al., 2024), Qwen2.5-Instruct, and Qwen2.5Coder-Instruct (Team, 2024; Hui et al., 2024), evaluated at both 3B and 7B scales. VisCoder models are trained on VisCode-200K and fine-tuned using the same instruction tuning setup. Table 1 summarizes model performance across the three plotting libraries. The following analysis focuses on execution success, task alignment, and visual fidelity, highlighting VisCoders comparative strengths and remaining challenges. Proprietary Models Remain Stronger. Proprietary models outperform open-source models by wide margin across all plotting libraries. GPT4o achieves the highest execution pass rates and the strongest judge-based scores, followed by its lightweight variant GPT-4o-mini. These results indicate more reliable execution and better semantic alignment with task instructions, particularly in complex visualization settings. In contrast, open-source models such as LLaMA and Qwen2.5Instruct consistently underperform across all metrics. This reinforces the gap between proprietary and open-source systems on execution-sensitive and semantically grounded code generation. Plotly Presents Harder Challenge. Performance differs across plotting libraries. While most models perform reliably on matplotlib and seaborn, results on plotly are markedly lower, especially for open-source models. Execution pass rates often fall below 35%, and task and visual scores drop accordingly. Generated plots frequently fail to reflect the intended semantics or produce complete visuals. This suggests that plotlys"
        },
        {
            "title": "Model",
            "content": "GPT-4o GPT-4o + Self Debug GPT-4o-mini GPT-4o-mini + Self Debug Llama-3.2-3B-Ins. Qwen-2.5-3B-Ins. Qwen-2.5-Coder-3B-Ins. VisCoder-3B VisCoder-3B + Self Debug Llama-3.1-8B-Ins. Qwen2.5-7B-Ins. Qwen2.5-Coder-7B-Ins. VisCoder-7B VisCoder-7B + Self Debug"
        },
        {
            "title": "Exec\nPass",
            "content": "94.9 99.4 88.6 97.7 65.1 74.3 71.4 81.7 85.1 81.1 77.1 78.3 87.4 91."
        },
        {
            "title": "Matplotlib\nMean",
            "content": "vis task Good(75) task vis"
        },
        {
            "title": "Mean",
            "content": "vis task Good(75) task vis"
        },
        {
            "title": "Mean",
            "content": "vis task Good(75) task vis 75 77 68 72 43 55 56 60 61 64 63 66 67 90 93 86 92 60 68 72 69 70 76 76 78 81 67% 93% 83.4 69% 96% 92.6 59% 86% 62.3 65% 94% 72.0 3B Scale 34% 55% 30.9 49% 66% 58.3 50% 69% 58.3 53% 69% 73.7 53% 69% 78.3 7B Scale 51% 74% 65.7 53% 75% 66.3 58% 75% 68.6 60% 80% 76.6 62% 83% 90.3 65 69 45 47 18 43 44 48 48 51 51 57 62 78 84 57 60 24 58 55 65 66 64 63 63 70 59% 80% 77.7 63% 86% 97.7 41% 57% 69.1 43% 61% 97.7 14% 21% 13.1 33% 51% 30.9 36% 51% 27.4 38% 61% 60.6 37% 62% 64.6 45% 63% 30.9 46% 62% 56.0 40% 62% 48.0 50% 68% 74.3 51% 75% 81.7 55 68 48 8 19 17 38 40 21 38 29 48 51 68 84 52 71 8 23 45 48 22 42 34 60 65 50% 70% 61% 83% 42% 51% 51% 67% 7% 8% 17% 21% 17% 18% 32% 44% 34% 47% 20% 21% 31% 40% 24% 31% 41% 61% 44% 65% Table 1: Performance of selected models on the PandasPlotBench benchmark. For each model, we report (1) execution pass rate (Exec Pass), (2) mean visual and task scores (Mean), and (3) the proportion of samples scoring at least 75 (Good). The best-performing model in each scale is shown in bold, and the second best is underlined. verbose syntax and less represented API structure pose greater challenges for current models. VisCoder Closes the Open-Source Gap. VisCoder models consistently outperform their untuned Qwen2.5-Coder baselines across all libraries. At 3B, VisCoder improves both execution success and semantic alignment, with larger gains on plotly and seaborn, where baseline generations often fail to capture visual intent. At 7B, VisCoder outperforms GPT-4o-mini on both seaborn and plotly, while remaining slightly behind on matplotlib. These results demonstrate that domain-specific instruction tuning improves functional reliability and output fidelity, especially in libraries with more complex plotting structures. Self-Debug Further Boosts Performance. GPT4o demonstrates strong self-debugging ability, reaching near-perfect execution pass rates after multiple rounds of correction. VisCoder models also improve substantially under this protocol. VisCoder-7B surpasses 90% execution success on both matplotlib and seaborn, with especially large gains on the latter. Task and visual scores improve consistently across rounds. These results show that VisCoder can generalize from its training data to refine failed outputs over multiple attempts, even without task-specific debugging supervision."
        },
        {
            "title": "5.2 Self-Debug Evaluation Results",
            "content": "To analyze the dynamics of self-debugging, we track execution pass rates over multiple correction rounds by evaluating GPT-4o and GPT-4o-mini as proprietary baselines, alongside VisCoder models at 3B and 7B scales. To isolate the effects of instruction tuning, we also include untuned Qwen2.5Coder models at matching sizes. Figure 2 shows execution pass rates from the initial generation (Attempt 0) through three rounds of self-debugging (Attempts 13), presented separately for each plotting library. Detailed breakdown of pass rates per model and library is provided in Appendix B. Self-debug is broadly effective. Execution pass rates increase steadily over self-debug rounds for most models and libraries, indicating the overall effectiveness of the protocol. The first attempt typically yields the largest improvement, with smaller gains in subsequent rounds. This pattern suggests that simple retry mechanism informed by execution feedback can recover substantial portion of initial failures. VisCoder yields stable behavior. Compared to their Qwen2.5-Coder baselines, VisCoder models show smaller per-round gains in execution pass rate but consistently achieve higher final performance. This suggests that VisCoder tends to generate stronger initial outputs and applies more stable corrections across rounds. The effect is most pro- (a) Matplotlib (b) Seaborn (c) Plotly Figure 2: Execution pass rate across self-debug rounds (Attempt 03), shown separately for three plotting libraries. Attempt 0 corresponds to the default output, while Attempts 13 represent subsequent correction rounds. Model groups are color-coded, with solid and dashed lines used to distinguish paired models. VisCoder models improve consistently across rounds, with VisCoder-7B gradually closing the gap to GPT-4o on seaborn. Y-axis ranges are scaled per subplot to match library-specific score distributions. nounced with VisCoder-7B on seaborn, where execution rates increase steadily and approach GPT-4o by the final attempt. Failures remain across models. Even the strongest model GPT-4o does not reach perfect execution rates after self-debugging. On seaborn, its performance plateaus after three rounds, leaving non-trivial portion of failures unresolved. In contrast, VisCoder-3B stands out among small-scale It surpasses GPT-4o-mini on seaborn models. and performs competitively across other libraries. Meanwhile, we observe that smaller models tend to reach their performance ceiling more quickly, exhibiting smoother but more limited improvements across rounds."
        },
        {
            "title": "5.3 Error Analysis",
            "content": "recovery behavior of To examine the error VisCoder-7B, we analyze how execution error counts transition before and after self-debugging. Table 2 summarizes four representative error types, grouped by plotting library. detailed breakdown by model and debug round is provided in Appendix C."
        },
        {
            "title": "KeyError\nValueError",
            "content": "5 2 7 5 1 1 4 5 15 2 8 4 0 0 8 7 5 1 3 1 1 1 29 Table 2: Execution error count transitions for VisCoder7B across four representative error types, segmented by plotting library. Each value shows the transition from the initial to the post-debugging error count (X Y). Effective Recovery from Structural Errors. VisCoder-7B demonstrates strong self-correction ability on shallow, structural errors. AttributeErrors in Seaborn are reduced from 15 to 2, and TypeErrors in Plotly from 3 to 1. These failures typically result from incorrect method calls, invalid argument types, or simple syntax mistakes, and are often accompanied by clear diagnostic messages. As illustrated in Figure 4 and Figure 6, VisCoder can reliably correct such cases using runtime feedback, frequently producing valid plots on retry. Persistent Failures in Semantic Execution Errors. Semantic execution errors such as KeyError and ValueError remain difficult to resolve (Figure 8). On Plotly, ValueErrors decrease from 29 to 23 across three rounds of correction, but substantial number still remain. Meanwhile, KeyErrors show no improvement, remaining at 1 throughout. These failures are often caused by invalid trace configurations or mismatched array lengths and typically require reasoning over the input data structure. However, the model does not dynamically reassess the DataFrame during self-debug, leading to retries that rely on faulty assumptions. Compared to structural errors, semantic failures are less localized and more difficult to resolve through symbolic correction alone."
        },
        {
            "title": "5.4 Training Data Ablation",
            "content": "We assess the contribution of each training data source in VisCode-200K through controlled ablation study, including two reference points: the model trained on the full VisCode-200K dataset and the untuned Qwen2.5-Coder-7B-Instruct baseline. Separate Qwen2.5-Coder-7B models are finetuned on subsets from stack-edu, CoSyn-400K, and Code-Feedback, using the same instruction tuning setup as the full configuration. All models are evaluated on PandasPlotBench under both default and self-debug modes. Table 3 shows execution pass rates across the three plotting libraries."
        },
        {
            "title": "Model",
            "content": "Self-Debug Matplotlib Seaborn Plotly Qwen2.5-Coder-7B-Ins + Stack-Edu-105K + CoSyn-50K* + Code-Feedback-45K + VisCode-200K 78.3 83. 66.3 72.0 0.0 0.0 88.0 90.9 87.4 91.4 68.6 86.3 55.4 69. 0.0 0.0 44.0 59.4 76.6 90.3 48.0 71.4 49.7 61.1 5.7 6. 62.9 77.7 74.3 81.7 Table 3: Execution pass rates of Qwen2.5-Coder-7B models trained on individual subsets of VisCode-200K. Each model is evaluated across three libraries under both default () and self-debug () modes. Stack-Edu provides moderate generalization. Using the subset from stack-edu results in modest gains over the baseline in plotly under the default setting (+1.7), but leads to significant drops on matplotlib and seaborn (12.0 and 13.2). Self-debug improves pass rates across all libraries compared to their respective defaults, yet all scores remain below the untuned baseline. These results suggest that while stack-edu offers broad task coverage, it lacks the structural supervision and feedback-guided correction patterns needed for robust generalization. CoSyn fails to generalize. The subset from CoSyn-400K fails to support effective instruction tuning for this task. Execution pass rates remain near zero across all libraries, and self-debug yields no meaningful improvement. Generated outputs often exhibit decoding instability, including repeated sequences, empty completions, or irrelevant boilerplate. key reason is the homogeneous structure of the source data: all samples follow fixed format consisting of imports, function definitions, and single function calls, which severely limits structural diversity during training. Combined with the synthetic and non-executable nature of the examples, this makes the single CoSyn subset ill-suited for executable visualization code generation. Code-Feedback enhances structure but lacks breadth. The subset from Code-Feedback improves execution reliability on matplotlib and plotly in the default setting, outperforming the baseline by 9.7 and 14.9 points, respectively. These gains suggest that examples grounded in execution feedback help the model generate structurally valid and complete code. However, performance on seaborn remains low (44.0), and gains on plotly are limited compared to the full model. This reflects the general-purpose nature of the source data, which is not designed for visualization and lacks the task-specific grounding needed for broader transfer. Self-debug improves pass rates across libraries, but overall performance remains below that achieved with our full VisCode-200K dataset. Full data offers complementary gains. The full VisCode-200K dataset yields the most consistent execution improvements across all plotting libraries and evaluation modes. Its performance under selfdebug is particularly robust, with high pass rates maintained across structurally diverse tasks. These results reinforce the importance of domain-specific instruction tuning and multi-turn correction data for building reliable visualization-capable models."
        },
        {
            "title": "6 Conclusion",
            "content": "In conclusion, VisCode-200K provides largescale instruction tuning dataset for Python visualization code generation, combining executable plotting examples with multi-turn correction dialogues grounded in runtime feedback. To validate its effectiveness, we evaluate VisCoder models on PandasPlotBench using the default setting. Additionally, We propose self-debug protocol to simulate realistic correction workflows and assess model performance in this extended evaluation mode. Experiments show that VisCoder substantially outperforms strong open-source baselines across execution and alignment metrics, and narrows the gap to proprietary models like GPT-4o-mini. Gains are particularly pronounced in settings that involve complex visualization structures, such as Plotly, and iterative correction through selfdebugging. Ablation studies further demonstrate that structurally diverse, executable training data and feedback-driven supervision contribute to more robust performance across plotting libraries. Looking forward, this work reinforces the importance of domain-specific instruction tuning and multi-turn correction supervision for building robust and semantically grounded visualizationcapable models. Future extensions may explore broader plotting libraries, richer correction supervision, and evaluation methods that measure models abilities to recover from execution errors."
        },
        {
            "title": "Limitations",
            "content": "Although VisCoder substantially improves visualization code generation, its scope is currently limited to Python, leaving visualization tasks involving other programming languages such as and JavaScript unexplored. Even within Python, performance on Plotly remains comparatively weaker due to its verbose syntax and complex API structure, frequently causing semantic execution errors that the existing self-debugging routine struggles to address. Furthermore, our evaluation relies on the default automatic judge model adopted from prior studies, without an independent analysis of its potential biases or reliability."
        },
        {
            "title": "References",
            "content": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, and 3 others. 2025. Smollm2: When smol goes big datacentric training of small language model. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374. Nan Chen, Yuge Zhang, Jiahang Xu, Kan Ren, and Yuqing Yang. 2024. Viseval: benchmark for data visualization in the era of large language models. IEEE Transactions on Visualization and Computer Graphics. Xiancai Chen, Zhengwei Tao, Kechi Zhang, Changzhi Zhou, Wanli Gu, Yuanpeng He, Mengdi Zhang, Xunliang Cai, Haiyan Zhao, and Zhi Jin. 2025. Revisit self-debugging with self-generated tests for code generation. ArXiv preprint, abs/2501.12793. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, and 1 others. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. ArXiv preprint, abs/2409.17146. Victor Dibia. 2023. Lida: tool for automatic generation of grammar-agnostic visualizations and infographics using large language models. ArXiv preprint, abs/2303.02927. Timur Galimzyanov, Sergey Titov, Yaroslav Golubev, and Egor Bogomolov. 2024. Drawing pandas: benchmark for llms in generating plotting code. ArXiv preprint, abs/2412.02764. Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. 2024. Rlef: Grounding code llms in execution feedback with reinforcement learning. ArXiv preprint, abs/2410.02089. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. ArXiv preprint, abs/2407.21783. Anastasiia Grishina, Vadim Liventsev, Aki Härmä, and Leon Moonen. 2025. Fully autonomous programming using iterative multi-agent debugging with large language models. ACM Transactions on Evolutionary Learning, 5(1):137. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, and 1 others. 2024. Deepseekcoder: When the large language model meets programmingthe rise of code intelligence. ArXiv preprint, abs/2401.14196. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, and 1 others. 2024. Qwen2. 5-coder technical report. ArXiv preprint, abs/2409.12186. John Hunter. 2007. Matplotlib: 2d graphics environment. Computing in science & engineering, 9(03):9095. Plotly Technologies Inc. 2015. Collaborative data science. Montreal: Plotly Technologies Inc Montral, 376. Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander Rush, Wenting Zhao, and Sanjiban Choudhury. 2025. Multi-turn code generation through single-step rewards. ArXiv preprint, abs/2502.20380. Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, and Anoop Deoras. 2024. Ledex: Training llms to better self-debug and explain code. Advances in Neural Information Processing Systems, 37:35517 35543. Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, and Wenhu Chen. 2025. Theoremexplainagent: Towards multimodal explanations for llm theorem understanding. ArXiv preprint, abs/2502.19400. Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, and Xiangdong Zhou. 2025. Cadllama: Leveraging large language models for computer-aided design parametric 3d model generation. ArXiv preprint, abs/2505.04481. Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris CallisonBurch, Ranjay Krishna, Aniruddha Kembhavi, and 1 others. 2025. Scaling text-rich image understanding via code-guided synthetic multimodal data generation. ArXiv preprint, abs/2502.14846. Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, and 1 others. 2024. Matplotagent: Method and evaluation for llm-based agentic scientific data visualization. ArXiv preprint, abs/2402.11453. Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. 2025. Acecoder: Acing coder rl via automated test-case synthesis. ArXiv preprint, abs/2502.01718. Xuanyu Zhang and Qing Yang. 2025. Extracting the essence and discarding the dross: Enhancing code generation with contrastive execution feedback. In Proceedings of the 31st International Conference on Computational Linguistics, pages 1056910575. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024. Opencodeinterpreter: Integrating code generation with execution and refinement. ArXiv preprint, abs/2402.14658. Jierui Li, Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. 2024. Codetree: Agent-guided tree search for code generation with large language models. ArXiv preprint, abs/2411.04329. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, and 1 others. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594. OpenAI. 2024a. Gpt-4o mini: advancing cost-efficient intelligence. https://openai.com/index/gpt-4o-miniadvancing-cost-efficient-intelligence/. OpenAI. Hello https://openai.com/index/hello-gpt-4o/. 2024b. gpt4-o. Luca Podo, Muhammad Ishmal, and Marco Angelini. 2024. Vi (e) va llm! conceptual stack for evaluating and interpreting generative ai-based visualizations. ArXiv preprint, abs/2402.02167. Fernando Vallecillos Ruiz, Max Hort, and Leon Moonen. 2025. The art of repair: Optimizing iterative program repair with instruction-tuned models. ArXiv preprint, abs/2505.02931. Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, and Seunghyun Lee. 2025. Vispath: Automated visualization code synthesis via multi-path reasoning and feedback-driven optimization. ArXiv preprint, abs/2502.11140. Qwen Team. 2024. Qwen2.5: party of foundation models. Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Haotian Hui, Weichuan Liu, Zhiyuan Liu, and 1 others. 2024. Debugbench: Evaluating debugging capability of large language models. ArXiv preprint, abs/2401.04621. Pere-Pau Vázquez. 2024. Are llms ready for visualization? Michael Waskom. 2021. Seaborn: statistical data Journal of Open Source Software, visualization. 6(60):3021. Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, and Ping Luo. 2024. Plot2code: comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots. ArXiv preprint, abs/2405.07990. Liwenhan Xie, Chengbo Zheng, Haijun Xia, Huamin Qu, and Chen Zhu-Tian. 2024. Waitgpt: Monitoring and steering conversational llm agent in data analysis with on-the-fly code visualization. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pages 114."
        },
        {
            "title": "A Prompts Used for Dataset Construction",
            "content": "B Breakdown Results in Self-Debug Mode Evaluation . . . B.1 Matplotlib . . B.2 Seaborn . . . B.3 Plotly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Breakdown Results by Error Type",
            "content": "C.1 VisCoder Series . C.2 GPT Series . C.3 Qwen2.5 Series . . C.4 LLaMA Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Case Study",
            "content": "D.1 Matplotlib: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Matplotlib: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Seaborn: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Seaborn: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Plotly: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6 Plotly: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 13 13 13 14 15 15 16 17 19 20 20 21 22 23"
        },
        {
            "title": "A Prompts Used for Dataset Construction",
            "content": "In this section, we present the system prompts used during the construction of VisCode-200K. These prompts guide the automatic extraction of standalone visualization code from mixed-context sources, and support the generation of structured natural language instructions aligned with rendered plots."
        },
        {
            "title": "Code Extraction Prompt",
            "content": "Model: GPT-4o-mini You are Python code extraction agent. Given Python code snippet and the used library, your task is to extract self-contained and runnable Python code block that demonstrates how the specified library is actually used in the original code. Use mock data where needed (e.g., pandas DataFrame, NumPy arrays), but keep it minimal and logically aligned with the original usage. Retain any important structure, function calls, or plotting styles that reflect meaningful usage of the library. - Do not include plt.close() or similar calls. - If the library is only imported but never used, or if there is insufficient information to construct meaningful runnable code block, return \"null\" (a literal string). - Return only the Python code block enclosed in triple backticks like this: python ... , with nothing else. Used Library: {used_libs} Code: {code} Instruction Generation Prompt: stack-edu Model: GPT-4o Write the general TASK to write code for plotting the given mock data. The code with mock data is given below, and the result of the generated plot image is given at the end. Split task into five parts: 1. Setup (describe programming language and libraries required to generate the plot). 2. Data Description (some short description of the mock data). 3. Data Generation (the data-generation lines copied verbatim). 4. Plot Description (describe the structural layout of the plot, without referencing libraries or function names. Begin with Generate... or Create...). 5. Plot Style Description (describe the visual styling aspects of the plot, without referencing libraries or function). CODE: {code} Each part of the task must start on new line, numbered 1 through 5. Use plain text only. Do not include any markdown symbols. Instruction Generation Prompt: CosyN-400K Model: GPT-4o Write the general TASK to write code for plotting the given data. The top two rows of the data are included in the code comments, showing the CSV structure, and the result of the generated plot image is given at the end. Split task into four parts: 1. Setup (describe programming language and libraries required to generate the plot). 2. Data Description (some short description of the given data). 3. Plot Description (describe the structural layout of the plot, without referencing libraries or function names. Begin with Generate... or Create...). 4. Plot Style Description (describe the visual styling aspects of the plot, without referencing libraries or function). CODE: {code} Each part of the task must start on new line, numbered 1 through 4. Use plain text only. Do not include any markdown symbols. Breakdown Results in Self-Debug Mode Evaluation In this section, we provide breakdown of model performance under the self-debug setting. For each visualization library, we report execution pass rates across up to three rounds of automatic correction, grouped by model series. B.1 Matplotlib"
        },
        {
            "title": "Model",
            "content": "GPT-4o GPT-4o-mini Llama-3.2-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B"
        },
        {
            "title": "Normal",
            "content": "Self-Debug Attempt Round 1 Round 2 Round 3 94.9 88.6 65.1 74.3 71.4 81.7 81.1 77.1 78.3 87.4 97.7 96.6 76.6 79.4 74.9 83. 89.7 83.4 82.9 90.9 99.4 97.7 80.0 82.9 76.6 85.1 92.6 88.0 83.4 91.4 99.4 97.7 81.7 84.6 76.6 85. 93.7 89.7 83.4 91.4 Table 4: Execution pass rates (%) on Matplotlib tasks under the normal and self-debug settings. Models that fail initially are allowed up to three rounds of automatic correction. [Back to Appendix Contents] B.2 Seaborn"
        },
        {
            "title": "Model",
            "content": "GPT-4o GPT-4o-mini Llama-3.2-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B"
        },
        {
            "title": "Normal",
            "content": "Self-Debug Attempt Round 1 Round 2 Round 3 83.4 62.3 30.9 58.3 58.3 73.7 65.7 66.3 68.6 76.6 90.3 69.1 64.6 64.0 65.7 77. 78.9 79.4 82.3 86.9 92.6 70.9 72.0 73.7 68.0 78.3 84.6 85.7 84.6 89.7 92.6 72.0 74.9 75.4 68.0 78. 90.3 89.7 86.3 90.3 Table 5: Execution pass rates (%) on Seaborn tasks under the normal and self-debug settings. All models undergo up to three rounds of automatic correction after initial failure. [Back to Appendix Contents] B.3 Plotly"
        },
        {
            "title": "Model",
            "content": "GPT-4o GPT-4o-mini Llama-3.2-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B"
        },
        {
            "title": "Normal",
            "content": "Self-Debug Attempt Round 1 Round 2 Round 3 77.7 69.1 13.1 30.9 27.4 60.6 30.9 56.0 48.0 74.3 92.0 88.0 20.6 36.0 34.9 64. 43.4 66.3 57.7 80.0 95.4 96.0 24.0 42.3 36.0 64.6 53.7 72.6 68.6 81.7 97.7 97.7 28.0 48.0 36.0 64. 58.3 77.1 71.4 81.7 Table 6: Execution pass rates (%) on Plotly tasks under the normal and self-debug settings. All models undergo up to three rounds of automatic correction after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "C Breakdown Results by Error Type",
            "content": "In this section, we provide detailed breakdown of execution error types across model families, plotting libraries, and self-debugging rounds. For each model series, we report the number of Python exceptions observed under default execution and across up to three rounds of automatic correction. C.1 VisCoder Series"
        },
        {
            "title": "Seaborn",
            "content": "Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round"
        },
        {
            "title": "AttributeError\nAxisError\nImportError\nIndexError\nKeyError\nKeyboardInterrupt\nNameError\nOSError\nSyntaxError\nTypeError\nValueError",
            "content": "5 - 1 1 1 1 - 1 1 7 4 2 - 0 0 2 1 - 1 0 5 5 2 - 0 0 1 1 - 1 0 5 5 2 - 0 0 1 1 - 1 0"
        },
        {
            "title": "Total Errors",
            "content": "22 16 15 15 15 1 1 - - 2 5 1 - 8 8 3 1 0 - - 1 4 1 - 5 8 2 1 0 - - 1 2 1 - 4 7 2 1 0 - - 1 1 1 - 4 7 23 18 5 - - 1 0 1 - 1 5 3 29 45 1 - - 1 1 1 - 1 3 1 26 35 1 - - 1 1 1 - 1 2 1 24 1 - - 1 2 1 - 1 2 1 23 32 Table 7: Distribution of execution errors for VisCoder-7B across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "Seaborn",
            "content": "Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round 3 7 - 2 2 0 0 - 3 5 13 32 3 - 2 3 0 1 - 3 5 29 2 - 2 3 1 0 - 2 4 12 26 2 - 2 3 4 0 - 0 4 11 26 20 - 4 3 2 1 1 1 2 46 10 - 3 4 2 1 1 1 4 13 39 10 - 3 4 2 1 1 1 3 13 38 9 - 3 4 3 2 1 0 3 38 4 1 1 - 5 0 - 8 9 41 69 4 1 1 - 4 2 - 6 6 38 62 3 1 1 - 4 0 - 6 6 62 2 0 1 - 29 0 - 1 6 23 62 Table 8: Distribution of execution errors for VisCoder-3B across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] C.2 GPT Series Error Type AttributeError Exception IndexError KeyError KeyboardInterrupt ModuleNotFoundError NameError RuntimeError SyntaxError TypeError ValueError Total Errors Matplotlib Seaborn Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round 3 - - 1 - - 1 - - - 2 5 9 - - 0 - - 0 - - - 1 4 - - 0 - - 0 - - - 0 1 1 - - 0 - - 0 - - - 0 1 1 - 1 1 - - 1 14 1 - 6 29 - 0 1 - - 0 12 0 - 1 3 17 - 0 0 - - 0 13 0 - 0 0 13 - 0 0 - - 0 13 0 - 0 13 4 1 - 1 2 - 2 - 2 3 24 39 1 0 - 1 1 - 0 - 0 1 10 14 0 0 - 0 2 - 0 - 0 1 8 0 0 - 0 2 - 0 - 0 0 2 4 Table 9: Distribution of execution errors for GPT-4o across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] Error Type AttributeError Exception FileNotFoundError ImportError IndexError KeyError KeyboardInterrupt ModuleNotFoundError NameError TypeError ValueError 3 1 1 1 0 2 - 1 1 1 9 Total Errors 20 Matplotlib Seaborn Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round 3 0 0 0 0 1 1 - 0 0 1 3 0 0 0 0 1 0 - 0 0 1 2 4 0 0 0 0 1 0 - 0 0 0 3 4 2 2 1 - 1 1 1 - 43 4 11 0 0 1 - 1 1 0 - 48 1 2 54 0 1 0 - 1 0 0 - 47 0 2 51 0 0 0 - 1 0 0 - 47 0 1 13 1 - - 0 - 1 2 11 5 21 54 2 1 - - 2 - 0 0 0 1 15 21 0 1 - - 1 - 0 0 0 1 4 0 1 - - 1 - 0 0 0 0 2 4 Table 10: Distribution of execution errors for GPT-4o-mini across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] C.3 Qwen2.5 Series Error Type AttributeError FileNotFoundError ImportError IndexError KeyError KeyboardInterrupt ModuleNotFoundError NameError SyntaxError TypeError ValueError Total Errors Matplotlib Seaborn Normal Round 1 Round 2 Round Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round 3 12 - 1 1 3 1 - 1 - 9 10 38 9 - 0 1 4 1 - 0 - 7 8 9 - 0 1 3 1 - 0 - 7 9 - 0 1 3 1 - 0 - 7 8 30 29 29 17 0 - 1 2 1 - 17 - 8 9 8 1 - 0 1 2 - 8 - 5 6 7 1 - 0 1 2 - 5 - 4 7 7 1 - 0 1 2 - 3 - 4 6 31 27 8 - - 1 14 2 0 - 6 7 53 91 5 - - 1 17 1 1 - 4 1 44 74 3 - - 1 0 1 1 - 7 1 41 3 - - 1 0 1 1 - 5 1 38 50 Table 11: Distribution of execution errors for Qwen2.5-Coder-7B-Instruct across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] Error Type AttributeError FileNotFoundError ImportError IndexError KeyError KeyboardInterrupt ModuleNotFoundError NameError RecursionError OSError RuntimeError SyntaxError TypeError ValueError Total Errors Matplotlib Seaborn Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round Plotly Normal Round 1 Round 2 Round 3 10 1 - - 2 - - 1 1 - - 1 12 12 40 7 0 - - 1 - - 1 1 - - 0 5 14 29 7 0 - - 1 - - 0 1 - - 0 3 3 0 - - 1 - - 0 1 - - 0 4 9 21 18 14 - 0 2 3 0 1 13 - 1 1 1 8 15 59 4 - 1 3 0 1 1 8 - 1 1 1 4 36 5 - 0 1 0 0 0 6 - 1 1 0 2 9 2 - 0 1 0 0 0 4 - 1 0 0 1 9 25 18 9 - - - - 1 - 10 - - - 4 2 77 7 - - - - 1 - 11 - - - 1 1 38 59 4 - - - - 2 - 9 - - - 2 2 29 48 3 - - - - 2 - 10 - - - 1 1 40 Table 12: Distribution of execution errors for Qwen2.5-7B-Instruct across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "Seaborn",
            "content": "Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round 3 1 14 2 3 1 1 - 3 11 14 50 1 9 1 3 1 1 - 4 10 44 1 8 1 3 1 1 - 3 10 13 41 1 8 1 3 1 3 - 1 10 13 41 - 32 - 4 3 1 6 0 7 73 - 23 - 4 1 0 1 1 9 21 60 - 21 - 4 1 0 0 1 9 20 56 - 21 - 4 1 1 0 0 9 56 - 31 - - 1 2 1 13 37 42 - 13 - - 2 2 2 16 23 56 - 12 - - 1 5 1 13 25 55 - 11 - - 1 36 2 3 24 35 114 112 112 Table 13: Distribution of execution errors for Qwen2.5-Coder-3B-Instruct across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "Seaborn",
            "content": "Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round 3 11 1 1 - 4 2 2 1 1 2 11 9 45 9 0 1 - 3 2 1 0 1 1 8 36 4 0 1 - 2 2 1 1 1 1 5 12 30 4 0 1 - 1 3 1 0 1 2 4 10 27 29 6 - 1 4 2 2 - 1 3 10 73 19 6 - 1 2 1 0 - 1 3 11 19 63 13 5 - 0 1 2 0 - 1 0 6 18 46 11 5 - 1 1 2 1 - 1 0 4 43 32 - - 0 2 1 1 - - 14 18 53 20 - - 0 2 1 1 - - 15 15 58 13 - - 0 2 2 3 - - 13 11 57 121 101 8 - - 1 2 36 2 - - 3 5 34 91 Table 14: Distribution of execution errors for Qwen2.5-3B-Instruct across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] C.4 LLaMA Series"
        },
        {
            "title": "Seaborn",
            "content": "Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round 3 10 1 0 1 - 1 - 1 5 14 33 3 0 1 1 - 0 - 1 5 1 0 1 2 - 1 - 1 4 3 1 0 0 2 - 0 - 0 3 4 18 13 10 21 2 2 1 0 5 3 0 7 60 6 3 1 1 4 6 1 1 5 10 38 8 0 0 2 1 5 0 1 3 7 3 0 0 1 2 2 0 0 2 6 27 - 2 2 - 1 - 19 27 27 16 121 20 - 0 2 - 2 - 16 19 40 99 10 - 3 3 - 1 - 12 13 80 9 - 2 2 - 1 - 7 4 38 63 Table 15: Distribution of execution errors for Llama-3.1-8B-Instruct across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "Seaborn",
            "content": "Normal Round 1 Round 2 Round 3 Normal Round 1 Round 2 Round 3 Plotly Normal Round 1 Round 2 Round 3 11 2 2 1 1 1 3 22 - 18 61 7 0 1 4 1 0 2 10 - 41 6 0 1 1 1 0 1 10 - 15 35 4 0 2 1 2 0 0 8 - 15 32 28 4 1 2 1 44 5 16 - 121 15 1 0 3 2 9 3 12 - 17 62 13 0 0 2 2 3 3 11 - 15 49 11 0 0 1 2 0 2 13 - 44 44 - 0 1 0 4 21 47 1 34 17 - 0 1 1 1 24 41 1 53 11 - 0 1 1 0 22 40 1 57 7 - 1 1 2 0 24 36 1 54 139 133 126 Table 16: Distribution of execution errors for Llama-3.2-3B-Instruct across Matplotlib, Seaborn, and Plotly. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "D Case Study",
            "content": "In this section, we present set of representative examples from VisCoder-7B to illustrate model behavior across the three visualization libraries. D.1 Matplotlib: Successful Generation Figure 3: Example of successful generation in Matplotlib (ID: 11). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] D.2 Matplotlib: Self-Debug Recovery Figure 4: Example of failed generation in Matplotlib (ID: 37), where the initial code raises AttributeError and is resolved in the first round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] D.3 Seaborn: Successful Generation Figure 5: Example of successful generation in Seaborn (ID: 20). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] D.4 Seaborn: Self-Debug Recovery Figure 6: Example of failed generation in Seaborn (ID: 104), where the initial code raises AttributeError and is resolved in the Third round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] D.5 Plotly: Successful Generation Figure 7: Example of successful generation in Plotly (ID: 4). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] D.6 Plotly: Self-Debug Recovery Figure 8: Example of failed generation in Plotly (ID: 15), where the initial code raises ValueError and is resolved in the Second round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents]"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Independent Researcher",
        "Netmind.ai",
        "University of Waterloo"
    ]
}