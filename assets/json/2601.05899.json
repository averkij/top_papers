{
    "paper_title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "authors": [
        "Dawei Wang",
        "Chengming Zhou",
        "Di Zhao",
        "Xinyuan Liu",
        "Marci Chi Ma",
        "Gary Ushaw",
        "Richard Davison"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind)."
        },
        {
            "title": "Start",
            "content": "TowerMind: Tower Defence Game Learning Environment and Benchmark for LLM as Agents Dawei Wang1, Chengming Zhou1, Di Zhao2, Xinyuan Liu1, Marci Chi Ma1, Gary Ushaw1, Richard Davison1 1Newcastle University, United Kingdom 2University of Auckland, New Zealand {d.wang28, c.zhou10, x.liu89, c.ma20, gary.ushaw, richard-gordon.davison}@newcastle.ac.uk, dzha866@aucklanduni.ac.nz 6 2 0 2 9 ] A . [ 1 9 9 8 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and multimodal observation space, including pixel-based, textual, and structured gamestate representations. In addition, TowerMind supports the evaluation of model hallucination and provides high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces new benchmark for the AI agent field. The source code is publicly available on GitHub."
        },
        {
            "title": "Introduction",
            "content": "One of the fundamental challenges in artificial intelligence (AI) is equipping agents with the ability to solve tasks across broader range of scenarios (Russell and Norvig 2016). Recent breakthroughs in large language models (LLMs) (Devlin et al. 2019; Achiam et al. 2023) have made them promising approach to addressing this challenge. Benefiting from their extensive cross-domain knowledge and diverse abilities, including reasoning (Wei et al. 2022b; Wang, Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/tb6147877/TowerMind Deng, and Sun 2022) and problem-solving (Lingo, Arroyo, and Chhajer 2024; Renze and Guven 2024), LLM-based agents have shown potential in various domains, such as healthcare (Li et al. 2024), office automation (Zhang et al. 2024), and design (C elen et al. 2024). Despite differences in context and specifics, these tasks consistently require two foundational capabilities from LLMs: long-term planning and decision-making, which are essential for accomplishing tasks: (1) LLMs leverage long-term planning to decompose high-level task into sequence of subgoals that guide progress toward the final objective; (2) LLMs perform decision-making to translate this sequence of subgoals into executable actions, conditioned on the evolving task state. Real-time strategy (RTS) games are an ideal platform for evaluating long-term planning and decision-making abilities, as they require players to engage simultaneously in both macromanagement and micromanagement (Barros Sa and Madeira 2025). Specifically, RTS games provide battlefield setting where macromanagement tends toward longterm strategic planning, in which players formulate highlevel strategies such as overall unit deployment and resource allocation; whereas micromanagement focuses on real-time decision-making, where players flexibly control units in response to dynamic changes on the battlefield to execute their combat plans. Currently, several RTS game-based benchmarks have recently been proposed for evaluating LLMs, including TextStarCraft II (Ma et al. 2025b), LLM-PySC2 (Li et al. 2024), and VLMs Play StarCraft II (Ma et al. 2025a), all of which are based on the StarCraft II Learning Environment (SC2LE) (Vinyals et al. 2017), known for its relatively high computational demands. These challenging RTS gamebased benchmarks are effective for assessing the long-term planning and decision-making capabilities of LLMs; nevertheless, the need for low-cost evaluation environments still persists in the field (Dubois et al. 2024). For example, in fastpaced continuous development pipelines (Koc 2025) and in the usage of reward models for instruction tuning (Yuan et al. 2024), lightweight benchmarks offer clear advantages. While several lightweight RTS game-based environments (e.g., ELF (Tian et al. 2017), DeepRTS (Andersen, Goodwin, and Granmo 2018), Gym-µrts (Huang et al. 2021)) have been proposed to alleviate the computational demands of SC2LE-based platforms, they fundamentally lack support for textual observations and action interfaces, which makes Figure 1: These are screenshots from four different TowerMind levels. The icons in the four corners of each image display key gameplay information, including the number of players current gold coins, players base health, and remaining enemy waves. The maps feature irregular, intersecting roads along which enemies advance toward the players base in successive waves. Players must strategically build different types of towers at designated locations along these roads to repel the incoming enemies. The cloud-shaped white areas represent fog of war, introducing partial observability to the environment. them incompatible with LLMs. To address the lack of lightweight RTS game-based environments with textual observation capabilities, we propose TowerMind, newly developed game environment built upon the tower defense (TD) subgenre of RTS games (Liu et al. 2019), its screenshot is shown in Figure 1. TD games share the same core game mechanics as classic RTS games (Tian et al. 2017), providing battlefield scenario where players must build towers and deploy units to defend against waves of invading enemies, requiring them to demonstrate long-term planning and decision-making. Unlike the player-versus-player mechanics of classic RTS games, TD games focus solely on defending against predefined waves of enemies. This allows for more isolated evaluation of LLMs ability to finish complex tasks using long-term planning and decision-making, without interference from opponent unpredictability. Furthermore, the fixed tower placement options and predefined enemy roads in TD games facilitate clearer analysis of the strategies employed by LLMs. In this work, TowerMind significantly reduces the computational demands compared to existing RTS gamebased LLM benchmarks. Specifically, existing RTS gamebased benchmarks for LLMs rely on the SC2LE environment, which requires approximately 30 GB of disk space, 2 GB of RAM, and dedicated GPU. In contrast, TowerMind requires only 0.15 GB of disk space and RAM, runs efficiently on CPUs without the need for dedicated GPU, and additionally offers advantages in ease of deployment and integration. This makes it well-suited for rapid research iteration, large-scale parallel training or fine-tuning, and similar scenarios in the LLM domain (Peng et al. 2023). Meanwhile, TowerMind supports pixel-based, textual, and structured game-state observations, enabling evaluation of multimodal LLMs. comprehensive comparison of TowerMind and other lightweight RTS game-based environments in terms of supported features is provided in Table 1. In evaluating LLMs, our metrics consider not only in-game score as measure of performance, but also the executability of actions as an indicator of hallucination. Hallucination refers to LLM outputs that conflict with factual or contextual information (Bang et al. 2023); in our setting, this specifically denotes actions that are invalid or inconsistent with the game state or rules. Such metric design allows for simultaneous evaluation of LLM capabilities and reliability; (2) Customizability: As both TD environment and engine, TowerMind includes graphical level editor that enables researchers to conveniently create custom levels. These levels can range from trivially easy to extremely difficult or structurally unique, supporting diverse research needs and reducing the risk of data contamination. The contributions of our work are four-fold: (1) We present TowerMind, lightweight and multimodal TD environment for evaluating long-term planning and decisionmaking in LLMs, while also supporting hallucination analysis and offering strong customizability. (2) The evaluation results show that while commercial LLMs outperform opensource ones, there remains significant gap between LLMs and human experts. Furthermore, we observe several behavioural shortcomings during evaluation, including inadequate planning validation, lack of multifinality in decisionmaking, and inefficient action use. (3) Based on the experimental results, we discuss three key aspects: how visual input enhances LLM capabilities, how correctness relates to effectiveness, and how LLMs handle misleading information. These findings provide insights for future research and highlight TowerMinds potential as versatile benchmark. (4) We evaluate two popular RL algorithms, Ape-X DQN (Horgan et al. 2018) and PPO (Schulman et al. 2017), and the results demonstrate that TowerMind is challenging environment that broadens the diversity of RL benchmarks."
        },
        {
            "title": "2 Related Work",
            "content": "In addition to addressing the limitations of existing RTS game-based environments, the design of TowerMind incorporates two new features: (1) Hallucination Evaluation: Long-Term Planning and Decision-Making with LLMs. Recent advances in LLMs have sparked growing interest in their capabilities beyond text generation, particularly in Environment ELF (Tian et al. 2017) DeepRTS (Andersen, Goodwin, and Granmo 2018) Gym-µrts (Huang et al. 2021) Mini HoK (Liu et al. 2024) TowerMind (Ours) Pixel Observation Textual Observation Stochastic Environments Partial Observability Level Editor Gym Interface Table 1: Comparison between TowerMind and other lightweight RTS game-based environments. long-term planning and decision-making tasks (Brown et al. 2020). ReAct (Yao et al. 2023) and Toolformer (Schick et al. 2023) demonstrate that LLMs can be guided to plan and act through reasoning traces and tool use, respectively. More recent approaches such as AutoGPT (Yang, Yue, and He 2023) and BabyAGI (Calegario et al. 2023) attempt to leverage LLMs in open-ended task planning by forming feedback loops that allow the models to iteratively set subgoals and update plans. Whether originating from LLMs themselves or enabled through prompt engineering or agentic systems, long-term planning and decision-making capabilities need to be systematically evaluated to support further improvement. Different benchmarks adopt various perspectives when evaluating long-term planning and decision-making. PlanGenLLMs (Wei et al. 2025), AGENTBENCH (Liu et al. 2023), and PLANET (Li et al. 2025) focus on simulated operating systems, web tasks, and other interactive environments. TowerMind, by leveraging the TD genre as subclass of RTS games, preserves the strengths of RTS-style evaluation while providing more computationally efficient alternative. RTS Game-Based Environments and Benchmarks. RTS games have long been one of the game genres most deeply intertwined with AI research (Buro 2003). Currently, available RTS game-based environments include SC2LE (Vinyals et al. 2017), StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al. 2019), Honor of Kings Arena and Honor of Kings 3v3 Arena (HoK3v3) (Wei et al. 2022a), along with several lightweight alternatives designed for lower computational demands, such as ELF (Tian et al. 2017), DeepRTS (Andersen, Goodwin, and Granmo 2018), Gym-µrts (Huang et al. 2021) and Mini Honor of Kings (Mini HoK) (Liu et al. 2024). To facilitate LLM-related research, several RTS game benchmarks featuring text-based observations have been introduced, such as TextStarCraft II (Ma et al. 2025b), LLM-PySC2 (Li et al. 2024), and VLMs Play StarCraft II (Ma et al. 2025a), all of which face relatively high computational demands because they are based on SC2LE. TowerMind serves as computationally efficient alternative to existing RTS game-based benchmarks for LLM evaluation. Moreover, it extends beyond current lightweight RTS game-based environments by supporting multimodal observations and functionalities that reflect the evolving needs of AI agent research. Tower Defense Games and AI Research. TD games represent rule-convergent subgenre of RTS games, characterized by combination of simple mechanics and substantial strategic depth (Avery et al. 2011). With the advancement of AI, TD games have increasingly attracted attention in AI research. Early work focused on applying AI algorithms to address TD-specific challenges (Rummell 2011; Tan et al. 2013; Wong and Kang 2015), while more recent studies have adopted TD games as testbeds for reinforcement learning (RL) research (Dias, Foleiss, and Lopes 2020; Bergdahl, Sestini, and Gisslen 2024) and human-AI collaboration (Haduong et al. 2024). However, the only TD game available to the research community is small module in the ELF (Tian et al. 2017) environment, which is highly limited in tower and enemy variety and lacks units control. To date, the research community still lacks dedicated and sufficiently challenging TD game environment. TowerMind fills this gap and enriches the diversity of benchmarks in the AI agent research domain."
        },
        {
            "title": "3.1 Game Mechanics\nTowerMind consists of a series of independent levels, each\nwith a distinct map and enemy configuration, where ene-\nmies spawn in sequential waves and advance toward the\nplayer’s base. Players must strategically build towers and\ncontrol units to prevent enemies from reaching the base and\ndepleting the player’s base health. For more details about the\ngame mechanics, please refer to the Appendix A.\nMaps. In TowerMind, the map is defined as a square area\nwith a side length of 6, centered at (0.0, 0.0), where both\nthe horizontal and vertical coordinates lie within the interval\n[−3.0, 3.0]. It consists of two fundamental elements: roads\nand tower points. Roads (the red and blue directional curves\nin Figure 2) are fixed paths that guide enemy movement\nthrough the map, represented as sequences of 2D coordi-\nnate waypoints along which enemies move in straight lines,\noften starting from different locations and converging at the\nplayer’s base. Tower points (the label ”F” in Figure 2) are\npredefined locations on the map where players can construct\ndefensive towers, typically positioned along both sides of the\nroads. Some tower points, however, are placed far from the\nroads and cannot engage any enemies, thus serving as mis-\nleading tower points. Together, the varying shapes of roads\nand the diverse locations of tower points form rich and dy-\nnamic map features, serving as a critical factor influencing\nplayers’ long-term planning and decision-making.",
            "content": "Towers, Knights and Hero. Players can control three types of game entities to defend against enemy attacks: towers, knight units, and hero unit, with all player actions related to these three entities. There are three types of buildable towers: Archer Tower, Magician Tower, and Knight Tower, each with distinct construction costs, attack styles, and optimal use cases. The Archer Tower deals strong single-target damage, the Magician Tower performs area-of-effect (AoE) attacks, and the Knight Tower summons knight units that can be manually controlled by the player. Knight units (the label in Figure 2) are melee fighters designed for individual combat with enemy units. They can either be summoned from knight towers or directly deployed anywhere on the map via the knight reinforcements action. And in each level, players can control hero unit with greater health, attack damage, and other attributes than knight units, and can manipulate its movement and skill usage with finer granularity. Overall, towers are relatively static and reflect high-level strategic planning, whereas knight units and the hero unit are more flexible and emphasize low-level tactical decisionmaking and action execution. Enemies. Enemies (the label in Figure 2) appear in waves, with each wave consisting of predefined number of enemies and fixed time interval between waves. There are 15 distinct enemy types, each exhibiting different attributes in terms of health, movement speed, and attack damage. Some also possess special abilities; for example, the Orc Sorcerer can disable nearby towers. By varying the types and quantities of enemies within each wave, the game generates diverse enemy compositions, making it impossible for players to rely on single fixed strategy to clear all levels. Resources. Gold coins (the label in Figure 2) serve as the sole in-game resource and are required for constructing towers, upgrading existing towers, and enhancing the heros maximum health. Gold coins periodically appear at random locations on the map (the label in Figure 2) and must be actively collected by dispatching knight units or the hero to the corresponding positions. Moreover, when the heros AoE skill inadvertently eliminates friendly knight units, Friendly Fire Compensation mechanism is triggered, awarding the player an amount of gold coins as compensation. Fog of War. In each level, white, cloud-shaped fog of war region is present and moves randomly across the map (the label in Figure 2), introducing partial observability to the environment. All towers, knight units, the hero unit, and enemies located within the fog of war are excluded from the observation space, and friendly units in these areas become inactive and do not attack enemies. Fog of war increases the difficulty and uncertainty of the environment."
        },
        {
            "title": "3.2 Environment Interface\nThe TowerMind environment conforms to the OpenAI Gym\nstandard (Brockman et al. 2016) for easy integration with\nexisting frameworks. For more details, see Appendix B.\nObservations. TowerMind provides three distinct types of\nobservations: pixel-based, textual, and structured game-state\nobservations. The pixel-based observation is a 512×512 ×3\ncolour image representing the raw game screen. The textual",
            "content": "and structured game-state observations represent two different formats derived from the same underlying game-state information, which includes both level-specific and real-time status data. For instance, Level Initial Gold Coins indicates the initial number of gold coins at the beginning of level, while Level Enemies Realtime Status provides real-time data about each enemy, such as their health points and coordinates. The textual observation explicitly encodes this information in JSON format, clearly associating each piece of data with its corresponding semantic field name, facilitating comprehension by LLMs. In contrast, the structured game-state observation presents the same information flattened into one-dimensional array. Actions. The action space in this environment is designed as hybrid action space, where each action is represented by three-dimensional vector: = (x, y, c). Here, the first two dimensions (x, y) are continuous variables specifying the spatial location of the action, expressed as Cartesian coordinates within two-dimensional plane centered at the maps midpoint, with both horizontal and vertical coordinates constrained by: x, [3.0, 3.0], consistent with the spatial boundaries of the game map. The third dimension denotes the discrete action type (e.g., upgrade tower, sell tower, dispatch knight reinforcements), represented by an integer index: {0, 1, 2, . . . , 11}. This action space design integrates continuous spatial coordinates with discrete action selections, enabling diverse interactions within the environment. Figure 2 provides more intuitive illustration of the action space. Additionally, only actions that comply with the game rules and current state are considered executable, and we refer to these as valid actions. In contrast, actions that violate these constraints, such as issuing towerbuilding command at coordinates where no tower point exists or when insufficient gold is available, are classified as invalid and are not executed. Reward. TowerMind provides sparse reward signal, assigning reward of 1.0 for each enemy that reaches the players base, which aligns with the game mechanic where the players base health (the label in Figure 2) is reduced by one in the same situation. Episode Dynamics. Each level in TowerMind is treated as single finite episode, terminating either when the players base health reaches zero or when all enemy waves have been eliminated. In all our experiments, actions are executed by default every 16 game steps, corresponding to 187 actions per minute, similar to the action frequency in SC2LE."
        },
        {
            "title": "3.3 Levels and Difficulty",
            "content": "TowerMind includes five built-in benchmark levels with increasing difficulty from Level 1 to Level 5. To characterize each level, we propose quantitative metric system grounded in the game mechanics described in Section 3.1, enabling the modeling and comparison of level difficulty. We define the difficulty of level in TowerMind as scalar value D(l), composed of four components: D(l) = dr(l) + dt(l) + de(l) + dre(l). Road: dr(l) = Rl , where Rl is the number of roads in Rmax level l, and Rmax is the design-time maximum number of Figure 2: Left: screenshot of the TowerMind environment with key game elements annotated. The coordinate axes illustrate the alignment between the game map and the 2D coordinate system. The red and blue arrowed curves represent the two roads used by enemies to attack. Labels AG indicate: (A) players current gold coins; (B) remaining enemy waves; (C) hero units current health; (D) players base health; (E) fog of war; (F) an unbuilt tower point; and (G) gold coins dropped on the map awaiting collection; (H) knights units; (I) an enemy. Right: Illustrations and brief descriptions of the 12 action types. roads in the TowerMind environment. Tower: dt(l) = Tl , where Tl is the number of tower Tmax points in level l, and Tmax is the design-time maximum number of tower points in the TowerMind environment. + Enemy: de(l) = El max , where El is the number of Etotal enemy types in level l, and Etotal is the total number of enemy types, Nl is the average number of enemies per wave in level l, and Nmax is the design-time maximum number of enemies per wave in the TowerMind environment. Resource: dre(l) = 1 , 3 where Imin is the design-time minimum initial gold coins in this environment, Il is the initial gold coins in level l, Gmin is the design-time minimum gold coins drop in the TowerMind environment, Gl is the gold coins drop amount in level l, and rsellback(l) is the tower sell-back ratio in level l. + (1 rsellback(l)) (cid:16) Imin Il + Gmin Gl (cid:17)"
        },
        {
            "title": "3.4 Environment Customizability",
            "content": "The customizability of TowerMind can be categorized into three aspects: (1) Level Customization, researchers can easily create new levels using the provided graphical user interface-based level editor; (2) Parameter Customization, we have made almost all game-related parameters modifiable, allowing researchers to freely adjust attributes of towers, enemies, heroes, and other game elements; (3) Feature Customization, researchers can selectively enable or disable various modes and features within TowerMind, such as enabling debugging mode or activating human trajectory recording functionality. See Appendix for more details."
        },
        {
            "title": "4.1 Evaluation Setting\nWe define two evaluation metrics: score and valid action\nrate. The score metric is identical to the raw reward sig-\nnal provided by the TowerMind environment interface. As\nall benchmark levels assign the player base a fixed health of\n20, the score metric is a real number ranging from –20 to 0.\nThe valid action rate metric is calculated as the ratio of valid\nactions to total actions within a given level: #Valid Actions\n#Total Actions ,\nranges from 0 to 1.",
            "content": "Our evaluation covers range of popular commercial and open-source models, including GPT-4.1 (Achiam et al. 2023), Gemini-2.5-Pro (Comanici et al. 2025), Claude 3.7 Sonnet (Anthropic 2025), Llama 3.2 (90B/11B) (MetaAI 2024), and Qwen2.5-VL (72B/7B) (Bai et al. 2025). We employ zero-shot prompting strategy, using identical prompts across all models to ensure fairness and consistency. The language-only modality receives only the prompt, whereas the vision-language modality includes both the prompt and 512 512 3 pixel-based observation. To support comprehensive benchmark, we evaluated the performance of five human experts across five benchmark levels to establish human experts baseline. See Appendix for more details."
        },
        {
            "title": "4.2 Results\nIn our experiments, each model was evaluated using five\nrandom seeds across each benchmark level under both\nlanguage-only and vision-language modalities. Tables 2 and\n3 respectively present the score and valid action rate per-",
            "content": "formances of different LLMs on each benchmark level. All values in these tables are normalized relative to the human experts baseline. Bold values indicate the best-performing results on each benchmark level under the language-only modality, underlined values denote the best-performing results on each benchmark level under the vision-language modality, and values highlighted in italic indicate performance worse than the random baseline. Further tables including standard errors and additional experimental information are provided in Appendix G. Model Lv 1 Lv 2 Lv 3 Lv 4 Lv 5 Avg. Language-Only 0.59 GPT-4.1 0.52 Gemini-2.5-Pro 0.62 Claude 3.7 Sonnet 0.42 Llama 3.2 90B 0.17 Llama 3.2 11B Qwen 2.5-VL 72B 0.47 0.00 Qwen 2.5-VL 7B Vision-Language 0.63 GPT-4.1 0.57 Gemini-2.5-Pro 0.67 Claude 3.7 Sonnet 0.30 Llama 3.2 90B 0.04 Llama 3.2 11B Qwen 2.5-VL 72B 0.54 0.05 Qwen 2.5-VL 7B Random 0.00 0.49 0.42 0.51 0.32 0.09 0.36 0.00 0.56 0.44 0.58 0.05 0.00 0.39 0.00 0.00 0.32 0.31 0.40 0.19 0.00 0.21 0.00 0.44 0.33 0.45 0.00 0.00 0.20 0. 0.00 0.19 0.11 0.24 0.12 0.00 0.00 0.00 0.32 0.16 0.20 0.00 0.00 0.12 0.00 0.00 0.07 0.33 0.01 0.27 0.15 0.38 0.21 0.00 0.00 0.05 0.00 0.21 0.00 0.00 0.15 0.42 0.01 0.30 0.41 0.16 0.00 0.07 0.00 0.01 0.05 0.26 0.00 0. 0.00 0.00 Table 2: The score performance of different LLMs on the benchmark levels. Model Lv 1 Lv 2 Lv 3 Lv 4 Lv 5 Avg. Language-Only 0.92 GPT-4.1 0.91 Gemini-2.5-Pro 0.90 Claude 3.7 Sonnet 0.48 Llama 3.2 90B 0.28 Llama 3.2 11B Qwen 2.5-VL 72B 0.87 0.11 Qwen 2.5-VL 7B Vision-Language 0.86 GPT-4.1 0.85 Gemini-2.5-Pro 0.85 Claude 3.7 Sonnet 0.44 Llama 3.2 90B Llama 3.2 11B 0.31 Qwen 2.5-VL 72B 0.79 0.21 Qwen 2.5-VL 7B Random 0.25 0.89 0.90 0.87 0.39 0.24 0.78 0.05 0.81 0.81 0.85 0.38 0.19 0.72 0.15 0.25 0.88 0.89 0.85 0.30 0.23 0.76 0. 0.75 0.80 0.83 0.33 0.18 0.66 0.05 0.24 0.84 0.83 0.85 0.21 0.23 0.58 0.01 0.68 0.73 0.80 0.31 0.13 0.54 0.04 0.24 0.75 0.86 0.82 0.87 0.79 0.85 0.20 0.32 0.22 0.24 0.51 0.70 0.04 0. 0.66 0.75 0.67 0.77 0.79 0.82 0.30 0.35 0.11 0.18 0.43 0.63 0.09 0.02 0.22 0.24 Table 3: The valid action rate performance of different LLMs on the benchmark levels."
        },
        {
            "title": "4.3 Quantitative Analysis\nBased on the data in Tables 2 and 3, including both individ-\nual benchmark level results and overall averages, we identify\nthe following key findings:",
            "content": "Limited Performance of LLMs. In terms of score, Claude 3.7 Sonnet achieved the best performance in the languageonly setting, and GPT-4.1 in the vision-language setting. However, they still lag behind human experts by 62% and 58%, with all other models showing even larger gaps. Notably, on the most difficult level, Level 5, all models underperform human experts by at least 84%. Vision Input Improves Performance. All evaluated models except Llama 3.2 (90B/11B) show improved score performance under the vision-language modality compared to the language-only modality. This suggests that multimodal cues enhance these models environmental understanding, whereas Llama 3.2 (90B/11B) seems to struggle with such complex and dynamic visual inputs. Hallucination Issues in Open-Source LLMs. From the perspective of valid action rate, the three commercial LLMs performed relatively well, each exhibiting gap of less than 20% compared to human experts. Among the opensource models, only Qwen 2.5-VL 72B showed acceptable results, while the other three models underperformed significantly. In particular, the smaller models Qwen 2.5-VL 7B and Llama 3.2 11B exhibited performance on several benchmark levels that was even lower than the random baseline. The high level of hallucination constrains their long-term planning and decision-making capabilities. Effect of Level Difficulty on Hallucination. As level difficulty increases, the degree of hallucination also rises across models. This suggests that harder levels tend to include more game elements, resulting in longer prompts that challenge the models generation stability and consistency."
        },
        {
            "title": "4.4 Qualitative Analysis\nWe analyzed model trajectories and identified common chal-\nlenges and limitations:\nInsufficient Validation of Long-Term Planning. In Lev-\nels 1 and 2, we placed one or more misleading tower points\nlocated far from the enemy attack roads. Building towers\non these tower points does not threaten any enemies, thus\nserving only to waste resources. However, the LLMs con-\nsistently chose to build towers on these misleading tower\npoints. Despite having access to all necessary information in\nthe prompt to compute that these towers would not engage\nany enemies, the models failed to perform such basic spatial\nor numerical reasoning during tower placement planning.\nDecision-Making Without Multifinality Thinking. Multi-\nfinality refers to the ability to achieve multiple goals with\na single action (Kruglanski et al. 2015), and it is a key\ndecision-making skill for optimizing task efficiency. This\ntype of behavior is frequently observed in human expert\ngameplay. For example, human experts may direct the hero\nunit to collect gold coins while simultaneously attacking\nnearby enemies. However, we have never observed such be-\nhavior in the gameplay trajectories of any LLMs.\nLimited Use and Understanding of Actions. We fre-\nquently observe that LLMs fail to fully utilize or under-\nstand the available action space. Typical behaviors include\nneglecting to upgrade towers despite sufficient gold, send-\ning knight reinforcements to empty areas, or using the hero’s\nAoE skill in the absence of enemies. This suggests that",
            "content": "LLMs tend to interpret the available actions only at surface level, lacking deeper understanding of their strategic use and appropriate contexts."
        },
        {
            "title": "5 RL Benchmark\nTo validate TowerMind’s feasibility and challenge in RL, we\nestablished a preliminary RL benchmark by adopting two\nwidely-used algorithms as baselines: Ape-X DQN (Horgan\net al. 2018) and PPO (Schulman et al. 2017). We trained two\nalgorithms on the five benchmark levels using pixel-based\nand structured game-state observations. Each algorithm was",
            "content": "Figure 3: The evaluation results on the benchmark levels, with scores normalized relative to the human expert. Error bars represent the standard error. Figure 4: Training curves, the horizontal axis shows the number of training samples, measured in millions. Error bars represent 95% confidence intervals. run three times with different random seeds, using 100 million environment steps per run, as shown in Figure 4. We further evaluated the trained models on the benchmark levels using five different random seeds, following the same score metric used in the LLM-based evaluation, as shown in Figure 3. See Appendix for detailed information about the RL experiments. The evaluation results indicate that, after 100 million environment steps, both RL algorithms were able to solve simpler levels to some extent, but their performance remained substantially inferior to that of human experts, which suggests that TowerMind is challenging environment for RL."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose TowerMind, lightweight TD game environment with multimodal observation capabilities designed for evaluating LLMs. Through our evaluation, TowerMind reveals substantial performance gap between current LLMs and human experts in terms of long-term planning and decision-making. It also clearly demonstrates the roles of visual input, hallucination, and misleading information in contributing to this gap, highlighting TowerMinds practical value for LLM research. Additionally, TowerMind can also be used for RL research. We believe that TowerMind can serve as practical platform to facilitate the development of more capable AI agents. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Andersen, P.-A.; Goodwin, M.; and Granmo, O.-C. 2018. Deep RTS: game environment for deep reinforcement learning in real-time strategy games. In 2018 IEEE conference on computational intelligence and games (CIG), 18. IEEE. Anthropic. 2025. Claude 3.7 Sonnet and Claude Code. https: //www.anthropic.com/news/claude-3-7-sonnet. Accessed: 2025-04-21. Avery, P.; Togelius, J.; Alistar, E.; and van Leeuwen, R. P. 2011. Computational intelligence and tower defence games. In 2011 IEEE Congress of Evolutionary Computation (CEC), 10841091. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Bang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie, B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023. Barros Sa, G. C.; and Madeira, C. A. G. 2025. Deep reinforcement learning in real-time strategy games: systematic literature review. Applied Intelligence, 55(3): 243. Bergdahl, J.; Sestini, A.; and Gisslen, L. 2024. Reinforcement Learning for High-Level Strategic Control in Tower In 2024 IEEE Conference on Games Defense Games. (CoG), 18. IEEE. Bianchi, F.; and Zou, J. 2024. Large language models are vulnerable to bait-and-switch attacks for generating harmful content. arXiv preprint arXiv:2402.13926. Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang, J.; and Zaremba, W. 2016. OpenAI Gym, June 2016. arXiv preprint arXiv:1606.01540. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877 1901. Buro, M. 2003. Real-time strategy games: new AI research challenge. In IJCAI, volume 2003, 15341535. Calegario, F.; Buregio, V.; Erivaldo, F.; Andrade, D. M. C.; Felix, K.; Barbosa, N.; Lucena, P. L. d. S.; and Franca, C. 2023. Exploring the intersection of Generative AI and Software Development. arXiv preprint arXiv:2312.14262. elen, A.; Han, G.; Schindler, K.; Van Gool, L.; Armeni, I.; Obukhov, A.; and Wang, X. 2024. I-design: Personalized llm interior designer. arXiv preprint arXiv:2404.02838. Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, arXiv preprint and next generation agentic capabilities. arXiv:2507.06261. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), 41714186. Dias, A.; Foleiss, J.; and Lopes, R. P. 2020. Reinforcement learning in tower defense. In International Conference on Videogame Sciences and Arts, 127139. Springer. Dubois, Y.; Galambosi, B.; Liang, P.; and Hashimoto, T. B. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Haduong, N.; Wang, I.; Lu, B.-R.; Ammanabrolu, P.; and Smith, N. A. 2024. CPS-TaskForge: Generating Collaborative Problem Solving Environments for Diverse Communication Tasks. In Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for Domain, Application, Group, or Individual (CustomNLP4U), 86112. Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2020. Measuring masarXiv preprint sive multitask language understanding. arXiv:2009.03300. Horgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel, M.; van Hasselt, H.; and Silver, D. 2018. Distributed prioritized experience replay. CoRR abs/1803.00933 (2018). arXiv preprint arXiv:1803.00933. Huang, S.; Ontanon, S.; Bamford, C.; and Grela, L. 2021. Gym-µrts: Toward affordable full game real-time strategy games research with deep reinforcement learning. In 2021 IEEE Conference on Games (CoG), 18. IEEE. Juliani, A.; Berges, V.-P.; Teng, E.; Cohen, A.; Harper, J.; Elion, C.; Goy, C.; Gao, Y.; Henry, H.; Mattar, M.; et al. 2018. Unity: general platform for intelligent agents. arXiv preprint arXiv:1809.02627. Koc, V. 2025. Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & SmokearXiv preprint Tests for Continuous LLM Evaluation. arXiv:2505.12058. Kruglanski, A. W.; Chernikova, M.; Babush, M.; Dugas, M.; and Schumpe, B. M. 2015. The architecture of goal systems: Multifinality, equifinality, and counterfinality in meansend relations. In Advances in motivation science, volume 2, 6998. Elsevier. Li, H.; Chen, Z.; Zhang, J.; and Liu, F. 2025. PLANET: Collection of Benchmarks for Evaluating LLMs Planning Capabilities. arXiv preprint arXiv:2504.14773. Li, J.; Lai, Y.; Li, W.; Ren, J.; Zhang, M.; Kang, X.; Wang, S.; Li, P.; Zhang, Y.-Q.; Ma, W.; et al. 2024. Agent hospital: simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957. Li, Z.; Ni, Y.; Qi, R.; Jiang, L.; Lu, C.; Xu, X.; Liu, X.; Li, P.; Guo, Y.; Ma, Z.; Guo, X.; Huang, K.; and Zhang, X. 2024. LLM-PySC2: Starcraft II learning environment for Large Language Models. arXiv e-prints, arXiv:2411.05348. Liang, E.; Liaw, R.; Nishihara, R.; Moritz, P.; Fox, R.; Goldberg, K.; Gonzalez, J. E.; Jordan, M. I.; and Stoica, I. 2018. RLlib: Abstractions for Distributed Reinforcement LearnIn International Conference on Machine Learning ing. (ICML). Lingo, R.; Arroyo, M.; and Chhajer, R. 2024. Enhancing llm problem solving with reap: Reflection, explicit problem deconstruction, and advanced prompting. arXiv preprint arXiv:2409.09415. Liu, L.; Zhao, J.; Hu, C.; Cao, Z.; Zhao, Y.; Ye, Z.; Meng, M.; Wang, W.; He, Z.; Li, H.; et al. 2024. Mini Honor of Kings: Lightweight Environment for Multi-Agent Reinforcement Learning. arXiv preprint arXiv:2406.03978. Liu, S.; Chaoran, L.; Yue, L.; Heng, M.; Xiao, H.; Yiming, S.; Licong, W.; Ze, C.; Xianghao, G.; Hengtong, L.; et al. 2019. Automatic generation of tower defense levels using PCG. In Proceedings of the 14th International Conference on the Foundations of Digital Games, 19. Liu, X.; Yu, H.; Zhang, H.; Xu, Y.; Lei, X.; Lai, H.; Gu, Y.; Ding, H.; Men, K.; Yang, K.; et al. 2023. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688. Ma, W.; Fu, Y.; Zhang, Z.; and Li, G. 2025a. VLMs Play StarCraft II: Benchmark and Multimodal Decision Method. arXiv preprint arXiv:2503.05383. Ma, W.; Mi, Q.; Zeng, Y.; Yan, X.; Lin, R.; Wu, Y.; Wang, J.; and Zhang, H. 2025b. Large language models play starcraft ii: Benchmarks and chain of summarization approach. Advances in Neural Information Processing Systems, 37: 133386133442. MetaAI. 2024. AI and https://ai.meta.com/blog/llama-3-2-connect-2024-visionedge-mobile-devices/. Accessed: 2025-04-07. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. nature, 518(7540): 529533. Peng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. arXiv preprint 2023. arXiv:2304.03277. Renze, M.; and Guven, E. 2024. Self-reflection in llm arXiv agents: Effects on problem-solving performance. preprint arXiv:2405.06682. Rummell, P. A. 2011. Adaptive ai to play tower defense game. In 2011 16th International Conference on Computer Games (CGAMES), 3840. IEEE. Russell, S. J.; and Norvig, P. 2016. Artificial intelligence: modern approach. pearson. Samvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.; Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Foerster, J.; and Whiteson, S. 2019. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043. Schick, T.; Dwivedi-Yu, J.; Dess`ı, R.; Raileanu, R.; Lomeli, M.; Hambro, E.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023. Toolformer: Language models can teach Llama 3.2: Revolutionizing edge customizable models. Instruction tuning with gpt-4. vision with open, themselves to use tools. Advances in Neural Information Processing Systems, 36: 6853968551. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Sowerby, H.; Zhou, Z.; and Littman, M. L. 2022. Designing rewards for fast learning. arXiv preprint arXiv:2205.15400. Tan, T. G.; Yong, Y. N.; Chin, K. O.; Teo, J.; and Alfred, R. 2013. Automated evaluation for AI controllers in tower defense game using genetic algorithm. In International MultiConference on Artificial Intelligence Technology, 135146. Springer. Tian, Y.; Gong, Q.; Shang, W.; Wu, Y.; and Zitnick, C. L. 2017. Elf: An extensive, lightweight and flexible research platform for real-time strategy games. Advances in Neural Information Processing Systems, 30. Vinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets, A. S.; Yeo, M.; Makhzani, A.; Kuttler, H.; Agapiou, J.; Schrittwieser, J.; et al. 2017. Starcraft ii: new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782. Wang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2019. Superglue: stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32. Wang, B.; Deng, X.; and Sun, H. 2022. Iteratively prompt pre-trained language models for chain of thought. arXiv preprint arXiv:2203.08383. Wei, H.; Chen, J.; Ji, X.; Qin, H.; Deng, M.; Li, S.; Wang, L.; Zhang, W.; Yu, Y.; Linc, L.; et al. 2022a. Honor of kings arena: an environment for generalization in competitive reinforcement learning. Advances in Neural Information Processing Systems, 35: 1188111892. Wei, H.; Zhang, Z.; He, S.; Xia, T.; Pan, S.; and Liu, F. 2025. Plangenllms: modern survey of llm planning capabilities. arXiv preprint arXiv:2502.11221. Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; et al. 2022b. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682. Wong, A. M. H.; and Kang, D.-K. 2015. Game layout and artificial intelligence implementation in mobile 3D tower defence game. International Journal of Security and Networks, 10(1): 4247. Yang, H.; Yue, S.; and He, Y. 2023. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Yuan, W.; Pang, R. Y.; Cho, K.; Sukhbaatar, S.; Xu, J.; and Weston, J. 2024. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 3. Zhang, X.; Luo, S.; Zhang, B.; Ma, Z.; Zhang, J.; Li, Y.; Li, G.; Yao, Z.; Xu, K.; Zhou, J.; et al. 2024. Tablellm: Enabling tabular data manipulation by llms in real office usage scenarios. arXiv preprint arXiv:2403.19318. Game Mechanics Additional Details tower point is not single coordinate, but square region centered at that coordinate with side length of 0.5. Any tower-related action such as building, upgrading, or selling tower will be executed on tower point if the actions coordinates fall within that tower points square region. In TowerMind, the Archer Tower can attack both flying and ground enemies, while the Magician Tower can only target ground enemies. The knight units summoned by the Knight Tower can also only attack ground enemies, the hero can attack both flying and ground enemies. Knight reinforcements can be deployed anywhere on the map, but are limited by cooldown, which is 10 seconds by default. The hero is versatile unit capable of auto-healing, dealing singletarget damage and unleashing AoE skills by sacrificing its own health, but realizing its full potential requires precise and fine-grained control. Enemies are categorized as either flying or ground. Among the 15 enemy types, only the Demon Bat is flying enemy; all others are ground enemies. The full list of 15 enemy types and their properties is provided in below JSON configuration files and screenshots are provided in Figure 5. In TowerMind, aside from gold coins, several other game elements can be considered resources in broader sense: (1) the hero units health can be viewed as damage-convertible resource, since casting the heros AoE skill consumes health, which passively regenerates over time; (2) deploying knight reinforcements incurs no cost. By managing their cooldown effectively, players can maintain up to two additional knight units on the map to help defend against enemies. Thus, knight reinforcements can also be regarded as resources that can be converted into damage; (3) due to the Friendly Fire Compensation mechanism, players receive gold coins when the hero unit kills friendly knight units. Therefore, knight units can also be seen as resources that can be converted into gold coins. Game elements covered by the Fog of War are removed from all forms of observation, including pixel-based, textual, and structured game-state observations. However, the Fog of War only inactivates friendly units, while enemies covered by it continue to move along the road. It is worth noting that players can temporarily illuminate the Fog of War by casting the heros flame-based AoE skill within its range, nullifying its effect for short duration. All randomness in the game, such as damage calculation, gold coin collection, and enemy path selection, is entirely controlled by user-defined integer seed. Given the same starting seed and the same sequence of actions, the environment produces identical outputs, ensuring the reproducibility for research purpose. The following are the configuration tables for towers, knight units, the hero unit, knight reinforcements, and enemies in the environment, containing detailed data for each. Listing 1: Configuration Table of Towers 1 {\"Towers\": 2 [ 3 {\"Type\":1,\"Price\":100,\"AttackSpeed\":4. 0, \"AttackDamage\":0,\" AttackExtraDamage\":0, \"AttackRange\" : 2.0, \"CanAttackAir\":false, \" CanAttackGround\":false, \"Name\":\" Knight Tower\", \"UpgradePrice\":100, \"UpgradeGrowth\":1.2, \"Description\": \"Knight Tower cannot directly attack the enemy, but it can summon up to three knights to the battlefield. After these knights die, the Knight Tower will continue to summon knights. You can deploy these three knights anywhere within the attack range of the Knight Tower.\"}, {\"Type\":2,\"Price\":110,\"AttackSpeed\":2. 0, \"AttackDamage\":100,\" AttackExtraDamage\":20, \"AttackRange \": 2.5, \"CanAttackAir\":false, \" CanAttackGround\":true, \"Name\":\" Magician Tower\",\"UpgradePrice\":110, \"UpgradeGrowth\":1.3,\"Description\": \"Magician Tower can only attack ground enemies, not air enemies. It can create blade trap area (a square with side length of 0.8) on the ground, any enemy passing through this area will be damaged. \"}, 5 {\"Type\":3,\"Price\":120,\"AttackSpeed\":0. 8, \"AttackDamage\":100,\" AttackExtraDamage\":50, \"AttackRange \": 3.0, \"CanAttackAir\":true, \" CanAttackGround\":true, \"Name\":\" Archer Tower\",\"UpgradePrice\":120, \" UpgradeGrowth\":1.4,\"Description\": \" Archer Tower can attack enemies on the ground and in the air. It can only hit one enemy at time. \"} 6 ]} Listing 2: Configuration Table of Knight Units 1 {\"FilePath\":\"Prefabs/Knight\", \"Health\":6 00, \"MovementSpeed\":0.6,\"AttackSpeed\" :0.7, \"AttackDamage\":150,\" AttackExtraDamage\": 50,\"AttackRange\": 1.0,\"CanAttackAir\":false, \" CanAttackGround\":true, \" FFCompensationValue\": 50, \" FFCompensationProbability\": 1.0, \" Description\": \"Whether they are knights summoned by Knight Tower(s) or knight squads sent directly to the battlefield, all knights are within the scope of the Friendly Fire Compensation (FFCompensation) mechanism.\"} Listing 3: Configuration Table of The Hero Unit 1 {\"FilePath\":\"Prefabs/Hero\", \"Health\":160 0, \"MovementSpeed\":0.9,\"AttackSpeed\": 0.7, \"AttackDamage\":200,\" AttackExtraDamage\": 150, \"AttackRange \": 1.0,\"SkillAttackDamage\": 100,\" SkillAttackExtraDamage\": 100,\" SkillCostHealth\": 100, \"SkillLastTime \": 5.0, \"SkillAttackRange\":0.5, \" UpgradeGoldCoinCost\":500, \" UpgradeHealthGrowthValue\":200, \" RecoverHealthPerSec\":50, \"ReviveTime\" :10.0, \"CanAttackAir\":true, \" CanAttackGround\":true, \"Description\": \"Your hero will restore certain amount of health every second and will be revived after certain period of time after death. The health value of your hero will not exceed its maximum health value, you can spend gold coins to increase the maximum health value of your hero.\",\" SkillDescription\": \"Your hero has skill called Fire of Rage that requires you to actively release it. When you release this skill, the hero will ignite raging fire at its feet. Any ground units (including your own knights) near this fire will be damaged, and the fire will continue to burn at this location for period of time. This skill has no cooldown, but your hero will lose lot of health when released. When this skill causes the death of your own knights, there is certain probability that Friendly Fire Compensation will be triggered, and you will receive certain amount of gold coins as compensation.\"} Listing 4: Configuration Table of Knight Reinforcements 1 {\"Number\":2, \"ExistTime\":10.0, \" Description\": \"You can also directly send knight reinforcements to the battlefield. This squad of knights will exist on the battlefield for certain period of time, and then they will disappear. After they disappear , you can send them again.\"} Listing 5: Configuration Table of Enemies 1 {\"Enemies\":[ 2 {\"Type\":0,\"FilePath\":\"Prefabs/ enemy_soldier\",\"Health\":500,\" MovementSpeed\":0.5,\"AttackSpeed\":0. 8, \"AttackDamage\":100,\" AttackExtraDamage\":50, \"Name\":\"Orc Warrior\",\"MovementType\":\"Ground\", \" Description\": \"Orc warriors move on the ground, they are heavily armored, so they have high health value.\"}, {\"Type\":1,\"FilePath\":\"Prefabs/ enemy_wizard\",\"Health\":500,\" MovementSpeed\":0.5,\"AttackSpeed\":0. 8, \"AttackDamage\":10,\" AttackExtraDamage\":10, \"Name\":\"Orc Sorcerer\",\"MovementType\":\"Ground\",\" Description\": \"Orc Sorcerers move on the ground, they have powerful 4 5 perception and magic abilities, and can freeze any defense tower that wants to attack it.\"}, {\"Type\":2,\"FilePath\":\"Prefabs/ enemy_air\",\"Health\":550,\" MovementSpeed\":0.8,\"AttackSpeed\":0. 8, \"AttackDamage\":10, \" AttackExtraDamage\":0,\"Name\":\"Demon Bat\",\"MovementType\":\"Flying\",\" Description\": \"Demon Bats move across the sky, they dont attack, they just fly towards their destination quickly.\"}, {\"Type\":3,\"FilePath\":\"Prefabs/ enemy_clown\",\"Health\":400,\" MovementSpeed\":0.5,\"AttackSpeed\":1. 0, \"AttackDamage\":300,\" AttackExtraDamage\":200, \"Name\":\" Clown\",\"MovementType\":\"Ground\",\" Description\": \"The clowns have skilled combat skills, so they have high attack power and attack speed , which can cause heavy damage to your knights and hero.\"}, 6 {\"Type\":4,\"FilePath\":\"Prefabs/ enemy_greeney\",\"Health\":400,\" MovementSpeed\":0.65,\"AttackSpeed\":0 .8, \"AttackDamage\":120,\" AttackExtraDamage\":30, \"Name\":\" Troll\",\"MovementType\":\"Ground\",\" Description\": \"Trolls have high movement speed and can easily pass through your defenses if you are not careful.\"}, {\"Type\":5,\"FilePath\":\"Prefabs/ enemy_zombie\",\"Health\":500,\" MovementSpeed\":0.5,\"AttackSpeed\":1. 0, \"AttackDamage\":80,\" AttackExtraDamage\":20, \"Name\":\" Zombie\",\"MovementType\":\"Ground\",\" Description\": \"Zombies always appear in groups, and numbers are their advantage.\"}, 8 {\"Type\":6,\"FilePath\":\"Prefabs/ enemy_bonesoldier\",\"Health\":600,\" MovementSpeed\":0.4,\"AttackSpeed\":0. 8, \"AttackDamage\":150,\" AttackExtraDamage\":20, \"Name\":\"Bone Soldier\",\"MovementType\":\"Ground\",\" Description\": \"Bone Soldiers move slower but have higher attack power .\"}, 9 {\"Type\":7,\"FilePath\":\"Prefabs/ enemy_bonechanter\",\"Health\":1000,\" MovementSpeed\":0.5,\"AttackSpeed\":1. 2, \"AttackDamage\":30,\" AttackExtraDamage\":20, \"Name\":\"Bone Chanter\",\"MovementType\":\"Ground\",\" Description\": \"Bone Chanters can not only freeze defense towers, but also has high health value.\"}, 10 {\"Type\":8,\"FilePath\":\"Prefabs/ enemy_piratesailor\",\"Health\":800,\" MovementSpeed\":0.5,\"AttackSpeed\":0. 8, \"AttackDamage\":100,\" AttackExtraDamage\":50, \"Name\":\" Pirate Sailor\",\"MovementType\":\" Ground\",\"Description\": \"Pirate Sailors have high health.\"}, 11 {\"Type\":9,\"FilePath\":\"Prefabs/ enemy_piratecaptain\",\"Health\":300,\" MovementSpeed\":0.5,\"AttackSpeed\":1. 2, \"AttackDamage\":100,\" AttackExtraDamage\":500, \"Name\":\" Pirate Captain\",\"MovementType\":\" Ground\",\"Description\": \"Pirate Captains have low health but have the potential to deal very high amounts of damage.\"}, {\"Type\":10,\"FilePath\":\"Prefabs/ 13 enemy_duckman\",\"Health\":400,\" MovementSpeed\":2.0,\"AttackSpeed\":0. 8, \"AttackDamage\":30,\" AttackExtraDamage\":20, \"Name\":\" Duckman\",\"MovementType\":\"Ground\",\" Description\": \"Duckmen have extremely high movement speed, but are vulnerable to attack.\"}, {\"Type\":11,\"FilePath\":\"Prefabs/ enemy_outlaw\",\"Health\":400,\" MovementSpeed\":0.35,\"AttackSpeed\":0 .9, \"AttackDamage\":80,\" AttackExtraDamage\":20, \"Name\":\" Outlaw\",\"MovementType\":\"Ground\",\" Description\": \"The Outlaws are group of robbers who have not received professional military training and are inferior in quality in all aspects.\"}, 14 {\"Type\":12,\"FilePath\":\"Prefabs/ enemy_hillking\",\"Health\":100000,\" MovementSpeed\":0.2,\"AttackSpeed\":0. 8, \"AttackDamage\":30000,\" AttackExtraDamage\":20, \"Name\":\"Hill King\",\"MovementType\":\"Ground\",\" Description\": \"Hill Kings cannot be defeated, and if you see one, your best option might be to run away.\" }, 15 {\"Type\":13,\"FilePath\":\"Prefabs/ enemy_trexrider1\",\"Health\":1000,\" MovementSpeed\":0.7,\"AttackSpeed\":0. 7, \"AttackDamage\":300,\" AttackExtraDamage\":100, \"Name\":\"TRex Rider\",\"MovementType\":\"Ground\", \"Description\": \"T-Rex Riders are elite warriors with high qualities in all aspects.\"}, 16 {\"Type\":14,\"FilePath\":\"Prefabs/ enemy_piratecommander\",\"Health\":110 0,\"MovementSpeed\":0.7,\"AttackSpeed\" :1.2, \"AttackDamage\":30,\" AttackExtraDamage\":20, \"Name\":\" Pirate Commander\",\"MovementType\":\" Ground\",\"Description\": \"Pirate Commanders have high health and high movement speed.\"} 17 ]} Figure 5: Screenshots of the 15 enemy types, with IDs from left to right ranging from 0 to 14. Environment Interface Additional Details The observation space in TowerMind has three modalities: pixel-based, textual, and structured game-state observations: (1) Pixel-based Observation, it is 512 512 RGB image representing the raw game screen of current step, as shown in Figure 6; (2) Textual Observation, it is JSONformatted text that contains the game state at the current step, as shown in Listing 6; (3) Structured Game-State Observation, it is obtained by extracting the numerical values from the textual observations JSON and flattening them into one-dimensional array of length 759, as shown in Table 1. Actions are categorized as valid or invalid based on whether they conform to the game rules and the current state. valid action is one that can be executed under the given constraints, whereas an invalid action violates these constraints and is therefore ignored. The validity of an action is influenced by several factors, including its target coordinates, the amount of available gold, and whether the hero unit is alive. For example, Figure 7 illustrates how the actions coordinates affect its validity. The Agent Last Action Info field in the Textual Observation stores information about the agents action at the current step, including the actions coordinates, type, execution status, and an associated error code if applicable. Different error codes indicate different reasons why an action may be considered invalid. complete mapping of error codes and their corresponding meanings is provided in Listing 7. Listing 6: Textual Observation of the TowerMind Environment. 1 { 2 \"Map_Center\": {\"X\": 0.0, \"Y\": 0.0}, \" Map_Side_Length\": 6.0, \" Map_Left_Boundary\": -3.0, \" Map_Right_Boundary\": 3.0, \" Map_Upper_Boundary\": 3.0, \" Map_Lower_Boundary\": -3.0, \" Tower_Points_Bounding_Box_Width_Height \": 0.5, \" Level_Gold_Coins_Collection_Count\": 1 , \" Level_Friendly_Fire_Compensation_Count \": 0, \"Level_Maximum_Gold_Coins\": 300 0, \"Level_Initial_Health\": 20, \" Level_Total_Waves_Number\": 5, \" Level_Inter_Wave_Interval\": 6.0, \" Level_Selling_Tower_Refund_Rate\": 1.0 , \"Level_Gold_Coins_Refresh_Interval\" : 2, \"Level_Gold_Coins_Retention_Time \": 15, \" Level_Gold_Coins_Minimum_Pickup_Amount \": 100, \" Level_Gold_Coins_Maximum_Pickup_Amount \": 130, \"Level_Enemy_Movement_Paths\": Figure 7: Each predefined tower point is associated with bounding box of dimensions 0.5 0.5, as indicated by the yellow squares in the figure. For an action involving tower construction or related operations to be considered valid, the coordinates provided must lie within one of these bounding boxes. Otherwise, the action is treated as invalid and will not be executed. In the example depicted, both Action 1 and Action 2 aim to construct Magician Tower, the circular numbered icons indicate the specific execution locations of the two actions. Action 1 specifies location that falls within valid bounding box and is thus valid and executable. Action 2, however, does not intersect with any valid bounding region and is consequently invalid and ignored. \"Name\": \"Knight\", \"Current_Health\": 6 00}, {\"Position\": {\"X\": 1.621, \"Y\": 0 .582}, \"Name\": \"Knight\", \" Current_Health\": 600}], \" Level_Dropped_Gold_Coins 3 _Realtime_Status\": {\"Position\": {\"X\": -1 .613, \"Y\": -1.222}, \" RemainingLifetime\": 14}, \" Agent_Last_Action_Info\": {\"Position\": {\"X\": -1.613, \"Y\": -1.222}, \" Action_Index\": 9, \"Is_Success\": True, \"Error_Code\": 0} 4 } Listing 7: Error Code Details. 1 0 - no error 2 3 1 - build tower where there is already tower 4 5 2 - build tower but dont have enough gold coins 6 7 3 - upgrade non-existent tower 8 9 4 - upgrade tower but dont have enough gold coins 10 11 5 - sell non-existent tower 12 13 6 - failure to provide valid coordinates for building, upgrading, selling tower or showing the attack range of Figure 6: Pixel-based Observation of the TowerMind Environment. [[{\"X\": -3.06, \"Y\": 0.28}, {\"X\": 3.0 9, \"Y\": 0.23}]], \" Level_Enemy_Destination\": {\"X\": 3.09, \"Y\": 0.23}, \"Level_Current_Step\": 19 2, \"Level_Current_Time\": 3.84, \" Level_Current_Wave\": 1, \" Level_Current_Wave_Enemies\": [0, 0, 0 , 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] , \"Level_Current_Wave_Countdown\": \"3\" , \"Level_Current_Gold_Coins\": 414, \" Level_Current_Health\": 20, \" Level_Remaining_Waves\": 5, \" Level_Fog_Of_War_Position\": {\"X\": -0. 063, \"Y\": 1.424}, \" Level_Knight_Reinforcements_Countdown \": 0, \"Level_Hero_Realtime_Status\": { \"Hero_Revive_Countdwon\": 10, \" Is_Hero_Dead\": False, \"Hero_Position\" : {\"X\": -0.985, \"Y\": -0.252}, \" Hero_Current_Health\": 1600}, \" Level_Hero_Fire_Of_Rage_Positions\": [], \"Level_Towers_Realtime_Status\": [{\"Position\": {\"X\": 1.68, \"Y\": 0.99}, \"Tower_Name\": \"Knight Tower\", \" Is_Bulit\": True, \"Is_Frozen\": False, \"Knights_Assembly_Position\": {\"X\": 1. 65, \"Y\": 0.34}}, {\"Position\": {\"X\": - 1.52, \"Y\": 0.9}, \"Tower_Name\": \" Waiting to be Built\", \"Is_Bulit\": False, \"Is_Frozen\": False, \" Knights_Assembly_Position\": {\"X\": -1. 57, \"Y\": 0.21}}], \" Level_Enemies_Realtime_Status\": [], \" Level_Knights_Realtime_Status\": [{\" Position\": {\"X\": 0.162, \"Y\": 0.127}, tower 14 15 7 - failed to provide the valid coordinates for changing the knights assembly location of Knight Tower 16 17 8 - deploy Knight Reinforcements that are on cooldown 18 19 9 - try to manipulate dead hero 20 21 10 - increase your heros maximum health but dont have enough gold coins 22 23 11 - show the attack range of nonexistent tower"
        },
        {
            "title": "Levels",
            "content": "Table 2 summarizes their level difficulty D(l) and the contributing components, calculated using the level difficulty quantitative metric system introduced in main body of the paper: D(l) = drd(l) + dt(l) + de(l) + dre(l). When computing the road component drd(l), we set Rmax = 5, the design-time maximum number of roads on any map. For the tower component dt(l), we use Tmax = 15, the design-time maximum number of tower points on any map. For the enemy component de(l), we set Nmax = 25, representing the design-time maximum number of enemies per wave in this environment. Finally, when computing the resource component dre(l), we use Imin = 120 and Gmin = 40, which denote the design-time minimum initial gold and the minimum gold drop per enemy, respectively."
        },
        {
            "title": "Customizability",
            "content": "The TowerMind level editor is user-friendly, GUI-based application (as illustrated in Figure 8). It allows users to design custom maps by drawing various road shapes using brush tool. Core gameplay elements such as road waypoints and tower points can be added to the map via drag-anddrop interface. Once the design is complete, users can export the map as an image along with corresponding JSON file, which can then be imported into the TowerMind environment for use. All configuration files for the TowerMind environment are located in the StreamingAssets/Config directory (the exact path may vary by platform; see the user documentation for details). All configuration files are in JSON format and can be freely modified as needed. Among all configuration files, EnvConfig manages auxiliary features such as enabling action space discretization preview and human player gameplay recording. For more details, please refer to the user documentation. Zero-shot Prompt The prompt consists of four components: (1) an explanation of the games overall objective and core rules in natural Figure 8: This is screenshot of the TowerMind level editor. On the left is the background pattern selector for the map. The central area is the map editing workspace, where red dots represent waypoints along the road, and blue dots indicate the default summoning points for knights at corresponding tower points. On the right, the upper section contains brush settings for map editing, while the lower section provides drag-and-drop interface for placing game elements. language; (2) an explanation of the action space in natural language; (3) the raw configuration tables for towers, knight units, the hero unit, knight reinforcements, and enemies in JSON format, listing their numerical attributes; (4) the observationaction pair history in JSON format, with default length of 3, showing the agents recent interactions with the environment. This is complete prompt example, excluding the configuration details for towers, knight units, the hero unit, knight reinforcements, and enemies, which are provided in the Appendix A."
        },
        {
            "title": "An Prompt Example",
            "content": "You are an AI agent playing video game, you need to build different types of defense towers at different locations on the map to prevent enemies from reaching their destination. Common rules: - You need to spend gold coins to build towers, upgrade towers and increase your heros maximum health. Gold coins will continue to drop at random locations on the map. You can send your knights or hero to pick up gold coins, and the gold coins will be picked up automatically when your knights or hers are near the gold coins. - If the number of gold coins you hold exceeds the maximum value, the excess will be discarded. - You will be given certain amount of health at the beginning of each level. Every time an enemy reaches its destination, you will lose point of health. When your health reaches 0, the game ends and the mission fails. Try your best to avoid losing any health points. - Enemies appear in waves, and each level has different number of enemy waves. There is certain amount of time between enemy waves. If your health is still greater than 0 after you have resisted all waves of enemy attacks, the mission is successful. - There are several paths for the enemies, and each enemy will randomly choose one. - The battlefield of this game is square area, the details have been included in the level state part blow. - Between each path point, enemies will only move in straight line. - The Fog Of War in the battlefield is an irregular cloud-shaped area that can obscure any element in the game. Its approximate dimensions are 3.5 wide and 1.7 tall. The obscured towers, knights and heroes will no longer attack the enemy, but if the Fog Of War obscures the Fire Of Rage released by the hero, it will lose its obstruction ability during this time. The following are the actions you can take. And for each action, you also need to provide horizontal and vertical coordinate between -3.0 and 3.0. 0 - Build an Archer Tower at the coordinates you specify, 1 - Build an Magician Tower at the coordinates you specify, 2 - Build an Knight Tower at the coordinates you specify, 3 - Upgrade tower at the coordinates you specify, 4 - Sell tower at the coordinates you specify, 5 - Show the attack range of tower at the coordinates you specify, 6 - Noop: do nothing, 7 - Change the knights assembly location of Knight Tower to the coordinates you specify, 8 - Deploy Knight Reinforcements to the coordinates you specify, 9 - Dispatch your hero to the coordinates you specify, 10 - Your hero casts Fire of Rage at your heros coordinates, 11 - Spend gold coins to increase your heros maximum health. Action Tips: - Building tower, upgrading tower or increasing your heros maximum health requires you to have enough gold coins, otherwise it will be an invalid action. - Action 0, 1, 2, 3, 4, 5 are only valid if the coordinates you specify are within the bounding box of the tower point. The bounding box of the tower point is square with its coordinate as the center and side length of 0.5. - Action 7 is only valid if the coordinates you specify is within the attack range of Knight Tower. - Action 8 will be invalid during the Knight Reinforcements cooldown. - Action 9 means that your hero starts moving to the coordinates you specify, not direct teleportation. If you set new target coordinate during its movement, it will start moving to the new target coordinates. - Actions 9, 10, 11 are invalid if your hero dies. - If tower point already has tower, you should not build tower at this tower point, which will result in an invalid action. - You should provide your action in json format, only three elements in this json structure: is floating point number representing the horizontal coordinate of the action you want to perform; is floating point number representing the vertical coordinate of the action you want to perform; Action is an integer representing the index of action you want to perform. spent on return the funds - Action 4 will it may but upgrade, and its not the on depends it refunded, Level Selling Tower Refund Rate value. construction be fully The following are the actions error code list, If you performed an invalid action, you can find out why here: 0 - no error 1 - build tower where there is already tower 2 - build tower but dont have enough gold coins 3 - upgrade non-existent tower 4 - upgrade tower but dont have enough gold coins 5 - sell non-existent tower 6 - failure to provide valid coordinates for building, upgrading, selling tower or showing the attack range of tower 7 - failed to provide the valid coordinates for changing the knights assembly location of Knight Tower 8 - deploy Knight Reinforcements that are on cooldown 9 - try to manipulate dead hero 10 - increase your heros maximum health but dont have enough gold coins 11 - show the attack range of non-existent tower The following is the configuration table of each component of the game, organized in Json format: - Towers Configuration: 1 {...} - Knight Configuration: 1 {...} - Hero Configuration: 1 {...} - Knight Reinforcements Configuration: 1 {...} - Enemies Configuration: 1 {...} Configuration Table Tips: - The attack range of the towers, hero, heros skill and knights is circular, the positions of the circle centers are their position, and the attack range described above is the diameter. When enemies enter this range they will attack. - The final attack value of the towers, hero, heros skill, knights and enemies is equal to AttackDamage plus random value in the range of 0 to AttackExtraDamage. - The unit of time in this tower defense game is seconds. - The unit of range or space in this tower defense game is virtual unified unit. It can be used directly for calculation during reasoning without conversion. - The AttackSpeed of the towers, hero, knights and enemies refers to the time interval between attacks. For the Knight Tower, it refers to the time interval between summoning knights. - Upgrading will increase the attack power of the Archer Tower and the Magician Tower, as well as the attack value and movement speed of the knights summoned by the Knight Tower. The following is the information about this level, organized in Json format: 1 { 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 } \"Map_Center\": {\"X\": 0.0, \"Y\": .0}, \"Map_Side_Length\": 6.0, \"Map_Left_Boundary\": -3.0, \"Map_Right_Boundary\": 3.0, \"Map_Upper_Boundary\": 3.0, \"Map_Lower_Boundary\": -3.0, \"Tower_Points_Bounding_Box_ Width_Height\": 0.5, \"Level_Maximum_Gold_Coins\": 300 0, \"Level_Initial_Health\": 20, \"Level_Total_Waves_Number\": 5, \"Level_Inter_Wave_Interval\": 6. 0, \"Level_Selling_Tower_Refund_ Rate\": 1.0, \"Level_Gold_Coins_Refresh_ Interval\": 2, \"Level_Gold_Coins_Retention_ Time\": 15, \"Level_Gold_Coins_Maximum_ Pickup_Amount\": 130, \"Level_Gold_Coins_Minimum_ Pickup_Amount\": 100, \"Level_Enemy_Movement_Paths\": [[{\"X\": -3.06, \"Y\": 0.28}, {\"X\": 3.09, \"Y\": 0.23}]], \"Level_Enemy_Destination\": {\"X\" : 3.09, \"Y\": 0.23} The following is the history of the past few steps, organized in Json format: 1 {\"state\": \"{\"Level_Gold_Coins_ 2 Collection_Count\": 1, \" Level_Friendly_Fire_ 3 Compensation_Count\": 0, \" Level_Current_Step\": 176, \" Level_Current_Time\": 3.52, \" Level_Current_Wave\": 1, \" Level_Current_Wave_Enemies\": [0 , 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \" Level_Current_Wave_Countdown\": \"3\", \"Level_Current_Gold_Coins\" : 254, \"Level_Current_Health\": 20, \"Level_Remaining_Waves\": 5, \"Level_Fog_Of_War_Position\": { \"X\": -0.049, \"Y\": 1.486}, \" Level_Knight_Reinforcements_ 4 Countdown\": 0, \" Level_Hero_Realtime_Status\": {\" Hero_Revive_Countdwon\": 10, \" Is_Hero_Dead\": False, \" Hero_Position\": {\"X\": -0.597, \" Y\": -0.183}, \" Hero_Current_Health\": 1600}, \" Level_Hero_Fire_Of_Rage_ 5 Positions\": [], \" Level_Towers_Realtime_Status\": [{\"Position\": {\"X\": 1.68, \"Y\": 0.99}, \"Tower_Name\": \"Archer Tower\", \"Is_Bulit\": True, \" Is_Frozen\": False, \" Knights_Assembly_Position\": {\"X \": 1.65, \"Y\": 0.34}}, {\" Position\": {\"X\": -1.52, \"Y\": 0. 9}, \"Tower_Name\": \"Archer Tower \", \"Is_Bulit\": True, \"Is_Frozen \": False, \" Knights_Assembly_Position\": {\"X \": -1.57, \"Y\": 0.21}}], \" Level_Enemies_Realtime_Status\": [], \" Level_Knights_Realtime_Status\": [], \"Level_Dropped_Gold_Coins_ 6 Realtime_Status\": {\"Position\": {\"X\" : 2.191, \"Y\": -2.272}, \" RemainingLifetime\": 14}, \" Agent_Last_Action_Info\": {\" Position\": {\"X\": 2.191, \"Y\": -2 .272}, \"Action_Index\": 9, \" Is_Success\": True, \"Error_Code\" : 0}}\", \"action\": array([ 2.191 , -2.272, 7 {\"state\": \"{\" ])}, 9. Level_Gold_Coins_Collection_ 8 Count\": 1, \"Level_Friendly_Fire_ 9 Compensation_Count\": 0, \" Level_Current_Step\": 192, \" Level_Current_Time\": 3.84, \" Level_Current_Wave\": 1, \" Level_Current_Wave_Enemies\": [0 , 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \" Level_Current_Wave_Countdown\": \"3\", \"Level_Current_Gold_Coins\" : 254, \"Level_Current_Health\": 20, \"Level_Remaining_Waves\": 5, \"Level_Fog_Of_War_Position\": { \"X\": -0.063, \"Y\": 1.424}, \" Level_Knight_Reinforcements_ 10 Countdown\": 0, \" Level_Hero_Realtime_Status\": {\" Hero_Revive_Countdwon\": 10, \" Is_Hero_Dead\": False, \" Hero_Position\": {\"X\": -0.367, \" Y\": -0.355}, \" Hero_Current_Health\": 1600}, \" Level_Hero_Fire_Of_Rage_ 11 Positions\": [], \"Level_Towers_ 12 Realtime_Status\": [{\"Position\": {\"X \": 1.68, \"Y\": 0.99}, \" Tower_Name\": \"Archer Tower\", \" Is_Bulit\": True, \"Is_Frozen\": False, \" Knights_Assembly_Position\": {\"X \": 1.65, \"Y\": 0.34}}, {\" Position\": {\"X\": -1.52, \"Y\": 0. 9}, \"Tower_Name\": \"Archer Tower \", \"Is_Bulit\": True, \"Is_Frozen \": False, \" Knights_Assembly_Position\": {\"X \": -1.57, \"Y\": 0.21}}], \" Level_Enemies_Realtime_Status\": [], \" Level_Knights_Realtime_Status\": [], \"Level_Dropped_Gold_Coins_ 13 Realtime_Status\": {\"Position\": {\"X\" : 2.191, \"Y\": -2.272}, \" RemainingLifetime\": 14}, \" Agent_Last_Action_Info\": {\" Position\": {\"X\": 2.191, \"Y\": -2 .272}, \"Action_Index\": 9, \" Is_Success\": True, \"Error_Code\" : 0}}\", \"action\": array([ 2.191 , -2.272, 14 {\"state\": \"{\" ])}, 9. Level_Gold_Coins_Collection_ 15 Count\": 1, \"Level_Friendly_Fire_ 16 Compensation_Count\": 0, \" Level_Current_Step\": 208, \" Level_Current_Time\": 4.16, \" Level_Current_Wave\": 1, \" Level_Current_Wave_Enemies\": [0 , 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \" Level_Current_Wave_Countdown\": \"2\", \"Level_Current_Gold_Coins\" : 254, \"Level_Current_Health\": 20, \"Level_Remaining_Waves\": 5, \"Level_Fog_Of_War_Position\": { \"X\": -0.078, \"Y\": 1.361}, \" Level_Knight_Reinforcements_ 17 Countdown\": 0, \" Level_Hero_Realtime_Status\": {\" Hero_Revive_Countdwon\": 10, \" Is_Hero_Dead\": False, \" Hero_Position\": {\"X\": -0.137, \" Y\": -0.528}, \" Hero_Current_Health\": 1600}, \" Level_Hero_Fire_Of_Rage_ 18 Positions\": [], \" Level_Towers_Realtime_Status\": [{\"Position\": {\"X\": 1.68, \"Y\": 0.99}, \"Tower_Name\": \"Archer Tower\", \"Is_Bulit\": True, \" Is_Frozen\": False, \" Knights_Assembly_Position\": {\"X \": 1.65, \"Y\": 0.34}}, {\" Position\": {\"X\": -1.52, \"Y\": 0. 9}, \"Tower_Name\": \"Archer Tower \", \"Is_Bulit\": True, \"Is_Frozen \": False, \" Knights_Assembly_Position\": {\"X \": -1.57, \"Y\": 0.21}}], \" Level_Enemies_Realtime_Status\": [], \" Level_Knights_Realtime_Status\": [], \"Level_Dropped_Gold_Coins_ 19 Realtime_Status\": {\"Position\": {\"X\" : 2.191, \"Y\": -2.272}, \" RemainingLifetime\": 13}, \" Agent_Last_Action_Info\": {\" Position\": {\"X\": 2.191, \"Y\": -2 .272}, \"Action_Index\": 9, \" Is_Success\": True, \"Error_Code\" : 0}}\", \"action\": array([ 2.191 , -2.272, ])} 9. The following is the current real-time game status of this step, organized in Json format: 1 { 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 \"Level_Gold_Coins_Collection_ Count\": 1, \"Level_Friendly_Fire_ Compensation_Count\": 0, \"Level_Current_Step\": 224, \"Level_Current_Time\": 4.48, \"Level_Current_Wave\": 1, \"Level_Current_Wave_Enemies\": [ 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"Level_Current_Wave_Countdown\": \"2\", \"Level_Current_Gold_Coins\": 254 , \"Level_Current_Health\": 20, \"Level_Remaining_Waves\": 5, \"Level_Fog_Of_War_Position\": {\" X\": -0.092, \"Y\": 1.299}, \"Level_Knight_Reinforcements_ Countdown\": 0, \"Level_Hero_Realtime_Status\": { \"Hero_Revive_Countdwon\": 10 , \"Is_Hero_Dead\": False, \"Hero_Position\": {\"X\": 0.09 4, \"Y\": -0.701}, \"Hero_Current_Health\": 1600 }, \"Level_Hero_Fire_Of_Rage_ Positions\": [], \"Level_Towers_Realtime_Status\": [ {\"Position\": {\"X\": 1.68, \"Y \": 0.99}, \"Tower_Name\": \"Archer Tower \", \"Is_Bulit\": True, \"Is_Frozen\": False, \"Knights_Assembly_Position\" : {\"X\": 1.65, \"Y\": 0.34 }}, {\"Position\": {\"X\": -1.52, \" Y\": 0.9}, \"Tower_Name\": \"Archer Tower \", \"Is_Bulit\": True, \"Is_Frozen\": False, \"Knights_Assembly_Position\" : {\"X\": -1.57, \"Y\": 0.2 1}} ], \"Level_Enemies_Realtime_Status\" : [], \"Level_Knights_Realtime_Status\" : [], \"Level_Dropped_Gold_Coins_ Realtime_Status\": { \"Position\": {\"X\": 2.191, \"Y \": -2.272}, \"RemainingLifetime\": 13 }, \"Agent_Last_Action_Info\": { \"Position\": {\"X\": 2.191, \"Y \": -2.272}, \"Action_Index\": 9, \"Is_Success\": True, \"Error_Code\": 0 } 24 25 26 27 28 30 31 32 33 34 36 37 38 39 40 42 43 44 45 46 48 49 50 } Image observation provided. Now please tell me the action you want to perform in this step, in JSON format, containing floating point coordinate, floating point coordinate and an integer action index. Your answer should not contain any other text, just provide this json."
        },
        {
            "title": "F Human Expert Performance",
            "content": "Five participants with prior experience in tower defense games were recruited from university environment. All participants expressed willingness and availability to engage in approximately 15 hours of study-related activities within one-week period. Each participant signed an informed consent form, which detailed the studys objectives, methodology, potential risks and discomforts, expected benefits, financial considerations, confidentiality terms, and their rights throughout and following their involvement in the study. Figure 9: The game guide for human participants. Each participant was compensated at rate of 17.89 per hour for their time. All five participants were provided with game guide for the TowerMind environment (as shown in Figure 9), along with five non-benchmark levels for training purposes. The participants conducted their training sessions on their personal laptops using mouse and keyboard, and we verified that the TowerMind environment ran smoothly on all of their devices. Following 13 hours of training, each participant was evaluated on the five benchmark levels, with unique random seed assigned to each. Their evaluation metrics were the same as those used for the LLMs: score and valid action rate. Their APM during evaluation ranged from 40 to 110. The evaluation results are summarized in Table 3."
        },
        {
            "title": "Evaluation",
            "content": "All evaluations of LLMs were conducted on single laptop equipped with 13th Gen Intel(R) Core(TM) i7-13650HX CPU (2.60 GHz), 64 GB of RAM, and an NVIDIA GeForce RTX 4060 Laptop GPU with 8 GB of VRAM. The average evaluation time per episode in the language-only modality is approximately 7 minutes, while the vision-language modality requires 15 minutes per episode. Figures 4 and 5 show the score and valid action rate performance with standard errors, respectively. All values are normalized relative to the human expert baseline. Given the level score or valid action rate of an LLM on , we compute the normalized human-relative level l, s(raw) score or valid action rate sl from the human baseline s(human) and the minimum possible game score or valid action rate s(min) : sl = s(min) s(raw) s(min) s(human) See the supplementary material (Section: Additional LLM Evaluation Results) for correlation analysis between score and valid action rate. Additional Details on RL Experiments Both Ape-X DQN and PPO algorithms use implementations from RLlib (Liang et al. 2018), take pixel-based observations as input, and adopt multiple-layers CNN-based architecture similar to that used in the original DQN paper (Mnih et al. 2015), with approximately 3.1 million parameters, as shown in Figure 11. We also evaluated both algorithms under the structured game-state observation setting, with their model architectures shown in Figure 7. Each algorithm was trained on personal computer equipped with an Intel(R) Core(TM) i7-14700K CPU (20 cores), an NVIDIA GeForce RTX 3090 GPU with 24 GB of VRAM, and 192 GB of RAM. We applied practicality-oriented environment preprocessing to the TowerMind environment interface for training: (1) we downsampled the original 512 512 3 pixel-based observations to 1281283; (2) we discretized TowerMinds hybrid action space by uniformly dividing the first two continuous action variables (representing horizontal and vertical coordinates) into 10 discrete intervals ranging from -3.0 to 3.0, while keeping the action type unchanged, resulting in discrete action space of size 10 10 12, as shown in Figure 12; and (3) we introduced an additional step penalty (Sowerby, Zhou, and Littman 2022) of 5104 to the original TowerMind reward structure. During RL training, each episode was initialized by randomly selecting one of the nine benchmark levels. See Table 6 for hyperparameters used for training Ape-X DQN and PPO. Figure 10: Model architecture of the multiple-layers CNN model, the 128 128 3 image is processed thorough convolution layers, ultimately producing 1200 dimensional policy output via an full connection network Figure 11: Model architecture of the MLP model, the structured game-state observation (length 759) is fed into fully connected layers, resulting in 1200-dimensional policy output. Figure 12: Illustration of action space discretization. The map is divided into 10 10 grid, and actions are performed at the centers of the grid cells instead of using continuous coordinates. Index Field Dimension 2 3 4 5 6 8 9 10 11 12 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 Map Center Map Side Length Map Left Boundary Map Right Boundary Map Upper Boundary Map Lower Boundary Tower Points Bounding Box Width Height Level Gold Coins Collection Count Level Friendly Fire Compensation Count Level Maximum Gold Coins Level Initial Health Level Total Waves Number Level Inter Wave Interval Level Selling Tower Refund Rate Level Gold Coins Refresh Interval Level Gold Coins Minimum Pickup Amount Level Gold Coins Maximum Pickup Amount Level Enemy Destination Level Current Step Level Current Time Level Current Wave Level Current Wave Countdown Level Current Gold Coins Level Current Health Level Remaining Waves Level Fog Of War Position Level Knight Reinforcements Countdown Level Hero Realtime Status Level Dropped Gold Coins Realtime Status Agent Last Action Info Level Enemy Movement Paths Level Current Wave Enemies Level Hero Fire Of Rage Positions Level Towers Realtime Status Level Enemies Realtime Status Level Knights Realtime Status Total 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 5 3 5 * 20 * 2 25 10 * 2 15 * 8 50 * 4 50 * 759 Table 4: Structured Game-State Observation of the TowerMind Environment. Level #Roads #Tower Points #Enemy Types #Enemies per Wave Initial Gold Coins Gold Coins Drop Amount Tower Sell-back Ratio Level Difficulty Lv1 Lv2 Lv3 Lv 1 1 3 3 4 12 12 14 13 14 20.8 9.2 12.0 17.0 500 500 500 100 40 60 100% 0% 10% 20% 2.45 2. 3.42 3.55 4 13 Lv5 500 * #Roads is the number of roads in this level; #Tower Points is the number of tower points in this level; #Enemy Types is the number of enemy types in this level; #Enemies per Wave is the average number of enemies per wave in this level; Initial Gold Coins is the initial gold coins in this level; Gold Coins Drop Amount is the amount of gold coins dropped each time in this level; 3. 16.4 0% 11 50 Table 5: The information of the benchmark levels difficulty and their contributing components. Level LV 1 LV 2 LV 3 LV 4 LV 5 Score Valid Action Rate 0.00 0.20 0.20 0.00 0.40 0.24 1.80 0.68 3.40 0.49 0.99 0.01 0.98 0.03 0.97 0.01 0.94 0.00 0.93 0.01 Average 1.16 0.40 0.96 0. Table 6: Average performance of five human experts across five benchmark levels. Scores range from 20 to 0, and valid action rates range from 0 to 1. Standard errors are included. Model LV 1 LV 2 LV 3 LV LV 5 Average Language-Only 0.59 0.17 GPT-4.1 0.52 0.14 Gemini-1.5-Pro 0.62 0.07 Claude 3.7 Sonnet 0.42 0.02 Llama 3.2 90B 0.17 0.13 Llama 3.2 11B Qwen 2.5-VL 72B 0.47 0.10 0.00 0.00 Qwen 2.5-VL 7B Vision-Language 0.63 0.19 GPT-4.1 0.57 0.19 Gemini-1.5-Pro 0.67 0.07 Claude 3.7 Sonnet 0.30 0.13 Llama 3.2 90B 0.04 0.04 Llama 3.2 11B Qwen 2.5-VL 72B 0.54 0.09 0.05 0.15 Qwen 2.5-VL 7B 0.49 0.06 0.42 0.14 0.51 0.07 0.32 0.13 0.09 0.03 0.36 0.10 0.00 0.00 0.56 0.10 0.44 0.12 0.58 0.06 0.05 0.05 0.00 0.00 0.39 0.12 0.00 0. 0.32 0.04 0.31 0.01 0.40 0.06 0.19 0.09 0.00 0.00 0.21 0.16 0.00 0.00 0.44 0.12 0.33 0.13 0.45 0.07 0.00 0.00 0.00 0.00 0.20 0.10 0.00 0.00 0.19 0.12 0.11 0.13 0.24 0.19 0.12 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.32 0.20 0.16 0.06 0.20 0.07 0.00 0.00 0.00 0.00 0.12 0.18 0.00 0.00 0.07 0.04 0.01 0.03 0.15 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.15 0.09 0.01 0.00 0.16 0.10 0.00 0.00 0.00 0.00 0.05 0.05 0.00 0. 0.33 0.10 0.27 0.09 0.38 0.07 0.21 0.10 0.05 0.08 0.21 0.10 0.00 0.00 0.42 0.08 0.30 0.09 0.41 0.09 0.07 0.08 0.01 0.04 0.26 0.11 0.01 0.11 Random 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 Table 7: The score performance of LLMs on the benchmark levels with standard error. Model LV LV 2 LV 3 LV 4 LV 5 Average Language-Only 0.92 0.01 GPT-4.1 0.91 0.02 Gemini-1.5-Pro 0.90 0.04 Claude 3.7 Sonnet 0.48 0.02 Llama 3.2 90B 0.28 0.02 Llama 3.2 11B Qwen 2.5-VL 72B 0.87 0.04 0.11 0.01 Qwen 2.5-VL 7B Vision-Language 0.86 0.01 GPT-4.1 0.85 0.05 Gemini-1.5-Pro 0.85 0.02 Claude 3.7 Sonnet 0.44 0.06 Llama 3.2 90B 0.31 0.02 Llama 3.2 11B Qwen 2.5-VL 72B 0.79 0.04 0.21 0.02 Qwen 2.5-VL 7B 0.89 0.02 0.90 0.02 0.87 0.03 0.39 0.02 0.24 0.03 0.78 0.08 0.05 0.00 0.81 0.02 0.81 0.09 0.85 0.01 0.38 0.07 0.19 0.12 0.72 0.06 0.15 0.01 0.88 0.02 0.89 0.03 0.85 0.02 0.30 0.02 0.23 0.03 0.76 0.10 0.03 0.10 0.75 0.02 0.80 0.05 0.83 0.01 0.33 0.08 0.18 0.04 0.66 0.07 0.05 0.02 0.84 0.02 0.83 0.04 0.85 0.03 0.21 0.03 0.23 0.03 0.58 0.08 0.01 0. 0.68 0.03 0.73 0.05 0.80 0.02 0.31 0.04 0.13 0.04 0.54 0.02 0.04 0.09 0.75 0.02 0.82 0.02 0.79 0.02 0.20 0.03 0.22 0.06 0.51 0.08 0.01 0.01 0.66 0.02 0.67 0.04 0.79 0.02 0.30 0.09 0.11 0.03 0.43 0.04 0.01 0.01 0.86 0.02 0.87 0.02 0.85 0.02 0.32 0.04 0.24 0.02 0.70 0.05 0.04 0.01 0.75 0.03 0.77 0.03 0.82 0.02 0.35 0.03 0.18 0.02 0.63 0.04 0.09 0.04 Random 0.25 0.01 0.25 0.01 0.24 0.03 0.24 0.01 0.22 0.01 0.24 0. Table 8: The valid action rate performance of LLMs on the benchmark levels with standard error. Parameter Ape-X DQN PPO Optimizer Learning rate Other optimizer params Discount factor (γ) GAE λ Policy clip ratio n-step returns Train batch size SGD minibatch size Number of SGD iterations Training intensity Double-Q Target network update freq Replay capacity Prioritized replay α Prioritized replay β Initial / final ϵ Adam 0.0003 ε = 0.00015 3 64 8 true 20 000 1500000 0.5 1.0 1.0 / 0. Adam 0.0003 0.99 0.95 0.20 512 128 30 Table 9: Key hyperparameters for Ape-X DQN and PPO in our experiments. dash () indicates the parameter is not specified or not applicable for the corresponding algorithm."
        }
    ],
    "affiliations": [
        "Newcastle University, United Kingdom",
        "University of Auckland, New Zealand"
    ]
}