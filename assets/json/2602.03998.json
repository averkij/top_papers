{
    "paper_title": "AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology",
    "authors": [
        "Ahmed Alagha",
        "Christopher Leclerc",
        "Yousef Kotp",
        "Omar Metwally",
        "Calvin Moras",
        "Peter Rentopoulos",
        "Ghodsiyeh Rostami",
        "Bich Ngoc Nguyen",
        "Jumanah Baig",
        "Abdelhakim Khellaf",
        "Vincent Quoc-Huy Trinh",
        "Rabeb Mizouni",
        "Hadi Otrok",
        "Jamal Bentahar",
        "Mahdi S. Hosseini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection, or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model. The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning, matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 ] . e [ 1 8 9 9 3 0 . 2 0 6 2 : r AtlasPatch: An Eﬀicient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology Package: https://github.com/AtlasAnalyticsLab/AtlasPatch Ahmed Alagha1,2, Christopher Leclerc1, Yousef Kotp1, 5, Omar Metwally1, Calvin Moras1, Peter Rentopoulos1, Ghodsiyeh Rostami1, 3, Bich Ngoc Nguyen4, Jumanah Baig4, Abdelhakim Khellaf4, Vincent Quoc-Huy Trinh4,8, Rabeb Mizouni6, Hadi Otrok6, Jamal Bentahar6,2, Mahdi S. Hosseini1,5,7* 1Department of Computer Science and Software Engineering (CSSE), Concordia University, Montreal, QC, Canada, H3G 1M8. 2Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montreal, QC, Canada, H3G 1M8. 3Department of Building, Civil, and Environmental Engineering, Concordia University, Montreal, QC, Canada, H3G 1M8. 4University of Montreal Hospital Center (CHUM), Montreal, QC, Canada, H2X 0C1. 5MilaQuebec AI Institute, Montreal, QC, Canada, H2S 3H1. 6Department of Computer Science, Khalifa University, Abu Dhabi, UAE. 7Department of Pathology, McGill University, Montreal, QC, Canada, H3A 2B4. 8Institute for Research in Immunology and Cancer, University of Montreal, Montreal, QC, Canada, H3T 1J4. *Corresponding author(s). E-mail(s): mahdi.hosseini@concordia.ca; Contributing authors: ahmed.alagha@mail.concordia.ca; christopher.leclerc@mail.concordia.ca; yousef.kotp@mila.quebec; omar.abdelwahed@mail.concordia.ca; calvin.moras@mail.concordia.ca; peter.rentopoulos@mail.concordia.ca; rose.rostami@mail.concordia.ca; bich.ngoc.nguyen.med@ssss.gouv.qc.ca; jumanah.baig@mail.concordia.ca; abdelhakim.khellaf@umontreal.ca; quoc-huy.trinh@umontreal.ca; rabeb.mizouni@ku.ac.ae; hadi.otrok@ku.ac.ae; jamal.bentahar@ku.ac.ae; These authors contributed equally to this work. Abstract Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology workflows. This remains major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection, or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an eﬀicient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatchs tissue detection module is trained on heterogeneous and semi-manually annotated dataset of 30,000 WSI thumbnails, using eﬀicient fine-tuning of the Segment-Anything model. The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly 1 into common image encoders for embedding or store patch images, all eﬀiciently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning, matching state-of-the-art performance while operating at fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch. Keywords: Computational Pathology, WSI Preprocessing, Tissue Detection, Patch Extraction"
        },
        {
            "title": "1 Introduction",
            "content": "Modern pathology is increasingly transitioning from conventional microscopy to digital workflows enabled by whole slide images (WSIs), which are high-resolution digital scans of conventional glass tissue slides [1]. WSIs support scalable archiving, rapid retrieval, and seamless multi-resolution viewing from slide-level context to cellular detail [2], and these capabilities helped unlock modern deep learning (DL) for tasks such as tumor detection, grading, prognosis, and biomarker discovery [3]. Because gigapixel WSIs exceed the practical limits of most neural network architectures, with large fraction of each slide being non-tissue background, analysis typically proceeds on small tissue patches, with slide-level diagnoses inferred via multiple-instance learning (MIL) or other aggregation methods [4 6]. Naïve tiling of such large-scale WSIs produces millions of candidate patches per cohort, much of it being background, inflating I/O, memory and wall-clock time. The scaling demands are magnified by the emergence of pathology foundation-models which require billions of diverse patches spanning organs, stains, scanners, and magnifications to achieve robustness [7]. Eﬀicient tissue detection coupled with high-throughput patch extraction is therefore not merely an implementation detail but hard constraint on the pace, cost, and reach of computational pathology. Current slide-preprocessing pipelines detect tissue then tessellate WSIs into fixed-size patches for downstream tasks [4, 813]. Tissue detection is typically implemented either with simple heuristics (e.g., color thresholding and morphological filtering) or with deep-learning segmenters like U-Net [14] and U-Net++ [15]. Thresholding-based tissue detectors, such as HistoQC [16, 17], dplabtools [12, 13], EntropyMasker [18, 19], TIAToolbox [11] and the WSI processor in CLAM [4, 8] often require manual tuning across different cohorts by setting appropriate thresholds, and commonly fail under stain/tissue variations or artifacts despite being computationally fast. Deep learning-based pipelines, such as PathML [20, 21] and TRIDENT [9, 10, 22] improve robustness via patch-level inference to identify tissue and background patch images, but scale poorly (hundredsthousands of forward passes per WSI). Patch-wise detection can also miss global slide context (tissue geometry, background structure), leading to inconsistent boundaries after stitching. Moreover, many of the existing toolkits offer limited end-to-end parallelization, with several modules (such as WSI fetching, patch coordinate extraction, 2 patch image writing) executed serially or with only coarse-grained parallelism. In practice, these factors make standard slide preprocessing acceptable in small studies, but increasingly expensive at foundation-model scale. Here we introduce AtlasPatch (Fig. 1a), high-throughput slide-preprocessing toolkit that addresses key bottlenecks in tissue detection and patch extraction while preserving downstream performance. AtlasPatch produces clean thumbnail-level tissue masks that retain informative tissue and suppress common artifacts (e.g., pen marks). We show that thumbnail-based tissue detection is suﬀicient for downstream MIL without measurable loss, while generalizing across variations in illumination, tissue fragmentation, heterogeneity, and boundary definition. AtlasPatch is an end-to-end modular preprocessing pipeline comprising four components: tissue detection, patch-coordinate extraction, patch embedding with widely used general-purpose and medical image encoders, and optional patch image export, all with simple Python interface for practical use with minimal setup. To ensure robustness, we curated large, heterogeneous multi-cohort corpus spanning organs, institutions, scanners, and tissue conditions (Fig. 1b) and generated high-quality thumbnail masks through structured semi-manual annotation via Labelbox [23]. Using this dataset, we apply selective fine-tuning on the Segment Anything 2 (SAM2) [24] model by freezing the backbone and updating only normalization layers (Fig. 1c), enabling eﬀicient adaptation with reduced training memory/time. AtlasPatch then batches thumbnails for GPU-parallel inference, vectorizes contours at thumbnail resolution, extrapolates them to target magnifications via WSI pyramid metadata, and generates patch grids/coordinates entirely in contour space to prune background and artifacts. Coordinate generation and patch I/O are parallelized over multicore CPUs, with optional GPU-accelerated embedding via common medical and general-purpose encoders. Across multi-organ downstream classification tasks (kidney, lung, breast, colorectal) on public and in-house datasets, AtlasPatch-derived patches achieve downstream performance (via multi-instance learning (MIL)) comparable toand sometimes exceedingwidely used tools, while dramatically reducing runtime by up to 16-fold. Together, these design choices make AtlasPatch computationally eﬀicient backbone for foundation-model-scale pathology workflows, where preprocessing can otherwise dominate runtime and cost."
        },
        {
            "title": "2 Results",
            "content": "2.1 Curated multi-cohort thumbnail dataset We assembled large corpus of roughly 36,000 WSI thumbnails to train and evaluate AtlasPatch, drawing samples from four centers (in-house from University of Montreal Hospital Center (CHUM), TCGA, Radboud UMC and Karolinska Institute) and spanning multiple organ systems (Fig. 2a). This multiinstitutional composition captures diversity in laboratory protocols, scanners, and acquisition settings. The corpus is predominantly H&E-stained, and includes small subset of 150 IHC-stained slides from the in-house CHUM cohort, retained to broaden stain diversity. We also ensured that the thumbnails 3 Fig. 1: Overview of the AtlasPatch pipeline, dataset curation, and eﬀicient finetuning of the tissue detection model. A. AtlasPatch slide-preprocessing pipeline. Following the acquisition of diagnostic WSIs as multi-resolution pyramids, AtlasPatch utilizes the thumbnail for tissue detection using the finetuned SAM2 model, then extrapolates the resulting contour into the desired high resolution. This is followed by computing patch coordinates for downstream patch image export or feature embeddings. B. Diverse dataset curation and semi-manual annotation for training the tissue detector. The model in AtlasPatch is finetuned on heterogeneous multi-organ corpus combining multiple public cohorts as well as private in-house WSIs. Slides from each dataset are curated, and annotators performed semi-manual boundary tracing of tissue on WSI thumbnails under standardized protocol using Labelbox, yielding large annotated dataset (30k pairs) of tissue-versus-background masks. C. Thumbnail-based eﬀicient finetuning of SAM2. The segmentation model is trained on the curated thumbnails dataset by only finetuning the normalization layers of the SAM2 model, which represent approximately 0.076% of the parameters in the hiera-tiny variant of SAM2. cover broad tissue variations (Fig. 2b) in terms of aspects like tissue coverage (percentage of the slide covered by tissue), boundary definition (contrast across the tissue boundary), object count (fragmentation), and global appearance descriptors on tissue pixels (mean brightness, hue entropy/heterogeneity, and colorfulness). We quantified this diversity using slide-level statistics derived for the aforementioned attributes (Fig. 2c). From the Tissue chroma distribution, we notice that tissue pixels cover wide region in the a*b* plane, with a* skewed strongly positive (red/magenta) while b* still spans both blue/purple (b*<0) and yellow/brown (b*>0) components. This pattern is consistent across lightness (L*) strata, although the spread varies with illumination level. This broad but structured spread mirrors the staining and acquisition variability encountered in routine practice, supporting the diversity of our slide collection. The histograms further confirm this real-life variance across tissue geometry and appearance. Tissue percentage exhibits wide spread, with many low-coverage slides (sparse biopsies/small fragments) and progressively fewer high-coverage cases (large resections). Object count is strongly long-tailed, indicating that while many thumbnails contain only few connected tissue components, non-trivial subset is highly fragmented into many islands. Edge definition (signed Michelson contrast) is concentrated near zero with long negative tail, where negative indicates brighter background than tissue, values near zero reflect weak boundary contrast, and larger-magnitude negatives correspond to cleaner tissuebackground separation. Finally, brightness, hue entropy (heterogeneity), and colorfulness show broad and largely unimodal distributions, consistent with the variability in stain intensity, color complexity, and acquisition conditions encountered in routine WSI practice, supporting the diversity of the slide cohort. To obtain reliable masks for training the tissue detector, we set up semi-manual annotation workflow in Labelbox [23], where annotators manually refined automated masks generated by the tool or drew masks directly on the tissue regions, and dedicated quality controller iteratively reviewed and returned ambiguous cases for revision (Fig. 2d). The resulting corpus provides dense tissue background supervision designed to support robust, cross-institution generalization. Thumbnails and annotations were vetted by board-certified pathologist. Altogether, this highly variable and carefully annotated dataset is central to the design of slide preprocessing tool that is robust to the diverse slide appearances encountered in downstream pipelines. 2.2 Accurate tissue detection across organs, cohorts and artifacts We evaluated the AtlasPatch tissue detector on held-out test set from our diverse dataset. Fig. 3 shows representative thumbnails alongside ground-truth masks, AtlasPatch predictions, and overlays (ground truth in green, predictions in red, and overlap in brown). It can be seen that the overlays are dominated by overlap across the different cohorts and organs. Beyond cohort and organ shifts, AtlasPatch handles substantial variation in tissue geometry, accurately segmenting both large contiguous resections and highly fragmented biopsies with many small tissue islands while largely excluding background. In some cases, the model predictions even improve on the ground truth mask, as seen in the top Camelyon17 sample in Fig. 3, where the predicted mask excludes white background region that was missed by the annotators. Furthermore, we show cases with common scanner and preparation artifacts (e.g., ink and other non-tissue structures). It can be seen that the tissue detector in AtlasPatch confines its predictions to biologically plausible tissue regions and eﬀiciently excludes most artifactual structures. We attribute this combination of cross-cohort generalization, robustness to diverse tissue complexities and geometries, improvements beyond annotations, and artifact suppression to two main factors: Fig. 2: a. Composition of the 36,000 WSI thumbnail corpus across four cohorts (CHUM in-house, TCGA, Radboud UMC and Karolinska), illustrating multi-institutional and multi-organ coverage. b. Example thumbnails ordered along key axes of variation, highlighting challenging edge cases for tissue detection. c. Quantitative characterization of this variability via Lab chroma maps and slide-level statistics, showing broad distributions. d. Semi-automatic annotation pipeline in Labelbox, where pre-segmentation is refined by annotators with quality control, yielding high-quality tissueversus-background masks for SAM2 finetuning and evaluation. An example illustrates the ineﬀiciency of the automated segmentation features in such tools, and the need for semi-manual annotations. (i) the breadth of our multi-institutional training corpus and the eﬀiciency of the annotation process, in which masks explicitly exclude common non-tissue artifacts and span wide range of tissue presentations, and (ii) the eﬀicient finetuning strategy that updates only small subset of model parameters (details in Methods), coupled with the pretrained SAM2 backbone. 2.3 Tissue detection performance against existing slide-preprocessing tools To validate the tissue detector in AtlasPatch, we benchmarked its performance against widely used thresholding-based methods (TIAToolbox [11], CLAM [4], dplabtools [12], EntropyMasker[18], HistoQC [16]), zero-shot SAM2 baseline (without finetuning), and recent deep learningbased methods (Trident-GrandQC [9, 25], Trident-Hest [9]), by showing their predicted masks for the given WSIs. The Trident tool comes with two variants that both operate at the patch-level, with Trident-GrandQC operating on thumbnail patches, while Trident-Hest operating on high magnification (10x) patches. Fig. 4 summarizes these comparisons qualitatively on representative slide thumbnails drawn from diverse 6 Fig. 3: Qualitative examples of annotated masks as well as AtlasPatch tissue masks across datasets and artifacts. Representative WSI thumbnails from multiple sources are shown alongside their annotated ground-truth tissue masks, AtlasPatch predictions, and mask overlays. Ground-truth tissue is displayed in green, model predictions in red, and overlapping regions in brown, illustrating close agreement between AtlasPatch and annotations across organs and cohorts. The bottom row shows challenging cases with scanner or preparation artifacts, where AtlasPatch masks largely follow true tissue while ignoring non-tissue structures. These samples come from testing set that was not seen by the model during training. More examples are available in Extended Data Figs. 1-2 showing different cohorts and IHC stained samples. 7 organs and tissue conditions (brightness, fragmentation, sparsity, scanner artifacts, and pen markings), and quantitatively using segmentation metrics on held-out test set and runtime-performance analysis. Thresholding-based methods show limited robustness to the heterogeneity present in WSIs. Across multiple samples in Fig. 4, CLAM and TIAToolbox can miss substantial tissue when contrast or stain appearance deviates from typical cases (samples 3, 4, and 7), and they frequently fail to distinguish artifacts from tissue (samples 7-9). Consistent with these qualitative observations, we found that the CLAM tool fails to detect any tissue at all in roughly half of the prostate WSIs from the PANDA dataset. The pretrained (non-finetuned) SAM2 model is also not reliable out-of-the-box tissue detector. Without histopathology domain exposure, it inconsistently captures tissue and can include visually salient non-tissue patterns, reflecting the gap between the generic object detection in SAM2 and the more subtle cues required for robust tissue detection. We notice patch-based deep learning methods offer improved robustness than thresholding-based baselines. However, Trident-GrandQC frequently misinterprets artifacts as tissue, particularly in slides with strong pen markings or scanner streaks. Trident-Hest shows more resilient behavior with respect to artifacts but can miss tissue at ambiguous boundaries (e.g. misses chunk of tissue in sample 7). While these methods operate at the patch level and are expected to recover fine-grained tissue details, they can lose global context which sometimes could be beneficial to classify ambiguous regions. In contrast, AtlasPatch delivers consistent tissue masks across all the scenarios shown. It accurately outlines tissue in low-contrast slides, preserves small tissue fragments in highly fragmented biopsies and effectively suppresses non-tissue artifacts. While AtlasPatch does not always reproduce every fine-grained internal hole within tissue regions, it systematically captures the full spatial footprint of biologically meaningful tissue and avoids large false negatives. Quantitatively, on held-out test set of 3,000 WSIs, AtlasPatch matches or exceeds competing tools across accuracy, precision, recall, F1 and IoU  (Fig. 4)  . Here, precision measures how much of what method labels as tissue is truly tissue, whereas recall captures how much of the annotated tissue is successfully recovered, with F1 being their harmonic mean and IoU directly measures the overlap between predicted and ground-truth tissue masks. These metrics are more meaningful compared to pixel accuracy, which can be inflated by the large background regions. Particularly, AtlasPatch has strong precision (0.986), slightly higher than the much more complex Trident-Hest (0.983), dplabtools (0.977), and HistoQC (0.985) as the nearest competitors. Despite outperforming competing methods across all metrics, we highlight precision because pixel-level ground truth can be ambiguous at tiny internal, and methods may correctly exclude these regions as background, which can be unfairly penalized in accuracy and recall, whereas precision primarily reflects whether predicted tissue pixels are truly tissue. The computational complexity plot further shows AtlasPatch on favorable performance-complexity frontier. Heuristic/thresholding methods are fast but less robust, whereas patch-based deep models 8 Fig. 4: Quantitative and qualitative analysis of AtlasPatch tissue detection against existing slide-preprocessing tools. Representative WSI thumbnails are shown from diverse tissue features and artifact conditions, with tissue masks predicted by thresholding methods (TIAToolbox, CLAM) and deep learning methods (pretrained non-finetuned SAM2 model, Trident-GrandQC, Trident-Hest, and AtlasPatch), highlighting differences in boundary detection, artifact suppression, and handling of fragmented tissue (more tools are shown in Extended Data Figs. 3-5). Tissue detection performance is also shown on the held-out test set, highlighting that AtlasPatch matches or exceeds the other methods. The segmentation complexityperformance trade-off shows F1-score against segmentation runtime (per 100 WSIs), shows AtlasPatch achieves high performance with substantially lower wall-clock time than patch-based methods, underscoring its suitability for large-scale WSI preprocessing. 9 incur substantially higher runtime due to processing many (up to thousands) patches per slide. By operating directly at thumbnail resolution with single SAM2-based forward pass and an eﬀicient finetuning strategy on diverse dataset, AtlasPatch achieves high segmentation quality with markedly lower wall-clock time than patch-wise detectors (2.6 faster than Trident-GrandQC and 20 faster than Trident-Hest), supporting scalable WSI preprocessing. 2.4 Impact of training data diversity on AtlasPatch generalization To test robustness, we created homogeneous training splits from our corpus by restricting data to single scanner (Hamamatsu/Aperio/Philips) or to low/medium/high strata of key slide attributes (brightness, object count, edge definition, entropy and tissue percentage). We used each split to finetune SAM2 and evaluated the model on the remainder of the dataset outside that split (Fig. 5a). As expected, training on these narrow sets yields large performance swings across strata, with precision drops of up to 13.5% for machine-, 7.3% for brightness-, 10.8% for object count-, 5.6% for edge definition-, 6.7% for entropy-, and 44.4% for tissue percentage-based training. In contrast, when we train on the full heterogeneous multi-institutional corpus and then test on homogeneous subsets (unseen during training), the range (max-min) in precision is minimal, with 1.9% for machine-, 0.71% for brightness-, 0.86% for object count-, 0.49% for edge definition-, 0.88% for entropy-, and 4.03% for tissue percentagebased testing sets. The slightly larger range for tissue-percentage subsets is expected because in cases of slides with small tissues, even small extra predicted regions (false positives) make up larger fraction of the predicted mask and therefore reduce precision more noticeably. These results validate our choice to train on deliberately heterogeneous multi-cohort dataset, which is essential for reliable generalization across real-world WSI variability. 2.5 Ablation on training configuration and model backbone size To assess sensitivity to implementation choices, we ablated AtlasPatchs training setup by varying input resolution (2562048), batch size (116), learning rate (1105 - 5103), and model size (tinylarge), with the default setup having input size of 1024, batch size of 2, learning rate of 5104, and tiny model size. The above variations produced only small changes in performance, with precision remaining between 0.984-0.989 and F1 between 0.985-0.988, indicating the tissue detector is robust to reasonable hyperparameter choices. The best overall trade-off is achieved at 10241024, batch size 2, and learning rate 5104, which we adopted as our default. This is aligned with SAM2 native operating scale (1024 input size), avoiding unnecessary resampling. When varying the backbone from tiny (38.9M parameters) to small/base-plus/large (up to 224.4M), larger models yield marginal gains (precision and F1 increase by 0.002 and 0.001, respectively) while incurring 6 increase in parameter count and substantial runtime overhead, motivating SAM2.1_hiera_tiny as the default AtlasPatch backbone 10 Fig. 5: Multiple experiments to validate the importance of data diversity in the curated dataset. A. Effect of restricted/unvaried training sets. Subsets of the original dataset are created for varying conditions (machines, tissue brightness, object count, etc). For each subset, the SAM2 model is finetuned on the given data, and tested on the remainder of the dataset. Results show training on restricted set with no diversity cannot generalize to larger heterogeneous data, as shown by the six examples. B. Model performance across varying test sets. The model finetuned on the full heterogeneous training set is tested on stratified subsets (unseen during training), each covering certain level of given feature. Results show more adaptability and robustness to variance. 2.6 Performance on Downstream Tasks We next assess the quality of the extracted patches on downstream slide-level prediction tasks via MIL while also reporting the computational complexity in terms of runtime, as shown in Fig. 7. To this end, we compare AtlasPatch to three representative slide-preprocessing pipelines: the two Trident variants (GrandQC and Hest) and CLAM. We separately use each tool to extract patch images for given dataset to represent WSIs, and feed those with their labels to MIL to evaluate slide-level performance on six classification tasks. Performance is summarized using accuracy, precision, recall, F1 score, and ROC AUC, while monitoring the average number of extracted patches per slide for each tool. Across the six different downstream tasks, patches extracted via AtlasPatch achieve slide-level performance that is effectively on par with the best competing pipelines. Specifically, for the PANDA, LUADvsLUSC, and KIRCvsKIRP tasks, AtlasPatch achieves the highest performance with accuracies of 73.5%, 95.8%, 11 Fig. 6: Ablative study on the tissue detection component in AtlasPatch under different training configurations and backbone size. Tissue detection performance is reported while varying key design choices, including input thumbnail resolution, training batch size, learning rate and the underlying SAM2 backbone variant. Across all tested settings, AtlasPatch maintains consistently high performance with only minor fluctuations, indicating that the method is robust to reasonable hyperparameter changes and that even lighter SAM2 backbones achieve competitive results, allowing practitioners to trade off memory and runtime against only marginal differences in segmentation quality. and 97.7%, respectively. For invasiveness and dysplasia, AtlasPatch achieves accuracies of 98.6% and 96.6%, respectively, remaining within at most 0.2% of the highest performing pipeline. On BRCA, it also stays competitive, with 93.7% accuracy compared to 94.4% for CLAM, while using substantially fewer patches per WSI, nearly one third of those produced by CLAM containing lot of background. Generally, we notice AtlasPatch achieves the aforementioned performance while producing, on average, lower number of patches per WSI (3047 patches/WSI), compared to 8976, 3204, and 3092 patches/WSI for CLAM, GrandQC, and Hest, respectively, indicating it yields more focused and information-dense patches with less redundant background, thereby lowering the computational and storage burden on downstream models. On average for the six tasks, AtlasPatch yields 3075 patches per slide, compared to 8976, 3093, and 3204 for CLAM, Trident-Hest, and Trident-GrandQC, respectively. AtlasPatch clearly distinguishes itself in runtime eﬀiciency. Under the same circumstances and using the same computational infrastructure, it achieves the aforementioned competitive performance with fraction of the computational complexity, when compared to the existing benchmarks. The plots in Fig. 7 report the patch coordinate extraction runtime (inclusive of tissue detection) for 100 randomly sampled slides, and matches it with the MIL accuracy achieved in each task. When compared to the slide preprocessing tool in CLAM, AtlasPatch processes 100 slides for patch coordinate extraction in 12 Fig. 7: Downstream MIL performance and complexity of several end-to-end WSI preprocessing tools on 6 different tasks: colorectal cancer invasiveness (in-house), dysplasia level (in-house), breast carcinoma subtyping (TCGA-BRCA IDC vs ILC), renal cell carcinoma subtyping (TCGA-KIRC vs KIRP), lung adenocarcinoma versus squamous carcinoma (TCGA LUAD vs LUSC) and prostate cancer grading (PANDA). The table reports slide-level metrics and the average number of patch embeddings per WSI. For each task, we train several MIL methods on the patches extracted from each tool, and report the average results (more details in Appendix). The scatter plots show complexityaccuracy trade-offs for each task. Across all datasets, AtlasPatch achieves comparable or better downstream performance while using fewer embeddings on average and substantially lower runtime. Note: for the PANDA dataset, we exclude CLAM because it fails to detect tissue in 54% of the slides. 195.51 seconds, which more is than 2 faster than CLAM and Trident-GrandQC, and more than 16 faster than Trident-Hest."
        },
        {
            "title": "3 Discussion",
            "content": "WSI preprocessing, being tissue detection followed by patch extraction, is often treated as routine step that is taken for granted, yet it can become the dominant constraint when scaling to foundation-model training. AtlasPatch addresses this preprocessing tax by adopting thumbnail-based design that detects tissue at low resolution and extrapolates contours to generate patch coordinates at the desired magnifications, avoiding repeated high-resolution reads and patch-level inference. Across qualitative stress cases, AtlasPatch produced consistent tissue masks under low contrast, heavy fragmentation, and common artifacts, conditions under which thresholding and patch-based deep learning pipelines often 13 mishandle. AtlasPatch did not always reproduce every fine-grained internal hole within tissue regions, but it reliably recovered the full spatial footprint of diagnostically meaningful tissue and avoided large false negatives, as shown when we analyzed the models precision. This eﬀiciency is enabled by pairing the approach with deliberately heterogeneous dataset and parameter-eﬀicient SAM2 adaptation strategy, which together improve robustness across varying conditions without incurring heavy training or deployment overhead. In contrast, pipelines that adopt deep segmenters at the patch level add complexity and compute through dense patch scanning and stitching, and their generalization can be constrained by limited training data (GrandQC [25] was trained on patches from only 600 WSIs) as illustrated in Fig. 4. Importantly, AtlasPatch does not trade accuracy for speed where it matters most: downstream tasks. Across diverse MIL tasks  (Fig. 7)  , AtlasPatch-derived patches achieve slide-level performance comparable to strong baselines while substantially reducing end-to-end runtime and redundant background-heavy sampling (more than twice as fast as Trident-GrandQC and 16 faster than TridentHest). In this sense, better tissue detection translates into more focused search space for downstream models, which is especially relevant when storage, runtime, and encoder time become limiting factors at scale. Beyond performance, AtlasPatch was designed for ease of use by both researchers and pathologists. The pipeline is presented as four modules, each with its checkpoint and human-readable commands (e.g., tissue visualization only, coordinate extraction, full processing with patch embeddings, and optional patch export), allowing users to check out at any stage depending on whether they need quick QC overlays, patch coordinates for MIL, or ready-to-train embeddings. Outputs are standardized (mask/contour/grid visualizations for rapid review, and per-slide HDF5 files containing coordinates and features), enabling straightforward integration into existing analysis workflows and reproducible reruns. Practical deployment is supported through simple single-slide or directory-level execution, explicit CPU/GPU controls, and robust run management (e.g., skipping completed slides and resumable outputs), alongside optimized parallel execution across modules for patch I/O, model calls, and coordinate extraction on CPU and GPU when applicable. AtlasPatchs eﬀiciency and modularity are especially valuable as computational pathology pivots toward foundation models, which require unprecedented data throughput and flexible preprocessing at scale [26]. Major foundation models exemplify this trend with massive dataset sizes, such as HIPT [27] (104M image patches), RudolfV [7] (134k WSIs 1.2B patches), PLIP [28] (208k image-caption pairs), UNI [29] (100k WSIs 100M patches), and CONCH [30] (more than 1.17M image-caption pairs). This scale comes with formidable runtime costs, as repeatedly extracting patches at multiple magnifications or patch sizes becomes severe I/O and computation bottleneck at such magnitudes. This is only the tip of the iceberg, as upcoming years will likely demand even larger models and bigger datasets, further amplifying the strain on preprocessing pipelines. Eﬀicient and scalable tools 14 like AtlasPatch will be essential to keep pace with these demands and unlock the full potential of foundation models in pathology. Although AtlasPatch streamlines WSI tissue segmentation and patch extraction, manual annotation remains key bottleneck that required substantial human effort and manpower. One practical future direction is to incorporate active learning, which helps prioritize uncertain or diverse WSI thumbnails for labeling to reduce the amount of annotation needed [31]. This opens the door to even larger and more diverse cohorts for training tissue detectors. Another potential direction is the tool integration into routine clinical QC workflows. While AtlasPatch is already designed to be easy to use, with modular stages, clear checkpoints, and standardized visual overlays, wider adoption will benefit from dedicated pathologist-friendly GUI components and tighter integration with slide viewers and QC tools, enabling rapid review, flagging, and lightweight corrections that can be fed back into model updates."
        },
        {
            "title": "4 Methods",
            "content": "4.1 Data gathering and curation To train the tissue detection component in AtlasPatch, we assembled multi-cohort corpus of WSIs comprising 35,827 slides from 11 datasets. Two large in-house cohorts were collected from the Centre hospitalier de lUniversité de Montréal (CHUM) in QC, Canada (institutional review board approved project 2025-12351), including 3,151 pancreas WSIs and 15,736 digestive-system WSIs scanned as part of routine clinical practice. Public data were obtained from the Camelyon17 challenge [32] for breast cancer metastases in WSIs of histological lymph node sections (1,000 slides), the PANDA prostate cancer challenge [33] (10,616 slides), and eight projects from The Cancer Genome Atlas (TCGA) [34], namely BLCA, BRCA, CRC, KIRC, KIRP, LUAD, STAD and UCEC, covering bladder, breast, colorectal, kidney, lung, stomach and uterine carcinomas (5,324 slides in total). Across cohorts, slides were digitized as multi-resolution pyramids at maximum optical magnification of 20 or 40, with heterogeneous scanning hardware and acquisition settings, reflecting real-world variability in clinical workflows. For the in-house CHUM cohorts, diagnostic WSIs and associated metadata were retrieved from the institutional digital pathology archive under appropriate research agreements, deidentified before export, and screened to exclude corrupted files. Public WSIs were downloaded from their oﬀicial portals and analyzed for the removal of any corrupt files. The corpus is predominantly H&E-stained, and includes small subset of 150 IHC-stained slides from the in-house CHUM cohort, retained to broaden stain diversity. To construct the thumbnail dataset used for tissue detection, WSIs were read using OpenSlidecompatible [3537] libraries and we extracted, for each slide, the coarsest non-empty pyramid level provided by the scanner. Thumbnails were stored as high-quality image files together with metadata indicating cohort, organ system, scanner vendor and maximum magnification. No stain normalization 15 or color augmentation was applied at this stage, so that the subsequent segmentation model would be exposed to the natural variability of laboratory protocols and scanners. We created ground-truth tissue-versus-background masks on these thumbnails via structured semi-manual workflow in Labelbox [23]. The collected dataset was split on three annotators, who initially underwent several training rounds on pilot slides, during which their annotations were systematically inspected by quality-control (QC) reviewer and aligned through feedback to ensure consistent interpretation of the guidelines. Within Labelbox, annotators primarily relied on the AutoSegmentBox tool to identify tissue regions by drawing bounding boxes at multiple scales to capture both large tissue areas and finer structures. The resulting automatic masks were then carefully refined with the Pen tool to adjust boundaries, recover small tissue islands, and remove artifacts. By combining AutoSegmentBox at different zoom levels with targeted Pen-based labeling, annotators eﬀiciently produced detailed tissue masks. An initial pass of annotations was followed by QC step in which more senior reviewer inspected each mask, requesting corrections in cases of underor over-segmentation or inconsistent inclusion of artifacts. This process yielded approximately 30,000 high-quality thumbnailmask pairs spanning varying organs, cohorts, and acquisition conditions. To quantify the diversity of the corpus, we computed set of slide-level statistics, as reported in Fig. 2. Using the binary tissue mask, we measured tissue coverage, defined as the fraction of thumbnail pixels labeled as tissue; object count, defined as the number of connected tissue components after mild morphological cleanup to remove tiny specks; and boundary definition, contrast-based index computed as the Michelson contrast between mean grayscale intensities just inside and just outside the tissue boundary, with higher magnitudes indicating sharper separation between tissue and background. On the RGB thumbnail restricted to tissue pixels, we derived global appearance descriptors including mean brightness, computed by converting the thumbnail to grayscale, averaging pixel values within the tissue mask and normalizing by 255 to obtain 01 score; heterogeneity, quantified as the Shannon entropy of the tissue hue histogram; and colorfulness, using the HaslerSüsstrunk opponentcolor metric on tissue pixels. To further characterize color variability, we additionally constructed Lab chroma map by randomly sampling 1% of tissue pixels from each thumbnail in the full dataset, converting their RGB values to CIE Lab, and plotting their distribution in the a*b* plane for different lightness levels. For each scalar statistic, we computed slide-level histograms over the full corpus. These histograms, shown in Fig. 2c, were also used to define low/medium/high bins when constructing stratified subsets in Fig. 5. For training the tissue detection backbone (SAM2), we applied an 80:10:10 split on the full set of 30,000 annotated thumbnails, yielding training, validation and internal test subsets for tissue detection. For the downstream MIL benchmarks, each MIL method was evaluated using 10-fold crossvalidation on each slide-level dataset. 16 4.2 Tissue detection via SAM Segment Anything Model 2 (SAM2) [24] is the state-of-the-art vision foundation model designed for prompt-based image and video segmentation. Building upon the success of the original SAM [38], SAM2 introduces fully trainable vision transformer architecture equipped with streaming memory for real-time and temporally consistent segmentation. It employs modular encoderdecoder design with hierarchical vision transformer backbone that eﬀiciently processes multi-scale features while maintaining global receptive field. The architecture consists of four main components:(i) an image encoder, (ii) prompt encoder, (iii) mask decoder, and (iv) memory module comprising memory attention and memory bank. The image encoder projects each frame into high-dimensional patch embeddings and extracts contextual features across multiple scales. The prompt encoder embeds user inputs, such as points, bounding boxes, or coarse masks, into latent space aligned with the image representations. The mask decoder fuses these image and prompt embeddings via cross-attention to produce one or multiple segmentation masks, each associated with confidence score. Its primary innovation is streaming memory mechanism for temporally consistent video segmentation. However, AtlasPatch operates on independent WSI thumbnails, and therefore uses SAM2 in an image-only setting without leveraging the memory bank or temporal propagation. We nonetheless adopt SAM2 (rather than the original SAM) because it is pretrained at larger scale and provides 6 faster inference than SAM due to its more eﬀicient image encoder backbone, being the Hiera hierarchical backbone instead of ViT-based encoder, making it better suited for high-throughput thumbnail segmentation in large WSI cohorts. SAM2s Hiera backbone is available in different sizes being tiny, small, base, and large. While SAM2 demonstrates strong generalization on natural images and even some medical imaging modalities, directly applying it to histopathology images is non-trivial. To fully leverage its rich learned representations and generalization strengths, the model must be adapted and fine-tuned to the WSI segmentation task. However, the adaptation of SAM2 to histopathological images presents challenges due to the distribution shift between natural and microscopic imagery. SAM2 is pretrained on billions of natural images where structures, colors, and object boundaries differ dramatically from histological tissue patterns. In WSIs, tissue and background boundaries can be subtle, textures are stochastic rather than object-like, and color distributions are dominated by H&E staining. Consequently, the pretrained features may fail to align with histopathological semantics unless adapted with domain-specific finetuning. With hundreds of millions of parameters, fully fine-tuning foundation models such as SAM2 is impractical. The relatively small scale of annotated histopathology datasets increases the risk of overfitting and catastrophic forgetting of the pretrained representations. Moreover, full parameter updates demand substantial computational resources and memory, making full fine-tuning both ineﬀicient and costly for large models. These limitations have motivated the development of lightweight and targeted 17 adaptation techniques referred to as Parameter-Eﬀicient Fine-Tuning (PEFT), which aim to achieve comparable performance to full fine-tuning while updating only small fraction of model parameters. In this work, the Layer Normalization Fine-Tuning strategy introduced in [39, 40] was adopted, which selectively updates only the aﬀine parameters of normalization layers, which are given as: LN(x) = γ µ(x) σ(x) + ϵ + β, (1) where denotes the input to an LN layer, µ(x) and σ(x) are the per-feature mean and standard deviation, γ and β are learnable parameters, and ϵ is small constant for numerical stability. During layer normalization fine-tuning, only γ and β are updated via backpropagation, effectively re-scaling and re-centering the pretrained feature activations to match the target WSI domain. The rationale behind the method is that Layer Normalization (LN) plays crucial role in transformer architectures by stabilizing feature distributions across tokens and enabling robust gradient propagation. In the context of domain adaptation, the aﬀine parameters of LN layers (scale γ and shift β) capture domain-specific feature statistics without altering the global representational structure. Thus, adapting only these small subsets of parameters can yield substantial domain alignment at minimal computational cost. The method has shown superior performance on the original SAM compared with alternative PEFT approaches such as LoRA and adapters, demonstrating strong adaptation with minimal parameter updates [40] . Building on these findings, the approach was used in this work for SAM2 to achieve eﬀicient and stable domain adaptation. In this work, the SAM2-Tiny variant was employed, which consists of approximately 40 million parameters, initialized with publicly released pretrained weights. The Hiera-tiny variant proved suﬀicient for the tissue detection task given our eﬀicient finetuning, as shown in Fig. 6. Fine-tuning was performed on the curated WSI thumbnailmask pairs. Each input image was resized to 1024 pixels and paired with its corresponding binary mask. During training, bounding-box prompt was automatically generated to encompass the entire image region, ensuring full spatial coverage and allowing the model to learn tissue segmentation without relying on manual or localized prompts. This prompt type provides simple yet effective supervision signal, enabling SAM2s prompt encoder to encode the global spatial context while allowing the mask decoder to delineate fine-grained tissue boundaries. The fine-tuning was conducted using mixed-precision training with batch size of 2, learning rate of 5 104, and the AdamW optimizer coupled with cosine-annealing learning rate scheduler to enable stable convergence. The loss function was weighted combination of Dice loss (LDice) and binary cross-entropy loss (LBCE), shown in [40] to effectively balance region overlap accuracy with pixel-wise consistency: Training was conducted for 50 epochs with early stopping based on the validation F1-score to prevent = 0.65LDice + LBCE. (2) overfitting. 18 In inference, the model processes each WSI thumbnail once using full-image bounding-box prompt to predict the tissue mask. This thumbnail-level segmentation eliminates the need for exhaustive tiling and enables fast and memory-eﬀicient ROI detection with negligible computational overhead. 4.3 AtlasPatch Full Tool Development and Implementation AtlasPatch is implemented as modular Python pipeline that processes each WSI through four sequential but optionally decoupled components: (i) tissue detection, (ii) tissue patch coordinate extraction, (iii) tissue patch embedding and (iv) tissue patch image export. single command-line call can execute the full pipeline end-to-end, but each component can also be invoked independently. Users may obtain only tissue masks, patch coordinates, patch image embeddings, or additionally export patch images for future use. All intermediate products (masks, coordinates, features and images) are written in consistent, slide-centric layout (HDF5 for coordinates and embeddings, PNG for optional patch images), allowing stages to be resumed or recombined. The implementation emphasizes streaming and parallel processing, in which slides are processed in parallel worker threads, and heavy operations (model inference and image embedding) are offloaded to GPUs. For tissue detection, WSIs are first converted to thumbnails at target objective power (default 1.25), using the scanners native pyramid levels and optional downscaling. When processing multiple slides, thumbnail preparation is parallelized across pool of worker threads so that I/O and image resizing overlap. The resulting thumbnails are then passed in batches to our fine-tuned SAM2 model, which runs batched segmentation and returns binary tissue-versus-background masks. Masks are stored and can be visualized as overlays on the original thumbnails for quality control. For tissue patch coordinate extraction, AtlasPatch converts each tissue mask to polygonal contours with explicit handling of holes and very small fragments, discarding tissue regions below configurable area threshold. The contours are mapped from thumbnail space to level-0 slide coordinates, and the tool automatically selects an appropriate pyramid level and read window size to match the requested target magnification. regular grid is then walked over each contours bounding box with configurable step size, and candidate patch locations are accepted only if central point or one of the probes (corners) lie within tissue and outside holes. In its default mode, the extractor yields only patch coordinates (without reading pixels) and writes them to per-slide HDF5 files along with metadata describing levelpatch footprint, magnification and overlap. When multiple slides are processed, each slides extraction runs in its own worker thread, with global cap on concurrently open WSIs for proper memory management and per-slide lock files to prevent duplicate work across processes. For patch embedding and image export, AtlasPatch reopens the HDF5 coordinate files, iterates over stored patch locations and re-reads the corresponding regions from the original WSIs at the predetermined size. Patch embedding is organized per extractor and per slide: for each chosen encoder from 19 the internal registry (including convolutional and transformer-based general-purpose and pathologyspecific models, as well as user-defined plugins). Per-slide lock files ensure that concurrent runs never corrupt feature groups and allow caching of existing embeddings to skip redundant computation. When the optional image-saving flag is enabled, RGB patches are dispatched to bounded thread pool for PNG encoding and disk writes while coordinate iteration continues, overlapping CPU-bound image I/O with ongoing slide processing. Taken together, this modular design makes AtlasPatch adaptable to wide range of users and workflows. Clinically oriented teams can rely on it purely as fast and robust tissue detector, while method developers can plug in their own encoders on top of the available standardized encoders. Largescale studies can leverage its batch processing, device configuration and parallel I/O to run eﬀiciently on workstations or HPC clusters. Because each stage is independently configurable and can be stopped or resumed, the tool can be easily tailored to new datasets and downstream tasks without changing the core implementation. 4.4 Downstream Tasks For the downstream evaluation, we considered six slide-level prediction tasks spanning both in-house and public cohorts, where multiple-instance learning (MIL) was used on bags of tissue patches to aggregate patch-level representations to predict slide label. For each task, WSIs were first processed by given preprocessing tool to obtain tissue patch coordinates and embeddings, and these bags were then passed to several MIL methods (ABMIL [41], CLAM [4], DFTD [42], DSMIL [43], MeanMIL [44], RRT [45], TransMIL [46], and WIKG [47]). The performance reported in Fig. 7 is averaged over MIL methods for each tooldataset pair. All patch images were embedded using the UNIv1 [29] foundation model. Two tasks were derived from an in-house digestive pathology cohort of colorectal biopsies and resections. The Invasiveness task defines binary label indicating the presence versus absence of invasive colorectal carcinoma on slide, as determined from the original pathology report. The Dysplasia task uses the same corpus but assigns dysplasia-level label (non-dysplastic, low-grade or high-grade dysplasia) at the slide level based on routine diagnostic grading in the report. Three tasks are based on The Cancer Genome Atlas (TCGA) cohorts. For breast carcinoma subtyping (BRCA), we used diagnostic H&E WSIs from the TCGA-BRCA project and restricted to cases annotated as invasive ductal carcinoma (IDC) or invasive lobular carcinoma (ILC). For renal cell carcinoma subtyping (KIRCvsKIRP), we combined WSIs from TCGA-KIRC (Kidney Renal Clear Cell Carcinoma) and TCGA-KIRP (Kidney Renal Papillary Cell Carcinoma) and assigned labels accordingly. For lung carcinoma subtyping (LUADvsLUSC), we analogously pooled TCGA-LUAD and TCGA-LUSC WSIs and labeled slides as lung adenocarcinoma or lung squamous cell carcinoma. Finally, the prostate cancer grading task uses the PANDA challenge dataset, consisting of digitized prostate biopsies. We adopted the oﬀicial slide-level Gleason grade group labels provided with PANDA metadata, treating grade group prediction as multi-class classification problem. 4.5 Baselines For tissue detection, we benchmarked widely used open-source baselines that span (i) classical thumbnail thresholding, and (ii) AI-based tissue detectors that operate on thumbnails or WSI tiles. For the thresholding methods, we evaluated the tissue detector shipped with CLAM [4, 8], the tissue masking utilities in TIAToolbox [11, 48, 49], and other commonly used pipelines including HistoQC [16, 17], dplabtools [12, 13], and EntropyMasker [18, 19]. HistoQC identifies tissue by thresholding low-resolution slide thumbnail for bright/dark content and color variance, then refines the mask using morphological operations to suppress background, adipose-like regions, and small artifacts. Dplabtools operates on user-specified slide magnification, generating binary tissue mask via thresholding in HSV/Lab space (or Otsu), with optional hole filling/dilation and pruning of small objects. TIAToolbox provides Otsuand morphology-based TissueMasker utilities that build tissue-versus-background masks from thumbnails, and integrates these masks directly into its patch extraction utilities so patches can be filtered based on tissue regions using either an automatically generated or user-supplied mask. Finally, although CLAM is primarily an end-to-end MIL pipeline, its patch extraction component is frequently used as standalone method. The tissue detection component in this method converts the slide to HSV, applies median filtering and thresholding (optionally Otsu), and extracts tissue contours while removing holes and small components via area-based contour filtering. For all these baselines, we used their corresponding public repositories and retained default parameters whenever configurable options were available to avoid cohort-specific tuning. Because not every baseline produces binary tissue mask exports as its final output (e.g., some return contours, probability maps, or patch-wise outputs), we added minimal adapters where needed so that all methods could be evaluated under common binary mask representation. For AI-based tissue detection, we benchmarked with SAM2 as plugand-play foundation model baseline to test how far strong generic segmenter can go without any domain adaptation. We applied SAM2 directly to WSI thumbnails and converted its predicted regions into tissue-versus-background masks. The TRIDENT [9, 10, 22] WSI processing tool provides several tissue detectors. We benchmarked against the GrandQC [25] and Hest [50] tissue detection variants in that tool. Both variants operate at the patch-level, where the model is fed tissue patch images and outputs the corresponding masks, which are later stitched to form slide-level mask. GrandQC operates on patches extracted from the WSI thumbnail, while Hest operates on patches extracted from higher magnification (10). 21 For end-to-end preprocessing pipelines (tissue detection through patch coordinate extraction), we focused on tools that support WSI loading, tissue detection, and coordinate export for downstream feature encoding and MIL. For both CLAM and Trident tools, at the user-specified magnification/patch size, patch extraction slides window over each contours bounding box at the chosen pyramid level, retains coordinates whose patch footprint lies within the tissue region, passes simple white/black heuristics for further filtering, and writes accepted patch coordinates into per-slide HDF5 bag files. Other tools considered in the tissue-detection benchmark were not included in the full-pipeline comparison because they either stop at tissue masking (i.e., do not provide complete coordinate export/patchification pipeline), or are older/out-of-date in ways that complicate large-scale reproducible WSI fetching and processing across heterogeneous cohorts. To ensure fair and reproducible comparisons, we standardized both compute settings and evaluation protocol across baselines. For AI-based tissue detection models, inference was executed on the same computational infrastructure using single RTX Ada 6000 GPU with fixed batch size of 32 and 4 workers, and segmentation performance was reported on held-out test set of 3,000 slides. Whenever thresholding tools allow for parallel workers, we use 4 workers. For computational complexity analysis, covering tissue segmentation runtime and, where applicable, full end-to-end preprocessing runtime, we again used the same infrastructure for all tools (RTX Ada 6000) and constrained each method to one GPU when GPU execution was supported. We further fixed downstream patch extractionrelated parameters across pipelines to 512512 patch size, 0 overlap, 20 magnification, batch size 32, and 4 workers. Runtime comparisons were conducted on shared subset by sampling 100 slides from the cohort and running every method on the exact same slide set, ensuring that both segmentation and pipeline-level complexity reflected differences in implementation rather than differences in data, hardware, or configuration."
        },
        {
            "title": "5 Data Availability",
            "content": "TCGA data can be downloaded from the GDC platform (https://portal.gdc.cancer.gov/). The CAMELYON17 dataset is available on the Grand Challenge data page (https://camelyon17.grand-challenge. org/Data/). The PANDA dataset can be downloaded from the Kaggle competition page (https: //www.kaggle.com/c/prostate-cancer-grade-assessment/data). In-house CHUM cohorts (pancreas and digestive-system) were used under institutional approvals and data-sharing agreements and are not publicly available."
        },
        {
            "title": "6 Code Availability",
            "content": "AtlasPatch is open-sourced at https://github.com/AtlasAnalyticsLab/AtlasPatch. The trained tissue detection model checkpoint is hosted on Hugging Face at https://huggingface.co/AtlasAnalyticsLab/ AtlasPatch. AtlasPatch automatically downloads/loads these weights from Hugging Face, but users 22 must provide valid Hugging Face access token to enable model retrieval. The use for commercial purposes is not permitted. Acknowledgments. This research was partially supported by the following grants: NSERC-DG RGPIN-2022-05378 [M.S.H], Amazon Research Award [M.S.H], Gina Cody RIF [M.S.H], the Canadian Cancer Society Breakthrough Grant [V.Q.H.T], FRQS-CRS-J1 [V.Q.H.T], institute for research in immunology and cancer start up funds [V.Q.H.T], and FRQNT scholarships [A.A and Y.K]. The experiments were enabled in part by support provided by Calcul Quebec (www.calculquebec.ca) and the Digital Research Alliance of Canada (www.alliance.can.ca)."
        },
        {
            "title": "References",
            "content": "[1] Hanna, M.G., Ardon, O., Reuter, V.E., Sirintrapun, S.J., England, C., Klimstra, D.S., Hameed, M.R.: Integrating digital pathology into clinical practice. Modern Pathology 35(2), 152164 (2022) https://doi.org/10.1038/s41379-021-00929-0 [2] Zia, S., Yildiz-Aktas, I.Z., Zia, F., Parwani, A.V., et al.: An update on applications of digital pathology: primary diagnosis; telepathology, education and research. Diagnostic Pathology 20, 17 (2025) https://doi.org/10.1186/s13000-025-01610- [3] Hosseini, M.S., Bejnordi, B.E., Trinh, V.Q.-H., Chan, L., Hasan, D., Li, X., Yang, S., Kim, T., Zhang, H., Wu, T., et al.: Computational pathology: survey review and the way forward. Journal of Pathology Informatics 15, 100357 (2024) [4] Lu, M.Y., Williamson, D.F.K., Chen, T.Y., Chen, R.J., Barbieri, M., Mahmood, F.: Data-eﬀicient and weakly supervised computational pathology on whole-slide images (2020). https://arxiv.org/ abs/2004. [5] Zhang, J., Nguyen, A.T., Han, X., Trinh, V.Q.-H., Qin, H., Samaras, D., Hosseini, M.S.: 2dmamba: Eﬀicient state space model for image representation with applications on giga-pixel whole slide image classification. In: Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 35833592 (2025) [6] Gadermayr, M., Tschuchnig, M.: Multiple instance learning for digital pathology: review of the state-of-the-art, limitations & future potential. Computerized Medical Imaging and Graphics 112, 102337 (2024) [7] Dippel, J., Feulner, B., Winterhoff, T., Milbich, T., Tietz, S., Schallenberg, S., Dernbach, G., Kunft, A., Heinke, S., Eich, M.-L., et al.: Rudolfv: foundation model by pathologists for pathologists. arXiv preprint arXiv:2401.04079 (2024) 23 [8] CLAM: Open source tools for computational pathology on WSIs. https://github.com/ mahmoodlab/CLAM [9] Zhang, A., Jaume, G., Vaidya, A., Ding, T., Mahmood, F.: Accelerating Data Processing and Benchmarking of AI Models for Pathology. arXiv preprint arXiv:2502.06750 (2025). https://github.com/mahmoodlab/trident [10] TRIDENT: Toolkit for large-scale whole-slide image processing. https://github.com/ mahmoodlab/TRIDENT [11] Pocock, J., Graham, S., Vu, Q.-D., Jahanifar, M., Deshpande, S., Hadjigeorghiou, G., Shephard, A., Smith, K., Raza, S.E.A., Minhas, F.u.A.A., Rajpoot, N.: Tiatoolbox as an end-to-end library for advanced tissue image analytics. Medical Image Analysis 76, 102305 (2022) [12] Shen, A., Wang, F., Paul, S., Bhuvanapalli, D., Alayof, J., Farris, A.B., Teodoro, G., Brat, D.J., Kong, J.: An integrative web-based software tool for multi-dimensional pathology whole-slide image analytics. Physics in Medicine & Biology 67(22), 224001 (2022) https://doi.org/10.1088/ 1361-6560/ac8fde [13] dplabtools Documentation. https://dplabtools.readthedocs.io/en/latest/ [14] Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Medical Image Computing and Computer-Assisted Intervention (MICCAI). LNCS, vol. 9351, pp. 234241 (2015). Springer [15] Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: Unet++: nested u-net architecture for medical image segmentation. In: International Workshop on Deep Learning in Medical Image Analysis, pp. 311 (2018). Springer [16] Janowczyk, A., Zuo, R., Gilmore, H., Feldman, M.D., Madabhushi, A.: HistoQC: An open-source quality control tool for digital pathology slides. JCO Clinical Cancer Informatics 3, 17 (2019) https://doi.org/10.1200/CCI.18.00157 [17] Janowczyk, A., contributors: HistoQC. https://github.com/choosehappy/HistoQC [18] Song, Y., Cisternino, F., Mekke, J.M., Borst, G.J., Kleijn, D.P., Pasterkamp, G., Vink, A., Glastonbury, C.A., Laan, S.W., Miller, C.L.: An automatic entropy method to eﬀiciently mask histology whole-slide images. Scientific Reports 13(1), 4321 (2023) [19] Song, Y., contributors: EntropyMasker. https://github.com/CirculatoryHealth/EntropyMasker 24 [20] Rosenthal, J., Carelli, R., Omar, M., Brundage, D., Halbert, E., Nyman, J., Hari, S.N., Allen, E.M.V., Marchionni, L., Umeton, R., Loda, M.: Building tools for machine learning and artificial intelligence in cancer research: Best practices and case study with the PathML toolkit for computational pathology. Molecular Cancer Research 20(2), 202206 (2022) https://doi.org/10. 1158/1541-7786.MCR-21-0665 [21] PathML Documentation. https://pathml.readthedocs.io/ [22] TRIDENT Documentation. https://trident-docs.readthedocs.io/ [23] Labelbox, I.: Labelbox: data labeling platform for AI. https://labelbox.com. Accessed 10 May 2025 (2025) [24] Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024) [25] Weng, Z., Seper, A., Pryalukhin, A., Mairinger, F., Wickenhauser, C., Bauer, M., Glamann, L., Bläker, H., Lingscheidt, T., Hulla, W., et al.: Grandqc: comprehensive solution to quality control problem in digital pathology. Nature Communications 15(1), 10685 (2024) [26] Xiong, C., Chen, H., Sung, J.J.: survey of pathology foundation model: Progress and future directions. arXiv preprint arXiv:2504.04045 (2025) [27] Chen, R.J., Chen, C., Li, Y., Chen, T.Y., Trister, A., Krishnan, R.G., Mahmood, F.: Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In: CVPR (2022) [28] Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T.J., Zou, J.: visuallanguage foundation model for pathology image analysis using medical twitter. Nature medicine 29(9), 2307 (2023) [29] Chen, R.J., Ding, T., Lu, M.Y., Williamson, D.F., Jaume, G., Song, A.H., Chen, B., Zhang, A., Shao, D., Shaban, M., et al.: Towards general-purpose foundation model for computational pathology. Nature medicine 30(3), 850862 (2024) [30] Lu, M.Y., Chen, B., Williamson, D.F., Chen, R.J., Liang, I., Ding, T., Jaume, G., Odintsov, I., Le, L.P., Gerber, G., et al.: visual-language foundation model for computational pathology. Nature medicine 30(3), 863874 (2024) [31] Meirelles, A.L., Kurc, T., Saltz, J., Teodoro, G.: Effective active learning in digital pathology: case study in tumor infiltrating lymphocytes. Computer Methods and Programs in Biomedicine 25 220, 106828 (2022) [32] Bandi, P., Geessink, O., Manson, Q., Van Dijk, M., Balkenhol, M., Hermsen, M., Bejnordi, B.E., Lee, B., Paeng, K., Zhong, A., et al.: From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE transactions on medical imaging 38(2), 550560 (2018) [33] Bulten, W., Kartasalo, K., Chen, P.-H.C., Ström, P., Pinckaers, H., Nagpal, K., Cai, Y., Steiner, D.F., Van Boven, H., Vink, R., et al.: Artificial intelligence for diagnosis and gleason grading of prostate cancer: the panda challenge. Nature medicine 28(1), 154163 (2022) [34] Weinstein, J.N., Collisson, E.A., Mills, G.B., Shaw, K.R., Ozenberger, B.A., Ellrott, K., Shmulevich, I., Sander, C., Stuart, J.M.: The cancer genome atlas pan-cancer analysis project. Nature genetics 45(10), 11131120 (2013) [35] Goode, A., Gilbert, B., Harkes, J., Jukic, D., Satyanarayanan, M.: Openslide: vendor-neutral software foundation for digital pathology. Journal of pathology informatics 4(1), 27 (2013) [36] OpenSlide. https://openslide.org/ [37] OpenSlide-Python. https://github.com/openslide/openslide-python [38] Kirillov, A., Mintun, E., Ravi, N., Mao, H., et al.: Segment anything. In: ICCV (2023) [39] Zhao, B., Tu, H., Wei, C., Mei, J., Xie, C.: Tuning layernorm in attention: Towards eﬀicient multimodal llm finetuning. In: The Twelfth International Conference on Learning Representations [40] Rostami, G., Chen, P.-H., Hosseini, M.S.: Segment Any Crack: Deep Semantic Segmentation Adaptation for Crack Detection (2025). https://arxiv.org/abs/2504. [41] Ilse, M., Tomczak, J., Welling, M.: Attention-based deep multiple instance learning. In: International Conference on Machine Learning, pp. 21272136 (2018). PMLR [42] Zhang, H., Meng, Y., Zhao, Y., Qiao, Y., Yang, X., Coupland, S.E., Zheng, Y.: Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1880218812 (2022) [43] Li, B., Li, Y., Eliceiri, K.W.: Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1431814328 (2021) 26 [44] Shao, D., Chen, R.J., Song, A.H., Runevic, J., Lu, M.Y., Ding, T., Mahmood, F.: Do multiple instance learning models transfer? In: Forty-second International Conference on Machine Learning [45] Tang, W., Zhou, F., Huang, S., Zhu, X., Zhang, Y., Liu, B.: Feature re-embedding: Towards foundation model-level performance in computational pathology. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1134311352 (2024) [46] Shao, Z., Bian, H., Chen, Y., Wang, Y., Zhang, J., Ji, X., et al.: Transmil: Transformer based correlated multiple instance learning for whole slide image classification. Advances in neural information processing systems 34, 21362147 (2021) [47] Li, J., Chen, Y., Chu, H., Sun, Q., Guan, T., Han, A., He, Y.: Dynamic graph representation with knowledge-aware attention for histopathology whole slide image analysis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1132311332 (2024) [48] TIAToolbox Documentation. https://tia-toolbox.readthedocs.io/ [49] Whole Slide Image Classification Using PyTorch and TIAToolbox. https://docs.pytorch.org/ tutorials/intermediate/tiatoolbox_tutorial.html [50] Jaume, G., Doucet, P., Song, A., Lu, M.Y., Almagro Pérez, C., Wagner, S., Vaidya, A., Chen, R., Williamson, D., Kim, A., et al.: Hest-1k: dataset for spatial transcriptomics and histology image analysis. Advances in Neural Information Processing Systems 37, 5379853833 (2024) 27 Extended Data Fig. 1: More qualitative examples to support Fig. 3, showing original slide thumbnails along with the ground truth annotation mask (overlayed on the slide in green), the models predicted mask (overlayed in red), and an overlay of both masks to show intersection. Extended Data Fig. 2: More qualitative examples to support Fig. 3, specifically for samples with IHC staining, showing the generalization capability of the tissue detector in AtlasPatch. Extended Data Fig. 3: More qualitative examples to support Fig. 4, showing original slide thumbnails along with the predicted masks of three thresholding tools, in addition to AtlasPatch. 29 Extended Data Fig. 4: More qualitative examples to support Fig. 4 for thresholding-based methods in comparison with AtlasPatch. 30 Extended Data Fig. 5: More qualitative examples to support Fig. 4 for AI-based methods in comparison with AtlasPatch."
        }
    ],
    "affiliations": [
        "Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montreal, QC, Canada",
        "Department of Building, Civil, and Environmental Engineering, Concordia University, Montreal, QC, Canada",
        "Department of Computer Science and Software Engineering (CSSE), Concordia University, Montreal, QC, Canada",
        "Department of Computer Science, Khalifa University, Abu Dhabi, UAE",
        "Department of Pathology, McGill University, Montreal, QC, Canada",
        "Institute for Research in Immunology and Cancer, University of Montreal, Montreal, QC, Canada",
        "MilaQuebec AI Institute, Montreal, QC, Canada",
        "University of Montreal Hospital Center (CHUM), Montreal, QC, Canada"
    ]
}