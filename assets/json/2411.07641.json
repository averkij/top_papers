{
    "paper_title": "Top-$nσ$: Not All Logits Are You Need",
    "authors": [
        "Chenxia Tang",
        "Jianchun Liu",
        "Hongli Xu",
        "Liusheng Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-$n\\sigma$, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-$p$, min-$p$) that inadvertently include more noise tokens at higher temperatures, top-$n\\sigma$ maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-$n\\sigma$ to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 1 4 6 7 0 . 1 1 4 2 : r Top-nσ: Not All Logits Are You Need Chenxia Tang Jianchun Liu Hongli Xu Liusheng Huang School of Computer Science and Technology, University of Science and Technology of China Suzhou Institute for Advanced Research, University of Science and Technology of China"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) typically employ greedy decoding or lowtemperature sampling for reasoning tasks, reflecting perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nσ, novel sampling method that operates directly on pre-softmax logits by leveraging statistical threshold. Our key insight is that logits naturally separate into Gaussian-distributed noisy region and distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nσ maintains stable sampling space regardless of temperature scaling. We also provide theoretical analysis of top-nσ to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable capabilities across various domains, from code generation [Chen et al., 2021] to mathematical reasoning [Lewkowycz et al., 2022], and complex problem-solving [Wei et al., 2022]. These advancements are largely driven by the models sophisticated text generation mechanisms, which underpin their versatility in diverse applications. Token sampling, as the first step in text generation, plays crucial role in determining the quality of model outputs. While sampling strategies offer the potential advantage of generating diverse outputs through multiple attempts, their effectiveness in reasoning-intensive tasks remains significant challenge. Conventional wisdom suggests that deterministic methods like greedy decoding often surpass stochastic sampling in tasks requiring precise reasoning. This perception arises from observations that existing sampling techniques prioritize diversity and reduced repetition over reasoning accuracy. For instance, probability-based methods such as temperature-scaling [Ackley et al., 1985], nucleus sampling [Holtzman et al., 2019], top-k [Fan et al., 2018], and min-p [Nguyen et al., 2024], as well as entropy-based techniques like mirostat sampling [Basu et al., 2020] and η-sampling [Hewitt et al., 2022], are not specifically designed to maintain reasoning quality. We argue that this limitation stems from these methods inability to effectively filter out irrelevant tokens, leading to trade-off between diversity and reasoning fidelity. Nonetheless, it is essential to recognize that LLMs are inherently probabilistic models; deterministic methods like greedy decoding introduce bias by failing to accurately reflect the models learned distribution. This insight underscores the need for more sophisticated sampling techniques that can leverage LLMs probabilistic nature while preserving robust reasoning capabilities. While existing approaches primarily focus on manipulating probability distributions, we argue that examining the pre-softmax logits can reveal deeper insights into the models generation process. As shown in Figure 1, an intriguing observation is that logit distributions exhibit highly regular pattern, typically comprising two distinct components: Gaussian-like distribution of background tokens (a) Distribution of logits (b) Descendingly sorted Probabilities. Only the top 20 tokens are shown. Figure 1: Distribution of logits and descendingly sorted probabilities of LLaMA3-8B-Instruct on an AQuA sample. Note that the leading tokens in the right plot (with higher probabilities) correspond to the right-side region of the logits distribution. The maximum logit is approximately 10σ above the mean of the distribution. and set of prominent outliers, respectively. Although the majority of tokens follow Gaussian distribution, as shown in Figure 1b, the outlier tail dominates the probability mass. We refer to these components as the noisy region and the informative region, respectively. It is worth noting that the maximum logit deviates from the mean by more than 5σ (the standard deviation), substantially exceeding typical statistical criteria for outlier identification. remarkable finding is that, despite not examining the logits distribution, accurately hypothesized that LLM output probabilities are mixture of the true distribution and smoothing component [Hewitt et al., 2022]. However, they mistakenly assume that this smooth component follows uniform distribution. Our observation reveals that the mixture involves not uniform distribution but log-normal distribution, which naturally arises from applying softmax to the Gaussian-distributed logits in the noisy region. Based on this observation, we propose top-nσ, simple yet effective sampling method that naturally distinguishes between noisy and informative tokens. While this approach shares the spirit of nucleus sampling in identifying core set of tokens (nucleus), top-nσ offers several unique advantages. Unlike top-k or greedy decoding, which imposes static constraints, or top-p and min-p, which inadvertently include more irrelevant tokens at higher temperatures, top-nσ maintains stable and meaningful sampling space independent of temperature scaling. This stability is particularly valuable for recent test-time scaling techniques that rely on extensive sampling to explore the solution space [Brown et al., 2024, Wang and Zhou, 2024, Qi et al., 2024, Zhong et al., 2024]. Furthermore, top-nσ operates directly on logits without requiring sorting operations or additional softmax transformations, making it computationally efficient. By identifying the informative tokens through statistical properties of the logit distribution, our method achieves both effectiveness and efficiency in guiding the sampling process. Our main contributions include: Novel Logit-Based Perspective: We introduce novel analytical framework focusing on pre-softmax logit distributions, providing fundamental insights that benefit not only sampling strategy development but also potential improvements in model training approaches. Efficient Top-nσ Algorithm: We propose conceptually simple yet powerful sampling method that operates directly on logits, achieving superior generation quality while maintaining computational efficiency through its sorting-free and softmax-free implementation. It is very easy to integrate top-nσ into existing LLM frameworks. Ready for Test-Time Scaling Techniques: Our top-nσ algorithm enables more granular exploration of the solution space, achieving better balance between exploration and exploitation. This characteristic makes it particularly effective for test-time scaling techniques. Theoretical Analysis: We provide comprehensive quantitative analyses of top-nσ, examining its cumulative probability mass characteristics and proving its temperature invariance 2 property. These theoretical foundations establish solid basis for both the implementation and understanding of the method. Extensive Empirical Validation: We demonstrate the effectiveness of our approach through rigorous experiments across four diverse datasets, showing significant improvements in generation quality compared to existing methods."
        },
        {
            "title": "Insights",
            "content": "From Figure 1, we observe that the models logits appear to follow well-defined statistical distribution. This empirical observation naturally raises several fundamental questions: 1. How can we interpret probability-based sampling methods (e.g., nucleus sampling) from the perspective of logit space? 2. What are the underlying characteristics of logit distributions in large language models? 3. How can we leverage these distributions to effectively distinguish between noisy and informative regions? In this section, we systematically address the aforementioned questions and present our findings. 2.1 From Probabilities to Logits Modern large language models (LLMs) generate text through two-step process: first producing logits, then converting them to probabilities via softmax transformation. To better understand sampling methods in the logit space, we start by examining how conventional probability-based methods, particularly nucleus sampling, can be reinterpreted from logit perspective. Given an input sequence, an LLM first generates logit vector = (l1, , lV ) RV , where is the vocabulary size. These logits are firstly scaled by temperature: l/T and then transformed into probabilities = (p1, , pV ) RV through the softmax function: pi = eli , where = (cid:88) j=1 elj , 1 (1) Fundamentally, all sampling methods operate by determining probability threshold p(t) [0, 1]. Tokens with probabilities above this threshold form the sampling nucleus, and their cumulative probability defines the nucleus mass. Formally, for threshold p(t), the nucleus is: = {i pi p(t)} = {i li t} (2) This probability-based selection can be equivalently performed in the logit space by determining corresponding logit threshold = ln(s p(t)). Based on our empirical observations in Figure 1, we hypothesize that logits follow certain distribution . Under this assumption, we can derive the relationship between nucleus sampling parameters and logit threshold t. Theorem 1. Consider logits {li = 1, , } independently and identically distributed according to (x). For any threshold t, we have: eli (cid:88) li>t (cid:90) + exf (x) dx The complete proof is provided in Appendix A.1. Although the theorem itself is conceptually simple, it gives rise to series of powerful lemmas with practical implications: Lemma 2. Denote I(t) = (cid:82) + threshold is exf (x) dx, and thus = I(). The nucleus mass of given pN = pi = I(t) I() (cid:88) iN (3) Remark. key implication of this lemma is that the logit threshold for nucleus sampling can be analytically derived given only the probability distribution of the logits. We proceed to demonstrate this by deriving closed-form expressions for two fundamental cases. Given the cumulative probability threshold p: 1. Gaussian distribution: li (µ, σ2). The threshold is = µ + 2σ(erf 1(1 2p)) + σ2 where erf() denotes the error function[Andrews, 1998]. 2. Uniform distribution: li (M a, ), The threshold is = ln (cid:20) 1 1 p(1 ea) (cid:21) (4) (5) The detailed derivations of these expressions are provided in Appendix A.2. These analytical expressions provide valuable insights into the relationship between the logit distribution and the sampling threshold. While the actual logit distribution in language models may be more complex, these fundamental cases serve as important theoretical benchmarks and can guide the design of more efficient sampling algorithms. 2.2 Understanding the Real Distribution of Logits 2.2.1 Noisy Region As demonstrated in Figure 1, the vast majority of tokens exhibit logits that follow Gaussian distribution. Since the probabilities corresponding to these logits are typically negligible and have been traditionally considered noise in previous works, we designate this region as the noisy region. This characterization aligns well with statistical intuition, where Gaussian distributions often indicate the presence of random noise in system. When the margin between this noisy region and the informative region narrows, the noise-derived probabilities after softmax transformation tend to interfere with the models generation process, potentially degrading the output quality. This phenomenon is particularly evident in high-temperature sampling scenarios, where all current non-deterministic sampling algorithms perform poorly. We attribute this degradation to the diminished gap between the two regions under high temperatures, allowing the noise distribution to dominate the probability landscape. We identify several key factors contributing to this noise: Training Data Noise The inherent noise and variations in the training data naturally propagate through the models learning process, ultimately manifesting as part of the Gaussian distribution in the logit space. Regularization Effects Various regularization techniques employed in the training process, while crucial for preventing overfitting, have the side effect of assigning small but non-zero probabilities to semantically irrelevant tokens in the vocabulary space. Noise of Silence The models architectural constraint of assigning finite values to irrelevant tokens (which ideally should be silent with logits) results in distinctive noise pattern. This is an inherent flaw of the softmax function, which is widely discussed in [Miller, 2023, Xiao et al., 2023]. While addressing these noise sources fundamentally lies beyond the scope of this paper, we believe these insights could contribute to improving training procedures in future work. In this paper, we will focus on how to eliminate these noisy tokens during the inference. 4 2.2.2 Informative Region As shown in Figure 1, small subset of tokens accounts for the majority of the probability mass. This concentration suggests that large language models possess specific knowledge about these tokens, hence we designate this as the informative region. Due to the limited number of tokens in this region, it is challenging to make definitive claims about the underlying distribution. However, recent empirical observations regarding sampling methods have provided interesting insights into this regions characteristics. Notably, we have observed that the min-p sampling method yields significant improvements in generation quality. This approach establishes baseline probability threshold and eliminates all probability values below pmax p, where pmax is the maximum probability. While Nguyen et al. [2024] derived this method empirically, we have made surprising theoretical discovery: Theorem 3. For logits following uniform distribution, min-p sampling is equivalent to top-(1 p) sampling. Proof. We first put min-p into the logits space. For li (, ), the threshold is = ln(s pmax p) = ln(eM p) = + ln (6) Considering Eq. (5), since li (, ), = , so the threshold for top-(1 p) is the exactly same: = ln (cid:20) 1 1 (1 p) (cid:21) = + ln (7) This analysis reveals that despite min-ps claimed adaptiveness, it essentially performs static truncation in the logits space. Furthermore, the effectiveness of min-p sampling suggests that the informative region approximately follows uniform distribution. 2.3 Determine the Boundary natural approach to distinguish between informative and noisy regions would be treating the informative region as outliers of the noisy distribution. Under this assumption, conventional methods such as the µ + 3σ rule [Kazmier, 2009] could be applied to determine the boundary. However, our empirical observations suggest that this approach may not be optimal for the task at hand. We define the σ-distance as the number of standard deviations between the maximum probability and the mean value of the distribution, i.e., σ-distance = µ σ , where is the maximum logit, following the same notation as in the uniform distribution case. As illustrated in Figure 2a, the distance between the maximum probability and the mean value consistently surpasses 10σ, exhibiting substantial fluctuations during the generation. In conjunction with Figure 2b, an intriguing pattern emerges: larger nucleus sizes correlate with lower σ-distances. This observation contradicts traditional outlier detection approaches such as the µ + 3σ criterion, where higher σ-distances should theoretically encompass more tokens. This finding strongly suggests that informative tokens should not be treated as outliers of noisy tokens. In fact, higher σ-distance indicates the models strong confidence in its output, resulting in smaller nucleus size. This leads us to fundamental reversal of perspective: instead of treating the minority as outliers from the Gaussian distribution, we should recognize the majority as outliers from the uniform distribution. This counterintuitive shift challenges the conventional assumption of outlier detection, where anomalies are typically rare events. In our case, the tokens in noisy regions are essentially irrelevant candidates that should be eliminated from consideration, despite their numerical majority. To implement this perspective, we introduce the top-nσ algorithm. Our algorithm begins from the maximum value and extends downward, using the standard deviation of the distribution to dynamically adjust the boundary. Specifically, we capture region that extends nσ below the maximum value, where is determined empirically to balance between preserving informative tokens and excluding the uniform noise. In practice, we find = 1.0 achieves satisfactory performance. 5 (a) σ-distance during generation (b) Nucleus size during generation Figure 2: Relationship between σ-distance and nucleus size when temperature = 1 during generation. The plots demonstrate an inverse relationship: high σ-distances correspond to small nucleus sizes (mostly 1), while lower σ-distances (though remaining above 10) correlate with larger nucleus sizes."
        },
        {
            "title": "3 Algorithm Description",
            "content": "Our method introduces statistical threshold to filter candidate tokens before sampling. Algorithm 1 outlines the main steps of our approach. Algorithm 1 Top-nσ Sampling 1: Input: Input context x, temperature , threshold multiplier 2: Output: Next token 3: Compute logits = LLM(x) 4: Scale logits: = l/T 5: Calculate = max(l) and σ = std(l) nσ 6: Create mask: mi = (cid:26)1 0 if otherwise 7: Apply mask: = (cid:26)l if mi = 1 otherwise 8: = softmax(l) 9: Sample token from distribution The algorithm is computationally efficient as it operates directly on logits without requiring extra softmax transformation or sorting. The core operations (max and std) are highly optimized for modern GPU implementations. The execution time typically remains within tens of microseconds, with the kernel launch overhead accounting for the majority of the computational cost. To better understand its behavior, we analyze two important aspects. First, we examine the probability mass of selected tokens in Section 3.1, which provides probabilistic view of our statistical filtering process. More importantly, we prove that our method selects consistent number of tokens regardless of the temperature parameter in Section 3.2. This temperature invariance property ensures stable behavior across different sampling temperatures, while still maintaining sufficient diversity in the candidate pool. 3.1 Nucleus Mass While the exact mixing mechanism of distributions in the logits space remains unknown, we focus our analysis on two boundary cases: (1) when the distribution approximates Gaussian, which typically occurs in high-temperature scenarios, and (2) when the distribution tends toward uniformity, as observed in low-temperature settings or when the model exhibits high confidence (usually accompanied 6 by larger standard deviations). These two cases provide theoretical bounds for analyzing our methods behavior in practice. In the Gaussian case, (µ, σ2), considering Eq. (4), we obtain the following equation: nσ = µ + 2σ(erf 1(1 2p)) + σ2 (8) Thus, (cid:20) 1 erf = 1 2 (cid:18) µ nσ σ2 2σ (cid:19)(cid:21) (9) key property of this formulation is that as σ approaches zero, the term inside erf approaches positive infinity, causing to converge to zero. This effectively prevents the inclusion of tokens from the noise region, providing natural safeguard against potential instabilities in the sampling process. In the uniform case, (M a, ), considering Eq. (5), we have: Thus, nσ = ln (cid:20) 1 1 p(1 ea) (cid:21) = 1 enσ 1 ea (10) (11) To determine an appropriate value for a, we use the fact that the overall standard deviation of the logits is σ. Given that the variance of uniform distribution is a2/12, we have a2/12 σ2 to maintain consistency with the observed standard deviation. This provides us with an upper bound 2 3σ. Hence, 1 enσ 3σ 1 e2 (12) For typical parameter values, such as = 1 and σ = 1.9, we obtain lower bound of 0.85, indicating that our top-nσ algorithm effectively preserves the informative tokens. This analysis also provides valuable guidance for choosing the hyperparameter n: it should not only be positive but also remain below 2 3 3.46, due to 1. Exceeding this upper threshold would likely result in the inclusion of noisy tokens, potentially degrading the algorithms performance. 3.2 Temperature Invariance key property of our sampling method is its temperature invariance. Specifically, the set of candidate tokens remains constant regardless of the temperature value used. Theorem 4. For any temperature > 0, the nucleus of top-nσ remains invariant. Proof. Consider any token and temperature > 0. Let li be the original logit. After temperature scaling, we have = li/T for all tokens. For any given token i: = max (l j) = M σ = (cid:115) 1 (cid:88) (l µ)2 = (cid:115) 1 (cid:88) (lj/T µj/T )2 = σ Token is selected if and only if nσ. Substituting: nσ li nσ li nσ This final condition is independent of . Therefore, for any token i, its inclusion in the selected set is determined by the same condition regardless of temperature. 7 This temperature invariance property distinguishes our method from other common sampling approaches. For top-p and min-p sampling, the selected token set varies with temperature. As temperature increases, the logit distribution tends toward the Gaussian noise part, causing top-p and min-p to include more noise tokens in their sampling pool. While top-k sampling does maintain temperature invariance, it uses fixed value, which merely shifts the problem: static cannot dynamically distinguish between valid tokens and noise tokens across different contexts. Nuanced Exploration Control With top-nσ, the exploration is decomposed into two distinct parameters. The parameter controls the size of the nucleus, while temperature solely regulates how to explore within the nucleus. This mechanism acts as soft masking approach, effectively eliminating potentially irrelevant tokens while still allowing for controlled degree of exploration. Such decoupling provides more nuanced control over the sampling process: the parameter determines the boundary between valid and noise tokens, while the temperature parameter fine-tunes the exploration strategy within the validated token space."
        },
        {
            "title": "4 Experiments",
            "content": "Language models typically employ deterministic algorithms (or low-temperature sampling) for reasoning tasks, while using stochastic approaches for generating diverse responses. Although diversity is often considered key advantage of stochastic methods, its metrics remain ambiguous and unreliable. In this section, we evaluate our top-nσ algorithm across four reasoning-focused datasets. Our experimental results show that top-nσ achieves competitive performance against various baselines, including greedy decoding, and exhibits slight improvements at moderately higher temperatures (T = 1.5), which challenges the conventional preference for low-temperature sampling in reasoning tasks. To further investigate whether our approach maintains sufficient exploration capabilities or not, we implemented repeated sampling [Brown et al., 2024] experiments, Monte Carlo technique that generates multiple samples and determines the final answer through majority voting. Results show that top-nσ does not compromise exploration ability while maintaining high accuracy. 4.1 Setup Model We use state-of-the-art open source language model LLaMA-3-8B-Instruct [Dubey et al., 2024] for evaluating our algorithm and vllm [Kwon et al., 2023] as our inference framework. Datasets We conduct our experiments on four question answering datasets, especially focusing on reasoning: AQuA [Ling et al., 2017], MATH [Hendrycks et al., 2021], GSM8K [Cobbe et al., 2021], GPQA [Rein et al., 2024]. The difficulty of the dataset ranges from elementary school math to doctoral-level problems. For all datasets, we transform them into an open-ended generation task. The model will generate response and then we extract the answer from it, comparing it with the ground truth. We dont employ complex prompting techniques, simply asking the model to reason and answer the question in prescribed form. More details can be found in Appendix B. Baselines We compare top-nσ with various baselines, including top-k , top-p , min-p , temperature sampling. We present results between various temperatures. For all methods, the same set of hyperparameters is used across different temperatures to demonstrate the stability of their hyperparameters. For the baselines, we adopt their recommended hyperparameter settings based on previous work [Hewitt et al., 2022, Nguyen et al., 2024] or practical guideline [Siml, 2024]. The hyperparameter table is presented in Table 1. Table 1: Hyperparameter Settings Hyperparameter Value 0.9 0.1 20 1.0 top-p min-p top-k top-nσ 8 3) (0, 3.46). In For top-nσ, we proved that the theoretical bounds for parameter lie in (0, 2 practice, we find that = 1.0 provides consistently strong and robust performance across different scenarios without requiring task-specific tuning. Empirically, we recommend constraining larger than 0.5 to avoid missing informative tokens. See Appendix for more hyperparameter discussions. Metrics We employ two primary metrics for evaluation. For single-pass evaluation, we report the Exact Match (EM) score, which measures the percentage of model responses that exactly match the reference answers. For multiple-pass evaluation, we introduce Maj@N, where the model generates different responses for each query. The final prediction is determined through majority voting among these samples, after which the EM score is calculated between this consensus answer and the reference. 4.2 Single-Pass Results Table 2: Performance comparison of different sampling methods across datasets (Exact Match values in %). Bold numbers indicate the best performance under each temperature setting, and underlined bold numbers represent the highest score for each dataset. Notably, temperature = 0.0 represents greedy decoding, deterministic algorithm rather than sampling method. Dataset Method GPQA GSM8K AQuA MATH Sample Top-p Top-k Min-p Top-nσ Sample Top-p Top-k Min-p Top-nσ Sample Top-p Top-k Min-p Top-nσ Sample Top-p Top-k Min-p Top-nσ 0.0 32.03 81.25 36.61 19.92 Temperature 1.5 14.84 20.31 25.00 31.25 32.42 21.48 66.02 62.11 76.56 82.03 30.71 37.01 40.94 10.55 14.45 16.02 2.0 7.03 8.98 19.14 26.95 27.73 0.00 0.00 21.88 66.41 79.30 21.65 33.07 40.16 3.91 10.94 14. 1.0 30.47 30.86 29.69 27.73 27.34 76.95 78.52 75.78 80.47 78.52 39.76 39.76 37.80 41.73 16.41 14.06 15.63 20.31 3.0 0.00 0.00 7.42 16.02 25.00 0.00 0.00 2.34 14.84 74.61 The experimental results provide compelling evidence for the superiority of our proposed top-nσ sampling method across various reasoning tasks. While other methods show inconsistent performance. For instance, min-p performs well on GSM8K but struggles on MATH500, and top-k shows decent results on GPQA but underperforms on AQuA. Our top-nσ method consistently achieves the best or near-best performance across all datasets. Notably, challenging the conventional wisdom that reasoning tasks benefit most from deterministic decoding, our stochastic sampling approach outperforms greedy decoding across all datasets. More remarkably, at higher temperatures where conventional methods suffer catastrophic degradation, top-nσ maintains robust performance. For instance, at temperature 3.0, while standard sampling and top-p completely fail on GPQA and GSM8K, our method still achieves 25.00% and 74.61% accuracy respectively. This stark contrast in high-temperature performance demonstrates the exceptional stability of our approach. Optimal Temperature and Exploration An intriguing finding emerges from our experiments that challenges conventional wisdom. Despite the common guideline that reasoning tasks benefit most from deterministic (greedy) or low-temperature decoding [Xu et al., 2022, Zhu et al., 2023, Renze and Guven, 2024], our results demonstrate that slightly higher temperatures (typically around 1.5) yield optimal performance when combined with our controlled sampling strategy. This phenomenon aligns with our theoretical framework: under the strict nucleus control of top-nσ, moderate exploration in the token space actually enhances model performance rather than degrading it. 9 Surpassing Greedy Decoding Perhaps the most significant achievement is that top-nσ sampling consistently outperforms greedy decoding (temperature = 0.0) across all datasets, feat that none of the other sampling methods could accomplish. This demonstrates that controlled stochastic sampling can outperform deterministic approaches even in reasoning-intensive tasks. Furthermore, these results represent only the baseline performance of our method - the incorporation of test-time scaling techniques promises to widen this performance gap substantially, potentially establishing an even more decisive advantage over greedy decoding. 4.3 Multiple-Pass Results Table 3: Maj@20 of Different Sampling Methods (%) Dataset Method GSM8K GPQA AQuA MATH Sample Top-p Top-k Min-p Top-nσ Sample Top-p Top-k Min-p Top-nσ Sample Top-p Top-k Min-p Top-nσ Sample Top-p Top-k Min-p Top-nσ 1.0 90.63 89.06 89.45 89.84 90.63 30.47 30.08 32.03 30.47 31.64 - 44.88 48.03 44.09 47.64 - 32.03 31.25 30.86 32.03 Temperature 2.0 1.5 0.00 75.00 0.00 89.45 91.41 62.89 89.84 90.63 91.80 91.41 12.89 27.34 12.89 27.34 26.17 31.64 33.20 31.25 32.42 33.20 - - - - 40.16 48.03 47.64 51.18 49.61 46.06 - - - - 12.50 20.70 23.83 28.91 33.98 35.16 3.0 0.00 0.00 2.73 53.13 90.23 0.00 0.00 24.61 30.47 30.47 - - - - - - - - - - As shown in Table 3, all random sampling methods, enhanced with repeated sampling, outperform greedy decoding. Notably, the top-nσ method demonstrates strong performance in most datasets, indicating that our approach maintains effective exploration capabilities. Regarding the impact of temperature in repeated sampling, we observe that slightly high temperatures have more significant effect. Most methods exhibit better performance at moderately higher temperature (T = 1.5), as this allows for broader exploration of the solution space. However, while other methods suffer from severe performance degradation at higher temperatures due to interference from noisy tokens, the top-nσ approach remains robust to such disturbances. Performance Ceiling The performance saturation phenomenon varies across different datasets. On GSM8K, all methods achieved remarkably high accuracy (90%+) after sufficient sampling, suggesting that this mathematical reasoning dataset might be too elementary to differentiate between sampling strategies effectively. In contrast, GPQA presents unique case where majority voting barely improved over single-sample performance. Being multiple-choice dataset with four options, GPQA has baseline accuracy of 25% for any strategy that can properly follow instructions. The observed accuracy of 30-32% across different methods, only marginally above the random baseline, suggests that the model struggles substantially with this dataset."
        },
        {
            "title": "5 Conclusion",
            "content": "We have presented top-nσ, demonstrating both theoretical and empirical advantages over existing sampling methods. Our analysis reveals fundamental insights about logit distributions in large 10 language models, challenging the conventional preference for greedy decoding in reasoning tasks. The methods temperature invariance and efficient computation make it particularly well-suited for emerging test-time scaling techniques. Beyond sampling strategies, our findings about the distinct separation between noisy and informative regions in logit space suggest potential improvements in model architecture and training procedures. Future work might explore how to leverage these statistical properties during training to enhance model performance."
        },
        {
            "title": "References",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. David Ackley, Geoffrey Hinton, and Terrence Sejnowski. learning algorithm for boltzmann machines. Cognitive science, 9(1):147169, 1985. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi:10.18653/v1/P18-1082. URL https: //aclanthology.org/P18-1082. Minh Nguyen, Andrew Baker, Andreas Kirsch, and Clement Neo. Min sampling: Balancing creativity and coherence at high temperature. arXiv preprint arXiv:2407.01082, 2024. Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav Varshney. Mirostat: neural text decoding algorithm that directly controls perplexity. arXiv preprint arXiv:2007.14966, 2020. John Hewitt, Christopher Manning, and Percy Liang. Truncation sampling as language model desmoothing. arXiv preprint arXiv:2210.15191, 2022. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint arXiv:2402.10200, 2024. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195, 2024. 11 Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al. Evaluation of openai o1: Opportunities and challenges of agi. arXiv preprint arXiv:2409.18486, 2024. Larry C. Andrews. Special Functions of Mathematics for Engineers. SPIE Press, 1998. ISBN 9780819426161. Evan Miller. Attention is off by one, 7 2023. URL https://www.evanmiller.org/ attention-is-off-by-one.html. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Leonard Kazmier. Schaums Outline of Business Statistics. McGraw-Hill, New York, 4 edition, 2009. ISBN 9780071635271. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum? id=Ti67584b98. Jan Siml. Is there an optimal temperature and top-p for code generation with paid LLM APIs?, 2024. URL https://siml.earth/scratchpad/llm_code_generation_experiment/. Accessed: 2024-03-17. Frank Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 110, 2022. Yuqi Zhu, Jia Li, Ge Li, YunFei Zhao, Jia Li, Zhi Jin, and Hong Mei. Improving code generation by dynamic temperature sampling. arXiv e-prints, pages arXiv2309, 2023. Matthew Renze and Erhan Guven. The effect of sampling temperature on problem solving in large language models. arXiv preprint arXiv:2402.05201, 2024. This preprint version is incomplete. We will supplement the missing experimental results and appendix in the full version later."
        },
        {
            "title": "A Theoretical Analysis",
            "content": "A.1 Proof of Theorem 1 A.2 Threshold Calculation"
        },
        {
            "title": "C Hyperparameters",
            "content": ""
        }
    ],
    "affiliations": [
        "School of Computer Science and Technology, University of Science and Technology of China",
        "Suzhou Institute for Advanced Research, University of Science and Technology of China"
    ]
}