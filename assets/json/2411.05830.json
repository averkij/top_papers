{
    "paper_title": "GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models",
    "authors": [
        "Nizar Islah",
        "Justine Gehring",
        "Diganta Misra",
        "Eilif Muller",
        "Irina Rish",
        "Terry Yue Zhuo",
        "Massimo Caccia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\textbf{\\GitChameleon{}}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. \\GitChameleon{} is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, \\textbf{GPT-4o} achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, \\GitChameleon{} serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at \\url{https://github.com/NizarIslah/GitChameleon}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 0 3 8 5 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "P D.C A T([D F1,D F2]) F1.A N D(D F2) GITCHAMELEON: UNMASKING THE VERSIONSWITCHING CAPABILITIES OF CODE GENERATION MODELS Nizar Islah, Justine Gehring & Diganta Misra Mila - Quebec AI Institute MPI-IS Tubingen, ELLIS Tubingen {firstname.lastname}@mila.quebec"
        },
        {
            "title": "Terry Yue Zhuo\nMonash University\nSea AI Lab",
            "content": "Eilif Muller, Irina Rish Mila - Quebec AI Institute Universite de Montreal"
        },
        {
            "title": "ABSTRACT",
            "content": "The rapid evolution of software libraries presents significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering limited perspective on models practical usability. To address this gap, we introduce GitChameleon, novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon is designed to rigorously assess the ability of modern large language models (LLMs) to generate versionspecific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, GPT-4o achieves pass@10 of only 39.9% (43.7% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, GitChameleon serves as critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of versionconditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have become highly popular in code completion, to the extent that they are now deployed as virtual coding assistants within popular code editors1, enhancing the overall coding workflow. Code, being dynamic and constantly evolving environment, necessitates continuous process of adaptation to stay in sync with the rapidly shifting paradigms, frameworks, and methodologies within the software development domain. The inherent variability in coding styles, the emergence of new programming languages, and the continuous evolution of libraries and packages underscore the imperative for an active approach in updating code generation models. In response to the needs of practical coding environments, several large language models (LLMs) have been introduced, including StarCoder (Li et al., 2023), DeepSeek-Coder (Guo et al., 2024), CodeLlama (Rozi`ere et al., 2023), among others. Despite these advancements, existing LLMs often equal contribution 1https://github.com/features/copilot"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Left: Modern LLMs often struggle with generating version-accurate code, highlighting the need for benchmarks that specifically assess their ability to handle versioning. Right: Cumulative year-over-year (YoY) version releases of popular Python-based machine learning libraries show consistent upward trend, reflecting the rapid pace of development and version updates of code libraries and packages. struggle to keep pace with the rapid changes in codebases, particularly when tasked with generating version-specific code that is both syntactically and functionally accurate. This issue is especially critical, as developers increasingly depend on AI-assisted coding tools to boost productivity and maintain code quality. recent Stack Overflow survey revealed that 70% of the participants are using or planning to integrate AI coding tools, 33% citing increased productivity as the primary motivation to incorporate these tools into their workflows2. Given the rapid development and release cycles of popular libraries, as shown in Figure 1 (right), the need for code generation models to continually adapt to changing APIs is more pressing than ever. For example, prominent machine learning and deep learning libraries like PyTorch (Paszke et al., 2019), NumPy (Harris et al., 2020), and Scikit-Learn (Buitinck et al., 2013) undergo frequent updates, which is reflected in consistent upward trend in user downloads and version releases. This dynamic nature of code requires models that can adapt and generate code that adheres to the latest versions and practices, need that current models often fail to meet comprehensively. In addition, certain hardware is restricted to compatibility with specific versions of commonly used packages, which adds an additional layer of complexity beyond merely updating the knowledge base of code LLM to the latest library versions. In response to these challenges, our work introduces novel benchmark designed to assess the ability of LLMs to generate version-specific code. We propose GitChameleon, benchmark that evaluates state-of-the-art code models by requiring them to produce executable code based on version-specific prompts. Then, this code is executed to verify its correctness against expected outputs. By highlighting the limitations of current models in generating accurate version-specific code, GitChameleon provides structured approach to enhance these models and ensure their practical utility in realworld coding environments. In summary, our contributions are highlighted as follows: 1) we introduce novel code completion benchmark GitChameleon consisting of 116 Python-based version conditioning problems, including human written unit tests; 2) we systematically analyze the version-specific performance of stateof-the-art code generation LLMs on API change type, version release year, and specific libraries. 3) we demonstrate the effectiveness of utilizing error log feedback as way to improve version conditioning performance of code generation LLMs. 2https://stackoverflow.co/labs/developer-sentiment-ai-ml/"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Compared to other popular code generation benchmarks, including those evaluating version conditioning, GitChameleon features libraryand version-specific problems with unit tests based on real version changes, closely aligning with practical settings."
        },
        {
            "title": "Execution based Real",
            "content": "HumanEval (Chen et al., 2021) MBPP (Austin et al., 2021) MTPB (Nijkamp et al., 2022) APPS (Hendrycks et al., 2021) CodeContests (Li et al., 2022) JulCe (Agashe et al., 2019) DSP (Chandel et al., 2022) CoNaLa (Yin et al., 2018) DS-1000 (Lai et al., 2023) BigCodeBench (Zhuo et al., 2024) Versicode (Wu et al., 2024b) CodeUpdateArena (Liu et al., 2024b) (Wang et al., 2024) 164 974 115 10000 117 1518049 1119 2879 1000 1140 98692 670 28125 Hand-Written Hand-Written Hand-Written Competitions Competitions Notebooks Notebooks StackOverflow StackOverflow Expert-Curated GitHub, StackOverflow LLM-Generated API change logs GitChameleon (Ours) 116 Handwritten and LLM-assisted - - - - - - - - - -"
        },
        {
            "title": "2 GITCHAMELEON BENCHMARK",
            "content": "We introduce GitChameleon, benchmark that comprises 116 Python-based version conditioning problems focused on popular code libraries. To evaluate LLM performance on GitChameleon, each problem is accompanied by handwritten assertion-based unit tests, enabling thorough executionbased assessment of the outputs generated by the code LLMs. This structured approach enables thorough understanding and categorization of LLM failures in common scenarios involving versionspecific code generation problems. In the following sections, we detail the benchmark statistics, data collection methodology, and sample verification process."
        },
        {
            "title": "2.1 STATISTICS",
            "content": "GitChameleon consists of 116 python-based version conditioned problems based on 11 libraries: PyTorch (Paszke et al., 2019), Geopandas (Jordahl et al., 2020), NLTK (Bird & Loper, 2004), NetworkX (Hagberg et al., 2008), GeoPy3, Gradio (Abid et al., 2019), Scikit-Learn (Buitinck et al., 2013), Matplotlib (Hunter, 2007), PyCaret4, Pandas (pandas development team, 2020; Wes McKinney, 2010) and NumPy (Harris et al., 2020). The samples were collected from version releases over period from the year 2014 to 2023 and exclude legacy and yanked version releases. Using the cl100k base tokenizer, we analyzed the token counts of the GitChameleon samples. The problem statements average 20.4 tokens, and the starter code averages 47.4 tokens, leading to combined average of 67.8 tokens per sample. Including the prompt template utilized for evaluating instruction-tuned LLMs, the total token count across all samples is 19,409 tokens. As demonstrated in Fig. 2b, most of the samples in GitChameleon are from versions of libraries released in the years 2021, 2022 and 2023, with 2021 released version samples accounting for 35% of the total sample count in the benchmark. Since some of the models evaluated on GitChameleon have disclosed their training data cutoff dates, we have ensured that most, if not all, samples fall within the training window of these models. This approach helps to ensure that the models during their training period are likely exposed to the versions on which the samples are based. Fig. 2a shows that NetworkX (Hagberg et al., 2008) and Gradio (Abid et al., 2019) have the most versions in our benchmark (8 and 7, respectively). Meanwhile, PyTorch (Paszke et al., 2019) and NumPy (Harris et al., 2020) have the highest number of samples (18 and 15, respectively), together accounting for 34% of the total sample count. In addition, we annotate each sample with the type of change that is classified into the following categories: Argument or Attribute change: The API call to function, method, or class has change in arguments (e.g. name, order, new, deprecated argument) between versions. 3https://pypi.org/project/geopy/ 4https://pycaret.org/"
        },
        {
            "title": "Preprint",
            "content": "(a) Number of unique versions per library in GitChameleon. (b) Number of samples by version release year. (c) Number of samples by type of change. Figure 2: Fine-grained statistics of the GitChameleon benchmark. (a) The library with the most unique versions in the dataset is networkx with 8, whereas only 1 version of spacy and PyCaret are represented in GitChameleon. (b) Most versions in the dataset were released between 2021-2023, with few versions released in earlier years. (c) The most common type of changes between versions were function name changes and argument/attribute changes, while semantic/output changes were least common. Function Name change: The name of the API call has changed between versions (e.g. pandas.append to pandas.concat). Semantics or Function Behavior change: The semantic / runtime behavior of the API call changed between versions (e.g. returning different type). New feature or additional dependency-based change: feature was introduced in specific version; therefore, to execute the same functionality, model using an older version should make use of an additional dependency (e.g. torch.special was introduced in TORCH 1.10, previously one could use NUMPY for the same). Most samples in the GitChameleon benchmark fall under the Argument or Attribute and Function Name change category, as these are the most frequent and expected types of changes in mature and stable libraries. Differentiating factor Several datasets examine LLM interactions with version-specific code, including Versicode (Wu et al., 2024b), CodeUpdateArena (Liu et al., 2024b), and the dataset by (Wang et al., 2024). While these datasets are valuable, our dataset offers unique and complementary perspective by focusing on the real-world scenario where developers are often constrained to specific library versions due to technical debt. CodeUpdateArena investigates model adaptation to synthetic API changes, we focus our evaluation on real API changes to assess how effectively an LLM can generate code for version-specific changes of library versions that they have been trained with. In contrast, Versicode and Wang et al. (2024)s datasets, while addressing library evolution, primarily rely on string matching for evaluation. Our approach diverges by incorporating executable tests, providing more practical and rigorous assessment of code generation capabilities."
        },
        {
            "title": "Expected Result",
            "content": "# Write function that checks if all np.all(arr) elements in an array are true."
        },
        {
            "title": "Assertion Test",
            "content": "import numpy as np def alltrue_fn(arr): return arr = np.array([1, 1, 1, 1]) result = alltrue_fn(arr) assert result == np.all(arr) Table 2: Example of problem statement derived from changelog entry from Numpy 1.25.0 The examples were manually crafted by the authors, who divided the task among themselves. We compiled list of popular Python libraries, focusing on those with which at least one author was familiar and that had detailed changelogs documenting changes between versions. For each library, we"
        },
        {
            "title": "Preprint",
            "content": "reviewed the changelogs to identify deprecated functions, argument changes, alterations in behavior, and newly introduced functions. For each identified change, we create concise problem statement, write the starter code, define the expected solution, and develop an assertion test. For instance, in Table 2, we illustrate an example based on the changelog for version 1.25.0 of NumPy (Harris et al., 2020), library for scientific computing in Python. This changelog notes that np.alltrue is deprecated. Use np.all instead. We used this change to craft problem statement that tests the LLMs ability to recognize and adapt to version-specific updates. Unit test and evaluation framework verification To assess the correctness of the evaluation framework of GitChameleon, we needed to verify three key aspects: Compilation: Ensure that the starter code compiles successfully. Assertion unit tests: Confirm that the assertion tests function correctly. Dependencies: Verify that all necessary external dependencies are installed, excluding the ones being tested. We used venv to create and manage virtual environments for testing. This process involved installing the appropriate library version and any additional dependencies. We then combined the starter code, expected result, and the assertion test into single script, which was executed to verify all three criteria. We provide pseudocode for our verification process in appendix A.3."
        },
        {
            "title": "3 EMPIRICAL STUDY",
            "content": "We evaluate state-of-the-art large language models (LLMs) using the GitChameleon benchmark to assess their ability to generate version-specific, executable code. This study highlights how well current models adapt to dynamic library versions and produce functionally correct code that passes the provided unit tests."
        },
        {
            "title": "3.1 EXPERIMENTAL SETUP",
            "content": "For each open-source LLM, we downloaded the corresponding Hugging Face (HF) weights and served the models using Text Generation Inference (TGI) for non-instruct models and VLLM (Kwon et al., 2023) for instruction tuned models. We used one NVIDIA 95GB H100 GPU for models with fewer than 70 billion parameters, two GPUs for models more than 70 billion parameters. We configured the generation parameters with top value of 0.95, top of 50, and temperature of 0.3 for Pass@1 and 0.8 for Pass@10. The maximum number of new tokens generated was set to 256. Additionally, we enabled flash attention (Dao et al., 2022) for all models to enhance inference efficiency."
        },
        {
            "title": "3.2 EVALUATION METRICS",
            "content": "To comprehensively evaluate the performance of code generation models using the GitChameleon dataset, we employ range of execution-based metrics. These metrics assess not only the correctness of the generated code, but also its efficiency and adaptability to different versions. Pass@k measures the proportion of problems for which at least one of the top generated solutions passes all assertion tests. This metric provides insight into the models ability to generate functionally correct code. For each problem, we generate code samples, and compute the pass at metric by the corrected formula: 1 2 3 def corrected_pass_at_k(n, c, k=10): if - < k: return 1.0 return 1.0 - np.prod(1.0 - / np.arange(n - + 1, + 1)) For instruct models, we run the models parsed output as standalone code, and for base models, the concatenation of the starting code and models parsed output (completion)."
        },
        {
            "title": "Preprint",
            "content": "Greedy refers to the standard greedy decoding method, where the most probable token from the next-token distribution is chosen deterministically. This is analogous to setting the temperature to 0. Error Feedback adds the error log to the prompt (after executing the generated code from the model with the initial prompt). Then, the pass@k metric is recalculated based on the models generated code using the prompt with error feedback. See appendix A.2 for an example. doc-prompting is method in which the documentation of the library function (specific to the version that the model is asked to use in the prompt) is given in the prompt. See Appendix A.1 for an example."
        },
        {
            "title": "3.3 MAIN RESULTS",
            "content": "We report the performance of both base and instruct-tuned models on the GitChameleon benchmark in Tables 3 and 4, respectively. Our analysis reveals positive correlation between model size and performance in version-specific code generation tasks. For base models, Spearmans rank correlation coefficients are 0.82 for Pass@1 and 0.69 for Pass@10 (both p-values <0.01), indicating that larger models generally perform better. Specifically, DeepSeek-Coder 33B achieved the highest Pass@1 score of 35.7%, highlighting its proficiency in generating correct solutions on the first attempt, while CodeLlama 34B outperformed others at Pass@10 with score of 42.8%, demonstrating its ability to produce correct solutions given multiple attempts."
        },
        {
            "title": "Model",
            "content": "CodeLlama-Python (Rozi`ere et al., 2024) Starcoder2 (Lozhkov et al., 2024) Llama-3 (Dubey et al., 2024) Qwen2 (Yang et al., 2024) Starcoderbase (Li et al., 2023) Starcoder (Li et al., 2023) Deepseek-coder (Guo et al., 2024)"
        },
        {
            "title": "Size",
            "content": "Pass@1 T=0.3 Pass@10 T=0.8 20.41.6 7B 13B 25.81.0 34B 30.61.4 11.91.9 3B 15.51.1 7B 15B 13.71.7 22.31.0 8B 70B 27.23.0 27.41.2 7B 72B 33.22. 13.31.0 1B 15.51.2 3B 20.00.9 7B 15B 16.91.8 36.15.5 36.42.0 42.81.4 27.11.9 23.12.6 27.03.4 32.02.1 41.32.5 37.71.8 39.75.5 20.31.2 26.51.5 31.34.1 30.82. 15B 16.01.2 35.91.9 1.3B 22.02.5 6.7B 31.01.8 33B 35.73.0 28.01.9 36.10.7 37.94.9 Table 3: Base Model Performance Metrics. Deepseek-coder-33B is the strongest model for Pass@1 (temperature 0.3), while CodeLlama-34B is the strongest model when we compute Pass@10 with an increased number of generations (20) sampled at temperature 0.8. We observe that there is strong positive correlation between model size and performance, with Spearmans rank correlation coefficients of 0.82 for Pass@1 and 0.69 for Pass@10. Similarly, for instruct-tuned models, we observe Spearmans rank correlation coefficients of 0.45 for Pass@1 and 0.42 for Pass@10 (both with p-values under 1%), confirming the positive correlation between model size and performance. Phi-3.5-MoE (163.8B) achieved the highest baseline Pass@1 score of 30.9% (33.6% greedy) and highest baseline Pass@10 (40.5%) for OSS models. Overall, Gemini achieves the highest baseline pass@10 (41.4%). Yi-1.5-Chat (34B) slightly outperformed GPT-4o and Gemini at Pass@10 with error feedback with score of 44.1% compared to 43.7% and 43.8%, respectively. Incorporating error feedback led to average improvements of 5.4% in Pass@1 and 4.7% in Pass@10 across instruct-tuned models. Additionally, Pass@10 with n=20 samples showed an average performance improvements of 10.6% for base models and 14.8% for instruct-tuned models over Pass@1 with n=5. These findings highlight that scaling model size, utilizing error feedback, and allowing multiple solution attempts are effective strategies for enhancing the ability of LLMs in handling version-specific code generation tasks. To highlight the uniqueness of our proposed evaluation dataset and difficulty of the samples in GitChameleon, we examine the performance correlation of models on GitChameleon in Figure 3 with three standard code generation evaluation benchmarks: HumanEval (Chen et al., 2021),"
        },
        {
            "title": "Preprint",
            "content": "Model Size (Context) / Pass@1 (T = 0.3) Pass@10 (T = 0.8) Version Baseline + Error Feedback Greedy Zero-shot CoT Baseline + Error Feedback Deepseek-Coder-V2-Lite (DeepSeek-AI et al., 2024) Starcoder2-v0.1 (Lozhkov et al., 2024) CodeLlama (Rozi`ere et al., 2024) Llama-3.1 (Dubey et al., 2024) Llama-3.2 (Dubey et al., 2024) CodeQwen1.5-Chat (Bai et al., 2023) Qwen2 (Yang et al., 2024) Qwen2.5-Coder (Hui et al., 2024) Codestral-v0.1 (https://mistral.ai/news/codestral/) Yi-Coder-Chat (AI et al., 2024) Yi-1.5-Chat (AI et al., 2024) codegemma (Team et al., 2024) stable-code (Pinnaparaju et al., 2024) granite-code (Mishra et al., 2024) Phi-3.5-mini (Abdin et al., 2024) Phi-3.5-MoE (Abdin et al., 2024) Nxcode-CQ-orpo (Hong et al., 2024) GPT (OpenAI et al., 2024) Gemini (Team, 2024) 16B 15B 7B 13B 34B 8B 1B 3B 7B 7B 72B 1.5B 7B 22B 1.5B 9B 6B 9B 34B 7B 3B 3B (128k) 8B (4k) 8B (128k) 20B (8k) 34B (8k) 3.8B 16x3.8B 7B 3.5 4o (2024-05-13) 1.5-pro-001 27.41. 22.00.9 16.41.1 20.30.6 17.40.8 16.00.4 9.70.4 10.60.8 21.40.8 18.30.7 25.00. 19.70.8 21.10.5 25.00.4 18.70.5 25.70.5 17.31.0 20.10.3 20.80.5 17.80.8 14.60. 23.91.1 24.70.9 23.90.7 28.20.6 29.80.9 24.00.7 30.90.8 20.80.5 19.61.0 23.62.7 25.10. 33.40.5 27.91.6 18.81.7 25.61.3 23.31.1 20.50.7 13.00.8 14.51.0 25.90. 26.71.5 30.10.3 22.31.2 24.40.4 31.30.8 22.00.9 30.80.6 23.91.7 24.71.0 26.31.0 23.60. 16.30.9 26.12.0 29.21.5 28.71.1 30.50.8 31.71.2 29.00.9 34.90.7 26.31.3 27.20.8 34.11. 35.90.2 26.7 21.6 19.0 22.4 18.1 17.2 9.5 12. 19.0 19.0 23.3 19.8 22.4 24.1 18.1 25.0 15.5 19.8 21. 17.2 14.7 22.4 24.1 25.0 29.3 31.0 25.9 33.6 19. 19.8 25.0 32.8 26.7 28.4 0.9 0.0 3.4 19. 3.4 11.2 27.6 19.0 24.1 17.2 14.7 34.5 21.6 22. 19.0 21.6 20.7 19.8 17.2 29.3 28.4 29.3 31.0 31.0 19.8 23. 27.6 20.7 27.6 37.1 40.30.7 37.80.3 29.01.5 35.70.8 34.71. 29.20.5 16.20.8 20.21.5 38.61.5 38.00.6 38.20.9 32.71.0 35.61.3 37.10. 32.61.7 38.60.9 32.81.0 30.50.6 35.30.8 33.90.6 23.91.4 33.61.3 38.50.4 35.00.5 37.00.7 38.30.1 35.11. 40.50.5 38.71.8 33.31.0 39.92.4 41.40.5 43.50.9 39.90. 33.11.5 41.21.0 39.91.9 34.90.6 21.10.6 23.92.7 42.40.8 43.30.5 40.70.8 38.80.7 41.50. 40.60.9 37.01.7 41.50.8 37.41.5 39.51.1 44.10.7 38.00.5 25.90.9 36.50.6 42.11.0 38.90.6 38.30.8 39.91. 39.21.0 43.20.1 43.21.3 37.81.7 43.72.4 43.80.6 Table 4: Instruct Model performance metrics. (Top) OSS models, (bottom) closed-sourced models. We observe 5.4% and 4.7% improvement with error feedback in Pass@1 and Pass@10, respectively. Additionally, there is positive correlation between model size and performance, with Spearmans rank correlation coefficients of 0.45 for Pass@1 and 0.42 for Pass@10. . Figure 3: Correlation of GitChameleon with the representative code benchmarks (HumanEval, EvalPlus, and BigCodeBench-Hard split). For each benchmark, the spearman correlation coefficient was 0.37, 0.50, and 0.36, respectively. EvalPlus (Liu et al., 2024a), and the Hard split of BigCodeBench (Zhuo et al., 2024). Although performance on these benchmarks shows positive correlation with that on GitChameleon, the relationship is not strictly predictive, as models that perform well on one or more of these benchmarks do not necessarily achieve equivalent success on GitChameleon. 3.4 IN-DEPTH ANALYSIS OF FINDINGS In this section, we delve deeper into the results obtained from our experiments, analyzing model performance across various dimensions, including model size, year of library release, the type of API changes encountered, and sample difficulty."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Instruct-tuned model performance breakdown by version release year (top) and type of change (bottom): We analyze model performance in terms of pass @ 10 for baseline and with error feedback generation across two dimensions: version release year and type of changes. Darker shaded bars represent values obtained via error feedback generation. Standard deviation is drawn as black line, obtained from 5 random seeds. (Top) Many models perform poorly on 2022, and generally perform worse on more recent versions. (Bottom) All models perform very poorly at semantic changes, indicating an potential area for massive improvement. Most models perform well on function name changes and new feature (with the exception of Llama-3.2-3B, which is also the smallest model in this comparison). Analysis of Performance by Release Date At the top of Figure 4, we present the year-over-year performance of subset of the instruction-finetuned models. The average performance of all models dropped significantly from 2019 (not shown) to 2023. This decline is likely due to the fact that the training datasets contain more data from earlier years, underscoring the need for code LLMs to better adapt to the evolving nature of code libraries and their versions. Interestingly, many models appear to improve with error feedback disproportionately across versions released in the years 20212023. For example, Qwen2-72B and CodeLlama-34B improve more with feedback in 2021-2022 compared to 2023, while GPT-4o and Yi-1.5 Chat 34B improve more with feedback in 2023. This raises question about the extent to which models training data influence the effectiveness of error feedback. Analysis of Performance by Type of API Change At the bottom of Figure 4, our analysis of model performance across different types of API changes in GitChameleon revealed significant variations. The models struggled the most with Semantics or Function Behavior changes. Argument and Attribute changes were the second most challenging. In contrast, models performed better on Function Name changes and New Feature or additional dependency based changes. In general, larger models are more robust to function name, argument/attribute changes, and new feature changes. However, all models perform very poorly on semantic changes, regardless of the availability of error feedback. This indicates weakness of SotA code generation models, and an area for further investigation. Furthermore, error feedback appears to have more significant impact in argument/attribute changes compared to the other types of changes. This indicates that 1) the models may be using the error feedback to directly address failures in version-conditioned"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Comparison of sample and model differences. The left panel shows the distribution of sample difficulty, measured by the frequency of pass@10 scores across all models and seeds. The right panel presents the same distribution, but averaged for specific models across their seeds. Interestingly, individual models tend to exhibit bimodal distributions, indicating they are either consistently good or bad at specific problems. However, the aggregate distribution is less bimodal, suggesting that different models excel at different problems. code generation, rather than non-version-specific errors, and 2) error feedback is most effective for version-specific argument/attribute changes. Sample difficulty analysis Figure 5 shows the distribution of sample difficulty. Notably, individual models (right panel) often display bimodal distributions, meaning they tend to perform consistently well or poorly on specific problems. In contrast, the aggregate distribution (left panel) is not bimodal, indicating that different models perform well on different sets of problems. The availability of error feedback shifts the distribution of the sample-wise difficulty to the right, as expected. Interestingly, some samples are not solved at all across models, even with feedback, and no samples are solved consistently by all models. As further investigation, we plan to qualitatively examine samples that shift from unsolved to solved given error feedback. Finally, the right panel shows that many samples are either easy or hard, however larger models tend to have more medium difficulty samples, indicating that scale can, at least partially, improve version-conditioned generation from unsolved to solved. We qualitatively demonstrate some of these examples in A.2. Figure 6: Doc-prompting vs no doc (greedy decoding) instruct-tuned model performance on subset of libraries. Doc-prompting refers to the prompting technique in which the prompt contains the documentation of the library function specific to the problem at hand. Darker shaded bars represent doc-prompting values, lighter shaded bars represent no doc values."
        },
        {
            "title": "Preprint",
            "content": "Doc-prompting vs no doc Figure 6 shows the instruct-tuned model performance breakdown between doc-prompting and no doc (standard prompt) with greedy decoding for subset of libraries in the dataset. There is no consistent trend for any library, and in some cases, doc-prompting performs worse than the standard prompt (no doc). Moreover, performance across models is generally low for torch compared to the other libraries, and does not improve with doc-prompting. This suggests that the poor performance may be due to vague or difficult problem statements making it challenging for the model to interpret the problem. Error Categorization The objective of error feedback prompting is to allow models to selfcorrect by interpreting the error trace from the initial execution of the generated solution. Figure 7 reveals that across all instruction-based models, the most frequent error type is NAME, indicating gap in contextual awareness within these models. Although error feedback prompting reduces the occurrence of most types of errors, it unexpectedly increases the frequency of SYNTAX and TIMEDOUT errors."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Code LLM training and evaluation protocols Code LLM evaluations mainly revolve around code completion (Zhang et al. (2023); van Dam et al. (2023); Lu et al. (2021)). Existing benchmarks emphasize generic code completion, yet recognized limitation is the inability of code LLM to generate and complete code that requires library and project-level knowledge (Xu & Zhu, 2022), let alone version-level knowledge, which is vital for real-world software applications. Figure 7: Most frequent categories of errors obtained from executing unit tests on model generated code, averaged across models. The most common error by far is NAME errors, which represents variable that was used but not defined. This suggests that models do not grasp the context in many cases. Furthermore, NAME, INDENTATION, ATTRIBUTE, IMPORT, ASSERTIONERROR, and MODULENOTFOUND errors were all reduced with error feedback, while TIMEOUT and SYNTAX errors increased. Recent initiatives address repository-level code understanding by LLMs (Bairi et al. (2023); Shrivastava et al. (2023a;b); Liu et al. (2023); Guo et al. (2024)). Attempts at library-level code generation (Zan et al. (2022)) and consideration of dependencies between files (Guo et al. (2024)) have been made. However, these efforts do not directly address the challenge of accommodating version-sensitive changes, adding complexity. The core issue arises from models being trained on library code without explicit knowledge of library versions or their functional changes. Consequently, when tasked with generating code specifically compatible with particular library version, there is significant risk models often encounter failures. Datasets Existing datasets like HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and MTPB (Nijkamp et al., 2022) provide sets of handwritten prompts and test cases to evaluate code generated by code LLM. However, these datasets are relatively small and lack context regarding models comprehension of repositories. APPS (Hendrycks et al., 2021) and CodeContest (Li et al., 2022) offer challenging datasets with coding competition questions, providing insights into models performance on difficult problems but without focus on library-specific challenges. DSP (Chandel et al., 2022) and DS-1000 (Lai et al., 2023) concentrate on the top data science libraries in Python, while JulCe (Agashe et al., 2019) uses Jupyter Notebooks for training and evaluation, but these notebooks do not necessarily need to be repository-specific. CoNaLa (Yin et al., 2018) contains problems collected from StackOverflow across multiple programming languages, including both library-specific questions and non-library-specific code. More recently, BigCodeBench (Zhuo et al., 2024) is constructed to evaluate the comprehensive capabilities of code generation with tool use and"
        },
        {
            "title": "Preprint",
            "content": "instruction following, which poses great challenge for existing models. Several datasets include version-specific code, such as Versicode (Wu et al., 2024b), CodeUpdateArena (Liu et al., 2024b), and the dataset by Wang et al. Versicodes dataset, compiled from academic papers, Stack Overflow, and library source code, supports tasks like token, line, and block-level code completion and code editing. Unlike our dataset, Versicode evaluates using exact matches. Wang et al.s dataset collects API mappings, such as torch.lstsq() is deprecated in favor of torch.linalg.lstsq(), and evaluates LLMs using exact match, edit similarity, and fixed rate metrics. Although Versicode and Wang et al.s datasets address the evolving nature of libraries, their evaluations are limited to string matching. In contrast, CodeUpdateArena evaluates LLMs ability to adapt to API changes, such as adding boolean flag, by running tests. However, the dataset is synthetic and are not extracted from real-life version changes. For CodeUpdateArena, they also take the approach of training LLMs using the updated API function using docstrings or examples. It then tests if without access to the update during inference, the LLMs reflects the synthetic changes. While these datasets provide valuable resources for training and evaluating models, our GitChameleon dataset advances research into version-conditioned code generation by LLMs. Runnable tests offer insights into LLM adaptability, and GitChameleon further assesses models ability to differentiate between library versions, and successfully use specific version. Implications for Lifelong Learning Continual/lifelong learning in code generation models is in its early stages (Yadav et al., 2023; Weyssow et al., 2023; Wu et al., 2024a; Gao et al., 2023). However, current efforts often focus on artificial sequential tasks rather than utilizing the natural distribution shift in the chronological evolution of code. Notably, continual learning mainly targets mitigating catastrophic forgetting and balancing forwardand backward-transfer on data stream, which may not align optimally with coding environment demands. In coding environments, obsolete or legacy libraries may prompt selective forgetting of irrelevant knowledge, particularly at the library/package level. Previous work, like Caccia et al. (2021), which explores continual adaptation to shifting data distributions, and Bornschein et al. (2022), which focuses on rapid adaptation to new distributions, could serve as foundations for enhancing continual learning in LLMs for code. We anticipate that GitChameleon will inspire advancements in LLMs continual version adaptability, guiding development in this area. Such adaptability would be highly valuable in practical applications like enterprise-level code migration."
        },
        {
            "title": "5 LIMITATIONS",
            "content": "We consider the lack of prompt optimization done for the instruct models as considerable limitation of our analysis. Furthermore, the dataset consists of 116 problems, which is relatively small compared to other code benchmarks. Finally, we do not explore approaches such as RAG, chain of thought, or finetuning on split of our benchmark to observe an upper bound of performance on this task. Future work could explore such approaches using our dataset."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Recognizing the crucial need for code LLM adaptation to evolving code environments, particularly in widely used libraries, we introduce novel and extensive Python-based version-specific benchmark named GitChameleon. By effectively leveraging GitChameleon, we expose the shortcomings of existing state-of-the-art (SoTA) models in producing version-specific code, representing an inaugural effort to draw attention to this challenge. While our work exposes this shortcoming, we acknowledge the datasets limitations. In future endeavors, we aim to enhance the datasets comprehensiveness across various programming languages and frameworks. Additionally, we plan to introduce new tasks that can benefit research on code LLM models using GitChameleon."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "The authors would like to express their sincere appreciation to Alex Gu for his insightful feedback, which significantly contributed to the development of the concept of the project. Furthermore, we extend our gratitude to Victor May for his technical assistance and expertise in support-"
        },
        {
            "title": "Preprint",
            "content": "ing the benchmarking process. The authors acknowledge Bharat Runwal for his assistance during the data curation process and; Deepanway Ghosal and Antonio Orvieto for their valuable feedback on the manuscript. This research was enabled in part by compute resources provided by Mila (mila.quebec) and the Max Planck Institut fur Intelligente Systeme (is.mpg.de). The authors acknowledge support from the Canada CIFAR AI Chair Program and from the Canada Excellence Research Chairs (CERC) Program."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel PerezBecker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. Gradio: Hassle-free sharing and testing of ml models in the wild, 2019. URL https://arxiv. org/abs/1906.02569. Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. Juice: large scale distantly supervised dataset for open domain context-based code generation. arXiv preprint arXiv:1910.02216, 2019. 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. URL https://arxiv.org/ abs/2403.04652. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D. C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B. Ashok, and Shashank Shet. CodePlan: Repository-level Coding using LLMs and Planning, September 2023. URL http://arxiv.org/abs/2309. 12499. arXiv:2309.12499 [cs]. Steven Bird and Edward Loper. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pp. 214217, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/P04-3031."
        },
        {
            "title": "Preprint",
            "content": "Jorg Bornschein, Alexandre Galashov, Ross Hemsley, Amal Rannen-Triki, Yutian Chen, Arslan Chaudhry, Xu Owen He, Arthur Douillard, Massimo Caccia, Qixuang Feng, et al. Nevis22: stream of 100 tasks sampled from 30 years of computer vision research. arXiv preprint arXiv:2211.11747, 2022. Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gael Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pp. 108122, 2013. Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, and Laurent Charlin. Online Fast Adaptation and Knowledge Accumulation: New Approach to Continual Learning, January 2021. URL http://arxiv.org/abs/2003.05856. arXiv:2003.05856 [cs]. Shubham Chandel, Colin Clement, Guillermo Serrato, and Neel Sundaresan. Training and evaluating jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901, 2022. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/ 2205.14135. DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, and Wenfeng Liang. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence, 2024. URL https://arxiv.org/abs/2406.11931. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der"
        },
        {
            "title": "Preprint",
            "content": "Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lind-"
        },
        {
            "title": "Preprint",
            "content": "say, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Shuzheng Gao, Hongyu Zhang, Cuiyun Gao, and Chaozheng Wang. Keeping pace with everincreasing data: Towards continual learning of code intelligence models, 2023. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming the rise of code intelligence, 2024. URL https: //arxiv.org/abs/2401.14196. Aric Hagberg, Pieter Swart, and Daniel Schult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Laboratory (LANL), Los Alamos, NM (United States), 2008. Charles R. Harris, K. Jarrod Millman, Stefan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernandez del Rıo, Mark Wiebe, Pearu Peterson, Pierre Gerard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357362, September 2020. doi: 10.1038/ s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model, 2024. URL https://arxiv.org/abs/2403.07691. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/ abs/2409.12186. J. D. Hunter. Matplotlib: 2d graphics environment. Computing in Science & Engineering, 9(3): 9095, 2007. doi: 10.1109/MCSE.2007.55. Kelsey Jordahl, Joris Van den Bossche, Martin Fleischmann, Jacob Wasserman, James McBride, Jeffrey Gerard, Jeff Tratner, Matthew Perry, Adrian Garcia Badaracco, Carson Farmer, Geir Arne Hjelle, Alan D. Snow, Micah Cochran, Sean Gillies, Lucas Culbertson, Matt Bartos, Nick Eubank, maxalbert, Aleksey Bilogur, Sergio Rey, Christopher Ren, Dani Arribas-Bel, Leah Wasser, Levi John Wolf, Martin Journois, Joshua Wilson, Adam Greenhall, Chris Holdgraf, Filipe, and Francois Leblanc. geopandas/geopandas: v1.0.1, July 2020. URL https://doi.org/10. 5281/zenodo.3946761. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023."
        },
        {
            "title": "Preprint",
            "content": "Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 1831918345. PMLR, 2023. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):10921097, December 2022. ISSN 10959203. doi: 10.1126/science.abq1158. URL http://dx.doi.org/10.1126/science. abq1158. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024a. Tianyang Liu, Canwen Xu, and Julian McAuley. RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems, October 2023. URL http://arxiv.org/abs/2306. 03091. arXiv:2306.03091 [cs]. Zeyu Leo Liu, Shrey Pandit, Xi Ye, Eunsol Choi, and Greg Durrett. Codeupdatearena: Benchmarking knowledge editing on api updates, 2024b. URL https://arxiv.org/abs/2407. 06249. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Munoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024. URL https://arxiv.org/abs/2402.19173. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: machine learning benchmark dataset for code understanding and generation, 2021. Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, Manish Sethi,"
        },
        {
            "title": "Preprint",
            "content": "Xuan-Hong Dang, Pengyuan Li, Kun-Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju Pavuluri, Yan Koyfman, Boris Lublinsky, Maximilien de Bayser, Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Yi Zhou, Chris Johnson, Aanchal Goyal, Hima Patel, Yousaf Shah, Petros Zerfos, Heiko Ludwig, Asim Munawar, Maxwell Crouse, Pavan Kapanipathi, Shweta Salaria, Bob Calio, Sophia Wen, Seetharami Seelam, Brian Belgodere, Carlos Fonseca, Amith Singhee, Nirmit Desai, David D. Cox, Ruchir Puri, and Rameswar Panda. Granite code models: family of open foundation models for code intelligence, 2024. URL https://arxiv.org/abs/2405.04324. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. conversational paradigm for program synthesis. arXiv preprint, 2022. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774."
        },
        {
            "title": "Preprint",
            "content": "The pandas development team. pandas-dev/pandas: Pandas, February 2020. URL https://doi. org/10.5281/zenodo.3509134. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, and Nathan Cooper. Stable code technical report, 2024. URL https://arxiv.org/abs/2404.01226. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open Foundation Models for Code, August 2023. URL http: //arxiv.org/abs/2308.12950. arXiv:2308.12950 [cs]. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. RepoFusion: Training Code Models to Understand Your Repository, June 2023a. URL http: //arxiv.org/abs/2306.10998. arXiv:2306.10998 [cs]. Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-Level Prompt Generation for Large Language Models of Code, June 2023b. URL http://arxiv.org/abs/2206. 12839. arXiv:2206.12839 [cs]. CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea Hu, Christopher A. Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Zhitao Gong, Jane Fine, Tris Warkentin, Ale Jakse Hartman, Bin Ni, Kathy Korevec, Kelly Schaefer, and Scott Huffman. Codegemma: Open code models based on gemma, 2024. URL https://arxiv.org/abs/2406.11409. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. Tim van Dam, Maliheh Izadi, and Arie van Deursen. Enriching source code with contextual data for code completion models: An empirical study, 2023. Chong Wang, Kaifeng Huang, Jian Zhang, Yebo Feng, Lyuye Zhang, Yang Liu, and Xin Peng. How and why llms use deprecated apis in code completion? an empirical study, 2024. URL https://arxiv.org/abs/2406.09834. Wes McKinney. Data Structures for Statistical Computing in Python. In Stefan van der Walt and Jarrod Millman (eds.), Proceedings of the 9th Python in Science Conference, pp. 56 61, 2010. doi: 10.25080/Majora-92bf1922-00a. Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari Sahraoui. On the usage of continual learning for out-of-distribution generalization in pre-trained language models of code. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 14701482, 2023. Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for large language models: survey, 2024a."
        },
        {
            "title": "Preprint",
            "content": "Tongtong Wu, Weigang Wu, Xingyu Wang, Kang Xu, Suyu Ma, Bo Jiang, Ping Yang, Zhenchang Xing, Yuan-Fang Li, and Gholamreza Haffari. Versicode: Towards version-controllable code generation, 2024b. URL https://arxiv.org/abs/2406.07411. Yichen Xu and Yanqiao Zhu. Survey on Pretrained Language Models for Neural Code Intelligence, December 2022. URL http://arxiv.org/abs/2212.10079. arXiv:2212.10079 [cs]. Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Xiaofei Ma, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, and Bing Xiang. Exploring Continual Learning for Code Generation Models, July 2023. URL http: //arxiv.org/abs/2307.02435. arXiv:2307.02435 [cs]. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning to mine aligned code and natural language pairs from stack overflow. In Proceedings of the 15th international conference on mining software repositories, pp. 476486, 2018. Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. CERT: Continual Pre-Training on Sketches for LibraryOriented Code Generation, June 2022. URL http://arxiv.org/abs/2206.06888. arXiv:2206.06888 [cs]. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation, October 2023. URL http://arxiv.org/abs/2303.12570. arXiv:2303.12570 [cs]. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 EXAMPLE OF DOC-PROMPTING Listing 1: Sample pytorch problem with documentation"
        },
        {
            "title": "1 You are to solve this in python using torch-1.11.0. Provide a self-",
            "content": "contained Python script that solves the following problem in markdown code block."
        },
        {
            "title": "3 Here is the documentation for the function to be used:\n4 ‘‘‘\n5 gammaln(x, /, out=None, *, where=True, casting=’same_kind’, order=’K’,",
            "content": "dtype=None, subok=True[, signature, extobj]) 6 7 gammaln(x, out=None) 8 9 Logarithm of the absolute value of the gamma function. 10 11 Defined as 12 13 .. math:: 14 15 ln(lvertGamma(x)rvert) 16 17 where :math:Gamma is the gamma function. For more details on 18 the gamma function, see [dlmf]_. 19 20 Parameters 21 ---------- 22 : array_like Real argument 23 24 out : ndarray, optional"
        },
        {
            "title": "Optional output array for the function results",
            "content": "26 27 Returns 28 ------- 29 scalar or ndarray"
        },
        {
            "title": "Values of the log of the absolute value of gamma",
            "content": "31 32 See Also 33 -------- 34 gammasgn : sign of the gamma function 35 loggamma : principal branch of the logarithm of the gamma function 36 37 Notes 38 ----- 39 It is the same function as the Python standard library function 40 :func:math.lgamma. 41 42 When used in conjunction with gammasgn, this function is useful 43 for working in logspace on the real axis without having to deal 44 with complex numbers via the relation exp(gammaln(x)) = 45 gammasgn(x) * gamma(x). 46 47 For complex-valued log-gamma, use loggamma instead of gammaln. 48 49 References 50 ---------- 51 .. [dlmf] NIST Digital Library of Mathematical Functions 52 https://dlmf.nist.gov/5 53 54 Examples"
        },
        {
            "title": "Preprint",
            "content": "55 -------- 56 >>> import numpy as np 57 >>> import scipy.special as sc 58 59 It has two positive zeros. 60 61 >>> sc.gammaln([1, 2]) 62 array([0., 0.]) 63 64 It has poles at nonpositive integers. 65 66 >>> sc.gammaln([0, -1, -2, -3, -4]) 67 array([inf, inf, inf, inf, inf]) 68 69 It asymptotically approaches * log(x) (Stirlings formula). 70 71 >>> = np.array([1e10, 1e20, 1e40, 1e80]) 72 >>> sc.gammaln(x) 73 array([2.20258509e+11, 4.50517019e+21, 9.11034037e+41, 1.83206807e+82]) 74 >>> * np.log(x) 75 array([2.30258509e+11, 4.60517019e+21, 9.21034037e+41, 1.84206807e+82]) 76 77 78 Please start with the following markdown code block: 79 80 import torch 81 input_tensor = torch.linspace(0, 10, steps=10) 82 # put answer in variable called output 83 84 A.2 QUALITATIVE SAMPLES OF ERROR FEEDBACK Listing 2: Original output from Codestral-22B-v0.1 1 # [problem] 2 Calculate the natural logarithm of the absolute value of the gamma function using pytorch special functions if available in this version , otherwise you may use another library. 3 # [starting code] 4 import torch 5 input_tensor = torch.linspace(0, 10, steps=10) 6 output = 7 # [sample solution] 8 torch.special.gammaln(input_tensor) 9 10 # [model output] 11 import torch 12 input_tensor = torch.linspace(0, 10, steps=10) 13 output = torch.log(torch.abs(torch.special.gamma(input_tensor))) 14 # [Error trace] 15 Traceback (most recent call last): 16 17 AttributeError: module torch.special has no attribute gamma. Did you output = torch.log(torch.abs(torch.special.gamma(input_tensor))) mean: digamma? Listing 3: Regenerated solution with Codestral-22B-v0.1 given error feedback 1 import torch 2 def natural_log_abs_gamma(input_tensor): 3 gamma_tensor = torch.lgamma(input_tensor) natural_log_abs_gamma_tensor = torch.abs(gamma_tensor) return natural_log_abs_gamma_tensor"
        },
        {
            "title": "Preprint",
            "content": "6 input_tensor = torch.linspace(0, 10, steps=10) 7 output = natural_log_abs_gamma(input_tensor) Listing 4: Original output from Phi-3.5-MoE-instruct 1 # torch-1.10.0 2 # [problem] 3 Calculate the error function using pytorch special functions if available in this version, otherwise you may use another library. 4 # [starting code] 5 import torchninput_tensor = torch.linspace(0, 10, steps=10)noutput = 6 # [sample solution] 7 torch.special.erf(input_tensor) 8 9 # [model output] 10 import torch 11 from scipy.special import erf 12 input_tensor = torch.linspace(0, 10, steps=10) 13 def calculate_erf(tensor): 14 16 17 18 19 # Convert tensor to numpy array numpy_array = tensor.numpy() # Calculate erf using scipy.special.erf output = erf(numpy_array) # Convert numpy array back to tensor output_tensor = torch.from_numpy(output) return output_tensor 20 21 output = calculate_erf(input_tensor) 22 # [Error trace] 23 Traceback (most recent call last): 24 25 ModuleNotFoundError: No module named scipy from scipy.special import erf Listing 5: Regenerated solution with Phi-3.5-MoE-instruct given error feedback 1 import torch 2 def compute_error_function(input_tensor): output = torch.erf(input_tensor) 3 return output 4 5 input_tensor = torch.linspace(0, 10, steps=10) 6 output = compute_error_function(input_tensor) Listing 6: Original output with Phi-3.5-MoE-instruct return 1 # numpy-1.21.0 2 # [problem] 3 Write function that rounds an array of numbers. 4 # [starting code] 5 import numpy as np 6 7 def custom_round(arr): 8 9 # [sample solution] 10 np.round_(arr) 11 # [model output] 12 13 import numpy as np 14 def custom_round(arr): return np.round(arr) 15 16 rounded_arr = custom_round(arr) 17 18 19 def test_custom_round(): 20 arr = np.array([1.5, 2.3, 3.7]) result = custom_round(arr)"
        },
        {
            "title": "Preprint",
            "content": "22 23 expected = np.round_(arr) assert np.array_equal(result, expected) 24 25 test_custom_round() 26 # [Error trace] 27 Traceback (most recent call last): rounded_arr = custom_round(arr) 28 29 NameError: name arr is not defined Listing 7: Regenerated solution with Phi-3.5-MoE-instruct given error feedback 1 import numpy as np 2 def custom_round(arr): return np.round(arr) Listing 8: Original output with Phi-3.5-MoE-instruct 1 # numpy-1.25.0 2 # [problem] 3 Write function that rounds an array of numbers. 4 # [starting code] 5 import numpy as np 6 7 def custom_round(arr): 8 9 # [sample solution] 10 np.round(arr) 11 # [model output] 12 13 import numpy as np 14 def custom_round(arr): 15 return np.round_(arr) return 17 18 def test_custom_round(): 19 arr = np.array([1.5, 2.3, 3.7]) result = custom_round(arr) expected = np.round(arr) assert np.array_equal(result, expected) 20 21 22 23 24 test_custom_round() 25 # [Error trace] 26 DeprecationWarning: round_ is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use round instead. 27 result = custom_round(arr) Listing 9: Regenerated solution with Phi-3.5-MoE-instruct given error feedback 1 import numpy as np 2 def custom_round(arr): 3 return np.round(arr) Listing 10: Original output with Phi-3.5-MoE-instruct 1 # pandas-1.4.0 2 # [problem] 3 Predict the correct type. 4 # [starting code] 5 import pandas as pdnindex = pd.Index([1, 2, 3], dtype=int32) nis_correct_type = index.dtype == 6 # [sample solution] 7 8 # [model output] int"
        },
        {
            "title": "Preprint",
            "content": "9 10 import pandas as pd 11 def check_dtype(index): 12 is_correct_type = check_dtype(index) 13 # [Error trace] 14 Sorry: IndentationError: expected an indented block after function definition on line 3 Listing 11: Regenerated solution with Phi-3.5-MoE-instruct given error feedback 1 import pandas as pd 2 def is_correct_type(index, expected_dtype): 3 4 index = pd.Index([1, 2, 3], dtype=int32) return index.dtype == expected_dtype Listing 12: Original output with Phi-3.5-MoE-instruct 1 # gradio-2.9.2 2 # [problem] 3 Write function that takes an image input and returns textbox output. 4 # [starting code] 5 import gradio as gr 6 def process_image(image): 7 return \"Processed\" 8 9 iface = gr.Interface 10 # [sample solution] 11 (fn=process_image, inputs=gr.inputs.Image(), outputs=gr.outputs.Textbox() ) 12 # [model output] 13 14 import gradio as gr 15 def process_image(image): 16 19 20 iface = gr.Interface( fn=process_image, 21 inputs=gr.Image(), outputs=\"text\" 22 # Perform processing on the image here # For simplicity, were just returning the image dimensions width, height = image.shape[1], image.shape[0] return f\"Width: {width}, Height: {height}\" 23 24 ) 25 # [Error trace] 26 Traceback (most recent call last): 27 28 AttributeError: module gradio has no attribute Image inputs=gr.Image(), Listing 13: Regenerated solution with Phi-3.5-MoE-instruct given error feedback 1 import gradio as gr 2 from PIL import Image 3 import numpy as np 4 def process_image(image): # Convert gradio image to PIL Image pil_image = Image.open(image) # Process the image (example: convert to grayscale) processed_image = pil_image.convert(L) # Convert processed image back to numpy array processed_image_array = np.array(processed_image) # Return the processed image array as string return str(processed_image_array.tolist()) 25 18 6 7 8 9 11 12 13"
        },
        {
            "title": "Preprint",
            "content": "16 iface = gr.Interface( fn=process_image, 17 inputs=gr.inputs.Image(shape=(None, None, 3)), outputs=gr.outputs.Textbox() 18 19 20 ) Listing 14: Original output with Phi-3.5-MoE-instruct 1 # networkx-2.5 2 # [problem] 3 4 Write function that returns the naive greedy modularity communities for graph. return nx.community. 5 6 # [starting code] 7 import networkx as nx 8 def naive_modularity_communities(G): 9 10 # [sample solution] 11 naive_greedy_modularity_communities(G) 12 # [model output] 13 14 import networkx as nx 15 def naive_modularity_communities(G): 16 communities = [] while G.number_of_nodes() > 0: node = max(G.degree, key=lambda x: x[1])[0] community = nx.algorithms.community.greedy_modularity_communities (G, nbunch=[node]) communities.append(community) G.remove_nodes_from(community) return communities 22 23 = nx.karate_club_graph() 24 communities = naive_modularity_communities(G) 25 26 = nx.karate_club_graph() 27 # [Error trace] 28 Traceback (most recent call last): 29 community = nx.algorithms.community.greedy_modularity_communities(G, nbunch=[node])"
        },
        {
            "title": "30 TypeError: greedy_modularity_communities() got an unexpected keyword",
            "content": "argument nbunch Listing 15: Regenerated solution with Phi-3.5-MoE-instruct given error feedback 1 import networkx as nx 2 def naive_modularity_communities(G): 3 communities = [] unvisited_nodes = list(G.nodes) while unvisited_nodes: current_node = unvisited_nodes[0] community = nx.algorithms.community.greedy_modularity_communities (G, start_nodes=[current_node]) visited_nodes = list(community[0]) unvisited_nodes = list(set(unvisited_nodes) - set(visited_nodes)) communities.append(visited_nodes) 17 18 19 20 4 5 6 7 8 10 11 12 return communities 13 14 = nx.karate_club_graph() 15 result = naive_modularity_communities(G) Listing 16: Original output with gpt4o"
        },
        {
            "title": "Preprint",
            "content": "1 # torch-1.10.0 2 # [problem] 3 Calculate the complementary error function using pytorchs special functions if available in this version, otherwise you may use another library. 4 # [starting code] 5 import torchninput_tensor = torch.linspace(0, 10, steps=10)noutput = 6 # [sample solution] 7 torch.special.erfc(input_tensor) 8 # [model output] 9 10 import torch 11 import scipy.special 12 def complementary_error_function(x): 13 # Check if special.erfc is available in the current version of"
        },
        {
            "title": "PyTorch",
            "content": "if hasattr(torch.special, erfc): return torch.special.erfc(x) else: # Use scipys erfc function if PyTorchs is not available x_np = x.detach().numpy() # Convert the tensor to numpy array # Calculate erfc using scipy erfc_np = scipy.special.erfc(x_np) # Convert the result back to return torch.from_numpy(erfc_np) 14 16 17 18 19 20 tensor 21 input_tensor = torch.linspace(0, 10, steps=10) 22 output = complementary_error_function(input_tensor) 23 # [Error trace] 24 Traceback (most recent call last): 25 26 ModuleNotFoundError: No module named scipy import scipy.special Listing 17: Regenerated solution with gpt4o given error feedback 1 import torch 2 def complementary_error_function(x): 3 # Check if special.erfc is available in the current version of"
        },
        {
            "title": "PyTorch",
            "content": "if hasattr(torch.special, erfc): return torch.special.erfc(x) else: # Raise an error if PyTorchs special.erfc is not available and scipy is not installed raise ImportError(\"PyTorch version does not support special.erfc 4 5 6 7 8 and scipy is not installed.\") 9 input_tensor = torch.linspace(0, 10, steps=10) 10 output = complementary_error_function(input_tensor) Listing 18: Original output with gpt4o 1 # scikit-learn-1.3 2 # [problem] 3 Retrieve and list all available scorer names, ensuring they are returned in list format. 4 # [starting code] 5 from sklearn import metricsnscorer_names = 6 # [sample solution] 7 metrics.get_scorer_names() 8 # [model output] 9 10 from sklearn import metrics 11 scorer_names = list(metrics.SCORERS.keys()) 12 # [Error trace] 13 Traceback (most recent call last):"
        },
        {
            "title": "14 AttributeError: module ’sklearn.metrics’ has no attribute ’SCORERS’",
            "content": "Listing 19: Regenerated solution with gpt4o given error feedback 1 from sklearn.metrics import get_scorer_names 2 scorer_names = list(get_scorer_names()) 3 conditions = isinstance(scorer_names, list) and len(scorer_names) > 0 Listing 20: Original output with gpt4o 1 # gradio-3.36.0 2 # [problem] 3 Write function that displays an image using Gradio where you cannot share the image. 4 # [starting code] 5 import gradio as gr 6 def display_image(): return \"https://image_placeholder.com/42\" 8 9 iface = gr.Interface 10 # [sample solution] 11 (fn=display_image, inputs=[], outputs=gr.Image(show_share_button=False)) 12 13 # [model output] 14 15 import gradio as gr 16 def display_image(): 17 # This is just placeholder URL; replace it with your actual image URL or file path. return \"https://via.placeholder.com/150\" 18 19 iface = gr.Interface( fn=display_image, 20 inputs=None, outputs=gr.Image(label=\"Display Image\", type=\"auto\"), live=False # Ensure live updates are off 21 22 23 24 ) 25 # [Error trace] 26 line 116, in __init__ raise ValueError( 27 28 ValueError: Invalid value for parameter type: auto. Please choose from one of: [numpy, pil, filepath] Listing 21: Regenerated solution with gpt4o given error feedback 1 import gradio as gr 2 def display_image(): 3 # This is just placeholder URL; replace it with your actual image URL or file path. return \"https://via.placeholder.com/150\" 4 5 iface = gr.Interface( fn=display_image, 6 inputs=None, outputs=gr.Image(label=\"Display Image\", type=\"filepath\"), live=False # Ensure live updates are off 7 8 9 10 ) 11 iface.share = False Listing 22: Original output with gpt4o 1 # gradio-3.24.0 2 # [problem] 3 Write function that takes an image input and returns label output. 4 # [starting code]"
        },
        {
            "title": "Preprint",
            "content": "5 import gradio as gr 6 def process_image(image): 7 return \"Processed\" 8 9 iface = gr.Interface 10 # [sample solution] 11 (fn=process_image, inputs=gr.Image(), outputs=gr.Label()) 12 # [model output] 13 14 import gradio as gr 15 def process_image(image): 16 17 # Placeholder function to process the image and return label # You can add your image processing and model inference logic here return \"Processed\" 18 19 iface = gr.Interface( fn=process_image, 20 inputs=gr.inputs.Image(), # The input component (an image) outputs=gr.outputs.Text() # The output component (a text label) 22 23 ) 24 # [Error trace] 25 python3.10/site-packages/gradio/inputs.py:257: UserWarning: Usage of # The function to process the image 21 gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components warnings.warn( 26 27 python3.10/site-packages/gradio/deprecation.py:40: UserWarning: optional parameter is deprecated, and it has no effect warnings.warn(value) 28 29 Traceback (most recent call last): 30 31 AttributeError: module gradio.outputs has no attribute Text outputs=gr.outputs.Text() # The output component (a text label) Listing 23: Regenerated solution with gpt4o given error feedback 4 # Placeholder function to process the image and return label # You can add your image processing and model inference logic here return \"Processed\" 1 import gradio as gr 2 def process_image(image): 3 5 6 iface = gr.Interface( fn=process_image, 7 inputs=gr.Image(), outputs=gr.Label() 8 9 10 ) # The function to process the image # The input component (an image) # The output component (a text label) Listing 24: Original output with gpt4o 1 # networkx-2.5 2 # [problem] 3 4 Write function that accesses the first edge of NetworkX graph. 5 6 # [starting code] 7 import networkx as nx 8 def get_first_edge(G): 9 10 # [sample solution] 11 list(G.edges)[0] 12 # [model output] 13 14 import networkx as nx 15 def get_first_edge(G): 16 return # Check if the graph has any edges if G.number_of_edges() == 0: 18 return None"
        },
        {
            "title": "Preprint",
            "content": "19 20 21 # Access the first edge first_edge = next(G.edges(data=True)) return first_edge 22 23 = nx.karate_club_graph() 24 # [Error trace] 25 Traceback (most recent call last): 26 line 8, in get_first_edge first_edge = next(G.edges(data=True)) 27 28 TypeError: EdgeDataView object is not an iterator Listing 25: Regenerated solution with gpt4o given error feedback 1 import networkx as nx 2 def get_first_edge(G): 3 # Check if the graph has any edges if G.number_of_edges() == 0: 4 return None 6 7 # Access the first edge first_edge = list(G.edges(data=True))[0] return first_edge 8 9 = nx.karate_club_graph() 10 first_edge = get_first_edge(G) A.3 PSEUDOCODE FOR OUR VERIFICAITON PROCESS 1 # Create and activate virtual environment 2 Run: \"python -m venv venv\" 3 Run: \"source venv/bin/activate\" 4 5 # Install specified library and version 6 Run: \"pip install $library==$version\" 7 8 # Install additional dependencies if specified 9 If additional_dep: Run: \"pip install $additional_dep\" 11 12 # Combine code snippets 13 complete_code = starter_code + expected_output + test 14 15 # Run the combined code with timeout 16 Run: \"timeout 60 python -c $complete_code\" 17 18 # Capture and print exit code 19 exit_code = LastCommandExitCode() 20 Print: \"THIS WAS THE EXIT CODE: $exit_code\" 21 22 # Print the complete code 23 Print: complete_code 24 25 # Deactivate and remove virtual environment 26 Run: \"deactivate\" 27 Run: \"rm -rf venv\" Each sample was validated using this method to ensure that it functioned as intended."
        }
    ],
    "affiliations": [
        "Mila - Quebec AI Institute",
        "MPI-IS Tubingen, ELLIS Tubingen"
    ]
}