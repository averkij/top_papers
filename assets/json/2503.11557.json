{
    "paper_title": "VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity",
    "authors": [
        "Jing Bi",
        "Junjia Guo",
        "Susan Liang",
        "Guangyu Sun",
        "Luchuan Song",
        "Yunlong Tang",
        "Jinxi He",
        "Jiarui Wu",
        "Ali Vosoughi",
        "Chen Chen",
        "Chenliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 5 5 1 1 . 3 0 5 2 : r VERIFY: Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity Jing Bi1 Junjia Guo1 Jinxi He1 Susan Liang1 Guangyu Sun2 Luchuan Song1 Yunlong Tang1 Jiarui Wu1 Ali Vosoughi1 Chen Chen2 Chenliang Xu1 1University of Rochester 2University of Central Florida {jing.bi, jguo40, sliang22, lsong11, yunlong.tang, ali.vosoughi, chenliang.xu}@rochester.edu {jhe44, jwu114}@u.rochester.edu guangyu@ucf.edu, chen.chen@crcv.ucf.edu"
        },
        {
            "title": "Abstract",
            "content": "Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page. 1. Introduction Visual reasoning is key aspect of human intelligence, shaping how individuals acquire knowledge and interpret the world [20, 26, 40]. As key mechanism for concept formation and understanding, it has been the focus of extensive research [35]. However, classic models that overlook reasoning abilities, often trained on small-scale datasets with limited access to formalized reasoning, struggle to generalize beyond specific tasks and domains [27, 35, 36, 44]. In contrast, recent advances in Multimodal Large LanFigure 1. This example demonstrates that current MLLMs primarily depend on straightforward visual signals (e.g., letters) for reasoning, frequently neglecting patterns based on other characteristics, such as shapes or line properties. VERIFY delivers humanannotated reasoning paths to enhance the evaluation and comprehension of why and when models fail. guage Models (MLLM) [6, 28, 32, 34, 47, 60, 62] have achieved near-human performance in various visionlanguage tasks [7, 17, 29, 45, 48]. Nevertheless, existing benchmarks predominantly evaluate recognition-based skills, such as object detection, image captioning, and optical character recognition (OCR), which primarily rely on perceptual abilities [11, 31, 54, 56]. Recent studies have started evaluating MLLM performance in more cognitively demanding domains, including mathematical reasoning and chart-based visual understanding [33, 37], which require 1 reasoning of numerical and logical operations. However, these benchmarks often conflate reasoning ability with domain knowledge drawn from the underlying LLM, making it hard to isolate and accurately assess reasoning based solely on visual information. Additionally, most evaluations emphasize accuracy as their primary metric while neglecting the decision-making quality and the depth of the reasoning process, such that even when models correctly identify the visual pattern, they often struggle to pinpoint the right answer. This gap raises an essential research question: To what extent can MLLMs genuinely perform visual reasoning and demonstrate systematic cognitive understanding? To address this gap, we introduce VERIFY (Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity), new benchmark designed to rigorously assess the reasoning fidelity of leading MLLMs, covering both open-source and proprietary systems. Inspired by The Ravens Progressive Matrices[5], VERIFY isolates visual reasoning ability by minimizing reliance on textual input, compelling models to reason primarily through visual information. As shown in Figure 1, each problem includes concise textual question and one image with answer options. This design isolates visual reasoning by requiring models to predominantly rely on non-textual cues. By providing human-annotated reasoning paths, VERIFY not only highlights when such biases occur but also offers insights into why models might misinterpret or overemphasize specific elements. This approach ultimately aims to foster more balanced evaluation of visual reasoning, encouraging models to integrate both semantic and abstract visual cues for more accurate and holistic understanding. VERIFY encompasses diverse and challenging set of visual reasoning tasks requiring generalization and abstraction, as exemplified in Figure 2, surpassing existing benchmarks in both diversity and difficulty as shown in Table 1. Leveraging this benchmark, we evaluate leading MLLMs from OpenAI, Google, and various most recent open-source models to provide comprehensive analysis of their visual reasoning abilities. Our results reveal that even the most advanced models achieve an accuracy of only 21.7%, which is below random chance (25%). Furthermore, we identify shortcomings in current automated evaluation methods [51], noting their insufficiency in capturing nuanced differences between models. Inspired by foundational studies in human visual reasoning [5], we propose novel metrics designed to evaluate the reasoning process itself, providing more refined assessment beyond accuracy. Our findings emphasize that for advanced visual reasoning models, the reasoning path holds greater importance than merely obtaining the correct answer, as models typically explore multiple reasoning paths during problem-solving. Extensive qualitative analyses of the models reasoning patterns reveal critical imbalance, where models proficient in perceiving visual details often struggle to deduce correct answers, while those excelling in abstract reasoning frequently overlook essential visual cues, highlighting deficiency in balanced reasoning capabilities. In summary, our contributions are as follows: VERIFY: We introduce novel visual reasoning benchmark isolating reasoning ability by minimizing domainspecific and linguistic biases. To the best of our knowledge, it is the first to provide human-annotated, clear, and evaluable reasoning trajectories for each question. Beyond Accuracy: We proposal novel metrics that assess the reasoning process itself, providing more nuanced and comprehensive evaluation beyond traditional accuracy, capturing the depth and quality of reasoning. Comprehensive Benchmarking: VERIFY systematically evaluates leading MLLMs on reasoning capabilities, perceptual alignment, and logical consistency, highlighting the impact of Chain-of-Thought (CoT) reasoning and the challenges in transitioning from abstraction to answer deduction, offering valuable insights. 2. Related Work Visual Reasoning. Visual reasoning involves comprehending and manipulating visual information to infer logical relationships [3, 15]. Early tasks like VQAv1 and VQAv2 [3, 15] focused on object-centric question answering, while CLEVR [25] and GQA [19] introduced compositional programmatic queries for structured reasoning. Beyond static images, video-based datasets such as CoPhy and Space [4, 12, 21] emphasize physical interactions and causal reasoning in motion. Our work aligns with abstract visual reasoning that does not require external domain knowledge, like RAVEN, SVRT, CVR, Bongard-HOI, and BongardLOGO [13, 23, 38, 58, 59], which explore relational patterns among shapes. However, as Van der Maas et al. [52] note, many existing words have rigid configurations that hinder generalization to complex problems. Reasoning Benchmarks. Most popular language reasoning benchmarks focus on text-based QA, mathematics, or code inference [10, 16, 42, 43], where step-by-step In the visolutions can be programmatically verified. sual domain, benchmarks like CLEVR [25], GQA [19], CLEVRER [55], Bongard [38], RAVEN [59], MMComposition [18], and VidComposition [46] introduce compositional reasoning over images or videos. Recent efforts [22] incorporate multi-modal CoT prompting, often blending external knowledge with visual cues. Among the most relevant is MM-IQ [8], which targets abstract visual puzzles but lacks step-by-step annotations to clarify reasoning transitions. In contrast, our work provides explicit human-annotated reasoning paths, offering more transparent, interpretable framework for fine-grained evaluation beyond the accuracy. Figure 2. Categories from the VERIFY dataset cover range of patterns, from logical operations to 3D geometry and mathematics. The right panel presents human reasoning path, demonstrating how visual transformations, rotations, and inside-outside shifts lead to the final answer. We encourage readers to test these examples with MLLM models (e.g., o1 or Gemini ) to assess their reasoning capabilities. Automatic Reasoning-Path Evaluation. Interest is growing in evaluating not just the accuracy model provides, but also how it draws its conclusions. 1. Embedding Methods. Golovneva et al. [14] use embedding similarities to compare reasoning chains, effective for text but weaker for capturing geometric or visual relationships. 2. Symbolic Methods. Structured parsing, such as subject-verb-object [39] or formal proofs [41], struggles with complex visual puzzles due to inconsistent symbolic representation of spatial relations. 3. Process Reward Models. Lightman et al. [30] introduce PRM800K for mathematical reasoning, but training such models requires vast amounts of data, making large-scale visual adaptation costly. 4. Prompting Methods. LLMs can verify reasoning by prompting without ground-truth references, depending solely on internal coherence. [51, 57] Applying these methods on visual reasoning yields poor results, as LLMs struggle to differentiate visual nuances. 3. Data Collection Our dataset stands apart from existing ones that rely on rendering engines to ensure compositionality. While synthesized data can be abundant, it often lacks the richness and complexity inherent in real-world scenarios. We curate our dataset primarily from publicly available questions in Chinas National Civil Servants Examination, rigorous postgraduate-level assessment. To mitigate any potential bias, we have incorporated questions from different provinces to ensure diverse and comprehensive dataset. Focusing on problems that require logical reasoning rather than simple recognition, we deliberately select complex questions demanding trial and error with multiple visual patterns. This ensures both correctness and validity while establishing higher standard of difficulty than existing datasets. To ensure quality and clarity, we included only questions with definitive answers, filtering out ambiguous ones, those with multiple correct answers, or requiring external knowledge. Details about selection criteria are in the supplementary materials. 3.1. Reasoning Path One of the key contributions of our dataset is the reasoning path, which consists of sequence of logical steps leading to the correct answer. The common practice [8, 61] is to rely on models to generate reasoning paths, which are then reviewed and evaluated by human annotators or an external verifier. While existing methods may work for moderately difficult tasks, they fail when faced with more challenging reasoning problems like ours. In our initial experiments, we found that state-of-the-art models struggle to generate correct reasoning paths, even as starting point. As result, we manually annotate each reasoning path to ensure accuracy. Each path is independently labeled by three annotators, and any discrepancies are resolved through discussion. Further details will be provided in supplementary materials. 3 Figure 3. We divide the reasoning process into four key stages inspired by human visual reasoning: perception, recognition, abstraction, and deduction. Unlike general visual tasks, where perception involves detecting raw visual features, humans often have implicit perception because the provided visual elements are already structured for direct recognition of useful components. Even for shown complex problems, model with strong visual abilitieslike Geminican effectively analyze patterns and logical structures to determine the correct answer. 3.2. Visual Reasoning Pattern To solve the proposed visual reasoning problem, humans must first identify the crucial visual elements necessary for detecting patterns. However, these elements are not always immediately apparent, and we often fall prey to Semantic Dominance Bias, which causes us to focus too much on the semantic meaning of figure rather than carefully analyzing its shape or other visual properties. To systematically approach the problem, we define the reasoning process through four stages, as shown in Figure 3, which also I. Perception, where the enables structured evaluation. raw visual input is processed, and features such as shapes, colors, and orientations are detected; II. Recognition, which involves extracting useful visual aspects from Perception, selecting the most relevant features that contribute to understanding the pattern; III. Abstraction, where these extracted visual aspects are used to identify patterns by filtering out unnecessary details and focusing on meaningful relationships and IV. Deduction, the final stage, in which logical reasoning is applied to infer missing details or predict patterns based on the extracted abstractions. For humans, the Perception stage is often implicit and may even be bypassed, as some initially relevant visual elements may later prove irrelevant. As result, Recognition becomes the first explicit step, where annotators identify only the essential visual aspects and structure the reasoning path accordingly. After annotating the reasoning path, we summarize and identify key visual reasoning patterns for subsequent evaluation, as illustrated in Figure 2. While MARVEL [24] and MM-IQ [8] provided useful insights, they proved insufficient for our dataset. To address this limitation, we extracted refined patterns as follows that build upon existing ones while introducing unique structural distinctions. Notably, we separated symmetry from geometric transformations to enhance clarity. I. Spatial Position (SP): How elements are positioned (e.g., top-bottom, center-corner) and how they change through II. Element Operamovement, rotation, or progression. tions (EO): Focuses on interactions between shapes, such as layering, merging, splitting, and reassembly. III. Geometric Transformations (GT): Involves transformations like rotation, translation, and reflection, often occurring in seIV. Gradual Change (GC): Tracks progressive quences. changes in shape, size, or position, forming logical patterns. V. Mathematics (MA): Observes numerical aspects, including element counts and repetitive patterns. VI. Symmetry and Axes (SA): Identifies different types of symmetry, including axial, central, and rotational. VII. Line Properties (LP): Analyzes stroke characteristics such as type, angle, continuity, and curvature. VIII. Structural Composition and Spatial Division (SC): Examines enclosures, layering, connections, and shading to uncover deeper structural patterns. IX. Positional Relationships (PR): Investigates how elements relate to each other in terms of position, alignment, and directional shifts. Figure 4. Category distribution of VERIFY: This chart illustrates the distribution of various mathematical and geometric concepts, with the largest segment, Mathematics, accounting for 24.3%. The remaining segments are fairly evenly distributed, reflecting balanced emphasis on all visual patterns. 3.3. Dataset Statistics The table1 compares various datasets used for reasoning tasks, highlighting differences in size, source, difficulty, Name SVRT [13] RAVEN [59] ARC [9] DOPT [53] IQTest [33] MARVEL [24] MM-IQ [8] VERIFY Source Synthesized Synthesized Synthesized Synthesized Mixed Web Size 23 14,000 600 95,200 228 770 2,710 Mixed 600 RP Difficulty Year Easy 2011 2019 Easy 2019 Medium 2020 Easy 2023 Medium 2024 Medium 2025 Medium 2025 National Exam Hard Table 1. Comparison of datasets from the corresponding paper. RP indicates the inclusion of reasoning path, and the difficulty level is based on both the human score and the participants education level as reported in the original study. and reasoning complexity. Among them, VERIFY stands out as the most challenging due to its high quality, realworld origin (national exams), deep reasoning complexity, and rigorous difficulty level. Compared to existing datasets, VERIFY is more challenging and diverse. While it has fewer samples than large-scale synthetic datasets, it surpasses other handcrafted datasets in both the number of samples and the diversity of reasoning patterns. Unlike synthetic datasets that focus on pattern recognition, VERIFY includes real-world, high-stakes reasoning tasks, making it more reliable benchmark for evaluating cognitive abilities. 4. Evaluation Given visual reasoning problem q, generated reasoning path ˆh = {ˆh1, . . . , ˆhN }, and generated answer ˆa produced by MLLMs, our goal is to evaluate the quality of both the reasoning process and the final answer. In our case, the ground truth answer is available, along with reference solution steps = {h1, . . . , hM }, as reliable benchmark. Unlike previous works that focus solely on answer matchingignoring the entire chain of reasoningwe independently evaluate both the final answer and the reasoning path. This approach provides more informative insights into the models reasoning process. Our empirical study reveals that models can sometimes generate the correct answer with an incorrect reasoning path or produce an incorrect answer despite following correct reasoning process. By separately assessing these components, we gain more comprehensive understanding of models. 4.1. Failure case of existing evaluation metrics Recent studies [51, 57] have explored the use of open source LLMs as automated evaluators for reasoning processes, by comparing each step of the whole path against reference reasoning trajectory. This trend aligns with the growing interest in using LLMs as judges to provide consistent and scalable evaluation frameworks. During the early stages of designing our evaluation protocol, we tested this methodology using three of the latest highcapacity LLMs as judges: DeepSeek-R1:32B, DeepSeekR1:70B, and LLaMA3.3-70B. We randomly selected 40 out of 600 samples, using the o1 response as the model response, and evaluated reasoning paths by comparing them against multiple reference responses I. o1: The reasoning path and final answer produced by the OpenAI-o1 model. II. Ground Truth Reasoning (GTR): human-annotated reasoning path that correctly leads to the answer. III. Expanded Ground Truth Reasoning (GTR+): Leverage LLM to expand and segment the GTR into three detailed stages: Recognition, Abstraction and Deduction. IV. Expanded Ground Truth Reasoning with Pattern (GTR+P): refined version that incorporates human-labeled pattern annotations, offering additional structural insights into the reasoning process. Scores were assigned on scale from 0 to 10 based on fidelity to human reasoning. The evaluation process involved assessing the alignment of model-generated reasoning paths with ground truth references. Additionally, we conducted two types of comparisons: I. r+c: The models response includes both its reasoning path and the final selected choice, with the ground truth answer. II. r: The models response includes only the reasoning path, without the final choice. Based on an accuracy of 0.15 for o1 across 40 problems, several key observations emerge: r1 r2 Model Score (r+c) Score GTR+P GTR+ GTR GTR+P GTR+ GTR o1 o1 o1 GTR GTR+P GTR GTR+ o1 o1 o1 GTR GTR+P GTR GTR+ o1 o1 o1 GTR GTR+P GTR GTR+ GTR+P GTR+ GTR deepseek-r1:32b deepseek-r1:32b deepseek-r1:32b deepseek-r1:32b deepseek-r1:32b deepseek-r1:70b deepseek-r1:70b deepseek-r1:70b deepseek-r1:70b deepseek-r1:70b llama3.3 llama3.3 llama3.3 llama3.3 llama3.3 7.85 7.60 7.80 8.45 8.70 8.10 8.35 7.65 8.65 8.80 8.05 8.15 7.95 9.05 9. 6.40 6.80 6.35 7.15 8.00 7.80 8.10 6.60 8.65 8.65 8.20 8.35 8.20 9.40 9.85 Table 2. Using an LLM as judge to compare the relevance scores of two reasoning paths often falls short in correctness. r1 and r2 are the two reasoning paths we compare against. Overestimation in LLM Judges. Table 2 shows striking discrepancy: human evaluators marked only 6 out of 40 answers as correct, yet LLMs consistently assigned scores in the range of 6.359.85. In our experiment, we ask each LLM to compare two reasoning paths and score them on scale from 1 to 10 based on how well they align. This disparity indicates that LLMs tend to overestimate reasoning quality, often relying on superficial similarity rather than rigorous evaluation of logical correctness. Notably, LLaMA3.3-70B assigns comparably high scores to both 5 Figure 5. This diagram illustrates three proposed evaluation metrics: Agreement, which extracts common elements across model answers and compares their consistency; Match, which assesses alignment between model responses and ground truth across Recognition, Abstraction, and Deduction; and Perception Similarity, which measures the similarity of extracted perception attributes across different models. GTR and its structured variants (GTR+ and GTR+P), while it awards slightly lower scores to o1 relative to the ground truth. Nonetheless, the overestimation bias persists, underscoring the challenge LLMs face in adequately penalizing flawed reasoning. Bias Toward Completed Choices. Surprisingly, LLMs tend to assign higher scores when the final choice is included (r+c), even though they should recognize that the answer is incorrect and penalize it accordingly. We suspect that including the choice makes the reasoning path feel more complete, biasing LLM judges toward more favorable answer. Our experiment reveals that LLMs prioritize completeness over actual correctness, favoring coherence even at the expense of reasoning accuracy. This bias results in failure to properly penalize flawed logic, leading to overestimation and missed nuances in visual reasoning evaluation. 4.2. Evaluation Framework Our evaluation framework is designed to systematically assess the reasoning capabilities of models by decomposing their responses into distinct cognitive stages. To achieve this, we extract structured elements from both the ground truth and model-generated answers, facilitating detailed comparison. 4.2.1. Decomposition of Reasoning Stages Based on the ground truth, we categorize the reasoning process into three key stages: Recognition (R): Identifying key visual elements necessary for solving the problem. Abstraction (A): Inferring higher-level patterns or relationships from R. Deduction (D): Applying logical reasoning to the abstracted patterns to reach conclusion. For model responses, we further introduce Perception ( ˆP ) stage, which represents the raw visual elements detected by the model before recognition and reasoning take place. This leads to an extracted model reasoning path: ˆh = { ˆP , ˆR, ˆA, ˆD} 4.2.2. Evaluation of Reasoning Stages For R, A, and D, we perform direct matching against the ground truth using strong reasoning model, QWQ[50]. This ensures robust comparison while accounting for linguistic variability in model-generated responses. 4.2.3. Perception Evaluation Unlike R, A, and D, the evaluation of perception ( ˆP ) lacks well-defined ground truth, as manually verifying each re6 Model COT GT MA GC LP EO PR SA SC SP ALL Qwen2.5-70b QVQ-72b Mulberry-8b LLaVA-CoT-11b GPT-4o Gemini OpenAI-o 0.210 0.197 0.230 0.280 0.262 0.311 0.279 0.180 0.158 0.240 0.200 0.178 0.185 0. Open-Source MLLMs 0.240 0.120 0.230 0.267 0.091 0.196 0.253 0.145 0.157 0.260 0.150 0.300 Proprietary MLLMs 0.091 0.353 0.145 0.275 0.164 0.275 0.200 0.280 0.293 0.110 0.147 0.067 0.100 0.093 0.093 0.120 0.170 0.200 0.164 0. 0.164 0.109 0.145 0.205 0.212 0.135 0.190 0.250 0.115 0.192 0.190 0.100 0.233 0.220 0.200 0.267 0.200 0.195 0.177 0.187 0. 0.192 0.193 0.215 Table 3. Compact performance comparison of MLLMs with or without CoT support. sponse is labor-intensive. To address this, we propose two unsupervised evaluation methods: Common Element Agreement. We prompt large language model (LLM) to extract common visual elements across all model responses. The agreement score for each model is then computed based on the frequency with which it includes these shared elements, offering measure of consistency across models. Inter-Model Perception Similarity. To understand how models interpret visual elements relative to one another, we compute pairwise similarity scores between model responses. This results in an matrix that quantifies perceptual alignment between models. Our evaluation framework provides scalable and structured approach to assess both the structured reasoning process and raw perception abilities of models. This enables more comprehensive understanding of their cognitive performance across multiple dimensions. More details and examples will be included in the supplementary material. 5. Experiments and Results In this section, we evaluate the visual reasoning abilities of MLLMs with or without CoT. During the initial phase of our evaluation, we tested range of open-source and smaller models (13B, 32B) on randomly selected set of 40 questions. The results revealed that these models struggled, exhibiting both low accuracy and flawed reasoning. Recognizing these limitations, we refined our approach by selecting the most advanced open-source models specifically designed for reasoning: LLaVA-CoT (11B), Mulberry (8B), and QVQ (72B) [49], alongside Qwen2.5-72B, cuttingedge vision model. For proprietary models, we included GPT-4o, as well as two of the most recent leading models: OpenAI-o1 [2] and Gemini 2.0 Flash Thinking [1]. To ensure fair comparison, all models were evaluated under identical conditions with the same default hyperparameters. The evaluation process consisted of three steps. 1. Generating Responses: Models produce answers to multi-choice questions. 2. Computing Multi-Choice Answer Accuracy: Accuracy is measured across reasoning categories. 3. Assessing Reasoning Path: Evaluates reasoning based on our proposed evaluation metrics. 5.1. Analysis of Accuracy With random-choice baseline of 0.25 for 1-out-of4 selection, all models underperform relative to chance. Our proposed benchmark reveals significant challenge in uncovering the subtle intricacies of visual reasoning. Among open-source MLLMs, LLaVA-CoT (11B) achieves the highest overall accuracy, closely followed by Qwen2.5 (72B). Notably, despite lacking Chain-ofThought (CoT) support, Qwen2.5 (72B) remains highly competitive. Mulberry (8B) and QVQ (72B) trail slightly behind. In the proprietary category, o1 demonstrates the best performance, surpassing both GPT-4o (0.192) and Gemini 2.0 Flash Thinking (0.193). Interestingly, despite lacking explicit CoT support, GPT-4o still performs at comparable level to Gemini. We include detailed QVQ response in the supplementary material to showcase its ability to switch reasoning paths. Impact of CoT. Models equipped with CoT reasoning generally exhibit improved performance in reasoning-intensive tasks. Gemini 2.0 Flash Thinking and o1 outperform GPT4o , reinforcing the hypothesis that step-by-step reasoning enhances accuracy. similar trend is observed among open-source models, where LLaVA-CoT (11B) leads the group in accuracy. Proprietary models continue to lead in performance over open-source alternatives, benefiting from extensive training data, optimized architectures, and greater computational resources. However, among opensource models, LLaVA-CoT (11B) emerges as strong contender, demonstrating balanced accuracy across various benchmarks. Notably, models incorporating Chainof-Thought (CoT) reasoning consistently excel in complex problem-solving, underscoring the significance of structured, step-by-step methodologies. Despite this trend, some open-source models, such as Qwen2.5 (72B), achieve re7 Figure 6. The first row visualizes the correlation between Recognition (R) and Abstraction (A), while the second examines the transition from Abstraction (A) to Deduction (D). Bubble sizes represent confidence levels. The analysis highlights inter-model variability, with stronger correlations in than D, indicating challenges in applying logical deduction even when abstraction is well-formed. The last column aggregates all models within the same figure, providing an overall trend. sults on par with proprietary counterparts, even without explicit CoT support, suggesting that advancements in model CoT strategy can help bridge the performance gap. Our analysis indicates that prioritizing the quality of the reasoning process can reveal crucial diagnostic insights that overall accuracy alone does not capture. In this work, the dataset and benchmark are strategically designed to highlight discrepancies along the reasoning path, thereby offering robust framework to assess and enhance model performance. 5.2. Analysis of Reasoning Path The result of matching between model over Recognition, Abstraction, and Deduction as shown in Figure 6, providing insight into how well models transition from one stage to the next. The first row compares Recognition to Abstraction, evaluating how effectively models generalize visual elements into higher-level patterns. The second row compares Abstraction to Deduction, assessing the models ability to apply logical reasoning to abstracted representations. The bubble size represents the count of the same score. Recognition vs. Abstraction. The scatter plots in the first row reveal strong positive correlation between Recognition (R) and Abstraction (A), highlighting that models adept at recognizing key visual elements also tend to excel at forming higher-level abstractions. However, models like GPT-4o and Mulberry exhibit greater variance, suggesting inconsistency in abstraction despite strong recognition capabilities. This inconsistency may indicate limitations in generalizing patterns beyond direct visual cues. In contrast, models such as QVQ and Gemini demonstrate more stable performance, implying robust ability to translate recognized elements into coherent abstract representations. This consistency makes them more reliable in tasks requiring structured pattern recognition. Figure 7. Pairwise Similarity Matrix of Model Responses Figure 8. Agreement scores compare each models consistency in identifying common visual elements. Abstraction vs. Deduction. The second row illustrates the relationship between Abstraction (A) and Deduction (D), revealing slightly weaker correlation in some models. This suggests that even when models successfully abstract information, their ability to apply logical deduction remains inconsistent. Notably, GPT-4o and QVQ show high variance in abstraction-to-deduction transitions, indicating unpredictability in logical reasoning. This suggests that while these models can form abstract representations, their deductive processes do not always follow clear or systematic pattern. Conversely, models like LLaVA-CoT and Qwen2.5 maintain more compact distribution, signify8 ing structured and consistent approach to deduction when abstraction is well-formed. Their ability to apply logical reasoning in predictable manner makes them more reliable for tasks requiring systematic inference. Perception similarity between models. As shown in Figure 7, LLaVA-CoT and Mulberry (4.13), as well as Gemini and GPT-4o (4.12), exhibit the strongest perceptual alignment, suggesting these pairs process visual data similarly. QVQ shows the weakest alignment with LLaVACoT and o1 , indicating distinct interpretation of visual inputs. GPT-4o and Qwen2.5 show relatively strong alignment with multiple models, suggesting broader generalization. o1 maintains moderate alignment across models but does not dominate in similarity with any particular one. Performance trends indicate that proprietary models hold an edge, but open-source models like LLaVA-CoT and Qwen2.5 remain highly competitive. Perception agreement between models. As shown in Figure8, the agreement score provides insight into the consistency of different models in identifying common visual elements across responses. Notably, proprietary models such as GPT-4o and o1 exhibit higher consistency, as indicated by their larger proportional representation in the agreement, suggesting that these models more frequently include commonly mentioned elements in the responses. Moreover, models like Qwen2.5 and QVQ demonstrate moderate agreement, indicating balance between consistency and diversity in interpretations. 6. Conclusion In this paper, we introduce VERIFY, the first benchmark designed to assess the fidelity of visual reasoning paths in MLLMs. Through comprehensive experiments and analysis, we highlight the limitations of existing reasoning path evaluation methods and propose new metrics for more thorough assessment of leading models. Our findings reveal significant shortcomings in current MLLMs ability to perform visual reasoning, emphasizing their tendency to excel in either perceptual grounding or logical reasoning, but rarely both. While models with CoT reasoning generally achieve higher accuracy, challenges persist in the transition from abstraction to deduction, limiting overall reasoning capabilities. Additionally, we observe strong correlations between recognition and abstraction, but inconsistencies arise in models ability to apply logical inference systematically. By providing structured evaluation framework, we aim to bridge this gap and pave the way for future advancements in MLLM development."
        },
        {
            "title": "References",
            "content": "[1] Gemini 2.0 flash thinking. 7 [2] Learning to reason with llms. 7 [3] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi Parikh. Vqa: Visual question answering, 2016. 2 [4] Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning of physical dynamics, 2020. 2 [5] Lawrence Barsalou. Perceptual symbol systems. Behavioral and brain sciences, 22(4):577660, 1999. 2 [6] Jing Bi, Yunlong Tang, Luchuan Song, Ali Vosoughi, Eagle: EgocenarXiv preprint Nguyen Nguyen, and Chenliang Xu. tric aggregated language-video engine. arXiv:2409.17523, 2024. [7] Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella The revolution of multiCornia, and Rita Cucchiara. arXiv preprint modal large language models: survey. arXiv:2402.12451, 2024. 1 [8] Huanqia Cai, Yijun Yang, and Winston Hu. Mm-iq: Benchmarking human-like abstraction and reasoning in multimodal models, 2025. 2, 3, 4, 5 [9] Francois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. 5 [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 2 [11] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. Benchmarking and improving detail image caption, 2024. [12] Jiafei Duan, Samson Yu Bai Jian, and Cheston Tan. Space: simulator for physical interactions and causal learning in 3d environments, 2021. 2 [13] Francois Fleuret, Ting Li, Charles Dubout, Emma Wampler, Steven Yantis, and Donald Geman. Comparing machines and humans on visual categorization test. Proceedings of the National Academy of Sciences, 108(43): 1762117625, 2011. 2, 5 [14] Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Roscoe: suite of metrics for scoring step-bystep reasoning. arXiv preprint arXiv:2212.07919, 2022. 3 [15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering, 2017. 2 [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 2 [17] Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo V2xum-llm: Cross-modal video summarization arXiv preprint Luo. with temporal prompt instruction tuning. arXiv:2404.12353, 2024. 9 [18] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo Luo. Mmcomposition: Revisiting the compositionality arXiv preprint of pre-trained vision-language models. arXiv:2410.09733, 2024. 2 [19] Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering, 2019. 2 [20] William Ittelson. Visual space perception. 1960. 1 [21] Steeven Janny, Fabien Baradel, Natalia Neverova, Madiha Nadri, Greg Mori, and Christian Wolf. Filtered-cophy: Unsupervised learning of counterfactual physics in pixel space, 2022. 2 [22] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. 2 [23] Huaizu Jiang, Xiaojian Ma, Weili Nie, Zhiding Yu, Yuke Zhu, Song-Chun Zhu, and Anima Anandkumar. Bongardhoi: Benchmarking few-shot visual reasoning for humanobject interactions, 2023. [24] Yifan Jiang, Jiarui Zhang, Kexuan Sun, Zhivar Sourati, Kian Ahrabian, Kaixin Ma, Filip Ilievski, and Jay Pujara. Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning, 2024. 4, 5 [25] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. 2 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [33] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. 1, 5 [34] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1 [35] Mikołaj Małkinski and Jacek Mandziuk. Deep learning methods for abstract visual reasoning: survey on ravens progressive matrices. ACM Computing Surveys, 57(7):136, 2025. 1 [36] Gary Marcus. Deep learning: critical appraisal. arXiv preprint arXiv:1801.00631, 2018. 1 [37] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. [38] Weili Nie, Zhiding Yu, Lei Mao, Ankit Patel, Yuke Zhu, and Anima Anandkumar. Bongard-logo: new benchmark for human-level concept learning and reasoning. Advances in Neural Information Processing Systems, 33:1646816480, 2020. 2 [39] Archiki Prasad, Swarnadeep Saha, Xiang Zhou, and Mohit Bansal. Receval: Evaluating reasoning chains via correctness and informativeness. arXiv preprint arXiv:2304.10703, 2023. 3 [40] Zenon Pylyshyn. Seeing and visualizing: Its not what [26] Stephen Kosslyn. Image and brain: The resolution of the you think. MIT press, 2003. 1 imagery debate. MIT press, 1996. [27] Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. 1 [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1 [29] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large In Proceedings of the 3rd International language models. Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. 1 [30] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 3 [31] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. 1 [41] Abulhair Saparov and He He. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022. [42] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a GarrigaAlonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. 2 [43] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. 2 [44] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. arXiv preprint Intriguing properties of neural networks. arXiv:1312.6199, 2013. 1 [45] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. arXiv preprint arXiv:2312.17432, 2023. 1 [46] Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing 10 [62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 Bi, Zeliang Zhang, et al. Vidcomposition: Can mllms anarXiv preprint alyze compositions in compiled videos? arXiv:2411.10979, 2024. 2 [47] Yunlong Tang, Daiki Shimada, Jing Bi, Mingqian Feng, Hang Hua, and Chenliang Xu. Empowering llms with pseudo-untrimmed videos for audio-visual temporal understanding. arXiv preprint arXiv:2403.16276, 2024. 1 [48] Yunlong Tang, Junjia Guo, Pinxin Liu, Zhiyuan Wang, Hang Hua, Jia-Xing Zhong, Yunzhong Xiao, Chao Huang, Luchuan Song, Susan Liang, et al. Generative ai for celarXiv preprint arXiv:2501.06250, animation: survey. 2025. 1 [49] Qwen Team. Qvq: To see the world with wisdom, 2024. 7 [50] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, 2024. 6 [51] Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. Llms cannot find reasoning errors, but can correct them given the error location. arXiv preprint arXiv:2311.08516, 2023. 2, 3, 5 [52] Han LJ Van der Maas, Lukas Snoek, and Claire Stevenson. How much intelligence is there in artificial intelligence? 2020 update. Intelligence, 87:101548, 2021. 2 [53] Taylor Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall OReilly, and Jonathan Cohen. LearnIn Internaing representations that support extrapolation. tional conference on machine learning, pages 1013610146. PMLR, 2020. 5 [54] Penghao Wu and Saining Xie. V?: Guided visual search as In Proceedings of core mechanism in multimodal llms. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. 1 [55] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. [56] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. International Journal of Computer Vision, pages 119, 2024. 1 [57] Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. Mr-gsm8k: meta-reasoning benchmark for large language model evaluation. arXiv preprint arXiv:2312.17080, 2023. 3, 5 [58] Aimen Zerroug, Mohit Vaishnav, Julien Colin, Sebastian Musslick, and Thomas Serre. benchmark for compositional visual reasoning. arXiv preprint arXiv:2206.05379, 2022. 2 [59] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and SongChun Zhu. Raven: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5317 5327, 2019. 2, 5 [60] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 1 [61] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models, 2024. 3 VERIFY: Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity"
        },
        {
            "title": "Supplementary Material",
            "content": "1. Qualitative result Below is detailed case study of the question, including reasoning paths from different models and human analysis. We identified few common mistake patterns: I. Misinterpreting Features: Errors arise from incorrect counting of elements or misjudging numerical patterns. II. Reliance on Intuition: Mistakes occur when relying on surface-level visual trends instead of logical transformations. III. Failure to Verify All Options: Incorrect answers result from selecting seemingly correct choices without checking alternatives. Figure 2. With human reasoning favoring option due to decreasing dot pattern. o1 incorrectly identifies the answer due to miscounting and failing to verify against options, while Qwen2.5 accidentally selects the correct answer despite miscount. Figure 1. Incorrect reasoning paths still lead to the correct answer 1 Figure 4. logical pattern recognition problem where the missing figure follows rotation and flipping rule. o1 incorrectly chose by relying on an intuitive pattern without full logical reasoning. Figure 3. The question asks for the missing figure in sequence, following structured pattern of shapes. The correct answer (D) is determined through consistent shape distribution across rows. Figure 5. The correct answer is determined through geometric transformations, while o1 incorrectly deduces numerical pattern in shapes. 2 Figure 6. Pattern-based reasoning puzzle where black area increases progressively; the correct answer (D) is fully black rectangle, completing the sequence. Figure 8. reasoning-based pattern recognition question where the correct answer (C) follows an intersection count sequence. o1 incorrect answer (D) misinterprets the geometric pattern, focusing on shape alternation rather than structural composition. Figure 7. The missing shape follows an incremental line property, with answer choice completing the sequence logically. 3 Figure 9. Illustration of pattern recognition puzzle where 2D shapes transform into their corresponding 3D forms, with the correct answer (A) requiring three-dimensional figure. o1 overthinks, selecting (B) based on misinterpretation of shape extrusion. Figure 10. The correct answer (C) follows sequence of 180 and 90 rotations, though analysis reveals challenges in accurately interpreting directional changes. Figure 11. Example of extracted reasoning and collaborative annotation interface. 4 2. Model Comparison We also include radar chart for quick comparison of model performance, as shown in Figure 12. The plot highlights variations across key reasoning and perception dimensions. Notably, models such as Qwen2.5 and GPT-4o exhibit higher agreement scores, suggesting stronger consensus in visual perception. Meanwhile, o1 demonstrates relatively higher abstraction and deduction scores, indicating superior reasoning capabilities. Differences in recognition performance suggest that models vary in their ability to identify key visual elements, which may impact subsequent reasoning stages. We also present new data accuracy evaluation in Table 4, which excludes invalid responses and computes the mean of the remaining data. This provides more accurate assessment of models like QVQ, which utilizes all tokens for reasoning without explicitly producing final answer. Figure 12. Holistic comparison of Model Reasoning and Perception Abilities 3. Data Process Our selection process ensures the highest quality and clarity by following strict criteria. First, we remove all ambiguous questions, excluding those with multiple correct answers or requiring external knowledge beyond the provided visual content. To maintain relevance, we only include data from the past 5-8 years, up to 2024. Additionally, we collect problems from various provinces, prioritizing those known for higher difficulty levels, such as Beijing and Shanghai, to ensure rigorous dataset. Each question is handcrafted and filtered by annotators with at least masters degree or higher, guaranteeing clear and precise final selections. To ensure clear and structured reasoning path for each problem, we follow systematic approach. First, annotators 5 attempt to solve the problem independently. If successful, we collect concise description of their solution process. If an annotator is unable to solve the problem, we provide the correct answer as hint to determine whether reasoning path can be constructed. If both steps fail, we assess whether the problem is inherently difficult or if it follows an exceptionally rare pattern. Any problems deemed too rare are removed, and only solutions provided by annotators serve as the gold standard for reasoning clarity. 4. Example of Data Extraction Below, we present the models response alongside the extracted reasoning elements, as illustrated in Figure 11. The extracted reasoning captures the core logical structure of the response, ensuring key insights are conveyed effectively. Additionally, we showcase the web-based UI used for collaboration with annotators. We will share these resources with the community to facilitate further research. 5. Analysis of QVQ 5.1. How the reasoning works with visual The reasoning process in multi-modal models remains an open challenge, particularly in proprietary MLLMs such as o1 , which do not reveal their internal reasoning paths. Meanwhile, weaker open-source models lack the ability to explore different strategies when analyzing visual patterns. In contrast, QVQ exhibits unique characteristic: it systematically attempts multiple reasoning strategies to solve given problem rather than following single fixed approach, as shown in Figure 16. Our observations indicate that QVQ genuinely explores various paths to reasoning, often exceeding 10 steps in an effort to find the correct answer. This characteristic differentiates it from other models, as it does not settle on single reasoning trajectory prematurely. However, this exhaustive approach contributes to its low performance, as the model frequently exceeds the available 16k context length before converging on final answer. Instead of providing clear and concise response, QVQ continues its reasoning process until it runs out of tokens, leading to incomplete or inconclusive outputs. One notable observation is that QVQ consistently uses Alternatively to introduce shift in reasoning, allowing us to identify alternative considerations as distinct reasoning steps. This enables us to analyze the step count and token distribution for each step. Building on this observation, Figure 13 reveals that while the majority of QVQ tasks require relatively small number of reasoning steps, long-tail distribution is evident, with some tasks demanding an excessive number of steps. This suggests that for more complex problems, QVQ struggles to Model COT GT MA GC LP EO PR SA SC SP ALL qwen2.5 qvq mulberry llava-cot o1 gemini gpt-4o 0.230 0.226 0.233 0.180 0.279 0.311 0. 0.179 0.172 0.245 0.186 0.236 0.189 0.178 Open-Source LMMs 0.235 0.208 0.157 0.235 0.091 0.104 0.154 0.091 0.240 0.290 0.257 0. Proprietary LMMs 0.280 0.275 0.360 0.164 0.148 0.096 0.293 0.280 0.200 0.080 0.172 0.069 0.160 0.120 0.093 0. 0.182 0.234 0.173 0.093 0.145 0.109 0.167 0.115 0.268 0.137 0.216 0.196 0.115 0.255 0.241 0.107 0.241 0.267 0.207 0.267 0. 0.174 0.199 0.192 0.184 0.217 0.195 0.194 Table 4. Unlike the previous evaluation, which considered all invalid cases as incorrect, this updated assessment excludes invalid cases from the computation. By removing invalid responses from the accuracy calculation, the reported scores better reflect the models true performance on valid instances. Figure 13. Histogram showing the distribution of reasoning steps. The x-axis represents the number of reasoning steps, while the yaxis represents the count of occurrences. Figure 14. Histogram showing the distribution of the length of reasoning steps in words. The x-axis represents the length of reasoning steps, while the y-axis represents the count of occurrences. Figure 15. Comparison of reasoning step distributions: (a) Number of steps and (b) Length of steps in words. streamline its reasoning and instead resorts to prolonged deliberation without efficiently converging on solution. The presence of tasks requiring over 30 or even 50 steps highlights the difficulty of the problem, as the model fails to resolve them within reasonable step count. Similarly, Figure 14 illustrates the distribution of token usage per reasoning step. While most steps remain relatively short, the steep long-tail distribution shows that some reasoning steps extend to several hundred or even thousands of tokens. This suggests that QVQ, when faced with difficult problems, often generates disproportionately lengthy reasoning sequences in an attempt to work through them. However, rather than leading to clear resolution, these extended steps further contribute to inefficiency and excessive token consumption. Together, these findings indicate that QVQ struggles with complex reasoning tasks, as evidenced by both excessive step counts and highly variable step lengths. The models inability to converge efficiently suggests that certain problems remain unsolved within reasonable context window. This inefficiency highlights fundamental limitation in QVQs reasoning approachrather than refining its logic, it frequently resorts to verbosity and fragmented steps, ultimately failing to reach satisfactory conclusion. Interestingly, while o1 does not return intermediate reasoning steps, we can infer that it undergoes similar iterative process. Based on token usage, we estimate that O1 requires an average of 6-8k tokens before reaching decision. This suggests that proprietary models may employ comparable multi-step reasoning strategies but optimize their token allocation more efficiently than QVQ. Overall, while QVQ demonstrates an impressive ability to explore diverse reasoning strategies, its lack of an effective stopping mechanism and excessive token consumption hinder its performance. Future improvements should focus on optimizing its reasoning efficiencypotentially by refining the models ability to prioritize promising reasoning paths early on and reducing unnecessary exploration. 6 Figure 16. The example reasoning path of QVQ shows that QVQ needs extensive tokens to solve the problem."
        }
    ],
    "affiliations": [
        "University of Central Florida",
        "University of Rochester"
    ]
}