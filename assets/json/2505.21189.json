{
    "paper_title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation",
    "authors": [
        "Gleb Mezentsev",
        "Ivan Oseledets"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space."
        },
        {
            "title": "Start",
            "content": "Exploring the Latent Capacity of LLMs for One-Step Text Generation * Gleb Mezentsev AIRI Skoltech mezentsev@airi.net Ivan Oseledets AIRI Skoltech oseledets@airi.net 5 2 0 2 7 2 ] . [ 1 9 8 1 1 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts up to thousands of tokens via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression.We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals surprising and underexplored capability of LLMs multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for given text, they form connected and local regions in embedding space property that suggests the potential of learning dedicated encoder into that space."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are typically trained to generate text in an autoregressive manner they predict one token at time based on the previously generated context. Recent work by Kuratov et al. (2025) demonstrated that LLMs can autoregressively generate an arbitrary text starting from single, specially trained input embedding corresponding to that text. This raises an intriguing question: is autoregressive generation an essential part of such reconstruction? Or, in other words, can LLMs reconstruct accurate multi-token sequences from some compressed representation in single forward pass, without any iterative generation, and if so, how? In this work, we show that this is possible, investigate what those compressed representations encode and whether this finding reveals anything about LLMs parallel generation capabilities. *Under review 1 Figure 1: Two \"proto-tokens\" (trainable embeddings) are fed into frozen, pre-trained LLM and optimized in such way, that the LLM predicts an arbitrary target token-sequence in single forward pass. One of the \"proto-tokens\" (et) is trained for each text separately, while the other (m) could be reused. Our contribution is as follows: 1. We show that LLMs can reconstruct arbitrary sequences from as few as two learned input embeddings, achieving perfect reconstruction of sequences of up to several hundred tokens. 2. We identify key design aspects for such setup, that enable this generation, including the critical importance of input token arrangement. 3. We study how the reconstruction capability varies with the model size and the nature of the target sequence (e.g. natural vs synthetic text). 4. We empirically characterize learned representations analyze their information content and embedding-space geometry."
        },
        {
            "title": "2 Related Work",
            "content": "The most direct influence for our work is paper by Kuratov et al. (2025), which showed that frozen LLMs can reconstruct an arbitrary sequence of tokens = [t1, . . . , tN ] if given set of special, so-called memory tokens [mem1, . . . , memK]. The embeddings for these tokens are trained by optimizing causal language modeling objective (next-token prediction cross-entropy loss) over concatenated input sequence = [mem1, . . . , memK, t1, . . . , tN ] passed through frozen LLM. In the case of perfect next-token prediction accuracy (which could be achieved for reasonable text length), this allows the model to autoregressively predict the whole text starting from the memory tokens. The number of memory tokens controls the maximum text length and can be as low as one. Although surprisingly long (up to 1568 tokens) texts could be compressed even into single memory token, the authors note that the embeddings trained from different random initializations for the same text often end up far apart. Moreover, linear interpolations between those embeddings produce very poor reconstruction accuracy, suggesting that the solution space lacks desirable smoothness and locality qualities, which are important for learning practical encoder that could replace the direct optimization. Our work also relates to efforts in prompt-tuning and its variants (Lester et al., 2021; Liu et al., 2024; Li and Liang, 2021). Most similarly, Lester et al. (2021) train task-specific soft tokens to condition the frozen LLMs to improve their performance in new tasks. Finally, several speculative (Xia et al., 2023) and parallel (Santilli et al., 2023) decoding approaches utilize similar mechanism for multiple token prediction using decoder architectures. More specifically, they add special [PAD] or [MASK] tokens at the end of the current context in order to make prediction for several tokens into the future at once. Critically, in these works either special training or multiple generative iterations are required. Unlike prior work, we show that frozen LLM can generate accurate multi-token sequences in one forward pass without additional LLM training or iterative decoding."
        },
        {
            "title": "3 Method",
            "content": "and only produce human-readable text after passing through the LLM. Our goal is to identify the smallest possible number of such \"proto-tokens\" needed for accurate reconstruction. Interestingly, we find that it is essential to have at least two the performance drops dramatically when using only one (see Section 4). There are many ways to arrange two vectors as an input sequence of arbitrary length. We report results for different variants later in the paper, but here we describe the arrangement that is used in the majority of the experiments. Exact scheme We introduce two \"proto-tokens\" and with trainable embeddings of dimension dmodel (model input embedding dimension) and construct the input sequence as follows: = [e, m, m, . . . , m] one copy of token is followed by 1 copies of token m, where is the target text length. We then train the vectors by optimizing cross-entropy loss between the target sequence = [t1, t2, . . . , tN ] and the frozen LLMs output for the input sequence. The prediction is obtained using standard causal attention masking, so that the predicted probabilities for the token ti depend on the first input \"proto-tokens\" (see Figure 1). Metrics Our main evaluation metric is the number of correctly reconstructed tokens in generated sequence, defined as: Ctokens = (cid:88) i=1 1(LM (Z[1:i]) = ti) (1) Additionally, we measure the amount of information contained in the reconstructed token sequence from the perspective of causal language modeling with given LLM. Specifically, we compute the cross-entropy between the compressed sequence and LLMs autoregressive probability distribution: HLM = (cid:88) i=1 logPLM (tit<i) (2) To adopt the approach from Kuratov et al. (2025) to non-autoregressive case, we replace all input tokens of the LLM with specially trained \"prototokens\" and predict the target token sequence in one forward pass. In practice, \"proto-tokens\" are just trainable vectors that are not tied to any real items in the vocabulary. The main difference between regular tokens and these \"proto-tokens\" is that \"proto-tokens\" encode multiple tokens at once This quantity measures how uncertain model is about the compressed text, that is, how much information it contains. Solution space connectivity To gain insights into the structure of the solution space of our problem, we analyze whether different proto-token embeddings obtained for the same text but from different random initializations are connected. We adopt 2 technique from (Garipov et al., 2018) which is used to find paths connecting different minima of the loss function in computer vision tasks. We optimize the parameters of degree-one Bezier curve, connecting two solutions, to maximize reconstruction accuracy along the curve. The curve is parameterized by control point π in the following way: ϕπ(t) = (1 t)2p1 + 2t(1 t)π + t2p2 (3) Here, p1 and p2 are the two original solutions that we aim to connect. The expectation of the cross-entropy loss function under the uniform distribution over [0, 1] (4) is minimized by iteratively sampling [0, 1] and making gradient step, effectively obtaining unbiased estimate of the gradient of lπ: lπ = (cid:90) 1 (cid:88) 0 i=1 logPLM (tiϕπ(t))dt (4) This acts as more tractable alternative to direct optimization under the uniform distribution along the curve itself. Token sequences similarity In Section 4, we aim to measure the similarity between two token sequences in order to control for this similarity. To measure token-level similarity we use the cosine distance between TF-IDF embeddings of two sequences. To measure semantic similarity we use cosine-distance between semantic sequence embeddings obtained from MiniLM model fine-tuned1 for the semantic sentence embedding."
        },
        {
            "title": "4 Experiments and results",
            "content": "We test the ability of different LLMs of varying sizes to generate predefined text from different sources in non-autoregressive (parallel) mode. Moreover, we compare different ways to feed our trainable \"proto-tokens\" into LLM. We also try to understand the structure of the solution space by examining the relations of solutions for different problems. Models We use six models for all experiments: three Pythia (Biderman et al., 2023) models of sizes 160M, 410M, and 1.4B, and three Llama3 (Grattafiori et al., 2024) models of sizes 1B, 3B, and 8B. 1https://huggingface.co/sentence-transformers/ all-MiniLM-L6-v2 Data Four text sources are used in the experiments to explore the possible connection between reconstruction performance and the text nature. set of random texts is generated by sampling from the top 100,000 words of the GloVe vocabulary (Pennington et al., 2014), to evaluate performance on unnatural texts. To assess generation performance on natural but unseen texts, we use collection of fanfiction texts from AO3 library 2, with publication date cutoff of October 2024, which is later than the end of training for all models. For data processing details, see Kuratov et al. (2025). The performance on seen natural texts is evaluated using PG-19 dataset (Rae et al., 2019) part of dataset used for training Pythia models. Finally, we include set of model-specific generated texts. Specifically, for each model and each context text from PG-19 dataset, suffix of the same length is generated as autoregressive continuation. The generation is done via multinomial sampling with sampling temperature = 1. Training details The embeddings of the prototokens are initialized randomly from standard normal distribution and optimized using AdamW optimizer (Loshchilov and Hutter) with 0.01 learning rate, β1, β2 set to 0.9 and weight decay of 0.01. The embeddings are trained for 5000 iterations with early stopping if perfect reconstruction accuracy is achieved. This number of iterations is often insufficient for convergence, but due to limited computational resources, we are unable to increase it. Instead, we aggregate results across multiple sequences. All models are loaded and trained using PyTorch framework and the Hugging Face Transformers library. Each experimental run is done on single A100 or H100 80GB GPU with gradient accumulation enabled where necessary. The code is available at this page3. Proto-token arrangement To select the best way to arrange two proto-tokens as input to an LLM for the main experiments, we conduct test runs on single dataset-model pair for the variety of arrangements. For each arrangement, the same 50 texts from the PG-19 are selected, and the Llama3.2-1B model is trained on prefixes of these texts at lengths [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024] to assess how token-level reconstruction accuracy 2https://archiveofourown.org/ 3https://github.com/Glebzok/ OneStepLLMGeneration 3 changes with respect to sequence length . representative selection of results is presented in Table 1. Arrangement = 1 = 2 = 4 = [e]N 1.000.00 0.450.31 0.170.18 0.010.01 [e](N/2)[m](N/2) 1.000.00 1.000.00 0.120.13 0.010.01 [e, m](N/2) 1.000.00 1.000.00 1.000.00 0.170.34 [e][m]N [e][m](N 1) 1.000.00 1.000.00 1.000.00 0.970.15 1.000.00 1.000.00 1.000.00 0.990.10 Table 1: Reconstruction accuracies for different input token arrangements across varying sequence lengths. Subscripts indicate the number of copies for each prototoken. The last two schemes differ as follows: with the first one, the LLM is trained to predict the first text token t1 for the proto-token e, while with the second one, the prediction for proto-token is not guided and t1 is target prediction for the first copy of instead. Interestingly, having two trainable tokens is essential for the performance the scheme with one trainable token fails to reconstruct even 2-token text, while best two-token schemes can reconstruct 256-token texts almost perfectly. Moreover, the way these two tokens are arranged is also important, with the best results obtained when the first token is followed by 1 copies of the second token m. This asymmetrical arrangement and critical necessity for two tokens suggest possible variation in functions of and m. It is possible, that while one of them mostly incorporates language information, the role of the other one is mainly structural or mechanistic. This could be related to the phenomenon of \"attention sinks\" Xiao et al. (2023) showed that LLMs strongly attend to the initial tokens in the sequence even when they are not relevant. Moreover, adding placeholder token as an attention sink could largely improve the performance of window-attention based models, which do not see the initial tokens by design. So, it is possible, that in order to successfully decode \"information\" proto-token, LLM needs distinguishable \"sink\" proto-token, which can be used as attention sink. Token sharing In the previous section, we showed that the quality of reconstruction is very dependent on having two separate proto-tokens as an input. This observation, led us to hypothesize that, second token plays some structural or mechanistic purposes and does not contain information about the sequence itself. In that case, the second token could be shared between texts, reducing the number of optimized parameters, and simplifying the training process of the potential encoder. To test this hypothesis, we run the same optimization process, but splitting 256 texts from the PG-19 dataset into groups of different sizes Sg [1, 4, 16, 64, 256] and sharing either or within each group. We selected the maximum length of the text that can be losslessly compressed in non-sharing mode - 256. The results are averaged over 10 random seeds. The selection of the results is presented in Table 2. Shared Agg Sg = 1 Sg = 16 Sg = max avg max avg 1.000.00 0.980.08 1.000.00 0.980. 0.990.01 0.900.17 1.000.00 0.860.19 0.990.02 0.860.20 1.000.01 0.830.18 Table 2: Reconstruction accuracy for schemes where one of the trainable tokens is shared within group across different group sizes. \"max\" aggregation indicates that for every text, maximum accuracy across ten random seeds is selected and then averaged across texts, while \"avg\" denotes averaging across both seeds and texts. Sharing either token yields comparable performance if provided with sufficiently large number of restarts (random seeds), but the required number of restarts increases significantly with group size. Depending on the proto-token being shared, we can build different intuitions behind the function of the shared tokens and the method itself. If e-token is shared, which is located in the very beginning of the input sequence, the analogy that comes to mind is prompt-tuning (Lester et al., 2021), where set of prompt embeddings is trained in order to improve performance in some specific task. In our case, shared token could be viewed as an \"instruction\" saying what an LLM should do with the upcoming embeddings (m-tokens) decode different pieces of information for different positions. If is shared, then training and prediction scheme resembles some of the speculative decoding approaches (Xia et al., 2023), where number of special \"[mask]\" tokens are appended at the end of the sequence and the prediction for all them is then done in parallel. For all other experiments, unless stated otherwise, we use scheme with sharing token between texts and random seeds and token being unique for each text/seed pair. 4 Share Pythia Llama 160M 410M 1.4B 3.2-1B 3.2-3B 3.1-8B Random Fanfics PG-19 PG-19 (gen) Ctokens HLM Ctokens HLM Ctokens HLM Ctokens HLM False True False True False True False True False True False True False True False True 90 45 92 22 90 45 256 181 362 256 512 507.5105.9 377.1133.1 470.7103.1 1551.3159.5 2193.4190.2 2974.4298.3 1292.2217.4 1309.4234.6 247.932.0 91.130.8 231.037.9 947.7155.0 128 45 128 45 131 45 362 512 288 724 362 358.973.3 395.497.8 261.056.4 1107.6129.1 1408.4179.5 1763.3280.2 1112.8168.6 145.026.2 82.328.1 147.929.7 576.490.4 835.9121.7 128 167 32 128 64 362 181 512 256 724 362 388.466.4 408.896.3 298.477.4 993.8183.4 156.030.2 456.556.5 156.033.9 88.130. 1346.0218.4 1659.8344.5 832.3171.0 826.1117.6 128 45 181 32 128 64 362 181 512 724 362 354.172.0 379.282.6 277.671.3 927.3103.4 153.017.8 106.938.5 197.139.3 478.785.7 1266.6125.9 1653.1211.4 771.7143.0 788.6130.8 Table 3: Maximum reconstruction capacities for different models on different datasets. Generation capacity We already see, that similar to autoregressive mode (Kuratov et al., 2025), LLMs can generate fairly long sequences in just one forward pass. To characterize this capability, and understand how it scales with model size, we run the optimization process for text prefixes of the predefined lengths [4, 5, 8, 11, 16, 22, 32, 45, 64, 90, 128, 181, 256, 362, 512, 724, 1024, 1448]. We report the maximum values of Ctokens, and Hmax which correspond to the longest prefix for which at least 0.99 token-level accuracy is achieved we treat such sequences as successfully predicted. In addition to scheme with shared token, we also run scheme with not shared, to eliminate the effect of the insufficient number of random initializations. While our results in Section 4, suggest that p, can in principal, be shared without any quality drop, we also note that the optimization process is highly sensitive to initialization, especially when the proto-tokens are shared. The results are presented in Table 3. Larger models in Llama family show greater reconstruction capabilities than the smaller ones of their family, while the situation with Pythia modelfamily is less obvious, with all the models showing approximately the same performance. Llama 1B model is also able to reconstruct almost three times larger sequence compared to Pythia model of the same size. The source of the natural language (unseen / seen / generated) doesnt seem to have any systematic influence on the quality of reconstruction in terms of the number of tokens, while for unnatural random texts the generation capacity is significantly worse. This suggests that our \"proto-tokens\" do not \"store\" text tokens directly, but encode some more highlevel representations, using language modeling capabilities of LLM. However, we also cant say that the compressibility of the text is determined by its likelihood under the sequential language model. In fact, we observe the opposite trend: lower total information content HLM is compressed for lessinformation dense texts, such as generated by the LLM itself. This difference is highlighted in Figure 2, where the amount of the language-information contained in trainable tokens is compared to autoregressive setup. The performance for unnatural texts is very similar and sometimes even identical, while for natural texts, the difference in capacity can be up to five times lower. However, more often the performance is just two times lower in non-autoregressive 5 Figure 2: Maximum language information (HLM for maximum text prefix that is accurately reconstructed) compressed for different models and datasets. In the left plot, single [mem] token is used in the autoregressive setting, and in the non-autoregressive one, proto-token is shared between all texts within each model. In the right plot, two [mem] tokens are used and proto-tokens are not shared. Each small point on the plots represents single text, larger points indicate the average within each (model, dataset) pair. case, suggesting that autoregressive decoding approximately doubles the \"effective\" information density for natural texts the density of the information that could be effectively decoded. Figure 3: Reconstruction throughput comparison between autoregressive and non-autoregressive setups. For each (model, dataset) pair, the throughput is calculated as maximum losslessly compressible length divided by the reconstruction time. To measure reconstruction time, we use PyTorch profiling tools. Although less information-dense, our oneforward method achieves significantly higher decoding throughput in the context of text reconstruction outperforming its autoregressive counterpart by factor of 279 on average (Figure 3). This dramatic difference is primary due to the number of forward passes. While an obvious downstream task is still to be found, such speed could matter for fast context-compression and decompression, on-device inference, or setting where decoding speed is particularly important. 6 Proto-tokens interpretation We examine the information encoded in proto-tokens and the implications this has for potential practical applications. In worst case scenario, they directly encode target tokens (imagine vector containing token_ids). If so, the entire \"language generation\" effort happens during encoding, making decoding irrelevant for accelerated inference though the approach could still be useful as context-compression tool. The alternative is that proto-tokens encode compressed representation of prefix which, when the model generates from it, produces the observed suffix. In that case, the hard work of text generation is done during decoding, which is more promising from the point of view of accelerated inference. All the intermediate options are also possible. Figure 4: Cosine embedding distances for different pairings of proto-tokens. We select 50 contexts from PG19 and for each context, generate 10 continuation texts. We find one solution for each of the first 9 generations and 10 different-seed solutions for the last generation. Figure 5: We compare proto-token embedding distances for same context text pairs and different-context text pairs. Token-level distance is measured as cosine distance between TF-IDF embeddings. Semantic distance is measured as cosine distance between semantic text embeddings (see Section 3 for details). We start by measuring the distances between three types proto-token embedding pairs: 1) corresponding to the same generated sequence, but different random seeds, 2) corresponding to the different texts but generated from the same context, 3) corresponding to the different texts generated from different contexts. As shown in Figure 4, the sametext solutions are almost always located closer to each other than different-texts solutions, which suggests locality in the learned representations. At the same time, same-context solutions are noticeably closer to each other than different-context ones. This may indicate that the encoded information at least partially reflects the potential context of the text. However, we should be careful to account for the texts generated from the same context being more similar in general. To do that, we measure pairwise distances between generated texts, and examine whether the distance between learned proto-token embeddings differ for fixed distance between the texts. We use token-level measure of text similarity and semanticlevel measure (see Section 3). For both measures, (Figure 5) we observer that, given similar distances between texts, the proto-token embeddings are consistently closer when the texts originate from the same context. We conclude that learned prototokens contain information beyond the information about the target sequence itself it somehow describes the potential context of the sequence. Kuratov et al. (2025) raised the following concern about the structure of the solution space in autoregressive setup. Even though the same-text token embeddings are on average closer to each other than different-text token embeddings, they seem to be disconnected linear interpolation between two solutions does not yield valid reconstruction. This could mean that the potential encoding to this space could be problematic as the same object could be mapped to disconnected regions. We find that in our non-autoregressive case, the linear interpolation between same-text solutions also does not produce solution (Figure 6). Figure 6: Pairwise interpolation accuracies between 10 solutions for 5 texts (5 10 9/2 pairs in total). However, the solutions could be connected using quadratic Bezier curves (parabolic segments) lying inside \"solution set\". This means that even though same-text solutions do not form convex set, they form connected set. In fact, our experiments show that the maximum ratio between Bezier curve length and the corresponding linear connection is only 1.2, indicating that the paths are nearly linear. These results demonstrate that the solution space is fairly well behaved, providing reasonable hope that an encoder model could be built to map into that space."
        },
        {
            "title": "5 Discussion and Conclusions",
            "content": "In this paper, we demonstrate that frozen LLMs have surprising ability to generate hundreds of accurate tokens in single forward pass without any iterative decoding when provided with just two specially trained \"proto-tokens\". We find that both the number and the arrangement of such tokens is crucial for enabling this generation capacity. Interestingly, with only one proto-token, LLMs are unable to generate more than single token of text. In contrast, two properly arranged proto-tokens can enable the generation of sequences hundreds tokens long. This significant leap in the performance, along the observation that one of the vectors can (in principal) be shared across many texts, suggest that proto-tokens play different functional roles during generation. However, the precise nature of the role differentiation remains unclear. We find that bigger model size does not universally imply better generation capacity. While larger models in Llama-3 family demonstrate improved reconstruction capacity, Pythia models show no such trend larger models do not outperform smaller one. Whether this difference is connected to the architectural variations is an open question. Additionally, we do not observe any consistent relationship between the source of the natural text and the reconstruction ability of LLMs. Surprisingly, even for the texts generated by the LLM itself, the number of successfully reconstructed tokens is the same as for any other natural text. However, for the texts composed of random tokens, performance drops noticeably. This suggests that our reconstruction process does not fully leverage the language modeling capabilities of LLMs, and may instead mostly rely on low-level token patterns. Although the reconstructed sequences in the nonautoregressive setting are, on average, about two times shorter than those in the autoregressive case, the computational efficiency of single-forward approach allows to achieve up to 279 greater generation throughput. We also observe that proto-tokens encode more than just the target sequence. Embeddings of the \"proto-tokens\" corresponding to the different texts generated from the same context are significantly closer to each other than those from unrelated sequences. This indicates that the learned representations capture some potential contextual information. Finally, we discover that the embedding space in which proto-tokens exist, has very desirable structural properties proto-tokens corresponding to the same text, form localized and connected regions, enabling smooth transitions via quadratic interpolation. These findings suggest that it may be feasible to build an encoder capable of mapping into this space, opening the door to future work on non-autoregressive inference and representation learning."
        },
        {
            "title": "6 Limitations",
            "content": "Although our paper demonstrates the surprising capability of LLMs to generate long sequences in single forward pass from just two learned embeddings, several important limitations should be acknowledged: 1. Lack of immediate practical application: Most importantly, this work highlights an interesting quirk of LLMs and does not suggest any immediate practical implications or real-life usages for the method. 2. Architectural dependence: The method demonstrates different behavior across model families, suggesting some architectural dependence. As results, our method may potentially not generalize to other model architectures. 3. Limited domain coverage: While we evaluate four different text sources , the results may not generalize beyond those explored in our experiments."
        },
        {
            "title": "References",
            "content": "Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, and 1 others. 2023. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR. Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Wilson. 2018. Loss surfaces, mode connectivity, and fast ensembling of 8 dnns. Advances in neural information processing systems, 31. pages 39093925, Singapore. Association for Computational Linguistics. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. 2025. Cramming 1568 tokens into single vector and back again: Exploring the limits of embedding space capacity. arXiv preprint arXiv:2502.13063. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 30453059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582 4597, Online. Association for Computational Linguistics. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2024. Gpt understands, too. AI Open, 5:208215. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 15321543, Doha, Qatar. Association for Computational Linguistics. Jack Rae, Anna Potapenko, Siddhant Jayakumar, and Timothy Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. 2023. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1233612355, Toronto, Canada. Association for Computational Linguistics. Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. 2023. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023,"
        }
    ],
    "affiliations": [
        "AIRI Skoltech"
    ]
}