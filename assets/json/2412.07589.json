{
    "paper_title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation",
    "authors": [
        "Jianzong Wu",
        "Chao Tang",
        "Jingbo Wang",
        "Yanhong Zeng",
        "Xiangtai Li",
        "Yunhai Tong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose a new task: \\textbf{customized manga generation} and introduce \\textbf{DiffSensei}, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates a diffusion-based image generator with a multimodal large language model (MLLM) that acts as a text-compatible identity adapter. Our approach employs masked cross-attention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking a significant advancement in manga generation by enabling text-adaptable character customization. The project page is https://jianzongwu.github.io/projects/diffsensei/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 9 8 5 7 0 . 2 1 4 2 : r DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation Jianzong Wu1,2 Chao Tang Jingbo Wang2 Yanhong Zeng2 Xiangtai Li3,4 Yunhai Tong1 1 Peking University 2 Shanghai AI Laboratory 3 Nanyang Technological University 4 Bytedance Seed Project Page: https://jianzongwu.github.io/projects/diffsensei/ Email: jzwu@stu.pku.edu.cn, xiangtai94@gmail.com Figure 1. Results of DiffSensei. (a) Customized manga generation with controllable character images, panel captions, and layout conditions. Our DiffSensei successfully generates detailed character expressions and states following the panel captions. (b) Manga creation for real human images. The dialogues are post-edited by humans. The continuation is in the Appendix Fig. 7. We strongly recommend that the readers see the Appendix for more comprehensive results. Manga reading order: Right to left. Top to bottom."
        },
        {
            "title": "Abstract",
            "content": "Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-toimage generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose new task: customized manga generation and introduce DiffSensei, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates diffusion-based image generator with multimodal large language model (MLLM) that acts as text-compatible identity adapter. Our approach employs masked crossattention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce MangaZero, large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking significant advancement in manga generation by enabling textadaptable character customization. The code, model, and dataset will be open-sourced to the community. 1 1. Introduction Story visualization, the process of generating visual narratives from textual descriptions, is rapidly evolving field [5, 18, 21, 25, 36, 45, 52]. Among its various applications, manga generation holds particular significance due to its popularity and unique narrative requirements. Unlike 1The work is done in Shanghai AI Laboratory. traditional story visualization, manga demands consistent characters across panels, precise layout control for positioning multiple characters, and the seamless integration of dialogues in coherent, visually engaging manner. Currently, manga generation remains an underexplored area. Most existing research focuses on low-level imageto-image tasks, primarily converting general images to manga style [20, 32, 47, 49]. While these tasks enhance the visual appeal of static images, they do not extend to generating fully customized manga content from scratch. In general story visualization, current approaches have demonstrated some success in generating coherent image sequences from text. Still, they cannot often customize the characters across scenes [21, 25, 45, 52], critical requirement for manga generation. Additionally, they do not provide the necessary control over layouts and dialogue placements, which are also essential for manga. likely reason for these limitations is that existing story visualization datasets typically lack character annotations and layout controls [15, 18, 21, 45]. Another research direction has explored zero-shot character customization, showing promise for achieving character customization across manga panels [4, 9, 19, 29, 35, 38, 46]. However, these approaches often result in rigid copy-pasting effects [4, 38, 46], which limit expressive character variation and detract from narrative depth. This limitation largely stems from the scarcity of datasets capturing multiple appearances of the same character in varied expressions and poses. To address these limitations, we propose new task: customized manga generation. As illustrated in Fig. 1, this task focuses on creating manga images with multiple characters, each customized based on character image and positioned according to user input. Characters must dynamically adapt to text prompts, altering their expressions, motions, and poses as the narrative unfolds. Dialog layout should also be managed to generate expressive manga panels. Compared to traditional story visualization tasks, this proposed task prioritizes essential manga-specific controllability, aiming to generate vivid manga panels that are both coherent and visually engaging while supporting the customization of multiple characters. To address the lack of dedicated dataset for customized manga generation, we collect dataset of Japanese black-and-white manga, forming the basis of the proposed task. The resulting dataset, MangaZero, is the first large-scale collection designed to support multi-character, multi-state manga generation. To tackle this task, we introduce novel framework, DiffSensei, which leverages diffusion-based image generator to produce customized manga panels. However, we observe that, even with training on multiple appearances of the same character, the generated portraits often tend to rigidly follow the pixel distribution of the input character image, resulting in limited variations in appearance, pose, and motion based on the text input. Inspired by recent advancements in image editing using Multimodal Large Language Models (MLLMs) [6, 8, 16, 33, 3942, 45, 48], we propose using an MLLM as text-compatible character adapter. This approach enables seamless, dynamic adjustments to characters in response to textual cues, thereby supporting coherent and expressive manga panel generation. Additionally, we incorporate masked attention injection to manage character layout, along with dialog embedding technique tailored specifically for manga, allowing for precise control over dialog placement. Through extensive experimentation, we validate DiffSenseis capability to generate coherent, expressive manga panels that maintain narrative consistency and offer improved character control. This represents significant advancement in story visualization. In summary, our key contributions are as follows: We introduce new task: customized manga generation, focused on generating manga images with multiple characters, each dynamically adapting to text prompts and positioned according to layout specifications. the first We present MangaZero, large-scale dataset specifically designed for multi-character, multi-state manga generation, addressing significant gap in story visualization training data. The dataset will be released for the image generation community. We propose DiffSensei. As far as we know, it is the first framework for customized manga generation that links diffusion models and MLLMs. The MLLM serves as an adaptable character feature adapter, enabling characters to respond dynamically to textual cues. Extensive experiments demonstrate the effectiveness of DiffSensei. 2. Related Work Story visualization. Story visualization, the process of generating visual narratives based on given stories, is rapidly evolving. Many approaches can generate consistent image sequences derived from story content [5, 21, 25, 36, 45, 52]. Despite recent advancements, the field faces significant limitations. Most existing methods generate story images solely from text and image-level prompts [21, 25, 45, 52], which restricts control over individual characters. This limited control over characters reduces the flexibility and depth of story visualization. key factor is that current training datasets [10, 15, 18, 21, 45] lack character-specific annotations. In response to the data limit, recent works [5, 36] explore multi-character control using training-free methods that leverage existing subject preservation techniques, such as IP-Adapter [46]. Other works [4, 7, 9, 11, 12, 19, 29, 35, 38, 43, 46] try to train diffusion models for multi-character customized generation. However, these approaches often result in copy-pasting effect, restricting the diversity of expressions and actions needed for dynamic storytelling. For training-free methods, Table 1. Comparison between MangaZero and related publically available datasets. story is defined as sequence of continuous images annotated consistently with character IDs. In MangaZero, story means manga page. panel means distinct story image, or called frame [21, 45]. Most series in MangaZero are still popular in 2024. Please see the Appendix for the dataset details. Dataset Type Resolution #Series #Stories #Panels Annotations Caption Character Dialog PororoSV [18] FlintstonesSV [10] StorySalon [21] StoryStream [45] Manga109 [2] MangaZero Animation Animation Animation Animation B/W Manga B/W Manga Fix Fix Fix Fix Vary Vary 1 1 446 3 109 48 15,336 25,184 18,255 12,614 10,602 73,665 122,560 159,778 257,850 103, 43,264 427,147 Origin 2003-2016 1960-1966 YouTube 1939-2013 1970-2010 1974-2024 combining multiple models can significantly slow down inference speeds. To address these challenges, we first introduce large-scale manga generation dataset with finely curated character annotations and then develop novel framework utilizing diffusion models and MLLMs that enables the dynamic generation of manga panels. Manga generation. The field of black-and-while manga generation has received limited exploration. Most existing researches focus on low-level image-to-image tasks, primarily transferring general images to manga style [20, 32, 47, 49]. Recent works contribute to manga content understanding [30, 31, 34]. In contrast, we propose the customized manga generation task beyond style transfer to offer complete character and story-driven manga generation. MLLMs for personalized image generation. MLLMs have shown remarkable potential in personalized image generation, particularly for tasks involving image editing and customization [3, 6, 8, 16, 33, 3941, 45, 48]. Notably, CAFE [51] explores customizing subject appearances through textual instructions. However, MLLM-driven image generation for multi-character narratives remains an open challenge, primarily due to the difficulties in maintaining inter-character relationships and scene continuity. Our framework proposes an MLLM-based identity adapter that enhances dynamic story consistency in multi-character manga generation. In contrast to previous works, our framework takes multi-character features as input and edits these features collectively, following the text prompt, enabling flexible subject editing across multiple characters. In summary, we propose the first large-scale dataset for customized manga generation, along with novel framework tailored for this task. It utilizes an MLLM as character feature adapter, significantly improving the text compatibility of character personalization. 3. The MangaZero Dataset In this section, we first define the problem in Sec. 3.1. Then, we introduce the dataset construction pipeline in Sec. 3.2. 3.1. Problem Formulation 0, Bc 0, Bd 1, . . . , Bc We introduce challenging new task: customized manga generation. This task focuses on generating manga images where multiple characters, each with their distinct image inputs, are customized and positioned by users. Importantly, characters must adapt to the text prompts by modifying their expressions, motions, and poses dynamically, even when only limited set of character images is available. To generate manga story across panels (or frames), the inputs include: text prompts for each panel T0, T1, . . . , TN 1, character images = I0, I1, . . . , IK1, character bounding boxes for each panel Bc 1, and dialogue bounding boxes for each panel Bd 1, . . . , Bd 1. is represented as Pi = The visualization of panel Φθ(Ti, I, Bc , Bd ), where Φ is the overall model function and θ represents the models learned parameters. Discussion. This task diverges from existing story visualization and continuation tasks [21, 25]. Specifically, in story visualization tasks, panel is produced using Pi = Φθ(Ti), while in story continuation tasks, the panel generation depends on previous panels as Pi = Φθ(Ti, Ti1, Pi1) for > 0. Both lack explicit character control, crucial element in storytelling. Furthermore, the proposed task differs from subject-driven image generation approaches [29, 38, 46], as it demands that models not only generate accurate character representations but also modify characters attributes in response to panel captions and layouts, resulting in varied and coherent narrative visuals. Our experiments, detailed in Sec. 5, demonstrate that our model significantly surpasses baseline models in these critical aspects. 3.2. Dataset Construction In this section, we introduce the proposed large-scale manga story visualization dataset MangaZero. Comparison with related datasets. comprehensive comparison with existing datasets is presented in Tab. 1. In contrast to current manga and story visualization datasets, the proposed MangaZero dataset stands out as being larger in size, newer in source, richer in annotations, diverse 4. Method In this section, we introduce the architecture of the proposed framework, DiffSensei, which generates vivid manga panels with precise character and dialog layout control while adapting the characters status flexibly. Motivation. There are two critical problems in customizing objects and layouts in image generation: 1) Preserving the subjects intrinsic features while avoiding direct copy-paste from source images, and 2) Ensuring reliable layout control with minimal computational cost during both training and inference. To avoid copy-pasting effects, our model converts character image features into tokens, preventing the direct transfer of fine-grained pixel details. Additionally, we integrate an MLLM as character image feature adapter. The MLLM adapter receives source character features and panel captions as inputs, generating text-compatible target character features. Compared with previous customization work [38], this approach enables text-compatible character encoding and flexible character adaptation to captions. For layout control, we employ lightweight masked encoding techniques for both character and dialog layouts, significantly reducing computational costs compared with previous works [37, 44] while maintaining high accuracy in both training and inference phases. Experiment results in Sec. 5 demonstrate the effectiveness of our design. Multi-character feature extraction. As illustrated in Fig. 3, we initially extract local image features using CLIP and image-level features from manga image encoder. These two sets of features are then processed by feature extractor, which is implemented as resampler module [46]. This process can be formalized as follows: ci = Resampler([CLIP(I), ψ(I)], q, qvoid), (1) where ψ represents the manga image encoder. and qvoid are trainable query vectors for character and non-character features, respectively. re-samples the image features into the U-Nets cross-attention dimension, while qvoid guides the cross-attention in regions without characters in the layout. ci RB((Nc+1)Nq)C is the output feature for all characters, where is the batch size, Nc is the maximum number of characters per panel (padded with all-zero features as needed), Nq is the number of query tokens per character, and is the cross-attention dimension of the U-Net. Through compressing the character images into few tokens, DiffSensei avoids encoding fine-grained spatial features from reference images into the model [21, 50]. This enables focusing on the characters semantic representations rather than rigid pixel distributions. Masked cross-attention injection. We replicate the key and value matrices in each cross-attention layer, creating separate character cross-attention layers. This allows the image query features to attend to text and character crossattentions independently and combine the results from both Figure 2. We construct MangaZero through three steps: 1) Download manga pages from the internet. 2) Annotate manga panels autonomously with pre-trained models. 3) Human calibration for the character ID annotation. in manga series, and varied in panel resolutions. Compared to the well-known black-and-white manga dataset Manga109 [2], the MangaZero dataset encompasses more manga series published after the year 2000, which inspired its naming. Additionally, MangaZero includes famous series from before the year 2000 that are not featured in Manga109, such as Doraemon (1974). Construction pipeline. To build our dataset, we first download manga pages from the internet, explicitly sourcing imIt is important to note that all ages from MangaDex [1]. data will be used solely for academic research, not commercial purposes. We select 48 manga series and download up to 1,000 pages per series, resulting in 43,264 doublepage images. These images are then annotated using pretrained models. For manga-specific annotations, including panel bounding boxes, character bounding boxes, character IDs, and dialog bounding boxes, we employ the recent manga understanding model, Magi [30]. It should be noted that character ID labeling is consistent only within individual pages, which is sufficient for achieving coherent character cross-reference. Once the panel bounding boxes are obtained, we utilize LLaVA-v1.6-34B [22] to generate captions for each panel. However, we observe that character ID labeling has relatively low accuracy, which poses significant challenge for training purposes. To address this, human annotators refine the machine-generated labels, resulting in accurate and clean annotations. Finally, we split 96 pages (2 for each series) as the evaluation set and the remaining 43,168 pages as the training set. Figure 3. The architecture of DiffSensei. In the first stage, we train multi-character customized manga image generation model with layout control. The dialog embedding is added to the noised latent after the first convolution layer. All the parameters in the U-Net and feature extractor are trained. In the second stage, we finetune LoRA and resampler weights of an MLLM to adapt the source character features corresponding to the text prompt. We use the model in the first stage as the image generator and freeze its weights. attentions. In the character cross-attention, we apply masked cross-attention injection mechanism to control the layout of each character. Here, each character feature only attends to query features within its designated bounding box region. In areas without characters, query features attend to placeholder vector, qvoid. This can be formulated as: ˆz = Softmax α Softmax (cid:18) QK (cid:18) QK (cid:19) Vt+ (cid:19) + Vi, (2) k, Wt k, Vi = ciWi k, Vt = ctWt v. is the query, Wq, Wt v, Ki = where = zWq, Kt = ctWt k, Wt ciWi are query, key, and value projection matrices for the text crossattention. Wt are key and value projection matrices for the character cross-attention, initialized from Wt and Wt v. is the key dimension. ct, ci are text and character features respectively. z, ˆz are the input and output image features. α is hyperparameter that controls character attention weight. is an attention mask to manage the characters layout. Its values are defined as follows: M[i, j] = 0, if = Nc and / Bc or Bc[j] (3) , otherwise where denotes the position of query tokens, {0, 1, ..., Nc} is the character indices. The Nc-th character feature represents the placeholder vector qvoid. Bc[j] is the bounding box of the j-th character. The masked attention injection mechanism ensures that each character attends only to its specified bounding box region, while areas without characters attend to the placeholder vector. This technique achieves efficient and precise layout control for each character with minimal computational overhead. Dialog layout encoding. Panels with dialog are distinctive feature of manga images. However, most current textto-image models struggle to generate coherent, readable text [26, 28]. While some recent models can produce stable text, they remain limited in terms of text length [17]. Generating extended text, such as dialogues, continues to pose challenges. Therefore, we propose controlling the layout of dialogs rather than the content itself. In this approach, human artists can manually edit the text within dialog bubbles, leaving image generation to the models. Concretely, we introduce trainable embedding to represent the dialog layout. The dialog embedding is first expanded to match the spatial shape of the noised latent and then masked with the dialog layout. By summing the masked dialog embedding with the noised latent, we can encode dialog positions within the image generator. This process is expressed as: ˆzt = Conv(zt) + Expand(ed, zt) Md, (4) where ed is the trainable dialog embedding, zt is the noised latent in time step t, Expand is function that expands ed to the latent shape, and Md is the dialog region mask derived from input dialog bounding boxes Bd. The output, ˆzt, serves as dialog-layout-aware latent representation. This is then input into the U-Net for noise prediction. The dialog embedding effectively encodes the dialog layout, adding minimal computational overhead in space and time. MLLM as text-compatible character feature adapter. After training the image generator, our model can effectively create manga panels that adhere to specified character appearances and layout conditions. However, the model often rigidly follows the input character images, lacking flexibility in adjusting expressions, poses, or motions based on panel captions. We propose incorporating MLLM as text-compatible character feature adapter. This approach allows dynamic modifications to character states based on text prompts. training sample for MLLM is organized as [panel caption, source character image features, target character image features]. The image features are encapsulated by two special tokens, <IMG>and </IMG>. To achieve this, we compute Language Modeling (LM) Loss on the special tokens to constrain the output format and Mean Squared Error (MSE) Loss to guide the target character features based on the panel caption. To ensure that the edited character features align with the image generator, we further pass the generated features into U-Nets character cross-attention and compute diffusion loss. In this stage, only the LoRA and resampler weights in MLLM are updated. This process can be formalized as follows: ˆh, ˆci = MLLM(T, ϕ(ci)), Llm = LMLoss(ˆh, ), Lmse = MSELoss(ϕ(ˆci), ci), Ldif = Eϵ,tϵ ϵθ(zt, T, ˆci, Bc, Bd, t)2, (5) where is the text prompt. ϕ and ϕ denote the input and output resamplers of MLLM, which consist of stacked attention layers to convert the embeddings between inner and outer dimensions. ˆh refers to the MLLM predicted special token embeddings. We calculate LM Loss on it. ˆci is the predicted character features. We compute MSE Loss between ˆci and ci, the ground truth target character embedding extracted from the feature extractor. By leveraging the character ID annotations of MangaZero, we can obtain features from the same character across different panels, thus facilitating the training of the MLLM feature adapter. The adapted character feature ˆci is then passed to the image generator ϵ, previously trained, to compute diffusion loss. The total loss for training MLLM is expressed as follows: = λlmLlm + λmseLmse + λdif Ldif , (6) where λlm, λmse, and λdif are loss weights. 5. Experiments In this section, we conduct thorough evaluation of DiffSensei and compare it with baseline models. 5.1. Experimental Settings Implementation details. The image generator is constructed on top of SDXL [26]. The feature extractors weights are initialized using the pre-trained IP-AdapterPlus-SDXL [46], while the MLLM (Multi-modal Large Language Model) is initialized from SEED-X [8]. Other newly introduced parameters, including the LoRA and resampler weights of the MLLM, are initialized randomly. In stage 1, the image generator is optimized using learning rate of 1e-5. Stage 2 training employs learning rate of 1e-4 and LoRA rank of 64 [14]. The optimizer is AdamW [23]. The loss function coefficients λlm, λmse, and λdif are set to 1.0, 6.0, and 1.0, respectively. We train 250k steps for the first stage and 20k for the second stage. The source character images are chosen randomly, with 50% probability of being from the same page; otherwise, they are selected from the target image. To handle varying image resolutions during training, we adopt the bucket-based approach from prior works [26], grouping images into resolution-specific buckets. For each training batch, images are loaded from the same resolution bucket. The batch size varies between 8 and 64 in stage 1, and between 8 and 128 in stage 2. This dynamic batch sizing is necessary to prevent out-of-memory (OOM) issues, especially when processing large-resolution images. Please see more details in the Appendix. Evaluation datasets and metrics. We evaluate our model using the MangaZero and Manga109 [2] evaluation sets. Note that the model is only trained on MangaZero. Characters in Manga109 are unseen during training, serving as benchmark for generalization. To assess the generation quality of individual images, we employ autonomous metrics, which include Frechet Inception Distance score (FID) [13], CLIP image-text similarity (CLIP) [27], DINO image similarity (DINO-I) [24], DINO character image similarity (DINO-C), and the dialog bounding box F1 score (F1 score). The source character images are randomly sampled on the same page. The dialog bounding boxes in the generated images are predicted using Magi [30]. For evaluating the story visualization quality of image sequences, human preference study proves more effective. We recruit human volunteers to choose their preferred story pages from our models output and baseline models in the MangaZero evaluation set. The evaluation criteria include five key aspects: text-image alignment, style consistency, character consistency, image quality, and overall preference. Baselines. We select recent advanced story visualization models as our baselines, including StoryDiffusion [52], AR-LDM [25], StoryGen [21], SEED-Story [45], and MS-Diffusion [38]. StoryDiffusion [52] is training-free method. We directly use an SDXL text-to-image model finetuned on MangaZero for evaluation. Despite that, we re-train other baselines on our dataset for fair comparison. Table 2. Quantitative comparisons on automatic metrics. Methods followed by * use reference images as input rather than characters. Methods marked by means re-trained with dialog embedding. (a) Comparison on MangaZero evaluation set. (b) Comparison on Manga109 evaluation set. Method FID CLIP DINO-I DINO-C F1 score Method FID CLIP DINO-I DINO-C F1 score AR-LDM* [25] StoryGen* [21] SEED-Story* [45] StoryDiffusion* [52] MS-Diffusion [38] DiffSensei 0.409 0.411 0.411 0.409 0. 0.407 0.257 0.219 0.169 0.244 0.229 0.235 0.548 0.536 0.416 0.461 0. 0.618 0.507 0.488 0.405 0.362 0.641 0.651 0.004 0.012 0.006 0.002 0. 0.727 AR-LDM* [25] StoryGen* [21] SEED-Story* [45] StoryDiffusion* [52] MS-Diffusion [38] DiffSensei 0.410 0.414 0.413 0.410 0. 0.410 0.254 0.214 0.167 0.238 0.227 0.237 0.527 0.540 0.442 0.442 0. 0.588 0.491 0.493 0.428 0.355 0.600 0.600 0.005 0.004 0.005 0.001 0. 0.648 Figure 4. Qualitative comparison with baselines. Baselines followed by * use reference images as input rather than character images. Methods marked by means re-trained with dialog embedding. Our model excels at preserving the characters while following the text prompt. Our DiffSensei successively generates highlighted details in panel captions. Better viewed with zoom-in. tual prompts. Furthermore, DiffSensei demonstrates superior image quality and character preservation, as evidenced by higher DINO-I and DINO-C scores. Although ARLDM [25] achieves higher CLIP metrics, it suffers from poor image quality and lacks the architectural capability to manage multiple characters, resulting in low DINOC scores. In contrast, our method strikes balance between maintaining character appearances and adapting to text prompts. We also compare the Manga109 evaluation set with the results shown in Tab. 2b. When using previously unseen characters as inputs, our model continues to outperform the baselines. These results underscore the strong generalization ability of DiffSensei, demonstrating effective adaptation to new characters. In Fig. 5, we present the results of human preference study comparing our model to the baselines across several dimensions. Our model receives the highest ratings from human evaluators, particularly in terms of overall preference, character consistency, and image quality. These findings confirm that DiffSensei excels in rendering vivid and engaging manga stories. Figure 5. Human preference study on MangaZero eval set. 5.2. Comparison to Baselines Quantitative comparison. We quantitatively compare our DiffSensei model with baseline models using automatic evaluation metrics. The results on the MangaZero evaluation set are presented in Tab. 2a. The results highlight that DiffSensei consistently outperforms the baseline models across five key metrics. Our model improves 0.06 in the CLIP metrics compared to the multi-subject customization baseline, MS-Diffusion [38], which struggles to modify characters states effectively in response to texFigure 6. Qualitative results. Character images in red boxes are from Manga109 (The rightmost example). Our DiffSensei can generate vivid manga pages in various scenarios. Better viewed with zoom-in. More results can be found in the appendix. Table 3. Ablation study. CM is character masked attention injection. DM is dialog masked encoding. Magi means using Magi [30] image encoder. MLLM means using MLLM for stage 2 training. CM DM Magi MLLM FID CLIP DINO-I DINO-C 0.410 0.411 0.407 0. 0.407 0.230 0.225 0.228 0.231 0.235 0.593 0.591 0.600 0.618 0.618 0.610 0.637 0.635 0. 0.651 F1 score 0.361 0.364 0.653 0.718 0.727 Qualitative comparison. Fig. 4 shows qualitative comparison between DiffSensei and baseline models. The results illustrate that our model significantly outperforms the baselines in generating an entire page of manga story. SEED-Story [45] employs an MLLM to create captions for each panel, leading to unnatural narrative text and chaotic story generation that fails to form coherent story. StoryDiffusion [52] is limited to producing fixed-resolution images due to its self-attention sharing mechanism, restricting its ability to generate diverse images. It shows inferior results, probably because the input reference panel has an unbalanced aspect ratio. MS-Diffusion [38] trains with source character images from the target panel and lacks the flexibility to modify characters states effectively. In contrast, our method excels in text-following, character preservation, and overall story presentation. 5.3. Qualitative Results Fig. 6 shows several manga pages generated by DiffSensei. The results demonstrate that our method can generate vivid manga panels and provide visually plausible results for the customized manga generation task. Our model can also generalize to unseen characters, as illustrated in the rightmost example, with character images from Manga109 [2] as input. Please see more results in the Appendix. 5.4. Ablation Study Tab. 3 presents the quantitative ablation study of DiffSensei, where we systematically remove components to assess their impact. Specifically, when the MLLM component, serving as the flexible character feature adapter, is excluded, the CLIP metrics decrease by 1.73%, and the DINO-C score also drops, underscoring its role in enhancing transferring character to the text-derived states. The absence of the Magi [30] image encoder results in general decline in metrics, particularly in image quality and character similarity, highlighting the importance of the Magi encoder for effectively encoding manga characters. Magi is trained specifically on manga datasets, performing better at preserving manga characters. To investigate alternative methods for encoding character and dialog layout conditions, we experimented with replacing the dialog embedding technique by inputting the Fourier embeddings of dialog bounding boxes into the timestep embedding of SDXL [26]. This modification led to significant decrease in layout control, evidenced by the F1 score plummeting from 0.653 to 0.364, demonstrating that directly incorporating dialog embeddings into the latent is superior approach for encoding dialog layouts. Furthermore, we explored adding the Fourier embeddings of character bounding boxes to the character features as an alternative to masked attention injection. This change caused marked drop in the DINO-C metric, reaffirming the effectiveness of our original masked attention strategy. For comprehensive effect showcases, please see the qualitative ablation study in the Appendix. 6. Conclusion This paper introduces DiffSensei, novel framework for multi-character customized story visualization that integrates diffusion-based image generator with an MLLM as text-compatible identity adapter. Key innovations include masked attention control for character layout management, dialog layout embedding, and an MLLM-based feature adapter for flexible character customization. Supported by the proposed MangaZero dataset, comprising 43,264 manga pages and 427,147 panels, DiffSensei achieves superior, character-consistent panels that dynamically respond to textual prompts, surpassing existing methods and advancing the field of story visualization."
        },
        {
            "title": "References",
            "content": "[1] Mangadex, 2024. 4 [2] Kiyoharu Aizawa, Azuma Fujimoto, Atsushi Otsubo, Toru Ogawa, Yusuke Matsui, Koki Tsubota, and Hikaru Ikuta. Building manga dataset manga109 with annotations for multimedia applications. TMM, 2020. 3, 4, 6, 8 [3] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. arXiv preprint arXiv:2410.08261, 2024. 3 [4] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In CVPR, 2024. 2 [5] Junhao Cheng, Xi Lu, Hanhui Li, Khun Loun Zai, Baiqiao Yin, Yuhao Cheng, Yiqiang Yan, and Xiaodan Liang. Autostudio: Crafting consistent subjects in multi-turn interactive image generation. arXiv preprint arXiv:2406.01388, 2024. 1, 2 [6] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. In ICLR, 2023. 2, 3 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [8] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 2, 3, 6 [9] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. NeurIPS, 2024. 2 [10] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine this! scripts to compositions to videos. In ECCV, 2018. 2, 3 [11] Yue Han, Jiangning Zhang, Junwei Zhu, Xiangtai Li, Yanhao Ge, Wei Li, Chengjie Wang, Yong Liu, Xiaoming Liu, and Ying Tai. generalist facex via learning unified facial representation. arXiv preprint arXiv:2401.00551, 2023. 2 [12] Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, and Yong Liu. Face adapter for pre-trained diffusion models with finegrained id and attribute control. ECCV, 2024. 2 [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6 [15] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In NAACL, 2016. 2 [16] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In CVPR, 2024. 2, 3 [17] Black Forest Labs. Announcing black forest labs, 2024. 5, 14 [18] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, and Jianfeng Gao. Storygan: sequential conditional gan for story visualization. In CVPR, 2019. 1, 2, [19] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing reIn CVPR, alistic human photos via stacked id embedding. 2024. 2 [20] Jian Lin, Xueting Liu, Chengze Li, Minshan Xie, and TienTsin Wong. Sketch2manga: Shaded manga screening from sketch with diffusion models. In ICIP, 2024. 2, 3 [21] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm-open-ended visual storytelling via latent diffusion models. In CVPR, 2024. 1, 2, 3, 4, 6, 7, 11 [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 4 [23] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [24] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 6 [25] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. Synthesizing coherent story with auto-regressive latent diffusion models. In WACV, 2024. 1, 2, 3, 6, 7, 11 [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 5, 6, 8, 11, 13, 14 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICCV, 2021. 6 [43] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. In NeurIPS, 2024. 2 [44] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In ICCV, 2023. 4 [45] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal arXiv long story generation with large language model. preprint arXiv:2407.08683, 2024. 1, 2, 3, 6, 7, 8, 11 [46] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 4, 6, [47] Zhang Yunqian. Ai-driven background generation for manga illustrations: deep generative model approach. ORES, 2024. 2, 3 [48] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. NeurIPS, 2024. 2, 3 [49] Lvmin Zhang, Xinrui Wang, Qingnan Fan, Yi Ji, and Chunping Liu. Generating manga from illustrations via mimicking manga creation workflow. In CVPR, 2021. 2, 3 [50] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 4 [51] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, and Tong Sun. Customization assistant for text-to-image generation. In CVPR, 2024. 3 [52] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Storydiffusion: Consistent selfIn Feng, and Qibin Hou. attention for long-range image and video generation. NeurIPS, 2024. 1, 2, 6, 7, 8, 11 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 5 [29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2, 3 [30] Ragav Sachdeva and Andrew Zisserman. The manga whisperer: Automatically generating transcriptions for comics. In CVPR, 2024. 3, 4, 6, 8, 11, 13 [31] Ragav Sachdeva, Gyungin Shin, and Andrew Zisserman. Tails tell tales: Chapter-wide manga transcriptions with character names. arXiv preprint arXiv:2408.00298, 2024. 3 [32] Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Jiahe Cui, and Ji Wan. Mangagan: Unpaired photo-to-manga translation based on the methodology of manga drawing. In AAAI, 2021. 2, 3 [33] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024. 2, 3 [34] Emanuele Vivoli, Andrey Barsky, Mohamed Ali Souibgui, Artemis LLabres, Marco Bertini, and Dimosthenis Karatzas. One missing piece in vision and language: survey on arXiv preprint arXiv:2409.09502, comics understanding. 2024. [35] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 2 [36] Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen. Autostory: Generating diverse storytelling images with minimal human effort. arXiv preprint arXiv:2311.11243, 2023. 1, 2 [37] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In CVPR, 2024. 4 [38] Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. 2, 3, 4, 6, 7, 8, 11 [39] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2, 3 [40] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. In NeurIPS, 2024. [41] Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, et al. Towards language-driven video inpainting via multimodal large language models. In CVPR, 2024. 3 [42] Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem, and Dacheng Tao. Towards open vocabulary learning: survey. T-PAMI, 2024. 2 DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation Supplementary Material Overview. Appendix A. More qualitative results. Appendix B. More qualitative comparison results. Appendix C. Implementation details of the experiments. Appendix D. More ablation studies. Appendix E. Limitations and failure cases of our model. Appendix F. Details of the MangaZero dataset. Appendix G. Broader impacts. A. More Qualitative results Due to the extensive qualitative results, we have presented them in separate PDF files. Please refer to the project page for more details. We summarize the content of the documents below: page results.pdf presents additional manga page examples similar to those in Fig. 6 of the main paper. The numerous examples demonstrate that our DiffSensei model can generate vivid manga pages featuring diverse storylines, characters, and backgrounds. Notably, when provided with previously unseen character images, DiffSensei also performs well, highlighting the models generalization capabilities. Some illustrations are also shown in Fig. 13, Fig. 14, Fig. 15, and Fig. 16. long story.pdf showcases complete, extended manga story about Hinton, LeCun, and Bengio winning the Nobel Prizean expansion of the real human manga shown in Fig. 1. The manga tells fictional story of researchers Hinton, LeCun, and Bengio taking on challenge to create an AI model surpassing Transformers. Facing failures and self-doubt, they persist through rigorous research and collaboration. After overcoming numerous hurdles, their model succeeds, and years later, they are awarded the Nobel Prize, celebrating their groundbreaking achievement and the power of perseverance in science. This full version illustrates that our model can create comprehensive long stories in zero-shot setting, effectively handling real humancentric manga narratives. This story is also shown in Fig. 7. B. More Qualitative Comparison Results The additional qualitative comparisons between DiffSensei and baseline models are presented in Fig. 8. The observations align closely with those discussed in the main paper. Models such as AR-LDM [25] and StoryGen [21] cannot process separate character images as inputs, limiting their ability to control the layout of individual characters explicitly. SEED-Story [45] incorporates an MLLM to predict panel captions, but its effectiveness is constrained, resulting in some unnatural storytelling outcomes. StoryDiffusion [52] employs training-free approach, which underperforms likely due to its inability to effectively handle varying-resolution reference image inputs. MSDiffusion [38] demonstrates comparatively better performance in identity preservation and character layout control. However, it tends to constrain character input images overly, lacking the flexibility to adjust character appearances based on textual inputs dynamically. In contrast, our DiffSensei model outperforms all baselines, achieving superior results on identity preservation, text compatibility, and overall image quality. C. Implementation Details Inference details. During inference, we follow prior works [38, 46] by setting the character feature weight to α = 0.6. Additionally, we weight the MLLM-adapted character features according to = (1 β)ci + βˆci. In all our experiments, we set β = 0.4, which provides an effective balance between identity preservation and text compatibility. Training details. Our model training is conducted on 8 NVIDIA A100 80G GPUs, requiring approximately one week for the first stage and one day for the second. Before beginning the first stage, we carry out pre-training phase where we fine-tune the SDXL model [26] on manga images with the text-to-image task. This pre-training helps the model adapt to the black-and-white manga distribution, accelerating subsequent training. The pre-training required only 10k steps and can be completed within 5 hours. D. Ablation Study Qualitative ablation about the proposed modules. Fig. 9 presents qualitative ablation study of the proposed modules. Without the character masked attention injection, the character layout cannot be effectively controlled. Similarly, replacing the masked embedding technology with Fourier embedding for dialog layout control results in incorrect dialog generation. The Magi [30] image encoder is explicitly trained on manga images. When it is not used, and only the CLIP image encoder is employed, the ability to preserve character identity is significantly degraded. Additionally, without using the MLLM as flexible character adapter, the model tends to rigidly adhere to the input character images pixel distribution, which limits its ability to adapt character Figure 7. complete long manga story about Hinton, LeCun, and Bengio winning the Nobel Prize. Table 4. Quantitative ablations. The first scores are bold. The second scores are underlined. (a) Ablation on the rate of character image sourced from target panel. (b) Ablation on the MLLM weighting hyperparameter β. Rate FID CLIP DINO-I DINO-C F1 score 0.0 0.5 1. 0.408 0.407 0.407 0.233 0.235 0.233 0.615 0.618 0.610 0.646 0.651 0.644 0.718 0.727 0.729 β 0.0 0.2 0.4 0.6 0.8 1.0 FID CLIP DINO-I DINO-C F1 score 0.408 0.407 0.407 0.406 0.407 0.407 0.231 0.231 0.235 0.237 0.237 0.236 0.618 0.620 0.618 0.608 0.604 0. 0.648 0.653 0.651 0.637 0.629 0.618 0.718 0.722 0.727 0.728 0.727 0.731 appearances, poses, and motions according to the text input. Specifically, the man is not kneeling or looking down on the ground. Rate of character image sourced from target panel. Thanks to the character annotations, we can identify the same character across multiple panels, which allows us to capture different appearances, poses, and motions. During training, we sample source character using its character ID, which remains consistent throughout the page. However, we find that using completely random sample made training difficult to converge, likely due to the artistic exaggerations often found in manga. These exaggerations make it challenging for the model to learn consistent character representations when the characters appearance changes significantly across panels. To address this, we introduced an alternative sampling strategy where, at set rate, the source character is sampled directly from the target image itself. This helps the model learn identity preservation more effectively. An ablation study examining the impact of the target character sampling rate is presented in Tab. 4a. The results indicate that higher sampling rate leads to decrease in text compatibility, as the model becomes overly focused on replicating the input character image. On the other hand, lower sampling rate makes it difficult to converge the training, which also negatively impacts the metrics. Ultimately, we choose sampling rate of 0.5 during training to provide balanced performance. Hyperparameter β. Tab. 4b presents quantitative abFigure 8. More qualitative comparisons with baselines. Baselines followed by * use reference images as input rather than character images. Methods marked by means re-trained with dialog embedding. ages achieve the best balance when β is set to 0.4 or 0.6. E. Limitations and Future Work Fig. 11 showcases several failure cases of DiffSensei. (a) Unclear input character images: When the input character image is vague or unclear, our model struggles to capture the characters explicit appearance, resulting in loss of identity. This issue could be mitigated by refining the dataset or restricting user inputs to ensure that the character images are sufficiently clear for accurate reproduction. (b) Multiple character fusing. When multiple characters are provided as input, our model sometimes exhibits fusing phenomenon, where the characters appear more similar than they actually are, especially when their original appearances are already quite alike. This is likely influenced by limitations of the base model (SDXL), which also demonstrates this problem [26]. Future work could investigate more advanced text-to-image models or develop methods that better disentangle multiple character representations. (c) Style control without character input. When generating manga panels without any character inputs, the model struggles to control the output style. While the ground truth consists of simple line drawings with distinct style, the generated images tend to adopt more generic manga appearance, failing to achieve precise style control. Notably, Figure 9. Qualitative ablation of the proposed modules. CM is character masked attention injection. DM is dialog masked encoding. Magi means using Magi [30] image encoder. MLLM means using MLLM for stage 2 training. lation study of the MLLM weighting hyperparameter, β. The results suggest that increasing β enhances the CLIP score but decreases the DINO score. This demonstrates that while the MLLM-adapted embeddings become more aligned with text, they may compromise identity preservation. Conversely, lower β decreases the CLIP score, favoring better identity retention. Empirically, we set β to 0.4 to achieve an optimal balance between these factors. Qualitative results in Fig. 10 further support this observation. As β increases, character preservation diminishes. Conversely, with smaller values of β, the generated character lacks compatibility with the text. In this example, the generated imFigure 10. Qualitative ablation of β. working with manga datasets. Potential usage and future work. Our dataset is designed primarily for the task of customized manga generation, offering substantial versatility for further applications. Beyond its core purpose, it can be utilized in other promising research areas. For instance, it is well-suited for customized manga continuation, where the goal is to generate coherent story extensions based on an initial panel or sequence. This task leverages the intrinsic reading order of manga, allowing panels to be organized in narrative flow, making it possible for models to autonomously extend storyline while maintaining visual and thematic consistency. Additionally, by expanding the scope of annotations, our dataset could foster research into style control for manga generation. Each manga series or artist possesses distinctive drawing style, and our dataset is well-positioned to support training models in style controllability. This would enable fine-tuning or conditional generation of manga that can mimic specific artistic styles, providing nuanced tool for researchers exploring stylistic variation and artist-specific customization in manga creation. Overall, the datasets adaptability and extensibility make it an excellent foundation for future research into both narrative continuation and stylistic control, promising avenues for extending the capabilities of manga generation models. G. Broader Impacts The broader impacts of this paper are significant across several domains. Manga industry. The proposed technology can directly benefit the manga industry by enabling artists, creators, and publishing houses to rapidly create customized manga with detailed control over characters and layouts. This innovation can potentially streamline the manga production process, reduce production costs, and allow for more personalized storytelling that caters to niche audiences or specific Figure 11. Failure cases. when character images are provided, our approach can control the overall image style to match that of the character. Future research could focus on improving style control for different manga styles, especially in scenarios without character inputs. F. MangaZero Dataset Details Manga sources. The MangaZero dataset contains the most famous Japanese black-and-white manga series. Fig. 12a shows the cover pages of all the 48 series. These manga series were selected primarily for their popularity, distinct art style, and extensive casts of characters, providing our model with the capacity to develop robust and flexible ability for identity preservation. Annotations. Examples of character and dialogue annotations are illustrated in Fig. 12b. Additionally, Fig. 12c depicts the panel resolution distribution within the dataset. To enhance clarity, we include three reference lines representing resolutions of 10241024, 512512, and 256256. Most manga panels cluster around the second and third lines, indicating that most panels are of relatively low resolution compared to those typically emphasized in recent studies [17, 26]. This characteristic is inherent to manga data, which our work specifically addresses. Consequently, multi-resolution training becomes essential for effectively (a) Covers of manga series. (b) Examples of character and dialog annotations. (c) Resolution distribution. Figure 12. Details of the MangaZero dataset. legal issues and supports ethical AI research practices in creative fields. market demands. Education, film, and media. Beyond manga, the frameworks ability to visualize stories from text could be impactful in fields like education, film, and media production. In educational settings, it can assist in generating visual aids that align with narratives, enhancing engagement and comprehension for students. In filmmaking, it could serve as pre-visualization tool to quickly draft visual storyboards, facilitating ideation and communication between writers, directors, and production teams. Ethical use of data. The approach emphasizes the importance of legal data usage, particularly stressing the licensing and ethical constraints of the MangaZero dataset or more manga data annotated like MangaZero. This sets precedent for responsible data handling in the domain, ensuring that generated content respects copyright laws and that the data is either properly licensed or restricted to academic and non-commercial usage. This awareness mitigates potential Figure 13. DiffSensei generated results with inputs (Part1). Figure 14. DiffSensei generated results with inputs (Part2). Figure 15. Manga pages generated by DiffSensei (Part 1). Figure 16. Manga pages generated by DiffSensei (Part 2)."
        }
    ],
    "affiliations": [
        "Bytedance Seed Project",
        "Nanyang Technological University",
        "Peking University",
        "Shanghai AI Laboratory"
    ]
}