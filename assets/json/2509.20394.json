{
    "paper_title": "Blueprints of Trust: AI System Cards for End to End Transparency and Governance",
    "authors": [
        "Huzaifa Sidhpurwala",
        "Emily Fox",
        "Garth Mollett",
        "Florencio Cano Gabarda",
        "Roman Zhukov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces the Hazard-Aware System Card (HASC), a novel framework designed to enhance transparency and accountability in the development and deployment of AI systems. The HASC builds upon existing model card and system card concepts by integrating a comprehensive, dynamic record of an AI system's security and safety posture. The framework proposes a standardized system of identifiers, including a novel AI Safety Hazard (ASH) ID, to complement existing security identifiers like CVEs, allowing for clear and consistent communication of fixed flaws. By providing a single, accessible source of truth, the HASC empowers developers and stakeholders to make more informed decisions about AI system safety throughout its lifecycle. Ultimately, we also compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and discuss how they can be used to complement each other, providing greater transparency and accountability for AI systems."
        },
        {
            "title": "Start",
            "content": "Blueprints of Trust: AI System Cards for EndtoEnd Transparency and Governance"
        },
        {
            "title": "Emily Fox",
            "content": "huzaifas@redhat.com efox@redhat.com Garth Mollett gmollett@redhat.com Florencio Cano Gabarda fcanogab@redhat.com Roman Zhukov rzhukov@redhat.com"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces the Hazard-Aware System Card (HASC), novel framework designed to enhance transparency and accountability in the development and deployment of AI systems. The HASC builds upon existing model card and system card concepts by integrating comprehensive, dynamic record of an AI system's security and safety posture. The framework proposes standardized system of identiﬁers, including novel AI Safety Hazard (ASH) ID, to complement existing security identiﬁers like CVEs, allowing for clear and consistent communication of ﬁxed ﬂaws. By providing single, accessible source of truth, the HASC empowers developers and stakeholders to make more informed decisions about AI system safety throughout its lifecycle. Ultimately, we also compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and discuss how they can be used to complement each other, providing greater transparency and accountability for AI systems. We encourage readers of this paper to review our our previous work1, as several concepts in this paper are built upon proposals and concepts described there."
        },
        {
            "title": "Keywords",
            "content": "Generative AI, Open AI System Cards, Hazard Aware System Card Framework 1 https://arxiv.org/pdf/2411."
        },
        {
            "title": "Introduction",
            "content": "Generative AI has evolved from an experimental curiosity to pervasive generalpurpose technology in just few years, reshaping workﬂows, products, and cultural expectations across every sector of the economy. Ultralarge multimodal foundation modelscapable of synthesizing text, images, code, audio, and even videonow anchor consumer chatbots, enterprise copilots, creative suites, and developer tools, with daily usage measured in the hundreds of millions. Businesses cite productivity gains, accelerated innovation cycles, and new revenue streams as key beneﬁts, while individuals increasingly rely on AI for tasks ranging from language translation and personalized education to content creation and software prototyping. At the same time, the technologys democratizing reach, signiﬁcantly propelled by the community AI ecosystem growth, raises urgent questions about data provenance, intellectual property rights, labor displacement, and the ampliﬁcation of misinformation, prompting an unprecedented wave of regulatory activity, from the EU AI Act to sectorspeciﬁc guidelines in healthcare and ﬁnance. Market Research estimates that the global artiﬁcialintelligence market is valued at USD 638.23 billion in 2024 and projected to reach USD 757.58 billion in 2025, expanding at compound annual growth rate of roughly 19 percent to an impressive USD 3.68 trillion by 2034.2 This sharp, sustained trajectory highlights how AIs perceived capacity to boost operational efﬁciency and datadriven decision making is accelerating its adoption across virtually every core industry, providing both the economic context and the technological imperative for the present study."
        },
        {
            "title": "From AI models to AI systems",
            "content": "Since mid2023, the Hugging Face Hub3 has doubled in size roughly every 10 months, crossing one million models in late 2024 and topping 1.7 million by mid2025. Yet usage remains highly skewed: foundational checkpoints such as BERT, bidirectional AI language representation model, still draw tens of millions of monthly downloads, while stateoftheart Mixture of Experts (MoE) giants attract ordersofmagnitude fewer pulls. The composition of the Hub is also shifting.Natural Language Processing (NLP) models now represent only about half of new uploads as use case speciﬁc models, such as. vision, audio, and code, surge. Combined with petabytescale storage and incomplete documentation for over half the repositories, these trends illustrate both the vitality of the community AI ecosystem and the escalating need for systemlevel governance artifacts. Faster-than-ever community contributions and shared advancements, simultaneously highlights the escalating need for standardized system-level governance artifacts like AI System Cards, which can be collaboratively deﬁned and adopted by the very communities driving this growth. 2 https://www.precedenceresearch.com/artificial-intelligence-market?utm_source=chatgpt.com 3 https://huggingface.co/models While advanced AI models like GPT4o demonstrate remarkable capabilities, they have limited practical use in isolation. The model itself is often likened to an engine: powerful component that by itself cant go anywhere without chassis, wheels, steering, and other parts to form complete car. Indeed, real-world deployments show that only small fraction of an ML system is the model code, whereas the required surrounding infrastructure is vast and complex4. In other words, the majority of an AI system consists of the supporting components that allow the model to function usefully, safely, and reliably. Thisproverbial vehicle makes the engines power accessible. To be practically useful and safe, an AI model must be deployed as part of complete system within secured lifecycle encompassing interfaces, integration, infrastructure, and oversight. For example, models like GPT4o only become valuable when embedded behind user-friendly interfaces,for example, ChatGPT in this case, and connected to other tools and data via APIs. Such integration provides the plumbing that feeds the model with the right information and channels its outputs into real-world actions like populating dashboard or sending an email. Equally important are safety guardrails and monitoring: powerful models can be unpredictable, biased, or produce harmful outputs if left unchecked. Deﬁning boundaries and ﬁlters are critical to keep the AIs behavior within acceptable limits. Only by integrating the model into full system of interfaces, infrastructure, guardrails, and governance can we harness its power effectively and securely. Growth of the AI ecosystem -infrastructure, tooling and applications Over the past two years, the rise of widely-available (and free of charge) tools across the AI, ML, and data ecosystem has not only broadened access but also fundamentally lowered the barrier to building advanced AI systems. As highlighted in the 2024 MAD Landscape,5 tools such as Hugging Face, Jupyter, Streamlit, Metabase, and Airbyte now cover nearly every stage of the AI development lifecyclefrom data ingestion and transformation to model training, deployment, and monitoring. This accessibility allows individual developers, startups, and research teams to assemble production-grade AI stacks without the prohibitive licensing costs and vendor lock-in associated with proprietary platforms. 4 https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper. pdf 5 https://mad.firstmark.com/ The above image is reproduced from:: https://mad.ﬁrstmark.com/ The open source model fosters rapid experimentation. Frameworks like PyTorch and TensorFlow, coupled with model hubs such as Hugging Face, let developers integrate state-of-the-art architectures with minimal setup. Data engineering and integration platforms like Meltano make it straightforward to connect heterogeneous data sources, streamlining the pipeline from raw data to model-ready datasets. Likewise, tools like Streamlit and Metabase allow developers to wrap models in user-friendly interfaces and dashboards quickly, enabling fast iteration cycles and stakeholder feedback loops. Crucially, these tools promote interoperability and modularity (building on cloud native concepts), meaning AI systems can be built by combining best-in-class components rather than reinventing the wheel. Developers can source pre-trained models, plug them into existing data pipelines, add MLOps layers for version control and monitoring, and ship working prototype in days rather than months. This modular ecosystem is why the pace of AI system development has accelerated dramatically; smaller teams can now deliver capabilities that previously required enterprise-scale resources. Ultimately, the prevalence of these platforms has democratized AI system development. By removing cost and complexity barriers, they have enabled more diverse range of innovatorsfrom independent researchers to mid-size enterprisesto contribute to and beneﬁt from the AI revolution."
        },
        {
            "title": "Models as Systems and the need for AI System Cards",
            "content": "As AI system development becomes accessible to broader range of innovators due to such platforms, the reliance on AI Model cards to effectively convey pertinent information to those developers, researchers, and organizations has revealed key gaps critical to allowing entities to assess their risk in using models and systems that integrate them. Where model cards provide the core documentation of individual machine learning models, integrators and adopters need more transparency to effectively identify and manage risks associated with the model in the context of the system it's being used. This information is essential not only in the risk determination, but also supports hazards and vulnerability management efforts, which heavily rely on understanding the system to triage correctly and assess impact."
        },
        {
            "title": "AI model cards and their extensions",
            "content": "Model cards, initially proposed in 2018,6 are not new to the AI ecosystem. They are short, human-readable docs that accompany Machine Learning (ML) model to explain what its for and how to use it responsibly. Currently, model cards are widely used in the AI industry, mainly by model makers .7 In our earlier work,8 we proposed several extensions to the standard model card. More speciﬁcally, the paper proposes to standardize and extend model cards so theyre comparable, actionable, and interoperable across tooling. Concretely, it recommends: Creating speciﬁcation with consistent minimum ﬁeld set. This means standardizing the model card across model creators. Adding an Intent and Scope of the model. The Intent talks about users and the use cases of the model, while the scope purpose of Scope is to exclude known issues that the Model producer has no intent or ability to resolve. Add data governance and pedigree information, such as data provenance. What data was used to train and ﬁne-tune the model, from where was the data obtained, how was the data cleaned, what criteria was used to clean the data etc. 6 https://arxiv.org/abs/1810.03993 7 https://huggingface.co/ibm-granite/granite-3.3-8b-instruct 8 https://arxiv.org/pdf/2411.12275 We spoke about adding security and safety information to the model cards. We proposed the HeX format ,similar to VeX9, for keeping track of safety hazards reported and ﬁxed in the model. Lastly, we spoke about AI SBOM10 and linking them to the model card, along with possible integration with OCI. In the absence of these changes, there remains an outstanding need for this information to be conveyed. With the increased integration of models into systems, AI system developers are well-positioned to record and convey these critical data points, akin to those we see within the software ecosystem. By shifting beyond unique model-focus to the actual use of these models within an operational environment and business context, we transparently bring the AI ecosystem and the existing technology ecosystem closer to achieving operational maturity together. The concept of system cards was formalized by Meta11 as System-Level Transparency, arguing that risk lives at the system boundary (data model product) rather than the model alone, further implying that the use of model cards alone are insufﬁcient when operating AI systems in enterprise environments. This shift aligns with guidance from NIST AI RMF12 to document end-to-end risks and mitigations as well as requirements detailed in ISO/IEC 42001 and the EU AI Act."
        },
        {
            "title": "Why Model Details and Data Provenance Matter",
            "content": "The Model and Data details category of content is closest to the model card with notable distinction model cards that talk about the model. Model cards are designed to explain the how and performance of the model most relevant to machine learning and data science audience. The Model and Data details conveyed in an AI System Card go beyond this by providing additional and comprehensive information about the data used for training and ﬁne tuning. That is where the concept of Data Provenance becomes essential in the entire AI implementation lifecycle. Organizations have already heavily relied on data for critical decisions, operations, and risk management, demanding stringent veriﬁcation of its origin, quality, and authorized use. The rapid adoption of AI brings the entire supply-chain trust and accountability concerns to dramatically new scale. To tackle these challenges it will be important to apply the new standards like data provenance. Data Provenance is consistent framework used to track the origin, movement, integrity, and characteristics of data, essential for understanding data set's origin, quality, and intended use. Data Provenance is critical because it establishes veriﬁable audit trail for data in the standardized machine-readable format, encompassing 9 https://www.cisa.gov/sites/default/files/publications/VEX_Use_Cases_Aprill2022.pdf 10 https://owasp.org/www-project-aibom/ 11 https://ai.meta.com/research/publications/system-level-transparency-of-machine-learning 12 https://www.nist.gov/itl/ai-risk-management-framework 13 https://artificialintelligenceact.eu/ its origin, transformations, and usage throughout its lifecycle. This granular lineage is essential for: Veriﬁable Lineage with Contextual Metadata: Provenance provides comprehensive record of data's journey, including timestamps, actors, processes, and systems involved at each stage. This contextual metadata (e.g., sensor calibration data, algorithm version, data cleaning scripts) allows for deep understanding of data quality and reliability. Built-in Integrity Checks and Audit-Readiness: By recording cryptographic hashes or digital signatures at various points in the data supply chain, provenance enables robust integrity checks. This ensures data has not been tampered with and facilitates efﬁcient auditing by providing an immutable record for regulatory compliance (e.g., GDPR, HIPAA) and internal governance. Centralized Access for Evaluating Reliability and Compliance: centralized provenance repository allows for systematic evaluation of data's trustworthiness. This enables data consumers to assess the data's ﬁtness for purpose, understand potential biases, and verify adherence to organizational policies and regulatory mandates. Supports Operational Transparency and Explainable AI (XAI): In complex data ecosystems, provenance offers transparency into data ﬂows, aiding in debugging and performance optimization. For AI/ML models, provenance links model outputs back to their input data and training processes, contributing to explainability by demonstrating how speciﬁc data points inﬂuenced model predictions, which is crucial for ethical AI and accountability. While the standardization work around the Data Provenance framework and its technical details (like machine-readable formats) is not completed yet, number of high value use-cases are already demanded by the industry. Various roles across data science, engineering, legal, and procurement would need to rely on data provenance to ensure trust, compliance, security and usability of data. For example, data scientists and engineers need to validate data rights, freshness, and privacy before building or sharing models. Product managers require transparency in data generation, source quality, and downstream use constraints to select or build effective AI solutions. Procurement, legal and governance specialists depend on provenance to assess legality, redundancy, and data quality. Across all cases, data provenance is critical to reduce risk, enable compliant data use, and maintain conﬁdence in AI and analytics outcomes. The model and data details such as Data Provenance can support adopters in deﬁning the potential quality of the AI systems output, building trust through self-reporting breadth and depth of its dataset. In simple terms, the model card provides an overview., and the model and data details of the system card provides you with manual of understanding on how the model was incorporated into the ﬁnal system. When it comes to risk management, this information can support discovery of hazard and vulnerability sources, which is essential for driving mitigation and remediation; the core to successful risk management program and necessary to demonstrate compliance with standards for quality, accuracy, and security such as those detailed by the EU AI Act."
        },
        {
            "title": "What makes AI system cards useful",
            "content": "AI System cards are generally divided into four categories of content, each with its own relevance to existing organizational processes for acquiring, adopting, and integrating technology and the corresponding risk these activities and technologies introduce. The System Overview and Intended Use category closely aligns with content conveyed on the model card itself, the details and intended use. We can and should consider the model card as sub-section or linked external content to the overall AI system card for completeness in consideration. For entities seeking to make use of AI systems, this section in the card can inform adopters what is out of scope or prohibited in use - allowing them to understand if the system functionally aligns with the need and understand where additional liability or risk can transpire from inappropriate use. It also deﬁnes the operational boundaries of the system, essential for understanding whether use can result in safety hazard likelihood of occurrence. For organizations considering AI risk frameworks like NIST and ISO/IEC 42001/23894, or in scope of legislation like the EU AI Act, this category provides governance and contextual mapping necessary to fulﬁll several requirements that contribute to risk classiﬁcation, deﬁning policies and controls, and make risk of inappropriate use explicit. The evaluation and performance metrics category details how the AI systems performance was measured, building on the performance information of the model itself, and the corresponding results of those evaluations. For adopters, this sets realistic expectations for the AI systems reliability and accuracy, further building trust in use. It can provide an indicator of effectiveness, curtailing the extensive needs to babysit an unreliable model that can negatively create more risk and harm in businesses using it. This does not remove the need for human in the loop for decision making when using AI systems, but can provide performance threshold essential for organizational process integration to assess where and at what level that should occur. These evaluation metrics directly contribute to understanding the AI systems inherent risk of failure. This information is crucial to industries such as ﬁnancial services, banking, healthcare, citizen services, and governments. If an AI systems evaluation metrics indicate low accuracy on key topics, the probability of hazard occurring from its use in those topical areas is increased and informs the quantitative analysis required to understand not only the likelihood of that hazard realization, but also the magnitude of the potential harm that may result. The last category of content in the AI system card is perhaps the most actionable for adopters because it directly conveys known weaknesses about the AI system. Limitations and known biases transparently inform the potential of hallucination by model, which can result from knowledge scope or freshness of the knowledge base. This allows organizations to identify compensating controls to reduce the risk introduced by the AI system. This can be building additional guardrails or introducing key checkpoints for human-in-the-loop to validate and verifying output before action is taken. In an industry seeing rapid development and change, the freshness of the knowledge base is essential to understanding if the output is based on topic that must be ﬂagged for review to understand its recency. The content in this category directly guides and informs any impact assessment being conducted, such as those required by ISO 42001 and is key point in the transparency pillar of the EU AI Act. These four categories of content, located in the AI system card, directly contribute to an organizations effective risk management for the use of AI in its infrastructure and business processes."
        },
        {
            "title": "AI system cards and their use in the industry",
            "content": "Currently, system cards are being published by several major frontier model creators: OpenAI System cards for frontier models and features, for example, GPT-5.14. They describe capabilities and limits across modalities, pre andpost-deployment safety tests, and third-party evaluations, such as METR and Apollo, for autonomy and persuasion risks. Anthropic Claude - System cards like Claude Opus 4.1.15 with Anthropic's Responsible Scaling Policy (RSP)-aligned safety testing, prompt-injection and computer-use risks, and extensive red-team evaluation sections. Recent cards and addenda track new reasoning and coding capabilities and the associated hazards. Meta broad library of 22 system cards explaining major AI systems behind Facebook andInstagram, ranking information, such as,reeds, reels, explore, and search, plus later cards for generative AI features. The cards aim to be readable for general users and show signals, controls, and personalization choices. Other model creators leverage alternative structured formats to convey similar data found within system card: Microsoft Publishes Transparency Notes16 for Copilot products that are functionally system-card equivalents, what data they use, safety boundaries, failure 14 https://cdn.openai.com/gpt-5-system-card.pdf 15 https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf 16 https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-transparency-note modes, and deployment controls. The notes now extend across services and developer surfaces. Google Emphasizes model cards, such as Gemini andGemma,17 and system-level safety docs and playbooks for developers, to include policies and evaluations. Lack of standardization on the content structure and the format for conveying this critical information can and will contribute to confusion by adopters of AI systems looking to compare products and projects against organizational needs and risk tolerances. Regardless of the structure and overall content, they all heavily rely on human-generated content to convey these concepts and statistics, which should begin to be shifted to machine-readable content for automated risk management and policy creation."
        },
        {
            "title": "Unaddressed elements in the currently published System Cards",
            "content": "Several of our proposed extensions to model cards are still outstanding, with system cards highlighting the continuing gap in making model consumption and deployment challenge for adopters considering the overall risk use of model presents to their systems and business. Interoperability and comparability is essential for both pre and post operational decisions of models. Across organizations and companies, the sections, metrics, and risk taxonomies differ, making cross-vendor comparison by AI system adopters exceedingly difﬁcult. Frameworks like CLeAR speciﬁcally call for Comparable, Legible, Actionable, Robust documentation, however many cards arent yet comparable or machine-testable.18 Most cards are PDFs and web pages, with very few exposing structured schema for tools to ingest and for use in policy enforcement and audits. NIST AI RMF encourages operationalization, but theres no de-facto schema in place today. With minimal or no information about the provenance and pedigree of the data used to train the model or the pipeline used to clean the data before training, consumers and adopters of AI systems cannot independently verify and validate any claims purported by the producer of an AI system card. few cards exist as living documents that log ﬁeld failures, drift, and mitigations over timedespite NISTs emphasis on lifecycle risk management, which goes beyond the point of creation to articulate everything that has transpired to the state at which the AI system exists today. These make no mention of any security or safety incidents remediated in the 17 https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf 18 https://shorensteincenter.org/clear-documentation-framework-ai-transparency-recommendations-pract itioners-context-policymakers current release(s) of the system. The dynamic nature of models and their premise for inherent bias demonstrates that the history of the model and the system built around it are equally important in understanding the risk system presents. Analogous to softwares changelogs, CVEs, and SBOMs, AI needs living system cards that record provenance, updates, and remediated hazards across releases.Without them, stakeholders cant contextualize risk or verify claims. While the current industry and regulatory focus remains on LLMs, the proliferation of powerful AI systems interfacing directly with humanity highlights the volume of poorly documented, yet signiﬁcant hidden risks to daily life. Metas library is exemplary for recommenders, but across the industry, many safety documents focus on LLMs, while other high-impact AI systems, such as ads ranking, trust and safety automation, remain unevenly documented,19 especially related to security and safety details. This spotlight effect needs to be addressed. Developers and innovators have moved beyond the models to the practical use of these in day-to-day activities, and so too must the documentation, controls, and regulatory requirements. It is imperative to ensure such impactful systems meet the same level of scrutiny that the LLMs theyre built upon are required to undergo. Models are only one part of an AI system. The end-to-end risk proﬁle is shaped just as much by the serving layer, theAPI gateway,inference server, and autoscaler, along with caching, such as KV and prompt caches, and also orchestration, for example, Kubernetes and OpenShift, data connectors and vector stores, guardrails and policy engines, and observability. Most system cards narrowly focus on the backend model, omitting these non-model components and their security and safety controls like authN/Z, isolation, patch levels, logging and retention, rate limits. Documenting them delivers concrete value. It reveals the attack surface and data ﬂows, explains failure modes, enables reproducibility and incident response like trace leak to cache retention rather than the model, and ties mitigations to versions for governance and audits. For example, RAG assistants risk often hinges on vector-DB ACLs, connector scopes, and cache policies more than the base model itself. Without these details, stakeholders cannot accurately assess system risk."
        },
        {
            "title": "Principles behind open and transparent system cards",
            "content": "In this paper, we present open system cards: standardized, machine-readable dossiers for AI systems that go beyond capability benchmarks to include security ﬂaws, safety hazards, and their remediations across versions. Rather than asserting that one model is better than another, system cards enable side-by-side, evidence-based comparisoncovering capability metrics under stated conditions, data provenance and governance, deployment and 19 https://transparency.meta.com/features/explaining-ranking/fb-feed-recommendations/ guardrail conﬁgurations, known limitations, and versioned record of hazards and ﬁxes. This is so stakeholders can assess suitability for given use case. We present the Hazard Aware System card framework (HASC), which is living document designed to be the central hub for identifying, monitoring, and responding to AI system hazards in real-time. It is designed to serve as the primary vehicle for achieving transparent and open system safety of AI, providing internal stakeholders with actionable intelligence and external stakeholders with clear, trustworthy information about system performance and incident response. By adopting this framework, model creators can move from reactive to proactive and transparent safety posture, building trust and demonstrating industry leadership. The remainder of this paper discusses this new framework, examines some signiﬁcant challenges and strategies, and provides real-life examples of how these open system cards can be useful and applied effectively. The Hazard-Aware System Card (HASC) The HASC expands the traditional AI System Card with new, dynamic sections focused on safety and response. It is designed to be integrated directly into the AI development lifecycle and operational monitoring tools. Equally important, the HASC is designed to plug into the AI development lifecycle and the ops toolchain. It is meant to be machine-readable entity, generated during the model training and AI system build process, for consumption by external entities wanting to use the systems."
        },
        {
            "title": "Components of HASC",
            "content": "The type and amount of information made available in system card depends on the amount of transparency the vendor would like to provide. In this section, we present few components that we feel are essential and need to be included for the HASC to make sense. We also include optional components that vendors can add to deepen assurance without forcing disclosure of sensitive intellectual property. These optional items can be phased in, based on the systems risk level and the organizations maturity. It is worth mentioning that some content is manually generated with less probability of change, whereas others can be automatically generated."
        },
        {
            "title": "Essential components",
            "content": "System blueprint The System Blueprint section serves as the foundational, static component of the HASC, providing comprehensive technical overview of the AI system. See the following, more detailed, breakdown of the components that would be included in the System Blueprint: System architecture: This is high-level overview of the AI system, detailing its AI and non-AI components, data ﬂows, and where the core models perform their inference. It includes diagrams showing how different microservices or modules interact, from data ingestion to user output. Model and data provenance: This section provides links to the \"AI Bill of Materials\" (AI SBOMs) to show which models are used by the system. It should list the speciﬁc core models used, including their version numbers, for example, model-name-v2.1, as well as links to the additional metadata such as veriﬁable data provenance Intent and scope of the system: This is inline with the concepts explored in our previous paper20. Proactive hazard analysis This section of the HASC is designed to proactively identify and document potential risks associated with the AI system before deployment. It is foundational component of the framework. Here is breakdown of its key components: Hazard Log: The Hazard Log is structured list of potential harms that have been identiﬁed through various pre-deployment safety exercises such as red teaming, threat modeling, and ethical reviews. Examples of these potential harms include severe bias, the generation of unsafe content, privacy violations, and manipulation risks. Ideally the Hazard Log should only reference HeXs regardless. We discuss HeX in more detail later in the paper. Hazard probability score: Risk is all about context, and without the operational environment or use, you cannot ascertain potential impact. If the hazard is identiﬁed in the system, its occurrence and interference can be expected . Therefore, the probability of the hazard occurring given an input value is the most appropriate metric here. Embedded guardrails: This component provides clear description of the speciﬁc technical and policy safeguards that have been designed to mitigate each identiﬁed hazard. It serves as record of the safety measures implemented to protect against the risks outlined in the Hazard Log. Incident response and hazard remediation Our previous paper discussed the distinction between AI security ﬂaws and safety hazards in detail. The AI system card should contain section that includes all of the security and safety issues ﬁxed in the current, speciﬁed version of the system. If dynamic section is not feasible, then at minimum, link to such publicly available document should be provided. 20 https://arxiv.org/abs/2411. This includes any CVEs21 ﬁxed to remediate security issues and any hazard numbers which were assigned and ﬁxed to remediate safety issues. Only issues related to components listed in the system card should be mentioned. We propose hazard numbers or identiﬁers in the section below, since they are applicable to both models as well as systems."
        },
        {
            "title": "Optional components",
            "content": "Several optional components can be included in the AI system card, depending on the type of system in question, and how transparent the vendors can be. These include, but are not limited to the following: The inference engine used in the models backend. The agentic architecture used in the system. The hosting platform. Open source components used to make up the system, for example, vector databases and front end tools. Hazard Identiﬁers and HeX In our previous work, we described in detail the difference between AI security and safety issues. In the case of AI security, the fact is that the security ecosystem is well researched and organized, with proper assignment, tracking, and management of vulnerabilities which each one has an identiﬁer called CVE22 ID. We also proposed possible system for doing the same with safety hazards which involved separate committee for managing such safety identiﬁers. In the absence of such centralized system, we propose assignment of IDs by the same organizations that build these AI systems. This is not an ideal solution, and such system can quickly lead to confusion, overlapping IDs and general misinformation about the nature of the hazard, its affected components, and its ﬁxes. However, the authors feel that in the absence of centralized solution this arrangement is better than having none at all. Authors of this paper strongly suggest engagement with public AI forums to centralize the management and governance of such system to deal with hazard identiﬁers. In either case, we propose an identiﬁer of the format: <common identiﬁer>-<year>-<number>. For example, ASH-2025-0023: AI Safety Hazard (ASH), the year in which it was discovered, and the running number of the hazard identiﬁer within that year. We also propose to use this hazard identiﬁer in the AI system card as well as HeX data, as discussed in our previous work. 21 https://www.cve.org/Media/News/item/blog/2024/07/09/CVE-and-AIrelated-Vulnerabilities 22 https://www.cve.org/"
        },
        {
            "title": "Generation and consumption of the AI system cards",
            "content": "AI system cards are most effective when treated as code and generated automatically from the software delivery pipeline. JSON Schema23 serves as the contract that details what must be captured: metadata, intent or scope, model and guardrails versions, data provenance, evaluation results, hazards and mitigations, governance contacts, and references. During build and deploy, CI jobs auto-populate the card from authoritative sources: model registries for model/version and evaluations, IaC and Kubernetes/OpenShift manifests for hosting and topology, guardrails/moderation conﬁgs for policy versions and thresholds, data catalogs or ML-BOMs for training/augmentation lineage, and issue trackers for hazard records and status. The resulting JSON instance is validated against the schema, signed with component such as in-toto/SLSA attestations, versioned with the release tag, and rendered to Markdown/HTML for human consumption. This automation minimizes manual burden, keeps cards current by construction, and yields diffable, auditable artifacts suitable for post-mortems and external reviews. Since the card is machine-readable, it can be consumed by multiple control points without bespoke integrations. Organizations product security policies can enforce release gates with policy-as-code, for example, blocking production deploy if the card lacks security contact, if hazards exist without mitigation status, or if the guardrails version regresses. Risk and compliance teams can aggregate cards into an inventory that highlights stale entries, missing references, or systems whose data provenance does not meet organizational standards. Operations teams can wire incident runbooks to the cards Known hazards Mitigations, accelerating containment, for example, toggling stricter prompt policy or rolling back model version documented in the card. Vendor-management and procurement can export concise assurance proﬁle from the same JSON to answer due-diligence questionnaires, map to frameworks, for example, NIST AI RMF-aligned controls, and demonstrate supply-chain transparency via SBOM/ML-BOM links. Standardized ﬁelds enable cross-team and cross-vendor comparison: two systems produced by different groups can still be scored by the same policy engine and visualized on common dashboard. System cards capture both technical and governance signals, making them source of durable evidence for audits. For example, model changes tied to dates, rationale, and eval deltas, guardrails versions and thresholds, and provenance updates. Over time, organizations can analyze card histories to identify systemic risks and trendssay, recurrent jailbreak classes or data-quality regressions, and prioritize structural ﬁxes rather than one-off patches. 23 https://github.com/RedHatProductSecurity/ai-system-card"
        },
        {
            "title": "Major challenges and mitigation strategies",
            "content": "AI system makers face clear challenges in adopting the AI system card paradigm. Some organizations have navigated this by issuing partial cards, including certain components but omitting others. Conversely, some completely ignore the paradigm, releasing only enough information to stay competitive. Challenge: Balancing competitiveness and transparency This challenge represents the fundamental tension between the desire for comprehensive AI transparency, as championed by frameworks like the HASC, and the competitive realities of the AI industry. While transparency is increasingly recognized as crucial for trust, regulation, and safety, companies often have strong incentives to guard proprietary information related to their models, data, and underlying infrastructure to maintain their market position. Mitigation: Transparency as strategic imperative The history of software and technology provides us with powerful lesson, proprietary systems can lead to slow, siloed innovation. The creation of Linux and the explosion of open source software were catalysts for an entire modern ecosystem of innovation, permitting AI to exist. Today, we see similar trajectory in the AI ecosystem. Massive repositories like Hugging Face provide thousands of publicly available, high-quality models24. There has been shift in the AI ecosystem since the early days of chatgpt, where large-scale data sets are increasingly more common25, and even frontier model developers now release models for research and consumption, sometimes under open source licenses26. These two distinct changes can be seen as an acknowledgement that community engagement, key characteristic in open source, drives progress and adoption. Continued withholding of models basic architecture and training methodology isn't where the value of the model or system lies, the basic blueprint is universally available. The value has shifted to the application, reﬁnement, and trust enabled. While the objection that training data is intellectual property is valid, it is frequently misapplied. Truly unique and curated prioritary data sets can provide competitive advantage and do warrant protection, however, as an industry we collectively and continuously overclassify data. The vast majority of data used in training is commonly derived from publicly available resources or represents universally known information, often through different lens. The AI system cards are not designed to disclose key strategic components, but address consumers needs, be they models or systems, for system transparency27 prior to buying and integrating AI enabled products within their pipelines. 24 https://huggingface.co/models 25 https://laion.ai/projects/ 26 https://openai.com/index/introducing-gpt-oss/ 27 https://kpmg.com/us/en/articles/2025/when-trust-ai-matters.html Remember that opaqueness has cost28 that shows up as risk aversion and increased regulatory scrutiny, and pushes the industry back towards castle and moat security models. Challenge: Transparency leads to systems being exploited by bad actors It is common thought that full transparency in AI system cards can unintentionally expose sensitive details about hazards and response mechanisms. Over-disclosure may give attackers roadmap to exploit vulnerabilities or bypass safeguards, while also creating legal and reputational liabilities if stakeholders interpret documented risks as negligence or unreliability. Mitigation Early critics of open source argued that revealing code would arm attackers; in practice, security through obscurity didnt hold29. Openness increased scrutiny, raised the discovery rate of ﬂaws, and crucially reduced time-to-ﬁx via coordinated disclosure, CI/fuzzing, reproducible builds, and shared CVE ecosystem, leaving software more secure overall. The lesson for AI system cards is similar: make design, evaluation methods, governance, and high-level hazards public to enable external review and accountability, while gating precise exploit paths and defense triggers such as red-team prompts, ﬁlter thresholds, bypass recipes to trusted channels. In short: open design and closed exploit details borrow the transparency that strengthened open source without giving attackers roadmap30. Challenge: Maintenance is resource intensive Due to some of the extra work required in keeping the system card updated, it is possible that engineering teams think of the HASC as blocker to rapid innovation. Especially when there is uncertainty about certain ﬁxes being properly applied or not. Mitigation Automation is the answer to reducing toil in this case. The machine readable AI system card should be designed to be generated automatically during the model build and deployment pipeline and should be available for consumers in machine readable form. Additionally, another layer of automation can be added by agentic servers, such as MCP or others, advertising system cards via publicly available end points. Consumers, agentic or otherwise, would contact these end points, download the system card, and make decisions based on pre-set safety standards, on whether they want to use this AI system or not. Further in this paper we discuss JSON schema and discuss how some of the previously discussed automation can be achieved. 28 https://www.oceg.org/what-does-transparency-really-mean-in-the-context-of-ai-governance/ 29 https://www.korte.co/2025/07/31/the-security-advantage-ofopen sourcee-software/ 30 https://arxiv.org/pdf/2501.18669v"
        },
        {
            "title": "Scenario",
            "content": "In this section we present use-case scenario for an open AI system card. There may be other uses depending on what the system is and what kind of data is shared in the system card. The system: public-facing \"AI Health Assistant\" chatbot. Its HASC is currently at v1.2. 1. The incident Live monitoring detects spike in user reports and downvotes. An internal review discovers that users are successfully tricking the chatbot into generating plausible but dangerously incorrect medical advice by framing their questions around celebrity wellness trends. 2. Triage and hazard identiﬁcation The AI Safety team investigates and determines this is new variant of known hazard. They assign it unique identiﬁer: ASH-2025-0142: \"Model safety bypass via social/pop-culture framing.\" 3. The ﬁx The engineering team implements two-part ﬁx: Guardrail update: They update the prompt of separate \"safety check\" model, guardrail, to be more sensitive to celebrity names and pop-culture jargon in medical queries. System prompt change: They add line to the main chatbot's system prompt, explicitly instructing it to defer to human medical professional when wellness trends are mentioned. 4. Updating the Hazard-Aware System Card (HASC) After testing and deploying the ﬁx, new version of the HASC is published: v1.3. The key is updated in the card. Adding new section, \"Version history and changelog,\" would make this transparent. v1.3 (Current) - Published 2025-07Change type: Minor (safety enhancement) Associated hazard(s): ASH-2025-0142 (Model bypass via social/pop-culture framing) Summary of changes: Strengthened guardrails against medical disinformation disguised as pop-culture queries. Affected components: Proactive hazard analysis: Added ASH-2025-0142 to the hazard log. Embedded guardrails: Updated description of the \"Medical query safety check\" guardrail model to reﬂect its new sensitivity. System blueprint: Updated the \"System prompt\" documentation to include the new instruction. Linked incident report: [Internal link to incident report 2025-07-23a] v1.2 - Published 2025-06Change type: Minor (performance tuning) Summary of changes: Adjusted model parameters to reduce latency. No safety components were affected. v1.0 - Published 2025-02-01 Change type: Major (initial release) Summary of changes: First public release of the AI Health Assistant system."
        },
        {
            "title": "Automated generation of AI System cards",
            "content": "As discussed in the earlier sections of the paper, the ability to automate generation and consumption of system cards is one of the biggest factors to consider when deploying these artifacts and critical to enabling their consistent, repeatable creation within the ecosystem. In this section, we present proposal for automated generation of system cards as part of the build process. Generating AI System cards at build time is the most accurate and efﬁcient method of consistently capturing key data coupled with the speciﬁc model system version and beneﬁts from existing supply chain security practices being applied such as signing. The proposal is but one example and may not be suitable for all use cases or deployment scenarios. However it should serve as basic guide on how such automation can be implemented. The system consists of data source, template, and glue script. The glue script combines the template and the data source to produce the ﬁnal artifacts. The format of the ﬁnal artifacts can be varied as required and can be stored at varied places within the infrastructure. For our proof of concept, we use yaml generated during the build process with additional inputs from Quality Engineering processes. This is optional but encouraged. Output from other teams, such as the security team is for any security or safety tracking data as the main data sources for creating the AI System card. The yaml is generated at each stage in bits and is combined after the process is complete. The glue script combines this with json template and creates an HTML artifact, which can be published along with the AI system. This process allows variations, such as using database to collect and furnish information for the system card. This can be beneﬁcial for mature organizations collecting and storing security and other testing metadata about artifacts generated from their build system to attest what has transpired for given software, application, or system. The ﬁnal system card can be made machine-readable only or both machine and human-readable by publishing an HTML as well as yaml/json for machine reading. It is recommended that both are supported as consumers of an AI system will likely require the contents of the AI System card to make an informed risk decision before integration and deployment. At deployment or runtime, organizations can establish policy rules for decision or enforcement of AI systems against their risk tolerances and pre-deployment checks; this is made signiﬁcantly easier when the data values used in the ruleset are machine readable. Using automated AI System card generation, two variants of the system card can be easily created to allow internal consumption and documentation containing information, which is not suitable for external release as well as public version. The system card can be embedded in the ﬁnal artifact of the AI system if they are to be shipped to customers. If shipping models or systems as OCI artifacts, we can embed the system card inside the OCI layers. proof of concept code, along with sample json schema and yaml data, is available for review and further integration at: https://github.com/RedHatProductSecurity/ai-system-card AI system cards and ISO/IEC 42001 standard The Hazard-Aware System Card (HASC) and the ISO/IEC 42001:202331 standard aim to promote responsible AI, but they approach this goal from different perspectives. The HASC is presented as dynamic, transparent, and machine-readable documentation framework for individual AI systems, focusing on their security and safety posture throughout their lifecycle. In contrast, ISO/IEC 42001 is an international standard for an AI Management System (AIMS), providing requirements and guidance for organizations to responsibly develop, provide, or use AI systems by establishing, implementing, maintaining, and continually improving comprehensive management system. The system card, speciﬁcally the HASC framework, directly supports and aligns with several core tenets of the ISO/IEC 42001 standard. For example, the HASC's four categories of content System Overview and Intended Use, Model and Data details, Evaluation and Performance metrics, and Limitations and Known Biases, provide crucial information that can fulﬁll various ISO requirements. The \"System Overview and Intended Use\" section of system card offers governance and contextual mapping necessary for ISO's risk classiﬁcation and deﬁning policies and controls, especially concerning inappropriate use. Similarly, the \"Limitations and Known Biases\" section directly guides and informs the AI system impact assessments required by ISO/IEC 42001, Clause 6.1.4 and Annex A.5. The HASC's emphasis on documenting data provenance and pedigree, along with evaluation and performance metrics, aligns with ISO's detailed requirements for data for AI systems, Annex A.7, and AI system veriﬁcation and validation, Annex A.6.2.4. Ideally generated automatically within the software delivery pipeline as machine-readable living document, the HASC's design directly addresses ISO/IEC 42001's pervasive requirement for \"documented information,\" Clause 7.5. This automated and standardized 31 https://www.iso.org/standard/42001 approach allows better version control, auditable artifacts, and supports continual improvement by recording provenance, updates, and remediated hazards across releases. The proposed AI Safety Hazard (ASH) ID within the HASC framework provides structured way to track and communicate safety ﬂaws, complementing the incident response and hazard remediation section of the card. This is essential for an ISO-compliant organization's AI risk treatment, Clause 6.1.3 and Annex A.6.2.6 related to repairs and updates, and communication of incidents, Annex A.8.4. While the HASC is speciﬁc artifact for individual AI systems, ISO/IEC 42001 provides the overarching framework for an organization to manage all of its AI systems responsibly, meaning that well-implemented HASCs would serve as valuable evidence and tools within an ISO-certiﬁed AIMS."
        },
        {
            "title": "Industry engagement and further directions",
            "content": "To realize the full potential of the Hazard-Aware System Card (HASC) framework, concerted effort is required to engage with industry partners, open source communities, and regulatory bodies. Authors suggest focusing on building consensus and extending the HASC's capabilities beyond its initial scope. Standardization with industry bodies: Collaboration is required within the AI community and the AI industry. The goal is to develop standardized, machine-readable schema for system cards that can be adopted across the industry, 32addressing the current lack of comparability and consistent ﬁelds. Establish shared hazard ecosystem: Similar to the Common Vulnerabilities and Exposures (CVE) system for software, we will work to promote the use of common AI Safety Hazard (ASH) identiﬁer. This will allow for shared, public database of known AI hazards and their remediations, fostering collective defense and faster time-to-ﬁx for the entire ecosystem. We proposed such system in the last paper33. Encourage phased transparency: Recognizing the competitive pressures and the \"magic sauce\" dilemma, we will advocate for tiered, phased approach to transparency. This strategy allows companies to gradually increase their disclosure, building trust with consumers who are increasingly looking for transparent systems before integrating them into their pipelines."
        },
        {
            "title": "Suggestions for moving forward",
            "content": "Automation and tooling integration: The long-term viability of the HASC depends on its integration into development workﬂows. Future work will focus on developing automated tooling to generate HASC data directly from model build and 32 https://github.com/RedHatProductSecurity/ai-system-card 33 https://arxiv.org/pdf/2411.12275 deployment pipelines. This includes creating APIs for automated system card generation and consumption, making it \"living document\" that reﬂects real-time changes. Integration with regulatory frameworks: We will explore how HASC can serve as compliance vehicle for emerging AI regulations, such as the EU AI Act and NIST AI RMF. The structured, auditable nature of the HASC's version history and incident log could provide clear record of due diligence, helping organizations demonstrate responsible development and risk management to regulators."
        },
        {
            "title": "Closing remarks",
            "content": "The Hazard-Aware System Card (HASC) framework emerges as vital blueprint for the future of AI governance, addressing the critical gap between powerful AI models and the complex systems in which they are deployed. By expanding upon traditional model and system cards, this framework introduces dynamic, living document that not only records an AI system's architecture and intent but also documents proactive analysis of potential hazards. The proposal for standardized AI Safety Hazard (ASH) ID, akin to CVE, represents signiﬁcant step toward creating shared language for communicating safety ﬂaws and their remediations across the industry. Ultimately, the HASC aims to serve as central source of truth, empowering developers and external stakeholders to make informed decisions and fostering new era of end-to-end transparency and accountability in AI development. The success of the HASC framework hinges on its broad adoption and integration into the broader AI ecosystem, and this paper presents several strategies to overcome key challenges. The shift from security-by-obscurity to transparency, as demonstrated by the open source community, can serve as model for AI safety. This framework is not merely technical proposal but call to action for the industry to collectively build foundation of trust and reliability, ensuring that AI's transformative power is harnessed safely for the beneﬁt of all."
        },
        {
            "title": "References",
            "content": "Sidhpurwala, H. (2024). Standardizing and Extending AI Model Cards. https://arxiv.org/pdf/2411.12275 Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems. https://proceedings.neurips.cc/paper_ﬁles/paper/2015/ﬁle/86df7dcfd896fcaf2674f757a24 63eba-Paper.pdf Mitchell, M., et al. (2018). Model Cards for Model Reporting. https://arxiv.org/abs/1810.03993 Meta AI. (n.d.). System-Level Transparency of Machine Learning. https://ai.meta.com/research/publications/system-level-transparency-of-machine-learning Korte, S. (2025). Open Design, Closed Exploit Details. https://arxiv.org/pdf/2501.18669v1 Deﬁnitions AI model: An AI model is software framework that learns from data to make predictions or decisions, simulating aspects of human cognitive processes. Source: GENAI Commons AI Security: The ﬁeld of security focused on protecting and securing AI, AI Systems, and AI workloads. It covers the security of: data and the model was trained on, the supply chain of the AI Model, security capabilities that support the operational security of models such as prompt injection protection and data exﬁltration detection, and other related areas. AI Safety: The ﬁeld of study and practices to ensure AI systems operate in manner that is safe and aligned with human values, preventing harm to individuals and society. It is considered subset of AI Trustworthiness. Source: Modiﬁed for Red Hat from GENAI Commons AI system: LLMs and everything support libraries and software, for example pytorch + inference software. RHEL AI is considered an AI system, as it is packaged to include the necessary components and software for use. Embargoed security ﬂaws: Embargoed security ﬂaws are vulnerabilities or weaknesses in software, hardware, or systems that have been identiﬁed but are not yet publicly disclosed. Information about these ﬂaws is temporarily restricted to limited group of trusted partiessuch as developers, security teams, and affected vendorsunder an agreement not to share details externally until speciﬁed date or event. The purpose of the embargo is to allow time for patches or ﬁxes to be developed, tested, and distributed before the vulnerability becomes widely known. LLM Guardrails: Guardrails are the set of security and safety controls that monitor and dictate user's interaction with LLM application. They are set of programmable, rule-based systems that sit in between users and foundational models in order to make sure the AI model is operating between deﬁned principles in an organization. Model Cards: model card is type of documentation that is created for, and provided with, machine learning models. model card functions as type of data sheet, similar in 34 https://genaicommons.org/glossary/ai-model/ principle to the consumer safety labels, food nutritional labels, material safety data sheet or product spec sheets. Security Weakness: weakness is condition in software, ﬁrmware, hardware, or service component that, under the right circumstances, could contribute to the introduction of vulnerabilities. Safety Hazard: An unexpected behavior or output outside of the deﬁned intent and scope of systems or softwares design. Safety hazards are linked with producing harmful content or outputs that can cause social, economic, and environmental harm to consumers and users. Sustainable AI: the development and use of carbon neutral and carbon negative practices to minimize the negative environmental impacts of AI technologies. While considered its own independent domain, the ecological and corresponding human harm that results from unsustainable AI development and use aligns with Responsible AI directly inﬂuencing the duration of beneﬁts for humans against environmental realities (akin to corporate social responsibility focuses). It is related to AI Ethics."
        }
    ],
    "affiliations": [
        "Red Hat"
    ]
}