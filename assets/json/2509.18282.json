{
    "paper_title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
    "authors": [
        "Jesse Zhang",
        "Marius Memmel",
        "Kevin Kim",
        "Dieter Fox",
        "Jesse Thomason",
        "Fabio Ramos",
        "Erdem Bıyık",
        "Abhishek Gupta",
        "Anqi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/."
        },
        {
            "title": "Start",
            "content": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies Jesse Zhang1,2,3, Marius Memmel1,2, Kevin Kim3, Dieter Fox1,4, Jesse Thomason3, Fabio Ramos2, Erdem Bıyık3, Abhishek Gupta1, Anqi Li2 5 2 0 2 2 2 ] . [ 1 2 8 2 8 1 . 9 0 5 2 : r Abstract Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict unified point-based intermediate representation: (1) endeffector paths specifying what actions to take, and (2) taskrelevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including 41.4 real-world improvement for 3D policy trained only in simulation, and 23.5 gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they needwhere, what, and how. Website at https://peek-robot.github.io. I. INTRODUCTION Imagine walking through crowded store when your child suddenly cries out, want the Labubu! Though youve never heard the word before, context clues guide your eyes to the fuzzy toy on the shelf, and you effortlessly weave through the crowd to grab it. What makes this possible is not raw perception ability, but the ability to interpret ambiguous instructions and distill them into just the right cueswhere to focus, what actions to take, and how to perform these actions at the low level. Similarly, if given where to focus and what motions to take, robot manipulation policy should be able to achieve the visual robustness and semantic generalization necessary for open-world deployment by focusing only on how to perform actions."
        },
        {
            "title": "A common tactic for",
            "content": "training manipulation policies is through imitation learning of human-collected robotics data [1][4], which attempts to learn the where, what, and how all at the same time. Yet their performance degrades on novel objects, clutter, or semantic variations [5], [6], since the policy alone bears the burden of handling task, semantic, and visual complexity. Such failures often entangle the axes of where, what, and howfor example, grasping distractor simultaneously reflects misplaced attention, an incorrect object choice, and wrong motion. Co-first authors, Equal Advising, 1University of Washington, 2NVIDIA, 3University of Southern California, 4Allen Institute for AI Fig. 1: PEEK enables policy generalization by modulating minimal representations of where to focus and what to do for robust policy learning. Our key idea is to offload high-level reasoning to visionlanguage models (VLMs), which can excel at semantic and visual generalization [7], [8], leaving the policy to determine how low-level behavior should be executed. Instead of forcing the policy to directly parse raw images and instructions, high-level VLM modulates the input representation to the low-level policy by providing: (1) path that encodes what the policy should do, and (2) masks showing where to attend. By absorbing semantic and visual variation, the VLM provides the policy simplified, annotated peek of the scene that gives the what and the where, while the policy only needs to learn how to perform the low-level actions. This intermediate representation helps policy execution inherit many of the VLMs semantic and visual generalization capabilities. Our VLM-modulated representation is naturally policy-agnostic, allowing it to be applied to arbitrary imageinput robot manipulation policies, including state-of-the-art RGB and 3D manipulation policies [1], [3], [9]. To concretely instantiate this insight into practical algorithm, we introduce PEEK (Policy-agnostic Extraction of Essential Keypoints), which proposes unified, pointbased intermediate representation that trains VLMs to predict what policies should do and where to focus on. Specifically, we propose to finetune pretrained VLMs [10] to predict sequence of points corresponding to (1) path that guides the robot end-effector in what actions to take and (2) set of task-relevant masking points that show the policy where to focus on (see Figure 1). During low-level visuomotor policy training and inference, we modulate the policys image observations by directly drawing these VLMpredicted paths and masks onto the image, allowing the policy to simply focus on how to act, rather than learning all three simultaneously. Doing so significantly bolsters policy generalization, combining the generality of high-level VLM predictions with the precision of low-level policy learning. In this paper, we instantiate full-stack implementation of PEEK, from devising scalable data annotation scheme that enables large-scale VLM finetuning on robotic datasets to representation-modulated training of low-level robot policies from simulation and real world data. In 535 real-world evaluations across 17 task variations, we demonstrate that PEEK consistently boosts zero-shot policy generalization: 3D policy (3DDA [9]) trained only in simulation achieves 41.4 higher success in the real world when guided by PEEK, and both large-scale vision-languageaction models (π0 [3]) and small transformer-based policies [1] see 23.5 success rate improvements. These results demonstrate the power of using high-level VLMs to absorb task complexity, providing low-level policies with exactly the minimal cues they need for generalizable manipulation. II. RELATED WORKS Object-Centric Representations. One approach to improving the visual generalization of imitation learning (IL) policies is to build object-centric representations [11][17]. Earlier works relied on human-selected abstractions or manual annotation [11], while more recent methods leverage pretrained, open-vocabulary segmentation models to visually isolate task-relevant objects [12][15], [17]. Among these, to our work, proposing policyARRO [13] is closest agnostic masking scheme using GroundingDINO [18] to filter images for task-relevant objects. However, we found in Section IV that such object detectors often fail in cluttered, realistic scenes. By contrast, PEEK queries fine-tuned VLM to predict task-relevant masking points directly, resulting in more robust masks than using object-detection models due to the VLMs extensive pre-training. Another approach, OTTER [16], implements implicit masking by filtering CLIP image patches, but this approach is architecture specifc. PEEKs policy-agnostic explicit masking allows us to integrate it with far more powerful policy backbones than OTTER, i.e., vision-language-action models like π0 [3]. Finally, while masking alone helps mitigate visual distractors, it alone cannot handle semantic variation; PEEK also provides explicit action guidance via predicted paths. Another line of object-centric methods relies on learning to decompose scenes into object-level representations in self-supervised manner, e.g., via slot-attention [19][22], which learns to map visual features into set of discrete, object-centric slots through competitive attention mechanisms. However, these methods have not been applied to real-world robot manipulation settings and generally do not work zero-shot. PEEKs use of pre-trained VLM helps it predict task-relevant points on new objects and tasks. Guiding Manipulation Policies. separate line of work improves generalization by explicitly guiding policies in how to perform tasks via 2D gripper paths. RT-Trajectory introduced this concept using human-drawn sketches at inference time [23]. Later methods integrated 2D path prediction into VLA training objectives [8], [24][26], but these approaches are tied to specific architectures. More relevant is HAMSTER [7], which trains VLM to predict future 2D gripper paths that lower-level 3D policy conditions on. While this approach aids with policy understanding of what high-level motions to perform, we found in Section IV that HAMSTER-trained policies are easily confused by visual variation. In contrast, PEEKs VLM predicts single pointbased representation that also includes masks helping the policy understand where to focus on. Other works propose guiding policies via relabeling language instructions [27][31] or behavioral priors, i.e., latent skills, learned from data [32][35]. These approaches are complementary to PEEKs image-based input representation. III. PEEK: GUIDING AND MINIMAL IMAGE REPRESENTATIONS We study how to enhance the generalization capability of arbitrary visuomotor policies to semantic and visual task variation. To do so, PEEK proposes to offload high-level task reasoning to VLMs to produce guiding (what) and minimal (where) image representation for low-level policy, which in turn actualizes how to actually perform the task through real-world actions. Concretely, we instantiate this representation via 1) 2D gripper paths and 2) task-relevant masks (see Figure 1). This hierarchical approach shifts the burden of generalization from the low-level policy to the high-level VLM, allowing the policy to focus only on how to execute low-level actions. A. Conceptual Insight Imitation learning methods train policy π(at ot, st, l) predicting an action at given RGB observations ot, proprioceptive and other sensory data (e.g., depth) st, and task instruction l. Given an expert-collected robot dataset Dπ, π is trained with maximium likelihood estimation, i.e., maxπ E(ot,st,l,at)Dπ [log π(at ot, st, l)]. PEEK explores how to improve imitation learning methods by training VLM to map (l, ot) to guiding but minimal representation, op,m , that enables zero-shot generalization to significant visual and semantic variation beyond that in Dπ. Downstream task variation can include any combination of, e.g., new scenes, visual clutter not present during training, new objects, and unseen language instructions. Formally, PEEK fine-tunes pre-trained VLM conditioned on (l, ot) to produce set of points, i.e., pt, mt VLM( ot, l), corresponding to: (1) 2D gripper paths, pt, indicating where the end-effector should move to solve the task, and (2) set of task-relevant masking points, mt, that indicate objects and regions of relevance. 2D gripper paths are defined as pt = [(x, y)t, ..., (x, y)T ] where (x, y) [0, 1]2 are normalized pixel locations of the end effectors positions at timestep until trajectory end point . Masking points are defined as mt = {(x, y)i}M i=1, an unordered set of pixel locations (x, y) [0, 1]2 of task-relevant points. Although any pre-trained text and image input VLM can be used to predict these path and mask points, prior work has found that even the best closed-source models struggle with predicting robot gripper paths without fine-tuning [7], [8], let alone masking points. Therefore, we need to fine-tune VLM on large dataset that grounds it to diverse set of robot scenes and embodiments. PEEK introduces scalable data-labeling scheme which we use to create dataset of over 2M VQA pairs, spanning 148k trajectories, 9 embodiments, and 21 robotics datasets. B. VLM Data Preparation To finetune VLMs for PEEK we assemble dataset DVLM = {(o, l, ans)i}V i=1 of image inputs o, instructions l, and text-based responses ans depending on the dataset. In this section, we introduce our datasets, and then we detail our automatic robot data labeling pipeline. Point Prediction and VQA Datasets. Like prior work [7], [8], we first incorporate readily available pixel point prediction and visual question answering (VQA) data into DVLM to maintain the VLMs general world knowledge reasoning capabilities. We use the Roboand object [36] with 770k pixel point prediction Point dataset = Point to the cushions, and ans = tasks, e.g., [(0.56, 0.69), (0.43, 0.67)], and 665k VQA examples, e.g., = What is the cat eating?, and ans = An apple. Robotics Datasets. Our main robotics dataset comes from the Open X-Embodiment (OXE) dataset [37], where we label 20 datasets from the OXE magic soup [2]. Notably, our data labeling pipeline works effectively on datasets with lots of clutter or awkward viewpoints that make task-relevant objects appear very small, such as DROID [38] (e.g., the pen in Figure 6). In contrast, we found the pre-trained object detection models [18] used by prior works to extract objectcentric representations [13], [17] to be ineffective. Finally, we also include robotics simulation dataset (LIBERO-90 [39]) in our training mix to broaden the visual feature coverage of the VLM. Now we describe how we scalably label our robotics datasets. Automatically Labeling Robotic Datasets. PEEKs VLM needs to predict list of 2D gripper path points pt and taskrelevant masking points mt given arbitrary task instructions and robot observations. Prior works label their dataset using calibrated 3D cameras (in simulation and the real world) or human annotations [7], [8], limiting the scalability of data annotation. In contrast, we devise an automatic and scalable multi-step tracking pipeline that extracts how to solve the task and what to focus on directly from robot videos. First, our representation should be minimal, i.e., it should encode task-relevant entities at each timestep t. To extract this information from video, we have to ask the following question: What entities are relevant to the task? We answer this question by tracking grid of points through time with visual point tracking model [40]. Points that move significantly throughout the trajectory correspond to the robot arm or objects being manipulated. We define this set as task-relevant points task i=1, tracked across all timesteps of trajectory [1, ], as they capture the minimal information needed by policy to solve the task. = {(x, y)i}N Second, our representation should be guiding, i.e., capture information about the (1) future relevant object movement and (2) robot gripper movement. (1) The tracking points tell us the entities position at each timestep t. To capture how they move and where they end up, e.g., object placement locations, we include points at the last timestep task . (2) We additionally construct set of end-effector points grip = [(x, y)]T by tracking the gripper throughout the video. Finally, we process the data into subtrajectories separated by when the robot manipulates an object, and conthe 2D paths pt = grip and masking points struct mt = task language prediction . The natural target ans for the VLM is then combination of the shortened pt and mt: TRAJECTORY: [(0.25, 0.1), ...] MASK: [(0.30,0.57), ...]. See Section I-A and Figure 6 for details regarding the data labeling pipeline. task t C. VLM and Policy Training/Inference with PEEK VLM Fine-tuning. We use VILA-1.5-3b [10] as our base VLM, 3B parameter VLM trained on interleaved imagetext datasets and video captioning data. We fine-tune our VLM for one epoch using the combined datasets totalling 3.5M samples with learning rate of 5e2 and batch size of 16. Fine-tuning takes 20h on 8 NVIDIA A100 GPUs. We fine-tune the VLM with standard supervised prediction objective to maximize the log-likelihood of the answers ans: maxVLM E(o,l,ans)DVLM log VLM(ans o, l). VLM Inference. During deployment, PEEKs VLM acts at higher level, absorbing the semantic complexity and visual clutter of the scene and providing lower-level policy with guiding and minimal representation. However, querying the high-level VLM at every timestep is unnecessary because the scene is unlikely to change significantly at the same frequency as the policy is acting. Since frequent VLM queries are expensive and must be run sequentially, we run the VLM at reduced frequency. While prior works predict paths either at the start of rollout [7] or at every timestep [24], our hybrid approach strikes balance between inference speed and responsiveness. To minimize the gap between training and deployment, our data labeling and training scheme reflects this design choice by querying the VLM at fixed frequency of every timesteps. VLM / Policy Interface. During inference, the policy receives an augmented image input op,m created by drawing the path pt and mask mt onto the image observation ot. We draw the 2D path pt by connecting each subsequent point in pt with colored line segment. This drawing guides the policy for which path to follow to accomplish the task. To indicate passage of time, the line segment changes from dark . To create masks, we start from black to light red canvas and use the area around the predicted task-relevant points to reveal parts of the image. For each predicted taskrelevant point (x, y) m, we create square centered around (x, y) with edge length 8% of the images size. See Figure 2 for visual depiction of path and mask drawing. We query the VLM every steps to generate pt, mt based on the current environment observation ot and apply the same Fig. 2: Policy Training and Inference Pipeline. The VLM is called every steps to generate path and task-relevant points. An (arbitrary) RGB-input policy is conditioned on the path and masked image to either predict actions for inference or for training. The same path and mask is applied onto incoming observations for steps, after which the VLM is re-queried. annotations pt and mt to all incoming observations op,m until steps have passed. t:t+H Each VLM query takes about 4-6 seconds on an RTX 3090 without any explicit speed optimization, but until the next VLM query, the policy π runs at its own inference speed. Policy Training. Consequently, we annotate all the trajectories in the policy training data Dπ to create an annotated . We train π on the PEEK-labeled dataset Dp,m dataset Dp,m using its original training objective, e.g., maximizing loglikelihood of the actions: maxπ EDp,m , st, l). We list policy and VLM query frequencies in Section I-C. log π(at op,m π π π IV. EXPERIMENTAL SETUP To demonstrate the broad applicability of PEEK, we evaluate across two real-world robot embodiments, both 2D (π0 [3], ACT [1]) and 3D (3DDA [9]) policy classes, fine-tuning and training policies from scratch. We evaluate zero-shot generalization from publicly available [41] and simulation-generated datasets to our custom setups, varying the task semantics and introducing visual clutter. Franka Sim-to-Real. To study the semantic generalization and visual robustness induced by PEEK, we require largescale robotic dataset to cover all possible motions the policy might encounter during inference. Simulation offers cheap, scalable approach to generate such dataset without going through the effort of manual data collection. We collect 2.5k trajectories of cube stacking with motion planner in MuJoCo environments with three colored cubes (sampled from {red, green, blue, yellow}) placed randomly on 40 40cm grid. See Figure 3 for visualization of the data. Our real-world setup consists of Franka Emika Panda robot [42], [43] with depth from processing RGB images from Zed 2 stereo camera with FoundationStereo [44]. In the real world, we first test policy transfer on four fixed cube configurations (BASIC), then add visual CLUTTER to assess visual robustness, and finally evaluate three SEMANTIC tasks requiring reasoning about unseen objects and placements (Figure 3). Each policy is evaluated for 5 trials per task, totaling 220 evaluations across 4 methods and 11 variations. WidowX BRIDGE. Our second environment uses WidowX250 robot with single Logitech C920 RGB camera, resembling the BRIDGE [41] environment, albeit without exactly reproducing camera angles and with different table, objects, and background wall. We re-label the BRIDGE-v2 dataset [41] (single camera angle) with PEEK according to Section III-C and zero-shot evaluate it on our setup. We evaluate on set of three tasks, representing basic generalization (to our custom robot setup), visualized in the BASIC column of Figure 4. We then evaluate CLUTTER, which adds significant visual clutter to each of the three BASIC tasks, and finally SEMANTIC, representing difficult tasks that require visual-language reasoning to complete. We perform 5 evals per task with randomized object locations. Baselines. In our Sim-to-Real experiments, we evaluate PEEKs application to 3D policies. We use 3DDA [9] as our base policy and implement all baselines on top of it. 3DDA [9]: state-of-the-art language-conditioned 3D policy conditioned on depth, RGB, and language. HAMSTER [7]: Fine-tunes 13B parameter VLM to predict 2D gripper paths for 3D policy to condition on. ARRO [13]: An explicit masking baseline using GroundingDINO [18] to segment gripper and objects. We apply masks from ARRO and PEEK to both the RGB image and point clouds input to 3DDA. To show PEEK also applies to 2D policies of different architectures, we evaluate it on ACT [1] and π0 [3]. ACT [1]: small 90M parameter transformer policy we additionally condition with language embeddings [45]. π0 [3]: 3.5B parameter VLA first pre-trained on large dataset, which we LoRA fine-tune on BRIDGE. OTTER [16]: 400M parameter transformer which implicitly masks observations by discarding image patches with low CLIP-feature alignment to the task instruction. ARRO [13]: Explicit masking baseline introduced above. We evaluate both ARRO and PEEK on top of both ACT and π0 as they are both policy-agnostic. V. EXPERIMENTAL RESULTS Our evaluation aims to address the following questions: (Q1) How much does PEEK improve semantic and visual generalization across diverse policy architectures? (Q2) How accurately does PEEK help with where and what? and (Q3) How much does each component of PEEK contribute? We answer these questions in order below. Fig. 3: Franka Sim-to-Real Tasks. Zero-shot evaluation environments along with associated path-drawn and masked images produced by PEEK. SIMULATION denotes the generated simulation data that the policies were trained on. Fig. 4: WidowX Tasks. Evaluation environments along with associated path-drawn and masked images produced by PEEK. Fig. 5: Real-World Zero-Shot Generalization Results. Task completion rates (including partial credit for grasping or reaching objects correctly) and task success rates across 3 task variants: BASIC, CLUTTER, and SEMANTIC in our Franka Sim-to-Real experiments (top) and WidowX BRIDGE experiments (bottom). Results are averaged over all trials and tasks within each variant. PEEK results are bolded for visibility. Full tables in Appendix Section I-E. A. Q1: Real-World Zero-Shot Generalization Experiments Franka Sim-to-Real. We plot results in Figure 5 (top). Overall, 3DDA+PEEK improves vanilla 3DDA by 41.4 and outperforms the best baseline, 3DDA+HAMSTER, by 2 in overall success rates. While HAMSTER shows some semantic generalization via drawing paths (40% partial success on SEMANTIC), it fails catastrophically when distractor objects are present in the scene (0% partial success on CLUTTER). Instead, PEEKs ability to also mask out irrelevant parts of the image usually completely hides task-irrelevant objects, allowing the policy to solve the task more often by only focusing on low-level control. Meanwhile, ARRO, which masks-in the robot end-effector and task-relevant objects with pre-trained object detection models, often also includes task-irrelevant objects, confusing the 3DDA policy. PEEKs VLM generalizes better to new objects and its paths help guide the policy even in cases where parts of irrelevant objects are included in the observation. The baseline results demonstrate that answering only one of where to focus or what to do is not enough to achieve semantic generalization and visual robustness. We visualize example PEEK VLM predictions in Figure 3. WidowX BRIDGE. Next, we plot the WidowX results in Figure 5 (bottom). ACT+PEEK and π0+PEEK outperform their base models by 3.4 and 2.5 in overall success rates. ARRO does not improve overall success rates of either base model as its pre-trained object detection module often fails to identify correct objects in clutter, and almost always fails to detect the robot gripper. PEEKs use of VLM allows it to consistently mask-in the correct object and draw paths accurately starting from the gripper. The VLM predictions visualized in Figure 4 show how PEEKs VLM provides effective paths and masks even in the face of distractors and tasks that require semantic and visual reasoning. Meanwhile, OTTER performs poorlybetter than ACT but worse than standard π0, and far worse than either PEEK variationπ0+PEEK overall achieves 4.5 better success rate. This result highlights the importance of policyagnostic approach, such as PEEK, that can provide explicit path and mask guidance even to already strong base policies. B. Q2: Does PEEK answer the where and what? Comparing the first columns of Franka Sim-to-Real (Figure 3) in SIMULATION, BASIC, and CLUTTER, the benefits of paths and masking become apparent: masks remove distractors from the imageshowing where to attend toand the paths guide the policy to pick up the objectshowing what to do. Similar findings hold for the WidowX (Figure 4). While the masks tell the policy what to focus on, they alone are insufficient for solving semantic variation. Take, for example, the SEMANTIC tasks; the policies training data does not contain demonstrations featuring celebrities (Give the banana to Jensen Huang in Figure 4) or various kinds of sweet treats (Knock over the syrup bottle, Put the blue cube next to the healthy items in Figure 3). By letting the high-level VLM absorb the semantic generalization proposing guiding pathsthe policy can simply actualize the path into low-level actions to solve the task. C. Q3: How does each component contribute? of on"
        },
        {
            "title": "Paths",
            "content": "performance ablate of Paths Masks Success (%) 33.5 3.1 52.8 2.9 65.6 3.1 73.6 3.9 and Ablating Masks. We the paths contributions and masks on the language-conditioned 3D (3DDA) policy the simulated cube stacking task in Table I. While the language-conditioned base policy can stack cubes, it often ignores instruction order, e.g., placing the blue cube on the red instead of the reverse. Adding only paths or only masks improves performance by +19.3% and +32.1%, respectively. Masks outperform paths since they simplify the scene by removing the distractor cube, while paths alone leave ambiguity. Yet both remain limited: cube stacking highlights the insufficiency of purely TABLE I: Ablation of paths and masks on success rate. predictive or minimal representations. Combining paths and masks, PEEK achieves gains of +7.9% over paths, +20.8% over masks, and +40.1% over the base policy. VLM Design Choices. To study VLM design choices, we evaluate on 1k holdout samples from BRIDGE-v2 [41], using Dynamic Time Warping (DTW) for paths [46] and Intersection over Union (IoU) for masks. Reducing the base model from 13B to 3B yields no loss in accuracy (both have DTW 0.12, IoU 0.68) while enabling faster closed-loop inference. Adding RoboPoint slightly improves these metrics and preserves semantic reasoning ability [7]. Finally, joint prediction of paths and masks improves performance, giving +19.3% relative gain over mask-only model (IoU 0.57) without degrading path accuracy (DTW 0.12). Full results in Appendix Section I-D. Overall, we see that both paths and masks are essential to PEEKs ability to enhance policy generalization, and our choice of small VLM model that jointly predicts unified path and mask representation great performance without sacrificing inference speed. VI. CONCLUSION AND LIMITATIONS We presented PEEK (Policy-agnostic Extraction of Essential Keypoints), framework that leverages VLMs to offload high-level reasoning in robot manipulation. By predicting point-based intermediate representationspaths that specify what to do and masks that indicate where to attendPEEK provides policies with simplified, annotated observations, allowing them to focus on how to act. Realworld evaluations demonstrate substantial improvements in zero-shot generalization across various policies. However, PEEK still inherits the biases and limitations of the underlying VLMs, which may fail in out-of-distribution scenarios or produce incorrect annotations. Our current representation is also limited to 2D point paths and masks; extending it to richer 3D or multimodal cues is an exciting direction. Moreover, although our annotation pipeline scales across existing robotics datasets, future work could explore how to bootstrap from much broader corpus of video data. ACKNOWLEDGEMENTS We thank Abrar Anwar for helping create the PEEK logo, Helen Wang for lending us difficult-to-obtain, official Labubu doll, Raymond Yu for help setting up the initial BRIDGE table and FoundationStereo pipeline, Markus Grotz for assistance in setting up the Franka controller stack (robits), Yi Li for HAMSTER baseline help, Andy Tang for assisting with initial BRIDGE camera alignment, and William Chen for providing exact measurements for us to align the BRIDGE camera positions as best as possible. We also thank Yondu.ai for hosting the Los Angeles Lerobot hackathon where we tried an early version of PEEK, and Yutai Zhou and Minjune Hwang for joining us in the competition. Additionally, we acknowledge funding from the Army Research Lab and compute resources from the University of Southern Californias Center for Advanced Research Computing (CARC)."
        },
        {
            "title": "REFERENCES",
            "content": "[1] [2] M. T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning finegrained bimanual manipulation with low-cost hardware, in Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, K. E. Bekris, K. Hauser, S. L. Herbert, and J. Yu, Eds., 2023. Openvla: An open-source vision-language-action model, arXiv preprint arXiv:2406.09246, 2024. J. Kim, K. Pertsch, S. Karamcheti, et al., [3] K. Black, N. Brown, D. Driess, et al., pi 0: vision-languagerobot control, arXiv preprint for general action flow model arXiv:2410.24164, 2024. [5] [4] G. Yan, J. Zhu, Y. Deng, et al., Maniflow: dexterous manipulation policy via flow matching, in Conference on Robot Learning (CoRL), 2025. J. Gao, S. Belkhale, S. Dasari, A. Balakrishna, D. Shah, and D. Sadigh, taxonomy for evaluating generalist robot policies, 2025. P. Atreya, K. Pertsch, T. Lee, et al., Roboarena: Distributed realworld evaluation of generalist robot policies, in Proceedings of the Conference on Robot Learning (CoRL 2025), 2025. [6] [9] [8] [7] Y. Li, Y. Deng, J. Zhang, et al., HAMSTER: Hierarchical action models for open-world robot manipulation, in The Thirteenth International Conference on Learning Representations, 2025. J. Lee, J. Duan, H. Fang, et al., Molmoact: Action reasoning models that can reason in space, 2025. T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki, 3d diffuser actor: Policy diffusion with 3d scene representations, in First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. J. Lin, H. Yin, W. Ping, P. Molchanov, M. Shoeybi, and S. Han, Vila: On pre-training for visual language models, in Proceedings the IEEE/CVF Conference on Computer Vision and Pattern of Recognition (CVPR), Jun. 2024, pp. 26 68926 699. J. Shi, J. Qian, Y. J. Ma, and D. Jayaraman, Plug-and-play objectcentric representations from what and where foundation models, ICRA, 2024. [10] [11] [12] D. Emukpere, R. Deffayet, B. Wu, et al., Disentangled object-centric image representation for robotic manipulation, 2025. [13] R. Mirjalili, T. Julg, F. Walter, and W. Burgard, Augmented reality for robots (arro): Pointing visuomotor policies towards visual robustness, arXiv preprint arXiv:2505.08627, 2025. [14] A. J. Hancock, A. Z. Ren, and A. Majumdar, Run-time observation interventions make vision-language-action models more visually robust, in 2025 IEEE International Conference on Robotics and Automation (ICRA), 2025, pp. 94999506. [15] C. Yuan, S. Joshi, S. Zhu, H. Su, H. Zhao, and Y. Gao, Roboengine: Plug-and-play robot data augmentation with semantic robot segmentation and background generation, arXiv preprint arXiv:2503.18738, 2025. [18] [17] [16] H. Huang, F. Liu, L. Fu, et al., Otter: vision-languageaction model with text-aware feature extraciton, arXiv preprint arXiv:2503.03734, 2025. P. Li, Y. Wu, Z. Xi, et al., Controlvla: Few-shot object-centric adaptation for pre-trained vision-language-action models, arXiv preprint arXiv:2506.16211, 2025. S. Liu, Z. Zeng, T. Ren, et al., Grounding dino: Marrying dino with grounded pre-training for open-set object detection, arXiv preprint arXiv:2303.05499, 2023. F. Locatello, D. Weissenborn, T. Unterthiner, et al., Object-centric learning with slot attention, in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33, Curran Associates, Inc., 2020, pp. 11 52511 538. [19] [20] O. Biza, S. van Steenkiste, M. S. M. Sajjadi, G. F. Elsayed, A. Mahendran, and T. Kipf, Invariant slot attention: Object discovery with slot-centric reference frames, in ICML, 2023. [21] Y. Zhang, D. W. Zhang, S. Lacoste-Julien, G. J. Burghouts, and C. G. M. Snoek, Unlocking slot attention by changing optimal transport costs, in Proceedings of the 40th International Conference on Machine Learning, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., ser. Proceedings of Machine Learning Research, vol. 202, PMLR, 2329 Jul 2023, pp. 41 93141 951. [22] M. Mosbach, J. N. Ewertz, A. Villar-Corrales, and S. Behnke, Sold: Slot object-centric latent dynamics models for relational manipulation learning from pixels, in International Conference on Machine Learning (ICML), 2025. [23] J. Gu, S. Kirmani, P. Wohlhart, et al., Rt-trajectory: Robotic task generalization via hindsight trajectory sketches, 2023. [24] D. Niu, Y. Sharma, G. Biamby, et al., LLARVA: Vision-action instruction tuning enhances robot learning, in 8th Annual Conference on Robot Learning, 2024. [25] C.-P. Huang, Y.-H. Wu, M.-H. Chen, Y.-C. F. Wang, and F.-E. Yang, Thinkact: Vision-language-action reasoning via reinforced visual latent planning, arXiv preprint arXiv:2507.16815, 2025. [27] [28] [26] R. Zheng, Y. Liang, S. Huang, et al., TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies, in The Thirteenth International Conference on Learning Representations, 2025. T. Xiao, H. Chan, P. Sermanet, et al., Robotic skill acquistion via instruction augmentation with vision-language models, in Proceedings of Robotics: Science and Systems, 2023. J. Zhang, J. Zhang, K. Pertsch, et al., Bootstrap your own skills: Learning to solve new tasks with large language model guidance, in 7th Annual Conference on Robot Learning, 2023. J. Zhang, K. Pertsch, J. Zhang, and J. J. Lim, Sprint: Scalable policy pre-training via language instruction relabeling, in International Conference on Robotics and Automation, 2024. L. Smith, A. Irpan, M. G. Arenas, et al., Steer: Flexible robotic manipulation via dense language grounding, in 2025 IEEE International Conference on Robotics and Automation (ICRA), 2025, pp. 16 51716 524. [29] [30] [31] W. Chen, S. Belkhale, S. Mirchandani, et al., Training strategies for efficient embodied reasoning, in 9th Annual Conference on Robot Learning, 2025. [32] K. Pertsch, Y. Lee, and J. J. Lim, Accelerating reinforcement learning with learned skill priors, in Conference on Robot Learning (CoRL), 2020. [33] A. Singh, H. Liu, G. Zhou, A. Yu, N. Rhinehart, and S. Levine, Parrot: Data-driven behavioral priors for reinforcement learning, in International Conference on Learning Representations, 2021. [34] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum, {opal}: Offline primitive discovery for accelerating offline reinforcement learning, in International Conference on Learning Representations, 2021. J. Zhang, M. Heo, Z. Liu, et al., EXTRACT: Efficient policy learning by extracting transferrable robot skills from offline data, in Conference on Robot Learning, 2024. [35] [36] W. Yuan, J. Duan, V. Blukis, et al., Robopoint: vision-language model for spatial affordance prediction in robotics, in 8th Annual Conference on Robot Learning, 2024. [37] O. X.-E. Collaboration, A. ONeill, A. Rehman, et al., Open XEmbodiment: Robotic learning datasets and RT-X models, 2023. [38] A. Khazatsky, K. Pertsch, S. Nair, et al., Droid: large-scale inthe-wild robot manipulation dataset, 2024. [39] B. Liu, Y. Zhu, C. Gao, et al., knowledge transfer for lifelong robot arXiv:2306.03310, 2023. Libero: Benchmarking learning, arXiv preprint [40] N. Karaev, I. Rocco, B. Graham, N. Neverova, A. Vedaldi, and C. Rupprecht, Cotracker: It is better to track together, in European Conference on Computer Vision, Springer, 2025, pp. 1835. [41] H. Walke, K. Black, A. Lee, et al., Bridgedata v2: dataset for robot learning at scale, in Conference on Robot Learning (CoRL), 2023. [42] M. Grotz, M. Shridhar, Y.-W. Chao, T. Asfour, and D. Fox, Peract2: Benchmarking and learning for robotic bimanual manipulation tasks, in CoRL 2024 Workshop on Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond. [43] K. Zakka, Mink: Python inverse kinematics based on MuJoCo, version 0.0.11, May 2025. [44] B. Wen, M. Trepte, J. Aribido, J. Kautz, O. Gallo, and S. Birchfield, Foundationstereo: Zero-shot stereo matching, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 52495260. [45] N. Reimers and I. Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Nov. 2019. [46] M. Memmel, J. Berg, B. Chen, A. Gupta, and J. Francis, Strap: Robot sub-trajectory retrieval for augmented policy learning, arXiv preprint arXiv:2412.15182, 2024. [47] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, and R. Girshick, Detectron2, https : / / github . com / facebookresearch / detectron2, 2019."
        },
        {
            "title": "APPENDIX I",
            "content": "A. Data Annotation Pipeline Details Tracking Points. Given trajectory of image observations o1:T , we apply CoTracker3 [40] to the video to track all moving points within the scene. Points are initialized in uniform grid across the entire pixel space (a 15 15 grid to 30 30 grid depending on how far objects are from the camera). We initialize this point grid from the middle of the trajectory because in many datasets, the gripper is not visible at the first timestep. Running CoTracker returns set of all points and their normalized image locations across the trajectory. We discard any point that does not move much throughout the trajectory, i.e., less than 5% of the image size, because they are unlikely to be task-relevant. The remaining points at each timestep t, [{(x, y)i}N t=1 with (x, y) [0, 1]2, indicate both objects that move at some point during the trajectory and the robot gripper location. Therefore, these are the task-relevant points task i=1}t. See Figure 6 (1) for an overview of point tracking. Tracking the Gripper. To track the end-effector, we apply an object detection model (Detectron2 [47] fine-tuned by [24] for end-effector detection) at every timestep. = {(x, y)N i=1]T However, we found that naıvely applying the gripper detector resulted in noisy predictions that often did not include the robot. To reduce the noise, we only keep pixels in ot around the significant points task , essentially masking out distractions irrelevant to the task. We apply the detector to this reduced representation to obtain per-timestep gripper bounding boxes (filling in frames with no detected gripper with the average of adjacent detections) and average across them to obtain the end-effector points grip = [(xt, yt)]t. See Figure 6 (2) for visual depiction. Segmenting into Subtrajectories. Because masks mt intask clude points at the final timestep , mt = task , constructing mt from long-horizon trajectories can violate the minimality principle. For example, the policy does not have to know the placement location of an object until it has actually picked it up. Therefore, we automatically break down the trajectories into subtrajectories. Our key insight is that when many task-relevant points stop moving, the robot is likely to be manipulating an object. Vice versa, when those points start moving again, the robot is likely reaching or carrying an object. This creates natural approach to splitting trajectory: by how many points in task stop moving. For each frame, ot, we track how many points in task dont move for the next 5 frames (3 for BRIDGE due to its higher control frequency). This creates list of length containing the number of stopped points for each timestep, t. On this list, we perform K-Means clustering with = 2, where the cluster with the smaller mean (fewer stopped points) corresponds to significant movement, e.g., the robot arm reaching an object, and the cluster with the larger mean corresponds to the robot performing fine-grained manipulation, e.g., grasping. Finally, we use these cluster assignments to find continuous sections i, + 1, .., 1, where the robot is manipulating an object, and use the middle frame of these sections (j + i)/2 as subtrajectory split points. This procedure results in split of subtrajectories that end when the robot finishes manipulation and start before it moves onto the next object manipulation. These subtrajectories create natural, shorter-horizon VLM prediction targets (see Figure 6 (3)). B. VLM Dataset Details OXE. We augment the dataset by re-sampling the start and end of each subtrajectory within the first and last 20% of the trajectory, repeating this procedure 5. Additionally, we discard the first 20% of each full trajectory as most of them dont feature the gripper or show very little movement. Finally, we exclude FurnitureBench, Roboturk, Dobbe, BerkeleyCableRouting, LangTable, Kuka, and FMB. LIBERO-90. We re-render LIBERO-90 [39] to 256x256 from 128x128 following [2], consisting of 3958 successfully replayed and re-rendered demonstrations across 50 tasks. Postprocessing. Due to the autoregressive nature of transformers, the inference time grows linearly with the number of tokens predicted. To further reduce inference time, we follow [7] in reducing the number of points in pt and mt by applying the RamerDouglasPeucker algorithm with tolerance thresholds ϵ = 0.05 and ϵ = 0.1, respectively. C. VLM and Policy Implementation Details For policy training, path and masks are labeled with VLM queries every = 30 and = 32 timesteps for BRIDGE and Franka Sim-to-Real. During rollouts, PEEKs VLM is queried every = 25 and = 32 timesteps for BRIDGE and Sim-to-Real respectively, and the policies all predict action chunks of length 5 and 8 respectively. D. VLM Ablation Results We ablate the VLM model size (3B and 13B parameters), training dataset mixtures (OXE labeled with Section I-A (OXE), the BRIDGE training split from our labeled OXE (BRIDGE), and the RoboPoint dataset [36]), and prediction target (p, m, + m) in Table II. The metrics recorded are DTW distance (DTW L2 distance between predicted and ground truth p), First Point L2 (L2 distance between the first point in predicted and ground truth p), Last Point L2 (L2 distance between the last point in predicted and ground truth p) for path predictions, intersection over union (IoU) for mask predictions m. Models are evaluated on 1k holdout samples from the BRIDGE test split from our labeled OXE. there is minimal difference in performance between the 3B and 13B parameter models; hence, we chose to use the 3B parameter VILA model for PEEK for its faster inference speed. The combination of predicting both paths and masks with the same model improves the performance on paths alone or masks alone on the 3B parameter model. Finally, including the full OXE dataset Overall, Fig. 6: Data Labeling Pipeline. detailed overview of the data labeling pipeline as described in Section I-A: We (1) use CoTracker3 [40] to detect moving points across each trajectory, points are discarded if they do not move significantly, and the rest become task-relevant points task; (2) mask areas without task to black and apply pre-trained gripper detector to construct 2D gripper path points grip; (3) segment each trajectory into subtrajectories; and (4) construct gripper paths and task-relevant masking points for each subtrajectory. Notice in (4) that target object placement areas become visible beforehand through including points from the last timestep task . T"
        },
        {
            "title": "Prediction Target",
            "content": "Training Dataset(s) DTW Distance (p) First Point L2 (p) Last Point L2 (p) IoU (m) 3B 13B 3B 13B 3B 13B 3B 13B 3B 13B 3B 13B m p+m p+m p+m p+m p+m p+m p+m p+m OXE+RoboPoint OXE+RoboPoint OXE+RoboPoint OXE+RoboPoint OXE+RoboPoint OXE+RoboPoint OXE (inc. BRIDGE) OXE (inc. BRIDGE) BRIDGE+RoboPoint BRIDGE+RoboPoint BRIDGE BRIDGE 0.1239 0.1197 N/A N/A 0.1229 0. 0.1237 0.1222 0.1249 0.1208 0.1237 0.1263 0.0445 0.0450 N/A N/A 0.0426 0.0419 0.0426 0.0435 0.0444 0.0428 0.0435 0.0437 0.1479 0.1447 N/A N/A 0.1422 0.1491 0.1467 0.1494 0.1430 0.1459 0.1438 0.1447 N/A N/A 0.5799 0.5858 0.6863 0. 0.6785 0.6835 0.6792 0.6855 0.6727 0.6798 TABLE II: VLM Ablations. Evaluation on 1000 hold-out BRIDGE dataset samples for paths and masks from our data labeling pipeline compared across VLM model size (3B and 13B parameters), prediction target (path p, mask m), and training datasets (OXE, BRIDGE, RoboPoint). The top half of the table ablates the model and prediction target, the bottom half ablates the training dataset. and including RoboPoint VQA/Pointing data overall helps performance on the BRIDGE evaluation dataset over just using BRIDGE alone. E. Full Results Tables We display full results tables for the Franka Sim-to-Real experiments in Table III and for the BRIDGE experiments in Table IV."
        },
        {
            "title": "Basic Tasks",
            "content": "3DDA 3DDA+HAMSTER 3DDA+ARRO 3DDA+PEEK"
        },
        {
            "title": "Put the red cube on the blue cube\nPut the blue cube on the red cube\nPut the red cube on the blue cube\nPut the blue cube on the red cube\nAverage",
            "content": "Vis & Obj Clutter"
        },
        {
            "title": "Knock over the syrup bottle\nPut the basketball in the bowl\nPut the blue cube next to the healthy items\nAverage",
            "content": "0.50 0.50 0.50 0.50 0.50 0.00 0.00 0.00 0.00 0.00 0.20 0.00 0.00 0.04 0.95 1.00 0.40 0.80 0.82 0.00 0.00 0.00 0.00 0.00 0.20 0.7 0.13 0. 0.25 0.20 0.80 0.35 0.45 0.10 0.00 0.50 0.20 0.20 0.50 0.00 0.26 0.20 1.00 0.85 0.70 0.80 0.83 1.00 0.70 0.60 0.80 0.77 0.80 0.60 0.80 0."
        },
        {
            "title": "Basic Tasks",
            "content": "3DDA 3DDA+HAMSTER 3DDA+ARRO 3DDA+PEEK (a) Partial completion rates per task."
        },
        {
            "title": "Put the red cube on the blue cube\nPut the blue cube on the red cube\nPut the red cube on the blue cube\nPut the blue cube on the red cube\nAverage",
            "content": "Vis & Obj Clutter"
        },
        {
            "title": "Knock over the syrup bottle\nPut the basketball in the bowl\nPut the blue cube next to the healthy items\nAverage",
            "content": "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.20 0.00 0.00 0.06 0.80 1.00 0.40 0.40 0.65 0.00 0.00 0.00 0.00 0.00 0.20 0.60 0.00 0. 0.20 0.20 0.20 0.20 0.15 0.10 0.00 0.50 0.20 0.20 0.00 0.00 0.00 0.00 1.00 0.60 0.40 0.60 0.65 1.00 0.70 0.60 0.80 0.77 0.60 0.60 0.80 0. TABLE III: Franka Sim-to-Real Results Table. All task success/completion rates for each baseline are averaged over 5 trials. (b) Success rates per task."
        },
        {
            "title": "ACT",
            "content": "ACT+ARRO ACT+PEEK"
        },
        {
            "title": "Slide the pot to shrimp\nPush the button\nPut carrot in drawer\nAverage",
            "content": "Obj & Vis Clutter"
        },
        {
            "title": "Put pepper in box\nGive banana to Jensen\nPut food on plate\nAverage",
            "content": "0.20 0.70 0.15 0.35 0.20 0.60 0.00 0.27 0.00 0.10 0.10 0.07 0.70 0.30 0.00 0.33 0.10 0.00 0.00 0.03 0.00 0.00 0.00 0. 0.40 0.40 0.20 0.33 0.30 0.40 0.30 0.33 0.10 0.15 0.00 0.08 0.70 0.80 0.45 0.65 0.50 0.70 0.55 0.58 0.65 0.15 0.10 0."
        },
        {
            "title": "ACT",
            "content": "ACT+ARRO ACT+PEEK (a) Partial completion rates per task."
        },
        {
            "title": "Slide the pot to shrimp\nPush the button\nPut carrot in drawer\nAverage",
            "content": "Obj & Vis Clutter"
        },
        {
            "title": "Put pepper in box\nGive banana to Jensen\nPut food on plate\nAverage",
            "content": "0.00 0.60 0.00 0.20 0.00 0.40 0.00 0.13 0.00 0.00 0.00 0.00 0.40 0.20 0.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.40 0.00 0.13 0.00 0.20 0.00 0.07 0.00 0.00 0.00 0.00 0.40 0.60 0.20 0.40 0.40 0.40 0.00 0.27 0.20 0.00 0.00 0. π0 0.50 0.50 0.40 0.33 0.40 0.40 0.20 0.33 0.50 0.20 0.20 0.30 π0 0.20 0.40 0.40 0. 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.07 π0+ARRO π0+PEEK 0.50 0.60 0.60 0.57 0.50 0.10 0.65 0. 0.05 0.20 0.00 0.08 1.00 0.50 0.80 0.77 0.70 0.80 0.70 0.73 0.60 0.70 0.50 0.60 π0+ARRO π0+PEEK 0.40 0.40 0.20 0.33 0.20 0.00 0.40 0.20 0.00 0.20 0.00 0.07 1.00 0.40 0.60 0.67 0.40 0.60 0.20 0.40 0.20 0.60 0.40 0. TABLE IV: WidowX BRIDGE Results Table. All task success/completion rates for each baseline are averaged over 5 trials. (b) Success rates per task."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "NVIDIA",
        "University of Southern California",
        "University of Washington"
    ]
}