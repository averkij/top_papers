{
    "paper_title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models",
    "authors": [
        "Mihai Nadas",
        "Laura Diosan",
        "Andrei Piscoran",
        "Andreea Tomescu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space. A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables. We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 0 6 0 2 . 4 0 5 2 : r TF1-EN-3M: THREE MILLION SYNTHETIC MORAL FABLES FOR TRAINING SMALL, OPEN LANGUAGE MODELS Mihai Nadas, Babes, -Bolyai University mihai.nadas@ubbcluj.ro Laura Dios, an Babes, -Bolyai University laura.diosan@ubbcluj.ro Andreea Tomescu KlusAI Labs andreea.tomescu@klusai.com Andrei Pis, coran KlusAI Labs andrei.piscoran@klusai.com April 30,"
        },
        {
            "title": "ABSTRACT",
            "content": "Moral stories are time-tested vehicle for transmitting values, yet modern NLP lacks large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3Mthe first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8 parameters. Each story follows six-slot scaffold (character trait setting conflict resolution moral), produced through combinatorial prompt engine that guarantees genre fidelity while covering broad thematic space. hybrid evaluation pipeline blends (i) GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8 B-parameter Llama-3 variant delivers the best qualityspeed trade-off, producing high-scoring fables on single consumer GPU (<24 GB VRAM) at $0.135 per 1000 fables. We release the dataset, generation code, evaluation scripts, and full metadata under permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AIdemonstrating that large-scale moral storytelling no longer requires proprietary giant models."
        },
        {
            "title": "Introduction",
            "content": "Stories that impart moral lessons fables have long served as compelling medium for teaching values and social norms. As Guan et al. [1] note, \"Teaching morals is one of the most important purposes of storytelling\". Traditionally, fables feature anthropomorphic characters and conclude with explicit morals that connect concrete events to abstract ethical principles. However, classical collections, such as Aesops Fables [2], are limited in size, which restricts data-driven approaches to modeling moral storytelling. In recent years, advances in large language models (LLMs) have unlocked new possibilities for synthetic data generation in natural language processing. Researchers have increasingly explored LLMs as cost-effective alternative to manual annotation, generating high-quality datasets that span various domains from classification to dialogue [3]. Projects like TinyStories [4] have demonstrated that even models with fewer than 10M parameters can learn to produce coherent narratives when trained on carefully curated synthetic data [5]. This progress underscores the potential for LLM-generated corpora to foster development in the realm of open model research, especially in low-data regimes. recent comprehensive survey further contextualizes these advances across both text and code domains, highlighting key methods such as prompt-based generation, retrieval-augmented pipelines, and reinforcement-driven refinement [6]. PREPRINT - APRIL 30, 2025 1.1 Research Questions Motivated by (i) the absence of any large-scale, structured fable corpus for data-driven moral storytelling and (ii) the need to understand whether compact, open-weight models can reliably generate such narratives, we investigate: RQ1: How effective is combinatorial prompt expansion methodology in generating diverse and high-quality fables using LLMs? RQ2: Which open-weight LLMs are best suited for fable generation under resource constraints? Motivated by these questions and emerging trends, we set out to construct large-scale dataset of fables using modern open-source LLMs. Our objective is to capture narratives with clear pedagogical intent and consistent structure, suitable for applications in story generation, moral reasoning, and instruction following. Key challenges include: (a) systematically covering wide diversity of scenarios via prompt design, and (b) ensuring the quality and consistency of generated stories while relying on resource-limited models that run on consumer-grade hardware rather than on expensive API-based systems. Our approach employs structured prompt template that encodes classic fable elementsprotagonist, character trait, setting, conflict, resolution, and moral. By combinatorially expanding lists for each element, we generate millions of unique prompts that span an expansive range of moral narratives. To determine the most suitable model for full-scale generation, we conduct an extensive evaluation of candidate models (including variants from Llama-3, Mistral, DeepSeek, and others). Evaluation proceeds along two complementary axes: 1. qualitative LLM-based critic, for which we use GPT-o3-mini as one illustrative choice to assess grammar, creativity, moral clarity, and prompt adherencemirroring recent work that demonstrates strong correlation between LLM-based and human ratings [7, 8]. 2. set of reference-free diversity and readability metrics, including Self-BLEU [9], Distinct-n [10], and the Flesch Reading Ease score [11], which provide an independent signal of lexical variety, textual novelty, and accessibility. This multi-perspective assessment ultimately leads us to choose Llama-3.1-8B-Instruct as the model for generating the TF1-EN-3M dataset. While other models achieved marginally higher scores in some dimensions, Llama-3.1-8B consistently balanced narrative quality with stylistic simplicity, producing fables best aligned with the 47 age group, as the naturally limited lexical range of this audience makes the corpus especially conducive to parameter-efficient fine-tuning of small downstream models. 1.2 Contributions Our work makes the following key contributions: 1. TF1-EN-3M Dataset: We release the first massive dataset of synthetic fables in English (3,000,000 stories), providing novel resource for training and evaluating models on story generation and moral reasoning. 2. Efficient High-Quality Generation: We demonstrate that high-quality, instructive content can be generated at scale using relatively small open models (18B parameters) that are deployable on consumer-grade hardware. This supports broader access to instructional NLP and fosters cost-effective dataset curation [4]. 3. Methodological Innovation: We propose layered evaluation framework combining LLM-based scoring and lexical diversity metrics enabling multi-criteria model selection and laying the groundwork for fable generators. The TF1-EN-3M dataset is published on Hugging Face Hub under the identifier klusai/ds-tf1-en-3m [12], and the full code to regenerate and evaluate it is available in the TinyFabulist repository. The remainder of this paper is organized as follows. In Section 2 we formalize our template-driven prompt schema and the large-scale dataset generation pipeline. Section 3 presents our hybrid LLM-based evaluation framework, compares the performance of ten open-weight models, and situates our approach within the broader related work. In Section 4 we describe the TF1-EN-3M datasetits structure, metadata schema, generation costs, and public release and in Section 5 we reflect on our results in light of the original research questions, explore practical applications, and outline key threats to validity. Finally, Section 6 summarizes our contributions and suggests directions for future work. 2 PREPRINT - APRIL 30,"
        },
        {
            "title": "2 Prompt design and dataset generation",
            "content": "We first define structured prompt schema informed by narrative theory and prior template-based generation work [13, 14], then construct controlled value space from which to sample inputs. Finally, we specify sampling strategies and length constraints to balance diversity, coherence, and computational tractability. 2.1 Prompt Schema and Value Space Our goal is to generate texts that are diverse, coherent, and tailored to the fable form. To achieve consistency in structure while covering broad range of content, we employ template-driven prompt design, which has shown effectiveness in guiding language models towards structured and purposeful text generation [13, 14]. Specifically, we designed our prompt templates around six key fable elements identified through analysis of traditional narrative structures: Character: The protagonist, frequently depicted as an animal or human archetype (e.g., fox, woodcutter). Trait: notable attribute that shapes character behavior and story outcomes (e.g., greedy, kind, lazy). Setting: Contextual locations that ground narratives within recognizable environments (e.g., in dense forest, on farm, in bustling town). Conflict: Challenges or dilemmas central to narrative tension (e.g., loses their food to someones trick, must choose between truth and lies). Resolution: Methods by which conflicts resolve, often demonstrating moral or ethical judgments (e.g., the character learns sharing, the trickster is exposed). Moral: Explicit lessons underscoring narrative intent (e.g., \"Dont cry wolf unless you mean it\", \"Honesty is the best policy\"). Prompt Construction: We developed prompt template incorporating these elements into structured narrative prompt. The general form reads: Create fable based on the following elements. Weave them naturally into story: Main Character: [Trait] [Character] Setting: [Setting] Challenge: [Conflict] Outcome: [Resolution] Teaching: [Moral] This explicit structuring ensures that generated stories follow conventional narrative patterns, providing clear beginnings, middles, and ends, alongside an explicit moral lesson, thus directly addressing the documented challenge of aligning generated stories with specified outcomes [15, 16]. Beyond slot-filling, we augment our prompts with stylistic constraints to promote narrative quality and adherence to the fable genre. The template includes explicit instructions for how the model should realize each element within the story: to begin with vivid scene-setting, avoid naming characters (instead referring to their role and trait), use meaningful dialogue, show the characters growth implicitly (rather than stating it), and conclude with clear, moral-driven resolution. These stylistic cues serve as soft controls, guiding the language model toward coherent and pedagogically meaningful outputs while maintaining creative freedom. This approach draws on narrative writing principles such as show, dont tell and mimics the tone and form of traditional fables, which often rely on concise, evocative storytelling to convey timeless ethical lessons. Similar strategies have been shown to improve narrative coherence and genre alignment in neural story generation [15, 16], and are consistent with recent work on instruction-based controllability in open-ended generation tasks [17, 14]. System Message Design: In addition to the structured narrative template, we provided each language model with dedicated system message that framed the task as moral fable writing. This instruction established expectations for output quality, genre-specific constraints, and age-appropriate narrative style. The system message emphasized three key principles: Imaginative and coherent storytelling, Audience awareness, including suitability for young readers, 3 PREPRINT - APRIL 30, 2025 Figure 1: Full pipeline for generating TF1-EN-3M. Adherence to the classic fable format, composed of six core elements: character, trait, setting, conflict, resolution, and moral. It also defined five distinct age groups (AE) used later during evaluation to assess target audience alignment. These audience categories helped our LLM-based critic assign each story to an appropriate demographic bracket. The system message complements the structured prompt by establishing tone, ethical clarity, and pedagogical intent critical elements in moral storytelling. It acts as global instruction layer, ensuring genre fidelity across millions of independently sampled and generated fables. Value Space and Prompt Design Variables We curated structured value sets for each slot in the prompt template, forming controlled input space for synthetic generation. These sets were derived from combination of classical fables, common moral themes, and creative extensions, ensuring both cultural relevance and narrative diversity, and were manually populated by our research team. We abstract the input space by introducing six formal parameters: n, m, k, c, r, where = # of Character options, = # of Trait options, = # of Setting options, = # of Conflict options, = # of Resolution options, 4 PREPRINT - APRIL 30, 2025 = # of Moral options."
        },
        {
            "title": "The total combinatorial value space is thus",
            "content": "By parametrizing in this way, we cleanly separate the schema definition from the concrete instantiation. Concrete parameter values are specified in Section 3. = l. 2.2 Full System Message and Prompt Template Below we present the exact system message and prompt template that drive the generator. System Message You are world-class creative assistant that generates captivating and morally-driven fables based on structured inputs. Each fable must be: - Imaginative and coherent. - Appropriate for wide audience, including young readers. - Structured around classic fable format (character, setting, conflict, resolution, and moral). Age groups are defined as: - A: 3 years or under - B: 4-7 years - C: 8-11 years - D: 12-15 years - E: 16 years or above Prompt Template Create fable based on the following elements. Weave them naturally into story: - Main Character: {{trait}} {{character}} - Setting: {{setting}} where our story unfolds - Challenge: {{conflict}} - Outcome: {{resolution}} - Teaching: {{moral}} The fable should: - Be appropriate for age group (4-7 years) - Use simple vocabulary that 4-7 year olds can understand - Use concrete rather than abstract language - Begin with vivid scene-setting - Not use names for the characters, instead use the trait and character - Include meaningful but simple dialogue - Show (don't tell) the character's growth - End with clear connection to the moral Keep the story concise but engaging, around 250 words. 2.3 Evaluation Methodology Evaluating open-ended narrative generation is notoriously challenging. Recent studies have explored the use of large language models (LLMs) as learned evaluators, prompting them with rubric-driven instructions to produce multi-axis scores that better align with human preferences [18, 7]. LLM-based Critic Building on the G-Eval framework [7] and similar work in summarization and dialogue evaluation [18], we prompt an instruction-tuned LLM to act as literary critic. The model receives explicit scoring criteria (and, when useful, chain-of-thought guidance) and assigns each fable score from 1 to 10 along four dimensions: Grammar & Style: linguistic correctness and syntactic fluency, Creativity: narrative originality and inventiveness, 5 PREPRINT - APRIL 30, 2025 Moral Clarity: explicitness and relevance of the ethical lesson, Prompt Adherence: fidelity to the templates structural and stylistic constraints. This approach leverages the LLMs deep contextual understanding to approximate human-like judgments at scale, while remaining more efficient than deploying top-tier evaluators for millions of examples. Age Group Classification To ensure our fables are appropriate for different developmental stages, we also prompt the same LLM to classify each story into one of five age bracketsA (03 yrs), (47 yrs), (811 yrs), (1215 yrs), (16+ yrs)based on vocabulary simplicity, thematic maturity, and syntactic complexity. This mirrors methodologies in microfiction readability research [4] and helps us verify that generated content aligns with intended educational use. Reference-Free Metrics Complementing these subjective evaluations, we compute three corpus-level, unreferenced statistics: Self-BLEU [9]: quantifies intra-set redundancy by evaluating each story against the rest (lower values indicate greater diversity), Distinct-n [10]: measures the proportion of unique n-grams (we report = 1) as proxy for lexical richness, Flesch Reading Ease [11]: assesses overall readability via sentence and syllable counts, critical for ageappropriate comprehension. By combining LLM-based critique, age-level classification, and automated diversity/readability metrics, our hybrid evaluation pipeline offers robust, multi-faceted assessment of fable qualityaddressing the known limitations of purely reference-based methods and reflecting best practices for generative evaluation [8]."
        },
        {
            "title": "3 LLM Evaluation and Comparison with Related Work",
            "content": "3.1 Experimental Setup To explore how well our synthetic fables hold up under scrutiny, we first generated up to MAX = 3,000,000 unique prompts from the full combinatorial space = l, with each of the six parameters n, m, k, c, r, set to 100. We sampled uniformly at random, enforcing three gentle safeguards to preserve diversity and balance: Uniquenesswe discarded any duplicate prompts; Frequency filteringwe downsampled overly common conflictmoral pairings; Coverage balancingwe made sure each slot appeared roughly the same number of times. Each fable was capped at max 1000 tokens, length that aligns well with traditional fable structure [19]. All used models were decoded using consistent setuptemperature set to 0.7, with default top-p (i.e., 1.0), and greedy decoding disabled as result of these parametersto ensure fair apples-to-apples comparison [20]. 3.2 Models Considered We evaluated ten instruction-tuned, open-weight LLMs deployable on consumer GPUs (<24 GB VRAM): 1. SmolLM2-1.7B-Instruct [21] 2. Aya-23-8B [22] 3. Llama-3.2-1B-Instruct [23] 4. Llama-3.1-8B-Instruct [24] 5. Llama-3.1-Tulu-3-8B [24] 6. Mistral-7B-Instruct-v0.3 [25] 7. Qwen2.5-7B-Instruct [26] 8. deepseek-llm-7b-chat [27] 6 PREPRINT - APRIL 30, 2025 9. Phi-3-mini-4k-instruct [28] 10. Falcon3-7B-Instruct [29] Each model generated 100 fables per prompt under identical decoding hyperparameters. 3.3 Results and Interpretation LLM-based Evaluation Table 1 presents head-to-head comparison of our models across Grammar, Creativity, Moral Clarity, and Prompt Adherence, along with the aggregate mean, token counts, and inference times. Table 1: LLM-based evaluation of fable generation (110 scale), plus generation metadata. Highest values in the first five metric columns (GrammarMean) are bolded; lowest latency is bolded. Model Aya-23-8B SmolLM2-1.7B-Instruct Qwen2.5-7B-Instruct Llama-3.1-Tulu-3-8B deepseek-llm-7b-chat Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Phi-3-mini-4k-instruct Mistral-7B-Instruct-v0.3 Falcon3-7B-Instruct Grammar Creativity Moral Clarity Adherence Mean 6.47 6.25 7.33 7.87 6.93 7.85 6.21 7.21 7.26 7.54 5.75 5.40 6.21 6.97 6.08 6.59 5.41 6.28 6.31 6.56 7.78 7.79 8.28 8.32 8.04 8.42 7.87 8.10 8.12 8.29 5.12 4.81 6.81 7.69 5.72 8.18 4.98 6.61 6.58 7.06 7.24 6.98 8.02 8.50 7.88 8.21 6.56 7.87 8.05 8.27 Input Tokens Output Tokens Latency (s) 500.6 414.7 404.2 368.5 439.8 337.6 358.5 0.0 426.4 400. 257.89 17.58 17.72 16.91 82.36 28.87 16.69 40.76 40.70 20.72 171.8 174.3 182.6 181.5 189.3 181.5 181.5 0.0 201.1 186.2 Results indicate that Llama-3.1-Tulu-3-8B takes the top spot with the highest overall mean of 7.87, driven by leading scores in Creativity (6.97) and Moral Clarity (8.50). Close behind, Llama-3.1-8B-Instruct records the strongest performance in Grammar (8.42) and Adherence (8.18), underlining its strong alignment with our structured prompts. Falcon3-7B-Instruct balances solid performance (mean 7.54) with high Moral Clarity (8.27) and moderate latency. Other mid-sized models such as Mistral-7B-Instruct-v0.3 and Phi-3-mini-4k-instruct also deliver respectable mean scores (7.26 and 7.21, respectively). At the efficiency end of the spectrum, Llama-3.2-1B-Instruct achieves the fastest inference time (16.69 s) while maintaining reasonable quality, demonstrating its suitability for latency-sensitive or resource-constrained deployments. Reference-Free Metrics Table 2 brings three corpus-level measures into view: Self-BLEU for internal diversity, Distinct-1 for lexical richness, and Flesch Reading Ease for readability. Table 2: Non-LLM text-quality metrics for all evaluated models (lower Self-BLEU indicates greater diversity; higher Distinct-1 indicates richer vocabulary; higher Flesch Reading Ease indicates greater readability). Lowest Self-BLEU and highest Distinct-1 and Flesch scores are bolded. Model Aya-23-8B SmolLM2-1.7B-Instruct Qwen2.5-7B-Instruct LLaMA-3.1-Tulu-3-8B deepseek-llm-7b-chat LLaMA-3.1-8B-Instruct LLaMA-3.2-1B-Instruct Phi-3-mini-4k-instruct Mistral-7B-Instruct-v0.3 Falcon3-7B-Instruct Self-BLEU Distinct-1 0.361 0.364 0.390 0.333 0.355 0.351 0.398 0.318 0.360 0.369 0.608 0.567 0.602 0.659 0.586 0.604 0.635 0.651 0.634 0.661 Flesch Reading Ease 73.868 72.808 80.846 74.205 70.731 80.071 80.832 77.912 73.974 74.379 Based on these metrics we observe that Llama-3.1-8B-Instruct demonstrates an attractive balance across Self-BLEU, Distinct-1, and Flesch Reading Ease, making its results both varied and accessible. Age Group Classification Finally, table 3 shows how often each models fables were judged suitable for each age bracket (AE). We noted that Llama-3.1-8B-Instruct and its Tulu variant generated the highest share of (47 years) stories, aligning with our target audience for moral and linguistic simplicity [4]. 7 Table 3: Age-group distribution of generated fables per model (percentages), as estimated by LLM-based classification. Highest Age percentage is bolded. PREPRINT - APRIL 30, 2025 Model CohereForAI/aya-23-8B HuggingFaceTB/SmolLM2-1.7B-Instruct Qwen/Qwen2.5-7B-Instruct allenai/Llama-3.1-Tulu-3-8B deepseek-llm-7b-chat meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-3.2-1B-Instruct microsoft/Phi-3-mini-4k-instruct mistralai/Mistral-7B-Instruct-v0.3 tiiuae/Falcon3-7B-Instruct Age Age Age Age Age 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 34.0% 66.0% 0.0% 47.0% 53.0% 0.0% 90.0% 10.0% 0.0% 71.0% 29.0% 0.0% 39.0% 61.0% 0.0% 92.0% 8.0% 0.0% 67.0% 33.0% 0.0% 62.0% 38.0% 0.0% 47.0% 53.0% 0.0% 76.0% 24.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% Model Selection Criterion To integrate both LLM-based and corpus-level metrics into single ranking, we define composite score Wm for each model over seven axes: Grammar, Creativity, Moral Clarity, Adherence, Self-BLEU, Distinct-1, and Flesch Reading Ease. We assign the greatest weight to Adherence, then Moral Clarity, and distribute the remaining weight equally among the other five metrics. Specifically, let wAdh = 0.35, wGra = wMor = 0.20, wCre = 0.10, wSB = wD1 = wFRE = 0.15 3 = 0.05. We first normalize each raw score Sm,k to Sm,k [0, 1] by inverting the normalization for Self-BLEU so that lower is better. Then Sm,k = Sm,k minm Sm,k maxm Sm,k minm Sm,k , Wm = wGra Sm,Grammar + wCre Sm,Creativity + wMor Sm,MoralClarity + wAdh Sm,Adherence + wSB Sm,SelfBLEU + wD1 Sm,Distinct1 + wFRE Sm,FleschEase. By construction 0 Wm 1. Table 4 reports the resulting weighted scores for all ten models. As shown, LLaMA3.1-8B-Instruct achieves the highest composite score of 0.891, confirming it as the optimal choice for TF1-EN-3M generation. Table 4: Composite weighted scores Wm for each model, computed with wAdh = 0.35, wGra = wMor = 0.20, wCre = 0.10, wSB = wD1 = wFRE = 0.05. Model LLaMA-3.1-8B-Instruct LLaMA-3.1-Tulu-3-8B Falcon3-7B-Instruct Qwen2.5-7B-Instruct Phi-3-mini-4k-instruct Mistral-7B-Instruct-v0.3 deepseek-llm-7b-chat Aya-23-8B LLaMA-3.2-1B-Instruct SmolLM2-1.7B-Instruct Grammar Creativity Moral Clarity Adherence Self-BLEU Distinct8.42 8.32 8.29 8.28 8.10 8.12 8.04 7.78 7.87 7.79 6.59 6.97 6.56 6.21 6.28 6.31 6.08 5.75 5.41 5.40 8.21 8.50 8.27 8.02 7.87 8.05 7.88 7.24 6.56 6.98 8.18 7.69 7.06 6.81 6.61 6.58 5.72 5.12 4.98 4.81 0.351 0.333 0.369 0.390 0.318 0.360 0.355 0.361 0.398 0.364 0.604 0.660 0.661 0.602 0.651 0.634 0.586 0.608 0.635 0. FRE 80.071 74.205 74.379 80.846 77.912 73.974 70.731 73.868 80.832 72.808 0.891 0.874 0.729 0.640 0.608 0.576 0.392 0.185 0.132 0.078 3.4 Comparison with Related Work Our approach sits at the intersection of several active research threads in synthetic data creation, narrative generation, and evaluation methodologies. First, the use of LLMs to generate large-scale synthetic corpora has gained momentum as remedy for data scarcity. Early investigations demonstrated mixed results for subjective tasks [30], but recent LLM-driven pipelines have shown dramatic scale-ups: for example, the Persona Hub uses billions of persona templates to produce instruction-style PREPRINT - APRIL 30, 2025 data at unprecedented volume [31], while self-instruct methods leverage GPT-4 to bootstrap fine-tuning corpora for smaller models [32, 5]. Our work follows this paradigm but focuses specifically on moral fables, using combinatorial slot-filling rather than persona personas or QA prompts to drive content diversity. Second, in the realm of story and fable generation, classic systems relied on heuristic planners or fixed plot templates to enforce narrative structure [33]. More recently, projects like TinyStories demonstrated that compact LLMs (GPT-3.5/4) can produce tens of millions of childrens tales with controlled vocabulary, and even evaluate them via LLM-based teacher models [4]. We extend this work by targeting the moral-fable genreeach story centered on an explicit ethical lessonand by providing an even richer metadata schema to track generation conditions and costs at scale. Third, bilingual corpora such as STORAL have assembled human-authored moral tales to study machine comprehension of implied lessons [1]. STORAL highlighted that off-the-shelf models struggle to align plot events with morals, motivating the need for specialized training data. TF1-EN-3M complements STORAL by offering synthetic, massively parallel corpus of moral fables, suitable both for fine-tuning narrative generators and for probing moral-reasoning capabilities in large-scale evaluations. Finally, evaluating open-ended narratives with traditional metrics (BLEU, ROUGE, METEOR) often fails to capture creativity and thematic coherence, penalizing legitimate lexical variation [34]. Frameworks like G-Eval have shown that GPT-4, when prompted with explicit rubrics and chain-of-thought, can produce human-aligned scores across multiple axes [7, 18]. Our hybrid evaluation pipelinecombining an LLM-based critic with reference-free metrics (Self-BLEU, Distinct-n, Flesch Ease) and age-group classificationbuilds on these insights, delivering robust, multidimensional assessment tailored to the pedagogical and structural demands of fable generation [8]. In weaving together these strandssynthetic data scaling, genrespecific narrative generation, and hybrid evaluation we present unified methodology for creating and rigorously assessing moral fables at unprecedented scale."
        },
        {
            "title": "4 TF1-EN-3M Dataset Description and Availability",
            "content": "As introduced above, the TF1-EN-3M Dataset comprises 3,000,000 English-language fables, each systematically generated via structured prompts and annotated with relevant metadata. Unlike many text corpora that provide only raw system inputs or final outputs, TF1-EN-3M stores detailed records in JSON lines format, enabling transparent inspection of both prompt specifics and generation context. By incorporating both metadata and narratives, TF1-EN-3M follows similar design ethos to other corpora that emphasize replicability and moral coherence [1, 30]. Each record contains fields grouped into two major categories: (1) Fable Content language: The language of the fable (currently en). prompt: string that describes the thematic elements (character, setting, conflict, resolution, moral) and stylistic constraints (e.g., word count, avoidance of character names). This prompt is provided to the model as input. fable: The complete story generated by the model, typically 13 paragraphs and ending with an explicit moral (e.g., Honesty is the best policy.). hash: SHA-256 (Secure Hash Algorithm 256-bit) cryptographic hash identifier is generated for each prompt, serving as means of ensuring integrity and enabling fallback mechanism. (2) Generation Metadata llm_name: The identifier of the model used for generation (e.g., tiiuae/Falcon3-7B-Instruct). llm_input_tokens, llm_output_tokens: Token counts for the prompt and the generated fable, useful for analyzing model efficiency and verbosity [35]. llm_inference_time: Elapsed time (in seconds) required for the model to generate its output, allowing latency and throughput analysis. host_provider, host_dc_provider, host_dc_location: Information about the inference infrastructure, including the service provider and data center location (e.g., Hugging Face Inference Endpoints in eu-west-1). host_gpu, host_gpu_vram, host_cost_per_hour: Details about the GPU hardware (type and VRAM) and the hourly cost of generation, facilitating reproducibility and cost benchmarking. generation_datetime: timestamp indicating when the story was generated. 9 PREPRINT - APRIL 30, pipeline_version: The internal version of the data generation pipeline, used for traceability and reproducibility across dataset updates. Generation costs: The total cost of generating all 3,000,000 fables was USD $405.76, corresponding to approximately USD $0.1353 per 1,000 fables [36]. This unified schema guarantees that each sample contains both the narrative essence (fable) and the metadata required to replicate or analyze its generation. The datasets prompt engineering design enforces consistent elementsmain character, trait, setting, conflict, resolution, and moralensuring that stories adhere to classic fable structure while maintaining rich thematic diversity. The structured nature of TF1-EN-3M thus offers direct insights into computational costs, model behaviors, and textual outputs at scale. We designed our generation metadata schema following the principles of Datasheets for Datasets [37], ensuring that each sample carries all the provenance, configuration, and evaluation details needed for reproducibility and ethical auditing. The dataset totals on the order of 1B tokens of model-generated text. Despite being synthetic, the stories are quite varied. We performed basic analyses: Diversity: The frequency of each character and moral is roughly uniform given our prompt design. No single template dominates. This intentional spread contrasts with many human story datasets that might have repetition of popular tales. Quality distribution: Using the same GPT-o3-mini critic, we spot-checked random samples of the final dataset. The average quality remained high (since its all produced by the chosen model). Minor issues such as occasional archaic language or slightly forced morals were observed but are part of the charm of fable-like text and were not pervasive. Length: The average story length is 120 tokens (excluding the moral), with standard deviation of 30. We instructed the model for short fable, so none is overly long. This uniformity in length is useful for training purposes (less variance). Data Format: TF1-EN-3M is stored as Hugging Face Dataset (datasets library), with each entry containing the prompt fields and story text. We release it under an open license, given it is entirely machine-generated. Users can easily load it for model training or analysis. We emphasize that while the data is synthetic, the writing style is intended to be similar to human-written fables, and preliminary human readings found the stories to be sensible and enjoyable. Availability: The dataset is published on Hugging Face Hub under the identifier klusai/ds-tf1-en-3m [12]. Researchers and practitioners can download it in full or in parts (it is chunked for convenience). Additionally, we provide the TinyFabulist GitHub repository [38] which contains the code to regenerate the dataset, including: The prompt lists for each element (so one can modify or extend them). The generation script used (for the Hugging Face transformers or peft library, etc., with our model weights or references to them). The evaluation scripts (for GPT-o3-mini scoring and the translation test). Guidelines for how to reproduce the process or create multilingual version (e.g., swapping out the moral list for French translations to get French fable dataset, which is planned extension). We believe releasing these resources will enable full reproducibility and encourage others to build upon TinyFabulist. Potential uses of TF1-EN-3M include: fine-tuning smaller models to serve as fable generators or moral reasoning evaluators, using the stories to train classifiers or question-answering models on moral content, or even as creative corpus for literary studies in computational linguistics [1]."
        },
        {
            "title": "5 Discussion and threats to validity",
            "content": "The TF1-EN-3M Synthetic Fables Dataset opens up several avenues for further exploration. Here we discuss broader implications, methodological findings, and potential applications of our work, particularly in the context of the research questions posed in Section 1. Answering RQ1: Prompt Expansion and Diversity. Our first research question asked whether combinatorial prompt expansion methodology can effectively generate diverse and high-quality fables using LLMs. Across 3 million generated stories, we find that combining structured templates with uniform sampling from six controlled 10 PREPRINT - APRIL 30, 2025 input domains (character, trait, setting, conflict, resolution, moral) yields high narrative diversity without sacrificing coherence. Evaluations by GPT-o3-mini across grammar, creativity, moral clarity, and prompt adherence show that even small-to-medium-sized models can reliably produce instructive, well-structured moral stories when given sufficient prompt scaffolding. Furthermore, the flexibility of our template design ensures coverage of vast thematic space, while our filtering and balancing procedures help avoid mode collapse or over-representation of stereotypical scenarios. Answering RQ2: Best Performing Models. Our second research question focused on identifying the best-performing open-weight LLMs under resource constraints. Through controlled evaluation of ten publicly available instructiontuned models (ranging from 1B to 8B parameters), we found that several models including Falcon3-7B-Instruct [29], Llama-3.1-8B-instruct [24], and Mistral-7B-Instruct-v0.3 [25] consistently achieved the highest average scores across all four evaluation axes. Notably, these models outperformed even larger ones in some cases, suggesting that instruction tuning quality, not just scale, is key determinant of fable generation performance. The evaluation also highlighted favorable tradeoff: smaller models such as Phi-3-mini-4k-instruct [28] and SmolLM2-1.7B-Instruct [21] exhibited strong grammar and moral clarity with fast inference times, making them well-suited for low-latency applications and on-device deployment. Ultimately we picked Llama-3.1-8B-instruct as the overall best-performing model. Efficiency and Accessibility. By showing that story generation can be accomplished with relatively small models, our work emphasizes the value of efficiency in natural language generation [35]. Not every application or community can afford the computational resources necessary for deploying giant LLMs such as GPT-4 [39]. corpus like TF1-EN-3M can help bootstrap smaller models for creative writing tasks, thereby widening access. For instance, educators or indie game developers could fine-tune 6B or 1.3B parameter model on TF1-EN-3M to generate moral stories or quest narratives on modest hardwarean approach akin to TinyStories [4]. Moral and Educational AI Applications. Fables have long been employed to impart values and social norms, making them compelling vehicle for AI-driven educational tools [1]. tutoring system might present dynamically generated fable to student, followed by comprehension and reflection questions about the moral lesson. Because each TF1-EN-3M entry explicitly encodes moral, one could train models to map stories to morals or detect whether given narrative even contains moral lessonpotentially informing AI moderation or generative-checking systems. Limitations of Model-Generated Narratives. Despite the generally positive results, one must recognize limitations in synthetic fables. They often follow well-worn templates derived from the prompt structure, featuring talking animals and fairy-tale motifs [4]. This does not cover the complexity of contemporary ethical dilemmas. model trained solely on TF1-EN-3M might lack the sophistication to tackle nuanced or modern moral issues. Future work could expand TF1-EN-3M to include fables with ambiguous or multi-layered morals, enhancing the datasets utility in modeling complex ethical reasoning. Comparisons with Human-Written Data. Combining TF1-EN-3M with human-curated corpora may yield richer stylistic and thematic diversity [1]. Mixing synthetic fables with human-authored stories could balance the creativity of human prose against the consistency of model-driven generation. Additionally, models trained on TF1-EN-3M may be evaluated using benchmarks like the Story Cloze Test [40], providing insight into their ability to generate and comprehend coherent story endings with ethical implications. LLM-based Feedback Loops. In our pipeline, GPT-o3-mini served primarily as an offline evaluator and critic. Future systems may incorporate model-in-the-loop feedback loops, where LLMs dynamically revise or critique fables during generationpotentially improving both efficiency and quality. However, this would require additional computational overhead and careful control of criticgenerator interactions. Benchmark for Moral Story Understanding. We propose that TF1-EN-3M can serve as benchmark for evaluating moral reasoning in generative models. Tasks such as moral inference (predicting the correct moral given story) or moral generation (producing story that fits given moral) can be built from TF1-EN-3M, facilitating broader research into narrative alignment, commonsense reasoning, and pedagogical text generation. 5.1 Threats to Validity While the TF1-EN-3M dataset and accompanying methodology demonstrate promising results, several threats to validity should be acknowledged in evaluating the robustness and generalizability of our findings. 11 PREPRINT - APRIL 30, 2025 5.2 Construct Validity primary concern lies in the reliance on LLM-based evaluationsspecifically GPT-o3-minito assess properties such as moral clarity, creativity, and coherence. Although recent studies have shown that LLM-as-a-judge paradigms often align with human preferences in open-ended tasks [7, 8], such evaluations are still proxies and may not perfectly reflect human judgment, especially for nuanced narrative quality. Furthermore, the criteria used in our evaluation rubricwhile inspired by educational and literary standardsare operationalized numerically in ways that can obscure qualitative subtleties [41]. For example, evaluating fables \"moral clarity\" on scale of 110 may overlook ambiguous yet pedagogically valuable narratives. Triangulating LLM-based assessments with human evaluations or crowd-sourced annotations, as done in prior work [15], would provide stronger construct validity. 5.3 External Validity Our dataset is based on prompt elements and moral lessons drawn primarily from Western fable traditions (e.g., Aesop [2]), which risks introducing cultural bias into the generated stories. While the structured template allows for extensive combinatorial variation, the resulting narratives may still reflect an implicit Western ethical framework that limits generalizability across cultural contexts. This issue is particularly salient in moral storytelling, where values can vary widely [42]. Future work should consider incorporating moral principles from diverse philosophical and religious traditions, as well as adapting prompts for multilingual or culturally localized variants of the TF1 framework. 5.4 Conclusion Validity Our conclusions about model performance and dataset quality are primarily derived from single LLM-based evaluator GPT-o3-mini. While this model was selected for its balance of accessibility and reasoning capacity, relying on single critic introduces the risk of model bias in scoring. Prior work has shown that different LLMs often yield significantly different preferences or evaluations when acting as judges [43]. Moreover, the ten models included in our study were trained under heterogeneous setupsvarying in pre-training corpora, parameter scales, fine-tuning objectives, and underlying architectureswhich inherently affects the quality and style of their generated texts. These differences underscore why model outputs may diverge in coherence, creativity, moral clarity, and adherence. To mitigate these sources of bias and variance, we also employed complementary reference-free metricsSelf-BLEU [9], Distinct-n [10], and Flesch Reading Ease [11]capturing diversity, lexical richness, and readability without requiring human or LLM references. Incorporating multiple evaluators and diverse metrics enhances evaluation robustness and provides more nuanced, multi-faceted view of story quality across models and generations."
        },
        {
            "title": "6 Conclusion",
            "content": "We have introduced the TF1-EN-3M Synthetic Fables Dataset, large-scale collection of morally oriented short stories generated through instruction-tuned, compact, openly licensed language models. Our findings demonstrate that with focused prompt engineering and carefully curated generation pipelines, even mid-sized LLMsrather than hundred-billion-parameter behemothscan produce diverse, ethically themed narratives [4]. TF1-EN-3M marries techniques from synthetic data augmentation [30], story generation, and moral natural language processing (NLP)[1] to create novel resource for both training and evaluating models on narrative tasks that require moral consistency as well as linguistic fluency. Quantitative and qualitative evaluations indicate that these synthetic fables exhibit strong coherence and moral clarity. We anticipate that TF1-EN-3M will serve as platform for fine-tuning smaller models on fable generation, as well as investigating how language models learn and represent moral concepts. Going forward, key extensions might expand the breadth of morals and scenarios, explore architectures specialized for narrative creation, or incorporate human-in-the-loop feedback to enhance quality further. In the broader landscape of AI, this project aligns with the goal of developing systems that are culturally and ethically sensitive. By empowering smaller, more accessible models to generate value-laden stories, we edge closer to AI that is not only technologically efficient but also socially grounded. We invite the research community to utilize and build upon TF1-EN-3M, believing it will catalyze advances in low-resource story generation, ethical content creation, and the integration of moral reasoning into language modeling. 12 PREPRINT - APRIL 30,"
        },
        {
            "title": "References",
            "content": "[1] Jian Guan, Ziqi Liu, and Minlie Huang. Corpus for Understanding and Generating Moral Stories, April 2022. arXiv:2204.09438 [cs]. [2] Aesop. Aesops Fables. OUP Oxford, July 2002. Google-Books-ID: n2LlrCeYl7gC. [3] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: Survey, June 2024. arXiv:2406.15126 [cs]. [4] Ronen Eldan and Yuanzhi Li. TinyStories: How Small Can Language Models Be and Still Speak Coherent English?, May 2023. arXiv:2305.07759 [cs]. [5] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks Are All You Need, October 2023. arXiv:2306.11644 [cs]. [6] Mihai Nadas, Laura Diosan, and Andreea Tomescu. Synthetic Data Generation Using Large Language Models: Advances in Text and Code, March 2025. arXiv:2503.14023 [cs]. [7] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment, May 2023. arXiv:2303.16634 [cs]. [8] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as You Desire, February 2023. arXiv:2302.04166 [cs]. [9] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: Benchmarking Platform for Text Generation Models. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 18, pages 10971100, New York, NY, USA, June 2018. Association for Computing Machinery. [10] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. Diversity-Promoting Objective Function for Neural Conversation Models, June 2016. arXiv:1510.03055 [cs]. [11] J. P. Kincaid and And Others. Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel. Technical report, National Technical Information Service, Springfield, Virginia 22151 (AD-A006 655/5GA, MF $2, February 1975. ERIC Number: ED108134. [12] klusai/ds-tf1-en-3m Datasets at Hugging Face. [13] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What Makes Good In-Context Examples for GPT-$3$?, January 2021. arXiv:2101.06804 [cs]. [14] Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing Instructional Prompts to GPTks Language, March 2022. arXiv:2109.07830 [cs]. [15] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical Neural Story Generation, May 2018. arXiv:1805.04833 [cs]. [16] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-and-Write: Towards Better Automatic Storytelling. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):73787385, July 2019. Number: 01. [17] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1440914428, Toronto, Canada, July 2023. Association for Computational Linguistics. [18] Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization, October 2023. arXiv:2305.13091 [cs]. [19] Angela Naimou. Short Fiction, Flash Fiction, Microfiction. In Joshua Miller, editor, The Cambridge Companion to Twenty-First Century American Fiction, Cambridge Companions to Literature, pages 2142. Cambridge University Press, Cambridge, 2021. [20] Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. Trading Off Diversity and Quality in Natural Language Generation. ArXiv, April 2020. [21] HuggingFaceTB/SmolLM2-1.7B-Instruct Hugging Face, February 2025. [22] CohereForAI/aya-23-8B Hugging Face, March 2025. [23] meta-llama/Llama-3.2-1B-Instruct Hugging Face, December 2024. 13 PREPRINT - APRIL 30, 2025 [24] meta-llama/Llama-3.1-8B-Instruct Hugging Face, December 2024. [25] mistralai/Mistral-7B-Instruct-v0.3 Hugging Face. [26] Qwen/Qwen2.5-7B-Instruct Hugging Face, February 2025. [27] deepseek-ai/deepseek-llm-7b-chat Hugging Face, August 2024. [28] microsoft/Phi-3-mini-4k-instruct Hugging Face, January 2025. [29] tiiuae/Falcon3-7B-Instruct Hugging Face, February 2025. [30] Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming Yin. Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations, October 2023. arXiv:2310.07849 [cs]. [31] Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling Synthetic Data Creation with 1,000,000,000 Personas, September 2024. arXiv:2406.20094 [cs]. [32] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada, July 2023. Association for Computational Linguistics. [33] M. O. Riedl and R. M. Young. Narrative Planning: Balancing Plot and Character. Journal of Artificial Intelligence Research, 39:217268, September 2010. [34] Huyen Nguyen, Haihua Chen, Lavanya Pobbathi, and Junhua Ding. Comparative Study of Quality Evaluation Methods for Text Summarization, June 2024. arXiv:2407.00747 [cs]. [35] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language Models, January 2020. arXiv:2001.08361 [cs]. [36] Pricing. [37] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):8692, December 2021. [38] klusai/tinyfabulist, April 2025. original-date: 2025-01-07T20:20:42Z. [39] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris 14 PREPRINT - APRIL 30, 2025 Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C. J. Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 Technical Report, March 2024. arXiv:2303.08774 [cs]. [40] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839849, San Diego, California, June 2016. Association for Computational Linguistics. [41] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A. Raffel. Scaling Data-Constrained Language Models. Advances in Neural Information Processing Systems, 36:5035850376, December 2023. [42] Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences, December 2020. arXiv:2012.15738 [cs]. [43] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena, December 2023. arXiv:2306.05685 [cs]. [44] NVIDIA L40S GPU. [45] AI Chip - AWS Inferentia - AWS. [46] NVIDIA A100 GPUs Power the Modern Data Center. [47] NVIDIA L4 Tensor Core GPU."
        },
        {
            "title": "A Hardware and Environment Configurations",
            "content": "We benchmarked inference several GPU configurations using the TF1-EN-3M dataset under Llama-3.1-8B-Instruct with identical prompts and decoding settings. All experiments were executed on Hugging Face Inference Endpoints; the hourly tariffs advertised by Hugging Face in April 2025 were used to compute cost. Table 5 reports the wall-clock time and billable cost to generate fixed-size batch of fables, while Table 6 contextualises those results with architectural specifications drawn from vendor documentation. for Table 5: (Llama-3.1-8B-Instruct). Empirical inference duration and Hugging Face Endpoint cost for fixed prompt batch Hardware (HF instance) L40S A10G A100 L4 Timestamp Range 2025-04-12 22:15 22:30 2025-04-12 22:15 23:16 2025-04-12 22:15 22:28 2025-04-12 22:15 00:06 HF Rate (USD/h) Duration (min) Cost (USD) $1.80 $1.00 $4.00 $0.80 15.4 61.1 12.7 110.9 $0.46 $1.02 $0.85 $1. Methodology. The Inference Time column captures the interval between the earliest and latest timestamps in generation logs. Cost was computed as Rate time (s) 3600 . Hugging Face bills by the minute, rounding up to the next minute1. All jobs used Hugging Face Text Generation Inference (TGI) as the backend. 1See HF pricing page [36]. 15 PREPRINT - APRIL 30, 2025 Interpretation. Although the A100 delivers the fastest turnaround, its higher tariff narrows the price gap: the L40S achieves the best timecost trade-off for our batch size, consistent with NVIDIAs own positioning of the L40S for high-throughput GenAI inference [44]. Table 6: Specification comparison of AWS Inferentia 2 and representative NVIDIA GPUs. Hourly prices correspond to Hugging Face Inference Endpoint list rates (April 2025). Feature Type Release Year Architecture GPU Memory INT8 TFLOPS FP16 TFLOPS BF16 Support Inference-Optimised Power (W) Form Factor Cloud Availability Typical Use Cases HF Endpoint $/hr Inferentia 2 Custom AWS silicon 2023 NeuronCore-v2 N/A (SRAM) 400 100 Yes Yes 150 AWS only AWS only High-throughput inf. $1.20 A10G DC GPU 2021 Ampere GA102 24 GB GDDR6 312 124 No Moderate 150300 PCIe A100 DC GPU 2020 Ampere GA100 40/80 GB HBM2e 624/312 312 Yes Train & large inf. 400 SXM / PCIe AWS/GCP/Azure AWS/GCP/Azure Train & large inf. $4.00 Balanced inf. $1. L4 DC GPU 2023 Ada Lovelace 24 GB GDDR6 1 466/733 183 Yes Yes 72 PCIe GCP/Azure Real-time inf. $0.80 L40S WS/DC GPU 2023 Ada Lovelace 48 GB GDDR6 2 805/1 402 742 Yes Balanced 300350 PCIe T4 DC GPU 2018 Turing TU104 16 GB GDDR6 260 65 No Yes 70 PCIe AWS (road-map) AWS/GCP/Azure GenAI / visual $1.80 Cost-eff. inf. $0.60 Summary of Findings. Vendor data show that AWS Inferentia 2 is engineered for large-scale, low-latency inference [45], while NVIDIAs portfolio spans cost-oriented (T4), balanced (A10G), and flagship (A100) accelerators [46]. The NVIDIA L4 offers exceptional performance per watt for edge and latency-sensitive deployments [47], whereas the newer L40S targets high-end generative workloads with FP8 and fourth-generation Tensor Cores [44]. Combining these published capabilities with our empirical timings  (Table 5)  clarifies the costperformance envelope for producing millions of synthetic fables: the L40S yields the lowest cost per fable in our regime, but workloads demanding minimal latency may still justify the premium for the A100 or Inferentia 2."
        }
    ],
    "affiliations": [
        "Babes-Bolyai University",
        "KlusAI Labs"
    ]
}