{
    "paper_title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for Detailed Video Caption",
    "authors": [
        "Luozheng Qin",
        "Zhiyu Tan",
        "Mengping Yang",
        "Xiaomeng Yang",
        "Hao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results."
        },
        {
            "title": "Start",
            "content": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for Detailed Video Caption Luozheng Qin 1 Zhiyu Tan 1,2 Mengping Yang Xiaomeng Yang 1 Hao Li 1,2 1 Shanghai Academy of Artificial Intelligence for Science 2 Fudan University qinluozheng@sais.com.cn Project: https://sais-fuxi.github.io/projects/cockatiel 5 2 0 2 2 1 ] . [ 1 9 7 2 9 0 . 3 0 5 2 : r Figure 1. Cockatiel, three-stage training pipeline that ensembles synthetic and human-aligned training. Cockatiel-13B are capable to generate detailed captions that consistently aligns with every visual element in the input video (top left). Furthermore, Cockatiel-13B achieves new state-of-the-art and considerable dimension-balanced performance on VDCSCORE while consistently voted as the most human-aligned models compared to baselines (right). The key contributor to these capabilities is the ensembling synthetic and human preferenced training, which infuses Cockatiel-13B with diverse strengths of leading VDC models and human preferences (bottom left)."
        },
        {
            "title": "Abstract",
            "content": "Video Detailed Captioning (VDC) is crucial task for vision-language bridging, enabling fine-grained descripIn this paper, we first tions of complex video content. comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, novel threestage training pipeline that ensembles synthetic and humanaligned training for improving VDC performance. In the first stage, we derive scorer from meticulously annotated dataset to select synthetic captions highperforming on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in dimension-balanced way but also surpass leading alternatives on human preference by large margin as depicted by the human evaluation results. 1. Introduction Video Detailed Captioning (VDC) aims to generate comprehensive and detailed captions that capture the visual semantics and temporal dynamics of input videos. This capacity serves as fundamental ability for the powerful Multimodal Large Language Models (MLLMs) [21, 46, 50] and provides nuanced textual guidance for the emerging video diffusion models [30, 43, 49], facilitating various video-related understanding and generation tasks, such as video question answering [38, 45], video retrieval [14, 35], text-to-video generation [11, 24], text-guided video editing [2, 12], etc. In general, video detailed captioning is pivotal for the performance and applicability of video-based AI systems. Unfortunately, through our comprehensive benchmark of current VDC models on VDCSCORE [3], we observe that existing VDC models are hindered by imbalanced finegrained video-caption alignment and misalignment with human preference. Specifically, single VDC model usually prefers specific aspects of VDC rather than providing comprehensive captions, as it is challenging to find model that performs collectively competitive across the evaluated dimensions of VDCSCORE. Besides, since existing VDC models are mainly trained on purely synthetic detailed image/video captions [6, 16, 19], they are unable to align well with human preferences. Nevertheless, to mitigate these challenges, we can ensemble data generated by various VDC models to leverage their respective strengths and incorporate human preferences into the ensemble for improving the alignment with them. However, to ensure the effective integration of diverse model strengths and human preferences, it is crucial to accurately select and involve the most advantageous one for training from captions produced by various models. Besides, we have to prioritize captions preferred by humans while disregarding those are disapproved, thereby improving the alignment with human preference. Consequently, robust, human-aligned selection policy is critical for our method, as it determines that, at the instance level, which models generated captions should be involved in training based on dimension-specific performance and human preference. To achieve this, we employ an innovative selection policy powered by human-aligned caption quality scorer, which is essentially MLLM fine-tuned on meticulously annotated dataset of structured human preference score on VDC. Our selection policy not only selects the highest-scored caption among its candidates for each video, but also applies threshold setting to filter out cases where all candidates are low in quality. Then, we use this curated dataset to fine-tune VDC models, thereby obtaining competitive, dimension-balanced, and human-preferred VDC models. For the ease of user usage and deployment, We further distill Cockatiel-8B by utilizing Cockatiel-13B as an additional base model. Cockatiel significantly alleviates the aforementioned challenges, achieving new state-of-the-art and considerable dimension-balanced performance on VDCSCORE, as depicted in Fig. 1. Notably, to the best of our knowledge, this is the first work that focuses on the imbalanced detailed video-caption alignment and human preference misalignment in VDC, filling critical gap in the field. To summarize, our primary contributions are: By comprehensively benchmarking existing alternatives, we identify two critical challenges of VDC: the imbalanced detailed video-caption alignment and the misalignment with human preference. We propose Cockatiel, novel framework that ensembles synthetic and human preferenced training on VDC models to address the above challenges. We carefully annotate dataset with structured human preference scores on detailed video captions and train human-aligned scorer on it, facilitating high-quality data selection. Extensive experiments validate that our method sets new state-of-the-art performance on VDCSCORE while achieving better alignment with human preference. 2. Benchmarking Existing Alternatives 2.1. Preliminary VDCSCORE. VDCSCORE [3] evaluates the similarity between the predicted and ground-truth detailed captions. The evaluation process involves three steps. First, it decomposes the ground-truth caption into set of concise questionanswer pairs using LLM, then generates corresponding responses from the predicted caption. Finally, the LLM is used to assess the accuracy of each response to provide an overall score. VDCSCORE evaluates how the generated captions describe the objects, backgrounds, camera Model Camera (Acc / Score) Short (Acc / Score) Background (Acc / Score) Main Object (Acc / Score) Detailed (Acc / Score) Average (Acc / Score) ShareGPT4Video-8B [5] Vriptor [42] AuroraCap-7B [3] VideoChatGPT [31] Video-LLaVA [25] LLaMA-Vid [48] PLLaVA-7B [41] PLLaVA-13B [41] Idefics2-8B [18] VILA-v1.5-8B [26] VILA-v1.5-13B [26] NVILA-15B [29] VideoChat2-7B [22] InternVL-v2.5-8B [7] LLaMA3.2-Vision-11B [32] InternVideo-v2.5 [37] mPLUG-Owl-Video [44] QwenVL-v2-8B [36] Aria-3.5Bx8 [20] LLaVA-OneVision-7B [19] LLaVA-Video-7B [48] Cockatiel-8B (Distilled) Cockatiel-13B 29.26/1.54 37.64/1.96 33.88/1.77 33.19/1.74 32.80/1.72 36.12/1.88 34.45/1.79 37.37/1.93 25.36/1.36 39.74/2.06 41.81/2.16 31.69/1.64 31.94/1.68 34.36/1.81 33.62/1.74 31.37/1.65 38.17/1.99 35.40/1.85 39.84/2.07 37.57/1.96 38.73/2.02 42.25/2.19 42.62/2.21 32.60/1.70 38.35/2.00 37.98/1. 35.68/1.87 36.23/1.89 38.04/1.98 34.19/1.78 38.10/1.97 34.38/1.78 39.29/2.04 40.18/2.08 40.83/2.11 40.23/2.08 42.25/2.19 35.66/1.83 39.60/2.05 40.18/2.09 40.80/2.11 38.61/2.01 41.65/2.15 43.75/2.26 44.01/2.27 43.45/2.25 30.59/1.60 37.11/1.94 33.96/1.77 33.77/1.76 32.86/1.70 34.27/1.79 34.10/1.78 37.20/1.92 31.80/1.66 39.84/2.06 42.27/2.18 32.80/1.71 34.88/1.82 37.58/1.95 35.44/1.83 32.57/1.70 37.15/1.95 38.87/2.01 38.97/2.02 34.31/1.79 37.50/1.95 43.89/2.26 44.13/2.28 30.67/1.61 37.02/1.93 35.95/1. 33.54/1.75 33.08/1.73 35.42/1.84 34.40/1.78 36.40/1.89 31.73/1.65 40.85/2.11 42.27/2.18 33.45/1.74 34.93/1.82 38.23/1.99 34.80/1.80 35.19/1.84 38.49/2.00 40.59/2.10 43.21/2.23 38.81/2.02 41.71/2.16 43.85/2.26 44.37/2.29 31.67/1.66 38.49/2.00 41.52/2.15 34.70/1.81 36.47/1.89 35.89/1.87 36.97/1.92 39.68/2.05 31.49/1.63 42.80/2.21 43.26/2.23 42.04/2.17 40.48/2.11 42.99/2.22 36.77/1.90 38.47/1.99 40.25/2.10 43.56/2.26 45.36/2.33 41.81/2.16 45.56/2.35 44.00/2.27 44.42/2.29 30.96/1.62 37.72/1.97 36.66/1. 34.18/1.79 34.29/1.79 35.95/1.87 34.82/1.81 37.75/1.95 30.95/1.62 40.50/2.10 41.96/2.17 36.16/1.87 36.49/1.90 39.08/2.03 35.26/1.82 35.44/1.85 38.85/2.03 39.84/2.07 41.20/2.13 38.83/2.02 41.45/2.15 43.60/2.25 43.80/2.26 Table 1. Quantitative Comparison of existing VDC models on VDCSCORE Benchmark. We incorporate most up-to-date specialized VDC models as well as general MLLMs, since both regard video detailed captioning as their key capability. For each video, we extract 32 frames for most models, with exceptions for AuroraCap-7B (16), NVILA-15B (16), LLaMA3.2 (16), and Video-LLaVA (8). This difference ensures every model can fit and perform inference on single GPU. The best and second-best results are emphasized using bold and underline. movements involved in the video, how VDC models generate short and detailed captions, dubbed as Main Object, Background, Camera, Short and Detailed in Tab. 1. poral information during inference. 2.2. Benchmark Results and Analysis Current popular video caption models. We involve two types of models for evaluation, specialized video detailed captioning models and general video MLLMs, both of which treat video detailed captioning as their fundamental ability. Specifically, for the specialized VDC models, we employ Vriptor [42], ShareGPT4Video [5], and AuroraCap [3]. As for the general video MLLMs, we include both pioneer early works in the field, such as VideoChatGPT [31], Video-LLaVA [25, 27], LLaMA-Vid [23], PLLaVA [41], and latest versions of popular MLLM series, such as Idefics2 [18], VILA v1.5 [26], NVILA [29], InternVL2.5 [7, 8], LLaMA3.2 [9, VideoChat2 [22], 32], InternVideo-v2.5 [37], mPLUG-Owl-Video [39, 44], QwenVL-v2 [36], Aria [20], LLaVA-OneVision [19, 28], LLaVA-Video [48]. Particularly, we focus on models with fewer than 34B parameters, and uniformly extract 32 frames for input videos. By applying such settings, we eliminate the need for model parallelism while preserving more temOne single caption model usually prefers specific aspects than providing comprehensive results. To generate detailed and comprehensive captions, the model must effectively capture all visual elements in the video, including the main objects, camera movements, and backgrounds. This requires the model-generated captions to align with videos with good accuracy and coverage in every aspect. Nevertheless, as evidenced by the evaluation results in Tab. 1, no model demonstrates uniformly competitive performance across all five evaluated dimensions. Instead, the evaluated models tend to excel in specific aspects while underperforming in others, resulting in captions with uneven finegrained caption-video alignment across the dimensions. For instance, LLaVA-OneVision and LLaVA-Video show strong performance in VDCSCORE on Short and Detailed but perform inadequately in Camera, while Aria exhibits significant advantages in Detailed and Main Object but performs poorly in other aspects. This imbalanced fine-grained video-caption alignment is especially problematic, as all five evaluated aspects are equally critical and fundamental for the effective employment of VDC models, highlighting pressing need for improvement. Human alignment is not involved in existing approaches. Thorough manual observation on the generated captions, we also noticed they often fails to align with human preference. Such misalignment can be attributed to the lack of publicly available human-annotated or human-aligned VDC datasets. As consequence, VDC models are mainly trained on purely synthesized detailed image/video captions which lacks the guidance of human preference. While training on data generated by commercial models (such as GPT4-o [34]) might be solution, it suffers from the task gap between general visual dialogue and VDC, as well as the cost and efficiency of API calling. Ensembling captions from multiple VDC models with human guidance could improve VDC performance. As result of these challenges, there is an urgent need of VDC model capable of providing comprehensive, detailed, and human-aligned captions that capture every visual element in the videos. To fulfill this need, we can ensemble the most dimension-specific advantageous and humanpreferred one from candidate captions. Following this approach in dimension-to-dimension way, we will obtain synthetic training dataset that fully leverages the strengths of each ensembled base model while maintaining considerable alignment with human preferences. Such dataset can serve as excellent training material for VDC models, featuring high in both dimension-specific video-caption alignment and human preference alignment. In view of this circumstance, we propose Cockatiel, three-stage training pipeline that implement the ensemble synthetic and human preferenced training for VDC models. 3. The Proposed Cockatiel 3.1. Human-alignment Caption Scorer & Selector We meticulously annotate dataset of structured human preference score on video detailed captions and fine-tune MLLM on it. In this way, we obtain human-aligned scorer that can determine the caption training value for the selection policy. We will introduce details about the data annotation and scorer training in Sec. 3.1.1 and Sec. 3.1.2. 3.1.1. Scoring Data Annotation Definition of the quality score. The proposed video caption scorer is expected to evaluate given video-caption pair by assigning quality score that reflects their alignment degree. higher quality score means that the caption accurately describes more elements present in the video, with fewer omissions or errors. To calculate the quality score, the scorer is instructed to generate five individual scores and calculate their average as the final quality score of the input caption. Each of these scores ranges from 1 to 5 and focuses on specific type of visual element: object, object feature, object action, camera movement, and background. If particular type of visual elements is not involved in the video (e.g., video without camera movement), score of 0 is assigned, and this score is excluded from the calculation. This ensures that videos are not unfairly penalized for lacking certain elements, as they may be training-valuable on the remaining aspects. Formally, the quality score of synthesized video caption can be calculated as follows: Quality Score = (cid:80)5 i=1 si I(si = 0) (cid:80) I(si = 0) i=1 , where si is the score for the i-th visual element type, {1, 2, 3, 4, 5} corresponds to: object, object feature, object action, camera movement, and background. I(si = 0) is an indicator function that equals 1 if si = 0 and otherwise 0. Structured annotation pipeline. The annotation procedure is structured as series of five question-answering that roughly assigning tasks since annotators report general and unstructured score is unclear and confusing, thereby significantly lowers annotation efficiency and quality. For each task, the annotators are shown with video, synthetic caption that describe the video, and five questions about one type of fine-grained video-caption alignment. Each question offers six answer options representing the aforementioned quality score, an integer ranging from 0 to 5. To complete the annotation, annotators have to answer the five given questions considering the alignment degree of the video-caption pair on specific type of visual elements. Finally, the average score of these five questions is employed as the overall quality score of the synthesized caption. In cases that the annotated video is low in imaging quality or descriptive value, such as video contains NSFW content or no meaningful content, we ask the annotators to tag these videos and drop them. Notably, we also incorporate rigorous quality control mechanism including user interface design, multi-turn trial annotation, multiturn quality inspection and so on (see supplementary files). In general, applying this structured annotation pipeline not only allows for consistent and comprehensive yielding of human-aligned caption scoring data but also strikes considerable balance between cost and efficiency. For more details related to the annotated dataset statistics, please refer to the supplementary files. Annotated data statistics. To summarize, with videos sampled from OpenVid-1M and captions generated by the three ensembled base models, we annotate 6,534 videocaption pairs in total. 2,536 video-caption pairs are dropped because of the low quality of the paired video, with the remaining 4,008 pairs are successfully annotated with the five Figure 2. Overall pipeline of our proposed Cockatiel. Our training pipeline successfully ensemble both the advantages of base models and human preferences, yielding our Cockatiel captioner series. Through the ensembling synthetic and human preferenced training, Cockatiel-13B achieves significant VDC performance while being preferred by humans. aforementioned structured quality scores. Consequently, we format these metadata into 20,040 question-answering pairs to train our scorer model. 3.1.2. Scorer Model Implementation We employ LoRA [13] fine-tune to the VILA-v1.5-13B model on the annotated data, resulting in our scorer model. During training, we finetune all the linear layers in the connector and LLM modules with the rank and alpha of LoRA set as 256 and 512, respectively. The whole training is conducted on 8 NVIDIA A100 GPUs with batch size of 16. The learning rate warms up from 0 to 2e-5 in the first 3% steps and applies cosine learning rate scheduler. 3.2. Ensemble Multi-models Advantages and Human Preference We devise three-stage training pipeline to implement the proposed ensembling synthetic and human preferenced training while meet common engineering need. Specifically, we curate data employing the scorer-based selection policy with threshold, train Cockatiel-13B based on the data, and distill Cockatiel-8B from Cockatiel-13B. More details can be found in the following paragraphs in Sec. 3.2.1, while model implementations of Cockatiel captioner can be accessed in Sec. 3.2.2. 3.2.1. Training Pipeline Human-aligned data curation. Since VDCSCORE evaluates captions across five distinct perspectives, each with specialized set of prompts, we directly employ these prompts to instruct the base models to generate captions focusing on these dimensions. Initially, for each perspective, we sample 100k videos from OpenVid-1M [33] and pair each video with perspective-centered captions generated by three base models, with each model producing exclusive one caption. Then, we score each candidate caption and exclusively involve the one with the highest score for training if it exceeds the preset threshold. Specifically, we first leverage our scorer model to assign quality scores to the captions. The paired caption with the highest score is considered as the optimal candidate for that video. Further, to introduce human preference and address cases where all three base models perform poorly on certain videos, we set threshold of 3.5 on the quality score to filter out samples that are less human-preferable or low in caption quality. After that, the remaining data of each perspective that meets our requirements is sampled to fixed size, 20K, to balance the inter-dimension proportion. Eventually, we curate highquality and human-preferable training dataset that contains 100K pairs between dimension-specific caption and videos, with 20K video-caption pair for each perspective. Ensemble synthesized training. Through the data curation, we obtain training dataset that not only ensembles the strength of each base model but also ensures high alignment in fine-grained video-caption and human preference. Then, we finetune VILA-v1.5-13B on the curated data, which features strength-ensemble of the frontier VDC models and high alignment with human preference. Notably, the large model capacity of the backboned VILAv1.5-13B also guarantees the generalization of our model. Distilling 8B captioner model. For the ease of user usage and deployment, we further distill smaller Cockatiel-8B model from the Cockatiel-13B, leveraging VILA-v1.5-8B as the foundation model. Rather than directly fine-tuning the model on captions generated by Cockatiel-13B, we incorporate Cockatiel-13B as an additional base model. In other words, the procedure of the distillation stage is similar to that in the ensemble training stage, except the model to be fine-tuned on is VILA-v1.5-8B and the integrated base models have an additional member, Cockatiel-13B. Such procedure is approved because the former considers the situation where the Cockatiel-13B does not consistently outperform other base models. Besides, compared to the latter procedure, we incur no additional costs beyond the cheap scoring of captions generated by Cockatiel-13B. 3.2.2. Captioner Model Implementation Cockatiel-13B and Cockatiel-8B share identical hyperparameter settings with the scorer model, which is described in Sec. 3.1.2. As for their optimization processes, it can be formulated as follows: For simplicity, we define the training sample for the captioner as triplet, = (V, I, C), where RT HW 3 is the input video requiring captioning, with frames, each of height width , and 3 color channels. While is the textual instruction that prompts the captioner to generate caption for the video and = (C1, C2, . . . , Cn) is the ground truth caption, which is the desired output, with tokens. First, we uniformly sample 32 frames from the video and represent it as frame sequence Vs = {v1, v2, . . . , v32}, where each vt RHW 3 is single frame. The frame sequence Vs is then fed into the Vision Transformer (ViT) and projection module to extract the projected video feature Rd32 : = Proj(ViT(Vs)). (1) The functions ViT() and Proj() denote the process of encoding the frame sequence and mapping the ViT output to fixed-dimensional feature , respectively. Next, the projected video feature is concatenated with the instruction and passed into the LLM to generate the caption. The optimization objective is the general autoregressive loss function calculated only on the ground truth caption [10, 17]: L(θ) = (cid:88) i=1 log p(Ci F, I, C<i; θ), (2) where is the length of the ground truth caption C, C<i = (C1, C2, . . . , Ci1) denotes the token sequence of preceding the i-th token, θ represents model parameter. 4. Results & Analysis 4.1. Main Results Quantitative results. Tab. 1 presents the quantitative comparison results between Cockatiel-13B and baseline methods. Owing to our ensembling synthetic and human preferenced training, our model achieves new state-of-the-art performance on VDCSCORE exclusively in considerable dimension-balanced way. It also notes that our model is capable to generate dimension-comprehensive and humanpreferred detailed video captions. Qualitative results. Fig. 3 showcases the qualitative results generated by Cockatiel-13B and baseline methods, give more direct and clearer comparison of them. Through these cases, it is obvious that our model generates captions with better fine-grained video-caption alignment than baselines. For instance, in the top case of Fig. 3, Cockatiel-13B exclusively captures the dynamic changes as the video progresses and differ the static part with the dynamic part of the video. Furthermore, even the state-of-the-art VDC models that performs best on these cases generate content unaligned with input videos while Cockatiel-13Bs generated captions demonstrate higher level of fine-grained videocaption alignment with no errors observed. Human evaluation. To validate whether we successfully infuse Cockatiel-13B with human preferences and evaluate the generalization ability of our scorer on unseen captions, we conduct pairwise human evaluation at instance level to compare Cockatiel-13B with three leading VDC models. The results of human evaluation are summarized in Fig. 5. Apparently, Cockatiel-13B is consistently voted as the most human-preferred VDC model, outperforming other baselines by significant margin. The results also evidence the effectiveness of our scorer and our method since they successfully align Cockatiel-13B with human preference. 4.2. Ablation Studies Ablation study on selection policy. Selection policy determines how we select the most advantageous caption among its candidates, ensuring the effectiveness of our method. Considering its importance, we devise various selection Figure 3. Qualitative comparison between Cockatiel-13B and the current sota VDC models. For detailed comparison between Cockatiel-13B and all leading VDC models, please refer to the supplementary files. The caption content that is exclusively captured by our model, captured by our model and other baselines, or misaligned with the detailed visual elements in the videos are emphasized using red, yellow and green backgrounds. policies such as random selection and ranking-based selection using VILA, and the scorer-based selection policy with/without the threshold setting. As depicted in Tab. 2, all selection policies enhance VDCSCORE performance, while the scorer-based one with threshold setting achieving the highest improvement, by an average gain of 1.84. Notably, the scorer-based policy solely provides marginal advantages. However, integrated with threshold setting, it can be further bootstrapped by 0.37 on VDCSCORE, demonstrating the superiority of this composition. Ablation study on training dataset size. To select training dataset size that optimally balances cost and effectiveness while serving practical training recipe for further research on VDC, we conduct ablation study on training dataset size. The results are shown in Fig. 4. Initially, the average accuracy of VDCSCORE consistently improves as the training dataset size increases. However, when the dataset size scales from 20K to 25K and 30K, the model performance reaches convergence with no notable improvement observed. Surprisingly, the ablation study on training a c 44 43.8 43.6 43.4 43. 43.84 43.8 43.72 43.7 44 43.6 43.243. 42.8 43.43 0 64 128 256 512 Full LoRA Rank 43. 43.74 43.56 43.6 42.97 44 43. 43.6 43.4 43.8 43.56 43.43 10K 15K 20K 25K 30K"
        },
        {
            "title": "Dataset Size",
            "content": "2.5 3.0 Threshold 3.5 Figure 4. Ablation studies on the LoRA rank (left), training dataset size (middle), and the quality score threshold (right). For brevity, we report only the average accuracy on VDCSCORE; more comprehensive results are provided in the supplementary materials. The hyper-parameter settings are consistent across all the ablation studies, except the ablated one. Specifically, the default settings are as follows: LoRA rank is set to 256, the training dataset size is 20k, and the threshold for the quality score is 3.5, the selection policy is the scorer-based selection policy with threshold on quality score. Policy Camera (Acc / Score) Short (Acc / Score) Background (Acc / Score) Main Object (Acc / Score) Detailed (Acc / Score) Average (Acc / Score) Random VILA-zeroshot Ours w/ threshold Ours w/o threshold 40.97/2.12 42.00/2.17 42.62/2.21 42.87/2.21 42.98/2.22 43.49/2.25 43.45/2.25 43.11/2.23 42.20/2.19 42.93/2.22 44.13/2.28 43.59/2. 43.16/2.23 43.51/2.25 44.37/2.29 43.94/2.26 42.71/2.21 43.95/2.26 44.42/2.29 43.65/2.25 42.40/2.19 43.18/2.23 43.80/2.26 43.43/2. Table 2. Ablation study results on selection policy. Random, VILA-zeroshot, Ours w/ threshold, Ours w/o threshold are abbreviations for the compared selection policies. Specifically, they represent random selection, ranking-based selection using VILA, and scorer-based selection with or without the threshold setting. from 2.5, the average accuracy of VDCSCORE improves significantly and monotonically, demonstrating the effectiveness of our scorer model. Notably, the maximum threshold explored is 3.5, as insufficient data meets the criteria for thresholds of 4.0 or higher. Combined with the ablation study on training dataset size, the results from the threshold study suggest that, for ensemble synthesized training in VDC, the quality of the training data has more significant impact on model performance than its quantity. Figure 5. Human evaluation results. Our method, Cockatiel13B, is obviously more human-preferred compared to baselines. dataset size reveals that data scaling is not always useful in our method, which may be caused by the synthetic nature of our training data. In conclusion, our findings indicate that 20K is an optimal training dataset size, and scaling beyond this size does not necessarily lead to better performance. Ablation study on threshold setting. In addition to the selection policy, we also employ threshold on the caption quality score to ensure the training data quality. To verify the effectiveness of our scorer and explore the optimal threshold setting, we conduct an ablation study on it. As the results depicted in Fig. 4, with the threshold increases Ablation sutdy on LoRA rank. Increasing the LoRA rank allows more trainable parameters and incorporates more VDC-related knowledge into the base model, but it also introduce impairment on model generalization and general task performance. As consequence, we also investigate the impact of LoRA rank on VDC performance and summarize the results in Fig. 4. Following common practices, throughout the study, the LoRA alpha is consistently set to twice the LoRA rank while other hyperparameters remain at their optimal settings. The model achieves its best performance when the LoRA rank is set to 128 or 256. However, the performance drops significantly when increasing the rank to 512 or applying full fine-tuning. In consideration of accommodating more captioning knowledge, we set the LoRA rank as 256 instead of 128. 5. Conclusions In this paper, we raise two challenges faced by existing VDC models, the imbalanced detailed video-caption alignment and the misalignment with human preference. To alleviate these challenges, we develop Cockatiel, threestage training pipeline which assembles the advantages of multiple VDC models and human preferences by conducting ensembling synthetic and human preference training on It inVDC models based on our caption quality scorer. volving exclusively the candidate captions with the highest score for training if it exceeds the preset threshold. Further, we develop Cockatiel captioner series employing our training pipeline. Extensive results verify the effectiveness of our method, as Cockatiel-13B demonstrates significant and dimension-balanced performance on VDCSCORE while achieving high alignment with human preferences."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 14 [2] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2320623217, 2023. 2 [3] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. 2, 3, 13, 14 [4] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190200, Portland, Oregon, USA, 2011. Association for Computational Linguistics. 13 [5] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understandarXiv preprint ing and generation with better captions. arXiv:2406.04325, 2024. 3, 12, 13, 14 [6] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 2, 13, [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 3, 14 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 3 [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3 [10] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30: 681694, 2020. 6 [11] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, pages 393411. Springer, 2024. 2 [12] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 2 [13] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [14] Jie Jiang, Shaobo Min, Weijie Kong, Hongfa Wang, Zhifeng Li, and Wei Liu. Tencent text-video retrieval: hierarchical cross-modal interactions with multi-level representations. IEEE Access, 2022. 2 [15] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. arXiv preprint arXiv:2407.06358, 2024. 13 [16] Taehee Kim, Yeongjae Cho, Heejun Shin, Yohan Jo, and Dongmyung Shin. Generalizing visual question answering from synthetic to human-written questions via chain of qa with large language model. In ECAI 2024, pages 298305. IOS Press, 2024. 2, 13 [17] Klemens Lagler, Michael Schindelegger, Johannes Bohm, Hana Krasna, and Tobias Nilsson. Gpt2: Empirical slant delay model for radio space geodetic techniques. Geophysical research letters, 40(6):10691073, 2013. 6 [18] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37:8787487907, 2025. 3 [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 3, 13, [20] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model. arXiv preprint arXiv:2410.05993, 2024. 3 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 2 [22] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 3, 14 [23] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2024. 3 [24] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 2 [25] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [26] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 3 [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 3 [28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 3 [29] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. 3, 14 [30] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 [31] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 3, 14 [32] Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https://ai.meta. com/blog/llama-3-2-connect-2024-visionedge-mobile-devices/, 2024. 3 [33] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 5 [34] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 4, [35] Luozheng Qin, Shaoyao Huang, Qian Qiao, Xu Yan, and Ziqiang Cao. Bvrcc: Bootstrapping video retrieval via crossmatching correction. In International Conference on Artificial Neural Networks, pages 1933. Springer, 2024. 2 [36] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 14 [37] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 3 [38] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. 2 [39] Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi, Guangwei Xu, et al. Youku-mplug: 10 million large-scale chinese videolanguage dataset for pre-training and benchmarks. arXiv preprint arXiv:2306.04362, 2023. 3 [40] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 13 [41] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [42] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: video is worth thousands of words. arXiv preprint arXiv:2406.06040, 2024. 3, 13, 14 [43] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [44] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: large language model with Revolutionizing multi-modal In Proceedings of the IEEE/CVF modality collaboration. Conference on Computer Vision and Pattern Recognition, pages 1304013051, 2024. 3 [45] Youngjae Yu, Jongseok Kim, and Gunhee Kim. joint sequence fusion model for video question answering and retrieval. In Proceedings of the European conference on computer vision (ECCV), pages 471487, 2018. 2 [46] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. [47] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 14 [48] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 3, 13, 14 [49] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 2 [50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 A. Appendix Here, we demonstrate the organization of the supplementary material to help our readers to quickly find the content they interest. First, we introduce the possible limitations as well as applications of Cockatiel, in Appendix and Appendix C. The related works of this paper are introduced in Appendix D. Specifically, we introduce the works of video captioning and video Multimodal Large Language Models (video MLLMs) in Appendix D.1 and Appendix D.2. After that, we introduce more details related to the annotation for the dataset of structured human preference score on video detailed captions in Appendix E. The annotation involves comprehensive quality control mechanism to ensure the quality, quantity and diversity of the annotation, which will be generally introduced in Appendix E.1. As for details within the quality control mechanism, such as details related to the annotator selection and training, annotation user interface design and the prompts we used to instruct both the annotators and our scorer model, are introduced in Appendix E.2, Appendix E.3, Appendix E.4, respectively. Finally, we additionally give more experimental results and analysis in Appendix F. We exhibit the detailed results of every ablation study involved in theis paper, details related to human evaluation, and more qualitative case study in Appendix F.1, Appendix F.2, and Appendix F.3, respectively. B. Limitations Scorer performance and base models capabilities. The potential of our method is restricted by two key factors: the performance of the scorer and the capabilities of the ensembled base models. On the one hand, the capabilities of the base models define the upper-bound performance of our method. If the base models collectively perform poorly in specific aspects of VDC, we cannot generate high-quality synthesized data for them, and consequently, cannot infuse knowledge of those aspects into our captioner model. On the other hand, the scorer determines how close we can approach the theoretical upper-bound performance. The more accurate the scores assigned, the higher the quality of training data we can ultimately obtain. Furthermore, like other works that utilize LLM, our methods are prone to encounter hallucination, where they generate content that appears plausible but is factually incorrect, fabricated, or misaligned with the input videos. Despite these constraints, we believe our method is off to strong start, serving as competitive baseline for future works. Our scorer is powerful MLLM fine-tuned on meticulously-annotated dataset of structured human preference scores for caption quality. This ensures its effectiveness and generalization ability, which is further validated through ablation studies on selection policy and threshold setting. As for the ensembled base models, we select the top-3 competitive models on VDCSCORE, which represent the frontier performance of existing VDC models. Together, these components provide solid foundation for our approach. Model capacity. Due to limited GPU resources, in this paper, we exclusively focus on VDC models that can be accommodated in single GPU, specifically those with 34B parameters or fewer. As result, the conclusions drawn in this paper remain untested and may not hold for VDC models larger than 34B, especially the findings related to LoRA rank and training dataset size. That said, while the optimal hyper-parameter settings might vary for larger models, the effectiveness and efficiency of our framework are likely to experience only marginal changes. This is because our framework is not strictly dependent on the model capacity of the base models, but rather on the idea of ensemble synthesized training based on human-aligned caption quality score, which is scalable across different model sizes. Video length Considering our captioner models are mainly trained on videos shorter than 60 seconds and without scene transitions, their performance is limited when handling longer videos or those containing scene transitions. Nevertheless, it is still feasible to input such videos into our models and obtain captions reasonably aligned with them by increasing the number of input frame or employing the differential captioning technique introduced in Chen et al. [5]. Besides, since our captioner models are fine-tuned versions of VILA v1.5, they may inherit the capability to handle such videos from the base model. However, this inherited capability is not explicitly optimized in our current framework, leaving room for future improvements in addressing these challenges. C. Applications Considering the fundamental effect of our method and video detailed captioning task, we believe our works can be applied to various scenarios, such as evaluating video detailed captions, and serve as synthetic data generator for the training of other video understanding models. Evaluating video detailed captions. The yield of our scorer model, the quality score, is closely aligned with human preferences on evaluating model-generated video detailed captions, which serves as an alternative for human evaluation, GPT4-o based evaluations, and other costly evaluation methods. Besides, the scorer component can be used for filtering low-quality caption data. Since the score assigned by our scorer has clear and practical meanings, we suggest users set threshold greater than 3, which indicates that almost half of the visual elements involved in the video is missed or not correctly described. Meanwhile, setting too high threshold may introduce astonishing computation Figure 6. snapshot of our annotation user interface, where the whole annotation procedure is carried on. In each annotation task, the annotators are present with video on the left, caption supposed to align with it on the top, and question related to the detailed videocaption alignment on the right of the user interface. O, F, A, C, are abbreviations for Object, object Feature, object Action, Camera and Background, respectively. Notably, our user interface is highly praised by our annotators for its user-friendly and intuitive design, which ensures both the quality and quantity of the annotated data. costs. Video detailed captioning for VQA, fine-grained video understanding. Cockatiel-13B achieves new state-of-theart performance on VDCSCORE, demonstrating considerably competitive performance on video detailed captioning and significant potential for providing structured detailed video captions. Thus, by feeding it with videos, our method can continuously generate high-quality captions that can be used for synthesized training. As employing synthetic video detailed caption data for training is quite common in works about video question-answering and video-understanding [16, 19]. D. Related Work D.1. Video Captioning Video captioning aims to understand the input videos and generate captions aligned with its visual content, dynamic semantics, and temporal information, bridging the modality gap between text and video. Typically, existing video captioning datasets, such as MSRVTT [40] and MSVD [4], only pair each video with one or two sentences, which is quite short and thereby insufficient to describe most of the semantic and temporal information involved in videos. Meanwhile, the emergence of multimodal large language models and text-to-video diffusion models has catalyzed transformative progress in video-based AI systems and high-demand on descriptive video captioning tools. Motivated by this, researchers turn to build state-of-the-art video detailed captioning models to bootstrap the development of video understanding and generation [3, 5, 6, 15, 42, 48]. Unfortunately, developing competitive video detailed captioning model is fraught with various challenges, including the scarcity of high-quality training data and the difficulty of achieving balanced alignment between video content and captions. These challenges are exacerbated by the inherent complexity of video detailed captioning. Specifically, video detailed captioning is demanding even for humans, as it requires capturing every visual element and its dynamics across the temporal dimension. To address the data scarcity challenge, Chen et al. [6] propose to train video retrieval model to identify the best-match candidates for each captioned video. Furthermore, Yang et al. [42] and Chen et al. [5] focus on training VDC models using synthesized data distilled from GPT-4V [1] and GPT-4o [34]. Chai et al. [3] propose the token merging technique to lower the computation requirement of video detailed captioning. In this paper, we propose to ensemble the data generated by open-source models with respective advantages and filter the data using human-aligned caption scoring models. In this way, our method successfully tackles both of the two aforementioned challenges. By leveraging data generated by open-source models and employing rigorous data filtering procedure, the training data of VDC can be both costeffective and high-quality, thus sufficiently mitigating the first challenge. As for the second challenge of imbalanced alignment, we alleviate it by ensembling data generated by models with diverse specialties, as the fine-tuned models exhibit improvements across all of the evaluated dimensions on VDCSCORE. Notably, the cost of our approach is substantially lower than distilling data from commercial-level models, making it more scalable and accessible to research groups with limited budgets. D.2. Video Multimodal Large Language Models (Video MLLMs) Recent advancements in multimodal large language models (MLLMs) have shown impressive performance in general image-text dialogue tasks, along with significant potential for extension to the video modality. Inspired by this progress, researchers begin to study video multimodal large language models (Video MLLMs), where MLLMs are enhanced to support the processing of text, images, videos, and any combination thereof. The classic architecture of MLLM consists of three main components: visual encoder, connector, and large language model. The visual encoder first extracts features from the input image or video frames. Then, these features are passed through the connector, which projects them into the semantic space of the large language model. Finally, the projected visual features are concatenated with the embeddings of the input text and fed into the large language model for further processing. For video inputs, videos are treated as sequences of ordered frames. Each frame is independently encoded by the vision encoder, and the resulting frame features are concatenated to form frame-level representation. As pioneer in this area, Video-ChatGPT [31] proposes to concatenate the frame-level features with their pooled version to derive comprehensive video-level feature. While Video-LLaMA [47] integrating video along with its corresponding audio signal to the architecture of Video MLLMs by introducing video Q-Former and audio Q-Former. integrating both video and corresponding audio signals into the MLLM architecture through the introduction of video Q-Former and audio Q-Former modules. To address the problems of data scarcity and task diversity, Zhang et al. [48] curates large-scale video instruction-tuning dataset, LLaVA-Video-178K, which covers three common tasks in video-text dialogue. VideoChat2 [22] transfers static image tasks into their dynamic video version and conduct progressive multi-modal training with diverse instructiontuning data to establish robust video MLLM baselines. Recently, to further scale training data and amplify applicability, the latest version of popular MLLM series, such as LLaVA-OneVision [19], NVILA [29], Qwen2-VL [36], and InternVL2.5 [7], have incorporated video understanding capabilities into their frameworks. These models have shown promising performance across variety of video understanding tasks, marking significant step forward in the field. E. More Annotation Details E.1. Annotation Quality Control Aside from the previously mentioned structured annotation pipeline, we also employ rigorous quality control mechanism to guarantee the quantity, quality and efficiency of the annotation. Specifically, the quality control mechanism involves user interface design design, annotator training, multi-turn trial annotation, as well as multi-turn quality inspection. The entire annotation process is carried out on specially designed user interface, which enables us to regulate annotator behavior and supports the quality control mechanism. Before the formal annotation, we write all the details related to the goal, instruction and principle of the annotation in an annotation guidelines, and train our annotators to understand them. Moreover, we have an official meeting with them to explain the guidelines to them and let them to raise questions on details they dont understand. Before the formal annotation begins, we prepare detailed annotation guideline that outlines the goals, instructions, and principles of the annotation. Then, we hold official meetings to explain the guidelines in detail and address any questions or uncertainties raised by the annotators. After that, we conduct multi-turn trial annotation to optimize the annotation procedure and ensure that annotators adhere to the guidelines, maintaining high intraand inter-annotator consistency. During the annotation, we conduct 8 turns of random sampling quality inspection over the present annotated samples to guarantee ongoing accuracy and consistency. Each inspection involves detailed review of the annotations by experts, who verify whether the annotations align with the guidelines and meet the required quality standards. If discrepancies or errors are identified, we feedback to the corresponding annotators and take corrective measures immediately. This iterative process not Figure 7. The training pipeline of our scorer. Our selection policy is critical as its performance determines which knowledge and strengths is ensembled into Cockatiel-13B. As consequence, since our aim is to infuse Cockatiel-13B all the strengths of the base models and human preferences, we devise selection policy with threshold setting based on our human-aligned caption quality scorer. To obtain our caption quality scorer, we need human-annotated data on it or off-the-shelf models. Since no publicly available dataset nor model suits this need, we build them on our own, as demonstrated in this figure. only helps maintain high annotation quality throughout the project but also allows us to identify and address potential issues early, ensuring the reliability of the final annotated dataset. E.2. Annotator Selection and Training The accuracy and reliability of annotated data are significantly depended on the capabilities of the human annotators involved in the annotation process. Particularly, in our cases, our task of structured quality scoring on detailed video captions with human preferences are quite demanding on annotators. Since it requires the human annotators to score abstract integers on detailed video captions and hold consistent and appropriate annotation standard based on the combination of their subjective and objective cognition. As consequence, at the beginning of the annotation, We first conduct annotator selection to build an appropriate and unbiased annotation team, and train this annotation team with our meticulously prepared annotation guidelines. For annotator selection, we let the candidates to accomplish test concentrating on 10 factors, domain expertise, resistance to disturbing content, attention to detail, communication skills, reliability, cultural and linguistic competence, technical skills, ethical considerations, describing ability, and motivation. Notably, since the videos and their paired captions may contain uncomfortable and inappropriate content, the candidates are notified with this inconvenience before the test. Only those agreed with this inconvenience are eligible to participate in the test, and they are welcome to withdraw at any time if they choose to do so. Based on the test results and candidate backgrounds, We try our best to ensure that the selected annotators are well-balanced in background and have generally competitive abilities of the 10 mentioned factors. To summarize, our annotation team includes 10 annotators carefully selected from 47 candidates, 5 males and 5 females, all have bachelors degree. We interview the annotators and ensure they are adequate for the annotation. E.3. Annotation User Interface Design Fig. 6 illustrates the annotation user interface designed by us. With this specialized user interface, we are able to establish sound communication mechanism, regulate annotator behavior, and support various quality control measures. Here, we introduce details about the interaction involved within the annotation user interface. At the top of the user interface,a series of task IDs are displayed as serial numbers. The ongoing one is emphasized with bold number and white background, while others are shown with grey background and task IDs with no bold. By clicking on the serial number, the user interface will navigate to the corresponding annotation task, showing the annotation content requires labeling. To move to earlier or later tasks, the annotators can click on the omission of both ends to let the user interface display the task ID the annotators looking for. The one on the left end will display the ID of earlier tasks, the one on the right will display the ID of later tasks. Below the serial numbers is the main panel which consists of three components: the input video with video player component on the left, caption expected to align with the vide on the top, and question with six options on its right. Annotators are supposed to watch the video thoroughly, read the caption carefully, and assess the alignment between them based on the question. Eventually, they select the most appropriate option according to the annotation guideline and their judgment to complete the annotation of the question. As we mentioned above, each annotation task contains five questions that rigorously assess the degree of alignment between the caption and specific type of visual elements involved in the video. Specifically, the five types of visual elements are object, object feature, object action, camera movement and background, represented as the alphabet on the right bottom of the user interface, O, F, A, C, B, respectively. Similar to the serial numbers on the top, the alphabet that represents the ongoing question is highlighted with bold number and white background, while others are no bold and with gray background. Users can jump to the panel related to question by clicking on its corresponding alphabet. Notably, if all the questions related to the present annotation task are accomplished, the user interface will automatically jump to the first question of the next task. Otherwise, it will jump to one question of the present task that have not annotated yet. Furthermore, on the bottom of the user interface, there are two buttons, one shaped like red cross and the other shaped like raised human hand, accompanied with the text describing their effect. The red cross button is used to drop the data if the video is low in quality, such as videos poor in imaging quality, contains NSFW content, or contains incomprehensible material. If the annotators are unsure about their selection on certain data, they can use the raise hand button to report to us by writing descriptions about their problem. In general, our user interface is human-friendly and easy-to-use, which ensures efficient and accurate annotation while maintaining high-quality standards throughout the annotation process. E.4. Annotation Instruction Template Here, we present the prompts used to instruct our scorer model to generate structured quality scores for detailed video captions. These prompts can be categorized into two types. First, we employ five task-specific prompts, each designed to evaluate the alignment degree between the caption and specific visual element in the paired video: object, object feature, object action, camera movement, and background. Second, these task-specific prompts share system prompt, which provides overarching instructions to the scorer, clarifying the aim and expected behavior of the task. Below are the demonstration of the system prompt and the five task-specific prompts (with input video and caption details omitted for brevity): [System] You are an expert in evaluating video captions, specifically focusing on how effectively they align with and describe the content of the videos. [Object] Evaluate how well the caption describes the objects in the video. Use the following scoring options: 0.Not Involved: The video and the caption do not involve any objects. 1.Totally Incorrect: All descriptions are incorrect or missed. 2.Mainly Incorrect: Most descriptions are incorrect or missed, with only few correct. 3.Moderately Incorrect: Some descriptions are correct, while others are incorrect or missed. 4.Mainly Correct: Most descriptions are correct, with only few incorrect or missed. 5.Totally Correct: All descriptions are correct [Object Feature] Evaluate how well the caption describes the static attributes of objects in the video (e.g., color, shape, size, and texture). Use the following scoring options: 0.Not Involved: The video and caption do not involve any static attributes. 1.Totally Incorrect: All descriptions are incorrect or missed . 2.Mainly Incorrect: Most descriptions are incorrect or missed, with only few correct. 3.Moderately Incorrect: Some descriptions are correct, while others are incorrect or missed. 4.Mainly Correct: Most descriptions are correct, with only few incorrect or missed. 5.Totally Correct: All descriptions are correct. [Object Action] Evaluate how well the caption describes the dynamic attributes of objects in the video, such as movement, action and interaction. Use the following scoring options: 0.Not Involved: The video and caption do not involve any dynamic attributes. 1.Totally Incorrect: All descriptions are incorrect or missed. 2.Mainly Incorrect: Most descriptions are incorrect or missed, with only few correct. 3.Moderately Incorrect: Some descriptions are correct, while others are incorrect or missed. 4.Mainly Correct: Most descriptions are correct, with only few incorrect or missed. 5.Totally Correct: All descriptions are correct. [Camera] Evaluate how well the caption describes the camera movement in the video, including moves, pans, tilts, and zooms. Use the following scoring options: 0.Not Involved: The video and caption do not involve any camera movements. 1.Totally Incorrect: All descriptions are incorrect or missed. 2.Mainly Incorrect: Most descriptions are incorrect or missed, with only few correct. 3.Moderately Incorrect: Some descriptions are correct, while others are incorrect or missed. 4.Mainly Correct: Most descriptions are correct, with only few incorrect or missed. 5.Totally Correct: All descriptions are correct. [Background] Evaluate how well the caption describes the background (such as setting and context) in the video. Use the following scoring options: 0.Not Involved: The video and caption do not involve any background elements. 1.Totally Incorrect: All descriptions are incorrect or missed. 2.Mainly Incorrect: Most descriptions are incorrect or missed, with only few correct. 3.Moderately Incorrect: Some descriptions are correct, while others are incorrect or missed. 4.Mainly Correct: Most descriptions are correct, with only few incorrect or missed. 5.Totally Correct: All descriptions are correct. F. More Experiment Results F.1. More Ablation Study Results In this subsection, we demonstrate more detailed results of the ablation study on training dataset size, LoRA rank. We also illustrate the ablation study and analysis on model distillation. Ablation study on model distillation. Typically, simple yet efficient way to distill Cockatiel-8B from Cockatiel13B is training the Cockatiel-8B on video detailed captions generated by Cockatiel-13B. However, in our case, we already possess 100K structured detailed video captions generated by three leading VDC models and accompanied by caption quality scores assigned by our human-aligned caption scorer. As consequence, compared the this direct approach, there is no additional cost associated with involving both three base models and Cockatiel-13B for ensemble synthetic training but the cheap scoring on Cockatiel-13B generated captions. To verify the effectiveness of this design, we conduct an ablation study on the distilled models, the results of which are summarized in Tab. 6. As can be concluded, conducting another round of ensemble synthesized training on Cockatiel-13B and the three base models yields the most competitive results on VDCSCORE. In addition, the gain of assembling the advantages of multiple base models is saliently significant. Even when training solely on the most competitive model on VDCSCORE, Cockatiel-13B produces VDC models that underperform compared to simply ensembling the three base models. F.2. More Details about Human Evaluation Here, we give full demonstration of the conducted human evaluation. To verify the alignment with human preferences of Cockatiel-13B, we conduct pair-by-pair human evaluation was conducted at the case level. Specifically, we compare Cockatiel-13B with VILA-v.15-13B, LLaVA-Video7B and Aria3.5Bx8, since they are the top-3 competitive VDC models and base models of Cockatiel-13B. Data Preparation To prepare the data utilized for human evaluation, we first sample another 500 videos for detailed video captioning, except for those used for training. Then, we let each compared models to caption the sampled 500 videos. Finally, we make pairs between video, caption generated by Cockatiel-13B that describes the video, and another one generated by one of the three compared alternatives. We sample 500 videos and involve 3 baseline methods, VILA-v1.5-13B, LLaVA-Video-7B and Aria3.5Bx8 for the human evaluation, yielding totally 1,500 pairs to annotate. To uniformly distribute the annotation tasks, we select 9 of 10 annotators from our annotation team and every is assigned 500 pairs of evaluation tasks. The left one annotator evaluate whether the sampled videos have describing value and are suitable for the human evaluation. In this way, the evaluation result of each caption pair has three annotations, we choose the option selected most as the evaluation result of this specific pair. Evaluation Procedure. To conduct the human evaluation effectively and efficiently, we derive an easy-to-understand human evaluation user interface from that of our scoring human annotation for these human evaluators. The difference between the two user interface are the user interface of human evaluation only choose caption that they prefer. Each time, users are provided with video and two captions describing the video, one generated by Cockatiel-13B and the other one generated by one of the three baseline methods, VILA-v1.5-13B, Aria3.5Bx8 and LLaVA-Video-7B. To accomplish the annotation, users have to read these two captions and choose the one that clearly aligns better with their general and personal preference. If users find it quite difficult to find the one from the given two caption that they prefer, they can mark this pair as Not Distinguishable and we recognize this as tie between the two evaluated models. Notably, since we already checked the videos to be reviewed before, the DROP button in the user interface is set but not used during the human evaluation. In cases that evaluators are confused about some details of certain assigned evaluation task, they can utilize the raise hand button to ask the help of the organizers. Thanks to our adequate user training, reasonable evaluation procedure and friendly user interface, the raise hand button is set but not used in our evaluation. Notably, the captions are disordered, thus users are blind to the models that generates the comparing captions at the time. F.3. More Qualitative Results In the following pages, we give more detailed version of the qualitative analysis conducted in the main content, containing the complete comparison between the captions generated by Cockatiel-13B and three baseline methods, also the current top-3 VDC models according to the benchmark results on VDCSCORE. Besides, we include four additional cases to further holistically demonstrate the effectiveness of our method and the superior performance of Cockatiel-13B."
        },
        {
            "title": "LoRA Rank",
            "content": "Camera (Acc / Score) Short (Acc / Score) Background (Acc / Score) Main Object (Acc / Score) Detailed (Acc / Score) Average (Acc / Score) 64 128 256 512 Full 42.80/2.22 43.02/2.22 42.62/2.21 43.70/2.21 42.60/2.20 43.62/2.25 43.47/2.24 43.45/2.25 43.36/2.24 43.29/2.24 43.99/2.27 44.19/2.27 44.13/2.28 43.64/2.25 43.86/2.26 44.14/2.28 44.44/2.29 44.37/2.29 43.93/2.27 43.72/2.26 44.05/2.27 44.10/2.27 44.42/2.29 43.88/2.27 43.69/2. 43.72/2.26 43.84/2.26 43.80/2.26 43.70/2.25 43.43/2.24 Table 3. Ablation study results on LoRA rank.Full is the abbreviation for Full Fine-tuning. Dataset Size Camera (Acc / Score) Short (Acc / Score) Background (Acc / Score) Main Object (Acc / Score) Detailed (Acc / Score) Average (Acc / Score) 10k 15k 20k 25k 30k 41.49/2.15 42.55/2.20 42.62/2.21 42.46/2.20 42.62/2.21 43.17/2.23 43.67/2.26 43.45/2.25 43.51/2.24 43.41/2. 42.95/2.22 43.66/2.26 44.13/2.28 43.85/2.26 44.27/2.28 43.52/2.25 43.95/2.27 44.37/2.29 43.90/2.27 44.10/2.28 43.71/2.26 43.95/2.27 44.42/2.29 44.28/2.28 44.34/2.28 42.97/2.22 43.56/2.25 43.80/2.26 43.60/2.25 43.74/2.26 Table 4. Ablation study results on training dataset size. Threshold Camera (Acc / Score) Short (Acc / Score) Background (Acc / Score) Main Object (Acc / Score) Detailed (Acc / Score) Average (Acc / Score) 2.5 3.0 3.5 42.39/2.19 42.80/2.22 42.62/2.21 43.32/2.24 43.25/2.24 43.45/2.25 43.54/2.25 43.85/2.26 44.13/2.28 44.06/2.27 43.86/2.27 44.37/2.29 43.86/2.26 44.06/2.26 44.42/2. 43.43/2.24 43.56/2.25 43.80/2.26 Table 5. Ablation study results on quality score threshold. Models Ours Base Camera (Acc / Score) Short (Acc / Score) Background (Acc / Score) Main Object (Acc / Score) Detailed (Acc / Score) Average (Acc / Score) 41.75/2.17 42.23/2.19 43.45/2.24 43.77/2. 43.54/2.25 43.63/2.25 43.91/2.21 43.76/2.26 43.62/2.25 44.04/2.27 43.25/2.22 43.49/2.25 Ours + Base 42.25/2. 44.01/2.27 43.89/2.26 43.85/2.26 44.00/2.27 43.60/2.25 Table 6. Ablation study results on model distillation. Ours and Base indicates Cockatiel-13B and its ensembled base models, respectively. Figure 8. The first qualitative case compared with three base models in the main content. The caption content that is exclusively captured by our model, captured by our model and other baselines, or misaligned with the detailed visual elements in the videos are emphasized using red, yellow and green backgrounds. Figure 9. The second qualitative case compared with three base models in the main content. The caption content that is exclusively captured by our model, captured by our model and other baselines, or misaligned with the detailed visual elements in the videos are emphasized using red, yellow and green backgrounds. Figure 10. More video detailed captions generated by Cockatiel-13B. The caption content that is captured by our models and align with the detailed visual elements in the videos are emphasized using yellow backgrounds."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Academy of Artificial Intelligence for Science"
    ]
}