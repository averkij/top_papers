{
    "paper_title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm",
    "authors": [
        "Jiale Chen",
        "Torsten Hoefler",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Quantizing the weights of large language models (LLMs) from 16-bit to lower bitwidth is the de facto approach to deploy massive transformers onto more affordable accelerators. GPTQ emerged as one of the standard methods for one-shot post-training quantization at LLM scale. Yet, its inner workings are described as a sequence of ad-hoc algebraic updates that obscure any geometric meaning or worst-case guarantees. In this work, we show that, when executed back-to-front (from the last to first dimension) for a linear layer, GPTQ is mathematically identical to Babai's nearest plane algorithm for the classical closest vector problem (CVP) on a lattice defined by the Hessian matrix of the layer's inputs. This equivalence is based on a sophisticated mathematical argument, and has two analytical consequences: (i) the GPTQ error propagation step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error upper bound of Babai's algorithm under the no-clipping condition. Taken together, these results place GPTQ on firm theoretical footing and open the door to importing decades of progress in lattice algorithms towards the design of future quantization algorithms for billion-parameter models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 3 5 5 8 1 . 7 0 5 2 : r The Geometry of LLM Quantization: GPTQ as Babais Nearest Plane Algorithm Jiale Chen Institute of Science and Technology Austria (ISTA) 3400 Klosterneuburg, Austria Torsten Hoefler ETH Zürich 8092 Zürich, Switzerland Dan Alistarh Institute of Science and Technology Austria (ISTA) 3400 Klosterneuburg, Austria Jiale.Chen@ist.ac.at torsten.hoefler@inf.ethz.ch Dan.Alistarh@ist.ac.at"
        },
        {
            "title": "Abstract",
            "content": "Quantizing the weights of large language models (LLMs) from 16-bit to lower bitwidth is the de facto approach to deploy massive transformers onto more affordable accelerators. GPTQ emerged as one of the standard methods for one-shot post-training quantization at LLM scale. Yet, its inner workings are described as sequence of ad-hoc algebraic updates that obscure any geometric meaning or worst-case guarantees. In this work, we show that, when executed back-to-front (from the last to first dimension) for linear layer, GPTQ is mathematically identical to Babais nearest plane algorithm for the classical closest vector problem (CVP) on lattice defined by the Hessian matrix of the layers inputs. This equivalence is based on sophisticated mathematical argument, and has two analytical consequences: (i) the GPTQ error propagation step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error upper bound of Babais algorithm under the no-clipping condition. Taken together, these results place GPTQ on firm theoretical footing and open the door to importing decades of progress in lattice algorithms towards the design of future quantization algorithms for billion-parameter models."
        },
        {
            "title": "1 Introduction",
            "content": "Modern generative pre-trained transformers (GPT) contain hundreds of billions of parameters and require hundreds of gigabytes of memory for inference. Post-training weight quantization has emerged as the default practical solution for reducing this footprint without retraining the model. Among growing family of methods, GPTQ (Frantar et al., 2023) was the first method to push one-shot integer quantization down to the 4-bit regime, while retaining near-baseline accuracies. Despite its (relative) age, the method is still very popular, and still yields state-of-the-art results in some regimes (Kurtic et al., 2024). theoretical blind spot. Despite its empirical success, the GPTQ algorithm is presented as sequence of algebraic operations, applied greedily: the procedure picks one weight at time, quantizes it via rounding or clipping, and then optimally updates the remaining unquantized weights to correct for the remaining per-layer loss; it then continues with the 1 next weight, and so on. This procedure leaves an obvious open question: Why does local greedy rule work so well globally? Current literature does not answer these questions sufficiently, leaving researchers and practitioners with little guidance for principled extensions or failure case analysis. Independently, the closest vector problem The connection to lattice algorithms. (CVP) in lattice theory asks for an integer combination of basis vectors closest to target external point. CVP is ubiquitous in communications, cryptography, and computational geometry. Its algorithms, therefore, come with long series of guarantees on numerical stability, approximation ratios, and pathological cases. simple yet powerful heuristic is Babais nearest plane algorithm (Babai, 1986), which rounds one coordinate at time in an orthogonalized basis, and is known to provide error bound guarantees. Our contribution. We show in Section 4.1 that: Linear-layer quantization with the L2 objective on the output is equivalent to the closest vector problem (CVP) problem w.r.t. L2 distance, and prove in Section 4.3 that GPTQ executed from the last to first dimension with or without weight clipping is the same as Babais nearest plane algorithm on the basis of the factorized Hessian matrix, without LLL basis reduction and with/without clipping, respectively. These results have the following implications. 1. Geometric interpretation. The local optimal weight update step (error propagation) of GPTQ gains an intuitive geometric interpretation (Section 4.3). 2. Provable accuracy. GPTQ inherits Babais error bound under the no-clipping condition, yielding formal guarantee for layer-wise quantization error (Section 4.4). 3. Improved quantization order. connection between the quantization order of the dimensions and the error bound is established. Based on this, new ordering heuristic is proposed (Section 4.5). 4. Cross-pollination. Decades of lattice-algorithm techniques, e.g., basis reduction, can now be ported to the quantization settings with little translation effort."
        },
        {
            "title": "2 Related Work",
            "content": "Second-order compression (pruning and quantization). The idea of using Hessian information to guide parameter removal dates back to Optimal Brain Damage (LeCun et al., 1989) and Optimal Brain Surgeon (OBS) (Hassibi et al., 1993). Optimal Brain Compression (OBC) (Frantar & Alistarh, 2022) generalizes OBS to the post-training setting and unifies structured pruning and quantization under single exact solver. GPTQ (Frantar et al., 2023) inherits OBCs error propagation method but applies it in fixed order, so that the inverse Hessian can be shared and only needs to be computed once. GPTQ only has cubic computational complexity in the column/row dimension, making it suitable for LLMs. 2 QuIP (Chee et al., 2023) proves an error guarantee for GPTQ and proposes the LDLQ method as an equivalent variant of GPTQ. Lattices, CVP algorithms, and hardness. The closest vector problem (CVP) is NPcomplete to approximate within any constant factor under polynomial-time reductions (van Emde Boas, 1981; Micciancio & Goldwasser, 2002; Dinur et al., 2003), motivating decades of approximation algorithms. Babais nearest plane heuristic (Babai, 1986) delivers solution in polynomial time and, when preceded by LLL basis reduction (Lenstra et al., 1982), enjoys 2O(n) approximation. Our work reveals that GPTQs error propagation step is, in fact, equivalent to lattice projection, providing complementary geometric explanation to these Hessian-based heuristics. To the best of our knowledge, our paper is the first to connect these classical lattice techniques to modern quantization, allowing Babais worst-case guarantee to inform the design of weight-only post-training quantization algorithms. Compared to the error bound obtained in Chee et al. (2023), ours has geometric interpretation and is more fine-grained with the scale and quantization order considerations."
        },
        {
            "title": "3 Preliminaries and Notations",
            "content": "Algorithm 1: GPTQ Algorithm 2: Babais Nearest Plane Input: , S, X, , λ, Output: Z, 1 (cid:0)X + λI(cid:1) 2 LDL (cid:0)H 1(cid:1) 3 , 1W , 1S 4 Q, , 0 5 for 1 to do 6 ζ [j, :]/S[j, :] Z[j, :] Round (cid:0)ζ, Z(cid:1) Q[j, :] Z[j, :] S[j, :] ε Q[j, :] [j, :] 8 9 10 [j :, :] [j :, :] + L[j :, j]ε 11 end 12 Z, Z, Input: B, Output: 1 LLL (B) // transformation 2 BT // basis reduction 3 Φ QR (A) // orthogonalize 4 y, y, 0 5 for to 1 do 6 ζ Φ[:, j], / Φ[:, j], A[:, j] z[j] Round (ζ, Z) y A[:, j]z[j] 8 9 end 10 z"
        },
        {
            "title": "3.1 Linear-Layer Quantization Problem\nProblem. Let X = [x1, . . . , xn]⊤ ∈ Rn×c be the sampled calibration input data of batch\nsize n and input dimension c with xi ∈ Rc and n ≥ c = rank (X). Let W = [w1, . . . , wr] ∈\nRc×r be the linear layer weights of input dimension c and output dimension r with wi ∈ Rc.\nLet S = [s1, . . . , sr] ∈ Rc×r be the quantization scales with si ∈ Rc. Here we consider\na general case that applies to any grouping pattern: each weight element wi[j] has its\nown scaling factor si[j]. Assume S is statically computed using methods like AbsMax",
            "content": "3 or MSE before any weight updates. Let be the quantization grid (representable integers). In the clipping cases, e.g., for INT4 format, = {8, . . . , 1, 0, 1, . . . , 7}. In the no-clipping cases, = Z, which allows any integers as the quantization results. Let = [z1, . . . , zr] Zcr be the (unknown) quantized integers with zi Zc. Denote = [q1, . . . , qr] Rcr as the dequantized weights with qi = diag (si) zi Rc. The goal is to minimize the L2 error on the layer output XW Rnr: XQ XW 2 (cid:80)r 1 r. 2 = i=1 diag (si) zi Xwi2 , i.e, finding argminziZc diag (si) zi Xwi2 for all GPTQ algorithm. Algorithm 1 is the GPTQ algorithm for linear-layer quantization. The algorithm is identical to the original GPTQ paper (Frantar et al., 2023) except for missing the blocking mechanism that only affects the memory access pattern and computational speed, but not the numerical results. Additional notations are as follows. [0, 1]cc is permutation matrix that modifies the dimensional order of GPTQ quantization. The default order is front-to-back (from the first to last dimension), i.e., = I. λ R+ is small damping factor for computing the Hessian matrix. Function LDL returns the lower triangular matrix in LDL decomposition. Function (cid:17) Round (cid:0), Z(cid:1) = min returns the element-wise closest values in Z. We use Python-style indexing inside square brackets to select sub-matrices, e.g., [j, :] selects the j-th row vector, [:, j] selects the j-th column vector, and [j :, j] selects the sub-column consisting of rows after j-th (included) row in j-th column, etc. , max , max min (cid:17) (cid:16) (cid:16)"
        },
        {
            "title": "3.2 Closest Vector Problem (CVP)\nProblem. Let B = [b1, . . . , bc] ∈ Rn×c be a set of c basis vectors of dimension n with\nbj ∈ Rn and n ≥ c = rank (B). Let y ∈ Rn be an external target vector to approximate.\nLet z ∈ Zc be the (unknown) integer vector representing the basis combinations of the\nlattice vector. The goal is to find the vector on the lattice defined by the basis B that is\nthe closest to the external vector y, i.e., finding argminz∈Zc ∥Bz − y∥2. A visualization of a\ntwo-dimensional CVP is shown in Figure 1 (a).",
            "content": "Babais nearest plane algorithm. Algorithm 2 is Babais nearest plane algorithm (Babai, 1986) to solve CVP, which iteratively projects target vector onto the nearest hyperplane and rounds the coefficient. Figure 1 (b-c) visualize the projection steps, and Figure 1 (d) visualizes the basis reduction step that can be executed before the projections. Additional notations are as follows. Function LLL returns the transformation matrix of the LLL reduction with parameter delta defaulting to 3 . Function QR returns the orthogonal 4 matrix in QR decomposition, the same as the normalized Gram-Schmidt orthogonalization process. Function Round and the indexing inside square brackets are defined as in the GPTQ algorithm. Babais error bound. Figure 1 shows the rounding boundaries of the optimal (e), roundto-nearest (RTN) (f), and Babais algorithm without basis reduction (g-h). Compared to RTN, Babais algorithm generates rectangular partitions and thus has smaller worst-case error. 4 Figure 1: Upper row: (a) CVP in two-dimensional lattice; (b-c) The projection steps in Babais nearest plane algorithm without basis reduction; (d) Basis reduction can find shorter, more orthogonal basis that can potentially improve the results. Lower row: rounding boundaries of (e) optimal rounding or Voronoi cells; (f) round-to-nearest (RTN); (g) Babais nearest plane algorithm without basis reduction; (h) Babais algorithm without basis reduction under reversed basis ordering. Formally, let Φ = [ϕ1, . . . , ϕc] be the set of normalized Gram-Schmidt vectors of the basis = [a1, . . . , ac]. Let = [a1, . . . , ac] denote the unnormalized Gram-Schmidt vectors with aj = ϕj, aj ϕj. At iteration j, the algorithm replaces the exact coefficient ζ by the closest . Hence the error component along bj has integer, so the deviation satisfies ζ z[j] 1 2 2 aj. Because the is orthogonal, these error components add in Euclidean norm at most 1 norm, giving bound on the residual vector y: y2 1 j=1 ϕj, aj2. 4 Babais algorithm guarantees to return the center vector of the hyper-cuboid (Figure 1 (g)) constructed by the unnormalized Gram-Schmidt vectors where the target is located. Equality is attained when the target lies at the corner of the hyper-cuboid, so the bound is tight. j=1 aj2 = 1 4 (cid:80)c (cid:80)c"
        },
        {
            "title": "4.1 Equivalence of Quantization and CVP\nA quantization problem with the L2 objective argminzi∈Z†c ∥X diag (si) zi − Xwi∥2 and a\nCVP with the L2 distance argminz∈Zc ∥Bz − y∥2 share the same solution (z = zi) whenever\nthe structural conditions B = X diag (si) and y = Xwi hold and the solution domain\nmatches. To ensure the solution domain matches, we can either disable the clipping in\nthe quantization setup (setting Z† = Z) or enable the clipping in the CVP setup (making\nz ∈ Z†c).",
            "content": "We can introduce factor of the Hessian matrix, = [χ1, . . . , χc] with = . The loss can then be reformulated as diag (si) zi wi2. 5 Theorem 1 The CVPs using any possible factors of the Hessian matrix are equivalent under an orthogonal transformation (rotation and sign changes) of the lattice and external target vectors. Proof Let and be two possible factors of the Hessian matrix with = . must be equal for all 1 j1, j2 c. In other words, The inner products χ χj2 j1 and the and χ the lengths of χj1 j1 and χ angle between χ j2 j1 and χ j1 must be the same, and the angle between χj1 must be the same, for all 1 j1, j2 c. and χj χ j2 According to Theorem 1, any decomposition factor of the Hessian matrix can be used instead of without changing the geometric properties of the CVP and its associated quantization problem. This is useful to reduce the computational cost, e.g., we can use square matrix Rcc instead of the rectangular matrix Rnc."
        },
        {
            "title": "4.2 Applying Babai’s Algorithm to Batched Quantization",
            "content": "Given the equivalence we have shown in Section 4.1, the quantization problem can be converted to CVP, allowing us to apply Babais nearest plane algorithm in the context of quantization. naive way is to compute B(i) = diag (si) and y(i) = wi and run Babais algorithm independently for all 1 r. However, this is computationally inefficient, as we will need to compute the expensive (O (cid:0)c4(cid:1)) LLL basis reduction transformation T(i) for the basis B(i) and the expensive (O (cid:0)c3(cid:1)) QR decomposition of A(i) = B(i)T(i) for times. However, few adjustments can be made to simplify the computation and enable batched processing. Disabling basis reduction. The LLL basis reduction is unfortunately scale-sensitive, generating different transformations T(i) for different scales si (unless all the si vectors are parallel), which prohibits the reuse of QR decomposition results. Furthermore, LLL basis reduction is incompatible with clipping, as the roundings are performed in another basis, and there is no easy way to do the clipping for the original basis. Changing quantization order. Quantization order is feature in GPTQ that controls the rounding and clipping order of the dimensions. This order influences the quantization error, as we will discuss later in Section 4.5. In the context of Babais algorithm, this corresponds to the order of the basis in the Gram-Schmidt orthogonalization and the hyperplane projections, as shown in Figure 1 (g-h). To do so, we can replace the LLL basis reduction in Babais algorithm with permutation by setting the transformation matrix to permutation matrix that is independent of i. Theorem 2 If is permutation matrix that does not depend on i, the orthogonal matrix Φ can be reused without recomputing the QR decomposition for each i. Proof The permutation matrix [0, 1]cc has exactly one non-zero element in each row and column. Scaling the rows of can also be interpreted as scaling the columns of , there- (cid:1). fore its multiplication with diagonal matrix has property: diag (si) = diag (cid:0)T 1si Let = , A(i) = diag (si) . Denote the QR decomposition of as = ΦR with Φ being an orthogonal matrix and being an upper triangular matrix. Then, the QR de- (cid:1) = (cid:1)(cid:1). Therefore, the QR decompositions of A(i) share the same orthogonal composition of A(i) becomes A(i) = diag (si) = diag (cid:0)T 1si Φ (cid:0)R diag (cid:0)T 1si matrix Φ for all 1 r. As shown in Theorem 2, changing quantization order does not require repeated computation of the QR decomposition. Note that, we also need to permute the scale accordingly to 1S. (cid:1) = diag (cid:0)T 1si Selecting basis. Putting things together, we are interested in = and its QR decomposition Φ. Theorem 1 allows us to choose any Hessian factor while keeping the result intact. Without loss of generality, we can choose such that is an upper triangular matrix and the QR decomposition becomes trivial: Φ = I, which simplifies the computation. The upper triangular matrix can be directly computed from the Cholesky decomposition of the permuted Hessian matrix AA = XT . Applying all the considerations in this subsection, we construct Algorithm 3 for batched quantization using Babais algorithm. Algorithm 3: Babais Quantize Input: , S, X, , λ, Output: Z, 1 (cid:0)X + λI(cid:1) 2 Cholesky (H) 3 , 1W , 1S 4 , Q, AW , , 0 5 for to 1 do 6 ω [j, :]/A[j, j] ζ ω/S[j, :] Z[j, :] Round (cid:0)ζ, Z(cid:1) Q[j, :] Z[j, :] S[j, :] A[:, j]Q[j, :] 7 8 9 10 11 end 12 Z, Z, Algorithm 4: Min-Pivot Input: Output: 1 {1, . . . , c} 2 0 3 for 1 to do 4 argminjJ H[j, j] H[:, j]H[j, :]/H[j, j] [j, j] 1 {j} 5 6 7 8 end"
        },
        {
            "title": "4.3 The GPTQ and Babai’s Algorithm\nBy default, GPTQ (Algorithm 1) runs from the first to the last dimension (j ← 1 to c) while\nBabai’s algorithm (Algorithm 3) runs from the last to the first dimension (j ← c to 1).",
            "content": "Theorem 3 Algorithm 1 and Algorithm 3 have the same results Z, if their input variables , S, X, λ, are the same and have opposite column orders. Proof This is our main technical contribution. The full proof is presented in Appendix A. Theorem 3 shows that, if we align the dimensional order of these two algorithms, e.g., multiplying with the anti-diagonal permutation matrix for GPTQ, or running GPTQ from the last to the first dimension, these two algorithms will become equivalent. 7 Geometric interpretation. The proof of Theorem 3 shows that each intermediate weight vector produced by GPTQ can be viewed as Babais residual in activation space: if we regard the floating-point weight vector as target point and the activations as the lattice basis, GPTQ performs an orthogonal walk through nested sequence of affine subspaces. At step it projects the current residual onto the hyperplane orthogonal to the j-th GramSchmidt vector, while the familiar error propagation update is exactly this orthogonal projection. Ineffectiveness of additional GPTQ refinement on Babais algorithm. seemingly appealing idea is to take the solution returned by each Babais iteration and then perform one further GPTQ-style error propagation step on the weights in the space projected by A, as further update on , hoping to push the approximation even closer to the optimum. However, as proved in Appendix A.4, such an extra update vanishes: the intermediate quantity ω and therefore the final results of and remain unchanged. In other words, once Babais projection has been executed, any subsequent GPTQ-style correction is algebraically redundant. This result confirms that the equivalence established in Theorem 3 is already tight and that neither algorithm can be strengthened by naively composing it with the other."
        },
        {
            "title": "4.4 Quantization Error Bound",
            "content": "Having established the correspondence between GPTQ and Babais nearest plane algorithm, we can now import Babais approximation guarantee to obtain an upper bound on the layer-wise quantization error in the no-clipping cases. Theorem 4 Assume there is no clipping (Z = Z). Let be the diagonal matrix in the LDL decomposition of the permuted Hessian matrix XT . For every output channel (1 r) produced by Babais algorithm (Algorithm 3), or equivalently GPTQ executed backto-front, the quantization error has tight error upper bound: diag (si) zi Xwi2 1 4 T DT 1si. Proof Denote B(i) = diag (si), y(i) = wi as in Section 4.1 so that the quantization problem is the CVP minimizing (cid:13) (cid:13) 2. Applying Babais algorithm with the (cid:13) permutation gives the permuted basis A(i) = B(i)T = diag (si) = diag (cid:0)T 1si (cid:1). (cid:3). Babais Write the unnormalized Gram-Schmidt vectors of A(i) as A(i) = (cid:2)a(i)1, . . . , a(i)c (cid:13) guarantee therefore yields the tight bound (cid:13) 2 (cid:13) 1 4 (cid:13)B(i)zi y(i) (cid:13)B(i)zi y(i) 2 = (cid:13) (cid:13) (cid:13) (cid:1) y(i) (cid:0)T 1zi (cid:13) (cid:13)a(i)j (cid:13)A(i) (cid:13) 2. (cid:13) (cid:80)c j=1 (cid:13)a(i)j (cid:12)A(i)[j, j](cid:12) (cid:13) simplifies to (cid:12) (cid:13) We may, without loss of generality, use Theorem 1 to rotate so that A(i) is upper triangular. In that case, the norm (cid:13) (cid:12). The summation on the right-hand side can then be expressed as tr (cid:0)D(i) (cid:1) with D(i) denoting the diagonal matrix of the LDL (i)A(i). Let be the lower triangular matrix in the LDL decomposition decomposition of (cid:1) = L(i), D(i), of XT , so that (i) with D(i) = diag (cid:0)T 1si (cid:1)1. The trace tr (cid:0)D(i) DT 1si. Dividing by 4 completes the bound. For no-clipping GPTQ with the default front-to-back order (Algorithm 1) and the permutation (i)A(i) = diag (cid:0)T 1si (cid:1) diag (cid:0)T 1si (cid:1) and L(i) = diag (cid:0)T 1si (cid:1) XT diag (cid:0)T 1si (cid:1) diag (cid:0)T 1si (cid:1) = T , the error bound is diag (si) zi Xwi2 1 diagonal matrix in the LDL decomposition of PT XT P. 4 T PDPPT 1si with DP being the If we assume the weights wi are uniformly distributed within the hyper-cuboid constructed by Babais orthogonalized basis vectors, the expected error will be 1 of the worst-case bound. 3 See Appendix for the proof."
        },
        {
            "title": "4.5 The Role of Quantization Order",
            "content": "The quadratic form on the right-hand side of the error bound in Theorem 4 depends on the permutation matrix . Re-ordering the dimensions changes the entries of the diagonal matrix before the scale si is weighted by them. poor order may place large entries against large si entries and hence inflate the bound. For batched quantization algorithm like GPTQ or our proposed Babais algorithm, should be independent of i. To develop good heuristic order, reasonable approximation to make, especially for large quantization group sizes, is that the elements of si[j] are equal for all 1 c. Then we can focus on finding the optimal pivot order for the LDL decomposition of the Hessian matrix to minimize the trace of D. Finding the optimal order is NP-hard, e.g. Rose et al. (1976). However, heuristics often effectively reduce the trace term in practice. Even in the clipping cases, the heuristics still can often reduce the error. GPTQ introduces the so-called act-order, the descending order of the Hessian diagonal. This translates to the ascending order of the Hessian diagonal when applied to Babais algorithm. This act-order is good heuristic, but it only considers the information from the Hessian diagonal instead of the full matrix. To improve the act-order, we propose the min-pivot order, which is essentially taking the minimum diagonal entry at each LDL (or Cholesky) decomposition step. This order can be calculated by Algorithm 4, which has cubic time complexity and does not increase the overall time complexity of the whole quantization process. This order also has geometric interpretation as the order of the Gram-Schmidt orthogonalization process of the basis: always taking the shortest residual vector as the next one to orthogonalize."
        },
        {
            "title": "5 Conclusion",
            "content": "We have shown that GPTQ, when executed back-to-front, is mathematically identical to Babais nearest plane algorithm applied to the lattice defined by layers Hessian without basis reduction. This equivalence has the following implications: (i) it endows GPTQs error propagation step with clear geometric meaning: an orthogonal projection onto successive hyperplanes; and (ii) it transfers Babais tight error bound to quantization, giving formal guarantee for GPTQ in the no-clipping regime. It also motivates our min-pivot quantization ordering, lightweight rule that should reduce the error bound. Looking ahead, extending the analysis to clipped grids and exploring scale-aware basis reductions are the immediate next steps. More broadly, the lattice perspective opens twoway channel: decades of CVP heuristics can refine practical quantizers, while the behavior of massive neural networks may, in turn, inspire new questions for lattice theory."
        },
        {
            "title": "References",
            "content": "László Babai. On lovász lattice reduction and the nearest lattice point problem. Combinatorica, 6(1):113, March 1986. ISSN 1439-6912. doi: 10.1007/BF02579403. URL https://doi.org/10.1007/BF02579403. 2, 3, 4 Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2In A. Oh, T. Naubit quantization of large language models with guarantees. mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 43964429. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 0df38cd13520747e1e64e5b123a78ef8-Paper-Conference.pdf. 3 I. Dinur, G. Kindler, R. Raz, and S. Safra. Approximating cvp to within almost-polynomial ISSN 1439-6912. doi: factors is np-hard. Combinatorica, 23(2):205243, apr 2003. 10.1007/s00493-003-0019-y. URL https://doi.org/10.1007/s00493-003-0019-y. 3 Elias Frantar and Dan Alistarh. Optimal brain compression: framework for In S. Koyejo, S. Mohamed, accurate post-training quantization and pruning. A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 44754488. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 1caf09c9f4e6b0150b06a07e77f2710c-Paper-Conference.pdf. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate In The Eleventh International quantization for generative pre-trained transformers. Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=tcbBPnfwxS. 1, 2, 4 Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. In IEEE International Conference on Neural Networks, pp. 293299 vol.1, 1993. doi: 10.1109/ICNN.1993.298572. 2 Eldar Kurtic, Alexandre Marques, Shubhra Pandit, Mark Kurtz, and Dan Alistarh. \" give me bf16 or give me death\"? accuracy-performance trade-offs in llm quantization. arXiv preprint arXiv:2411.02355, 2024. 1 Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky (ed.), Advances in Neural Information Processing Systems, volume 2. MorganKaufmann, 1989. URL https://proceedings.neurips.cc/paper_files/paper/1989/ file/6c9882bbac1c7093bd25041881277658-Paper.pdf. 2 Arjen Klaas Lenstra, Hendrik Willem Lenstra, and László Lovász. Factoring polynomials with rational coefficients. Mathematische Annalen, 261(4):515534, dec 1982. ISSN 1432-1807. doi: 10.1007/BF01457454. URL https://doi.org/10.1007/BF01457454. Daniele Micciancio and Shafi Goldwasser. Complexity of Lattice Problems: Cryptographic Perspective, volume 671 of The Springer International Series in Engineering and Computer 10 Science. Springer, New York, NY, 1 edition, 2002. ISBN 978-0-7923-7688-0. doi: 10.1007/ 978-1-4615-0897-7. URL https://doi.org/10.1007/978-1-4615-0897-7. 3 Donald J. Rose, Robert E. Tarjan, and George S. Lueker. Algorithmic aspects of vertex elimination on graphs. SIAM Journal on Computing, 5(2):266283, 1976. doi: 10.1137/ 0205021. 9 P. van Emde Boas. Another np-complete problem and the complexity of computing short vectors in lattice. Technical Report 8104, University of Amsterdam, Department of Mathematics, Netherlands, 1981. 3 Appendix A. Equivalence Proof of GPTQ and Babais Algorithm In this section, we prove Theorem 3 that GPTQ (Algorithm 1) and Babais algorithm (Algorithm 3) are equivalent if the dimensional orders are opposite. Because permutation matrix acts only as re-ordering coordinates, we may apply once at the beginning (to , S, and X) and once at the end (to and Q) without affecting any intermediate arithmetic. Hence, all algebra performed inside the two algorithms can be analyzed in the permuted basis where is the identity. On that basis, the sole distinction between GPTQ and Babais algorithm lies in the direction of the iterations. Proving that GPTQ running back-to-front (j to 1) reproduces Babais updates in Babais default iteration direction would complete the equivalence proof. We follow three-step proof scheme. Step 1. Proving that the original GPTQ algorithm (Algorithm 5) that uses relative quantization error row vector ε R1r is equivalent to new algorithm (Algorithm 6) using the absolute quantization error matrix Rcr. Step 2. Reversing the iteration in Algorithm 6 and writing the reversed-iteration algorithm as Algorithm 7. Step 3. Proving that the reversed-iteration algorithm Algorithm 7 is equivalent to Babais algorithm Algorithm 8. Algorithms 5 to 8 are intentionally written in the linear algebra form. ej Rc is the standard basis vector whose elements are 0 except the j-th element being 1, which is used as the row or column selector of matrix. The superscripts in parentheses denote the versions of the variables during the iterations. ω, ζ R1r are intermediate row vectors. Additionally, is the LDL decomposition of the Hessian inverse 1 = LD where is lower triangular matrix with all diagonal elements being 1, and is non-negative diagonal matrix. Similarly, is the UDU decomposition of the Hessian inverse 1 = UD where is an upper triangular matrix with all diagonal elements being 1, and non-negative diagonal matrix. UU is D 1 2 1 2 1 2 1 2 1 2 1 Note: the symbols are overloaded in Algorithms 5 to 8, and the variables using the same symbols may carry different values, even if the inputs to the algorithms are the same. A.1 Step 1 To distinguish the variables using the same symbol in Algorithms 5 and 6, we use symbols without ˆ to denote the symbols in Algorithm 5, and use the symbols with ˆ for Algorithm 6."
        },
        {
            "title": "Claim",
            "content": "and consequently, ωj = ˆωj, 1 c, Z(j) = ˆZ(j), 0 c, (1) (2) Algorithm 5: GPTQ Original (Front-to-Back) Input: , S, X, λ, Output: Z, 1 + λI 2 LDL (cid:0)H 1(cid:1) 3 (0) 4 Q(0), Z(0) (0), 0 5 for 1 to do ω(j) ζ(j) ω(j) diag (cid:0)Sej Z(j) Z(j1) + ej (j1) (cid:16) 8 6 7 9 Q(j) Q(j1) + ej ε(j) Q(j) ω(j) 10 11 (j) (j1) + Lejε(j) 12 end 13 Z, Z(c), Q(c) (cid:1)1 Round (cid:0)ζ(j), Z(cid:1) Z(j) diag (cid:0)Sej (cid:1) Z(j1)(cid:17) Q(j1)(cid:17) (cid:16) Algorithm 6: GPTQ Type-2 (Front-to-Back) Input: , S, X, λ, Output: Z, 1 + λI 2 LDL (cid:0)H 1(cid:1) 3 (0) 4 Q(0), Z(0) (0), 0 5 for 1 to do ω(j) ζ(j) ω(j) diag (cid:0)Sej Z(j) Z(j1) + ej (j1) (cid:16) 7 6 9 Q(j) Q(j1) + ej (j) Q(j) (0) // new 10 11 (j) (0) L1(j) // new 12 end 13 Z, Z(c), Q(c) (cid:1)1 Round (cid:0)ζ(j), Z(cid:1) Z(j) diag (cid:0)Sej (cid:1) Z(j1)(cid:17) Q(j1)(cid:17) (cid:16) 13 Algorithm 7: GPTQ Type-2 (Back-to-Front) Input: , S, X, λ, Output: Z, 1 + λI 2 UDU (cid:0)H 1(cid:1) // new 3 (c+1) 4 Q(c+1), Z(c+1) (c+1), 0 5 for to 1 do ω(j) ζ(j) ω(j) diag (cid:0)Sej Z(j) Z(j+1) + ej (j+1) (cid:16) 6 8 9 8 (cid:1)1 Round (cid:0)ζ(j), Z(cid:1) Z(j) diag (cid:0)Sej Q(j) Q(j+1) + ej (j) Q(j) (c+1) (cid:16) Z(j+1)(cid:17) Q(j+1)(cid:17) (cid:1) 10 11 (j) (c+1) 1(j) // new 12 end 13 Z, Z(1), Q(1) Algorithm 8: Babai-Quantize (Default Order) Input: , S, X, λ, Output: Z, 1 + λI 2 Cholesky (H) 3 (c+1), Q(c+1), Z(c+1) AW , , 0 4 for to 1 do (j+1) ω(j) Aej ζ(j) ω(j) diag (cid:0)Sej Z(j) Z(j+1) + ej (cid:1) (cid:16) 5 6 7 Round (cid:0)ζ(j), Z(cid:1) (cid:16) Z(j) diag (cid:0)Sej Q(j) Q(j+1) + ej Q(j) (j) (j+1) Aeje Z(j+1)(cid:17) (cid:1) Q(j+1)(cid:17) 9 10 end 11 Z, Z(1), Q(1) 14 and Q(j) = ˆQ(j), 0 c. Proof Eq. 1 by Induction The following equalities are held by the design of Algorithms 5 and 6: Q(0) = ˆQ(0) = (0) = ˆW (0). ω(j) = W (j1), 1 c. ˆω(j) = ˆW (j1), 1 c. Q(j) = Q(j1) + ej ˆQ(j) = ˆQ(j1) + ej (cid:16) (cid:16) Z(j) diag e ˆZ(j) diag (cid:16) (cid:16) Sej Sej (cid:17) (cid:17) Q(j1)(cid:17) ˆQ(j1)(cid:17) e , , 1 c. 1 c. ε(j) = Q(j) ω(j), 1 c. (j) = ˆQ(j) ˆW (0), 1 c. (j) = (j1) + Lejε(j), 1 c. ˆW (j) = ˆW (0) L1(j), 1 c. Extend the definition of (j) (Eq. 10) for = 0, (j) = ˆQ(j) ˆW (0), 0 c. (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) Then we have (0) = ˆQ(0) ˆW (0) = ˆW (0) ˆW (0) = 0 , so that Eq. 12 can also be extended for = 0, ˆW (j) = ˆW (0) L1(j), 0 c. (1) Eq. 1 holds for = 1: Using Eqs. 4, 5, 6, ω(1) = 1 (0) = ˆW (0) = ˆω(1). (14) (15) (2) Assume Eq. 1 holds for all j, 1 < c. Because is lower triangular matrix with all diagonal elements being 1, L1 is also lower triangular matrix with all diagonal elements being 1. For 1 < c, For 1 c, Lek = e L1ek = 0. Lej = e L1ej = 1. 15 (16) (17) For 1 < c, (cid:32) (cid:88) (cid:33) eke e j+1L k=1 (cid:32) (cid:88) =e j+1L (cid:33) eke ej+1e j+1 (cid:88) eke k=1 (cid:32)j+1 (cid:88) k=1 eke =e j+1L =e j+1LI j+1 (cid:33) k=j+2 Lej+1e j+1 j+1L (cid:88) j+1Leke =e j+1L j+1 k=j+2 (cid:88) 0e k=j+2 =e j+1 (L I) . With Eq. 7, for 1 c, 1 and = k, (cid:88) k=j+2 eke (Eq. 17) (Eq. 16) Q(j) =e (cid:16) Q(j1) + ej (cid:16) =e Q(j1) + ej (cid:16) =e =e Q(j1) + 0 Q(j1). Z(j) diag (cid:16) Z(j) diag (cid:16) Sej (cid:16) Sej (cid:17) (cid:17) (cid:17) Q(j1)(cid:17)(cid:17) Q(j1)(cid:17) Q(j1)(cid:17) Z(j) diag Sej (cid:16) Recursively applying Eq. 19, for 1 c, 1 c, Q(j) = (cid:40) Q(k) Q(0) = e (0) if 1 c, if 1 < c. Similar to Eq. 20, with Eq. 8, for 1 c, 1 c, ˆQ(j) = (cid:40) e ˆQ(k) ˆQ(0) = ˆW (0) if 1 c, if 1 < c. With Eq. 21, for 1 c, 1 c, (cid:16) ˆQ(j) ˆW (0)(cid:17) ˆW (0) ˆQ(j) k (j) =e =e (cid:40) = e ˆQ(k) ˆW (0) ˆW (0) = ˆW (0) = if 1 c, (k) (0) = 0 if 1 < c. (Eq. 13) (18) (19) (20) (21) (22) For 1 c, L(j) LI(j) =e (cid:32) =e (cid:88) (cid:33) eke (j) k=1 (cid:88) Leke k(j) = = = = = = k=1 (cid:32) (cid:88) k=1 (cid:32) (cid:88) k=1 (cid:32) (cid:88) k=1 (cid:32) (cid:88) k=1 (cid:88) (cid:33) Leke k(j) + (cid:32) (cid:88) (cid:33) Leke k(j) Leke k(k) k=k+1 (cid:32) (cid:88) (cid:33) + k=k+1 (cid:33) 0e k(j) (cid:33) 0e k(k) (cid:33) Leke k(k) + (cid:33) Leke k(k) + (cid:32) (cid:88) k=k+1 (cid:32) (cid:88) k=k+1 (cid:33) Leke k(k) (Eq. 16) (Eqs. 16, 22) (Eq. 22) (23) Leke k(k) (cid:33) eke (k) k= =e (cid:32) (cid:88) k=1 LI(k) L(k). =e =e 17 For 1 c, L1(j1) L1I(j1) =e (cid:32) (cid:33) =e (cid:88) eke (j1) k=1 (cid:88) = L1eke (j1) L1eke (j1) (cid:33) (cid:33) k=1 (cid:32)j1 (cid:88) k=1 (cid:32)j1 (cid:88) k=1 (cid:32)j1 (cid:88) k=1 (cid:32)j1 (cid:88) k=1 (cid:32) (cid:88) k=1 = = = = = + L1eje (j1) + (cid:88) k=j+1 L1eke (j1) L1eke (j1) + L1ej0 + (cid:88) 0e (j1) (Eqs. 16, 22) (cid:33) L1eke (j1) + (cid:88) k=j+1 k=j+1 + 0e (j1) (j) j (j) (cid:33) L1eke (j) + (cid:88) L1eke (j) k=j+1 (cid:33) L1eke (j) (j) + L1eje (j) (j) =e L1 (cid:32) (cid:88) (cid:33) eke (j) (j) k=1 L1I(j) (cid:0)L1 I(cid:1) (j). =e =e j (j) According to the assumption, for 1 < c, we have (k1) = ω(k) = ˆω(k) = e ˆW (k1) and Q(k) = ˆQ(k). 18 (Eqs. 17, 22) (24) (25) (26) For 1 j, ε(k) =e =e Q(k) ω(k) Q(k) =e =e =e =e =e (cid:16) (k1) Q(k) (k1)(cid:17) (cid:16) ˆQ(k) ˆW (k1)(cid:17) (cid:16) ˆQ(k) (cid:16)(cid:16) ˆQ(k) ˆW (0)(cid:17) (k) + L1(k1)(cid:17) (cid:16) (k) + (cid:0)L1 I(cid:1) (k)(cid:17) (cid:16) (cid:16) ˆW (0) L1(k1)(cid:17)(cid:17) + L1(k1)(cid:17) =e L1(k) =e L1(j) =e (Eq. 9) (Eqs. 25, 26) (Eq. 14) (Eq. 13) (Eq. 24) (Eq. 23). (Eq. 5) (Eq. 11) (Eq. 11) ω(j+1) =e j+1W (j) (cid:16) =e j+1 (j1) + Lejε(j)(cid:17) (cid:32) (cid:32) (cid:88) (0) + Lekε(k) (cid:33)(cid:33) =e j+1 =e j+1 (cid:32) (cid:32) k=1 (cid:32) (cid:88) ˆW (0) + (cid:33)(cid:33) Leke L1(j) (Eq. 27) =e j+1 ˆW (0) + k=1 (cid:32) (cid:88) (cid:33) (cid:33) eke L1(j) k=1 =e j+1 =e j+ =e j+1 =e j+1 (cid:16) ˆW (0) + (L I) L1(j)(cid:17) (cid:16) ˆW (0) L1(j) + (j)(cid:17) (cid:16) ˆW (0) L1(j) + 0 (cid:16) ˆW (0) L1(j)(cid:17) ˆW (j) (cid:17) =e j+1 = ˆω(j+1) (Eq. 18) (Eq. 22) (Eq. 14) (Eq. 6). (27) (28) Eq. 1 holds for = + 1. A.2 Step 2 Algorithm 7 (back-to-front order) is generated by reversing the iteration direction of Algorithm 6. Besides changing the direction of the index j, we also need to change the LDL decomposition to so-called UDU decomposition so that the error propagation is correctly applied to the not-yet-quantized weights in the front dimensions."
        },
        {
            "title": "Justification",
            "content": "Let be the anti-diagonal permutation matrix with = = P1. Let ˆL be the LDL 1 ˆL where ˆL is lower ˆD decomposition of the permuted Hessian inverse PH 1P = ˆL ˆD 2 triangular matrix with all diagonal elements being 1, and ˆD is non-negative diagonal matrix. 1 2 1 2 Since we are changing the iteration direction instead of applying the permutation, we permute the matrix ˆL back, yielding = ˆLP. Alternatively, can be calculated using PP ˆLP = the decomposition 1 = ˆLPP ˆD UU where is an upper triangular matrix with all diagonal elements being 1, and is non-negative diagonal matrix. UD = ˆD PP ˆD 1 2 1 2 1 2 1 2 1 2 1 The decomposition to calculate from 1 is what we call UDU decomposition, which can be considered as variant of the LDL decomposition. A.3 Step 3 To distinguish the variables using the same symbol in Algorithms 7 and 8, we use symbols with ˆ to denote the symbols in Algorithm 7, and use the symbols with for Algorithm 8. We have the Cholesky decomposition of H: = (cid:0)H 1(cid:1)1 = (cid:18)"
        },
        {
            "title": "U D",
            "content": "1 2 UD 1 2 UU (cid:19)1 = (cid:18) 1 1 2 (cid:19) 1 1, so that = 1 1."
        },
        {
            "title": "Claim",
            "content": "and consequently, and ˆωj = ωj, 1 c, ˆZ(j) = Z(j), 1 + 1, ˆQ(j) = Q(j), 1 + 1. (29) (30) (31) Proof Eq. 29 by Induction 20 For 1 c, ω(j) = = = Y (j+1) Aej Y (j+1) 1 1ej D (j+1) 1 [j, j] 2 U[j, j]e 1 2 =D (j+1) =e 1 2 UY (j+1). The following equalities are held by the design of Algorithms 6 and 8: ˆQ(c+1) = Q(c+1) = ˆW (c+1) = . (c+1) = = ˆω(j) = (cid:16) ˆW (j+1), (cid:16) ˆZ(j) diag Sej 1 1 . 2 1 c. (cid:17) ˆQ(j) = ˆQ(j+1) + ej Q(j) = Q(j+1) + ej (cid:16) Z(j) diag (cid:16) Sej (cid:17) ˆQ(j+1)(cid:17) Q(j+1)(cid:17) , , 1 c. 1 c. (j) = ˆQ(j) ˆW (c+1), ˆW (j) = ˆW (c+1) 1(j), 1 1eje 2 Q(j) = (j+1) 1 c. 1 c. (j) = (j+1) Aeje Q(j), 1 c. (32) (33) (34) (35) (36) (37) (38) (39) (40) Because is an upper triangular matrix with all diagonal elements being 1, 1 is also an upper triangular matrix with all diagonal elements being 1. For 1 < c, For 1 c, (1) Eq. 29 holds for = c: Using Eqs. 32, 33, 34, 35, 42, ω(c) = D 1 2 UY (c+1) = D ek = e 1ek = 0. = e . ej = e 1ej = 1. (41) (42) (43) 1 2 UD 1 1 = 2 1 = W = c ˆW (c+1) = ˆω(c). (44) 21 (2) Assume Eq. 29 holds for all j, 1 < c. With Eq. 36, for 1 c, 1 and = k, ˆQ(j) =e =e =e =e (cid:16) (cid:16) ˆQ(j+1) + ej ˆQ(j+1) + ej (cid:16) ˆQ(j+1) + 0 ˆQ(j+1). Z(j) diag (cid:16) Z(j) diag (cid:16) Sej (cid:16) Sej (cid:17) (cid:17) (cid:17) ˆQ(j+1)(cid:17)(cid:17) ˆQ(j+1)(cid:17) ˆQ(j+1)(cid:17) Z(j) diag Sej j (cid:16) Recursively applying Eq. 45, for 1 c, 1 c, ˆQ(j) = (cid:40) e ˆQ(k) ˆQ(c+1) = ˆW (c+1) if 1 c, if 1 < c. Similar to Eq. 46, with Eq. 37, for 1 c, 1 c, Q(j) = (cid:40) e Q(k) Q(c+1) = if 1 c, if 1 < c. For 1 c, (45) (46) (47) (j) =Y (j+1) =Y (c+1) Q(j) 1 1eje 2 (cid:88) 1 1eke 2 Q(k) (Eq. 40) (Eq. 40) k=j =D 1 1 2 (cid:88) 1 1eke 2 Q(j) (Eq. 34) (48) =D 1 1 2 k=j (cid:88) Q(j) eke k=j 22 For 1 < c, ω(j) =e 1 2 UY (j+1) =e 1 2 UD 1 1 2 (cid:88) Q(j+1) eke =e 1 c (cid:88) k=j+1 k=j+1 Q(j+1) eke (Eq. 32) (Eq. 48) (cid:88) k=j+ 1eke Q(j+1) (cid:32)(cid:32) (cid:88) k=1 (cid:32)(cid:32) (cid:88) (cid:33) 1eke (cid:33) 1eke k (cid:33) (cid:33) 1eke e 1eje Q(j+1) (cid:33) (cid:33) 0e 1e Q(j+1) (Eqs. 41, 43) (cid:32)j1 (cid:88) k=1 (cid:32)j1 (cid:88) k=1 (cid:33) 1eke k Q(j+1) + Q(j+1) (cid:33) 1eke Q(j+1) + W (Eq. 47) =e 1 =e 1 =e 1 =e 1 =e 1 (cid:32) k=1 (cid:32) (cid:88) k=1 (cid:32) (cid:88) k=1 (cid:32)(cid:32) (cid:88) k=1 =e 1 (cid:33) (cid:33)(cid:33) eke Q(j+1) =e =e (cid:16) 1 (cid:16) Q(j+1) (cid:16) 1 (cid:16) Q(j+1) (cid:17)(cid:17) (cid:17)(cid:17) . (49) Because (cid:16) 1 (cid:16) Q(c+1) (cid:17)(cid:17) = W = ω(c), Eq. 49 can be extended for = c, ω(j) = (cid:16) 1 (cid:16) Q(j+1) (cid:17)(cid:17) , 1 c. According to the assumption, for 1 < c, we have ˆQ(k) = Q(k). 23 (50) (51) ω(j1) =e j1 =e j1 =e j1 (cid:17)(cid:17) (cid:16) 1 (cid:16) Q(j) (cid:16) ˆW (c+1) 1 (cid:16) ˆQ(j) ˆW (c+1)(cid:17)(cid:17) (cid:16) ˆW (c+1) 1(j)(cid:17) ˆW (j) =e j1 = ˆω(j1) Eq. 29 holds for = 1. (Eq. 50) (Eq. 51) (Eq. 38) (Eq. 39) (Eq. 35). (52) A.4 Proof of ineffectiveness of additional GPTQ refinement on Babais algorithm We may try to apply further GPTQ updates in Babais algorithm by changing Line 9 in Algorithm 8 to (j) (j) + AU ejε(j) = (j+1) Aeje Q(j) + AU ejε(j) (53) However, as = 1 1, the ω(j1) remains the same: 2 1 2 1 2 ω(j1) =e j1D =e j1D =e j1D =e j1D =e j1D =e j1D = ω(j1) 1 2 UY (j) (cid:18) (j) + (Eq. 32) 1 1U ejε(j) 2 (cid:19) 1 2 j1D UD j1ejε(j) 1 2 UY (j) + UY (j) + UY (j) + 0ε(j) UY (j) 1 2 1 2 1 1U ejε(j) 2 (54) (Eq. 32). Appendix B. Expected Quantization Error over Uniform Hyper-Cuboid In Section 4.4 we showed that, when clipping is disabled, Babais nearest-plane (hence back-to-front GPTQ) ensures the tight worst-case bound diag (si) zi Xwi2 1 (cid:88) j=1 aj2 , = [a1, . . . , ac] (55) where aj are the unnormalized GramSchmidt vectors of the permuted lattice basis A. 24 Introduce the half-edge lengths 1 2 so that the Babai residual always lies in the axis-aligned hyper-cuboid (cid:81)c Eq. 55 is rewritten as = 1, . . . , c, aj , aj = (56) j=1 [aj, aj] and ϵworst = (cid:88) j=1 a2 . (57) Uniform prior on the unknown weight vector. Assume now that the continuous unquantized weight offset = (wi diag(si)zi) is uniformly distributed inside this hypercuboid, i.e., each coordinate uj Uniform (aj, aj) and the coordinates are independent. The squared error becomes the random variable ϵ = (cid:88) j=1 u2 . Lemma 5 For scalar Uniform (a, a) one has E[u2] = a2 3 ."
        },
        {
            "title": "Proof",
            "content": "E[u2] = 1 2a (cid:90) u2du = 1 2a (cid:20) 1 3 x3 (cid:21)a = a2 . Expected residual norm. Using independence, E[ϵ] = (cid:88) j=1 (cid:2)u2 (cid:3) = 1 3 (cid:88) j=1 a2 . (58) (59) (60) Ratio to the worst-case bound. Comparing Eq. 60 with Eq. 57 gives E[ϵ] = 1 3 ϵworst = (cid:104) diag (si) zi Xwi2(cid:105) = 1 12 (cid:88) j= aj2. (61) Hence, under uniform prior on the weights inside Babais orthogonal hyper-cuboid, the average layer-wise quantization error is exactly 1 of the worst-case guarantee stated in 3 Theorem 4."
        }
    ],
    "affiliations": [
        "ETH Zürich",
        "Institute of Science and Technology Austria (ISTA)"
    ]
}