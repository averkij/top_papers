{
    "paper_title": "Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery",
    "authors": [
        "Mahmoud El Hussieni",
        "Bahadır K. Güntürk",
        "Hasan F. Ateş",
        "Oğuz Hanoğlu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate building instance segmentation and height classification are critical for urban planning, 3D city modeling, and infrastructure monitoring. This paper presents a detailed analysis of YOLOv11, the recent advancement in the YOLO series of deep learning models, focusing on its application to joint building extraction and discrete height classification from satellite imagery. YOLOv11 builds on the strengths of earlier YOLO models by introducing a more efficient architecture that better combines features at different scales, improves object localization accuracy, and enhances performance in complex urban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000 annotated buildings across 12 cities -- we evaluate YOLOv11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLOv11 achieves strong instance segmentation performance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust classification accuracy across five predefined height tiers. The model excels in handling occlusions, complex building shapes, and class imbalance, particularly for rare high-rise structures. Comparative analysis confirms that YOLOv11 outperforms earlier multitask frameworks in both detection accuracy and inference speed, making it well-suited for real-time, large-scale urban mapping. This research highlights YOLOv11's potential to advance semantic urban reconstruction through streamlined categorical height modeling, offering actionable insights for future developments in remote sensing and geospatial intelligence."
        },
        {
            "title": "Start",
            "content": "Mask-to-Height: YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery Mahmoud El Hussieni, Bahadır K. Gunturk Istanbul Medipol University 34810, Istanbul, Turkiye mahmoud.moawed@std.medipol.edu.tr bkgunturk@medipol.edu.tr Hasan F. Ates Dept. of AI and Data Eng. Ozyegin University 34794, Istanbul, Turkiye hasan.ates@ozyegin.edu.tr Oguz Hanoglu Huawei Turkiye R&D Center 34768, Istanbul, Turkiye oguz.hanoglu1@huawei.com 5 2 0 2 1 ] . [ 1 4 2 2 7 2 . 0 1 5 2 : r AbstractAccurate building instance segmentation and height classification are critical for urban planning, 3D city modeling, and infrastructure monitoring. This paper presents detailed analysis of YOLOv11, the recent advancement in the YOLO series of deep learning models, focusing on its application to joint building extraction and discrete height classification from satellite imagery. YOLOv11 builds on the strengths of earlier YOLO models by introducing more efficient architecture that better combines features at different scales, improves object localization accuracy, and enhances performance in complex urban scenes. Using the DFC2023 Track 2 datasetwhich includes over 125,000 annotated buildings across 12 citieswe evaluate YOLOv11s performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLOv11 achieves strong instance segmentation performance with 60.4% mAP@50 and 38.3% mAP@5095 while maintaining robust classification accuracy across five predefined height tiers. The model excels in handling occlusions, complex building shapes, and class imbalance, particularly for rare high-rise structures. Comparative analysis confirms that YOLOv11 outperforms earlier multitask frameworks in both detection accuracy and inference speed, making it well-suited for real-time, large-scale urban mapping. This research highlights YOLOv11s potential to advance semantic urban reconstruction through streamlined categorical height modeling, offering actionable insights for future developments in remote sensing and geospatial intelligence. Index TermsBuilding instance segmentation, height classification, satellite imagery, multitask learning, YOLO I. INTRODUCTION Urban planning, disaster response, and environmental monitoring increasingly rely on accurate geospatial intelligence derived from satellite imagery. key challenge lies in extracting both spatial boundaries and vertical characteristics of built environments at scale. This paper presents unified framework for joint building instance segmentation and discrete height classification using the latest version of the You Only Look Once (YOLO) architectureYOLOv11. Unlike regression-based methods that output continuous height values (e.g., 17m), we classify buildings into interpretable height categories (e.g., low-rise, 979-8-3315-9727-6/25/$31.00 2025 IEEE high-rise). This reframing offers significant advantages: it simplifies downstream applications such as zoning and infrastructure planning, enhances robustness to noisy or incomplete elevation data, and eliminates the need for complex postprocessing. Our approach leverages the DFC2023 Track 2 dataset, which consists of multimodal satellite imagery from 12 cities across five continents. It includes over 125,000 annotated buildings, with normalized Digital Surface Models (nDSMs) providing ground truth height information. Buildings are categorized into five height classes, as defined in Table I. We compare our method against recent state-of-the-art models including LIGHT [2], HGDNet [3], and the multitask network by Huo et al. [5]. While these models perform continuous height regression and often rely on dense supervision and complex feature fusion, our categorical approach integrates height classification directly with instance segmentation, enabling more interpretable and deployment-friendly outputs. Additionally, we demonstrate that YOLOv11s real-time inference capabilities make it particularly suitable for largescale urban mapping. By modeling height as structured classification task, we improve interpretability, deployment efficiency, and resilience to label noise, while maintaining high spatial fidelity and detection accuracy. The remainder of this paper is organized as follows: Section II reviews related work; Section III describes the dataset and methodology; Section IV presents experimental results and comparisons; Section concludes the paper with discussion of future directions. II. RELATED WORK Recent advances in remote sensing and deep learning have led to the development of several multitask frameworks for joint building extraction and height estimation. One of the most notable approaches is LIGHT [2], which combines Mask R-CNN with Pyramid Pooling Module (PPM) to perform pixel-wise height regression alongside instance segmentation. While LIGHT achieves competitive performance on the DFC2023 dataset, its reliance on continuous height prediction introduces complexity and sensitivity to noisy ground truth data. The model also employs Gated Cross Task Interaction (GCTI) module to enhance feature sharing between tasks, further increasing architectural overhead. Another prominent method is HGDNet [3], which uses hierarchical guided distillation to align semantic and geometric features across branches. This approach improves consistency between segmentation and height estimation but comes at the cost of increased training time and dependency on teacher networks. Similarly, Huo et al. [5] proposed multitask framework that jointly estimates building footprints and heights using shared backbone features. However, their method lacks the ability to distinguish individual building instancesa critical requirement for urban planning and 3D reconstruction applications. Unlike prior works that rely on continuous height regression or external modules for task interaction, our framework adopts categorical classification paradigm for building height modeling. By integrating instance segmentation and discrete height classification within single, streamlined architecturespecifically YOLOv11we avoid the need for complex multi-branch designs commonly used in models like LIGHT [2] and HGDNet [3]. Our approach achieves strong performance with 60.4% mAP@50 and 38.3% mAP@5095 on average for five height classes, this indicates that discrete height classification can serve as an effective alternative to explicit regression in many cases. Moreover, the model supports real-time inference and deployment, making it wellsuited for large-scale urban mapping tasks. These improvements highlight the practical advantages of discrete height modeling, particularly in the presence of class imbalance and measurement uncertainty typical of real-world remote sensing data indicating that discrete height classification can be an effective alternative to explicit regression in many cases. III. METHODOLOGY A. Dataset and Height Classification Pipeline The experiments were conducted using the IEEE GRSS Data Fusion Contest 2023 (DFC2023) Track 2 dataset [1], large-scale benchmark designed to support semantic urban reconstruction by combining satellite imagery with normalized Digital Surface Models (nDSMs). The dataset comprises 1,773 multimodal satellite images from 12 cities across five continents, featuring: Multimodal Satellite Imagery: Includes both optical orthophotos (RGB channels) and Synthetic Aperture Radar (SAR) data. Polygon annotations: Precise vector outlines suitable for complex shapes and interior structures Normalized DSMs used as ground truth for height values total of 125,153 annotated buildings are included in the DFC2023 Track 2 dataset, with segmentation masks provided in polygon format. As defined in the challenge guidelines [1], Track 2 focuses on the joint task of building extraction and continuous height estimation from multimodal satellite imagery (optical and SAR). Participants are required to reconstruct building footprints and predict pixel-wise height values using Digital Surface Models (DSMs) as ground truth. In contrast to this regression-based objective, our approach adopts classification-based formulation, strategy that has shown effectiveness in various vision tasks. We reformulate height estimation as discrete classification task, assigning each building to one of five predefined height classes based on DSM-derived statistics as shown in Table I. This approach offers greater robustness to noisy elevation data and aligns with practical urban planning needs, where coarse-grained tiers (e.g., 010m, 1120m, etc.) are more actionable than precise values. The classes reflect typical zoning and building typologies, ranging from low-rise homes to skyscrapers, enabling more effective integration into applications like 3D city modeling, zoning regulation, and risk assessment [4]. TABLE HEIGHT CLASS DEFINITIONS BASED ON DSM RANGES Class Height Range (meters) 1 2 3 4 5 010 1120 2130 3140 41+ 1) Digital Surface Model Processing and YOLO Annotation Generation: To enable joint building instance segmentation and discrete height classification, we implemented structured preprocessing pipeline that converts raw DSM data into YOLOv11-compatible annotations. This process follows established practices in remote sensing data preparation [1] while adapting to the specific requirements of YOLOv11s architecture [6]. 1) Data Loading and Configuration COCO-formatted annotations are loaded to extract building instances. Corresponding DSM rasters are read using the rasterio library for geospatial alignment. Output directories for YOLO-formatted labels are created to organize training and evaluation. 2) Annotation Handling Polygon annotations are prioritized for accurate boundary extraction. All coordinates are normalized to the [0,1] range required by YOLO format: = , = where , are image width and height. 3) Height Calculation We convert continuous DSM values into five discrete height classes (Table I), enabling interpretable, category-based height modeling that aligns with practical urban planning needs. For each building instance, binary mask is generated from its polygon, and DSM values within the masked area are extracted, with invalid (NaN) entries filtered out. robust height estimate is then computed as the rounded mean of valid DSM values: hmean = round (cid:32) (cid:33)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 hi (1) where hi are valid DSM height samples within the building boundary. 4) Height Class Assignment Based on the mean height value calculated in the previous step, each building is assigned to one of the five predefined height classes, consistent with our discrete modeling approach. 5) YOLO-Compatible Label Generation The final output is stored in the standard YOLO format: <class> <x1> <y1> <x2> <y2>...<xn> <yn> (normalized polygon coordinates) This format enables seamless integration with YOLOv11s instance segmentation and classification heads. 2) Class Distribution and Balancing: The dataset exhibits natural class imbalance, with lower-rise buildings dominating: Class 1 (010 m): 35.4% Class 2 (1120 m): 28.0% Class 3 (2130 m): 23.8% Class 4 (3140 m): 9.8% Class 5 (41+ m): 3.1% To mitigate this imbalance during training, we employed focal loss and adaptive class weighting strategies, preventing bias toward the majority classes while maintaining sensitivity to rare, taller building types. The DFC2023 Track 2 dataset offers large-scale, multimodal benchmark with precisely annotated buildings, combining spatial, elevation, and polygon-level data. Its complexity and inherent height class imbalance make it well-suited for evaluating multitask models that unify instance detection with structured categorical outputsaligning closely with our approach. Fig. 1. Structure diagram of the YOLOv11 network B. YOLOv11 Architecture The You Only Look Once version 11 (YOLOv11), introduced by Ultralytics in late 2024, represents breakthrough in real-time object detection and instance segmentation [8]. Building on advancements from YOLOv8 and YOLOv10, YOLOv11 introduces architectural innovations that enhance multi-scale feature fusion, computational efficiency, and robustnesskey requirements for remote sensing applications such as urban building segmentation. Motivated by its state-of-the-art performance in large-scale benchmarks, we adopt YOLOv11 as the foundation for our joint building instance segmentation and height classification framework. YOLOv11 sets new standard in object detection by achieving top-ranked accuracy and computational efficiency, with optimal parameter and FLOPs optimization and strong inference speed, offering an exceptional balance between performance and deployability, even in resourceconstrained environments. Ablation studies on the DFC2023 Track 2 dataset under identical conditions confirm its superiority over YOLOv8 and YOLOv10, demonstrating higher mAP@50 and mAP@5095 scores and greater robustness to class imbalance and complex urban scenes, thereby validating YOLOv11 as the most effective architecture for highprecision, scalable geospatial analysis. Architectural Overview: YOLOv11 adopts unified architecture comprising three core components (see Figure 1): Backbone: CSPDarknet-based feature extractor with enhanced gradient flow Neck: Improved PANet++ for multi-scale feature aggregation Head: Decoupled design for simultaneous bounding box, class, and mask prediction Key Innovations: Enhanced CSPDarknet Backbone: Optimizes hierarchical feature learning through revised residual connections and channel attention mechanisms [6]. Decoupled Head Architecture: Separates regression (bounding boxes), classification, and mask prediction branches for task-specific optimization [8]. Improved PANet++ Neck: Incorporates bidirectional cross-scale connections with depth-wise separable convolutions, improving feature fusion efficiency [8]. Cross-Scale Pixel Spatial Attention (C2PSA): Hybrid attention mechanism combining: Channel-wise attention for feature recalibration Spatial attention for positional awareness Cross-scale aggregation for multi-resolution processing This design enhances both global context and fine-grained detail capture [6]. Dataset-Specific Advantages: The architecture provides distinct benefits for satellite imagery analysis: Polygon Annotation Support: Native compatibility with DFC2023s building footprint requirements Multi-Scale Robustness: Handles building size variations (10200m) through adaptive feature pyramids Shadow/Occlusion Resilience: C2PSA modules suppress noise while enhancing structural features Computational Efficiency: Achieves 30 FPS on NVIDIA RTX 2080 at 512512 resolution (batch size=8) As demonstrated in [8], YOLOv11 shows particular improvements in: Small-target detection (AP@50 improvement: +4.2% vs YOLOv10) Multi-target scenarios (mAP@50:95 gain: +3.8%) Complex backgrounds (false positive reduction: 31%) These capabilities directly address the core challenges of the DFC2023 Track 2 dataset, particularly in dense urban environments with height-discrete building classes. The integration of modules like C3k2 blocks and optimized SPPF layers further enhances performance for satellite-scale object detection tasks. C. Training Configuration The YOLOv11 model was initialized with weights pretrained on the COCO (Common Objects in Context) dataset and fine-tuned on the DFC2023 Track 2 dataset with the following hyperparameters and strategies. The dataset was split into training and validation sets using an 80%20% ratio to ensure robust evaluation while maintaining sufficient sample diversity for learning. Model Size: 62.1M Input size: 512512 pixels Batch size: 8 Optimizer: Rectified Adam (RAdam) Epochs: 300 Learning rate: 1.0 105 with cosine decay Weight Decay: 0.0005 Loss function: Combination of box loss, mask loss, classification loss, and Distribution Focal Loss (DFL) To address class imbalance, we apply focal loss [7] and class weighting during training based on custom weighted [9], which assigns higher sampling probabilities dataloader to images containing rare classes, based on inverse class frequency. This way, the model sees underrepresented classes more often during training, leading to better performance without changing the loss function or removing any data. All experiments were performed using an NVIDIA GeForce RTX 2080 Ti with CUDA version 12.4. IV. RESULTS AND EVALUATION A. Model Training Performance The YOLOv11 model demonstrated strong performance for building height classification despite the challenges of class imbalance and the complexity of instance segmentation. Training results demonstrated consistent improvement across 300 epochs, with all (box, segmentation, classification, and DFL) exhibiting steady the training process. Comprehensive reduction throughout hyperparameter optimization evaluated training durations from 100-500 epochs alongside varying learning rates, batch sizes (4-16), and optimizer configurations. Analysis confirmed that components loss Fig. 2. Training and validation metrics for building instance segmentation. Loss functions (Box Loss, Segmentation Loss, Classification Loss, DFL) are reported for both training and validation sets. Fig. 3. Training metrics for building instance segmentation (Precision, Recall, mAP@50, mAP@5095) are reported for training set only 300 epochs achieved optimal convergence, where validation loss plateaued without overfitting, as validated by mAP@50 metrics stabilizing within 0.5% variance over the final 50 epochs. The training set performance metrics, summarized in Table II, demonstrate strong detection and segmentation capabilities of the model. Notably, bounding box predictions achieved precision of 85% and mAP@50 of 84%, while mask predictions showed slightly lower recall (75%) but comparable mAP@50 (83%). These results establish robust baseline for evaluating generalization to the validation set. As shown in Table IV, mask recall (Recall(M)) is consistently lower than bounding box recall (Recall(B)), which is expected in instance segmentation [10], [11]. Bounding box recall uses less stringent IoU threshold (typically 0.5), while mask recall requires precise pixel-level alignment, making TABLE II TRAINING SET PERFORMANCE METRICS FOR BOUNDING BOX (B) AND MASK (M) FOR THE FIVE HEIGHT CLASSES Metric Bounding Box (B) Mask (M) Precision 0.85 0.84 Recall mAP@50 mAP@5095 0.79 0.75 0.84 0. 0.68 0.60 TABLE III VALIDATION SET RESULTS FOR BUILDING SEGMENTATION ONLY Metric LIGHT [2] HGNet [3] Ours mAP@50 0.57 0.73 0.84 mAP@5095 0.25 0.45 0.56 it more sensitive to boundary inaccuracies. This reflects the greater challenge of accurate mask delineation compared to object localization. the full Figures 2 and 3 present training dynamics of YOLOv11 in the context of building instance segmentation. The loss curves show rapid convergence within the first 150 epochs, with Box Loss decreasing from 1.0728 to 0.6272 and DFL Loss following similar trendindicating effective boundary refinement through distribution focal learning. Both detection and segmentation branches exhibit stable learning behavior: precision for bounding boxes (Prec.(B) = 85%) and mean Average Precision at 0.50 IoU ( Intersection over Union) threshold (mAP@50(B) = 84%) stabilize early in training, reflecting the models ability to learn accurate object localization from polygon-based annotations. Similarly, masklevel metrics such as Prec.(M) = 83% and mAP@50(M) = 64% demonstrate consistent performance, although slightly lower than their bounding box counterparts due to the increased complexity of pixel-wise segmentation. Validation losses remain within 1520% of their training values, indicating minimal overfitting and strong generalization to unseen urban layouts. These results confirm that YOLOv11 achieves robust convergence and spatial fidelity when applied to complex satellite imageryparticularly in dense urban environments where precise instance delineation is critical. The overall performance metrics, such as precision, recall, and mean Average Precision (mAP), demonstrate consistent improvement throughout training. Notably, the model achieves strong results in both bounding box (B) and mask (M) evaluations, highlighting its effectiveness in joint building instance segmentation and height classification. B. Instance Segmentation and Height Classification Performance on the Validation Set We evaluate our model using standard segmentation metrics. The instance segmentation branch of YOLOv11 delivers exceptional performance. Our proposed model attains 84.2% mAP@50 and 56% mAP@5095 for building segmentation on the validation set of DFC23, outperforming leading multitask frameworks such as LIGHT and HGDNet as shown in Table III. Remarkably, these gains are achieved without Fig. 4. Qualitative results on DFC2023 validation set: (left) original imagery, (middle) predicted segmentation with height classification, (right) ground truth. resorting to complex feature-distillation or teacherstudent training schemes, demonstrating that straightforward, endto-end design can outperform state-of-the-art approaches. For height estimation, we evaluate our YOLOv11-based model using standard instance segmentation metrics across all five height classes, as shown in Table IV. The framework achieves strong detection and segmentation performance while preserving meaningful categorical distinctions between lowrise, mid-rise, and high-rise buildings. Overall, we obtain 61.2% mAP@50(B) and 60.4% mAP@50(M), demonstrating accurate localization and boundary delineation without explicit regression to continuous height values. At stricter IoU thresholds (mAP@5095), performance remains robust at 49.0% for bounding boxes and 38.3% for masks, indicating stable generalization across varying object scales and shapesparticularly important in dense urban scenes where occlusions and irregular building forms are common. Per-class analysis reveals consistent accuracy across height tiers, despite significant class imbalance: Class 5 (41+ m) constitutes only 3.1% of the dataset, yet reaches 67.5% mAP@50(B) and 66.7% mAP@50(M). This confirms the effectiveness of focal loss and adaptive weighting in maintaining sensitivity to rare structures. Class 1 (010 m), the most frequent category (35.4%), attains 59.2% mAP@50(B) and 58.7% mAP@50(M), showing that the model maintains precision without overfitting to dominant classes. The results in Figure 4 demonstrate the effectiveness of our TABLE IV VALIDATION SET PERFORMANCE METRICS ACROSS ALL CLASSES Class Images Buildings Prec.(B) Recall(B) mAP@50(B) mAP@5095(B) Prec.(M) Recall(M) mAP@50(M) mAP@5095(M) All 1 2 3 4 5 177 176 162 138 80 43 12505 4421 3499 2980 1222 383 0.615 0.659 0.625 0.636 0.544 0.611 0.541 0.489 0.512 0.583 0.487 0.634 0.612 0.592 0.599 0.650 0.546 0. 0.490 0.426 0.452 0.534 0.452 0.588 0.605 0.651 0.611 0.624 0.542 0.598 0.532 0.483 0.500 0.571 0.484 0.621 0.604 0.587 0.586 0.637 0.541 0.667 0.383 0.337 0.349 0.416 0.357 0.456 YOLOv11-based framework in achieving precise instance segmentation and accurate height classification for buildings. The model successfully delineates building footprints with high fidelity, as evidenced by the close alignment between predicted and ground truth masks across diverse urban environments. These results validate that discrete height modeling not only avoids the instability of direct regression but also provides actionable outputs aligned with real-world urban planning requirements. By learning from normalized polygon annotations directly, our method retains spatial fidelity and supports precise height-tier prediction, even under noisy or incomplete DSM conditions. In conclusion, this work confirms discrete height modelings practical advantages over continuous regression, particularly for handling label noise and measurement uncertainty in realworld remote sensing data. The YOLOv11-based framework provides an efficient, deployable solution for semantic urban reconstruction supporting infrastructure monitoring and municipal planning. Future research will introduce novel multiscale attention mechanism for cross-sensor height estimation and deliver the highest impact in urban remote sensing. ACKNOWLEDGMENT This work is supported in part by TUBITAK Grant Project No: 5230108. V. CONCLUSION REFERENCES This study demonstrates the effectiveness of YOLOv11 for joint building instance segmentation and discrete height classification from satellite imagery. By reframing height estimation as structured classification task rather than continuous regression, we achieve improved robustness to noisy Digital Surface Model (DSM) readings and enhanced interpretability for urban planning applications such as zoning, risk modeling, and 3D city reconstruction. Our preprocessing pipeline successfully converts raw DSM data into YOLOv11-compatible annotations by computing mean height per mask and mapping it to one of five predefined height classes. This approach enables seamless integration with modern object detection frameworks. The model achieves 84.2% mAP@50 and 56% mAP@5095 for building instance segmentation, surpassing state-of-the-art multitask frameworks like LIGHT and HGDNet without requiring complex feature distillation or teacher-student training schemes. Furthermore, our method shows strong performance in discrete height classification, particularly for rare high-rise buildings (Class 5), where we achieve 67.5% mAP@50(B) and 66.7% mAP@50(M) despite Class 5 constituting only 3.1% of the dataset. These results validate that focal loss and adaptive class weighting effectively mitigate class imbalance while maintaining high spatial fidelity and detection accuracy. YOLOv11s decoupled head design, enhanced backbone, and real-time inference capabilities make it well-suited for large-scale urban mapping tasks. Its native support for multiclass instance segmentation allows direct integration with the DFC2023 Track 2 benchmark, enabling scalable deployment and precise categorical modeling of building heights. [1] G. Xia, C. Persello, G. Vivone, K. Chen, Z. Yan, D. Tang, H. Huang, M. Schmitt, and X. Sun, The 2023 IEEE GRSS Data Fusion Contest: Large-Scale Fine-Grained Building Classification, IEEE Geoscience and Remote Sensing Magazine, vol. 11, no. 1, 2023. [2] Y. Mao, X. Sun, X. Huang, and K. Chen, LIGHT: Joint Individual Building Extraction and Height Estimation from Satellite Images Through Unified Multitask Learning Network, in Proc. IGARSS, 2023, pp. 53205323. [3] G. Liu, Z. Yan, M. Tang, and S. Li, HGDNet: Hierarchical Guided Distillation Network for Joint Building Extraction and Height Estimation, in Proc. CVPR, 2023. [4] C. Stipek, J. Epting, D. Adams, V. Lebakula, T. Hauser, R. Stewart, C. Brelsford, and A. Ross, Empirically Categorizing the Built Environment in Relation to Height, in Proc. IEEE Int. Conf. Big Data (BigData), 2024, pp. 58475856. [5] H. Huo, Y. Li, M. Gong, H. Yang, and Z. Li, Joint Building Footprint Segmentation and Height Estimation from Single Satellite Image, arXiv preprint arXiv:2304.01090, 2022. [6] R. Khanam and M. Hussain, YOLOv11: An Overview of the Key Architectural Enhancements, arXiv preprint arXiv:2410.17725, 2024. [7] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, Focal Loss for Dense Object Detection, arXiv preprint arXiv:1708.02002, 2017. [8] N. Jegham et al., Evaluating the Evolution of YOLO (You Only Look Once) Models: Comprehensive Benchmark Study of YOLOv11 and Its Predecessors, arXiv preprint arXiv:2411.00201v1, 2024. [9] M. Yasin, Balance Classes During YOLO Training Using Weighted Dataloader, Yasins Keep, Blog post, Sep. 1, 2024. [Online]. Available: https://y-t-g.github.io/tutorials/yolo-class-balancing/ [10] K. He, G. Gkioxari, P. Dollar, and R. Girshick, Mask R-CNN, in Proc. ICCV, 2017, pp. 296309. [11] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollar, Panoptic Segmentation, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 94049413. [12] A. Chaurasia, G. Jocher, and N. Edgar, You Only Look Once Version 11 (YOLOv11): Real-Time Object Detection and Instance Segmentation, Ultralytics Whitepaper, 2024. [Online]. Available: https://github.com/ ultralytics/ultralytics/tree/main/docs"
        }
    ],
    "affiliations": [
        "Huawei Turkiye R&D Center",
        "Istanbul Medipol University",
        "Ozyegin University"
    ]
}