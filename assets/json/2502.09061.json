{
    "paper_title": "CRANE: Reasoning with constrained LLM generation",
    "authors": [
        "Debangshu Banerjee",
        "Tarun Suresh",
        "Shubham Ugare",
        "Sasa Misailovic",
        "Gagandeep Singh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO."
        },
        {
            "title": "Start",
            "content": "CRANE: Reasoning with constrained LLM generation Debangshu Banerjee 1 * Tarun Suresh 1 * Shubham Ugare 1 Sasa Misailovic 1 Gagandeep Singh"
        },
        {
            "title": "Abstract",
            "content": "Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO. 5 2 0 2 3 1 ] . [ 1 1 6 0 9 0 . 2 0 5 2 : r 1. Introduction Transformer-based large language models (LLMs) are widely used in AI systems that interact with traditional software tools like Python interpreters (OpenAI, 2024; Chen et al., 2023) for code generation (Suresh et al., 2024a;b; *Equal contribution 1Department of Computer Science, University of Illinois Urbana-Champaign, USA. Correspondence to: Debangshu Banerjee <db21@illinois.edu>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Jiang et al., 2024), logical solvers (Pan et al., 2023; Olausson et al., 2023), and theorem provers (Wu et al., 2022; Yang et al., 2023). These tools impose specific syntactic and semantic constraints on their inputs, requiring LLMs to produce outputs in the correct format. For instance, if an LLM provides output to specific logical solver (Han et al., 2024), the output must be parsable by that solver. Similarly, Wolfram Alpha (wolfram, 2024) translates user queries about mathematical problems into domain-specific language (DSL) to utilize symbolic solvers. However, as highlighted in recent studies (Ugare et al., 2024b; Lundberg et al., 2023; Poesia et al., 2022), pre-trained LLM outputs do not always comply with downstream tools input requirements. Constrained decoding algorithms (Ugare et al., 2024b; Poesia et al., 2022) address this issue by projecting the LLM output onto user-specified formal constraints (e.g., syntactic rules defined by context-free grammar G), thereby ensuring that the input requirements of downstream tasks are satisfied. As illustrated in Fig. 1, constrained decoding improves the syntactic correctness of LLM outputs (e.g., generating well-formed mathematical expression). However, it does not guarantee functional correctness (e.g., ensuring the expression correctly answers the users query). Recent works such as Tam et al. (2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. Tam et al. (2024) attributes this reduction in functional accuracy to decline in the LLMs reasoning capabilities under constrained decoding. This observation raises the following open questions: RQ1: Do LLMs truly lose reasoning capabilities under constrained decoding? RQ2: How can we leverage the benefits of constrained decoding in reducing syntax errors while preserving the unconstrained reasoning capabilities of LLMs? Key Challenges: First, we need to formally identify the root cause of the reduction in functional accuracy of endto-end systems when pre-trained LLM operates under constrained generation. Unlike the empirical observations in (Tam et al., 2024), we seek formal justification for this reduction that is not limited to specific LLMs used in experiments but extends to any LLM, including more CRANE: Reasoning with constrained LLM generation Figure 1. An example from the GSM-symbolic dataset (variables in blue) where unconstrained generation produces syntactically incorrect output, while constrained generation provides syntactically valid but incorrect answer. CRANE, however, generates correct answer. powerful ones developed in the future. Second, we must design cost-efficient decoding strategies that address the shortcomings of existing constrained decoding methods while improving functional accuracy. In this work, we do not consider task-specific fine-tuning of LLMs, as fine-tuning for each task is compute-intensive. Unlike constrained decoding, fine-tuning does not guarantee that the LLM output adheres to formal constraints. Contributions: We make the following contributions to improve the functional accuracy of the end-to-end system: We theoretically show that LLMs with constant number of layers, which are known to be capable of simulating steps of any given Turing machine with O(n) reasoning steps (Merrill & Sabharwal, 2024), can only solve problems within relatively restrictive circuit complexity class when constrained to generate outputs that always conform to restrictive grammar defining only the valid output strings. This demonstrates that, for restrictive grammar, constrained decoding reduces the problem-solving capabilities of LLMs. We theoretically show that the loss of expressivity of LLMs under constrained decoding arises because the output grammar is too restrictive to accommodate the intermediate reasoning steps required to compute the answer. We further demonstrate that augmenting the grammar with specific additional production rules enables the LLM to generate the intermediate reasoning steps while ensuring that the final output always adheres to the intended output structure. With the augmented grammar Ga, the LLM retains its expressivity under constrained decoding. We propose simple and cost-efficient decoding strategy, CRANE (Constrained Reasoning Augmented Generation). CRANE effectively alternates between unconstrained generation for reasoning and constrained generation for producing structurally correct outputs. This allows the model to produce syntactically valid outputs while enabling the LLM to reason. Our detailed experiments on multiple open-source LLMs and benchmarks demonstrate that CRANE significantly outperforms both SOTA constrained decoding strategies and standard unconstrained decoding, showing up to 10% improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic (Mirzadeh et al., 2024)) and FOLIO (Han et al., 2024). Next, we provide the notations and necessary background on constrained decoding, including the definition of Turing machines and relevant circuit complexity classes. 2. Preliminaries Notations: In the rest of the paper, we use small case letters (x) for constants, bold small case letters (xxx) for strings, capital letters for functions, for string concatenation, xxx to dentone the length of the string xxx. We use LLM to refer to transformer-based LLMs with fixed number of layers. 2.1. Constrained LLM Decoding Autoregressive language models decode output iteratively by generating tokens from probability distribution over the vocabulary . The distribution is derived by applying the softmax function to the models scores S. Common decoding methods include greedy decoding, temperature sampling, and beam search. Constrained LLM decoding extends this process by excluding specific tokens at certain positions, such as avoiding harmful words or adhering to user-defined output grammar for languages like JSON or SQL (Poesia et al., 2022; Ugare et al., 2024c; Willard & Louf, 2023). At each decoding step, binary mask {0, 1}V , generated by function fm, specifies valid tokens CRANE: Reasoning with constrained LLM generation (mi = 1) and excluded tokens (mi = 0). Decoding is then performed on the masked probability distribution softmax(S), where denotes element-wise multiplication. 2.2. Deterministic LLM Decoding CRANE is compatible with various decoding strategies, both constrained and unconstrained, allowing the output of to be stochastic. However, following existing works (Hahn, 2020; Merrill & Sabharwal, 2023; Li et al., 2024) and for simplicity in the theoretical setup in Section 3, we assume that the output of on any input string xxx is deterministic in both constrained and unconstrained settings. Similar to prior works (Merrill & Sabharwal, 2023; 2024), we model single autoregressive step as deterministic function Lf that predicts the next token given specific input. Formally, Definition 2.1 (Deterministic LLM Step). single autoregressive step of an LLM is modeled as deterministic function Lf : , where is the finite vocabulary and represents the set of all finite strings over . For an input string xxx , the LLM predicts the next token Lf (xxx). Definition 2.2 (Deterministic Unconstrained Decoding). For an input string xxx, the deterministic output string yyy selected from the output distribution of LLM using decoding algorithm (e.g., greedy decoding) is denoted as yyy = L(xxx) where : . L(xxx) is the most likely output sequence according to learned distribution on xxx. The output yyy = L(xxx) is computed iteratively with yyy autoregressive steps defined by Lf . For each 1 yyy, and (xxx) = L(i1) the recurrence relation L(i) (xxx)) where L(0) (xxx) = xxx and denotes string concatenation. Here, xxx yyy = Lyyy (xxx). Similarly, under constrained decoding with grammar we define: (xxx)Lf (L(i1) Definition 2.3 (Deterministic Constrained Decoding under Grammar). Under constrained decoding with formal grammar G, the output string yyyG is selected from the constrained output distribution and is denoted as yyyG = LG(xxx). The output of i-th constrained autoregressive step with is xxx yyy(i) (xxx) and xxx yyyG = L(yyy) = L(i) (xxx). The constrained output yyyG is always in the grammar yyyG L(G) where L(G) is the language defined by G. For soundconstrained decoding algorithms, if the unconstrained output yyy = L(xxx) in the grammar yyy L(G), the constrained output remains unchanged, i.e., L(xxx) = LG(xxx). 2.3. LLM Expressivity We discuss the notations and background related to Turing machines, and relevant uniform circuit complexity classes. Turing machines are popular mathematical computation models used to analyze resource requirements (e.g. time and space complexity) and the hardness of computation problems. Formally, Turing machine is defined as: Definition 2.4 (Turing Machine). Turing machine with work tapes and an output tape is 8-tuple = Σ, Γ, k, b, Q, q0, δ, , where Σ is the finite input alphabet, Γ is the finite tape alphabet with Σ Γ, Γ Σ is special blank symbol, is finite set of states, q0 is the initial state, δ : (Q ) Γk+2 Γk+1 {0, +1, 1}k+2 is the transition function (where 1, 1, 0 represent moving the tape head left, right, or staying in place, respectively), and is the set of halting states. Let Σ denote the set of all finite strings over the input alphabet Σ. Given an input string sss Σ, the computation of on is sequence of configurations starting from the initial configuration. Each configuration γ is tuple containing the current state Q, the contents of the input tape, the work tapes, the output tape, and the current head positions of all + 2 tapes. For each configuration, γi (i N), the transition function δ computes the next configuration γi+1 based on the current state and the values on the + 2 tapes at the current head positions. It updates the head positions, writes to the output tape (possibly leaving it unchanged if no new symbol is written), and advances to the next configuration. For each i, computation of γi+1 from γi defines single step of the Turing machine. The computation of on input sss halts if reaches halting state . If halts, the output corresponding to sss is written on the output tape. Additional details about the computation of the Turing machine are in Appendix A. Before discussing existing expressivity results for constantlayer LLMs, we briefly introduce relevant uniform constantdepth circuit complexity classes, e.g. logspace uniformT 0, which provide an upper bound on the computational power of LLMs that do not employ reasoning steps, as seen in methods like Chain-of-Thought (Wei et al., 2022). Definition 2.5 (Boolean Circuit). Boolean circuit is computational model for evaluating Boolean functions over fixed-length binary strings. It is represented as directed acyclic graph (DAG), where the leaf nodes correspond to input binary variables or their negations, and the internal nodes perform operations from predefined set of operations (e.g., AND (), OR (), etc.). One or more marked nodes in the graph represent the circuits output. The structure of the DAG specifies the computation of the Boolean function by propagating input values through the graph. The complexity of circuit is determined by its size (the number of nodes) and depth (the longest path in the graph). Since single circuit only defines boolean CRANE: Reasoning with constrained LLM generation function for fixed-length inputs, family of circuits is requiredone for each input lengthto characterize computational problem where input lengths vary. Unlike Turing machines, whose computation does not depend on input length, circuit families have separate circuit for each input length, which can differ entirely. This non-uniformity can lead to degenerate cases where non-uniform circuit families solve undecidable problems (Arora & Barak, 2009). To address this issue, complexity theorists enforce uniformity conditions, requiring circuits for different input sizes to be related, resulting in uniform circuit families. For further details and formal definitions of circuit classes, refer to (Arora & Barak, 2009). In this work, we focus on constant-depth, polynomial-sized logspace-uniform threshold circuits (T 0), where contains only threshold gates (a formal definition is in Appendix B). 3. Expressivity of Constrained Decoding First, we show that any constant-layer LLM under constrained decoding loses expressivity. We identify the class of problems and the corresponding output grammars such that when imposed on the outputs of any constant-layer LLM, the problems cannot be solved unless there is collapse in fundamental complexity classes that are widely believed to be unequal (e.g., 0 = L)1. 3.1. Limitation of Constrained Decoding Next, we present the high-level idea behind Proposition 3.1 that shows the limitation of constrained LLM decoding when the output grammar is too restrictive. We consider problems where the number of possible outputs is finite, and thus the set of all possible outputs can be expressed as simple regular language. Consequently, Gc that encodes the output set O, i.e., = L(Gc), where L(Gc) denotes the language defined by the grammar Gc. For instance, any decision problem (yes/no answer) such as st-connectivity that asks for vertices and in directed graph, if is reachable from can be answered within single-bit output i.e. L(Gc) = {0, 1}. This implies that constrained decoding with the output grammar Gc allows only single autoregressive step for any on all inputs. series of existing works (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Merrill & Sabharwal, 2023) establish that, under suitable assumptions, single autoregressive step on an input with length for any constant-depth LLM can be represented as constant-depth circuit. Since, for decision problems, the constrained decoding step permits only single autoregressive step, any LLM can only solve problems within the corresponding circuit complexity class. We build on the most recent result from (Merrill & Sabharwal, 2023), which shows that single autoregressive step of any 1NL refers to nondeterministic log-space LLM with constant number of layers on an input of length can be simulated by logspace-uniform constant-depth threshold circuit family. This result allows the LLM to use floating-point numbers with log(n) precision when processing inputs of size n, ensuring that the precision scales with and preventing floating-point representation issues for large n. We denote such LLMs as log-precision LLMs. Let xxxyyy(i) denote the output after the i-th autoregressive step of an LLM under constrained decoding with an output grammar on input xxx. Then, we have xxx yyy(i) = L(i) (xxx), and for any i, yyy(i) is always valid prefix of string in L(G), i.e., there exists (possibly empty) string ααα(i) such that yyy(i) ααα(i) L(G). Now, for any output grammar Gc where the output set = L(Gc) is finite, we show that the output LGc(xxx) for any input xxx of size xxx = can be computed using constant-depth threshold circuits. Proposition 3.1. For any log-precision LLM with constant layers there exists logspace-uniform thershold circuit hn such that LGc(xxx) = hn(xxx) holds for all inputs xxx with size xxx = and N. Proof: The formal proof is in Appendix C. From Proposition 3.1, it follows that for any decision problem under constrained decoding, an LLM can only solve problems within the logspace-uniform 0 class (constantdepth threshold circuits). Consequently, any decision problem believed to lie outside this class cannot be solved under constrained decoding. The previously mentioned stconnectivity problem is known to be L-complete (Arora & Barak, 2009). This implies that unless 0 = L, no LLM under constrained decoding can solve st-connectivity. Additionally, (Li et al., 2024; Merrill & Sabharwal, 2024) show that given any Turing machine there exists logprecision LLM with constant number of layers that can simulate O(t(n)) steps of using O(t(n)) autoregressive steps, where t(n) denotes polynomial in the input size n. Lemma 3.2. For any Turing machine with tape alphabet Γ, there exists constant depth LLM LM with finite vocabulary Γ VM and log-precision that can simulate t(n) steps of with t(n) autoregressive steps. Proof: The proof follows from Theorem 2 in (Merrill & Sabharwal, 2024) further details in Appendix C. Proposition 3.1 and Lemma 3.2 together imply that there exist problems, such as st-connectivity, an LLM can solve that in an unconstrained setting but cannot be solved under constrained decoding (unless logspace-uniform 0 = L). 3.2. Reasoning with Augmented Grammar The reduction in LLM expressivity under constrained decoding, as established in Proposition 3.1, arises primarily because the language of all valid output strings, L(Gc), 4 CRANE: Reasoning with constrained LLM generation is too restrictive and does not permit large (non-constant) reasoning chains. This naturally leads to the question of whether it is possible to augment any output grammar with additional production rules to construct an augmented grammar Ga that can accommodate reasoning steps while preserving the expressivity of even under constrained decoding. At the same time, Ga should remain nontrivialmeaning it should not accept all possible strings, as in the unconstrained settingso that it aligns with the practical objective of constrained decoding: guiding the LLM to generate syntactically and semantically valid outputs. To achieve this, we enforce that the augmented grammar Ga always follows the structure Ga RG, where the nonterminal symbol captures the reasoning steps, and represents the final output. This guarantees that for any string sss L(Ga), the final answer aaa extracted from sss = rrr aaa always belongs to the original output grammar G, i.e., aaa L(G), with rrr serving as the reasoning sequence leading up to the final output. Formally, we show that for any Turing machine and grammar containing all valid outputs of , there exists an LLM LM with constant number of layers and logprecision, along with an augmented grammar Ga in the specified format, such that LM can simulate t(n) steps of using t(n) autoregressive steps under constrained decoding with Ga. Here and t(n) is polynomial over n. The augmented grammar Ga may not be unique, and we provide one such construction. At high level, LM simulates the Turing machine by computing the encoded representations γi of the machines configurations γi at each step and storing them within the reasoning component (i.e., the string rrr) of the output. During each autoregressive step, LM generates the next configuration based on the transition function of and appends its encoding to the reasoning sequence. This process continues until reaches halting state, at which point LM produces the final output aaa, which belongs to L(G). For any given , we define the rules RM that can parse the encodings γ of all possible configurations γ. This ensures that the output LGa(xxx) represents the full reasoning-augmented sequence, i.e., γ1 γt(n) (xxx), where (xxx) is the final output of on input xxx of size after t(n) computational steps. The encodings γ1, . . . , γt(n) correspond to the configurations γ1, . . . , γt(n), as described below. We begin by defining the vocabulary VM for LM , which contains all tape symbols Γ of along with finite set of auxiliary symbols γ that encode the corresponding configurations γ. Similar to prior works (Merrill & Sabharwal, 2024), each configuration encoding γ represents the current state q, the symbols at the current head position of + 2 tapes (input, output and work tapes), and the head movement directions {0, +1, 1} for each tape. Directions {0, +1, 1} denote either staying in place (0), moving left (1), or moving right (+1) by single position. Since the set of states Q, the tape alphabet Γ, and the number of tapes are all constants, the total number of possible encodings γ is also constant. Let Γ denote the set of all possible configuration encodings, i.e., Γ = {γ(1), . . . , γ(l)}, where = Γ. Given Γ is finite and enumerable, we can define the rules of the augmented grammar Ga accordingly as follows. Ga RM G; RM SRM ; γ(1) γ(l) The set of reasoning strings in L(RM ) essentially define regular language over the configuration encodings Γ. Let, for any input xxx with size = xxx given Turing machine halts and compute the output (xxx) in t(n) steps that are polynomial in n. Then there exist LM compute (xxx) with t(n) autoregressive steps under constrined decoding with the augmented grammar Ga RM G. Suppose, LM,Ga (xxx) denotes the output of the LLM LM on input xxx under constrained decoding with grammar Ga then Proposition 3.3. For any Turing machine with tape alphabet Γ, there exists constant depth LLM LM with finite vocabulary Γ VM and log precision such that for any input xxx with xxx = n, LM,Ga (xxx) = rrr (xxx) with assuming halts on xxx in t(n) steps. Proof: The proof is in Appendix C. 4. CRANE Algorithm Given any Turing machine , Proposition 3.3 establishes that constrained decoding with the augmented grammar Ga on specific LLM LM can simulate the computation of . However, this result does not directly translate into practical constrained decoding algorithm that preserves the expressivity of general LLMs. The construction assumes specific LLM LM with the vocabulary VM and knowledge of the particular Turing machine for defining the rules RM . In practice, we require an efficient approach that can be applied to diverse open-source LLMs, various grammars, and different constrained decoding algorithms. Importantly, we know that enforcing the output grammar from the beginning can limit expressivity. Instead, we impose grammar constraints judiciously to avoid restricting the LLMs reasoning capabilities. For example, in the case of reasoning-augmented output of the form γ1 γt(n) (xxx), we apply constrained decoding only from the t(n) + 1-th autoregressive step onward, ensuring that the reasoning process remains unrestricted while the final answer adheres to the desired grammar. The primary challenge here is deciding when to transition between an unconstrained generation for reasoning and 5 CRANE: Reasoning with constrained LLM generation constrained generation. For instance, grammar for generalpurpose programming languages such as Python can allow any text string at the start (e.g. program starting variable names) making it hard to detect the end of reasoning string. To avoid this, we augment the output grammar with specific delimiter symbols S1 and S2 that mark the start and end of the constrained generation. We incentivize the LLM to generate these delimiters via explicit instructions in the prompt and few-shot examples. This aligns with common general-purpose LLMs that already use specific delimiters such as backticks () for programs like python, SQL, and (<<, >>) to enclose math-expression blocks. This approach allows simple and cost-efficient approach for detecting the transitions to and from constrained decoding. For the construction in the previous section, in this setup, we will generate the string rrr S1 (xxx) S2 where the reasoning rrr is generated unconstrained and the LLM moves to constrained mode after seeing the symbol S1. However, in practical cases, the delimiters may be generated multiple times (ie. for intermediate operations), even during the reasoning step. Therefore, upon encountering the end symbol S2, we switch back to unconstrained generation to avoid unnecessarily restricting the output. Algorithm 1 CRANE Algorithm 1: Input: LLM, tokens, CSD (constrained decoder), (output grammar), S1 (start delimiter), S2 (end delimiter) currGen detokenize(tokens[pointer :]) if S1 currGen then isConstrained True isConstrained False if isConstrained then constrained extractConstrained(currGen) ti LLM(tokens) CSD(constrained) else 2: Output: Output string 3: S1GS2 4: CSD.INITIALIZE(G) 5: pointer len(tokens) 6: isConstrained False 7: while True do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: ti LLM(tokens) tokens tokens + ti if ti = EOS then 18: 19: 20: 21: 22: 23: 24: 25: return detokenize(tokens) break else if isConstrained then constrained constrained + detokenize(ti) if constrained.endswith(S2) then pointer len(tokens) Figure 2. CRANE adaptively switches between constrained LLM generation and unconstrained LLM generation based on start and end delimiters (in this example << and >>). Using these delimiters, CRANE dynamically tracks which windows (highlighted in the figure) of the LLM generation constraints should be applied to. We implement our approach into the CRANE algorithm (Algo 1), which extends standard autoregressive LLM generation. CRANE takes an arbitrary LLM, constrained decoding algorithm (denoted as CSD), output grammar G, and symbols S1 and S2 as input. It first initializes CSD with G, the output grammar augmented with S1 and S2. CRANE starts in unconstrained generation and maintains pointer that marks the start of the current window of LLM generation following the last constrained generation. In each iteration, the algorithm checks if S1 is present in the current generation window currGen, which is the portion of the sequence from the current pointer position onwards. If S1 is detected, CRANE switches to constrained generation mode. In this mode, the current constrained window (the portion of currGen that is in G) is extracted, and the next token is sampled based on the constraints defined by the CSD. If S1 is not present, the next token is computed directly without any constraints applied. Additionally, if the current constrained window ends with S2, the pointer is updated to the length of the current token sequence, effectively switching back to unconstrained generation until S1 is generated again. Figure 2 further illustrates LLM generation with CRANE. The underlined portion of the LLM generation represents currGen, and the current constrained window is highlighted in yellow. 5. Evaluation In this section, we evaluate CRANE on math reasoning task (GSM-Symbolic (Mirzadeh et al., 2024)) and logical reasoning task (FOLIO (Han et al., 2024)) and demonstrate significant improvement over both unconstrained and SOTA constrained generation baselines. Experimental Setup. We run experiments on 48-core Intel Xeon Silver 4214R CPU with 2 NVidia RTX A5000 GPUs. CRANE is implemented using PyTorch (Paszke et al., 2019) and the HuggingFace transformers library (Wolf et al., 2020). Our primary baseline for unconstrained generation is Chain-of-Thought (CoT) Prompting (Wei et al., CRANE: Reasoning with constrained LLM generation Table 1. Comparison of CRANE and baselines with different models on GSM-Symbolic. Model Method Acc. (%) Parse (%) Tokens Qwen2.5-1.5B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Math-7B-Instruct Llama-3.1-8B-Instruct Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained DeepSeek-R1-Distill-Qwen-7B Unconstrained CoT CRANE Unconstrained w/o CoT Constrained DeepSeek-R1-Distill-Llama-8B Unconstrained CoT CRANE 21 22 26 36 35 37 39 27 29 29 38 21 26 30 33 18 20 24 29 12 13 21 31 97 97 90 94 99 88 94 89 99 82 94 73 98 95 95 89 99 89 92 77 96 87 92 23.34 25.29 128.97 131. 17.92 25.28 138.38 155.32 25.7 26.81 155.26 158.86 128.38 35.97 163.55 170.22 21.64 17.21 212.24 235.78 29.2 16.89 250.83 268.82 2022), which enables LLMs to decompose and reason about problem through series of intermediate steps before outputting the final answer. Furthermore, we run constrained semantic generation for GSM-Symbolic (Mirzadeh et al., 2024) with the ITERGEN library (Ugare et al., 2024a) and use the SYNCODE framework for FOLIO (Han et al., 2024) evaluation. In all experiments, CRANE is initialized with the same constrained decoders and uses the same constraints as the constrained generation baselines. GSM-Symbolic: We first evaluate CRANE on GSMSymbolic (Mirzadeh et al., 2024), dataset consisting of math word problems designed to assess LLMs mathematical reasoning skills. In the word problems, names and numerical values are replaced with symbolic variables, and the LLMs are tasked with generating correct symbolic expression solutions (see Appendix C.1 for examples). To evaluate correctness, we extract the final expressions from the LLM generations and verify if they are functionally equivalent to the ground truth expressions with the Z3 solver (De Moura & Bjørner, 2008). We compare CRANE against three baselines: (1) unconstrained generation without chain-of-thought prompting, (2) unconstrained generation with CoT, and (3) constrained generation. We use ITERGEN for the constrained generation baseline and also initialize CRANE with ITERGEN. For ITERGEN and CRANE, we enforce syntactic constraints via the context-free grammar provided in Appendix C.5.1 and apply the semantic constraint ensuring that generated expressions contain only valid problem-defined variables. Since ITERGEN uses selective rejection sampling to enforce semantic constraints, we also include comparisong against unconstrained generation with sampling in Table 4 in the Appendix. For CRANE, we use << and >> for the delimeters S1 and S2, respectively. We evaluate Qwen2.5-1.5B-Instruct (Qwen, 2024), Qwen2.5-Math-7BInstruct (Qwen, 2024), Qwen2.5-Coder-7B-Instruct (Qwen, 2024),Llama-3.1-8B-Instruct (Llama, 2024), DeepSeek-R1Distill-Qwen-7B (DeepSeek-AI et al., 2025), and DeepSeekR1-Distill-Llama-8B (DeepSeek-AI et al., 2025). We use greedy decoding with maximum new token limit of 600 and prompt the LLMs with the 8-shot examples from GSMSymbolic (Mirzadeh et al., 2024) (the prompts can be found in Appendix C.1). Table 1 compares the performance of CRANE with the baseline methods. The Accuracy (%) column reports the percentage of functionally correct LLM-generated expres7 CRANE: Reasoning with constrained LLM generation We compare CRANE against grammar-constrained generation with SYNCODE using the Prover9 grammar (Appendix C.5.2). The Prover9 grammar divides FOL formulas into Predicates, Premises, and Conclusions and allows intermediate reasoning in comments (an example can be found in Appendix C.2). We also compare CRANE against unconstrained generation with CoT. For all approaches and models, we run greedy decoding with maximum new tokens limit of 800 and use 2 few-shot examples in the prompt. We also compare CRANE against unconstrained CoT with temperature sampling in Table 5 in the Appendix. Table 2 presents the results of our experiment. The Accuracy (%) column in the table reports the percentage of functionally correct FOL translations while the Compiles (%) column reports the percentage of FOL formulas extracted from LLM output that are syntactically valid and compile into Prover9 program. CRANE outperforms the unconstrained and constrained generation baselines for all models evaluated. Limitation: Our work has the following limitations. First, Proposition 3.1 only demonstrates reduction in expressivity when the language L(Gc) is finite. This leaves open the question of whether Proposition 3.1 can be extended to grammars where L(G) is infinite. Second, CRANE for constrained decoding relies on existing tools (Ugare et al., 2024b) that require access to output logits, rendering CRANE inapplicable to models that do not expose logits. 6. Related Works Constrained LLM Decoding: Recent works have introduced techniques to enforce LLM generations to adhere to context-free grammar using constrained decoding (Ugare et al., 2024c; Willard & Louf, 2023; Beurer-Kellner et al., 2024; Melcer et al., 2024a). Additionally, Poesia et al. (2022); Ugare et al. (2024a) have extended grammar-guided generation to incorporate task-specific semantic constraints. These approaches demonstrate that constrained decoding can improve the syntactic and semantic quality of LLM outputs for various structured generation tasks. More recently, Tam et al. (2024) demonstrated that constrained structured generation can negatively impact the quality of generated outputs. Similarly, Park et al. (2024) showed that greedily masking out tokens that do not lead to valid string during next-token prediction can distort the output distribution, causing it to deviate from the true distribution of all grammatically valid outputs of for given input. To mitigate the distortion introduced by the greedy masking approach, these grammar aligned methods (Park et al., 2024; Melcer et al., 2024b) use trie to track previous generations, reducing generation divergence iteratively. However, they are computationally expensive and require large no. of resamplings per prompt to converge. Figure 3. Accuracy (%) of Qwen2.5-Math-7B-Instruct By Method and Number of Shots on GSM-Symbolic sions, Parse (%) indicates the percentage of syntactically valid expressions (i.e., expressions without invalid operations), and Tokens provides the average number of tokens generated. As shown in the table, CRANE consistently improves functional correctness across all evaluated models. For example, with the Qwen2.5-Math-7B-Instruct model, CRANE achieves 38% accuracy, outperforming both constrained generation and unconstrained generation with CoT, which achieves 29% accuracy. Similarly, with the Qwen2.5-1.5BInstruct model, CRANE achieves 31% accuracy5 percentage points higher than an unconstrained generation with CoT and 9 percentage points higher than constrained generation. Moreover, CRANE significantly enhances the syntactic correctness of generated expressions compared to unconstrained generation. Notably, none of the expressions generated using CRANE contain syntax errors, whereas 10% of the expressions from unconstrained generation with CoT do. Although, for several instances, CRANE produces slightly more syntax errors than purely constrained generation, it offers substantial improvement in functional correctness over this baseline. Ablation Study on Few-shot examples: We evaluate CRANE and baselines on varying numbers of few-shot examples in the prompt and display the results for Qwen2.5Math-7B-Instruct in Figure 3. Results for all models are presented in Table 3 in the Appendix. CRANE consistently achieves higher accuracy on GSM-Symbolic than the baselines for all evaluated numbers of few-shot examples. FOLIO: We further evaluate CRANE on the validation split of FOLIO dataset, which comprises 203 expert-written natural language reasoning instances and corresponding first-order logic (FOL) annotations. We evaluate the ability of LLMs to correctly translate the natural language reasoning instances into FOL formulas and leverage Prover9 (McCune, 20052010) FOL solver to verify the correctness of the LLM-generated FOL formulas. CRANE: Reasoning with constrained LLM generation Table 2. Comparison of CRANE and baselines with various models on FOLIO. Model Method Acc. (%) Compiles (%) Tokens Qwen2.5-Math-7B-Instruct Constrained"
        },
        {
            "title": "Unconstrained CoT",
            "content": "Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct CRANE"
        },
        {
            "title": "Unconstrained CoT\nConstrained\nCRANE",
            "content": "18.72 28.08 31.03 36.95 37.44 42.36 32.02 39.41 46.31 54.19 76.85 75.86 70.94 87.68 87.68 57.14 86.21 85. 629.59 679.44 690.17 350.64 775.62 726.88 371.52 549.75 449.77 In contrast, our work focuses on the fundamental question of the theoretical expressivity of any constant layered constrained LLM, even under an ideal constrained decoding algorithm, and uses the insights to propose practical solution. We propose an adaptive constrained decoding approach that can support various constrained decoding methods, including grammar-aligned techniques while preserving the LLMs expressivity by reasoning chains. LLM Expressivity: (Strobl et al., 2024) provides detailed survey of existing results from the perspective of formal language theory and complexity classes. series of existing works (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Merrill & Sabharwal, 2023) establish that, under suitable assumptions, single autoregressive step on an input of any length for constant-depth LLM can be represented as constant-depth Boolean circuit. (Merrill & Sabharwal, 2024; Li et al., 2024) show that the expressivity of LLMs significantly improves under popular reasoning approaches like Chain of Thought (CoT) (Wei et al., 2022), where LLMs take intermediate steps before generating the final answer. To the best of our knowledge, there is no prior work on LLM expressivity under grammar constraints. 7. Conclusion In conclusion, tasks requiring both syntactic and semantic correctness, such as code generation and symbolic math reasoning, benefit significantly from constrained decoding strategies. However, strict enforcement of constraints can hinder LLM reasoning capabilities. Theoretically, we demonstrate why restrictive grammars diminish reasoning and show that augmenting grammars with carefully designed rules preserves reasoning while maintaining correctness. Building on these insights, our proposed reasoning-augmented constrained decoding algorithm, CRANE, achieves state-of-the-art performance, with up to 10% improvement on symbolic reasoning benchmarks such as GSM-symbolic and FOLIO, effectively balancing the strengths of constrained and unconstrained generation. 9 CRANE: Reasoning with constrained LLM generation 8. Impact and Ethics This paper introduces research aimed at advancing the field of Machine Learning. We do not identify any specific societal consequences of our work that need to be explicitly emphasized here."
        },
        {
            "title": "References",
            "content": "Arora, S. and Barak, B. Computational Complexity: Modern Approach. Cambridge University Press, USA, 1st edition, 2009. ISBN 0521424267. Beurer-Kellner, L., Fischer, M., and Vechev, M. Guiding llms the right way: Fast, non-invasive constrained generation, 2024. Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. ISSN 28358856. URL https://openreview.net/forum? id=YfZ4ZPt8zd. De Moura, L. and Bjørner, N. Z3: an efficient smt In Proceedings of the Theory and Practice of solver. Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS08/ETAPS08, pp. 337340, Berlin, Heidelberg, 2008. Springer-Verlag. ISBN 3540787992. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Hahn, M. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156171, 2020. doi: 10. 1162/tacl 00306. URL https://aclanthology. org/2020.tacl-1.11/. Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Zhou, W., Coady, J., Peng, D., Qiao, Y., Benson, L., Sun, L., Wardle-Solano, A., Szabo, H., Zubova, E., Burtell, M., Fan, J., Liu, Y., Wong, B., Sailor, M., Ni, A., Nan, L., Kasai, J., Yu, T., Zhang, R., Fabbri, A. R., Kryscinski, W., Yavuz, S., Liu, Y., Lin, X. V., Joty, S., Zhou, Y., Xiong, C., Ying, R., Cohan, A., and Radev, D. Folio: Natural language reasoning with first-order logic, 2024. URL https://arxiv.org/abs/2209.00840. Hao, Y., Angluin, D., and Frank, R. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Transactions of the Association for Computational Linguistics, 10:800810, 07 2022. ISSN 2307-387X. doi: 10.1162/tacl 00490. URL https://doi.org/10.1162/tacl_a_00490. Jiang, J., Wang, F., Shen, J., Kim, S., and Kim, S. survey on large language models for code generation, 2024. URL https://arxiv.org/abs/2406.00515. Li, Z., Liu, H., Zhou, D., and Ma, T. Chain of thought empowers transformers to solve inherently serial problems. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=3EWTEy9MTM. Llama. The llama 3 herd of models, 2024. URL https: //arxiv.org/abs/2407.21783. Lundberg, S., Ribeiro, M. T. A. p., and et. al. Guidanceai/guidance: guidance language for controlling large language models., 2023. URL https://github. com/guidance-ai/guidance. McCune, W. mace4. http://www.cs.unm.edu/mccune/prover9/, 20052010. Prover9 and CRANE: Reasoning with constrained LLM generation Melcer, D., Fulton, N., Gouda, S. K., and Qian, H. Constrained decoding for fill-in-the-middle code language models via efficient left and right quotienting of contextsensitive grammars, 2024a. URL https://arxiv. org/abs/2402.17988. Melcer, D., Gonugondla, S., Perera, P., Qian, H., Chiang, W.-H., Wang, Y., Jain, N., Garg, P., Ma, X., and Deoras, A. Approximately aligned decoding, 2024b. URL https: //arxiv.org/abs/2410.01103. Merrill, W. and Sabharwal, A. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531 545, 2023. doi: 10.1162/tacl 00562. URL https: //aclanthology.org/2023.tacl-1.31/. Merrill, W. and Sabharwal, A. The expressive power of In The Twelfth transformers with chain of thought. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=NjNGlPh8Wh. Merrill, W., Sabharwal, A., and Smith, N. A. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843856, 2022. doi: 10.1162/tacl 00493. URL https://aclanthology.org/ 2022.tacl-1.49/. Mirzadeh, I., Alizadeh, K., Shahrokhi, H., Tuzel, O., Bengio, S., and Farajtabar, M. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models, 2024. URL https://arxiv.org/ abs/2410.05229. Olausson, T., Gu, A., Lipkin, B., Zhang, C., Solar-Lezama, A., Tenenbaum, J., and Levy, R. Linc: neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.emnlp-main. 313. URL http://dx.doi.org/10.18653/v1/ 2023.emnlp-main.313. OpenAI. Opneai tools, 2024. URL https://platform. openai.com/docs/assistants/tools. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 80248035. Curran Associates, Inc., 2019. http://papers.neurips.cc/paper/ URL 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf. Poesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek, C., and Gulwani, S. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=KmtVD97J43e. Qwen. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/blog/ qwen2.5/. Strobl, L., Merrill, W., Weiss, G., Chiang, D., and Angluin, D. What formal languages can transformers express? survey. Trans. Assoc. Comput. Linguistics, 12:543561, 2024. URL https://doi.org/10.1162/tacl_ a_00663. Suresh, T., Reddy, R. G., Xu, Y., Nussbaum, Z., Mulyar, A., Duderstadt, B., and Ji, H. Cornstack: High-quality contrastive data for better code ranking, 2024a. URL https://arxiv.org/abs/2412.01007. Suresh, T., Ugare, S., Singh, G., and Misailovic, S. Is watermarking llm-generated code robust?, 2024b. URL https://arxiv.org/abs/2403.17983. Tam, Z. R., Wu, C.-K., Tsai, Y.-L., Lin, C.-Y., Lee, H.-y., and Chen, Y.-N. Let me speak freely? study on the impact of format restrictions on large language model performance. In Dernoncourt, F., Preotiuc-Pietro, D., and Shimorina, A. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 12181236, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry. 91. URL https://aclanthology.org/2024. emnlp-industry.91/. Pan, L., Albalak, A., Wang, X., and Wang, W. Y. Logiclm: Empowering large language models with symbolic solvers for faithful logical reasoning, 2023. URL https: //arxiv.org/abs/2305.12295. Ugare, S., Gumaste, R., Suresh, T., Singh, G., and Misailovic, S. Iterative structured llm generation, 2024a. URL https://arxiv.org/abs/ 2410.07295. Itergen: Park, K., Wang, J., Berg-Kirkpatrick, T., Polikarpova, N., and DAntoni, L. Grammar-aligned decoding, 2024. URL https://arxiv.org/abs/2405.21047. Ugare, S., Suresh, T., Kang, H., Misailovic, S., and Singh, G. Syncode: Llm generation with grammar augmentation, 2024b. 11 CRANE: Reasoning with constrained LLM generation Ugare, S., Suresh, T., Kang, H., Misailovic, S., and Singh, G. Syncode: Llm generation with grammar augmentation, 2024c. URL https://arxiv.org/abs/ 2403.01632. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Willard, B. T. and Louf, R. Efficient guided generation for large language models, 2023. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural language processing. In Liu, Q. and Schlangen, D. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6. wolfram. Wolfram alpha, 2024. URL https:// writings.stephenwolfram.com/2023/01/ wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/. Wu, Y., Jiang, A. Q., Li, W., Rabe, M. N., Staats, C. E., Jamnik, M., and Szegedy, C. Autoformalization with large language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=IUikebJ1Bf0. Yang, K., Swope, A. M., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, S., Prenger, R., and Anandkumar, A. Leandojo: Theorem proving with retrieval-augmented language models, 2023. URL https://arxiv.org/ abs/2306.15626. 12 A. Turing Machine Computation CRANE: Reasoning with constrained LLM generation Turing machine processes an input string xxx Σ. Its configuration consists of finite state set Q, an input tape c0, work tapes c1, . . . , ck, and an output tape ck+1. Additionally, each tape τ has an associated head position hτ . Initially, the machine starts in the initial state q0 with the input tape c0 surrounded by infinite blank symbols (b). The head on the input tape is set to h0 blank symbols bs and have their heads positioned at 0. 0 containing xxx, positioned at index 0, and 0 = 0, while all other tapes contain only At each time step i, if qi / (F is set of halting states), the configuration updates recursively by computing: qi+1, γi 1, . . . , γi k+1, di 0, . . . , di k+1 = δ(qi, ci 0[hi 0], . . . , ci k+1[hi k+1]) where δ is the transition function. The machine updates each tape τ by setting ci+1 τ , leaving all other tape cells unchanged. The head position for each tape is updated as hi+1 τ . If qi , the Turing machine halts and outputs the sequence of tokens on the output tape, starting from the current head position and continuing up to (but not including) the first blank symbol (b). Turing machine can also function as language recognizer by setting the input alphabet Σ = {0, 1} and interpreting the first output token as either 0 or 1. τ = hi τ ] = γi τ + di [hi τ B. Thershold Circuit Class 0 is class of computational problems that can be recognized by constant-depth, polynomial-size circuits composed of threshold gates. threshold gate, such as θk, outputs 1 if the sum of its input bits is at most k, while θk outputs 1 if the sum is at least k. These circuits also include standard logic gates like , , and as special cases of threshold functions. Since 0 circuits can simulate AC 0 circuits ( polysize, constant-depth {, , }-circuit family), they are at least as powerful as AC 0 in the computational hierarchy. The circuit families we have defined above are non-uniform, meaning that there is no requirement for the circuits processing different input sizes to be related in any way. In degenerate cases, non-uniform circuit families can solve undecidable problems making them an unrealizable model of computation (Arora & Barak, 2009). Intuitively, uniform circuit family requires that the circuits for different input sizes must be somewhat similar to each other. This concept is formalized by stating that there exists resource-constrained Turing machine that, given the input 1n, can generate serialization of the corresponding circuit Cn for that input size. Specifically, logspace uniform 0 family can be constructed by logspace-bounded Turing machine from the string 1n. C. Proofs Lemma C.1 (Constant depth circuit for Lf ). For any log-precision constant layer transformer-based LLM with finite vocabulary , single deterministic auto-regressive step Lf (x) operating on any input of size with xxx can be simulated by logspace-uniform threshold circuit family of depth where is constant. Proof. The construction is from Theorem 2 in (Merrill & Sabharwal, 2023). Proposition 3.1. For any log-precision LLM with constant layers there exists logspace-uniform thershold circuit hn such that LGc (xxx) = hn(xxx) holds for all inputs xxx with size xxx = and N. Proof. The language L(Gc) is finite; therefore, for any string sss L(Gc), the length satisfies sss , where is constant. Consequently, for any input xxx, the output yyyG = LG(xxx) has constant length, i.e., yyyG . The number of autoregressive steps is also bounded by . From Lemma C.1, each unconstrained autoregressive computation Lf (xxx) can be simulated by constant-depth threshold circuit C. This implies that Lf (xxx, Gc) can also be simulated by constant-depth threshold circuit since it only involves an additional multiplication by constant-sized precomputed Boolean mask {0, 1}V (see Section 2). Given that the number of autoregressive steps is constant , and each step can be simulated by constant-depth circuit C, we can simulate all steps using depth circuit by stacking the circuits for each step sequentially. For uniformity, we are just stacking together constant number of constant depth circuits we can do it in log-space bounded Turning machine . Note that this proof holds only because L(Gc) allows only constant-size strings in the output. 13 CRANE: Reasoning with constrained LLM generation Lemma 3.2. For any Turing machine with tape alphabet Γ, there exists constant depth LLM LM with finite vocabulary Γ VM and log-precision that can simulate t(n) steps of with t(n) autoregressive steps. Proof. The construction follows from Theorem 2 (Merrill & Sabharwal, 2024). In this construction, the deterministic Turing machine run captured by sequence of γ1, . . . , γt(n) capturing the state entered, tokens written, and directions moved after each token before generating the output (xxx). Then on any input the xxx the output LM (xxx) = γ1, , γt(n) (xxx) (assuming halts within on xxx within t(n) steps where = xxx and t(n) is polynomial over n). Proposition 3.3. For any Turing machine with tape alphabet Γ, there exists constant depth LLM LM with finite vocabulary Γ VM and log precision such that for any input xxx with xxx = n, LM,Ga (xxx) = rrr (xxx) with assuming halts on xxx in t(n) steps. Proof. LM (xxx)) = γ1 γt(n) (xxx). We show that LM (xxx) L(Ga). Ga RM G. Since, is output grammar of then (xxx) L(G). For all 1 t(n) γi Γ. Then, γ1 γt(n) Γ L(RM ). Then LM (xxx) L(Ga) then under constrained decoding the output LM (xxx) remains unchanged and LM (xxx) = LM,Ga (xxx) = rrr (xxx) where rrr = γ1 γt(n). CRANE: Reasoning with constrained LLM generation C.1. GSM-Symbolic Examples and Prompt GSM-Symbolic Problem Solution Examples: Question: fog bank rolls in from the ocean to cover city. It takes {t} minutes to cover every {d} miles of the city. If the city is {y} miles across from oceanfront to the opposite inland edge, how many minutes will it take for the fog bank to cover the whole city? Answer: y//d*t Question: {name} makes {drink} using teaspoons of sugar and cups of water in the ratio of {m}:{n}. If she used total of {x} teaspoons of sugar and cups of water, calculate the number of teaspoonfuls of sugar she used. Answer: ((m*x)//(m+n)) GSM-Symbolic Prompt: Listing 1. Problem Solution Examples for GSM-Symbolic You are an expert in solving grade school math tasks. You will be presented with grade-school math word problem with symbolic variables and be asked to solve it. Before answering you should reason about the problem (using the <reasoning> field in the response described below). Intermediate symbolic expressions generated during reasoning should be wrapped in << >>. Then, output the symbolic expression wrapped in << >> that answers the question. The expressions must use numbers as well as the variables defined in the question. You are only allowed to use the following operations: +, -, /, //, %, (), and int(). You will always respond in the format described below: Lets think step by step. <reasoning> The final answer is <<symbolic expression>> There are {t} trees in the {g}. {g} workers will plant trees in the {g} today. After they are done, there will be {tf} trees. How many trees did the {g} workers plant today? Lets think step by step. Initially, there are {t} trees. After planting, there are {tf} trees. The number of trees planted is <<tf - t>>. The final answer is <<tf - t>>. If there are {c} cars in the parking lot and {nc} more cars arrive, how many cars are in the parking lot? Lets think step by step. Initially, there are {c} cars. {nc} more cars arrive, so the total becomes <<c + nc>>. The final answer is <<c + nc>>. {p1} had {ch1} {o1} and {p2} had {ch2} {o1}. If they ate {a} {o1}, how many pieces do they have left in total? Lets think step by step. Initially, {p1} had {ch1} {o1}, and {p2} had {ch2} {o1}, making total of <<ch1 + ch2>>. After eating {a} {o1}, the remaining total is <<ch1 + ch2 - a>>. The final answer is <<ch1 + ch2 - a>>. {p1} had {l1} {o1}. {p1} gave {g} {o1} to {p2}. How many {o1} does {p1} have left? Lets think step by step. {p1} started with {l1} {o1}. After giving {g} {o1} to {p2}, {p1} has <<l1 - g>> {o1} left. The final answer is <<l1 - g>>. {p1} has {t} {o1}. For Christmas, {p1} got {tm} {o1} from {p2} and {td} {o1} from {p3}. How many {o1} does {p1} have now? Lets think step by step. {p1} started with {t} {o1}. {p1} received {tm} {o1} from {p2} and {td} {o1} from {p3}. The total is <<t + tm + td>>. The final answer is <<t + tm + td>>. There were {c} {o1} in the server room. {nc} more {o1} were installed each day, from {d1} to {d2}. How many {o1} are now in the server room? Lets think step by step. Initially, there were {c} {o1}. {nc} {o1} were added each day for <<d2 - d1 + 1>> days, which is <<nc * (d2 - d1 + 1)>>. The total is <<c + nc * (d2 - d1 + 1)>>. The final answer is <<c + nc * (d2 - d1 + 1)>>. {p1} had {gb1} {o1}. On {day1}, {p1} lost {l1} {o1}. On {day2}, {p1} lost {l2} more. How many {o1} does {p1} have at the end of {day2}? Lets think step by step. Initially, {p1} had {gb1} {o1}. After losing {l1} {o1} on {day1}, {p1} had <<gb1 - l1>>. After losing {l2} {o1} on {day2}, the total is <<gb1 - l1 - l2>>. The final answer is <<gb1 - l1 - l2>>. {p1} has ${m}. {p1} bought {q} {o1} for ${p} each. How much money does {p1} have left? 1 2 3 4 5 6 7 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 25 26 27 28 29 30 31 33 34 35 36 37 38 15 CRANE: Reasoning with constrained LLM generation Lets think step by step. Initially, {p1} had ${m}. {p1} spent <<q * p>> on {q} {o1}. The remaining money is <<m - * p>>. The final answer is <<m - * p>>. {question} Listing 2. CoT Prompt Template For GSM-Symbolic Evaluation You are an expert in solving grade school math tasks. You will be presented with grade-school math word problem with symbolic variables and be asked to solve it. Only output the symbolic expression wrapped in << >> that answers the question. The expression must use numbers as well as the variables defined in the question. You are only allowed to use the following operations: +, -, /, //, %, (), and int(). You will always respond in the format described below: <<symbolic expression>> There are {t} trees in the {g}. {g} workers will plant trees in the {g} today. After they are done, there will be {tf} trees. How many trees did the {g} workers plant today? <<tf - t>> If there are {c} cars in the parking lot and {nc} more cars arrive, how many cars are in the parking lot? <<c + nc>> {p1} had {ch1} {o1} and {p2} had {ch2} {o1}. If they ate {a} {o1}, how many pieces do they have left in total? <<ch1 + ch2 - a>> {p1} had {l1} {o1}. {p1} gave {g} {o1} to {p2}. How many {o1} does {p1} have left? <<l1 - g>> {p1} has {t} {o1}. For Christmas, {p1} got {tm} {o1} from {p2} and {td} {o1} from {p3}. How many {o1} does {p1} have now? <<t + tm + td>> There were {c} {o1} in the {loc}. {nc} more {o1} were installed each day, from {d1} to {d2}. How many {o1} are now in the {loc}? <<c + nc * (d2 - d1 + 1)>> {p1} had {gb1} {o1}. On {day1}, {p1} lost {l1} {o1}. On {day2}, {p1} lost {l2} more. How many {o1} does {p1} have at the end of {day2}? <<gb1 - l1 - l2>> {p1} has ${m}. {p1} bought {q} {o1} for ${p} each. How much money does {p1} have left? <<m - * p>> {question} Listing 3. Prompt Template For GSM-Symbolic Evaluation Without CoT C.2. FOLIO Examples and Prompt FOLIO Problem Solution Examples: Question: People in this club who perform in school talent shows often attend and are very engaged with school events. People in this club either perform in school talent shows often or are inactive and disinterested community members. People in this club who chaperone high school dances are not students who attend the school. All people in this club who are inactive and disinterested members of their community chaperone high school dances. All young children and teenagers in this club who wish to further their academic careers and educational opportunities are students who attend the school. Bonnie is in this club and she either both attends and is very engaged with school events and is student who attends the school or is not someone who both attends and is very engaged with school 39 40 41 42 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 33 34 35 36 37 38 39 40 1 2 3 4 5 6 16 CRANE: Reasoning with constrained LLM generation events and is not student who attends the school. Based on the above information, is the following statement true, false, or uncertain? Bonnie performs in school talent shows often. ### FOL Solution: Predicates: InClub(x) ::: is member of the club. Perform(x) ::: performs in school talent shows. Attend(x) ::: attends school events. Engaged(x) ::: is very engaged with school events. Inactive(x) ::: is an inactive and disinterested community member. Chaperone(x) ::: chaperones high school dances. Student(x) ::: is student who attends the school. Wish(x) ::: wishes to further their academic careers and educational opportunities. Premises: {forall} (InClub(x) {and} Attend(x) {and} Engaged(x) {implies} Attend(x)) ::: People in this club who perform in school talent shows often attend and are very engaged with school events. {forall} (InClub(x) {implies} (Perform(x) {xor} Inactive(x))) ::: People in this club either perform in school talent shows often or are inactive and disinterested community members. {forall} (InClub(x) {and} Chaperone(x) {implies} {not}Student(x)) ::: People in this club who chaperone high school dances are not students who attend the school. {forall} (InClub(x) {and} Inactive(x) {implies} Chaperone(x)) ::: All people in this club who are inactive and disinterested members of their community chaperone high school dances. {forall} (InClub(x) {and} (Young(x) {or} Teenager(x)) {and} Wish(x) {implies} Student(x)) ::: All young children and teenagers in this club who wish to further their academic careers and educational opportunities are students who attend the school. {forall} (InClub(x) {implies} (Attend(x) {and} Engaged(x)) {xor} {not}(Attend(x) {and} Engaged(x)) { and} {not}Student(x) {xor} Student(x)) ::: Bonnie is in this club and she either both attends and is very engaged with school events and is student who attends the school or is not someone who both attends and is very engaged with school events and is not student who attends the school. Conclusion: InClub(bonnie) {and} Perform(bonnie) ::: Bonnie performs in school talent shows often. Answer: Uncertain FOLIO Prompt: Listing 4. Problem Solution Examples for FOLIO Given problem description and question. The task is to parse the problem and the question into first-order logic formulas. The grammar of the first-order logic formula is defined as follows: 1) logical conjunction of expr1 and expr2: expr1 {and} expr2 2) logical disjunction of expr1 and expr2: expr1 {or} expr2 3) logical exclusive disjunction of expr1 and expr2: expr1 {xor} expr2 4) logical negation of expr1: {not}expr1 5) expr1 implies expr2: expr1 {implies} expr2 6) expr1 if and only if expr2: expr1 {iff} expr2 7) logical universal quantification: {forall} 8) logical existential quantification: {exists} x. These are the ONLY operations in the grammar. ------ Answer the question EXACTLY like the examples. Problem: All people who regularly drink coffee are dependent on caffeine. People either regularly drink coffee or joke about being addicted to caffeine. No one who jokes about being addicted to caffeine is unaware that caffeine is drug. Rina is either student and unaware that caffeine is drug, or neither student nor unaware that caffeine is drug. If Rina is not person dependent on caffeine and student, then Rina is either person dependent on caffeine and student, or neither person dependent on caffeine nor student. Question: Based on the above information, is the following statement true, false, or uncertain? Rina is either person who jokes about being addicted to caffeine or is unaware that caffeine is drug. ### We take three steps: first, we define the necessary predicates and premises, and finally, we encode the question Rina is either person who jokes about being addicted to caffeine or is unaware that caffeine is drug. in the conclusion. Now, we will write only the logic program, nothing else. Predicates: Dependent(x) ::: is person dependent on caffeine. Drinks(x) ::: regularly drinks coffee. Jokes(x) ::: jokes about being addicted to caffeine. Unaware(x) ::: is unaware that caffeine is drug. Student(x) ::: is student. Premises: {forall} (Drinks(x) {implies} Dependent(x)) ::: All people who regularly drink coffee are dependent 17 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 25 26 27 28 29 30 31 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26 27 28 29 CRANE: Reasoning with constrained LLM generation on caffeine. {forall} (Drinks(x) {xor} Jokes(x)) ::: People either regularly drink coffee or joke about being addicted to caffeine. {forall} (Jokes(x) {implies} {not}Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is drug. (Student(rina) {and} Unaware(rina)) {xor} {not}(Student(rina) {or} Unaware(rina)) ::: Rina is either student and unaware that caffeine is drug, or neither student nor unaware that caffeine is drug. Conclusion: Jokes(rina) {xor} Unaware(rina) ::: Rina is either person who jokes about being addicted to caffeine or is unaware that caffeine is drug. ------ Problem: Miroslav Venhoda was Czech choral conductor who specialized in the performance of Renaissance and Baroque music. Any choral conductor is musician. Some musicians love music. Miroslav Venhoda published book in 1946 called Method of Studying Gregorian Chant. Question: Based on the above information, is the following statement true, false, or uncertain? Miroslav Venhoda loved music. ### We take three steps: first, we define the necessary predicates and premises, and finally, we encode the question Miroslav Venhoda loved music. in the conclusion. Now, we will write only the logic program, nothing else. Predicates: Czech(x) ::: is Czech person. ChoralConductor(x) ::: is choral conductor. Musician(x) ::: is musician. Love(x, y) ::: loves y. Author(x, y) ::: is the author of y. Book(x) ::: is book. Publish(x, y) ::: is published in year y. Specialize(x, y) ::: specializes in y. Premises: Czech(miroslav) {and} ChoralConductor(miroslav) {and} Specialize(miroslav, renaissance) {and} Specialize(miroslav, baroque) ::: Miroslav Venhoda was Czech choral conductor who specialized in the performance of Renaissance and Baroque music. {forall} (ChoralConductor(x) {implies} Musician(x)) ::: Any choral conductor is musician. {exists} (Musician(x) {and} Love(x, music)) ::: Some musicians love music. Book(methodOfStudyingGregorianChant) {and} Author(miroslav, methodOfStudyingGregorianChant) {and} Publish(methodOfStudyingGregorianChant, year1946) ::: Miroslav Venhoda published book in 1946 called Method of Studying Gregorian Chant. Conclusion: Love(miroslav, music) ::: Miroslav Venhoda loved music. ------ {question} 30 31 32 33 34 35 36 37 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 Listing 5. Prompt Template Used For FOLIO Evaluation 18 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 CRANE: Reasoning with constrained LLM generation C.3. Case Study For GSM-Symbolic Question: {name} hires {vehicle} from {start_hour} to {end_hour}. He gets {free_hours} hours free. The first paid hour is ${first_hour_cost} and each hour after that is {multiplier} the cost. How much did he pay? Constrained: <<(int(end_hour - start_hour) - free_hours) * first_hour_cost + free_hours * first_hour_cost + (int(end_hour - start_hour) - free_hours - 1) * multiplier * first_hour_cost>> Unconstrained With CoT: Lets think step by step. The total time hired is from {start_hour} to { end_hour}, which is <<int((end_hour - start_hour).total_seconds() / 3600)>> hours. Subtracting the {free_hours} free hours, the paid hours are <<int((end_hour - start_hour).total_seconds() / 3600) - free_hours>>. The final answer is <<first_hour_cost + (int((end_hour - start_hour). total_seconds() / 3600) - free_hours - 1) * multiplier * first_hour_cost>>. CRANE: Lets think step by step. The total time hired is from {start_hour} to {end_hour}, totaling << int(end_hour - start_hour)>> hours. Subtracting {free_hours} free hours, the paid hours are <<int (end_hour - start_hour) - free_hours>>. The first hour costs {first_hour_cost}, and each additional hour costs {multiplier} times that. The final answer is <<first_hour_cost + (int( end_hour - start_hour) - free_hours - 1) * multiplier * first_hour_cost>>. Listing 6. Case Study for GSM-Symbolic CRANE effectively alternates between constrained and unconstrained generation to produce intermediate expressions, the final answer, and to maintain the reasoning capabilities of the LLM. In contrast, unconstrained generation with CoT results in syntactically incorrect expression, while constrained generation produces syntactically valid but incorrect expression. C.4. Sampling Ablation for GSM-Symbolic In our GSM-Symbolic case study, we use IterGen as the constrained generation baseline and initialize CRANE with IterGen. Both IterGen and CRANE employ selective rejection sampling to filter tokens that do not satisfy semantic constraints. For comparison, we also run unconstrained generation using temperature sampling and evaluate its performance against CRANE. Specifically, for Qwen2.5-1.5B-Instruct and Llama-3.1-8B-Instruct, we generate three samples with unconstrained generation at temperature of = 0.7 and compute pass@1/2/3 metrics. As shown in Table 4, CRANE with greedy decoding achieves higher accuracy than pass@1/2/3 for unconstrained generation with Chain-of-Thought (CoT) and temperature sampling on Qwen2.5-1.5B-Instruct. Although, for Llama-3.1-8B-Instruct, unconstrained generation with CoT and temperature sampling achieves pass@3 accuracy of 35%2% higher than CRANEit generates approximately 4 times as many tokens as CRANE. C.5. Grammars C.5.1. GSM-SYMBOLIC GRAMMAR start: space? \"<\" \"<\" space? expr space? \">\" \">\" space? expr: expr space? \"+\" space? term expr space? \"-\" space? term term term: term space? \"*\" space? factor term space? \"/\" space? factor term space? \"//\" space? factor term space? \"%\" space? factor factor space? factor: \"-\" space? factor TYPE \"(\" space? expr space? \")\" primary space? primary: NUMBER VARIABLE \"(\" space? expr space? \")\" TYPE.4: \"int\" space: \" \" %import common.CNAME -> VARIABLE %import common.NUMBER 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 CRANE: Reasoning with constrained LLM generation Listing 7. GSM-Symbolic Grammar C.5.2. PROVER9 GRAMMAR start: predicate_section premise_section conclusion_section predicate_section: \"Predicates:\" predicate_definition+ premise_section: \"Premises:\" premise+ conclusion_section: \"Conclusion:\" conclusion+ predicate_definition: PREDICATE \"(\" VAR (\",\" VAR)* \")\" COMMENT premise: quantified_expr COMMENT -> define_premise conclusion: quantified_expr COMMENT -> define_conclusion -> define_predicate quantified_expr: quantifier VAR \"(\" expression \")\" expression quantifier: \"{forall}\" -> forall \"{exists}\" -> exists expression: bimplication_expr ?bimplication_expr: implication_expr (\"{iff}\" bimplication_expr)? ?implication_expr: xor_expr (\"{implies}\" implication_expr)? ?xor_expr: or_expr (\"{xor}\" xor_expr)? ?or_expr: and_expr (\"{or}\" or_expr)? ?and_expr: neg_expr (\"{and}\" and_expr)? ?neg_expr: \"{not}\" quantified_expr -> and -> neg -> xor -> or -> iff -> imply atom ?atom: PREDICATE \"(\" VAR (\",\" VAR)* \")\" -> predicate \"(\" quantified_expr \")\" // Variable names begin with lowercase letter VAR.-1: /[a-z][a-zA-Z0-9_]*/ /[0-9]+/ // Predicate names begin with capital letter PREDICATE.-1: /[A-Z][a-zA-Z0-9]*/ COMMENT: /:::.*n/ %import common.WS %ignore WS Listing 8. Prover9 Grammar 20 CRANE: Reasoning with constrained LLM generation Table 3. Comparison of CRANE and baselines with various models on GSM-Symbolic based on accuracy, number of tokens, and average time. Model Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct 2 4 Qwen2.5-Coder-7B-Instruct 2 Qwen2.5-Coder-7B-Instruct 4 Qwen2.5-Coder-7B-Instruct Qwen2.5-Math-7B-Instruct 2 Qwen2.5-Math-7B-Instruct 4 Qwen2.5-Math-7B-Instruct Llama-3.1-8B-Instruct Llama-3.1-8B-Instruct Llama-3.1-8B-Instruct 2 4 Method Acc. (%) Parse (%) Tokens Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE Unconstrained w/o CoT Constrained Unconstrained CoT CRANE 21 20 21 22 28 18 18 24 30 21 22 26 37 36 32 37 36 36 35 37 36 35 37 39 20 26 28 32 22 29 28 37 27 29 29 19 23 29 35 18 24 26 30 21 26 30 33 98 95 90 96 95 96 94 98 97 97 90 96 99 84 96 96 100 89 97 94 99 88 94 66 95 72 89 83 98 76 88 89 99 82 61 95 84 94 68 96 92 97 73 98 95 95 18.23 34.28 130.74 140.52 18.23 34.28 130.74 140.52 23.34 25.29 128.97 131. 17.22 18.61 148.87 155.65 16.89 18.81 151.29 163.21 17.92 25.28 138.38 155.32 115.22 26.99 190.51 195.65 47 27.08 184.35 194.77 25.7 26.81 155.26 158. 157.36 45.58 198.64 206.85 131.5 37.38 172.21 179.95 128.38 35.97 163.55 170.22 CRANE: Reasoning with constrained LLM generation Table 4. Comparison of CRANE and greedy and sampling baselines with different models on GSM-Symbolic. Model Method pass@1/2/3 (%) Parse (%) Tokens Unconstrained w/o CoT (Greedy) Unconstrained w/o CoT (t = 0.7) Constrained (Greedy) Qwen2.5-1.5B-Instruct Unconstrained CoT (Greedy) Unconstrained CoT (t = 0.7) CRANE Llama-3.1-8B-Instruct Unconstrained w/o CoT (Greedy) Unconstrained w/o CoT (t = 0.7) Constrained (Greedy) Unconstrained CoT (Greedy) Unconstrained CoT (t = 0.7) CRANE (Greedy) 21 15/19/22 22 26 21/25/30 31 21 15/21/25 26 30 24/29/35 33 97 88/96/98 97 90 78/91/96 100 73 51/74/84 98 95 89/98/98 23.34 20.19/39.76/60.57 25.29 128.97 146.22/292.96/444.61 131.3 128.38 106.88/232.75/369.86 35.97 163.55 196.01/403.68/607.7 170.22 Table 5. Comparison of CRANE and greedy and sampling baselines with different models on FOLIO. Model Method pass@1/2/3 (%) Compile (%) Tokens Qwen2.5-7B-Instruct Unconstrained CoT (Greedy) Unconstrained CoT (t = 0.7) Constrained (Greedy) CRANE (Greedy) Unconstrained CoT (Greedy) Unconstrained CoT (t = 0.7) Llama-3.1-8B-Instruct Constrained (Greedy) CRANE (Greedy) 36.95 16.75/28.57/34.98 37.44 42.36 32.02 14.29/22.66/29.06 39.41 46.31 70.94 35.96/55.67/68.47 87.68 87.68 350.64 401.5/800.19/1219.33 775.62 726.88 57.14 33.99/46.8/57.64 86.21 85. 371.52 435.35/877.33/1307.45 549.75 449."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Illinois Urbana-Champaign, USA"
    ]
}