{
    "paper_title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
    "authors": [
        "Gang Liu",
        "Michael Sun",
        "Wojciech Matusik",
        "Meng Jiang",
        "Jie Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 3 2 2 4 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Multimodal Large Language Models for Inverse\nMolecular Design with Retrosynthetic Planning",
            "content": "Gang Liu1,4, Michael Sun2,4, Wojciech Matusik2, Meng Jiang1, Jie Chen3 1University of Notre Dame 3 MIT-IBM Watson AI Lab 2MIT CSAIL 4 This work was done while GL and MS interned at the MIT-IBM Watson AI Lab, IBM Research {gliu7, mjiang2}@nd.edu, {msun415, wojciech}@csail.mit.edu, chenjie@us.ibm.com"
        },
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are transforming machine learning across many domains [Achiam et al., 2023]. Their success in natural language processing has extended to areas handling not only text but also image and speech data [Dong et al., 2023, Wu et al., 2024], thanks to the development of multimodal capabilities. Recently, the potential of LLMs for molecular discovery has been actively explored [Jablonka et al., 2023]. However, LLMs struggle in the chemical domain, exhibiting poor generation quality and planning capability [Guo et al., 2023]. This is due to the unique, graph structures of molecular data, which are challenging for LLMs that typically handle sequential texts. Inverse molecular design requires LLMs to be controllable for generating molecular structures that meet multi-property and synthesizability requirements [Chen et al., 2020, Gao et al., 2021]. These requirements can be detailed as questions for LLM input, as shown in Figure 2. Answering these questions demands comprehensive understanding of molecular structures and their relationship to properties. However, sequence-based LLMs struggle with this because they are pre-trained or fine-tuned solely on Figure 1: Comparison of Controllability: Results are averaged from the best numbers from Table 1. Figure 2: Three LLM-based methods for molecular design. The question outlines requirements for properties, structures, and synthesis, addressed as follows: (a) In-Context Learning and (b) Supervised Fine-Tuning use text-only data for demonstrations and instruction tuning, respectively. (c) The proposed Llamole uses graph-text multimodal data to fine-tune the LLM, integrating parameterfrozen graph models for interleaved text and graph generation with reaction inference. text representations of molecules, e.g., SMILES [Weininger, 1988]. To illustrate this, we investigate 14 LLMs for molecular generation in Figure 1 across 10K drug and material questions: ten using in-context learning (ICL) and four with supervised fine-tuning (SFT). LLMs generate molecular structures based on the questions, and their properties are obtained through oracles to assess the differences between requirements and generated outputs. Details of the experimental set-ups and results can be found in Section 5. In summary, even the best LLMs perform worse than GraphGA [Gao et al., 2022], simple yet effective graph-based method, in designing molecules with satisfactory properties. As illustrated in Figure 2, practical answers for molecular design are more complex than what can be achieved by using graph methods or LLMs alone. The generation begins with paragraph describing the intended molecule for multi-conditional generation, followed by retrosynthetic planning, detailing each synthesis stepone reaction per paragraphin reverse order, from the target molecule to purchasable reactants. Thus, multimodal LLMs (MLLMs) are essential, with LLMs handling text generation and graph models managing molecular design. In this work, we propose the multimodal Large language model for molecular discovery (Llamole). As shown in Figure 2 (c), the model seamlessly integrates LLMs and graph models within multimodal autoregressive framework, enabling the interleaved generation of text, molecules, and reactions. It predicts the next token across both word and chemical spaces, framed as multi-class prediction tasks for word vocabulary, atom/bond types, and reaction templates. For retrosynthetic planning, Llamole integrates A* search to efficiently identify synthesis pathways for the designed molecule. To implement Llamole, we augment base LLM with two pre-trained graph modules: the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecule generation [Liu et al., 2024c] and GNN for reaction template prediction. The base LLM controls the generation flow using trigger-query-prediction approach with two sets of trigger tokens for the Graph DiT and GNN, respectively. Upon predicting trigger token, one or few query tokens summarize the prior text as vectors, activating the corresponding graph modules and generating molecules or predicting reaction templates. Afterward, the base LLM can resume text generation, aided by graph encoder that encodes the previously generated molecule. In retrosynthetic planning, the LLM computes heuristics to efficiently assist the A* search in navigating the vast reaction space for multi-step generation. Our work has several highlights. First, Llamole is the first MLLM for molecular design, capable of interleaved generation of text and graphs. Second, we curated dataset along with fine-tuning instructions to benchmark complex yet realistic molecular design outcomes, including human conversation. Third, we present compelling experimental results that demonstrate the competitiveness of Llamole against 14 LLMs and GraphGA, as shown in Figure 1. With details in Tables 1 and 2, 2 Llamole improves LLM performance by up to 80.9% across 12 metrics for controllable molecular generation and increases the success rate for retrosynthetic planning from 5.5% to 35%."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Autoregressive Language Modeling Given sequence of word tokens = {w1, w2, . . . , wL} of length from the vocabulary W, LLMs parameterized by θ1 decompose the joint distribution as pθ1(W ) = (cid:81)L i=1 pθ1(wiW<i), where W<i represents the tokens preceding the i-th position. These models are optimized by minimizing the negative log-likelihood between their predictions and the empirical data distribution, resulting in: LLM = (cid:88) log pθ1(wiW<i). (1) 2.2 Molecular Design with Graph Diffusion Models xt1 Molecular graphs can be modeled through diffusion in discrete spaces [Austin et al., 2021, Vignac et al., 2022, Liu et al., 2024c]. Given one-hot encoded data point RF with categories (e.g., node or an edge), discrete models perform diffusion using transition matrix Q, where [Qt]ij = q(xt ) for i, [1, ]. The forward diffusion with is: q(xt xt1) = Cat(xt; = xt1Qt), where Cat(x; p) denotes the categorical distribution over with probabilities given by p. Starting from the original data point = x0, we have q(xt x0) = Cat (cid:0)xt; = x0 Qt(cid:1), where Qt = (cid:81) it Qi. The forward diffusion gradually corrupts data points. When the total timestep is large enough, q(xT ) converges to stationary distribution. The reverse process samples from q(xT ) and gradually removes noise. The posterior distribution q(xt1 xt) is calculated as q(xt1xt, x0) xt(Qt) x0 Qt1. Using denoising model parameterized by θ2, this posterior can be approximated by pθ2 (xt1xt, x0). For inverse molecular design with multi-property constraints, the denoising model can be optimized by minimizing the negative log-likelihood for x0: LDM = Eq(x0)Eq(xtx0) (cid:2) log pθ (cid:0)x0 c1, c2, . . . , cM , ctext, xt(cid:1)(cid:3) , (2) where molecular properties are denoted by {ci}M i=1, and the text embedding is ctext. These conditions can be handled by Graph DiT [Liu et al., 2024c] without introducing additional predictors for guidance [Ho and Salimans, 2022]. 2.3 One-Step Reaction Prediction with Graph Neural Networks Retrosynthesis needs to predict the reverse of synthetic reaction, which decomposes chemical products into reactants. GNN parameterized by θ3 takes the product Gproduct to predict the label in the reaction space R. This label is interpreted as the template and determines the reactants. With the text condition ctext, we minimize the negative log-likelihood of the label distribution q(r): Lpredictor = Eq(r) [ log pθ3(r ctext, Gproduct)] . (3) 2.4 Retrosynthetic Planning with A* Search Given molecules from the structure space G, subset Gavail represents available molecular structures that can be purchased as building blocks for synthesis. For any target Gtarget, one-step prediction of the reversed reaction may not yield reactants within Gavail. Thus, retrosynthesis typically requires multi-step planning to find pathways from building blocks to the target in reverse order. The search space of chemical reactions can be navigated using A* on an AND-OR tree , with Gtarget as the root. Reaction nodes follow an AND relation, requiring all child reactants, while molecule nodes follow an OR relation, meaning the product can be synthesized by any child reaction [Chen et al., 2020]. Selection: We select nodes from the frontier F(T ) containing unexplored molecule nodes to expand the tree. Given an oracle cost function J(), the next node is selected as Gnext = arg minGF (T ) J(G) to minimize the cost. well-designed J() improves search efficiency and aids in global optimality. 3 Figure 3: Overview of Llamole: Trigger tokens (<design> and <retro>) switch active modules from the base LLM to the respective graph component. The subsequent <query> token utilizes output vectors from the LLM to summarize past texts as conditions. Using these, Llamole generates molecules and predicts one-step reactions. Enhanced with graph encoder and A* search, Llamole efficiently plans synthesis routes through selection and expansion iterations on the AND-OR Tree. Expansion: After selecting Gnext, single GNN predictor call can generate many one-step retrosynthesis proposals. The GNN provides top-candidate reaction templates, each linked to different reactants. Thus we can form molecule nodes under the reaction node as an AND-OR stump. Update and Cost: After expanding Gnext, the tree becomes . We update the nodes in for the next iteration. A* selects the path that minimizes J() = Jcurrent() + Jheuristic(), which includes the cost from the start to the current node Jcurrent() and heuristic estimate of the cost to the goal Jheuristic(). With the GNN predictor, the negative log-likelihood of the reaction can be used to compute path cost Jcurrent() to the leaf molecule node, we design Jheuristic() with the LLM in Llamole."
        },
        {
            "title": "3 Llamole: Multimodal Large Language Model for Molecular Discovery",
            "content": "3.1 Multimodal Autoregressive Modeling In molecular discovery, the sequence may include molecular structures and retrosynthetic reactions with each molecule or reaction tokenized. The sequence = {y1, y2, . . . , yN }, where yi R, combines these tokens. The sequence is interleaved with tokens in different spaces. Suppose the molecule appears at position i; then, we typically see: . . . , Yi G, Yi+1:i+L W, Yi+L+1 R, . . . where is the length of the text following the molecule at position i. The sequence starts with text. If position denotes the first molecule in the sequence, then Y<i W; otherwise, yi1 R. To handle non-word tokens, we integrate domain-specific Graph DiT and GNN with the LLM, forming multimodal LLM, i.e., Llamole. Parameterized by Θ, Llamole unifies the cross-entropy losses from Eqs. (1) to (3) into autoregressive modeling: LLlamole = LLM + LDM + Lpredictor = (cid:88) log pΘ(yiY<i). (4) LDM interprets Y<i as the input conditions, including desirable molecular properties and text conditions {ci}M i=1 {ctext} for the autoregression of Yi in G. In Lpredictor, Y<i represents Gproduct and ctext. Here, Gproduct is generated from previous diffusion models or as intermediate / Gavail in retrosynthesis. The autoregression for the label Yi is performed in the reaction space R. We present an overview of multimodal autoregression with Llamole in Figure 3, divided into controllable molecular generation and retrosynthetic planning. The base LLMs perform multiple roles: generating text, controlling the switch of active modules, and providing cost functions for A* search. 4 Augmented with the graph models, the overall parameters in Llamole are Θ = {θ1, θ2, θ3, ϕ1, ϕ2, ϕ3}, where ϕ1 and ϕ2 project text into ctext for the Graph DiT and GNN predictor, respectively. The graph encoder with ϕ3 projects molecule tokens into the LLM. Next, we detail the design space of Llamole. 3.2 Llamole Design Space Llamole consists of base LLM and two pre-trained graph modules: the Graph DiT for molecule generation and the GNN for one-step reaction prediction. The base LLM employs trigger-queryprediction approach using two sets of special tokens to switch between modules. Trigger Tokens. Llamole defines two special trigger tokens to augment the word vocabulary W: <design> for switching between the LLM and Graph DiT, and <retro> for switching between the LLM and GNN predictor. When trigger token is predicted, Llamole activates the corresponding graph model. After molecule generation or reaction prediction, the active modules revert to the LLM. Query Tokens. We introduce another set of special tokens, named query tokens <query> automatically placed after triggers. They use the LLM to query previous tokens and output hidden states as chidden. linear layer is applied: ctext = Linear(chidden), adjusting the input size for the graph models. We use different query tokens for different triggers. Query tokens allow us to share parameters ϕ1 and ϕ2 with θ1, enhancing both efficiency and effectiveness. We can apply ensemble methods by repeating the query tokens multiple times and averaging the chidden values [Dong et al., 2023]. Besides the special tokens, Llamole enhances molecule understanding with graph encoder and uses the LLM to provide the cost function in A* search for retrosynthetic planning. Graph Encoder. The graph encoder parameterized by ϕ3 replaces the word encoder in the LLM tokenizer for molecule tokens. The LLM decoder takes molecule embeddings from the graph encoder, along with text embeddings from the tokenizer, into the Transformer layers for next token generation. We use pre-trained Graph Isomorphism Network (GIN) [Xu et al., 2018] as the graph encoder, optimized via molecule-text contrastive learning similar to CLIP [Radford et al., 2021]. A* Cost Function with LLM. We define Jheuristic as multi-choice problem, where each choice, assigned score, represents synthesis complexity, from few to many steps. The LLM estimates the remaining synthesis steps for the leaf molecule node F(T ) Gavail in the search tree . It outputs probabilities for each choice, and Jheuristic is computed as the weighted score by averaging the scores with their probabilities. For F(T ) Gavail, Jheuristic = 0. 3.3 End-to-End Model Fine-Tuning and Generation Supervised Fine-Tuning. We use multimodal SFT to connect the base LLM and other graph modules in Llamole [Ouyang et al., 2022]. Specifically, we freeze the parameters for the graph modules (θ2 and θ3) and fine-tune the LLM parameters θ1, the learnable special tokens, and the linear layers for the query tokens (ϕ1 and ϕ2). We freeze the parameters of the pre-trained graph encoder (ϕ3) and add tunable linear layer between it and the LLM decoder. The optimization can be conducted end-to-end with Eq. (4). The SFT aligns the LLM with domain-specific graph models. To maintain generality in the base LLM, we employ parameter-efficient LoRA [Hu et al., 2021]. Interleaved Generation. Given question as shown in Figure 2, Llamole performs controllable and synthesizable molecular designs, as presented in Figure 3. For the controllable generation, Llamole uses the base LLM to analyze the requirements and switches to the Graph DiT for generating Gtarget when the trigger is predicted. For the synthesizable generation, Llamole plans synthesis routes for Gtarget. A* search on the AND-OR tree aids in multi-step generation, interleaving molecule and reaction nodes, with Gtarget as the root. During each selection-expansion iteration, A* selects Gnext = arg minGF (T ) J(G) from the leaf nodes F(T ). The graph encoder embeds molecule tokens into the LLM, which generates reaction conditions until the token <retro> is triggered, activating the GNN predictor. The predictor then predicts the top-50 templates as reaction nodes, along with corresponding reactants as molecule nodes for the next iteration. A* stops after finding route from Gtarget to Gavail with satisfying all AND-OR constraints, or if it fails after 30 seconds or 300 iterations. Upon success, the text with the corresponding reaction along the route is returned for retrosynthesis; otherwise, the base LLM directly generates texts. 5 Figure 4: Creation of MolQA and MolPair: MolQA comprises two sets: training set for ICL and (multimodal) SFT, and test set for evaluation. MolPair consists of graph-text and reaction-text pairs, with red highlights indicating synthetic complexity, structure, and properties information. 4 Benchmarking for Multimodal Molecular Design To train Llamole, we need instruction data that provide detailed language supervision and evaluation covering synthetic complexity, drug and material utility, and reaction conditions. However, existing data based on PubChem [Kim et al., 2021] are only usable for small molecules and lack such details. Thus, we create MolQA, large-scale graph-text multimodal instruction dataset for systematic LLM benchmarking used in Section 5. We also create MolPair with graph-text and reaction-text pairwise data to pre-train graph modules, as detailed in appendix C. To this end, we first collect multisource molecule data (Figure 4), with details in appendix B. Then we create MolQA and MolPair. MolQA: Instruction Data Creation. USPTO reactions include text descriptions. We use Enamines 1.3 million small molecules as Gavail. The depth-first search identifies routes from reaction products (i.e. target molecules) to molecules within Gavail, resulting in about 139K routes with lengths ranging from 1 to 10. We sample around 11K routes (750 for materials and 9986 for drugs) for testing and use the rest for instruction tuning. We focus on eight popular properties for benchmarking (i.e, = 8 for {ci}M 1 in Eq. (2)). They include three drug-related categorical properties [Wu et al., 2018]: (1) HIV virus replication inhibition (HIV), (2) blood-brain barrier permeability (BBBP), and (3) human β-secretase 1 inhibition (BACE) and five continuous material properties [Thornton et al., 2012]: (4) CO2 permeability, (5) N2 permeability, (6) O2 permeability, (7) fractional free volume (FFV), and (8) thermal conductivity (TC). Not all target molecules have these properties. To enrich properties and texts, two supervised GNNs predict drug and material properties with confidence scores as in Liu et al. [2022, 2023a]. Only high-confident predictions are selected for annotation. Llama-3-70B then generates descriptions using template that incorporates these properties with structural and synthesis information from toolkits like RDKit. There are no polymerization reactions; we consider the monomer structure of the polymer as the synthesis target. We assemble molecule descriptions, text, and reactions from synthesis routes as answer data. Then Llama-3-70B is prompted to generate questions, resulting in MolQA with the example as shown in Figure 2. Details are in appendix B.2. MolPair: Pairwise Data Creation. After excluding the target molecules from the instruction data, we use the remaining text-reaction data from USPTO to pre-train the GNN reaction predictor. Similarly, we utilize all other small molecules and polymers to pre-train the Graph DiT and graph encoder. For generalization, we expand beyond the eight properties used in the instruction data. For drug utility, we train another GNN to predict 41 properties, including toxicity, safety, enzyme interaction, absorption, distribution, metabolism, excretion (ADME), and biological activity [Swanson et al., 2024]. For material utility, we consider 14 properties, such as thermal, physical, thermodynamic, permeability, solubility, and dielectric properties. Llama-3-70B generates related texts for these properties, incorporating structural and synthetic information. Finally, there are around 600K graphtext pairs for both small molecules and polymers to support pre-training. Details are in appendix B.3."
        },
        {
            "title": "5 Experiment",
            "content": "We conduct systematic evaluation to demonstrate Llamoles superior performance in controllable and synthesizable molecular design (RQ1). We investigate Llamoles performance in controllable 6 Table 1: Multi-Conditional Molecular Design with LLMs: Best overall results in each metric are in bold , best baseline results are in italic . Balanced Accuracy (BA) = True Positive Rate+True Negative Rate . 2 Base LLM or Method Structure () Text () Drug (BA ) Material (MAE ) Validity Similarity BLEU-4 ROUGE-L HIV BBBP BACE CO2Perm N2Perm O2Perm FFV TC GraphGA 0. 0.112 NA NA 0.536 0.515 0.560 0.847 1. 0.747 0.020 0.042 In-Context Learning 0.167 Llama-2-7B 0.251 Mistral-7B 0.180 Qwen2-7B Llama-3-8B 0.656 Flan-T5-XXL 0.570 Granite-13B 0.498 Llama-2-13B 0.346 Mistral-8x7B 0.546 Llama-2-70B 0.299 Llama-3-70B 0.706 Supervised Fine-tuning 0.718 Mistral-7B 0.768 Qwen2-7B Llama-3-8B 0.797 Llama-3.1-8B 0.692 Llamole 0.900 Mistral-7B Qwen2-7B 0.888 Llama-3.1-8B 0.913 0.024 0.044 0.012 0.112 0.094 0.079 0.058 0.094 0.045 0. 0.125 0.133 0.136 0.121 0.139 0.135 0.142 Improvement of Llamole (%) +4.4 vs. All +4.4 vs. LLMs +3.2 +14.6 0.030 0.066 0.030 0.155 0.226 0.170 0.121 0.181 0.099 0.210 0.105 0.221 0.093 0. 0.262 0.261 0.254 0.141 0.203 0.147 0.307 0.388 0.326 0.279 0.345 0.222 0.367 0.216 0.377 0.206 0.250 0.434 0.432 0.427 0.051 0.060 0.053 0.163 0.153 0.200 0.089 0.091 0.085 0.471 0.473 0.562 0.329 0.333 0.403 0.260 0.293 0.285 0.236 0.250 0.259 0.345 0.346 0.388 0.237 0.242 0.274 0.415 0.403 0.484 0.460 0.483 0.515 0.436 0.457 0.457 0.426 0.445 0.440 0.417 0.432 0. 0.596 0.617 0.740 0.600 0.639 0.746 0.623 0.629 0.713 5.463 5.062 5.552 3.233 2.869 2.994 5.031 3.695 5.368 2.659 3.269 2.691 2.222 3.210 0.593 0.645 0.653 3.982 3.824 4.251 3.106 3.039 3.165 4.285 3.150 4.336 2.848 3.094 2.562 2.322 2. 1.409 1.452 1.344 4.943 4.657 5.068 2.924 2.799 2.993 4.816 3.440 5.017 2.421 2.985 2.721 2.119 2.974 0.308 0.199 0.289 0.186 0.322 0.211 0.171 0.123 0.165 0.120 0.180 0.123 0.291 0.184 0.191 0.138 0.319 0.202 0.135 0.099 0.184 0.128 0.147 0.106 0.110 0.086 0.179 0.122 0.565 0.581 0. 0.021 0.028 0.021 0.026 0.021 0.030 +15.9 +15.9 +11.9 +11.9 +16.2 +24.1 +32.7 +32.3 +32.3 +32.7 +22.9 +70.6 +6.7 +37. +22.2 -5.0 +28.6 +72.6 +80.9 +65.1 molecular generation through ablation and case studies (RQ2). We analyze retrosynthetic performance of LLMs, focusing on error analysis and the efficiency and effectiveness of Llamole (RQ3). Set-ups: We include LLM baselines from 7B to 70B, such as Llama, Mistral, Qwen, Granite, and Flan-T5, using either ICL or LoRA-based SFT. The MolQA test set contains 9,986 QA pairs for material design and 750 for drug design. LLMs are prompted with questions to generate responses for texts, molecules, and reactions. For controllability, we evaluate up to 12 metrics across four aspects: (1) chemical validity, (2) similarity to the reference based on Morgan fingerprints [Rogers and Hahn, 2010], (3) BLEU-4 and ROUGE-L scores against reference texts, and (4) deviation from desired properties. We follow Gao et al. [2022] to use well-trained random forests as the oracle functions for obtaining properties of designed molecules. We focus on three drug-related categorical properties assessed by balanced accuracy (BA) and five continuous material properties assessed by mean absolute error (MAE). For retrosynthesis, we evaluate the success rate of designed molecules against those available in Gavail from Enamine. Details are in appendix D.1. 5.1 RQ1: LLMs for Controllable and Synthesizable Molecular Design Table 1 and Table 2 detail LLM performance in controllability and retrosynthesis success rate. The overall performance rankings are summarized in Figure 5. Our key observations are: (1) Llamole significantly outperforms other LLMs in text generation, controllable molecule generation, and retrosynthetic planning. Llamole fine-tuned on various 7B-parameter LLMs, as shown in Table 2, results in top-3 rankings, surpassing 70B models that are 10 larger across all 12 metrics for controllability and planning success. Specifically, Llamole enhances chemical structure validity by 14.6%, structure controllability by 4.4%, and text generation by 11.9%-15.9%. Additionally, Llamole improves property controllability by 32% to 80%. In retrosynthesis, Table 2 indicates Llamole increases the success ratio from 5% to 35% for drugs and to 17.9% for polymers. 7 (a) LLM for Drug (Small Molecule) Design (b) LLM for Material (Polymer) Design Figure 5: Overall Comparison of LLMs for Controllability and Synthesizability: Performance is ranked by averaged BA/MAE (x-axis) and retrosynthesis success rate (y-axis). Circle size indicates model size. LLMs with ICL, SFT, and Llamole are highlighted in blue, orange, and red, respectively. Table 2: Retrosynthetic Success Rate: Best results are in bold , best baseline results are in italic . In-Context Learning Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Drug (%) Material (%) 0.1 0.3 0.2 0.4 0.0 0.0 5.5 4. 0.4 0.8 0.6 1.6 Supervised Fine-tuning 1.2 1.2 Llamole 1.6 1. 1.0 0.8 Mistral-7B Qwen2-7B Llama-3-8B Llama-3.1-8B Mistral-7B Qwen2-7B Llama-3.1-8B Drug (%) Material (%) 1.5 0.8 0.2 0.1 0.6 0. 0.8 0.8 29.9 14.3 33.7 17.9 35.1 17.6 (2) SFT improves molecular design but may not always enhance retrosynthesis. According to Figure 5 and Table 1, SFT enables 7B LLMs to achieve chemical validity, structure, and property control comparable to 70B LLMs with ICL. However, it offers minimal improvement in planning ability for the generated target molecule. notable example is Llama-3-8B from Table 2, where SFT reduces its retrosynthesis planning success from 5.5% to below 1%. Except for Llama-3-8B, we connect LLM performance with the same baseline but different learning methods in Figure 5. The results show that SFT methods still outperform ICL with the same base 7B models in most cases. (3) Larger models without domain-specific adaptation do not necessarily perform better in molecular designs. We calculate the average Pearson correlation coefficient between model size and molecular design metrics, yielding value of 0.366, indicating weak correlation (below 0.5) between size and performance. We also compare LLM performance with GraphGA, which has been shown to be simple yet powerful [Gao et al., 2022, Liu et al., 2024c]. Our observations confirm that GraphGA serves as strong molecular design baseline, challenging most LLM models with ICL and SFT in generating molecules with precise multi-condition control. 5.2 RQ2: Discussion on Controllable Molecular Generation 5.2.1 Ablation Studies on LLM and Graph DiT Synergy We investigate the synergy effect of Graph DiT and LLM in Llamole for molecule controllability. We first remove text conditions ctext. In this case, Graph DiT uses learned null embedding to represent the dropped condition ctext = . Next, we remove the drug or material property conditions {ci}M associated with the question. Results in Figure 6 show that text instructions enhance the chemical structure understanding ability of Graph DiT, while Llamole leverages Graph DiTs capabilities with property inputs to generate molecules with desirable properties. 8 Figure 6: Ablation Studies for the Graph DiT Module in Llamole: First, we remove the text conditions from the input, i.e., ctext = . Next, we remove both text and property conditions, {ci}M ctext. There are learned embeddings that represent the null value for different conditions. Figure 7: Interleaved generation with the base Qwen2-7B: Red indicates positions where molecules and reactions (with templates) are generated, forming three parts. The properties of the designed molecules are obtained from the oracle. Reference and other LLM responses are shown in Figure 9. 5.2.2 Case Studies for Property and Structure Controllability In Figure 7, Llamole can design satisfactory molecule that meets both functional and structural constraints. Functionally, the oracle function confirms that the properties of BACE and HIV align with the criteria. Structurally, all key criteria are satisfied, including molecular weight, two aromatic rings, and connected to aliphatic chains. Llamole also adds details for structure design, such as carboxyl ( COOH) group and an amino group ( NH2). While the amino group is present in the structure, it is connected to the carbonyl group ( C( O) ) instead of the carboxyl group. This subtle difference may require precise control based on the text condition. More results are in appendix D.3. 5.3 RQ3: Discussion on Retrosynthetic Planning Retrosynthesis challenges LLMs in two aspects: (1) one-step reaction generation and (2) multistep planning. Table 2 highlights the weaknesses of LLMs with ICL and SFT in overall planning ability and the promise of Llamole. We examine the failure reasons in LLMs and the synergy between the GNN and LLMs to avoid them. 5.3.1 One-step Reaction Generation We conduct error analysis for LLMs in reaction generation. Results in Figure 8 average performance across all LLMs using ICL or SFT methods. We identify five types of errors related to instruction adherence, format compliance, and Figure 8: Error Analysis in Reaction Generation 9 template matching. We find that LLMs using ICL frequently fail to follow instructions for generating reactions in text format, with high probability (68.4%) of not producing valid formats and templates. In contrast, LLMs with SFT reduce this probability to 57.6%. However, neither ICL nor SFT guarantees that the templates are correct or match the generated reactions. In comparison, Llamole avoids these errors by using GNN predictors, which estimate probabilities for over 300K templates derived from USPTO reactions. This enables Llamole to apply templates directly to derive reactions in retrosynthesis, avoiding hallucination. 5.3.2 Multi-step Retrosynthetic Planning From the success cases in Table 2, we find that 96.40% of 777 success cases in ICL-adapted LLMs and 94.14% of 324 success cases in SFT-adapted LLMs arise from one-step reaction generation. However, not all designed molecules can be synthesized via one-step reactions. Compared to LLMs, Llamole achieves over 10K success cases, with 40.48% resulting from two or more steps. Figure 7 illustrates two-step planning case for the designed molecule. The generation interleaves reaction conditions and specific formulas based on the template in both steps. Table 3: Analysis of Jheuristics and Planning Time on Material Questions Llamole is influenced by two factors for retrosynthesis: (1) the size of the search space and (2) the quality of the cost Jheuristic. The results reported in Table 2 limited the total planning time to 30 seconds (on an A6000 card). We remove this time constraint and report comparisons for material tasks in Table 3. We find that success rates for all base LLMs significantly improve, but this comes at the cost of long inference time. While there is trade-off between efficiency and effectiveness, extending response time by few minutes is often acceptable to improve the success rate of finding synthesis paths for single designed molecule. In Table 3, we also compare the Jheuristics designed by LLMs (default) with the domain model trained from Chen et al. [2020]. we find that LLMs are competitive with these domain models in providing the cost function for A*, contrasting with previous observations where LLMs struggled with retrosynthetic planning. Llama-3.1 Mistral Qwen2 w/ Unlimited Time w/ Domain Heuristics 0.312 0.273 0.273 0.176 0.147 0. 0.176 0.143 0.179 Base LLM Default"
        },
        {
            "title": "6 Related Work",
            "content": "Since the emergence of ChatGPT [Achiam et al., 2023], LLMs [Dubey et al., 2024] have become foundation models for text-based problems and are revolutionizing domains like vision and speech [Dong et al., 2023, Wu et al., 2024]. These advancements extend to chemistry, biology, and material sciences, focusing on molecules [Guo et al., 2023, Jablonka et al., 2023, Jin et al., 2023]. Prior work explores LLMs in molecular generation, property prediction, and one-step reaction prediction in retrosynthesis [Guo et al., 2023, Jablonka et al., 2023]. key lesson is the limitation of LLMs in sequential modeling of molecules (e.g., SMILES or SELFIES) [Guo et al., 2023]. Multimodal LMs have been developed in combination with GNNs [Zhao et al., 2023, Liu et al., 2023b], but they have not been scaled up to leverage LLMs with at least 7 billion parameters. Additionally, LLMs struggle with planning tasks [Kambhampati et al., 2024], which are essential for retrosynthesis. We address these issues using graph-text multimodal LLMs, augmented by A* for efficient planning. Domain-specific molecular design methods have evolved from sequential models [Segler et al., 2018] to graph diffusion models [Jo et al., 2022, Vignac et al., 2022, Liu et al., 2024c]. Studies show that older graph-based methods like GraphGA remain competitive [Gao et al., 2022]. To incorporate property constraints, one can use molecular optimization approaches such as Bayesian optimization or REINFORCE [Gao et al., 2022], or employ diffusion models with or without predictor guidance [Vignac et al., 2022, Liu et al., 2024c]. For synthesizable molecular design, prior work has focused on bottom-up methods [Gao et al., 2021, Sun et al., 2024]. These methods explore chemical space defined by discrete action space of reaction templates and purchasable starting materials, which may limit flexibility. Thus, retrosynthesis algorithms [Chen et al., 2020] are also studied as separate solutions to find synthesis routes for generated molecules in top-down manner."
        },
        {
            "title": "7 Conclusion",
            "content": "We have presented the first graph-text MLLM, Llamole, for multi-conditional molecular generation and retrosynthetic planning. By integrating base LLM with specialized graph modules, Llamole interleaved the generation of text, molecular graphs, and reactions, enabling controllable and synthesizable designs. Extensive benchmarking against 14 LLMs revealed their limitations in controlling molecular structures and planning synthesis routes. In contrast, Llamole significantly outperformed these LLMs. These findings underscored the value of multimodal approaches in molecular discovery and highlighted Llamoles potential to connect text and chemical structures. The new benchmarking dataset also laid the groundwork for future MLLM research in molecular applications."
        },
        {
            "title": "References",
            "content": "Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar Panda, Yara Rizk, GP Bhargav, Maxwell Crouse, Chulaka Gunasekara, et al. Granite-function calling model: Introducing function calling abilities via multi-task learning of granular tasks. arXiv preprint arXiv:2407.00121, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019. Binghong Chen, Chengtao Li, Hanjun Dai, and Le Song. Retro*: learning retrosynthetic planning with neural guided a* search. In International conference on machine learning, pages 16081616. PMLR, 2020. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Connor Coley, Luke Rogers, William Green, and Klavs Jensen. Scscore: synthetic complexity learned from reaction corpus. Journal of chemical information and modeling, 58(2):252261, 2018. Connor Coley, William Green, and Klavs Jensen. Rdchiral: An rdkit wrapper for handling stereochemistry in retrosynthetic template extraction and application. Journal of chemical information and modeling, 59(6):25292537, 2019. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1:111, 2009. Wenhao Gao, Rocıo Mercado, and Connor Coley. Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design. arXiv preprint arXiv:2110.06389, 2021. Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: benchmark for practical molecular optimization. Advances in neural information processing systems, 35: 2134221357, 2022. Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, et al. What can large language models do in chemistry? comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems, 36:5966259688, 2023. 11 Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Kevin Maik Jablonka, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua Bocarsly, Andres Bran, Stefan Bringuier, Catherine Brinson, Kamal Choudhary, Defne Circi, et al. 14 examples of how llms can transform materials science and chemistry: reflection on large language model hackathon. Digital Discovery, 2(5):12331250, 2023. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. Large language models on graphs: comprehensive survey. arXiv preprint arXiv:2312.02783, 2023. Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, volume 162, pages 1036210383. PMLR, 2022. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Llms cant plan, but can help planning in llm-modulo frameworks. arXiv preprint arXiv:2402.01817, 2024. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin Shoemaker, Paul Thiessen, Bo Yu, et al. Pubchem in 2021: new data content and improved web interfaces. Nucleic acids research, 49(D1):D1388D1395, 2021. Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph rationalization with environment-based augmentations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 10691078, 2022. Gang Liu, Tong Zhao, Eric Inae, Tengfei Luo, and Meng Jiang. Semi-supervised graph imbalanced regression. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 14531465, 2023a. Gang Liu, Eric Inae, Tengfei Luo, and Meng Jiang. Rationalizing graph neural networks with data augmentation. ACM Transactions on Knowledge Discovery from Data, 18(4):123, 2024a. Gang Liu, Eric Inae, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Data-centric learning from unlabeled graphs with diffusion model. Advances in neural information processing systems, 36, 2024b. Gang Liu, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Inverse molecular design with multi-conditional diffusion guidance. arXiv preprint arXiv:2401.13858, 2024c. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1562315638, 2023b. Daniel Lowe. Chemical reactions from US patents (1976 Sep2016). 6 2017. doi: 10.6084/m9.figshare. 5104873.v1. URL https://figshare.com/articles/dataset/Chemical_reactions_ from_US_patents_1976-Sep2016_/5104873. Ruimin Ma and Tengfei Luo. Pi1m: benchmark database for polymer informatics. Journal of Chemical Information and Modeling, 60(10):46844690, 2020. Shingo Otsuka, Isao Kuwajima, Junko Hosoya, Yibin Xu, and Masayoshi Yamazaki. Polyinfo: Polymer database for polymeric materials design. In 2011 International Conference on Emerging Intelligent Data and Web Technologies, pages 2229. IEEE, 2011. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 12 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5):742754, 2010. Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1): 120131, 2018. Teague Sterling and John Irwin. Zinc 15ligand discovery for everyone. Journal of chemical information and modeling, 55(11):23242337, 2015. Michael Sun, Alston Lo, Wenhao Gao, Minghao Guo, Veronika Thost, Jie Chen, Connor Coley, and Wojciech Matusik. Syntax-guided procedural synthesis of molecules. arXiv preprint arXiv:2409.05873, 2024. Kyle Swanson, Parker Walther, Jeremy Leitz, Souhrid Mukherjee, Joseph Wu, Rabindra Shivnaraine, and James Zou. Admet-ai: machine learning admet platform for evaluation of large-scale chemical libraries. Bioinformatics, 40(7):btae416, 2024. Thornton, Robeson, Freeman, separation URL https://research.csiro.au/virtualscreening/ and Uhlmann. Polymer gas membrane database, 2012. membrane-database-polymer-gas-separation-membranes/. Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022. David Weininger. Smiles, chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):3136, 1988. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. NExt-GPT: Any-to-any multimodal LLM. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=NZQkumsNlf. Zhenqin Wu, Bharath Ramsundar, Evan Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh Pappu, Karl Leswing, and Vijay Pande. Moleculenet: benchmark for molecular machine learning. Chemical science, 9(2):513530, 2018. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Barbara Zdrazil, Eloy Felix, Fiona Hunter, Emma Manners, James Blackshaw, Sybilla Corbett, Marleen de Veij, Harris Ioannidis, David Mendez Lopez, Juan Mosquera, et al. The chembl database in 2023: drug discovery platform spanning multiple bioactivity data types and time periods. Nucleic acids research, 52(D1):D1180D1192, 2024. Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng, Lingpeng Kong, and Qi Liu. Gimlet: unified graph-text model for instruction-based molecule zero-shot learning. Advances in Neural Information Processing Systems, 36:58505887, 2023."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Preliminaries"
        },
        {
            "title": "2.4 Retrosynthetic Planning with A* Search . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 Llamole: Multimodal Large Language Model for Molecular Discovery"
        },
        {
            "title": "3.2 Llamole Design Space",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 End-to-End Model Fine-Tuning and Generation . . . . . . . . . . . . . . . . . . . 4 Benchmarking for Multimodal Molecular Design 5 Experiment 5.1 RQ1: LLMs for Controllable and Synthesizable Molecular Design . . . . . . . . . 5.2 RQ2: Discussion on Controllable Molecular Generation . . . . . . . . . . . . . . . 5.2.1 Ablation Studies on LLM and Graph DiT Synergy . . . . . . . . . . . . . 5.2.2 Case Studies for Property and Structure Controllability . . . . . . . . . . . 5.3 RQ3: Discussion on Retrosynthetic Planning . . . . . . . . . . . . . . . . . . . . 5.3.1 One-step Reaction Generation . . . . . . . . . . . . . . . . . . . . . . . . 3 3 3 3 3 4 5 5 6 6 8 8 9 9 9 5.3.2 Multi-step Retrosynthetic Planning . . . . . . . . . . . . . . . . . . . . . 10 6 Related Work 7 Conclusion Additional Details for Llamole A.1 Details of Special Tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Details of LLM-based A* Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . Additional Benchmarking and Datasets Details B.1 Details of Quality Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Details on the Creation of MolQA . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.1 Creation of Synthesis Routes . . . . . . . . . . . . . . . . . . . . . . . . . B.2.2 Creation of Property Annotations . . . . . . . . . . . . . . . . . . . . . . B.2.3 Creation of Text Data for Molecular Description . . . . . . . . . . . . . . B.2.4 Creation of Question Answering Data . . . . . . . . . . . . . . . . . . . . B.3 Details on the Creation of MolPair . . . . . . . . . . . . . . . . . . . . . . . . . . 14 11 16 16 16 16 17 17 17 18 19 Additional Pre-training and Fine-tuning Details C.1 Pre-training of Graph Diffusion Transformers . . . . . . . . . . . . . . . . . . . . C.2 Pre-training of GNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Fine-tuning of Llamole . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experimental Details and Discussions D.1 Additional Details on Experimental Set-ups . . . . . . . . . . . . . . . . . . . . . D.1.1 Set-ups for Figure 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1.2 Extraction of SMILES from LLM Responses . . . . . . . . . . . . . . . . D.2 Additional Discussion on One-Step Generation . . . . . . . . . . . . . . . . . . . D.3 Additional Discussion on Case Studies . . . . . . . . . . . . . . . . . . . . . . . . 21 21 21 21 22 22 23 23"
        },
        {
            "title": "A Additional Details for Llamole",
            "content": "A.1 Details of Special Tokens In total, there are nine special tokens divided into three groups. These tokens augment the word vocabulary W, enabling flexible control of the generation flow: Trigger and Query tokens: <design start>, <design body>, <design end>, <retro start>, <retro body>, <retro end> Molecule token: <molecule> Callback tokens: <callback start>, <callback end> The tokens <design start> and <retro start> switch between the LLM and the Graph DiT or GNN, respectively. The tokens <design body> and <retro body> serve as query tokens, repeated eight times. After tokenization, the LLM takes their embeddings as input and outputs vector from the last layer. The tokens <design end> and <retro end> indicate the end of these switches. The <molecule> token marks the position of the molecular graph where the graph encoder is applied. In the instruction dataset, the segment <mol start>SMILES<mol end> denotes the position and identity of the molecule. SMILES will be converted to molecular graphs using RDKit, and this segment will be replaced by the <molecule> token for Llamole inputs. Finally, callback tokens control the LLM to generate backup results as complements to the specialized graph modules. For instance, if the Graph DiT fails to produce valid molecule, the base LLM can generate an alternative, regardless of validity. A.2 Details of LLM-based A* Heuristics Llamole models Jheuristics in A* search as multi-choice problem, filling in information from the molecule node, its parent reaction nodes and siblings using the template below. Parameters such as step, reaction template, and reactants are optional. Estimate remaining steps for the target { smiles } given the following parameters : Current step { step } , Current template : { template } , Reactants : { reactants }. Consider the following factors : 1. Intermediate complexity 2. Reagent availability 3. Side reactions 4. Stereochemistry challenges . Using this question to estimate remaining steps, we input the text into the base LLM and formulate five choices with corresponding scores: . All readily available // Score : 0 . Some commercial , some need 1 -2 steps // Score : 1 . Mix of commercial and multi - step synthesis // Score : 2.5 . Mostly require complex synthesis // Score : 4.5 . All require extensive multi - step synthesis // Score : 7 The LLM outputs logits for the next token, which we average for each choice to obtain overall probabilities. The Jheuristics is calculated as the weighted score using these probabilities."
        },
        {
            "title": "B Additional Benchmarking and Datasets Details",
            "content": "We collect small drug molecules from PubChem [Kim et al., 2021], MoleculeNet [Wu et al., 2018], ChEMBL [Zdrazil et al., 2024], and ZINC [Sterling and Irwin, 2015]. Polymers are macromolecules with one repeating unit called monomers. We collect polymers from PI1M [Ma and Luo, 2020], 16 the Membrane Society of Australia (MSA) [Thornton et al., 2012], and others [Liu et al., 2024b]. Additionally, we collect 3.8 million patent chemical reactions with descriptions from USPTO [Lowe, 2017], spanning from 1976 to 2016. B.1 Details of Quality Control After collecting molecules and polymers from various sources, we deduplicate and merge the label information for identical molecules. We use RDKit to obtain canonical SMILES. For small molecules, we calculate the first 14 characters of the InChIKey as the unique identifier, while for polymers, where the polymerization point is represented by *, we use the canonical SMILES directly. For drug-like small molecules, we apply the following rules to filter out alert structures, known as the Rule of Five (Ro5): Molecular Weight (MW): Must be 500 Da. Hydrogen Bond Acceptors (HBA): Must not exceed 10. Hydrogen Bond Donors (HBD): Must not exceed 5. LogP: Must be 5, indicating lipophilicity. molecule passes the Ro5 test if at least three of these four conditions are met, indicating potential oral bioavailability. We also apply 15 filter rules from the RDKit package, including the following from the FilterCatalogs Class: BRENK, CHEMBL, CHEMBL BMS, CHEMBL Dundee, CHEMBL Glaxo, CHEMBL Inpharmatica, CHEMBL LINT, CHEMBL MLSMR, CHEMBL SureChEMBL, NIH, PAINS, PAINS A, PAINS B, PAINS C, and ZINC. B.2 Details on the Creation of MolQA B.2.1 Creation of Synthesis Routes The USPTO has 3.7 million reactions. There are approximately 1.3 million unique product molecules. The purchasable compounds come from the Enamine Building Block (June 2024 version), supplemented with other common ions and starting materials, totaling around 1.3 million. We check each product from USPTO as target molecule in the retrosynthesis task, exploring whether they can be synthesized using existing USPTO reactions through depth-first search (DFS). Ultimately, we identify about 139K target molecules with synthesis routes, supporting the creation of MolQA. Since there are no polymerization reactions, we consider only monomer structures by replacing the * point with hydrogen. Among the 139K small molecules with synthesis routes, 2196 fit the monomer structures and serve as target molecules for polymer retrosynthesis. The length of synthesis routes ranges from 1 to 10. For each length of the routes, we split half of the molecules into the testing set, with maximum of 3000, while the remainder is retained in the training set. It results in around 11K routes (750 for materials and 9986 for drugs) for testing and 126K target molecules for training. B.2.2 Creation of Property Annotations We focus on eight benchmarking properties: three drug-related categorical properties [Wu et al., 2018](1) HIV virus replication inhibition (HIV), (2) blood-brain barrier permeability (BBBP), and (3) human β-secretase 1 inhibition (BACE)and five continuous material properties [Thornton et al., 2012](4) CO2 permeability (CO2Perm), (5) N2 permeability (N2Perm), (6) O2 permeability (O2Perm), (7) fractional free volume (FFV), and (8) thermal conductivity (TC). First, we check existing sources for annotations of these properties. To enrich the label space, we use well-trained GNN models [Liu et al., 2022] to generate confident pseudo-labels, following the method in [Liu et al., 2023a]. We collect all labeled data to train two supervised multi-task GIN models for drug and material property annotation. The GIN models employ rationalization techniques [Liu et al., 2024a] to split the molecular graph into rationale and environment subgraphs in the latent space, predicting labels from the rationale subgraph. The confidence score is computed by combining the 17 rationale subgraph with various environment subgraphs, using the reciprocal of prediction variance. We annotate properties when prediction confidence exceeds the median threshold. B.2.3 Creation of Text Data for Molecular Description In addition to property annotations, we consider structural and synthesis information of the molecules using RDKit and heuristic complexity estimation scores. First, for any molecule, we extract the following structural information: Scaffold: Extracted scaffold from the molecule structure. Molecular Weight: Calculated using the molecular weight descriptor. Number of Rings: Total number of rings in the molecule. Number of Aromatic Rings: Total number of aromatic rings in the molecule. Number of Aliphatic Rings: Total number of aliphatic rings in the molecule. Number of Rotatable Bonds: Total number of rotatable bonds in the molecule. Number of Hydrogen Bond Donors: Total number of hydrogen bond donors. Number of Hydrogen Bond Acceptors: Total number of hydrogen bond acceptors. Next, we compute the synthetic accessibility score (SAScore) [Ertl and Schuffenhauer, 2009] and SCScore [Coley et al., 2018]. Based on this information, we use the following template: Generate summary description that starts directly with \" The molecule / polymer ...\" based on the predicted chemical properties , synthetic complexity scores , and structural information for the molecule with SMILES : {{ smiles }}. Use your own knowledge , focus on functions , and avoid using numbers , redundant words , or mentioning SMILES . Ensure the output sentence is complete and ends with period . This is for Drug / Material Utility of Molecule / Polymer : The structural context of molecule includes its scaffold , which is the core structure around which the molecule is built . Key structural features include the presence of aromatic rings , aliphatic chains , and common functional groups such as hydroxyl , carboxyl , and amino groups . The complexity of the molecule structure can significantly influence its physical and chemical properties . Scaffold : {{ scaffold }} Molecular Weight : {{ mw }} Number of Rings : {{ num_rings }} Number of Aromatic Rings : {{ num_arom_rings }} Number of Aliphatic Rings : {{ num_aliph_rings }} Number of Rotatable Bonds : {{ num_rot_bonds }} Number of Hydrogen Bond Donors : {{ num_h_donors }} Number of Hydrogen Bond Acceptors : {{ num_h_acceptors }} { utility_context } {{ properties }} The pre-defined utility context for the small molecule is as follows: The drug utility of molecule is assessed based on its potential to serve as therapeutic agent . Key properties considered include pharmacokinetics , which encompasses absorption , distribution , metabolism , excretion ( ADME ) , and toxicity . Bioactivity is another critical factor , measured by the molecule ability to interact with biological targets , typically through binding affinity . Additionally , 18 drug - likeness , which refers to the molecule adherence to established rules such as Lipinski Rule of Five , is essential . This rule evaluates molecular weight , hydrogen bond donors and acceptors , and lipophilicity to predict molecule suitability as an oral drug . The pre-defined utility context for the polymer is as follows: The material utility of molecule , particularly for creating polymeric materials , is evaluated based on properties like mechanical strength , flexibility , and thermal and electrical behavior . For polymer membranes used in gas separation , crucial factors include gas permeability , which determines the efficiency of gas diffusion , and chemical stability , ensuring resistance to degradation . Additionally , thermal properties such as melting point and thermal conductivity are vital , as they affect the material performance under various temperature conditions . Electrical properties , such as conductivity and dielectric constant , may also be significant depending on the intended application . For the property variable, we include the property name with values, as well as the minimum, maximum, and percentile among the labels in the template. We repeat all annotated properties in the property variable. The estimated synthesis complexity scores are included among them. We also prompt Llama-3-70B to generate short responses of 50-70 words, producing molecular description for each molecule based on its properties, structures, and synthesis estimation. If molecule has description from PubChem [Kim et al., 2021], we concatenate these descriptions. The generated texts may not always be meaningful or valid. We can establish filter rules based on patterns observed in poorly generated texts to remove them. We then regenerate texts for these items. After several iterations, we obtain the final text data for molecular utility descriptions, improving overall text quality. We also apply this strategy to other steps that involves prompting LLMs for synthetic data creation. B.2.4 Creation of Question Answering Data After annotating molecular description texts from appendix B.2.3, we combine them with reaction descriptions, including the reaction formula and template from synthesis routes in appendix B.2.1. This forms the answer data in QA data pair. Next, we prompt Llame-3-70B to generate questions for each answer based on the following template. creating question - answer dataset for LLM fine - tuning . The question is about designing molecule / polymer with these properties : { property_info } and the following structure information : { structure_info }. The expected answer for the question is : { answer } Generate SINGLE question about designing and synthesizing such molecule / polymer that meets these criteria : (1) Start with Question : ; (2) End with question mark ; (3) Sound natural ; (4) Be diverse ; (5) Avoid redundancy and introductory words ( like Here is question that meets the criteria : ) (6) Do not include the answer ; (7) Do not include incorrect information . Example questions : (1) How can design and synthesize molecule with , , and properties ? (2) What is the best way to create polymer with , , and characteristics ? 19 (3) How to design molecule with , , and features and synthesize it ? (4) want molecule with , properties and structures . Please design it and describe the synthesis path . The template is applied to any answer with the corresponding structure, property information, and complete answer texts. B.3 Details on the Creation of MolPair MolPair consists of two parts: reaction-text pairs and graph-text pairs. We curate reaction-text pairs from USPTO [Lowe, 2017], pairing each reaction with its corresponding description of the reaction conditions. We first deduplicate product molecules in reactions, obtaining input data as the product molecule alongside the reaction condition texts. Next, we extract reaction templates from the reaction formula using rdchiral [Coley et al., 2019], resulting in approximately 300K templates, which will serve as labels for predictions. Finally, we have approximately 1.6 million training examples. For the graph-text pairs, we use small molecules and polymers from the multisource collection, excluding those in MolQA. We follow the same pipeline used to create property and text annotations for the MolQA data, focusing on broader properties that describe drug-related utility with 41 small molecule properties [Swanson et al., 2024]. Besides the three used in MolQA, others include: Toxicity and Safety: AMES, Carcinogens Lagunin, ClinTox, DILI, Skin Reaction, hERG Enzyme Interaction: CYP1A2 Veith, CYP2C19 Veith, CYP2C9 Substrate CarbonMangels, CYP2C9 Veith, CYP2D6 Substrate CarbonMangels, CYP2D6 Veith, CYP3A4 Substrate CarbonMangels, CYP3A4 Veith Absorption, Distribution, Metabolism, and Excretion (ADME): BBB Martins, Bioavailability Ma, Caco2 Wang, Clearance Hepatocyte AZ, Clearance Microsome AZ, HIA Hou, Half Life Obach, Hydration Free Energy FreeSolv, Lipophilicity AstraZeneca, PAMPA NCATS, PPBR AZ, Pgp Broccatelli, Solubility AqSolDB, VDss Lombardo Stress Response: SR-ARE, SR-ATAD5, SR-HSE, SR-MMP, SR-p53 Nuclear Receptor Interaction: NR-AR-LBD, NR-AR, NR-AhR, NR-Aromatase, NR-ERLBD, NR-ER, NR-PPAR-gamma We describe polymeric material utility based on 14 polymer properties collected from Otsuka et al. [2011]: Thermal Properties: Melting temperature [C]; Specific heat capacity at constant pressure (Cp) [cal/(gC)]; Specific heat capacity at constant volume (Cv) [cal/(gC)]; Thermal conductivity [W/(mK)] Physical & Thermodynamic Properties: Density [g/cm3]; Fractional Free Volume (dimensionless); Radius of Gyration (Rg) [nm] Permeability Properties: Gas diffusion coefficient (D) [cm2/s]; Gas permeability coefficient (P ) [cm3 (STP)cm/(cm2sPa)]; Oxygen (O2) Gas Permeability (Barrer); Nitrogen (N2) Gas Permeability (Barrer); Carbon Dioxide (CO2) Gas Permeability (Barrer) Solubility Properties: Gas solubility coefficient (S) [cm3 (STP)cm/(cm2sPa)] Dielectric & Optical Properties: Dielectric constant. We train two multi-task GIN models based on the rationalization method [Liu et al., 2022] using all existing labeled data for drug and material property prediction, respectively. We use these models to predict properties for millions of small molecules and polymers, retaining the top ten thousand predictions by confidence score for each property. These are then used to prompt Llama-3-70B to create molecular descriptions, using the same prompt template as in appendix B.2.3. Additionally, we apply the same strategy as in appendix B.2.3 to annotate labels for the eight studied properties, which can serve as input for pretraining the multi-conditional Graph DiT. Finally, we have approximately 300K graph-text pairs for small molecules and 300K graph-text pairs for polymers. 20 Additional Pre-training and Fine-tuning Details We pre-train three graph models including Graph DiT [Liu et al., 2024c] for multi-conditional molecular generation, GIN-based GNN predictor for reaction template prediction, and GIN-based graph encoder for molecule understanding [Xu et al., 2018]. C.1 Pre-training of Graph Diffusion Transformers Suppose the node has FV categories and the edge has FE categories (including non-bond). Graph DiT models the node token by concatenating all its edge configurations to other nodes. For each node RF , we have = FV + NG FE, where NG denotes the graph size. This facilitates defining the transition matrix for the joint distribution of nodes and edges [Liu et al., 2024c]. Graph DiT uses Transformer layers, replacing layer normalization with adaptive layer normalization (AdaLN): AdaLN (h, c) = γθ(c) µ (h) σ (h) + βθ(c), where denotes the hidden state of and is the vector representing the input conditions. Given multiple conditions with categorical, continuous properties, and text, Graph DiT uses one-hot encoding for categorical properties and clustering-based approach with Linear (Softmax (Linear(c))) to embed continuous condition values c. We employ pre-trained SciBERT [Beltagy et al., 2019] to embed input texts into 768-dimensional vector by averaging the representations of all text tokens in the sentence, then using linear layer to adjust the dimension for Graph DiT. For each condition, the model also learns drop embedding. The drop embedding is used when no values are provided. Finally, the model sums the representation vectors of different conditions as input for c. In the reverse diffusion process, the denoising model uses predictor-free guidance to sample molecular graphs given multiple conditions. We pre-train the denoising model with the loss function in Eq. (2) using 600K graph-text pairwise data and the eight properties defined in appendix B.3. The model employs the following hyperparameters: depth of 28, hidden size of 1024, 16 heads, and MLP hidden size of 4096. The total model size is around 574 million parameters. We pre-train the model for 45 epochs, which takes approximately one week on single A100 card. C.2 Pre-training of GNNs We pre-train three-layer GIN to predict reaction templates among 30,124 labels, using hidden size of 512. Reaction template prediction is multi-class classification task. Given reaction-text pairs from MolPair, we extract the product molecular graph from the reaction formula, using the reaction condition text as input. SciBERT [Beltagy et al., 2019] is used as the text encoder with frozen parameters. We average the text representations to obtain sentence-level representation. The prediction target is the reaction template extracted from the reaction [Coley et al., 2019]. GIN naturally uses molecular graphs, employing the AdaLN approach as the normalization layer added after each message-passing layer to incorporate text conditions. We pre-train the model for 5 epochs on single V100 card, with 632 million parameters. This model serves as the reaction predictor to suggest reaction templates for Llamole. For molecular understanding, we pre-train five-layer GIN model with hidden size of 768. SciBERT [Beltagy et al., 2019] is used as the text encoder with frozen parameters. We average the text representations to obtain sentence-level representation, while the GIN model uses sum pooling to produce the graph representation. For each graph-text pair from MolPair, we optimize the graph encoder using the CLIP loss [Radford et al., 2021] for 40 epochs. The CLIP loss consists of two contrastive losses: it first computes the similarity score between graph-text pairs, then contrasts it with all other similarity scores by pairing the graph with other texts and pairing the text with other graphs as negative pairs. The model has around 43 million parameters. The model can be pre-trained on single V100 card in few days. This graph encoder will replace the word encoder in the LLM tokenizer module for molecules indicated by the token <molecule> as shown in appendix A. C.3 Fine-tuning of Llamole Llamole is fine-tuned on graph-text multimodal instruction data, freezing the parameters of the Graph DiT, GNN predictor, and graph encoder. It automatically adds eight query tokens to the sequence once the trigger tokens are predicted, allowing the base LLM to continue autoregression and output vectors for all eight query tokens. We average these output vectors as queries for prior generated texts and use them as input text vectors for the subsequent Graph DiT or GNN predictor module via tunable linear layer. For the <molecule> token, we add tunable linear layer on top of the token embedding after the graph encoder outputs it. Without loss of generality, we study three variants of Llamole with different base LLMs: Llama-3.1-8B [Dubey et al., 2024], Mistral-7B [Jiang et al., 2023], and Qwen2-7B [Yang et al., 2024]. All LLMs are fine-tuned using LoRA [Hu et al., 2021] for four epochs, taking approximately two days on single A100 card."
        },
        {
            "title": "D Additional Experimental Details and Discussions",
            "content": "D.1 Additional Details on Experimental Set-ups In Tables 1 and 2 and Figures 1, 5a and 5b, Llamole is compared with fourteen LLMs with sizes ranging from 7B to 70B, including Llama [Dubey et al., 2024], Mistral [Jiang et al., 2023], Qwen [Yang et al., 2024], Granite [Abdelaziz et al., 2024], and Flan-T5 [Chung et al., 2024]. We prefer the instruct version of the model when available. Using the MolQA training set, previous work can implement these LLMs in two ways: in-context learning (ICL) and text-only supervised fine-tuning (SFT). For ICL, we retrieve five closest QA pairs from the training set based on the average property difference from desired properties. The template used to construct the prompt with demonstrations is: working on designing and synthesizing molecules . Here are some example questions and answers about molecular requirements , design , and synthesis : {{ examples }} Now , based on these examples , please answer the following question about molecular design and synthesis : {{ question }} For SFT, we fine-tune the LLMs with LoRA after converting molecules into SMILES strings. The MolQA test set contains 9,986 QA pairs for small molecules in drug applications and 750 pairs for polymeric materials. The questions serve as input to prompt the LLMs to generate responses. For the controllability of multi-conditional molecular generation, we evaluate up to 12 metrics across four aspects: (1) chemical validity, (2) similarity to the truth based on Morgan fingerprints, (3) BLEU4 and ROUGE-L scores compared to reference texts, and (4) deviation from desired properties. For polymer validity, we further examine whether the generated molecular structures contain at least two polymerization points (*). To obtain the properties of the designed structure, we define an oracle function based on well-trained random forests from all annotated molecules, following previous work [Gao et al., 2022, Liu et al., 2024c]. We evaluate three drug-related categorical properties using balanced accuracy (BA) and five continuous material properties using mean absolute error (MAE). As baseline, we consider GraphGA [Gao et al., 2022] to reference the performance of LLMs compared to domain-specific methods. For retrosynthesis, we evaluate the success rate from the designed molecule to those available in Gavail, purchasable from the Enamine Building Block (June 2024 version), supplemented with other common ions and starting materials, totaling around 1.3 million. D.1.1 Set-ups for Figure 1 For Figure 1, we average the balanced accuracy for three drug-related properties and five MAEs for the polymeric material properties. We then select the model with the best performance in each category based on these average metrics. For drug tasks, the best ICL model is Llama-3-8B-ICL, the best SFT model is Mistral-7B-SFT, and the best Llamole variant is based on Qwen2-7B. For material tasks, the best ICL model is Llama-3-70B-ICL, the best SFT model is Llama-3-8B-SFT, and the best Llamole variant is based on Llama-3.1-8B. Their average performance is visualized in Figure 1 in comparison with GraphGA. 22 Table 4: Text Generation for Reaction Conditions: Best results and best baselines are highlighted. In-Context Learning Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Llama-2-70B BLEU-4 ROUGE-L 0.021 0.112 0.036 0.141 0.005 0. 0.107 0.205 0.130 0.250 0.077 0.202 0.051 0.159 Supervised Fine-tuning 0.048 0. Llamole 0.136 0.248 0.054 0.152 0.059 0.164 Mistral-7B Qwen2-7B Llama-3-8B Llama-3.1-8B Mistral-7B Qwen2-7B Llama-3.1-8B BLEU-4 ROUGE-L 0.085 0.191 0.141 0.222 0.114 0.195 0.111 0.201 0.049 0.192 0.074 0. 0.085 0.268 D.1.2 Extraction of SMILES from LLM Responses ICL or SFT-based LLMs generate free-form text that includes both natural language and SMILESrepresented molecular structures. We need method to automatically extract SMILES strings from LLM outputs for evaluation. Practically, one can observe generation patterns to summarize rules for regular expressions to accomplish this. In the MolQA training set, the designed molecular structures typically follow the phrase the designed molecule is: as shown in examples Figures 9 and 10. LLMs may not always adhere strictly to this pattern, so we may need to extend this rule to cover more cases. In the future, more sophisticated regular expressions could be developed to extract SMILES strings from text directly. However, these will still need to be combined with additional rules to identify the designed molecules, as LLMs may generate intermediate SMILES strings before and after the designed molecule. Compared to them, Llamole uses <design start> or <retro start> to indicate the position of generated molecular structures. D.2 Additional Discussion on One-Step Generation We further examine the text generation results for reaction conditions. Since the answer represents just one possibility in retrosynthesis, we use the template to retrieve the best-matching reaction condition descriptions as references for Table 4, based on the available templates within the USPTO reaction space. One template may correspond to thousands of reactions, so we limit our search to five items to manage costs while identifying the best matching generated and reference pairs. The results of generating reaction texts are shown in Table 4, where Llamole achieves the highest ROUGE-L but low BLEU-4 scores. The best ROUGE-L score for Llamole indicates its capacity to understand and maintain the overall structure of the answer after fine-tuning. The lower BLEU-4 scores may result from the A* search nature in Llamole, which explores vast space (300K) of possible reactions, leading to fewer exact n-gram matches with reference sentences. The manyto-many relationships between products and reactants, along with various conditions for the same reaction, diminish BLEU-4s effectiveness in evaluating Llamoles capabilities. Overall, Llamole is not merely memorizing reaction conditions but actively exploring possibilities, yielding more contextually coherent and meaningful outputs. D.3 Additional Discussion on Case Studies We present case studies for baseline LLMs using the same question as in Figure 7. Results are shown in Figure 7. The reference indicates one possible ground truth for molecular design with retrosynthetic pathways, noting that many alternatives exist. Compared to the reference, results in Figure 7 demonstrate that Llamole designs another molecule with similar structures, properties, and shorter synthesis routes, showcasing its potential for controllability and generating synthesizable molecules. Using ICL, Qwen2-7B fails to generate meaningful responses, despite indicating it possesses rich knowledge about molecular design. SFT allows Qwen2-7B to more strictly follow instructions, producing meaningful responses. However, text-only generation leads to hallucinations, as the generated templates do not yield expected products in retrosynthetic planning. Another example based on Llama-3.1/3-8B is provided in Figure 10. The ICL method may copy from the demonstrations to get the SMILES string CC(=O)C=Cc1cc(Cl)ccc1Cl. It also includes one SMILES string before the designed molecule, such as CN(C)c1ccc(C=NNc2ccc(I)cc2)cc1. However, it does not follow the instruction pattern and is therefore not automatically extracted for evaluation, as illustrated in appendix D.1.2. SFT follows the instructions through fine-tuning, using the pattern the designed molecule is: but generates invalid structures with meaninglessly repeated 23 sentences. In contrast, Llamole generates meaningful and valid molecular structures that generally satisfy the questions requirements. During text generation for molecular design, Llamole analyzes the question and includes more details about desirable structures, such as aromatic rings and aliphatic chains. Some functional groups, like hydroxyl, may not be precisely represented in the structure. This indicates need for enhanced text instruction adherence in Graph DiT. In addition to small molecules, we present polymer inverse design case in Figure 11 based on Qwen2-7B. The polymer has polymerization points denoted by * in the reference structure. Since polymerization reactions are not considered, we focus on the retrosynthetic routes to the monomer structures by replacing polymerization points with hydrogen atoms. In this case, ICL-based Qwen27B fails molecular design due to the same issue as in Figure 9, not following instructions to generate polymer structures. SFT-based Qwen2-7B generates polymer in SMILES format but invalid in chemical space. In contrast, Llamole successfully generates valid molecular structures through Graph DiT, satisfying the requirements of high CO2 permeability and low permeability to N2 and O2, and suggests two-step retrosynthetic pathway for the monomer structure. 24 Figure 9: Reference Answer with Generation Results from ICL or SFT Using the base LLM Qwen27B. The question is the same as in Figure 7. 25 Figure 10: Case Study for the Small Molecule: We include the reference answer and the generation results from ICL with Llama-3-8B and SFT and Llamole with Llama-3.1-8B. 26 Figure 11: Case Study for the Polymer: We include the reference answer and the generation results from ICL, SFT, and Llamole with Qwen2-7B."
        }
    ],
    "affiliations": [
        "MIT CSAIL",
        "MIT-IBM Watson AI Lab",
        "University of Notre Dame"
    ]
}