{
    "paper_title": "Dens3R: A Foundation Model for 3D Geometry Prediction",
    "authors": [
        "Xianze Fang",
        "Jingnan Gao",
        "Zhe Wang",
        "Zhuo Chen",
        "Xingyu Ren",
        "Jiangjing Lyu",
        "Qiaomu Ren",
        "Zhonglei Yang",
        "Xiaokang Yang",
        "Yichao Yan",
        "Chengfei Lyu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in dense 3D reconstruction have led to significant progress, yet achieving accurate unified geometric prediction remains a major challenge. Most existing methods are limited to predicting a single geometry quantity from input images. However, geometric quantities such as depth, surface normals, and point maps are inherently correlated, and estimating them in isolation often fails to ensure consistency, thereby limiting both accuracy and practical applicability. This motivates us to explore a unified framework that explicitly models the structural coupling among different geometric properties to enable joint regression. In this paper, we present Dens3R, a 3D foundation model designed for joint geometric dense prediction and adaptable to a wide range of downstream tasks. Dens3R adopts a two-stage training framework to progressively build a pointmap representation that is both generalizable and intrinsically invariant. Specifically, we design a lightweight shared encoder-decoder backbone and introduce position-interpolated rotary positional encoding to maintain expressive power while enhancing robustness to high-resolution inputs. By integrating image-pair matching features with intrinsic invariance modeling, Dens3R accurately regresses multiple geometric quantities such as surface normals and depth, achieving consistent geometry perception from single-view to multi-view inputs. Additionally, we propose a post-processing pipeline that supports geometrically consistent multi-view inference. Extensive experiments demonstrate the superior performance of Dens3R across various dense 3D prediction tasks and highlight its potential for broader applications."
        },
        {
            "title": "Start",
            "content": "Dens3R: Foundation Model for 3D Geometry Prediction XIANZE FANG, Alibaba Group, China JINGNAN GAO, Shanghai Jiao Tong University, China ZHE WANG, Alibaba Group, China ZHUO CHEN, Shanghai Jiao Tong University, China XINGYU REN, Shanghai Jiao Tong University, China JIANGJING LV, Alibaba Group, China QIAOMU REN, Alibaba Group, China ZHONGLEI YANG, Alibaba Group, China XIAOKANG YANG, Shanghai Jiao Tong University, China YICHAO YAN, Shanghai Jiao Tong University, China CHENGFEI LV, Alibaba Group, China Fig. 1. Dens3R is feed-forward visual foundation model that takes unposed images as input and outputs high-quality 3D pointmap with unified geometric dense prediction. Dens3R also accepts generalized inputs, supporting both multi-view and multi-resolution inputs. As versatile backbone, Dens3R achieves robust dense prediction under several scenarios and can be easily extended to downstream applications. Equal Contribution Project Leader Corresponding Author Authors Contact Information: Xianze Fang, Alibaba Group, China, fangxianze.fxz@ taobao.com; Jingnan Gao, Shanghai Jiao Tong University, China, gjn0310@sjtu.edu.cn; Zhe Wang, Alibaba Group, China, xingmi.wz@taobao.com; Zhuo Chen, Shanghai Jiao Tong University, China, ningci5252@sjtu.edu.cn; Xingyu Ren, Shanghai Jiao Tong University, China, rxy_sjtu@sjtu.edu.cn; Jiangjing Lv, Alibaba Group, China, jiangjing.ljj@taobao.com; Qiaomu Ren, Alibaba Group, China, renqiaomu.rqm@taobao. com; Zhonglei Yang, Alibaba Group, China, zhonglei.yzl@taobao.com; Xiaokang Yang, Shanghai Jiao Tong University, China, xkyang@sjtu.edu.cn; Yichao Yan, Shanghai Jiao Tong University, China, yanyichao@sjtu.edu.cn; Chengfei Lv, Alibaba Group, China, chengfei.lcf@taobao.com. Recent advances in dense 3D reconstruction have led to significant progress, yet achieving accurate unified geometric prediction remains major challenge. Most existing methods are limited to predicting single geometry quantity from input images. However, geometric quantities such as depth, surface normals, and point maps are inherently correlated, and estimating them in isolation often fails to ensure consistency, thereby limiting both accuracy and practical applicability. This motivates us to explore unified framework that explicitly models the structural coupling among different geometric properties to enable joint regression. In this paper, we present Dens3R, 3D foundation model designed for joint geometric dense prediction and adaptable to wide range of downstream tasks. Dens3R adopts two-stage training framework to progressively build pointmap representation that is both generalizable and intrinsically invariant. Specifically, we design lightweight shared encoder-decoder backbone and introduce 5 2 0 2 2 2 ] . [ 1 0 9 2 6 1 . 7 0 5 2 : r 2 Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv position-interpolated rotary positional encoding to maintain expressive power while enhancing robustness to high-resolution inputs. By integrating image-pair matching features with intrinsic invariance modeling, Dens3R accurately regresses multiple geometric quantities such as surface normals and depth, achieving consistent geometry perception from single-view to multi-view inputs. Additionally, we propose post-processing pipeline that supports geometrically consistent multi-view inference. Extensive experiments demonstrate the superior performance of Dens3R across various dense 3D prediction tasks and highlight its potential for broader applications. CCS Concepts: Computing methodologies Computer Vision. Additional Key Words and Phrases: Visual Foundation Model, 3D Geometry Prediction"
        },
        {
            "title": "Introduction",
            "content": "Recovering 3D geometric structures from static images is longstanding and fundamental problem in computer vision. Classical approaches, such as Structure-from-Motion (SfM) and Multi-View Stereo (MVS), demonstrate strong performance in controlled settings and have been widely adopted in broad range of 3D reconstruction applications. However, in unconstrained scenarioswhere camera intrinsics, extrinsics, or viewpoint information are unavailableachieving accurate and dense geometric prediction remains highly challenging. These conditions demand more generalizable and robust solutions capable of handling diverse and unstructured visual inputs. Existing methods for dense geometric prediction primarily fall into two categories. The first category mostly adopts generative models, utilizing strong image priors from pre-trained diffusion models or large-scale training datasets for dense prediction. For example, GenPercept [Xu et al. 2025] is used for depth prediction, and StableNormal [Ye et al. 2024] for normal estimation. This raises key issue: while image generation tasks typically benefit from their inherent ambiguity and multi-modal output characteristics, geometric prediction is fundamentally different. Geometric prediction is essentially deterministic task that needs to closely reflect the structural information of the underlying scene. Moreover, the pixel continuity and spatial smoothness required by geometric representations are difficult to naturally obtain through standard diffusion sampling mechanisms without structural constraints. Therefore, the direct application of diffusion models in geometric regression tasks faces significant challenges, especially in such tasks where strict one-to-one correspondence between input and output needs to be maintained. Based on this, we adopt regression-oriented framework to construct geometric mapping models in more efficient and interpretable way. Furthermore, the aforementioned methods mainly handle only one geometric quantity prediction and cannot generalize to output multiple geometric quantities in single forward pass. The second category includes DUSt3R [Wang et al. 2024] and its follow-up works [Leroy et al. 2024; Wang et al. 2025a,b]. These methods use regression models that can regress 3D point map representations with geometric properties, applied to dense prediction, including image pair matching and depth estimation. However, these methods typically focus on single prediction task, and other geometric quantities suffer severe performance degradation due to representation influences. This raises natural question: can we build unified model that simultaneously regresses multiple geometric quantities with high quality? We observe that existing methods like DUSt3R, when handling dense geometric regression tasks, overlook crucial geometric informationsurface normals. Traditionally, normals have been used to add high-frequency details to rough geometric structures to enhance rendering quality. However, our research finds that introducing normal information during geometric prediction can significantly improve the accuracy of point maps, resulting in more detailed and structurally consistent 3D representations. This is mainly because: 1) From the perspective of normal prediction, the inherent image pair matching capability in dense vision backbone networks helps alleviate monocular ambiguity and improve the stability and accuracy of normal prediction; 2) From the feature modeling perspective, normals possess good intrinsic invariance, which simplifies the mapping learning process and aids in model convergence and generalization. This modeling approach enables the model to simultaneously predict multiple geometric quantities (such as depth, normals, and point maps) from single view, effectively reducing dependence on multi-view supervision and simplifying the training process. However, training such multi-task, multi-output 3D foundation model still faces significant challenges. Geometric quantities are tightly coupled, and how to coordinate these relationships to achieve optimal overall performance requires carefully designed training strategies and architectural support. In this paper, we present Dens3R, foundation model for highquality geometric prediction. To this end, we design two-stage training framework that gradually builds versatile pointmap representation, which generalizes well to various downstream tasks. Specifically, we first construct dense vision backbone network with multi-task prediction capabilities. This network adopts shared encoder-decoder architecture, which significantly reduces model parameters while maintaining expressive power. To accommodate high-resolution inputs, we introduce position-interpolated rotary positional encoding, which effectively mitigates prediction degradation caused by increased input resolution. For the training strategy, we propose novel two-staged approach. In the first stage, the model leverages image pair matching features to learn scale-invariant point maps, capturing consistent spatial geometric structures across viewpoints. Subsequently, to fully exploit the one-to-one mapping property in normal estimation, we extend the pointmap representation into an intrinsically invariant form. This allows the model to independently attend to each viewpoint, thereby improving the accuracy of normal prediction. The learned geometric structures also assist in estimating other geometric quantities, such as depth, thereby simplifying their training processes. Finally, we design simple and efficient post-processing pipeline that supports multi-view inputs during inference, which enhances the geometric consistency of the model in real-world applications. In summary, we make the following contributions: We introduce Dens3R, dense 3D visual foundation model that demonstrates high-quality performance in various 3D tasks including pointmap reconstruction, depth estimation, normal prediction and image matching under several benchmark evaluations. We design novel training strategy with the intrinsicinvariant pointmap and shared Encoder-Decoder visual backbone to incorporate surface normals in unconstrained image-based dense 3D reconstruction, simplifying the training complexity of other 3D quantities and achieving better results without requiring excessive computation resources. We employ position-interpolated rotary positional encoding to preserve prediction accuracy at higher resolutions and support multi-resolution inputs. Extensive experiments on various benchmarks showcase our high-quality predictions of 3D geometric quantities, which further enable wide range of applications."
        },
        {
            "title": "2 Related Works\n2.1 Monocular depth and normal prediction",
            "content": "Monocular depth prediction has been extensively investigated and demonstrates strong capability in providing geometric priors for multitude of downstream tasks like image understanding and 3D reconstruction. The earliest pioneering researchers [Bhat et al. 2021, 2023; Eigen et al. 2014; Hu et al. 2024; Piccinelli et al. 2024; Yin et al. 2023] addressed this issue by estimating depth with metric scale. These methods usually rely heavily on data from specific sensors, which restricts the applicability and deteriorates the performance when confronted with complex scenes. Subsequently, deep learning approaches involve predicting relative depth either through direct regression [Chen et al. 2016, 2020; Godard et al. 2019; Li and Snavely 2018a; Ranftl et al. 2022; Yang et al. 2024a,b] or via generative modeling based on diffusion priors [Fu et al. 2024; Gui et al. 2024; Ke et al. 2024; Wan et al. 2023]. While monocular depth estimation has made significant strides, accurate 3D shape reconstruction from depth maps remains fundamentally dependent on precise camera intrinsic parameters. Meanwhile, normal maps serve as supervision for neural scene representation, bridging 2D and 3D worlds. The accurate estimation of the normal map can open up broader applications like material decomposition and relighting. On one hand, regression-based methods [Bansal et al. 2016; Eftekhar et al. 2021; Ranftl et al. 2021; Wang et al. 2015] utilize large-scale training datasets for robust estimation. DSINE [Bae and Davison 2024] proposes to leverage the per-pixel ray direction and try to model the inductive biases for surface normal estimation correctly. On the other hand, diffusion-based methods [Fu et al. 2024; Long et al. 2024; Ye et al. 2024] adapt the pretrained diffusion model as geometric cues predictor. Geowizard includes geometry switcher to disentangle mixed-sourced data into distinct sub-distributions for normal prediction. StableNormal repurposes the diffusion model for deterministic estimation tasks and can estimate sharp normals steadily. Nevertheless, these normal estimation methods often suffer from monocular ambiguity, leading to inaccurate and inconsistent results for complex scenes. In contrast, our method allows the communication between 3D geometric representation and normal prediction without known camera poses. This not only resolves the ambiguity but also achieves accurate 3D reconstruction with accurate normals. Dens3R: Foundation Model for 3D Geometry Prediction 3 2.2 Image Pair Matching in 3D Dense matching [Edstedt et al. 2023, 2024; Efe et al. 2021; Melekhov et al. 2019; Sarlin et al. 2020; Sun et al. 2021; Truong et al. 2021, 2020, 2023; Zhu and Liu 2023] has been proved to be effective in many scenarios and results in top performance in many benchmarks. However, these approaches cast matching as 2D problem, which restricts the application for visual localization. Thus anchoring image correspondence in 3D space is essential when these 2D-based methods fall short. Early methods [Bhalgat et al. 2023; He et al. 2020; Toft et al. 2020; Wang et al. 2020a; Yao et al. 2019; Yifan et al. 2022; Zhou et al. 2021] leverage epipolar constraints in order to improve accuracy or robustness. Recently, researchers [Wang et al. 2023b; Zhang et al. 2024] leverage diffusion models for pose estimation and demonstrate promising results by incorporating 3D geometric constraints into estimation formulation. MASt3R [Leroy et al. 2024] retrieves correspondences via 3D reconstruction from uncalibrated images by explicitly training local features for pairwise matching. However, MASt3R only grounds image-pair matching and overlooks other geometric predictions like depth and normal, while our method achieves unified geometric predictions and better matching."
        },
        {
            "title": "2.3 Dense Unconstrained Geometric Representations",
            "content": "Neural scene reconstructions [Barron et al. 2021, 2023; Kerbl et al. 2023; Lu et al. 2024; Martin-Brualla et al. 2021; Mildenhall et al. 2020; Wang et al. 2021a, 2023a; Yariv et al. 2021; Yu et al. 2024] usually require the camera intrinsic parameters and poses for optimization. The reconstruction quality of these methods is highly dependent on the accuracy of the camera intrinsics and poses. Later methods [Hong et al. 2024; Smart et al. 2024; Ye et al. 2025] propose to optimize the scene without known camera poses, but these methods usually take longer time and sacrifice reconstruction quality. To bypass estimation of camera parameters and poses, DUSt3R [Wang et al. 2024] proposes to directly map two input images in single forward pass, leading to more straightforward geometry representation. Subsequently, Spann3R [Wang and Agapito 2025] and Fast3R [Yang et al. 2025] augment DUSt3R to process an ordered set of images. MoGe [Wang et al. 2025b] further proposes affine-invariant pointmaps for monocular geometry estimation. VGGT [Wang et al. 2025a] utilizes 3D pointmaps and multiple prediction heads to predict geometric quantities from multi-view images input. However, the aforementioned methods overlook the normal attribute and fall short in prediction for complex scenarios. In contrast, our model takes advantage of pointmap representation and employs several prediction heads including the normal head to achieve unified geometric predictions."
        },
        {
            "title": "3 Method",
            "content": "This work aims to utilize single model to predict various geometric data from unconstrained images, including 3D pointmaps, depth maps, normal maps, and image-pair matching. To this end, we built backbone network based on dense visual transformers and designed input configurations that adapt to multi-resolution and multi-view requirements (Sec. 3.1). Since achieving accurate results through direct training with single model is challenging, we adopted twostage training approach. In the first stage, we train the backbone 4 Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Fig. 2. Overview of Dens3R. We propose Dens3R, dense visual transformer backbone featuring shared encoder-decoder architecture and multiple task-specific heads for geometric prediction. To train this foundation model, we adopt two-stage strategy. In Stage 1, we learn scale-invariant pointmap by enforcing cross-view mapping consistency across multiple viewpoints. In Stage 2, we incorporate surface normals and leverage one-to-one correspondence constraints to transform the representation into an intrinsic-invariant pointmap. Built upon this unified backbone, additional geometric prediction heads and downstream task branches can be seamlessly integrated to support wide range of applications. and heads to obtain scale-invariant pointmaps. In the second stage, we fine-tune the backbone on this foundation to obtain intrinsicinvariant pointmaps (Sec. 3.2). Finally, we further fine-tune the prediction heads for each downstream task to adapt to different application scenarios. Meanwhile, extending the model inputs to multi-view images in the inference stage significantly improves the overall inference quality. (Sec. 3.3)."
        },
        {
            "title": "3.1 Model Formulation",
            "content": "Shared Backbone. Motivated by recent advances in 3D vision [Jin et al. 2025; Leroy et al. 2024; Wang et al. 2025a, 2024], we aim to build foundation model capable of predicting diverse geometric quantities across different scenes and tasks. To this end, we adopt dense visual transformer as the backbone, learning from rich 3D annotated data. Given an image pair of image sequence (𝐼𝑖 )2 𝑖=1 R3𝐻 𝑊 , Dens3Rs dense visual transformer is function 𝑓 that maps the input to corresponding set of 3D quantities per frame: (𝐶𝑖, 𝑃𝑖, 𝐷𝑖, 𝑁𝑖, 𝑀𝑖 )2 𝑖=1 = 𝑓 ((𝐼𝑖 )2 𝑖=1), (1) where 𝐶𝑖 R9 is the camera parameters including both intrinsics and extrinsics, 𝐷𝑖 R𝐻 𝑊 is the depth map, 𝑁𝑖 R3𝐻 𝑊 is the normal map, and 𝑀𝑖 R𝐶 𝐻 𝑊 is the image-pair-matching with 𝐶-dimensional features. The overall architecture is illustrated in the upper part of Fig. 2. Similar to prior DUSt3R-based approaches [Leroy et al. 2024; Wang et al. 2025a,b, 2024], we first employ shared-weight encoder to process input image sequences and extract image features 𝐹𝑒𝑎𝑖 , which are then fed into the decoder. Unlike previous works, our approach introduces novel weight-sharing mechanism within the decoders, allowing the backbone to better capture spatial relationships across viewpoints and to model the holistic 3D scene structure. Given the need to predict wider range of geometric outputs, this design also significantly reduces memory and computational overhead, keeping the training and inference efficient. Moreover, the shared-weight strategy facilitates high-resolution input processing while effectively preventing memory overflow. Multi-resolution Input. Existing methods represented by DUSt3R perform excellently at fixed resolutions (such as 512), but their prediction accuracy significantly decreases when processing higherresolution inputs. The main challenge for this issue lies in the rotary positional encoding (RoPE) used in their ViT structure, which becomes unstable when inferring images beyond the training resolution range. Inspired by context window extension techniques in LLMs [Chen et al. 2023], we incorporate the position-interpolated RoPE into the ViT as simple yet effective improvement. We adapt the idea from context window to image resolution in the image domain, addressing the instability at higher resolutions. Considering the smooth properties of trigonometric functions in RoPE, interpolation is more stable than direct extrapolation when handling high resolutions. Specifically, let the original RoPE be 𝑅, the input sequence length be 𝐿, and for any RoPE embedding vector 𝑥, we obtain new encoding representation 𝑅 through interpolation. That is: 𝑅 (𝑥, 𝑚) = 𝑅(𝑥, (2) where 𝑚 is the position index and 𝐿 is the longer sequence. This position-interpolation encoding strategy significantly enhances the models robustness under high-resolution inputs, effectively avoiding the performance degradation caused by RoPE extrapolation. 𝑚𝐿 𝐿 ),"
        },
        {
            "title": "3.2 Foundation Model Training",
            "content": "The main challenge in training 3D geometric foundation models lies in the coupling among multiple prediction outputs, where mutual interference often leads to performance degradation. Existing methods typically focus on only one or two geometric tasks, resulting in poor generalization to others. To this end, we propose to build upon unified geometric representation since all geometric representations are inherently interconvertible. We adopt two-stage training paradigm, progressively learns strong geometric prior, which can be efficiently transferred to variety of 3D geometry prediction tasks via lightweight fine-tuning. Scale-Invariant Pointmap Training. In the first stage, we train the ViT backbone (Encoder-Decoder), pointmap head, and matching head to obtain scale-invariant pointmap 𝑃𝑖 . Following MASt3Rs [Leroy et al. 2024], we adopted (1) local 3D regression loss Lpts_loc , (2) Global 3D Regression Loss Lpts_glb , (3) Pointmap Normal Loss Lpts_n, (4) Pixel Matching Loss Lmatch. The details are as follows: (1) Local 3D Regression Loss Lpts_loc . For predicted camera, we use the local 3D regression loss to quantify the pointmap in its own coordinate frame. We apply mask derived from the ground-truth data to the pointmap and only evaluate the valid points when calculating the loss. We also employ normalization factor to handle the scale ambiguity between ground-truth and the predicted pointmaps. We set the factor 𝑧𝑣 as the average distance of all valid points in 𝑣𝑡ℎ camera coordinate frame to the origin: (cid:13) 𝑃 1,𝑣 (cid:13) 𝑚𝑎𝑠𝑘𝑒𝑑 (cid:13) (cid:13) 𝑃 1,𝑣 (cid:13) 𝑚𝑎𝑠𝑘𝑒𝑑 (cid:13) (cid:13) 𝑃 2,𝑣 (cid:13) 𝑚𝑎𝑠𝑘𝑒𝑑 (cid:13) (cid:13) 𝑃 2,𝑣 (cid:13) 𝑚𝑎𝑠𝑘𝑒𝑑 (cid:13) where 𝑧𝑣 is the corresponding factor of the ground-truth. Then the local 3D regression loss can be formulated as: (cid:13) (cid:13) (cid:13) (cid:13) 𝑃 𝑣,𝑣 𝑚𝑎𝑠𝑘𝑒𝑑 𝑃 𝑣,𝑣 𝑚𝑎𝑠𝑘𝑒𝑑 , 𝑣 {1, 2}, , 𝑣 {1, 2}, , 𝑣 {1, 2}, Lpts_loc = 𝑧𝑣 = 𝑧𝑣 = 1 𝑧𝑣 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 𝑧𝑣 (cid:13) (cid:13) (cid:13) (cid:13) (3) (4) + + where 𝑃𝑛,𝑚 denotes the pointmap from camera 𝑛 expressed in the coordinate frame of camera 𝑚. (2) Global 3D Regression Loss Lpts_glb . The global 3D regression loss is applied to quantify the pointmap expressed in another cameras coordinate frame. This loss function simultaneously optimizes for two objectives. It not only constrains the network to fit the pointmap shape of the image, but also aligns the pointmap to another paired image. The global regression loss is formulated as: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (5) 1 𝑧𝑡 Lpts_glb = 𝑃 𝑣,𝑡 𝑚𝑎𝑠𝑘𝑒𝑑 𝑃 𝑣,𝑡 𝑚𝑎𝑠𝑘𝑒𝑑 , 𝑣, 𝑡 {1, 2}, 𝑣 𝑡, 1 𝑧𝑡 where 𝑧𝑡 and 𝑧𝑡 is the normalization factor of the pointmap and the ground-truth. (3) Pointmap Normal Loss Lpts_n. To train an intrinsic-invariant pointmap from the scale-invariant pointmap, we use pointmap normal loss to encourage the pointmap learn smooth surface and sharp edge, making the pointmap perceives the normal information and the intrinsic-invariant property. Suppose 𝑁 𝑣,𝑣 is the groundtruth view-space normal expressed in its own camera coordinate frame and 𝑁 𝑣,𝑡 is the ground-truth normal expressed in another camera coordinate frame, the pointmap normal loss is the absolute error loss between the transformed normal and the ground-truth normal: Lpts_n = L1 (𝑁 𝑣,𝑣, ˆ𝑁 𝑣,𝑣) + L1 (𝑁 𝑣,𝑡 , ˆ𝑁 𝑣,𝑡 ), 𝑣, 𝑡 {1, 2}, 𝑣 𝑡, (6) where the ˆ𝑁 𝑣,𝑣 is the normal transformed from the local pointmap and ˆ𝑁 𝑣,𝑡 is the normal transformed from the global pointmap. Dens3R: Foundation Model for 3D Geometry Prediction 5 Fig. 3. Normal comparison. We demonstrate that the normal derived directly from the scale-invariant pointmap and MoGe both are not accurate enough. (4) Pixel Matching Loss Lmatch. We utilize the pixel matching loss proposed in MASt3R [Leroy et al. 2024] to learn accurate imagematching. This loss is based on the infoNCE [Oord et al. 2018] loss and ensures that each pixels descriptor in the first image match at most one pixels descriptor in another image. Suppose ˆM = (𝑖, 𝑗) is the set of ground-truth correspondences where the 𝑖𝑡ℎ pixel in the first image matches the 𝑗𝑡ℎ pixel in another, the loss can then be formulated as: Lmatch = log (𝑖,𝑗 ) ˆM 𝑠𝜏 (𝑖, 𝑗) (cid:205)𝑘 P1 𝑠𝜏 (𝑘, 𝑗) (cid:104) 𝑠𝜏 (𝑖, 𝑗) = exp 𝜏𝐷1 𝑖 𝐷2 𝑗 (cid:105) , + log 𝑠𝜏 (𝑖, 𝑗) (cid:205)𝑘 P2 𝑠𝜏 (𝑖, 𝑘) , (7) where 𝜏 is hyper-parameter, and 𝐷𝑖 and 𝐷 𝑗 are the corresponding descriptors in each image. With the above losses, we summarize the training objective as: L𝑠𝑡𝑎𝑔𝑒1 = Lpts_loc + 𝜂1Lpts_glb + 𝜂2Lpts_n + 𝜂3Lmatch, (8) where the loss weights 𝜂1, 𝜂2, and 𝜂3 are set as 1.0, 0.1 and 0.075, respectively. After training, we obtained scale-invariant pointmap capable of capturing rich spatial information. However, as shown in Fig. 3, the accuracy of normals obtained directly from the pointmap at this stage is still not ideal. Intrinsic-Invariant Pointmap Training. Although the pointbased representation learned in the first stage achieves good performance, it remains limited in its ability to generalize to other tasksparticularly surface normal estimation. Existing methods often struggle with monocular ambiguity in normal prediction, leading to inaccurate and inconsistent results. To this end, we expanded the pointmap representation in the second stage, proposing an intrinsic-invariant pointmap. This representation allows the model to form consistent geometric understanding of the same structure under different viewpoints, effectively improving the stability and generalization capability of normal estimation. Specifically, we introduce high-quality normal supervision based on the first stages point map, and jointly finetune the encoder-decoder module, point map prediction head, and newly added normal prediction head to achieve end-to-end optimization. In terms of supervision mechanism, we adjusted the initial \"one-to-many\" mapping (one image corresponding to multiple view supervisions) to \"one-to-one\" mapping, enabling the model to independently optimize normal prediction under single viewpoint. This strategy not only significantly reduces the ambiguity brought by multi-view supervision but also simplifies the training process and improves training efficiency and stability. We observe that the commonly-used confidence loss in previous works [Leroy et al. 2024; Wang et al. 2025a, 2024] tends to cause models to ignore complex scenarios such as reflective surfaces Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv and low textured areas. However, naively removing the loss without additional constraints leads to degraded performance, since previous models rely heavily on confidence weighting for point-view regression. In contrast, by utilizing the deterministic nature of normals, we obviate the need to rely on additional views, which further enables stable and accurate prediction. For detailed implementation, we explicitly connect normal to the pointmap representation, that is 𝑃𝑛 𝑖 = 𝑃𝑖 𝑛, where represents feature concatenation operation. The normal prediction head is connected after the initial point map training is completed, allowing the model to consistently output coherent normal mappings from the same input image, thereby internalizing this intrinsic invariance in the point map and maintaining geometric consistency across different views. (9) In the second stage, we add normal loss Ln for finetuning. (5) Predicted Normal Loss Ln. Apart from the intrinsic-invariant pointmap, we also design normal head to predict the view-space normal of each frame in input image pairs. We also use the L1 loss to supervise the normal prediction: Ln = L1 (𝑁 𝑣,𝑣, 𝑁 𝑣,𝑣), 𝑣 {1, 2}, (10) where 𝑁 is the ground-truth normal and 𝑁 is the direct prediction of the normal prediction head. The complete training objective for training stage 2 is as follows: L𝑠𝑡𝑎𝑔𝑒2 = Lpts_loc + 𝜆1Lpts_glb + 𝜆2Lpts_n + 𝜆3Ln, (11) where the loss weights 𝜆1, 𝜆2, and 𝜆3 are set as 1.0, 0.1 and 1.0, respectively. To further improve the performance of Dens3R on high-resolution inputs, we introduce coarse-to-fine training strategy. Specifically, we first fine-tune the model on 512 resolution images to establish stable geometric prior, and then fine-tune it on 1024 resolution images to further improve the prediction accuracy. In addition, combining high-resolution data also significantly improves the fidelity of point-based representations, ultimately enhancing the overall quality of dense 3D prediction."
        },
        {
            "title": "3.3 Model Inference",
            "content": "Heads Training. After training, we fine-tune it for downstream tasks by adding task-specific prediction heads on top of the frozen backbone network. Training only these new heads enables extension to various tasks such as depth estimation, normal estimation, matching estimation, and even segmentation and object detection. Multi-view Inputs. To enable Dens3R to efficiently process multiview inputs during inference, we design simple yet effective postprocessing step. This step ensures both computational efficiency in multi-view data processing and the consistency and accuracy of results. Specifically, based on Dens3Rs high-precision image pair matching predictions, we establish geometric mappings between different viewpoints by constructing and optimizing dense correspondence network across views. This approach effectively guides the model to understand geometric consistency between multiple viewpoints and accurately captures spatial relationships between views. Additionally, while maintaining high computational efficiency, it significantly improves the performance and stability of multi-view processing."
        },
        {
            "title": "4 Experiments\n4.1 Normal and Matching Prediction",
            "content": "We evaluate our Dens3R on several surface normal prediction datasets that include both indoor and outdoor scenes. We compare our method with regression-based methods such as DSINE [Bae and Davison 2024] and diffusion-based methods like StableNormal [Ye et al. 2024], GeoWizard [Fu et al. 2024] and Lotus [He et al. 2025]. Quantitative results are shown in Tab. 1, where Dens3R outperforms other methods across multiple benchmarks. Qualitative comparisons are provided in Fig. 4, also demonstrating that Dens3R generates more accurate and detailed normal maps. On the DIODE dataset, our method produces more accurate normals for reflective surfaces (e.g., car window) and finer details in backgrounds and tree structures. On in-the-wild scenes, Dens3R handles both object-centric and unbounded scenarios, producing more stable and intricate surface normals. Thanks to the intrinsic-invariant pointmap and the proposed training strategy, our method effectively reduces the ambiguity from monocular estimation, enabling more accurate and detailed predictions across various settings. For the image-matching task, we evaluate our method on the ZEB benchmark as shown in Tab. 2. We compare our method with previous dense image-matching methods and MASt3R [Leroy et al. 2024]. It can be seen that our method yields higher accuracy and surpasses previous methods across nearly all datasets, demonstrating our superior performance across various evaluation protocols."
        },
        {
            "title": "4.2 Pointmap and Depth Prediction",
            "content": "For monocular depth prediction, we evaluate our model on several datasets containing both indoor and outdoor scenes. We compare our method with MoGe [Wang et al. 2025b], VGGT [Wang et al. 2025a], MASt3R [Leroy et al. 2024] and DUSt3R [Wang et al. 2024]. The quantitative results are presented in Tab. 3. It can be seen that our method achieves accurate results in depth estimation. The qualitative comparison of the depth prediction is shown in Fig. 5 with the corresponding pointmaps. Our method achieves high-quality pointmap prediction and depth estimation with the intrinsic-invariant pointmap and the novel training strategy. As for pointmap prediction, MoGe and VGGT often fail to recover depth for reflective surfaces and tend to produce flattened pointmaps in background regions. In contrast, our method accomplishes to predict accurate depth with high-quality pointmaps. Moreover, our method yields more stable and high-quality predictions than MASt3R. It is also noteworthy that our method generates more accurate depth maps than DUSt3R, which can be reflected from the depth predictions for the Chandeliers."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We present high-quality geometric predictions for high-resolution inputs and various scenarios in Fig. 6 and Fig. 7. More evaluations and comparisons are also provided in the Supplementary Materials. Dens3R: Foundation Model for 3D Geometry Prediction 7 Fig. 4. Qualitative comparison of normal prediction. Dens3R generates more accurate and detailed normal maps than previous methods. Our method is capable of predicting accurate normals for reflective surfaces and in backgrounds. For both object-centric and unbounded scenes, our method also accomplishes to predict stable and intricate surface normals. NYUv2 ScanNet IBims-1 Method Mean Med DSINE Lotus* GeoWizard StableNormal Ours 18.6 17.5 20.4 19.7 16.1 9.9 8.6 11.9 10.5 7. 𝛿11.25 Mean Med 9.9 18.6 56.1 8.8 18.1 58.7 13.9 21.4 47.0 10.1 18.1 53.0 7.1 16.9 62.5 𝛿11.25 Mean Med 8.3 18.8 56.1 5.6 19.2 58.2 9.7 19.7 37.1 8.1 17.2 56.0 4.3 16.0 64.0 Sintel 𝛿11.25 Mean Med 28.1 34.9 64.1 28.0 35.7 66.2 34.3 41.6 58.4 27.0 35.0 66.7 21.4 30.7 72.2 DIODE-outdoor 𝛿11.25 Mean Med 14.5 22.0 21.5 15.9 24.7 20.5 19.8 27.0 11.8 16.1 26.9 19.5 12.8 20.8 28.9 𝛿11.25 39.6 32.9 24.0 36.1 43. Table 1. Quantitative comparison of normal prediction. We report the mean and median angular errors with each cell colored to indicate the best and the second . Dens3R achieves accurate normal prediction for both indoor and outdoor scenes. *Note that we utilize Lotus-G for fair comparison. Method SIFT SuperGlue LoFTR DKM ROMA MASt3R Ours Mean Real AUC@5 Simulate AUC@5 AUC@5 GL3 BLE ETI ETO KIT WEA SEA NIG MUL SCE ICL GTA 31.8 34.3 39.1 51.2 53.2 59.9 64.5 43.5 43.2 50.6 63.3 61.8 57.8 61.3 33.6 34.2 43.9 53.0 53.8 52.3 59.2 49.9 58.7 62.6 73.9 76.7 66.2 74.7 48.7 61.0 61.6 76.7 82.7 78.1 81.1 35.2 29.0 35.9 43.4 43.2 46.2 55. 21.4 28.3 26.8 34.6 36.7 52.8 57.4 44.1 48.4 47.5 52.5 53.2 70.5 71.7 14.7 18.8 17.6 24.5 26.6 43.7 50.4 33.4 34.8 41.4 56.6 60.7 70.1 71.3 7.6 2.8 10.2 32.2 33.8 53.9 53.7 14.8 15.4 25.6 42.5 45.4 60.1 66. 43.9 36.5 45.0 61.6 64.3 67.7 71.7 Table 2. Benchmark on image matching on ZEB dataset. We report the AUC values with each cell colored to indicate the best and the second . Our method outperforms previous methods in almost all the metrics. Method GenPercept Lotus* DepthAnythingV2 DUSt3R VGGT MoGe Ours REL RMSE 0.214 0.052 0.262 0.053 0.204 0.049 0.197 0.046 0.194 0.038 0.167 0.035 0.189 0.042 NYUv2 𝛿1 96.7 96.5 97.3 97.1 98.0 97.9 97. DIODE-indoor DIODE-outdoor 𝛿2 99.3 99.1 99.3 99.3 99.4 99.4 99.3 𝛿3 REL RMSE 0.924 0.107 99.8 1.123 0.111 99.7 0.878 0.091 99.8 0.375 0.083 99.8 0.404 0.064 99.8 0.879 0.080 99.9 0.372 0.072 99.8 𝛿1 89.1 88.7 92.5 92.0 93.1 92.6 93.7 𝛿2 96.0 96.0 97.3 97.7 98.0 97.3 97. 𝛿3 REL RMSE 5.571 0.727 98.1 9.960 0.488 98.4 5.525 0.705 98.6 5.217 0.451 99.0 4.861 0.400 99.2 5.177 0.578 98.7 4.740 0.387 98.8 𝛿1 67.3 47.1 67.8 67.7 70.6 72.8 72.2 𝛿2 84.2 63.3 83.4 84.3 84.9 86.7 87.0 𝛿3 90.6 71.8 89.7 90.7 90.6 91.9 92.3 Table 3. Quantitative comparison on monocular depth prediction. We report the relative point error (REL), root mean square error (RMSE) and the percentage of inliers 𝛿1, 𝛿2, 𝛿3 with each cell colored to indicate the best and the second . *Note that we utilize Lotu-G disparity model for comparison. We then conducted comprehensive ablation studies for our key components: the position-interpolated rotary positional encoding, the intrinsic-invariant training and the coarse-to-fine training strategy. Method NYUv2 ScanNet IBims Mean 𝛿11.25 Mean 𝛿11.25 Mean 50.6 50.5 62.5 17.8 17.6 16.1 w/o IIT w/o C2F Ours 18.6 17.8 16.9 Table 4. Normal quantitative metrics for ablation. We demonstrate that both the intrinsic-invariant training and coarse-to-fine strategy contributes to accurate normal predictions. 20.2 18.6 16. 49.4 58.8 64.0 𝛿11.25 56.8 63.9 72.2 Position-Interpolated Rotary Positional Encoding. Dens3R can support multi-resolution image inputs. With the position-interpolated rotary positional encoding and the coarse-to-fine training strategy, our method can prevent performance degradation when handling high-resolution inputs. As shown in Fig. 8a, we can generate accurate and well-structured pointmaps with the position-interpolated 8 Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv RoPE, preventing the model from producing overlapping or inconsistent pointmaps at higher resolutions. Intrinsic-Invariant Training. Our approach first learns scaleinvariant pointmap, which is then transformed into an intrinsicinvariant pointmap via subsequent intrinsic-invariant training. We find that jointly training the pointmap and normal at the initial scale-invariant stage leads to instability and poor convergence. This is because pointmaps and normal maps lie in different data domains, and coupling their supervision potentially increases training complexity. While GeoWizard [Fu et al. 2024] addresses this domain gap with task switcher, we adopt two-stage training scheme to learn an intrinsic-invariant pointmap, ensuring stable learning. As shown in Tab. 4 and in Fig. 8b, the performance of the model will degrade without the intrinsic-invariant training. Coarse-to-Fine Training. Our model is trained on diverse training dataset of varying quality. To better utilize the full training set, we implement coarse-to-fine training strategy that gradually increases resolution and data fidelity. In the coarse stage, we set the max resolution of the training images as 512 and enable all the training data. In the fine stage, we increase the image resolution to 1024 pixels and restrict training to the high-resolution data only. As demonstrated in Tab. 4 and in Fig. 8b, this strategy improves prediction accuracy, particularly for high-resolution outputs."
        },
        {
            "title": "4.4 Downstream Applications",
            "content": "Segmentation Head Training. Dens3R serves as visual foundation model that can be finetuned for several downstream tasks. We demonstrate this by training new prediction head for segmentation task while keeping our backbone frozen. As shown in Fig. 8c, the segmentation head can generate accurate results, with much more effortless training than large segmentation model. Surface Reconstruction. Dens3R can improve surface reconstruction quality by its sharp and accurate normals. We demonstrate this by utilizing our predicted normals as the supervision for NeuS [Wang et al. 2021a] training. The results are showcased in Fig. 8d. It can be seen that the final reconstruction results are improved due to the strong normal prior provided by our Dens3R."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose Dens3R, 3D foundation model for dense geometric prediction that jointly regresses multiple geometric quantities, including depth, surface normals, and pointmaps, from unconstrained image inputs. Unlike previous approaches that estimate geometry in isolation, Dens3R explicitly models the structural coupling among these properties to ensure consistency and improves overall accuracy. We utilize two-stage training framework with coarse-to-fine strategy and build an accurate intrinsic-invariant pointmap representation. In addition, we design lightweight encoder-decoder architecture and position-interpolated rotary positional encoding to enable scalable and high-fidelity inference for high-resolution inputs. Moreover, Dens3R incorporates geometrically consistent post-processing pipeline for multi-view inputs. Extensive experiments demonstrate our superior performance across various 3D prediction benchmarks and highlight the potential as versatile backbone for broader downstream applications."
        },
        {
            "title": "References",
            "content": "Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, Áron Monszpart, Victor Adrian Prisacariu, Daniyar Turmukhambetov, and Eric Brachmann. 2022. Map-free Visual Relocalization: Metric Pose Relative to Single Image. In ECCV, Vol. 13661. 690708. Gwangbin Bae and Andrew J. Davison. 2024. Rethinking Inductive Biases for Surface Normal Estimation. In CVPR. 95359545. Aayush Bansal, Bryan C. Russell, and Abhinav Gupta. 2016. Marr Revisited: 2D-3D Alignment via Surface Normal Prediction. In CVPR. 59655974. Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, and Pratul P. Srinivasan. 2021. Mip-NeRF: Multiscale Representation for Anti-Aliasing Neural Radiance Fields. In ICCV. 58355844. Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. 2023. Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. In ICCV. 19697 19705. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. 2021. ARKitScenes - Diverse Real-World Dataset for 3D Indoor Scene Understanding Using Mobile RGB-D Data. In NeurIPS Datasets and Benchmarks. Yash Bhalgat, João F. Henriques, and Andrew Zisserman. 2023. Light Touch Approach to Teaching Transformers Multi-view Geometry. In CVPR. 49584969. Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. 2021. AdaBins: Depth Estimation Using Adaptive Bins. In CVPR. 40094018. Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. 2023. ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth. arxiv preprint arXiv:2302.12288 (2023). Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays. 2019. Argoverse: 3D Tracking and Forecasting with Rich Maps. In CVPR. 87488757. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending Context Window of Large Language Models via Positional Interpolation. arxiv preprint arXiv:2306.15595 (2023). Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. 2016. Single-Image Depth Perception in the Wild. In NeurIPS. 730738. Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. 2020. OASIS: Large-Scale Dataset for Single Image 3D in the Wild. In CVPR. 676685. Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon Sohn. 2019. large RGB-D dataset for semi-supervised monocular depth estimation. arXiv preprint arXiv:1904.10230 (2019). Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. 2022. ABO: Dataset and Benchmarks for Real-World 3D Object Understanding. In CVPR. 2109421104. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. 2023. Objaverse-XL: Universe of 10M+ 3D Objects. In NeurIPS. Johan Edstedt, Ioannis Athanasiadis, Mårten Wadenbäck, and Michael Felsberg. 2023. DKM: Dense Kernelized Feature Matching for Geometry Estimation. In CVPR. 17765 17775. Johan Edstedt, Qiyu Sun, Georg Bökman, Mårten Wadenbäck, and Michael Felsberg. 2024. RoMa: Robust Dense Feature Matching. In CVPR. 1979019800. Ufuk Efe, Kutalmis Gokalp Ince, and A. Aydin Alatan. 2021. DFM: Performance Baseline for Deep Feature Matching. In CVPRW. 42844293. Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. 2021. Omnidata: Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans. In ICCV. 1076610776. David Eigen, Christian Puhrsch, and Rob Fergus. 2014. Depth Map Prediction from Single Image using Multi-Scale Deep Network. In NeurIPS. 23662374. Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. 2024. GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from Single Image. In ECCV, Vol. 15080. 241258. Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. 2016. Virtual worlds as proxy for multi-object tracking analysis. In CVPR. 43404349. Clément Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. 2019. Digging Into Self-Supervised Monocular Depth Estimation. In ICCV. 38273837. Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, and Björn Ommer. 2024. DepthFM: Fast Monocular Depth Estimation with Flow Matching. arxiv preprint arXiv:2403.13788 (2024). Jose L. Gómez, Manuel Silva, Antonio Seoane, Agnés Borràs, Mario Noriega, German Ros, Jose A. Iglesias-Guitian, and Antonio M. López. 2025. All for one, and one for all: UrbanSyn Dataset, the third Musketeer of synthetic driving scenes. Neurocomputing Dens3R: Foundation Model for 3D Geometry Prediction 637 (2025), 130038. 16231637. Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Liu, Bingbing Liu, and Ying-Cong Chen. 2025. Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction. In ICLR. Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. 2021. Common Objects in 3D: Large-Scale Learning and Evaluation of Real-Life 3D Category Reconstruction. In ICCV. 1088110891. Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. 2020. Epipolar Transformers. Stephan Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. 2016. Playing for In CVPR. 77767785. Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jiaolong Yang, Seungryong Kim, and Chong Luo. 2024. Unifying Correspondence, Pose and NeRF for Pose-Free Novel View Synthesis from Stereo Pairs. In CVPR. Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. 2024. Metric3D v2: Versatile Monocular Geometric Foundation Model for Zero-Shot Metric Depth and Surface Normal Estimation. IEEE Trans. Pattern Anal. Mach. Intell. 46, 12 (2024), 1057910596. Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. 2025. LVSM: Large View Synthesis Model with Minimal 3D Inductive Bias. In ICLR. Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. 2024. Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation. In CVPR. 94929502. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph. 42, 4 (2023), 139:1139:14. Vincent Leroy, Yohann Cabon, and Jérôme Revaud. 2024. Grounding Image Matching in 3D with MASt3R. In ECCV, Vol. 15130. 7191. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, Mona Anvari, Minjune Hwang, Manasi Sharma, Arman Aydin, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R. Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, and Li Fei-Fei. 2022. BEHAVIOR-1K: Benchmark for Embodied AI with 1, 000 Everyday Activities and Realistic Simulation.. In CORL, Vol. 205. 8093. Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. 2023. Matrixcity: large-scale city dataset for city-scale neural rendering and beyond. In ICCV. 32053215. Zhengqi Li and Noah Snavely. 2018a. MegaDepth: Learning Single-View Depth Prediction From Internet Photos. In CVPR. 20412050. Zhengqi Li and Noah Snavely. 2018b. MegaDepth: Learning Single-View Depth Prediction from Internet Photos. In CVPR. 20412050. Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. 2024. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR. 2216022169. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. 2024. Wonder3D: Single Image to 3D Using Cross-Domain Diffusion. In CVPR. 99709980. Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. 2024. Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering. CVPR (2024). Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. 2021. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In CVPR. 72107219. Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andrés Bruhn. 2023. Spring: High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo. In CVPR. 49814991. Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Pollefeys, Esa Rahtu, and Juho Kannala. 2019. DGC-Net: Dense Geometric Correspondence Network. In WACV. 10341042. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV. Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 2019. 3D Ken Burns Effect from Single Image. ACM Trans. Graph. 38, 6 (2019), 184:1184:15. Aaronvanden Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. arXiv preprint arXiv:1807.03748 (2018). Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segù, Siyuan Li, Luc Van Gool, and Fisher Yu. 2024. UniDepth: Universal Monocular Metric Depth Estimation. In CVPR. 1010610116. Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, and Jia Deng. 2024. Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation. In CVPR. 2178321794. René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. 2021. Vision Transformers for Dense Prediction. In ICCV. 1215912168. René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. 2022. Towards Robust Monocular Depth Estimation: Mixing Datasets for ZeroShot Cross-Dataset Transfer. IEEE Trans. Pattern Anal. Mach. Intell. 44, 3 (2022), data: Ground truth from computer games. In ECCV, Vol. 9906. 102118. Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. 2021. Hypersim: Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. In ICCV. 1089210902. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez. 2016. The SYNTHIA Dataset: Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes. In CVPR. 32343243. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. 2020. SuperGlue: Learning Feature Matching With Graph Neural Networks. In CVPR. 49374946. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. 2019. Habitat: Platform for Embodied AI Research. In ICCV. 93389346. Philipp Schröppel, Jan Bechtold, Artemij Amiranashvili, and Thomas Brox. 2022. Benchmark and Baseline for Robust Multi-view Depth Estimation. In 3DV. 637 645. Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. 2024. Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs. arxiv preprint arXiv:2408.13912 (2024). Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. 2021. LoFTR: Detector-Free Local Feature Matching With Transformers. In CVPR. 89228931. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. 2020. Scalability in Perception for Autonomous Driving: Waymo Open Dataset. In CVPR. 24432451. Carl Toft, Daniyar Turmukhambetov, Torsten Sattler, Fredrik Kahl, and Gabriel J. Brostow. 2020. Single-Image Depth Prediction Makes Feature Matching Easier. In ECCV, Vol. 12361. 473492. Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. 2021. SMD-Nets: Stereo Mixture Density Networks. In CVPR. 89428952. Prune Truong, Martin Danelljan, Luc Van Gool, and Radu Timofte. 2021. Learning Accurate Dense Correspondences and When To Trust Them. In CVPR. 57145724. Prune Truong, Martin Danelljan, and Radu Timofte. 2020. GLU-Net: Global-Local Universal Network for Dense Flow and Correspondences. In CVPR. 62576267. Prune Truong, Martin Danelljan, Radu Timofte, and Luc Van Gool. 2023. PDC-Net+: Enhanced Probabilistic Dense Correspondence Network. IEEE Trans. Pattern Anal. Mach. Intell. 45, 8 (2023), 1024710266. Qiang Wan, Zilong Huang, Bingyi Kang, Jiashi Feng, and Li Zhang. 2023. Harnessing Diffusion Models for Visual Perception with Meta Prompts. arxiv preprint arXiv:2312.14733 (2023). Hengyi Wang and Lourdes Agapito. 2025. 3D Reconstruction with Spatial Memory. In 3DV. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. 2025a. VGGT: Visual Geometry Grounded Transformer. In CVPR. Jianyuan Wang, Christian Rupprecht, and David Novotný. 2023b. PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment. In ICCV. 97399749. Kaixuan Wang and Shaojie Shen. 2020. Flow-motion and depth network for monocular stereo and beyond. IEEE Robotics and Automation Letters 5, 2 (2020), 33073314. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. 2021a. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction. In NeurIPS. 2717127183. Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. 2021b. Irs: large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation. In ICME. 16. Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, and Noah Snavely. 2020a. Learning Feature Descriptors Using Camera Pose Supervision. In ECCV, Vol. 12346. 757774. Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. 2025b. MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision. In CVPR. Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jérôme Revaud. 2024. DUSt3R: Geometric 3D Vision Made Easy. In CVPR. 2069720709. Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. 2020b. TartanAir: Dataset to Push the Limits of Visual SLAM. (2020), 49094916. Xiaolong Wang, David F. Fouhey, and Abhinav Gupta. 2015. Designing deep networks for surface normal estimation. In CVPR. 539547. 10 Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. 2023a. NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction. In ICCV. Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. 2024. RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos. In CVPR. 2237822389. Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. 2025. What Matters When Repurposing Diffusion Models for General Dense Perception Tasks?. In ICLR. Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. 2025. Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass. In CVPR. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. 2024a. Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data. In CVPR. 1037110381. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. 2024b. Depth Anything V2. In NeurIPS. Yuan Yao, Yasamin Jafarian, and Hyun Soo Park. 2019. MONET: Multiview SemiSupervised Keypoint Detection via Epipolar Divergence. In ICCV. 753762. Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. 2020. BlendedMVS: Large-Scale Dataset for Generalized Multi-View Stereo Networks. In CVPR. 17871796. Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume Rendering of Neural Implicit Surfaces. In NeurIPS. 48054815. Botao Ye, Sifei Liu, Haofei Xu, Li Xueting, Marc Pollefeys, Ming-Hsuan Yang, and Peng Songyou. 2025. No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images. In ICLR. Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. 2024. StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal. ACM Trans. Graph. 43, 6 (2024), 250:1 250:18. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. 2023. ScanNet++: High-Fidelity Dataset of 3D Indoor Scenes. In ICCV. 1222. Wang Yifan, Carl Doersch, Relja Arandjelovic, João Carreira, and Andrew Zisserman. 2022. Input-level Inductive Biases for 3D Reconstruction. In CVPR. 61666176. Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. 2023. Metric3D: Towards Zero-shot Metric 3D Prediction from Single Image. In ICCV. 90099019. Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. 2024. MipSplatting: Alias-free 3D Gaussian Splatting. CVPR (2024). Amir Zamir, Alexander Sax, , William Shen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. 2018. Taskonomy: Disentangling Task Transfer Learning. In CVPR. 37123722. Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. 2024. Cameras as Rays: Pose Estimation via Ray Diffusion. In ICLR. Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. 2020. Structured3D: Large Photo-realistic Dataset for Structured 3D Modeling. In ECCV, Vol. 12354. 519535. Qunjie Zhou, Torsten Sattler, and Laura Leal-Taixé. 2021. Patch2Pix: Epipolar-Guided Pixel-Level Correspondences. In CVPR. 46694678. Shengjie Zhu and Xiaoming Liu. 2023. PMatch: Paired Masked Image Modeling for Dense Geometric Matching. In CVPR. 2190921918. Dens3R: Foundation Model for 3D Geometry Prediction 11 Fig. 5. Qualitative comparison of depth maps and pointmaps. We compare our method with previous DUSt3R-based methods and demonstrate high-quality depth prediction results. Dens3R also reconstructs more stable and accurate pointmap than previous methods. Fig. 6. High-quality geometric predictions for high-resolution (2K) inputs. Please zoom in to better observe the fine-grained details. Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Fig. 7. High-quality unified geometric predictions for various scenarios. We demonstrate accurate normal and depth predictions with high-quality 3D pointmaps for challenging object-centric, indoor and outdoor scenes. (a) High-resolution inference comparison. Our method supports highresolution input and generates accurate and well-structured pointmaps. (b) Normal comparison for ablation. The intrinsic-invariant training enables accurate normal prediction and the coarse-to-fine training enhances details. (c) Segmentation results. Our model can be easily extended to segmentation tasks by training new prediction head with the backbone frozen. (d) Normal supervision results. We demonstrate the effectiveness of using our normal as the supervision of surface reconstruction. Fig. 8. Ablation and downstream applications."
        },
        {
            "title": "A Implementation Details",
            "content": "Datasets. To train the visual foundation model, we collect and reorganize large-scale training dataset containing various data types. The dataset includes indoor scenes, outdoor scenes, and object-level data. It is noteworthy that the quality of training data has substantial impact on model performance. We then make the most of high-quality synthetic data in the training process for more accurate and robust predictions. We divide all the data into three types based on their quality. Data of type is collected from synthetic rendering process with the highest quality. Data of type also originates from synthetic rendering, but they possess certain quality issues like insufficient resolution or absence of background or imprecise original 3D models, etc. Data of type is obtained from the real world using cameras and depth sensors. We also carefully allocate the proportions of each dataset to attain the optimal model training performance. We summarize and present this dataset information in Tab. 5. Training Details. During our coarse-to-fine training, we first utilize all the images with 512 resolution and train our model for about two weeks in the coarse-stage training. Then we only utilize the images from type dataset and minor portion of type dataset and set the image resolution to 1024 for the fine-stage training. We utilize 32 Nvidia A800 GPUs for both the coarse and fine stage training. As for model inference, our model only requires single Nvidia RTX3090 GPU for 1024-resolution image inputs."
        },
        {
            "title": "B Additional Visualization",
            "content": "We provide additional unified geometric prediction results of monocular inputs in Fig. 9 and 2-view inputs in Fig. 10. It can be seen that our model achieves robust and high-quality unified geometric predictions across several scenarios."
        },
        {
            "title": "C Normal and Depth Comparison",
            "content": "Dens3R predicts robust and accurate normal and depth for various scenarios. As shown in Fig. 11, we demonstrate that the intrinsicinvariant training assists the pointmap to capture the geometric information from normal. Then the normal prediction head further predicts sharper edges and more accurate results. We provide the normal prediction comparison of the Kitti dataset in Fig. 12. It can be seen that our method generates the most accurate and sharp normals. We also provide more comparison of normal map prediction in Fig 15 using in-the-wild images and in Fig. 16 using DL3DV dataset. It can be seen that our method predicts sharper and more accurate normal across various scenarios. We also compare our method with the normal map derived from the predicted depth map of MoGe [Wang et al. 2025b], the results are shown in Fig. 13. It can be seen that Dens3R can handle normal predictions for reflective surfaces and accomplishes to generate richer details. We also provide the full quantitative comparison in Tab. 6 and full ablation comparison in Tab. 7, which are partly shown in Tab. 1 and Tab. 4 in the main paper correspondingly. We provide the depth prediction comparison in Fig. 17, it can be seen that our method generates the most accurate depth maps even for reflective surfaces. Since VGGT [Wang et al. 2025a] also predicts Dens3R: Foundation Model for 3D Geometry Prediction 13 multiple quantities including depth and matching, we further compare our predicted depth map with VGGT. We demonstrate more accurate depth predictions of NYUv2 dataset in Fig. 18. We also showcase the accurate prediction of both indoor scenes of NYUv2 dataset and outdoor scenes of Kitti dataset. It can be seen in Fig. 19 and Fig. 20 that our model also achieves accurate human depth estimation that can be further utilized for detection and autonomous driving."
        },
        {
            "title": "D Camera Pose Estimation Comparison",
            "content": "Dens3R can also perform accurate camera pose estimation through single feed-forward pass. We conduct extended experiments to demonstrate its accuracy. We utilize the map-free benchmark [Arnold et al. 2022] following the MASt3R protocol [Leroy et al. 2024], which is challenging dataset aiming at localizing the camera in metric space given single reference image without any map. We present the camera pose estimation (Map-free relocalization) comparison in Tab. 8. It can be seen that Dens3R outperforms previous methods in nearly all the metrics, demonstrating highly accurate camera pose estimation results."
        },
        {
            "title": "E Image Matching Visualization Results",
            "content": "For image-matching, apart from the ZEB dataset, we also provide the quantitative comparison of the Scannet-1500 dataset in Tab. 9 and the MegaDepth-1500 dataset in Tab. 10. The comparisons on the ScanNet-1500 and the MegaDepth-1500 benchmarks further demonstrate our superior performance over pervious DUSt3R-based method MASt3R [Leroy et al. 2024] and VGGT [Wang et al. 2025a]. We also demonstrate our dense and accurate matching results of several challenging cases in Fig. 21. We visualize the matching results from MegaDepth-1500 dataset, ScanNet-1500 dataset and Aachen dataset. It can be seen that our method can handle imagematching for 1) inputs with different lighting conditions, 2) inputs taken from different views with large-angle difference, 3) inputs with small overlapping regions. The accuracy of our method is shown on the upper left of each matching image-pair. High-Resolution Inference Comparison We showcase more comparison of high-resolution inputs with DUSt3R [Wang et al. 2024] and VGGT [Wang et al. 2025a] in Fig. 22. It can be seen that our method can handle higher-resolution inputs without causing degenerated predictions like previous methods with our proposed position-interpolated rotary positional encoding. Shared Encoder-Decoder Backbone Ablation Dens3R employs dense visual transformer backbone designed to capture spatial relationships across viewpoints and capture the global 3D geometric information of scenes. Different from previous methods, both the encoder and decoder components in our architecture share weights. The comparison of the network parameters and the memory cost is shown in Tab. 11. Since our model deals with more 3D quantities than previous methods, the framework initially requires higher memory cost. Employing the shared encoderdecoder structure also resolves this issue, reducing the memory cost and network parameters without losing the prediction quality. Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Fig. 9. Additional visual results for unified geometric predictions of monocular inputs by our methods Dens3R: Foundation Model for 3D Geometry Prediction 15 Fig. 10. Additional visual results for unified geometric predictions of 2-view images inputs by our method. Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Dataset Type Hypersim[Roberts et al. 2021] UnrealStereo4K[Tosi et al. 2021] MatrixCity[Li et al. 2023] Infinigen[Raistrick et al. 2024] Behavior [Li et al. 2022] Structure3D[Zheng et al. 2020] GTASFM[Wang and Shen 2020] GTAV[Richter et al. 2016] VirtualKitti[Gaidon et al. 2016] IRS[Wang et al. 2021b] UrbanSyn[Gómez et al. 2025] Spring[Mehl et al. 2023] ScanNet++[Yeshwanth et al. 2023] ABO[Collins et al. 2022] GObjaverseXL[Deitke et al. 2023] StaticThings3D[Schröppel et al. 2022] BlendedMVS[Yao et al. 2020] Habitat[Savva et al. 2019] Taskonomy[Zamir et al. 2018] ARKitScenes[Baruch et al. 2021] Tartanair[Wang et al. 2020b] Synthia[Ros et al. 2016] KenBurns[Niklaus et al. 2019] MegaDepth[Li and Snavely 2018b] Waymo[Sun et al. 2020] Co3dv2[Reizenstein et al. 2021] WildRGBD[Xia et al. 2024] NianticMapFree[Arnold et al. 2022] DL3DV[Ling et al. 2024] DIMLIndoor[Cho et al. 2019] ArgoverseStereo[Chang et al. 2019] A A A B B B C C Applied Losses Lpts_loc, Lpts_glb Lpts_n Lmatch Ln Image Pairs Ratio 6.77% 1.8M 6.77% 0.9M 6.77% 0.7M 6.77% 2.8M 6.77% 6.8M 0.2M 4.06% 0.2M 13.53% 0.6M 13.53% 4.0M 13.53% 0.41% 74K 0.41% 7.0K 0.41% 10K 3.5M 2.0M 6.8M 0.3M 1.1M 1.3M 1.8M 2.2M 4.5M 2.6M 0.3M 1.8M 1.1M 1.2M 1.1M 3.7M 1.2M 0.9M 4.0K 1.35% 1.35% 1.35% 1.35% 1.35% 0.68% 0.68% 0.68% 0.68% 0.68% 0.68% 1.35% 1.35% 1.35% 1.35% 1.35% 1.35% 0.68% 0.68% Table 5. Training dataset information. We reorganize large-scale training dataset and divide the data into three types based on their quality. We also showcase the training objectives we apply during training, the number of image pairs and the corresponding dataset ratio. integrating the predicted attributes with modern structure-frommotion (SfM) pipelines such as Glomap."
        },
        {
            "title": "I Limitation",
            "content": "Although Dens3R outperforms previous methods in geometric predictions, predicting accurate results for inputs with thin structures remains significant challenge. Restricted by the networks limited capacity and the presence of noisy training data, our method may predict inaccurate results for these inputs. As shown in Fig. 14, the prediction quality for thin structures still require further improvement. Fig. 11. Normal prediction comparison of different training stages. Multi-view Image Inputs Apart from multi-resolution image inputs, our model also design simple yet effective post-processing pipeline and supports multiview inputs. The results are demonstrate in Fig. 23. It can be seen that our method enables high-quality 3D reconstruction even without known camera poses. In addition, the predictions of Dens3R can be used to initialize robust and accurate 3D reconstructions by Method Mean Med 𝛿11.25 𝛿22.5 𝛿30 DSINE Lotus GeoWizard StableNormal Ours DSINE Lotus GeoWizard StableNormal Ours DSINE Lotus GeoWizard StableNormal Ours DSINE Lotus GeoWizard StableNormal Ours NYUv2 (indoor) 9.9 8.6 11.9 10.5 7.4 56.1 58.7 47.0 53.0 62.5 ScanNet (indoor) 9.9 8.8 13.9 10.1 7. 56.1 58.2 37.1 56.0 64.0 IBims-1 (indoor) 8.3 5.6 9.7 8.1 4.3 64.1 66.2 58.4 66.7 72.2 Sintel (outdoor) 28.1 28.0 34.3 27.0 21. 21.5 20.5 11.8 19.5 28.9 18.6 17.5 20.4 19.7 16.1 18.6 18.1 21.4 18.1 16.9 18.8 19.2 19.7 17.2 16.0 34.9 35.7 41.6 35.0 30.7 DIODE-outdoor (outdoor) 76.9 76.4 73.8 75.9 78.8 76.9 75.3 71.7 78.8 78.1 78.6 74.9 77.6 81.1 80.1 41.5 41.8 31.8 42.4 51.9 82.6 82.0 80.8 81.7 84.0 82 80.8 79.7 84.1 82. 82.2 78.1 81.6 84.6 83.0 52.7 52.8 43.9 54.6 62.2 DSINE Lotus GeoWizard StableNormal Ours 14.5 15.9 19.8 16.1 12.8 Table 6. Full quantitative comparison of normal prediction. We report the mean and median angular errors with each cell colored to indicate the best 67.5 63.9 56.6 60.6 70.7 75.4 71.9 68.9 67.5 77. 39.6 32.9 24.0 36.1 43.0 22.0 24.7 27.0 26.9 20.8 and the second . Dataset NYUv2 ScanNet IBims Sintel DIODE-outdoor Metrics w/o IIT w/o C2F Ours Mean 16.1 𝛿11.25 62.5 Mean 16.9 𝛿11.25 64.0 Mean 16.0 𝛿11.25 72.2 Mean 30.7 𝛿11.25 28.9 Mean 20.8 𝛿11.25 43.0 17.8 50.6 18.6 49.4 20.2 56.8 35.9 18.9 23.5 33.7 17.6 50.5 17.8 58.8 18.6 63.9 35.8 22.3 21.6 40. Table 7. Full normal quantitative metrics for ablation. We demonstrate that both the intrinsic-invariant training and coarse-to-fine strategy contributes to accurate normal predictions. Dens3R: Foundation Model for 3D Geometry Prediction 17 Fig. 12. Normal comparison of Kitti dataset. We present more normal comparison of outdoor scenes, our method produces more accurate and sharper normals than previous methods. Fig. 13. Normal comparison with MoGe. We provide more normal comparison with the normal maps derived from the depth map of MoGe [Wang et al. 2025b]. Our method yields sharper and more accurate predictions. Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Method Reproj. Error Precision AUC Median Error (m) Median Error () Pose Precision Pose AUC DUSt3R MASt3R VGGT Ours 125.8 px 57.2 px 48.8 px 30.4 px 45.2% 75.9% 78.9% 82.1% 0.704 0.934 0.789 0.944 1.10 0.46 0.36 0.24 9.4 3.0 3.6 3.4 17.0% 51.7% 57.7% 65.5% 0.344 0.746 0.577 0.852 Table 8. Camera pose estimation results of the Map-free dataset. We report the metrics with each cell colored to indicate the best and the second . Method AUC@5 AUC@10 AUC@20 ROMA VGGT MASt3R Ours 31.8 33.9 62.4 65. 53.4 55.2 77.4 80.3 70.9 73.4 86.9 89.2 Table 9. Two-view matching comparison on ScanNet-1500 Dataset. We report the AUC values with each cell colored to indicate the best and the second . Our method achieves state-of-the-art for two-view matching, surpassing all the previous methods. Method AUC@5 AUC@10 AUC@20 SP+SG SP+LG LoFTR MASt3R Ours 42.2 49.9 52.8 73.3 73.9 61.2 67.0 69.2 84.1 84.4 76.0 80.1 81.2 90.9 91.2 Table 10. Two-view matching comparison on MegaDepth-1500 Dataset. We report the AUC values with each cell colored to indicate the best and the second . Our method also achieves state-of-the-art for the two-view matching using the MegaDepth-1500 Dataset. Setting Compute Cost Memory Cost Network Params w/o Shared w/ Shared 1.362 TFlops 1.362 TFlops 4.6 GB 4.1 GB 737.591 624.152 Table 11. Ablation on shared encoder-decoder structure. We conduct experiments for both of the model on image pairs with 512 resolution. With the shared encoder-decoder structure, our model yields lower memory cost and less network parameters. Fig. 14. Limitations. Despite that our method outperforms previous methods in geometric predictions, the prediction quality for thin structures still require further improvement. Dens3R: Foundation Model for 3D Geometry Prediction 19 Fig. 15. More qualitative comparison of normal map. We provide more normal comparison of both object-centric and human scenes. Dens3R is able to produce more accurate and sharper results 20 Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Fig. 16. More qualitative comparison of normal map. We provide more normal comparison of both indoor and outdoor scenes. Dens3R is able to produce sharper and more accurate results and surpasses previous methods. Dens3R: Foundation Model for 3D Geometry Prediction 21 Fig. 17. Additional depth comparison. We provide more depth comparison with previous methods and our method can predict more accurate and detailed results. Fig. 18. Additional depth comparison with VGGT. We compare our depth prediction results with VGGT and Dens3R demonstrates more robust and accurate predictions. Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Fig. 19. Additional depth comparison of indoor scenes with VGGT. Dens3R demonstrates more accurate results for human depth estimation. Fig. 20. Additional depth comparison of outdoor scenes with VGGT. We compare our depth prediction results of autonomous driving dataset. Our methods achieves much more accurate predictions. Dens3R: Foundation Model for 3D Geometry Prediction 23 Fig. 21. Image-matching visualization. We provide the visualization of our dense and accurate image-matching results. The accuracy is also presented on the upper left of each matching image-pair. 24 Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lv, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, and Chengfei Lv Fig. 22. Additional high-resolution inference comparison. We provide more high-resolution inference results to demonstrate the effectiveness of the proposed position-interpolated rotary positional encoding. We present the pointmap of the main frame and our method accomplishes to prevent the degeneration problem that occured in previous methods. Dens3R: Foundation Model for 3D Geometry Prediction 25 Fig. 23. Multi-view reconstruction results. We demonstrate high-quality 3D reconstruction for various scenarios without known camera poses. The predicted camera poses are also shown on the right of the reconstruction results."
        }
    ],
    "affiliations": [
        "Alibaba Group, China",
        "Shanghai Jiao Tong University, China"
    ]
}