{
    "paper_title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance",
    "authors": [
        "Zun Wang",
        "Jaemin Cho",
        "Jialu Li",
        "Han Lin",
        "Jaehong Yoon",
        "Yue Zhang",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 6 7 8 1 2 . 5 0 5 2 : r EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance Zun Wang Jaemin Cho Jialu Li Han Lin Jaehong Yoon Yue Zhang UNC Chapel Hill {zunwang, jmincho, jialuli, hanlincs}@cs.unc.edu {jhyoon, yuezhan, mbansal}@cs.unc.edu https://zunwang1.github.io/Epic Mohit Bansal"
        },
        {
            "title": "Abstract",
            "content": "Controllable 3D camera trajectories in video diffusion models are highly sought after for content creation, yet remain significant challenge. Recent approaches often create anchor videos (i.e., rendered videos that approximate desired camera motions) to guide diffusion models as structured prior, by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained video diffusion models, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves state-of-the-art performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video (V2V) scenarios. This is compelling as it is trained exclusively on I2V data, where anchor videos are derived from source videos, using only their first frame for visibility referencing."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in video diffusion models (VDMs) [6, 19, 26, 33, 57, 78, 9, 34] have dramatically enhanced the ability to generate dynamic and realistic videos. As video generation becomes increasingly practical and widespread, controllability has emerged as crucial requirement for creating personalized and creative content. Previous works have explored various control signals to guide video generation, such as optical flow [31, 35, 15], object trajectories [71, 65, 80, 52, 12, 60], human poses [43, 39], and depth maps [39, 14]. In particular, controlling camera trajectories during the video generation process has emerged as key research focus, facilitating precise spatio-temporal manipulation essential for downstream applications such as film recapturing [4, 74], virtual cinematography [48], and augmented reality rendering [51]. To achieve precise camera control, recent works [48, 74, 11, 77, 48, 75] have adopted explicit 3D-informed guidance for generation. The core idea is to construct an anchor video (i.e., video that approximates the desired camera motion to guide diffusion model as structured prior), by lifting condition image into 3D point cloud and rendering it along the camera trajectory. Training the camera control module typically requires anchor video and the corresponding full source video as input-output pairs, ideally with perfect geometric alignment. This assumes access to ground-truth 3D point clouds and camera trajectories, which are hard to obtain. As workaround, existing methods synthesize training anchor-source video pairs by using source videos with high-quality camera annotations and estimating point cloud from the first frame via off-the-shelf estimators [58, 68], which is then rendered along the annotated trajectory as the anchor video. However, these estimators often introduce geometric inaccuracies, leading to misaligned regions in the rendered anchor videos (as illustrated in Fig. 1 (a)), making training more challenging, as the model must additionally learn to correct random misalignments beyond filling invisible regions. Moreover, the requirement of annotated camera trajectories from the source video restricts training data to multi-view video datasets such as RealEstate10K [84] and DL3DV [40]. These datasets mainly feature static scenes, thereby limiting the generalization ability of the trained camera control module to more dynamic or diverse real-world settings. To address the issues, we propose EPiC, for learning Efficient and Precise Video Camera control by crafting precisely-aligned training anchor videos with lightweight ControlNet model design (Sec. 4). Our key insight is that anchor videos should be well-aligned with the source videos to make learning both easier and more efficient, transforming the task from one of repairing misaligned content to the simpler task of copying visible regions. Thus, unlike previous approaches that render anchor videos from inaccurate 3D point clouds which often misaligned with the source video and reliant on annotated camera trajectories (Fig. 1 (a) right), we directly synthesize anchor videos by masking the source video based on first-frame visibility (Sec. 4.1), as described in Fig. 1 (b). Specifically, for each subsequent frame, we estimate its pixels trajectory with respect to the first frame from dense optical flow [54], preserving only those pixels that can be reliably traced back to the first frame. Pixels with no valid correspondence in the first frame are masked out. This process effectively mimics the key property of anchor videosall new regions relative to the first frame are invisiblewhile ensuring precise alignment in visible regions (Fig. 1 (b) right). Furthermore, our approach eliminates the need for camera trajectory annotations, allowing anchor videos to be created from any in-the-wild source. Furthermore, in contrast to prior methods that require extensive backbone modifications or heavy finetuning, we introduce lightweight Anchor-ControlNet (Sec. 4.2), which has only 30M parameters (less than 1% parameters of the CogVideoX 5B backbone) and injects anchor-video-based control signals into the generation process with the base model frozen. Unlike previous methods, such as ViewCrafter [75], which condition on the entire anchor video without visibility awareness, we apply visibility-aware masking to the outputs of our Anchor-ControlNet. Specifically, the ControlNets output is added to the latent representation only within the visible regions, leaving the unseen areas untouched. This design simplifies the ControlNets task to copying visible content, while delegating the synthesis of occluded or invisible regions entirely to the base diffusion model. This clear division of responsibility not only reduces learning difficulty but also improves overall generation quality. Combining these components, we demonstrate that anchor-video-based camera control can be learned in highly efficient manner, achieving strong performance with just 5K in-the-wild training videos and 500 training steps, which is less than 10% of the data and iterations used in prior approaches. Extensive experiments demonstrate that EPiC achieves state-of-the-art performance in camera accuracy (e.g., RotErr, TransErr) and camera motion stability (measured by the standard deviation of generated trajectories across different seeds), on image-to-video (I2V) camera control tasks in both indoor and game environments. In addition to being significantly more efficient in data, computation, and model size, EPiC also generalizes effectively to video-to-video (V2V) camera control in zero-shot manner, despite being trained solely on I2V data. Ablation study shows the effectiveness of our anchor video method and ControlNet design. Our contributions are as follows: novel anchor video construction pipeline with visibility-based masking that produces well-aligned anchorsource video pairs without requiring camera trajectory annotations, enabling learning from in-the-wild videos. 2 Figure 1: Comparison of anchor video creation methods for training camera control models. (a) Previous methods ([48, 75]) estimate the 3D point cloud (through depth estimation) using the first frame and render anchor videos with annotated camera trajectories, but suffer from region misalignment due to point-cloud estimation errors while limited to camera-pose annotated data, resulting in inefficient training. (b) Our method creates anchor videos via visibility masking based on first-frame pixel tracking. This not only guarantees accurate geometric alignment but also supports diverse data while largely reducing training costs. We highlight the video regions in red and green boxes to compare the alignment quality. lightweight Anchor-ControlNet architecture with visibility-aware output masking, allowing efficient and precise conditioning on anchor videos. State-of-the-art performance on both I2V and V2V camera control tasks with high efficiency in training, data, and model size compared to state-of-the-art methods."
        },
        {
            "title": "2 Related Work",
            "content": "Image/Text-Based Camera Control in VDMs. Controlling camera trajectories in text-to-video (T2V) generation and I2V generation has recently received increasing attention. common approach is to inject explicit camera parameters (e.g. plücker Embedding) into VDMs [62, 28, 2, 1, 53, 24, 81, 67, 63, 76, 38, 81, 23, 83, 37] for conditioning. However, such parameter-conditioned models often generate world-inconsistent content due to the lack of explicit 3D guidance, especially in outof-distribution scenarios. To mitigate this, recent works have shifted toward guiding generation with point-cloud renderings (anchor videos) as conditions to leverage geometric cues for more accurate camera control [75, 46, 27, 48, 82, 50, 11, 44, 41, 77, 79, 86, 69, 7]. Alternatively, some methods rely on trajectory tracking and encoding as intermediate guidance [31, 17, 66, 21], but such guidance is generally less direct than anchor video conditions and often results in lower accuracy. Despite these advances, rendered anchor videos are often misaligned due to point-cloud estimation errors and require accurate camera annotations, limiting training to datasets like RealEstate10K. In addition, these methods rely on large-scale data to correct misalignment and address limited diversity. To overcome these limitations, we propose masking-based anchor video construction method that achieves precise alignment while eliminating the need for camera annotations during training. We further introduce visibility-aware ControlNet that learns to condition on the anchor video both efficiently and effectively. Video-Based Camera Control. V2V camera control (also known as video recapturing) refers to redirecting camera trajectories in existing videos, enabling new possibilities in filmmaking, augmented reality, and other applications. However, such task presents unique challenges compared to T2V and I2V tasks. Specifically, it is difficult to capture comprehensive 4D information from original videos, making accurately reconstruction challenging. Additionally, obtaining ground-truth paired 4D videos for effective end-to-end training remains challenging. To address these issues, one research direction explores test-time optimization or fine-tuning on specific scenes [72, 77], allowing models to capture individual videos, thus reducing the reliance on large-scale annotated datasets. However, these methods require adaptation or optimization for each new video, resulting in considerable inferencetime overhead. Another direction involves collecting large-scale paired videos from simulators such 3 as Unreal Engine5 [4, 5], the Kubric simulator [20, 55], or Animated Objaverse [16, 64, 18, 73, 56], but simulated videos often lack realism and diversity, reducing generalization to diverse real-world scenarios. The most closely related approaches to ours are [8, 74], which also use structured 3D priors like anchor video to guide video-to-video camera controllable generation. Unlike their methods that require extensive backbone tuning on large-scale, carefully crafted 4D dataset for V2V camera control, our method achieves efficient training using only small amount of I2V data, with minimal backbone modification, yet generalizes well to the V2V setting."
        },
        {
            "title": "3 Background: Video Diffusion Models",
            "content": "We build on the framework of latent video diffusion models (VDMs), which generate videos by iteratively denoising latent representations in compressed space. Given an RGB video RL3HW , pre-trained 3D-VAE is used to encode the video into latent variable = E(x) RLChw, where is the number of input frames and the frame resolution; and L, C, and the sequence length, channel count, and spatial resolution of the respectively. Training diffusion models involves learning the reverse of forward (noising) process. In the forward process, clean latent sample z0 pdata(z) is gradually corrupted with Gaussian noise ϵ (0, I). At each timestep t, the model is trained to predict the zt = noise ϵ from the noisy latent zt conditioned on external signals (e.g., image or text), by minimizing the denoising objective: 1 αt ϵ, αt z0 + Ldenoise = Ez0,t,ϵ,c (cid:104) ϵθ(zt, t, c) ϵ2 2 (cid:105) (1) At inference time, the model progressively denoises from Gaussian noise to the final latent representations ˆz, which is decoded by the 3D VAE decoder to generate the output video: ˆx = D(ˆz). Base Model. We adopt CogVideoX [70] as our base model, which employs DiT-style [45] transformer backbone with full 3D self-attention to jointly model spatial and temporal dependencies across video frames. Specifically, we use the CogVideoX-5B-I2V variant, which supports both image and text conditions for flexible multimodal control during video generation. Guiding VDMs with Anchor Video as Structured Prior for Camera Control. Recent methods [75, 74, 11, 77] have leveraged anchor videos to enable controllable video generation with explicit camera motion control. Anchor videos are typically rendered given camera trajectories from 3D point clouds constructed by lifting single RGB image into 3D space, either using multi-view stereo approaches like DUST3R [59], or by pixel unprojection from estimated monocular depth [68]. These anchor videos provide explicit geometry and camera motion signals, serving as structured prior to guide the video generation to follow the intended camera trajectory. During training, the anchor video is created by lifting the first frame of the source video into 3D and rendering it along the source videos camera trajectory. The model then learns to reconstruct the source video conditioned on the anchor video. During inference, the anchor video is constructed similarly using the input image and user-specified camera trajectory. However, existing methods face two major challenges: (1) Anchor videos derived from 3D point cloud estimations are often imprecise (as shown in Fig. 1 (a)), leading to difficulties during training ( Fig. 5 (a)). The model must not only inpaint missing regions but also correct misaligned visible areas, resulting in inefficient learning. (2) Conditioning on anchor videos in the latent space typically requires fine-tuning the base model or injecting dense additional modules, which increases computational overhead and reduces model generalization  (Table 1)  . To overcome these limitations, we introduce EPiC, novel and efficient framework for learning precise camera control with masking-based anchor video and lightweight Anchor-ControlNet, which we will describe in detail next."
        },
        {
            "title": "4 EPiC: An Efficient Framework for Learning Precise Camera Control",
            "content": "Our key idea is to enable controllable video generation through precise anchor-video guidance. Fig. 2 illustrates the overall architecture of our framework. We first construct precisely aligned anchor and source videos as training input-output pairs with visibility-based masking strategy (Sec. 4.1). Then, we introduce lightweight Anchor-ControlNet that learns to reconstruct the source video from the anchor video efficiently (Sec. 4.2). Finally, we describe our training and inference details (Sec. 4.3). 4 Figure 2: EPiC Model Architecture. (a) shows an overview of our EPiC framework. EPiC supports multiple inference scenarios. (b) and (c) illustrate our I2V inference scenarios using full and masked point clouds, respectively. (d) depicts V2V inference scenario employing dynamic point clouds. 4.1 Constructing Precise Anchor Videos from Source Videos via Visibility-Based Masking We aim to construct anchor videos that are well-aligned with the source videos, making the learning process easier and more efficient. To achieve this, we construct anchor videos through masking strategy that preserves alignment while mimicking the geometric characteristics of point-cloudrendered videos. Specifically, our process consists of the following two steps: Step 1: Pixel-Level Visibility Tracking and Masking. We estimate pixel trajectories in the source video using dense optical flow from the first frame (computed via RAFT [54]) to determine whether each pixel remains visible from the original viewpoint (see Appendix for details). This pixel tracking simulates how content moves or disappears due to viewpoint shifts or occlusion. We provide binary visibility mask for each frame based on such tracking information, retaining only regions consistently traced from the original view and masking out the rest. This process effectively mimics the core property of anchor videos, which excludes newly revealed content while ensuring precise alignment in the visible regions. In cases where the visible region becomes too small due to large viewpoint shifts, we freeze the mask in subsequent frames to prevent further degradation. The masked source video is obtained by applying the visibility mask to the source video, as shown in Fig. 3. Figure 3: Anchor video construction. Step 2: Artifact Injection. major limitation of estimated point clouds is the presence of flyingpixel artifacts, especially around object boundaries (see Fig.2(d), where splatted flying pixels appear near the dogs edges in both point cloud examples). These errors propagate to the anchor video, resulting in flying-pixel artifacts (see Fig.2(d)). To improve robustness, we simulate this flying-pixel effect during training by injecting synthetic dashed rays into the masked anchor video to better align training and inference gap (see Fig. 3 bottom red box). Specifically, we randomly sample direction and draw multiple rays perpendicular to it, with colors sampled from the first frame to ensure temporal consistency. These rays are faded and dashed to resemble flying-pixel artifacts, and are applied only within the visible regions defined by the mask, which helps the model learn to ignore such artifacts during inference. The artifact-injected video is used as the final anchor video for training. 5 4.2 Guiding Video Diffusion with Anchor-ControlNet We introduce Anchor-ControlNet, variant of ControlNet to guide the base video diffusion model using the constructed anchor video as the condition (Fig. 2 (a)). Unlike previous methods such as ViewCrafter [75], which fine-tune the entire model, or Gen3C [48], which fine-tunes all temporal layers of the backbone, we follow the principle of using minimal parameters for downstream adaptation to preserve the models core generation capability [49]. To this end, we adopt lightweight ControlNet design (<30M parameters) and keep the entire backbone frozen during training. Model Architecture. Anchor-ControlNet is lightweight DiT-based module designed to inject anchor video guidance into the base diffusion model. Given an anchor video A, we encode it using the 3D VAE from the backbone model to obtain latent features zanchor. During the reverse diffusion process, the noisy latent zt is concatenated with zanchor along the channel dimension. The combined representation is then patchified and fed into the ControlNet DiT block. The DiT block in Anchor-ControlNet adopts reduced hidden dimension (256 compared to 3072 in the base model) to maintain efficiency. Its output is projected back to match the backbones dimension and added to the corresponding layer in the base DiT model. The projection layer is zero-initialized, following the standard practice in ControlNet, to ensure stable integration at the beginning of training. Visibility-Aware Output Masking. Previous work, such as ViewCrafter [75], condition directly on the entire anchor video without visibility awareness. This forces the model to simultaneously repair misaligned regions and inpaint invisible (black) areas, making the learning task unnecessarily difficult and increasing the risk of incorrect region repair during inference. In contrast, with our aligned anchor videos, we can address these issues by clearly distinguishing visible and invisible content: the ControlNet focuses solely on copying visible content, while the synthesis of occluded or invisible regions is entirely delegated to the base diffusion model. Formally, we require the control signal from the anchor video only affecting visible regions by applying binary visibility mask {0, 1}T hw to the output of the ControlNet. We downsample the invisibility mask derived from the renderings to match the latent resolution, and use it to selectively update the base models latent features (Fig. 2 (a) latent mask). The ControlNet output is first computed as = Proj(DiTctrl([zt, zanchor])), and then added to the base model output at visible positions: (cid:26)DiTbase(zt)i,j + zi,j, ˆzi,j = DiTbase(zt)i,j, if Mi,j = 1 otherwise, (2) where i, are the indices for height and width. This visibility-aware latent fusion is applied during both training and inference, allowing the base model to inpaint disoccluded or invisible regions, while Anchor-ControlNet focuses on controlling the visible content aligned with the anchor video. 4.3 Training and Inference In this section, we outline the training and inference paradigm of our framework. EPiC supports multiple inference scenarios, including I2V and V2V, enabling flexible adaptation to diverse applications. Training. We create our masking-based anchor video from in-the-wild source videos to construct training data. We train the Anchor-ControlNet on our collected anchor and source video pairs by conditioning on the anchor video to predict the source video with the training objective in Eq. 1. Details of our in-the-wild video data are provided in Sec. 5.1. I2V Inference. We consider two distinct inference scenarios for I2V: inference (i) with full point clouds (illustrated in Fig. 2 (b)) and (ii) with masked point clouds (shown in Fig. 2 (c)). In the first scenario, given an input image and target camera trajectory, we first estimate the metric depth using DAv2 [68]. We then unproject the image into 3D point cloud and render the anchor video along the specified camera trajectory. However, this approach produces anchor videos where objects remain static, as rendering is performed from stationary point cloud. For example, the character in Fig. 2 (b) retains the same position and pose throughout the video, limiting its dynamic realism. To overcome this limitation and support dynamic object movement while preserving precise camera control, we propose inference with masked point clouds. Specifically, given single input image, we employ GroundedSAM [47] to identify and segment potentially dynamic objects (e.g., person, animal) from predefined category list. Users may also provide customized category lists or click-based prompts to generate tailored segmentation masks. During 3D point cloud projection, we exclude points within the segmented regions (note that we dilate each mask boundary to capture outlier points 6 Table 1: Quantitative evaluation results on RealEstate10K [85] and MiraData [32] for I2V camera control task. The best numbers are highlighted in bold. The total score is computed by averaging all quality metrics. indicates re-implementation results on the I2V task. Dataset Method Subject Bg Consist Consist Smooth Quality Score Motion Temporal Aesthetic Quality Flicker Total Imaging Quality Rotation Error () Camera Score Transition Error () CamMC () RE10K MIRA CameraCtrl [22] AC3D [1] ViewCrafter [75] EPiC (Ours) 78.35 89.95 82.63 91.96 90.23 81.18 91.62 82. CameraCtrl [22] AC3D [1] ViewCrafter [75] EPiC (Ours) 89.28 78.06 91.75 82.78 79.87 86.56 82.89 91.82 91.25 92.77 92.99 93.43 91.15 92.81 91.55 92.94 97.16 98.30 97.74 98.48 97.30 98.20 96.26 98. 91.99 96.23 93.51 96.47 90.22 94.77 91.71 94.86 43.32 50.97 48.29 51.19 49.35 57.64 54.21 57.94 56.43 65.56 64.33 64.57 51.11 61.51 58.92 61. 2.36 1.01 1.78 0.93 1.12 0.44 1.97 0.86 1.50 0.82 0.86 0.37 0.50 0.16 1.35 0.40 1.05 0.32 0.40 0.11 0.86 0.18 1.17 0.23 5.66 2.06 4.67 1.47 1.62 0.84 4.79 1.53 3.98 1.50 1.13 0.74 1.16 0.34 3.42 1.04 2.95 0.98 0.66 0.22 1.78 0.67 2.10 0.60 near the edges). These masked areas are omitted when rendering the anchor video. Our design allows the reserved background to drive camera motion while leaving the segmented foreground objects unconstrained, enabling natural movement within the generated video. V2V Inference. EPiC also supports V2V camera control (Fig. 2 (d)). Given an input video, we apply DepthCrafter [29] to estimate continuous depths and construct dynamic point cloud. The anchor video is then rendered by replaying the target trajectory over 4D representation. Note that since the base I2V model is frozen, we provide the first frame of the conditional video as input to the model."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Datasets and Baselines. We compare EPiC and recent baselines for I2V setting on the RealCamVid test set [38] from two data source, RealEstate10K (RE10K) [85] and MiraData (MIRA) [32], consisting of mainly indoor scene and gaming environments. For each dataset, we sample 500 videos for evaluation. For baselines, we consider SoTA methods including CameraCtrl [22], AC3D [1] and ViewCrafter [75]. For consistency, we use similar anchor videos per test sample for both ViewCrafter and EPiC. For V2V setting, we follow Gen3C [48] to qualitatively evaluate it using Sora videos [10] and provide quantitative results on Kubric4D [20] scenes in the Appendix. Implementation Details. EPiC is trained on 5,000 videos from the Panda70M dataset [13] for 500 iterations, using total batch size of 16 across 8 40G A100 GPUs. The text condition for the I2V backbone is obtained from the annotated captions in Panda70M. Training takes less than 3 hours with learning rate of 2 104, using the AdamW [42] optimizer. During inference, we apply classifier-free guidance (CFG) with scale of 6.0 for text conditioning. More details are in the Appendix. Table 2: Training efficiency comparison. EPiC achieves better results (see Table 1) with significantly fewer data and steps. Method Evaluation Metrics. For camera-related metrics, we follow prior works [61, 22] and report Rotation Error (RotError), Translation Error (TransError), and CamMC, which respectively measure orientation differences, positional errors, and overall camera pose consistency between the predicted and ground-truth trajectories. To account for randomness, we sample five fixed random seeds per test instance and report the mean and standard deviation of each camera metric. For visual quality, we adopt the evaluation protocol from VBench [30], including metrics such as Subject Consistency, Background Consistency, Motion Smoothness, Temporal Flickering, Aesthetic Quality, and Imaging Quality. Detailed definitions of these metrics are provided in the Appendix. CameraCtrl [22] AC3D [1] ViewCrafter [75] EPiC (Ours) 50K 10K 50K 5K 0.5K # Videos # Iter. Batch Size >70K 70K 630K 32 8 16 16 5.2 Quantitative Evaluation In Table 1, we compare EPiC and recent SOTA camera control methods (CameraCtrl, AC3D, ViewCrafter) on RealEstate10K (RE10K) and MiraData (MIRA). EPiC achieves comparable quality scores to those of prior approaches across both the RE10K and MIRA benchmarks. EPiC attains the highest total score on both datasets (82.63 on RE10K and 82.89 on MIRA), suggesting strong 7 Figure 4: Generated videos comparing with other camera control methods for I2V and V2V tasks. subject/background consistency, smooth motion, and reduced temporal flicker. Furthermore, our method significantly outperforms existing baselines in Camera Score, achieving the lowest rotation and transition errors as well as CamMC. This demonstrates superior fidelity in controlling camera trajectories, along with the best robustness across different seeds, as reflected by the lowest standard deviations. These results highlight EPiCs ability to ensure both high-quality video generation and precise camera control. Notably, as shown in Table 2, EPiC achieves better performance while using less than 10% of the training data and at most 5% of the training steps required by baseline methods. 5.3 Qualitative Examples Fig. 4 compares camera control results from EPiC and SOTA open-source baselines on both I2V and V2V settings. For I2V, we include ViewCrafter [75] and AC3D [1]; for V2V, we compare against GCD [55] and ViewCrafter. AC3D is excluded from the V2V comparison as it is conditioned on single image and cannot follow dense source video motions. AC3D and GCD are conditioned on camera embeddings, whereas ViewCrafter, like ours, is conditioned on anchor videos. I2V Camera Control. As shown in Fig. 4 (a), both ViewCrafter (3rd row) and our method (4th row) are capable of following anchor videos. However, as shown in the ViewCrafter row, it often introduces content inconsistencies (red boxes): for example, it gradually changes painting to glass-like material (3rd column), and produces severe distortions around the sofa (4th column) and chairs (5th column). Such deviations from the anchor video are potentially due to ViewCrafter learning to over-repair misaligned regionsa side effect of being trained with misaligned point-cloud-based anchor videos. In contrast, our method faithfully preserves visible content thanks to learning from aligned anchor videos (shown in green boxes). As baseline without anchor video guidance, AC3D fails to follow the desired camera trajectory. It is worth noting that this example is taken from the RealEstate10K test set, which is an in-domain evaluation setting for both ViewCrafter and AC3D, as they are trained densely with RealEstate10K videos. Even so, our method demonstrates superior accuracy and quality. V2V Camera Control. As shown in Fig.4 (b), while ViewCrafter can roughly follow the anchor video in the background (e.g., beach and trees), it fails to reproduce the foreground motion accurately. In the 2nd column of ViewCrafter row, the dog does not turn its head as in the reference video, and in the 3rd column, the dogs shape appears distorted (e.g. hind leg and nose). GCD produces blurry foregrounds and lacks fidelity. In contrast, our method successfully captures both background and foreground motion, faithfully recapturing the reference video through anchor-video guidance. 5.4 Ablation Studies Effects of Different Types of Anchor Videos. We evaluate the effects of different types of anchor videos in Table 3 and Fig. 5 (a). For fair comparison, we select 5K videos with significant camera movement from RealEstate10K, and obtain the anchor video using either classical point cloud-based method or our visibility-based masking method. We train on point cloud-based anchor videos for 1500 iterations, and masking-based ones for 500 iterations. Table 3 shows that training with point cloud-based anchors leads to higher errors and less stable results with larger standard deviation. 8 Table 3: Results of training with different anchor video types on the RealEstate10K dataset. Anchor Video Type RotErr () TransErr () CamMC () 0.60 0.20 Point cloud-based (1500 iters) Masking-based (500 iters; Ours) 0.40 0.11 1.07 0.39 0.86 0.18 1.45 0.62 1.17 0.23 Figure 5: Qualitative examples for ablation study. In Fig. 5(a), due to misalignment, point cloud-based anchor videos lead to slower convergence, producing significantly higher loss than masking-based ones, even with 3 more training. Qualitative results show that models trained with point cloud-based anchors fail to follow the anchor precisely, producing misaligned geometry (red dashed lines in the point cloud-based row), as the model learns an additional task of repairing visible regions, whereas ours faithfully follow (green dashed lines). Effects of Artifact Injection for Constructing Training Anchor Videos. Fig. 5 (b) demonstrates the effectiveness of artifact injection, as described in Sec. 4.1. Due to point cloud estimation errors, flying pixels often appear when rendering from rapidly changing camera poses, resulting in incorrect guidance even within visible regions. Without artifact injection, the model follows these flawed inputs, leading to similar artifacts at inference (red box). In contrast, with artifact injection, the model learns to repair such artifacts during training, resulting in cleaner outputs (green box). Effects of Visibility-Aware Output Masking. One crucial design in our Anchor-ControlNet is the visibility-aware output masking strategy, which enables the model to control only the visible regions, as described in Sec. 4.2. We conduct an ablation study by training modules without mask awareness, similar to ViewCrafter. As shown in Fig. 5 (c), without output masking, the model is influenced by tearing artifacts rendered from the point cloud, which guide it to generate ambiguous content in these corrupted regions (see red boxes). In contrast, our method excludes such regions from the control signal, allowing the model to generate reasonable and faithful content (green boxes). Effects of Masked Point Clouds for Dynamic Objects. Fig. 5 (d) shows examples of results using the masked point cloud to enable dynamic objects, as described in Sec. 4.3. Without masking (with full point cloud), the generated video is staticthe character (in the red boxes) stands still due to strong 3D guidance in the anchor video. In contrast, masking the point cloud removes control signals from the character, allowing it to move freely and enabling natural walking motion (as shown in the green box)."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose EPiC, an efficient framework that constructs high-quality training anchors by masking source videos based on first-frame visibility, reducing the need for any camera-trajectory annotations and enabling application to in-the-wild videos. We further introduce Anchor-ControlNet, lightweight adapter that learns to copy visible regions from the anchor video, requiring neither large models, extensive data, nor backbone modifications to correct misalignment. EPiC outperforms previous methods in various visual quality and camera control metrics. Qualitative experiments in I2V and V2V scenarios, along with comprehensive ablation studies, also validate our design choices."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, Accelerate Foundation Models Research program, and Bloomberg Data Science PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "[1] S. Bahmani, I. Skorokhodov, G. Qian, A. Siarohin, W. Menapace, A. Tagliasacchi, D. B. Lindell, and S. Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024. [2] S. Bahmani, I. Skorokhodov, A. Siarohin, W. Menapace, G. Qian, M. Vasilkovsky, H.-Y. Lee, C. Wang, J. Zou, A. Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. [3] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [4] J. Bai, M. Xia, X. Fu, X. Wang, L. Mu, J. Cao, Z. Liu, H. Hu, X. Bai, P. Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. [5] J. Bai, M. Xia, X. Wang, Z. Yuan, X. Fu, Z. Liu, H. Hu, P. Wan, and D. Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. Proc. ICLR, 2025. [6] O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada, A. Ephrat, J. Hur, G. Liu, A. Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [7] E. Bernal-Berdun, A. Serrano, B. Masia, M. Gadelha, Y. Hold-Geoffroy, X. Sun, and D. GutierarXiv preprint rez. Precisecam: Precise camera control for text-to-image generation. arXiv:2501.12910, 2025. [8] W. Bian, Z. Huang, X. Shi, Y. Li, F.-Y. Wang, and H. Li. Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. arXiv preprint arXiv:2501.02690, 2025. [9] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [10] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video generation models as world simulators. OpenAI technical reports, 2024. [11] C. Cao, J. Zhou, S. Li, J. Liang, C. Yu, F. Wang, X. Xue, and Y. Fu. Uni3c: Unifying precisely 3d-enhanced camera and human motion controls for video generation. arXiv preprint arXiv:2504.14899, 2025. [12] T.-S. Chen, C. H. Lin, H.-Y. Tseng, T.-Y. Lin, and M.-H. Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023. [13] T.-S. Chen, A. Siarohin, W. Menapace, E. Deyneka, H.-w. Chao, B. E. Jeon, Y. Fang, H.-Y. Lee, J. Ren, M.-H. Yang, and S. Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 10 [14] W. Chen, Y. Ji, J. Wu, H. Wu, P. Xie, J. Li, X. Xia, X. Xiao, and L. Lin. Control-a-video: Controllable text-to-video diffusion models with motion prior and reward feedback learning. arXiv preprint arXiv:2305.13840, 2023. [15] Y. Cong, M. Xu, C. Simon, S. Chen, J. Ren, Y. Xie, J.-M. Perez-Rua, B. Rosenhahn, T. Xiang, and S. He. Flatten: optical flow-guided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. [16] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. [17] W. Feng, J. Liu, P. Tu, T. Qi, M. Sun, T. Ma, S. Zhao, S. Zhou, and Q. He. I2vcontrol-camera: Precise video camera control with adjustable motion strength. arXiv preprint arXiv:2411.06525, 2024. [18] R. Gao, A. Holynski, P. Henzler, A. Brussee, R. Martin-Brualla, P. Srinivasan, J. T. Barron, and B. Poole. Cat3d: Create anything in 3d with multi-view diffusion models. In Proc. NeurIPS, 2024. [19] R. Girdhar, M. Singh, A. Brown, Q. Duval, S. Azadi, S. Rambhatla, A. Shah, X. Yin, D. Parikh, and I. Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning (2023). arXiv preprint arXiv:2311.10709, 2023. [20] K. Greff, F. Belletti, L. Beyer, C. Doersch, Y. Du, D. Duckworth, D. J. Fleet, D. Gnanapragasam, F. Golemo, C. Herrmann, et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37493761, 2022. [21] Z. Gu, R. Yan, J. Lu, P. Li, Z. Dou, C. Si, Z. Dong, Q. Liu, C. Lin, Z. Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. [22] H. He, Y. Xu, Y. Guo, G. Wetzstein, B. Dai, H. Li, and C. Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [23] H. He, Y. Xu, Y. Guo, G. Wetzstein, B. Dai, H. Li, and C. Yang. Cameractrl: Enabling camera control for video diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. [24] H. He, C. Yang, S. Lin, Y. Xu, M. Wei, L. Gui, Q. Zhao, G. Wetzstein, L. Jiang, and H. Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. [25] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [26] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [27] C. Hou, G. Wei, Y. Zeng, and Z. Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. [28] Y. Hou, L. Zheng, and P. Torr. Learning camera movement control from real-world drone videos. arXiv preprint arXiv:2412.09620, 2024. [29] W. Hu, X. Gao, X. Li, S. Zhao, X. Cun, Y. Zhang, L. Quan, and Y. Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. [30] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 11 [31] W. Jin, Q. Dai, C. Luo, S.-H. Baek, and S. Cho. Flovd: Optical flow meets video diffusion model for enhanced camera-controlled video synthesis. arXiv preprint arXiv:2502.08244, 2025. [32] X. Ju, Y. Gao, Z. Zhang, Z. Yuan, X. Wang, A. Zeng, Y. Xiong, Q. Xu, and Y. Shan. Miradata: large-scale video dataset with long durations and structured captions. Advances in Neural Information Processing Systems, 37:4895548970, 2024. [33] L. Khachatryan, A. Movsisyan, V. Tadevosyan, R. Henschel, Z. Wang, S. Navasardyan, and H. Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15954 15964, 2023. [34] D. Kondratyuk, L. Yu, X. Gu, J. Lezama, J. Huang, G. Schindler, R. Hornung, V. Birodkar, J. Yan, M.-C. Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [35] M. Koroglu, H. Caselles-Dupré, G. J. Sanmiguel, and M. Cord. Onlyflow: Optical flow based motion conditioning for video diffusion models. arXiv preprint arXiv:2411.10501, 2024. [36] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [37] L. Li, Z. Zhang, Y. Li, J. Xu, W. Hu, X. Li, W. Cheng, J. Gu, T. Xue, and Y. Shan. Nvcomposer: Boosting generative novel view synthesis with multiple sparse and unposed images. arXiv preprint arXiv:2412.03517, 2024. [38] T. Li, G. Zheng, R. Jiang, T. Wu, Y. Lu, Y. Lin, X. Li, et al. Realcam-i2v: Real-world image-tovideo generation with interactive complex camera control. arXiv preprint arXiv:2502.10059, 2025. [39] H. Lin, J. Cho, A. Zala, and M. Bansal. Ctrl-adapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. arXiv preprint arXiv:2404.09967, 2024. [40] L. Ling, Y. Sheng, Z. Tu, W. Zhao, C. Xin, K. Wan, L. Yu, Q. Guo, Z. Yu, Y. Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. [41] F. Liu, W. Sun, H. Wang, Y. Wang, H. Sun, J. Ye, J. Zhang, and Y. Duan. ReconX: reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. [42] I. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [43] Y. Ma, Y. He, X. Cun, X. Wang, S. Chen, X. Li, and Q. Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. [44] N. Müller, K. Schwarz, B. Rössle, L. Porzi, S. R. Bulò, M. Nießner, and P. Kontschieder. Multidiff: Consistent novel view synthesis from single image. In Proc. CVPR, 2024. [45] W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proc. ICCV, 2023. [46] S. Popov, A. Raj, M. Krainin, Y. Li, W. T. Freeman, and M. Rubinstein. Camctrl3d: Singleimage scene exploration with precise 3d camera control. arXiv preprint arXiv:2501.06006, 2025. [47] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. [48] X. Ren, T. Shen, J. Huang, H. Ling, Y. Lu, M. Nimier-David, T. Müller, A. Keller, S. Fidler, and J. Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. arXiv preprint arXiv:2503.03751, 2025. 12 [49] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023. [50] J. Seo, K. Fukuda, T. Shibuya, T. Narihira, N. Murata, S. Hu, C.-H. Lai, S. Kim, and Y. Mitsufuji. Genwarp: Single image to novel views with semantic-preserving generative warping. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [51] J. Shi, Q. Wang, Z. Li, and P. Wonka. Stereocrafter-zero: Zero-shot stereo video generation with noisy restart. arXiv preprint arXiv:2411.14295, 2024. [52] X. Shi, Z. Huang, F.-Y. Wang, W. Bian, D. Li, Y. Zhang, M. Zhang, K. C. Cheung, S. See, H. Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [53] W. Sun, S. Chen, F. Liu, Z. Chen, Y. Duan, J. Zhang, and Y. Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. [54] Z. Teed and J. Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [55] B. Van Hoorick, R. Wu, E. Ozguroglu, K. Sargent, R. Liu, P. Tokmakov, A. Dave, C. Zheng, and C. Vondrick. Generative camera dolly: Extreme monocular dynamic novel view synthesis. In European Conference on Computer Vision, pages 313331. Springer, 2024. [56] C. Wang, P. Zhuang, T. D. Ngo, W. Menapace, A. Siarohin, M. Vasilkovsky, I. Skorokhodov, S. Tulyakov, P. Wonka, and H.-Y. Lee. 4real-video: Learning generalizable photo-realistic 4d video diffusion. arXiv preprint arXiv:2412.04462, 2024. [57] J. Wang, H. Yuan, D. Chen, Y. Zhang, X. Wang, and S. Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [58] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [59] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud. Dust3r: Geometric 3d vision made easy. In Proc. CVPR, 2024. [60] Z. Wang, J. Li, H. Lin, J. Yoon, and M. Bansal. Dreamrunner: Fine-grained storytelling video generation with retrieval-augmented motion adaptation. arXiv preprint arXiv:2411.16657, 2024. [61] Z. Wang, Z. Yuan, X. Wang, T. Chen, M. Xia, P. Luo, and Y. Shan. Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH, 2024. [62] Z. Wang, Z. Yuan, X. Wang, Y. Li, T. Chen, M. Xia, P. Luo, and Y. Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [63] D. Watson, S. Saxena, L. Li, A. Tagliasacchi, and D. J. Fleet. Controlling space and time with diffusion models. In The Thirteenth International Conference on Learning Representations, 2024. [64] R. Wu, R. Gao, B. Poole, A. Trevithick, C. Zheng, J. T. Barron, and A. Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. Proc. CVPR, 2025. [65] W. Wu, Z. Li, Y. Gu, R. Zhao, Y. He, D. J. Zhang, M. Z. Shou, Y. Li, T. Gao, and D. Zhang. Draganything: Motion control for anything using entity representation. In Proc. ECCV, 2024. [66] Z. Xiao, W. Ouyang, Y. Zhou, S. Yang, L. Yang, J. Si, and X. Pan. Trajectory attention for fine-grained video motion control. arXiv preprint arXiv:2411.19324, 2024. [67] D. Xu, W. Nie, C. Liu, S. Liu, J. Kautz, Z. Wang, and A. Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. [68] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. [69] X. Yang, J. Xu, K. Luan, X. Zhan, H. Qiu, S. Shi, H. Li, S. Yang, L. Zhang, C. Yu, et al. Omnicam: Unified multimodal video generation via camera control. arXiv preprint arXiv:2504.02312, 2025. [70] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [71] S. Yin, C. Wu, J. Liang, J. Shi, H. Li, G. Ming, and N. Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. [72] M. You, Z. Zhu, H. Liu, and J. Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024. [73] H. Yu, C. Wang, P. Zhuang, W. Menapace, A. Siarohin, J. Cao, L. Jeni, S. Tulyakov, and H.-Y. Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. Advances in Neural Information Processing Systems, 37:4525645280, 2024. [74] M. YU, W. Hu, J. Xing, and Y. Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. arXiv preprint arXiv:2503.05638, 2025. [75] W. Yu, J. Xing, L. Yuan, W. Hu, X. Li, Z. Huang, X. Gao, T.-T. Wong, Y. Shan, and Y. Tian. ViewCrafter: taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [76] W. Yu, S. Yin, S. Easterbrook, and A. Garg. Egosim: Egocentric exploration in virtual worlds In The Thirteenth International Conference on Learning with multi-modal conditioning. Representations, 2025. [77] D. J. Zhang, R. Paiss, S. Zada, N. Karnad, D. E. Jacobs, Y. Pritch, I. Mosseri, M. Z. Shou, N. Wadhwa, and N. Ruiz. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. arXiv preprint arXiv:2411.05003, 2024. [78] D. J. Zhang, J. Z. Wu, J.-W. Liu, R. Zhao, L. Ran, Y. Gu, D. Gao, and M. Z. Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024. [79] Z. Zhang, D. Chen, and J. Liao. I2v3d: Controllable image-to-video generation with 3d guidance. arXiv preprint arXiv:2503.09733, 2025. [80] Z. Zhang, J. Liao, M. Li, Z. Dai, B. Qiu, S. Zhu, L. Qin, and W. Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. [81] G. Zheng, T. Li, R. Jiang, Y. Lu, T. Wu, and X. Li. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024. [82] S. Zheng, Z. Peng, Y. Zhou, Y. Zhu, H. Xu, X. Huang, and Y. Fu. Vidcraft3: Camera, object, and lighting control for image-to-video generation. arXiv preprint arXiv:2502.07531, 2025. [83] J. J. Zhou, H. Gao, V. Voleti, A. Vasishta, C.-H. Yao, M. Boss, P. Torr, C. Rupprecht, and V. Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv e-prints, pages arXiv2503, 2025. [84] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely. Stereo magnification: Learning view synthesis using multiplane images. In SIGGRAPH, 2018. [85] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. [86] Z. Zhou, J. An, and J. Luo. Latent-reframe: Enabling camera control for video diffusion model without training. arXiv preprint arXiv:2412.06029, 2024."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Method Details EPiC is trained on subset of 5, 000 videos from the Panda70M dataset [13] for 500 iterations, using total batch size of 16 across 8 40GB A100 GPUs. The text condition for the I2V backbone is obtained from the annotated captions in Panda70M. The subset is selected based on optical flow scores, where we rank videos by their average flow magnitude and retain those with sufficient motion to ensure meaningful camera control training. Training takes less than 3 hours with learning rate of 2 104, using the AdamW [42] optimizer. For our visibility-aware output masking, we apply average pooling to downsample the raw visibility mask to the latent resolution. We train the Anchor-ControlNet at resolution of 480 720 for 49 frames per video (which is the default setting of CogVideoX-5B-I2V [70]), with ControlNet weights set to 1.0. During inference, we apply classifier-free guidance (CFG) [25] with scale of 6.0 for text conditioning. Following AC3D [1], we only inject the ControlNet into the first 40% diffusion steps at inference. We apply max pooling to downsample the raw visibility mask to the latent resolution for visibility-aware output masking. For videos with caption annotations, we directly use the annotations as the textual condition. For those without annotations, we either generate the text condition using advanced vision-language models [36, 3] based on the visual input, or manually write prompts for specific usage scenarios. A.2 Evaluation Metrics We adopt three standard camera pose evaluation metrics to measure the alignment between predicted and ground-truth camera trajectories: Rotation Error (RotErr), Translation Error (TransErr), and Camera Matrix Consistency (CamMC) following MotionCtrl [61] and CameraCtrl [22]. Rotation Error (RotErr) measures the angular deviation (in radians) between the predicted and ground-truth camera rotations: RotErr = arccos (cid:32) tr( RiR ) 1 2 (cid:33) (cid:88) i=1 where Ri and Ri are the predicted and ground-truth rotation matrices at frame i, and is the number of frames in the video. Translation Error (TransErr) computes the L2 distance between normalized translation vectors: (cid:88) TransErr = i=1 where Ti and Ti are the predicted and ground-truth camera translations, and si, si are their respective scene scalesdefined as the L2 distance between the first and farthest frame in each video. Camera Matrix Consistency (CamMC) evaluates overall pose alignment by comparing full camera-to-world matrices with scale normalization: CamMC = (cid:88) i=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:34) (cid:35)34 Ri Ti si (cid:21)34 (cid:20) Ri Ti si (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 where Ri, Ti, and si are the predicted rotation, translation, and scene scale; Ri, Ti, and si are their ground-truth counterparts. For visual quality, we adopt the evaluation protocol from VBench [30], including metrics such as Subject Consistency, Background Consistency, Motion Smoothness, Temporal Flickering, Aesthetic Quality, and Imaging Quality. We refer to VBench [30] for more details. Additional V2V Camera Control Quantitative Evaluation We evaluate our method in the zero-shot video-to-video (V2V) camera control setting on the Kubric4D [55] test set. Specifically, we sample 20 held-out examples and compare our method with 15 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Ti si Ti si (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 Table 4: V2V camera control results on Kubric-4D. Method PSNR SSIM GCD [55] EPiC (Ours) 19.72 19.65 0.59 0.60 Figure 6: Qualitative V2V camera control results of models trained from different data sources. GCD, one of the state-of-the-art methods on Kubric-4D v2v camera control, using its publicly released checkpoint (gradual mode, max 180 rotation) trained on Kubric. For fair comparison, we downsample our generated videos to 256 384. Quantitative results are provided in Tab. 4. Despite performing V2V camera control in zero-shot manner, our method achieves performance comparable to GCD. Moreover, as shown in Fig. 4(b) of the main paper, our model generalizes better to wild domains with complex and dynamic motions."
        },
        {
            "title": "C Ablation Studies",
            "content": "In this section, we provide additional ablations on the training data, the use of Anchor-ControlNet, and the lightweight ControlNet design. C.1 Effects of Training Data Sources key advantage of our method is that it does not rely on camera pose annotations, which enables training on diverse, in-the-wild video datasets beyond multi-view datasets with limited domain coverage. To validate this, we conduct an ablation comparing training on the widely used RealEstate10K [85], which is mulit-view dataset limited to static indoor scenes, with training on Panda70M [13], which contains more diverse and dynamic videos. We report quantitative results in Tab. 5. We observe that both data sources yield comparable performance on RealEstate10K, while training with Panda70M achieves slightly better results on MiraData, likely due to its more diverse training content. However, in the V2V setting, especially when the reference video involves fine-grained motion (e.g., detailed limb articulation), models trained on RealEstate10K fail to generalize effectively. Specifically, as shown in Fig. 6, the crabs legs exhibit intricate, localized motion patterns. While the model trained on Panda70M is able to precisely follow these details by following the anchor video, the model trained on RealEstate10K can only capture coarse moving direction, failing to reproduce the fine motion in the crabs legs. This limitation is likely due to the lack of diverse and dynamic videos in the RealEstate10K dataset, which mainly consists of indoor scenes that differ significantly from the domain of the crab video. 16 Table 5: Ablation of using different data sources for training EPiC. Training Data Source RealEstate10K MiraData Rot. Err () Trans. Err () CamMC () Rot. Err () Trans. Err () CamMC () RealEstate10K [85] Panda70M [13] 0.43 0.10 0.40 0. 0.84 0.22 0.86 0.18 1.06 0.25 1.17 0.23 0.73 0.32 0.66 0.22 1.88 0.75 1.78 0.67 2.21 0.65 2.10 0.60 Table 6: Ablation on lightweight ControlNet design. Our selected setting is bolded (no pretrain, 256 hidden dimension, 8 layers). Pretrained Hidden Dimension #Layers RealEstate10K Rot. Err Trans. Err CamMC 3072 256 256 256 21 21 8 2 0.42 0.38 0.40 0. 0.83 0.90 0.86 1.32 1.19 1.21 1.17 1.89 C.2 Effects of Lightweight Anchor-ControlNet Design We ablate the design of our lightweight ControlNet in Tab. 6. Specifically, we compare injecting into half of the backbone layers (21 layers here (CogVideoX-5B-I2V has 42 layers totally), as in the default ControlNet setting) with and without using pretrained weights, and further study the effect of reducing the number of injection layers. Our results show that using high-dimensional feature space (3072) with pretrained CogVideoX weights performs comparably to using no pretraining and much smaller dimension (256), suggesting that the region-copying control is relatively easy to learn. In addition, reducing the number of injection layers to 8 does not hurt performance, while further reducing it to only 2 layers results in noticeable decreased control accuracy. Based on these findings, we adopt the most cost-effective configuration: injecting into 8 layers with control dimension of 256. C.3 Training Anchor-ControlNet only vs. Full-Finetuning As ViewCrafter [75] directly fine-tunes the entire backbone, we compare our ControlNet-based training strategy with this standard full-finetuning approach to highlight the efficiency of our design. Specifically, we encode the anchor video directly as the conditioning input,replacing the original image-conditioned latent, and full-finetune the base model for 1000 iterations. As shown in Fig. 7, despite training for twice as many steps, the output remains blurry and noisy. We attribute this to mismatch in the conditioning distribution: replacing image-based conditioning with anchor-video conditioning disrupts the pre-learned first-frame embedding priors, making end-to-end fine-tuning less effective and harder to optimize. In contrast, our ControlNet design enables effective anchorvideo conditioning without modifying the backbone, by treating the anchor video as an external control signal. Figure 7: Results of training with Anchor-ControlNet compared to full-finetuning."
        },
        {
            "title": "D Robustness to Different Random Seeds",
            "content": "We demonstrate the robustness of our method in Fig. 8. Given conditioned image, we use specific object (highlighted with white box) as the reference for spatial consistency. For AC3D, varying the random seed leads to noticeable changes in the spatial positions of other objects (highlighted in red 17 Figure 8: Robustness to different random seeds Figure 9: Examples of text-guided scene control. boxes). This is especially evident in Seed 3, where the generated objects position drifts significantly from the reference, failing to maintain spatial alignment. In contrast, our method consistently preserves the spatial relationship across different seeds. The objects in our generated videos (highlighted in green boxes) remain stable and aligned with the referenced object, demonstrating strong robustness to seed variation. Additional Applications: Fine-Grained Control We present several additional applications demonstrating different types of fine-grained control based on single image with our anchor-video conditioning. Text-Guided Scene Control. Our model effectively demonstrates dynamic text-guided video generation capabilities, enabling flexible scene synthesis across different styles while maintaining temporal and spatial consistency. Fig. 9 illustrates examples of our text-guided scene control. Starting from an initial frame with fixed forward camera trajectory, our method generates subsequent video frames conditioned on different textual prompts. The newly prompted objects are introduced into the generated scene (highlighted in red text and boxes), while the objects present in the initial frame remain consistently visible throughout the video (highlighted in green text and boxes). Object 3D Trajectory Control via Anchor Video Manipulation. We also demonstrate the flexibility of our method in enabling 3D trajectory control for objects. The input is usually 3D trajectory (e.g., indicating moving backwards with 2 meters) applied to specific object (e.g. corgi). We encode the desired motion into the anchor video by manipulating it based on the 3D trajectory. Specifically, following similar approach to our inference setup with masked point clouds, we use GroundedSAM [47] to obtain the segmentation mask of the corgi, extract the point cloud corresponding to the corgi, and isolate the background point cloud without the corgi. We then simulate motion by translating the corgis point cloud backward by 2 meters relative to the background over time (we dont move the background point cloud), producing dynamic point cloud sequence for rendering. In this setup, we focus solely on trajectory control, thus, we remain the camera trajectory static during rendering. The resulting anchor video depicts the corgi moving backward and serves as strong 18 Figure 10: Examples of object 3D trajectory control via anchor video manipulation. guidance. Our results are illustrated in Fig. 10, where our approach successfully generates scenarios in which the corgi steps backward. In contrast, AC3D, which conditions only on camera embeddings, which lack explicit trajectory information, fails to generate this backward motion even with stepping backward included in the textual condition. This comparison highlights the strength of our method in interpreting and executing precise object-level movements in 3D space, showcasing its superior capability for controllable video generation. Regional Animation. Our method is also applicable to regional image animation, where motion is localized to specific area based on short text prompt and user-provided click or prior mask. To achieve this, we directly create the anchor video by repeating the source image and applying the regional mask to each frame. As shown in Fig.11 (a), given the prompt the corgi shakes its head,\" with corresponding corgi head mask, our method generates video in which only the corgis head moves while the rest of its body remains still, accurately following both the textual instruction and the specified region. In contrast, Fig.11 (b) highlights failure case of AC3Dwhen the intended motion is for the palm tree to move, AC3D incorrectly animates the corgi instead. Our method, however, successfully isolates and animates the palm tree, demonstrating its ability to localize motion precisely based on regional guidance and text. This showcases the fine-grained spatial control ability enabled by our approach."
        },
        {
            "title": "F Additional Visual Examples",
            "content": "Examples of Constructed Anchor Videos. We present examples of high-quality anchor videos constructed from Panda70M source videos in Fig. 12. Our method consistently maintains spatial coherence and masks regions that were initially not visible in the first frame, even when objects exhibit significant movements across frames, while the Panda70M provides both diverse and dynamic video data. Such high-quality and diverse anchor videos further help the efficient learning by our model. Examples of I2V Camera Control. Fig. 13 shows additional qualitative examples of I2V camera control. Given diverse image inputs and variety of camera trajectories, our method consistently generates high-quality videos that accurately follow the specified motions. The results demonstrate effective camera control across multiple scene types, including gaming (firstand third-person), outdoor, close-up views, etc. Moreover, it effectively maintains dynamic objects and preserves scene coherence across different scenarios, highlighting the flexibility and robustness of our approach in handling diverse I2V scenarios. Examples of V2V Camera Control. We provide additional visualizations demonstrating our V2V camera control capabilities. As illustrated in examples of Fig 14, our method successfully generates high-quality videos given challenging source videos such as movie clips, which typically contain 19 Figure 11: Examples of Regional Animation complex objects and dynamic movements. This underscores the robustness and versatility of our approach in handling realistic and demanding V2V scenarios."
        },
        {
            "title": "G Limitations and Broader Impacts",
            "content": "EPiC trains lightweight adapter on backbone video diffusion model. As such, its performance, output quality, and potential visual artifacts are inherently influenced by the capabilities and limitations of the underlying backbone models it relies on. For instance, if the backbone model struggles with generating complex, rare, or previously unseen scenes and objects, then EPiC may also exhibit suboptimal generation results. This dependency highlights the importance of selecting strong and reliable backbone models when applying EPiC. While EPiC can benefit numerous applications in video generation, similar to other visual generation frameworks, it can also be used for potentially harmful purposes (e.g., creating false information or misleading videos). Therefore, it should be used with caution in real-world applications. 20 Figure 12: Examples of constructed anchor videos. The source video and corresponding captions are obtained from Panda70M. 21 Figure 13: Qualitative examples of I2V camera control with diverse image inputs and camera trajectories. 22 Figure 14: Qualitative examples of V2V camera control on movie clips with multiple kinds of camera trajectories."
        }
    ],
    "affiliations": [
        "UNC Chapel Hill"
    ]
}